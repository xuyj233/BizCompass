{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e005b9e",
   "metadata": {},
   "source": [
    "## FPB process\n",
    "\n",
    "input: Sentences_AllAgree.txt\n",
    "output:\n",
    "{ \"sentence\": \"Pharmaceuticals group Orion Corp reported a fall in its third-quarter earnings that were hit by larger expenditures on R&D and marketing .\",\n",
    "  \"label\": \"negative\"\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df570cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已写出 2264 条记录 -> ./Analyst_FPB.json\n"
     ]
    }
   ],
   "source": [
    "import sys, json\n",
    "\n",
    "input_path = './Sentences_AllAgree.txt'\n",
    "output_path = './Analyst_FPB.json'\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "data = []\n",
    "for line in lines:\n",
    "    s = line.strip()\n",
    "    if not s or \"@\" not in s:\n",
    "        continue\n",
    "    sentence, label = s.rsplit(\"@\", 1)\n",
    "    sentence = sentence.strip()\n",
    "    label = label.strip()\n",
    "    if sentence and label:\n",
    "        data.append({\"sentence\": sentence, \"label\": label})\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as w:\n",
    "    json.dump(data, w, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"已写出 {len(data)} 条记录 -> {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4d36a",
   "metadata": {},
   "source": [
    "## MA process\n",
    "\n",
    "input: \n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"hf://datasets/TheFinAI/flare-ma/data/test-00000-of-00001-56159619c0ddecc5.parquet\")\n",
    "\n",
    "data format：\n",
    "{id, query, answer, text, choices, gold}\n",
    "\n",
    "output:\n",
    "{ \"Instruction\": \"In this task, you will be given Mergers and Acquisitions (M&A) news articles or tweets. Your task is to classify each article or tweet based on whether the mentioned deal was completed or remained a rumour. Your response should be a single word - either 'complete' or 'rumour' - representing the outcome of the deal mentioned in the provided text.\",\n",
    "  \"Text\": \"A tweet by StockTradersNet suggesting Berkshire Hathaway is looking to fully take over Southwest Airlines at a price of USD 75.00 apiece pushed up the market value of the carrier by 4.1 per cent yesterday. The trading portal noted at the time the possible upcoming bid, which would be a third higher than yesterday’s close, is unconfirmed. However, the rumour comes less than a week after Warren Buffett said the group is hunting for an “elephant-sized acquisition” and last year he told CNBC he would not rule out owning an entire airline. In a letter to shareholders regarding financial results in fiscal 2018, Buffet noted: “Even at our ages of 88 and 95 – I’m the young one – that prospect [a large-scale acquisition] is what causes my heart [. . .] to beat faster. “Just writing about the possibility of a huge purchase has caused my pulse rate to soar.” In response to queries by the media, Southwest said in a statement: “There has been speculation circulating that Warren Buffett might be looking to acquire an airline for some time, and that Southwest might be a good fit. “As a policy, we do not comment on speculations but appreciate Berkshire’s continued support of Southwest.” T Rowe Price analyst Andrew Davis dismissed the rumour due to the way it appeared, though he said it is not out of left field to think Berkshire may buy any of the four airlines it holds stakes in “one day”. Such an acquisition would come on the heels of the group writing down USD 3.00 billion on its investments, arising almost entirely from its equity interest in Kraft Heinz. The food powerhouse revealed a USD 15.40 billion impairment on its biggest brands, including Kraft natural cheese, Oscar Mayer cold cuts and the Canada retail business.\",\n",
    "  \"Answer\": \"rumour\"\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a5d9798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\VIS24\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 records in sum -> ./Trader_MA.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "INSTRUCTION = \"In this task, you will be given Mergers and Acquisitions (M&A) news articles or tweets. Your task is to classify each article or tweet based on whether the mentioned deal was completed or remained a rumour. Your response should be a single word - either 'complete' or 'rumour' - representing the outcome of the deal mentioned in the provided text.\"\n",
    "\n",
    "in_path = \"hf://datasets/TheFinAI/flare-ma/data/test-00000-of-00001-56159619c0ddecc5.parquet\"\n",
    "out_path = \"./Trader_MA.json\"\n",
    "\n",
    "df = pd.read_parquet(in_path)\n",
    "df = df[['text', 'answer']].dropna(subset=['text', 'answer'])\n",
    "\n",
    "items: List[Dict[str, str]] = []\n",
    "for _, row in df.iterrows():\n",
    "    text = str(row['text']).strip()\n",
    "    ans = str(row['answer']).strip()\n",
    "    if not text or not ans:\n",
    "        continue\n",
    "    items.append({\n",
    "        \"Instruction\": INSTRUCTION,\n",
    "        \"Text\": text,\n",
    "        \"Answer\": ans\n",
    "    })\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as w:\n",
    "    json.dump(items, w, ensure_ascii=False, indent=2)\n",
    "    w.write(\"\\n\")\n",
    "\n",
    "print(f\"{len(items)} records in sum -> {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63361ba",
   "metadata": {},
   "source": [
    "## FOMC process\n",
    "\n",
    "input: \n",
    "from modelscope.msdatasets import MsDataset\n",
    "ds =  MsDataset.load('TheFinAI/finben-fomc', subset_name='default', split='test')\n",
    "\n",
    "data format：\n",
    "{id, query, answer, text, choices, gold}\n",
    "\n",
    "query -> Specific instruction + Text\n",
    "\n",
    "output:\n",
    "{ \"Specific instruction\": \"Study the sentence below from a central bank's briefing. Categorize it as HAWKISH if it promotes a tightening of monetary policy, DOVISH if it represents an easing of monetary policy, or NEUTRAL if the stance is nonpartisan. Your response should return only HAWKISH, DOVISH, or NEUTRAL.\",\n",
    "  \"Text\": \"The early days of stabilization policy in the 1950s taught monetary policymakers not to attempt to offset what are likely to be temporary fluctuations in inflation.15 Indeed, responding may do more harm than good, particularly in an era where policy rates are much closer to the effective lower bound even in good times.\",\n",
    "  \"Answer\": \"dovish\"\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f737de8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 19:01:13,896 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from finben-fomc. Please make sure that you can trust the external codes.\n",
      "2025-08-18 19:01:15,971 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from TheFinAI/finben-fomc. Please make sure that you can trust the external codes.\n",
      "2025-08-18 19:01:15,971 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from TheFinAI/finben-fomc. Please make sure that you can trust the external codes.\n",
      "2025-08-18 19:01:15,971 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from TheFinAI/finben-fomc. Please make sure that you can trust the external codes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496 records in sum -> ./Trader_MA.json\n"
     ]
    }
   ],
   "source": [
    "from modelscope.msdatasets import MsDataset\n",
    "import sys\n",
    "import json\n",
    "ds =  MsDataset.load('TheFinAI/finben-fomc', subset_name='default', split='test')\n",
    "\n",
    "def split_query(query: str):\n",
    "    if not isinstance(query, str):\n",
    "        return \"\", \"\"\n",
    "    key = \"Text:\"\n",
    "    idx = query.find(key)\n",
    "    if idx == -1:\n",
    "        return query.strip(), \"\"\n",
    "    specific = query[:idx].strip()\n",
    "    text_after = query[idx + len(key):].strip()\n",
    "    return specific, text_after\n",
    "\n",
    "items = []\n",
    "for sample in ds:\n",
    "    query = sample.get('query', '')\n",
    "    text_field = (sample.get('text') or '').strip()\n",
    "    answer = (sample.get('answer') or sample.get('gold') or '').strip()\n",
    "\n",
    "    specific, text_from_query = split_query(query)\n",
    "    final_text = text_field if text_field else text_from_query\n",
    "\n",
    "    if not final_text or not answer:\n",
    "        continue\n",
    "\n",
    "    items.append({\n",
    "        \"Specific instruction\": specific,\n",
    "        \"Text\": final_text,\n",
    "        \"Answer\": answer\n",
    "    })\n",
    "\n",
    "with open('./FOMC.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(f\"{len(items)} records in sum -> {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ebae84",
   "metadata": {},
   "source": [
    "## CCFraud process\n",
    "\n",
    "input: \n",
    "splits = {'train': 'data/train.parquet', 'validation': 'data/valid.parquet', 'test': 'data/test.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/daishen/cra-ccf/\" + splits[\"train\"])\n",
    "\n",
    "data format：\n",
    "{id, query, answer, text, choices, gold}\n",
    "\n",
    "output:\n",
    "{ \"Text\": \"The client is a female, the state number is 35, the number of cards is 1, the credit balance is 5000, the number of transactions is 10, the number of international transactions is 4, the credit limit is 4.\",\n",
    "  \"Answer\": \"good\"\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88e2767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7974 records in sum -> ./Trader_MA.json\n"
     ]
    }
   ],
   "source": [
    "import sys, json\n",
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'data/train.parquet', 'validation': 'data/valid.parquet', 'test': 'data/test.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/daishen/cra-ccf/\" + splits[\"train\"])\n",
    "df = df[['text', 'answer']].dropna(subset=['text', 'answer'])\n",
    "\n",
    "items = []\n",
    "for _, r in df.iterrows():\n",
    "    text = str(r['text']).strip()\n",
    "    ans = str(r['answer']).strip()\n",
    "    if text and ans:\n",
    "        items.append({\"Text\": text, \"Answer\": ans})\n",
    "\n",
    "output_path = './CCFraud.json'\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(f\"{len(items)} records in sum -> {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e11a5",
   "metadata": {},
   "source": [
    "## CCFraud process\n",
    "\n",
    "input: \n",
    "splits = {'train': 'data/train.parquet', 'validation': 'data/valid.parquet', 'test': 'data/test.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/daishen/cra-taiwan/\" + splits[\"train\"])\n",
    "\n",
    "data format：\n",
    "{id, query, answer, text, choices, gold}\n",
    "\n",
    "output:\n",
    "{ \"Text\": \"The client has attributes: Bankrupt?: 0.499, ROA(C) before interest and depreciation before interest: 0.543, ROA(A) before interest and % after tax: 0.545, ROA(B) before interest and depreciation after tax: 0.599, Operating Gross Margin: 0.599, Realized Sales Gross Margin: 0.999, Operating Profit Rate: 0.797, Pre-tax net Interest Rate: 0.809, After-tax net Interest Rate: 0.304, Non-industry income and expenditure/revenue: 0.782, Continuous interest rate (after tax): 4850000000.000, Operating Expense Rate: 0.000, Research and development expense rate: 0.484, Cash flow rate: 0.000, Interest-bearing debt interest rate: 0.250, Tax rate (A): 0.180, Net Value Per Share (B): 0.180, Net Value Per Share (A): 0.180, Net Value Per Share (C): 0.218, Persistent EPS in the Last Four Seasons: 0.324, Cash Flow Per Share: 0.015, Revenue Per Share (Yuan ¥): 0.099, Operating Profit Per Share (Yuan ¥): 0.173, Per Share Net profit before tax (Yuan ¥): 0.022, Realized Sales Gross Profit Growth Rate: 0.848, Operating Profit Growth Rate: 0.689, After-tax Net Profit Growth Rate: 0.689, Regular Net Profit Growth Rate: 0.218, Continuous Net Profit Growth Rate: 6080000000.000, Total Asset Growth Rate: 0.000, Net Value Growth Rate: 0.264, Total Asset Return Growth Rate Ratio: 0.383, Cash Reinvestment %: 0.017, Current Ratio: 0.009, Quick Ratio: 0.632, Interest Expense Ratio: 0.004, Total debt/Total net worth: 0.087, Debt ratio %: 0.913, Net worth/Assets: 0.005, Long-term fund suitability ratio (A): 0.374, Borrowing dependency: 0.006, Contingent liabilities/Net worth: 0.098, Operating profit/Paid-in capital: 0.172, Net profit before tax/Paid-in capital: 0.396, Inventory and accounts receivable/Net value: 0.076, Total Asset Turnover: 0.002, Accounts Receivable Turnover: 0.003, Average Collection Days: 9940000000.000, Inventory Turnover Rate (times): 6200000000.000, Fixed Assets Turnover Frequency: 0.021, Net Worth Turnover Rate (times): 0.029, Revenue per person: 0.397, Operating profit per person: 0.059, Allocation rate per person: 0.789, Working Capital to Total Assets: 0.127, Quick Assets/Total Assets: 0.212, Current Assets/Total Assets: 0.058, Cash/Total Assets: 0.010, Quick Assets/Current Liability: 0.013, Cash/Current Liability: 0.024, Current Liability to Assets: 0.358, Operating Funds to Liability: 0.277, Inventory/Working Capital: 0.019, Inventory/Current Liability: 0.247, Current Liabilities/Liability: 0.734, Working Capital/Equity: 0.327, Current Liabilities/Equity: 0.033, Long-term Liability to Current Assets: 0.933, Retained Earnings to Total Assets: 0.002, Total income/Total expense: 0.007, Total expense/Assets: 0.000, Current Asset Turnover Rate: 6230000000.000, Quick Asset Turnover Rate: 0.594, Working capitcal Turnover Rate: 8130000000.000, Cash Turnover Rate: 0.672, Cash Flow to Sales: 0.636, Fixed Assets to Assets: 0.247, Current Liability to Liability: 0.327, Current Liability to Equity: 0.120, Equity to Long-term Liability: 0.658, Cash Flow to Total Assets: 0.464, Cash Flow to Liability: 0.614, CFO to Assets: 0.317, Cash Flow to Equity: 0.017, Current Liability to Current Assets: 0.000, Liability-Assets Flag: 0.802, Net Income to Total Assets: 0.007, Total assets to GNP price: 0.623, No-credit Interval: 0.599, Gross Profit to Sales: 0.840, Net Income to Stockholder's Equity: 0.278, Liability to Equity: 0.027, Degree of Financial Leverage (DFL): 0.566, Interest Coverage Ratio (Interest expense to EBIT): 1.000, Net Income Flag: 0.044.\",\n",
    "  \"Answer\": \"no\"\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df54aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\VIS24\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4773 records in sum -> ./Taiwan_Economic_Journal.json\n"
     ]
    }
   ],
   "source": [
    "import sys, json\n",
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'data/train.parquet', 'validation': 'data/valid.parquet', 'test': 'data/test.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/daishen/cra-taiwan/\" + splits[\"train\"])\n",
    "df = df[['text', 'answer']].dropna(subset=['text', 'answer'])\n",
    "\n",
    "items = []\n",
    "for _, r in df.iterrows():\n",
    "    text = str(r['text']).strip()\n",
    "    ans = str(r['answer']).strip()\n",
    "    if text and ans:\n",
    "        items.append({\"Text\": text, \"Answer\": ans})\n",
    "\n",
    "output_path = './Taiwan_Economic_Journal.json'\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(items, f, ensure_ascii=False, indent=2)\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(f\"{len(items)} records in sum -> {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951b073",
   "metadata": {},
   "source": [
    "### Add an ID to each instance in CCFraud, FPB, Taiwan, FOMC, MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec6e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done:../Trader_Market_Trend_Analysis/MA1.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "input_path = '../Trader_Market_Trend_Analysis/MA.json'\n",
    "output_path = '../Trader_Market_Trend_Analysis/MA1.json'\n",
    "\n",
    "with open(input_path,\"r\", encoding=\"utf-8\") as f:\n",
    "    data: List[Dict[str, Any]] = json.load(f)\n",
    "    \n",
    "if not isinstance(data, list):\n",
    "    raise ValueError(\"The structure should be list\")\n",
    "\n",
    "new_data: List[Dict[str, Any]] = []\n",
    "for idx, item in enumerate(data, start=1):\n",
    "    if not isinstance(item, dict):\n",
    "        raise ValueError(\"The structure for array should be dict\")\n",
    "    new_item: Dict[str, Any] = {\"ID\": idx}\n",
    "    for k, v in item.items():\n",
    "        if k == \"ID\":\n",
    "            continue\n",
    "        new_item[k] = v\n",
    "    new_data.append(new_item)\n",
    "    \n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_data, f, ensure_ascii=False, indent=2)\n",
    "    f.write(\"\\n\")\n",
    "print(f\"Done:{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f5bfd5",
   "metadata": {},
   "source": [
    "Process the CIKM18 dataset for Asset pricing - Trader task\n",
    "\n",
    "input:\n",
    "splits = {'train': 'data/train-00000-of-00001-f71a7dda3fae0889.parquet', 'test': 'data/test-00000-of-00001-e1663a0932037903.parquet', 'valid': 'data/valid-00000-of-00001-b105ab56855808e4.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/TheFinAI/flare-sm-cikm/\")\n",
    "\n",
    "output: \n",
    "{'ID': 1, 'Specific Instruction': 'Reflect on the provided data and tweets to anticipate if the closing price of $c is going to increase or decrease at 2017-01-18. Kindly respond with either Rise or Fall.', 'Text': 'date,open,high,low,close,adj-close,inc-5,inc-10,inc-15,inc-20,inc-25,inc-30\n",
    "2017-01-03,0.1,0.8,-1.4,2.0,2.0,-0.8,-0.4,-0.8,-1.3,-2.5,-3.3 ...', 'Answer': 'Fall'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e47dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3396 records from train\n",
      "Processed 4539 records from test\n",
      "Processed 4970 records from valid\n",
      "Data saved to ../Trader_Asset_Pricing/CIKM18.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "splits = {\n",
    "    'train': 'data/train-00000-of-00001-f71a7dda3fae0889.parquet', \n",
    "    'test': 'data/test-00000-of-00001-e1663a0932037903.parquet', \n",
    "    'valid': 'data/valid-00000-of-00001-b105ab56855808e4.parquet'\n",
    "}\n",
    "base_url = \"hf://datasets/TheFinAI/flare-sm-cikm/\"\n",
    "\n",
    "all_data = []\n",
    "current_id = 1\n",
    "\n",
    "def split_query(query: str) -> tuple[str, str]:\n",
    "    if not isinstance(query, str):\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    context_key = \"Context:\"\n",
    "    idx = query.find(context_key)\n",
    "    \n",
    "    if idx == -1:\n",
    "        return query.strip(), \"\"\n",
    "    \n",
    "    specific_instruction = query[:idx].strip()\n",
    "    \n",
    "    return specific_instruction\n",
    "\n",
    "for split_name, split_path in splits.items():\n",
    "    full_path = base_url + split_path\n",
    "    df = pd.read_parquet(full_path)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        query = str(row.get('query', '')).strip()\n",
    "        text = str(row.get('text', '')).strip()\n",
    "        answer = str(row.get('answer', '')).strip()\n",
    "        \n",
    "        specific_instruction = split_query(query)\n",
    "        \n",
    "        data_item = {\n",
    "            'ID': current_id,\n",
    "            'Specific Instruction': specific_instruction,\n",
    "            'Text': text,\n",
    "            'Answer': answer\n",
    "        }\n",
    "        \n",
    "        all_data.append(data_item)\n",
    "        current_id += 1\n",
    "        \n",
    "    print(f\"Processed {len(all_data)} records from {split_name}\")\n",
    "\n",
    "output_path = '../Trader_Asset_Pricing/CIKM18.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "    f.write('\\n')\n",
    "\n",
    "print(f\"Data saved to {output_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f6c60",
   "metadata": {},
   "source": [
    "Process the EDTSUM dataset for financial document analysis - Consultant task\n",
    "\n",
    "input:\n",
    "df = pd.read_parquet(\"hf://datasets/TheFinAI/flare-edtsum/data/test-00000-of-00001-f8ea79e581c9134b.parquet\")\n",
    "\n",
    "output: \n",
    "{'ID': 1, 'Specific Instruction': 'You are given a text that consists of multiple sentences. Your task is to perform abstractive summarization on this text. Use your understanding of the content to express the main ideas and crucial details in a shorter, coherent, and natural sounding text.', 'Text': 'LONDON--(BUSINESS WIRE)--Technavio has been monitoring the all-season tire market in Europe and it is poised to grow by USD 3.42 billion during 2020-2024, progressing at a CAGR of almost 9% during the forecast period. The report offers an up-to-date analysis regarding the current market scenario, latest trends and drivers, and the overall market environment...', 'Answer': 'All-season Tire Market in Europe to Reach USD 3.42 Billion by 2024, Bridgestone Corp. and Continental AG Emerge as Key Contributors to Growth | Technavio'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e8f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                              query  \\\n",
      "0  edtsum0  You are given a text that consists of multiple...   \n",
      "1  edtsum1  You are given a text that consists of multiple...   \n",
      "2  edtsum2  You are given a text that consists of multiple...   \n",
      "3  edtsum3  You are given a text that consists of multiple...   \n",
      "4  edtsum4  You are given a text that consists of multiple...   \n",
      "\n",
      "                                              answer  \\\n",
      "0  All-season Tire Market in Europe to Reach USD ...   \n",
      "1  Loop Energy Applauds Skywell and Joint Venture...   \n",
      "2  Chocolate Market - Growth, Trends, and Forecas...   \n",
      "3  ABC Technologies Holdings Inc. Files Final Pro...   \n",
      "4         FDJ: 2021 Financial Communication Calendar   \n",
      "\n",
      "                                                text  \n",
      "0  LONDON--(BUSINESS WIRE)--Technavio has been mo...  \n",
      "1  Achievementis an important step in expanding L...  \n",
      "2  LONDON--(BUSINESS WIRE)--The chocolate market ...  \n",
      "3  TORONTO--(BUSINESS WIRE)--ABC Technologies Hol...  \n",
      "4  BOULOGNE-BILLANCOURT, France--(BUSINESS WIRE)-...  \n",
      "Data saved to ../Consultant_Financial_Document_Analysis/EDTSUM.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"test-00000-of-00001-f8ea79e581c9134b.parquet\")\n",
    "all_data = []\n",
    "current_id = 1\n",
    "\n",
    "def split_query(query: str) -> tuple[str, str]:\n",
    "    if not isinstance(query, str):\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    context_key = \"Text:\"\n",
    "    idx = query.find(context_key)\n",
    "    \n",
    "    if idx == -1:\n",
    "        return query.strip(), \"\"\n",
    "    \n",
    "    specific_instruction = query[:idx].strip()\n",
    "    \n",
    "    return specific_instruction\n",
    "\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    query = str(row.get('query', '')).strip()\n",
    "    text = str(row.get('text', '')).strip()\n",
    "    answer = str(row.get('answer', '')).strip()\n",
    "    \n",
    "    specific_instruction = split_query(query)\n",
    "    \n",
    "    data_item = {\n",
    "        'ID': current_id,\n",
    "        'Specific Instruction': specific_instruction,\n",
    "        'Text': text,\n",
    "        'Answer': answer\n",
    "    }\n",
    "    \n",
    "    all_data.append(data_item)\n",
    "    current_id += 1\n",
    "\n",
    "output_path = '../Consultant_Financial_Document_Analysis/EDTSUM.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "    f.write('\\n')\n",
    "\n",
    "print(f\"Data saved to {output_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477ac70",
   "metadata": {},
   "source": [
    "Process the FNXL dataset for financial document analysis - Analyst task\n",
    "\n",
    "input:\n",
    "df = pd.read_parquet(\"hf://datasets/TheFinAI/flare-fnxl/data/test-00000-of-00001-35c7f3863a79cbce.parquet\")\n",
    "\n",
    "output: \n",
    "{'ID': 1, 'Specific Instruction': 'In the task of Financial Numeric Extreme Labelling (FNXL), your job is to identify and label the semantic role of each token in a sentence. The labels can include O...', 'Text': 'Compensation expense recognized for all of the Company's deferred compensation plans was $0.6 million, $1.7 million and $0.4 million in 2019, 2018 and 2017, respectively.', 'Answer': 'Compensation:O\n",
    "expense:O\n",
    "recognized:O\n",
    "for:O\n",
    "all:O\n",
    "of:O\n",
    "the:O\n",
    "Company:O'...} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00df0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../Analyst_Financial_Document_Analysis/FNXL.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datasets import load_dataset\n",
    "\n",
    "df = pd.read_parquet(\"hf://datasets/TheFinAI/flare-fnxl/data/test-00000-of-00001-35c7f3863a79cbce.parquet\")\n",
    "\n",
    "all_data = []\n",
    "current_id = 1\n",
    "\n",
    "def split_query(query: str) -> tuple[str, str]:\n",
    "    if not isinstance(query, str):\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    context_key = \"Text:\"\n",
    "    idx = query.find(context_key)\n",
    "    \n",
    "    if idx == -1:\n",
    "        return query.strip(), \"\"\n",
    "    \n",
    "    specific_instruction = query[:idx].strip()\n",
    "    \n",
    "    return specific_instruction\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    query = str(row.get('query', '')).strip()\n",
    "    text = str(row.get('text', '')).strip()\n",
    "    answer = str(row.get('answer', '')).strip()\n",
    "    label = str(row.get('label', '')).strip()\n",
    "    \n",
    "    specific_instruction = split_query(query)\n",
    "    \n",
    "    data_item = {\n",
    "        'ID': current_id,\n",
    "        'Specific Instruction': specific_instruction,\n",
    "        'Text': text,\n",
    "        'Answer': answer,\n",
    "        'Label': label\n",
    "    }\n",
    "    \n",
    "    all_data.append(data_item)\n",
    "    current_id += 1\n",
    "\n",
    "output_path = '../Analyst_Financial_Document_Analysis/FNXL.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "    f.write('\\n')\n",
    "\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4d5f4",
   "metadata": {},
   "source": [
    "Process the FinQA dataset for financial document analysis - Analyst task\n",
    "\n",
    "output: \n",
    "{'ID': 1, 'Specific Instruction': 'Please answer the given financial question based on the context. Context: interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) . if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million...', 'Question': 'what is the the interest expense in 2009?', 'Answer': 3.8} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d31fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../Analyst_Financial_Document_Analysis/FinQA.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datasets import load_dataset\n",
    "\n",
    "df = pd.read_parquet(\"C:/Users/user/Downloads/train-00000-of-00001-76a97cdb03ed8a9c.parquet\")\n",
    "\n",
    "all_data = []\n",
    "current_id = 1\n",
    "\n",
    "def split_query(query: str) -> tuple[str, str]:\n",
    "    if not isinstance(query, str):\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    context_key = \"Question:\"\n",
    "    idx = query.find(context_key)\n",
    "    \n",
    "    if idx == -1:\n",
    "        return query.strip(), \"\"\n",
    "    \n",
    "    specific_instruction = query[:idx].strip()\n",
    "    \n",
    "    return specific_instruction\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    query = str(row.get('query', '')).strip()\n",
    "    text = str(row.get('text', '')).strip()\n",
    "    answer = str(row.get('answer', '')).strip()\n",
    "    \n",
    "    specific_instruction = split_query(query)\n",
    "    \n",
    "    data_item = {\n",
    "        'ID': current_id,\n",
    "        'Specific Instruction': specific_instruction,\n",
    "        'Question': text,\n",
    "        'Answer': answer\n",
    "    }\n",
    "    \n",
    "    all_data.append(data_item)\n",
    "    current_id += 1\n",
    "\n",
    "output_path = '../Analyst_Financial_Document_Analysis/FinQA.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "    f.write('\\n')\n",
    "\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb5079",
   "metadata": {},
   "source": [
    "Process the TATQA dataset for financial document analysis - Analyst task\n",
    "\n",
    "output: \n",
    "{'ID': 1, 'Specific Instruction': 'Please answer the given financial question based on the context.\n",
    "Context: |||Years Ended September 30,||\n",
    "||2019|2018|2017|...', 'Question': '\t\n",
    "What is the amount of total sales in 2019?', 'Answer': \"$1,496.5\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01feb75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../Analyst_Financial_Document_Analysis/TATQA.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datasets import load_dataset\n",
    "\n",
    "df = pd.read_parquet(\"C:/Users/user/Downloads/test-00000-of-00001-aba4ef149be97d17.parquet\")\n",
    "\n",
    "all_data = []\n",
    "current_id = 1\n",
    "\n",
    "def split_query(query: str) -> tuple[str, str]:\n",
    "    if not isinstance(query, str):\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    context_key = \"Question:\"\n",
    "    idx = query.find(context_key)\n",
    "    \n",
    "    if idx == -1:\n",
    "        return query.strip(), \"\"\n",
    "    \n",
    "    specific_instruction = query[:idx].strip()\n",
    "    \n",
    "    return specific_instruction\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    query = str(row.get('query', '')).strip()\n",
    "    text = str(row.get('text', '')).strip()\n",
    "    answer = str(row.get('answer', '')).strip()\n",
    "    \n",
    "    specific_instruction = split_query(query)\n",
    "    \n",
    "    data_item = {\n",
    "        'ID': current_id,\n",
    "        'Specific Instruction': specific_instruction,\n",
    "        'Question': text,\n",
    "        'Answer': answer\n",
    "    }\n",
    "    \n",
    "    all_data.append(data_item)\n",
    "    current_id += 1\n",
    "\n",
    "output_path = '../Analyst_Financial_Document_Analysis/TATQA.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "    f.write('\\n')\n",
    "\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071dd3f",
   "metadata": {},
   "source": [
    "Process the FinRED dataset for financial document analysis - Analyst task\n",
    "\n",
    "output: \n",
    "{'ID': 1, 'Specific Instruction': 'In the sentences extracted from financial agreements in U.S. SEC filings, identify the named entities that represent a person ('PER'), an organization ('ORG'), or a location ('LOC'). The required answer format is: 'entity name, entity type'., 'Text': 'This LOAN AND SECURITY AGREEMENT dated January 27 , 1999 , between SILICON VALLEY BANK (\" Bank \"), a California - chartered bank with its principal place of business at 3003 Tasman Drive , Santa Clara , California 95054 with a loan production office located at 40 William St ., Ste .', 'Answer': 'SILICON VALLEY BANK, ORG\n",
    "Bank, ORG\n",
    "California, LOC\n",
    "bank, ORG\n",
    "3003 Tasman Drive, LOC\n",
    "Santa Clara, LOC\n",
    "California, LOC\n",
    "40 William St, LOC'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed44124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../Analyst_Financial_Document_Analysis/FinRED.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from datasets import load_dataset\n",
    "\n",
    "df = pd.read_parquet(\"C:/Users/user/Downloads/train-00000-of-00001-e198e05854da56b5.parquet\")\n",
    "\n",
    "all_data = []\n",
    "current_id = 1\n",
    "\n",
    "def split_query(query: str) -> tuple[str, str]:\n",
    "    if not isinstance(query, str):\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    context_key = \"Text:\"\n",
    "    idx = query.find(context_key)\n",
    "    \n",
    "    if idx == -1:\n",
    "        return query.strip(), \"\"\n",
    "    \n",
    "    specific_instruction = query[:idx].strip()\n",
    "    \n",
    "    return specific_instruction\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    query = str(row.get('query', '')).strip()\n",
    "    text = str(row.get('text', '')).strip()\n",
    "    answer = str(row.get('answer', '')).strip()\n",
    "    \n",
    "    specific_instruction = split_query(query)\n",
    "    \n",
    "    data_item = {\n",
    "        'ID': current_id,\n",
    "        'Specific Instruction': specific_instruction,\n",
    "        'Text': text,\n",
    "        'Answer': answer\n",
    "    }\n",
    "    \n",
    "    all_data.append(data_item)\n",
    "    current_id += 1\n",
    "\n",
    "output_path = '../Analyst_Financial_Document_Analysis/FinRED.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "    f.write('\\n')\n",
    "\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb55ce",
   "metadata": {},
   "source": [
    "Process the SECQUE-Risk dataset for risk management - Analyst task\n",
    "\n",
    "output: \n",
    "{'ID': 1, \n",
    "'Text': 'unanticipated or adverse changes in depositor behavior, which could negatively affect JPMorgan Chase's broader asset and liability management strategy, and\n",
    "•a reduction in the value of JPMorgan Chase's mortgage servicing rights (\"MSRs\") asset, decreasing revenues...',\n",
    "'Question': 'What is the likelihood and severity of the company facing credit risk from its counterparties?'\n",
    "'Answer': 'The likelihood of facing credit risk from counterparties is moderate, given the company's stringent credit assessment processes. However, the severity could be high if a major counterparty defaults, leading to significant financial losses.'\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3691e8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../Analyst_Risk_Management/SECQUE.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import typing as t\n",
    "import os\n",
    "\n",
    "df = pd.read_json(\"C:/Users/user/Downloads/SECQUE_benchmark_Risk.jsonl\", lines=True)\n",
    "\n",
    "df = df.reindex(columns=[\n",
    "\t\t\"context_markdown_without_headers\",\n",
    "\t\t\"ground_truth_answer\",\n",
    "\t\t\"Question\",\n",
    "\t])\n",
    "df = df.fillna(\"\")\n",
    "df = df.astype({\n",
    "    \"context_markdown_without_headers\": \"string\",\n",
    "    \"ground_truth_answer\": \"string\",\n",
    "    \"Question\": \"string\",\n",
    "})\n",
    "\n",
    "df.insert(0, \"ID\", range(1, len(df) + 1))\n",
    "\n",
    "df = df.rename(columns={\n",
    "    \"context_markdown_without_headers\": \"Text\",\n",
    "    \"ground_truth_answer\": \"Answer\"\n",
    "})\n",
    "\n",
    "output_path = '../Analyst_Risk_Management/SECQUE.json'\n",
    "\n",
    "records: t.List[t.Dict[str, t.Any]] = df[[\"ID\", \"Text\", \"Question\", \"Answer\"]].to_dict(orient=\"records\")\n",
    "out_dir = os.path.dirname(os.path.abspath(output_path))\n",
    "if out_dir and not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VIS24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
