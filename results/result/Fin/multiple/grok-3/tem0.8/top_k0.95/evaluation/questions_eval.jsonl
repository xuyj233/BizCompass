{"ID": 2, "Question": "### Background\n\n**Research Question.** This case examines the satirical conclusion of the paper: whether patently absurd variables, such as celestial phenomena, can be shown to have statistically significant power to predict asset returns, thereby critiquing the standard empirical asset pricing methodology.\n\n**Setting / Data-Generating Environment.** The fundamental theorem of asset pricing states that the absence of arbitrage is equivalent to the existence of a positive stochastic discount factor (SDF), `m_{t+1}`, such that `E_t[m_{t+1} R_{i,t+1}^e] = 0` for any excess return `R_{i,t+1}^e`. This implies that expected returns are determined by the covariance of returns with the SDF: `E_t[R_{i,t+1}^e] = -\\frac{\\operatorname{Cov}_t(m_{t+1}, R_{i,t+1}^e)}{E_t[m_{t+1}]}`. In rational models, the SDF is high in 'bad times' (when marginal utility of consumption is high) and low in 'good times'.\n\n---\n\n### Data / Model Specification\n\nThe paper finds a statistically significant relationship where lagged sunspot activity `X_t` predicts future excess returns `R_{i,t+1}^e`:\n  \nR_{i,t+1}=a+b X_{t}+\\varepsilon_{t+1}\n \n\n---\n\n### Question\n\nFor the observed statistical relationship between sunspots and asset returns to be consistent with a rational, no-arbitrage asset pricing model, which of the following statements must be true or are valid implications? Select all that apply.", "Options": {"A": "A valid SDF, `m_{t+1}`, must be perfectly correlated with the number of sunspots observed at time `t`.", "B": "The absence of arbitrage requires that the expected excess return on any asset predicted by sunspots must be zero.", "C": "Sunspots must be a priced risk factor, meaning sunspot activity is correlated with a state variable that systematically affects investors' marginal utility.", "D": "Given the economic implausibility of sunspots as a priced risk factor, the significant regression results more likely reflect methodological flaws like data mining rather than a true economic relationship."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the deep theoretical critique from the source QA. It uses the Premise/Assumption Packaging strategy to test understanding of the conditions required for a variable to be a genuine priced risk factor within the SDF framework. Options A and B correctly state the two possible interpretations: either sunspots are a real (but implausible) risk factor, or the methodology is flawed. Distractor C is a Conceptual Error, misstating the no-arbitrage condition. Distractor D is a Formula Misuse/Oversimplification, as the theory requires a non-zero covariance, not perfect correlation.", "qid": "2", "question": "### Background\n\n**Research Question.** This case examines the satirical conclusion of the paper: whether patently absurd variables, such as celestial phenomena, can be shown to have statistically significant power to predict asset returns, thereby critiquing the standard empirical asset pricing methodology.\n\n**Setting / Data-Generating Environment.** The fundamental theorem of asset pricing states that the absence of arbitrage is equivalent to the existence of a positive stochastic discount factor (SDF), `m_{t+1}`, such that `E_t[m_{t+1} R_{i,t+1}^e] = 0` for any excess return `R_{i,t+1}^e`. This implies that expected returns are determined by the covariance of returns with the SDF: `E_t[R_{i,t+1}^e] = -\\frac{\\operatorname{Cov}_t(m_{t+1}, R_{i,t+1}^e)}{E_t[m_{t+1}]}`. In rational models, the SDF is high in 'bad times' (when marginal utility of consumption is high) and low in 'good times'.\n\n---\n\n### Data / Model Specification\n\nThe paper finds a statistically significant relationship where lagged sunspot activity `X_t` predicts future excess returns `R_{i,t+1}^e`:\n  \nR_{i,t+1}=a+b X_{t}+\\varepsilon_{t+1}\n \n\n---\n\n### Question\n\nFor the observed statistical relationship between sunspots and asset returns to be consistent with a rational, no-arbitrage asset pricing model, which of the following statements must be true or are valid implications? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 3, "Question": "### Background\n\n**Research Question.** This case examines the proposed link between local weather, investor mood, and asset prices, and critically evaluates whether this relationship is causal or merely a proxy for seasonality.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for anomaly strategies from July 1973 to December 2012. The predictor is the monthly high temperature recorded at the Central Park weather station in New York City.\n\n**Variables & Parameters.**\n- `R_t`: Monthly excess return of an anomaly strategy (dimensionless).\n- `X_{t-1}`: The highest temperature (°F) recorded at Central Park in the preceding month.\n- `b`: Sensitivity of `R_t` to `X_{t-1}`.\n\n---\n\n### Data / Model Specification\n\nThe analysis uses the following OLS regression:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Anomaly Performance by NYC Temperature (Monthly Excess Returns, %)**\n\n| Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A: Better after cold weather** | | | | |\n| Size | 0.28 | -0.06 | -0.60 | 1.05 |\n| | [1.48] | [-4.54] | [-2.18] | [4.10] |\n\n*Note: `t`-statistics are in brackets. `a_H` and `a_L` are average returns after hot (above median) and cold (below median) months. The Size strategy is long small caps, short large caps.*\n\n---\n\n### Question\n\nBased on the provided information regarding the **Size** strategy, select all of the following statements that are correct. Assume the standard deviation of monthly high temperatures is 15°F.", "Options": {"A": "The paper's 'Bozeman critique'—that weather in remote locations has similar predictive power—suggests the NYC temperature finding is likely a proxy for seasonality (e.g., the January effect) rather than a direct effect of local trader mood.", "B": "The difference in average returns between cold and hot months (`a_L - a_H`) is 0.45%.", "C": "The Size premium is positive and statistically significant following both hot and cold months, but larger after cold months.", "D": "Based on the linear model's coefficient `b`, a one-standard-deviation drop in temperature is predicted to increase the monthly Size premium by 0.90%."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test multiple facets of the source QA: calculation, interpretation, and critique. Option A requires a simple calculation based on the provided data (Computational Judgment). Option B assesses comprehension of the paper's core satirical argument against naive behavioral stories. Distractor C is a Factual Error, as the premium is negative and significant after hot months. Distractor D is a Sign Error in the calculation of the difference between `a_L` and `a_H`.", "qid": "3", "question": "### Background\n\n**Research Question.** This case examines the proposed link between local weather, investor mood, and asset prices, and critically evaluates whether this relationship is causal or merely a proxy for seasonality.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for anomaly strategies from July 1973 to December 2012. The predictor is the monthly high temperature recorded at the Central Park weather station in New York City.\n\n**Variables & Parameters.**\n- `R_t`: Monthly excess return of an anomaly strategy (dimensionless).\n- `X_{t-1}`: The highest temperature (°F) recorded at Central Park in the preceding month.\n- `b`: Sensitivity of `R_t` to `X_{t-1}`.\n\n---\n\n### Data / Model Specification\n\nThe analysis uses the following OLS regression:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Anomaly Performance by NYC Temperature (Monthly Excess Returns, %)**\n\n| Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A: Better after cold weather** | | | | |\n| Size | 0.28 | -0.06 | -0.60 | 1.05 |\n| | [1.48] | [-4.54] | [-2.18] | [4.10] |\n\n*Note: `t`-statistics are in brackets. `a_H` and `a_L` are average returns after hot (above median) and cold (below median) months. The Size strategy is long small caps, short large caps.*\n\n---\n\n### Question\n\nBased on the provided information regarding the **Size** strategy, select all of the following statements that are correct. Assume the standard deviation of monthly high temperatures is 15°F.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 1, "Question": "### Background\n\n**Research Question.** This case examines the satirical conclusion of the paper: whether patently absurd variables, such as celestial phenomena, can be shown to have statistically significant power to predict asset returns, thereby critiquing the standard empirical asset pricing methodology.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for anomaly strategies from July 1973 to December 2012. Predictors include the angular separation of planets (aspects) and the number of observed sunspots.\n\n**Variables & Parameters.**\n- `R_t`: Monthly excess return of an anomaly strategy (dimensionless).\n- `X_{t-1}`: The value of the celestial predictor at the start of month `t`.\n- `P_1`, `P_2`: Orbital periods of two planets (years).\n- `S`: Synodic period of the two planets (years).\n- `b`: The sensitivity of `R_t` to `X_{t-1}`.\n\n---\n\n### Data / Model Specification\n\nThe core predictive regression is:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \nThe synodic period `S` (the time between conjunctions) of two planets with orbital periods `P_1` and `P_2` is given by:\n  \nS = \\frac{P_1 \\times P_2}{|P_1 - P_2|} \\quad \\text{(Eq. (2))}\n \nThe paper states the orbital periods of Mars and Saturn are 1.881 and 29.46 years, respectively.\n\n**Table 1: Celestial Phenomena and Anomaly Performance (Monthly Excess Returns, %)**\n\n| Predictive Variable / Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Angle between Mars and Saturn** | | | | |\n| Market | 0.51 | 0.53 | 0.98 | 0.04 |\n| | [2.37] | [2.29] | [3.22] | [0.15] |\n| **Observed number of sunspots** | | | | |\n| UMD (Momentum) | 0.66 | 0.01 | 0.99 | 0.32 |\n| | [3.14] | [2.25] | [3.36] | [1.08] |\n\n*Note: `t`-statistics are in brackets. For the angle predictor, `H` means the planets are opposed and `L` means they are in conjunction. For sunspots, `H` and `L` refer to above/below median activity.*\n\n---\n\n### Question\n\nBased on the provided information, select all of the following statements that are correct.", "Options": {"A": "The Market anomaly performs significantly better when Mars and Saturn are in conjunction (`a_L`), a state where their 'energies are strongly blended'.", "B": "According to the paper's narrative, high sunspot activity is associated with higher momentum strategy returns because it impairs investors' ability to process information.", "C": "The Momentum (UMD) strategy earns a higher average return when sunspot activity is below the median (`a_L`) compared to when it is above the median (`a_H`).", "D": "The synodic period of the Mars-Saturn aspect, calculated using Eq. (2), is approximately 2.01 years."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses core comprehension and calculation from the source QA. It uses the Atomic Decomposition strategy to test the calculation of the synodic period and the interpretation of the paper's satirical narrative for sunspots. Distractor C is a Conceptual Opposite, reversing the finding in Table 1 (market performs better when planets are opposed). Distractor D is a direct factual contradiction of the values for `a_H` and `a_L` for the UMD strategy in Table 1.", "qid": "1", "question": "### Background\n\n**Research Question.** This case examines the satirical conclusion of the paper: whether patently absurd variables, such as celestial phenomena, can be shown to have statistically significant power to predict asset returns, thereby critiquing the standard empirical asset pricing methodology.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for anomaly strategies from July 1973 to December 2012. Predictors include the angular separation of planets (aspects) and the number of observed sunspots.\n\n**Variables & Parameters.**\n- `R_t`: Monthly excess return of an anomaly strategy (dimensionless).\n- `X_{t-1}`: The value of the celestial predictor at the start of month `t`.\n- `P_1`, `P_2`: Orbital periods of two planets (years).\n- `S`: Synodic period of the two planets (years).\n- `b`: The sensitivity of `R_t` to `X_{t-1}`.\n\n---\n\n### Data / Model Specification\n\nThe core predictive regression is:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \nThe synodic period `S` (the time between conjunctions) of two planets with orbital periods `P_1` and `P_2` is given by:\n  \nS = \\frac{P_1 \\times P_2}{|P_1 - P_2|} \\quad \\text{(Eq. (2))}\n \nThe paper states the orbital periods of Mars and Saturn are 1.881 and 29.46 years, respectively.\n\n**Table 1: Celestial Phenomena and Anomaly Performance (Monthly Excess Returns, %)**\n\n| Predictive Variable / Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Angle between Mars and Saturn** | | | | |\n| Market | 0.51 | 0.53 | 0.98 | 0.04 |\n| | [2.37] | [2.29] | [3.22] | [0.15] |\n| **Observed number of sunspots** | | | | |\n| UMD (Momentum) | 0.66 | 0.01 | 0.99 | 0.32 |\n| | [3.14] | [2.25] | [3.36] | [1.08] |\n\n*Note: `t`-statistics are in brackets. For the angle predictor, `H` means the planets are opposed and `L` means they are in conjunction. For sunspots, `H` and `L` refer to above/below median activity.*\n\n---\n\n### Question\n\nBased on the provided information, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "\"B,D\"", "model_answer": ["B", "D"], "error": null}}
{"ID": 5, "Question": "### Background\n\n**Research Question.** This case investigates whether the political party of the sitting U.S. president has predictive power for the performance of well-known asset pricing anomalies.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for various long-short anomaly strategies from January 1961 to December 2012.\n\n---\n\n### Data / Model Specification\n\nTwo alternative OLS regression models are used, where `X_{t-1}` is a dummy variable equal to 1 for a Democratic president and 0 for a Republican president.\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n  \nR_{t}=a_{D}X_{t-1}+a_{R}(1-X_{t-1})+\\varepsilon_{t} \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Anomaly Performance by Presidential Party (Monthly Excess Returns, %)**\n\n| Strategy | `m` | `b` | `a_D` | `a_R` |\n| :--- | :--- | :--- | :--- | :--- |\n| Idiosyncratic Volatility | 0.36 | -1.95 | -0.74 | 1.21 |\n| | [1.21] | [-3.23] | [-1.63] | [3.04] |\n\n*Note: `t`-statistics are in brackets. The Idiosyncratic Volatility strategy is long low-volatility stocks and short high-volatility stocks.*\n\n---\n\n### Question\n\nBased on the provided models and data for the **Idiosyncratic Volatility** strategy, select all of the following statements that are correct.", "Options": {"A": "The strategy earns a statistically significant 1.21% per month under Republican presidents, which the paper links to a 'flight to quality' narrative.", "B": "The intercept `a` in Eq. (1) is equal to `a_D` from Eq. (2), representing the average return under Democratic presidents.", "C": "The strategy's performance is statistically indistinguishable between Democratic and Republican administrations, as shown by the t-statistic on `m`.", "D": "The coefficient `b = -1.95%` in Eq. (1) represents the difference in the strategy's average monthly return between Democratic and Republican administrations (`a_D - a_R`)."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test the relationship between dummy variable regression specifications and the interpretation of their coefficients. Option A correctly identifies the meaning of the slope coefficient `b`. Option B correctly interprets the coefficient `a_R` and links it to the paper's narrative. Distractor C is a Conceptual Error regarding the interpretation of the intercept in a dummy variable regression (`a` equals `a_R`, not `a_D`). Distractor D is a Conceptual Opposite, as the highly significant t-statistic on `b` shows a strong statistical difference between administrations.", "qid": "5", "question": "### Background\n\n**Research Question.** This case investigates whether the political party of the sitting U.S. president has predictive power for the performance of well-known asset pricing anomalies.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for various long-short anomaly strategies from January 1961 to December 2012.\n\n---\n\n### Data / Model Specification\n\nTwo alternative OLS regression models are used, where `X_{t-1}` is a dummy variable equal to 1 for a Democratic president and 0 for a Republican president.\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n  \nR_{t}=a_{D}X_{t-1}+a_{R}(1-X_{t-1})+\\varepsilon_{t} \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Anomaly Performance by Presidential Party (Monthly Excess Returns, %)**\n\n| Strategy | `m` | `b` | `a_D` | `a_R` |\n| :--- | :--- | :--- | :--- | :--- |\n| Idiosyncratic Volatility | 0.36 | -1.95 | -0.74 | 1.21 |\n| | [1.21] | [-3.23] | [-1.63] | [3.04] |\n\n*Note: `t`-statistics are in brackets. The Idiosyncratic Volatility strategy is long low-volatility stocks and short high-volatility stocks.*\n\n---\n\n### Question\n\nBased on the provided models and data for the **Idiosyncratic Volatility** strategy, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 34, "Question": "### Background\n\nA study evaluates several forecasting models for the ASE 20 index. After an initial comparison, the performance is re-evaluated under realistic institutional trading conditions, including transaction costs and the application of leverage to normalize volatility.\n\n### Data / Model Specification\n\nThe study applies a leverage factor to each strategy to normalize its volatility to a common target. The final performance, including transaction costs of 0.14% per trade and leverage costs of 4% p.a., is reported. The table below excerpts the final results for the Genetic Programming (GP) algorithm and the standard Recurrent Neural Network (RNN) from the paper's Table 8.\n\n**Table 1. Final Trading Performance with Costs and Leverage**\n| Model | Ann. Return (incl. costs) (%) | Positions Taken (ann.) | Leverage Factor | Transaction and Leverage Costs (%) |\n| :--- | :--- | :--- | :--- | :--- |\n| GP algorithm | 33.34 | 67 | 1.10 | 9.95 |\n| RNN | 4.64 | 147 | 1.055 | 19.11 |\n\n---\n\nBased on the data in **Table 1** and the provided cost structure, select all of the following statements that are correct.\n", "Options": {"A": "The leverage cost for the GP algorithm is 0.40% per annum.", "B": "The GP algorithm's superior net return is primarily driven by its significantly higher leverage factor compared to the RNN.", "C": "The transaction costs for the RNN model are more than double the transaction costs for the GP algorithm.", "D": "The RNN model's total costs (19.11%) consume over 80% of its gross (pre-cost) annualized return of 23.75%."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to perform multi-step calculations and draw valid conclusions from a data table, focusing on the practical impact of trading frictions. Strategy: Computational Judgment. Distractor Logic: Option A requires calculating transaction costs for both models (RNN: 147 * 0.14% = 20.58%; GP: 67 * 0.14% = 9.38%) and comparing them. Option B requires calculating the leverage cost for the GP model ((1.10 - 1) * 4% = 0.40%). Option C requires a percentage calculation (19.11 / 23.75 ≈ 80.5%). Option D is a 'Conceptual Opposite' distractor; the leverage factors are very similar, and the performance difference is driven by trading frequency and signal quality, not leverage.", "qid": "34", "question": "### Background\n\nA study evaluates several forecasting models for the ASE 20 index. After an initial comparison, the performance is re-evaluated under realistic institutional trading conditions, including transaction costs and the application of leverage to normalize volatility.\n\n### Data / Model Specification\n\nThe study applies a leverage factor to each strategy to normalize its volatility to a common target. The final performance, including transaction costs of 0.14% per trade and leverage costs of 4% p.a., is reported. The table below excerpts the final results for the Genetic Programming (GP) algorithm and the standard Recurrent Neural Network (RNN) from the paper's Table 8.\n\n**Table 1. Final Trading Performance with Costs and Leverage**\n| Model | Ann. Return (incl. costs) (%) | Positions Taken (ann.) | Leverage Factor | Transaction and Leverage Costs (%) |\n| :--- | :--- | :--- | :--- | :--- |\n| GP algorithm | 33.34 | 67 | 1.10 | 9.95 |\n| RNN | 4.64 | 147 | 1.055 | 19.11 |\n\n---\n\nBased on the data in **Table 1** and the provided cost structure, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 17, "Question": "### Background\n\n**Research Question.** How is the fair periodic premium for a unit-linked endowment with guarantees determined by equating the expected present values of premiums and benefits?\n\n**Setting.** An insurance contract for a policyholder aged `x` at inception, with `n` yearly premium payments. The valuation assumes independence between financial markets and mortality risk.\n\n### Data / Model Specification\n\nThe principle of actuarial equivalence states that the expected present value (EPV) of premiums must equal the EPV of benefits. The detailed equation for the fair premium `P` is:\n\n  \nP \\cdot \\underbrace{\\sum_{k=0}^{n-1} P(0,k) \\cdot_k p_x}_{\\text{EPV of Annuity}} = \\underbrace{\\sum_{k=1}^{n} {_{k-1}p_x q_{x+k-1}} \\cdot B_k + _n p_x \\cdot B_n}_{\\text{EPV of Benefits}} \\quad \\text{(Eq. 1)}\n \n\nwhere `B_k` is the time-0 value of the benefit paid at time `k`, given by `B_k = G \\cdot P(0,k) + C_0(X(k),G)`. The term `_k p_x` is the probability of surviving `k` years, and `_{k-1}p_x q_{x+k-1}` is the probability of dying in year `k`.\n\n---\n\nAn insurer is pricing this contract but discovers their mortality tables are outdated and do not account for future improvements in longevity. The true survival probabilities (`_k p_x`) are higher, and thus the true probabilities of dying in any given year (`_{k-1}p_x q_{x+k-1}`) are lower, than the values used in their pricing model. According to the principles in **Eq. (1)**, which of the following are necessary consequences of this mis-estimation?\n", "Options": {"A": "The true Expected Present Value of the death benefits (paid out between year 1 and year `n`) is lower than what the insurer calculated.", "B": "The fair premium `P` calculated by the insurer using the outdated tables is too high, leading to excess profits.", "C": "The true Expected Present Value of the survival benefit (paid out at year `n`) is higher than what the insurer calculated.", "D": "The true Expected Present Value of the premium stream paid by the policyholder is higher than what the insurer calculated."}, "Answer": ["A", "C", "D"], "pi_justification": "This item uses a Scenario Application strategy, asking the user to apply the core premium formula to a novel scenario (mis-estimated mortality). It assesses a deep understanding of how actuarial components drive the valuation.\n- **Option A (Correct):** Higher survival probabilities (`_k p_x`) mean the policyholder is more likely to be alive to pay premiums in later years. This increases the value of the annuity factor on the LHS of Eq. (1), increasing the EPV of premiums.\n- **Option B (Correct):** Higher longevity means lower probabilities of dying in any given year before maturity. This directly reduces the EPV of the death benefit component of the liability.\n- **Option C (Correct):** Higher longevity means a higher probability of surviving to the maturity date `n`. This increases the EPV of the final survival benefit.\n- **Option D (Distractor - Conceptual Opposite):** This is the ultimate conclusion the user must avoid. While the EPV of death benefits decreases, the EPV of the survival benefit (paid at the latest date) increases significantly. The net effect is that the total EPV of benefits increases, meaning the calculated premium `P` was too low, not too high.", "qid": "17", "question": "### Background\n\n**Research Question.** How is the fair periodic premium for a unit-linked endowment with guarantees determined by equating the expected present values of premiums and benefits?\n\n**Setting.** An insurance contract for a policyholder aged `x` at inception, with `n` yearly premium payments. The valuation assumes independence between financial markets and mortality risk.\n\n### Data / Model Specification\n\nThe principle of actuarial equivalence states that the expected present value (EPV) of premiums must equal the EPV of benefits. The detailed equation for the fair premium `P` is:\n\n  \nP \\cdot \\underbrace{\\sum_{k=0}^{n-1} P(0,k) \\cdot_k p_x}_{\\text{EPV of Annuity}} = \\underbrace{\\sum_{k=1}^{n} {_{k-1}p_x q_{x+k-1}} \\cdot B_k + _n p_x \\cdot B_n}_{\\text{EPV of Benefits}} \\quad \\text{(Eq. 1)}\n \n\nwhere `B_k` is the time-0 value of the benefit paid at time `k`, given by `B_k = G \\cdot P(0,k) + C_0(X(k),G)`. The term `_k p_x` is the probability of surviving `k` years, and `_{k-1}p_x q_{x+k-1}` is the probability of dying in year `k`.\n\n---\n\nAn insurer is pricing this contract but discovers their mortality tables are outdated and do not account for future improvements in longevity. The true survival probabilities (`_k p_x`) are higher, and thus the true probabilities of dying in any given year (`_{k-1}p_x q_{x+k-1}`) are lower, than the values used in their pricing model. According to the principles in **Eq. (1)**, which of the following are necessary consequences of this mis-estimation?\n", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 22, "Question": "### Background\n\nA study examines how the identity of other large shareholders influences the value of family-controlled firms. It tests two hypotheses regarding blockholder identity:\n\n*   **Hypothesis 3a:** Blockholders that are other families, individuals, or non-financial firms (`IDENTITY1`) are likely to negatively influence firm value due to collusion or rivalry.\n*   **Hypothesis 3b:** Foreign companies as blockholders (`IDENTITY2`) are likely to positively influence firm value due to enhanced monitoring and expertise.\n\n### Data / Model Specification\n\nRegression models were run on a sample of Spanish family firms to estimate the effect of these blockholder identities on industry-adjusted firm value (`AVALUE`). The results are summarized below.\n\n**Table 1: Impact of Blockholder Identity on Family Firm Value**\n\n| Variable    | Coefficient | p-value Category |\n| :---------- | :---------- | :--------------- |\n| `IDENTITY1` | -0.05       | Not Significant  |\n| `IDENTITY2` | 0.27        | p < 0.10         |\n*Source: Table 5, Models 5 and 6 from the paper.*\n\n---\n\nBased on the study's findings, which of the following statements are **NOT** supported conclusions?\n", "Options": {"A": "The presence of foreign investors as significant blockholders is associated with a positive and statistically significant increase in family firm value.", "B": "The paper concludes that blockholder identity is irrelevant for firm valuation in the Spanish context.", "C": "The study's results for blockholder identity are inconclusive, as neither `IDENTITY1` nor `IDENTITY2` showed a statistically significant effect on firm value.", "D": "The data provide strong evidence that having other families or non-financial firms as co-blockholders leads to value destruction through collusion or rivalry."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: The user must interpret a mix of significant and non-significant regression results and correctly identify which conclusions are invalidated by the evidence. This tests for careful reading and the ability to distinguish supported claims from unsupported ones.\nDepth Strategy: Single-Correct-Answer Inversion Protocol. The core assessment objective is to identify the single correct conclusion from the table (Statement A). To create a multiple-response item, the stem is inverted to ask for conclusions that are NOT supported. This transforms the three incorrect statements into the correct answers.\nDistractor Logic:\n- A (New Distractor, was the single correct answer): This statement is a direct and accurate summary of the significant positive result for `IDENTITY2`. It is a supported conclusion, and therefore an incorrect answer to the inverted question.\n- B (Correct Answer): The coefficient for `IDENTITY1` is statistically insignificant. Therefore, the data do NOT provide strong evidence for the hypothesized negative effect. This statement is an unsupported conclusion.\n- C (Correct Answer): This statement is false because `IDENTITY2` DOES show a statistically significant effect. The results are not entirely inconclusive. This statement is an unsupported conclusion.\n- D (Correct Answer): This statement is an overgeneralization and is false. The significant result for `IDENTITY2` explicitly shows that at least one dimension of identity (foreign vs. domestic) is relevant. This statement is an unsupported conclusion.", "qid": "22", "question": "### Background\n\nA study examines how the identity of other large shareholders influences the value of family-controlled firms. It tests two hypotheses regarding blockholder identity:\n\n*   **Hypothesis 3a:** Blockholders that are other families, individuals, or non-financial firms (`IDENTITY1`) are likely to negatively influence firm value due to collusion or rivalry.\n*   **Hypothesis 3b:** Foreign companies as blockholders (`IDENTITY2`) are likely to positively influence firm value due to enhanced monitoring and expertise.\n\n### Data / Model Specification\n\nRegression models were run on a sample of Spanish family firms to estimate the effect of these blockholder identities on industry-adjusted firm value (`AVALUE`). The results are summarized below.\n\n**Table 1: Impact of Blockholder Identity on Family Firm Value**\n\n| Variable    | Coefficient | p-value Category |\n| :---------- | :---------- | :--------------- |\n| `IDENTITY1` | -0.05       | Not Significant  |\n| `IDENTITY2` | 0.27        | p < 0.10         |\n*Source: Table 5, Models 5 and 6 from the paper.*\n\n---\n\nBased on the study's findings, which of the following statements are **NOT** supported conclusions?\n", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 31, "Question": "### Background\n\nThe study investigates whether the investment-stimulating effect of the 2017 Tax Cuts and Jobs Act (TCJA) varied with a firm's capital intensity. A key TCJA provision was 100% \"bonus depreciation,\" allowing immediate expensing of certain capital assets, which would be most valuable to firms that frequently purchase such assets.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated on sub-samples of firms sorted by proxies for capital intensity. The key coefficient on `USFirm*Yr2019` captures the treatment effect.\n\n**Table 1: Heterogeneous Effects by Capital Intensity**\n\n| Sub-sample Split      | High Capital Intensity Group | Coefficient (t-stat) | Low Capital Intensity Group | Coefficient (t-stat) |\n| :-------------------- | :--------------------------- | :------------------- | :-------------------------- | :------------------- |\n| Based on `Tangibility`| High Tangibility             | 0.0038*** (4.60)     | Low Tangibility             | 0.0002 (0.49)        |\n| Based on `R&D`        | Zero R&D                     | 0.0035*** (5.21)     | Non-Zero R&D                | 0.0015* (1.69)       |\n\n*Note: Table is constructed from the paper's Table 3.*\n\n---\n\nBased on the evidence in **Table 1**, which of the following conclusions about the TCJA's effect on investment are supported?\n", "Options": {"A": "Firms with non-zero R&D expenditures increased their capital expenditures more than firms with zero R&D.", "B": "Firms with high asset tangibility showed a statistically significant increase in capital expenditures post-TCJA.", "C": "The results are consistent with the bonus depreciation provision being a key driver of the investment increase.", "D": "The investment-stimulating effect of the TCJA was concentrated in capital-intensive firms."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item tests the ability to interpret heterogeneous treatment effects and link them to specific policy mechanisms discussed in the paper (bonus depreciation).\nDepth Strategy: Reverse-Reasoning. The user must connect the empirical pattern (effects concentrated in capital-intensive firms) to the underlying policy cause (bonus depreciation).\nDistractor Logic:\n- C (Conceptual Opposite): This is a direct contradiction of the results in the table, where the coefficient for Zero R&D firms (0.0035) is larger and more significant than for Non-Zero R&D firms (0.0015).", "qid": "31", "question": "### Background\n\nThe study investigates whether the investment-stimulating effect of the 2017 Tax Cuts and Jobs Act (TCJA) varied with a firm's capital intensity. A key TCJA provision was 100% \"bonus depreciation,\" allowing immediate expensing of certain capital assets, which would be most valuable to firms that frequently purchase such assets.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated on sub-samples of firms sorted by proxies for capital intensity. The key coefficient on `USFirm*Yr2019` captures the treatment effect.\n\n**Table 1: Heterogeneous Effects by Capital Intensity**\n\n| Sub-sample Split      | High Capital Intensity Group | Coefficient (t-stat) | Low Capital Intensity Group | Coefficient (t-stat) |\n| :-------------------- | :--------------------------- | :------------------- | :-------------------------- | :------------------- |\n| Based on `Tangibility`| High Tangibility             | 0.0038*** (4.60)     | Low Tangibility             | 0.0002 (0.49)        |\n| Based on `R&D`        | Zero R&D                     | 0.0035*** (5.21)     | Non-Zero R&D                | 0.0015* (1.69)       |\n\n*Note: Table is constructed from the paper's Table 3.*\n\n---\n\nBased on the evidence in **Table 1**, which of the following conclusions about the TCJA's effect on investment are supported?\n", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 24, "Question": "### Background\n\n**Research Question.** How can the key parameters of the Generalized Pareto Distribution (GPD), which describes tail behavior, be estimated simply and non-parametrically from financial data?\n\n**Setting / Data-Generating Environment.** The paper applies Pickands' method, a non-parametric procedure, to estimate the GPD shape (`k`) and scale (`σ`) parameters. The method relies on specific order statistics from a sample of excesses. For the same dataset, a Maximum Likelihood Estimation (MLE) can also be performed.\n\n**Variables & Parameters.**\n- `k̂_p`: The Pickands estimate for the GPD shape parameter.\n- `k̂_MLE`: The Maximum Likelihood estimate for the GPD shape parameter.\n\n---\n\n### Data / Model Specification\n\nFor Sample 1 (1988-1993), the paper reports two different estimates for the GPD shape parameter `k`:\n- Pickands estimate: `k̂_p = 0.4309`\n- Maximum Likelihood estimate: `k̂_MLE = 0.1158`\n\n---\n\nBased on the statistical properties of these estimators, which of the following are valid reasons that could explain the substantial discrepancy between `k̂_p` and `k̂_MLE` for the same dataset? (Select all that apply)", "Options": {"A": "The Pickands estimator is known to be biased in small samples, whereas the Maximum Likelihood estimator is always unbiased regardless of sample size.", "B": "The MLE procedure requires assuming the underlying returns follow a normal distribution, an assumption the non-parametric Pickands estimator avoids.", "C": "The Pickands estimator is known to be inefficient because it uses only three specific order statistics from the tail, whereas MLE uses all available tail observations.", "D": "The value of the Pickands estimate is highly sensitive to the choice of `h` (the number of order statistics used), a parameter which may not have been optimally selected."}, "Answer": ["C", "D"], "pi_justification": "This item assesses conceptual understanding of the properties of different statistical estimators. It uses a Reverse-Reasoning strategy, asking for causes of an observed outcome. The correct options (A, B) highlight the key weaknesses of the Pickands estimator (inefficiency, sensitivity). Distractors are based on common misconceptions: C) an overstatement of MLE's properties (it is only asymptotically unbiased), and D) a misstatement of the distributional assumptions of the MLE method used in this context.", "qid": "24", "question": "### Background\n\n**Research Question.** How can the key parameters of the Generalized Pareto Distribution (GPD), which describes tail behavior, be estimated simply and non-parametrically from financial data?\n\n**Setting / Data-Generating Environment.** The paper applies Pickands' method, a non-parametric procedure, to estimate the GPD shape (`k`) and scale (`σ`) parameters. The method relies on specific order statistics from a sample of excesses. For the same dataset, a Maximum Likelihood Estimation (MLE) can also be performed.\n\n**Variables & Parameters.**\n- `k̂_p`: The Pickands estimate for the GPD shape parameter.\n- `k̂_MLE`: The Maximum Likelihood estimate for the GPD shape parameter.\n\n---\n\n### Data / Model Specification\n\nFor Sample 1 (1988-1993), the paper reports two different estimates for the GPD shape parameter `k`:\n- Pickands estimate: `k̂_p = 0.4309`\n- Maximum Likelihood estimate: `k̂_MLE = 0.1158`\n\n---\n\nBased on the statistical properties of these estimators, which of the following are valid reasons that could explain the substantial discrepancy between `k̂_p` and `k̂_MLE` for the same dataset? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 38, "Question": "### Background\n\n**Research Question:** This case investigates the novel information content of the ECB's 2014 Comprehensive Assessment (CA) from a \"production side\" perspective. The goal is to quantify how much of the supervisory-imposed capital curtailments from the Asset Quality Review (AQR) and Stress Test Exercise (STE) can be explained by publicly available information, thereby inferring the supervisor's implicit decision rule.\n\n**Setting and Sample:** The analysis uses a cross-section of 122 European banks that participated in the 2014 CA. Two competing hypotheses for the supervisor's behavior are considered: a **risk-based approach** (harsher stance on riskier banks) and **regulatory capture** (lenient stance on larger, more relevant banks).\n\n### Data / Model Specification\n\nThe following OLS regression models are estimated to explain the capital curtailments (in bps):\n  \nAQR_{i} = \\beta_{0} + \\beta_{1}(Leverage)_{i} + \\beta_{2}(NPE.R)_{i} + \\beta_{3}(Tot.Ass. (ln))_{i} + ... + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n \n  \nSTE_{i} = \\alpha_{0} + \\alpha_{1}(Leverage)_{i} + \\alpha_{2}(NPE.R)_{i} + ... + \\vartheta_{i} \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Regression and Summary Statistics**\n\n| | **Panel A: Regression Results** | | **Panel B: Summary Statistics** |\n| :--- | :--- | :--- | :--- | :--- |\n| **Variable** | **AQR Coeff. (p-val)** | **STE Coeff. (p-val)** | **Variable** | **Std. Dev.** |\n| Leverage | 1.67*** (0.0001) | 12.19** (0.0116) | Non-Performing Exp. (NPE.R) | 9.09% |\n| Total Assets (ln) | -7.02** (0.0195) | -3.86 (0.8503) | | |\n| Non-Performing Exp. (NPE.R) | 463.00*** (0.0021) | 1558.30*** (0.0010) | | |\n| **R-squared** | **0.60** | **0.47** | | |\n\n*Source: Adapted from Tables 2 and 3 in the source paper. `***` and `**` denote significance at 1% and 5% levels.*\n\n### Question\n\nBased on the provided models and results in **Table 1**, which of the following statements are correct?", "Options": {"A": "The R-squared of 0.60 for the AQR model suggests that the upper bound for new, idiosyncratic information revealed by the AQR is 40% of the total cross-sectional variation.", "B": "A one-standard-deviation increase in `NPE.R` (9.09%) is associated with an increase of approximately 42.1 bps in the STE curtailment.", "C": "The evidence regarding the supervisor's motives in the AQR is ambiguous; the significant positive coefficient on `Leverage` supports a risk-based approach, whereas the significant negative coefficient on `Total Assets (ln)` is consistent with regulatory capture.", "D": "The significant positive coefficient on `Leverage` in the AQR model indicates that the supervisor was more lenient towards highly levered banks, consistent with regulatory capture."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests multiple skills required for interpreting regression analysis: conceptual understanding of R-squared (Option A), inference about competing hypotheses from coefficient signs (Option B), and a direct calculation of economic significance. It combines **Conceptual Judgment** and **Computational Judgment**. Distractors are designed to trap common errors: using the wrong coefficient for a calculation (**Formula Misuse**, Option C) and misinterpreting a coefficient's sign in the context of the stated hypotheses (**Conceptual Opposite**, Option D).", "qid": "38", "question": "### Background\n\n**Research Question:** This case investigates the novel information content of the ECB's 2014 Comprehensive Assessment (CA) from a \"production side\" perspective. The goal is to quantify how much of the supervisory-imposed capital curtailments from the Asset Quality Review (AQR) and Stress Test Exercise (STE) can be explained by publicly available information, thereby inferring the supervisor's implicit decision rule.\n\n**Setting and Sample:** The analysis uses a cross-section of 122 European banks that participated in the 2014 CA. Two competing hypotheses for the supervisor's behavior are considered: a **risk-based approach** (harsher stance on riskier banks) and **regulatory capture** (lenient stance on larger, more relevant banks).\n\n### Data / Model Specification\n\nThe following OLS regression models are estimated to explain the capital curtailments (in bps):\n  \nAQR_{i} = \\beta_{0} + \\beta_{1}(Leverage)_{i} + \\beta_{2}(NPE.R)_{i} + \\beta_{3}(Tot.Ass. (ln))_{i} + ... + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n \n  \nSTE_{i} = \\alpha_{0} + \\alpha_{1}(Leverage)_{i} + \\alpha_{2}(NPE.R)_{i} + ... + \\vartheta_{i} \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Regression and Summary Statistics**\n\n| | **Panel A: Regression Results** | | **Panel B: Summary Statistics** |\n| :--- | :--- | :--- | :--- | :--- |\n| **Variable** | **AQR Coeff. (p-val)** | **STE Coeff. (p-val)** | **Variable** | **Std. Dev.** |\n| Leverage | 1.67*** (0.0001) | 12.19** (0.0116) | Non-Performing Exp. (NPE.R) | 9.09% |\n| Total Assets (ln) | -7.02** (0.0195) | -3.86 (0.8503) | | |\n| Non-Performing Exp. (NPE.R) | 463.00*** (0.0021) | 1558.30*** (0.0010) | | |\n| **R-squared** | **0.60** | **0.47** | | |\n\n*Source: Adapted from Tables 2 and 3 in the source paper. `***` and `**` denote significance at 1% and 5% levels.*\n\n### Question\n\nBased on the provided models and results in **Table 1**, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 88, "Question": "### Background\n\nThe study performs a robustness check by examining a sample of firms that existed during two opposing legislative periods: the 1991 Employment Contracts Act (which weakened unions) and the 2000 Employment Relations Act (which strengthened them). The analysis focuses on the long-run Average Cumulative Abnormal Returns (ACARs) for the same firms across these two events.\n\n### Data / Model Specification\n\nThe study reports two key findings for this restricted sample of firms:\n1.  The ACAR around the anti-union 1991 legislation was significantly positive.\n2.  The Pearson correlation between the ACARs from the 1991 and 2000 periods was negative and statistically significant: `ρ(ACAR_1991, ACAR_2000) = -0.2507` (p-value = 0.068).\n\n---\n\nBased on this price-reversal test across two opposing legislative eras, which of the following inferences are supported?", "Options": {"A": "Firms that benefited most from the anti-union law in 1991 were also the ones harmed most by the pro-union law in 2000.", "B": "The correlation result implies that firms that benefited from the 1991 law also benefited from the 2000 law, indicating a persistent 'quality' factor.", "C": "The results help refute the alternative explanation that the negative returns in 2000 were due to a persistent, long-term decline in the industries of unionized firms.", "D": "The test provides an out-of-sample validation of the main findings, suggesting that union power is a causal driver of firm valuation."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to understand the causal logic of using an opposing historical event as a natural experiment and to correctly interpret the meaning of a negative correlation in this context. Depth Strategy: Reverse-Reasoning. Given the empirical result (negative correlation across opposing treatments), the user must identify all plausible causal inferences that can be drawn. Distractor Logic: D is a conceptual opposite. It correctly identifies the idea of a persistent factor but misinterprets the sign of the correlation, leading to a conclusion that is the exact opposite of what the data show.", "qid": "88", "question": "### Background\n\nThe study performs a robustness check by examining a sample of firms that existed during two opposing legislative periods: the 1991 Employment Contracts Act (which weakened unions) and the 2000 Employment Relations Act (which strengthened them). The analysis focuses on the long-run Average Cumulative Abnormal Returns (ACARs) for the same firms across these two events.\n\n### Data / Model Specification\n\nThe study reports two key findings for this restricted sample of firms:\n1.  The ACAR around the anti-union 1991 legislation was significantly positive.\n2.  The Pearson correlation between the ACARs from the 1991 and 2000 periods was negative and statistically significant: `ρ(ACAR_1991, ACAR_2000) = -0.2507` (p-value = 0.068).\n\n---\n\nBased on this price-reversal test across two opposing legislative eras, which of the following inferences are supported?", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 30, "Question": "### Background\n\nThe study investigates which specific provisions of the 2017 Tax Cuts and Jobs Act (TCJA) drove an observed increase in corporate investment. The TCJA contained distinct provisions: some primarily benefited multinational corporations (MNCs) by shifting to a territorial tax system, while others, like accelerated depreciation, were available to all U.S. firms.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated separately for purely domestic U.S. firms and for multinational U.S. firms. The dependent variable is either the GAAP effective tax rate (`GAAPETR`) or capital expenditures (`CapEx`). The key coefficient captures the post-TCJA change for U.S. firms relative to Canadian firms.\n\n**Table 1: DiD Results for Domestic vs. Multinational Firms**\n\n| Dependent Variable | Sub-sample    | Coefficient on `USFirm*Yr2019` | t-statistic |\n| :----------------- | :------------ | :------------------------------ | :---------- |\n| `GAAPETR`          | Domestic      | -0.0518***                      | [-3.49]     |\n| `GAAPETR`          | Multinational | -0.0943***                      | [-3.83]     |\n| `CapEx`            | Domestic      | 0.0031***                       | [4.24]      |\n| `CapEx`            | Multinational | -0.0001                         | [-0.07]     |\n\n*Note: Table is constructed from the paper's Table 4.*\n\n---\n\nBased on the results in **Table 1**, select all of the following statements that are valid conclusions.\n", "Options": {"A": "Multinational firms experienced a larger reduction in their GAAP effective tax rate than domestic firms.", "B": "The investment response (change in `CapEx`) was statistically significant for domestic firms.", "C": "The investment response (change in `CapEx`) was statistically significant for multinational firms.", "D": "The results suggest that the shift to a territorial tax system was the primary driver of the aggregate increase in investment."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: The item assesses the ability to synthesize results from multiple regressions to draw a higher-order conclusion about policy mechanisms. It tests the core finding that domestic-facing provisions, not international ones, drove the investment increase.\nDepth Strategy: Reverse-Reasoning. Given the empirical results, the user must identify the most plausible causal mechanism.\nDistractor Logic:\n- C (Conceptual Opposite): This is directly contradicted by the t-statistic of -0.07 for multinational firms' `CapEx`.\n- D (Conceptual Opposite): This is the key incorrect inference. The group that benefited most from the territorial system (MNCs) did not increase investment, which refutes this conclusion. This is a common misinterpretation error.", "qid": "30", "question": "### Background\n\nThe study investigates which specific provisions of the 2017 Tax Cuts and Jobs Act (TCJA) drove an observed increase in corporate investment. The TCJA contained distinct provisions: some primarily benefited multinational corporations (MNCs) by shifting to a territorial tax system, while others, like accelerated depreciation, were available to all U.S. firms.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated separately for purely domestic U.S. firms and for multinational U.S. firms. The dependent variable is either the GAAP effective tax rate (`GAAPETR`) or capital expenditures (`CapEx`). The key coefficient captures the post-TCJA change for U.S. firms relative to Canadian firms.\n\n**Table 1: DiD Results for Domestic vs. Multinational Firms**\n\n| Dependent Variable | Sub-sample    | Coefficient on `USFirm*Yr2019` | t-statistic |\n| :----------------- | :------------ | :------------------------------ | :---------- |\n| `GAAPETR`          | Domestic      | -0.0518***                      | [-3.49]     |\n| `GAAPETR`          | Multinational | -0.0943***                      | [-3.83]     |\n| `CapEx`            | Domestic      | 0.0031***                       | [4.24]      |\n| `CapEx`            | Multinational | -0.0001                         | [-0.07]     |\n\n*Note: Table is constructed from the paper's Table 4.*\n\n---\n\nBased on the results in **Table 1**, select all of the following statements that are valid conclusions.\n", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 26, "Question": "**Research Question.** Does the idiosyncratic component of stock order flow, orthogonal to major macroeconomic shocks, have predictive power for future market returns?\n\n**Setting / Data-Generating Environment.** A structural VAR model isolates an idiosyncratic shock to the buy/sell ratio (`ε_4t`), which is found to negatively predict future real stock returns.\n\n**Data / Model Specification**\nConsider a simple two-period market model to formalize the price pressure hypothesis:\n- The log price at t=0 is `p_0`, which is also the true fundamental value, `v`.\n- At t=1, an uninformed order flow shock `u` occurs, where `u > 0` represents net buying pressure. The paper's idiosyncratic shock `ε_{4t}` is a proxy for `u`.\n- The price at t=1 is determined by `p_1 = p_0 + λu`, where `λ > 0` is the price impact coefficient (inverse of market depth).\n- At t=2, the fundamental value `v` is revealed, and the price becomes `p_2 = v`.\n\nBased on the model above, select all of the following statements that are correct.", "Options": {"A": "If two markets have price impact coefficients `λ_A = 0.1` and `λ_B = 0.2`, Market B will exhibit a more negative expected return for the same order flow shock `u`.", "B": "The total return from t=0 to t=2 is `λu`.", "C": "The expected log return from t=1 to t=2, conditional on a positive shock `u > 0`, is negative.", "D": "The price at t=1, `p_1`, is an unbiased estimate of the fundamental value `v`."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: Tests the ability to formalize and interpret a simple market microstructure model of price pressure. Strategy: Computational Judgment, requiring the student to derive the expected return (`E[Return] = -λu`) and interpret its components. Distractor Logic: (C) tests for a Sign Error and misunderstanding of bias. (D) is a Step-Omission Error, confusing the temporary price impact with the permanent total return.", "qid": "26", "question": "**Research Question.** Does the idiosyncratic component of stock order flow, orthogonal to major macroeconomic shocks, have predictive power for future market returns?\n\n**Setting / Data-Generating Environment.** A structural VAR model isolates an idiosyncratic shock to the buy/sell ratio (`ε_4t`), which is found to negatively predict future real stock returns.\n\n**Data / Model Specification**\nConsider a simple two-period market model to formalize the price pressure hypothesis:\n- The log price at t=0 is `p_0`, which is also the true fundamental value, `v`.\n- At t=1, an uninformed order flow shock `u` occurs, where `u > 0` represents net buying pressure. The paper's idiosyncratic shock `ε_{4t}` is a proxy for `u`.\n- The price at t=1 is determined by `p_1 = p_0 + λu`, where `λ > 0` is the price impact coefficient (inverse of market depth).\n- At t=2, the fundamental value `v` is revealed, and the price becomes `p_2 = v`.\n\nBased on the model above, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 50, "Question": "### Background\n\nA study examines the impact of economic policy uncertainty (EPU) on corporate board structure using a panel regression framework. The central hypothesis is that boards shift their focus from advising to monitoring when uncertainty rises.\n\n### Data / Model Specification\n\nThe estimated model is of the form:\n`Y_{i,t+1} = α_j + β * EPU_t + Γ'X_{it} + ε_{it}`\nwhere `Y` is a board characteristic, `X` is a vector of controls, and `α` represents industry fixed effects.\n\n**Table 1. Determinants of board structure**\n\n| | (1) | (2) | (4) |\n| :--- | :--- | :--- | :--- |\n| **Dependent variable (t+1)** | **Board size** | **Board independence** | **%Insider** |\n| EPU | -0.083** | 0.095*** | -0.039*** |\n| | (0.008) | (0.005) | (0.004) |\n\n*Note: The paper also finds significant negative coefficients for EPU on `%Outside Executive` and `Outside Busyness`, and a significant positive coefficient on `%Female`.*\n\n### Question\n\nAccording to the paper's theoretical framework and the regression results presented, which of the following board structure changes are considered evidence of an increased focus on **monitoring** in response to a rise in EPU?", "Options": {"A": "An increase in the total number of outside board seats held by independent directors (Outside Busyness).", "B": "An increase in the percentage of independent directors.", "C": "An increase in the percentage of outside directors with executive titles.", "D": "A decrease in the percentage of inside directors."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the user's understanding of how different board characteristics are theoretically linked to the concepts of monitoring and advising, and their ability to connect this theory to the paper's empirical findings. It uses a Reverse-Reasoning strategy. \n- **Correct A:** The paper argues that insiders are primarily advisors. The regression shows a significant negative coefficient for `%Insider`, so a decrease is consistent with a shift away from advising toward monitoring.\n- **Correct C:** Independent directors are considered the primary monitors. The regression shows a significant positive coefficient for `Board Independence`, directly supporting an increased focus on monitoring.\n- **Distractor B:** This is a Conceptual Opposite. The paper argues that less busy directors have more time for monitoring. The regression finds a *negative* coefficient for `Outside Busyness`, meaning a *decrease* in busyness is associated with more monitoring, not an increase.\n- **Distractor D:** This is a Conceptual Opposite. The paper frames outside executives as valuable advisors. The regression finds a *negative* coefficient, indicating a shift *away* from this advising role.", "qid": "50", "question": "### Background\n\nA study examines the impact of economic policy uncertainty (EPU) on corporate board structure using a panel regression framework. The central hypothesis is that boards shift their focus from advising to monitoring when uncertainty rises.\n\n### Data / Model Specification\n\nThe estimated model is of the form:\n`Y_{i,t+1} = α_j + β * EPU_t + Γ'X_{it} + ε_{it}`\nwhere `Y` is a board characteristic, `X` is a vector of controls, and `α` represents industry fixed effects.\n\n**Table 1. Determinants of board structure**\n\n| | (1) | (2) | (4) |\n| :--- | :--- | :--- | :--- |\n| **Dependent variable (t+1)** | **Board size** | **Board independence** | **%Insider** |\n| EPU | -0.083** | 0.095*** | -0.039*** |\n| | (0.008) | (0.005) | (0.004) |\n\n*Note: The paper also finds significant negative coefficients for EPU on `%Outside Executive` and `Outside Busyness`, and a significant positive coefficient on `%Female`.*\n\n### Question\n\nAccording to the paper's theoretical framework and the regression results presented, which of the following board structure changes are considered evidence of an increased focus on **monitoring** in response to a rise in EPU?", "model_evaluation_result": {"model_raw_response": "\"B,D\"", "model_answer": ["B", "D"], "error": null}}
{"ID": 45, "Question": "### Background\n\n**Research Question.** How do analysts revise their earnings forecasts for firms whose bonds have been downgraded, and how can the economic magnitude of these revisions be assessed, particularly across firms with different valuation levels?\n\n**Setting / Data-Generating Environment.** The study examines monthly abnormal earnings forecast revisions (`AFR`) for downgraded firms. The cumulative effect is measured by `CAFR`, which sums the `AFR`s from month `t=0` to `t=3`. To translate this price-scaled measure into an intuitive percentage, it is multiplied by the firm's price-to-earnings (`PE`) ratio.\n\n**Variables & Parameters.**\n- `CAFR`: Cumulative abnormal forecast revision over months 0 to 3 (dimensionless, as a percentage).\n- `PE Ratio`: Price-to-earnings ratio.\n- **Group 1, 2, 3:** Categories based on the firm's rating change.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents the cumulative abnormal forecast revisions (`CAFR`) and average `PE` ratios for the downgraded firms themselves, categorized by credit quality group.\n\n**Table 1. Downgraded Firms’ Revisions and Valuations**\n\n| Group | CAFR (0-3 months) | t-statistic | Average PE Ratio |\n| :--- | :--- | :--- | :--- |\n| 1 | -0.71 | (-3.63) | 14.9 |\n| 2 | -2.74 | (-4.16) | 26.4 |\n| 3 | -3.69 | (-4.96) | 26.9 |\n\nThe abnormal percentage change in earnings forecasts is estimated as:\n  \n\\text{Abnormal % Change} = CAFR \\times \\text{PE Ratio} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nUsing the data and methodology presented, evaluate the economic significance of earnings forecast revisions for downgraded firms. Select all of the following statements that are correct.", "Options": {"A": "A potential methodological weakness is that the high PE ratio for Group 3 firms, possibly driven by near-zero earnings, could mechanically inflate the calculated percentage change.", "B": "The calculated abnormal percentage change in earnings forecasts for Group 3 firms is approximately -99.3%.", "C": "The raw CAFR value is a more direct measure of economic significance than the PE-adjusted percentage change because it is not affected by market valuation.", "D": "The abnormal percentage change in earnings forecasts for Group 1 firms is -71%."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item combines 'Computational Judgment' with a test of methodological critique. It requires the user to first perform a calculation based on the provided formula (Option A) and then to identify a key limitation of that same calculation (Option B). This assesses both procedural knowledge and critical thinking. Distractor C targets a common calculation slip (unit error), while Distractor D presents a conceptually flawed argument about the purpose of the transformation.", "qid": "45", "question": "### Background\n\n**Research Question.** How do analysts revise their earnings forecasts for firms whose bonds have been downgraded, and how can the economic magnitude of these revisions be assessed, particularly across firms with different valuation levels?\n\n**Setting / Data-Generating Environment.** The study examines monthly abnormal earnings forecast revisions (`AFR`) for downgraded firms. The cumulative effect is measured by `CAFR`, which sums the `AFR`s from month `t=0` to `t=3`. To translate this price-scaled measure into an intuitive percentage, it is multiplied by the firm's price-to-earnings (`PE`) ratio.\n\n**Variables & Parameters.**\n- `CAFR`: Cumulative abnormal forecast revision over months 0 to 3 (dimensionless, as a percentage).\n- `PE Ratio`: Price-to-earnings ratio.\n- **Group 1, 2, 3:** Categories based on the firm's rating change.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents the cumulative abnormal forecast revisions (`CAFR`) and average `PE` ratios for the downgraded firms themselves, categorized by credit quality group.\n\n**Table 1. Downgraded Firms’ Revisions and Valuations**\n\n| Group | CAFR (0-3 months) | t-statistic | Average PE Ratio |\n| :--- | :--- | :--- | :--- |\n| 1 | -0.71 | (-3.63) | 14.9 |\n| 2 | -2.74 | (-4.16) | 26.4 |\n| 3 | -3.69 | (-4.96) | 26.9 |\n\nThe abnormal percentage change in earnings forecasts is estimated as:\n  \n\\text{Abnormal % Change} = CAFR \\times \\text{PE Ratio} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nUsing the data and methodology presented, evaluate the economic significance of earnings forecast revisions for downgraded firms. Select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 33, "Question": "### Background\n\nThe study uses a difference-in-differences (DiD) research design to estimate the causal impact of the 2017 Tax Cuts and Jobs Act (TCJA) on corporate capital expenditures (`CapEx`). The design compares U.S. firms (treatment group) to Canadian firms (control group) before and after the TCJA's implementation in 2018. The post-treatment period is defined as calendar year 2019.\n\n### Data / Model Specification\n\nThe core DiD model is specified as:\n\n  \nCapEx_{it} = \\beta_1 (USFirm_i \\times Yr19_t) + \\gamma'X_{it} + \\Lambda_i + \\Omega_t + \\Pi_t + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nwhere `USFirm` is an indicator for U.S. firms, `Yr19` is an indicator for the year 2019, `Λ_i` represents firm fixed effects, and `Π_t` represents year fixed effects.\n\n---\n\nBased on the principles of this research design, select all of the following statements that are true.\n", "Options": {"A": "The firm fixed effects (`Λ_i`) control for time-invariant differences between U.S. and Canadian firms, such as industry composition or average size.", "B": "The year fixed effects (`Π_t`) control for macroeconomic trends, such as global business cycles, that affect both U.S. and Canadian firms.", "C": "A key identifying assumption is that, absent the TCJA, the trend in capital expenditures for U.S. firms would have been the same as the trend for Canadian firms.", "D": "The coefficient `β_1` measures the average level of capital expenditures for U.S. firms in 2019."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the mechanics and assumptions of the difference-in-differences (DiD) methodology, which is the core econometric strategy of the paper.\nDepth Strategy: Conceptual Judgment. The user must evaluate the role of each component of the DiD regression and its key underlying assumption.\nDistractor Logic:\n- A, B, C (Correct): These are all fundamental, correct statements about the components and assumptions of a fixed-effects DiD model. A tests the role of firm fixed effects, B tests the role of time fixed effects, and C tests the crucial parallel trends assumption.\n- D (Step-Omission Error): This is a common misinterpretation of a DiD coefficient. `β_1` does not measure a level; it measures the *difference* in the *change* of `CapEx` for the treatment group relative to the control group. It isolates the treatment effect, not the post-treatment outcome level.", "qid": "33", "question": "### Background\n\nThe study uses a difference-in-differences (DiD) research design to estimate the causal impact of the 2017 Tax Cuts and Jobs Act (TCJA) on corporate capital expenditures (`CapEx`). The design compares U.S. firms (treatment group) to Canadian firms (control group) before and after the TCJA's implementation in 2018. The post-treatment period is defined as calendar year 2019.\n\n### Data / Model Specification\n\nThe core DiD model is specified as:\n\n  \nCapEx_{it} = \\beta_1 (USFirm_i \\times Yr19_t) + \\gamma'X_{it} + \\Lambda_i + \\Omega_t + \\Pi_t + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nwhere `USFirm` is an indicator for U.S. firms, `Yr19` is an indicator for the year 2019, `Λ_i` represents firm fixed effects, and `Π_t` represents year fixed effects.\n\n---\n\nBased on the principles of this research design, select all of the following statements that are true.\n", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 92, "Question": "### Background\n\n**Research Question.** Do conflicting empirical findings on mutual fund 'tournament behavior' stem from a 'sorting bias' in the traditional testing methodology?\n\n**Setting / Data-Generating Environment.** The paper critiques the traditional methodology which sorts funds on first-half returns and measures second-half risk changes.\n\n### Data / Model Specification\n\n**Simulation Evidence**\nA simulation is run where fund returns are generated by a single-factor model and fund betas mean-revert exogenously. **Crucially, no strategic tournament behavior is programmed into the simulation.** The same metrics from the traditional test are then computed on this simulated data.\n-   `Frequency Difference`: Measures apparent tournament behavior.\n-   `Before Ratio`: Measures the degree of risk sorting in the first half.\n\n**Table 1. Simulated Tournament Results (No Actual Tournament Behavior)**\n\n| Year | Frequency Difference | Before Ratio |\n|:----:|:---:|:---:|\n| 1991 | 8.00 | 1.57 |\n| 2001 | -7.64 | 0.60 |\n| **Corr.** | **0.69** | |\n\n### Question\n\nThe paper uses a simulation to critique the causal validity of the traditional methodology. The simulation was designed with **no strategic tournament behavior**. Based on the results in Table 1, select all valid conclusions that can be drawn from this simulation exercise.", "Options": {"A": "Since the simulation (Table 1) and the empirical data show similar patterns, this validates the traditional methodology as a reliable measure of tournament behavior.", "B": "The simulation fails to replicate the empirical findings, as the simulated `Frequency Difference` values are much smaller than the empirical ones, suggesting that real tournament behavior is much stronger than the sorting bias.", "C": "The simulation replicates the key qualitative pattern of the empirical data, generating apparent risk-increasing behavior in a strong market year (1991) and apparent risk-decreasing behavior in a weak market year (2001), purely as a statistical artifact.", "D": "The simulation demonstrates that the strong positive correlation between the `Before Ratio` and the `Frequency Difference` can arise mechanically from risk mean-reversion, without any strategic action from managers."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses a sophisticated concept: the use of simulation for causal inference and methodological critique. It uses a **Reverse-Reasoning** strategy, asking what can be concluded from the simulation's outcome. The correct options require understanding that when a null model (no behavior) replicates empirical patterns, the test methodology is invalidated. Distractors are based on fundamental misinterpretations of this logic, including two **Conceptual Opposites** that draw the exact wrong conclusion from the evidence.", "qid": "92", "question": "### Background\n\n**Research Question.** Do conflicting empirical findings on mutual fund 'tournament behavior' stem from a 'sorting bias' in the traditional testing methodology?\n\n**Setting / Data-Generating Environment.** The paper critiques the traditional methodology which sorts funds on first-half returns and measures second-half risk changes.\n\n### Data / Model Specification\n\n**Simulation Evidence**\nA simulation is run where fund returns are generated by a single-factor model and fund betas mean-revert exogenously. **Crucially, no strategic tournament behavior is programmed into the simulation.** The same metrics from the traditional test are then computed on this simulated data.\n-   `Frequency Difference`: Measures apparent tournament behavior.\n-   `Before Ratio`: Measures the degree of risk sorting in the first half.\n\n**Table 1. Simulated Tournament Results (No Actual Tournament Behavior)**\n\n| Year | Frequency Difference | Before Ratio |\n|:----:|:---:|:---:|\n| 1991 | 8.00 | 1.57 |\n| 2001 | -7.64 | 0.60 |\n| **Corr.** | **0.69** | |\n\n### Question\n\nThe paper uses a simulation to critique the causal validity of the traditional methodology. The simulation was designed with **no strategic tournament behavior**. Based on the results in Table 1, select all valid conclusions that can be drawn from this simulation exercise.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 52, "Question": "### Background\n\nA study investigates whether the interaction between product life cycle (PLC) and the book-to-market (BM) effect is state-dependent, varying with macroeconomic conditions and investor sentiment. The study calculates the Fama-French five-factor alphas for hedge portfolios that are long high-BM stocks and short low-BM stocks (HML portfolios), constructed within different PLC groups. The key metric is the 'Long-Short HML Alpha Spread': the alpha of the long-PLC HML portfolio minus the alpha of the short-PLC HML portfolio.\n\n### Data / Model Specification\n\nTable 1 summarizes this key metric across different market states.\n\n**Table 1: Long-Short HML Alpha Spreads (%) in Subsamples**\n\n| Subsample Condition | Long-Short HML Alpha Spread (%) | *t-statistic* |\n| :--- | :---: | :---: |\n| **Panel A: Business Cycle** | |\n| Expansion | 0.30 | (1.76) |\n| Recession | 2.09 | (2.48) |\n| **Panel B: Investor Sentiment** | |\n| Low Sentiment | 0.21 | (1.20) |\n| High Sentiment | 0.87 | (3.37) |\n\n*Source: Table 3 of the paper.* \n\n---\n\nBased on the results in Table 1, which of the following statements accurately describe the findings and their implications? Select all that apply.", "Options": {"A": "The PLC-BM interaction effect is significantly more pronounced during periods of high investor sentiment, supporting a behavioral mispricing explanation.", "B": "During periods of low investor sentiment, the Long-Short HML Alpha Spread is large and highly statistically significant.", "C": "The results suggest the PLC-BM interaction is a consistent, stable effect that does not vary with market conditions.", "D": "The anomalous return spread associated with the PLC-BM interaction is significantly stronger during economic recessions than during expansions."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item tests the ability to interpret state-dependent portfolio returns and synthesize findings from two different sample splits (business cycle and sentiment) to evaluate competing hypotheses (risk vs. mispricing). Depth Strategy: Reverse-Reasoning. Given the empirical results, the user must identify the plausible theoretical explanations. Distractor Logic: C is a conceptual opposite of the clear state-dependency shown in the table. D is a direct misinterpretation of the insignificant result for the low-sentiment period.", "qid": "52", "question": "### Background\n\nA study investigates whether the interaction between product life cycle (PLC) and the book-to-market (BM) effect is state-dependent, varying with macroeconomic conditions and investor sentiment. The study calculates the Fama-French five-factor alphas for hedge portfolios that are long high-BM stocks and short low-BM stocks (HML portfolios), constructed within different PLC groups. The key metric is the 'Long-Short HML Alpha Spread': the alpha of the long-PLC HML portfolio minus the alpha of the short-PLC HML portfolio.\n\n### Data / Model Specification\n\nTable 1 summarizes this key metric across different market states.\n\n**Table 1: Long-Short HML Alpha Spreads (%) in Subsamples**\n\n| Subsample Condition | Long-Short HML Alpha Spread (%) | *t-statistic* |\n| :--- | :---: | :---: |\n| **Panel A: Business Cycle** | |\n| Expansion | 0.30 | (1.76) |\n| Recession | 2.09 | (2.48) |\n| **Panel B: Investor Sentiment** | |\n| Low Sentiment | 0.21 | (1.20) |\n| High Sentiment | 0.87 | (3.37) |\n\n*Source: Table 3 of the paper.* \n\n---\n\nBased on the results in Table 1, which of the following statements accurately describe the findings and their implications? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 63, "Question": "### Background\n\n**Research Question.** How sensitive are an insurer's optimal dividend and reinsurance policies to the magnitude of the penalty associated with financial ruin?\n\n**Setting.** A numerical example illustrating the theoretical predictions that a higher ruin penalty `P` makes the insurer more conservative. The table shows the optimal policy thresholds (`x_1`, `x_b`) and the initial risk retention `q*(0)` for a range of ruin penalty values `P`.\n\n**Variables and Parameters.**\n- `P`: The penalty for ruin. `P < 0` indicates a salvage value (units: currency).\n- `x_1`: The surplus threshold separating partial and full reinsurance (units: currency).\n- `x_b`: The optimal dividend barrier (units: currency).\n- `q*(0)`: The optimal retention proportion at infinitesimally small surplus (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe following table presents numerical solutions for the optimal policy under the parameterization: `a=1`, `b=0.2`, `θ=0.1`, `β=0.05`, and `η=0.15`.\n\n**Table 1: The effect of `P` on `x_1`, `x_b`, and `q*(0)`**\n\n| P    | x_1   | x_b   | q*(0) |\n|:-----|:------|:------|:------|\n| -1.0 | 0.080 | 0.651 | 0.863 |\n| -0.5 | 0.192 | 0.763 | 0.724 |\n| 0.0  | 0.263 | 0.834 | 0.667 |\n| 0.5  | 0.313 | 0.885 | 0.637 |\n| 1.0  | 0.352 | 0.924 | 0.620 |\n| 5.0  | 0.514 | 1.085 | 0.582 |\n| 10.0 | 0.606 | 1.177 | 0.574 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following statements accurately describe how the insurer's strategy changes as the ruin penalty `P` increases from a salvage value (`P=-1.0`) to a high penalty (`P=10.0`)? Select all that apply.", "Options": {"A": "The initial retention proportion `q*(0)` decreases by more than 30%, indicating a more cautious stance at low surplus levels.", "B": "The region of partial reinsurance, defined by the threshold `x_1`, shrinks as the penalty `P` increases.", "C": "The size of the surplus region where the insurer operates with full risk retention (`x_b - x_1`) remains constant across all values of `P`.", "D": "The dividend barrier `x_b` decreases as the penalty `P` rises, as the firm tries to pay out dividends faster to avoid the penalty."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses quantitative interpretation and policy synthesis. It uses the 'Atomic Decomposition' strategy by breaking down the original QA's findings into distinct, verifiable statements. Options A and B are correct calculations/observations from the table. Distractor C (Conceptual Opposite) posits an incorrect relationship for `x_b`. Distractor D (Conceptual Opposite) incorrectly describes the trend in `x_1`, which actually expands dramatically.", "qid": "63", "question": "### Background\n\n**Research Question.** How sensitive are an insurer's optimal dividend and reinsurance policies to the magnitude of the penalty associated with financial ruin?\n\n**Setting.** A numerical example illustrating the theoretical predictions that a higher ruin penalty `P` makes the insurer more conservative. The table shows the optimal policy thresholds (`x_1`, `x_b`) and the initial risk retention `q*(0)` for a range of ruin penalty values `P`.\n\n**Variables and Parameters.**\n- `P`: The penalty for ruin. `P < 0` indicates a salvage value (units: currency).\n- `x_1`: The surplus threshold separating partial and full reinsurance (units: currency).\n- `x_b`: The optimal dividend barrier (units: currency).\n- `q*(0)`: The optimal retention proportion at infinitesimally small surplus (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe following table presents numerical solutions for the optimal policy under the parameterization: `a=1`, `b=0.2`, `θ=0.1`, `β=0.05`, and `η=0.15`.\n\n**Table 1: The effect of `P` on `x_1`, `x_b`, and `q*(0)`**\n\n| P    | x_1   | x_b   | q*(0) |\n|:-----|:------|:------|:------|\n| -1.0 | 0.080 | 0.651 | 0.863 |\n| -0.5 | 0.192 | 0.763 | 0.724 |\n| 0.0  | 0.263 | 0.834 | 0.667 |\n| 0.5  | 0.313 | 0.885 | 0.637 |\n| 1.0  | 0.352 | 0.924 | 0.620 |\n| 5.0  | 0.514 | 1.085 | 0.582 |\n| 10.0 | 0.606 | 1.177 | 0.574 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following statements accurately describe how the insurer's strategy changes as the ruin penalty `P` increases from a salvage value (`P=-1.0`) to a high penalty (`P=10.0`)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A", "model_answer": ["A"], "error": null}}
{"ID": 18, "Question": "### Background\n\n**Research Question.** How can the price of a call option on a sum of correlated log-normal variables, `Y(k)`, be analytically approximated using convex bounds?\n\n**Setting.** The value of the reference portfolio `Y(k)` is a sum of correlated log-normal random variables. Pricing an option on `Y(k)` requires calculating `E[(Y(k) - d)_+]`, which is analytically intractable. The method of comonotonic approximation provides a lower bound `Y(k)^l` and an upper bound `Y(k)^u`.\n\n### Data / Model Specification\n\nThe bounds are established using the concept of convex ordering (`\\leq_{cx}`), where `A \\leq_{cx} B` means `E[\\nu(A)] \\leq E[\\nu(B)]` for any convex function `\\nu(x)`. The paper establishes the relationship:\n\n  \nY(k)^l \\leq_{cx} Y(k) \\leq_{cx} Y(k)^u \\quad \\text{(Eq. 1)}\n \n\nThis ordering implies relationships between their expected values and variances:\n\n  \nE[Y(k)^l] = E[Y(k)] = E[Y(k)^u] \\quad \\text{(Eq. 2)}\n \n\n  \n\\mathrm{Var}[Y(k)^l] \\leq \\mathrm{Var}[Y(k)] \\leq \\mathrm{Var}[Y(k)^u] \\quad \\text{(Eq. 3)}\n \n\nThe quality of the lower bound approximation `Y(k)^l` depends on how well a single conditioning variable `\\Lambda(k)` captures the randomness of the components of `Y(k)`.\n\n---\n\nBased on the theoretical properties of these convex bounds, which of the following statements are **INCORRECT**?\n", "Options": {"A": "The payoff function of a European call option, `f(x) = (x-d)_+`, is a convex function, which is why these bounds are applicable for option pricing.", "B": "The equality of the expected values in Eq. (2) implies that the bounds `Y(k)^l` and `Y(k)^u` will always provide a tight approximation for the price of a call option on `Y(k)`.", "C": "For any convex function `\\nu`, the inequality `E[\\nu(Y(k)^l)] \\leq E[\\nu(Y(k)^u)]` must hold.", "D": "If the underlying components of `Y(k)` become perfectly correlated (comonotonic), then `\\mathrm{Var}[Y(k)^l] = \\mathrm{Var}[Y(k)] = \\mathrm{Var}[Y(k)^u]`."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the theoretical underpinnings of the paper's core methodology. The assessment objective inherently leads to multiple true statements and only a few false ones, making it a good candidate for the Single-Correct-Answer Inversion Protocol. The stem is inverted to ask for incorrect statements.\n- **Option A (Distractor - True):** This follows directly from the transitivity of the convex order in Eq. (1). It is a correct statement, thus an incorrect answer to the inverted question.\n- **Option B (Distractor - True):** The convexity of the call payoff is the fundamental reason this entire framework is useful for option pricing. It is a correct statement, thus an incorrect answer.\n- **Option C (Correct - False Statement):** If `Y(k)` becomes comonotonic, then `Y(k)` is equivalent to `Y(k)^u` in distribution, so `Var[Y(k)] = Var[Y(k)^u]`. However, `Y(k)^l` is a conditional expectation, and its variance will only equal `Var[Y(k)]` if the conditioning variable `\\Lambda(k)` perfectly explains `Y(k)`, which is not guaranteed. Therefore, the three-way equality is not necessarily true.\n- **Option D (Correct - False Statement):** This is a critical conceptual error. Option prices depend on the entire distribution, especially the variance (and higher moments), not just the mean. Equal means do not guarantee tight option price bounds. The variance inequalities in Eq. (3) show that the distributions can be very different, leading to wide option price bounds despite having the same mean.", "qid": "18", "question": "### Background\n\n**Research Question.** How can the price of a call option on a sum of correlated log-normal variables, `Y(k)`, be analytically approximated using convex bounds?\n\n**Setting.** The value of the reference portfolio `Y(k)` is a sum of correlated log-normal random variables. Pricing an option on `Y(k)` requires calculating `E[(Y(k) - d)_+]`, which is analytically intractable. The method of comonotonic approximation provides a lower bound `Y(k)^l` and an upper bound `Y(k)^u`.\n\n### Data / Model Specification\n\nThe bounds are established using the concept of convex ordering (`\\leq_{cx}`), where `A \\leq_{cx} B` means `E[\\nu(A)] \\leq E[\\nu(B)]` for any convex function `\\nu(x)`. The paper establishes the relationship:\n\n  \nY(k)^l \\leq_{cx} Y(k) \\leq_{cx} Y(k)^u \\quad \\text{(Eq. 1)}\n \n\nThis ordering implies relationships between their expected values and variances:\n\n  \nE[Y(k)^l] = E[Y(k)] = E[Y(k)^u] \\quad \\text{(Eq. 2)}\n \n\n  \n\\mathrm{Var}[Y(k)^l] \\leq \\mathrm{Var}[Y(k)] \\leq \\mathrm{Var}[Y(k)^u] \\quad \\text{(Eq. 3)}\n \n\nThe quality of the lower bound approximation `Y(k)^l` depends on how well a single conditioning variable `\\Lambda(k)` captures the randomness of the components of `Y(k)`.\n\n---\n\nBased on the theoretical properties of these convex bounds, which of the following statements are **INCORRECT**?\n", "model_evaluation_result": {"model_raw_response": "B", "model_answer": ["B"], "error": null}}
{"ID": 29, "Question": "### Background\n\n**Research Question.** How can a hedonic regression model be specified to predict real estate values, and how do its properties and limitations affect its use in a trading rule?\n\n**Setting.** A log-linear hedonic regression is used to estimate the value of apartment buildings. The model's coefficients provide insight into valuation drivers, while its residuals (`ε`) are used to identify potentially mispriced properties. The trading rule's validity rests on the assumption that a negative residual primarily reflects \"transactor error\" (seller mispricing).\n\n**Variables and Parameters.**\n- `ln P`: Natural logarithm of the transaction price.\n- `age`: Age of the structure in years.\n- `Lfar`: Natural logarithm of the gross floor area.\n- `Lfas`: Natural logarithm of the average floor area per suite.\n- `B_1, B_2, B_3`: Regression coefficients.\n- `ε`: Regression residual.\n\n---\n\n### Data / Model Specification\n\nThe hedonic regression model is specified as:\n  \n\\ln P = B_{0} + B_{1} \\cdot \\text{age} + B_{2} \\cdot \\text{Lfar} + B_{3} \\cdot \\text{Lfas} + \\epsilon\n\n\\quad \\text{(Eq. 1)}\n \nThe residual `ε` is assumed to comprise three components: analyst error (model misspecification), transactor error (seller mispricing), and idiosyncratic risk.\n\n**Table 1.** Hedonic Coefficient Estimates for 1979.\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `B_1` (Age) | -0.006 | -6.4 |\n| `B_2` (Lfar) | 0.95 | 34.9 |\n| `B_3` (Lfas) | -0.33 | -3.4 |\n\n---\n\n### Question\n\nBased on the hedonic model specification and the empirical results for 1979 shown in **Table 1**, which of the following statements about the model and its application are correct?", "Options": {"A": "A large negative residual could also be caused by 'analyst error,' such as the model failing to account for a property's undesirable location, leading to a flawed investment signal.", "B": "The negative coefficient on `Lfas` (log of floor area per suite) indicates that for a building of a given size, properties with fewer, larger suites are more valuable.", "C": "The coefficient on `Lfar` (log of floor area) is an elasticity, implying that a 1% increase in a building's size is associated with approximately a 0.95% increase in its price.", "D": "The trading rule's logic assumes that a negative residual (`ε`) is primarily due to 'transactor error,' representing a genuine buying opportunity."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses understanding of the paper's core empirical tool: the hedonic regression. It uses an **Atomic Decomposition** strategy, breaking down the model's interpretation into distinct components: coefficient interpretation (A), core assumption of the trading rule (C), and a key limitation/critique (D). The primary distractor (B) is a **Conceptual Opposite**, testing for careful reading of the coefficient's implication.", "qid": "29", "question": "### Background\n\n**Research Question.** How can a hedonic regression model be specified to predict real estate values, and how do its properties and limitations affect its use in a trading rule?\n\n**Setting.** A log-linear hedonic regression is used to estimate the value of apartment buildings. The model's coefficients provide insight into valuation drivers, while its residuals (`ε`) are used to identify potentially mispriced properties. The trading rule's validity rests on the assumption that a negative residual primarily reflects \"transactor error\" (seller mispricing).\n\n**Variables and Parameters.**\n- `ln P`: Natural logarithm of the transaction price.\n- `age`: Age of the structure in years.\n- `Lfar`: Natural logarithm of the gross floor area.\n- `Lfas`: Natural logarithm of the average floor area per suite.\n- `B_1, B_2, B_3`: Regression coefficients.\n- `ε`: Regression residual.\n\n---\n\n### Data / Model Specification\n\nThe hedonic regression model is specified as:\n  \n\\ln P = B_{0} + B_{1} \\cdot \\text{age} + B_{2} \\cdot \\text{Lfar} + B_{3} \\cdot \\text{Lfas} + \\epsilon\n\n\\quad \\text{(Eq. 1)}\n \nThe residual `ε` is assumed to comprise three components: analyst error (model misspecification), transactor error (seller mispricing), and idiosyncratic risk.\n\n**Table 1.** Hedonic Coefficient Estimates for 1979.\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `B_1` (Age) | -0.006 | -6.4 |\n| `B_2` (Lfar) | 0.95 | 34.9 |\n| `B_3` (Lfas) | -0.33 | -3.4 |\n\n---\n\n### Question\n\nBased on the hedonic model specification and the empirical results for 1979 shown in **Table 1**, which of the following statements about the model and its application are correct?", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 140, "Question": "### Background\n\n**Research Question.** What economic and behavioral factors explain the wide cross-sectional variation in the returns from bond refunding operations?\n\n**Setting / Data-Generating Environment.** An empirical study of 40 bond refunding operations by public utilities in 1962-63 found a very wide range of after-tax internal rates of return (IRR), as shown in Table 1. The author investigated potential drivers of this dispersion and found no correlation between the IRRs and quantitative firm characteristics like size, profitability, or debt-to-equity ratios.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Rates of Return Earned on Forty Bond Refundings by Public Utilities in 1962 and 1963 (Percent)**\n\n| | | | |\n| :--- | :--- | :--- | :--- |\n| 3.6* | 9.1 | 11.5* | 14.7* |\n| 5.5 | 9.5 | 11.9* | 14.8 |\n| 5.7 | 9.6* | 12.3 | 15.3* |\n| 6.5 | 9.7 | 12.3 | 16.1 |\n| 6.8 | 9.9 | 12.8* | 16.1 |\n| 7.1* | 10.1 | 12.8 | 17.2 |\n| 7.3 | 10.4* | 13.0 | 19.7 |\n| 8.0 | 10.4 | 13.1 | 23.8 |\n| 8.2 | 11.2 | 13.8 | 26.7* |\n| 8.7 | 11.3 | 14.6* | 43.4 |\n\n*Note: An asterisk (*) denotes that reasons in addition to interest savings were cited for the refunding.*\n\n---\n\n### Question\n\nAccording to the paper's analysis, which of the following factors help explain the wide dispersion of IRRs observed in Table 1? Select all that apply.", "Options": {"A": "The fact that utilities in the natural gas sector were more aggressive and consistently achieved higher IRRs than electric utilities.", "B": "Firms' differing expectations about the future path of interest rates, which affected the timing of their refunding decisions.", "C": "The use of diverse and sometimes theoretically incorrect methods by different firms to measure interest savings, leading to different decision thresholds.", "D": "A systematic relationship where firms with higher debt-to-equity ratios demanded higher IRRs to compensate for financial risk."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses comprehension of the author's qualitative explanations for the empirical results. It uses a Domain Enumeration strategy, asking the user to select valid reasons for the observed IRR dispersion from a list. The correct options (A, B) are two of the five reasons explicitly stated in the paper. The distractors (C, D) are designed to be high-fidelity because they represent plausible hypotheses that the paper explicitly tested and rejected, stating that 'No correlations whatsoever were found' with financial characteristics or the 'particular type of utility'.", "qid": "140", "question": "### Background\n\n**Research Question.** What economic and behavioral factors explain the wide cross-sectional variation in the returns from bond refunding operations?\n\n**Setting / Data-Generating Environment.** An empirical study of 40 bond refunding operations by public utilities in 1962-63 found a very wide range of after-tax internal rates of return (IRR), as shown in Table 1. The author investigated potential drivers of this dispersion and found no correlation between the IRRs and quantitative firm characteristics like size, profitability, or debt-to-equity ratios.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Rates of Return Earned on Forty Bond Refundings by Public Utilities in 1962 and 1963 (Percent)**\n\n| | | | |\n| :--- | :--- | :--- | :--- |\n| 3.6* | 9.1 | 11.5* | 14.7* |\n| 5.5 | 9.5 | 11.9* | 14.8 |\n| 5.7 | 9.6* | 12.3 | 15.3* |\n| 6.5 | 9.7 | 12.3 | 16.1 |\n| 6.8 | 9.9 | 12.8* | 16.1 |\n| 7.1* | 10.1 | 12.8 | 17.2 |\n| 7.3 | 10.4* | 13.0 | 19.7 |\n| 8.0 | 10.4 | 13.1 | 23.8 |\n| 8.2 | 11.2 | 13.8 | 26.7* |\n| 8.7 | 11.3 | 14.6* | 43.4 |\n\n*Note: An asterisk (*) denotes that reasons in addition to interest savings were cited for the refunding.*\n\n---\n\n### Question\n\nAccording to the paper's analysis, which of the following factors help explain the wide dispersion of IRRs observed in Table 1? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 14, "Question": "### Background\n\nThe New Basel Capital Accord proposes a hierarchy of three approaches for calculating credit risk capital: Standardized, Foundation Internal Ratings-Based (F-IRB), and Advanced Internal Ratings-Based (A-IRB). A bank's ability to use the more advanced approaches depends on supervisory approval of its internal systems.\n\n### Data / Model Specification\n\n*   **Standardized:** Banks use external ratings to determine a regulator-prescribed Risk Weight (`RW`).\n*   **F-IRB:** Banks use internal models to estimate Probability of Default (`PD`), but other parameters like Loss Given Default (`LGD`) are supplied by the supervisor.\n*   **A-IRB:** Banks use internal models to estimate all key risk components (`PD`, `LGD`, etc.).\n\nA simplified formula for required capital (`K`) under the IRB approaches is `K = EAD × LGD × f(PD, ρ)`, where `f(·)` is a regulatory function.\n\n### Question\n\nAccording to the principles outlined in the New Basel Capital Accord, which of the following are **NOT** valid characteristics or consequences of this tiered framework? (Select all that apply.)\n", "Options": {"A": "The framework creates a supervisory challenge related to model risk, where banks may be incentivized to use overly optimistic assumptions to lower their capital requirements.", "B": "The Standardized approach is considered the most risk-sensitive because it relies on objective, external credit ratings from agencies.", "C": "A bank moving from the F-IRB to the A-IRB approach gains the ability to use its own internal estimate for Probability of Default (PD).", "D": "The framework provides a direct financial incentive (capital reduction) for banks to invest in more sophisticated, risk-sensitive internal models and data systems."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the key distinctions and rationale behind the tiered Basel framework for credit risk.\nDepth Strategy: Reverse-Reasoning. The candidate must identify statements that contradict the logic of the framework.\nInversion Protocol: The core assessment objective is to identify the single most accurate description of the framework's logic. However, several options (A, C) are also valid consequences, making a 'select all correct' format difficult. The question was inverted to 'Select all that are NOT valid'. The original correct answers (A, C) become distractors, and the original distractors (B, D) become the correct answers, satisfying the ≥2 correct answer rule.\nDistractor Logic (Post-Inversion):\n- A (Now a distractor): This is a correct statement. The primary incentive for moving up the tiers is indeed capital reduction based on more risk-sensitive measures.\n- C (Now a distractor): This is a correct statement. Allowing internal models inherently creates model risk and the potential for regulatory arbitrage, a key supervisory challenge.\n- B (Now a correct answer): This is an incorrect statement. Banks *already* use their own PD under F-IRB; the move to A-IRB allows them to also use internal estimates for LGD and EAD.\n- D (Now a correct answer): This is an incorrect statement. The paper explicitly positions the internal model approaches (IRB) as *more* risk-sensitive than the broad-brush Standardized approach.", "qid": "14", "question": "### Background\n\nThe New Basel Capital Accord proposes a hierarchy of three approaches for calculating credit risk capital: Standardized, Foundation Internal Ratings-Based (F-IRB), and Advanced Internal Ratings-Based (A-IRB). A bank's ability to use the more advanced approaches depends on supervisory approval of its internal systems.\n\n### Data / Model Specification\n\n*   **Standardized:** Banks use external ratings to determine a regulator-prescribed Risk Weight (`RW`).\n*   **F-IRB:** Banks use internal models to estimate Probability of Default (`PD`), but other parameters like Loss Given Default (`LGD`) are supplied by the supervisor.\n*   **A-IRB:** Banks use internal models to estimate all key risk components (`PD`, `LGD`, etc.).\n\nA simplified formula for required capital (`K`) under the IRB approaches is `K = EAD × LGD × f(PD, ρ)`, where `f(·)` is a regulatory function.\n\n### Question\n\nAccording to the principles outlined in the New Basel Capital Accord, which of the following are **NOT** valid characteristics or consequences of this tiered framework? (Select all that apply.)\n", "model_evaluation_result": {"model_raw_response": "\"A,D\"", "model_answer": ["A", "D"], "error": null}}
{"ID": 78, "Question": "### Background\n\n**Research Question.** The simple correlations presented in the study are suggestive but vulnerable to omitted variable bias and endogeneity, making it difficult to establish a causal link between monitoring pressure and shorter holding periods. A more robust empirical test is needed to provide more convincing evidence.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, with a focus on the subsample of pension fund managers for correlational analysis.\n\n---\n\n### Data / Model Specification\n\nThe following table presents simple correlations for the pension fund manager subsample.\n\n**Table 1. Correlation of estimates of average holding periods (in months) for pension fund managers**\n\n| Correlation between average holding period (in months) and: | Correlation Coefficient | Significance |\n| :--- | :---: | :---: |\n| frequency of performance monitoring | -0.1184 | 0.217 |\n| proportion of pay composed of bonus | -0.1760 | 0.121 |\n\n---\n\n### Question\n\nGiven the limitations of the simple correlational evidence in Table 1, which of the following features would be essential components of a more robust empirical design to test for a causal link between monitoring and short-termism?", "Options": {"A": "Using panel data that tracks individual managers over several years to see if changes in monitoring intensity for a given manager lead to changes in their holding period.", "B": "Limiting the sample to only the top-performing decile of managers to reduce noise from underperformers.", "C": "Replacing self-reported holding periods with official press commentary on short-termism to get a more objective measure.", "D": "Incorporating manager fixed effects to control for unobserved, time-invariant manager characteristics like skill or risk preference."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to move from critique to constructive research design, a key higher-order skill. It uses a 'Premise/Assumption Packaging' strategy, asking the user to select the necessary components of a superior econometric model. Correct options A and B identify the two core features of a panel fixed-effects model that would address the endogeneity and omitted variable bias issues inherent in the original study's cross-sectional correlations. Distractor C introduces a methodologically flawed idea (selection bias), while Distractor D suggests a 'True but Irrelevant' data source that does not solve the core causal inference problem.", "qid": "78", "question": "### Background\n\n**Research Question.** The simple correlations presented in the study are suggestive but vulnerable to omitted variable bias and endogeneity, making it difficult to establish a causal link between monitoring pressure and shorter holding periods. A more robust empirical test is needed to provide more convincing evidence.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, with a focus on the subsample of pension fund managers for correlational analysis.\n\n---\n\n### Data / Model Specification\n\nThe following table presents simple correlations for the pension fund manager subsample.\n\n**Table 1. Correlation of estimates of average holding periods (in months) for pension fund managers**\n\n| Correlation between average holding period (in months) and: | Correlation Coefficient | Significance |\n| :--- | :---: | :---: |\n| frequency of performance monitoring | -0.1184 | 0.217 |\n| proportion of pay composed of bonus | -0.1760 | 0.121 |\n\n---\n\n### Question\n\nGiven the limitations of the simple correlational evidence in Table 1, which of the following features would be essential components of a more robust empirical design to test for a causal link between monitoring and short-termism?", "model_evaluation_result": {"model_raw_response": "\"A,D\"", "model_answer": ["A", "D"], "error": null}}
{"ID": 71, "Question": "### Background\n\n**Research Question.** How can the performance of Bonus-Malus Systems (BMSs) be evaluated, and what are the practical limitations of common evaluation metrics, particularly when comparing systems of varying complexity?\n\n**Setting / Data-Generating Environment.** The analysis considers two key performance dimensions of a BMS. The first is its ability to differentiate between high- and low-risk drivers in its steady state, measured by elasticity. The second is the time it takes for the system to reach this steady state, measured by the rate of convergence.\n\n**Variables & Parameters.**\n- `λ`: A policyholder's individual claim frequency (claims per year).\n- `P(λ)`: The stationary average premium for a driver with frequency `λ` (currency units).\n- `η(λ)`: The elasticity of the mean stationary premium with respect to the claim frequency (dimensionless).\n- `n`: Number of years (transitions) since the policyholder entered the system.\n- `p_{ij}^n(λ)`: The probability of moving from a starting class `i` to class `j` in exactly `n` years (dimensionless).\n- `(TV)_n`: The total variation distance after `n` years, measuring the distance from the steady-state distribution (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Elasticity**\nThe elasticity `η(λ)` measures the percentage change in the long-run average premium `P(λ)` for a one percent change in the underlying risk `λ`. It is a steady-state measure of risk differentiation.\n\n  \nη(λ) = \\frac{d P(λ)/P(λ)}{dλ/λ} = \\frac{d \\ln P(λ)}{d \\ln λ} \\quad \\text{(Eq. (1))}\n \nA system is considered effective at differentiating risk if its elasticity is close to 1. Most real-world systems have `η(λ) < 1`, implying good drivers subsidize bad drivers.\n\n**Rate of Convergence**\nThe speed at which a BMS approaches its steady state can be measured by tracking the total variation distance `(TV)_n` over time. A value near 2 indicates the system is far from its steady state, while a value of 0 indicates it has fully converged.\n\n**Table 1: Total Variation for Four Systems (Starting from Entry Class, λ=0.10)**\n| Years (n) | Belgium | Japan | Taiwan | Switzerland |\n|:---------:|:-------:|:------:|:------:|:-----------:|\n| 0         | 1.9913  | 1.9950 | 2.000  | 1.9742      |\n| 10        | 1.7769  | 1.1551 | 0      | 1.0124      |\n| 20        | 0.9120  | 0.3217 | 0      | 0.3541      |\n| 30        | 0.4209  | 0.0529 | 0      | 0.1348      |\n| 60        | 0.0382  | 0.0007 | 0      | 0.0061      |\n\n---\n\n### Question\n\nBased on the provided information, select all statements that are correct interpretations of BMS performance metrics.", "Options": {"A": "Based on Table 1, the Belgian system converges to its steady state much more slowly than the Japanese system.", "B": "According to Table 1, the Taiwanese system takes more than 20 years to fully stabilize.", "C": "A low elasticity value (e.g., `η(λ) < 1`) indicates that cross-subsidization from bad drivers to good drivers is minimal.", "D": "An elasticity of `η(λ) = 0.4` implies that a driver who is 20% riskier (higher `λ`) will pay approximately 8% more in long-run average premiums."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret two key BMS performance metrics: elasticity (a measure of risk differentiation) and rate of convergence (a measure of speed to stationarity). It uses an Atomic Decomposition strategy, breaking down the original QA's complex synthesis into distinct, verifiable statements. Distractor B is a direct misreading of the provided table (Numerical Misreading). Distractor D represents a fundamental misunderstanding of elasticity's implication for cross-subsidization (Conceptual Opposite).", "qid": "71", "question": "### Background\n\n**Research Question.** How can the performance of Bonus-Malus Systems (BMSs) be evaluated, and what are the practical limitations of common evaluation metrics, particularly when comparing systems of varying complexity?\n\n**Setting / Data-Generating Environment.** The analysis considers two key performance dimensions of a BMS. The first is its ability to differentiate between high- and low-risk drivers in its steady state, measured by elasticity. The second is the time it takes for the system to reach this steady state, measured by the rate of convergence.\n\n**Variables & Parameters.**\n- `λ`: A policyholder's individual claim frequency (claims per year).\n- `P(λ)`: The stationary average premium for a driver with frequency `λ` (currency units).\n- `η(λ)`: The elasticity of the mean stationary premium with respect to the claim frequency (dimensionless).\n- `n`: Number of years (transitions) since the policyholder entered the system.\n- `p_{ij}^n(λ)`: The probability of moving from a starting class `i` to class `j` in exactly `n` years (dimensionless).\n- `(TV)_n`: The total variation distance after `n` years, measuring the distance from the steady-state distribution (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Elasticity**\nThe elasticity `η(λ)` measures the percentage change in the long-run average premium `P(λ)` for a one percent change in the underlying risk `λ`. It is a steady-state measure of risk differentiation.\n\n  \nη(λ) = \\frac{d P(λ)/P(λ)}{dλ/λ} = \\frac{d \\ln P(λ)}{d \\ln λ} \\quad \\text{(Eq. (1))}\n \nA system is considered effective at differentiating risk if its elasticity is close to 1. Most real-world systems have `η(λ) < 1`, implying good drivers subsidize bad drivers.\n\n**Rate of Convergence**\nThe speed at which a BMS approaches its steady state can be measured by tracking the total variation distance `(TV)_n` over time. A value near 2 indicates the system is far from its steady state, while a value of 0 indicates it has fully converged.\n\n**Table 1: Total Variation for Four Systems (Starting from Entry Class, λ=0.10)**\n| Years (n) | Belgium | Japan | Taiwan | Switzerland |\n|:---------:|:-------:|:------:|:------:|:-----------:|\n| 0         | 1.9913  | 1.9950 | 2.000  | 1.9742      |\n| 10        | 1.7769  | 1.1551 | 0      | 1.0124      |\n| 20        | 0.9120  | 0.3217 | 0      | 0.3541      |\n| 30        | 0.4209  | 0.0529 | 0      | 0.1348      |\n| 60        | 0.0382  | 0.0007 | 0      | 0.0061      |\n\n---\n\n### Question\n\nBased on the provided information, select all statements that are correct interpretations of BMS performance metrics.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 86, "Question": "### Background\n\nThe study employs a \"price-reversal\" test to enhance the internal validity of its findings. This test examines the correlation of firm-level abnormal returns across consecutive legislative news events. The core hypothesis is that the correlation should be negative for contradictory news (e.g., an `Aggressive` event followed by a `Moderating` one) and positive for similar news (e.g., two `Moderating` events).\n\n### Data / Model Specification\n\nThe table below presents Pearson Correlation Coefficients (PCC) for two pairs of consecutive events.\n\n**Table 1: Pearson Correlation Coefficients (PCC) for Consecutive Events**\n\n| Correlation Between Events                  | Expected Sign | PCC (p-value)   |\n| :------------------------------------------ | :-----------: | :-------------: |\n| Mar 30 (Aggressive) and May 30 (Moderating) | -             | -0.160 (.041)   |\n| May 30 (Moderating) and Jun 13 (Moderating) | +             | -0.184 (.020)   |\n\n---\n\nBased on the price-reversal test logic and the results in Table 1, which of the following conclusions are supported?\n", "Options": {"A": "The results for the March/May event pair help rule out the alternative explanation that returns are driven by persistent, firm-specific trends unrelated to the legislation.", "B": "The correlation between the two moderating events (May 30 and June 13) is positive and statistically significant, confirming the study's hypothesis.", "C": "The significant negative correlation between the March 30 (Aggressive) and May 30 (Moderating) events is consistent with the price-reversal hypothesis.", "D": "The significant negative correlation between the two moderating events (May 30 and June 13) is an anomaly that contradicts the price-reversal hypothesis."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to understand the causal logic of the price-reversal test, interpret correlation coefficients and p-values, and identify results that are either consistent or anomalous with the stated hypothesis. Depth Strategy: Reverse-Reasoning. The user is given empirical results and must identify the plausible causal inferences and methodological conclusions. Distractor Logic: D is a conceptual opposite that correctly identifies the expected sign but misreads the actual result from the table, representing a common error of conflating expectation with observation.", "qid": "86", "question": "### Background\n\nThe study employs a \"price-reversal\" test to enhance the internal validity of its findings. This test examines the correlation of firm-level abnormal returns across consecutive legislative news events. The core hypothesis is that the correlation should be negative for contradictory news (e.g., an `Aggressive` event followed by a `Moderating` one) and positive for similar news (e.g., two `Moderating` events).\n\n### Data / Model Specification\n\nThe table below presents Pearson Correlation Coefficients (PCC) for two pairs of consecutive events.\n\n**Table 1: Pearson Correlation Coefficients (PCC) for Consecutive Events**\n\n| Correlation Between Events                  | Expected Sign | PCC (p-value)   |\n| :------------------------------------------ | :-----------: | :-------------: |\n| Mar 30 (Aggressive) and May 30 (Moderating) | -             | -0.160 (.041)   |\n| May 30 (Moderating) and Jun 13 (Moderating) | +             | -0.184 (.020)   |\n\n---\n\nBased on the price-reversal test logic and the results in Table 1, which of the following conclusions are supported?\n", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 35, "Question": "### Background\n\nThe study proposes and compares two distinct architectures for combining a linear ARMA model with a non-linear Neural Network (NN) to forecast financial returns. This approach is motivated by the assumption that financial time series contain both linear and non-linear predictable components.\n\n### Data / Model Specification\n\nThe study defines two combination architectures:\n\n1.  **Hybrid Architecture:** A three-step, additive process. First, an ARMA model is fitted. Second, an NN is trained to forecast the ARMA residuals (`ε_t`). Third, the final forecast is the sum of the two component forecasts.\n\n      \n    \\hat{Y}_{t+1|t}^{\\text{Hybrid}} = \\hat{Y}_{t+1|t}^{\\text{ARMA}} + \\hat{\\varepsilon}_{t+1|t}^{\\text{NN}} \n     \n\n2.  **Mixed Architecture:** A two-step process where the ARMA forecast is used as an input feature for the NN. The NN's target is the actual return `Y_{t+1}`, and its input vector is augmented with the ARMA forecast.\n\n      \n    \\hat{Y}_{t+1|t}^{\\text{Mixed}} = \\text{NN}([X_t, \\hat{Y}_{t+1|t}^{\\text{ARMA}}])\n     \n\n---\n\nBased on these definitions, which of the following statements accurately describe one or both of these architectures?\n", "Options": {"A": "The Mixed architecture allows the NN to learn non-linear interactions between the ARMA forecast and other input variables.", "B": "Both architectures use the actual return `Y_{t+1}` as the direct target variable for the NN training process.", "C": "In the Hybrid architecture, the NN is trained to predict the errors made by the ARMA model.", "D": "The Hybrid architecture implicitly assumes a strictly additive relationship between the linear and non-linear components of the return series."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the fundamental structural and conceptual differences between the Hybrid and Mixed modeling approaches. Strategy: Reverse-Reasoning (identifying correct descriptions of the models). Distractor Logic: Options A, B, and D are direct, correct interpretations of the model definitions. Option A correctly identifies the NN's target in the Hybrid model. Option B correctly describes the flexibility of the Mixed model. Option D correctly identifies the implicit assumption of the Hybrid model's additive structure. Option C is a 'Conceptual Distractor' that is only true for the Mixed model; in the Hybrid model, the NN's target is the ARMA residual, not the actual return, making the statement false.", "qid": "35", "question": "### Background\n\nThe study proposes and compares two distinct architectures for combining a linear ARMA model with a non-linear Neural Network (NN) to forecast financial returns. This approach is motivated by the assumption that financial time series contain both linear and non-linear predictable components.\n\n### Data / Model Specification\n\nThe study defines two combination architectures:\n\n1.  **Hybrid Architecture:** A three-step, additive process. First, an ARMA model is fitted. Second, an NN is trained to forecast the ARMA residuals (`ε_t`). Third, the final forecast is the sum of the two component forecasts.\n\n      \n    \\hat{Y}_{t+1|t}^{\\text{Hybrid}} = \\hat{Y}_{t+1|t}^{\\text{ARMA}} + \\hat{\\varepsilon}_{t+1|t}^{\\text{NN}} \n     \n\n2.  **Mixed Architecture:** A two-step process where the ARMA forecast is used as an input feature for the NN. The NN's target is the actual return `Y_{t+1}`, and its input vector is augmented with the ARMA forecast.\n\n      \n    \\hat{Y}_{t+1|t}^{\\text{Mixed}} = \\text{NN}([X_t, \\hat{Y}_{t+1|t}^{\\text{ARMA}}])\n     \n\n---\n\nBased on these definitions, which of the following statements accurately describe one or both of these architectures?\n", "model_evaluation_result": {"model_raw_response": "\"A,C,D\"", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 95, "Question": "### Background\n\nThe author critiques a specific provision in the Dodd-Frank Act which states that margin required by a derivatives clearing organization (CCP) \"shall be sufficient to cover potential exposures in normal market conditions.\" The author argues this standard is fundamentally inadequate for preventing systemic crises, which are, by definition, abnormal, tail-risk events.\n\n### Scenario\n\nA CCP implements its margin policy based on the Dodd-Frank provision. It calculates required margin (`M`) using a Value-at-Risk (`VaR`) model based on recent historical data. This model is known to be pro-cyclical: in calm markets, low volatility leads to low `VaR` and low margins; in volatile markets, high `VaR` leads to high margins and large margin calls.\n\n### Question\n\nAccording to the author's analysis, which of the following are valid criticisms or consequences of a margin policy based strictly on covering losses in \"normal market conditions\"?\n\nSelect all that apply.", "Options": {"A": "The policy effectively ensures that systemically threatening tail risks—losses that occur in abnormal conditions—are not collateralized at the transaction level.", "B": "The policy creates a dangerous feedback loop where forced selling to meet margin calls increases market volatility, which in turn triggers even higher margin requirements.", "C": "This margin policy is robust because it prevents the CCP from over-collateralizing positions, thereby improving market liquidity and efficiency during periods of stress.", "D": "During a market crisis, the pro-cyclical nature of the margin calculation can force the CCP to make massive, simultaneous margin calls, amplifying the crisis by causing forced asset sales."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses understanding of the author's critique of the Dodd-Frank margin rule and the concept of pro-cyclicality. It uses a Reverse-Reasoning strategy, asking for the consequences of a flawed policy. Correct options (A, B, D) describe the core problems identified in the paper: the failure to price tail risk (A) and the dangerous, crisis-amplifying feedback loop of pro-cyclical margin calls (B, D). The distractor (C) is a 'Conceptual Opposite,' presenting a positive but false interpretation of the policy's effects during a crisis, which a candidate must correctly identify as incorrect.", "qid": "95", "question": "### Background\n\nThe author critiques a specific provision in the Dodd-Frank Act which states that margin required by a derivatives clearing organization (CCP) \"shall be sufficient to cover potential exposures in normal market conditions.\" The author argues this standard is fundamentally inadequate for preventing systemic crises, which are, by definition, abnormal, tail-risk events.\n\n### Scenario\n\nA CCP implements its margin policy based on the Dodd-Frank provision. It calculates required margin (`M`) using a Value-at-Risk (`VaR`) model based on recent historical data. This model is known to be pro-cyclical: in calm markets, low volatility leads to low `VaR` and low margins; in volatile markets, high `VaR` leads to high margins and large margin calls.\n\n### Question\n\nAccording to the author's analysis, which of the following are valid criticisms or consequences of a margin policy based strictly on covering losses in \"normal market conditions\"?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "\"A,B,D\"", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 47, "Question": "### Background\n\n**Research Question.** How robust are hedging strategies for Variable Annuities to the choice of the underlying asset price model, particularly the standard Black-Scholes assumption of a Geometric Brownian Motion (GBM)?\n\n**Setting.** An insurer implements a sophisticated hedging strategy for a long-maturity VA guarantee. The effectiveness of the hedge depends critically on the assumed dynamics of the underlying financial index, `S_t`. The paper's primary analysis relies on the GBM model but includes a robustness check using a model with price jumps.\n\n**Variables and Parameters.**\n- `S_t`: Value of the underlying index at time `t`.\n- `μ`: Constant expected return (drift) of the index.\n- `σ`: Constant volatility of the index.\n- `W_t`: A standard Brownian motion.\n\n---\n\n### Data / Model Specification\n\nThe primary analysis assumes the index `S_t` follows a Geometric Brownian Motion (GBM), which implies that log-returns are normally distributed with constant volatility:\n\n  \n\\frac{d S_{t}}{S_{t}}=\\mu d t+\\sigma d W_{t}\n \n(Eq. 1)\n\n---\n\n### Question\n\nThe Geometric Brownian Motion (GBM) model in Eq. (1) makes key simplifying assumptions about asset returns. Which of the following are well-documented empirical features of financial asset returns that **contradict** the assumptions of the GBM model? (Select all that apply)", "Options": {"A": "The absence of risk-free arbitrage opportunities in the market.", "B": "Leptokurtosis (the return distribution has \"fat tails,\" meaning extreme events are more common than the model predicts).", "C": "Log-returns follow a perfect normal distribution.", "D": "Volatility Clustering (periods of high volatility tend to be followed by periods of high volatility)."}, "Answer": ["B", "D"], "pi_justification": "This question assesses foundational knowledge of empirical asset pricing, which is critical context for the paper's model risk discussion. It uses a select-all-that-apply format to test for specific, named concepts. Distractor C is a 'Conceptual Opposite'—it states the model's assumption, not the empirical contradiction. Distractor D is 'True but Irrelevant'—it's a foundational assumption for the model's framework, not a contradiction of its dynamic properties.", "qid": "47", "question": "### Background\n\n**Research Question.** How robust are hedging strategies for Variable Annuities to the choice of the underlying asset price model, particularly the standard Black-Scholes assumption of a Geometric Brownian Motion (GBM)?\n\n**Setting.** An insurer implements a sophisticated hedging strategy for a long-maturity VA guarantee. The effectiveness of the hedge depends critically on the assumed dynamics of the underlying financial index, `S_t`. The paper's primary analysis relies on the GBM model but includes a robustness check using a model with price jumps.\n\n**Variables and Parameters.**\n- `S_t`: Value of the underlying index at time `t`.\n- `μ`: Constant expected return (drift) of the index.\n- `σ`: Constant volatility of the index.\n- `W_t`: A standard Brownian motion.\n\n---\n\n### Data / Model Specification\n\nThe primary analysis assumes the index `S_t` follows a Geometric Brownian Motion (GBM), which implies that log-returns are normally distributed with constant volatility:\n\n  \n\\frac{d S_{t}}{S_{t}}=\\mu d t+\\sigma d W_{t}\n \n(Eq. 1)\n\n---\n\n### Question\n\nThe Geometric Brownian Motion (GBM) model in Eq. (1) makes key simplifying assumptions about asset returns. Which of the following are well-documented empirical features of financial asset returns that **contradict** the assumptions of the GBM model? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 10, "Question": "### Background\n\n**Research Question.** How can joint statistical tests distinguish between predictors whose apparent long-horizon success is a statistical artifact of their own persistence, and those that exhibit genuine predictive power for stock returns?\n\n**Setting and Environment.** This case examines the empirical results from regressing U.S. stock returns on two different predictors over multiple horizons (1-5 years) for the period 1926–2004. The analysis contrasts a traditional, highly persistent predictor (log dividend yield) with a more modern, less persistent one (log net payout yield).\n\n### Data / Model Specification\n\nThe following table synthesizes empirical findings for the two predictors. The joint test evaluates the null hypothesis `H₀: β₁ = β₂ = β₃ = β₄ = β₅ = 0`.\n\n**Table 1: Empirical Results for Two Predictors (1926-2004)**\n\n| Predictor / Statistic | Horizon 1 | Horizon 2 | Horizon 3 | Horizon 4 | Horizon 5 | Joint Test |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **Log Dividend Yield** | | | | | | |\n| `β̂_j` | 0.131 | 0.257 | 0.390 | 0.461 | 0.521 | |\n| `R_j²` (%) | 5.16 | 9.55 | 15.84 | 18.14 | 20.76 | |\n| `β̂_j / β̂₁` | 1.00 | 1.96 | 2.98 | 3.53 | 3.99 | |\n| Persistence `ρ̂₁c` | 0.953 | | | | | |\n| **Wald (Sim. p-value)** | | | | | | **0.293** |\n| **Log Net Payout Yield** | | | | | | |\n| `β̂_j` | 0.718 | 1.321 | 1.536 | 1.537 | 1.512 | |\n| `R_j²` (%) | 23.40 | 37.99 | 36.89 | 30.25 | 26.25 | |\n| Persistence `ρ̂₁c` | 0.713 | | | | | |\n| **Wald (Sim. p-value)** | | | | | | **0.001** |\n\n### Question\n\nThe table presents empirical results for two predictors: the highly persistent Log Dividend Yield and the less persistent Log Net Payout Yield. According to the paper's framework, which of the following interpretations are valid? Select all that apply.", "Options": {"A": "Any predictor with high persistence like the Log Dividend Yield (ρ=0.953) will fail to reject the joint Wald test, regardless of its underlying predictive power.", "B": "The results provide stronger evidence that the Log Net Payout Yield, rather than the Log Dividend Yield, represents a genuine priced risk factor in the context of asset pricing models.", "C": "The joint Wald test for the Log Dividend Yield is not statistically significant (p=0.293) because its monotonically increasing coefficient ratios are almost identical to the spurious pattern predicted by the null hypothesis for a predictor with high persistence (ρ=0.953).", "D": "The Log Net Payout Yield is identified as a genuine predictor because its non-monotonic R² pattern is highly inconsistent with the pattern expected under the null, leading to a significant joint test (p=0.001)."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the application of the paper's joint testing framework to contrast a spurious predictor with a genuine one. It uses the Atomic Decomposition strategy, converting the three parts of the original QA into three correct, independent statements (A, B, D). The distractor (C) is a high-fidelity 'Almost Right' option that presents a common oversimplification of the paper's nuanced argument about patterns, not just persistence.", "qid": "10", "question": "### Background\n\n**Research Question.** How can joint statistical tests distinguish between predictors whose apparent long-horizon success is a statistical artifact of their own persistence, and those that exhibit genuine predictive power for stock returns?\n\n**Setting and Environment.** This case examines the empirical results from regressing U.S. stock returns on two different predictors over multiple horizons (1-5 years) for the period 1926–2004. The analysis contrasts a traditional, highly persistent predictor (log dividend yield) with a more modern, less persistent one (log net payout yield).\n\n### Data / Model Specification\n\nThe following table synthesizes empirical findings for the two predictors. The joint test evaluates the null hypothesis `H₀: β₁ = β₂ = β₃ = β₄ = β₅ = 0`.\n\n**Table 1: Empirical Results for Two Predictors (1926-2004)**\n\n| Predictor / Statistic | Horizon 1 | Horizon 2 | Horizon 3 | Horizon 4 | Horizon 5 | Joint Test |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **Log Dividend Yield** | | | | | | |\n| `β̂_j` | 0.131 | 0.257 | 0.390 | 0.461 | 0.521 | |\n| `R_j²` (%) | 5.16 | 9.55 | 15.84 | 18.14 | 20.76 | |\n| `β̂_j / β̂₁` | 1.00 | 1.96 | 2.98 | 3.53 | 3.99 | |\n| Persistence `ρ̂₁c` | 0.953 | | | | | |\n| **Wald (Sim. p-value)** | | | | | | **0.293** |\n| **Log Net Payout Yield** | | | | | | |\n| `β̂_j` | 0.718 | 1.321 | 1.536 | 1.537 | 1.512 | |\n| `R_j²` (%) | 23.40 | 37.99 | 36.89 | 30.25 | 26.25 | |\n| Persistence `ρ̂₁c` | 0.713 | | | | | |\n| **Wald (Sim. p-value)** | | | | | | **0.001** |\n\n### Question\n\nThe table presents empirical results for two predictors: the highly persistent Log Dividend Yield and the less persistent Log Net Payout Yield. According to the paper's framework, which of the following interpretations are valid? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\"B,C,D\"", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 82, "Question": "### Background\n\n**Research Question.** How does severe multicollinearity manifest in the estimated coefficients of a housing price model, and can automated variable selection methods like Stepwise Regression (SWR) reliably solve this problem?\n\n**Setting.** Ordinary Least Squares (OLS) and Stepwise Regression (SWR) models are estimated to explain housing sales prices using a large set of property characteristics.\n\n**Variables & Parameters.**\n\n*   `SQFT`: Square feet of living area.\n*   `BDRMS`: Number of bedrooms.\n*   `BATHRMS`: Number of bathrooms.\n*   `hat(b)ⱼ`: The estimated regression coefficient for variable `j`.\n\n---\n\n### Data / Model Specification\n\nThe paper assumes that explanatory variables should be linearly independent for their coefficients to be accurate measures of marginal influence. When this is violated, structural analysis is compromised. Table 1 summarizes the OLS and SWR model results.\n\n**Table 1: Summary of OLS and SWR Models**\n\n| Selected Variables | OLS Regression Coefficient | SWR Regression Coefficient |\n| :--- | :---: | :---: |\n| SQFT | 17.179 | 17.150 |\n| BDRMS | -877.782 | -1019.325 |\n| BATHRMS | -1058.141 | -1096.783 |\n\n*Note: The SWR procedure used a partial F-statistic criterion for variable selection.*\n\n---\n\nBased on the results in Table 1 and the principles discussed in the paper, which of the following statements are valid conclusions regarding the effects of multicollinearity and the utility of Stepwise Regression (SWR)?", "Options": {"A": "The negative coefficients for `BDRMS` and `BATHRMS` in the OLS model are likely statistical artifacts caused by the model's inability to disentangle their effects from the highly correlated `SQFT` variable.", "B": "The persistence of theoretically incorrect negative signs for `BDRMS` and `BATHRMS` in the SWR model demonstrates that variable selection based on individual significance tests (like the partial F-test) does not guarantee an independent set of final predictors.", "C": "The SWR model successfully resolves the multicollinearity issue, as evidenced by its selection of a smaller, more parsimonious set of variables.", "D": "The OLS coefficient for `SQFT` (17.179) is an unbiased and accurate measure of the marginal value of a square foot, as it has the correct sign and is highly significant."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret the classic symptoms of multicollinearity (counter-intuitive coefficient signs) and critique the limitations of automated variable selection methods like SWR. Depth Strategy: Reverse-Reasoning. The user is given the empirical results (negative signs) and must identify the plausible statistical causes and methodological failures. Distractor Logic: Option B is a conceptual opposite, incorrectly claiming SWR solved the problem. Option D is 'Almost Right'—the sign is correct, but the paper's core argument is that multicollinearity makes this coefficient an inaccurate measure of marginal impact, as it also captures the effects of correlated variables.", "qid": "82", "question": "### Background\n\n**Research Question.** How does severe multicollinearity manifest in the estimated coefficients of a housing price model, and can automated variable selection methods like Stepwise Regression (SWR) reliably solve this problem?\n\n**Setting.** Ordinary Least Squares (OLS) and Stepwise Regression (SWR) models are estimated to explain housing sales prices using a large set of property characteristics.\n\n**Variables & Parameters.**\n\n*   `SQFT`: Square feet of living area.\n*   `BDRMS`: Number of bedrooms.\n*   `BATHRMS`: Number of bathrooms.\n*   `hat(b)ⱼ`: The estimated regression coefficient for variable `j`.\n\n---\n\n### Data / Model Specification\n\nThe paper assumes that explanatory variables should be linearly independent for their coefficients to be accurate measures of marginal influence. When this is violated, structural analysis is compromised. Table 1 summarizes the OLS and SWR model results.\n\n**Table 1: Summary of OLS and SWR Models**\n\n| Selected Variables | OLS Regression Coefficient | SWR Regression Coefficient |\n| :--- | :---: | :---: |\n| SQFT | 17.179 | 17.150 |\n| BDRMS | -877.782 | -1019.325 |\n| BATHRMS | -1058.141 | -1096.783 |\n\n*Note: The SWR procedure used a partial F-statistic criterion for variable selection.*\n\n---\n\nBased on the results in Table 1 and the principles discussed in the paper, which of the following statements are valid conclusions regarding the effects of multicollinearity and the utility of Stepwise Regression (SWR)?", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 37, "Question": "### Background\n\n**Research Question:** This case dissects the informational value of the ECB's 2014 Comprehensive Assessment (CA) by testing two competing hypotheses: Did the CA provide new, bank-specific information about capital adequacy (the \"sorting hypothesis\"), or did it primarily signal the supervisor's future policy stance and risk preferences (the \"signaling hypothesis\")?\n\n**Setting and Sample:** The analysis uses a cross-sectional regression of post-announcement stock returns for 37 listed European banks. The dependent variable is the cumulative abnormal return (CAR) over the 5 trading days following the CA announcement, `CAR_i[0,+5]`.\n\n### Data / Model Specification\n\nThe two hypotheses are tested with a horse race between two types of models explaining `CAR_i[0,+5]`.\n\n1.  **Sorting Hypothesis Model:** Assumes CARs are driven by the new information in the announced capital curtailments.\n      \n    CAR_{i}[0,+5] = \\beta_{0} + \\beta_{1}AQR_{i} + \\beta_{2}STE_{i} + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n     \n2.  **Signaling Hypothesis Model:** Assumes CARs are driven by a market re-evaluation of banks based on pre-existing characteristics (`Z_i`) that the CA revealed to be disfavored by the supervisor.\n      \n    CAR_{i}[0,+5] = \\gamma_{0} + \\gamma_{1}(SME \\, Exposure)_{i} + \\gamma_{2}\\delta_{Italy} + ... + e_{i} \\quad \\text{(Eq. (2))}\n     \n\n**Table 1: Sorting vs. Signaling Hypothesis Regression Results**\n\n| | **Panel A: Sorting Model** | **Panel B: Signaling Model** |\n| :--- | :--- | :--- |\n| **Variable** | **Coefficient (p-value)** | **Coefficient (p-value)** |\n| AQR (%) | -1.62 (0.1542) | | \n| STE (%) | -0.06 (0.9190) | | \n| SME Exposure | | -0.56*** (0.0028) |\n| Dummy Italy | | -0.05** (0.0416) |\n| **R-squared** | **0.07** | **0.62** |\n| **Adj. R-squared** | **0.02** | **0.51** |\n\n*Source: Adapted from Table 7 in the source paper. `***` and `**` denote significance at the 1% and 5% levels. Panel B shows results from the more parsimonious stepwise regression.*\n\n### Question\n\nBased on the provided models and regression results in **Table 1**, which of the following conclusions are supported by the evidence?", "Options": {"A": "The \"signaling hypothesis\" is supported, as the model in Panel B explains a substantial portion of the variation in returns (Adj. R² = 0.51), and key pre-existing bank characteristics are significant predictors.", "B": "The \"sorting hypothesis\" is rejected, as the model in Panel A has negligible explanatory power (Adj. R² = 0.02) and the coefficients for `AQR` and `STE` are statistically insignificant.", "C": "The significant negative coefficient on `SME Exposure` indicates that the market viewed banks with high SME lending as less risky following the CA announcement.", "D": "The results suggest that the announced capital curtailments (`AQR` and `STE`) were the primary drivers of the market's reaction, but their effect was captured indirectly through the signaling model's variables."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret a 'horse race' between two competing empirical models, a central task in empirical finance. It uses an **Atomic Decomposition** strategy, breaking down the overall conclusion into testable statements about each model's validity (Options A & B). Distractors are based on common errors: a **Conceptual Opposite** interpretation of a coefficient's economic meaning (Option C) and a misattribution of explanatory power between the models (Option D).", "qid": "37", "question": "### Background\n\n**Research Question:** This case dissects the informational value of the ECB's 2014 Comprehensive Assessment (CA) by testing two competing hypotheses: Did the CA provide new, bank-specific information about capital adequacy (the \"sorting hypothesis\"), or did it primarily signal the supervisor's future policy stance and risk preferences (the \"signaling hypothesis\")?\n\n**Setting and Sample:** The analysis uses a cross-sectional regression of post-announcement stock returns for 37 listed European banks. The dependent variable is the cumulative abnormal return (CAR) over the 5 trading days following the CA announcement, `CAR_i[0,+5]`.\n\n### Data / Model Specification\n\nThe two hypotheses are tested with a horse race between two types of models explaining `CAR_i[0,+5]`.\n\n1.  **Sorting Hypothesis Model:** Assumes CARs are driven by the new information in the announced capital curtailments.\n      \n    CAR_{i}[0,+5] = \\beta_{0} + \\beta_{1}AQR_{i} + \\beta_{2}STE_{i} + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n     \n2.  **Signaling Hypothesis Model:** Assumes CARs are driven by a market re-evaluation of banks based on pre-existing characteristics (`Z_i`) that the CA revealed to be disfavored by the supervisor.\n      \n    CAR_{i}[0,+5] = \\gamma_{0} + \\gamma_{1}(SME \\, Exposure)_{i} + \\gamma_{2}\\delta_{Italy} + ... + e_{i} \\quad \\text{(Eq. (2))}\n     \n\n**Table 1: Sorting vs. Signaling Hypothesis Regression Results**\n\n| | **Panel A: Sorting Model** | **Panel B: Signaling Model** |\n| :--- | :--- | :--- |\n| **Variable** | **Coefficient (p-value)** | **Coefficient (p-value)** |\n| AQR (%) | -1.62 (0.1542) | | \n| STE (%) | -0.06 (0.9190) | | \n| SME Exposure | | -0.56*** (0.0028) |\n| Dummy Italy | | -0.05** (0.0416) |\n| **R-squared** | **0.07** | **0.62** |\n| **Adj. R-squared** | **0.02** | **0.51** |\n\n*Source: Adapted from Table 7 in the source paper. `***` and `**` denote significance at the 1% and 5% levels. Panel B shows results from the more parsimonious stepwise regression.*\n\n### Question\n\nBased on the provided models and regression results in **Table 1**, which of the following conclusions are supported by the evidence?", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 74, "Question": "### Background\n\n**Research Question.** How does the design complexity of a Bonus-Malus System (BMS)—specifically the number of classes and the severity of penalties—influence its function as a risk classification tool and its financial stability?\n\n**Setting / Data-Generating Environment.** The analysis contrasts two real-world BMS designs: the simple 7-class Brazilian system and the more sophisticated 23-class Belgian system. Their key distinction lies in their transition rules for claim-free years versus years with at-fault claims.\n\n---\n\n### Data / Model Specification\n\n**Brazilian System Rules:**\n- Number of classes: 7\n- Reward (0 claims): `ΔC(0) = -1` (one-class discount)\n- Penalty (1 claim): `ΔC(1) ≈ +1` (one-class penalty, for most classes)\n\n**Table 1: Brazilian System Transition Rules (Selected Classes)**\n| Class | Premium | 0 Claims | 1 Claim | 2 Claims |\n|:-----:|:-------:|:--------:|:-------:|:--------:|\n| 7     | 100     | 6        | 7       | 7        |\n| 6     | 90      | 5        | 7       | 7        |\n| 5     | 85      | 4        | 6       | 7        |\n\n**Belgian System Rules:**\n- Number of classes: 23\n- Reward (0 claims): `ΔC(0) = -1` (one-class discount)\n- Penalty (1st claim): `ΔC(1) = +4` (four-class penalty)\n\n**Aggregate Portfolio Dynamics:**\nFor a BMS to be in financial equilibrium, the total number of penalty classes applied across the portfolio must roughly balance the total number of discount classes awarded. In a typical portfolio with an average claim frequency `λ_avg ≈ 0.1`, about 90% of drivers are claim-free in a given year.\n\n---\n\n### Question\n\nBased on the provided information, select all statements that correctly compare the BMS designs or describe the conditions for financial equilibrium.", "Options": {"A": "For a BMS portfolio with an average claim frequency of `λ=0.1` to remain in financial equilibrium, the average penalty (in classes) for a driver with claims must be significantly larger than the one-class discount for a claim-free driver.", "B": "The Belgian system's design, where a single claim is penalized by four classes, implies it takes four claim-free years to recover from one at-fault claim.", "C": "In the Brazilian system, a driver in Class 6 who has one at-fault claim will move to Class 5.", "D": "The Brazilian system's lenient, symmetric penalty structure is more effective at separating high-risk and low-risk drivers than the Belgian system's asymmetric structure."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to compare different BMS designs and understand the concept of portfolio-level financial equilibrium. It uses an Atomic Decomposition strategy. Option A requires interpreting the penalty-reward structure. Option C tests the conceptual understanding of the equilibrium condition. Distractor B is a Conceptual Opposite regarding the effectiveness of different penalty structures. Distractor D is a simple Table Misreading error.", "qid": "74", "question": "### Background\n\n**Research Question.** How does the design complexity of a Bonus-Malus System (BMS)—specifically the number of classes and the severity of penalties—influence its function as a risk classification tool and its financial stability?\n\n**Setting / Data-Generating Environment.** The analysis contrasts two real-world BMS designs: the simple 7-class Brazilian system and the more sophisticated 23-class Belgian system. Their key distinction lies in their transition rules for claim-free years versus years with at-fault claims.\n\n---\n\n### Data / Model Specification\n\n**Brazilian System Rules:**\n- Number of classes: 7\n- Reward (0 claims): `ΔC(0) = -1` (one-class discount)\n- Penalty (1 claim): `ΔC(1) ≈ +1` (one-class penalty, for most classes)\n\n**Table 1: Brazilian System Transition Rules (Selected Classes)**\n| Class | Premium | 0 Claims | 1 Claim | 2 Claims |\n|:-----:|:-------:|:--------:|:-------:|:--------:|\n| 7     | 100     | 6        | 7       | 7        |\n| 6     | 90      | 5        | 7       | 7        |\n| 5     | 85      | 4        | 6       | 7        |\n\n**Belgian System Rules:**\n- Number of classes: 23\n- Reward (0 claims): `ΔC(0) = -1` (one-class discount)\n- Penalty (1st claim): `ΔC(1) = +4` (four-class penalty)\n\n**Aggregate Portfolio Dynamics:**\nFor a BMS to be in financial equilibrium, the total number of penalty classes applied across the portfolio must roughly balance the total number of discount classes awarded. In a typical portfolio with an average claim frequency `λ_avg ≈ 0.1`, about 90% of drivers are claim-free in a given year.\n\n---\n\n### Question\n\nBased on the provided information, select all statements that correctly compare the BMS designs or describe the conditions for financial equilibrium.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 76, "Question": "### Background\n\nAn insurer is evaluating three alternative reinsurance strategies for its crop insurance portfolio. The performance of each strategy is assessed using an Asset-Liability Management (ALM) framework, simulated 5,000 times over a 30-year horizon.\n\n**Reinsurance Models:**\n- **Model A (Joint Self-Managed Pool):** Retains all risks internally.\n- **Model B (Group Private Reinsurance):** Cedes all risks to private reinsurers.\n- **Model C (Optimized Portfolio):** Strategically retains a lower-risk group and cedes a higher-risk group.\n\nA regulator imposes a new solvency requirement: the reinsurance pool must hold sufficient initial capital (U₀) to ensure that the 5th percentile of its surplus distribution at year 30 is non-negative (≥ $0).\n\n### Data / Model Specification\n\nThe performance of the three models is evaluated based on simulations assuming zero initial surplus (U₀ = $0). Table 1 presents the 5th percentile surplus from these simulations.\n\n**Table 1: 5th Percentile Surplus at End of Year 30 (U₀ = $0)**\n\n| Model | 5th Percentile Surplus |\n| :--- | :--- |\n| Model A | -$112,903,478 |\n| Model B | -$976,132,222 |\n| Model C | -$552,079,233 |\n\n### Question\n\nBased on the regulatory requirement and the data in Table 1, which of the following conclusions are valid? Select all that apply.", "Options": {"A": "Model C requires more than four times the initial capital of Model A to satisfy the regulatory constraint.", "B": "The required initial capital for Model C is $552,079,233.", "C": "Model B is the least capital-efficient, requiring an initial capital of $976,132,222.", "D": "Model A is the most capital-efficient, requiring the lowest initial capital ($112,903,478) to meet the solvency standard."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "This item assesses the ability to apply a novel constraint (a regulatory rule) to existing data to derive new insights. The depth strategy is 'Scenario Application' combined with 'Computational Judgment.' The user must first calculate the required capital for each model by taking the absolute value of the 5th percentile surplus. Then, they must evaluate four statements based on these calculations, including direct values and relative comparisons. All options are designed to be correct to test for comprehensive calculation and analysis, ensuring the user performs all required steps and comparisons accurately.", "qid": "76", "question": "### Background\n\nAn insurer is evaluating three alternative reinsurance strategies for its crop insurance portfolio. The performance of each strategy is assessed using an Asset-Liability Management (ALM) framework, simulated 5,000 times over a 30-year horizon.\n\n**Reinsurance Models:**\n- **Model A (Joint Self-Managed Pool):** Retains all risks internally.\n- **Model B (Group Private Reinsurance):** Cedes all risks to private reinsurers.\n- **Model C (Optimized Portfolio):** Strategically retains a lower-risk group and cedes a higher-risk group.\n\nA regulator imposes a new solvency requirement: the reinsurance pool must hold sufficient initial capital (U₀) to ensure that the 5th percentile of its surplus distribution at year 30 is non-negative (≥ $0).\n\n### Data / Model Specification\n\nThe performance of the three models is evaluated based on simulations assuming zero initial surplus (U₀ = $0). Table 1 presents the 5th percentile surplus from these simulations.\n\n**Table 1: 5th Percentile Surplus at End of Year 30 (U₀ = $0)**\n\n| Model | 5th Percentile Surplus |\n| :--- | :--- |\n| Model A | -$112,903,478 |\n| Model B | -$976,132,222 |\n| Model C | -$552,079,233 |\n\n### Question\n\nBased on the regulatory requirement and the data in Table 1, which of the following conclusions are valid? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 132, "Question": "### Background\n\nTo isolate firm-specific trading activity, the study first controls for market-wide influences on volume by estimating a 'turnover market model'. The residual from this model is defined as 'abnormal turnover' and is used as the dependent variable in all subsequent analyses.\n\n### Data / Model Specification\n\nThe turnover market model is estimated for each stock `i` using a 3-year rolling window:\n  \nVT_{it} = a_{i} + b_{i}VTM_{t} + e_{it} \\quad \\text{(Eq. 1)}\n \nwhere `VT_it` is the turnover of stock `i` in month `t`, and `VTM_t` is the average market turnover. Abnormal turnover (`AVT_it`) for a month in the 4th year is then calculated as the residual, using the coefficients estimated from the prior 3 years:\n  \nAVT_{it} = VT_{it} - (\\hat{a}_{i} + \\hat{b}_{i}VTM_{t}) \\quad \\text{(Eq. 2)}\n \n\n---\n\nWhich of the following statements accurately describe this methodology or its implications?\n\nSelect all that apply.", "Options": {"A": "If a firm undergoes a major structural change that permanently increases its trading volume (e.g., S&P 500 inclusion) at the start of the 4th year, the `AVT_it` calculated for that year will be systematically biased upwards.", "B": "This procedure is designed to remove the confounding effect of market-wide shifts in trading sentiment, allowing for a cleaner test of firm-specific drivers of volume.", "C": "A key assumption of this method is that the relationship between a stock's turnover and the market's turnover (i.e., `a_i` and `b_i`) is relatively stable over the 4-year period.", "D": "The coefficient `b_i` represents the sensitivity of stock `i`'s turnover to market-wide trading activity, analogous to beta in the capital asset pricing model."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the paper's core methodological step: the calculation of abnormal volume. It tests the interpretation of the model's parameters, its purpose, its underlying assumptions, and its limitations in the face of structural breaks. Depth Strategy: Reverse-Reasoning. Option C requires the candidate to reason backward from a given outcome (a structural break) to identify its effect on the calculated variable. Distractor Logic: All options are correct statements about the methodology. A tests the economic interpretation of the slope coefficient. B tests the causal inference goal of the procedure. C tests the candidate's ability to identify the consequences of violating the model's stationarity assumption. D explicitly states the stationarity assumption itself. This forces a comprehensive check of understanding rather than identifying a single correct fact.", "qid": "132", "question": "### Background\n\nTo isolate firm-specific trading activity, the study first controls for market-wide influences on volume by estimating a 'turnover market model'. The residual from this model is defined as 'abnormal turnover' and is used as the dependent variable in all subsequent analyses.\n\n### Data / Model Specification\n\nThe turnover market model is estimated for each stock `i` using a 3-year rolling window:\n  \nVT_{it} = a_{i} + b_{i}VTM_{t} + e_{it} \\quad \\text{(Eq. 1)}\n \nwhere `VT_it` is the turnover of stock `i` in month `t`, and `VTM_t` is the average market turnover. Abnormal turnover (`AVT_it`) for a month in the 4th year is then calculated as the residual, using the coefficients estimated from the prior 3 years:\n  \nAVT_{it} = VT_{it} - (\\hat{a}_{i} + \\hat{b}_{i}VTM_{t}) \\quad \\text{(Eq. 2)}\n \n\n---\n\nWhich of the following statements accurately describe this methodology or its implications?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 6, "Question": "### Background\n\n**Research Question.** A researcher wishes to test if a single, latent investor sentiment factor drives asset returns, and proposes using the presidential party dummy as an instrumental variable (IV) for this unobserved factor.\n\n**Setting / Data-Generating Environment.** In an IV framework, a valid instrument must satisfy two key conditions:\n1.  **Relevance:** The instrument must be correlated with the endogenous variable (here, latent sentiment).\n2.  **Exclusion Restriction:** The instrument can only affect the outcome variable (asset returns) *through* the endogenous variable. It must be uncorrelated with the regression's error term.\n\n---\n\n### Data / Model Specification\n\n- **Model:** `R_{i,t} = \\alpha_i + \\beta_i (Sentiment_t) + \\varepsilon_{i,t}`\n- **Instrument:** Presidential Party Dummy (`X_{t-1}`)\n\n---\n\n### Question\n\nRegarding the proposal to use the presidential party dummy as an instrument for latent investor sentiment, select all of the following statements that are correct.", "Options": {"A": "The exclusion restriction is likely violated because the president's party can influence asset returns through other channels, such as fiscal policy or regulation, that are not captured by a single sentiment factor.", "B": "If the presidential party dummy is a valid instrument, it guarantees that the estimated model will have a high R-squared.", "C": "The exclusion restriction requires that the presidential party dummy must be completely uncorrelated with asset returns.", "D": "The relevance condition requires that the political party of the president must be correlated with investor sentiment."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the high-level econometric critique from the source QA. It uses the Premise/Assumption Packaging strategy to test the core concepts of instrumental variables. Options A and B correctly state the relevance condition and the likely violation of the exclusion restriction in this specific context. Distractor C is a common Conceptual Error, misstating the exclusion restriction (the instrument must be uncorrelated with the error term, not the dependent variable itself). Distractor D is True but Irrelevant, confusing the purpose of IV (causal identification) with predictive power (R-squared).", "qid": "6", "question": "### Background\n\n**Research Question.** A researcher wishes to test if a single, latent investor sentiment factor drives asset returns, and proposes using the presidential party dummy as an instrumental variable (IV) for this unobserved factor.\n\n**Setting / Data-Generating Environment.** In an IV framework, a valid instrument must satisfy two key conditions:\n1.  **Relevance:** The instrument must be correlated with the endogenous variable (here, latent sentiment).\n2.  **Exclusion Restriction:** The instrument can only affect the outcome variable (asset returns) *through* the endogenous variable. It must be uncorrelated with the regression's error term.\n\n---\n\n### Data / Model Specification\n\n- **Model:** `R_{i,t} = \\alpha_i + \\beta_i (Sentiment_t) + \\varepsilon_{i,t}`\n- **Instrument:** Presidential Party Dummy (`X_{t-1}`)\n\n---\n\n### Question\n\nRegarding the proposal to use the presidential party dummy as an instrument for latent investor sentiment, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 73, "Question": "### Background\n\n**Research Question.** What are the behavioral and market-level consequences of a \"tough\" Bonus-Malus System (BMS) that imposes severe penalties for claims?\n\n**Setting / Data-Generating Environment.** The analysis focuses on the Belgian BMS, a system known for its tough penalties. The policyholder's decision of whether to report a claim is modeled as an infinite-horizon dynamic programming problem, where the goal is to minimize the present value of future costs. The solution to this problem yields an optimal retention level for each class in the system.\n\n**Variables & Parameters.**\n- `x_i^*`: The optimal retention level for a policyholder in class `i`. Claims below this value are paid out-of-pocket (currency units).\n- `C_{i.m}`: An augmented state, where `i` is the class and `m` is the number of consecutive claim-free years.\n- `λ_true`: The true underlying claim frequency of a driver (claims per year).\n- `β`: The policyholder's annual discount factor (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe calculated optimal retention levels for the Belgian BMS are presented in **Table 1**, expressed as a percentage of the average premium. A key feature is that for many classes, the retention level is extremely high. The phenomenon where policyholders are incentivized to pay for claims out-of-pocket to avoid future premium hikes is known as \"hunger for bonus.\" A special rule states that no policy can be in a class above 14 after four consecutive claim-free years.\n\n**Table 1: Optimal Retentions for Belgian BMS (Selected Classes)**\n| Class | Optimal Retention (% of avg. premium) |\n|:-----:|:-------------------------------------:|\n| 10    | 224.98%                               |\n| 11    | 239.38%                               |\n| 12    | 254.56%                               |\n| 18.0  | 288.16%                               |\n| 18.1  | 326.98%                               |\n| 18.2  | 382.01%                               |\n| 18.3  | 457.52%                               |\n\n---\n\n### Question\n\nBased on the provided information, select all statements that correctly describe the \"hunger for bonus\" phenomenon and its consequences.", "Options": {"A": "According to the model, impatient drivers (low discount factor `β`) are more likely to pay large claims out-of-pocket than patient drivers (high `β`).", "B": "A driver's optimal retention increases as they approach the 4-year claim-free threshold because the opportunity cost of reporting a claim (i.e., forfeiting an imminent large premium reduction) increases.", "C": "A rational policyholder would never pay a claim out-of-pocket if its cost exceeds their annual premium.", "D": "The \"hunger for bonus\" effect can lead an insurer to underestimate the true claim frequency (`λ_true`) because many claims go unreported."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the economic logic and second-order consequences of the 'hunger for bonus' phenomenon. It uses an Atomic Decomposition strategy to test understanding of the underlying NPV logic, the dynamics of incentives, and the resulting data censoring problem. Distractor A is a Conceptual Opposite to the core forward-looking logic of the model. Distractor C is a Reverse-Reasoning Error regarding the effect of the discount factor `β`.", "qid": "73", "question": "### Background\n\n**Research Question.** What are the behavioral and market-level consequences of a \"tough\" Bonus-Malus System (BMS) that imposes severe penalties for claims?\n\n**Setting / Data-Generating Environment.** The analysis focuses on the Belgian BMS, a system known for its tough penalties. The policyholder's decision of whether to report a claim is modeled as an infinite-horizon dynamic programming problem, where the goal is to minimize the present value of future costs. The solution to this problem yields an optimal retention level for each class in the system.\n\n**Variables & Parameters.**\n- `x_i^*`: The optimal retention level for a policyholder in class `i`. Claims below this value are paid out-of-pocket (currency units).\n- `C_{i.m}`: An augmented state, where `i` is the class and `m` is the number of consecutive claim-free years.\n- `λ_true`: The true underlying claim frequency of a driver (claims per year).\n- `β`: The policyholder's annual discount factor (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe calculated optimal retention levels for the Belgian BMS are presented in **Table 1**, expressed as a percentage of the average premium. A key feature is that for many classes, the retention level is extremely high. The phenomenon where policyholders are incentivized to pay for claims out-of-pocket to avoid future premium hikes is known as \"hunger for bonus.\" A special rule states that no policy can be in a class above 14 after four consecutive claim-free years.\n\n**Table 1: Optimal Retentions for Belgian BMS (Selected Classes)**\n| Class | Optimal Retention (% of avg. premium) |\n|:-----:|:-------------------------------------:|\n| 10    | 224.98%                               |\n| 11    | 239.38%                               |\n| 12    | 254.56%                               |\n| 18.0  | 288.16%                               |\n| 18.1  | 326.98%                               |\n| 18.2  | 382.01%                               |\n| 18.3  | 457.52%                               |\n\n---\n\n### Question\n\nBased on the provided information, select all statements that correctly describe the \"hunger for bonus\" phenomenon and its consequences.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 49, "Question": "### Background\n\nA study investigates how the effect of economic policy uncertainty (EPU) on board structure is moderated by firm characteristics. The analysis involves splitting the sample based on factors like CEO power and capital intensity and running separate regressions.\n\n### Data / Model Specification\n\nThe table below shows regression coefficients for `EPU` from models where board characteristics are the dependent variables, run on different firm subsamples.\n\n**Table 1. Board Change Moderators (Selected Panels)**\n\n**Panel C: CEO Power**\n\n| | CEO Power | Mean | EPU Coeff. | Std. Err. |\n| :--- | :--- | :--- | :--- | :--- |\n| Board size | High | 10.75 | -0.032 | (0.037) |\n| | Low | 10.36 | -0.093** | (0.009) |\n| Board independence | High | 78.04% | 0.009 | (0.028) |\n| | Low | 74.30% | 0.096*** | (0.006) |\n\n**Panel D: Capital Intensity**\n\n| | Capital Intensity | Mean | EPU Coeff. | Std. Err. |\n| :--- | :--- | :--- | :--- | :--- |\n| Board size | High | 10.44 | -0.106*** | (0.011) |\n| | Low | 10.32 | -0.080*** | (0.012) |\n| Board independence | High | 73.76% | 0.074*** | (0.008) |\n| | Low | 75.45% | 0.098*** | (0.008) |\n\n*Note: ***, ** denote significance at 1% and 5% levels.*\n\n### Question\n\nBased on a synthesis of the mean values and regression coefficients in the provided tables, which of the following conclusions about the moderating effects of firm characteristics are supported?", "Options": {"A": "Firms with high capital intensity exhibit a weaker response to EPU in terms of board size changes, as their boards are already optimized for their operating environment.", "B": "Firms with high capital intensity, which start with weaker monitoring (lower mean board independence), react more strongly to EPU by reducing board size compared to low capital intensity firms.", "C": "The significant response of board independence to EPU is concentrated in firms with powerful CEOs, suggesting these boards must work harder to enhance monitoring.", "D": "Firms with low CEO power, which have weaker initial monitoring structures (lower mean board independence), significantly increase their board independence in response to rising EPU."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the ability to synthesize information from a complex table, combining mean comparisons (static differences) with regression coefficients (dynamic responses) to evaluate the paper's hypotheses on heterogeneity. It uses a Reverse-Reasoning strategy. \n- **Correct A:** This option correctly links the lower mean independence of high-CI firms (73.76% vs 75.45%) with their stronger reaction in board size reduction (-0.106 vs -0.080), which aligns with the paper's argument.\n- **Correct C:** This option correctly notes that low-CEO-power firms have lower mean independence (74.30% vs 78.04%) and that their response to EPU is large and significant (coeff=0.096***), while the high-power group's response is insignificant.\n- **Distractor B:** This is a Conceptual Opposite. The table shows the significant response is in firms with *low* CEO power, not high.\n- **Distractor D:** This is a Conceptual Opposite. The table shows high-CI firms have a *stronger* response in board size reduction, not weaker.", "qid": "49", "question": "### Background\n\nA study investigates how the effect of economic policy uncertainty (EPU) on board structure is moderated by firm characteristics. The analysis involves splitting the sample based on factors like CEO power and capital intensity and running separate regressions.\n\n### Data / Model Specification\n\nThe table below shows regression coefficients for `EPU` from models where board characteristics are the dependent variables, run on different firm subsamples.\n\n**Table 1. Board Change Moderators (Selected Panels)**\n\n**Panel C: CEO Power**\n\n| | CEO Power | Mean | EPU Coeff. | Std. Err. |\n| :--- | :--- | :--- | :--- | :--- |\n| Board size | High | 10.75 | -0.032 | (0.037) |\n| | Low | 10.36 | -0.093** | (0.009) |\n| Board independence | High | 78.04% | 0.009 | (0.028) |\n| | Low | 74.30% | 0.096*** | (0.006) |\n\n**Panel D: Capital Intensity**\n\n| | Capital Intensity | Mean | EPU Coeff. | Std. Err. |\n| :--- | :--- | :--- | :--- | :--- |\n| Board size | High | 10.44 | -0.106*** | (0.011) |\n| | Low | 10.32 | -0.080*** | (0.012) |\n| Board independence | High | 73.76% | 0.074*** | (0.008) |\n| | Low | 75.45% | 0.098*** | (0.008) |\n\n*Note: ***, ** denote significance at 1% and 5% levels.*\n\n### Question\n\nBased on a synthesis of the mean values and regression coefficients in the provided tables, which of the following conclusions about the moderating effects of firm characteristics are supported?", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 91, "Question": "### Background\n\n**Research Question.** Do conflicting empirical findings on mutual fund 'tournament behavior'—where managers alter risk based on mid-year performance—stem from a fundamental flaw in the traditional testing methodology? This paper hypothesizes a 'sorting bias' as the explanation.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual funds from 1990-2006. The traditional methodology sorts funds into 'winners' and 'losers' based on first-half returns and then examines their change in risk in the second half.\n\n### Data / Model Specification\n\n**Traditional Methodology**\nFunds are placed in a 2x2 contingency table based on being above/below the median first-half return (`RTN`) and a Risk Adjustment Ratio (`RAR`).\n\n**Table 1. Traditional Contingency Table Results (Selected Years)**\n\n| Year | Low RTN (\"Losers\") \"Low\" RAR | Low RTN (\"Losers\") \"High\" RAR |\n|:----:|:---:|:---:|\n| 1991 | 16.49% | 33.58% |\n| 2001 | 38.51% | 11.49% |\n\n**Testing the Sorting Bias Hypothesis**\nThe paper proposes two metrics to test the sorting bias hypothesis:\n-   `Frequency Difference`: For the 'Low RTN' group, this is `% in High RAR cell - % in Low RAR cell`.\n-   `Before Ratio`: The ratio of the median first-half risk of 'High RTN' funds to 'Low RTN' funds.\n\n**Table 2. Empirical Test of the Sorting Bias**\n\n| Year | Frequency Difference | Before Ratio |\n|:----:|:---:|:---:|\n| 1991 | 17.09 | 1.356 |\n| 2001 | -27.01 | 0.609 |\n| **Corr.** | **0.81** | |\n\n*Note: The correlation of 0.81 is calculated over the full 1990-2006 sample.*\n\n### Question\n\nThe paper argues that traditional tests of tournament behavior are flawed due to a 'sorting bias'. Based on the data in Table 1 and Table 2, select all statements that correctly describe this bias and the empirical evidence supporting it.", "Options": {"A": "The correlation of 0.81 between the `Frequency Difference` and the `Before Ratio` indicates that managers of funds that were riskier in the first half (`Before Ratio`) are behaviorally more likely to increase risk in the second half (`Frequency Difference`).", "B": "The empirical data shows a strong link between the initial risk sort and the final test outcome: in 1991, a high `Before Ratio` (1.356) was followed by a large positive `Frequency Difference` (17.09), while in 2001, a low `Before Ratio` (0.609) was followed by a large negative `Frequency Difference` (-27.01).", "C": "The results in Table 1 for 1991 and 2001 show that manager behavior is inconsistent, with underperforming managers increasing risk in 1991 but also increasing risk in 2001.", "D": "The sorting bias hypothesis posits that in a strong market, sorting on high returns inadvertently selects for high-risk funds, whose risk then mechanically mean-reverts downwards, creating a spurious result."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the student's understanding of the 'sorting bias' mechanism and the empirical evidence supporting it. It uses an **Atomic Decomposition** strategy, combining a conceptual statement about the bias's logic with a data-driven statement from the tables. This tests both theoretical comprehension and data interpretation. Distractors include a **Conceptual Opposite** (misreading the results in Table 1) and a subtle **Conceptual Error** (proposing an incorrect causal interpretation of a correlation).", "qid": "91", "question": "### Background\n\n**Research Question.** Do conflicting empirical findings on mutual fund 'tournament behavior'—where managers alter risk based on mid-year performance—stem from a fundamental flaw in the traditional testing methodology? This paper hypothesizes a 'sorting bias' as the explanation.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual funds from 1990-2006. The traditional methodology sorts funds into 'winners' and 'losers' based on first-half returns and then examines their change in risk in the second half.\n\n### Data / Model Specification\n\n**Traditional Methodology**\nFunds are placed in a 2x2 contingency table based on being above/below the median first-half return (`RTN`) and a Risk Adjustment Ratio (`RAR`).\n\n**Table 1. Traditional Contingency Table Results (Selected Years)**\n\n| Year | Low RTN (\"Losers\") \"Low\" RAR | Low RTN (\"Losers\") \"High\" RAR |\n|:----:|:---:|:---:|\n| 1991 | 16.49% | 33.58% |\n| 2001 | 38.51% | 11.49% |\n\n**Testing the Sorting Bias Hypothesis**\nThe paper proposes two metrics to test the sorting bias hypothesis:\n-   `Frequency Difference`: For the 'Low RTN' group, this is `% in High RAR cell - % in Low RAR cell`.\n-   `Before Ratio`: The ratio of the median first-half risk of 'High RTN' funds to 'Low RTN' funds.\n\n**Table 2. Empirical Test of the Sorting Bias**\n\n| Year | Frequency Difference | Before Ratio |\n|:----:|:---:|:---:|\n| 1991 | 17.09 | 1.356 |\n| 2001 | -27.01 | 0.609 |\n| **Corr.** | **0.81** | |\n\n*Note: The correlation of 0.81 is calculated over the full 1990-2006 sample.*\n\n### Question\n\nThe paper argues that traditional tests of tournament behavior are flawed due to a 'sorting bias'. Based on the data in Table 1 and Table 2, select all statements that correctly describe this bias and the empirical evidence supporting it.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 68, "Question": "### Background\nThis problem analyzes the impact of moral hazard (unobservable effort) on an innovative sector. The model assumes conditions under which moral hazard is a binding constraint, specifically when the perceived probability of a negative shock, $\\theta_t$, is moderately high:\n\n$$ \\theta_{t} > \\frac{Y-(1+r)}{Y} \\quad \\text{(Eq. 1)} $$\n\nUnder these conditions, the paper proves that neither a pure high-effort equilibrium nor a separating equilibrium (with different contracts for high- and low-effort managers) can exist. The market settles on a pooling equilibrium where a single contract is offered to all managers.\n\n### Data / Model Specification\nIn the pooling equilibrium, some managers who accept the contract will exert high effort, while others exert low effort. This arrangement is sustained by a mechanism of cross-subsidization.\n\n- **High-effort managers** generate a large positive social surplus but receive an expected compensation that is less than this surplus.\n- **Low-effort managers** may generate a negative social surplus but receive a compensation that is greater than their contribution, effectively earning informational rents.\n\nThis dynamic leads to several key differences compared to the first-best (observable effort) scenario.\n\n### Question\nAccording to the paper's analysis of the moral hazard case, which of the following are valid consequences of the pooling equilibrium that emerges when moral hazard is a binding constraint? Select all that apply.", "Options": {"A": "Aggregate losses, in the event of a negative shock, are higher than they would be in the first-best case for the same level of confidence.", "B": "The expected compensation for managers exerting low effort is less than the social surplus they generate.", "C": "The inflated growth of the sector is funded by investors who accept lower-than-market returns on average.", "D": "For certain levels of confidence, the total size of the innovative sector can be larger than it would be in the first-best (observable effort) case."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item tests the candidate's understanding of the key implications of the pooling equilibrium under moral hazard, as contrasted with the first-best scenario.\nDepth Strategy: Reverse-Reasoning. The question presents several outcomes and asks the candidate to identify which ones are plausible consequences of the specified moral hazard equilibrium.\nDistractor Logic:\n- A (Correct): Implication 8 explicitly states that moral hazard spurs the entry of inefficient managers, which can inflate the size of the innovative sector beyond its first-best level for a range of $\\theta_t$.\n- B (Conceptual Opposite): This is the opposite of the cross-subsidization mechanism. Low-effort managers earn informational rents, meaning their expected compensation *exceeds* the (often negative) surplus they generate.\n- C (Formula Misuse/Conceptual Distractor): The model assumes competitive investors who must break even. The inflated growth is funded by the *high-effort managers* (who are paid less than their surplus) subsidizing the low-effort managers, not by investors taking losses.\n- D (Correct): Implication 10 states that the value-destroying entry of low-effort managers makes the sector more fragile, leading to higher aggregate losses in case of a shock compared to the first-best outcome.", "qid": "68", "question": "### Background\nThis problem analyzes the impact of moral hazard (unobservable effort) on an innovative sector. The model assumes conditions under which moral hazard is a binding constraint, specifically when the perceived probability of a negative shock, $\\theta_t$, is moderately high:\n\n$$ \\theta_{t} > \\frac{Y-(1+r)}{Y} \\quad \\text{(Eq. 1)} $$\n\nUnder these conditions, the paper proves that neither a pure high-effort equilibrium nor a separating equilibrium (with different contracts for high- and low-effort managers) can exist. The market settles on a pooling equilibrium where a single contract is offered to all managers.\n\n### Data / Model Specification\nIn the pooling equilibrium, some managers who accept the contract will exert high effort, while others exert low effort. This arrangement is sustained by a mechanism of cross-subsidization.\n\n- **High-effort managers** generate a large positive social surplus but receive an expected compensation that is less than this surplus.\n- **Low-effort managers** may generate a negative social surplus but receive a compensation that is greater than their contribution, effectively earning informational rents.\n\nThis dynamic leads to several key differences compared to the first-best (observable effort) scenario.\n\n### Question\nAccording to the paper's analysis of the moral hazard case, which of the following are valid consequences of the pooling equilibrium that emerges when moral hazard is a binding constraint? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 62, "Question": "### Background\n\n**Research Question.** What are the relative economic magnitudes of default risk, the risk-mitigating effect of catastrophe (CAT) bonds, and the countervailing effect of basis risk in the valuation of a reinsurance contract?\n\n**Setting.** This analysis uses numerical simulations to quantify the fair premium, or Rate on Line (ROL), for a reinsurance contract under several scenarios. The goal is to isolate and compare the impact of the reinsurer's credit risk, its use of CAT bonds as a hedge, and the imperfection of that hedge due to basis risk.\n\n**Variables and Parameters.**\n- `ROL`: Rate on Line, the fair premium for the reinsurance contract.\n- `Default Risk Premium`: The reduction in ROL due to the reinsurer's credit risk, calculated as `ROL_DefaultFree - ROL_DefaultRisky`.\n- `Basis Risk Premium`: The reduction in ROL due to an imperfect hedge, calculated as `ROL_NoBasisRisk - ROL_WithBasisRisk`.\n- `V/L`: The reinsurer's initial asset-to-liability ratio, a measure of its capital position.\n- `(λ, σ_c)`: The frequency and severity parameters of catastrophe risk.\n- `K`: The trigger level of the CAT bond.\n- `ρ_c`: The correlation between the reinsurer's losses and the CAT bond's trigger index. `ρ_c=1` implies no basis risk.\n\n---\n\n### Data / Model Specification\n\nThe following tables, derived from the paper's numerical analysis, provide the ROL and default risk premiums for a reinsurance contract under different conditions. All values are for a one-year contract with an attachment point `A=20` and cap `M=100`.\n\n**Table 1: Reinsurance Contract Values (ROL) without CAT Bonds**\n| (`λ`, `σ_c`) | Default-Free ROL | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- |\n| (2, 2) | 0.30082 | 0.21425 |\n\n**Table 2: Reinsurance Contract Values (ROL) with CAT Bonds (No Basis Risk, `ρ_c=1`)**\n| (`λ`, `σ_c`) | Trigger `K` | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- |\n| (2, 2) | 80 | 0.22703 |\n\n**Table 3: Reinsurance Contract Values (ROL) with CAT Bonds and Basis Risk**\n| (`λ`, `σ_c`) | Trigger `K` | Correlation `ρ_c` | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- | :--- |\n| (2, 2) | 80 | 0.3 | 0.21774 |\n\n---\n\n### Question\n\nBased on the provided data for a reinsurer with V/L=1.1 and facing high catastrophe risk `(λ, σ_c) = (2, 2)`, which of the following statements are correct?", "Options": {"A": "The default risk premium for the contract *with* a CAT bond (K=80, ρ_c=1.0) is 865.7 basis points.", "B": "The basis risk premium associated with the imperfect hedge (ρ_c = 0.3) is larger than the value increase from issuing a perfect-hedge CAT bond.", "C": "The default risk premium for the reinsurance contract, when no CAT bond is issued, is 865.7 basis points.", "D": "Issuing a CAT bond with a trigger of K=80 (assuming no basis risk) increases the contract's value (ROL) by 127.8 basis points."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the user's ability to perform the core calculations from the paper's numerical analysis and make quantitative comparisons. It uses an Atomic Decomposition strategy, breaking the original multi-part question into discrete, verifiable statements. The strategy is Computational Judgment. Distractor C is a Conceptual Opposite, testing the user's ability to compare the magnitudes of two key effects. Distractor D is a Step-Omission Error, using a value from a previous step (default risk premium without a bond) incorrectly in a new context.", "qid": "62", "question": "### Background\n\n**Research Question.** What are the relative economic magnitudes of default risk, the risk-mitigating effect of catastrophe (CAT) bonds, and the countervailing effect of basis risk in the valuation of a reinsurance contract?\n\n**Setting.** This analysis uses numerical simulations to quantify the fair premium, or Rate on Line (ROL), for a reinsurance contract under several scenarios. The goal is to isolate and compare the impact of the reinsurer's credit risk, its use of CAT bonds as a hedge, and the imperfection of that hedge due to basis risk.\n\n**Variables and Parameters.**\n- `ROL`: Rate on Line, the fair premium for the reinsurance contract.\n- `Default Risk Premium`: The reduction in ROL due to the reinsurer's credit risk, calculated as `ROL_DefaultFree - ROL_DefaultRisky`.\n- `Basis Risk Premium`: The reduction in ROL due to an imperfect hedge, calculated as `ROL_NoBasisRisk - ROL_WithBasisRisk`.\n- `V/L`: The reinsurer's initial asset-to-liability ratio, a measure of its capital position.\n- `(λ, σ_c)`: The frequency and severity parameters of catastrophe risk.\n- `K`: The trigger level of the CAT bond.\n- `ρ_c`: The correlation between the reinsurer's losses and the CAT bond's trigger index. `ρ_c=1` implies no basis risk.\n\n---\n\n### Data / Model Specification\n\nThe following tables, derived from the paper's numerical analysis, provide the ROL and default risk premiums for a reinsurance contract under different conditions. All values are for a one-year contract with an attachment point `A=20` and cap `M=100`.\n\n**Table 1: Reinsurance Contract Values (ROL) without CAT Bonds**\n| (`λ`, `σ_c`) | Default-Free ROL | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- |\n| (2, 2) | 0.30082 | 0.21425 |\n\n**Table 2: Reinsurance Contract Values (ROL) with CAT Bonds (No Basis Risk, `ρ_c=1`)**\n| (`λ`, `σ_c`) | Trigger `K` | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- |\n| (2, 2) | 80 | 0.22703 |\n\n**Table 3: Reinsurance Contract Values (ROL) with CAT Bonds and Basis Risk**\n| (`λ`, `σ_c`) | Trigger `K` | Correlation `ρ_c` | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- | :--- |\n| (2, 2) | 80 | 0.3 | 0.21774 |\n\n---\n\n### Question\n\nBased on the provided data for a reinsurer with V/L=1.1 and facing high catastrophe risk `(λ, σ_c) = (2, 2)`, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 32, "Question": "### Background\n\nThe study examines whether the 2017 Tax Cuts and Jobs Act (TCJA) stimulated investment by alleviating firms' pre-existing financial constraints. The theory is that the tax savings provided a cash windfall that allowed constrained firms to fund positive-NPV projects they previously could not.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated on sub-samples of firms classified as 'constrained' or 'unconstrained' using four different standard measures. The key coefficient on `USFirm*Yr2019` captures the treatment effect on capital expenditures (`CapEx`).\n\n**Table 1: DiD Results by Financial Constraint Status**\n\n| Constraint Measure | Constrained Firms (Coefficient) | Unconstrained Firms (Coefficient) |\n| :----------------- | :------------------------------ | :-------------------------------- |\n| KZ-Index           | 0.0038***                       | -0.0002                           |\n| Free Cash Flow     | 0.0040***                       | 0.0007                            |\n| Modified Z-score   | 0.0030***                       | 0.0003                            |\n| Whited-Wu Index    | 0.0045***                       | 0.0012                            |\n\n*Note: All coefficients for constrained firms are statistically significant; none for unconstrained firms are.*\n\n---\n\nAccording to the results in **Table 1**, which of the following statements are **INCORRECT** interpretations of the findings?\n", "Options": {"A": "The investment-stimulating effect of the TCJA was concentrated in financially constrained firms.", "B": "The results suggest that the TCJA's impact on investment was independent of firms' pre-existing financial health.", "C": "The findings are inconsistent with a perfect Modigliani-Miller world where financing is irrelevant to investment.", "D": "Unconstrained firms showed a statistically significant increase in investment across all four measures."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret a consistent pattern of heterogeneous treatment effects and understand its theoretical implications for corporate finance (i.e., the violation of Modigliani-Miller assumptions).\nDepth Strategy: Reverse-Reasoning and Theoretical Application.\nDistractor Logic: The Single-Correct-Answer Inversion Protocol was applied here. The core finding is that the effect is *only* in constrained firms. This makes statements A and C correct interpretations. Statements B and D are therefore incorrect interpretations. By inverting the stem to ask for INCORRECT statements, we can create a compliant multiple-response item.\n- A (Correct Interpretation, now a distractor): This is the main takeaway from the table.\n- B (Incorrect Interpretation, now a correct answer): The table note explicitly states the coefficients for unconstrained firms are not significant.\n- C (Correct Interpretation, now a distractor): The results show financing matters, which violates the MM theorem, making this a correct interpretation.\n- D (Incorrect Interpretation, now a correct answer): The results show the impact is highly dependent on financial health, making this statement false.", "qid": "32", "question": "### Background\n\nThe study examines whether the 2017 Tax Cuts and Jobs Act (TCJA) stimulated investment by alleviating firms' pre-existing financial constraints. The theory is that the tax savings provided a cash windfall that allowed constrained firms to fund positive-NPV projects they previously could not.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated on sub-samples of firms classified as 'constrained' or 'unconstrained' using four different standard measures. The key coefficient on `USFirm*Yr2019` captures the treatment effect on capital expenditures (`CapEx`).\n\n**Table 1: DiD Results by Financial Constraint Status**\n\n| Constraint Measure | Constrained Firms (Coefficient) | Unconstrained Firms (Coefficient) |\n| :----------------- | :------------------------------ | :-------------------------------- |\n| KZ-Index           | 0.0038***                       | -0.0002                           |\n| Free Cash Flow     | 0.0040***                       | 0.0007                            |\n| Modified Z-score   | 0.0030***                       | 0.0003                            |\n| Whited-Wu Index    | 0.0045***                       | 0.0012                            |\n\n*Note: All coefficients for constrained firms are statistically significant; none for unconstrained firms are.*\n\n---\n\nAccording to the results in **Table 1**, which of the following statements are **INCORRECT** interpretations of the findings?\n", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 100, "Question": "### Background\n\nTwo competing theories attempt to explain corporate financing decisions. The Pecking Order (PO) theory posits that firms follow a strict hierarchy (Internal Funds -> Debt -> Equity) to minimize information costs, without a specific leverage target. The Target Adjustment (TAR) theory posits that firms have an optimal leverage ratio and adjust towards it over time, balancing costs and benefits like tax shields and financial distress costs.\n\n### Data / Model Specification\n\nTo distinguish between these theories, a \"horse race\" regression is often estimated:\n\n  \n\\Delta D_{i,t} = \\alpha + \\beta_1 DEF_{i,t} + \\beta_2 (Lev^*_{i} - Lev_{i,t-1}) + \\epsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `\\Delta D_{i,t}` is the change in firm `i`'s debt in year `t`.\n- `DEF_{i,t}` is the financing deficit (investment needs minus internal cash flow).\n- `(Lev^*_{i} - Lev_{i,t-1})` is the leverage deficit (the gap between the target leverage and last period's actual leverage).\n\n---\n\nWhich of the following statements accurately describe the predictions, interpretations, or empirical challenges related to these models? Select all that apply.", "Options": {"A": "A key reason the Pecking Order theory prefers debt over equity for external financing is that the value of debt is less sensitive to managers' private information about firm prospects, thus reducing the 'lemons' problem associated with security issuance.", "B": "The Target Adjustment theory predicts that the coefficient `β₂` should be positive and statistically significant, reflecting that firms issue debt to close a leverage deficit (i.e., when actual leverage is below the target).", "C": "A finding that both `β₁` and `β₂` are statistically significant would invalidate both theories, as they are mutually exclusive.", "D": "The Pecking Order theory predicts that the coefficient `β₁` should be positive and close to 1, while `β₂` should be statistically insignificant."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses a comprehensive understanding of the two main competing theories of capital structure: Pecking Order and Target Adjustment. It uses a Reverse-Reasoning strategy by asking the user to identify valid statements about the theories and their empirical tests. Option A correctly states the empirical prediction of the PO theory in the horse race regression. Option B correctly states the prediction of the TAR theory. Option C correctly identifies the core information-asymmetry logic behind the PO theory's preference for debt. Distractor D is a Conceptual Opposite; finding both coefficients significant would suggest that both financing flows and stock adjustments matter, leading to a more integrated theory. Distractor E is also a Conceptual Opposite; the PO theory predicts these firms will have *low* leverage because their high cash flows mean they rarely need external financing.", "qid": "100", "question": "### Background\n\nTwo competing theories attempt to explain corporate financing decisions. The Pecking Order (PO) theory posits that firms follow a strict hierarchy (Internal Funds -> Debt -> Equity) to minimize information costs, without a specific leverage target. The Target Adjustment (TAR) theory posits that firms have an optimal leverage ratio and adjust towards it over time, balancing costs and benefits like tax shields and financial distress costs.\n\n### Data / Model Specification\n\nTo distinguish between these theories, a \"horse race\" regression is often estimated:\n\n  \n\\Delta D_{i,t} = \\alpha + \\beta_1 DEF_{i,t} + \\beta_2 (Lev^*_{i} - Lev_{i,t-1}) + \\epsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `\\Delta D_{i,t}` is the change in firm `i`'s debt in year `t`.\n- `DEF_{i,t}` is the financing deficit (investment needs minus internal cash flow).\n- `(Lev^*_{i} - Lev_{i,t-1})` is the leverage deficit (the gap between the target leverage and last period's actual leverage).\n\n---\n\nWhich of the following statements accurately describe the predictions, interpretations, or empirical challenges related to these models? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 93, "Question": "### Background\n\n**Research Question.** What are the long-run macroeconomic determinants of venture capital (VC) prices?\n\n**Setting.** A Johansen cointegration analysis is used to estimate the long-run equilibrium relationship between the US VC price index and a set of key macroeconomic and financial variables.\n\n---\n\n### Data / Model Specification\n\nThe estimated long-run equilibrium equation is:\n\n  \nP_{VC,t} = -29.67 + 0.70 NASDAQ_t - 2.62 TBILL_t + 3.68 GBY_t + 0.29 CPI_t + 7.22 IP_t \\quad \\text{(Eq. (1))}\n \n\nWhere `P_VC` is the VC price index, `NASDAQ` is the Nasdaq index, `TBILL` is the short-term interest rate, `GBY` is the long-term interest rate, `CPI` is the consumer price index, and `IP` is industrial production.\n\n---\n\nAccording to the estimated long-run relationship in Eq. (1) and the economic arguments presented in the paper, which of the following statements are accurate interpretations of the model's findings? Select all that apply.", "Options": {"A": "The model suggests that the Nasdaq has a smaller positive long-run impact on VC prices than Industrial Production.", "B": "An increase in the short-term T-Bill rate is associated with higher long-run VC prices, as entrepreneurs switch from bank financing to VC.", "C": "A sustained increase in the Nasdaq is associated with higher long-run VC prices, consistent with the hypothesis that a strong exit channel boosts VC valuations.", "D": "An increase in the long-term Government Bond Yield is associated with higher long-run VC prices, suggesting the demand-side effect (entrepreneurs seeking VC over debt) outweighs the supply-side effect (investors switching to bonds)."}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: This question assesses the ability to correctly interpret the signs and economic meaning of coefficients in a multivariate regression model, linking them back to the paper's theoretical framework. Strategy: Scenario Application (applying the model to interpret relationships). Distractor Logic: (A) is a correct interpretation of the positive NASDAQ coefficient. (B) is incorrect as it contradicts the negative sign on TBILL. (C) is a correct interpretation of the positive GBY coefficient, reflecting the paper's reasoning. (D) is a classic 'True but Irrelevant/Misleading' distractor; it correctly compares the numerical values of the unstandardized coefficients (0.70 < 7.22) but represents a flawed interpretation, as one cannot compare the magnitudes of coefficients for variables with different scales and units.", "qid": "93", "question": "### Background\n\n**Research Question.** What are the long-run macroeconomic determinants of venture capital (VC) prices?\n\n**Setting.** A Johansen cointegration analysis is used to estimate the long-run equilibrium relationship between the US VC price index and a set of key macroeconomic and financial variables.\n\n---\n\n### Data / Model Specification\n\nThe estimated long-run equilibrium equation is:\n\n  \nP_{VC,t} = -29.67 + 0.70 NASDAQ_t - 2.62 TBILL_t + 3.68 GBY_t + 0.29 CPI_t + 7.22 IP_t \\quad \\text{(Eq. (1))}\n \n\nWhere `P_VC` is the VC price index, `NASDAQ` is the Nasdaq index, `TBILL` is the short-term interest rate, `GBY` is the long-term interest rate, `CPI` is the consumer price index, and `IP` is industrial production.\n\n---\n\nAccording to the estimated long-run relationship in Eq. (1) and the economic arguments presented in the paper, which of the following statements are accurate interpretations of the model's findings? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 44, "Question": "### Background\n\n**Research Question.** The paper's central finding is that significant contagion effects on rivals only occur when a firm with low-rated (non-investment grade) debt is downgraded. The proposed explanation is the \"market visibility\" hypothesis: low-rated firms are monitored less intensely, so their downgrades are more surprising and informationally potent.\n\n**Setting / Data-Generating Environment.** To test this mechanism, the study compares the mean number of analysts following downgraded firms across three credit quality groups.\n\n**Variables & Parameters.**\n- **Analyst Coverage:** The number of earnings analysts per downgraded firm as reported by IBES.\n- **Group 1:** Downgrade within investment grade.\n- **Group 2:** Downgrade from investment to non-investment grade.\n- **Group 3:** Downgrade within non-investment grade.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents summary statistics for analyst coverage across the three groups of downgraded firms.\n\n**Table 1. Distribution of Earnings Analysts per Downgraded Firm**\n\n| Group | Mean | Standard Deviation | Median |\n| :--- | :--- | :--- | :--- |\n| Group 1 | 18.83 | 8.71 | 19 |\n| Group 2 | 14.73 | 6.85 | 14.5 |\n| Group 3 | 9.74 | 6.56 | 9 |\n\n*T-statistics for the difference in means are all highly significant (p < 0.01) for all pairwise comparisons (1 vs 2, 1 vs 3, 2 vs 3).*\n\n---\n\n### Question\n\nThe paper's 'market visibility' hypothesis posits that `Low Credit Rating` → `Low Analyst Coverage` → `High Surprise` → `Strong Contagion`. Based on the data in Table 1, select all statements that are consistent with this hypothesis.", "Options": {"A": "The data imply that high analyst coverage for Group 1 firms is the primary cause of their investment-grade credit rating.", "B": "The data show a monotonic relationship where firms with lower credit quality (from Group 1 to Group 3) are followed by progressively fewer analysts.", "C": "The median number of analysts for Group 1 firms (19) is not statistically different from the median for Group 3 firms (9).", "D": "The significant difference in analyst coverage between groups provides a plausible, empirically-grounded mechanism for why contagion effects are concentrated in Group 3."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the understanding of the paper's central causal mechanism ('market visibility'). It uses a 'premise packaging' strategy, asking the user to select the empirical premises that support the main hypothesis. Option A tests direct data interpretation, while Option B tests the ability to link that data to the broader argument. Distractors target factual misreading (Option C) and a sophisticated logical error of reverse causality (Option D).", "qid": "44", "question": "### Background\n\n**Research Question.** The paper's central finding is that significant contagion effects on rivals only occur when a firm with low-rated (non-investment grade) debt is downgraded. The proposed explanation is the \"market visibility\" hypothesis: low-rated firms are monitored less intensely, so their downgrades are more surprising and informationally potent.\n\n**Setting / Data-Generating Environment.** To test this mechanism, the study compares the mean number of analysts following downgraded firms across three credit quality groups.\n\n**Variables & Parameters.**\n- **Analyst Coverage:** The number of earnings analysts per downgraded firm as reported by IBES.\n- **Group 1:** Downgrade within investment grade.\n- **Group 2:** Downgrade from investment to non-investment grade.\n- **Group 3:** Downgrade within non-investment grade.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents summary statistics for analyst coverage across the three groups of downgraded firms.\n\n**Table 1. Distribution of Earnings Analysts per Downgraded Firm**\n\n| Group | Mean | Standard Deviation | Median |\n| :--- | :--- | :--- | :--- |\n| Group 1 | 18.83 | 8.71 | 19 |\n| Group 2 | 14.73 | 6.85 | 14.5 |\n| Group 3 | 9.74 | 6.56 | 9 |\n\n*T-statistics for the difference in means are all highly significant (p < 0.01) for all pairwise comparisons (1 vs 2, 1 vs 3, 2 vs 3).*\n\n---\n\n### Question\n\nThe paper's 'market visibility' hypothesis posits that `Low Credit Rating` → `Low Analyst Coverage` → `High Surprise` → `Strong Contagion`. Based on the data in Table 1, select all statements that are consistent with this hypothesis.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 65, "Question": "### Background\n\n**Research Question.** How are an insurer's optimal dynamic reinsurance and dividend policies jointly determined as the solution to a stochastic optimal control problem?\n\n**Setting.** The insurer's problem is formulated as a free-boundary problem. Below a surplus barrier `x_b`, the value function `v(x)` satisfies a Hamilton-Jacobi-Bellman (HJB) equation. The insurer chooses the retention proportion `q` to maximize the value function.\n\n**Variables and Parameters.**\n- `v(x)`: The value function at surplus `x`.\n- `q`: The retention proportion, the control variable for reinsurance.\n- `v'(x), v''(x)`: First and second derivatives of the value function.\n- `θ, η, a, b, β`: Standard model parameters.\n\n---\n\n### Data / Model Specification\n\nIn the continuation region `0 < x < x_b`, the value function `v(x)` solves the HJB equation:\n\n  \n\\beta v(x) = \\operatorname*{max}_{0\\leq q\\leq1}\\left[(\\theta-(1-q)\\eta)a v^{\\prime}(x)+\\frac{1}{2}q^{2}b^{2}v^{\\prime\\prime}(x)\\right] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the HJB equation in **Eq. (1)**, which of the following statements about the optimal reinsurance strategy `q*(x)` and its economic interpretation are correct? Assume an interior solution for `q*` and that the value function `v(x)` is concave. Select all that apply.", "Options": {"A": "The term `v'(x)` in the formula represents the total value of the firm, while `v''(x)` represents its growth rate.", "B": "The optimal retention `q*(x)` is inversely proportional to the insurer's absolute risk aversion, `ARA(x) = -v''(x)/v'(x)`.", "C": "An increase in the cost of reinsurance, `η`, leads the insurer to retain a smaller proportion of its risk (a lower `q*`).", "D": "The formula for the unconstrained optimal retention is `q*(x) = - (ηa/b²) * (v'(x)/v''(x))`."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item assesses the derivation and economic interpretation of the optimal control. It uses the 'Atomic Decomposition' strategy. Option B is the correct mathematical derivation from the HJB equation. Option A is the correct economic interpretation of that formula in terms of risk aversion. Distractor C (Conceptual Opposite) states the incorrect relationship between reinsurance cost and retention. Distractor D (Formula Misuse) provides incorrect definitions for the derivatives of the value function.", "qid": "65", "question": "### Background\n\n**Research Question.** How are an insurer's optimal dynamic reinsurance and dividend policies jointly determined as the solution to a stochastic optimal control problem?\n\n**Setting.** The insurer's problem is formulated as a free-boundary problem. Below a surplus barrier `x_b`, the value function `v(x)` satisfies a Hamilton-Jacobi-Bellman (HJB) equation. The insurer chooses the retention proportion `q` to maximize the value function.\n\n**Variables and Parameters.**\n- `v(x)`: The value function at surplus `x`.\n- `q`: The retention proportion, the control variable for reinsurance.\n- `v'(x), v''(x)`: First and second derivatives of the value function.\n- `θ, η, a, b, β`: Standard model parameters.\n\n---\n\n### Data / Model Specification\n\nIn the continuation region `0 < x < x_b`, the value function `v(x)` solves the HJB equation:\n\n  \n\\beta v(x) = \\operatorname*{max}_{0\\leq q\\leq1}\\left[(\\theta-(1-q)\\eta)a v^{\\prime}(x)+\\frac{1}{2}q^{2}b^{2}v^{\\prime\\prime}(x)\\right] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the HJB equation in **Eq. (1)**, which of the following statements about the optimal reinsurance strategy `q*(x)` and its economic interpretation are correct? Assume an interior solution for `q*` and that the value function `v(x)` is concave. Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 58, "Question": "### Background\n\n**Research Question.** What are the true determinants of equilibrium interest rates on auto loans, and how does failing to account for truncation from legally-imposed rate ceilings bias standard econometric estimates?\n\n**Setting / Data-Generating Environment.** The analysis uses a sample of 1,039 auto loans. The key challenge is that observed interest rates are truncated from above by state-level ceilings. The paper contrasts estimates from a standard Ordinary Least Squares (OLS) regression with those from a consistent maximum likelihood procedure (Hausman-Wise, or HW) that explicitly models this truncation.\n\n---\n\n### Data / Model Specification\n\nThe underlying hedonic model for the unobserved equilibrium rate `r_i` is:\n  \n\\ln r_{i} = X_{i}^{\\prime}\\beta + \\epsilon_{i} \\quad \\text{(Eq. (1))}\n \nwhere `ε_i` is an unobserved error term, assumed `N(0, σ^2)`. We only observe `ln(r_i)` if `r_i ≤ c_i`, where `c_i` is the applicable ceiling. This truncation violates a key OLS assumption.\n\n**Table 1: Selected OLS and HW Regression Estimates**\n(Dependent Variable: `ln(r)`)\n\n| Variable | OLS Estimate | HW Estimate |\n|:---|---:|---:|\n| HDCGGD (College Grad) | -0.0493 | -0.1606 |\n| | (0.0293) | (0.0905) |\n\n**Table 2: OLS and HW Estimates for a Low-Ceiling State Indicator**\n(Relative to no-ceiling states)\n\n| Variable | OLS Estimate | HW Estimate |\n|:---|---:|---:|\n| LOCEIL (Low-Ceiling State) | -0.3432 | -0.0302 |\n| | (0.0404) | (0.1338) |\n\n*Asymptotic standard errors in parentheses.*\n\n---\n\n### Question\n\nGiven the model and the regression results comparing OLS and the consistent Hausman-Wise (HW) estimator, which of the following statements correctly describe the econometric problem of truncation bias and its consequences?", "Options": {"A": "The large, negative OLS coefficient for the `LOCEIL` indicator is spurious; it misinterprets a mechanical truncation effect (the removal of high-rate loans from the sample) as a behavioral shift in lender pricing.", "B": "The OLS estimate for the effect of a college degree (`HDCGGD`) is biased toward zero (attenuation bias) compared to the consistent HW estimate, understating the true impact of this characteristic.", "C": "The HW model is preferred because it confirms the OLS finding that equilibrium interest rates are structurally lower in states with low ceilings.", "D": "In the sample of observed loans (where the equilibrium rate is at or below the ceiling), the conditional expectation of the error term `ε_i` is negative, violating a key OLS assumption."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses a deep understanding of the paper's central econometric argument regarding truncation bias. It uses an Atomic Decomposition strategy to test three distinct facets of the issue: the theoretical origin of the bias (A), its consequence for policy variables (B), and its consequence for borrower characteristic variables (C). This requires the user to synthesize theory and empirical results from multiple tables. Distractor D is a Conceptual Opposite that presents the incorrect OLS conclusion as the correct HW conclusion, effectively testing whether the student understands the core methodological critique of the paper.", "qid": "58", "question": "### Background\n\n**Research Question.** What are the true determinants of equilibrium interest rates on auto loans, and how does failing to account for truncation from legally-imposed rate ceilings bias standard econometric estimates?\n\n**Setting / Data-Generating Environment.** The analysis uses a sample of 1,039 auto loans. The key challenge is that observed interest rates are truncated from above by state-level ceilings. The paper contrasts estimates from a standard Ordinary Least Squares (OLS) regression with those from a consistent maximum likelihood procedure (Hausman-Wise, or HW) that explicitly models this truncation.\n\n---\n\n### Data / Model Specification\n\nThe underlying hedonic model for the unobserved equilibrium rate `r_i` is:\n  \n\\ln r_{i} = X_{i}^{\\prime}\\beta + \\epsilon_{i} \\quad \\text{(Eq. (1))}\n \nwhere `ε_i` is an unobserved error term, assumed `N(0, σ^2)`. We only observe `ln(r_i)` if `r_i ≤ c_i`, where `c_i` is the applicable ceiling. This truncation violates a key OLS assumption.\n\n**Table 1: Selected OLS and HW Regression Estimates**\n(Dependent Variable: `ln(r)`)\n\n| Variable | OLS Estimate | HW Estimate |\n|:---|---:|---:|\n| HDCGGD (College Grad) | -0.0493 | -0.1606 |\n| | (0.0293) | (0.0905) |\n\n**Table 2: OLS and HW Estimates for a Low-Ceiling State Indicator**\n(Relative to no-ceiling states)\n\n| Variable | OLS Estimate | HW Estimate |\n|:---|---:|---:|\n| LOCEIL (Low-Ceiling State) | -0.3432 | -0.0302 |\n| | (0.0404) | (0.1338) |\n\n*Asymptotic standard errors in parentheses.*\n\n---\n\n### Question\n\nGiven the model and the regression results comparing OLS and the consistent Hausman-Wise (HW) estimator, which of the following statements correctly describe the econometric problem of truncation bias and its consequences?", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 41, "Question": "### Background\n\nThe evolution of the optimal early exercise boundary `c(τ)` for an American put option in the Black-Scholes framework is governed by a specific differential equation. This equation links the speed of the boundary's movement to the option's sensitivities and the economic trade-offs of early exercise.\n\n### Data / Model Specification\n\nThe differential equation governing the evolution of the early exercise boundary `c(τ)` is:\n  \n\\frac{\\partial c(\\tau)}{\\partial\\tau} = -\\frac{\\partial^{2}p(c(\\tau),\\tau)}{\\partial x\\partial\\tau} \\frac{\\sigma^{2}c^{2}(\\tau)}{2q r - 2(r-b)c(\\tau)} \\quad \\text{(Eq. 1)}\n \nwhere `p` is the option price, `x` is asset price, `τ` is time-to-expiry, `q` is strike, `σ` is volatility, `r` is the risk-free rate, and `b` is the cost of carry.\n\n---\n\nBased on the structure of this boundary evolution equation, which of the following statements are valid interpretations or implications? Select all that apply.", "Options": {"A": "For a stock paying a high dividend such that `b` approaches `r` (cost of carry approaches zero), the denominator approaches zero, causing the boundary speed to become infinite.", "B": "For an American put on a futures contract (where `b=0`), the denominator simplifies to `2r(q-c(τ))`, implying that early exercise can still be optimal and the boundary evolves at a finite speed.", "C": "The speed of the boundary, `∂c/∂τ`, is inversely proportional to the net benefit of waiting to exercise, which is captured by the term `rq - (r-b)c(τ)`.", "D": "If the option's Delta were insensitive to the passage of time at the boundary (i.e., `∂²p/∂x∂τ = 0`), the boundary would not move (`∂c/∂τ = 0`)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: Test the user's ability to interpret the financial meaning of the terms in the boundary evolution equation and analyze its behavior under different parameterizations (e.g., for different underlying assets like futures).\nDepth Strategy: Scenario Application. The user must apply the general formula to specific cases (futures contract, high dividend stock) and interpret the components.\nDistractor Logic:\n- D (Formula Misuse / Conceptual Error): This distractor confuses the case of `b=r` (non-dividend paying stock) with `b=0` (futures). When `b=r`, the denominator becomes `2(rq - (r-r)c) = 2rq`, which is positive and finite. The user might incorrectly think `r-b` approaching zero is the key, but the full term is `rq - (r-b)c`. This tests careful application of the formula.", "qid": "41", "question": "### Background\n\nThe evolution of the optimal early exercise boundary `c(τ)` for an American put option in the Black-Scholes framework is governed by a specific differential equation. This equation links the speed of the boundary's movement to the option's sensitivities and the economic trade-offs of early exercise.\n\n### Data / Model Specification\n\nThe differential equation governing the evolution of the early exercise boundary `c(τ)` is:\n  \n\\frac{\\partial c(\\tau)}{\\partial\\tau} = -\\frac{\\partial^{2}p(c(\\tau),\\tau)}{\\partial x\\partial\\tau} \\frac{\\sigma^{2}c^{2}(\\tau)}{2q r - 2(r-b)c(\\tau)} \\quad \\text{(Eq. 1)}\n \nwhere `p` is the option price, `x` is asset price, `τ` is time-to-expiry, `q` is strike, `σ` is volatility, `r` is the risk-free rate, and `b` is the cost of carry.\n\n---\n\nBased on the structure of this boundary evolution equation, which of the following statements are valid interpretations or implications? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\"B,C,D\"", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 12, "Question": "### Background\n\nAn insurer implements a dynamic hedging program using derivatives to mitigate the risk of its segregated fund guarantees. This program is incorporated into its internal model, which generates a probability distribution for the future cost of the guarantees, `C`. The capital requirement, `K`, is calculated as `K = CTE(95) - L`, where `L` is the policy liability (set to the mean of the cost distribution) and `CTE(95)` is the Conditional Tail Expectation at the 95th percentile.\n\n### Question\n\nAn effective hedging program is designed to pay off when guarantee costs are high (i.e., when markets perform poorly), thereby reducing the severity of large losses. Which of the following are plausible outcomes of introducing such an effective hedging program? (Select all that apply.)\n", "Options": {"A": "The 95th percentile of the cost distribution decreases, as the hedges are designed to be most effective in adverse scenarios.", "B": "The policy liability `L` increases because the insurer must now provision for the cost of the hedging instruments (e.g., option premiums).", "C": "The required capital `K` decreases, as the hedging program reduces the gap between the average tail cost and the mean cost.", "D": "The `CTE(95)` of the cost distribution decreases, reflecting mitigation of the most extreme losses."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to reason about the impact of a risk management action (hedging) on the shape of a probability distribution and the resulting regulatory capital implications.\nDepth Strategy: Reverse-Reasoning. Given the action (hedging), the candidate must identify all plausible effects on the underlying risk model's outputs.\nDistractor Logic:\n- C (Conceptual Distractor - Almost Right): This is a sophisticated distractor. While hedging has a cost, an *effective* program's reduction in expected guarantee payouts should be greater than the cost of the hedge itself. Therefore, the net effect on the mean cost (and thus the liability `L`) should be a decrease, not an increase. This option tempts the user to focus only on the cost of the hedge without considering its benefit.", "qid": "12", "question": "### Background\n\nAn insurer implements a dynamic hedging program using derivatives to mitigate the risk of its segregated fund guarantees. This program is incorporated into its internal model, which generates a probability distribution for the future cost of the guarantees, `C`. The capital requirement, `K`, is calculated as `K = CTE(95) - L`, where `L` is the policy liability (set to the mean of the cost distribution) and `CTE(95)` is the Conditional Tail Expectation at the 95th percentile.\n\n### Question\n\nAn effective hedging program is designed to pay off when guarantee costs are high (i.e., when markets perform poorly), thereby reducing the severity of large losses. Which of the following are plausible outcomes of introducing such an effective hedging program? (Select all that apply.)\n", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 13, "Question": "### Background\n\nThe paper contrasts the traditional factor-based Risk-Based Capital (RBC) system with a proposed Unified Valuation System (UVS). The UVS framework requires an insurer to use an internal model to generate a full probability distribution of the present value of future net costs, `PV(NC)`, for a block of policies. Liabilities and capital are then defined from this single distribution.\n\n### Data / Model Specification\n\nA potential UVS implementation defines the policy liability and total required assets as:\n\n  \n\\text{Policy Liability: } L_{UVS} = \\mathrm{CTE}(70) \\quad \\text{(Eq. 1)}\n \n\n  \n\\text{Total Required Assets: } L_{UVS} + K_{UVS} = \\mathrm{CTE}(95) \\quad \\text{(Eq. 2)}\n \n\nwhere `CTE(α)` is the Conditional Tail Expectation at the `α`-th percentile of the `PV(NC)` distribution and `K_UVS` is the required capital.\n\n### Question\n\nBased on the principles of the UVS framework as described, which of the following statements are valid conclusions or characteristics? (Select all that apply.)\n", "Options": {"A": "A primary weakness of the traditional factor-based RBC system that UVS addresses is its static, retrospective nature, as it relies on a company's latest financial statements rather than forward-looking projections.", "B": "The UVS framework is inherently more risk-sensitive than a traditional factor-based RBC system because it directly reflects a company's specific risk profile and risk management effectiveness in its capital calculation.", "C": "The required capital, `K_UVS`, represents the financial resources needed to cover the average loss in the region between the 70th and 95th percentiles of the cost distribution.", "D": "The required capital can be expressed as `K_UVS = CTE(95) - CTE(70)`."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to synthesize the conceptual advantages of the UVS framework and derive the capital formula from its definitional components.\nDepth Strategy: Scenario Application / Computational Judgment. The candidate must apply the UVS principles to contrast it with the older system and perform an algebraic derivation.\nDistractor Logic:\n- A (Conceptual Distractor - Almost Right): This is a subtle misinterpretation of the derived formula `K_UVS = CTE(95) - CTE(70)`. This formula represents the difference between two conditional averages, not the average of the values *between* two percentiles. The latter would be a different statistical quantity. This tests for a precise understanding of the CTE measure.", "qid": "13", "question": "### Background\n\nThe paper contrasts the traditional factor-based Risk-Based Capital (RBC) system with a proposed Unified Valuation System (UVS). The UVS framework requires an insurer to use an internal model to generate a full probability distribution of the present value of future net costs, `PV(NC)`, for a block of policies. Liabilities and capital are then defined from this single distribution.\n\n### Data / Model Specification\n\nA potential UVS implementation defines the policy liability and total required assets as:\n\n  \n\\text{Policy Liability: } L_{UVS} = \\mathrm{CTE}(70) \\quad \\text{(Eq. 1)}\n \n\n  \n\\text{Total Required Assets: } L_{UVS} + K_{UVS} = \\mathrm{CTE}(95) \\quad \\text{(Eq. 2)}\n \n\nwhere `CTE(α)` is the Conditional Tail Expectation at the `α`-th percentile of the `PV(NC)` distribution and `K_UVS` is the required capital.\n\n### Question\n\nBased on the principles of the UVS framework as described, which of the following statements are valid conclusions or characteristics? (Select all that apply.)\n", "model_evaluation_result": {"model_raw_response": "\"A,B,D\"", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 139, "Question": "### Background\n\n**Research Question.** What were the realized returns from bond refunding operations by public utilities in 1962-63, and were these operations profitable according to a theoretically sound decision rule?\n\n**Setting / Data-Generating Environment.** The analysis is based on an empirical study of 40 bond refunding operations undertaken by public utilities during 1962-63. The primary metric for evaluation is the after-tax internal rate of return (IRR) on the net cash investment required for the refunding.\n\n**Variables & Parameters.**\n- `IRR`: The after-tax internal rate of return on the refunding investment.\n- `I_0`: Net cash investment required at time 0 to execute the refunding.\n- `S_t`: Net after-tax cash savings at time `t` resulting from the refunding.\n- `y_{new}`: The pre-tax yield on the new refunding bonds. For the sample period, this was between 4% and 5%.\n- `k_d`: The after-tax cost of the new debt, which serves as the hurdle rate for the investment.\n- `τ`: The corporate income tax rate, assumed to be 50%.\n\n---\n\n### Data / Model Specification\n\nThe Internal Rate of Return (`IRR`) for each refunding operation is calculated by solving the following equation for `IRR`:\n  \nI_0 = \\sum_{t=1}^{T} \\frac{S_t}{(1+IRR)^t} \\quad \\text{(Eq. (1))}\n \nThe author's proposed decision rule is that a refunding operation is profitable if its `IRR` exceeds the relevant cost of funds:\n  \n\\text{Refund if } IRR > k_d = y_{new}(1-\\tau) \\quad \\text{(Eq. (2))}\n \nEmpirical results for the 40 refunding operations are presented in Table 1.\n\n**Table 1: Rates of Return Earned on Forty Bond Refundings by Public Utilities in 1962 and 1963 (Percent)**\n\n| | | | |\n| :--- | :--- | :--- | :--- |\n| 3.6* | 9.1 | 11.5* | 14.7* |\n| 5.5 | 9.5 | 11.9* | 14.8 |\n| 5.7 | 9.6* | 12.3 | 15.3* |\n| 6.5 | 9.7 | 12.3 | 16.1 |\n| 6.8 | 9.9 | 12.8* | 16.1 |\n| 7.1* | 10.1 | 12.8 | 17.2 |\n| 7.3 | 10.4* | 13.0 | 19.7 |\n| 8.0 | 10.4 | 13.1 | 23.8 |\n| 8.2 | 11.2 | 13.8 | 26.7* |\n| 8.7 | 11.3 | 14.6* | 43.4 |\n\n*Note: An asterisk (*) denotes that reasons in addition to interest savings were cited for the refunding.*\n\n---\n\n### Question\n\nBased on the provided data and decision rule, select all of the following statements that are correct assessments of the 1962-63 public utility bond refundings.", "Options": {"A": "According to the author's proposed framework, all 40 refunding operations were profitable for the firms.", "B": "The refunding with the lowest IRR of 3.6% was unprofitable because this return is below the firm's overall cost of capital, which is typically higher than the cost of debt.", "C": "The appropriate hurdle rate is the pre-tax yield on the new bonds (4-5%), which implies the refunding with a 3.6% IRR was unprofitable.", "D": "The appropriate after-tax hurdle rate (`k_d`) for these decisions was in the range of 2.0% to 2.5%."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to perform a quantitative evaluation using the paper's central decision rule. It uses a Computational Judgment strategy, decomposed into two atomic propositions (calculating the hurdle rate and applying it to the data) to create a genuine multi-select question. Distractor C represents a common error of using a pre-tax instead of after-tax cost of debt. Distractor D introduces a conceptual error discussed in the paper: incorrectly using the overall cost of capital as the hurdle rate for a low-risk refunding decision.", "qid": "139", "question": "### Background\n\n**Research Question.** What were the realized returns from bond refunding operations by public utilities in 1962-63, and were these operations profitable according to a theoretically sound decision rule?\n\n**Setting / Data-Generating Environment.** The analysis is based on an empirical study of 40 bond refunding operations undertaken by public utilities during 1962-63. The primary metric for evaluation is the after-tax internal rate of return (IRR) on the net cash investment required for the refunding.\n\n**Variables & Parameters.**\n- `IRR`: The after-tax internal rate of return on the refunding investment.\n- `I_0`: Net cash investment required at time 0 to execute the refunding.\n- `S_t`: Net after-tax cash savings at time `t` resulting from the refunding.\n- `y_{new}`: The pre-tax yield on the new refunding bonds. For the sample period, this was between 4% and 5%.\n- `k_d`: The after-tax cost of the new debt, which serves as the hurdle rate for the investment.\n- `τ`: The corporate income tax rate, assumed to be 50%.\n\n---\n\n### Data / Model Specification\n\nThe Internal Rate of Return (`IRR`) for each refunding operation is calculated by solving the following equation for `IRR`:\n  \nI_0 = \\sum_{t=1}^{T} \\frac{S_t}{(1+IRR)^t} \\quad \\text{(Eq. (1))}\n \nThe author's proposed decision rule is that a refunding operation is profitable if its `IRR` exceeds the relevant cost of funds:\n  \n\\text{Refund if } IRR > k_d = y_{new}(1-\\tau) \\quad \\text{(Eq. (2))}\n \nEmpirical results for the 40 refunding operations are presented in Table 1.\n\n**Table 1: Rates of Return Earned on Forty Bond Refundings by Public Utilities in 1962 and 1963 (Percent)**\n\n| | | | |\n| :--- | :--- | :--- | :--- |\n| 3.6* | 9.1 | 11.5* | 14.7* |\n| 5.5 | 9.5 | 11.9* | 14.8 |\n| 5.7 | 9.6* | 12.3 | 15.3* |\n| 6.5 | 9.7 | 12.3 | 16.1 |\n| 6.8 | 9.9 | 12.8* | 16.1 |\n| 7.1* | 10.1 | 12.8 | 17.2 |\n| 7.3 | 10.4* | 13.0 | 19.7 |\n| 8.0 | 10.4 | 13.1 | 23.8 |\n| 8.2 | 11.2 | 13.8 | 26.7* |\n| 8.7 | 11.3 | 14.6* | 43.4 |\n\n*Note: An asterisk (*) denotes that reasons in addition to interest savings were cited for the refunding.*\n\n---\n\n### Question\n\nBased on the provided data and decision rule, select all of the following statements that are correct assessments of the 1962-63 public utility bond refundings.", "model_evaluation_result": {"model_raw_response": "D", "model_answer": ["D"], "error": null}}
{"ID": 130, "Question": "### Background\n\nThe study estimates a four-way ANOVA model to assess the impact of a stock's 5, 11, 23, and 35-month winner/loser status on its monthly abnormal volume. The model is estimated separately for different periods of the year to detect seasonal patterns driven by tax motives.\n\n### Data / Model Specification\n\nThe estimated model is:\n  \nAV_{i} = \\alpha + \\beta_{1}D5_{i} + \\beta_{2}D11_{i} + \\beta_{3}D23_{i} + \\beta_{4}D35_{i} + e_{i} \\quad \\text{(Eq. 1)}\n \nwhere `AV_i` is the abnormal volume for stock `i` and `D5_i`, `D11_i`, etc., are binary variables equal to 1 if the stock was a 'winner' over the corresponding prior period (e.g., 5 months), and 0 otherwise. The coefficient `α` represents the abnormal volume for a stock that was a loser across all four periods.\n\nThe following table, derived from Table VI in the paper, presents a selection of estimated coefficients for NYSE stocks for three distinct periods: the baseline (Feb-Oct), tax-loss selling season (Dec), and tax-deferral season (Jan).\n\n**Table 1: Selected ANOVA Coefficients for NYSE Stocks**\n| Period | `α` | `β_1` (5-month) | `β_2` (11-month) |\n| :--- | :--- | :--- | :--- |\n| Feb-Oct | -0.50 | 0.28 | 0.41 |\n| Dec | -0.09 | 0.16 | 0.08 |\n| Jan | -0.55 | 0.48 | 0.46 |\n\n---\n\nBased on the model and data provided, which of the following statements are correct interpretations of the results?\n\nSelect all that apply.", "Options": {"A": "The predicted abnormal volume for a stock that was a 5-month winner but an 11-month loser in December is 0.07%.", "B": "In January, the abnormal volume for a stock that was a loser across all four periods is predicted to be -0.55%, which is lower than the abnormal volume for a similar consistent loser during the Feb-Oct period.", "C": "The combined impact of recent gains (5-month and 11-month) on abnormal volume is substantially dampened in December compared to the Feb-Oct baseline, providing evidence for a tax-induced 'lock-in' effect for winners.", "D": "In the baseline Feb-Oct period, the combined marginal impact on abnormal volume for a stock that is a winner over both the 5-month and 11-month horizons is 0.69%."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret coefficients from a multi-variable regression model, perform calculations based on those coefficients, and connect the numerical results to the paper's core hypotheses about seasonal tax effects ('lock-in' and 'present value' motives). Depth Strategy: Computational Judgment. Candidates must calculate predicted values and differences between periods to evaluate the options. Distractor Logic: All options require careful calculation and interpretation. A is a direct calculation (0.28 + 0.41 = 0.69). B requires comparing the sum for Dec (0.16+0.08=0.24) to the sum for Feb-Oct (0.69) and interpreting the large drop. C requires comparing the 'α' coefficients for Jan (-0.55) and Feb-Oct (-0.50). D is a calculation for a specific scenario in December: α + β1 = -0.09 + 0.16 = 0.07. All are correct, testing comprehensive understanding.", "qid": "130", "question": "### Background\n\nThe study estimates a four-way ANOVA model to assess the impact of a stock's 5, 11, 23, and 35-month winner/loser status on its monthly abnormal volume. The model is estimated separately for different periods of the year to detect seasonal patterns driven by tax motives.\n\n### Data / Model Specification\n\nThe estimated model is:\n  \nAV_{i} = \\alpha + \\beta_{1}D5_{i} + \\beta_{2}D11_{i} + \\beta_{3}D23_{i} + \\beta_{4}D35_{i} + e_{i} \\quad \\text{(Eq. 1)}\n \nwhere `AV_i` is the abnormal volume for stock `i` and `D5_i`, `D11_i`, etc., are binary variables equal to 1 if the stock was a 'winner' over the corresponding prior period (e.g., 5 months), and 0 otherwise. The coefficient `α` represents the abnormal volume for a stock that was a loser across all four periods.\n\nThe following table, derived from Table VI in the paper, presents a selection of estimated coefficients for NYSE stocks for three distinct periods: the baseline (Feb-Oct), tax-loss selling season (Dec), and tax-deferral season (Jan).\n\n**Table 1: Selected ANOVA Coefficients for NYSE Stocks**\n| Period | `α` | `β_1` (5-month) | `β_2` (11-month) |\n| :--- | :--- | :--- | :--- |\n| Feb-Oct | -0.50 | 0.28 | 0.41 |\n| Dec | -0.09 | 0.16 | 0.08 |\n| Jan | -0.55 | 0.48 | 0.46 |\n\n---\n\nBased on the model and data provided, which of the following statements are correct interpretations of the results?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 119, "Question": "### Background\n\n**Research Question.** Has the efficiency of Indonesian banks converged over time, and how did the Asian financial crisis and subsequent reforms affect the speed of this convergence?\n\n**Setting / Data-Generating Environment.** Panel data analysis of Indonesian bank efficiency is used to test for `β`-convergence (laggards catching up) and `σ`-convergence (reduction in overall dispersion) before and after the 1997-1999 financial crisis.\n\n**Variables & Parameters.**\n\n*   `$\\ln(\\hat{\\hat{\\rho}}_{j,t})$`: Log of the bias-corrected efficiency score for bank `j` at time `t`.\n*   `$E_{j,t}$`: Log deviation of bank `j`'s efficiency from the industry mean at time `t`.\n*   `$\\theta$`: The `β`-convergence parameter. A negative value indicates catch-up.\n*   `$\\eta$`: The `σ`-convergence parameter. A negative value indicates shrinking dispersion.\n\n---\n\n### Data / Model Specification\n\nThe models for `β`- and `σ`-convergence are given by:\n\n  \n\\ln\\hat{\\hat{\\rho}}_{j,t} - \\ln\\hat{\\hat{\\rho}}_{j,t-1} = \\phi + \\theta \\ln\\hat{\\hat{\\rho}}_{j,t} + \\nu_{j,t} \\quad \\text{(Eq. 1: } \\beta \\text{-convergence)}\n \n\n  \nE_{j,t} - E_{j,t-1} = \\alpha + \\eta E_{j,t-1} + \\omega_{j,t} \\quad \\text{(Eq. 2: } \\sigma \\text{-convergence)}\n \n\n**Table 1. Convergence Results (Model 2)**\n\n| Period | Parameter | Coefficient | P-value |\n| :--- | :--- | :--- | :--- |\n| **`β`-Convergence** | | |\n| 1992-1996 | `$\\theta$` | -1.0436 | 0.0000 |\n| 2000-2007 | `$\\theta$` | -0.8249 | 0.0000 |\n| **`σ`-Convergence** | | |\n| 1992-1996 | `$\\eta$` | -1.0509 | 0.0000 |\n| 2000-2007 | `$\\eta$` | -0.8997 | 0.0000 |\n\n---\n\n### Question\n\nBased on the convergence models and the results for Model 2 in Table 1, select all statements that are correct interpretations of the findings.", "Options": {"A": "`β`-convergence is a sufficient condition for `σ`-convergence, meaning a reduction in dispersion is guaranteed if laggard banks are catching up.", "B": "The results indicate that the speed of `β`-convergence was faster in the post-crisis period (2000-2007) than in the pre-crisis period (1992-1996).", "C": "The significant negative coefficient for `$\\eta$` in the post-crisis period provides evidence of `σ`-convergence, meaning the overall dispersion of efficiency scores across the banking sector was decreasing over time.", "D": "The significant negative coefficient for `$\\theta$` in the post-crisis period provides evidence of `β`-convergence, meaning that less efficient banks tended to improve their efficiency at a faster rate than more efficient banks."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the fundamental interpretation of the two types of convergence, which is central to the paper's conclusion. It uses an atomic decomposition strategy. Distractor C is a conceptual opposite based on a direct comparison of coefficients from the table. Distractor D targets a common theoretical misconception about the relationship between the two convergence types.", "qid": "119", "question": "### Background\n\n**Research Question.** Has the efficiency of Indonesian banks converged over time, and how did the Asian financial crisis and subsequent reforms affect the speed of this convergence?\n\n**Setting / Data-Generating Environment.** Panel data analysis of Indonesian bank efficiency is used to test for `β`-convergence (laggards catching up) and `σ`-convergence (reduction in overall dispersion) before and after the 1997-1999 financial crisis.\n\n**Variables & Parameters.**\n\n*   `$\\ln(\\hat{\\hat{\\rho}}_{j,t})$`: Log of the bias-corrected efficiency score for bank `j` at time `t`.\n*   `$E_{j,t}$`: Log deviation of bank `j`'s efficiency from the industry mean at time `t`.\n*   `$\\theta$`: The `β`-convergence parameter. A negative value indicates catch-up.\n*   `$\\eta$`: The `σ`-convergence parameter. A negative value indicates shrinking dispersion.\n\n---\n\n### Data / Model Specification\n\nThe models for `β`- and `σ`-convergence are given by:\n\n  \n\\ln\\hat{\\hat{\\rho}}_{j,t} - \\ln\\hat{\\hat{\\rho}}_{j,t-1} = \\phi + \\theta \\ln\\hat{\\hat{\\rho}}_{j,t} + \\nu_{j,t} \\quad \\text{(Eq. 1: } \\beta \\text{-convergence)}\n \n\n  \nE_{j,t} - E_{j,t-1} = \\alpha + \\eta E_{j,t-1} + \\omega_{j,t} \\quad \\text{(Eq. 2: } \\sigma \\text{-convergence)}\n \n\n**Table 1. Convergence Results (Model 2)**\n\n| Period | Parameter | Coefficient | P-value |\n| :--- | :--- | :--- | :--- |\n| **`β`-Convergence** | | |\n| 1992-1996 | `$\\theta$` | -1.0436 | 0.0000 |\n| 2000-2007 | `$\\theta$` | -0.8249 | 0.0000 |\n| **`σ`-Convergence** | | |\n| 1992-1996 | `$\\eta$` | -1.0509 | 0.0000 |\n| 2000-2007 | `$\\eta$` | -0.8997 | 0.0000 |\n\n---\n\n### Question\n\nBased on the convergence models and the results for Model 2 in Table 1, select all statements that are correct interpretations of the findings.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 21, "Question": "### Background\n\nA study uses a Heckman two-stage model to analyze the effect of ownership structure on the value of family firms. This method is chosen to correct for potential self-selection bias, as firms with certain unobserved characteristics might be more likely to be family-owned, and those same characteristics could also affect firm value.\n\n### Data / Model Specification\n\nThe Heckman model consists of two equations:\n\n1.  **Selection Equation (First Stage):** A probit model estimates the probability that a firm is a family firm (`FF_i=1`) based on a vector of characteristics `Z_i`. The error term is `μ_i`.\n      \nP(FF_i=1) = \\Phi(\\gamma'Z_i)\n \n2.  **Outcome Equation (Second Stage):** A regression estimates firm value (`AVALUE`) for the subsample of family firms. The error term is `ε_it`.\n      \nAVALUE_{it} = \\alpha_{0} + \\beta X_{it-1} + \\delta \\lambda_i + ... + \\varepsilon_{it}\n \n\nThe key correction term is the **Inverse Mills Ratio (`λ_i`)**, which is included as a regressor in the second stage. The errors `(μ_i, ε_it)` are assumed to be bivariate normal with correlation `ρ`.\n\n---\n\nWhich of the following statements about the Heckman model's application in this study are theoretically correct?\n", "Options": {"A": "A statistically significant coefficient `δ` on the Inverse Mills Ratio implies that unobserved factors influencing a firm's choice to be family-owned are correlated with unobserved factors that determine its value.", "B": "If the coefficient `δ` were found to be statistically insignificant, it would suggest that a standard OLS regression on the family firm subsample would have produced unbiased estimates of `β`.", "C": "The model requires at least one variable in the selection equation (`Z_i`) that is excluded from the outcome equation (`X_it`) to ensure proper identification.", "D": "The coefficient `δ` on the Inverse Mills Ratio is a consistent estimator of the product of the correlation `ρ` and the standard deviation of the outcome equation's error term, `σ_ε`."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This question assesses deep knowledge of the econometric methodology (Heckman model) used in the paper. It requires understanding the purpose of the model, the interpretation of its key parameters, and the conditions for its application.\nDepth Strategy: Reverse-Reasoning. The user is given the model and must identify all correct theoretical statements about its properties and interpretation.\nDistractor Logic: All options provided are correct statements about the Heckman two-stage model as described in econometrics literature and applied in the paper. This format rigorously tests comprehensive knowledge, as the user must be confident in affirming all four distinct theoretical points.\n- A (Correct): This is the fundamental interpretation of a significant `δ`; it confirms the presence of selection bias.\n- B (Correct): This is the formal definition of the coefficient `δ = ρσ_ε`.\n- C (Correct): An insignificant `δ` implies `ρ=0`, meaning there is no correlation between the errors and thus no selection bias. OLS would be consistent in that case.\n- D (Correct): This is the exclusion restriction, a critical assumption for the identification of the Heckman model. The paper explicitly mentions using CEO tenure for this purpose.", "qid": "21", "question": "### Background\n\nA study uses a Heckman two-stage model to analyze the effect of ownership structure on the value of family firms. This method is chosen to correct for potential self-selection bias, as firms with certain unobserved characteristics might be more likely to be family-owned, and those same characteristics could also affect firm value.\n\n### Data / Model Specification\n\nThe Heckman model consists of two equations:\n\n1.  **Selection Equation (First Stage):** A probit model estimates the probability that a firm is a family firm (`FF_i=1`) based on a vector of characteristics `Z_i`. The error term is `μ_i`.\n      \nP(FF_i=1) = \\Phi(\\gamma'Z_i)\n \n2.  **Outcome Equation (Second Stage):** A regression estimates firm value (`AVALUE`) for the subsample of family firms. The error term is `ε_it`.\n      \nAVALUE_{it} = \\alpha_{0} + \\beta X_{it-1} + \\delta \\lambda_i + ... + \\varepsilon_{it}\n \n\nThe key correction term is the **Inverse Mills Ratio (`λ_i`)**, which is included as a regressor in the second stage. The errors `(μ_i, ε_it)` are assumed to be bivariate normal with correlation `ρ`.\n\n---\n\nWhich of the following statements about the Heckman model's application in this study are theoretically correct?\n", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 136, "Question": "### Background\n\n**Research Question.** How does the Basel II Advanced Internal Ratings-Based (A-IRB) framework translate a bank's internal risk estimates for a credit card portfolio into a regulatory capital requirement?\n\n### Data / Model Specification\n\nThe Basel II risk weight (`RW`) for Qualifying Revolving Retail Exposures (QRREs) is calculated as:\n\n  \nRW = 12.5 \\times \\left[ \\left( LGD \\times N\\left(\\frac{N^{-1}(PD) + \\sqrt{AVC} \\times N^{-1}(0.999)}{\\sqrt{1-AVC}}\\right) \\right) - (LGD \\times PD) \\right] \n \n(Eq. 1)\n\nWhere `PD` is Probability of Default, `LGD` is Loss Given Default, `AVC` is a fixed Asset Value Correlation parameter (4% for QRREs), and `N(·)` is the standard normal CDF. The framework's philosophy is that regulatory capital should cover **unexpected losses**, while **expected losses** (`EL = PD × LGD`) should be covered by provisions and pricing.\n\n---\n\nWhich of the following statements are valid interpretations or properties of the Basel II risk-weight formula in **Eq. (1)**?\n\nSelect all that apply.", "Options": {"A": "The formula isolates unexpected loss by subtracting the average expected loss from the total loss calculated under the stress scenario.", "B": "The term `LGD × N(...)` represents the total loss per dollar of exposure under a severe stress scenario (99.9th percentile), while the term `LGD × PD` represents the average expected loss.", "C": "An increase in the Asset Value Correlation (`AVC`) parameter, holding `PD` and `LGD` constant, would lead to a higher risk weight (`RW`).", "D": "A higher `AVC` increases the required capital because it implies greater systematic risk, meaning more borrowers are likely to default simultaneously in a downturn, thus increasing tail risk."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's conceptual understanding of the Basel II A-IRB risk-weight formula, including the economic intuition behind its components and its sensitivity to key parameters. Depth Strategy: Computational Judgment. While not requiring a full calculation, the user must judge the properties and comparative statics of the formula. Distractor Logic: All options are correct, forming a complete conceptual check on the formula. A tests interpretation of the components. B tests understanding of the 'unexpected loss' concept. C tests the comparative statics with respect to `AVC`. D provides the economic reasoning for the result in C. This structure ensures the candidate understands not just what the formula does, but why it is structured that way.", "qid": "136", "question": "### Background\n\n**Research Question.** How does the Basel II Advanced Internal Ratings-Based (A-IRB) framework translate a bank's internal risk estimates for a credit card portfolio into a regulatory capital requirement?\n\n### Data / Model Specification\n\nThe Basel II risk weight (`RW`) for Qualifying Revolving Retail Exposures (QRREs) is calculated as:\n\n  \nRW = 12.5 \\times \\left[ \\left( LGD \\times N\\left(\\frac{N^{-1}(PD) + \\sqrt{AVC} \\times N^{-1}(0.999)}{\\sqrt{1-AVC}}\\right) \\right) - (LGD \\times PD) \\right] \n \n(Eq. 1)\n\nWhere `PD` is Probability of Default, `LGD` is Loss Given Default, `AVC` is a fixed Asset Value Correlation parameter (4% for QRREs), and `N(·)` is the standard normal CDF. The framework's philosophy is that regulatory capital should cover **unexpected losses**, while **expected losses** (`EL = PD × LGD`) should be covered by provisions and pricing.\n\n---\n\nWhich of the following statements are valid interpretations or properties of the Basel II risk-weight formula in **Eq. (1)**?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 107, "Question": "### Background\n\nA study investigates the debt-substitution effect, which posits a negative relationship between corporate tax aggressiveness and corporate debt. The analysis uses a fixed-effects panel regression on 6,967 firm-year observations from 2001-2010.\n\n### Data / Model Specification\n\n**Table 1: Descriptive Statistics**\n| Variable | Mean | Std. Dev. |\n| :--- | :---: | :---: |\n| `BDEBT` | 0.326 | 0.895 |\n| `BTG1` | 0.097 | 0.739 |\n\n**Table 2: Regression Results for `BDEBT`**\n| Dependent Variable | Variable | Coefficient | t-statistic |\n| :--- | :--- | :---: | :---: |\n| `BDEBT` (Model FEM1) | `BTG1` | -0.471 | (-2.03) |\n\n---\n\nBased on the provided data, select all of the following statements that are valid.\n", "Options": {"A": "A one-unit increase in `BTG1` is associated with a 0.471% decrease in the `BDEBT` ratio.", "B": "A one standard deviation increase in tax aggressiveness (`BTG1`) is associated with a decrease of approximately 0.348 in the `BDEBT` ratio.", "C": "The coefficient on `BTG1` is statistically significant at the 5% level, supporting the debt-substitution hypothesis.", "D": "The economic magnitude of the debt-substitution effect is substantial, as a one standard deviation increase in `BTG1` is associated with a change in `BDEBT` that is larger than the mean `BDEBT`."}, "Answer": ["B", "C", "D"], "pi_justification": "This item assesses the ability to synthesize information from descriptive and regression tables to evaluate statistical and economic significance. It uses a Computational Judgment strategy. Option A is correct because the t-statistic |-2.03| > 1.96. Option B requires a calculation: -0.471 * 0.739 = -0.348069. Option C requires a second-level judgment based on the result from B, comparing the magnitude of the change (0.348) to the mean `BDEBT` (0.326), confirming the effect is economically large. Option D is a Unit/Scale Error distractor; the coefficient implies a 0.471 *unit* (or 47.1 percentage point) decrease, not a 0.471% decrease.", "qid": "107", "question": "### Background\n\nA study investigates the debt-substitution effect, which posits a negative relationship between corporate tax aggressiveness and corporate debt. The analysis uses a fixed-effects panel regression on 6,967 firm-year observations from 2001-2010.\n\n### Data / Model Specification\n\n**Table 1: Descriptive Statistics**\n| Variable | Mean | Std. Dev. |\n| :--- | :---: | :---: |\n| `BDEBT` | 0.326 | 0.895 |\n| `BTG1` | 0.097 | 0.739 |\n\n**Table 2: Regression Results for `BDEBT`**\n| Dependent Variable | Variable | Coefficient | t-statistic |\n| :--- | :--- | :---: | :---: |\n| `BDEBT` (Model FEM1) | `BTG1` | -0.471 | (-2.03) |\n\n---\n\nBased on the provided data, select all of the following statements that are valid.\n", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 105, "Question": "### Background\n\n**Research Question.** This case investigates the market's immediate reaction to the announcement of a Reverse Takeover (RT) and seeks to identify the firm- and deal-specific characteristics that explain this reaction.\n\n**Setting.** An event study is conducted on a sample of RT announcements to measure the wealth effects for the public firm's shareholders. A multivariate regression is then used to explain the cross-sectional variation in these wealth effects.\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return, the dependent variable in the regression.\n- `Cash_Assets`: The ratio of cash to total assets for the public firm, a measure of liquidity.\n- `Strength_Private`: A dummy variable, =1 if 'strength of the private firm' is cited as a reason for the RT.\n\n---\n\n### Data / Model Specification\n\nThe wealth effect of the RT announcement is measured using Cumulative Abnormal Returns (CARs) around the announcement date. A regression model is then estimated to explain these CARs.\n\n**Table 1: Cumulative Abnormal Returns (CARs) around RT Announcements**\n\n| Sample | No. | CAR (-1,+1) |\n| :--- | :-: | :--- |\n| All firms | 121 | 25.10%*** |\n| Distressed | 44 | 10.17%*** |\n| Functional | 77 | 33.64%*** |\n\n*Note: Table adapted from Table 6. `***` denotes significance at the 1% level.*\n\n**Table 2: Multivariate Regression of 3-day `(-1,+1)` CARs**\n\n| Variable | Model 2 Coefficient (t-stat) |\n| :--- | :--- |\n| Intercept | 0.0032 (2.19)** |\n| Cash to Total Assets | 0.0045 (2.69)*** |\n| Strength of Private Firm | -0.0023 (-2.72)*** |\n\n*Note: Table adapted from Model 2 of Table 7. `**`, `***` denote significance at the 5% and 1% levels, respectively. Other variables are omitted for brevity.*\n\n---\n\n### Question\n\nAccording to the event study and regression results, which of the following statements correctly describe the market's reaction to Reverse Takeover (RT) announcements? Select all that apply.", "Options": {"A": "After controlling for other factors, public firms with greater pre-merger liquidity (higher cash-to-assets ratio) experienced significantly more positive announcement returns.", "B": "The regression results suggest that when a public firm explicitly cited the 'strength of the private firm' as a rationale, the market interpreted this as a positive signal, leading to higher abnormal returns.", "C": "The market reacted more favorably to announcements from financially distressed public firms than to those from financially functional ones.", "D": "On average, the announcement of an RT was associated with a large, statistically significant positive wealth effect for the public firm's shareholders."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an Atomic Decomposition strategy to test the interpretation of both event study results and multivariate regression coefficients. It assesses whether a student can connect the univariate findings (Table 1) with the ceteris paribus relationships identified in the regression (Table 2). The distractors are designed as Conceptual Opposites of the paper's key findings, effectively discriminating between students who have understood the results and those who have not.", "qid": "105", "question": "### Background\n\n**Research Question.** This case investigates the market's immediate reaction to the announcement of a Reverse Takeover (RT) and seeks to identify the firm- and deal-specific characteristics that explain this reaction.\n\n**Setting.** An event study is conducted on a sample of RT announcements to measure the wealth effects for the public firm's shareholders. A multivariate regression is then used to explain the cross-sectional variation in these wealth effects.\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return, the dependent variable in the regression.\n- `Cash_Assets`: The ratio of cash to total assets for the public firm, a measure of liquidity.\n- `Strength_Private`: A dummy variable, =1 if 'strength of the private firm' is cited as a reason for the RT.\n\n---\n\n### Data / Model Specification\n\nThe wealth effect of the RT announcement is measured using Cumulative Abnormal Returns (CARs) around the announcement date. A regression model is then estimated to explain these CARs.\n\n**Table 1: Cumulative Abnormal Returns (CARs) around RT Announcements**\n\n| Sample | No. | CAR (-1,+1) |\n| :--- | :-: | :--- |\n| All firms | 121 | 25.10%*** |\n| Distressed | 44 | 10.17%*** |\n| Functional | 77 | 33.64%*** |\n\n*Note: Table adapted from Table 6. `***` denotes significance at the 1% level.*\n\n**Table 2: Multivariate Regression of 3-day `(-1,+1)` CARs**\n\n| Variable | Model 2 Coefficient (t-stat) |\n| :--- | :--- |\n| Intercept | 0.0032 (2.19)** |\n| Cash to Total Assets | 0.0045 (2.69)*** |\n| Strength of Private Firm | -0.0023 (-2.72)*** |\n\n*Note: Table adapted from Model 2 of Table 7. `**`, `***` denote significance at the 5% and 1% levels, respectively. Other variables are omitted for brevity.*\n\n---\n\n### Question\n\nAccording to the event study and regression results, which of the following statements correctly describe the market's reaction to Reverse Takeover (RT) announcements? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 60, "Question": "### Background\n\n**Research Question.** How can a unified affine model framework be used to parsimoniously capture key stylized facts of commodity prices, such as mean reversion, stochastic volatility, seasonality, and jumps, and how do these features interact?\n\n**Setting.** A continuous-time model for the log-spot price of a commodity, designed to nest several prominent models as special cases.\n\n**Variables and Parameters.**\n- `y_t`: Log-spot price of the commodity.\n- `ν_t`: Instantaneous variance of the log-price return.\n- `g_t`: A deterministic function capturing seasonality and time trends.\n- `T_1, T_2`: Binary indicator variables (0 or 1) that activate the seasonality and jump components, respectively.\n\n---\n\n### Data / Model Specification\n\nThe general form for the affine styled-facts dynamics of the log-spot price `y_t` is given by `Eq. (1)`:\n\n  \nd y_{t}=\\left(a-b(y_{t}-T_{1} g_{t})-\\frac{1}{2}\\nu_{t}+T_{1}\\frac{d g_{t}}{d t}-T_{2}\\lambda\\bar{J}\\right)d t+\\sqrt{\\nu_{t}}d W_{S,t}^{P}+{ T}_{2} J d N_{t} \n \n\nThis general framework nests four models based on the settings of `T_1` and `T_2`:\n- MRSV (Mean Reversion, Stochastic Volatility): `{T_1, T_2} = {0, 0}`\n- MRSVJ (MRSV with Jumps): `{T_1, T_2} = {0, 1}`\n- MRSVS (MRSV with Seasonality): `{T_1, T_2} = {1, 0}`\n- MRSVJS (MRSV with Jumps and Seasonality): `{T_1, T_2} = {1, 1}`\n\n---\n\nConsider a scenario where a natural gas market is characterized by strong, predictable winter price spikes and occasional, unpredictable supply disruptions from geopolitical events. According to the model framework in `Eq. (1)`, which of the following statements correctly describe how the different model specifications would capture these market features?\n\nSelect all that apply.", "Options": {"A": "The baseline MRSV model (`T_1=0, T_2=0`) would capture both the winter spikes and supply disruptions through its stochastic volatility component `ν_t`.", "B": "In the MRSVJ model (`T_1=0, T_2=1`), the predictable winter price spikes would likely be misinterpreted as part of the stochastic jump process, potentially leading to a misspecified jump intensity `λ`.", "C": "In the full MRSVJS model (`T_1=1, T_2=1`), the term `T_1 * (d g_t / dt)` in the drift of `y_t` would capture the expected rate of price change due to seasonal factors.", "D": "The MRSVS model (`T_1=1, T_2=0`) would capture the predictable winter price spikes through the deterministic `g_t` function but would be unable to account for the sudden, unpredictable supply disruptions."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to map specific economic phenomena to the correct mathematical components of a complex, nested model. Strategy: Scenario Application. A qualitative market scenario is provided, and the user must identify which model components correctly represent the described dynamics. Distractor Logic: Option A is correct; without a seasonality term, the jump component is forced to explain large, periodic movements. Option B is correct; MRSVS is designed for seasonality but lacks a mechanism for sudden shocks. Option C is correct, identifying the specific term for the seasonal drift. Option D is a conceptual distractor (Formula Misuse); stochastic volatility captures the magnitude of random daily movements, not large, discrete events like supply disruptions or predictable seasonal trends.", "qid": "60", "question": "### Background\n\n**Research Question.** How can a unified affine model framework be used to parsimoniously capture key stylized facts of commodity prices, such as mean reversion, stochastic volatility, seasonality, and jumps, and how do these features interact?\n\n**Setting.** A continuous-time model for the log-spot price of a commodity, designed to nest several prominent models as special cases.\n\n**Variables and Parameters.**\n- `y_t`: Log-spot price of the commodity.\n- `ν_t`: Instantaneous variance of the log-price return.\n- `g_t`: A deterministic function capturing seasonality and time trends.\n- `T_1, T_2`: Binary indicator variables (0 or 1) that activate the seasonality and jump components, respectively.\n\n---\n\n### Data / Model Specification\n\nThe general form for the affine styled-facts dynamics of the log-spot price `y_t` is given by `Eq. (1)`:\n\n  \nd y_{t}=\\left(a-b(y_{t}-T_{1} g_{t})-\\frac{1}{2}\\nu_{t}+T_{1}\\frac{d g_{t}}{d t}-T_{2}\\lambda\\bar{J}\\right)d t+\\sqrt{\\nu_{t}}d W_{S,t}^{P}+{ T}_{2} J d N_{t} \n \n\nThis general framework nests four models based on the settings of `T_1` and `T_2`:\n- MRSV (Mean Reversion, Stochastic Volatility): `{T_1, T_2} = {0, 0}`\n- MRSVJ (MRSV with Jumps): `{T_1, T_2} = {0, 1}`\n- MRSVS (MRSV with Seasonality): `{T_1, T_2} = {1, 0}`\n- MRSVJS (MRSV with Jumps and Seasonality): `{T_1, T_2} = {1, 1}`\n\n---\n\nConsider a scenario where a natural gas market is characterized by strong, predictable winter price spikes and occasional, unpredictable supply disruptions from geopolitical events. According to the model framework in `Eq. (1)`, which of the following statements correctly describe how the different model specifications would capture these market features?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 135, "Question": "### Background\n\n**Research Question.** During a period of financial stress, how does the Basel II rule for securitized assets affect a Credit Card Specialty Bank's (CCSB) capital adequacy?\n\n**Setting.** A hypothetical average CCSB that has adopted Basel II experiences a downturn in its portfolio's performance. This triggers a 15% Credit Conversion Factor (CCF) on its off-balance-sheet securitized assets, effectively bringing a portion of their risk back into the regulatory capital calculation.\n\n### Data / Model Specification\n\nDuring a stress period, deteriorating performance of credit card asset-backed securities (CC-ABS) can trigger a positive `CCF`. For a bank with a typical securitization rate of 60.7%, a 15% `CCF` results in a 23.2% increase in the total assets subject to a capital charge. This further increases the bank's Risk-Weighted Assets (RWA), on top of the initial increase from adopting Basel II.\n\n**Table 1. Impact of a 15% CCF on a Basel II CCSB's Total Capital Ratio**\n\n| Scenario | Basel II (Zero CCF) | Basel II (15% CCF Triggered) |\n| :--- | :---: | :---: |\n| Total Capital Ratio | 12.8% | 10.4% |\n*Note: Values reflect the higher-impact scenario for a diversified bank structure. The minimum regulatory Total Capital Ratio is 8%, and the 'well-capitalized' threshold is 10%.*\n\n---\n\nBased on the scenario and data in **Table 1**, which of the following statements are valid consequences or interpretations of the CCF rule being triggered?\n\nSelect all that apply.", "Options": {"A": "The CCF mechanism acts as a pro-cyclical amplifier of financial distress, increasing a bank's capital requirement precisely when its asset quality is deteriorating.", "B": "Triggering the 15% CCF erodes the bank's capital buffer to a razor-thin margin above the 10% 'well-capitalized' threshold, making the Total Capital requirement a binding constraint on management's decisions.", "C": "The CCF rule could create a competitive disadvantage for Basel II banks relative to Basel I banks (which have no CCF), as the former face an explicit, automatic capital penalty during a downturn that the latter do not.", "D": "The 23.2% increase in the asset base for capital calculation is derived from applying the 15% CCF to the 60.7% of the bank's managed portfolio that is securitized."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret the quantitative impact of the CCF rule, understand its derivation, and grasp its broader strategic and economic implications. Depth Strategy: Scenario Application. The user must apply the concept of the CCF rule to the specific scenario of a bank under stress and identify all correct consequences. Distractor Logic: This is another comprehensive assessment item where all options are correct. Option A tests economic intuition (pro-cyclicality). Option B tests quantitative interpretation of the table. Option C tests understanding of the underlying calculation. Option D tests the ability to draw a high-level strategic conclusion. Requiring the selection of all four ensures a holistic understanding of the CCF mechanism's impact.", "qid": "135", "question": "### Background\n\n**Research Question.** During a period of financial stress, how does the Basel II rule for securitized assets affect a Credit Card Specialty Bank's (CCSB) capital adequacy?\n\n**Setting.** A hypothetical average CCSB that has adopted Basel II experiences a downturn in its portfolio's performance. This triggers a 15% Credit Conversion Factor (CCF) on its off-balance-sheet securitized assets, effectively bringing a portion of their risk back into the regulatory capital calculation.\n\n### Data / Model Specification\n\nDuring a stress period, deteriorating performance of credit card asset-backed securities (CC-ABS) can trigger a positive `CCF`. For a bank with a typical securitization rate of 60.7%, a 15% `CCF` results in a 23.2% increase in the total assets subject to a capital charge. This further increases the bank's Risk-Weighted Assets (RWA), on top of the initial increase from adopting Basel II.\n\n**Table 1. Impact of a 15% CCF on a Basel II CCSB's Total Capital Ratio**\n\n| Scenario | Basel II (Zero CCF) | Basel II (15% CCF Triggered) |\n| :--- | :---: | :---: |\n| Total Capital Ratio | 12.8% | 10.4% |\n*Note: Values reflect the higher-impact scenario for a diversified bank structure. The minimum regulatory Total Capital Ratio is 8%, and the 'well-capitalized' threshold is 10%.*\n\n---\n\nBased on the scenario and data in **Table 1**, which of the following statements are valid consequences or interpretations of the CCF rule being triggered?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 121, "Question": "### Background\n\n**Research Question.** What bank-specific and macroeconomic factors are associated with bank inefficiency?\n\n**Setting / Data-Generating Environment.** A second-stage truncated regression is used to model the determinants of the bootstrap bias-corrected bank inefficiency scores (`$\\hat{\\hat{\\gamma}}_j$`) in the post-crisis Indonesian banking sector (2000-2007).\n\n**Variables & Parameters.**\n\n*   `$\\hat{\\hat{\\gamma}}_j$`: The bootstrap bias-corrected Shephard inefficiency score for bank `j`. Higher values mean higher inefficiency.\n*   `$\\hat{\\hat{\\rho}}_j = 1/\\hat{\\hat{\\gamma}}_j$`: The corresponding cost efficiency score.\n*   `OWN`: Ownership dummy; 1 if >50% foreign ownership, 0 otherwise.\n*   `Size`: Natural logarithm of total assets.\n*   `SOB`: State-Owned Bank dummy; 1 if state-owned, 0 otherwise.\n*   `$\\hat{\\hat{\\beta}}$`: The vector of estimated coefficients.\n\n---\n\n### Data / Model Specification\n\nThe second-stage model regresses inefficiency on a set of environmental variables:\n\n  \n\\hat{\\hat{\\gamma}}_{j} = z_{j}\\hat{\\hat{\\beta}} + \\varepsilon_{j} \\quad \\text{(Eq. 1)}\n \n\n**Table 1. Truncated Regression Results (Model 3, Post-Crisis 2000-2007)**\n\n| Variable | Coefficient (`$\\hat{\\hat{\\beta}}$`) |\n| :--- | :--- |\n| OWN | -1.1239* |\n| Size | 0.2316* |\n| SOB | -1.1659* |\n\n*Note: Dependent variable is the inefficiency score `$\\hat{\\hat{\\gamma}}_j$`. `*` denotes significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the regression results in Table 1, select all of the following statements that are correct.", "Options": {"A": "The results indicate that state-owned banks (`SOB`) were significantly less cost-efficient than foreign-owned banks (`OWN`).", "B": "The coefficient on `OWN` implies that, holding other factors constant, majority foreign-owned banks were significantly more cost-efficient than the baseline group (private domestic banks).", "C": "A bank with a predicted inefficiency score (`$\\hat{\\hat{\\gamma}}_j$`) of 2.5 would have a corresponding cost efficiency score (`$\\hat{\\hat{\\rho}}_j$`) of 0.25.", "D": "The coefficient on `Size` implies that larger banks were, on average, significantly less cost-efficient than smaller banks during this period."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the interpretation of regression coefficients where the dependent variable represents a 'bad' outcome (inefficiency). It uses an atomic decomposition strategy. The correct options test the direct interpretation of signs. Distractor C requires a more nuanced comparison of two coefficients, a common point of error. Distractor D uses a simple calculation to test for a formula misuse error in the fundamental reciprocal relationship between the efficiency and inefficiency scores.", "qid": "121", "question": "### Background\n\n**Research Question.** What bank-specific and macroeconomic factors are associated with bank inefficiency?\n\n**Setting / Data-Generating Environment.** A second-stage truncated regression is used to model the determinants of the bootstrap bias-corrected bank inefficiency scores (`$\\hat{\\hat{\\gamma}}_j$`) in the post-crisis Indonesian banking sector (2000-2007).\n\n**Variables & Parameters.**\n\n*   `$\\hat{\\hat{\\gamma}}_j$`: The bootstrap bias-corrected Shephard inefficiency score for bank `j`. Higher values mean higher inefficiency.\n*   `$\\hat{\\hat{\\rho}}_j = 1/\\hat{\\hat{\\gamma}}_j$`: The corresponding cost efficiency score.\n*   `OWN`: Ownership dummy; 1 if >50% foreign ownership, 0 otherwise.\n*   `Size`: Natural logarithm of total assets.\n*   `SOB`: State-Owned Bank dummy; 1 if state-owned, 0 otherwise.\n*   `$\\hat{\\hat{\\beta}}$`: The vector of estimated coefficients.\n\n---\n\n### Data / Model Specification\n\nThe second-stage model regresses inefficiency on a set of environmental variables:\n\n  \n\\hat{\\hat{\\gamma}}_{j} = z_{j}\\hat{\\hat{\\beta}} + \\varepsilon_{j} \\quad \\text{(Eq. 1)}\n \n\n**Table 1. Truncated Regression Results (Model 3, Post-Crisis 2000-2007)**\n\n| Variable | Coefficient (`$\\hat{\\hat{\\beta}}$`) |\n| :--- | :--- |\n| OWN | -1.1239* |\n| Size | 0.2316* |\n| SOB | -1.1659* |\n\n*Note: Dependent variable is the inefficiency score `$\\hat{\\hat{\\gamma}}_j$`. `*` denotes significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the regression results in Table 1, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 106, "Question": "### Background\n\nA study investigates how board independence (the proportion of outside directors) influences corporate debt and moderates the relationship between tax aggressiveness and debt (the debt-substitution effect). The study uses a fixed-effects panel regression analysis of 6,967 firm-year observations.\n\n### Data / Model Specification\n\nThe study estimates the following fixed-effects interaction model:\n\n  \nBDEBT_{it} = \\alpha_i + \\lambda_t + \\beta_1 BTG1_{it} + \\beta_2 OUTDIR_{it} + \\beta_3 (OUTDIR_{it} \\times BTG1_{it}) + \\text{Controls}_{it} + \\varepsilon_{it} \n \n\nwhere `BDEBT` is the book debt-to-assets ratio, `BTG1` is a measure of tax aggressiveness, and `OUTDIR` is the proportion of non-employee directors on the board.\n\n**Table 1: Regression Results (Model FEM1 for `BDEBT`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :---: | :---: |\n| `BTG1` (for `\\beta_1`) | -0.458 | (-1.94) |\n| `OUTDIR` (for `\\beta_2`) | -0.497 | (-2.08) |\n| `OUTDIR*BTG1` (for `\\beta_3`) | -0.784 | (-3.48) |\n\n---\n\nBased on the model and results, select all of the following statements that are correct interpretations or valid conclusions.", "Options": {"A": "The results imply that as board independence increases, a firm's debt policy becomes *less* sensitive to its tax aggressiveness.", "B": "The negative and statistically significant coefficient on `OUTDIR` supports a \"substitutive\" relationship, where effective board monitoring replaces the need for debt as a disciplinary mechanism.", "C": "The marginal effect of tax aggressiveness (`BTG1`) on debt (`BDEBT`) is given by the expression: `-0.458 - 0.784 * OUTDIR`.", "D": "For a firm where 70% of the board are outside directors (`OUTDIR` = 0.70), a one-unit increase in `BTG1` is associated with a 1.0068 unit decrease in the `BDEBT` ratio."}, "Answer": ["B", "C", "D"], "pi_justification": "This item assesses the ability to interpret direct and interaction effects from a regression table and perform a multi-step calculation. It uses a Computational Judgment strategy. Option A correctly interprets the direct effect of `OUTDIR` as supporting the substitutive view. Option B correctly derives the marginal effect from the interaction model. Option C correctly applies this marginal effect formula to a specific scenario (`OUTDIR`=0.70), requiring calculation: -0.458 - 0.784 * 0.70 = -1.0068. Option D is a conceptual distractor; the negative interaction term means sensitivity *increases* with `OUTDIR`, making the statement incorrect.", "qid": "106", "question": "### Background\n\nA study investigates how board independence (the proportion of outside directors) influences corporate debt and moderates the relationship between tax aggressiveness and debt (the debt-substitution effect). The study uses a fixed-effects panel regression analysis of 6,967 firm-year observations.\n\n### Data / Model Specification\n\nThe study estimates the following fixed-effects interaction model:\n\n  \nBDEBT_{it} = \\alpha_i + \\lambda_t + \\beta_1 BTG1_{it} + \\beta_2 OUTDIR_{it} + \\beta_3 (OUTDIR_{it} \\times BTG1_{it}) + \\text{Controls}_{it} + \\varepsilon_{it} \n \n\nwhere `BDEBT` is the book debt-to-assets ratio, `BTG1` is a measure of tax aggressiveness, and `OUTDIR` is the proportion of non-employee directors on the board.\n\n**Table 1: Regression Results (Model FEM1 for `BDEBT`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :---: | :---: |\n| `BTG1` (for `\\beta_1`) | -0.458 | (-1.94) |\n| `OUTDIR` (for `\\beta_2`) | -0.497 | (-2.08) |\n| `OUTDIR*BTG1` (for `\\beta_3`) | -0.784 | (-3.48) |\n\n---\n\nBased on the model and results, select all of the following statements that are correct interpretations or valid conclusions.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 131, "Question": "### Background\n\nThe study uses a one-way analysis of variance to measure the difference in abnormal trading volume between 'winner' and 'loser' stocks. This helps disentangle the baseline effects of non-tax motives from the seasonal effects of tax-motivated trading.\n\n### Data / Model Specification\n\nThe following cross-sectional regression is run for each month:\n  \nAV_{i} = \\alpha + \\beta DN_{i} + e_{i} \\quad \\text{(Eq. 1)}\n \nwhere `AV_i` is the abnormal volume (in percent per month), and `DN_i` is a binary variable equal to 1 for 'winners' and 0 for 'losers'. Thus, `α` represents the average abnormal volume for losers, and `α+β` represents the average for winners.\n\nThe table below summarizes the average abnormal volumes for NYSE winners and losers, using an 11-month lookback period.\n\n**Table 1: Average Abnormal Volume (%) for NYSE Winners and Losers (N=11)**\n| Group | Jan | Feb-Oct | Dec |\n| :--- | :--- | :--- | :--- |\n| Winners (`α+β`) | 0.48 | 0.32 | 0.12 |\n| Losers (`α`) | -0.37 | -0.37 | -0.16 |\n\n---\n\nBased on the model and data, which of the following conclusions are supported by the evidence?\n\nSelect all that apply.", "Options": {"A": "The difference in abnormal volume between winners and losers (the 'winner-loser gap') shrinks from 0.69% in the baseline Feb-Oct period to 0.28% in December.", "B": "The shrinkage of the winner-loser gap in December is driven by both an increase in the abnormal volume of losers and a decrease in the abnormal volume of winners, relative to the baseline period.", "C": "The winner-loser gap in January (0.85%) is wider than the baseline gap, consistent with the 'present value motive' where investors postpone realizing gains until the new tax year.", "D": "The estimated value of the `β` coefficient from Eq. (1) for the December period is 0.28%."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item tests the ability to calculate and interpret the key metric—the winner-loser volume gap—across different seasonal periods. It requires understanding how changes in this gap provide evidence for the paper's main hypotheses. Depth Strategy: Computational Judgment. Candidates must calculate the gap for each period and compare them to assess the statements. Distractor Logic: A requires two calculations: Feb-Oct (0.32 - (-0.37) = 0.69) and Dec (0.12 - (-0.16) = 0.28). B requires comparing the components: Loser volume rises (-0.37 to -0.16) and Winner volume falls (0.32 to 0.12). C requires calculating the Jan gap (0.48 - (-0.37) = 0.85) and comparing it to the baseline. D tests the interpretation of β as the difference, which is the same as the gap calculated in A for December. All options are correct and test different facets of the same core result.", "qid": "131", "question": "### Background\n\nThe study uses a one-way analysis of variance to measure the difference in abnormal trading volume between 'winner' and 'loser' stocks. This helps disentangle the baseline effects of non-tax motives from the seasonal effects of tax-motivated trading.\n\n### Data / Model Specification\n\nThe following cross-sectional regression is run for each month:\n  \nAV_{i} = \\alpha + \\beta DN_{i} + e_{i} \\quad \\text{(Eq. 1)}\n \nwhere `AV_i` is the abnormal volume (in percent per month), and `DN_i` is a binary variable equal to 1 for 'winners' and 0 for 'losers'. Thus, `α` represents the average abnormal volume for losers, and `α+β` represents the average for winners.\n\nThe table below summarizes the average abnormal volumes for NYSE winners and losers, using an 11-month lookback period.\n\n**Table 1: Average Abnormal Volume (%) for NYSE Winners and Losers (N=11)**\n| Group | Jan | Feb-Oct | Dec |\n| :--- | :--- | :--- | :--- |\n| Winners (`α+β`) | 0.48 | 0.32 | 0.12 |\n| Losers (`α`) | -0.37 | -0.37 | -0.16 |\n\n---\n\nBased on the model and data, which of the following conclusions are supported by the evidence?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 111, "Question": "### Background\n\n**Research Question.** This case investigates how empirical estimates of the correlation between currency strategies and a benchmark equity portfolio affect the theoretical bounds on return predictability, and how this framework can be used to confront the joint hypothesis problem in asset pricing.\n\n**Setting.** The analysis uses monthly data from 1994–2016 for forward contracts on 23 currencies against the US Dollar. The benchmark portfolio for the marginal investor (`$r_m$`) is the CRSP value-weighted portfolio of US stocks.\n\n**Variables and Parameters.**\n- `$\\phi_1$`: The unadjusted predictability bound (assuming `$\\rho^2=1$`).\n- `$\\phi_2$`: The correlation-adjusted predictability bound.\n- `$RRA_V$`: The upper bound on the marginal investor's relative risk aversion, set to 2.5 or 5.0.\n- `$r_m$`: Excess return on the CRSP value-weighted US stock portfolio.\n- `$\\rho^2(r_i^*, r_m)$`: The squared sample correlation between the optimal predictability-based strategy for currency `i` and the benchmark portfolio `$r_m$`.\n\n---\n\n### Data / Model Specification\n\nThe unadjusted and correlation-adjusted predictability bounds are given by:\n\n  \n\\phi_1 \\approx RRA_V^2 \\sigma^2(r_m) \\quad \\text{(Eq. (1))}\n \n\n  \n\\phi_2 \\equiv \\rho^2(r_i^*, r_m) RRA_V^2 \\sigma^2(r_m) \\quad \\text{(Eq. (2))}\n \n\n**Table 1** below reports the components for calculating these bounds for various currencies. All figures are in percentage points. The estimated predictability (`$\\widehat{R}^2$`) for the Mexican Peso (MEX) forward return is 2.12%.\n\n| Currency | Unadjusted Bound (`$\\phi_1$`) | `$\\rho^2$` (%) | Adjusted Bound (`$\\phi_2$`) |\n| :--- | :--- | :--- | :--- |\n| **RRA = 2.5** | | | |\n| BRA | 1.16 | 0.14 | *[missing]* |\n| CZECH | 1.15 | 0.12 | 0.0014 |\n| HUN | 1.22 | 0.36 | 0.0043 |\n| MEX | 1.25 | 15.92 | 0.1990 |\n| **RRA = 5.0** | | | |\n| BRA | 4.62 | 0.14 | 0.0063 |\n| MEX | 5.00 | 15.92 | 0.7959 |\n\n---\n\n### Question\n\nBased on the provided data and relationships, select all of the following statements that are correct.", "Options": {"A": "The missing correlation-adjusted bound (`$\\phi_2$`) for the Brazilian Real (BRA) at `$RRA_V=2.5$` is 0.1624%.", "B": "To rationalize the observed predictability (`$\\widehat{R}^2 = 2.12\\%` for MEX) without violating the correlation-adjusted bound (`$\\phi_2$`), the marginal investor's relative risk aversion (`$RRA_V$`) must be at least 8.1.", "C": "The market variance `$\\sigma^2(r_m)$` implied by the data for the Mexican Peso (MEX) with an assumed `$RRA_V$` of 2.5 is 0.002.", "D": "The low `$\\rho^2$` values for most currencies indicate that predictability-based currency strategies carry significant systematic equity market risk."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an Atomic Decomposition strategy to test multiple facets of the original problem: a preliminary calculation (market variance), the main 'apex' calculation (implied RRA), and a core conceptual interpretation. This preserves the depth of the original QA while creating a sharp, diagnostic multi-select item. Distractor C is a Conceptual Opposite of the paper's finding on idiosyncratic risk. Distractor D introduces a common decimal/unit error into a simple calculation, testing for careful application of the formula.", "qid": "111", "question": "### Background\n\n**Research Question.** This case investigates how empirical estimates of the correlation between currency strategies and a benchmark equity portfolio affect the theoretical bounds on return predictability, and how this framework can be used to confront the joint hypothesis problem in asset pricing.\n\n**Setting.** The analysis uses monthly data from 1994–2016 for forward contracts on 23 currencies against the US Dollar. The benchmark portfolio for the marginal investor (`$r_m$`) is the CRSP value-weighted portfolio of US stocks.\n\n**Variables and Parameters.**\n- `$\\phi_1$`: The unadjusted predictability bound (assuming `$\\rho^2=1$`).\n- `$\\phi_2$`: The correlation-adjusted predictability bound.\n- `$RRA_V$`: The upper bound on the marginal investor's relative risk aversion, set to 2.5 or 5.0.\n- `$r_m$`: Excess return on the CRSP value-weighted US stock portfolio.\n- `$\\rho^2(r_i^*, r_m)$`: The squared sample correlation between the optimal predictability-based strategy for currency `i` and the benchmark portfolio `$r_m$`.\n\n---\n\n### Data / Model Specification\n\nThe unadjusted and correlation-adjusted predictability bounds are given by:\n\n  \n\\phi_1 \\approx RRA_V^2 \\sigma^2(r_m) \\quad \\text{(Eq. (1))}\n \n\n  \n\\phi_2 \\equiv \\rho^2(r_i^*, r_m) RRA_V^2 \\sigma^2(r_m) \\quad \\text{(Eq. (2))}\n \n\n**Table 1** below reports the components for calculating these bounds for various currencies. All figures are in percentage points. The estimated predictability (`$\\widehat{R}^2$`) for the Mexican Peso (MEX) forward return is 2.12%.\n\n| Currency | Unadjusted Bound (`$\\phi_1$`) | `$\\rho^2$` (%) | Adjusted Bound (`$\\phi_2$`) |\n| :--- | :--- | :--- | :--- |\n| **RRA = 2.5** | | | |\n| BRA | 1.16 | 0.14 | *[missing]* |\n| CZECH | 1.15 | 0.12 | 0.0014 |\n| HUN | 1.22 | 0.36 | 0.0043 |\n| MEX | 1.25 | 15.92 | 0.1990 |\n| **RRA = 5.0** | | | |\n| BRA | 4.62 | 0.14 | 0.0063 |\n| MEX | 5.00 | 15.92 | 0.7959 |\n\n---\n\n### Question\n\nBased on the provided data and relationships, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 113, "Question": "### Background\n\n**Research Question.** This case examines how the paper's long-run risk model is applied to price real bonds.\n\n**Setting.** In a representative agent economy with Epstein-Zin preferences, real bonds are claims to risk-free future consumption. Bond prices are determined by the single, persistent state variable `x_t`, which represents the expected growth rate of durable consumption.\n\n**Variables and Parameters.**\n- `b_{n,t}`: Log price of an `n`-period real zero-coupon bond.\n- `x_t`: Long-run risk state variable, following `x_{t+1} = φx_t + σ_x ω_t ε_{x,t+1}`.\n- `B_{n,1}`: Sensitivity of the `n`-period log bond price to `x_t`.\n- `m_{t+1}`: Log SDF.\n- `ψ`, `α`, `φ`: Model parameters.\n\n---\n\n### Data / Model Specification\n\nThe log bond price is assumed to be linear in the state variable:\n  \nb_{n,t} \\approx B_{n,0} + B_{n,1} x_t \\quad \\text{(Eq. (1))}\n \nAsset prices are determined by the log-linearized Euler equation `0 ≈ E_t[m_{t+1} + r_{asset,t+1}]`. The key term from the expected log SDF is `E_t[m_{t+1}] = const + (1 - 1/ψ)α x_t`.\n\n---\n\n### Question\n\nBased on the model specification, select ALL statements that are correct regarding the pricing of real zero-coupon bonds.", "Options": {"A": "With `ψ=1.5`, real bond yields are counter-cyclical (low when `x_t` is high) because the agent's desire to smooth consumption between goods (goods substitution) dominates the desire to smooth consumption over time (intertemporal substitution).", "B": "The sensitivity of the `n`-period log bond price to the long-run risk state variable, `B_{n,1}`, is correctly given by `B_{n,1} = (1 - 1/ψ)α * (1 - φ^n) / (1 - φ)`.", "C": "The sensitivity of the `n`-period log bond price to the long-run risk state variable, `B_{n,1}`, is correctly given by `B_{n,1} = (1 - 1/ψ)α * φ^n`.", "D": "Real bonds are a hedge against long-run risk and therefore command a negative risk premium, as they pay off well when `x_t` is unexpectedly low."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item uses the Atomic Decomposition strategy to assess understanding of real bond pricing. Option A tests the correct derived formula for bond price sensitivity. Option B tests the core economic intuition behind counter-cyclical yields, which hinges on the relative strength of substitution effects. Distractor C represents a Formula Misuse error, confusing the recursive step with the full solution. Distractor D presents a Conceptual Opposite regarding the bond's risk characteristics and premium.", "qid": "113", "question": "### Background\n\n**Research Question.** This case examines how the paper's long-run risk model is applied to price real bonds.\n\n**Setting.** In a representative agent economy with Epstein-Zin preferences, real bonds are claims to risk-free future consumption. Bond prices are determined by the single, persistent state variable `x_t`, which represents the expected growth rate of durable consumption.\n\n**Variables and Parameters.**\n- `b_{n,t}`: Log price of an `n`-period real zero-coupon bond.\n- `x_t`: Long-run risk state variable, following `x_{t+1} = φx_t + σ_x ω_t ε_{x,t+1}`.\n- `B_{n,1}`: Sensitivity of the `n`-period log bond price to `x_t`.\n- `m_{t+1}`: Log SDF.\n- `ψ`, `α`, `φ`: Model parameters.\n\n---\n\n### Data / Model Specification\n\nThe log bond price is assumed to be linear in the state variable:\n  \nb_{n,t} \\approx B_{n,0} + B_{n,1} x_t \\quad \\text{(Eq. (1))}\n \nAsset prices are determined by the log-linearized Euler equation `0 ≈ E_t[m_{t+1} + r_{asset,t+1}]`. The key term from the expected log SDF is `E_t[m_{t+1}] = const + (1 - 1/ψ)α x_t`.\n\n---\n\n### Question\n\nBased on the model specification, select ALL statements that are correct regarding the pricing of real zero-coupon bonds.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 127, "Question": "### Background\n\n**Research Question.** This paper investigates how the market for retired CEOs as corporate directors has evolved, with a particular focus on the structural changes surrounding the Sarbanes-Oxley Act (SOX) of 2002. The core question is whether SOX, by increasing director liability and workload, altered both the supply of and demand for retired CEOs, and changed the criteria by which they are selected for board seats.\n\n**Setting and Sample.** The study analyzes three cohorts of retired CEOs from S&P 500 firms: 1989–1993, 1995–1999 (pre-SOX), and 2001–2005 (post-SOX). The analysis focuses on the number of outside directorships held two years after retirement. SOX is treated as a major regulatory shock that increased the costs (legal liability, workload) of serving as a director.\n\n**Variables and Parameters.**\n- `ROA`: Return on assets (%), a measure of accounting performance.\n- `ASR`: Abnormal stock return (%), the compound average annual return minus the CRSP value-weighted index.\n- `LnAsset`: Natural logarithm of total assets, a proxy for firm size.\n\n---\n\n### Data / Model Specification\n\n**Table 1: CEO and Firm Characteristics**\n\n| | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| Number of observations | 197 | 143 | 151 |\n| Firm size ($ billion in 2005) | 18.14 | 20.99 | 36.36 |\n| Abnormal stock return (ASR) (%)| -5.71 | 0.86 | 11.41 |\n\n**Table 2: Mean Number of Outside Directorships**\n\n| Sample period | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| Mean Outside Directorships | 1.73 | 2.38 | 1.70 |\n| *t-test vs prior period* | | *** | *** |\n\n**Table 3: Ordered Logit Regression for Number of Outside Directorships**\n\n| Variable | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| ASR | 1.040 | 1.383 | -1.968*** |\n| LnAsset | 0.672*** | 0.535*** | 0.195 |\n\n*Note: *** represents 1% significance level. A positive coefficient implies the variable increases the likelihood of holding more directorships.*\n\n---\n\n### Question\n\nBased on the paper's analysis of the post-SOX (2001–2005) period, which of the following statements accurately describe the hypothesized supply-side dynamics in the market for retired CEOs as directors? Select all that apply.", "Options": {"A": "The data show that post-SOX, both high ASR and large firm size (LnAsset) became strong negative predictors of directorships, confirming that the market began penalizing CEOs from successful firms.", "B": "The significant negative relationship between a CEO's prior abnormal stock return (ASR) and subsequent directorships is interpreted as evidence that wealthier CEOs (proxied by high ASR) were less willing to accept the increased liability risks associated with board seats.", "C": "The disappearance of the previously positive effect of firm size (LnAsset) on directorships suggests that a reduced supply from high-profile CEOs, who were also averse to the new risks, offset the reputational benefit of hiring them.", "D": "The negative coefficient on ASR indicates that boards post-SOX viewed high stock performance as a sign of excessive risk-taking, leading them to demand fewer of these CEOs."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize regression results with the paper's core supply-side hypothesis for the post-SOX director market. It uses an **Atomic Decomposition** strategy, breaking the complex argument into two distinct, correct propositions (the wealth/risk aversion channel via ASR and the profile/risk aversion channel via LnAsset). The depth strategy is **Reverse-Reasoning**, requiring the user to infer the economic mechanism from statistical results. Distractors are designed to test for a common demand-side misinterpretation (C) and a factual misreading of the regression results in Table 3 (D, as the LnAsset coefficient became insignificant, not negative).", "qid": "127", "question": "### Background\n\n**Research Question.** This paper investigates how the market for retired CEOs as corporate directors has evolved, with a particular focus on the structural changes surrounding the Sarbanes-Oxley Act (SOX) of 2002. The core question is whether SOX, by increasing director liability and workload, altered both the supply of and demand for retired CEOs, and changed the criteria by which they are selected for board seats.\n\n**Setting and Sample.** The study analyzes three cohorts of retired CEOs from S&P 500 firms: 1989–1993, 1995–1999 (pre-SOX), and 2001–2005 (post-SOX). The analysis focuses on the number of outside directorships held two years after retirement. SOX is treated as a major regulatory shock that increased the costs (legal liability, workload) of serving as a director.\n\n**Variables and Parameters.**\n- `ROA`: Return on assets (%), a measure of accounting performance.\n- `ASR`: Abnormal stock return (%), the compound average annual return minus the CRSP value-weighted index.\n- `LnAsset`: Natural logarithm of total assets, a proxy for firm size.\n\n---\n\n### Data / Model Specification\n\n**Table 1: CEO and Firm Characteristics**\n\n| | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| Number of observations | 197 | 143 | 151 |\n| Firm size ($ billion in 2005) | 18.14 | 20.99 | 36.36 |\n| Abnormal stock return (ASR) (%)| -5.71 | 0.86 | 11.41 |\n\n**Table 2: Mean Number of Outside Directorships**\n\n| Sample period | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| Mean Outside Directorships | 1.73 | 2.38 | 1.70 |\n| *t-test vs prior period* | | *** | *** |\n\n**Table 3: Ordered Logit Regression for Number of Outside Directorships**\n\n| Variable | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| ASR | 1.040 | 1.383 | -1.968*** |\n| LnAsset | 0.672*** | 0.535*** | 0.195 |\n\n*Note: *** represents 1% significance level. A positive coefficient implies the variable increases the likelihood of holding more directorships.*\n\n---\n\n### Question\n\nBased on the paper's analysis of the post-SOX (2001–2005) period, which of the following statements accurately describe the hypothesized supply-side dynamics in the market for retired CEOs as directors? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 128, "Question": "### Background\n\n**Research Question.** Why might a retired CEO remain on their own company's board, and how have the criteria for this decision evolved over time? This question explores the tension between two competing corporate governance theories: (1) the \"reward for excellence\" view, where firms retain high-performing leaders, and (2) the \"managerial entrenchment\" view, where long-tenured CEOs secure a board seat irrespective of performance.\n\n**Setting and Sample.** The analysis compares retired CEOs who serve on their own board two years after retirement (`Inside+2years = 1`) with those who do not (`Inside+2years = 0`) across pre- and post-SOX periods.\n\n**Variables and Parameters.**\n- `ASR`: Abnormal stock return (%), a proxy for recent performance.\n- `Tenure`: The CEO's tenure in years, a proxy for experience and potential entrenchment.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Mean Characteristics by Inside Directorship Status**\n\n| | Do Not Stay (Inside+2yrs=0) | Stay (Inside+2yrs=1) |\n|:---|---:|---:|\n| **1995-1999 (Pre-SOX)** | |\n| ASR (%) | -1.46 | 5.05 |\n| Tenure | 9.41 | 12.71 |\n| **2001-2005 (Post-SOX)** | |\n| ASR (%) | 8.59 | 15.81 |\n| Tenure | 8.28 | 14.12 |\n\n**Table 2: Logit Model for Probability of Serving on Own Board**\n\n| Variable | 1995-1999 | 2001-2005 |\n|:---|---:|---:|\n| ASR | 2.756** | 1.626* |\n| | (0.026) | (0.070) |\n| Tenure | 0.051** | 0.140*** |\n| | (0.039) | (0.000) |\n\n*P-values in parentheses. ***, **, * represent 1%, 5%, and 10% significance levels. A positive coefficient increases the probability of staying on the board.*\n\n---\n\n### Question\n\nThe paper's analysis of CEOs serving on their own boards post-retirement reveals a significant shift after the Sarbanes-Oxley Act (SOX). According to the provided data and the paper's interpretation, which of the following statements correctly characterize this post-SOX change? Select all that apply.", "Options": {"A": "Post-SOX, firms began to penalize high ASR, retaining CEOs with lower stock performance to signal a move away from a \"shareholder value\" focus.", "B": "The increased importance of tenure post-SOX provides conclusive evidence for the managerial entrenchment hypothesis, as longer-tenured CEOs were better able to secure board seats regardless of performance.", "C": "The coefficient on CEO tenure became larger and more statistically significant, suggesting that in the high-scrutiny post-SOX environment, boards placed a higher value on the deep, firm-specific institutional knowledge possessed by long-serving leaders.", "D": "The statistical significance of abnormal stock returns (ASR) as a predictor weakened, consistent with boards becoming more cautious about rewarding recent stock performance, which could have been driven by excessive risk-taking or accounting manipulation."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the understanding of a structural shift in corporate governance priorities post-SOX regarding inside directorships. It uses an **Atomic Decomposition** strategy to present the two key facets of the paper's explanation: the rising importance of firm-specific knowledge (Tenure) and the declining weight on recent stock performance (ASR). The depth strategy is **Reverse-Reasoning**. Distractors are designed to identify over-interpretation of the evidence (C, claiming conclusive proof of entrenchment) and a conceptual reversal of the findings (D, incorrectly suggesting high ASR was penalized, which contradicts both Table 1 and Table 2).", "qid": "128", "question": "### Background\n\n**Research Question.** Why might a retired CEO remain on their own company's board, and how have the criteria for this decision evolved over time? This question explores the tension between two competing corporate governance theories: (1) the \"reward for excellence\" view, where firms retain high-performing leaders, and (2) the \"managerial entrenchment\" view, where long-tenured CEOs secure a board seat irrespective of performance.\n\n**Setting and Sample.** The analysis compares retired CEOs who serve on their own board two years after retirement (`Inside+2years = 1`) with those who do not (`Inside+2years = 0`) across pre- and post-SOX periods.\n\n**Variables and Parameters.**\n- `ASR`: Abnormal stock return (%), a proxy for recent performance.\n- `Tenure`: The CEO's tenure in years, a proxy for experience and potential entrenchment.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Mean Characteristics by Inside Directorship Status**\n\n| | Do Not Stay (Inside+2yrs=0) | Stay (Inside+2yrs=1) |\n|:---|---:|---:|\n| **1995-1999 (Pre-SOX)** | |\n| ASR (%) | -1.46 | 5.05 |\n| Tenure | 9.41 | 12.71 |\n| **2001-2005 (Post-SOX)** | |\n| ASR (%) | 8.59 | 15.81 |\n| Tenure | 8.28 | 14.12 |\n\n**Table 2: Logit Model for Probability of Serving on Own Board**\n\n| Variable | 1995-1999 | 2001-2005 |\n|:---|---:|---:|\n| ASR | 2.756** | 1.626* |\n| | (0.026) | (0.070) |\n| Tenure | 0.051** | 0.140*** |\n| | (0.039) | (0.000) |\n\n*P-values in parentheses. ***, **, * represent 1%, 5%, and 10% significance levels. A positive coefficient increases the probability of staying on the board.*\n\n---\n\n### Question\n\nThe paper's analysis of CEOs serving on their own boards post-retirement reveals a significant shift after the Sarbanes-Oxley Act (SOX). According to the provided data and the paper's interpretation, which of the following statements correctly characterize this post-SOX change? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 25, "Question": "**Research Question.** Does the idiosyncratic component of stock order flow, orthogonal to major macroeconomic shocks, have predictive power for future market returns?\n\n**Setting / Data-Generating Environment.** A structural VAR model isolates an idiosyncratic shock to the buy/sell ratio (`ε_4t`). This shock is, by construction, orthogonal to oil supply, aggregate demand, and oil-specific demand shocks. The paper finds that this shock negatively predicts future real stock returns.\n\n**Finding:** An unexpected positive shock to the stock order flow imbalance (`ε_{4t} > 0`) causes a sustained decrease in U.S. real stock returns over the subsequent months.\n\nBased on the provided context and standard market microstructure theory, select all statements that correctly characterize the idiosyncratic buy/sell ratio shock (`ε_{4t}`) and the price pressure mechanism explaining its negative predictability for future returns.", "Options": {"A": "The negative predictability arises because `ε_{4t} > 0` reveals negative private information about future firm fundamentals that was not captured by the macroeconomic shocks.", "B": "The shock `ε_{4t}` likely captures non-fundamental trading pressure, such as from large institutional portfolio rebalancing or sentiment-driven trades.", "C": "The mechanism implies that a positive shock (`ε_{4t} > 0`) pushes the market price temporarily *above* its fundamental value, leading to a subsequent correction.", "D": "A market with greater depth (lower price impact `λ`) would exhibit stronger negative predictability following a shock of a given size."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: Tests the conceptual understanding of idiosyncratic order flow shocks and the temporary price pressure mechanism. Strategy: Atomic Decomposition, breaking the concept into two testable parts: the nature of the shock (A) and the direction of the price impact (B). Distractor Logic: (C) presents a plausible but incorrect alternative theory (information-based trading). (D) presents a conceptual opposite regarding the role of market depth, a common point of confusion.", "qid": "25", "question": "**Research Question.** Does the idiosyncratic component of stock order flow, orthogonal to major macroeconomic shocks, have predictive power for future market returns?\n\n**Setting / Data-Generating Environment.** A structural VAR model isolates an idiosyncratic shock to the buy/sell ratio (`ε_4t`). This shock is, by construction, orthogonal to oil supply, aggregate demand, and oil-specific demand shocks. The paper finds that this shock negatively predicts future real stock returns.\n\n**Finding:** An unexpected positive shock to the stock order flow imbalance (`ε_{4t} > 0`) causes a sustained decrease in U.S. real stock returns over the subsequent months.\n\nBased on the provided context and standard market microstructure theory, select all statements that correctly characterize the idiosyncratic buy/sell ratio shock (`ε_{4t}`) and the price pressure mechanism explaining its negative predictability for future returns.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 36, "Question": "### Background\n\nAn MLP neural network is trained to forecast one-day-ahead returns. The training process involves minimizing an error function, but the stopping criterion is based on trading performance to prevent overfitting.\n\n### Data / Model Specification\n\nThe MLP is trained by adjusting its weights to minimize the Mean Squared Error (MSE) on the training data:\n\n  \nE = \\frac{1}{T}\\sum_{t=1}^{T}(y_{t}-\\tilde{y}_{t})^{2} \\quad \\text{(Eq. (1))}\n \n\nHowever, the paper states: \"Our networks... will stop training when the profit of our forecasts in the test sub-period is maximized.\" A simple trading rule generates profit `π_t = sign(ỹ_t) ⋅ y_t`.\n\n---\n\nWhich of the following statements correctly identify potential issues or valid characteristics related to this training and evaluation methodology?\n", "Options": {"A": "The profit function `π_t` is generally non-differentiable with respect to the network's output `ỹ_t`, making it unsuitable as a direct objective function for gradient-based training methods like backpropagation.", "B": "The MSE objective function penalizes large forecast errors more heavily than the profit function, which only depends on the forecast's sign.", "C": "Using test set profit as an early stopping criterion helps mitigate overfitting by selecting a model based on its generalization performance on an economic, rather than purely statistical, metric.", "D": "The set of network weights that minimizes test set MSE is guaranteed to be the same set of weights that maximizes test set profit."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the conflict between statistical and economic objective functions in machine learning for finance, including concepts of optimization and overfitting. Strategy: Reverse-Reasoning (identifying valid statements about the methodology). Distractor Logic: Option B is correct because the `sign()` function's derivative is zero almost everywhere, providing no gradient for optimization. Option C correctly contrasts the symmetric, magnitude-sensitive nature of MSE with the sign-dependent profit function. Option D correctly identifies the purpose of the early stopping rule. Option A is a 'Conceptual Opposite' distractor; it is the core mismatch that the question explores. Minimizing large errors (MSE) is not the same as maximizing correct directional bets (profit), so the optimal weights are not guaranteed to be the same.", "qid": "36", "question": "### Background\n\nAn MLP neural network is trained to forecast one-day-ahead returns. The training process involves minimizing an error function, but the stopping criterion is based on trading performance to prevent overfitting.\n\n### Data / Model Specification\n\nThe MLP is trained by adjusting its weights to minimize the Mean Squared Error (MSE) on the training data:\n\n  \nE = \\frac{1}{T}\\sum_{t=1}^{T}(y_{t}-\\tilde{y}_{t})^{2} \\quad \\text{(Eq. (1))}\n \n\nHowever, the paper states: \"Our networks... will stop training when the profit of our forecasts in the test sub-period is maximized.\" A simple trading rule generates profit `π_t = sign(ỹ_t) ⋅ y_t`.\n\n---\n\nWhich of the following statements correctly identify potential issues or valid characteristics related to this training and evaluation methodology?\n", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 148, "Question": "### Background\n\nA model of futures pricing is developed where speculator participation is limited by a fixed transaction cost, making the number of traders endogenous. Speculators enter the market until the marginal speculator is indifferent between paying the cost to trade versus not trading at all. This equilibrium condition determines the futures risk premium.\n\n### Data / Model Specification\n\nThe equilibrium percentage futures risk premium (`π`) is determined by the indifference condition of the marginal speculator, leading to the following pricing relation from Proposition 3:\n\n  \n\\pi = \\beta_{\\pi m} \\bar{R}_m \\pm \\sigma_{\\pi} \\sqrt{2\\alpha t(1-\\rho^2)} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `π`: Percentage futures risk premium.\n- `t`: Fixed transaction/setup cost for trading futures.\n- `α`: Coefficient of absolute risk aversion.\n- `β_πm`: Market beta of the percentage futures return.\n- `R̄_m`: Expected net return on the stock market.\n- `σ_π`: Standard deviation of the percentage futures return.\n- `ρ`: Correlation between the futures return `π̃` and the market return `R_m`.\n\n### Question\n\nBased on the equilibrium pricing model in `Eq. (1)`, which of the following statements are valid interpretations or consequences of the model's structure?\n\nSelect all that apply.", "Options": {"A": "The model predicts that the risk premium is determined by the marginal speculator's indifference condition, which makes the premium sensitive to the parameters of that speculator (`α`, `t`) and the statistical properties of the futures contract itself (`β_πm`, `σ_π`, `ρ`).", "B": "The component of the risk premium related to hedging pressure is proportional to the residual variance of the futures return, `σ_π^2(1-ρ^2)`.", "C": "The premium for bearing residual risk is independent of the covariance between the futures return and producers' nonmarketable revenue risks.", "D": "If the setup cost `t` were to double, the absolute magnitude of the hedging-related component of the premium would also double."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to correctly interpret the components of the central pricing equation (Proposition 3) and understand its key theoretical implications. Depth Strategy: Reverse-Reasoning. The candidate is given the final equation and must identify the valid conceptual underpinnings and consequences that led to it. Distractor Logic: Option A is a Formula Misuse error, incorrectly stating proportionality to residual variance instead of residual standard deviation, a key distinction made in the paper. Option C is a Numerical Distractor (Formula Misuse), as the premium is proportional to the square root of `t`, not `t` itself, so doubling `t` increases the premium by a factor of √2, not 2. Options B and D are correct and capture two of the most surprising and important results of the model: the irrelevance of nonmarketable risk covariance for the premium (a departure from Mayers CAPM) and the central role of the marginal speculator's parameters.", "qid": "148", "question": "### Background\n\nA model of futures pricing is developed where speculator participation is limited by a fixed transaction cost, making the number of traders endogenous. Speculators enter the market until the marginal speculator is indifferent between paying the cost to trade versus not trading at all. This equilibrium condition determines the futures risk premium.\n\n### Data / Model Specification\n\nThe equilibrium percentage futures risk premium (`π`) is determined by the indifference condition of the marginal speculator, leading to the following pricing relation from Proposition 3:\n\n  \n\\pi = \\beta_{\\pi m} \\bar{R}_m \\pm \\sigma_{\\pi} \\sqrt{2\\alpha t(1-\\rho^2)} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `π`: Percentage futures risk premium.\n- `t`: Fixed transaction/setup cost for trading futures.\n- `α`: Coefficient of absolute risk aversion.\n- `β_πm`: Market beta of the percentage futures return.\n- `R̄_m`: Expected net return on the stock market.\n- `σ_π`: Standard deviation of the percentage futures return.\n- `ρ`: Correlation between the futures return `π̃` and the market return `R_m`.\n\n### Question\n\nBased on the equilibrium pricing model in `Eq. (1)`, which of the following statements are valid interpretations or consequences of the model's structure?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 67, "Question": "### Background\nAn economic model analyzes an innovative sector where managers can exert either high effort ($e_i=1$) or low effort ($e_i=\\underline{e}$) in risk prevention. Managers are heterogeneous in their non-monetary cost of effort, $C_i$. The market's perceived probability of a negative shock is denoted by $\\theta_t$. As long as no shocks occur, confidence increases, and $\\theta_t$ decreases.\n\n### Data / Model Specification\nIn a competitive market with **observable effort**, managers capture the full surplus. The expected surplus from a high-effort manager is:\n\n$$ \\alpha_{t}=(1-\\mu\\theta_{t})Y-(1+r) \\quad \\text{(Eq. 1)} $$\n\nThe expected surplus from a low-effort manager is $\\alpha_t - \\theta_t \\Delta Y$. A manager $i$ will choose the action (high effort, low effort, or stay out) that maximizes their personal rent. This leads to a critical confidence threshold, $\\hat{\\theta}$, that separates two distinct market regimes.\n\n- When confidence is low ($\\theta_t \\ge \\hat{\\theta}$), only high-effort managers enter the market.\n- When confidence is high ($\\theta_t < \\hat{\\theta}$), it becomes profitable for some managers to enter and exert low effort.\n\n### Question\nAssume the innovative sector is in the **high-confidence regime** (i.e., $\\theta_t < \\hat{\\theta}$). According to the model, which of the following statements accurately describe the state of the market and its dynamics as confidence continues to grow (i.e., as $\\theta_t$ continues to decrease)? Select all that apply.", "Options": {"A": "The proportion of managers exerting high effort, relative to the total number of managers in the sector, increases.", "B": "The total potential loss, in the event of a negative shock, increases.", "C": "The total size of the innovative sector increases.", "D": "The cross-sectional variance of default probabilities among managers in the sector is zero."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the dynamics within the high-confidence regime ($\\theta_t < \\hat{\\theta}$) of the observable-effort model. It requires applying knowledge of how market composition, size, and systemic risk evolve as confidence grows.\nDepth Strategy: Scenario Application. The question sets a specific scenario (high-confidence regime with growing confidence) and asks for all correct consequences.\nDistractor Logic:\n- A (Correct): As $\\theta_t$ falls, $\\gamma_t$ (the threshold for low-effort entry) increases, expanding the total number of managers, $F(\\gamma_t)$.\n- B (Conceptual Opposite): As $\\theta_t$ falls, $\\beta_t$ (the threshold for high-effort) decreases while $\\gamma_t$ increases. The fraction of high-effort managers, $F(\\beta_t)/F(\\gamma_t)$, therefore *decreases*, representing a decline in standards.\n- C (Correct): The total potential loss increases due to both a scale effect (more projects in total) and a composition effect (a larger fraction of higher-risk, low-effort projects).\n- D (Almost Right): The cross-sectional variance of default probabilities is non-zero in this regime because there are two distinct groups of managers with different failure rates ($\\\theta_t \\mu$ and $\\\theta_t (\\mu+\\Delta)$). The variance is zero only in the low-confidence regime ($\\theta_t \\ge \\hat{\\theta}$).", "qid": "67", "question": "### Background\nAn economic model analyzes an innovative sector where managers can exert either high effort ($e_i=1$) or low effort ($e_i=\\underline{e}$) in risk prevention. Managers are heterogeneous in their non-monetary cost of effort, $C_i$. The market's perceived probability of a negative shock is denoted by $\\theta_t$. As long as no shocks occur, confidence increases, and $\\theta_t$ decreases.\n\n### Data / Model Specification\nIn a competitive market with **observable effort**, managers capture the full surplus. The expected surplus from a high-effort manager is:\n\n$$ \\alpha_{t}=(1-\\mu\\theta_{t})Y-(1+r) \\quad \\text{(Eq. 1)} $$\n\nThe expected surplus from a low-effort manager is $\\alpha_t - \\theta_t \\Delta Y$. A manager $i$ will choose the action (high effort, low effort, or stay out) that maximizes their personal rent. This leads to a critical confidence threshold, $\\hat{\\theta}$, that separates two distinct market regimes.\n\n- When confidence is low ($\\theta_t \\ge \\hat{\\theta}$), only high-effort managers enter the market.\n- When confidence is high ($\\theta_t < \\hat{\\theta}$), it becomes profitable for some managers to enter and exert low effort.\n\n### Question\nAssume the innovative sector is in the **high-confidence regime** (i.e., $\\theta_t < \\hat{\\theta}$). According to the model, which of the following statements accurately describe the state of the market and its dynamics as confidence continues to grow (i.e., as $\\theta_t$ continues to decrease)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 112, "Question": "### Background\n\n**Research Question.** This case examines how the paper's long-run risk model is applied to price the aggregate stock market.\n\n**Setting.** In a representative agent economy with Epstein-Zin preferences, the aggregate stock market is a claim to a dividend stream. The stock price is determined by the single, persistent state variable `x_t`, which represents the expected growth rate of durable consumption.\n\n**Variables and Parameters.**\n- `z_t = log(P_t/Y_t)`: Log price-dividend ratio.\n- `x_t`: Long-run risk state variable, following `x_{t+1} = φx_t + σ_x ω_t ε_{x,t+1}`.\n- `A_1`: Sensitivity of the log stock price-dividend ratio to `x_t`.\n- `r_{t+1}`: Log stock return.\n- `m_{t+1}`: Log SDF.\n- `Δy_{t+1}`: Log dividend growth.\n- `λ`: Exposure of dividend growth to `x_t`.\n- `ψ`, `α`, `φ`, `ζ`, `κ_1`: Model parameters.\n\n---\n\n### Data / Model Specification\n\nThe log price-dividend ratio is assumed to be linear in the state variable:\n  \nz_t \\approx A_0 + A_1 x_t \\quad \\text{(Eq. (1))}\n \nAsset prices are determined by the log-linearized Euler equation `0 ≈ E_t[m_{t+1} + r_{asset,t+1}]`. The key term from the expected log SDF is `E_t[m_{t+1}] = const + (1 - 1/ψ)α x_t`. The expected log dividend growth is `E_t[Δy_{t+1}] = μ_y + λx_t`.\n\nThe equity risk premium is given by:\n  \nE_t[r_{t+1} - r_{f,t}] + \\text{Jensen's term} \\approx \\beta_x \\pi_x \\sigma_x^2 \\omega_t^2 = \\beta_x \\pi_x \\sigma_x^2 (1 - \\zeta x_t) \\quad \\text{(Eq. (2))}\n \nwhere `β_x = κ_1 A_1` is the stock's exposure to long-run risk and `π_x` is the market price of that risk.\n\n---\n\n### Question\n\nBased on the model specification, select ALL statements that are correct regarding the pricing of the aggregate stock market.", "Options": {"A": "The sensitivity of the log price-dividend ratio to the long-run risk state variable, `A_1`, is correctly given by `A_1 = (λ + (1 - 1/ψ)α) / (1 - κ_1 φ)`.", "B": "The equity risk premium is pro-cyclical because in good times (high `x_t`), the conditional volatility `ω_t` is high, increasing the compensation required for bearing risk.", "C": "The sensitivity of the log price-dividend ratio to the long-run risk state variable, `A_1`, is correctly given by `A_1 = λ / (1 - κ_1 φ)`.", "D": "The stock's exposure to long-run risk, `β_x = κ_1 A_1`, is large primarily because the high persistence of `x_t` (`φ` ≈ 1) makes the denominator `(1 - κ_1 φ)` very small, amplifying the sensitivity of the stock price to growth news."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item uses the Atomic Decomposition strategy to test both the derivation of the stock valuation sensitivity (`A_1`) and the economic interpretation of the resulting risk premium. Option A tests the precise formula. Option B tests the key insight about persistence amplification. Distractor C represents a common Step-Omission Error of ignoring the wealth channel's effect on the SDF. Distractor D presents a Conceptual Opposite, incorrectly linking high `x_t` to high volatility and a pro-cyclical premium.", "qid": "112", "question": "### Background\n\n**Research Question.** This case examines how the paper's long-run risk model is applied to price the aggregate stock market.\n\n**Setting.** In a representative agent economy with Epstein-Zin preferences, the aggregate stock market is a claim to a dividend stream. The stock price is determined by the single, persistent state variable `x_t`, which represents the expected growth rate of durable consumption.\n\n**Variables and Parameters.**\n- `z_t = log(P_t/Y_t)`: Log price-dividend ratio.\n- `x_t`: Long-run risk state variable, following `x_{t+1} = φx_t + σ_x ω_t ε_{x,t+1}`.\n- `A_1`: Sensitivity of the log stock price-dividend ratio to `x_t`.\n- `r_{t+1}`: Log stock return.\n- `m_{t+1}`: Log SDF.\n- `Δy_{t+1}`: Log dividend growth.\n- `λ`: Exposure of dividend growth to `x_t`.\n- `ψ`, `α`, `φ`, `ζ`, `κ_1`: Model parameters.\n\n---\n\n### Data / Model Specification\n\nThe log price-dividend ratio is assumed to be linear in the state variable:\n  \nz_t \\approx A_0 + A_1 x_t \\quad \\text{(Eq. (1))}\n \nAsset prices are determined by the log-linearized Euler equation `0 ≈ E_t[m_{t+1} + r_{asset,t+1}]`. The key term from the expected log SDF is `E_t[m_{t+1}] = const + (1 - 1/ψ)α x_t`. The expected log dividend growth is `E_t[Δy_{t+1}] = μ_y + λx_t`.\n\nThe equity risk premium is given by:\n  \nE_t[r_{t+1} - r_{f,t}] + \\text{Jensen's term} \\approx \\beta_x \\pi_x \\sigma_x^2 \\omega_t^2 = \\beta_x \\pi_x \\sigma_x^2 (1 - \\zeta x_t) \\quad \\text{(Eq. (2))}\n \nwhere `β_x = κ_1 A_1` is the stock's exposure to long-run risk and `π_x` is the market price of that risk.\n\n---\n\n### Question\n\nBased on the model specification, select ALL statements that are correct regarding the pricing of the aggregate stock market.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 51, "Question": "### Background\n\nAn analyst is investigating whether the interaction between product life cycle (PLC) and the book-to-market (BM) effect is driven by investor inattention. The hypothesis is that the effect should be concentrated in firms that are less scrutinized by the market. To test this, the analyst splits a sample of firms into 'Low Attention' and 'High Attention' groups and runs Fama-MacBeth regressions separately on each subsample.\n\n### Data / Model Specification\n\nThe Fama-MacBeth regression model is estimated for each group:\n\n  \nReturn_{i,t+1} = \\gamma_{0,t} + ... + \\gamma_{BM \\times PLC, t} (BM_{i,t} \\times PLC_{i,t}) + ... + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n \n\nTable 1 presents the key coefficient estimates for the interaction term in each subsample.\n\n**Table 1: Fama-MacBeth Results for `BM × PLC` in Attention Subsamples**\n\n| Subsample | Coefficient (`\\hat{\\gamma}_{BM × PLC}`) | *t-statistic* |\n| :--- | :---: | :---: |\n| Low Investor Attention | 0.02 | (2.75) |\n| High Investor Attention | 0.01 | (1.40) |\n| **Wald Test for Difference** | *p-value* | *(<0.01)* |\n\n*Source: Abridged from Table 5 of the paper. The Wald test confirms the coefficients are statistically different.* \n\n---\n\nBased on the provided results, which of the following conclusions are supported by the evidence? Select all that apply.", "Options": {"A": "The Wald test indicates that the coefficients for the interaction term in the low and high attention groups are statistically indistinguishable.", "B": "The results are consistent with the hypothesis that investor inattention is a key channel through which PLC moderates the book-to-market effect.", "C": "For highly scrutinized firms, a longer product life cycle significantly strengthens the return predictability of the book-to-market ratio.", "D": "The amplifying effect of PLC on the BM-return relationship is statistically significant only for firms with low investor attention."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret split-sample regression results and draw correct inferences about a proposed mechanism (investor inattention). Depth Strategy: Scenario Application. The user must apply the general findings to the specific numerical results in the table. Distractor Logic: C is a conceptual opposite, incorrectly interpreting the insignificant result for the high-attention group. D is a direct contradiction of the Wald test result, testing careful reading of the table's footnote.", "qid": "51", "question": "### Background\n\nAn analyst is investigating whether the interaction between product life cycle (PLC) and the book-to-market (BM) effect is driven by investor inattention. The hypothesis is that the effect should be concentrated in firms that are less scrutinized by the market. To test this, the analyst splits a sample of firms into 'Low Attention' and 'High Attention' groups and runs Fama-MacBeth regressions separately on each subsample.\n\n### Data / Model Specification\n\nThe Fama-MacBeth regression model is estimated for each group:\n\n  \nReturn_{i,t+1} = \\gamma_{0,t} + ... + \\gamma_{BM \\times PLC, t} (BM_{i,t} \\times PLC_{i,t}) + ... + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n \n\nTable 1 presents the key coefficient estimates for the interaction term in each subsample.\n\n**Table 1: Fama-MacBeth Results for `BM × PLC` in Attention Subsamples**\n\n| Subsample | Coefficient (`\\hat{\\gamma}_{BM × PLC}`) | *t-statistic* |\n| :--- | :---: | :---: |\n| Low Investor Attention | 0.02 | (2.75) |\n| High Investor Attention | 0.01 | (1.40) |\n| **Wald Test for Difference** | *p-value* | *(<0.01)* |\n\n*Source: Abridged from Table 5 of the paper. The Wald test confirms the coefficients are statistically different.* \n\n---\n\nBased on the provided results, which of the following conclusions are supported by the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 72, "Question": "### Background\n\n**Research Question.** How does the structure of a Bonus-Malus System's (BMS) transition rules affect its rate of convergence to a steady state?\n\n**Setting / Data-Generating Environment.** The rate of convergence of a Markov chain, which models a BMS, is governed by its second largest eigenvalue modulus (SLEM). A smaller SLEM implies faster convergence. A SLEM of 0 implies convergence in a single step.\n\n---\n\n### Data / Model Specification\n\nConsider a simple \"reset\" BMS with `s` classes. The transition rules are independent of the current class:\n- **0 claims:** The driver moves to the best class (Class 1).\n- **1 or more claims:** The driver moves to the worst class (Class `s`).\n\nThe probability of having 0 claims is `p_0(λ)`, and the probability of having 1 or more claims is `1 - p_0(λ)`.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the mathematical properties of this \"reset\" BMS.", "Options": {"A": "The trace of the transition matrix `M(λ)` is equal to `p_0(λ)`.", "B": "This system design is an example of a slowly converging BMS whose steady-state properties are of little practical relevance.", "C": "The `s x s` transition matrix `M(λ)` for this system has `s-1` eigenvalues equal to 0.", "D": "The system reaches its steady-state distribution in a single transition period."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the understanding of the mathematical properties of a stylized Markov chain, specifically connecting the structure of the transition matrix to its eigenvalues and convergence rate. It uses an Atomic Decomposition strategy, testing key results from the original derivation. Distractor B is a Formula Misuse error, as the trace is 1. Distractor D is a Conceptual Opposite, as the system exhibits the fastest possible convergence.", "qid": "72", "question": "### Background\n\n**Research Question.** How does the structure of a Bonus-Malus System's (BMS) transition rules affect its rate of convergence to a steady state?\n\n**Setting / Data-Generating Environment.** The rate of convergence of a Markov chain, which models a BMS, is governed by its second largest eigenvalue modulus (SLEM). A smaller SLEM implies faster convergence. A SLEM of 0 implies convergence in a single step.\n\n---\n\n### Data / Model Specification\n\nConsider a simple \"reset\" BMS with `s` classes. The transition rules are independent of the current class:\n- **0 claims:** The driver moves to the best class (Class 1).\n- **1 or more claims:** The driver moves to the worst class (Class `s`).\n\nThe probability of having 0 claims is `p_0(λ)`, and the probability of having 1 or more claims is `1 - p_0(λ)`.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the mathematical properties of this \"reset\" BMS.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 97, "Question": "### Background\n\n**Research Question.** Is the non-linear relationship between foreign bank entry and a country's gross domestic investment robust to the exclusion of potentially influential geographic regions?\n\n**Setting & Data.** A cross-sectional regression analysis for 54 countries. The analysis performs a robustness check by re-estimating the main quadratic model on a subsample that excludes all 13 Latin American countries.\n\n### Data / Model Specification\n\nThe non-linear estimating equation is given by:\n  \nI = A + \\dots + \\alpha_{3}FBSASS + \\alpha_{6}FBSASS^{2} + \\varepsilon \\quad \\text{(Eq. (1))}\n \nwhere `I` is the investment-to-GDP ratio and `FBSASS` is the share of foreign bank assets.\n\n**Table 1. Estimation Results for Non-Linear Model**\n\n| | (1) Baseline | (2) Excluding Latin America |\n|:---|:---:|:---:|\n| **Variable** | **Coefficient (t-stat)** | **Coefficient (t-stat)** |\n| `FBSASS` (α₃) | -14.24 (-2.38) | -19.61 (-3.25) |\n| `FBSASS²` (α₆) | 22.84 (3.46) | 27.89 (4.24) |\n| | | |\n| **Sample** | Full (N=54) | N=41 |\n\n*Notes: The baseline results are from Table 4 of the paper. The results in column (2) are from a robustness test in Table 5.*\n\n### Question\n\nBy comparing the estimation results from the baseline model (column 1) with the model excluding Latin American countries (column 2), select all of the following conclusions that are supported by the data.", "Options": {"A": "The analysis suggests the U-shaped relationship is not robust, as the coefficients and the turning point change substantially after excluding the Latin American subsample.", "B": "The results imply that the Latin American countries in the sample were driving the U-shaped finding; without them, the relationship becomes insignificant.", "C": "The exclusion of Latin American countries reveals a steeper estimated U-curve, as the absolute magnitudes of both the linear and quadratic coefficients increase.", "D": "After excluding Latin American countries, the estimated turning point for `FBSASS` increases from approximately 0.31 to 0.35."}, "Answer": ["C", "D"], "pi_justification": "REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to evaluate the robustness of an empirical finding by comparing results across different samples. It uses a Scenario Application strategy. The correct options (A, B) require both calculation and interpretation of the change in the model's shape, representing an atomic decomposition of the robustness check's conclusion. Distractor C is a Conceptual Opposite, as a stable sign and significance pattern despite coefficient changes is a sign of robustness. Distractor D is also a Conceptual Opposite, as the t-statistics actually increase, indicating the relationship becomes even stronger.", "qid": "97", "question": "### Background\n\n**Research Question.** Is the non-linear relationship between foreign bank entry and a country's gross domestic investment robust to the exclusion of potentially influential geographic regions?\n\n**Setting & Data.** A cross-sectional regression analysis for 54 countries. The analysis performs a robustness check by re-estimating the main quadratic model on a subsample that excludes all 13 Latin American countries.\n\n### Data / Model Specification\n\nThe non-linear estimating equation is given by:\n  \nI = A + \\dots + \\alpha_{3}FBSASS + \\alpha_{6}FBSASS^{2} + \\varepsilon \\quad \\text{(Eq. (1))}\n \nwhere `I` is the investment-to-GDP ratio and `FBSASS` is the share of foreign bank assets.\n\n**Table 1. Estimation Results for Non-Linear Model**\n\n| | (1) Baseline | (2) Excluding Latin America |\n|:---|:---:|:---:|\n| **Variable** | **Coefficient (t-stat)** | **Coefficient (t-stat)** |\n| `FBSASS` (α₃) | -14.24 (-2.38) | -19.61 (-3.25) |\n| `FBSASS²` (α₆) | 22.84 (3.46) | 27.89 (4.24) |\n| | | |\n| **Sample** | Full (N=54) | N=41 |\n\n*Notes: The baseline results are from Table 4 of the paper. The results in column (2) are from a robustness test in Table 5.*\n\n### Question\n\nBy comparing the estimation results from the baseline model (column 1) with the model excluding Latin American countries (column 2), select all of the following conclusions that are supported by the data.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 150, "Question": "### Background\n\nA risk-averse agent with mean-variance utility must choose their optimal holdings in a futures contract (`ξ`) and the stock market (`S`). The agent may also have a nonmarketable, risky revenue endowment (`r`).\n\n### Data / Model Specification\n\nThe agent's objective is to maximize:\n\n  \nU = E[C] - \\frac{\\alpha}{2} \\mathrm{var}(C) \\quad \\text{(Eq. (1))}\n \n\nFor an agent who trades futures, consumption is given by:\n\n  \nC = W - t + r + \\xi \\tilde{\\Pi} + S R_m \\quad \\text{(Eq. (2))}\n \n\nThis optimization leads to the following optimal futures position (from Proposition 1):\n\n  \n\\xi = \\frac{\\Pi/\\alpha - \\mathrm{cov}(\\tilde{\\Pi}, r + S R_m)}{\\mathrm{var}(\\tilde{\\Pi})} \\quad \\text{(Eq. (3))}\n \n\nWhere `Π` is the risk premium `E[Π̃]`, `α` is risk aversion, and `S` is the simultaneously chosen stock position.\n\n### Question\n\nBased on the structure of the optimal futures position in `Eq. (3)`, which of the following statements are valid interpretations of an agent's motives for holding a futures position?\n\nSelect all that apply.", "Options": {"A": "The term `Π/α` represents the speculative component of demand; its size increases with the expected profit (`Π`) and the agent's risk aversion (`α`).", "B": "The term `-cov(Π̃, S R_m)` represents a cross-hedge against the risk of the agent's stock market portfolio.", "C": "The term `-cov(Π̃, r)` represents a direct hedge against the agent's nonmarketable background risk.", "D": "If the futures payoff `Π̃` is uncorrelated with both the stock market and the agent's nonmarketable revenue `r`, the agent will hold a zero futures position regardless of the premium `Π`."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to decompose and interpret the components of an agent's optimal demand function (Proposition 1), identifying the distinct economic motives for trading. Depth Strategy: Reverse-Reasoning. The candidate is given the optimal demand equation and must identify the correct conceptual interpretation of its constituent parts. Distractor Logic: Option A is a Conceptual Distractor (Sign Error); speculative demand increases with risk *tolerance* (`1/α`), not risk aversion (`α`). Option D is a Conceptual Distractor (Step-Omission Error); if both covariances are zero, the demand simplifies to `ξ = Π / (α * var(Π̃))`, which is the pure speculative demand. This position is only zero if the premium `Π` is zero. Options B and C are correct and accurately describe the two distinct hedging motives embedded in the demand function: hedging background risk (`r`) and cross-hedging the traded asset portfolio (`S R_m`).", "qid": "150", "question": "### Background\n\nA risk-averse agent with mean-variance utility must choose their optimal holdings in a futures contract (`ξ`) and the stock market (`S`). The agent may also have a nonmarketable, risky revenue endowment (`r`).\n\n### Data / Model Specification\n\nThe agent's objective is to maximize:\n\n  \nU = E[C] - \\frac{\\alpha}{2} \\mathrm{var}(C) \\quad \\text{(Eq. (1))}\n \n\nFor an agent who trades futures, consumption is given by:\n\n  \nC = W - t + r + \\xi \\tilde{\\Pi} + S R_m \\quad \\text{(Eq. (2))}\n \n\nThis optimization leads to the following optimal futures position (from Proposition 1):\n\n  \n\\xi = \\frac{\\Pi/\\alpha - \\mathrm{cov}(\\tilde{\\Pi}, r + S R_m)}{\\mathrm{var}(\\tilde{\\Pi})} \\quad \\text{(Eq. (3))}\n \n\nWhere `Π` is the risk premium `E[Π̃]`, `α` is risk aversion, and `S` is the simultaneously chosen stock position.\n\n### Question\n\nBased on the structure of the optimal futures position in `Eq. (3)`, which of the following statements are valid interpretations of an agent's motives for holding a futures position?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 109, "Question": "### Background\n\nCorporate finance theory offers competing views on the relationship between debt and other managerial monitoring mechanisms, such as an independent board of directors. These views are termed the \"complementary\" and \"substitutive\" hypotheses.\n\n### Data / Model Specification\n\n*   **Complementary View:** Argues that strong monitoring mechanisms are used together. A powerful board would impose higher debt on management to further constrain them, predicting a positive correlation between outside directors and debt.\n*   **Substitutive View:** Argues that different monitoring mechanisms can replace one another. If a board provides effective oversight, the firm needs less of the disciplinary (but costly) effect of debt, predicting a negative correlation.\n\nThe paper tests these competing views and finds a statistically significant negative correlation between the proportion of outside directors and corporate debt levels.\n\n---\n\nBased on these theories and the paper's findings, which of the following statements are **INCORRECT**?\n", "Options": {"A": "The paper's finding of a negative correlation is consistent with the theory that managers in firms with weak board oversight use high leverage to bond themselves and establish a reputation.", "B": "The economic logic of the substitutive view is that since an independent board already reduces agency costs, the marginal benefit of using costly debt for additional discipline is low.", "C": "The complementary view predicts that firms with more outside directors will have higher leverage ratios.", "D": "The paper's empirical results provide support for the complementary view of corporate governance mechanisms."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the ability to distinguish between two competing theories (complementary vs. substitutive) and evaluate the paper's findings in light of them. The Single-Correct-Answer Inversion Protocol was used. The original objective was to identify the single correct description of the paper's findings, which would have been a statement supporting the substitutive view. By inverting the stem to 'INCORRECT', the distractors become the correct answers. Option A is incorrect because the paper's finding of a *negative* correlation refutes the complementary view. Option D is incorrect because the paper's finding supports the substitutive view, which implies that in the absence of strong board monitoring (i.e., weak oversight), firms would have *higher* leverage, not that the finding itself is consistent with that logic in isolation. Options B and C are correct statements describing the substitutive and complementary theories, respectively, and thus are the new distractors.", "qid": "109", "question": "### Background\n\nCorporate finance theory offers competing views on the relationship between debt and other managerial monitoring mechanisms, such as an independent board of directors. These views are termed the \"complementary\" and \"substitutive\" hypotheses.\n\n### Data / Model Specification\n\n*   **Complementary View:** Argues that strong monitoring mechanisms are used together. A powerful board would impose higher debt on management to further constrain them, predicting a positive correlation between outside directors and debt.\n*   **Substitutive View:** Argues that different monitoring mechanisms can replace one another. If a board provides effective oversight, the firm needs less of the disciplinary (but costly) effect of debt, predicting a negative correlation.\n\nThe paper tests these competing views and finds a statistically significant negative correlation between the proportion of outside directors and corporate debt levels.\n\n---\n\nBased on these theories and the paper's findings, which of the following statements are **INCORRECT**?\n", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 90, "Question": "### Background\n\n**Research Question.** A central challenge in corporate finance is to devise empirical tests of strategic risk-shifting that are robust to mechanical artifacts like mean reversion. This paper proposes two novel methodologies based on portfolio holdings to overcome a 'sorting bias' found in traditional return-based tests of mutual fund tournament behavior.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual fund holdings data from 1990-2006. The goal is to test whether underperforming managers ('losers') in the first half of the year actively change their portfolio risk in the second half. The paper also re-examines whether this behavior is linked to overall market conditions.\n\n### Data / Model Specification\n\n**Application: Re-evaluating the Link to Market Conditions**\nPrior research found that tournament behavior depends on first-half market returns. The paper tests whether this finding holds with the new, bias-free measures, and compares it to the behavior of the `Before Ratio`, a proxy for the sorting bias.\n\n**Table 1. Tournament Behavior and Market Conditions**\n\n| Variable | ρ with Return | -Market Mean | +Market Mean | +/- p-value |\n|:---|:---:|:---:|:---:|:---:|\n| Before Ratio | 0.45 | 0.832 | 1.172 | 0.01** |\n| Share Change (Low Dummy) | 0.04 | 1.055 | 0.519 | 0.55 |\n| Portfolio (Low Dummy) | 0.26 | -0.003 | 0.079 | 0.16 |\n\n*Note: `ρ with Return` is the correlation with first-half market return. `-Market Mean` and `+Market Mean` are average values in negative and positive market years. `+/- p-value` tests the difference in means. **Significant at 1% level.*\n\n### Question\n\nThe paper's central argument is that prior findings linking tournament behavior to market conditions are an artifact of a 'sorting bias'. Based on the evidence in Table 1, select all statements that correctly form part of this argument.", "Options": {"A": "The paper's new methodologies confirm the prior literature's finding that tournament behavior is significantly stronger in years with positive market returns.", "B": "The `Before Ratio`, a proxy for the sorting bias, is systematically related to market conditions, as its mean value is significantly different in positive-return versus negative-return years (p-value = 0.01).", "C": "The `Share Change` measure of tournament behavior has a positive correlation (0.04) with market returns, providing weak but confirming evidence that risk-taking by losers increases in stronger markets.", "D": "The new, bias-corrected measure of tournament behavior derived from the portfolio bootstrap method (`Portfolio (Low Dummy)`) shows no statistically significant relationship with market conditions (p-value = 0.16 for the difference in means)."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the student's ability to synthesize evidence to understand the paper's main critique of the prior literature. It uses a **Premise Packaging** strategy, asking the student to select the key premises of the argument against the market-conditions hypothesis. Distractors are designed to test for deep understanding: one is a **Conceptual Opposite** (falsely claiming the new methods confirm the old results), and the other is **True but Irrelevant** (citing a statistically insignificant correlation as meaningful evidence).", "qid": "90", "question": "### Background\n\n**Research Question.** A central challenge in corporate finance is to devise empirical tests of strategic risk-shifting that are robust to mechanical artifacts like mean reversion. This paper proposes two novel methodologies based on portfolio holdings to overcome a 'sorting bias' found in traditional return-based tests of mutual fund tournament behavior.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual fund holdings data from 1990-2006. The goal is to test whether underperforming managers ('losers') in the first half of the year actively change their portfolio risk in the second half. The paper also re-examines whether this behavior is linked to overall market conditions.\n\n### Data / Model Specification\n\n**Application: Re-evaluating the Link to Market Conditions**\nPrior research found that tournament behavior depends on first-half market returns. The paper tests whether this finding holds with the new, bias-free measures, and compares it to the behavior of the `Before Ratio`, a proxy for the sorting bias.\n\n**Table 1. Tournament Behavior and Market Conditions**\n\n| Variable | ρ with Return | -Market Mean | +Market Mean | +/- p-value |\n|:---|:---:|:---:|:---:|:---:|\n| Before Ratio | 0.45 | 0.832 | 1.172 | 0.01** |\n| Share Change (Low Dummy) | 0.04 | 1.055 | 0.519 | 0.55 |\n| Portfolio (Low Dummy) | 0.26 | -0.003 | 0.079 | 0.16 |\n\n*Note: `ρ with Return` is the correlation with first-half market return. `-Market Mean` and `+Market Mean` are average values in negative and positive market years. `+/- p-value` tests the difference in means. **Significant at 1% level.*\n\n### Question\n\nThe paper's central argument is that prior findings linking tournament behavior to market conditions are an artifact of a 'sorting bias'. Based on the evidence in Table 1, select all statements that correctly form part of this argument.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 98, "Question": "### Background\n\n**Research Question.** Can the non-linear relationship between foreign bank entry and investment be characterized as a discrete regime shift at an endogenously determined threshold?\n\n**Setting & Data.** A threshold regression model is estimated on a cross-section of 54 countries. The model allows the marginal effect of foreign bank penetration to differ depending on whether it is above or below an optimally chosen threshold value.\n\n### Data / Model Specification\n\nThe paper estimates a piecewise linear model where the sample is split into two regimes based on the threshold variable `FBSASS` (share of foreign bank assets). The coefficients `α₅` and `α₆` represent the constant marginal effect of `FBSASS` in the low-penetration and high-penetration regimes, respectively.\n\n**Table 1. Threshold Effect Estimation Results**\n\n| Parameter / Variable | Estimate | t-statistic |\n|:---|:---:|:---:|\n| `FBSASS` (if `FBSASS < THR`) (α₅) | -14.10 | -3.07 |\n| `FBSASS` (if `FBSASS > THR`) (α₆) | 4.57 | 1.85 |\n| Estimated `THR` | 0.42 | - |\n| LR test (p-value) | 20.44 (0.01) | - |\n\n*Source: Table 6, column 12 from the paper.*\n\n### Question\n\nBased on the threshold model results presented in **Table 1**, select all of the following statements that are correct.", "Options": {"A": "The Likelihood Ratio (LR) test result (p=0.01) indicates that the threshold model provides a statistically superior fit compared to a simple linear model, corroborating the non-linearity hypothesis.", "B": "The optimal threshold of 0.42 was chosen because it represents the median `FBSASS` value in the sample, ensuring the two regimes have an equal number of countries.", "C": "The coefficient `α̂₆ = 4.57` means that for a country with `FBSASS` = 0.50, the model's predicted investment-to-GDP ratio is 4.57.", "D": "The model estimates a statistically significant negative marginal effect of foreign bank entry on investment for countries with an `FBSASS` below 42%."}, "Answer": ["A", "D"], "pi_justification": "REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the output of a threshold regression model. It uses a Scenario Application strategy, asking for interpretation of the two regimes. The correct options (A, B) are an atomic decomposition of the key findings: the sign/significance of the regime-specific coefficients and the meaning of the test for non-linearity. Distractor C is a Conceptual (Almost Right) error, confusing a marginal effect with a predicted level. Distractor D is a Factual Error regarding the methodology, as the threshold is chosen to minimize SSR, not to balance sample sizes.", "qid": "98", "question": "### Background\n\n**Research Question.** Can the non-linear relationship between foreign bank entry and investment be characterized as a discrete regime shift at an endogenously determined threshold?\n\n**Setting & Data.** A threshold regression model is estimated on a cross-section of 54 countries. The model allows the marginal effect of foreign bank penetration to differ depending on whether it is above or below an optimally chosen threshold value.\n\n### Data / Model Specification\n\nThe paper estimates a piecewise linear model where the sample is split into two regimes based on the threshold variable `FBSASS` (share of foreign bank assets). The coefficients `α₅` and `α₆` represent the constant marginal effect of `FBSASS` in the low-penetration and high-penetration regimes, respectively.\n\n**Table 1. Threshold Effect Estimation Results**\n\n| Parameter / Variable | Estimate | t-statistic |\n|:---|:---:|:---:|\n| `FBSASS` (if `FBSASS < THR`) (α₅) | -14.10 | -3.07 |\n| `FBSASS` (if `FBSASS > THR`) (α₆) | 4.57 | 1.85 |\n| Estimated `THR` | 0.42 | - |\n| LR test (p-value) | 20.44 (0.01) | - |\n\n*Source: Table 6, column 12 from the paper.*\n\n### Question\n\nBased on the threshold model results presented in **Table 1**, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 141, "Question": "### Background\n\n**Research Question.** How do specific assumptions about the claim size distribution, such as being exponential, affect the analytical form of the Gerber-Shiu function, and what are the limitations of such assumptions?\n\n**Setting / Data-Generating Environment.** We consider a semi-Markovian risk model where claim sizes `Y_j` in each state `j` can be modeled by different distributions. The choice of distribution has significant analytical and practical consequences.\n\n**Variables & Parameters.**\n- `~b(s)`: Laplace transform of the claim size density.\n- `ε_k`: Exponents in the solution, determined as roots of a characteristic equation.\n\n---\n\n### Data / Model Specification\n\nThe analytical solution for the Gerber-Shiu function often involves finding roots of the generalized Lundberg equation:\n  \n\\det(\\mathbf{I} - \\tilde{\\mathbf{k}}(\\delta - cs) \\mathbf{P} \\tilde{\\mathbf{b}}(s)) = 0\n \nThe validity and form of this analysis depend critically on the properties of the claim size distributions and their Laplace transforms `~b(s)`.\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the role and limitations of claim size distribution assumptions in this model? (Select all that apply)", "Options": {"A": "Assuming exponential claim sizes is analytically convenient because their rational Laplace transforms often reduce complex integral equations to algebraic problems.", "B": "The exponential distribution is empirically well-suited for modeling insurance portfolios with high exposure to catastrophic events.", "C": "Phase-type distributions are useful because they are the only class of distributions capable of representing heavy-tailed phenomena.", "D": "The method of finding exponential decay rates `ε_k` is valid only if the claim size distributions' Laplace transforms exist for positive real arguments."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item tests the understanding of the trade-offs and mathematical requirements of using specific claim size distributions. It assesses foundational knowledge about the model's assumptions. Distractors include a conceptual opposite (B: exponential is ill-suited for catastrophes) and a common misconception about phase-type distributions (D: they are dense but not the only way to model heavy tails, and are not inherently heavy-tailed).", "qid": "141", "question": "### Background\n\n**Research Question.** How do specific assumptions about the claim size distribution, such as being exponential, affect the analytical form of the Gerber-Shiu function, and what are the limitations of such assumptions?\n\n**Setting / Data-Generating Environment.** We consider a semi-Markovian risk model where claim sizes `Y_j` in each state `j` can be modeled by different distributions. The choice of distribution has significant analytical and practical consequences.\n\n**Variables & Parameters.**\n- `~b(s)`: Laplace transform of the claim size density.\n- `ε_k`: Exponents in the solution, determined as roots of a characteristic equation.\n\n---\n\n### Data / Model Specification\n\nThe analytical solution for the Gerber-Shiu function often involves finding roots of the generalized Lundberg equation:\n  \n\\det(\\mathbf{I} - \\tilde{\\mathbf{k}}(\\delta - cs) \\mathbf{P} \\tilde{\\mathbf{b}}(s)) = 0\n \nThe validity and form of this analysis depend critically on the properties of the claim size distributions and their Laplace transforms `~b(s)`.\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the role and limitations of claim size distribution assumptions in this model? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 108, "Question": "### Background\n\nCorporate governance literature distinguishes between the **monitoring role** and the **advisory role** of outside directors. The monitoring role focuses on constraining managerial misbehavior, while the advisory role involves providing expert counsel to improve strategic decisions.\n\n### Data / Model Specification\n\nThe paper's hypotheses link these roles to corporate debt policy:\n*   **Hypothesis 2 (H2):** Posits a correlation between the proportion of outside directors and corporate debt, which relates to the board's **monitoring** function. A substitutive relationship (negative correlation) implies strong board monitoring replaces the need for debt's disciplinary role.\n*   **Hypothesis 3 (H3):** Posits that outside directors magnify the negative correlation between tax aggressiveness and debt. This is explained by the board's **advisory** function, where expert directors provide better counsel on making optimal capital structure adjustments in response to changes in the firm's non-debt tax shields.\n\n---\n\nSelect all statements that correctly apply these theoretical concepts.\n", "Options": {"A": "According to the logic of H3, a board with more outside directors is expected to react more strongly to an increase in non-debt tax shields by pushing for a larger reduction in corporate debt.", "B": "A board vetoing a CEO's value-destroying pet project is an example of its advisory role.", "C": "The finding of a negative correlation between outside directors and debt supports the view that these two governance mechanisms are substitutes for monitoring management.", "D": "If a firm has valuable growth options, a skilled advisory board would likely push for higher debt levels to fund these options, counteracting the debt-substitution effect."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the understanding of the distinct theoretical roles of a board (monitoring vs. advisory) as presented in the paper. It uses a Scenario Application strategy. Option B correctly links the empirical finding (negative correlation) to the substitutive monitoring theory. Option C correctly interprets the paper's core argument for H3: better advice from outside directors leads to a more pronounced (magnified) debt-substitution effect. Option A is a Conceptual Opposite distractor; vetoing a project is a classic monitoring action, not an advisory one. Option D is also a Conceptual Opposite; the presence of valuable growth options increases the cost of debt (underinvestment risk), meaning a skilled board would push for *less* debt, reinforcing, not counteracting, the debt-substitution effect.", "qid": "108", "question": "### Background\n\nCorporate governance literature distinguishes between the **monitoring role** and the **advisory role** of outside directors. The monitoring role focuses on constraining managerial misbehavior, while the advisory role involves providing expert counsel to improve strategic decisions.\n\n### Data / Model Specification\n\nThe paper's hypotheses link these roles to corporate debt policy:\n*   **Hypothesis 2 (H2):** Posits a correlation between the proportion of outside directors and corporate debt, which relates to the board's **monitoring** function. A substitutive relationship (negative correlation) implies strong board monitoring replaces the need for debt's disciplinary role.\n*   **Hypothesis 3 (H3):** Posits that outside directors magnify the negative correlation between tax aggressiveness and debt. This is explained by the board's **advisory** function, where expert directors provide better counsel on making optimal capital structure adjustments in response to changes in the firm's non-debt tax shields.\n\n---\n\nSelect all statements that correctly apply these theoretical concepts.\n", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 134, "Question": "### Background\n\n**Research Question.** How does the shift from Basel I to the Basel II A-IRB framework differentially impact the required Tier 1 and Total Capital of monoline credit card banks versus diversified banking organizations?\n\n**Setting.** The analysis compares the change in minimum regulatory capital requirements for credit card portfolios under two polar cases: a specialized monoline credit card bank and a credit card subsidiary within a large, diversified bank. The differences arise from portfolio-wide calculations for loan loss reserves.\n\n### Data / Model Specification\n\nThe transition from Basel I to Basel II involves two critical rule changes regarding the Allowance for Loan and Lease Losses (`ALLL`):\n\n1.  **ALLL Cap (affects Total Capital):** The maximum amount of `ALLL` that can be included in Tier 2 capital changes. The Basel II cap is more generous for high-loss portfolios like credit cards.\n2.  **Reserve Shortfall (affects Tier 1 Capital):** Basel II introduces a deduction from Tier 1 capital if `ALLL` is less than Expected Losses (`EL`).\n\nBoth calculations are performed on a whole-bank basis. The estimated impact of these changes is summarized in the table below.\n\n**Table 1. Estimated Percentage Change in Required Capital from Basel I to Basel II**\n\n| Bank Type | Change in Required Total Capital | Change in Required Tier 1 Capital |\n| :--- | :---: | :---: |\n| Diversified Holding Co. | +44.3% | +2.2% |\n| Monoline Credit Card Bank | +23.6% | +13.2% |\n\n---\n\nBased on the rules and the data in **Table 1**, which of the following statements accurately explain the divergent outcomes for the two bank types?\n\nSelect all that apply.", "Options": {"A": "A diversified bank avoids a large increase in required Tier 1 Capital because reserve surpluses from its other loan portfolios (e.g., mortgages) can offset the reserve shortfall from its credit card portfolio in the whole-bank calculation.", "B": "A monoline bank's required Tier 1 Capital increases more than a diversified bank's because its portfolio-wide reserves are likely less than its expected losses, triggering a significant Tier 1 capital deduction for the reserve shortfall.", "C": "A diversified bank experiences a larger increase in required Total Capital because its portfolio-wide reserves were likely already below the Basel I ALLL cap, so it receives no benefit from the new, more generous Basel II cap.", "D": "A monoline bank's required Total Capital increases less than a diversified bank's because the more generous Basel II ALLL cap allows it to count more of its large, pre-existing reserves as Tier 2 capital, providing a partial capital relief."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's understanding of the specific, nuanced mechanisms within the Basel II framework (ALLL cap and reserve shortfall) and their differential impact based on a bank's business model. Depth Strategy: Reverse-Reasoning. The user is given the final outcomes (the table) and must identify all the correct causal explanations based on the provided rules. Distractor Logic: Similar to the previous item, this question is structured with four correct options, each explaining a different facet of the observed phenomenon. This comprehensive format ensures the candidate understands the full picture—why each bank type is affected differently for both Tier 1 and Total capital—rather than just one piece of the puzzle.", "qid": "134", "question": "### Background\n\n**Research Question.** How does the shift from Basel I to the Basel II A-IRB framework differentially impact the required Tier 1 and Total Capital of monoline credit card banks versus diversified banking organizations?\n\n**Setting.** The analysis compares the change in minimum regulatory capital requirements for credit card portfolios under two polar cases: a specialized monoline credit card bank and a credit card subsidiary within a large, diversified bank. The differences arise from portfolio-wide calculations for loan loss reserves.\n\n### Data / Model Specification\n\nThe transition from Basel I to Basel II involves two critical rule changes regarding the Allowance for Loan and Lease Losses (`ALLL`):\n\n1.  **ALLL Cap (affects Total Capital):** The maximum amount of `ALLL` that can be included in Tier 2 capital changes. The Basel II cap is more generous for high-loss portfolios like credit cards.\n2.  **Reserve Shortfall (affects Tier 1 Capital):** Basel II introduces a deduction from Tier 1 capital if `ALLL` is less than Expected Losses (`EL`).\n\nBoth calculations are performed on a whole-bank basis. The estimated impact of these changes is summarized in the table below.\n\n**Table 1. Estimated Percentage Change in Required Capital from Basel I to Basel II**\n\n| Bank Type | Change in Required Total Capital | Change in Required Tier 1 Capital |\n| :--- | :---: | :---: |\n| Diversified Holding Co. | +44.3% | +2.2% |\n| Monoline Credit Card Bank | +23.6% | +13.2% |\n\n---\n\nBased on the rules and the data in **Table 1**, which of the following statements accurately explain the divergent outcomes for the two bank types?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 126, "Question": "### Background\n\n**Research Question.** How can the stochastic dynamics of an epidemic at the population level be modeled and applied to the pricing of insurance contracts?\n\n**Setting.** A population starts with `S_0` susceptible and `I_0` infected individuals. Each individual's health state is assumed to follow an independent SIR Markov process. This framework is used to price an insurance policy that provides benefits upon infection and removal.\n\n**Variables and Parameters.**\n- `S̃(∞)`: Random variable for the final number of individuals who are never infected.\n- `C = S_0 - S̃(∞)`: Random variable for the total number of individuals who get infected (total claims).\n- `D`: Random variable for the total duration of the epidemic.\n- `P`: The continuous premium rate for an individual insurance policy.\n- `S¹`: Lump sum benefit paid on infection.\n- `H`: Continuous annuity benefit rate paid while infected.\n- `S²`: Lump sum benefit paid on removal.\n\n---\n\n### Data / Model Specification\n\nUnder the assumption of individual independence, the following results hold:\n- The probability that an initially susceptible person is never infected is `p = P^{00}(0,∞) = s(∞)/s(0)`.\n- The probability that an initially susceptible person is removed by time `t` is `P^{02}(0,t)`.\n- The probability that an initially infected person is removed by time `t` is `P^{12}(0,t) = 1 - e^{-αt}`.\n\nActuarial functions `ā^{ij}(0,n)` and `Ā^{ij}(0,n)` represent the expected present values of $1 annuities and $1 lump-sum benefits, respectively, for an `n`-year policy.\n\n---\n\nFor a population with `S_0` susceptible and `I_0` infected individuals at time 0, which of the following statements correctly describe the aggregate stochastic outcomes under the model's independence assumption?", "Options": {"A": "The final number of susceptible individuals `S̃(∞)` is independent of the removal rate `α`.", "B": "The duration `D` is defined as the first time `t` at which the number of infected individuals `Ĩ(t)` becomes zero.", "C": "The total number of individuals who eventually get infected, `C`, follows a distribution with mean `S_0 * (1 - s(∞)/s(0))`.", "D": "The probability that the epidemic duration `D` is less than or equal to `t` is given by `(P^{00}(0,∞) + P^{02}(0,t))^{S_0} * (1 - e^{-αt})^{I_0}`."}, "Answer": ["C", "D"], "pi_justification": "This item assesses understanding of the key population-level stochastic results from the paper (total claims and epidemic duration), which were the focus of Q1 and Q2 in the source QA. It uses a Multiple Choice format to efficiently test two major formulas. Distractor C tests a deeper understanding of the model's parameters, as `s(∞)` implicitly depends on `α`. Distractor D targets a specific, high-frequency misconception about the definition of epidemic duration that the paper explicitly addresses.", "qid": "126", "question": "### Background\n\n**Research Question.** How can the stochastic dynamics of an epidemic at the population level be modeled and applied to the pricing of insurance contracts?\n\n**Setting.** A population starts with `S_0` susceptible and `I_0` infected individuals. Each individual's health state is assumed to follow an independent SIR Markov process. This framework is used to price an insurance policy that provides benefits upon infection and removal.\n\n**Variables and Parameters.**\n- `S̃(∞)`: Random variable for the final number of individuals who are never infected.\n- `C = S_0 - S̃(∞)`: Random variable for the total number of individuals who get infected (total claims).\n- `D`: Random variable for the total duration of the epidemic.\n- `P`: The continuous premium rate for an individual insurance policy.\n- `S¹`: Lump sum benefit paid on infection.\n- `H`: Continuous annuity benefit rate paid while infected.\n- `S²`: Lump sum benefit paid on removal.\n\n---\n\n### Data / Model Specification\n\nUnder the assumption of individual independence, the following results hold:\n- The probability that an initially susceptible person is never infected is `p = P^{00}(0,∞) = s(∞)/s(0)`.\n- The probability that an initially susceptible person is removed by time `t` is `P^{02}(0,t)`.\n- The probability that an initially infected person is removed by time `t` is `P^{12}(0,t) = 1 - e^{-αt}`.\n\nActuarial functions `ā^{ij}(0,n)` and `Ā^{ij}(0,n)` represent the expected present values of $1 annuities and $1 lump-sum benefits, respectively, for an `n`-year policy.\n\n---\n\nFor a population with `S_0` susceptible and `I_0` infected individuals at time 0, which of the following statements correctly describe the aggregate stochastic outcomes under the model's independence assumption?", "model_evaluation_result": {"model_raw_response": "\"B,C\"", "model_answer": ["B", "C"], "error": null}}
{"ID": 122, "Question": "### Background\n\n**Research Question.** How do key methodological choices in Data Envelopment Analysis (DEA) affect the measurement of bank efficiency?\n\n**Setting / Data-Generating Environment.** The study estimates bank efficiency using DEA and compares standard DEA scores with bootstrap bias-corrected scores.\n\n**Variables & Parameters.**\n\n*   `$\\hat{\\rho}$`: The standard, non-bootstrapped DEA cost efficiency score.\n*   `$\\hat{\\hat{\\rho}}$`: The bootstrap bias-corrected DEA cost efficiency score.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Bootstrapped Cost Efficiency Results (Model 3, Selected Years)**\n\n| Year | `$\\hat{\\rho}$` (Raw) | `$\\hat{\\hat{\\rho}}$` (Bias-Corrected) |\n| :--- | :--- | :--- |\n| 1994 | 0.3239* | 0.2003 |\n| 2001 | 0.4620* | 0.3282 |\n| 2007 | 0.4200* | 0.2853 |\n\n*Note: `*` denotes that the raw DEA efficiency score is significantly different from the bias-corrected score.*\n\n---\n\n### Question\n\nThe study's DEA methodology involves several key choices that affect the results. Based on the principles described in the paper and the results in Table 1, select all of the following statements that are methodologically correct.", "Options": {"A": "The raw DEA efficiency scores (`$\\hat{\\rho}$`) are systematically biased upwards because the estimated frontier, based on an in-sample envelopment, is almost certain to lie inside the true production frontier.", "B": "The significant difference between `$\\hat{\\rho}$` and `$\\hat{\\hat{\\rho}}$` in Table 1 suggests that the environmental variables used in the second stage are likely endogenous.", "C": "If the true banking technology exhibited increasing returns to scale for small banks, imposing a Constant Returns to Scale (CRS) assumption would tend to overstate the efficiency of these small banks.", "D": "Using a \"common-frontier\" for all years is a primary reason the study's efficiency scores are lower than those in other studies, as banks are compared against the best performers from the entire sample period."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a premise/assumption packaging strategy to test understanding of the core methodological assumptions of DEA and their consequences. The correct options are direct statements of principle from the paper. Distractor C is a conceptual opposite regarding the effect of the CRS assumption. Distractor D is a 'true but irrelevant concept' distractor, confusing the issue of DEA score bias with the separate econometric problem of endogeneity.", "qid": "122", "question": "### Background\n\n**Research Question.** How do key methodological choices in Data Envelopment Analysis (DEA) affect the measurement of bank efficiency?\n\n**Setting / Data-Generating Environment.** The study estimates bank efficiency using DEA and compares standard DEA scores with bootstrap bias-corrected scores.\n\n**Variables & Parameters.**\n\n*   `$\\hat{\\rho}$`: The standard, non-bootstrapped DEA cost efficiency score.\n*   `$\\hat{\\hat{\\rho}}$`: The bootstrap bias-corrected DEA cost efficiency score.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Bootstrapped Cost Efficiency Results (Model 3, Selected Years)**\n\n| Year | `$\\hat{\\rho}$` (Raw) | `$\\hat{\\hat{\\rho}}$` (Bias-Corrected) |\n| :--- | :--- | :--- |\n| 1994 | 0.3239* | 0.2003 |\n| 2001 | 0.4620* | 0.3282 |\n| 2007 | 0.4200* | 0.2853 |\n\n*Note: `*` denotes that the raw DEA efficiency score is significantly different from the bias-corrected score.*\n\n---\n\n### Question\n\nThe study's DEA methodology involves several key choices that affect the results. Based on the principles described in the paper and the results in Table 1, select all of the following statements that are methodologically correct.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 117, "Question": "### Background\n\n**Research Question.** In an exploratory analysis with many potential explanatory variables and likely multicollinearity, how can a researcher develop a reliable and parsimonious regression model?\n\n**Setting / Data-Generating Environment.** The paper employs a two-stage methodology to build predictive models for local employment. First, it uses a diagnostic procedure to detect and remove collinear variables. Second, it uses a model selection criterion to choose the \"best\" subset of the remaining predictors. This approach is common in exploratory econometric work where theory provides only loose guidance on the exact model specification.\n\n---\n\n### Data / Model Specification\n\n1.  **Collinearity Diagnostic:** The study uses the Belsley, Kuh, and Welsch (BKW) procedure. A degrading collinearity is flagged if a high **condition index** (e.g., > 30) is associated with high **variance proportions** (e.g., > 0.5) for two or more variables. When collinearity is detected, the author eliminates the variable with the lower Student's t-statistic in a preliminary regression.\n\n2.  **Model Selection:** From the remaining variables, the final model is chosen using Mallows' Cp statistic, which estimates the total prediction error.\n      \n    C_p = \\frac{RSS_p}{s^2} - (n - 2p) \\quad \\text{(Eq. 1)}\n     \n    where `p` is the number of parameters, `n` is the sample size, `RSS_p` is the residual sum of squares, and `s^2` is the estimated error variance from the largest model. For a model with no systematic bias, its expected `C_p` value is approximately `p`.\n\n---\n\n### The Question\n\nGiven the study's small sample size (n=26 years), what are the likely risks of the author's two-stage procedure (using t-tests to screen for collinearity, then minimizing Cp)? Select all that apply.", "Options": {"A": "An increased risk of Type I error, where an irrelevant variable is incorrectly kept in the final model.", "B": "An increased risk of Type II error, where a truly important economic variable is incorrectly dropped from the final model.", "C": "The final selected model may be sensitive to small changes in the data, reflecting sampling noise rather than a stable underlying relationship.", "D": "A guaranteed increase in omitted variable bias compared to simply running the full model."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the ability to synthesize the flaws of the paper's methodology and evaluate its risks in a small-sample context. It uses a Scenario Application strategy. The correct answers (A, B) identify the key risks: the conservative nature of the procedure increases the chance of false negatives (Type II error), and data-driven model selection in small samples leads to instability. Distractors include an overly strong claim (C) and the conceptual opposite (D), as the procedure is more likely to commit Type II than Type I errors.", "qid": "117", "question": "### Background\n\n**Research Question.** In an exploratory analysis with many potential explanatory variables and likely multicollinearity, how can a researcher develop a reliable and parsimonious regression model?\n\n**Setting / Data-Generating Environment.** The paper employs a two-stage methodology to build predictive models for local employment. First, it uses a diagnostic procedure to detect and remove collinear variables. Second, it uses a model selection criterion to choose the \"best\" subset of the remaining predictors. This approach is common in exploratory econometric work where theory provides only loose guidance on the exact model specification.\n\n---\n\n### Data / Model Specification\n\n1.  **Collinearity Diagnostic:** The study uses the Belsley, Kuh, and Welsch (BKW) procedure. A degrading collinearity is flagged if a high **condition index** (e.g., > 30) is associated with high **variance proportions** (e.g., > 0.5) for two or more variables. When collinearity is detected, the author eliminates the variable with the lower Student's t-statistic in a preliminary regression.\n\n2.  **Model Selection:** From the remaining variables, the final model is chosen using Mallows' Cp statistic, which estimates the total prediction error.\n      \n    C_p = \\frac{RSS_p}{s^2} - (n - 2p) \\quad \\text{(Eq. 1)}\n     \n    where `p` is the number of parameters, `n` is the sample size, `RSS_p` is the residual sum of squares, and `s^2` is the estimated error variance from the largest model. For a model with no systematic bias, its expected `C_p` value is approximately `p`.\n\n---\n\n### The Question\n\nGiven the study's small sample size (n=26 years), what are the likely risks of the author's two-stage procedure (using t-tests to screen for collinearity, then minimizing Cp)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 110, "Question": "### Background\n\nA study uses a fixed-effects model (FEM) to estimate the effect of tax aggressiveness (`TAG`) on corporate debt (`DEBT`). The model includes firm-specific fixed effects (`\\alpha_i`) and time-specific fixed effects (`\\lambda_t`).\n\n### Data / Model Specification\n\nThe estimated model is:\n\n  \nDEBT_{it} = \\alpha_i + \\lambda_t + \\beta_1 TAG_{it} + \\delta' X_{it} + \\varepsilon_{it} \n \n\nwhere `X_{it}` is a vector of time-varying control variables.\n\n---\n\nSelect all of the following statements that correctly describe the properties and limitations of this empirical strategy.\n", "Options": {"A": "The coefficient `\\beta_1` is estimated using only the variation *between* different firms at a single point in time.", "B": "To address potential time-varying endogeneity, a valid instrumental variable for the change in tax aggressiveness (`\\Delta TAG_{it}`) in a first-differenced GMM model could be a lagged level of tax aggressiveness (e.g., `TAG_{i, t-2}`).", "C": "The inclusion of firm fixed effects (`\\alpha_i`) controls for bias from unobserved, time-invariant firm characteristics, such as a firm's corporate culture regarding risk.", "D": "This model is robust to time-varying endogeneity, such as the simultaneous determination of debt and tax policy within a given year."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the student's understanding of the econometric methodology used in the paper, specifically the purpose of fixed effects and potential solutions for remaining endogeneity. It uses a Reverse-Reasoning strategy (identifying valid properties of a given method). Option A correctly identifies the primary purpose of firm fixed effects. Option D correctly proposes a standard solution (GMM with lagged instruments) for the endogeneity problem that fixed effects do not solve. Option B is a Conceptual Opposite distractor; fixed effects models use *within-firm* variation over time, not between-firm variation. Option C is also a Conceptual Opposite; fixed effects do *not* solve for time-varying endogeneity like simultaneity, which is a key limitation of the approach.", "qid": "110", "question": "### Background\n\nA study uses a fixed-effects model (FEM) to estimate the effect of tax aggressiveness (`TAG`) on corporate debt (`DEBT`). The model includes firm-specific fixed effects (`\\alpha_i`) and time-specific fixed effects (`\\lambda_t`).\n\n### Data / Model Specification\n\nThe estimated model is:\n\n  \nDEBT_{it} = \\alpha_i + \\lambda_t + \\beta_1 TAG_{it} + \\delta' X_{it} + \\varepsilon_{it} \n \n\nwhere `X_{it}` is a vector of time-varying control variables.\n\n---\n\nSelect all of the following statements that correctly describe the properties and limitations of this empirical strategy.\n", "model_evaluation_result": {"model_raw_response": "\"B,C\"", "model_answer": ["B", "C"], "error": null}}
{"ID": 39, "Question": "### Background\n\n**Research Question:** This case examines the market's valuation of the ECB's 2014 Comprehensive Assessment (CA) through an event study. The central question is whether the market reacted to new, bank-specific (\"idiosyncratic\") information or to news about a systematic or systemic factor affecting all banks.\n\n**Setting and Sample:** The analysis focuses on the stock returns of 38 listed European banks around the CA results announcement on October 26, 2014. The study forgoes a control group of non-CA banks, arguing it would introduce confounding biases.\n\n### Data / Model Specification\n\nThe study computes Cumulative Average Abnormal Returns (CAARs) for portfolios of banks. Banks are grouped into quartiles based on the total capital curtailment they received (AQR + STE), with the 1st Quartile containing the banks that performed the worst.\n\n**Table 1: Event Study Results (CAARs)**\n\n| | **Pre-Event** | **Post-Event** |\n| :--- | :--- | :--- |\n| **Group of Banks** | **CAAR [-5, 0]** | **CAAR [0, +5]** |\n| Full Sample (38 banks) | 5.26%*** | -7.66%** |\n| 1st Quartile (Worst performing) | 7.47%* | -14.68%* |\n| 4th Quartile (Best performing) | 3.22%** | -4.70%*** |\n\n*Source: Adapted from Table 6, Panel A in the source paper. `***`, `**`, `*` denote significance at 1%, 5%, and 10% levels.*\n\nThe paper also reports that the difference in post-event CAARs between the 1st and 4th quartiles is not statistically significant.\n\n### Question\n\nBased on the event study results presented in **Table 1** and the accompanying information, which of the following interpretations are valid?", "Options": {"A": "The fact that the post-event CAAR difference between the worst-performing (1st Quartile) and best-performing (4th Quartile) banks was not statistically significant indicates the CA was largely ineffective at helping investors \"sort\" banks based on their individual results.", "B": "The reversal from a significant positive CAAR (+5.26%) before the event to a significant negative CAAR (-7.66%) after the event for the full sample suggests the market was caught by a widespread negative surprise.", "C": "The results show a clear sorting effect, as the best-performing banks (4th Quartile) experienced positive abnormal returns after the announcement, while the worst-performing banks experienced negative returns.", "D": "The positive CAAR of +7.47% for the worst-performing banks before the announcement is strong evidence of information leakage, as the market correctly anticipated these banks would perform poorly."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize time-series and cross-sectional evidence from an event study to form a coherent economic conclusion. It uses an **Atomic Decomposition** strategy, with correct options representing the key time-series finding (widespread surprise, Option A) and the key cross-sectional finding (failure to sort, Option B). Distractors target frequent misinterpretations of event study results, such as confusing the meaning of pre-event returns (**Conceptual Opposite**, Option C) and making a claim factually contradicted by the data table (**Factual Error**, Option D).", "qid": "39", "question": "### Background\n\n**Research Question:** This case examines the market's valuation of the ECB's 2014 Comprehensive Assessment (CA) through an event study. The central question is whether the market reacted to new, bank-specific (\"idiosyncratic\") information or to news about a systematic or systemic factor affecting all banks.\n\n**Setting and Sample:** The analysis focuses on the stock returns of 38 listed European banks around the CA results announcement on October 26, 2014. The study forgoes a control group of non-CA banks, arguing it would introduce confounding biases.\n\n### Data / Model Specification\n\nThe study computes Cumulative Average Abnormal Returns (CAARs) for portfolios of banks. Banks are grouped into quartiles based on the total capital curtailment they received (AQR + STE), with the 1st Quartile containing the banks that performed the worst.\n\n**Table 1: Event Study Results (CAARs)**\n\n| | **Pre-Event** | **Post-Event** |\n| :--- | :--- | :--- |\n| **Group of Banks** | **CAAR [-5, 0]** | **CAAR [0, +5]** |\n| Full Sample (38 banks) | 5.26%*** | -7.66%** |\n| 1st Quartile (Worst performing) | 7.47%* | -14.68%* |\n| 4th Quartile (Best performing) | 3.22%** | -4.70%*** |\n\n*Source: Adapted from Table 6, Panel A in the source paper. `***`, `**`, `*` denote significance at 1%, 5%, and 10% levels.*\n\nThe paper also reports that the difference in post-event CAARs between the 1st and 4th quartiles is not statistically significant.\n\n### Question\n\nBased on the event study results presented in **Table 1** and the accompanying information, which of the following interpretations are valid?", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 20, "Question": "### Background\n\nA study investigates the impact of other large shareholders on family firm value. It tests Hypothesis 1, which posits that both the mere presence of multiple shareholders and their increasing voting power relative to the family should positively influence firm value.\n\n### Data / Model Specification\n\nTwo key variables are used to test this hypothesis:\n\n*   `MLSH`: A dummy variable that is 1 if multiple large shareholders exist, and 0 otherwise.\n*   `VOTING2341`: A continuous variable measuring the contestability of the family's power, calculated as `(SSH + TSH + IVSH) / FSH`, where `FSH` is the family's voting rights and `SSH`, `TSH`, `IVSH` are the rights of the next three largest shareholders.\n\nRegression results for the effect of these variables on industry-adjusted firm value (`AVALUE`) are shown below.\n\n**Table 1: Regression Results for Hypothesis 1**\n\n| Variable     | Model 1 Coeff. (t-stat) | Model 2 Coeff. (t-stat) |\n| :----------- | :---------------------- | :---------------------- |\n| `MLSH`       | 0.52** (3.42)           | —                       |\n| `VOTING2341` | —                       | -0.08 (-0.75)           |\n*Source: Table 5 from the paper. **p<0.01.*\n\n---\n\nBased on the theoretical motivation and the empirical results in Table 1, which of the following conclusions are supported by the study?\n", "Options": {"A": "The market positively values the transition from a single-owner structure to one with at least one other significant blockholder, supporting the idea that baseline monitoring is beneficial.", "B": "The study provides strong evidence that as the voting power of other blockholders increases relative to the family's, firm value consistently increases due to enhanced contestability.", "C": "The results suggest that the *identity* and *intent* of other blockholders might be more important than their mechanical voting power, as the `VOTING2341` ratio fails to capture a significant valuation effect.", "D": "The findings for `MLSH` and `VOTING2341` are contradictory, indicating a fundamental flaw in the study's methodology."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: The user must interpret two separate regression models to evaluate a two-part hypothesis. This requires reconciling a statistically significant result with a null result and drawing nuanced conclusions about the underlying governance mechanisms.\nDepth Strategy: Reverse-Reasoning. Given the empirical results (a significant effect for presence, a null effect for relative power), the user must identify the plausible theoretical explanations.\nDistractor Logic:\n- A (Correct): The significant positive coefficient on `MLSH` directly supports this conclusion.\n- B (Distractor - Conceptual Opposite): This is the opposite of what the results show. The coefficient on `VOTING2341` is statistically insignificant and negative, directly contradicting this statement.\n- C (Correct): The null result for `VOTING2341` suggests it is a noisy proxy for true contestability. A plausible explanation, discussed in the paper, is that the variable fails to account for blockholder identity (e.g., passive ally vs. active monitor), which is a key driver of behavior.\n- D (Distractor - Almost Right): While the results are nuanced, they are not necessarily contradictory. They can be reconciled by concluding that baseline monitoring matters, but the specific measure of contestability used is not a significant driver of value. This points to a limitation of the proxy, not a fundamental methodological flaw.", "qid": "20", "question": "### Background\n\nA study investigates the impact of other large shareholders on family firm value. It tests Hypothesis 1, which posits that both the mere presence of multiple shareholders and their increasing voting power relative to the family should positively influence firm value.\n\n### Data / Model Specification\n\nTwo key variables are used to test this hypothesis:\n\n*   `MLSH`: A dummy variable that is 1 if multiple large shareholders exist, and 0 otherwise.\n*   `VOTING2341`: A continuous variable measuring the contestability of the family's power, calculated as `(SSH + TSH + IVSH) / FSH`, where `FSH` is the family's voting rights and `SSH`, `TSH`, `IVSH` are the rights of the next three largest shareholders.\n\nRegression results for the effect of these variables on industry-adjusted firm value (`AVALUE`) are shown below.\n\n**Table 1: Regression Results for Hypothesis 1**\n\n| Variable     | Model 1 Coeff. (t-stat) | Model 2 Coeff. (t-stat) |\n| :----------- | :---------------------- | :---------------------- |\n| `MLSH`       | 0.52** (3.42)           | —                       |\n| `VOTING2341` | —                       | -0.08 (-0.75)           |\n*Source: Table 5 from the paper. **p<0.01.*\n\n---\n\nBased on the theoretical motivation and the empirical results in Table 1, which of the following conclusions are supported by the study?\n", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 66, "Question": "### Background\n\n**Research Question.** How can the non-linear valuation equation arising from the optimal reinsurance problem be solved?\n\n**Setting.** After substituting the optimal reinsurance control `q*` back into the Hamilton-Jacobi-Bellman (HJB) equation, the result is a non-linear ordinary differential equation (ODE) for the value function `v(x)`. This ODE can be linearized by applying a Legendre transform, which maps the problem from the primal space (`x`, `v`) to a dual space (`y`, `ŷ`).\n\n---\n\n### Data / Model Specification\n\nThe non-linear ODE for `v(x)` is:\n\n  \n\\beta v(x)=-(\\eta-\\theta)a v_{x}(x)-\\frac{1}{2}\\left(\\frac{\\eta a}{b}\\right)^{2}\\frac{(v_{x}(x))^{2}}{v_{x x}(x)} \\quad \\text{(Eq. (1))}\n \n\nThe Legendre transform provides the following mapping between the primal (`x`, `v`) and dual (`y`, `ŷ`) spaces:\n- `y = v_x(x)`\n- `v(x) = ŷ(y) - y ŷ_y(y)`\n- `v_{xx}(x) = -1 / ŷ_{yy}(y)`\n\n---\n\n### Question\n\nWhich of the following statements correctly describe the application of the Legendre transform to linearize **Eq. (1)**? Select all that apply.", "Options": {"A": "The primal value function `v(x)` is correctly substituted by the expression `ŷ(y) + y ŷ_y(y)`.", "B": "The non-linear term in Eq. (1), `- (1/2)(ηa/b)² (v_x(x))² / v_{xx}(x)`, transforms into a linear term involving the dual function: `(1/m) y² ŷ_{yy}(y)`, where `m = 2(b/(ηa))²`.", "C": "A key step in the transformation is replacing the second derivative of the primal value function, `v_{xx}(x)`, with the expression `-1 / ŷ_{yy}(y)`, where `ŷ_{yy}(y)` is the second derivative of the dual value function.", "D": "The Legendre transform is used because the original HJB equation is non-linear in the control variable `q`."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item assesses understanding of the advanced mathematical technique (Legendre transform) used in the paper. It uses 'Atomic Decomposition' on the derivation process itself. Options A and B correctly identify key mapping relationships and the transformation of the most complex term. Distractor C (Sign Error) presents one of the core mapping relations with an incorrect sign. Distractor D (Conceptual Error) misidentifies the source of the non-linearity that necessitates the transform.", "qid": "66", "question": "### Background\n\n**Research Question.** How can the non-linear valuation equation arising from the optimal reinsurance problem be solved?\n\n**Setting.** After substituting the optimal reinsurance control `q*` back into the Hamilton-Jacobi-Bellman (HJB) equation, the result is a non-linear ordinary differential equation (ODE) for the value function `v(x)`. This ODE can be linearized by applying a Legendre transform, which maps the problem from the primal space (`x`, `v`) to a dual space (`y`, `ŷ`).\n\n---\n\n### Data / Model Specification\n\nThe non-linear ODE for `v(x)` is:\n\n  \n\\beta v(x)=-(\\eta-\\theta)a v_{x}(x)-\\frac{1}{2}\\left(\\frac{\\eta a}{b}\\right)^{2}\\frac{(v_{x}(x))^{2}}{v_{x x}(x)} \\quad \\text{(Eq. (1))}\n \n\nThe Legendre transform provides the following mapping between the primal (`x`, `v`) and dual (`y`, `ŷ`) spaces:\n- `y = v_x(x)`\n- `v(x) = ŷ(y) - y ŷ_y(y)`\n- `v_{xx}(x) = -1 / ŷ_{yy}(y)`\n\n---\n\n### Question\n\nWhich of the following statements correctly describe the application of the Legendre transform to linearize **Eq. (1)**? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 80, "Question": "### Background\n\n**Research Question.** This case challenges the canonical definitions of risk from Modern Portfolio Theory (MPT) and the Capital Asset Pricing Model (CAPM). The survey findings reveal a profound disconnect between the practitioner's context-driven view of risk and the abstract, universal definitions used in finance theory.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, who operate as agents within an organizational context. The survey shows that for these managers, practical risks like 'asset-liability matching' and 'commercial risk' (career/business risk) dominate theoretical risks like 'beta' and 'total variability of return'.\n\n---\n\n### Question\n\nThe empirical findings challenge the universality of finance theory's standard risk models. Select all of the following theoretical critiques that are directly supported by the survey evidence.", "Options": {"A": "The survey confirms that while managers' stated risk preferences differ from theory, their actual portfolio construction closely follows the principles of mean-variance optimization.", "B": "The evidence suggests that a fund manager's perception of risk is endogenous to their institutional and contractual environment, not an objective property of an asset's return distribution.", "C": "The results show that asset-liability matching is simply a practical application of minimizing the beta of a portfolio relative to a liability index.", "D": "The agent-principal relationship in delegated management, with its focus on career risk and mandate fulfillment, makes the 'rational individual investor' assumed in MPT/CAPM a poor behavioral model for professional fund managers."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to translate empirical findings into a coherent theoretical critique. It uses an 'Atomic Decomposition' strategy, breaking down the original QA's high-level critique into testable propositions. Correct options A and B capture the core arguments that risk is endogenous and that the standard behavioral model of the 'investor' is inappropriate for agents. Distractor C is a 'Conceptual Opposite,' creating a false conflict between perception and action. Distractor D creates a false equivalence between a practical heuristic (A-L matching) and a theoretical concept (beta) that the paper explicitly shows managers disregard.", "qid": "80", "question": "### Background\n\n**Research Question.** This case challenges the canonical definitions of risk from Modern Portfolio Theory (MPT) and the Capital Asset Pricing Model (CAPM). The survey findings reveal a profound disconnect between the practitioner's context-driven view of risk and the abstract, universal definitions used in finance theory.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, who operate as agents within an organizational context. The survey shows that for these managers, practical risks like 'asset-liability matching' and 'commercial risk' (career/business risk) dominate theoretical risks like 'beta' and 'total variability of return'.\n\n---\n\n### Question\n\nThe empirical findings challenge the universality of finance theory's standard risk models. Select all of the following theoretical critiques that are directly supported by the survey evidence.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 48, "Question": "### Background\n\nA study investigates the causal link between economic policy uncertainty (EPU) and corporate board structure, addressing the concern that EPU may be correlated with unobserved factors that also drive governance changes. A two-stage least squares (2SLS) instrumental variable (IV) framework is used to isolate the exogenous variation in EPU, with `Political Polarization` serving as a key instrument.\n\n### Data / Model Specification\n\nThe table below presents results from the 2SLS regression analysis.\n\n**Table 1. 2SLS Regression of Board Structure on Instrumented EPU**\n\n| | **First Stage** | **Second Stage** | **Second Stage** |\n| :--- | :--- | :--- | :--- |\n| **Variable** | **(1) EPU** | **(2) Board size** | **(3) Board independence** |\n| Instrumented EPU | | -0.031*** | 0.150*** |\n| | | (0.008) | (0.006) |\n| Political polarization | 0.474*** | | |\n| | (0.086) | | |\n| *Other IVs & Controls* | *Included* | *Included* | *Included* |\n| Weak instrument F-stat | F = 91.48*** | | |\n\n*Note: Standard errors are in parentheses. *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the provided 2SLS results and the principles of instrumental variable estimation, which of the following statements are valid conclusions or interpretations?", "Options": {"A": "The second-stage results suggest that an exogenous increase in EPU causes firms to increase board size and decrease board independence.", "B": "The exclusion restriction for the instrument `Political Polarization` is directly confirmed by its statistical significance in the first-stage regression.", "C": "The instrument `Political Polarization` strongly satisfies the relevance condition, as evidenced by the highly significant first-stage coefficient and the F-statistic of 91.48.", "D": "The second-stage coefficients (-0.031 and 0.150) support the paper's hypothesis that firms enhance board monitoring capacity in response to greater uncertainty."}, "Answer": ["C", "D"], "pi_justification": "This item assesses the user's ability to interpret the results of a 2SLS/IV regression, a core component of the paper's identification strategy. It uses a Reverse-Reasoning strategy, asking the user to identify valid conclusions from the provided table. \n- **Correct A:** This option correctly interprets the first-stage results. The F-stat of 91.48 is well above the conventional threshold of 10, and the coefficient is highly significant, confirming instrument relevance.\n- **Correct D:** This option correctly interprets the second-stage results in the context of the paper's theory. A decrease in board size and an increase in independence are both framed as enhancements to monitoring.\n- **Distractor B:** This is a Conceptual Opposite distractor. It correctly identifies the variables but reverses the direction of the effects shown in the table.\n- **Distractor C:** This is a Conceptual Distractor. It conflates the test for relevance with the untestable assumption of the exclusion restriction. Statistical significance in the first stage speaks to relevance, not the exclusion restriction.", "qid": "48", "question": "### Background\n\nA study investigates the causal link between economic policy uncertainty (EPU) and corporate board structure, addressing the concern that EPU may be correlated with unobserved factors that also drive governance changes. A two-stage least squares (2SLS) instrumental variable (IV) framework is used to isolate the exogenous variation in EPU, with `Political Polarization` serving as a key instrument.\n\n### Data / Model Specification\n\nThe table below presents results from the 2SLS regression analysis.\n\n**Table 1. 2SLS Regression of Board Structure on Instrumented EPU**\n\n| | **First Stage** | **Second Stage** | **Second Stage** |\n| :--- | :--- | :--- | :--- |\n| **Variable** | **(1) EPU** | **(2) Board size** | **(3) Board independence** |\n| Instrumented EPU | | -0.031*** | 0.150*** |\n| | | (0.008) | (0.006) |\n| Political polarization | 0.474*** | | |\n| | (0.086) | | |\n| *Other IVs & Controls* | *Included* | *Included* | *Included* |\n| Weak instrument F-stat | F = 91.48*** | | |\n\n*Note: Standard errors are in parentheses. *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the provided 2SLS results and the principles of instrumental variable estimation, which of the following statements are valid conclusions or interpretations?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": ["C"], "error": null}}
{"ID": 59, "Question": "### Background\n\n**Research Question.** How do different stylized facts—specifically jump risk and seasonality—contribute to the out-of-sample option pricing performance of an affine asset pricing model for natural gas, and how does this contribution vary across option moneyness and maturity?\n\n**Setting.** The paper evaluates four nested models for Henry Hub natural gas prices: a baseline Mean Reversion Stochastic Volatility (MRSV) model, and extensions that add Jumps (MRSVJ), Seasonality (MRSVS), and both (MRSVJS). The models are tested on their ability to price European call options out-of-sample.\n\n**Variables and Parameters.**\n- `S/K`: Moneyness of an option, the ratio of the spot price to the strike price.\n- OTM (Out-of-the-Money): `S/K < 0.95`\n- ATM (At-the-Money): `0.95 < S/K < 1.05`\n- ITM (In-the-Money): `S/K > 1.05`\n- Pricing Error: The absolute difference between the observed market price of an option and the theoretical price predicted by a model, `|C_market - C_model|`.\n\n---\n\n### Data / Model Specification\n\nThe out-of-sample option pricing performance of the four models is evaluated by calculating the average absolute pricing error for options categorized by moneyness and maturity. The results are summarized in Table 1 below.\n\n**Table 1: Out-of-Sample Mean Absolute Option Pricing Errors ($)**\n\n| | <30 days | 30-60 days | 60-90 days | 90-120 days | 120-180 days | >180 days |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **OTM (S/K < 0.95)** | | | | | | |\n| MRSV | 0.0370 | 0.0532 | 0.0548 | 0.0578 | 0.0745 | 0.1127 |\n| MRSVJ | 0.0256 | 0.0366 | 0.0402 | 0.0477 | 0.0665 | 0.1140 |\n| MRSVS | 0.0237 | 0.0370 | 0.0470 | 0.0563 | 0.0672 | 0.0850 |\n| MRSVJS | 0.0229 | 0.0308 | 0.0371 | 0.0448 | 0.0646 | 0.0793 |\n| **ATM (0.95 < S/K < 1.05)** | | | | | | |\n| MRSV | 0.0614 | 0.0821 | 0.0809 | 0.0903 | 0.1010 | 0.1574 |\n| MRSVJ | 0.0485 | 0.0604 | 0.0667 | 0.0740 | 0.0897 | 0.1720 |\n| MRSVS | 0.0473 | 0.0662 | 0.0808 | 0.0897 | 0.0936 | 0.1353 |\n| MRSVJS | 0.0444 | 0.0547 | 0.0629 | 0.0716 | 0.0838 | 0.1128 |\n| **ITM (1.05 < S/K)** | | | | | | |\n| MRSV | 0.0697 | 0.0949 | 0.1007 | 0.1124 | 0.1315 | 0.1931 |\n| MRSVJ | 0.0648 | 0.0775 | 0.0763 | 0.0847 | 0.1097 | 0.2079 |\n| MRSVS | 0.0688 | 0.0887 | 0.0876 | 0.1038 | 0.0987 | 0.1627 |\n| MRSVJS | 0.0600 | 0.0687 | 0.0736 | 0.0807 | 0.0908 | 0.1415 |\n\n---\n\nBased on the data in Table 1, which of the following statements are valid conclusions about the relative performance of the models?\n\nSelect all that apply.", "Options": {"A": "For short-term ATM options (30-60 days to maturity), adding a jump component (MRSVJ) reduces the pricing error by more than 25% relative to the baseline MRSV model.", "B": "The MRSVS model consistently outperforms the MRSVJ model for all options with less than 90 days to maturity.", "C": "The MRSVJS model demonstrates the lowest pricing error in every single moneyness-maturity category presented in the table.", "D": "For long-term options (>180 days to maturity), adding a jump component without a seasonal component (MRSVJ) consistently increases pricing error compared to the baseline MRSV model."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to perform detailed, multi-cell comparisons and calculations from a data table to evaluate the performance of competing financial models. Strategy: Computational Judgment. The user must verify multiple claims by referencing specific data points and performing percentage change calculations. Distractor Logic: Option A is correct, requiring comparison across all three moneyness categories for >180 days. Option B is correct, requiring a comprehensive scan of the table to confirm MRSVJS's dominance. Option C is correct, requiring a specific calculation: (0.0821 - 0.0604) / 0.0821 ≈ 26.4%. Option D is a conceptual distractor (Almost Right); it is true for OTM <30 days but false for ATM 30-60 days and other categories, requiring careful checking.", "qid": "59", "question": "### Background\n\n**Research Question.** How do different stylized facts—specifically jump risk and seasonality—contribute to the out-of-sample option pricing performance of an affine asset pricing model for natural gas, and how does this contribution vary across option moneyness and maturity?\n\n**Setting.** The paper evaluates four nested models for Henry Hub natural gas prices: a baseline Mean Reversion Stochastic Volatility (MRSV) model, and extensions that add Jumps (MRSVJ), Seasonality (MRSVS), and both (MRSVJS). The models are tested on their ability to price European call options out-of-sample.\n\n**Variables and Parameters.**\n- `S/K`: Moneyness of an option, the ratio of the spot price to the strike price.\n- OTM (Out-of-the-Money): `S/K < 0.95`\n- ATM (At-the-Money): `0.95 < S/K < 1.05`\n- ITM (In-the-Money): `S/K > 1.05`\n- Pricing Error: The absolute difference between the observed market price of an option and the theoretical price predicted by a model, `|C_market - C_model|`.\n\n---\n\n### Data / Model Specification\n\nThe out-of-sample option pricing performance of the four models is evaluated by calculating the average absolute pricing error for options categorized by moneyness and maturity. The results are summarized in Table 1 below.\n\n**Table 1: Out-of-Sample Mean Absolute Option Pricing Errors ($)**\n\n| | <30 days | 30-60 days | 60-90 days | 90-120 days | 120-180 days | >180 days |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **OTM (S/K < 0.95)** | | | | | | |\n| MRSV | 0.0370 | 0.0532 | 0.0548 | 0.0578 | 0.0745 | 0.1127 |\n| MRSVJ | 0.0256 | 0.0366 | 0.0402 | 0.0477 | 0.0665 | 0.1140 |\n| MRSVS | 0.0237 | 0.0370 | 0.0470 | 0.0563 | 0.0672 | 0.0850 |\n| MRSVJS | 0.0229 | 0.0308 | 0.0371 | 0.0448 | 0.0646 | 0.0793 |\n| **ATM (0.95 < S/K < 1.05)** | | | | | | |\n| MRSV | 0.0614 | 0.0821 | 0.0809 | 0.0903 | 0.1010 | 0.1574 |\n| MRSVJ | 0.0485 | 0.0604 | 0.0667 | 0.0740 | 0.0897 | 0.1720 |\n| MRSVS | 0.0473 | 0.0662 | 0.0808 | 0.0897 | 0.0936 | 0.1353 |\n| MRSVJS | 0.0444 | 0.0547 | 0.0629 | 0.0716 | 0.0838 | 0.1128 |\n| **ITM (1.05 < S/K)** | | | | | | |\n| MRSV | 0.0697 | 0.0949 | 0.1007 | 0.1124 | 0.1315 | 0.1931 |\n| MRSVJ | 0.0648 | 0.0775 | 0.0763 | 0.0847 | 0.1097 | 0.2079 |\n| MRSVS | 0.0688 | 0.0887 | 0.0876 | 0.1038 | 0.0987 | 0.1627 |\n| MRSVJS | 0.0600 | 0.0687 | 0.0736 | 0.0807 | 0.0908 | 0.1415 |\n\n---\n\nBased on the data in Table 1, which of the following statements are valid conclusions about the relative performance of the models?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 19, "Question": "### Background\n\nA study analyzes how the distribution of power among large shareholders affects the value of family-controlled firms in Spain. The research uses a Heckman two-stage model to control for the fact that firms are not randomly assigned to be family firms.\n\n### Data / Model Specification\n\nThe study defines three mutually exclusive states for the distribution of power in a family firm:\n\n*   `WHOCONTROLS1`: The family is the sole large shareholder.\n*   `WHOCONTROLS2`: Multiple large shareholders exist, but the family's voting rights exceed the combined rights of the next three largest shareholders (`FSH > SSH + TSH + IVSH`).\n*   `WHOCONTROLS3`: Multiple large shareholders exist, and their combined voting rights exceed the family's (`FSH < SSH + TSH + IVSH`).\n\nThe second stage of the Heckman model estimates the effect of these power structures on industry-adjusted firm value (`AVALUE`). The results are presented in the table below, where `WHOCONTROLS2` is the omitted reference category.\n\n**Table 1: Impact of Power Distribution on Family Firm Value**\n\n| Variable         | Coefficient | t-statistic |\n| :--------------- | :---------- | :---------- |\n| `WHOCONTROLS1`   | -0.47       | -2.74       |\n| `WHOCONTROLS3`   | -0.28       | -1.85       |\n*Source: Table 5, Model 3 from the paper. Both coefficients are statistically significant (p<0.10).*\n\n---\n\nBased on these results, which of the following statements are valid interpretations or conclusions?\n", "Options": {"A": "The relationship between the concentration of non-family blockholder power and firm value is linear and positive.", "B": "A state of contested control, where other blockholders' power exceeds the family's (`WHOCONTROLS3`), is valued more highly by the market than a state of sole family ownership (`WHOCONTROLS1`).", "C": "The presence of a single, unchecked family owner (`WHOCONTROLS1`) is associated with the lowest firm valuation among the three states, suggesting high agency costs.", "D": "The market valuation of family firms is highest when the family retains dominant control but coexists with other large shareholders (`WHOCONTROLS2`)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: The user must synthesize regression coefficients from a table where a reference category is omitted to correctly rank the valuation impact of three distinct corporate governance states. This tests the ability to interpret relative effects and understand the non-linear relationship implied by the results.\nDepth Strategy: Computational Judgment. The user must compare the magnitudes of negative coefficients (-0.47 and -0.28) relative to the baseline (0 for WHOCONTROLS2) to establish a rank ordering of firm value across the three states.\nDistractor Logic:\n- A (Correct): Since WHOCONTROLS2 is the reference and the other two states have negative coefficients, WHOCONTROLS2 must correspond to the highest value.\n- B (Correct): The coefficient for WHOCONTROLS1 (-0.47) is the most negative, indicating the lowest valuation.\n- C (Correct): The coefficient for WHOCONTROLS3 (-0.28) is less negative than that for WHOCONTROLS1 (-0.47), meaning firm value is higher in the WHOCONTROLS3 state than in the WHOCONTROLS1 state.\n- D (Distractor - Conceptual Opposite): The results clearly show a non-linear (concave) relationship. Value is lowest at zero blockholder power (WHOCONTROLS1), highest at intermediate power (WHOCONTROLS2), and lower again at high blockholder power (WHOCONTROLS3).", "qid": "19", "question": "### Background\n\nA study analyzes how the distribution of power among large shareholders affects the value of family-controlled firms in Spain. The research uses a Heckman two-stage model to control for the fact that firms are not randomly assigned to be family firms.\n\n### Data / Model Specification\n\nThe study defines three mutually exclusive states for the distribution of power in a family firm:\n\n*   `WHOCONTROLS1`: The family is the sole large shareholder.\n*   `WHOCONTROLS2`: Multiple large shareholders exist, but the family's voting rights exceed the combined rights of the next three largest shareholders (`FSH > SSH + TSH + IVSH`).\n*   `WHOCONTROLS3`: Multiple large shareholders exist, and their combined voting rights exceed the family's (`FSH < SSH + TSH + IVSH`).\n\nThe second stage of the Heckman model estimates the effect of these power structures on industry-adjusted firm value (`AVALUE`). The results are presented in the table below, where `WHOCONTROLS2` is the omitted reference category.\n\n**Table 1: Impact of Power Distribution on Family Firm Value**\n\n| Variable         | Coefficient | t-statistic |\n| :--------------- | :---------- | :---------- |\n| `WHOCONTROLS1`   | -0.47       | -2.74       |\n| `WHOCONTROLS3`   | -0.28       | -1.85       |\n*Source: Table 5, Model 3 from the paper. Both coefficients are statistically significant (p<0.10).*\n\n---\n\nBased on these results, which of the following statements are valid interpretations or conclusions?\n", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 102, "Question": "### Background\n\n**Research Question.** What are the main econometric challenges in identifying the true effects of inflation-driven wealth transfers on cross-sectional stock returns?\n\n**Setting.** The study uses a multivariate cross-sectional regression where firm characteristics like leverage, asset tangibility, and systematic risk are often correlated, creating potential for omitted variable bias (OVB). Furthermore, systematic risk (`β`) is not directly observable and must be estimated, introducing a potential errors-in-variables (EIV) problem, which the authors address by forming portfolios.\n\n### Data / Model Specification\n\nThe regression model is:\n  \ny_{i}=\\alpha+a_{0}\\beta_{i}+a_{1}X_{1i}+a_{2}X_{2i}+a_{3}X_{3i}+u_{i} \\quad \\text{(Eq. (1))}\n \nwhere `y` is return, `β` is risk, `X_1` is debt, and `X_2` is fixed assets. The paper's theory and results suggest `a_0 > 0` and `a_2 < 0`.\n\n**Table 1: Correlation Matrix of Explanatory Variables (Period 1964-68)**\n\n| | `β` | `X_1` | `X_2` |\n|:---|:---:|:---:|:---:|\n| **`β`** | 1 | | |\n| **`X_1`** | 0.294 | 1 | |\n| **`X_2`** | -0.370 | 0.391 | 1 |\n\n### Question\n\nConsidering the data in **Table 1** and the econometric methods described, select all correct statements regarding the study's methodological challenges.", "Options": {"A": "The portfolio grouping procedure is used to address the errors-in-variables problem stemming from the estimation of `β`, which would otherwise cause the estimate of its coefficient (`a_0`) to be biased towards zero.", "B": "Based on Table 1, omitting the fixed assets variable (`X_2`) from the regression would likely introduce a negative bias on the debt coefficient (`a_1`), as `X_1` and `X_2` are positively correlated while the true coefficient `a_2` is negative.", "C": "The portfolio grouping method is primarily designed to solve the omitted variable bias problem by ensuring the explanatory variables are orthogonal.", "D": "Based on Table 1, omitting systematic risk (`β`) would introduce a negative bias on the debt coefficient (`a_1`)."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the understanding of key econometric challenges in the paper: omitted variable bias (OVB) and errors-in-variables (EIV). It uses a Premise Packaging strategy, asking to identify correct statements about methodology. Distractors are based on traceable errors: a sign error in calculating OVB direction (distractor C) and a conceptual confusion between the purposes of EIV and OVB mitigation techniques (distractor D).", "qid": "102", "question": "### Background\n\n**Research Question.** What are the main econometric challenges in identifying the true effects of inflation-driven wealth transfers on cross-sectional stock returns?\n\n**Setting.** The study uses a multivariate cross-sectional regression where firm characteristics like leverage, asset tangibility, and systematic risk are often correlated, creating potential for omitted variable bias (OVB). Furthermore, systematic risk (`β`) is not directly observable and must be estimated, introducing a potential errors-in-variables (EIV) problem, which the authors address by forming portfolios.\n\n### Data / Model Specification\n\nThe regression model is:\n  \ny_{i}=\\alpha+a_{0}\\beta_{i}+a_{1}X_{1i}+a_{2}X_{2i}+a_{3}X_{3i}+u_{i} \\quad \\text{(Eq. (1))}\n \nwhere `y` is return, `β` is risk, `X_1` is debt, and `X_2` is fixed assets. The paper's theory and results suggest `a_0 > 0` and `a_2 < 0`.\n\n**Table 1: Correlation Matrix of Explanatory Variables (Period 1964-68)**\n\n| | `β` | `X_1` | `X_2` |\n|:---|:---:|:---:|:---:|\n| **`β`** | 1 | | |\n| **`X_1`** | 0.294 | 1 | |\n| **`X_2`** | -0.370 | 0.391 | 1 |\n\n### Question\n\nConsidering the data in **Table 1** and the econometric methods described, select all correct statements regarding the study's methodological challenges.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 81, "Question": "### Background\n\n**Research Question.** This case investigates the core of the principal-agent problem in delegated asset management: how the formal evaluation and incentive structure shapes a fund manager's objectives and behavior. It examines how the choice of benchmark dictates what managers value (relative vs. absolute return) and what motivates them (career risk vs. direct financial reward).\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, categorized by the primary type of fund they manage and their performance benchmark.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the survey findings on benchmark incidence, the perceived importance of absolute returns, and the key motivating factors for managers.\n\n**Table 1. The incidence of particular benchmarks, according to type of fund**\n\n| Performance Benchmark | Pension Fund (n=41) (%) | Private Client (n=12) (%) |\n| :--- | :---: | :---: |\n| Median fund | 71 | 20 |\n| All-share index | 42 | 70 |\n\n**Table 2. Importance of absolute performance according to type of fund**\n\n| | Pension Fund (Mean value*) | Private Client (Mean value*) | Significance |\n| :--- | :---: | :---: | :---: |\n| **Score** | 2.3 | 3.9 | 0.06 |\n\n*`1 = no importance`, `5 = extremely important`*\n\n**Table 3. Importance of motivating factors according to performance benchmark**\n\n| Motivator | Median Fund (Mean value*) | All-Share (Mean value*) | Significance |\n| :--- | :---: | :---: | :---: |\n| **Trustee approval** | **4.3** | **3.8** | **0.12** |\n| Salary | 3.0 | 3.4 | 0.44 |\n\n*`1 = no importance`, `5 = extremely important`*\n\n---\n\n### Question\n\nThe survey provides a detailed view of the agency problem in fund management. Based on the evidence presented across all three tables, select all statements that accurately describe the incentive structure and resulting behavior of UK pension fund managers.", "Options": {"A": "Private client fund managers face the most intense pressure, as they are judged against both the median fund and the all-share index simultaneously.", "B": "Pension fund managers are primarily judged against a relative benchmark (median fund), a framework that rationally leads them to de-emphasize the importance of absolute investment returns.", "C": "The data show a strong, statistically significant link between higher salary and a manager's focus on trustee approval.", "D": "For managers subject to relative benchmarking, non-financial motivators like 'Trustee approval'—a proxy for job security—are more influential than direct pay, highlighting the dominance of career risk."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an 'Atomic Decomposition' strategy to test the synthesis of the paper's entire argument about the agency problem. It requires connecting evidence across three tables to form a coherent narrative. Correct option A links the benchmark type (Table 1) to the manager's objective function (Table 2). Correct option B identifies the true driver of behavior (Table 3) and interprets it in the context of agency theory. Distractor C mischaracterizes the pressure on a different group, testing for careful reading of Table 1. Distractor D posits a relationship between variables that is not supported by the significance levels in Table 3.", "qid": "81", "question": "### Background\n\n**Research Question.** This case investigates the core of the principal-agent problem in delegated asset management: how the formal evaluation and incentive structure shapes a fund manager's objectives and behavior. It examines how the choice of benchmark dictates what managers value (relative vs. absolute return) and what motivates them (career risk vs. direct financial reward).\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, categorized by the primary type of fund they manage and their performance benchmark.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the survey findings on benchmark incidence, the perceived importance of absolute returns, and the key motivating factors for managers.\n\n**Table 1. The incidence of particular benchmarks, according to type of fund**\n\n| Performance Benchmark | Pension Fund (n=41) (%) | Private Client (n=12) (%) |\n| :--- | :---: | :---: |\n| Median fund | 71 | 20 |\n| All-share index | 42 | 70 |\n\n**Table 2. Importance of absolute performance according to type of fund**\n\n| | Pension Fund (Mean value*) | Private Client (Mean value*) | Significance |\n| :--- | :---: | :---: | :---: |\n| **Score** | 2.3 | 3.9 | 0.06 |\n\n*`1 = no importance`, `5 = extremely important`*\n\n**Table 3. Importance of motivating factors according to performance benchmark**\n\n| Motivator | Median Fund (Mean value*) | All-Share (Mean value*) | Significance |\n| :--- | :---: | :---: | :---: |\n| **Trustee approval** | **4.3** | **3.8** | **0.12** |\n| Salary | 3.0 | 3.4 | 0.44 |\n\n*`1 = no importance`, `5 = extremely important`*\n\n---\n\n### Question\n\nThe survey provides a detailed view of the agency problem in fund management. Based on the evidence presented across all three tables, select all statements that accurately describe the incentive structure and resulting behavior of UK pension fund managers.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 133, "Question": "### Background\n\n**Research Question.** Are capital levels at Credit Card Specialty Banks (CCSBs) determined by Basel I regulatory minimums, which are based on on-balance-sheet assets, or by market discipline, which considers total managed assets including off-balance-sheet securitizations?\n\n**Setting.** A cross-sectional regression analysis of 275 US banks as of June 2004. The study compares 13 CCSBs against a peer group of 262 other commercial banks, controlling for differences in risk, size, and growth.\n\n### Data / Model Specification\n\nThe study estimates two key regression models using OLS with heteroskedasticity-consistent standard errors. The results for the primary variable of interest are summarized below.\n\n**Table 1. Regression Results for Equity Capital Ratios**\n\n| Independent Variable | Model 1: Equity / Total Assets | Model 2: Equity / Managed Assets |\n| :--- | :---: | :---: |\n| **Credit card bank indicator** | **0.09623*** | **0.00917** |\n| *Standard Error* | *(0.0170)* | *(0.0127)* |\n*Note: *** denotes significance at the 1% level. Control variables for risk, size, and growth were included in both models.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following conclusions are supported by the evidence?\n\nSelect all that apply.", "Options": {"A": "When capital is measured against on-balance-sheet assets (Model 1), CCSBs hold significantly higher capital ratios than comparable non-CCSBs.", "B": "The evidence suggests that market discipline, which focuses on total managed assets, is the primary driver of capital policy at CCSBs.", "C": "When capital is measured against total managed assets (Model 2), the difference in capital ratios between CCSBs and non-CCSBs is statistically insignificant.", "D": "The Basel I regulatory capital requirements, which are based on on-balance-sheet assets, appear to be a non-binding constraint for CCSBs."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret the results of two contrasting regression models and synthesize them into a coherent conclusion about the drivers of bank capital policy. Depth Strategy: Reverse-Reasoning. The user is given the empirical results (the table) and must identify all valid conclusions that can be drawn from them. Distractor Logic: This item is designed with no distractors; all options are correct and represent a logical step in the paper's core argument. This forces the candidate to evaluate each statement's validity independently and comprehensively, testing for a full understanding of the empirical evidence rather than just identifying a single correct interpretation.", "qid": "133", "question": "### Background\n\n**Research Question.** Are capital levels at Credit Card Specialty Banks (CCSBs) determined by Basel I regulatory minimums, which are based on on-balance-sheet assets, or by market discipline, which considers total managed assets including off-balance-sheet securitizations?\n\n**Setting.** A cross-sectional regression analysis of 275 US banks as of June 2004. The study compares 13 CCSBs against a peer group of 262 other commercial banks, controlling for differences in risk, size, and growth.\n\n### Data / Model Specification\n\nThe study estimates two key regression models using OLS with heteroskedasticity-consistent standard errors. The results for the primary variable of interest are summarized below.\n\n**Table 1. Regression Results for Equity Capital Ratios**\n\n| Independent Variable | Model 1: Equity / Total Assets | Model 2: Equity / Managed Assets |\n| :--- | :---: | :---: |\n| **Credit card bank indicator** | **0.09623*** | **0.00917** |\n| *Standard Error* | *(0.0170)* | *(0.0127)* |\n*Note: *** denotes significance at the 1% level. Control variables for risk, size, and growth were included in both models.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following conclusions are supported by the evidence?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 83, "Question": "### Background\n\n**Research Question.** Which modeling approach—OLS, SWR, or Principal Component Regression (PCR)—provides the best out-of-sample predictive performance for housing prices, and what does this imply about the trade-off between dimensionality reduction and information loss?\n\n**Setting.** The predictive accuracy of four models (OLS, SWR, PCR16, and PCR44) is evaluated on a holdout sample of housing sales. The PCR16 model uses 16 factors based on an eigenvalue-one criterion, while the PCR44 model uses all 44 possible factors.\n\n---\n\n### Data / Model Specification\n\nIn-sample, the OLS and PCR44 models have identical explanatory power (`SEE` = $3,737). The PCR16 model has a lower in-sample fit (`SEE` = $5,649). Table 1 presents the out-of-sample prediction results.\n\n**Table 1: Prediction Results on Holdout Sample**\n\n| Model | MAE | RMSE | In-Sample SEE |\n| :--- | :---: | :---: | :---: |\n| OLS | 3,369 | 4,500 | 3,737 |\n| SWR | 3,409 | 4,504 | 3,709 |\n| PCR16 | 5,085 | 7,151 | 5,649 |\n| PCR44 | 3,338 | 4,497 | 3,737 |\n\n---\n\nBased on the prediction results in Table 1 and the paper's discussion, which of the following conclusions about the models' predictive performance are supported?\n", "Options": {"A": "The PCR16 model's poor out-of-sample performance suggests that factors with small eigenvalues, which explain little variance in the predictors, can still contain significant information for predicting the outcome variable.", "B": "The eigenvalue-one criterion used for the PCR16 model is shown to be the optimal strategy for maximizing out-of-sample predictive accuracy.", "C": "The PCR44 model, which uses the full set of factors, demonstrates the best predictive accuracy on the holdout sample, outperforming both OLS and SWR.", "D": "Because the OLS and PCR44 models had identical in-sample fit (SEE), their nearly identical out-of-sample performance (MAE, RMSE) was an expected outcome."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item tests the ability to interpret out-of-sample prediction metrics and understand the trade-off between dimensionality reduction and information loss in PCR. Depth Strategy: Scenario Application. The user must apply the concepts of information loss and model robustness to interpret the specific numerical results in the table. Distractor Logic: Option C is 'Almost Right' but misses the paper's subtle point that even with identical in-sample fit, PCR44's slight outperformance suggests greater robustness. Option D is a conceptual opposite; the data clearly shows PCR16 had the worst predictive accuracy, making the eigenvalue-one criterion suboptimal for prediction in this case.", "qid": "83", "question": "### Background\n\n**Research Question.** Which modeling approach—OLS, SWR, or Principal Component Regression (PCR)—provides the best out-of-sample predictive performance for housing prices, and what does this imply about the trade-off between dimensionality reduction and information loss?\n\n**Setting.** The predictive accuracy of four models (OLS, SWR, PCR16, and PCR44) is evaluated on a holdout sample of housing sales. The PCR16 model uses 16 factors based on an eigenvalue-one criterion, while the PCR44 model uses all 44 possible factors.\n\n---\n\n### Data / Model Specification\n\nIn-sample, the OLS and PCR44 models have identical explanatory power (`SEE` = $3,737). The PCR16 model has a lower in-sample fit (`SEE` = $5,649). Table 1 presents the out-of-sample prediction results.\n\n**Table 1: Prediction Results on Holdout Sample**\n\n| Model | MAE | RMSE | In-Sample SEE |\n| :--- | :---: | :---: | :---: |\n| OLS | 3,369 | 4,500 | 3,737 |\n| SWR | 3,409 | 4,504 | 3,709 |\n| PCR16 | 5,085 | 7,151 | 5,649 |\n| PCR44 | 3,338 | 4,497 | 3,737 |\n\n---\n\nBased on the prediction results in Table 1 and the paper's discussion, which of the following conclusions about the models' predictive performance are supported?\n", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 75, "Question": "### Background\n\nAn insurer is evaluating three alternative reinsurance strategies for its crop insurance portfolio, which is composed of risks from 10 different regions. The goal is to find a strategy that optimizes the trade-off between retaining risk for cost savings and spreading risk for stability. The performance of each strategy is assessed using an Asset-Liability Management (ALM) framework, simulated 5,000 times over a 30-year horizon.\n\n**Reinsurance Models:**\n- **Model A (Joint Self-Managed Pool):** All risks from participating regions are pooled and retained internally. This model avoids paying loading fees to private reinsurers but is exposed to undiversified, widespread catastrophic losses.\n- **Model B (Group Private Reinsurance):** The pool acts as a single entity to purchase private reinsurance for all its risks, transferring them to the international market. This model is protected from catastrophic events but incurs significant costs from reinsurance premium loading fees.\n- **Model C (Optimized Portfolio):** A genetic algorithm allocates crop risks into two groups: a lower-risk group is retained in the self-managed pool (like Model A), and a higher-risk group is ceded to private reinsurers (like Model B).\n\n### Data / Model Specification\n\nThe performance of the three models is evaluated based on the simulated distributions of the actuarial risk measures. Table 1 presents key statistics from these simulations at the end of the 30-year horizon, under the assumption of **zero initial surplus (U₀ = $0)**.\n\n**Table 1: Simulated Actuarial Risk Measures at End of Year 30 (U₀ = $0)**\n\n| Risk Measure | Model A (Self-Managed) | Model B (Private Reinsurance) | Model C (Optimized Portfolio) |\n| :--- | :--- | :--- | :--- |\n| Survival Probability | 35.8% | 16.7% | 33.6% |\n| Average Surplus | $1,432,003,119 | $360,857,396 | $842,471,082 |\n| 5th Percentile Surplus | -$112,903,478 | -$976,132,222 | -$552,079,233 |\n| Average Deficit at Ruin | -$157,799,946 | -$65,466,143 | -$73,023,649 |\n\n### Question\n\nBased on the model descriptions and the data in Table 1, which of the following statements accurately characterize the performance trade-offs of the pure strategies (Model A and Model B)? Select all that apply.", "Options": {"A": "Model B's primary weakness is its severe capital erosion due to high costs, demonstrated by its 5th Percentile Surplus of -$976,132,222, the most negative of all models.", "B": "Model A's primary strength is its cost efficiency, demonstrated by its Average Surplus of $1,432,003,119, which is the highest among all models.", "C": "Model B's primary strength is its robust protection against catastrophic losses, reflected in its Average Deficit at Ruin of -$65,466,143, the smallest (least severe) among all models.", "D": "Model A's primary weakness is its high cost structure, evidenced by its low Survival Probability of 35.8%."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret quantitative results from a table in the context of qualitative model descriptions. It targets the core trade-offs discussed in the paper. The depth strategy is 'Computational Judgment,' requiring the user to compare values across the table to identify strengths (best performance on a metric) and weaknesses (worst performance). Distractor C is a 'Conceptual Opposite' error; Model A's weakness is catastrophic risk, not high cost, and its low survival probability is a symptom of that risk, not its cost structure.", "qid": "75", "question": "### Background\n\nAn insurer is evaluating three alternative reinsurance strategies for its crop insurance portfolio, which is composed of risks from 10 different regions. The goal is to find a strategy that optimizes the trade-off between retaining risk for cost savings and spreading risk for stability. The performance of each strategy is assessed using an Asset-Liability Management (ALM) framework, simulated 5,000 times over a 30-year horizon.\n\n**Reinsurance Models:**\n- **Model A (Joint Self-Managed Pool):** All risks from participating regions are pooled and retained internally. This model avoids paying loading fees to private reinsurers but is exposed to undiversified, widespread catastrophic losses.\n- **Model B (Group Private Reinsurance):** The pool acts as a single entity to purchase private reinsurance for all its risks, transferring them to the international market. This model is protected from catastrophic events but incurs significant costs from reinsurance premium loading fees.\n- **Model C (Optimized Portfolio):** A genetic algorithm allocates crop risks into two groups: a lower-risk group is retained in the self-managed pool (like Model A), and a higher-risk group is ceded to private reinsurers (like Model B).\n\n### Data / Model Specification\n\nThe performance of the three models is evaluated based on the simulated distributions of the actuarial risk measures. Table 1 presents key statistics from these simulations at the end of the 30-year horizon, under the assumption of **zero initial surplus (U₀ = $0)**.\n\n**Table 1: Simulated Actuarial Risk Measures at End of Year 30 (U₀ = $0)**\n\n| Risk Measure | Model A (Self-Managed) | Model B (Private Reinsurance) | Model C (Optimized Portfolio) |\n| :--- | :--- | :--- | :--- |\n| Survival Probability | 35.8% | 16.7% | 33.6% |\n| Average Surplus | $1,432,003,119 | $360,857,396 | $842,471,082 |\n| 5th Percentile Surplus | -$112,903,478 | -$976,132,222 | -$552,079,233 |\n| Average Deficit at Ruin | -$157,799,946 | -$65,466,143 | -$73,023,649 |\n\n### Question\n\nBased on the model descriptions and the data in Table 1, which of the following statements accurately characterize the performance trade-offs of the pure strategies (Model A and Model B)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 147, "Question": "### Background\n\n**Research Question.** How does the degree of convexity in the corporate tax code affect the systematic risk (equity beta) of a firm?\n\n**Setting / Data-Generating Environment.** The study uses simulations from a structural model to quantify the impact of tax asymmetry on equity beta. The model is parameterized with baseline values, and the effect of tax asymmetry is examined for firms with different levels of operating leverage.\n\n**Variables & Parameters.**\n- `β_E`: The firm's equity beta.\n- `δ`: The level of tax asymmetry, representing tax convexity. `δ=0` is a linear tax system.\n- `X`: The firm's profit flow, an inverse proxy for leverage. Lower `X` implies higher leverage.\n\n---\n\n### Data / Model Specification\n\nThe model's simulation results for the base case parameters are presented for varying levels of tax asymmetry (`δ`) and profit (`X`). Lower profit levels, for a fixed debt payment, correspond to higher leverage.\n\n**Table 1. Equity Beta for Different Tax Asymmetry and Leverage Levels**\n\n| `δ` | Low Profit (X=2) (High Leverage) | Medium Profit (X=4) (Medium Leverage) | High Profit (X=6) (Low Leverage) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.4718 | 1.0964 | 0.9800 |\n| 0.05 | 1.5006 | 1.1152 | 1.0039 |\n| 0.10 | 1.5326 | 1.1321 | 1.0248 |\n| 0.15 | 1.5623 | 1.1474 | 1.0439 |\n| 0.20 | 1.5895 | 1.1614 | 1.0615 |\n| 0.25 | 1.6143 | 1.1742 | 1.0779 |\n| 0.30 | 1.6370 | 1.1859 | 1.0930 |\n| 0.35 | 1.6578 | 1.1968 | 1.1072 |\n\n---\n\n### Question\n\nBased on the simulation results in Table 1 and the underlying economic theory, which of the following statements are correct?", "Options": {"A": "In a linear tax system (`δ=0`), a firm's financial leverage (as proxied by `X`) has no impact on its equity beta.", "B": "The model demonstrates a positive monotonic relationship between tax asymmetry (`δ`) and equity beta (`β_E`) for all leverage levels shown.", "C": "The economic mechanism for the relationship is that a higher `δ` reduces the government's role as a risk-sharing partner in downturns, thereby increasing the risk borne by equity holders.", "D": "The absolute increase in equity beta when `δ` changes from 0.00 to 0.35 is largest for the low-leverage (High Profit, X=6) firm."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the main finding from a simulation table and connect it to the core economic intuition. It uses an Atomic Decomposition strategy, combining a direct data interpretation (A) with a theoretical explanation (B). Distractor C is a 'Numerical: Step-Omission Error' that requires a simple calculation to disprove. Distractor D is a 'Conceptual Opposite' that contradicts a basic principle of leverage shown clearly in the `δ=0` row of the table.", "qid": "147", "question": "### Background\n\n**Research Question.** How does the degree of convexity in the corporate tax code affect the systematic risk (equity beta) of a firm?\n\n**Setting / Data-Generating Environment.** The study uses simulations from a structural model to quantify the impact of tax asymmetry on equity beta. The model is parameterized with baseline values, and the effect of tax asymmetry is examined for firms with different levels of operating leverage.\n\n**Variables & Parameters.**\n- `β_E`: The firm's equity beta.\n- `δ`: The level of tax asymmetry, representing tax convexity. `δ=0` is a linear tax system.\n- `X`: The firm's profit flow, an inverse proxy for leverage. Lower `X` implies higher leverage.\n\n---\n\n### Data / Model Specification\n\nThe model's simulation results for the base case parameters are presented for varying levels of tax asymmetry (`δ`) and profit (`X`). Lower profit levels, for a fixed debt payment, correspond to higher leverage.\n\n**Table 1. Equity Beta for Different Tax Asymmetry and Leverage Levels**\n\n| `δ` | Low Profit (X=2) (High Leverage) | Medium Profit (X=4) (Medium Leverage) | High Profit (X=6) (Low Leverage) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.4718 | 1.0964 | 0.9800 |\n| 0.05 | 1.5006 | 1.1152 | 1.0039 |\n| 0.10 | 1.5326 | 1.1321 | 1.0248 |\n| 0.15 | 1.5623 | 1.1474 | 1.0439 |\n| 0.20 | 1.5895 | 1.1614 | 1.0615 |\n| 0.25 | 1.6143 | 1.1742 | 1.0779 |\n| 0.30 | 1.6370 | 1.1859 | 1.0930 |\n| 0.35 | 1.6578 | 1.1968 | 1.1072 |\n\n---\n\n### Question\n\nBased on the simulation results in Table 1 and the underlying economic theory, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 40, "Question": "### Background\n\nIn the Black-Scholes framework, the valuation of an American put option is a free-boundary problem. The domain is split into a continuation region, where the option price `p(x, τ)` follows a PDE, and an exercise region, where the price is its intrinsic value. The optimal exercise boundary, `c(τ)`, is the critical asset price that separates these regions.\n\n### Data / Model Specification\n\nIn the continuation region (`x > c(τ)`), the option price is governed by the Black-Scholes PDE:\n  \n\\frac{\\partial p}{\\partial\\tau}=\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}p}{\\partial x^{2}}+b x\\frac{\\partial p}{\\partial x}-r p \\quad \\text{(Eq. 1)}\n \nAt the optimal exercise boundary `x = c(τ)`, two conditions must hold:\n1.  **Value Matching Condition**: The option's price must equal its intrinsic value.\n      \n    p(c(\\tau), \\tau) = q - c(\\tau) \\quad \\text{(Eq. 2)}\n     \n2.  **Smooth Pasting Condition**: The option's delta must be continuous and equal to the delta of the payoff function.\n      \n    \\frac{\\partial p(c(\\tau), \\tau)}{\\partial x} = -1 \\quad \\text{(Eq. 3)}\n     \n\n---\n\nBased on the provided model, which of the following statements are valid consequences of applying the boundary conditions to the Black-Scholes PDE at the exercise boundary `x = c(τ)`? Select all that apply.", "Options": {"A": "The time decay of the option price at the boundary, `∂p/∂τ`, is equal to the risk-free rate `r` times the option's intrinsic value `(q - c(τ))`.", "B": "The option's Gamma at the boundary, `∂²p/∂x²`, is given by `(2 / (σ²c(τ)²)) * (rq - (r-b)c(τ))`.", "C": "The option's Gamma at the boundary, `∂²p/∂x²`, must be zero to ensure a smooth transition to the exercise region.", "D": "The time decay of the option price at the boundary, `∂p/∂τ`, is zero."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: Test the user's ability to derive the direct consequences of the value matching and smooth pasting conditions when applied to the Black-Scholes PDE at the free boundary. This is a foundational step for deriving the main boundary evolution equation.\nDepth Strategy: Computational Judgment. The user must perform a multi-step derivation to validate options A and B.\nDistractor Logic:\n- C (Conceptual Opposite): This is a common misconception. Gamma is continuous at the boundary but is generally non-zero. The Gamma in the exercise region (for x < c(τ)) is zero, but the smooth pasting condition applies to the first derivative (Delta), not the second (Gamma).\n- D (Step-Omission Error): This distractor arises if the user incorrectly evaluates the PDE at the boundary, ignoring the Gamma and Delta terms and only keeping the `rp` term, leading to `∂p/∂τ = -rp`.", "qid": "40", "question": "### Background\n\nIn the Black-Scholes framework, the valuation of an American put option is a free-boundary problem. The domain is split into a continuation region, where the option price `p(x, τ)` follows a PDE, and an exercise region, where the price is its intrinsic value. The optimal exercise boundary, `c(τ)`, is the critical asset price that separates these regions.\n\n### Data / Model Specification\n\nIn the continuation region (`x > c(τ)`), the option price is governed by the Black-Scholes PDE:\n  \n\\frac{\\partial p}{\\partial\\tau}=\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}p}{\\partial x^{2}}+b x\\frac{\\partial p}{\\partial x}-r p \\quad \\text{(Eq. 1)}\n \nAt the optimal exercise boundary `x = c(τ)`, two conditions must hold:\n1.  **Value Matching Condition**: The option's price must equal its intrinsic value.\n      \n    p(c(\\tau), \\tau) = q - c(\\tau) \\quad \\text{(Eq. 2)}\n     \n2.  **Smooth Pasting Condition**: The option's delta must be continuous and equal to the delta of the payoff function.\n      \n    \\frac{\\partial p(c(\\tau), \\tau)}{\\partial x} = -1 \\quad \\text{(Eq. 3)}\n     \n\n---\n\nBased on the provided model, which of the following statements are valid consequences of applying the boundary conditions to the Black-Scholes PDE at the exercise boundary `x = c(τ)`? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B", "model_answer": ["B"], "error": null}}
{"ID": 77, "Question": "### Background\n\n**Research Question.** This case provides a comprehensive examination of agency-induced short-termism in delegated asset management. It investigates the entire causal chain, from fund managers' *perception* that performance monitoring creates pressure, to the *behavioral evidence* of shortened investment horizons, and finally to the direct *correlates* of this behavior.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of 64 UK fund managers. For some analyses, the focus is on the subsample of pension fund managers, who are identified as being subject to the most intense degree of relative performance monitoring. The average holding period of a UK equity is used as a proxy for the manager's investment time horizon.\n\n---\n\n### Data / Model Specification\n\nThe following tables present evidence on managers' perceptions of monitoring, their reported holding periods, and the correlation of those holding periods with incentive structures.\n\n**Table 1. Attitudes towards the influence of external performance measurement**\n\n| To what extent does external performance measurement: | Not at all (%) | Slight (%) | Moderate (%) | Considerable (%) | Extreme (%) |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| ...influence the management of a fund | 25.0 | 9.4 | 17.2 | 23.4 | 25.0 |\n| ...result in a short-term approach | 18.8 | 15.6 | 21.9 | 28.1 | 15.6 |\n\n**Table 2. UK equities: interviewees’ estimates of average holding periods (in months) according to principal type of fund managed**\n\n| | Pension Fund | Life Fund | Private Client | Kruskal-Wallis (chi²) | Significance |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| **Median** | **36** | **48** | **60** | **5.55** | **0.06** |\n\n**Table 3. Correlation of estimates of average holding periods (in months) for pension fund managers**\n\n| Correlation between average holding period (in months) and: | Correlation Coefficient | Test | Significance |\n| :--- | :---: | :---: | :---: |\n| frequency of performance monitoring | -0.1184 | S | 0.217 |\n| proportion of pay composed of bonus | -0.1760 | P | 0.121 |\n\n---\n\n### Question\n\nBased on the survey results presented, which of the following conclusions about the link between performance monitoring and fund manager behavior are supported by the data?", "Options": {"A": "Pension funds, the group subject to the most intense relative monitoring, exhibit a median equity holding period (36 months) that is statistically significantly shorter than that of other fund types.", "B": "The data show that more frequent performance monitoring leads to significantly longer holding periods, as managers take a more considered approach.", "C": "For pension fund managers, a higher proportion of performance-related bonus pay is significantly correlated with shorter holding periods.", "D": "A substantial portion of managers (over 40%) perceive that external performance measurement directly encourages a short-term approach to fund management."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the core empirical findings linking perception, behavior, and incentives. It uses an 'Atomic Decomposition' strategy, breaking the original QA's multi-part question into distinct, verifiable statements. Correct options A and B require correct interpretation of percentages from Table 1 and the median and significance from Table 2. Distractor C is an 'Almost Right' option that tests attention to statistical significance in Table 3. Distractor D is a 'Conceptual Opposite' that tests understanding of the hypothesized relationship's direction.", "qid": "77", "question": "### Background\n\n**Research Question.** This case provides a comprehensive examination of agency-induced short-termism in delegated asset management. It investigates the entire causal chain, from fund managers' *perception* that performance monitoring creates pressure, to the *behavioral evidence* of shortened investment horizons, and finally to the direct *correlates* of this behavior.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of 64 UK fund managers. For some analyses, the focus is on the subsample of pension fund managers, who are identified as being subject to the most intense degree of relative performance monitoring. The average holding period of a UK equity is used as a proxy for the manager's investment time horizon.\n\n---\n\n### Data / Model Specification\n\nThe following tables present evidence on managers' perceptions of monitoring, their reported holding periods, and the correlation of those holding periods with incentive structures.\n\n**Table 1. Attitudes towards the influence of external performance measurement**\n\n| To what extent does external performance measurement: | Not at all (%) | Slight (%) | Moderate (%) | Considerable (%) | Extreme (%) |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| ...influence the management of a fund | 25.0 | 9.4 | 17.2 | 23.4 | 25.0 |\n| ...result in a short-term approach | 18.8 | 15.6 | 21.9 | 28.1 | 15.6 |\n\n**Table 2. UK equities: interviewees’ estimates of average holding periods (in months) according to principal type of fund managed**\n\n| | Pension Fund | Life Fund | Private Client | Kruskal-Wallis (chi²) | Significance |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| **Median** | **36** | **48** | **60** | **5.55** | **0.06** |\n\n**Table 3. Correlation of estimates of average holding periods (in months) for pension fund managers**\n\n| Correlation between average holding period (in months) and: | Correlation Coefficient | Test | Significance |\n| :--- | :---: | :---: | :---: |\n| frequency of performance monitoring | -0.1184 | S | 0.217 |\n| proportion of pay composed of bonus | -0.1760 | P | 0.121 |\n\n---\n\n### Question\n\nBased on the survey results presented, which of the following conclusions about the link between performance monitoring and fund manager behavior are supported by the data?", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 69, "Question": "### Background\n\nIn a competitive insurance market where customer age is unobservable (`θ=0`), conventional firms cannot observe a new customer's risk type. Due to adverse selection, the pool of applicants is riskier than the general population. To compete, firms may engage in \"lowballing\"—setting an initial price `p^c` that is potentially unprofitable in the first period.\n\n### Data / Model Specification\n\nIn equilibrium, the expected lifetime profit `Π^c` for a conventional insurer from a new customer must be zero. This profit consists of two parts: the expected profit/loss in the first period, and the present value of all future expected profits, known as \"information rents.\" These rents are generated by identifying and retaining profitable customers over time.\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the economics of a conventional insurer's pricing strategy in this equilibrium? (Select all that apply)", "Options": {"A": "Firms may offer an initial price `p^c` that is below the expected first-period cost, a strategy known as 'lowballing'.", "B": "An increase in the probability `α` of learning a customer's type unambiguously increases a conventional firm's lifetime profit for any given price `p^c`.", "C": "The viability of lowballing depends on future 'information rents' earned from retaining profitable low-risk customers after their type is learned.", "D": "An increase in the firm's time discount factor `β` (i.e., becoming more patient) decreases the present value of information rents, making lowballing less attractive."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the conceptual understanding of the dynamic pricing strategy at the heart of the model. The correct options (A, B) define 'lowballing' and correctly link it to 'information rents'. Distractor C is a Conceptual Opposite regarding the effect of the discount factor `β`. Distractor D is an 'Almost Right' error; while higher `α` increases future rents, it also worsens adverse selection in the `θ=0` case, so the net effect on profit is ambiguous and is the central topic of the paper.", "qid": "69", "question": "### Background\n\nIn a competitive insurance market where customer age is unobservable (`θ=0`), conventional firms cannot observe a new customer's risk type. Due to adverse selection, the pool of applicants is riskier than the general population. To compete, firms may engage in \"lowballing\"—setting an initial price `p^c` that is potentially unprofitable in the first period.\n\n### Data / Model Specification\n\nIn equilibrium, the expected lifetime profit `Π^c` for a conventional insurer from a new customer must be zero. This profit consists of two parts: the expected profit/loss in the first period, and the present value of all future expected profits, known as \"information rents.\" These rents are generated by identifying and retaining profitable customers over time.\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the economics of a conventional insurer's pricing strategy in this equilibrium? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 28, "Question": "### Background\n\n**Research Question.** Can a trading rule based on identifying properties with large negative residuals from a hedonic pricing model generate superior risk-adjusted returns, and what does this imply for real estate market efficiency?\n\n**Setting.** An empirical test using portfolios of apartment buildings sorted by their hedonic pricing error (`e`). Performance is evaluated by comparing Sharpe ratios, and using the Jobson-Korkie (`z_{Sh}`) test for statistical significance against the total sample of properties.\n\n**Variables and Parameters.**\n- `Sharpe Ratio`: A measure of risk-adjusted return, calculated as `Mean Excess Return / Std. Dev.`.\n- `z_{Sh}`: Jobson-Korkie test statistic for the difference in Sharpe ratios.\n\n---\n\n### Data / Model Specification\n\nTo formally test for a difference in risk-adjusted performance, the Jobson-Korkie test was used. The null hypothesis is that the Sharpe ratios of the two portfolios are equal.\n\n**Table 1.** Risk-Adjusted Performance and Statistical Test (1971-1980).\n\n| Portfolio | Sharpe Ratio | `z_{Sh}` vs. Total Sample |\n| :--- | :--- | :--- |\n| Total sample | 0.88 | -- |\n| `e ≤ -0.10` | 0.75 | 0.28 |\n\n---\n\n### Question\n\nGiven the risk-adjusted performance metrics and the formal statistical test results in **Table 1**, which of the following conclusions are supported by the paper's analysis? (Use a 5% significance level, where the two-sided critical z-value is approximately 1.96).", "Options": {"A": "The statistical test fails to reject the null hypothesis, meaning there is no significant evidence that the trading rule generates abnormal risk-adjusted returns.", "B": "The paper's final conclusion is that while the trading rule identifies properties with high raw returns, this outperformance disappears after adjusting for risk, consistent with an efficient market.", "C": "The `z_Sh` statistic of 0.28 indicates that the 'undervalued' portfolio's risk-adjusted performance is statistically superior to the total sample's.", "D": "The findings provide strong evidence against the semi-strong form of market efficiency in the Vancouver apartment market."}, "Answer": ["A", "B"], "pi_justification": "This item tests the ability to interpret a formal statistical test (the Jobson-Korkie test) and connect its result to the broader economic concept of market efficiency. It uses a **Reverse-Reasoning** strategy, asking the user to infer the correct theoretical conclusions from the empirical evidence. Distractor A represents a fundamental misinterpretation of statistical significance. Distractor C is a conceptual opposite of the paper's main finding.", "qid": "28", "question": "### Background\n\n**Research Question.** Can a trading rule based on identifying properties with large negative residuals from a hedonic pricing model generate superior risk-adjusted returns, and what does this imply for real estate market efficiency?\n\n**Setting.** An empirical test using portfolios of apartment buildings sorted by their hedonic pricing error (`e`). Performance is evaluated by comparing Sharpe ratios, and using the Jobson-Korkie (`z_{Sh}`) test for statistical significance against the total sample of properties.\n\n**Variables and Parameters.**\n- `Sharpe Ratio`: A measure of risk-adjusted return, calculated as `Mean Excess Return / Std. Dev.`.\n- `z_{Sh}`: Jobson-Korkie test statistic for the difference in Sharpe ratios.\n\n---\n\n### Data / Model Specification\n\nTo formally test for a difference in risk-adjusted performance, the Jobson-Korkie test was used. The null hypothesis is that the Sharpe ratios of the two portfolios are equal.\n\n**Table 1.** Risk-Adjusted Performance and Statistical Test (1971-1980).\n\n| Portfolio | Sharpe Ratio | `z_{Sh}` vs. Total Sample |\n| :--- | :--- | :--- |\n| Total sample | 0.88 | -- |\n| `e ≤ -0.10` | 0.75 | 0.28 |\n\n---\n\n### Question\n\nGiven the risk-adjusted performance metrics and the formal statistical test results in **Table 1**, which of the following conclusions are supported by the paper's analysis? (Use a 5% significance level, where the two-sided critical z-value is approximately 1.96).", "model_evaluation_result": {"model_raw_response": "A", "model_answer": ["A"], "error": null}}
{"ID": 79, "Question": "### Background\n\n**Research Question.** This case challenges the canonical definitions of risk from Modern Portfolio Theory (MPT) and the Capital Asset Pricing Model (CAPM). It investigates what professional fund managers—operating as agents within an organizational context—actually perceive as risk, and how this perception is shaped by their evaluation benchmark.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers. Respondents were asked to rate the importance of various risk factors.\n\n---\n\n### Data / Model Specification\n\nThe following tables report the percentage of fund managers who rated different risk factors as 'important', first broken down by fund type and then by performance benchmark.\n\n**Table 1. Attitudes to risk according to type of fund (% rating factor as 'important')**\n\n| Risk Factor | Total Sample (%) | Pension Fund (%) | Private Client (%) |\n| :--- | :---: | :---: | :---: |\n| Commercial risk | 40 | 51 | 25 |\n| Asset-liability matching | 79 | 61 | 75 |\n| Total variability of return | 29 | 27 | 17 |\n| Beta of portfolio | 14 | 19 | 8 |\n\n**Table 2. Attitudes to risk measures according to performance benchmark (% rating factor as 'important')**\n\n| Risk Factor | Median Fund (n=23) (%) | All-Share Index (n=15) (%) | None (n=8) (%) | Kruskal-Wallis Significance |\n| :--- | :---: | :---: | :---: | :---: |\n| **Commercial risk** | **54** | **21** | **25** | **0.07** |\n| Asset-liability matching | 79 | 64 | 75 | 0.46 |\n\n---\n\n### Question\n\nThe survey data reveals a significant divergence between practitioner and academic definitions of risk. Based on the tables, which of the following statements accurately characterize fund managers' attitudes toward risk?", "Options": {"A": "Asset-liability matching is considered the most important risk by a vast majority of managers, regardless of their fund type or performance benchmark.", "B": "The canonical CAPM risk measure, beta, is the second most important risk factor for pension fund managers after asset-liability matching.", "C": "The perceived importance of 'commercial risk' is significantly higher for managers evaluated against a relative benchmark (median fund) than for those with an absolute or no benchmark.", "D": "Private client fund managers, who are not subject to intense relative monitoring, view 'total variability of return' as their primary risk."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an 'Atomic Decomposition' strategy to test comprehension of the paper's key findings on risk perception. It requires synthesizing data from two tables. Correct option A identifies the dominant risk factor across all groups. Correct option B identifies the key statistically significant difference related to benchmarking. Distractor C is an 'Almost Right' option that incorrectly ranks the importance of risk factors from Table 1. Distractor D misattributes the primary risk for a specific subgroup, testing for careful reading of Table 1.", "qid": "79", "question": "### Background\n\n**Research Question.** This case challenges the canonical definitions of risk from Modern Portfolio Theory (MPT) and the Capital Asset Pricing Model (CAPM). It investigates what professional fund managers—operating as agents within an organizational context—actually perceive as risk, and how this perception is shaped by their evaluation benchmark.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers. Respondents were asked to rate the importance of various risk factors.\n\n---\n\n### Data / Model Specification\n\nThe following tables report the percentage of fund managers who rated different risk factors as 'important', first broken down by fund type and then by performance benchmark.\n\n**Table 1. Attitudes to risk according to type of fund (% rating factor as 'important')**\n\n| Risk Factor | Total Sample (%) | Pension Fund (%) | Private Client (%) |\n| :--- | :---: | :---: | :---: |\n| Commercial risk | 40 | 51 | 25 |\n| Asset-liability matching | 79 | 61 | 75 |\n| Total variability of return | 29 | 27 | 17 |\n| Beta of portfolio | 14 | 19 | 8 |\n\n**Table 2. Attitudes to risk measures according to performance benchmark (% rating factor as 'important')**\n\n| Risk Factor | Median Fund (n=23) (%) | All-Share Index (n=15) (%) | None (n=8) (%) | Kruskal-Wallis Significance |\n| :--- | :---: | :---: | :---: | :---: |\n| **Commercial risk** | **54** | **21** | **25** | **0.07** |\n| Asset-liability matching | 79 | 64 | 75 | 0.46 |\n\n---\n\n### Question\n\nThe survey data reveals a significant divergence between practitioner and academic definitions of risk. Based on the tables, which of the following statements accurately characterize fund managers' attitudes toward risk?", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 87, "Question": "### Background\n\nTo isolate the causal impact of the 2000 pro-union legislation, the study compares the abnormal returns of affected New Zealand firms (\"Sample firms\") to a \"Control group\" of NZX-listed firms that operate overseas and are therefore not subject to the law. This difference-in-differences approach aims to control for concurrent market-wide shocks affecting all firms listed on the New Zealand exchange (NZX).\n\n### Data / Model Specification\n\nThe table below shows the combined 3-day Average Cumulative Abnormal Return (ACAR) for the five `Aggressive` legislative events for both the sample and control groups.\n\n**Table 1: Combined 3-Day ACAR for Aggressive Events**\n\n| Group                    | 3-Day ACAR | BMP t-statistic |\n| :----------------------- | :--------: | :-------------: |\n| Panel A: Sample Firms    | -0.044     | [-3.06]***      |\n| Panel B: Control Firms   | -0.012     | [-1.20]         |\n\n*Note: *** denotes significance at the 1% level.*\n\n---\n\nBased on the research design and the data in Table 1, which of the following statements are **NOT** valid conclusions?", "Options": {"A": "The control group experienced a statistically significant negative return, indicating a broad market downturn was the primary driver of the sample firms' negative returns.", "B": "The difference-in-differences estimate of the legislation's impact is -0.032, or -3.2%.", "C": "The use of the control group helps to isolate the legislation's effect from confounding factors specific to the New Zealand capital market.", "D": "The results suggest that the legislation had no impact, as both the sample and control firms experienced negative returns."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item assesses the ability to calculate and interpret a difference-in-differences estimate from a table and understand the causal logic of using a control group. Depth Strategy: Computational Judgment and Reverse-Reasoning. The user must first calculate the DiD estimate and then evaluate the validity of several conclusions based on that calculation and the research design. Inversion Protocol: The core assessment objective is to correctly calculate and interpret the DiD estimate (-3.2%) and understand the research design's logic. This leads to two unique correct conclusions (A and C). To meet the '≥2 correct answers' rule, the Single-Correct-Answer Inversion Protocol was applied. The stem was changed to ask for what is 'NOT' a valid conclusion, making the original distractors (B and D) the new correct answers.", "qid": "87", "question": "### Background\n\nTo isolate the causal impact of the 2000 pro-union legislation, the study compares the abnormal returns of affected New Zealand firms (\"Sample firms\") to a \"Control group\" of NZX-listed firms that operate overseas and are therefore not subject to the law. This difference-in-differences approach aims to control for concurrent market-wide shocks affecting all firms listed on the New Zealand exchange (NZX).\n\n### Data / Model Specification\n\nThe table below shows the combined 3-day Average Cumulative Abnormal Return (ACAR) for the five `Aggressive` legislative events for both the sample and control groups.\n\n**Table 1: Combined 3-Day ACAR for Aggressive Events**\n\n| Group                    | 3-Day ACAR | BMP t-statistic |\n| :----------------------- | :--------: | :-------------: |\n| Panel A: Sample Firms    | -0.044     | [-3.06]***      |\n| Panel B: Control Firms   | -0.012     | [-1.20]         |\n\n*Note: *** denotes significance at the 1% level.*\n\n---\n\nBased on the research design and the data in Table 1, which of the following statements are **NOT** valid conclusions?", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 42, "Question": "### Background\n\nWhen extending the American option pricing model to include stochastic volatility, the pricing PDE becomes more complex, incorporating additional terms to account for the new source of risk and its correlation with the asset price.\n\n### Data / Model Specification\n\nThe pricing PDE for an American option `p(x, y, τ)` in a general stochastic volatility framework is:\n  \n\\frac{\\partial p}{\\partial\\tau} = \\frac{1}{2}x^{2}f(y)^{2}\\frac{\\partial^{2}p}{\\partial x^{2}} + \\frac{1}{2}\\lambda(y)^{2}\\frac{\\partial^{2}p}{\\partial y^{2}} + \\rho\\lambda(y)f(y)x\\frac{\\partial^{2}p}{\\partial x\\partial y} + r x\\frac{\\partial p}{\\partial x} + \\eta(y)\\frac{\\partial p}{\\partial y} - r p \\quad \\text{(Eq. 1)}\n \nwhere `y` is the volatility factor, `f(y)` is asset volatility, `λ(y)` is the 'vol of vol', `ρ` is the correlation between asset and volatility shocks, and `η(y)` is the risk-neutral drift of volatility.\n\n---\n\nWhich of the following statements provide an INCORRECT financial interpretation of the terms in the stochastic volatility PDE? Select all that apply.", "Options": {"A": "The term `ρλ(y)f(y)x(∂²p/∂x∂y)` captures the pricing impact of the correlation between asset price and volatility movements.", "B": "The term `η(y)(∂p/∂y)` captures the impact of the expected trend in volatility on the option's price, where `∂p/∂y` is the option's Vega.", "C": "The term `(1/2)λ(y)²(∂²p/∂y²)` represents the option's sensitivity to the risk-neutral drift of the volatility process.", "D": "The term `(1/2)x²f(y)²(∂²p/∂x²)` represents the option's sensitivity to the 'volatility of volatility'."}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: Test the user's ability to correctly identify and interpret the new terms that appear in the pricing PDE under stochastic volatility.\nDepth Strategy: Reverse-Reasoning / Conceptual Judgment. The user must evaluate the correctness of several interpretations.\nInversion Protocol: The initial question, \"Select all correct interpretations,\" would have yielded only two correct answers (B and C). To meet the mission requirement of having at least two correct answers in the key, the Single-Correct-Answer Inversion Protocol was triggered. The stem was changed to ask for INCORRECT interpretations, making the original distractors (A and D) the new correct answers.\nDistractor Logic (Post-Inversion):\n- B (Now a distractor): This is a correct interpretation. `η(y)` is the risk-neutral drift and `∂p/∂y` is Vega.\n- C (Now a distractor): This is a correct interpretation. The term contains the correlation `ρ` and the cross-derivative `∂²p/∂x∂y` (Vanna).\n- A (Now a correct answer): This is an incorrect interpretation. This term represents the convexity adjustment for volatility risk (related to 'Volga' or 'Vomma'), not the sensitivity to the drift.\n- D (Now a correct answer): This is an incorrect interpretation. This is the standard Gamma term, representing convexity with respect to the asset price `x`. The sensitivity to 'vol of vol' is captured by the `λ(y)` term.", "qid": "42", "question": "### Background\n\nWhen extending the American option pricing model to include stochastic volatility, the pricing PDE becomes more complex, incorporating additional terms to account for the new source of risk and its correlation with the asset price.\n\n### Data / Model Specification\n\nThe pricing PDE for an American option `p(x, y, τ)` in a general stochastic volatility framework is:\n  \n\\frac{\\partial p}{\\partial\\tau} = \\frac{1}{2}x^{2}f(y)^{2}\\frac{\\partial^{2}p}{\\partial x^{2}} + \\frac{1}{2}\\lambda(y)^{2}\\frac{\\partial^{2}p}{\\partial y^{2}} + \\rho\\lambda(y)f(y)x\\frac{\\partial^{2}p}{\\partial x\\partial y} + r x\\frac{\\partial p}{\\partial x} + \\eta(y)\\frac{\\partial p}{\\partial y} - r p \\quad \\text{(Eq. 1)}\n \nwhere `y` is the volatility factor, `f(y)` is asset volatility, `λ(y)` is the 'vol of vol', `ρ` is the correlation between asset and volatility shocks, and `η(y)` is the risk-neutral drift of volatility.\n\n---\n\nWhich of the following statements provide an INCORRECT financial interpretation of the terms in the stochastic volatility PDE? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 8, "Question": "### Background\n\n**Research Question.** This case examines the theoretical requirements for testing a conditional asset pricing model where a factor's price of risk varies over time.\n\n**Setting / Data-Generating Environment.** A researcher proposes a conditional single-factor model where the expected excess return on asset `i` depends on its constant beta (`\\beta_i`) and a time-varying risk premium (`\\lambda_t`):\n  \nE_t[R_{i,t+1}^e] = \\beta_i \\lambda_t\n \nThey further model the risk premium as a linear function of a predictive instrument `Z_t` (e.g., lagged El Niño), which is known at time `t`:\n  \n\\lambda_t = \\gamma_0 + \\gamma_1 Z_t\n \nThis model is typically tested using the Generalized Method of Moments (GMM).\n\n---\n\n### Question\n\nRegarding the specification and testing of this conditional factor model, select all of the following statements that are correct.", "Options": {"A": "The GMM test of this model relies on moment conditions which state that the pricing error is unpredictable using instruments `W_t` from the time `t` information set.", "B": "If the number of GMM moment conditions is greater than the number of parameters, the model is untestable due to underidentification.", "C": "In this model, the term `\\lambda_t` represents the time-varying expected compensation an investor receives for bearing one unit of the factor risk associated with `\\beta_i`.", "D": "The asset's beta (`\\beta_i`) in this model is time-varying and is a linear function of the instrument `Z_t`."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of advanced econometric theory from the source QA. It uses the Premise/Assumption Packaging strategy to test the key concepts of conditional asset pricing models. Option A correctly defines the time-varying risk premium. Option B correctly states the fundamental principle behind GMM testing in this context. Distractor C is a Conceptual Error, confusing a time-varying risk premium with a time-varying beta. Distractor D is a Conceptual Opposite, as more moments than parameters leads to an *overidentified* system that is testable via the J-statistic.", "qid": "8", "question": "### Background\n\n**Research Question.** This case examines the theoretical requirements for testing a conditional asset pricing model where a factor's price of risk varies over time.\n\n**Setting / Data-Generating Environment.** A researcher proposes a conditional single-factor model where the expected excess return on asset `i` depends on its constant beta (`\\beta_i`) and a time-varying risk premium (`\\lambda_t`):\n  \nE_t[R_{i,t+1}^e] = \\beta_i \\lambda_t\n \nThey further model the risk premium as a linear function of a predictive instrument `Z_t` (e.g., lagged El Niño), which is known at time `t`:\n  \n\\lambda_t = \\gamma_0 + \\gamma_1 Z_t\n \nThis model is typically tested using the Generalized Method of Moments (GMM).\n\n---\n\n### Question\n\nRegarding the specification and testing of this conditional factor model, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "\n\nA,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 155, "Question": "### Background\n\n**Research Question.** This case investigates whether an unexpected decline in asset market liquidity was a primary, causal factor in the financial distress of firms that had undergone highly leveraged recapitalizations (recaps).\n\n**Setting / Data-Generating Environment.** For the nine firms that ultimately entered financial distress, the study estimates the total cash shortfall from asset sales and compares it to the cash required to avoid default in the year of distress.\n\n### Data / Model Specification\n\n- The **Total Asset Sale Shortfall (`S`)** is the difference between the expected and actual proceeds from asset sales, estimated from market reactions and press reports.\n- The **Additional Cash Required (`C_req,d`)** is the firm's debt obligations minus its net cash flow for the year.\n\n**Table 1: Comparison of Asset Sale Shortfall and Cash Required to Avoid Default ($ millions)**\n| Firm Name | Total Asset Sale Shortfall (`S`) | Additional Cash Required (`C_req,d`) |\n|---|:---:|:---:|\n| Carter Hawley Hale | $612.6 | $38.6 |\n| Goodyear | $489.1 | $139.5 |\n| Harcourt Brace Jovanovich | $924.9 | $243.6 |\n| Holiday | $35.2 | $-33.7 |\n| Interco | $288.5 | $223.1 |\n| Quantum Chemical | $140.7 | $164.4 |\n| Standard Brands Paint | $0.0 | $44.6 |\n| Swank | $0.0 | $8.3 |\n| USG | $54.5 | $351.0 |\n\n*Source: Adapted from Table 4 of the study.* \n\n### Question\n\nAccording to the data in **Table 1**, for which of the following firms was the total asset sale shortfall greater than or equal to the additional cash required to avoid default, providing direct quantitative evidence that asset illiquidity could have been the cause of their distress?\n", "Options": {"A": "USG", "B": "Quantum Chemical", "C": "Carter Hawley Hale", "D": "Interco"}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: This item tests the ability to perform a quantitative comparison to verify a central causal claim of the paper: that the magnitude of the asset sale shortfall was sufficient to cause default.\nStrategy: Domain Enumeration. The user must apply a specific condition (S >= C_req,d) to the dataset and select all firms that satisfy it.\nDistractor Logic:\n- C (Quantum Chemical) is a high-fidelity 'Almost Right' distractor. The paper notes its shortfall was ~85% of the cash needed, making it a very close but incorrect choice.\n- D (USG) is a clear Conceptual Opposite, where the cash required far exceeded the shortfall, making it an easy case to rule out if the core concept is understood.", "qid": "155", "question": "### Background\n\n**Research Question.** This case investigates whether an unexpected decline in asset market liquidity was a primary, causal factor in the financial distress of firms that had undergone highly leveraged recapitalizations (recaps).\n\n**Setting / Data-Generating Environment.** For the nine firms that ultimately entered financial distress, the study estimates the total cash shortfall from asset sales and compares it to the cash required to avoid default in the year of distress.\n\n### Data / Model Specification\n\n- The **Total Asset Sale Shortfall (`S`)** is the difference between the expected and actual proceeds from asset sales, estimated from market reactions and press reports.\n- The **Additional Cash Required (`C_req,d`)** is the firm's debt obligations minus its net cash flow for the year.\n\n**Table 1: Comparison of Asset Sale Shortfall and Cash Required to Avoid Default ($ millions)**\n| Firm Name | Total Asset Sale Shortfall (`S`) | Additional Cash Required (`C_req,d`) |\n|---|:---:|:---:|\n| Carter Hawley Hale | $612.6 | $38.6 |\n| Goodyear | $489.1 | $139.5 |\n| Harcourt Brace Jovanovich | $924.9 | $243.6 |\n| Holiday | $35.2 | $-33.7 |\n| Interco | $288.5 | $223.1 |\n| Quantum Chemical | $140.7 | $164.4 |\n| Standard Brands Paint | $0.0 | $44.6 |\n| Swank | $0.0 | $8.3 |\n| USG | $54.5 | $351.0 |\n\n*Source: Adapted from Table 4 of the study.* \n\n### Question\n\nAccording to the data in **Table 1**, for which of the following firms was the total asset sale shortfall greater than or equal to the additional cash required to avoid default, providing direct quantitative evidence that asset illiquidity could have been the cause of their distress?\n", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": ["C"], "error": null}}
{"ID": 96, "Question": "### Background\n\n**Research Question.** Is the relationship between foreign bank entry and a country's gross domestic investment non-linear, and what is the nature of this relationship?\n\n**Setting & Data.** A cross-sectional regression analysis for 54 countries using data averaged over 1990–1997. The analysis tests a quadratic specification for the effect of foreign bank penetration on the investment-to-GDP ratio.\n\n### Data / Model Specification\n\nThe non-linear estimating equation is given by:\n  \nI = A + \\alpha_{1}LGDP + \\alpha_{2}GRO + \\alpha_{3}FBSASS + \\alpha_{4}LGDP^{2} + \\alpha_{5}GRO^{2} + \\alpha_{6}FBSASS^{2} + \\varepsilon \\quad \\text{(Eq. (1))}\n \nwhere `I` is the investment-to-GDP ratio, `FBSASS` is the share of foreign bank assets in total bank assets, and `LGDP` and `GRO` are controls.\n\n**Table 1. Baseline Estimation Results**\n\n| Variable | Coefficient (t-stat) |\n|:---|:---:|\n| `FBSASS` (α₃) | -14.24 (-2.38) |\n| `FBSASS²` (α₆) | 22.84 (3.46) |\n\n*Notes: The results are from Table 4, column (2a) of the paper.*\n\n### Question\n\nBased on the model specification in **Eq. (1)** and the results in **Table 1**, select all of the following statements that are correct interpretations of the findings.", "Options": {"A": "The calculated turning point of the U-curve is approximately 0.62, derived from the ratio of the coefficients (-α₃ / α₆).", "B": "The calculated turning point of the U-curve, where the marginal effect of `FBSASS` on investment becomes positive, is approximately 0.31.", "C": "The model implies that for a country with a low foreign bank share (e.g., `FBSASS` = 0.10), a small increase in that share is associated with a decrease in the investment ratio.", "D": "The statistically significant positive coefficient on `FBSASS²` implies that the marginal effect of foreign bank entry on investment is always positive."}, "Answer": ["B", "C"], "pi_justification": "REPLACE with Multiple Choice (Table QA rule). This item assesses the core quantitative skill of calculating and interpreting the turning point of a quadratic model. It uses a Computational Judgment strategy. The correct options (A, B) are an atomic decomposition of the main finding, testing both the calculation (`-α₃ / (2α₆)`) and its direct implication for the marginal effect. Distractor C is a high-fidelity Formula Misuse error, omitting the factor of 2 from the denominator. Distractor D represents a common Conceptual error in interpreting quadratic terms in isolation.", "qid": "96", "question": "### Background\n\n**Research Question.** Is the relationship between foreign bank entry and a country's gross domestic investment non-linear, and what is the nature of this relationship?\n\n**Setting & Data.** A cross-sectional regression analysis for 54 countries using data averaged over 1990–1997. The analysis tests a quadratic specification for the effect of foreign bank penetration on the investment-to-GDP ratio.\n\n### Data / Model Specification\n\nThe non-linear estimating equation is given by:\n  \nI = A + \\alpha_{1}LGDP + \\alpha_{2}GRO + \\alpha_{3}FBSASS + \\alpha_{4}LGDP^{2} + \\alpha_{5}GRO^{2} + \\alpha_{6}FBSASS^{2} + \\varepsilon \\quad \\text{(Eq. (1))}\n \nwhere `I` is the investment-to-GDP ratio, `FBSASS` is the share of foreign bank assets in total bank assets, and `LGDP` and `GRO` are controls.\n\n**Table 1. Baseline Estimation Results**\n\n| Variable | Coefficient (t-stat) |\n|:---|:---:|\n| `FBSASS` (α₃) | -14.24 (-2.38) |\n| `FBSASS²` (α₆) | 22.84 (3.46) |\n\n*Notes: The results are from Table 4, column (2a) of the paper.*\n\n### Question\n\nBased on the model specification in **Eq. (1)** and the results in **Table 1**, select all of the following statements that are correct interpretations of the findings.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 27, "Question": "### Background\n\n**Research Question.** Can a trading rule based on identifying properties with large negative residuals from a hedonic pricing model generate superior risk-adjusted returns, and what does this imply for real estate market efficiency?\n\n**Setting.** An empirical test using portfolios of apartment buildings sorted by their hedonic pricing error (`e`). Performance is evaluated by comparing raw returns and calculating Sharpe ratios against the total sample of properties.\n\n**Variables and Parameters.**\n- `e`: Estimation error from the hedonic model, used for sorting properties into portfolios.\n- `Mean Excess Return`: Average annualized return from appreciation minus a maturity-matched risk-free rate.\n- `Std. Dev.`: Standard deviation of excess returns.\n- `Sharpe Ratio`: A measure of risk-adjusted return, calculated as `Mean Excess Return / Std. Dev.`.\n\n---\n\n### Data / Model Specification\n\nProperties purchased between 1971 and 1980 were sorted into portfolios based on the estimation error `e` at the time of purchase. The two key portfolios for comparison are the \"undervalued\" portfolio, identified by the trading rule (`e ≤ -0.10`), and the portfolio of all properties in the sample.\n\n**Table 1.** Return and Risk Statistics for Portfolios (Purchases 1971-1980).\n\n| Portfolio | Mean Excess Return (%) | Std. Dev. (%) |\n| :--- | :--- | :--- |\n| Total sample | 12.0 | 13.6 |\n| `e ≤ -0.10` | 24.6 | 33.0 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following statements are correct?", "Options": {"A": "The higher raw return of the 'undervalued' portfolio (`e ≤ -0.10`) was accompanied by disproportionately higher risk (standard deviation), leading to its lower risk-adjusted performance.", "B": "The total sample portfolio has a Sharpe ratio of approximately 0.88.", "C": "The 'undervalued' portfolio (`e ≤ -0.10`) delivered a superior risk-adjusted return, as measured by the Sharpe ratio, compared to the total sample.", "D": "The 'undervalued' portfolio (`e ≤ -0.10`) generated a higher raw mean excess return than the total sample."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses the ability to distinguish between raw and risk-adjusted performance, a central theme of the paper. It uses a **Computational Judgment** strategy, requiring the user to calculate Sharpe ratios from provided data to evaluate the options. Option A is a direct data interpretation. Options C and D require calculation and synthesis. The key distractor (B) represents the naive conclusion an investor might draw by ignoring risk, a classic conceptual error.", "qid": "27", "question": "### Background\n\n**Research Question.** Can a trading rule based on identifying properties with large negative residuals from a hedonic pricing model generate superior risk-adjusted returns, and what does this imply for real estate market efficiency?\n\n**Setting.** An empirical test using portfolios of apartment buildings sorted by their hedonic pricing error (`e`). Performance is evaluated by comparing raw returns and calculating Sharpe ratios against the total sample of properties.\n\n**Variables and Parameters.**\n- `e`: Estimation error from the hedonic model, used for sorting properties into portfolios.\n- `Mean Excess Return`: Average annualized return from appreciation minus a maturity-matched risk-free rate.\n- `Std. Dev.`: Standard deviation of excess returns.\n- `Sharpe Ratio`: A measure of risk-adjusted return, calculated as `Mean Excess Return / Std. Dev.`.\n\n---\n\n### Data / Model Specification\n\nProperties purchased between 1971 and 1980 were sorted into portfolios based on the estimation error `e` at the time of purchase. The two key portfolios for comparison are the \"undervalued\" portfolio, identified by the trading rule (`e ≤ -0.10`), and the portfolio of all properties in the sample.\n\n**Table 1.** Return and Risk Statistics for Portfolios (Purchases 1971-1980).\n\n| Portfolio | Mean Excess Return (%) | Std. Dev. (%) |\n| :--- | :--- | :--- |\n| Total sample | 12.0 | 13.6 |\n| `e ≤ -0.10` | 24.6 | 33.0 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 56, "Question": "### Background\n\n**Research Question.** What are the aggregate and distributional consequences of imposing interest rate ceilings on the market for motor vehicle loans?\n\n**Setting / Data-Generating Environment.** This analysis uses the parameters from a consistently estimated hedonic model of the auto loan market to simulate the market-wide effects of different, counterfactual rate ceiling policies. The simulations calculate the average probability that a potential borrower will be rejected (rationed out of the market) and the average interest rate paid by those who successfully obtain a loan under each policy regime.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Simulated Aggregate Impact of Rate Ceilings**\n\n| Ceiling Level | Avg. Rejection Prob. (`ρ̂`) | Avg. Rate for Approved Loans (`θ̂^*`) |\n|:---|---:|---:|\n| No Ceiling | 0.0% | 18.5% |\n| 21% | 30.7% | 13.2% |\n| 16% | 50.6% | 11.1% |\n| 12% | 71.5% | 9.0% |\n\n**Table 2: Simulated Impact on Specific Borrower Groups**\n\n| Borrower Group | Avg. Equilibrium Rate (No Ceiling), `θ̂` | Avg. Rejection Prob. (Ceiling = 12%), `ρ̂` |\n|:---|---:|---:|\n| **Expenditure ($)** | |\n| 0-10,000 | 22.1% | 82.6% |\n| 10,000-18,000 | 17.2% | 68.2% |\n| 18,000 or more | 16.2% | 61.7% |\n| **Household Type** | |\n| Single Male | 24.2% | 86.8% |\n| Single w/children | 26.1% | 90.7% |\n| Couple w/children | 17.3% | 68.4% |\n\n---\n\n### Question\n\nBased on the simulation results in Table 1 and Table 2, which of the following statements are correct interpretations of the impact of interest rate ceilings?", "Options": {"A": "Tightening the ceiling from 21% to 16% increases the average rejection probability by approximately 20 percentage points while reducing the average interest rate for approved loans by approximately 2 percentage points.", "B": "According to Table 2, households with expenditures over $18,000 have a higher unconditional equilibrium rate than couple w/children households, and are therefore less likely to be rejected under a 12% ceiling.", "C": "Under a restrictive 12% ceiling, borrower groups with the highest unconditional equilibrium rates (e.g., single-parent households) experience the highest probabilities of being rationed out of the market.", "D": "The results suggest that as rate ceilings are lowered, the average interest rate for approved loans equalizes across groups primarily because lenders start offering more uniform pricing to all borrowers."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret and synthesize quantitative results from simulation tables. It uses an Atomic Decomposition strategy, breaking down the original QA's analysis of aggregate and distributional effects into distinct, verifiable statements. Option A is a Computational Judgment based on Table 1. Option B requires synthesizing information across columns in Table 2 to understand the mechanism of rationing. Distractor C presents a plausible but incorrect mechanism (behavioral change vs. selection effect), a key conceptual error. Distractor D is an 'Almost Right' distractor that tests for careful reading and avoidance of hasty generalization (the premise is true, but the conclusion is false: 61.7% vs 68.4%).", "qid": "56", "question": "### Background\n\n**Research Question.** What are the aggregate and distributional consequences of imposing interest rate ceilings on the market for motor vehicle loans?\n\n**Setting / Data-Generating Environment.** This analysis uses the parameters from a consistently estimated hedonic model of the auto loan market to simulate the market-wide effects of different, counterfactual rate ceiling policies. The simulations calculate the average probability that a potential borrower will be rejected (rationed out of the market) and the average interest rate paid by those who successfully obtain a loan under each policy regime.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Simulated Aggregate Impact of Rate Ceilings**\n\n| Ceiling Level | Avg. Rejection Prob. (`ρ̂`) | Avg. Rate for Approved Loans (`θ̂^*`) |\n|:---|---:|---:|\n| No Ceiling | 0.0% | 18.5% |\n| 21% | 30.7% | 13.2% |\n| 16% | 50.6% | 11.1% |\n| 12% | 71.5% | 9.0% |\n\n**Table 2: Simulated Impact on Specific Borrower Groups**\n\n| Borrower Group | Avg. Equilibrium Rate (No Ceiling), `θ̂` | Avg. Rejection Prob. (Ceiling = 12%), `ρ̂` |\n|:---|---:|---:|\n| **Expenditure ($)** | |\n| 0-10,000 | 22.1% | 82.6% |\n| 10,000-18,000 | 17.2% | 68.2% |\n| 18,000 or more | 16.2% | 61.7% |\n| **Household Type** | |\n| Single Male | 24.2% | 86.8% |\n| Single w/children | 26.1% | 90.7% |\n| Couple w/children | 17.3% | 68.4% |\n\n---\n\n### Question\n\nBased on the simulation results in Table 1 and Table 2, which of the following statements are correct interpretations of the impact of interest rate ceilings?", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 54, "Question": "### Background\n\nA study tests the hypothesis that the interaction between product life cycle (PLC) and the book-to-market effect is driven by mispricing, which is exacerbated by investor inattention. The market-to-book (M/B) ratio is decomposed into a `Mispricing` component and a `Value-to-Book` component. Fama-MacBeth regressions are run in 'Low' and 'High' investor attention subsamples to test the interaction of PLC with each M/B component.\n\n### Data / Model Specification\n\nThe regression includes interaction terms for PLC with both `Mispricing` and `Value-to-Book`.\n\n**Table 1: Fama-MacBeth Results for Interaction Terms in Attention Subsamples**\n\n| | Low Attention | High Attention |\n| :--- | :---: | :---: |\n| **Variable** | **Coeff. (t-stat)** | **Coeff. (t-stat)** |\n| `PLC × Mispricing` | **-0.04 (-3.64)** | -0.01 (-0.53) |\n| `PLC × Value-to-Book` | 0.01 (0.16) | -0.01 (-0.74) |\n\n*Source: Abridged from Table 7. A high 'Mispricing' value corresponds to overvaluation, so a negative coefficient on its interaction implies that a long PLC strengthens the subsequent return reversal for mispriced stocks.* \n\n---\n\nBased on the full pattern of results in Table 1, which of the following conclusions are directly supported? Select all that apply.", "Options": {"A": "The amplifying effect of PLC on return predictability is specifically linked to the mispricing component of the M/B ratio, not the fundamental value component.", "B": "For firms with high investor attention, a long PLC significantly strengthens the return predictability of both the mispricing and fundamental value components of the M/B ratio.", "C": "Regardless of investor attention, PLC has a significant interaction with the fundamental `Value-to-Book` component in predicting returns.", "D": "The interaction between PLC and mispricing is economically and statistically significant, but only within the low investor attention subsample."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret a complex 'triple-interaction' result from a regression table, which is the paper's sharpest identification test. Depth Strategy: Scenario Application. The user must apply the paper's core theory to the specific pattern of coefficients and significance levels in the table. Distractor Logic: C and D are conceptual opposites that misinterpret the insignificant coefficients. C incorrectly claims significance for the high-attention group, while D incorrectly claims significance for the `Value-to-Book` interaction.", "qid": "54", "question": "### Background\n\nA study tests the hypothesis that the interaction between product life cycle (PLC) and the book-to-market effect is driven by mispricing, which is exacerbated by investor inattention. The market-to-book (M/B) ratio is decomposed into a `Mispricing` component and a `Value-to-Book` component. Fama-MacBeth regressions are run in 'Low' and 'High' investor attention subsamples to test the interaction of PLC with each M/B component.\n\n### Data / Model Specification\n\nThe regression includes interaction terms for PLC with both `Mispricing` and `Value-to-Book`.\n\n**Table 1: Fama-MacBeth Results for Interaction Terms in Attention Subsamples**\n\n| | Low Attention | High Attention |\n| :--- | :---: | :---: |\n| **Variable** | **Coeff. (t-stat)** | **Coeff. (t-stat)** |\n| `PLC × Mispricing` | **-0.04 (-3.64)** | -0.01 (-0.53) |\n| `PLC × Value-to-Book` | 0.01 (0.16) | -0.01 (-0.74) |\n\n*Source: Abridged from Table 7. A high 'Mispricing' value corresponds to overvaluation, so a negative coefficient on its interaction implies that a long PLC strengthens the subsequent return reversal for mispriced stocks.* \n\n---\n\nBased on the full pattern of results in Table 1, which of the following conclusions are directly supported? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 57, "Question": "### Background\n\n**Research Question.** What are the aggregate and distributional consequences of imposing interest rate ceilings on the market for motor vehicle loans?\n\n**Setting / Data-Generating Environment.** The paper uses a consistently estimated hedonic model to simulate market outcomes. A key methodological challenge is that the estimation sample contains only *approved* loans. To produce market-wide estimates of rejection rates, the model uses a specific weighting scheme. The paper also considers a hypothetical \"risk-adjusted\" ceiling policy to evaluate its theoretical properties.\n\n---\n\n### Data / Model Specification\n\nThe probability that a prospective loan `i` will be rejected under a ceiling `c` is given by:\n  \n\\rho_{i} = 1 - \\Phi\\left(\\frac{\\ln c - X_{i}^{\\prime}\\beta}{\\sigma}\\right) \\quad \\text{(Eq. (1))}\n \nThe unconditional equilibrium rate for loan `i` is:\n  \n\\theta_{i} = \\exp(X_{i}^{\\prime}\\beta + \\sigma^{2}/2) \\quad \\text{(Eq. (2))}\n \nwhere `Φ(·)` is the standard normal CDF, `X_i` is a vector of borrower characteristics, `β` is a vector of coefficients, and `σ` is the standard deviation of the error term.\n\n---\n\n### Question\n\nBased on the paper's methodology for analyzing the effects of rate ceilings, which of the following statements are correct?", "Options": {"A": "To estimate the market-wide rejection rate from a sample of only approved loans, individual observations must be weighted to correct for sample selection bias; a simple average would systematically understate the true rationing effect by under-representing high-risk borrowers.", "B": "A hypothetical policy that sets a personalized ceiling for each borrower equal to their predicted unconditional equilibrium rate (`c_i = θ_i`) would fail to eliminate rationing, instead imposing a uniform rejection probability on all borrowers that depends only on the error variance (`σ^2`).", "C": "The weighting scheme is necessary because the sample of approved loans contains a disproportionately high number of high-risk borrowers, whose influence must be down-weighted to reflect the general population.", "D": "The proposed risk-adjusted ceiling policy (`c_i = θ_i`) is shown to be optimal because it allows lenders to perfectly price risk, thereby eliminating credit rationing for all borrower types."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the paper's core methodological contributions and theoretical implications, using a Reverse-Reasoning strategy (evaluating conclusions from complex procedures). It uses Atomic Decomposition to test two key, non-obvious insights from the original QA: the logic of weighting for sample selection and the critique of a hypothetical policy. Distractors C and D are Conceptual Opposites, directly contradicting the correct logic and findings, which helps discriminate between deep and superficial understanding.", "qid": "57", "question": "### Background\n\n**Research Question.** What are the aggregate and distributional consequences of imposing interest rate ceilings on the market for motor vehicle loans?\n\n**Setting / Data-Generating Environment.** The paper uses a consistently estimated hedonic model to simulate market outcomes. A key methodological challenge is that the estimation sample contains only *approved* loans. To produce market-wide estimates of rejection rates, the model uses a specific weighting scheme. The paper also considers a hypothetical \"risk-adjusted\" ceiling policy to evaluate its theoretical properties.\n\n---\n\n### Data / Model Specification\n\nThe probability that a prospective loan `i` will be rejected under a ceiling `c` is given by:\n  \n\\rho_{i} = 1 - \\Phi\\left(\\frac{\\ln c - X_{i}^{\\prime}\\beta}{\\sigma}\\right) \\quad \\text{(Eq. (1))}\n \nThe unconditional equilibrium rate for loan `i` is:\n  \n\\theta_{i} = \\exp(X_{i}^{\\prime}\\beta + \\sigma^{2}/2) \\quad \\text{(Eq. (2))}\n \nwhere `Φ(·)` is the standard normal CDF, `X_i` is a vector of borrower characteristics, `β` is a vector of coefficients, and `σ` is the standard deviation of the error term.\n\n---\n\n### Question\n\nBased on the paper's methodology for analyzing the effects of rate ceilings, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 159, "Question": "### Background\n\nA study argues that incumbent banks strategically use branch expansion to deter entry by their most threatening competitors. The analysis uses a two-stage model: Stage 1 models firm entry and profitability, while Stage 2 models incumbent firms' branching decisions after correcting for the endogeneity of market structure.\n\n### Data / Model Specification\n\nKey findings from the analysis are summarized below:\n\n**Table 1: Stage 1 Competitive Effects on an Incumbent Multimarket Bank's Payoff**\n\n| Competitor Type Entering Market | Effect on Payoff | t-statistic |\n| :--- | :---: | :---: |\n| First Multimarket (MM) competitor | -1.111 | (-16.65) |\n| First Single-Market (SM) competitor | -0.304 | (-3.64) |\n\n**Table 2: Stage 2 Adjusted Marginal Effect on Number of Branches**\n\n| Variable | Adjusted Effect |\n| :--- | :---: |\n| Effect of one additional MM Competitor on an incumbent's branches | 0.159 |\n| | (5.85) |\n\n---\n\n### Question\n\nTo construct the paper's central argument that incumbents use branching to deter entry specifically by multimarket (MM) banks, which of the following findings are essential premises? Select all that apply.", "Options": {"A": "The competitive harm to an incumbent MM bank's profitability is substantially greater from another MM bank than from a single-market (SM) bank.", "B": "When MM banks compete directly in a market, they respond by expanding their branch networks, indicating that branching is a key dimension of their competitive strategy.", "C": "Single-market banks tend to operate more in markets with significant agricultural activity.", "D": "After correcting for endogeneity, the presence of MM competitors has no statistically significant effect on incumbent branching."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the student's ability to synthesize findings from different parts of the analysis to build the paper's main causal story. It uses a 'Premise/assumption packaging' strategy. Correct option (A) establishes the motive for deterrence (from Table 1). Correct option (C) establishes the strategic weapon used (from Table 2). Distractor (B) is 'True but Irrelevant,' a finding from the paper that is not part of this specific causal chain. Distractor (D) is a 'Conceptual Opposite' that directly contradicts the key finding in Table 2.", "qid": "159", "question": "### Background\n\nA study argues that incumbent banks strategically use branch expansion to deter entry by their most threatening competitors. The analysis uses a two-stage model: Stage 1 models firm entry and profitability, while Stage 2 models incumbent firms' branching decisions after correcting for the endogeneity of market structure.\n\n### Data / Model Specification\n\nKey findings from the analysis are summarized below:\n\n**Table 1: Stage 1 Competitive Effects on an Incumbent Multimarket Bank's Payoff**\n\n| Competitor Type Entering Market | Effect on Payoff | t-statistic |\n| :--- | :---: | :---: |\n| First Multimarket (MM) competitor | -1.111 | (-16.65) |\n| First Single-Market (SM) competitor | -0.304 | (-3.64) |\n\n**Table 2: Stage 2 Adjusted Marginal Effect on Number of Branches**\n\n| Variable | Adjusted Effect |\n| :--- | :---: |\n| Effect of one additional MM Competitor on an incumbent's branches | 0.159 |\n| | (5.85) |\n\n---\n\n### Question\n\nTo construct the paper's central argument that incumbents use branching to deter entry specifically by multimarket (MM) banks, which of the following findings are essential premises? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 156, "Question": "### Background\n\n**Research Question.** This case examines whether specific, plausibly exogenous legal and regulatory events that damaged the high-yield bond market had a causal, negative impact on the viability of firms that had recently completed leveraged recapitalizations.\n\n**Setting / Data-Generating Environment.** An event study is conducted on a sample of recap firms, split into 'Distressed' and 'Nondistressed' groups. The study measures stock price reactions to seven announcements that signaled the collapse of the market for highly leveraged transactions (HLTs).\n\n### Data / Model Specification\n\nAbnormal returns (`AR`) are calculated using a standard market model:\n  \nAR_{it} = R_{it} - (\\hat{\\alpha}_i + \\hat{\\beta}_i R_{mt}) \\quad \\text{(Eq. 1)}\n \n**Table 1: Average Stock Price Reactions to HLT Market Events**\n| Event Date | Event Description | Nondistressed (Avg. AR) | Distressed (Avg. AR) |\n|---|---|:---:|:---:|\n| 7/26/89 | Congressional ban on junk bond investment by thrifts | -0.77% | -4.02%*** |\n| **All 7 Events** | **Cumulative Abnormal Return (Avg. CAR)** | **-6.06%*** | **-19.96%*** |\n\n*Source: Adapted from Table 5 of the study.* \n\n### Question\n\nFor the 7/26/89 event, assume the market return `R_mt` was -1.0%, `α_i` was 0 for all firms, the estimated beta (`β_i`) was 1.2 for the nondistressed group, and 1.5 for the distressed group. Based on this information and the study's logic, which of the following statements are correct?\n", "Options": {"A": "The cumulative abnormal return of -19.96% for distressed firms suggests that these external regulatory events destroyed nearly one-fifth of their equity value, supporting a causal link between the shocks and financial distress.", "B": "The total raw return (R_it) for the average nondistressed firm on this day was -0.23%.", "C": "The significantly more negative reaction of distressed firms proves they were poorly structured from the outset, confirming the 'overheating' hypothesis.", "D": "The total raw return (R_it) for the average distressed firm on this day was -5.52%."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item tests the ability to perform a calculation using the market model and to interpret the causal implications of a differential event study result.\nStrategy: Computational Judgment and Atomic Decomposition. The question requires both a calculation and a conceptual interpretation, which are presented as separate, testable statements.\nDistractor Logic:\n- C is a Numerical (Formula Misuse) error. The correct calculation is R_it = -0.77% + 1.2*(-1.0%) = -1.97%. The value -0.23% could arise from an incorrect formula like R_it = AR_it - R_mt.\n- D is a Conceptual Opposite. The paper explicitly uses this evidence to argue *against* the 'overheating' hypothesis and in favor of an external shock explanation.", "qid": "156", "question": "### Background\n\n**Research Question.** This case examines whether specific, plausibly exogenous legal and regulatory events that damaged the high-yield bond market had a causal, negative impact on the viability of firms that had recently completed leveraged recapitalizations.\n\n**Setting / Data-Generating Environment.** An event study is conducted on a sample of recap firms, split into 'Distressed' and 'Nondistressed' groups. The study measures stock price reactions to seven announcements that signaled the collapse of the market for highly leveraged transactions (HLTs).\n\n### Data / Model Specification\n\nAbnormal returns (`AR`) are calculated using a standard market model:\n  \nAR_{it} = R_{it} - (\\hat{\\alpha}_i + \\hat{\\beta}_i R_{mt}) \\quad \\text{(Eq. 1)}\n \n**Table 1: Average Stock Price Reactions to HLT Market Events**\n| Event Date | Event Description | Nondistressed (Avg. AR) | Distressed (Avg. AR) |\n|---|---|:---:|:---:|\n| 7/26/89 | Congressional ban on junk bond investment by thrifts | -0.77% | -4.02%*** |\n| **All 7 Events** | **Cumulative Abnormal Return (Avg. CAR)** | **-6.06%*** | **-19.96%*** |\n\n*Source: Adapted from Table 5 of the study.* \n\n### Question\n\nFor the 7/26/89 event, assume the market return `R_mt` was -1.0%, `α_i` was 0 for all firms, the estimated beta (`β_i`) was 1.2 for the nondistressed group, and 1.5 for the distressed group. Based on this information and the study's logic, which of the following statements are correct?\n", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 9, "Question": "### Background\n\n**Research Question.** A celebrated finding in empirical finance is that patterns of stock return predictability, such as monotonically increasing coefficients and R-squareds, become stronger at longer horizons. This analysis uses a simulation-based identification strategy to determine if these patterns are evidence of true predictability or statistical artifacts generated under the null hypothesis of no predictability.\n\n**Setting and Environment.** The analysis is based on 100,000 Monte Carlo simulations. Each simulation generates 75 years of annual data where returns are IID noise (no predictability) and the predictor `X_t` follows a persistent AR(1) process calibrated to the historical log dividend yield (`ρ=0.953`). From each simulated dataset, predictive regressions are estimated for horizons of 1 to 5 years.\n\n### Data / Model Specification\n\nThe following tables summarize key results from the simulations conducted under the null hypothesis of no predictability. For context, the actual empirical coefficient ratios from regressing 1- to 5-year stock returns on the log dividend yield over the 1926–2004 sample period are 1.96, 2.98, 3.53, and 3.99.\n\n**Table 1: Simulated Distribution of Coefficient Estimates (`β̂_j`)**\n\n| Horizon (j) | Correlation with `β̂_2` | Correlation with `β̂_3` | Correlation with `β̂_4` | Correlation with `β̂_5` |\n| :--- | :--- | :--- | :--- | :--- |\n| **1** | 0.966 | 0.926 | 0.885 | 0.843 |\n| **2** | 1.000 | 0.980 | 0.946 | 0.909 |\n\n- **Percentage of simulations with monotonic coefficients:** 66.02%\n\n**Table 2: Simulated Distribution of `R²` Statistics**\n\n| Horizon (j) | Correlation with `R₁²` | Correlation with `R₂²` |\n| :--- | :--- | :--- |\n| **1** | 1.000 | 0.949 |\n| **2** | | 1.000 |\n\n- **Percentage of simulations with monotonic `R²`s:** 52.21%\n\n**Table 3: Simulated Distribution of Cross-Horizon Ratios**\n\n| Horizon (j) | Mean Ratio `β̂_j / β̂_1` | Mean Ratio `R_j² / R_1²` |\n| :--- | :--- | :--- |\n| 2 | 1.93 | 1.96 |\n| 3 | 2.80 | 2.88 |\n| 4 | 3.59 | 3.77 |\n| 5 | 4.32 | 4.61 |\n\n### Question\n\nBased on the simulation results conducted under the null hypothesis of no predictability, which of the following conclusions are supported by the provided tables? Select all that apply.", "Options": {"A": "The near-perfect correlation (0.966) between the 1-year and 2-year coefficient estimators implies that they offer little independent information, as both reflect the same underlying sampling error.", "B": "The close match between the simulated mean coefficient ratio for the 2-year horizon (1.93) and the actual empirical ratio (1.96) strongly suggests the observed empirical pattern is a statistical artifact.", "C": "The high frequency of monotonic coefficient patterns (66.02%) is a natural artifact of predictor persistence under the null, not evidence of strengthening predictability.", "D": "The fact that R-squareds are monotonic over half the time (52.21%) provides robust evidence that long-horizon regressions have superior explanatory power."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the core simulation evidence from the paper. It uses the Atomic Decomposition strategy, breaking down the original QA problem's three parts into three independently correct options (A, B, C). The distractor (D) represents a Conceptual Opposite, stating the very misinterpretation the paper seeks to debunk, making it a high-fidelity foil.", "qid": "9", "question": "### Background\n\n**Research Question.** A celebrated finding in empirical finance is that patterns of stock return predictability, such as monotonically increasing coefficients and R-squareds, become stronger at longer horizons. This analysis uses a simulation-based identification strategy to determine if these patterns are evidence of true predictability or statistical artifacts generated under the null hypothesis of no predictability.\n\n**Setting and Environment.** The analysis is based on 100,000 Monte Carlo simulations. Each simulation generates 75 years of annual data where returns are IID noise (no predictability) and the predictor `X_t` follows a persistent AR(1) process calibrated to the historical log dividend yield (`ρ=0.953`). From each simulated dataset, predictive regressions are estimated for horizons of 1 to 5 years.\n\n### Data / Model Specification\n\nThe following tables summarize key results from the simulations conducted under the null hypothesis of no predictability. For context, the actual empirical coefficient ratios from regressing 1- to 5-year stock returns on the log dividend yield over the 1926–2004 sample period are 1.96, 2.98, 3.53, and 3.99.\n\n**Table 1: Simulated Distribution of Coefficient Estimates (`β̂_j`)**\n\n| Horizon (j) | Correlation with `β̂_2` | Correlation with `β̂_3` | Correlation with `β̂_4` | Correlation with `β̂_5` |\n| :--- | :--- | :--- | :--- | :--- |\n| **1** | 0.966 | 0.926 | 0.885 | 0.843 |\n| **2** | 1.000 | 0.980 | 0.946 | 0.909 |\n\n- **Percentage of simulations with monotonic coefficients:** 66.02%\n\n**Table 2: Simulated Distribution of `R²` Statistics**\n\n| Horizon (j) | Correlation with `R₁²` | Correlation with `R₂²` |\n| :--- | :--- | :--- |\n| **1** | 1.000 | 0.949 |\n| **2** | | 1.000 |\n\n- **Percentage of simulations with monotonic `R²`s:** 52.21%\n\n**Table 3: Simulated Distribution of Cross-Horizon Ratios**\n\n| Horizon (j) | Mean Ratio `β̂_j / β̂_1` | Mean Ratio `R_j² / R_1²` |\n| :--- | :--- | :--- |\n| 2 | 1.93 | 1.96 |\n| 3 | 2.80 | 2.88 |\n| 4 | 3.59 | 3.77 |\n| 5 | 4.32 | 4.61 |\n\n### Question\n\nBased on the simulation results conducted under the null hypothesis of no predictability, which of the following conclusions are supported by the provided tables? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 89, "Question": "### Background\n\n**Research Question.** A central challenge in corporate finance is to devise empirical tests of strategic risk-shifting that are robust to mechanical artifacts like mean reversion. This paper proposes two novel methodologies based on portfolio holdings to overcome a 'sorting bias' found in traditional return-based tests of mutual fund tournament behavior.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual fund holdings data from 1990-2006. The goal is to test whether underperforming managers ('losers') in the first half of the year actively change their portfolio risk in the second half. The paper also re-examines whether this behavior is linked to overall market conditions.\n\n### Data / Model Specification\n\n**Method 1: Active Holdings Change Regression**\nThis method analyzes individual security trades. The dependent variable is the active change in a security's weight, `WgtChg`, defined as:\n  \n\\mathrm{WgtChg}_{j i y}=\\frac{(\\mathrm{DecShares}_{j i y}-\\mathrm{JuneShares}_{j i y}) \\times \\mathrm{DecPrice}_{j y}}{\\mathrm{DecAssets}_{i y}} \\quad \\text{(Eq. 1)}\n \nThis is regressed on security characteristics, including the `Adjusted Standard Deviation` (`Adj. Std.`), which is the security's first-half standard deviation minus the fund's average. The key test is for a differential effect for underperforming funds, identified by an interaction term `LD*AS` (`LowDummy` × `Adj. Std.`).\n\n**Table 1. Active Trading Regression Results (Overall, Ranked Together)**\n\n| Variable | Coefficient | t-value |\n|:---|:---:|:---:|\n| Adj. Std. | 1.217 | 7.74** |\n| LD*AS | 0.677 | 1.64 |\n\n*Note: The dependent variable is `WgtChg` scaled by 100. **Significant at 1% level.*\n\n**Method 2: Bootstrap-Based Aggregate Risk Measure**\nThis method constructs a counterfactual second-half portfolio risk level for each fund. It does so by observing the manager's actual trading volume (e.g., number of stocks sold/bought) but randomizing *which* stocks are traded, assuming non-risk-motivated (e.g., momentum-based) trading. This produces an 'expected' standard deviation (`σ_expected`). The dependent variable is `Excess Risk = σ_actual - σ_expected`. This is regressed on `LowDummy`, an indicator for underperforming funds.\n\n**Table 2. Bootstrap-Based Tournament Results (Average over 1990-2006)**\n\n| Variable | Value | p-value |\n|:---|:---:|:---:|\n| Low Dummy | 0.055 | 0.00** |\n\n*Note: **Significant at 1% level.*\n\n### Question\n\nBased on the results from the paper's two new methodologies (Active Holdings Change and Bootstrap-Based Aggregate Risk), select all of the following statements that are supported by the evidence in Table 1 and Table 2.", "Options": {"A": "The coefficient on the interaction term `LD*AS` in Table 1 (0.677) is statistically significant at the 1% level, confirming that underperforming managers' risk-taking is higher.", "B": "The coefficient on `Adj. Std.` in Table 1 (1.217) indicates that outperforming managers tend to sell securities that are riskier than their portfolio average.", "C": "Based on the bootstrap analysis in Table 2, underperforming funds increase their second-half portfolio risk by an average of 5.5 basis points more than outperforming funds, a result that is statistically significant at the 1% level.", "D": "According to the active holdings regression in Table 1, the total marginal effect of a security's adjusted standard deviation on its scaled weight change for an underperforming manager is 1.894."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the student's ability to interpret the results of the two novel methodologies proposed by the paper. It uses an **Atomic Decomposition** strategy, breaking down the findings from Table 1 and Table 2 into distinct, verifiable statements. The core task is a mix of **Computational Judgment** (calculating the marginal effect for losers) and direct interpretation. Distractors are based on common errors: a **Conceptual Opposite** (misinterpreting the sign of a coefficient) and a **Significance Error** (misstating the statistical significance of a coefficient).", "qid": "89", "question": "### Background\n\n**Research Question.** A central challenge in corporate finance is to devise empirical tests of strategic risk-shifting that are robust to mechanical artifacts like mean reversion. This paper proposes two novel methodologies based on portfolio holdings to overcome a 'sorting bias' found in traditional return-based tests of mutual fund tournament behavior.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual fund holdings data from 1990-2006. The goal is to test whether underperforming managers ('losers') in the first half of the year actively change their portfolio risk in the second half. The paper also re-examines whether this behavior is linked to overall market conditions.\n\n### Data / Model Specification\n\n**Method 1: Active Holdings Change Regression**\nThis method analyzes individual security trades. The dependent variable is the active change in a security's weight, `WgtChg`, defined as:\n  \n\\mathrm{WgtChg}_{j i y}=\\frac{(\\mathrm{DecShares}_{j i y}-\\mathrm{JuneShares}_{j i y}) \\times \\mathrm{DecPrice}_{j y}}{\\mathrm{DecAssets}_{i y}} \\quad \\text{(Eq. 1)}\n \nThis is regressed on security characteristics, including the `Adjusted Standard Deviation` (`Adj. Std.`), which is the security's first-half standard deviation minus the fund's average. The key test is for a differential effect for underperforming funds, identified by an interaction term `LD*AS` (`LowDummy` × `Adj. Std.`).\n\n**Table 1. Active Trading Regression Results (Overall, Ranked Together)**\n\n| Variable | Coefficient | t-value |\n|:---|:---:|:---:|\n| Adj. Std. | 1.217 | 7.74** |\n| LD*AS | 0.677 | 1.64 |\n\n*Note: The dependent variable is `WgtChg` scaled by 100. **Significant at 1% level.*\n\n**Method 2: Bootstrap-Based Aggregate Risk Measure**\nThis method constructs a counterfactual second-half portfolio risk level for each fund. It does so by observing the manager's actual trading volume (e.g., number of stocks sold/bought) but randomizing *which* stocks are traded, assuming non-risk-motivated (e.g., momentum-based) trading. This produces an 'expected' standard deviation (`σ_expected`). The dependent variable is `Excess Risk = σ_actual - σ_expected`. This is regressed on `LowDummy`, an indicator for underperforming funds.\n\n**Table 2. Bootstrap-Based Tournament Results (Average over 1990-2006)**\n\n| Variable | Value | p-value |\n|:---|:---:|:---:|\n| Low Dummy | 0.055 | 0.00** |\n\n*Note: **Significant at 1% level.*\n\n### Question\n\nBased on the results from the paper's two new methodologies (Active Holdings Change and Bootstrap-Based Aggregate Risk), select all of the following statements that are supported by the evidence in Table 1 and Table 2.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 160, "Question": "### Background\n\nThis paper examines the structure and performance of Canadian stock exchanges in the late 1950s, contrasting them with their U.S. counterparts. A key feature of the Canadian market is the prevalence of speculative, low-priced mining and oil stocks, which are traded alongside more conventional industrial stocks. This analysis investigates how structural differences manifest in observable market outcomes like trading volume, share turnover, and price volatility.\n\n### Data / Model Specification\n\nThe analysis uses data from 1957. The following tables provide key statistics on share turnover and price volatility for samples of stocks listed on the Toronto Stock Exchange (TSE).\n\n**Table 1: Percentage Distributions of Combined Turnovers for 114 Toronto Listings During 1957**\n\n| SHARE TURNOVER (PER CENT) | Industrial | Oil   | Mining |\n|:--------------------------|:-----------|:------|:-------|\n| 0-5                       | 54.0       | 12.5  | 3.6    |\n| 5-10                      | 30.0       | 25.0  | 12.5   |\n| 10-20                     | 10.0       | 12.5  | 26.7   |\n| 20-30                     | 4.0        |       | 12.5   |\n| 30-50                     | 2.0        | 25.0  | 17.9   |\n| 50-70                     |            | 25.0  | 10.7   |\n| 70-100                    |            |       | 3.6    |\n| 100 and over              |            |       | 12.5   |\n| **Total**                 | **100.0**  | **100.0** | **100.0**|\n| **No. of Issues**         | **50**     | **8**     | **56**     |\n\n**Table 2: Ratios of Yearly High to Low Prices in 1957 for 114 Toronto Listings**\n\n| SHARE PRICE   | 1.0-1.5 | 1.5-2.0 | 2.0-3.0 | 3.0 and Over | TOTAL ISSUES |\n|:--------------|:--------|:--------|:--------|:-------------|:-------------|\n| **All industrials** | 32      | 11      | 3       | 4            | 50           |\n| **All mining and oil** | 3       | 8       | 21      | 32           | 64           |\n| **Totals**    | **35**  | **19**  | **24**  | **36**       | **114**      |\n\nBased on a quantitative analysis of the provided data, which of the following statements accurately characterize the Toronto Stock Exchange in 1957? Select all that apply.", "Options": {"A": "More than half of all mining and oil stocks had a share turnover of 30% or greater.", "B": "Fewer than one in five industrial stocks experienced high price volatility, defined as a yearly high-to-low price ratio of 2.0 or greater.", "C": "The vast majority (over 80%) of industrial stocks were relatively illiquid, exhibiting a share turnover of 10% or less.", "D": "The vast majority (over 80%) of mining and oil stocks were highly volatile, with their yearly high price being at least double their yearly low price."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to perform multiple calculations from two different tables and synthesize the results to form a coherent, multi-faceted picture of the Canadian market structure as presented in the paper. It directly tests the core empirical findings that support the paper's thesis about market segmentation.\nStrategy: Computational Judgment. The user must calculate four separate quantitative claims and judge their validity against the data.\nDistractor Logic: This item is designed with four correct options to rigorously test comprehension. An incorrect answer would stem from calculation errors (e.g., misinterpreting ranges, incorrect weighting), misreading the tables, or confusing the industrial and mining/oil categories. For example, a user might only sum one column for a category or miscalculate the weighted average for the combined mining and oil group.", "qid": "160", "question": "### Background\n\nThis paper examines the structure and performance of Canadian stock exchanges in the late 1950s, contrasting them with their U.S. counterparts. A key feature of the Canadian market is the prevalence of speculative, low-priced mining and oil stocks, which are traded alongside more conventional industrial stocks. This analysis investigates how structural differences manifest in observable market outcomes like trading volume, share turnover, and price volatility.\n\n### Data / Model Specification\n\nThe analysis uses data from 1957. The following tables provide key statistics on share turnover and price volatility for samples of stocks listed on the Toronto Stock Exchange (TSE).\n\n**Table 1: Percentage Distributions of Combined Turnovers for 114 Toronto Listings During 1957**\n\n| SHARE TURNOVER (PER CENT) | Industrial | Oil   | Mining |\n|:--------------------------|:-----------|:------|:-------|\n| 0-5                       | 54.0       | 12.5  | 3.6    |\n| 5-10                      | 30.0       | 25.0  | 12.5   |\n| 10-20                     | 10.0       | 12.5  | 26.7   |\n| 20-30                     | 4.0        |       | 12.5   |\n| 30-50                     | 2.0        | 25.0  | 17.9   |\n| 50-70                     |            | 25.0  | 10.7   |\n| 70-100                    |            |       | 3.6    |\n| 100 and over              |            |       | 12.5   |\n| **Total**                 | **100.0**  | **100.0** | **100.0**|\n| **No. of Issues**         | **50**     | **8**     | **56**     |\n\n**Table 2: Ratios of Yearly High to Low Prices in 1957 for 114 Toronto Listings**\n\n| SHARE PRICE   | 1.0-1.5 | 1.5-2.0 | 2.0-3.0 | 3.0 and Over | TOTAL ISSUES |\n|:--------------|:--------|:--------|:--------|:-------------|:-------------|\n| **All industrials** | 32      | 11      | 3       | 4            | 50           |\n| **All mining and oil** | 3       | 8       | 21      | 32           | 64           |\n| **Totals**    | **35**  | **19**  | **24**  | **36**       | **114**      |\n\nBased on a quantitative analysis of the provided data, which of the following statements accurately characterize the Toronto Stock Exchange in 1957? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 85, "Question": "### Background\n\nThe study examines the stock market impact of pro-union legislation in New Zealand, segmenting firms into High (HU), Medium (MU), and Low (LU) unionization groups based on prior unionization density. It compares their Average Cumulative Abnormal Returns (ACARs) in response to `Aggressive` legislative events that signal stronger union power. The central hypothesis is that these events are more costly for LU firms than for HU firms, for whom the new regulations may be *inframarginal*—that is, less binding than their existing labor-management practices.\n\n### Data / Model Specification\n\nThe table below presents the 5-day ACARs for the HU and LU groups in response to the combined set of `Aggressive` events. A two-sample t-test is used to determine if the difference in returns between the two groups is statistically significant.\n\n**Table 1: 5-Day ACAR in Response to Aggressive Events**\n\n| Group    | 5-Day ACAR | BMP t-statistic |\n| :------- | :--------: | :-------------: |\n| HU group | -0.030     | [-0.13]         |\n| LU group | -0.093     | [-2.50]***      |\n\n**Two-sample test of difference:**\n- `t-statistic (HU vs. LU)`: 1.67**\n\n*Note: ***, ** denote significance at the 1% and 5% levels, respectively.*\n\n---\n\nBased on the data in Table 1 and the study's framework, which of the following statements are valid interpretations or conclusions?", "Options": {"A": "The LU group experienced a smaller negative valuation impact than the HU group, as shown by the smaller absolute value of its t-statistic.", "B": "The legislation's impact was likely inframarginal for the HU group, as evidenced by their statistically insignificant stock price reaction.", "C": "The market reaction for the LU group was statistically significant, suggesting investors priced in a substantial negative impact from the legislation for these firms.", "D": "The difference in the market's reaction between the HU and LU groups is statistically significant at the 5% level."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret statistical results from an event study table (significance, magnitude) and connect them to the core economic concept of 'inframarginality' as described in the paper. Depth Strategy: Computational Judgment. The user must evaluate multiple pieces of evidence from the table (t-statistics for individual groups, t-statistic for the difference) to form a comprehensive conclusion. Distractor Logic: D is a numerical distractor that misinterprets the meaning of the t-statistic relative to the ACAR value and incorrectly compares the groups' impacts.", "qid": "85", "question": "### Background\n\nThe study examines the stock market impact of pro-union legislation in New Zealand, segmenting firms into High (HU), Medium (MU), and Low (LU) unionization groups based on prior unionization density. It compares their Average Cumulative Abnormal Returns (ACARs) in response to `Aggressive` legislative events that signal stronger union power. The central hypothesis is that these events are more costly for LU firms than for HU firms, for whom the new regulations may be *inframarginal*—that is, less binding than their existing labor-management practices.\n\n### Data / Model Specification\n\nThe table below presents the 5-day ACARs for the HU and LU groups in response to the combined set of `Aggressive` events. A two-sample t-test is used to determine if the difference in returns between the two groups is statistically significant.\n\n**Table 1: 5-Day ACAR in Response to Aggressive Events**\n\n| Group    | 5-Day ACAR | BMP t-statistic |\n| :------- | :--------: | :-------------: |\n| HU group | -0.030     | [-0.13]         |\n| LU group | -0.093     | [-2.50]***      |\n\n**Two-sample test of difference:**\n- `t-statistic (HU vs. LU)`: 1.67**\n\n*Note: ***, ** denote significance at the 1% and 5% levels, respectively.*\n\n---\n\nBased on the data in Table 1 and the study's framework, which of the following statements are valid interpretations or conclusions?", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 61, "Question": "### Background\n\n**Research Question.** How is the latent volatility process modeled and estimated within a particle filtering framework, particularly when its innovations are correlated with innovations in the price process?\n\n**Setting.** The discrete-time state equation for the evolution of the latent variance process (`ν_t`) in the MRSVJ model, which is a core component of the particle filtering algorithm.\n\n**Variables and Parameters.**\n- `ν_t`: Variance at time `t`.\n- `ρ`: Correlation between price and variance shocks.\n- `y_t`: Log-spot price at time `t`.\n- `ε_2`: A standard normal random variable independent of the price shock.\n\n---\n\n### Data / Model Specification\n\nFor the particle filter, the state update equation for the latent variance `ν_t` incorporates information from the observed log-price `y_t`:\n\n  \n\\nu_{t}=\\kappa\\theta+(1-\\kappa)\\nu_{t-1}+\\sigma_{\\nu}\\rho\\left(y_{t}-a-(1-b)y_{t-1}+\\frac{\\nu_{t-1}}{2}+\\lambda\\bar{J}-\\sum_{i=1}^{N_{t}}J_{i}\\right) +\\sigma_{\\nu}\\sqrt{\\nu_{t-1}(1-\\rho^{2})}\\varepsilon_{2} \n \n\nThe paper's empirical results find a significant negative correlation, `ρ < 0`.\n\n---\n\nGiven the state update equation for `ν_t` and the empirical finding that `ρ` is negative, which of the following statements are valid inferences about the behavior of the particle filter?\n\nSelect all that apply.", "Options": {"A": "The term containing `(1-ρ^2)` ensures that the model's update to volatility remains well-defined and real-valued, as correlation must be between -1 and 1.", "B": "If the correlation `ρ` were exactly zero, the observed price `y_t` would have no direct influence on the updated estimate of `ν_t` beyond its role in determining jumps.", "C": "A large positive price surprise (`y_t` is much higher than expected) will cause the filter to decrease its estimate of `ν_t`.", "D": "If an observed price `y_t` is unexpectedly low (a negative surprise), the filter will revise its estimate of the current volatility `ν_t` upwards."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret a complex state-space update equation and reason about its behavior under specific conditions, linking it to the economic concept of the leverage effect. Strategy: Reverse-Reasoning. Given the model equation and an empirical result (`ρ < 0`), the user must deduce the consequences for the filtering process. Distractor Logic: Option A is correct; a negative surprise `(y_t - E[y_t])` multiplied by a negative `ρ` results in a positive update to `ν_t`. Option B is correct, as `1-ρ^2` must be non-negative. Option D is correct; a positive surprise multiplied by a negative `ρ` results in a negative update. Option C is a conceptual distractor (Almost Right); if `ρ=0`, the main term linking `y_t` to `ν_t` disappears, but `y_t` is still used to calculate the weights for resampling particles, so it has an indirect influence. However, the question asks about the *direct influence in the update equation*, making the statement largely correct in that context, but the other three are more directly and unequivocally true. To ensure a clear set of correct answers, A, B, and D are chosen as the most robustly correct interpretations of the equation's structure. *Self-correction: Re-evaluating C. The prompt asks about the state update equation itself. In that equation, if ρ=0, the entire term containing the price surprise `y_t` vanishes. Therefore, C is also a correct statement about the direct update mechanism. The final answer should be A, B, C, D. However, to maintain a high bar and avoid ambiguity about 'direct influence', I will select the three most certain options.* Final Decision: A, B, and D are the most direct and unambiguous consequences of the equation's structure and the negative `ρ`.", "qid": "61", "question": "### Background\n\n**Research Question.** How is the latent volatility process modeled and estimated within a particle filtering framework, particularly when its innovations are correlated with innovations in the price process?\n\n**Setting.** The discrete-time state equation for the evolution of the latent variance process (`ν_t`) in the MRSVJ model, which is a core component of the particle filtering algorithm.\n\n**Variables and Parameters.**\n- `ν_t`: Variance at time `t`.\n- `ρ`: Correlation between price and variance shocks.\n- `y_t`: Log-spot price at time `t`.\n- `ε_2`: A standard normal random variable independent of the price shock.\n\n---\n\n### Data / Model Specification\n\nFor the particle filter, the state update equation for the latent variance `ν_t` incorporates information from the observed log-price `y_t`:\n\n  \n\\nu_{t}=\\kappa\\theta+(1-\\kappa)\\nu_{t-1}+\\sigma_{\\nu}\\rho\\left(y_{t}-a-(1-b)y_{t-1}+\\frac{\\nu_{t-1}}{2}+\\lambda\\bar{J}-\\sum_{i=1}^{N_{t}}J_{i}\\right) +\\sigma_{\\nu}\\sqrt{\\nu_{t-1}(1-\\rho^{2})}\\varepsilon_{2} \n \n\nThe paper's empirical results find a significant negative correlation, `ρ < 0`.\n\n---\n\nGiven the state update equation for `ν_t` and the empirical finding that `ρ` is negative, which of the following statements are valid inferences about the behavior of the particle filter?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 53, "Question": "### Background\n\nTo test whether the book-to-market (BM) effect is driven by mispricing or risk, a study examines the Fama-French five-factor alphas of HML (High-BM minus Low-BM) portfolios. These portfolios are constructed separately for firms with short, medium, and long product life cycles (PLC).\n\n### Data / Model Specification\n\nThe excess returns of the HML portfolios within each PLC tercile are regressed on the Fama-French five factors:\n\n  \nR_{HML, PLC, t} = \\alpha_{HML, PLC} + \\beta' f_t + \\epsilon_{HML, PLC, t} \\quad \\text{(Eq. (1))}\n \n\nA non-zero alpha ($\\alpha_{HML, PLC}$) indicates that the five-factor model cannot fully explain the returns of the strategy. Table 1 presents these alphas.\n\n**Table 1: Fama-French 5-Factor Alphas (%) for HML Portfolios**\n\n| HML Portfolio | Short PLC | Medium PLC | Long PLC |\n| :--- | :---: | :---: | :---: |\n| **Alpha (%)** | **-0.11** | **0.02** | **0.46** |\n| *t-statistic* | *(-1.82)* | *(0.23)* | *(3.44)* |\n| **Long-Short (Alpha Spread)** | | | **0.57** |\n| *t-statistic* | | | *(4.03)* |\n\n*Source: Table 2, Panel B of the paper. Alphas are in percent per month.* \n\n---\n\nAccording to the results in Table 1, which of the following statements are INVALID interpretations of the evidence? Select all that apply.", "Options": {"A": "The Fama-French five-factor model successfully explains the returns of the HML strategy for firms with long product life cycles.", "B": "For long-PLC firms, the HML portfolio generates a positive and statistically significant alpha, suggesting the five-factor model fails to price this strategy.", "C": "The moderating effect of PLC on the value premium is fully captured by exposures to the five common risk factors.", "D": "The HML strategy applied to short-PLC firms generates a positive, statistically significant abnormal return after adjusting for common risk factors."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret regression alphas and their statistical significance in the context of testing asset pricing models. Depth Strategy: Reverse-Reasoning. The user must identify which conclusions are contradicted by the data. Inversion Protocol: The core assessment objective—identifying the correct interpretation of the long-PLC alpha—yields only one correct answer (D). To meet the multiple-answer requirement, the Single-Correct-Answer Inversion Protocol was applied. The stem was changed to ask for INVALID interpretations, making the original distractors (A, B, C) the new correct answers.", "qid": "53", "question": "### Background\n\nTo test whether the book-to-market (BM) effect is driven by mispricing or risk, a study examines the Fama-French five-factor alphas of HML (High-BM minus Low-BM) portfolios. These portfolios are constructed separately for firms with short, medium, and long product life cycles (PLC).\n\n### Data / Model Specification\n\nThe excess returns of the HML portfolios within each PLC tercile are regressed on the Fama-French five factors:\n\n  \nR_{HML, PLC, t} = \\alpha_{HML, PLC} + \\beta' f_t + \\epsilon_{HML, PLC, t} \\quad \\text{(Eq. (1))}\n \n\nA non-zero alpha ($\\alpha_{HML, PLC}$) indicates that the five-factor model cannot fully explain the returns of the strategy. Table 1 presents these alphas.\n\n**Table 1: Fama-French 5-Factor Alphas (%) for HML Portfolios**\n\n| HML Portfolio | Short PLC | Medium PLC | Long PLC |\n| :--- | :---: | :---: | :---: |\n| **Alpha (%)** | **-0.11** | **0.02** | **0.46** |\n| *t-statistic* | *(-1.82)* | *(0.23)* | *(3.44)* |\n| **Long-Short (Alpha Spread)** | | | **0.57** |\n| *t-statistic* | | | *(4.03)* |\n\n*Source: Table 2, Panel B of the paper. Alphas are in percent per month.* \n\n---\n\nAccording to the results in Table 1, which of the following statements are INVALID interpretations of the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n\nB,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 43, "Question": "### Background\n\n**Research Question.** Is the negative stock price contagion from a bond downgrade transmitted to rivals through changes in their expected future cash flows, and does this transmission mechanism depend on the downgraded firm's credit quality?\n\n**Setting / Data-Generating Environment.** The study investigates the link between stock price reactions (measured by Cumulative Abnormal Returns, `CAR`) and earnings forecast revisions (measured by Cumulative Abnormal Forecast Revisions, `CAFR`) for rival firms following a downgrade announcement. The central hypothesis is that revisions to expected cash flows (proxied by `CAFR`) are the channel driving the valuation changes (measured by `CAR`).\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return for rival firms over the `[-1, 0]` day window (dimensionless, as a percentage).\n- `CAFR`: Cumulative Abnormal Forecast Revision for rival firms over months `t=0` to `t=3` (dimensionless, as a percentage).\n- **Group 1:** Downgrade occurs within the investment-grade category.\n- **Group 2:** Downgrade is from investment to non-investment grade.\n- **Group 3:** Downgrade occurs within the non-investment-grade category.\n\n---\n\n### Data / Model Specification\n\nTable 1 synthesizes the key contagion results for rival firms, showing both the stock market reaction and the earnings forecast revision across the three groups of downgraded firms.\n\n**Table 1. Contagion Effects on Rival Firms**\n\n| Group | Rival CAR (%) <br> (Days -1, 0) | Rival CAR <br> Z-stat | Rival CAFR <br> (Months 0-3) | Rival CAFR <br> t-stat |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | -0.12 | (-1.45) | -0.40 | (-1.42) |\n| 2 | -0.37 | (-0.29) | -1.19 | (-1.69) |\n| 3 | -0.34 | (-2.15)* | -0.69 | (-2.75)** |\n\n*Significant at the 5% level. **Significant at the 1% level.\n\n---\n\n### Question\n\nBased on the evidence in Table 1 regarding contagion effects on rival firms, select all of the following statements that are correct.", "Options": {"A": "The evidence suggests that for rivals of Group 1 firms, the market prices in a negative reaction (significant CAR) even though analysts do not revise earnings forecasts downwards (insignificant CAFR).", "B": "Rivals of firms in Group 3 experience a statistically significant negative revision to their earnings forecasts (CAFR).", "C": "Rivals of firms in Group 2 experience a statistically significant negative stock return (CAR), but not a significant earnings forecast revision (CAFR).", "D": "Rivals of firms in Group 3 experience a statistically significant negative stock return (CAR) following a downgrade announcement."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize results from a table to identify the specific conditions under which contagion occurs. It uses an 'atomic decomposition' strategy, breaking the paper's main finding into two independently verifiable statements (significant CAR and significant CAFR in Group 3). Distractors test for common misinterpretations of statistical significance (Option C) and misreading of table values for other groups (Option D).", "qid": "43", "question": "### Background\n\n**Research Question.** Is the negative stock price contagion from a bond downgrade transmitted to rivals through changes in their expected future cash flows, and does this transmission mechanism depend on the downgraded firm's credit quality?\n\n**Setting / Data-Generating Environment.** The study investigates the link between stock price reactions (measured by Cumulative Abnormal Returns, `CAR`) and earnings forecast revisions (measured by Cumulative Abnormal Forecast Revisions, `CAFR`) for rival firms following a downgrade announcement. The central hypothesis is that revisions to expected cash flows (proxied by `CAFR`) are the channel driving the valuation changes (measured by `CAR`).\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return for rival firms over the `[-1, 0]` day window (dimensionless, as a percentage).\n- `CAFR`: Cumulative Abnormal Forecast Revision for rival firms over months `t=0` to `t=3` (dimensionless, as a percentage).\n- **Group 1:** Downgrade occurs within the investment-grade category.\n- **Group 2:** Downgrade is from investment to non-investment grade.\n- **Group 3:** Downgrade occurs within the non-investment-grade category.\n\n---\n\n### Data / Model Specification\n\nTable 1 synthesizes the key contagion results for rival firms, showing both the stock market reaction and the earnings forecast revision across the three groups of downgraded firms.\n\n**Table 1. Contagion Effects on Rival Firms**\n\n| Group | Rival CAR (%) <br> (Days -1, 0) | Rival CAR <br> Z-stat | Rival CAFR <br> (Months 0-3) | Rival CAFR <br> t-stat |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | -0.12 | (-1.45) | -0.40 | (-1.42) |\n| 2 | -0.37 | (-0.29) | -1.19 | (-1.69) |\n| 3 | -0.34 | (-2.15)* | -0.69 | (-2.75)** |\n\n*Significant at the 5% level. **Significant at the 1% level.\n\n---\n\n### Question\n\nBased on the evidence in Table 1 regarding contagion effects on rival firms, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "\n\nA,D,B", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 169, "Question": "### Background\n\n**Research Question.** What are the dynamic effects of a shock to housing market tightness on market liquidity and prices, and do these empirical effects align with predictions from search and matching theory?\n\n**Setting.** The analysis uses a panel VAR model estimated on quarterly data for 403 Dutch municipalities from 2011-2013. The model examines the interrelationships between market tightness, liquidity, and prices.\n\n**Variables & Parameters.**\n- `Δpr`: Quarterly change in the log house price index.\n- `Δros`: Quarterly change in the rate of sale (liquidity).\n- `Δcph`: Quarterly change in the log clicks per house (market tightness).\n- `t-1`: Subscript denoting a time lag of one quarter.\n\n**Theoretical Prediction.** Search and matching models with information asymmetries predict that a positive shock to market tightness causes an immediate increase in liquidity (matching probability), followed by a more gradual increase in prices as sellers slowly update their reservation prices.\n\n### Data / Model Specification\n\nThe core empirical findings are derived from a panel VAR model. Table 1 below presents a selection of key coefficient estimates, and the text summarizes the impulse-response function (IRF) analysis.\n\n**Table 1. Selected Panel VAR Regression Results**\n\n| Dependent Variable | Regressor | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- |\n| `Δpr_t` | `Δcph_{t-1}` | 0.0036 | (2.6) |\n| `Δros_t` | `Δcph_{t-1}` | 0.0031 | (1.9) |\n\n*Notes: The model includes two lags of all variables and fixed effects. Coefficients are estimated using GMM.*\n\n**Impulse-Response Function Summary:**\nA one-standard-deviation shock to `Δcph` (an increase of 4.6%) leads to:\n- A temporary increase in the rate of sale (`ros`) of approximately 0.1 percentage points after one quarter, which reverts to the pre-shock level by the second quarter.\n- A gradual and permanent increase in the price level (`pr`) of roughly 0.4%.\n\n### Question\n\nBased on the provided regression results and impulse-response function summary, which of the following statements about the impact of a market tightness shock (`Δcph`) are supported by the evidence? (Select all that apply)", "Options": {"A": "The effect of lagged market tightness (`Δcph_{t-1}`) on house price growth (`Δpr_t`) is positive and statistically significant at the 5% level.", "B": "The impulse-response analysis indicates that a one-standard-deviation shock to market tightness results in a permanent increase in the house price level.", "C": "The impulse-response analysis suggests that the impact of a market tightness shock on market liquidity (`ros`) is both gradual and permanent.", "D": "The effect of lagged market tightness (`Δcph_{t-1}`) on the change in the rate of sale (`Δros_t`) is statistically significant at the 1% level."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret regression coefficients and impulse-response function summaries. It uses an **Atomic Decomposition** strategy, breaking down the findings into distinct, verifiable statements. Correct options (A, B) accurately reflect the statistical significance from the table and the permanent nature of the price shock from the IRF summary. Distractors are based on common errors: (C) misstates the significance level for the liquidity effect (t=1.9 is significant at 10%, not 1%), and (D) incorrectly describes the liquidity response, which is immediate and temporary, not gradual and permanent.", "qid": "169", "question": "### Background\n\n**Research Question.** What are the dynamic effects of a shock to housing market tightness on market liquidity and prices, and do these empirical effects align with predictions from search and matching theory?\n\n**Setting.** The analysis uses a panel VAR model estimated on quarterly data for 403 Dutch municipalities from 2011-2013. The model examines the interrelationships between market tightness, liquidity, and prices.\n\n**Variables & Parameters.**\n- `Δpr`: Quarterly change in the log house price index.\n- `Δros`: Quarterly change in the rate of sale (liquidity).\n- `Δcph`: Quarterly change in the log clicks per house (market tightness).\n- `t-1`: Subscript denoting a time lag of one quarter.\n\n**Theoretical Prediction.** Search and matching models with information asymmetries predict that a positive shock to market tightness causes an immediate increase in liquidity (matching probability), followed by a more gradual increase in prices as sellers slowly update their reservation prices.\n\n### Data / Model Specification\n\nThe core empirical findings are derived from a panel VAR model. Table 1 below presents a selection of key coefficient estimates, and the text summarizes the impulse-response function (IRF) analysis.\n\n**Table 1. Selected Panel VAR Regression Results**\n\n| Dependent Variable | Regressor | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- |\n| `Δpr_t` | `Δcph_{t-1}` | 0.0036 | (2.6) |\n| `Δros_t` | `Δcph_{t-1}` | 0.0031 | (1.9) |\n\n*Notes: The model includes two lags of all variables and fixed effects. Coefficients are estimated using GMM.*\n\n**Impulse-Response Function Summary:**\nA one-standard-deviation shock to `Δcph` (an increase of 4.6%) leads to:\n- A temporary increase in the rate of sale (`ros`) of approximately 0.1 percentage points after one quarter, which reverts to the pre-shock level by the second quarter.\n- A gradual and permanent increase in the price level (`pr`) of roughly 0.4%.\n\n### Question\n\nBased on the provided regression results and impulse-response function summary, which of the following statements about the impact of a market tightness shock (`Δcph`) are supported by the evidence? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 195, "Question": "### Background\n\n**Research Question.** This case examines the economic trade-offs of supervisory board size and its empirical impact on firm performance within the Danish “semi-two-tier” corporate governance system.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish firms (1998–2001). The institutional context is characterized by supervisory boards that are, on average, much smaller than their U.S. counterparts.\n\n---\n\n### Data / Model Specification\n\nThe primary empirical model is a cross-sectional OLS regression:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot BOARDSIZE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       |\n| :---------- | :--------- |\n| BOARDSIZE (N) | 5.2241     |\n\n**Table 2: Regression of Tobin's Q on Board Size**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Board size             | -0.051      | (0.482)   |\n\n---\n\n### Question\n\nSelect all statements that provide a valid interpretation of or explanation for the regression results concerning board size.", "Options": {"A": "A plausible explanation for the null finding is that the true relationship is concave (initially beneficial, then detrimental), and the sample of small Danish boards is concentrated in the initial, flatter portion of this curve.", "B": "The paper reports a strong positive correlation between `Board size` and firm `SIZE`, which invalidates the regression results entirely due to multicollinearity.", "C": "The negative sign of the coefficient (-0.051), despite being insignificant, confirms the agency cost theory that larger boards are always harmful.", "D": "The statistically insignificant coefficient on `Board size` fails to support the hypothesis that larger boards are detrimental to firm performance in this sample."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to interpret a null result and critique a model's functional form based on theory and sample characteristics. It uses a **Premise/Assumption Packaging** strategy, asking for valid interpretations and explanations. Distractors are designed to be subtle: (B) incorrectly treats an insignificant coefficient as confirmatory evidence (a conceptual error), and (D) overstates the consequence of multicollinearity, a common 'Almost Right' misconception.", "qid": "195", "question": "### Background\n\n**Research Question.** This case examines the economic trade-offs of supervisory board size and its empirical impact on firm performance within the Danish “semi-two-tier” corporate governance system.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish firms (1998–2001). The institutional context is characterized by supervisory boards that are, on average, much smaller than their U.S. counterparts.\n\n---\n\n### Data / Model Specification\n\nThe primary empirical model is a cross-sectional OLS regression:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot BOARDSIZE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       |\n| :---------- | :--------- |\n| BOARDSIZE (N) | 5.2241     |\n\n**Table 2: Regression of Tobin's Q on Board Size**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Board size             | -0.051      | (0.482)   |\n\n---\n\n### Question\n\nSelect all statements that provide a valid interpretation of or explanation for the regression results concerning board size.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 171, "Question": "### Background\n\n**Research Question.** Is the transmission of a housing demand shock from market tightness to prices and liquidity state-dependent, varying with local market conditions such as urbanicity and recent performance?\n\n**Setting.** The study investigates geographical and state-dependent heterogeneity by splitting its panel of Dutch municipalities into subsamples and re-estimating the panel VAR model for each.\n\n**Variables & Parameters.**\n- `Δpr_t`: Quarterly change in the log house price index.\n- `Δros_t`: Quarterly change in the rate of sale (liquidity).\n- `Δcph_{t-1}`, `Δcph_{t-2}`: Changes in log clicks per house (market tightness), lagged one and two quarters.\n\n### Data / Model Specification\n\nThe panel VAR model is estimated separately for different subsamples. Table 1 summarizes the key coefficients for the effect of lagged market tightness on price and liquidity changes.\n\n**Table 1. Panel VAR Results for Urban vs. Rural and High- vs. Low-Return Areas**\n\n| Panel | Sample | Dependent Var. | Regressor | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| A | **Urban** | `Δros_t` | `Δcph_{t-1}` | 0.0093 | (2.3) |\n| | **Rural** | `Δros_t` | `Δcph_{t-1}` | 0.0012 | (0.4) |\n| B | **50% Highest Returns** | `Δpr_t` | `Δcph_{t-1}` | 0.0053 | (2.7) |\n| | | `Δpr_t` | `Δcph_{t-2}` | -0.0011 | (-0.5) |\n| | **50% Lowest Returns** | `Δpr_t` | `Δcph_{t-1}` | 0.0042 | (1.7) |\n| | | `Δpr_t` | `Δcph_{t-2}` | 0.0039 | (1.9) |\n\n*Notes: Results are from separate VAR estimations on each subsample.*\n\n### Question\n\nBased on the subsample analysis in Table 1, which of the following conclusions about the heterogeneity of housing market dynamics are supported by the results? (Select all that apply)", "Options": {"A": "The effect of a market tightness shock on liquidity is stronger and more statistically significant in rural areas than in urban areas.", "B": "Price discovery is slower in low-return regions, as the impact of a market tightness shock on prices (`Δpr_t`) is spread across both the first and second lags.", "C": "In high-return markets, the price response to a market tightness shock is statistically insignificant.", "D": "The transmission of a market tightness shock to liquidity (`Δros_t`) is statistically significant in urban areas but not in rural areas."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to compare and synthesize results across different subsamples. It uses an **Atomic Decomposition** strategy, creating separate, testable claims about the urban/rural and high/low return findings. Correct options (A, B) accurately describe the key differences shown in the table: the presence of a liquidity channel only in urban areas, and the delayed price response in low-return areas. Distractors are designed based on direct misreadings of the table: (C) is a conceptual opposite (the effect is highly significant), and (D) reverses the urban/rural finding.", "qid": "171", "question": "### Background\n\n**Research Question.** Is the transmission of a housing demand shock from market tightness to prices and liquidity state-dependent, varying with local market conditions such as urbanicity and recent performance?\n\n**Setting.** The study investigates geographical and state-dependent heterogeneity by splitting its panel of Dutch municipalities into subsamples and re-estimating the panel VAR model for each.\n\n**Variables & Parameters.**\n- `Δpr_t`: Quarterly change in the log house price index.\n- `Δros_t`: Quarterly change in the rate of sale (liquidity).\n- `Δcph_{t-1}`, `Δcph_{t-2}`: Changes in log clicks per house (market tightness), lagged one and two quarters.\n\n### Data / Model Specification\n\nThe panel VAR model is estimated separately for different subsamples. Table 1 summarizes the key coefficients for the effect of lagged market tightness on price and liquidity changes.\n\n**Table 1. Panel VAR Results for Urban vs. Rural and High- vs. Low-Return Areas**\n\n| Panel | Sample | Dependent Var. | Regressor | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| A | **Urban** | `Δros_t` | `Δcph_{t-1}` | 0.0093 | (2.3) |\n| | **Rural** | `Δros_t` | `Δcph_{t-1}` | 0.0012 | (0.4) |\n| B | **50% Highest Returns** | `Δpr_t` | `Δcph_{t-1}` | 0.0053 | (2.7) |\n| | | `Δpr_t` | `Δcph_{t-2}` | -0.0011 | (-0.5) |\n| | **50% Lowest Returns** | `Δpr_t` | `Δcph_{t-1}` | 0.0042 | (1.7) |\n| | | `Δpr_t` | `Δcph_{t-2}` | 0.0039 | (1.9) |\n\n*Notes: Results are from separate VAR estimations on each subsample.*\n\n### Question\n\nBased on the subsample analysis in Table 1, which of the following conclusions about the heterogeneity of housing market dynamics are supported by the results? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 182, "Question": "### Background\n\n**Research Question.** Do certain equity styles share common long-run stochastic trends (i.e., are they cointegrated), making them redundant for long-term diversification? And if so, is a portfolio constructed of only the independent styles better diversified?\n\n**Setting and Data.** The analysis uses Johansen cointegration tests on Wilshire style indices (WLG, WLV, WSG, WSV). Based on these findings, it constructs two portfolios from Fama-French indices: `INDE` (Large Growth and Small Value) and `4STYLE` (all four styles). The diversification of these portfolios is then tested.\n\n---\n\n### Data / Model Specification\n\n1.  **Cointegration Test:** The Johansen trace statistic tests the null hypothesis of `r` cointegrating relationships. A low p-value for `H_0: r = 0` indicates the presence of cointegration.\n\n2.  **Diversification Test:** The residuals (`ε_it`) from a four-factor model regression on a portfolio's returns are regressed on two style factors: `VAL_t` (Value minus Growth) and `CAP_t` (Small minus Large).\n\n  \n\\varepsilon_{it} = \\psi_{i0} + \\psi_{i1} \\mathrm{VAL}_t + \\psi_{i2} \\mathrm{CAP}_t + \\omega_t \\quad \\text{(Eq. (1))}\n \nA well-diversified portfolio should have insignificant coefficients `ψ_i1` and `ψ_i2`.\n\n**Table 1: Bivariate Cointegration Rank Test of Wilshire Indices**\n\n| Style Pair | Null Hypothesis (`H_0`) | `λ_trace` Statistic | p-value |\n| :--- | :--- | :--- | :--- |\n| WLV and WSG | `r = 0` | 16.03 | 0.04 |\n\n**Table 2: Style Portfolio Diversification Comparison**\n\n| Portfolio | `ψ_i1` (VAL Coeff.) | t-statistic | `ψ_i2` (CAP Coeff.) | t-statistic |\n| :--- | :--- | :--- | :--- | :--- |\n| 4STYLE | 0.011 | 1.994 | 0.019 | 3.082 |\n| INDE | -0.001 | -0.029 | -0.002 | -0.032 |\n\n---\n\n### Question\n\nBased on the cointegration and diversification test results provided, select all statements that accurately describe the paper's findings on style diversification.", "Options": {"A": "The `INDE` portfolio, consisting of only Large Growth and Small Value styles, is shown to be well-diversified, as its four-factor model residuals have no significant remaining exposure to broad value or size factors.", "B": "The cointegration test provides statistically significant evidence (at the 5% level) that the Large Value (WLV) and Small Growth (WSG) styles share a common long-run trend, making them redundant diversifiers.", "C": "The diversification test for the `4STYLE` portfolio suggests that its risk-return profile could be improved by increasing its allocation to large-cap and growth stocks.", "D": "The cointegration between WLV and WSG implies that a portfolio combining only these two styles would have lower total risk (standard deviation) than a portfolio of independent styles."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: This item tests the ability to synthesize findings from two different econometric tests (cointegration and OLS on residuals) to form a cohesive conclusion. It uses an Atomic Decomposition strategy. Correct options A and B are direct interpretations of the two key tables. Distractor C is a Conceptual Opposite, as the positive coefficients in Table 2 for 4STYLE imply a need for more *value* and *small-cap* exposure. Distractor D targets a common misconception about cointegration, confusing long-run co-movement with short-term volatility reduction, making it a high-fidelity distractor.", "qid": "182", "question": "### Background\n\n**Research Question.** Do certain equity styles share common long-run stochastic trends (i.e., are they cointegrated), making them redundant for long-term diversification? And if so, is a portfolio constructed of only the independent styles better diversified?\n\n**Setting and Data.** The analysis uses Johansen cointegration tests on Wilshire style indices (WLG, WLV, WSG, WSV). Based on these findings, it constructs two portfolios from Fama-French indices: `INDE` (Large Growth and Small Value) and `4STYLE` (all four styles). The diversification of these portfolios is then tested.\n\n---\n\n### Data / Model Specification\n\n1.  **Cointegration Test:** The Johansen trace statistic tests the null hypothesis of `r` cointegrating relationships. A low p-value for `H_0: r = 0` indicates the presence of cointegration.\n\n2.  **Diversification Test:** The residuals (`ε_it`) from a four-factor model regression on a portfolio's returns are regressed on two style factors: `VAL_t` (Value minus Growth) and `CAP_t` (Small minus Large).\n\n  \n\\varepsilon_{it} = \\psi_{i0} + \\psi_{i1} \\mathrm{VAL}_t + \\psi_{i2} \\mathrm{CAP}_t + \\omega_t \\quad \\text{(Eq. (1))}\n \nA well-diversified portfolio should have insignificant coefficients `ψ_i1` and `ψ_i2`.\n\n**Table 1: Bivariate Cointegration Rank Test of Wilshire Indices**\n\n| Style Pair | Null Hypothesis (`H_0`) | `λ_trace` Statistic | p-value |\n| :--- | :--- | :--- | :--- |\n| WLV and WSG | `r = 0` | 16.03 | 0.04 |\n\n**Table 2: Style Portfolio Diversification Comparison**\n\n| Portfolio | `ψ_i1` (VAL Coeff.) | t-statistic | `ψ_i2` (CAP Coeff.) | t-statistic |\n| :--- | :--- | :--- | :--- | :--- |\n| 4STYLE | 0.011 | 1.994 | 0.019 | 3.082 |\n| INDE | -0.001 | -0.029 | -0.002 | -0.032 |\n\n---\n\n### Question\n\nBased on the cointegration and diversification test results provided, select all statements that accurately describe the paper's findings on style diversification.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 7, "Question": "### Background\n\n**Research Question.** This case investigates whether non-seasonal, global climatic phenomena can predict anomaly returns, as a test to rule out simple seasonality as an explanation for weather-related predictability.\n\n**Setting / Data-Generating Environment.** The analysis uses a 12-month moving average (MA) of the global temperature anomaly to predict the monthly excess returns of the Earnings-to-price (E/P) strategy.\n\n---\n\n### Data / Model Specification\n\nThe core predictive regression is:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \nwhere `X_{t-1}` is the 12-month MA of the global temperature anomaly.\n\n**Table 1: Climatic Variables and Anomaly Performance (Monthly Excess Returns, %)**\n\n| Predictive Variable / Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Global temp. anomaly (12-mo MA)** | | | | |\n| Earnings-to-price | 1.11 | -2.24 | 0.75 | 1.47 |\n| | [5.10] | [-2.18] | [2.44] | [4.78] |\n\n*Note: `t`-statistics are in brackets. `a_H` and `a_L` are returns conditional on the predictor being above/below its median.*\n\n---\n\n### Question\n\nBased on the provided information, select all of the following statements that are correct.", "Options": {"A": "The E/P strategy performs significantly better when the 12-month MA of global temperature is above its median (`a_H > a_L`).", "B": "The estimated coefficient `b = -2.24` implies that a 0.5°C increase in the smoothed global temperature anomaly is predicted to decrease the monthly E/P premium by 1.12%.", "C": "A primary reason for using a 12-month moving average of temperature is to remove the influence of seasonal weather cycles from the predictor variable.", "D": "By using a global temperature measure, the author confirms that the mood of NYC-based traders is the primary channel for weather effects."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test the purpose of a time-series filter and the interpretation of a regression coefficient. Option A correctly identifies the purpose of a 12-month moving average. Option B is a correct calculation based on the coefficient (Computational Judgment). Distractor C is a Factual Error, directly contradicting the values in Table 1. Distractor D is a Conceptual Opposite to the paper's argument; using a global measure is meant to make the local trader mood story *less* plausible, thereby strengthening the satire.", "qid": "7", "question": "### Background\n\n**Research Question.** This case investigates whether non-seasonal, global climatic phenomena can predict anomaly returns, as a test to rule out simple seasonality as an explanation for weather-related predictability.\n\n**Setting / Data-Generating Environment.** The analysis uses a 12-month moving average (MA) of the global temperature anomaly to predict the monthly excess returns of the Earnings-to-price (E/P) strategy.\n\n---\n\n### Data / Model Specification\n\nThe core predictive regression is:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \nwhere `X_{t-1}` is the 12-month MA of the global temperature anomaly.\n\n**Table 1: Climatic Variables and Anomaly Performance (Monthly Excess Returns, %)**\n\n| Predictive Variable / Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Global temp. anomaly (12-mo MA)** | | | | |\n| Earnings-to-price | 1.11 | -2.24 | 0.75 | 1.47 |\n| | [5.10] | [-2.18] | [2.44] | [4.78] |\n\n*Note: `t`-statistics are in brackets. `a_H` and `a_L` are returns conditional on the predictor being above/below its median.*\n\n---\n\n### Question\n\nBased on the provided information, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 175, "Question": "### Background\n\nA monopolistic dealer uses two tools—the ask price `a` and the ask depth `z`—to manage risk from informed traders while profiting from liquidity traders. The dealer's expected profit function when facing a risk-neutral informed trader is given below.\n\n**Variables & Parameters.**\n- `a`: The ask price set by the dealer.\n- `z`: The ask quantity limit (depth).\n- `v`: The informed trader's private valuation of the asset.\n- `d(a)`: The demand from the liquidity trader at price `a`.\n- `I[·]`: The indicator function.\n\n---\n\n### Data / Model Specification\n\nThe dealer's expected profit function is:\n  \nE[\\pi]=E\\big[I[v\\leq a](a-v)\\big]E\\big[I[d(a)\\geq0]\\operatorname*{min}(d(a),z)\\big] + E\\big[I[v>a](a-v)\\big]z \\quad \\text{(Eq. (1))}\n \nThe first-order condition with respect to depth `z` is found by setting its partial derivative to zero:\n  \n\\frac{\\partial E[\\pi]}{\\partial z} = E\\big[I[v\\leq a](a-v)\\big] P(d(a)>z) + E\\big[I[v>a](a-v)\\big] = 0 \\quad \\text{(Eq. (2))}\n \nThis can be rearranged to show the optimal balance between the marginal benefit and marginal cost of providing depth:\n`Marginal Benefit = E\\big[I[v\\leq a](a-v)\\big] P(d(a)>z) = -E\\big[I[v>a](a-v)\\big] = Marginal Cost`\n\n---\n\nBased on this framework, which of the following statements are valid interpretations or consequences of the model?", "Options": {"A": "An increase in information asymmetry (e.g., a higher probability that `v > a`) increases the marginal cost of providing depth, leading the dealer to set a lower optimal depth `z*`.", "B": "The marginal benefit of increasing depth `z` is the expected profit from a marginal sale to a liquidity trader, which occurs only if the informed trader is absent and the liquidity trader's demand is large enough to be constrained by the depth limit.", "C": "The depth `z` is a tool of *selection* that filters out informed traders, while the price `a` is a tool of *mitigation* that caps losses on trades that do occur.", "D": "The model implies that depth is a more targeted instrument than price for managing adverse selection because informed traders are distinguished primarily by their desire for large trade sizes."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the user's understanding of the economic trade-offs involved in setting market depth, as formalized by the model's first-order conditions. It tests the interpretation of marginal costs/benefits, comparative statics, and the core economic intuition of the paper. \nDepth Strategy: Reverse-Reasoning. The user is given the formal model and must identify the correct economic interpretations and conclusions that follow from it. \nDistractor Logic: Option C is a Conceptual Opposite. It correctly identifies the two tools (price and depth) and the two mechanisms (selection and mitigation) but incorrectly swaps their roles. Price is the selection tool (filtering who trades), while depth is the mitigation tool (capping losses).", "qid": "175", "question": "### Background\n\nA monopolistic dealer uses two tools—the ask price `a` and the ask depth `z`—to manage risk from informed traders while profiting from liquidity traders. The dealer's expected profit function when facing a risk-neutral informed trader is given below.\n\n**Variables & Parameters.**\n- `a`: The ask price set by the dealer.\n- `z`: The ask quantity limit (depth).\n- `v`: The informed trader's private valuation of the asset.\n- `d(a)`: The demand from the liquidity trader at price `a`.\n- `I[·]`: The indicator function.\n\n---\n\n### Data / Model Specification\n\nThe dealer's expected profit function is:\n  \nE[\\pi]=E\\big[I[v\\leq a](a-v)\\big]E\\big[I[d(a)\\geq0]\\operatorname*{min}(d(a),z)\\big] + E\\big[I[v>a](a-v)\\big]z \\quad \\text{(Eq. (1))}\n \nThe first-order condition with respect to depth `z` is found by setting its partial derivative to zero:\n  \n\\frac{\\partial E[\\pi]}{\\partial z} = E\\big[I[v\\leq a](a-v)\\big] P(d(a)>z) + E\\big[I[v>a](a-v)\\big] = 0 \\quad \\text{(Eq. (2))}\n \nThis can be rearranged to show the optimal balance between the marginal benefit and marginal cost of providing depth:\n`Marginal Benefit = E\\big[I[v\\leq a](a-v)\\big] P(d(a)>z) = -E\\big[I[v>a](a-v)\\big] = Marginal Cost`\n\n---\n\nBased on this framework, which of the following statements are valid interpretations or consequences of the model?", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 177, "Question": "### Background\n\n**Research Question.** This case examines whether executive stock options incentivize managers to strategically report *small* positive earnings surprises, a practice often interpreted as \"storing\" earnings for future periods. The analysis is conditional on firms having already met or beaten analyst forecasts, aiming to distinguish incentives for earnings management from incentives for strong performance.\n\n**Hypothesis.** The core hypothesis (H2) is that the likelihood a firm reports a small earnings surprise (0 to 1 cent per share), conditional on reporting a non-negative surprise, is positively related to the extent to which its senior managers are compensated with stock options.\n\n### Data / Model Specification\n\nThe study estimates two logistic regression models. The first models the probability of meeting or beating the forecast (`MEET=1`). The second models the probability of reporting a small surprise (`SMALL=1`), conditional on having met or beaten the forecast.\n\n1.  `P(MEET_{i,q}=1)` is modeled on the full sample (N=40,661).\n2.  `P(SMALL_{i,q}=1 | MEET_{i,q}=1)` is modeled on the subsample with non-negative surprises (N=29,672).\n\n`SMALL` is an indicator equal to 1 if the earnings surprise is in [0, 1] cents per share, and 0 if the surprise is > 1 cent per share. `OPT` is the proportion of executive pay from stock options. `ROA` is the quarterly return on assets.\n\n**Table 1: Logistic Regression Results**\n\n| Variable | Panel A: P(MEET=1) | Panel B: P(SMALL=1 | MEET=1) |\n| :--- | :---: | :---: |\n| | Coefficient (β) | Coefficient (γ) |\n| `OPT` | 0.199** | 0.323** |\n| `ROA` | 6.740** | -1.581* |\n\n*N=40,661 (Panel A), N=29,672 (Panel B). **p<0.01, *p<0.05.*\n\n### Question\n\nBased on the provided regression results and context, select all statements that accurately interpret the findings regarding the `ROA` coefficient and the causal argument for earnings management.", "Options": {"A": "The positive `ROA` coefficient in Panel A reflects that fundamentally more profitable firms are more likely to meet earnings targets.", "B": "The positive `OPT` coefficient in Panel A (0.199) is stronger evidence of earnings management than the `OPT` coefficient in Panel B (0.323) because it applies to a larger sample.", "C": "The negative `ROA` coefficient in Panel B is consistent with the idea that, conditional on meeting a forecast, the most profitable firms are likely to report large surprises, making a small surprise less probable.", "D": "The Panel B result is less likely to be driven by an unobserved 'firm quality' confounder, as high-quality firms would arguably signal their strength with large earnings beats, not small ones."}, "Answer": ["A", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret contrasting regression coefficients and evaluate causal arguments, core skills for consuming empirical research. It uses an Atomic Decomposition strategy, breaking down the original multi-part question into distinct, verifiable statements. Options A, B, and D are correct interpretations drawn directly from the source answer. Option C is a high-fidelity distractor (True but Irrelevant premise, False conclusion) that misrepresents the paper's central argument about the strength of causal evidence.", "qid": "177", "question": "### Background\n\n**Research Question.** This case examines whether executive stock options incentivize managers to strategically report *small* positive earnings surprises, a practice often interpreted as \"storing\" earnings for future periods. The analysis is conditional on firms having already met or beaten analyst forecasts, aiming to distinguish incentives for earnings management from incentives for strong performance.\n\n**Hypothesis.** The core hypothesis (H2) is that the likelihood a firm reports a small earnings surprise (0 to 1 cent per share), conditional on reporting a non-negative surprise, is positively related to the extent to which its senior managers are compensated with stock options.\n\n### Data / Model Specification\n\nThe study estimates two logistic regression models. The first models the probability of meeting or beating the forecast (`MEET=1`). The second models the probability of reporting a small surprise (`SMALL=1`), conditional on having met or beaten the forecast.\n\n1.  `P(MEET_{i,q}=1)` is modeled on the full sample (N=40,661).\n2.  `P(SMALL_{i,q}=1 | MEET_{i,q}=1)` is modeled on the subsample with non-negative surprises (N=29,672).\n\n`SMALL` is an indicator equal to 1 if the earnings surprise is in [0, 1] cents per share, and 0 if the surprise is > 1 cent per share. `OPT` is the proportion of executive pay from stock options. `ROA` is the quarterly return on assets.\n\n**Table 1: Logistic Regression Results**\n\n| Variable | Panel A: P(MEET=1) | Panel B: P(SMALL=1 | MEET=1) |\n| :--- | :---: | :---: |\n| | Coefficient (β) | Coefficient (γ) |\n| `OPT` | 0.199** | 0.323** |\n| `ROA` | 6.740** | -1.581* |\n\n*N=40,661 (Panel A), N=29,672 (Panel B). **p<0.01, *p<0.05.*\n\n### Question\n\nBased on the provided regression results and context, select all statements that accurately interpret the findings regarding the `ROA` coefficient and the causal argument for earnings management.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 55, "Question": "### Background\n\n**Research Question.** This case seeks to explain the observed asymmetry in monetary policy spillovers, where U.S. quantitative easing (QE) significantly impacts Chinese bond yields, while QE from other major central banks (BoE, ECB, BoJ) does not.\n\n**Setting and Sample.** The analysis is based on an event study of two-day changes in Chinese Treasury yields following QE announcements from the U.S., U.K., Eurozone, and Japan.\n\n---\n\n### Data / Model Specification\n\nThe empirical analysis reveals a stark asymmetry. Following major U.S. QE announcements, Chinese yields respond significantly. In contrast, following major QE announcements from other central banks that produced large moves in their own domestic yields, the impact on Chinese yields was marginal and statistically insignificant, as shown in **Table 1**.\n\n**Table 1: Asymmetric Spillover Effects of QE (Two-day yield changes in basis points)**\n| Announcement Event | Foreign 10Y Yield Change | Chinese 10Y Yield Change |\n| :--- | :--- | :--- |\n| U.S. QE I (Nov 25, 2008) | -31.30 | -28.04 |\n| U.K. QE (Mar 05, 2009) | -63.0 | -2.7 |\n| ECB QE (Jan 22, 2015) | -12.4 | -1.5 |\n| BoJ QE (Jan 29, 2016) | -16.1 | +4.9 |\n\n*Source: Abridged from Tables 1 and 2 in the paper.*\n\nThe paper notes that China's managed floating exchange rate regime gives the U.S. dollar a heavy weight in its currency basket, creating a tight linkage between the two countries' monetary policies.\n\n---\n\n### Question\n\nBased on the principles of international finance and the context provided, which of the following are the most likely economic reasons for the uniquely powerful spillover effect of U.S. monetary policy on China, as observed in **Table 1**? Select all that apply.", "Options": {"A": "The U.S. has a significantly larger GDP than the U.K., Eurozone, or Japan, meaning its domestic economic shocks are proportionally larger for the global economy.", "B": "The U.S. Treasury market serves as the global benchmark for pricing risk-free assets, forcing a worldwide repricing of duration and risk following a U.S. QE shock.", "C": "China's large trade surplus with the U.S. makes its economy exceptionally sensitive to changes in U.S. consumer demand, which is directly manipulated by QE.", "D": "The U.S. dollar's role as the world's primary reserve currency creates a tight policy linkage, as China's central bank must manage its exchange rate against the dollar, making U.S. policy signals highly relevant for the future path of Chinese policy."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the student's ability to synthesize standard international finance theory with the paper's core empirical finding. It replaces part (a) of the source QA. The correct options (A, B) identify the two primary channels discussed in the literature: the portfolio rebalancing channel amplified by the U.S. Treasury's benchmark status, and the signaling channel amplified by the dollar's reserve currency status. Distractor (C) is 'True but Irrelevant' for bond yield spillovers, as it relates to trade channels, not financial transmission. Distractor (D) is a plausible but less precise explanation; while GDP size matters, the unique *financial* roles of the dollar and Treasury market are the direct mechanisms for spillover, making it an 'Almost Right' distractor.", "qid": "55", "question": "### Background\n\n**Research Question.** This case seeks to explain the observed asymmetry in monetary policy spillovers, where U.S. quantitative easing (QE) significantly impacts Chinese bond yields, while QE from other major central banks (BoE, ECB, BoJ) does not.\n\n**Setting and Sample.** The analysis is based on an event study of two-day changes in Chinese Treasury yields following QE announcements from the U.S., U.K., Eurozone, and Japan.\n\n---\n\n### Data / Model Specification\n\nThe empirical analysis reveals a stark asymmetry. Following major U.S. QE announcements, Chinese yields respond significantly. In contrast, following major QE announcements from other central banks that produced large moves in their own domestic yields, the impact on Chinese yields was marginal and statistically insignificant, as shown in **Table 1**.\n\n**Table 1: Asymmetric Spillover Effects of QE (Two-day yield changes in basis points)**\n| Announcement Event | Foreign 10Y Yield Change | Chinese 10Y Yield Change |\n| :--- | :--- | :--- |\n| U.S. QE I (Nov 25, 2008) | -31.30 | -28.04 |\n| U.K. QE (Mar 05, 2009) | -63.0 | -2.7 |\n| ECB QE (Jan 22, 2015) | -12.4 | -1.5 |\n| BoJ QE (Jan 29, 2016) | -16.1 | +4.9 |\n\n*Source: Abridged from Tables 1 and 2 in the paper.*\n\nThe paper notes that China's managed floating exchange rate regime gives the U.S. dollar a heavy weight in its currency basket, creating a tight linkage between the two countries' monetary policies.\n\n---\n\n### Question\n\nBased on the principles of international finance and the context provided, which of the following are the most likely economic reasons for the uniquely powerful spillover effect of U.S. monetary policy on China, as observed in **Table 1**? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 179, "Question": "### Background\n\n**Research Context.** A central motivation for this paper is the dramatic shift in executive compensation practices during the 1980s and 1990s, characterized by an \"explosion in stock option grants.\" The paper investigates the potential unintended consequences of this shift on managerial incentives for short-term earnings reporting.\n\n### Data / Model Specification\n\nThe paper documents this trend using data on S&P 1500 firms from 1992-2002.\n\n**Table 1: Median Stock-Based Compensation (`OPT_i`) Over Time**\n\n| Time Period | Median `OPT_i` |\n| :--- | :---: |\n| 1992–1994 | ~20% |\n| 1999–2002 | ~40% |\n\n*`OPT_i` is the Black-Scholes value of options granted as a percentage of total executive compensation.*\n\nThe Black-Scholes formula for a European call option is:\n  \nC(S, K, T, r, \\sigma) = S N(d_1) - K e^{-rT} N(d_2) \\quad \\text{(Eq. 1)}\n \nwhere `d_1 = \\frac{\\ln(S/K) + (r + \\sigma^2/2)T}{\\sigma\\sqrt{T}}` and `d_2 = d_1 - \\sigma\\sqrt{T}`. The option's sensitivity to the stock price is its Delta (`Δ = N(d_1)`).\n\n### Question\n\nRegarding the use of the Black-Scholes model for valuing executive stock options (ESOs) and the incentives they create, select all of the following statements that are correct.", "Options": {"A": "An executive compensated with deep OTM options has a weaker incentive to engage in small-scale earnings management than one with ATM options, because the OTM package has a lower total Delta, reducing the wealth gain from a small stock price increase.", "B": "The standard Black-Scholes model tends to overvalue ESOs because it fails to account for factors like forfeiture risk (if an executive leaves the firm) and the non-tradability that often leads to early exercise.", "C": "To provide an incentive package of equal value, a firm would grant fewer deep out-of-the-money (OTM) options than at-the-money (ATM) options, because OTM options have higher leverage.", "D": "The doubling of median option-based pay from 1992 to 2002 is particularly significant because it coincided with the market placing greater emphasis on meeting analyst forecasts, creating a powerful incentive for managers to hit short-term targets."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). Total Suitability Score = 9.5 (Conceptual Clarity = 9/10, Discriminability = 10/10). The original question's components are highly structured and rely on applying core finance principles (Black-Scholes assumptions, option greeks), making them ideal for conversion. High-fidelity distractors can be built around common misconceptions in option theory. This item uses an Atomic Decomposition strategy to test understanding of model limitations, incentive effects (Reverse-Reasoning), and the paper's motivating context. Distractor B contains a conceptual error about option pricing and the number of options required for equivalent value.", "qid": "179", "question": "### Background\n\n**Research Context.** A central motivation for this paper is the dramatic shift in executive compensation practices during the 1980s and 1990s, characterized by an \"explosion in stock option grants.\" The paper investigates the potential unintended consequences of this shift on managerial incentives for short-term earnings reporting.\n\n### Data / Model Specification\n\nThe paper documents this trend using data on S&P 1500 firms from 1992-2002.\n\n**Table 1: Median Stock-Based Compensation (`OPT_i`) Over Time**\n\n| Time Period | Median `OPT_i` |\n| :--- | :---: |\n| 1992–1994 | ~20% |\n| 1999–2002 | ~40% |\n\n*`OPT_i` is the Black-Scholes value of options granted as a percentage of total executive compensation.*\n\nThe Black-Scholes formula for a European call option is:\n  \nC(S, K, T, r, \\sigma) = S N(d_1) - K e^{-rT} N(d_2) \\quad \\text{(Eq. 1)}\n \nwhere `d_1 = \\frac{\\ln(S/K) + (r + \\sigma^2/2)T}{\\sigma\\sqrt{T}}` and `d_2 = d_1 - \\sigma\\sqrt{T}`. The option's sensitivity to the stock price is its Delta (`Δ = N(d_1)`).\n\n### Question\n\nRegarding the use of the Black-Scholes model for valuing executive stock options (ESOs) and the incentives they create, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "\"A,B,D\"", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 167, "Question": "### Background\n\nA study estimated a logistic regression model to predict the probability of *de novo* bank entry into a market. The sample consisted of 207 markets with entry and 205 matched markets without entry. The model included various market-relative variables to identify characteristics of attractive entry markets.\n\n### Data / Model Specification\n\nThe probability of entry is modeled as:\n  \nP(\\text{Entry}=1 | X) = \\frac{\\exp(X'\\beta)}{1 + \\exp(X'\\beta)}\n \n\n**Table 1: Logistic Regression Results for De Novo Entry**\n\n| Variable | Estimate (β) | p-value |\n| :--- | :--- | :--- |\n| MRROA (Market-relative ROA) | -0.8970 | 0.0072 |\n| MRSMLAMT (Market-relative amount of small-business loans) | 0.3523 | 0.1123 |\n| MRSMLNUM (Market-relative number of small-business loans) | -0.4076 | 0.0022 |\n| MRLOAN2A (Market-relative loans-to-assets) | 2.7092 | 0.0022 |\n\n---\n\nBased on the regression results in **Table 1**, which of the following market characteristics are statistically significant predictors of a higher likelihood of *de novo* bank entry? Select all that apply.", "Options": {"A": "Markets with a higher relative ratio of total loans to assets (higher MRLOAN2A).", "B": "Markets with a lower relative number of existing small-business loans (lower MRSMLNUM).", "C": "Markets with lower relative profitability (lower MRROA).", "D": "Markets with a higher relative dollar amount of existing small-business loans (higher MRSMLAMT)."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret the direction and statistical significance of coefficients in a logistic regression model. The depth strategy is 'Computational Judgment,' requiring the user to identify significant predictors and their directional impact on the outcome. Options A, B, and C correspond to variables with statistically significant coefficients (p < 0.05) and the correct sign interpretation (negative coefficients for MRROA and MRSMLNUM mean lower values predict entry; positive for MRLOAN2A means higher values predict entry). Option D is a distractor based on a statistically insignificant coefficient (p=0.1123), representing a plausible but unsupported conclusion.", "qid": "167", "question": "### Background\n\nA study estimated a logistic regression model to predict the probability of *de novo* bank entry into a market. The sample consisted of 207 markets with entry and 205 matched markets without entry. The model included various market-relative variables to identify characteristics of attractive entry markets.\n\n### Data / Model Specification\n\nThe probability of entry is modeled as:\n  \nP(\\text{Entry}=1 | X) = \\frac{\\exp(X'\\beta)}{1 + \\exp(X'\\beta)}\n \n\n**Table 1: Logistic Regression Results for De Novo Entry**\n\n| Variable | Estimate (β) | p-value |\n| :--- | :--- | :--- |\n| MRROA (Market-relative ROA) | -0.8970 | 0.0072 |\n| MRSMLAMT (Market-relative amount of small-business loans) | 0.3523 | 0.1123 |\n| MRSMLNUM (Market-relative number of small-business loans) | -0.4076 | 0.0022 |\n| MRLOAN2A (Market-relative loans-to-assets) | 2.7092 | 0.0022 |\n\n---\n\nBased on the regression results in **Table 1**, which of the following market characteristics are statistically significant predictors of a higher likelihood of *de novo* bank entry? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 188, "Question": "### Background\n\nA three-step methodology is used to value the U.S. government's implicit guarantee on Freddie Mac's mortgage portfolio. The method first uses historical data to price the *total* guarantee, then uses this price to infer the underlying asset volatility, and finally re-prices the government's portion of the guarantee by accounting for Freddie Mac's own capital.\n\n### Data / Model Specification\n\nThe methodology proceeds in three steps:\n1.  **Actuarial Premium:** An actuarial model estimates `P_total`, the premium for the combined Freddie Mac and government guarantee, assuming Freddie Mac has zero capital.\n2.  **Implied Volatility:** The actuarial premium `P_total` is used in the Black-Scholes model to solve for the implied volatility `σ` of the underlying real estate assets.\n3.  **Government Guarantee Value:** The implied volatility `σ` is used to price a new put option representing only the government's liability, `P_gov`. This is done by reducing the option's exercise price by the amount of Freddie Mac's loss-absorbing capital, `K`.\n\n### Question\n\nBased on the logic of this three-step valuation methodology, which of the following statements are valid conclusions or direct consequences of the model's structure? Select all that apply.", "Options": {"A": "The value of the total premium, `P_total`, is calculated by directly observing the market price of credit default swaps on Freddie Mac's debt.", "B": "An increase in Freddie Mac's loss-absorbing capital `K` will decrease the value of the government guarantee `P_gov`, holding all other parameters constant.", "C": "The model assumes that the government's guarantee is exercised only after Freddie Mac's own capital and reserves have been completely exhausted.", "D": "The implied volatility `σ` calculated in Step 2 reflects the risk of the underlying real estate assets, independent of Freddie Mac's capital structure."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the comprehension of the logical structure and key assumptions of the paper's central valuation methodology. Depth Strategy: Reverse-Reasoning. The user must identify the necessary assumptions and consequences that flow from the described model. Distractor Logic: Option D is a plausible but incorrect method for valuing the guarantee; the paper explicitly states it uses an actuarial model based on historical losses, not market-based instruments. Options A, B, and C are all direct and crucial features of the model's design: (A) `σ` is a property of the assets, derived before capital is considered; (B) increasing capital `K` reduces the government's effective exercise price, lowering the put option's value; (C) the reduction of the exercise price by `K` mathematically models the fact that Freddie Mac's capital is the first-loss tranche.", "qid": "188", "question": "### Background\n\nA three-step methodology is used to value the U.S. government's implicit guarantee on Freddie Mac's mortgage portfolio. The method first uses historical data to price the *total* guarantee, then uses this price to infer the underlying asset volatility, and finally re-prices the government's portion of the guarantee by accounting for Freddie Mac's own capital.\n\n### Data / Model Specification\n\nThe methodology proceeds in three steps:\n1.  **Actuarial Premium:** An actuarial model estimates `P_total`, the premium for the combined Freddie Mac and government guarantee, assuming Freddie Mac has zero capital.\n2.  **Implied Volatility:** The actuarial premium `P_total` is used in the Black-Scholes model to solve for the implied volatility `σ` of the underlying real estate assets.\n3.  **Government Guarantee Value:** The implied volatility `σ` is used to price a new put option representing only the government's liability, `P_gov`. This is done by reducing the option's exercise price by the amount of Freddie Mac's loss-absorbing capital, `K`.\n\n### Question\n\nBased on the logic of this three-step valuation methodology, which of the following statements are valid conclusions or direct consequences of the model's structure? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 199, "Question": "### Background\n\nThe **Editorial Differences Hypothesis** posits that publication times differ across peer journals due to internal operational factors. The paper identifies three key mechanisms: editorial structure, editorial philosophy, and individual editor speed.\n\n- **Editorial Structure:** The organization of the editorial team (e.g., single editor vs. many delegated editors).\n- **Editorial Philosophy:** The degree to which an editor actively manages the review process. A 'passive' editor forwards all referee requests to the author, while an 'active' editor filters, prioritizes, and resolves contradictory requests to create a clearer path to convergence.\n\n### Scenario\n\nAn 'active' editor at the *Journal of Financial Economics* receives two referee reports on a paper. Referee 1 loves the paper and recommends acceptance after minor revisions. Referee 2 raises a valid but secondary point and suggests a new, time-consuming robustness test that would take 6 months to complete. The editor, after their own reading, believes the new test is not essential for the paper's main conclusion.\n\n---\n\nAccording to the principles of the Editorial Differences Hypothesis, which of the following actions would be characteristic of this 'active' editor's approach, designed to decrease acceptance time?\n\nSelect all that apply.", "Options": {"A": "The editor sends a decision letter asking the author to complete the minor revisions from Referee 1 and to write a brief discussion in the paper acknowledging Referee 2's point and justifying why the additional test is not essential.", "B": "The editor sends both referee reports to the author with a note saying, \"Please address all referee comments.\"", "C": "The editor sends a decision letter asking the author to complete only the minor revisions from Referee 1 and to ignore the time-consuming test from Referee 2.", "D": "The editor makes a final decision on the paper based on their own judgment and the existing referee reports, without requiring another round of revisions."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to apply a theoretical concept ('active' vs. 'passive' editorial philosophy) to a specific, realistic scenario. It tests for a deep understanding of the mechanisms underlying the paper's main hypothesis.\n\nChosen Strategy: Scenario Application. The question provides a micro-case and asks the user to identify all actions consistent with the 'active editor' model described in the paper.\n\nDistractor Logic:\n- **A, B, C (Correct):** These are all plausible actions of an 'active' editor. They involve making a judgment call to resolve conflict or ambiguity, providing a clear path forward, and reducing the author's workload, all of which would decrease acceptance time. Option A is a direct filter. Option B is the most active choice. Option C is a pragmatic compromise.\n- **D (Incorrect):** This is the definition of a 'passive' editor. It is a conceptually opposite distractor that a candidate who understands the active/passive distinction should easily identify as incorrect.", "qid": "199", "question": "### Background\n\nThe **Editorial Differences Hypothesis** posits that publication times differ across peer journals due to internal operational factors. The paper identifies three key mechanisms: editorial structure, editorial philosophy, and individual editor speed.\n\n- **Editorial Structure:** The organization of the editorial team (e.g., single editor vs. many delegated editors).\n- **Editorial Philosophy:** The degree to which an editor actively manages the review process. A 'passive' editor forwards all referee requests to the author, while an 'active' editor filters, prioritizes, and resolves contradictory requests to create a clearer path to convergence.\n\n### Scenario\n\nAn 'active' editor at the *Journal of Financial Economics* receives two referee reports on a paper. Referee 1 loves the paper and recommends acceptance after minor revisions. Referee 2 raises a valid but secondary point and suggests a new, time-consuming robustness test that would take 6 months to complete. The editor, after their own reading, believes the new test is not essential for the paper's main conclusion.\n\n---\n\nAccording to the principles of the Editorial Differences Hypothesis, which of the following actions would be characteristic of this 'active' editor's approach, designed to decrease acceptance time?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 185, "Question": "### Background\n\n**Research Question.** What is the causal effect of institutional fund flows on equity style returns?\n\n**Setting and Data.** A researcher estimates the following model using OLS, where `R_pt` is the return of a style portfolio and `DIFF_t` is a measure of net institutional fund flows:\n\n  \nR_{pt} = a_{i0} + b_{i1}R_{mt} + b_{i2}SMB_t + b_{i3}HML_t + b_{i4}UMD_t + b_{i5}\\mathrm{DIFF}_t + \\upsilon_t \\quad \\text{(Eq. (1))}\n \n\nThere is a concern that `DIFF_t` is endogenous because an unobserved economic variable could drive both fund flows and portfolio returns simultaneously. An instrumental variable (IV) approach is proposed to address this.\n\n---\n\n### Data / Model Specification\n\nAn instrumental variable, `Z_t`, can be used to obtain a consistent estimate of `b_i5` if it meets two critical conditions.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the necessary conditions for a variable `Z_t` to be a valid instrument for `DIFF_t` in **Eq. (1)**.", "Options": {"A": "The instrument `Z_t` must be correlated with the endogenous variable `DIFF_t`, a condition known as 'instrument relevance'.", "B": "The one-month lagged portfolio return, `R_{pt-1}`, would be a valid instrument because it is known to be a strong predictor of fund flows.", "C": "The instrument `Z_t` must be uncorrelated with the error term `υ_t` in Eq. (1), meaning it only affects portfolio returns `R_pt` through its effect on fund flows `DIFF_t`. This is known as the 'exclusion restriction'.", "D": "The primary purpose of using an instrumental variable is to correct for heteroskedasticity in the error term `υ_t`."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item tests foundational knowledge of causal inference and econometrics, specifically the theory of instrumental variables. It uses a Premise/Assumption Packaging strategy, asking for the core conditions of a valid IV. Correct options A and B state the two canonical conditions (relevance and exclusion). Distractor C presents a classic example of an *invalid* instrument that violates the exclusion restriction, making it a high-fidelity distractor for students who only focus on the relevance condition. Distractor D is a Conceptual Error, confusing the purpose of IV (endogeneity) with that of robust standard errors (heteroskedasticity).", "qid": "185", "question": "### Background\n\n**Research Question.** What is the causal effect of institutional fund flows on equity style returns?\n\n**Setting and Data.** A researcher estimates the following model using OLS, where `R_pt` is the return of a style portfolio and `DIFF_t` is a measure of net institutional fund flows:\n\n  \nR_{pt} = a_{i0} + b_{i1}R_{mt} + b_{i2}SMB_t + b_{i3}HML_t + b_{i4}UMD_t + b_{i5}\\mathrm{DIFF}_t + \\upsilon_t \\quad \\text{(Eq. (1))}\n \n\nThere is a concern that `DIFF_t` is endogenous because an unobserved economic variable could drive both fund flows and portfolio returns simultaneously. An instrumental variable (IV) approach is proposed to address this.\n\n---\n\n### Data / Model Specification\n\nAn instrumental variable, `Z_t`, can be used to obtain a consistent estimate of `b_i5` if it meets two critical conditions.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the necessary conditions for a variable `Z_t` to be a valid instrument for `DIFF_t` in **Eq. (1)**.", "model_evaluation_result": {"model_raw_response": "\"A,C\"", "model_answer": ["A", "C"], "error": null}}
{"ID": 16, "Question": "### Background\n\n**Research Question.** How do analytical bounds for the fair periodic premium of a unit-linked endowment perform numerically, and what are the implications for pricing practice?\n\n**Setting.** The fair periodic premium for a unit-linked contract with guarantee `G=1000` is calculated using comonotonic lower and upper bounds. The calculations are performed for different policyholder ages (`x=30, 50`), contract maturities (`T=10, 15`), investment shares (`a=0.4, 0.6`), and initial term structures of interest rates (TSIR I: flat, II: normal, III: inverse).\n\n### Data / Model Specification\n\nThe fair premium `P` is bounded by `P^l \\leq P \\leq P^u`. These bounds are calculated by replacing the true (but intractable) embedded call option prices with their analytical lower and upper bounds, respectively. The following table presents a selection of these calculated premium bounds.\n\n**Table 1: Lower and Upper Bounds for the Fair Periodic Premium (G=1000)**\n\n| Age | Share (a) | Maturity T=10, TSIR II | Maturity T=15, TSIR III |\n|:---:|:---------:|:----------------------:|:-----------------------:|\n| 30  | 0.4       | 75.17 / 75.32          | 54.93 / 55.08          |\n| 30  | 0.6       | 79.51 / 85.09          | 61.64 / 70.45          |\n| 50  | 0.4       | 79.12 / 79.31          | 61.29 / 61.46          |\n| 50  | 0.6       | 84.03 / 90.31          | 69.48 / 80.55          |\n\n*Note: Table values are presented as `P^l / P^u`.*\n\n---\n\nBased on the data in **Table 1**, which of the following statements are valid interpretations or conclusions?\n", "Options": {"A": "The gap between the lower and upper premium bounds (`P^u - P^l`) is consistently wider for the higher investment share (`a=0.6`) compared to the lower share (`a=0.4`) in all presented scenarios.", "B": "For a 30-year-old with a 10-year maturity and TSIR II, increasing the investment share `a` from 0.4 to 0.6 increases the average premium by more than 5 currency units.", "C": "Holding the investment share `a` constant at 0.6, the average premium for a 50-year-old is consistently higher than for a 30-year-old across both scenarios shown in the table.", "D": "The tightness of the premium bounds for the `a=0.4` cases suggests that the underlying portfolio's distribution is well-approximated by a comonotonic distribution under the paper's model parameters."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses the ability to perform calculations from a table and draw valid theoretical and comparative inferences, targeting the core numerical results of the paper. It uses a Computational Judgment strategy.\n- **Option A (Correct):** Requires calculating two average premiums and their difference. Age 30, a=0.4 avg = (75.17+75.32)/2 = 75.245. Age 30, a=0.6 avg = (79.51+85.09)/2 = 82.3. The difference is 7.055, which is > 5.\n- **Option B (Correct):** This is a key theoretical conclusion from the paper. The tightness of the bounds indicates the approximation is very good, which happens when the underlying sum of log-normals is 'rather' comonotonic.\n- **Option C (Distractor - Almost Right):** This is true for T=10, TSIR II (87.17 vs 82.3) but false for T=15, TSIR III (75.015 vs 66.045). This tests careful, comprehensive checking of the data.\n- **Option D (Correct):** Requires calculating the gap for all four pairs of scenarios. For a=0.4, gaps are 0.15 and 0.15. For a=0.6, gaps are 5.58 and 8.81. The gaps are indeed consistently wider for a=0.6.", "qid": "16", "question": "### Background\n\n**Research Question.** How do analytical bounds for the fair periodic premium of a unit-linked endowment perform numerically, and what are the implications for pricing practice?\n\n**Setting.** The fair periodic premium for a unit-linked contract with guarantee `G=1000` is calculated using comonotonic lower and upper bounds. The calculations are performed for different policyholder ages (`x=30, 50`), contract maturities (`T=10, 15`), investment shares (`a=0.4, 0.6`), and initial term structures of interest rates (TSIR I: flat, II: normal, III: inverse).\n\n### Data / Model Specification\n\nThe fair premium `P` is bounded by `P^l \\leq P \\leq P^u`. These bounds are calculated by replacing the true (but intractable) embedded call option prices with their analytical lower and upper bounds, respectively. The following table presents a selection of these calculated premium bounds.\n\n**Table 1: Lower and Upper Bounds for the Fair Periodic Premium (G=1000)**\n\n| Age | Share (a) | Maturity T=10, TSIR II | Maturity T=15, TSIR III |\n|:---:|:---------:|:----------------------:|:-----------------------:|\n| 30  | 0.4       | 75.17 / 75.32          | 54.93 / 55.08          |\n| 30  | 0.6       | 79.51 / 85.09          | 61.64 / 70.45          |\n| 50  | 0.4       | 79.12 / 79.31          | 61.29 / 61.46          |\n| 50  | 0.6       | 84.03 / 90.31          | 69.48 / 80.55          |\n\n*Note: Table values are presented as `P^l / P^u`.*\n\n---\n\nBased on the data in **Table 1**, which of the following statements are valid interpretations or conclusions?\n", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 138, "Question": "### Background\n\n**Research Question.** This case examines the cross-sectional variation of the 'turn-of-the-month' (TOM) stock market anomaly across different Chinese market segments, seeking to understand if the effect is driven by local investor behavior or international spillovers.\n\n**Setting and Sample.** The study uses daily returns from 1994-2010 for various indices. Key distinctions are made between Mainland China's A-shares (primarily for domestic investors) vs. B-shares (accessible to foreign investors).\n\n**Variables and Parameters.**\n- `Return_{I,t}`: Daily return for index `I`.\n- `TOM_t`: Binary indicator for the turn-of-the-month period.\n- `SP500(lag,1)_t`: Lagged daily return on the S&P 500 index.\n- `β₁`: Coefficient measuring the TOM premium.\n- `β₂`: Coefficient measuring sensitivity to U.S. market spillovers.\n\n---\n\n### Data / Model Specification\n\nTwo models are estimated. The first is a simple regression to capture the baseline TOM effect. The second is an augmented model that controls for spillovers from the U.S. market.\n\n  \n\\text{Model 1: } \\mathrm{Return}_{I,t} = \\alpha_I + \\beta_{1,I} \\cdot \\mathrm{TOM}_{t} + e_{I,t}\n \n  \n\\text{Model 2: } \\mathrm{Return}_{I,t} = \\alpha_I + \\beta_{1,I} \\cdot \\mathrm{TOM}_{t} + \\beta_{2,I} \\cdot \\mathrm{SP500(lag,1)}_{t} + u_{I,t}\n \n\n**Table 1. Baseline TOM Effect (from Model 1)**\n\n| Market Index | Description | `β₁` (in %) | t-statistic |\n| :--- | :--- | :---: | :---: |\n| SHGSEA | Shanghai A-Share | 0.164 | (1.922)* |\n| SHGSEB | Shanghai B-Share | 0.270 | (2.655)*** |\n\n**Table 2. TOM Effect with US Market Control (from Model 2)**\n\n| Market Index | `β₁` (in %) | t-stat (`β₁`) | `β₂` | t-stat (`β₂`) |\n| :--- | :---: | :---: | :---: | :---: |\n| SHGSEA | 0.154 | (1.805)* | 0.117 | (4.701)*** |\n| SHGSEB | 0.251 | (2.478)** | 0.189 | (6.333)*** |\n\n*Notes: ***, **, * denote significance at 1%, 5%, and 10% levels respectively.*\n\n---\n\n### Question\n\nBased on the provided models and results, which of the following statements accurately compare the Shanghai A-share (SHGSEA) and B-share (SHGSEB) markets? Select all that apply.", "Options": {"A": "The sensitivity to U.S. market returns (`β₂`) is statistically insignificant for the A-share market.", "B": "The B-share market is more integrated with the U.S. market than the A-share market.", "C": "The B-share market exhibits a stronger baseline TOM effect than the A-share market.", "D": "The A-share market's TOM effect is statistically insignificant in the baseline model (Table 1)."}, "Answer": ["B", "C"], "pi_justification": "This item replaces part of a QA problem (Suitability Score: 9.5). It assesses the ability to extract and compare coefficients from regression tables and link them to economic concepts (anomaly magnitude, market integration). Option A requires comparing `β₁` from Table 1. Option B requires comparing `β₂` from Table 2. Distractors C and D are based on common misreadings of statistical significance (confusing 10% significance with insignificance, and ignoring a large t-statistic).", "qid": "138", "question": "### Background\n\n**Research Question.** This case examines the cross-sectional variation of the 'turn-of-the-month' (TOM) stock market anomaly across different Chinese market segments, seeking to understand if the effect is driven by local investor behavior or international spillovers.\n\n**Setting and Sample.** The study uses daily returns from 1994-2010 for various indices. Key distinctions are made between Mainland China's A-shares (primarily for domestic investors) vs. B-shares (accessible to foreign investors).\n\n**Variables and Parameters.**\n- `Return_{I,t}`: Daily return for index `I`.\n- `TOM_t`: Binary indicator for the turn-of-the-month period.\n- `SP500(lag,1)_t`: Lagged daily return on the S&P 500 index.\n- `β₁`: Coefficient measuring the TOM premium.\n- `β₂`: Coefficient measuring sensitivity to U.S. market spillovers.\n\n---\n\n### Data / Model Specification\n\nTwo models are estimated. The first is a simple regression to capture the baseline TOM effect. The second is an augmented model that controls for spillovers from the U.S. market.\n\n  \n\\text{Model 1: } \\mathrm{Return}_{I,t} = \\alpha_I + \\beta_{1,I} \\cdot \\mathrm{TOM}_{t} + e_{I,t}\n \n  \n\\text{Model 2: } \\mathrm{Return}_{I,t} = \\alpha_I + \\beta_{1,I} \\cdot \\mathrm{TOM}_{t} + \\beta_{2,I} \\cdot \\mathrm{SP500(lag,1)}_{t} + u_{I,t}\n \n\n**Table 1. Baseline TOM Effect (from Model 1)**\n\n| Market Index | Description | `β₁` (in %) | t-statistic |\n| :--- | :--- | :---: | :---: |\n| SHGSEA | Shanghai A-Share | 0.164 | (1.922)* |\n| SHGSEB | Shanghai B-Share | 0.270 | (2.655)*** |\n\n**Table 2. TOM Effect with US Market Control (from Model 2)**\n\n| Market Index | `β₁` (in %) | t-stat (`β₁`) | `β₂` | t-stat (`β₂`) |\n| :--- | :---: | :---: | :---: | :---: |\n| SHGSEA | 0.154 | (1.805)* | 0.117 | (4.701)*** |\n| SHGSEB | 0.251 | (2.478)** | 0.189 | (6.333)*** |\n\n*Notes: ***, **, * denote significance at 1%, 5%, and 10% levels respectively.*\n\n---\n\n### Question\n\nBased on the provided models and results, which of the following statements accurately compare the Shanghai A-share (SHGSEA) and B-share (SHGSEB) markets? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 210, "Question": "### Background\n\nThe paper critiques the FHA's use of a **mortgage-based** data framework for risk assessment, arguing it systematically understates risk compared to a **borrower-based** framework. The key difference is the treatment of internal FHA-to-FHA refinances.\n\n*   In the **mortgage-based** framework, an internal refinance is recorded as a *prepayment* of the old loan. When forecasting, the model treats all predicted prepayments as terminal events, removing the loan's risk from the portfolio.\n*   In the **borrower-based** framework, an internal refinance is an intermediate event. The borrower's risk exposure only terminates upon a true exit from the FHA system (e.g., sale) or a default.\n\nThe FHA defends its mortgage-based approach by arguing that *in-sample*, the new refinanced loan replaces the old one, so the risk is not lost from the historical data used for model estimation.\n\n### Question\n\nAccording to the paper's argument, which of the following are valid consequences or critiques of the FHA's mortgage-based risk assessment methodology?", "Options": {"A": "The FHA's in-sample defense is valid, proving that the mortgage-based framework is robust for both estimation and forecasting.", "B": "It causes a temporal misattribution of risk, where a default by a borrower from an early vintage (e.g., 2007) who refinanced is incorrectly assigned to a later vintage (e.g., 2009).", "C": "The methodology suffers from an out-of-sample forecasting fallacy, as the simulation of future prepayments fails to carry forward the risk of borrowers who internally refinance.", "D": "It systematically overestimates the rate at which risk leaves the FHA system, leading to an underestimation of future cumulative defaults."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the comprehension of the paper's central methodological critique regarding the mortgage-based vs. borrower-based frameworks. Depth Strategy: Reverse-Reasoning (identifying all valid consequences of a flawed methodology). Distractor Logic: Options A, B, and C are all distinct, correct restatements of the paper's core arguments against the FHA's methodology. Option D is a 'Conceptual Opposite' distractor; it presents the FHA's flawed defense as a valid conclusion, directly contradicting the paper's main thesis.", "qid": "210", "question": "### Background\n\nThe paper critiques the FHA's use of a **mortgage-based** data framework for risk assessment, arguing it systematically understates risk compared to a **borrower-based** framework. The key difference is the treatment of internal FHA-to-FHA refinances.\n\n*   In the **mortgage-based** framework, an internal refinance is recorded as a *prepayment* of the old loan. When forecasting, the model treats all predicted prepayments as terminal events, removing the loan's risk from the portfolio.\n*   In the **borrower-based** framework, an internal refinance is an intermediate event. The borrower's risk exposure only terminates upon a true exit from the FHA system (e.g., sale) or a default.\n\nThe FHA defends its mortgage-based approach by arguing that *in-sample*, the new refinanced loan replaces the old one, so the risk is not lost from the historical data used for model estimation.\n\n### Question\n\nAccording to the paper's argument, which of the following are valid consequences or critiques of the FHA's mortgage-based risk assessment methodology?", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 15, "Question": "### Background\n\n**Research Question.** What are the empirical determinants of eurobond issuance spreads?\n\n**Setting.** A linear regression model is specified to explain issuance spreads. One of the explanatory variables is `FEES`, the gross fees charged by the underwriting syndicate, intended as a proxy for primary market efficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper notes that there is no clear theoretical prediction for the sign of the coefficient on `FEES`. The empirical analysis finds a statistically significant positive coefficient.\n\n---\n\n### Question\n\nAccording to the paper's discussion, which of the following are valid economic rationales that could explain the relationship between `FEES` and bond spreads? Select all that apply.", "Options": {"A": "Issuers that are less appealing to investors may have to pay both higher spreads to attract capital and higher fees to underwriters, creating a positive correlation.", "B": "Higher fees may reflect a more intensive marketing effort by the syndicate, which generates stronger investor demand and allows the issuer to place the bond at a lower spread.", "C": "Issuers pay higher fees to reputable underwriters to signal high bond quality to the market, which in turn leads to lower spreads.", "D": "Higher fees are a direct component of the bond's yield-to-maturity, so the relationship between fees and spread must be positive by definition."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: Tests comprehension of competing theoretical arguments presented in the text. Strategy: Scenario Application. Distractor Logic: The question requires identifying the two competing hypotheses discussed in the paper. (A) describes the 'Risk Correlation Hypothesis' (predicts positive sign). (B) describes the 'Cost Pass-Through / Effort Hypothesis' (predicts negative sign). Both are discussed as valid a priori arguments. (C) is a conceptual error, confusing fees paid by the issuer with the spread paid to investors. (D) describes a plausible signaling story, but it is not one of the two hypotheses mentioned in the paper, making it a 'True but Irrelevant' type of distractor.", "qid": "15", "question": "### Background\n\n**Research Question.** What are the empirical determinants of eurobond issuance spreads?\n\n**Setting.** A linear regression model is specified to explain issuance spreads. One of the explanatory variables is `FEES`, the gross fees charged by the underwriting syndicate, intended as a proxy for primary market efficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper notes that there is no clear theoretical prediction for the sign of the coefficient on `FEES`. The empirical analysis finds a statistically significant positive coefficient.\n\n---\n\n### Question\n\nAccording to the paper's discussion, which of the following are valid economic rationales that could explain the relationship between `FEES` and bond spreads? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"empirical determinants of eurobond issuance spreads FEES proxy for primary market efficiency positive coefficient\"\n> 📊 **Results requested:** 20\n\n> 🔍 **20 results found**\n\n> [🔗 \"Euro area sovereign yield spreads as determinants of private sector borrowing costs - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0264999318313907)\n> [🔗 \"Sovereign bond yield spreads and market sentiment and expectations: Empirical evidence from Euro area countries - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0148619514000526)\n> [🔗 \"Sovereign Eurobond Liquidity and Yields in: IMF Working Papers Volume 2022 Issue 098 (2022)\" IMF eLibrary](https://www.elibrary.imf.org/view/journals/001/2022/098/article-A001-en.xml?rskey=dXwSV3&amp%25253Bresult=8)\n> [🔗 \"The determinants of sovereign bond yield spreads in the EMU\" European Central Bank](https://www.ecb.europa.eu/pub/pdf/scpwps/ecbwp1781.en.pdf)\n> [🔗 \"The determinants of sovereign risk premiums in the UK and the European government bond market: the impact of Brexit - PMC\" PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC9152663/)\n> [🔗 \"Expected issuance fees and market liquidity - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1386418119300795)\n> [🔗 \"Heterogeneity of the determinants of euro-area sovereign bond spreads; what does it tell us about financial stability? - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0378426613002951)\n> [🔗 \"Determinants of sovereign bond yield spreads in the EMU: An optimal currency area perspective - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0014292114000919)\n> [🔗 \"The euro area bond market\" European Central Bank](https://www.ecb.europa.eu/press/key/date/2025/html/ecb.sp250611_1~cd38594925.en.html)\n> [🔗 \"Europa\" European Commission](https://economy-finance.ec.europa.eu/system/files/2021-06/dp141_en.pdf)\n> [🔗 \"Sovereign Eurobond Liquidity and Yields\" IMF](https://www.imf.org/en/Publications/WP/Issues/2022/05/20/Sovereign-Eurobond-Liquidity-and-Yields-518222)\n> [🔗 \"Determinants of Sovereign Eurobonds Yield Spread by Khaled Amira :: SSRN\" SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=563436)\n> [🔗 \"Determinants of intra-euro area government bond spreads during the financial crisis - Publications Office of the EU\" Publications Office of the EU](https://op.europa.eu/en/publication-detail/-/publication/0ef2315c-6b52-4dd1-b345-9df9ca6ba9c9/language-en)\n> [🔗 \"Determinants of the sovereign bond yield spreads in the Euro Area in the context of the economic and financial crisis | Banco de Portugal\" Banco de Portugal](https://www.bportugal.pt/paper/determinants-sovereign-bond-yield-spreads-euro-area-context-economic-and-financial-crisis)\n> [🔗 \"Issue costs in the Eurobond market: The effects of market integration - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0378426605000269)\n> [🔗 \"Intra-euro area government bond spreads\" European Commission](https://ec.europa.eu/economy_finance/publications/pages/publication16255_en.pdf)\n> [🔗 \"Emerging market yield spreads: Domestic, external determinants, and volatility spillovers - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1044028311000196)\n> [🔗 \"Determinants of intra-euro area government bond spreads during the financial crisis\" IDEAS/RePEc](https://ideas.repec.org/p/euf/ecopap/0388.html)\n> [🔗 \"Determinants of the sovereign bond yield spreads in the Euro Area in the context of the economic and financial crisis\" IDEAS/RePEc](https://ideas.repec.org/p/ptu/wpaper/w201022.html)\n> [🔗 \"Eurobond: Definition, Issuers, Advantages and Disadvantages\" Investopedia](https://www.investopedia.com/terms/e/eurobond.asp)\n\n\nA", "model_answer": ["A"], "error": null}}
{"ID": 217, "Question": "### Background\n\n**Research Question.** Can circuit breakers effectively stabilize financial markets by taming endogenous volatility, and how does this conclusion hold up against the efficient-market critique that such interventions merely delay necessary price discovery, especially when the fundamental value is non-stationary?\n\n**Setting.** The paper's agent-based model is used as a laboratory to study the effects of circuit breakers. The analysis is made more realistic by allowing the log fundamental value `F_t` to evolve as a random walk, which provides a framework for evaluating Fama's critique that circuit breakers hinder the market's ability to track new information.\n\n**Variables and Parameters.**\n- `P_t`: Log price of the stock at time `t`.\n- `F_t`: Log fundamental value at time `t`.\n- `n_t`: A normally distributed fundamental shock, `n_t \\sim N(0, (\\sigma^F)^2)`.\n- `s`: The circuit breaker threshold, i.e., the maximum allowed absolute log price change.\n- `V_t`: Market volatility, an EWMA of squared price changes.\n- `\\sigma_t^2`: Trading intensity (variance of speculator demand).\n- `\\rho_t`: Correlation of speculator demands (herding).\n- `C_t`: A condition measuring sharp price reversals.\n- Distortion: A measure of the deviation between `P_t` and `F_t`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental value evolves as a random walk:\n  \nF_t = F_{t-1} + n_t \\quad \\text{(Eq. 1)}\n \nThe potential price `P_{t+1}^*` is calculated based on aggregate demand, and the circuit breaker rule determines the actual price `P_{t+1}`:\n  \nP_{t+1} = \\begin{cases} P_t + s & \\text{if } (P_{t+1}^* - P_t) > s \\\\ P_t - s & \\text{if } (P_{t+1}^* - P_t) < -s \\\\ P_{t+1}^* & \\text{otherwise} \\end{cases} \\quad \\text{(Eq. 2)}\n \nKey endogenous feedback variables are defined as:\n  \nV_{t} = d V_{t-1} + (1-d)(P_{t}-P_{t-1})^{2} \\quad \\text{(Eq. 3)}\n \n  \n\\sigma_{t}^{2} = e^{l} + \\frac{e^{h}-e^{l}}{1+\\exp[-e^{s}(V_{t}-\\bar{V})]} \\quad \\text{(Eq. 4)}\n \n  \nC_{t} = ((P_{t}-P_{t-1})-(P_{t-1}-P_{t-2}))^{2} \\quad \\text{(Eq. 5)}\n \n  \n\\rho_{t} = f^{l} + \\frac{f^{h}-f^{l}}{1+\\exp[-f^{s}(C_{t}-\\bar{C})]} \\quad \\text{(Eq. 6)}\n \n\n---\n\n### Question\n\nThe paper uses the model to evaluate the effectiveness of circuit breakers, particularly in a market with a non-stationary fundamental value (`F_t`). Select all conclusions that are supported by the model's analysis.", "Options": {"A": "A trade-off exists: while generally stabilizing, an overly restrictive circuit breaker can increase price distortion by preventing the market price from tracking large, legitimate shocks to the fundamental value.", "B": "The model suggests that for any level of fundamental volatility (`\\sigma^F > 0`), circuit breakers will always reduce both price volatility and price distortion.", "C": "The model's results contradict the 'volatility spillover' hypothesis; by capping a large price change, circuit breakers reduce the input to the volatility feedback loop, leading to lower, not higher, subsequent volatility.", "D": "The primary reason for modeling the fundamental value `F_t` as a random walk is to show that circuit breakers are only effective in markets without new information."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to synthesize the model's key policy conclusions and trade-offs. It uses a **Premise Packaging** strategy, asking for all valid conclusions from the policy experiment. **Correct options (B, D)** capture the two central, nuanced findings: the refutation of the volatility spillover hypothesis and the existence of a trade-off where overly tight breakers can harm price discovery. **Distractors** target common misinterpretations: (A) misstates the reason for the random-walk assumption (Conceptual Opposite), while (C) presents an oversimplified 'always better' conclusion, ignoring the critical trade-off detailed in the paper (Almost Right).", "qid": "217", "question": "### Background\n\n**Research Question.** Can circuit breakers effectively stabilize financial markets by taming endogenous volatility, and how does this conclusion hold up against the efficient-market critique that such interventions merely delay necessary price discovery, especially when the fundamental value is non-stationary?\n\n**Setting.** The paper's agent-based model is used as a laboratory to study the effects of circuit breakers. The analysis is made more realistic by allowing the log fundamental value `F_t` to evolve as a random walk, which provides a framework for evaluating Fama's critique that circuit breakers hinder the market's ability to track new information.\n\n**Variables and Parameters.**\n- `P_t`: Log price of the stock at time `t`.\n- `F_t`: Log fundamental value at time `t`.\n- `n_t`: A normally distributed fundamental shock, `n_t \\sim N(0, (\\sigma^F)^2)`.\n- `s`: The circuit breaker threshold, i.e., the maximum allowed absolute log price change.\n- `V_t`: Market volatility, an EWMA of squared price changes.\n- `\\sigma_t^2`: Trading intensity (variance of speculator demand).\n- `\\rho_t`: Correlation of speculator demands (herding).\n- `C_t`: A condition measuring sharp price reversals.\n- Distortion: A measure of the deviation between `P_t` and `F_t`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental value evolves as a random walk:\n  \nF_t = F_{t-1} + n_t \\quad \\text{(Eq. 1)}\n \nThe potential price `P_{t+1}^*` is calculated based on aggregate demand, and the circuit breaker rule determines the actual price `P_{t+1}`:\n  \nP_{t+1} = \\begin{cases} P_t + s & \\text{if } (P_{t+1}^* - P_t) > s \\\\ P_t - s & \\text{if } (P_{t+1}^* - P_t) < -s \\\\ P_{t+1}^* & \\text{otherwise} \\end{cases} \\quad \\text{(Eq. 2)}\n \nKey endogenous feedback variables are defined as:\n  \nV_{t} = d V_{t-1} + (1-d)(P_{t}-P_{t-1})^{2} \\quad \\text{(Eq. 3)}\n \n  \n\\sigma_{t}^{2} = e^{l} + \\frac{e^{h}-e^{l}}{1+\\exp[-e^{s}(V_{t}-\\bar{V})]} \\quad \\text{(Eq. 4)}\n \n  \nC_{t} = ((P_{t}-P_{t-1})-(P_{t-1}-P_{t-2}))^{2} \\quad \\text{(Eq. 5)}\n \n  \n\\rho_{t} = f^{l} + \\frac{f^{h}-f^{l}}{1+\\exp[-f^{s}(C_{t}-\\bar{C})]} \\quad \\text{(Eq. 6)}\n \n\n---\n\n### Question\n\nThe paper uses the model to evaluate the effectiveness of circuit breakers, particularly in a market with a non-stationary fundamental value (`F_t`). Select all conclusions that are supported by the model's analysis.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 204, "Question": "### Background\n\n**Research Question.** The paper suggests a causal link between the introduction of an electronic trading (ET) system in 1999 and an observed improvement in market efficiency for FTSE100 futures. However, this simple pre-post empirical design is vulnerable to confounding variables.\n\n**Setting / Data-Generating Environment.** The analysis compares a measure of market inefficiency, `|VR(2) - 1|`, for FTSE100 futures in a period before the 1999 introduction of ET versus a period that includes the post-ET era. A decrease in this metric is interpreted as an efficiency gain caused by the new trading system.\n\n---\n\n### Question\n\nSelect ALL of the following statements that represent plausible confounding factors, challenging the paper's causal claim that the ET system *caused* the observed efficiency improvement.", "Options": {"A": "The implementation of the Sarbanes-Oxley Act in the US in 2002 is a likely cause for the observed efficiency gains in the UK futures market.", "B": "The results are confounded by the 2008 global financial crisis, which occurred during the post-automation sample period and dramatically increased market volatility.", "C": "The analysis period overlaps with the peak and collapse of the Dot-com bubble, a significant market regime shift that could be a confounding variable explaining the change in efficiency.", "D": "The study's pre-post comparison fails to account for the introduction of the Euro in 1999, a major structural change in European financial markets that could independently alter price dynamics."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses higher-order reasoning about causal inference and research design. It uses a 'Premise/Assumption Packaging' strategy, asking the user to identify valid critiques of the paper's methodology. This is a 'Reverse-Reasoning' task, requiring the user to think about alternative explanations for an observed outcome. The correct options (A, B) are major, contemporaneous historical events that serve as powerful confounders. Distractor C is a 'True but Irrelevant' fact due to incorrect timing (the sample ends in 2002), while D is thematically and geographically irrelevant, testing for careful consideration of context.", "qid": "204", "question": "### Background\n\n**Research Question.** The paper suggests a causal link between the introduction of an electronic trading (ET) system in 1999 and an observed improvement in market efficiency for FTSE100 futures. However, this simple pre-post empirical design is vulnerable to confounding variables.\n\n**Setting / Data-Generating Environment.** The analysis compares a measure of market inefficiency, `|VR(2) - 1|`, for FTSE100 futures in a period before the 1999 introduction of ET versus a period that includes the post-ET era. A decrease in this metric is interpreted as an efficiency gain caused by the new trading system.\n\n---\n\n### Question\n\nSelect ALL of the following statements that represent plausible confounding factors, challenging the paper's causal claim that the ET system *caused* the observed efficiency improvement.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 198, "Question": "### Background\n\nThe paper investigates whether differences in publication speeds across finance journals are persistent over time, testing two competing hypotheses.\n\n- **Journal Competition Hypothesis:** Predicts that publication times should converge across peer journals due to competitive pressures.\n- **Editorial Differences Hypothesis:** Predicts that publication times can differ persistently due to stable, internal factors like editorial structure and philosophy.\n\n### Data / Model Specification\n\nTo test for persistence, the paper presents median acceptance times for individual years.\n\n**Table 1. Median Acceptance Time by Year for Select Journals (2012-2015)**\n\n| Journal | 2012 | 2013 | 2014 | 2015 |\n| :--- | :--- | :--- | :--- | :--- |\n| Journal of Financial Economics (JFE) | 9.5 | 10.4 | 9.9 | 9.4 |\n| Review of Financial Studies (RFS) | 13.5 | 16.5 | 16.2 | 15.6 |\n| Journal of Finance (JF) | 17.0 | 19.8 | 19.7 | N/A |\n| **Top Twenty Journals (Combined)** | **11.7** | **11.8** | **11.7** | **11.6** |\n\n---\n\nBased on the data in **Table 1** and the descriptions of the competing hypotheses, which of the following statements are supported by the evidence?\n\nSelect all that apply.", "Options": {"A": "The data contradict the Journal Competition Hypothesis, as there is no evidence of convergence in acceptance times between these major competing journals.", "B": "In every year reported, the median acceptance time for JFE was faster than that of RFS.", "C": "The median acceptance time for the top twenty journals as a group is remarkably consistent, varying by only 0.2 months over the entire period.", "D": "The data support the Editorial Differences Hypothesis, as the speed rankings and large time gaps between JFE, RFS, and JF remain stable over the four-year period."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret time-series data to evaluate competing scientific hypotheses. It requires synthesizing the data patterns with the theoretical predictions of each hypothesis.\n\nChosen Strategy: Reverse-Reasoning. The user is given the empirical results (the table) and must identify all the theoretical conclusions and factual observations that are consistent with that evidence.\n\nDistractor Logic: All options are designed to be correct statements derived from the table and the background. This format forces the candidate to evaluate the evidence from multiple angles: its support for one hypothesis, its contradiction of another, a direct observation of the aggregate trend, and a direct comparison between two entities. This ensures a robust and comprehensive test of the candidate's analytical skills.", "qid": "198", "question": "### Background\n\nThe paper investigates whether differences in publication speeds across finance journals are persistent over time, testing two competing hypotheses.\n\n- **Journal Competition Hypothesis:** Predicts that publication times should converge across peer journals due to competitive pressures.\n- **Editorial Differences Hypothesis:** Predicts that publication times can differ persistently due to stable, internal factors like editorial structure and philosophy.\n\n### Data / Model Specification\n\nTo test for persistence, the paper presents median acceptance times for individual years.\n\n**Table 1. Median Acceptance Time by Year for Select Journals (2012-2015)**\n\n| Journal | 2012 | 2013 | 2014 | 2015 |\n| :--- | :--- | :--- | :--- | :--- |\n| Journal of Financial Economics (JFE) | 9.5 | 10.4 | 9.9 | 9.4 |\n| Review of Financial Studies (RFS) | 13.5 | 16.5 | 16.2 | 15.6 |\n| Journal of Finance (JF) | 17.0 | 19.8 | 19.7 | N/A |\n| **Top Twenty Journals (Combined)** | **11.7** | **11.8** | **11.7** | **11.6** |\n\n---\n\nBased on the data in **Table 1** and the descriptions of the competing hypotheses, which of the following statements are supported by the evidence?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 235, "Question": "### Background\n\n**Research Question.** How does the choice of a specific distribution within the Generalised Hyperbolic (GH) family, even when all choices provide a good statistical fit, quantitatively impact the estimation of extreme tail risk?\n\n**Setting.** A risk manager has fitted two different univariate distributions from the skewed GH family—the Student-t and the Variance Gamma (VG)—to the daily loss data of Goldman Sachs stock. Both models pass standard goodness-of-fit tests. The paper's final empirical results show that the Student-t model yields a `TCE_{0.999}` of 7.104, while the VG model yields a `TCE_{0.999}` of 4.841.\n\n---\n\n### Data / Model Specification\n\nThe Tail Conditional Expectation for a `GH_1` random variable `X` is given by:\n  \nTCE_q(X) = \\mu + \\frac{\\gamma}{1-q}k_{\\lambda}\\bar{F}_{GH_{1}}(x_{q};\\lambda+1) + \\frac{\\sigma^{2}}{1-q}k_{\\lambda}f_{GH_{1}}(x_{q};\\lambda+1) \\quad \\text{(Eq. (1))}\n \nThe Student-t distribution is known for its ability to model 'fat tails' (i.e., a higher probability of extreme events) compared to distributions like the Variance Gamma. This is governed by its degrees of freedom parameter `ν`, where a lower `ν` implies fatter tails.\n\n---\n\n### Question\n\nGiven the significant difference between the final `TCE_{0.999}` estimates (7.104 for Student-t vs. 4.841 for VG), select all of the following statements that represent a valid interpretation or conclusion.", "Options": {"A": "A prudent risk manager should favor the Student-t model's estimate because it provides a more conservative quantification of risk, which is appropriate when modeling extreme, low-probability loss events.", "B": "Since both models pass goodness-of-fit tests, the choice between them is arbitrary and should be based on whichever model is computationally simpler.", "C": "The Student-t model's higher TCE is primarily driven by its fatter tails, which increases the magnitude of the skewness (`γ`) and variance (`σ²`) risk contribution terms in Eq. (1).", "D": "The primary reason for the large difference in TCE estimates is the difference in the unconditional mean (`μ`) between the two models."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the quantitative results of risk modeling and connect them to the underlying conceptual differences between models (tail fatness). It uses a Reverse-Reasoning strategy (inferring model properties from outcomes) and premise packaging. Correct options (A, B) synthesize the quantitative results with the paper's core argument for using fat-tailed distributions. Distractor C is a Conceptual Opposite, misattributing the cause of the difference. Distractor D presents a common but flawed argument (True but Irrelevant premise leading to a wrong conclusion), testing the student's understanding of prudential risk management.", "qid": "235", "question": "### Background\n\n**Research Question.** How does the choice of a specific distribution within the Generalised Hyperbolic (GH) family, even when all choices provide a good statistical fit, quantitatively impact the estimation of extreme tail risk?\n\n**Setting.** A risk manager has fitted two different univariate distributions from the skewed GH family—the Student-t and the Variance Gamma (VG)—to the daily loss data of Goldman Sachs stock. Both models pass standard goodness-of-fit tests. The paper's final empirical results show that the Student-t model yields a `TCE_{0.999}` of 7.104, while the VG model yields a `TCE_{0.999}` of 4.841.\n\n---\n\n### Data / Model Specification\n\nThe Tail Conditional Expectation for a `GH_1` random variable `X` is given by:\n  \nTCE_q(X) = \\mu + \\frac{\\gamma}{1-q}k_{\\lambda}\\bar{F}_{GH_{1}}(x_{q};\\lambda+1) + \\frac{\\sigma^{2}}{1-q}k_{\\lambda}f_{GH_{1}}(x_{q};\\lambda+1) \\quad \\text{(Eq. (1))}\n \nThe Student-t distribution is known for its ability to model 'fat tails' (i.e., a higher probability of extreme events) compared to distributions like the Variance Gamma. This is governed by its degrees of freedom parameter `ν`, where a lower `ν` implies fatter tails.\n\n---\n\n### Question\n\nGiven the significant difference between the final `TCE_{0.999}` estimates (7.104 for Student-t vs. 4.841 for VG), select all of the following statements that represent a valid interpretation or conclusion.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 229, "Question": "### Background\n\n**Research Question.** How can the theoretical framework for solving partial integro-differential equations (PIDEs) be applied to the practical problem of pricing and hedging a catastrophe (CAT) bond, an instrument exposed to both non-hedgeable insurance risk and hedgeable market risk?\n\n**Setting / Data-Generating Environment.** The price of a CAT bond depends on a three-dimensional state process `X = (Z^1, Z^2, L)`, where `Z^1` is a factor driving the loss intensity, `Z^2` is a factor driving the interest rate, and `L` is the cumulative loss process. The goal is to price the bond and construct a mean-variance optimal hedge for its interest rate exposure using a traded zero-coupon bond.\n\n### Data / Model Specification\n\nThe state process `X_t = (Z^1_t, Z^2_t, L_t)` follows the SDE system under a risk-neutral measure `Q`:\n  \n\\mathrm{d}Z_{t}^{1} = a_{1}(b_{1}-Z_{t}^{1})\\mathrm{d}t+\\sqrt{1-\\rho^{2}}\\sigma_{1}\\mathrm{d}W_{t}^{1}+\\rho\\sigma_{1}\\mathrm{d}W_{t}^{2}+\\gamma^{Z}\\mathrm{d}L_{t} \\quad \\text{(Eq. (1))}\n \n  \n\\mathrm{d}Z_{t}^{2} = a_{2}(b_{2}-Z_{t}^{2})\\mathrm{d}t+\\sigma_{2}\\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (2))}\n \nwhere `W^1, W^2` are independent Brownian motions. The price of the CAT bond is `P_t^CAT = p^CAT(t, X_t)`. The hedging instrument is a zero-coupon bond with price `P_t = p(t, Z^2_t)`. The dynamics of the discounted zero-coupon bond price `P̃_t` are:\n  \n\\mathrm{d}\\widetilde{P}_{t} = \\widetilde{P}_{t} \\frac{\\sigma_{2}p_{z_{2}}(t,Z_{t}^{2})}{p(t,Z_{t}^{2})} \\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nAn investor wants to implement a mean-variance optimal hedge for the CAT bond's interest rate risk by trading the zero-coupon bond. Select ALL of the following statements that are correct regarding the hedging strategy and its theoretical requirements.", "Options": {"A": "The optimal number of zero-coupon bonds to hold, `h_t^*`, is given by `p_{z_{2}}^{\\mathrm{CAT}}/p_{z_{2}} + \\rho(\\sigma_{1}/\\sigma_{2}) (p_{z_{1}}^{\\mathrm{CAT}}/p_{z_{2}})`.", "B": "The optimal hedge ratio `h_t^*` only includes the direct exposure to `Z^2`, given by `p_{z_{2}}^{\\mathrm{CAT}}/p_{z_{2}}`, because the risk from `W^1` is unhedgeable.", "C": "The hedging strategy requires a classical solution because viscosity solutions do not guarantee that the bond price remains positive.", "D": "This hedging strategy is only well-defined if the CAT bond price function, `p^CAT`, is a classical solution to its PIDE, because the formula for `h_t^*` requires the existence of its first-order partial derivatives."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item tests the ability to identify the correct mean-variance hedge ratio in an incomplete market and to understand the theoretical justification for why a classical (smooth) solution of the pricing PIDE is a necessary prerequisite for implementing the hedge. Strategy: Combination of Computational Judgment (identifying the correct formula) and Reverse-Reasoning (identifying the necessary precondition for the formula to be valid). Distractor Logic: (C) is a Step-Omission Error, incorrectly ignoring the cross-hedge term that arises from the correlation `ρ`. (D) is a Conceptual: True but Irrelevant/Incorrect Reason; while positivity is desirable, the direct reason a classical solution is needed is the explicit dependence of the hedge ratio on derivatives, which are not guaranteed to exist for a mere viscosity solution.", "qid": "229", "question": "### Background\n\n**Research Question.** How can the theoretical framework for solving partial integro-differential equations (PIDEs) be applied to the practical problem of pricing and hedging a catastrophe (CAT) bond, an instrument exposed to both non-hedgeable insurance risk and hedgeable market risk?\n\n**Setting / Data-Generating Environment.** The price of a CAT bond depends on a three-dimensional state process `X = (Z^1, Z^2, L)`, where `Z^1` is a factor driving the loss intensity, `Z^2` is a factor driving the interest rate, and `L` is the cumulative loss process. The goal is to price the bond and construct a mean-variance optimal hedge for its interest rate exposure using a traded zero-coupon bond.\n\n### Data / Model Specification\n\nThe state process `X_t = (Z^1_t, Z^2_t, L_t)` follows the SDE system under a risk-neutral measure `Q`:\n  \n\\mathrm{d}Z_{t}^{1} = a_{1}(b_{1}-Z_{t}^{1})\\mathrm{d}t+\\sqrt{1-\\rho^{2}}\\sigma_{1}\\mathrm{d}W_{t}^{1}+\\rho\\sigma_{1}\\mathrm{d}W_{t}^{2}+\\gamma^{Z}\\mathrm{d}L_{t} \\quad \\text{(Eq. (1))}\n \n  \n\\mathrm{d}Z_{t}^{2} = a_{2}(b_{2}-Z_{t}^{2})\\mathrm{d}t+\\sigma_{2}\\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (2))}\n \nwhere `W^1, W^2` are independent Brownian motions. The price of the CAT bond is `P_t^CAT = p^CAT(t, X_t)`. The hedging instrument is a zero-coupon bond with price `P_t = p(t, Z^2_t)`. The dynamics of the discounted zero-coupon bond price `P̃_t` are:\n  \n\\mathrm{d}\\widetilde{P}_{t} = \\widetilde{P}_{t} \\frac{\\sigma_{2}p_{z_{2}}(t,Z_{t}^{2})}{p(t,Z_{t}^{2})} \\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nAn investor wants to implement a mean-variance optimal hedge for the CAT bond's interest rate risk by trading the zero-coupon bond. Select ALL of the following statements that are correct regarding the hedging strategy and its theoretical requirements.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 215, "Question": "### Background\n\n**Research Question.** How is the fair spread of a longevity index swap determined, and how are real-world mortality dynamics adjusted for risk aversion to arrive at a framework suitable for pricing?\n\n**Setting.** A QxX index swap is priced using the principle of risk-neutral valuation. This requires specifying the dynamics of the underlying mortality factors under both the real-world (P) measure and a risk-adjusted (Q) measure. The fair spread `σ` is the rate that sets the initial value of the swap to zero.\n\n### Data / Model Specification\n\nThe dynamics of the mortality factors under the physical measure P are given by:\n\n  \nA(t+1) = A(t) + \\upmu + C Z(t+1) \\quad \\text{(Eq. (1))}\n \n\nUnder the risk-adjusted measure Q, the drift is adjusted by the market price of risk `λ`:\n\n  \nA(t+1) = A(t) + (\\mu - C\\lambda) + C \\tilde{Z}(t+1) \\quad \\text{(Eq. (2))}\n \n\nThe time-0 value of the swap to the fixed payer is the present value of expected future cash flows under the Q-measure. Setting this value to zero defines the fair spread `σ`:\n\n  \n\\upsigma=\\frac{12\\sum_{k=1}^{T}P(0,\\frac{k}{12})\\left(\\mathrm{E}_{Q(\\uplambda)}[S_{k-1}]-\\mathrm{E}_{Q(\\uplambda)}[S_{k}]\\right)}{\\sum_{k=1}^{T}P(0,\\frac{k}{12})\\mathrm{E}_{Q(\\uplambda)}[S_{k-1}]} \\quad \\text{(Eq. (3))}\n \n\n---\n\nBased on the provided pricing framework, which of the following statements are **INCORRECT** interpretations or conclusions?\n", "Options": {"A": "The numerator of Eq. (3) represents the annualized present value of the risk-neutral expected payments on the floating (mortality-contingent) leg of the swap.", "B": "The risk-adjusted drift `μ - Cλ` in Eq. (2) represents the market's consensus forecast for the future trend in mortality rates.", "C": "The denominator of Eq. (3) can be interpreted as a stochastic annuity factor, representing the present value of the expected surviving notional on which the fixed spread is paid.", "D": "If longevity shocks and interest rates were negatively correlated, the fair spread `σ` would increase because the risk-free discount factors would be higher during periods of high mortality."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item assesses the user's ability to correctly interpret the components of the risk-neutral pricing framework for a longevity swap and identify common misconceptions. Depth Strategy: Reverse-Reasoning. The user must evaluate given interpretations against the correct model logic. Single-Correct-Answer Inversion Protocol: This protocol was applied. The original goal was to find correct interpretations, but this naturally led to multiple correct options and only one or two incorrect ones. To ensure at least two correct answers for the user to find, the stem was inverted to 'Select all that are INCORRECT'. The original distractors became the new correct answers. Distractor Logic: Options A and B are correct economic interpretations of the formula's components, making them distractors in this inverted question. Option C is incorrect; the risk-adjusted drift is for pricing, while the real-world drift `μ` is for forecasting. Option D is incorrect; negative correlation between longevity and rates would *decrease* the fair spread, making this a 'Conceptual Opposite' error.", "qid": "215", "question": "### Background\n\n**Research Question.** How is the fair spread of a longevity index swap determined, and how are real-world mortality dynamics adjusted for risk aversion to arrive at a framework suitable for pricing?\n\n**Setting.** A QxX index swap is priced using the principle of risk-neutral valuation. This requires specifying the dynamics of the underlying mortality factors under both the real-world (P) measure and a risk-adjusted (Q) measure. The fair spread `σ` is the rate that sets the initial value of the swap to zero.\n\n### Data / Model Specification\n\nThe dynamics of the mortality factors under the physical measure P are given by:\n\n  \nA(t+1) = A(t) + \\upmu + C Z(t+1) \\quad \\text{(Eq. (1))}\n \n\nUnder the risk-adjusted measure Q, the drift is adjusted by the market price of risk `λ`:\n\n  \nA(t+1) = A(t) + (\\mu - C\\lambda) + C \\tilde{Z}(t+1) \\quad \\text{(Eq. (2))}\n \n\nThe time-0 value of the swap to the fixed payer is the present value of expected future cash flows under the Q-measure. Setting this value to zero defines the fair spread `σ`:\n\n  \n\\upsigma=\\frac{12\\sum_{k=1}^{T}P(0,\\frac{k}{12})\\left(\\mathrm{E}_{Q(\\uplambda)}[S_{k-1}]-\\mathrm{E}_{Q(\\uplambda)}[S_{k}]\\right)}{\\sum_{k=1}^{T}P(0,\\frac{k}{12})\\mathrm{E}_{Q(\\uplambda)}[S_{k-1}]} \\quad \\text{(Eq. (3))}\n \n\n---\n\nBased on the provided pricing framework, which of the following statements are **INCORRECT** interpretations or conclusions?\n", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 226, "Question": "### Background\n\n**Research Question.** This case examines the core hypotheses of public finance regarding the capitalization of local fiscal policies. Specifically, it investigates whether residential and business property owners have different preferences for public services versus tax burdens, leading to differential impacts on their respective property values. The analysis tests two primary hypotheses:\n\n- **Hypothesis 1 (H1):** Residential property values are more responsive to changes in school spending than to changes in property taxes.\n- **Hypothesis 2 (H2):** Business property values are more responsive to changes in property taxes than to changes in school spending.\n\n**Setting / Data-Generating Environment.** The study uses a panel of 152 communities in southeast Michigan from 1983-2002. In this setting, public schools are primarily financed by property taxes. The Tiebout model suggests that mobile agents (both residents and firms) sort into communities that offer their preferred tax-service package. Residents, who directly benefit from schools, may value them highly. Firms, modeled as profit-maximizers, may view property taxes as a direct cost of business while receiving little direct benefit from K-12 education.\n\n### Data / Model Specification\n\nThe study estimates a dynamic first-difference 2SLS model to determine the long-run elasticities of property values with respect to fiscal policy. The model includes contemporaneous and three-year lagged values of policy variables to capture the full capitalization effect over time. Table 1 below presents the estimated coefficients for both residential and business property value growth.\n\n**Table 1: Selected 2SLS Regression Coefficients for Property Value Growth**\n\n| Dependent Variable | Regressor | Coefficient | Std. Error |\n| :--- | :--- | :--- | :--- |\n| **ΔLog Residential Property Value** | | | |\n| | Log Property Tax | -0.256*** | (0.040) |\n| | Log Property Tax t-1 | -0.123*** | (0.020) |\n| | Log Property Tax t-2 | -0.029*** | (0.010) |\n| | Log Property Tax t-3 | 0.004 | (0.012) |\n| | Log School Spending | 0.623*** | (0.236) |\n| | Log School Spending t-1 | -0.029 | (0.186) |\n| | Log School Spending t-2 | -0.156 | (0.162) |\n| | Log School Spending t-3 | 0.157 | (0.111) |\n| **ΔLog Business Property Value** | | | |\n| | Log Property Tax | -0.194 | (0.136) |\n| | Log Property Tax t-1 | -0.403** | (0.185) |\n| | Log Property Tax t-2 | -0.601** | (0.275) |\n| | Log Property Tax t-3 | 0.216 | (0.172) |\n| | Log School Spending | -1.458* | (0.810) |\n| | Log School Spending t-1 | 0.057 | (0.428) |\n| | Log School Spending t-2 | -0.231 | (0.194) |\n| | Log School Spending t-3 | 0.494 | (0.471) |\n\n*Note: *, **, *** indicate significance at the 10%, 5%, and 1% levels, respectively. The long-run elasticity is the sum of the contemporaneous and lagged coefficients.*\n\n### Question\n\nBased on the long-run elasticities derived from the coefficients in Table 1, select all of the following statements that are correct.", "Options": {"A": "The long-run elasticity of residential property values with respect to school spending is approximately +0.60.", "B": "The results support Hypothesis 2 (business values are more responsive to taxes than spending), as the long-run tax effect is driven by statistically significant coefficients while the school spending effect is not statistically significant in most periods.", "C": "The long-run elasticity of residential property values with respect to property taxes is -0.256.", "D": "The results reject Hypothesis 1 (residential values are more responsive to spending than taxes) because the long-run tax elasticity is negative while the spending elasticity is positive."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to calculate long-run elasticities from a dynamic model and interpret them in the context of the paper's hypotheses. It uses an Atomic Decomposition strategy to create multiple correct statements from the original multi-part QA. The depth strategy is Computational Judgment. Distractors are based on common errors: (C) a Step-Omission Error, using only the contemporaneous coefficient, and (D) a Conceptual Opposite error, misinterpreting responsiveness as being about the sign of the effect rather than its absolute magnitude.", "qid": "226", "question": "### Background\n\n**Research Question.** This case examines the core hypotheses of public finance regarding the capitalization of local fiscal policies. Specifically, it investigates whether residential and business property owners have different preferences for public services versus tax burdens, leading to differential impacts on their respective property values. The analysis tests two primary hypotheses:\n\n- **Hypothesis 1 (H1):** Residential property values are more responsive to changes in school spending than to changes in property taxes.\n- **Hypothesis 2 (H2):** Business property values are more responsive to changes in property taxes than to changes in school spending.\n\n**Setting / Data-Generating Environment.** The study uses a panel of 152 communities in southeast Michigan from 1983-2002. In this setting, public schools are primarily financed by property taxes. The Tiebout model suggests that mobile agents (both residents and firms) sort into communities that offer their preferred tax-service package. Residents, who directly benefit from schools, may value them highly. Firms, modeled as profit-maximizers, may view property taxes as a direct cost of business while receiving little direct benefit from K-12 education.\n\n### Data / Model Specification\n\nThe study estimates a dynamic first-difference 2SLS model to determine the long-run elasticities of property values with respect to fiscal policy. The model includes contemporaneous and three-year lagged values of policy variables to capture the full capitalization effect over time. Table 1 below presents the estimated coefficients for both residential and business property value growth.\n\n**Table 1: Selected 2SLS Regression Coefficients for Property Value Growth**\n\n| Dependent Variable | Regressor | Coefficient | Std. Error |\n| :--- | :--- | :--- | :--- |\n| **ΔLog Residential Property Value** | | | |\n| | Log Property Tax | -0.256*** | (0.040) |\n| | Log Property Tax t-1 | -0.123*** | (0.020) |\n| | Log Property Tax t-2 | -0.029*** | (0.010) |\n| | Log Property Tax t-3 | 0.004 | (0.012) |\n| | Log School Spending | 0.623*** | (0.236) |\n| | Log School Spending t-1 | -0.029 | (0.186) |\n| | Log School Spending t-2 | -0.156 | (0.162) |\n| | Log School Spending t-3 | 0.157 | (0.111) |\n| **ΔLog Business Property Value** | | | |\n| | Log Property Tax | -0.194 | (0.136) |\n| | Log Property Tax t-1 | -0.403** | (0.185) |\n| | Log Property Tax t-2 | -0.601** | (0.275) |\n| | Log Property Tax t-3 | 0.216 | (0.172) |\n| | Log School Spending | -1.458* | (0.810) |\n| | Log School Spending t-1 | 0.057 | (0.428) |\n| | Log School Spending t-2 | -0.231 | (0.194) |\n| | Log School Spending t-3 | 0.494 | (0.471) |\n\n*Note: *, **, *** indicate significance at the 10%, 5%, and 1% levels, respectively. The long-run elasticity is the sum of the contemporaneous and lagged coefficients.*\n\n### Question\n\nBased on the long-run elasticities derived from the coefficients in Table 1, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 221, "Question": "### Background\n\n**Research Question.** How can the differential impact of corporate governance on the risk and performance of Islamic banks, relative to conventional banks, be empirically identified and interpreted through the lens of financial theory?\n\n**Setting.** A panel data regression analysis is conducted to explain bank risk-taking (`Log_Z`) and performance (`ROA`). The study's key insight is that the effect of a corporate governance index (`CGI`) may differ between Islamic and conventional banks.\n\n### Data / Model Specification\n\nThe primary regression model for a dependent variable `Y` (either `Log_Z` or `ROA`) is:\n\n  \nY_{i,t} = \\alpha_0 + \\alpha_1 Islamic_i + \\beta_1 CGI_{i,t-1} + \\beta_2 (Islamic_i \\times CGI_{i,t-1}) + \\text{Controls} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Selected Regression Results (from paper's Tables 4 & 5)**\n| Dependent Variable | `Islamic` (`α₁`) | `CGI` (`β₁`) | `Islamic × CGI` (`β₂`) | `STDROA` |\n| :--- | :---: | :---: | :---: | :---: |\n| `Log_Z` (Risk) | 0.446 (0.346) | 0.105* (0.057) | -0.808** (0.343) | Not in model |\n| `ROA` (Performance) | -0.029** (0.014) | -0.002 (0.008) | 0.070*** (0.026) | Positive & Significant*** |\n*Notes: Robust standard errors in parentheses. *** p<0.01, ** p<0.05, * p<0.10. `Log_Z` is a measure of bank stability (higher value means lower risk). `STDROA` is a proxy for bank risk.*\n\n### Question\n\nBased on the regression model, empirical results, and principles of corporate finance, select all of the following statements that are correct.", "Options": {"A": "For an Islamic bank, a one-unit increase in the Corporate Governance Index (CGI) is associated with a calculated decrease of 0.703 in the Log_Z score, implying higher insolvency risk.", "B": "For a conventional bank, the marginal effect of CGI on the Log_Z score is -0.808, indicating that governance significantly increases its risk.", "C": "The positive and significant coefficient on `STDROA` in the `ROA` regression implies that risk-taking by Islamic banks is generally uncompensated.", "D": "The finding that better-governed Islamic banks take more risk is consistent with the Merton (1977) model, where shareholder value can be increased by raising asset volatility (`σ`) due to the convex payoff from the deposit insurance guarantee."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to calculate and interpret marginal effects from an interaction model and connect empirical findings to foundational corporate finance theory. It uses a combination of Computational Judgment and Scenario Application strategies. Correct options (A, B) require both calculation (A: 0.105 - 0.808) and theoretical synthesis (B). Distractor C reflects a common error of misinterpreting an interaction coefficient as a main effect (Formula Misuse). Distractor D presents a Conceptual Opposite regarding risk compensation.", "qid": "221", "question": "### Background\n\n**Research Question.** How can the differential impact of corporate governance on the risk and performance of Islamic banks, relative to conventional banks, be empirically identified and interpreted through the lens of financial theory?\n\n**Setting.** A panel data regression analysis is conducted to explain bank risk-taking (`Log_Z`) and performance (`ROA`). The study's key insight is that the effect of a corporate governance index (`CGI`) may differ between Islamic and conventional banks.\n\n### Data / Model Specification\n\nThe primary regression model for a dependent variable `Y` (either `Log_Z` or `ROA`) is:\n\n  \nY_{i,t} = \\alpha_0 + \\alpha_1 Islamic_i + \\beta_1 CGI_{i,t-1} + \\beta_2 (Islamic_i \\times CGI_{i,t-1}) + \\text{Controls} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Selected Regression Results (from paper's Tables 4 & 5)**\n| Dependent Variable | `Islamic` (`α₁`) | `CGI` (`β₁`) | `Islamic × CGI` (`β₂`) | `STDROA` |\n| :--- | :---: | :---: | :---: | :---: |\n| `Log_Z` (Risk) | 0.446 (0.346) | 0.105* (0.057) | -0.808** (0.343) | Not in model |\n| `ROA` (Performance) | -0.029** (0.014) | -0.002 (0.008) | 0.070*** (0.026) | Positive & Significant*** |\n*Notes: Robust standard errors in parentheses. *** p<0.01, ** p<0.05, * p<0.10. `Log_Z` is a measure of bank stability (higher value means lower risk). `STDROA` is a proxy for bank risk.*\n\n### Question\n\nBased on the regression model, empirical results, and principles of corporate finance, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 213, "Question": "### Background\n\n**Research Question.** How can the market price of longevity risk be inferred from traded derivatives, and what are the limitations of this approach when multiple risk factors are present?\n\n**Setting.** The market prices of risk, `λ_1` (for level risk) and `λ_2` (for slope risk), are calibrated by forcing a two-factor mortality model to match the observed 500 basis point spread of the 10-year QxX index swap. This process is known as 'backing out' or 'implying' the risk prices from the market.\n\n### Data / Model Specification\n\nThe market prices of risk `(λ_1, λ_2)` are found by solving the pricing equation for the swap, setting the fair spread `σ` to the market quote of 500 basis points. Since there is one equation (one price) and two unknowns (`λ_1`, `λ_2`), the solution is not unique. The tables below show two sets of results: Table 1 shows three possible `(λ_1, λ_2)` pairs implied by the 500 bps spread. Table 2 shows the swap spreads that would be calculated if one were to instead use `λ` values calibrated from an older, non-traded longevity bond (the BNP/EIB bond).\n\n**Table 1.** Implied values of `(λ_1, λ_2)` from the 500 bps QxX swap spread.\n\n| λ_1    | λ_2    |\n|--------|--------|\n| 0      | 3.4345 |\n| 6.4732 | 0      |\n| 2.2357 | 2.2357 |\n\n**Table 2.** Estimates of `σ` (in basis points) using `λ` calibrated from the BNP/EIB bond.\n\n| λ_1   | λ_2   | Fair Spread (σ) |\n|-------|-------|-----------------|\n| 0     | 0.316 | 625             |\n| 0.375 | 0     | 631             |\n| 0.175 | 0.175 | 627             |\n\n---\n\nBased on the data and the paper's discussion, which of the following statements are valid conclusions regarding the calibration of the market prices of risk `(λ_1, λ_2)`?\n", "Options": {"A": "The large, positive `λ` values in Table 1 imply that market participants demand a significant premium for bearing the risk of unexpected longevity improvements.", "B": "The existence of multiple `(λ_1, λ_2)` pairs in Table 1 that produce the same 500 bps spread is a direct consequence of an incomplete market for longevity risk.", "C": "The discrepancy between the ~630 bps fair value in Table 2 and the 500 bps market price (Table 1) likely represents a pure arbitrage opportunity.", "D": "The `λ` values in Table 1 are more credible for pricing the QxX swap than those used in Table 2 because the BNP/EIB bond price was stale and subject to significant basis risk."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret implied risk prices and synthesize information from two tables to understand the concept of market incompleteness and basis risk. Depth Strategy: Reverse-Reasoning. The user is given the results (two different calibrations) and must identify the plausible causes and implications. Distractor Logic: Option A is correct; market incompleteness (two risks, one price) is the reason for non-uniqueness. Option B is correct; positive lambdas signify a premium for bearing longevity risk. Option D is correct, reflecting the paper's explicit limitations on the BNP/EIB calibration (stale price, different underlying population). Option C is a 'Conceptual Opposite' distractor; the paper argues this discrepancy is due to basis risk and other factors, making it an illusory arbitrage, not a pure one.", "qid": "213", "question": "### Background\n\n**Research Question.** How can the market price of longevity risk be inferred from traded derivatives, and what are the limitations of this approach when multiple risk factors are present?\n\n**Setting.** The market prices of risk, `λ_1` (for level risk) and `λ_2` (for slope risk), are calibrated by forcing a two-factor mortality model to match the observed 500 basis point spread of the 10-year QxX index swap. This process is known as 'backing out' or 'implying' the risk prices from the market.\n\n### Data / Model Specification\n\nThe market prices of risk `(λ_1, λ_2)` are found by solving the pricing equation for the swap, setting the fair spread `σ` to the market quote of 500 basis points. Since there is one equation (one price) and two unknowns (`λ_1`, `λ_2`), the solution is not unique. The tables below show two sets of results: Table 1 shows three possible `(λ_1, λ_2)` pairs implied by the 500 bps spread. Table 2 shows the swap spreads that would be calculated if one were to instead use `λ` values calibrated from an older, non-traded longevity bond (the BNP/EIB bond).\n\n**Table 1.** Implied values of `(λ_1, λ_2)` from the 500 bps QxX swap spread.\n\n| λ_1    | λ_2    |\n|--------|--------|\n| 0      | 3.4345 |\n| 6.4732 | 0      |\n| 2.2357 | 2.2357 |\n\n**Table 2.** Estimates of `σ` (in basis points) using `λ` calibrated from the BNP/EIB bond.\n\n| λ_1   | λ_2   | Fair Spread (σ) |\n|-------|-------|-----------------|\n| 0     | 0.316 | 625             |\n| 0.375 | 0     | 631             |\n| 0.175 | 0.175 | 627             |\n\n---\n\nBased on the data and the paper's discussion, which of the following statements are valid conclusions regarding the calibration of the market prices of risk `(λ_1, λ_2)`?\n", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 70, "Question": "### Background\n\n**Research Question**: This case considers the reliability of regression estimates when the dependent variable is measured with error.\n\n**Setting / Data-Generating Environment**: In the study, the dependent variable is the house value-to-income ratio. The house value is not an audited appraisal but is instead \"as estimated by the owner\" within one year of purchase. This self-reported value may contain measurement error.\n\n---\n\n### Question\n\nConsidering the potential for measurement error in the self-reported house value, which of the following statements are true? Select all that apply.", "Options": {"A": "If the estimation error is classical, the OLS coefficients on income will be biased towards zero (attenuation bias).", "B": "If the estimation error is classical (i.e., random and uncorrelated with income or other regressors), the OLS coefficients on income remain unbiased, but their standard errors will increase.", "C": "If lower-income households tend to systematically overestimate their home's value and higher-income households tend to underestimate it, the OLS coefficients on income will be biased downwards (i.e., become more negative).", "D": "Any measurement error in the dependent variable, regardless of its nature, will render the OLS estimates biased and inconsistent."}, "Answer": ["B", "C"], "pi_justification": "This item assesses understanding of both classical and non-classical measurement error in the dependent variable. It uses a Scenario Application strategy. Option A is a correct statement of the consequences of classical ME in the DV. Option C correctly diagnoses the direction of bias from a specific non-classical ME scenario where the error is negatively correlated with a regressor. Distractors are based on high-frequency misconceptions: B incorrectly applies the rule for ME in an *independent* variable (attenuation bias) to the dependent variable. D is an overgeneralization; classical ME in the DV does not cause bias.", "qid": "70", "question": "### Background\n\n**Research Question**: This case considers the reliability of regression estimates when the dependent variable is measured with error.\n\n**Setting / Data-Generating Environment**: In the study, the dependent variable is the house value-to-income ratio. The house value is not an audited appraisal but is instead \"as estimated by the owner\" within one year of purchase. This self-reported value may contain measurement error.\n\n---\n\n### Question\n\nConsidering the potential for measurement error in the self-reported house value, which of the following statements are true? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n\nA,B\n\n## Explanation\n\nThe reasoning begins by identifying the key econometric issue: measurement error in the dependent variable (house value) and its impact on ordinary least squares (OLS) estimates. In regression analysis, classical measurement error in the dependent variable is defined as random error that is uncorrelated with the regressors (e.g., income), which affects only the precision of estimates, not their consistency.\n\nNext, evaluate each option systematically:\n\n- For option A: Classical measurement error in the dependent variable does not introduce bias in OLS coefficients, as the error term remains uncorrelated with the regressors. Attenuation bias applies to measurement error in independent variables, not the dependent one. Thus, this statement is false.\n\n- For option B: With classical measurement error, the OLS coefficients remain unbiased because the error does not correlate with the regressors, preserving the expected value of the estimator. However, the error increases the residual variance, leading to larger standard errors and wider confidence intervals. This statement is true.\n\n- For option C: If lower-income households overestimate and higher-income households underestimate house value, the measurement error correlates negatively with income, violating the classical assumption. This induces a downward bias in the coefficient on income (e.g., making it more negative if originally positive). This statement is true\n> 🔍 **Analyzing measurement error**.\n\n\n> ***- The dependent variable, house value-to-income ratio, uses self-reported house values, which may have measurement error.***\n- For option D: Measurement error in the dependent variable biases OLS estimates only if non-classical (correlated with regressors). Classical error does not cause bias or inconsistency. Thus, this statement is false.\n\nThe correct options are those that accurately describe the effects under the specified conditions: B (classical error effects) and C (non-classical error scenario).", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 238, "Question": "### Background\n\nIn the multivariate Unobserved Components (UC) model, the information share (`IS_j`) for market `j` can be derived under the identifying assumption that the idiosyncratic noise covariance matrix, `Ω`, is diagonal.\n\n### Data / Model Specification\n\nThe derived formula for the information share is:\n\n  \nI S_{j}=\\frac{\\beta_{j}^{2}/\\omega_{j}^{2}}{1/\\sigma^{2}+\\sum_{i}\\beta_{i}^{2}/\\omega_{i}^{2}} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n*   `β_j` is the sensitivity of market `j`'s price innovation to the efficient price innovation `r_t`.\n*   `ω_j^2` is the variance of market `j`'s idiosyncratic noise.\n*   `σ^2` is the variance of the efficient price innovation `r_t`.\n\n---\n\nConsider the behavior of the information shares in this system under certain limiting conditions. Which of the following statements are mathematically and economically correct interpretations of **Eq. (1)**?", "Options": {"A": "If all markets have identical parameters (`β_j = β` and `ω_j^2 = ω` for all `j`), then each of the `N` markets will have an information share of `1/N`.", "B": "If a single market `k` becomes infinitely noisy (`ω_k^2 → ∞`), its information share `IS_k` approaches 0.", "C": "A market's information share is directly proportional to its \"signal-to-noise\" ratio, defined as `β_j^2 / ω_j^2`.", "D": "If a single market `k` becomes perfectly efficient (`ω_k^2 → 0`), its information share `IS_k` approaches 1, and the shares of all other markets approach 0."}, "Answer": ["B", "C", "D"], "pi_justification": "This item assesses a deep conceptual understanding of the paper's central theoretical result (Theorem 1) by requiring the user to analyze its properties in limiting cases. It uses a Scenario Application strategy with hypothetical scenarios. Options A and B test the correct analysis of the formula as a market becomes perfectly efficient or infinitely noisy, respectively. Option D correctly identifies the 'signal-to-noise' interpretation. Option C is a plausible numerical distractor; while the shares would be equal, they would not sum to 1 due to the `1/σ^2` term in the denominator, so `IS_j ≠ 1/N` unless `1/σ^2` is zero, which is not generally true.", "qid": "238", "question": "### Background\n\nIn the multivariate Unobserved Components (UC) model, the information share (`IS_j`) for market `j` can be derived under the identifying assumption that the idiosyncratic noise covariance matrix, `Ω`, is diagonal.\n\n### Data / Model Specification\n\nThe derived formula for the information share is:\n\n  \nI S_{j}=\\frac{\\beta_{j}^{2}/\\omega_{j}^{2}}{1/\\sigma^{2}+\\sum_{i}\\beta_{i}^{2}/\\omega_{i}^{2}} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n*   `β_j` is the sensitivity of market `j`'s price innovation to the efficient price innovation `r_t`.\n*   `ω_j^2` is the variance of market `j`'s idiosyncratic noise.\n*   `σ^2` is the variance of the efficient price innovation `r_t`.\n\n---\n\nConsider the behavior of the information shares in this system under certain limiting conditions. Which of the following statements are mathematically and economically correct interpretations of **Eq. (1)**?", "model_evaluation_result": {"model_raw_response": "\"A,B,C,D\"", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 189, "Question": "### Background\n\nA Fama-MacBeth two-pass regression is used to test a two-factor model on monthly returns of US stock portfolios from 1983 to 1997. The market factor is the US-MSCI index return, and the active management factor is constructed relative to the S&P 500 benchmark. The model's key prediction is that the average active management risk premium, `γ̄_S&P`, is positive.\n\n### Data / Model Specification\n\nThe second-pass cross-sectional regression for each month `t` is:\n  \nR_{i,t}=γ_{0,t} + γ_{M,t}β̂_{M,t-1}^i + γ_{S&P,t}β̂_{S&P,t-1}^i + η_{i,t} \n \nwhere `γ_{M,t}` and `γ_{S&P,t}` are the estimated prices of market risk and active management risk, respectively.\n\n**Table 1. Fama-MacBeth Estimates of Risk Premia (×100)**\n*Panel A: US-MSCI (Market) vs. S&P 500 (Benchmark)*\n\n| Period      | `γ̄_{M,t}` | `γ̄_{S&P,t}` | `t(γ̄_{M,t})` | `t(γ̄_{S&P,t})` |\n| :---------- | :---------- | :------------ | :------------- | :--------------- |\n| 1983-1997   | 2.41        | 1.78          | 1.65           | 1.80             |\n| 1983-1991   | -0.28       | 0.36          | -0.17          | 0.41             |\n| 1992-1997   | 6.44        | 3.91          | 2.34           | 1.87             |\n\n---\n\nBased on the results in **Table 1**, which of the following conclusions are statistically supported?\n(Assume a one-tailed test for the active management premium and a two-tailed test for the market premium.)", "Options": {"A": "For the 1983-1991 sub-period, the market risk premium (`γ̄_M`) is statistically indistinguishable from zero at the 10% significance level.", "B": "For the full 1983-1997 period, the active management risk premium (`γ̄_S&P`) is positive and statistically significant at the 5% level.", "C": "For the 1992-1997 sub-period, the market risk premium (`γ̄_M`) is positive and statistically significant at the 1% level.", "D": "The positive and significant active management risk premium observed in the full sample is primarily driven by the results from the 1992-1997 sub-period."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to correctly interpret the results of a Fama-MacBeth regression, including hypothesis testing with appropriate significance levels and identifying the source of statistical significance across different time periods. Depth Strategy: Computational Judgment. The user must perform multiple hypothesis tests based on the provided t-statistics and critical values (e.g., ~1.65 for 5% one-tailed, ~1.96 for 5% two-tailed) and synthesize the findings. Distractor Logic: Option D is a numerical distractor. The t-statistic of 2.34 is significant at the 5% level (critical value ~1.96) but not at the 1% level (critical value ~2.58), testing for precision in statistical inference.", "qid": "189", "question": "### Background\n\nA Fama-MacBeth two-pass regression is used to test a two-factor model on monthly returns of US stock portfolios from 1983 to 1997. The market factor is the US-MSCI index return, and the active management factor is constructed relative to the S&P 500 benchmark. The model's key prediction is that the average active management risk premium, `γ̄_S&P`, is positive.\n\n### Data / Model Specification\n\nThe second-pass cross-sectional regression for each month `t` is:\n  \nR_{i,t}=γ_{0,t} + γ_{M,t}β̂_{M,t-1}^i + γ_{S&P,t}β̂_{S&P,t-1}^i + η_{i,t} \n \nwhere `γ_{M,t}` and `γ_{S&P,t}` are the estimated prices of market risk and active management risk, respectively.\n\n**Table 1. Fama-MacBeth Estimates of Risk Premia (×100)**\n*Panel A: US-MSCI (Market) vs. S&P 500 (Benchmark)*\n\n| Period      | `γ̄_{M,t}` | `γ̄_{S&P,t}` | `t(γ̄_{M,t})` | `t(γ̄_{S&P,t})` |\n| :---------- | :---------- | :------------ | :------------- | :--------------- |\n| 1983-1997   | 2.41        | 1.78          | 1.65           | 1.80             |\n| 1983-1991   | -0.28       | 0.36          | -0.17          | 0.41             |\n| 1992-1997   | 6.44        | 3.91          | 2.34           | 1.87             |\n\n---\n\nBased on the results in **Table 1**, which of the following conclusions are statistically supported?\n(Assume a one-tailed test for the active management premium and a two-tailed test for the market premium.)", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 183, "Question": "### Background\n\n**Research Question.** Do certain equity styles share common long-run stochastic trends (i.e., are they cointegrated)?\n\n**Setting and Data.** The paper finds that the log prices of Large Value (WLV) and Small Growth (WSG) styles, denoted `p_WLV,t` and `p_WSG,t`, are cointegrated.\n\n---\n\n### Data / Model Specification\n\nThe cointegration between WLV and WSG implies their log prices follow a Vector Error Correction Model (VECM). In a VECM, the change in each variable's price is partly a reaction to the previous period's deviation from their long-run equilibrium. This deviation is called the error-correction term, `z_{t-1}`.\n\nLet the long-run equilibrium relationship be `p_{WLV,t} = β p_{WSG,t}`. The error-correction term is then `z_{t-1} = p_{WLV,t-1} - β p_{WSG,t-1}`.\n\nThe VECM for the price changes can be written as:\n  \n\\Delta p_{WLV,t} = \\alpha_1 z_{t-1} + \\text{short-run dynamics} + \\epsilon_{1,t}\n \n  \n\\Delta p_{WSG,t} = \\alpha_2 z_{t-1} + \\text{short-run dynamics} + \\epsilon_{2,t}\n \nThe coefficients `α_1` and `α_2` are the 'speed-of-adjustment' parameters.\n\n---\n\n### Question\n\nSelect all statements that correctly interpret the economic meaning of the speed-of-adjustment coefficients (`α_1` for WLV, `α_2` for WSG) in the VECM.", "Options": {"A": "For the system to revert to its long-run mean, the speed-of-adjustment coefficients `α_1` and `α_2` must both be negative.", "B": "If the WLV price was too high relative to equilibrium (`z_{t-1} > 0`), the adjustment coefficient `α_2` for the WSG price must be positive to induce a positive price change `Δp_{WSG,t}`, helping to close the equilibrium gap from the other side.", "C": "For the system to be stable, if the WLV price was too high relative to its equilibrium level in the previous period (i.e., `z_{t-1} > 0`), its adjustment coefficient `α_1` must be negative to induce a negative price change `Δp_{WLV,t}`.", "D": "A larger absolute value of `α_1` implies that the Large Value style is less responsive to deviations from the long-run equilibrium."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item tests a deep understanding of time-series econometrics (VECM), focusing on the economic interpretation of model parameters. It uses a Scenario Application strategy, asking how the system behaves given a specific deviation from equilibrium. Correct options A and B correctly describe the mechanics of error correction for a two-variable system. Distractor C is a Sign Error, as the coefficients must have opposite signs for this error term specification. Distractor D is a Conceptual Opposite, as a larger coefficient means a *faster* adjustment.", "qid": "183", "question": "### Background\n\n**Research Question.** Do certain equity styles share common long-run stochastic trends (i.e., are they cointegrated)?\n\n**Setting and Data.** The paper finds that the log prices of Large Value (WLV) and Small Growth (WSG) styles, denoted `p_WLV,t` and `p_WSG,t`, are cointegrated.\n\n---\n\n### Data / Model Specification\n\nThe cointegration between WLV and WSG implies their log prices follow a Vector Error Correction Model (VECM). In a VECM, the change in each variable's price is partly a reaction to the previous period's deviation from their long-run equilibrium. This deviation is called the error-correction term, `z_{t-1}`.\n\nLet the long-run equilibrium relationship be `p_{WLV,t} = β p_{WSG,t}`. The error-correction term is then `z_{t-1} = p_{WLV,t-1} - β p_{WSG,t-1}`.\n\nThe VECM for the price changes can be written as:\n  \n\\Delta p_{WLV,t} = \\alpha_1 z_{t-1} + \\text{short-run dynamics} + \\epsilon_{1,t}\n \n  \n\\Delta p_{WSG,t} = \\alpha_2 z_{t-1} + \\text{short-run dynamics} + \\epsilon_{2,t}\n \nThe coefficients `α_1` and `α_2` are the 'speed-of-adjustment' parameters.\n\n---\n\n### Question\n\nSelect all statements that correctly interpret the economic meaning of the speed-of-adjustment coefficients (`α_1` for WLV, `α_2` for WSG) in the VECM.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 211, "Question": "### Background\n\nThe paper employs a competing risk proportional hazard model to forecast borrower outcomes. The model's forecasts are sensitive to the assumptions made about the future paths of time-varying covariates.\n\n### Data / Model Specification\n\nThe proportional hazard rates for prepayment (`p`) and default (`d`) at duration `t` are given by:\n  \nh^{k}(t|X_{t}^{k})=\\exp(g^{k}(t))\\exp(X_{t}^{k}\\beta^{k}) \\quad \\text{(Eq. 1)}\n \nwhere `X_t` is a vector of explanatory variables at time `t`. One key time-varying covariate is the current Loan-to-Value (LTV) ratio, which is updated monthly based on projected house price index (HPI) changes.\n\nThe probability of defaulting over a future horizon from `t+1` to `T` is then calculated as:\n  \n\\operatorname*{Pr}^{D}(t,T)=\\sum_{j=t+1}^{T} S(j-1) h^{d}(j) \\quad \\text{(Eq. 2)}\n \nwhere `S(j-1)` is the probability of the borrower remaining active through month `j-1`.\n\n### Question\n\nBased on the structure of the forecasting model described, which of the following statements are valid implications of using time-varying covariates like current LTV?", "Options": {"A": "The term `S(j-1)h^d(j)` represents the conditional probability of defaulting in month `j` given survival through month `j-1`.", "B": "The model's core assumption is that covariates have an additive, rather than multiplicative, effect on the baseline hazard.", "C": "A more pessimistic forecast for future house prices would lead to higher projected LTVs, which in turn would increase the forecasted default hazard `h^d(j)` for many borrowers.", "D": "The model's loss forecasts are critically dependent on the accuracy of external macroeconomic forecasts, such as future house prices."}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: This item assesses the understanding of the practical implications and underlying assumptions of the econometric model used in the paper, particularly the concept of model risk introduced by time-varying covariates. Depth Strategy: Scenario Application (considering the effect of a change in macro forecasts). Distractor Logic: Option A is a correct high-level implication about model risk. Option B correctly traces the causal chain from a macro forecast (HPI) to a key covariate (LTV) to the model output (hazard rate). Option C is a 'Conceptual Opposite' distractor; the model is explicitly multiplicative (`exp(X*beta)`), not additive. Option D is a 'Conceptual Distractor' that misinterprets a key term; `S(j-1)h^d(j)` is the *unconditional* probability of default in month j, while the *conditional* probability is simply `h^d(j)`.", "qid": "211", "question": "### Background\n\nThe paper employs a competing risk proportional hazard model to forecast borrower outcomes. The model's forecasts are sensitive to the assumptions made about the future paths of time-varying covariates.\n\n### Data / Model Specification\n\nThe proportional hazard rates for prepayment (`p`) and default (`d`) at duration `t` are given by:\n  \nh^{k}(t|X_{t}^{k})=\\exp(g^{k}(t))\\exp(X_{t}^{k}\\beta^{k}) \\quad \\text{(Eq. 1)}\n \nwhere `X_t` is a vector of explanatory variables at time `t`. One key time-varying covariate is the current Loan-to-Value (LTV) ratio, which is updated monthly based on projected house price index (HPI) changes.\n\nThe probability of defaulting over a future horizon from `t+1` to `T` is then calculated as:\n  \n\\operatorname*{Pr}^{D}(t,T)=\\sum_{j=t+1}^{T} S(j-1) h^{d}(j) \\quad \\text{(Eq. 2)}\n \nwhere `S(j-1)` is the probability of the borrower remaining active through month `j-1`.\n\n### Question\n\nBased on the structure of the forecasting model described, which of the following statements are valid implications of using time-varying covariates like current LTV?", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 201, "Question": "### Background\n\nAn insurance company's management aims to translate a single, clear operational metric—the combined ratio—into superior shareholder value. Progressive Insurance has established a core corporate strategy of maximizing growth subject to maintaining specific targets for its underwriting margin, leverage, and investment policies. The company's central mantra, communicated to all employees, is \"96 and grow.\"\n\n---\n\n### Data / Model Specification\n\nProgressive's corporate strategy is defined by four explicit financial policies, summarized in Table 1.\n\n**Table 1: Progressive's Stated Financial Policies**\n\n| Policy Metric | Target Value |\n| :--- | :--- |\n| Combined Ratio (`CR`) | `≤ 96%` |\n| Sales-to-Capital (`P/S`) | `3-to-1` |\n| Debt-to-Capital (`D/S`) | `25%` |\n\nSuppose a new CFO proposes a more aggressive capital structure, increasing the debt-to-capital ratio (`D/S`) from 25% to 40%, while keeping all other policy targets the same.\n\n---\n\n### Question\n\nBased on the provided model and strategic context, select all of the following statements that represent valid strategic conclusions.", "Options": {"A": "Focusing line managers on the '96 combined ratio' is effective because it is a clear, controllable target that aligns them with the firm's core underwriting competency.", "B": "The CFO's proposal to increase the debt-to-capital ratio to 40% would mechanically increase the firm's target pre-tax ROE, assuming all other policies and returns remain constant.", "C": "The CFO's proposal would decrease the firm's reliance on investment income to achieve its target ROE.", "D": "The primary benefit of the '96 and grow' mantra is that it fully captures all drivers of shareholder value, including investment performance and capital structure."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect quantitative analysis with strategic rationale, using a Scenario Application strategy. Correct options (A, B) test the understanding of the core communication strategy and the mechanical effect of leverage. Distractors C and D are 'Conceptual Opposites' that test for a deeper understanding of the strategic trade-offs and the deliberate simplification embodied in the company's mantra.", "qid": "201", "question": "### Background\n\nAn insurance company's management aims to translate a single, clear operational metric—the combined ratio—into superior shareholder value. Progressive Insurance has established a core corporate strategy of maximizing growth subject to maintaining specific targets for its underwriting margin, leverage, and investment policies. The company's central mantra, communicated to all employees, is \"96 and grow.\"\n\n---\n\n### Data / Model Specification\n\nProgressive's corporate strategy is defined by four explicit financial policies, summarized in Table 1.\n\n**Table 1: Progressive's Stated Financial Policies**\n\n| Policy Metric | Target Value |\n| :--- | :--- |\n| Combined Ratio (`CR`) | `≤ 96%` |\n| Sales-to-Capital (`P/S`) | `3-to-1` |\n| Debt-to-Capital (`D/S`) | `25%` |\n\nSuppose a new CFO proposes a more aggressive capital structure, increasing the debt-to-capital ratio (`D/S`) from 25% to 40%, while keeping all other policy targets the same.\n\n---\n\n### Question\n\nBased on the provided model and strategic context, select all of the following statements that represent valid strategic conclusions.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 207, "Question": "### Background\n\n**Research Question:** This case investigates the primary drivers of Seasoned Equity Offering (SEO) timing in China's emerging market. It seeks to distinguish between two competing explanations for why firms issue equity: opportunistic market timing versus regulatory pressure.\n\n**Setting / Data-Generating Environment:** The analysis uses a Log-Logistic duration model to analyze the time from a firm's Initial Public Offering (IPO) to its first SEO. The key empirical challenge is that firm profitability, stock returns, and market-to-book ratios are often correlated, making it difficult to isolate the true driver of SEO timing.\n\n### Data / Model Specification\n\nThe duration model estimates the effect of covariates on the time until an SEO. The model's interpretation rule is: *\"a significantly positive (negative) sign suggests that the variable is positively (negatively) related to the duration, but negatively (positively) related to the hazard probability and the probability of equity financing.\"* A negative coefficient thus implies a shorter time to an SEO and a higher probability of issuance.\n\nTwo primary hypotheses are considered:\n1.  **Market Timing Hypothesis:** Managers opportunistically issue equity when they perceive their stock is overvalued, as proxied by a high Market-to-Book (`M/B`) ratio or high recent `Stock returns`.\n2.  **Regulatory Pressure Hypothesis:** The Chinese Securities Regulatory Commission (CSRC) requires firms to meet a minimum Return on Equity (`ROE`) to qualify for an SEO. This creates a powerful incentive for firms to report high profitability (`ROA` is used as a proxy) immediately before an issuance.\n\nTable 1 presents results from models testing the market timing hypothesis in isolation. Table 2 presents results from a multivariate \"horse race\" regression that includes both a profitability measure (`ROA`) and a market timing measure (`M/B` ratio) to see which has more explanatory power.\n\n**Table 1: Duration Analysis of Market Timing**\n\n| Variable | Model (1) Coeff. (t-stat) | Model (2) Coeff. (t-stat) |\n| :--- | :--- | :--- |\n| Market-to-book ratio (`M/B`) | -0.017** (-2.145) | | \n| Stock returns | | -1.515*** (-4.239) |\n\n*Source: Adapted from Table 6 of the paper. ** and *** denote significance at the 5% and 1% levels, respectively.*\n\n**Table 2: Duration Analysis of Profitability vs. Market Timing**\n\n| Variable | Coefficient (t-statistic) |\n| :--- | :--- |\n| Return on asset (`ROA`) | -4.477*** (-4.939) |\n| Market-to-book ratio (`M/B`) | -0.009 (-1.135) |\n\n*Source: Adapted from Table 8, Panel B of the paper. *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the provided data and interpretation rule, which of the following statements are correct interpretations of the regression results? (Select all that apply)", "Options": {"A": "In the joint model (Table 2), both the `M/B` ratio and `ROA` are statistically significant predictors of SEO timing.", "B": "When both `ROA` and `M/B` are included in the model (Table 2), `ROA` is the only statistically significant predictor of SEO timing.", "C": "The results in Table 1 suggest that a higher Market-to-Book ratio is associated with a longer time to an SEO.", "D": "In the standalone model (Table 1), higher recent stock returns are associated with a significantly shorter time to an SEO."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the fundamental skill of reading and interpreting regression tables, specifically focusing on coefficient signs and statistical significance. It is a select-all-valid task. Options A and B are correct readings of the tables. Distractor C represents a classic 'Sign Error,' misinterpreting the negative coefficient on M/B. Distractor D represents a 'Significance Error,' failing to notice that the M/B ratio's t-statistic in Table 2 is well below the threshold for significance.", "qid": "207", "question": "### Background\n\n**Research Question:** This case investigates the primary drivers of Seasoned Equity Offering (SEO) timing in China's emerging market. It seeks to distinguish between two competing explanations for why firms issue equity: opportunistic market timing versus regulatory pressure.\n\n**Setting / Data-Generating Environment:** The analysis uses a Log-Logistic duration model to analyze the time from a firm's Initial Public Offering (IPO) to its first SEO. The key empirical challenge is that firm profitability, stock returns, and market-to-book ratios are often correlated, making it difficult to isolate the true driver of SEO timing.\n\n### Data / Model Specification\n\nThe duration model estimates the effect of covariates on the time until an SEO. The model's interpretation rule is: *\"a significantly positive (negative) sign suggests that the variable is positively (negatively) related to the duration, but negatively (positively) related to the hazard probability and the probability of equity financing.\"* A negative coefficient thus implies a shorter time to an SEO and a higher probability of issuance.\n\nTwo primary hypotheses are considered:\n1.  **Market Timing Hypothesis:** Managers opportunistically issue equity when they perceive their stock is overvalued, as proxied by a high Market-to-Book (`M/B`) ratio or high recent `Stock returns`.\n2.  **Regulatory Pressure Hypothesis:** The Chinese Securities Regulatory Commission (CSRC) requires firms to meet a minimum Return on Equity (`ROE`) to qualify for an SEO. This creates a powerful incentive for firms to report high profitability (`ROA` is used as a proxy) immediately before an issuance.\n\nTable 1 presents results from models testing the market timing hypothesis in isolation. Table 2 presents results from a multivariate \"horse race\" regression that includes both a profitability measure (`ROA`) and a market timing measure (`M/B` ratio) to see which has more explanatory power.\n\n**Table 1: Duration Analysis of Market Timing**\n\n| Variable | Model (1) Coeff. (t-stat) | Model (2) Coeff. (t-stat) |\n| :--- | :--- | :--- |\n| Market-to-book ratio (`M/B`) | -0.017** (-2.145) | | \n| Stock returns | | -1.515*** (-4.239) |\n\n*Source: Adapted from Table 6 of the paper. ** and *** denote significance at the 5% and 1% levels, respectively.*\n\n**Table 2: Duration Analysis of Profitability vs. Market Timing**\n\n| Variable | Coefficient (t-statistic) |\n| :--- | :--- |\n| Return on asset (`ROA`) | -4.477*** (-4.939) |\n| Market-to-book ratio (`M/B`) | -0.009 (-1.135) |\n\n*Source: Adapted from Table 8, Panel B of the paper. *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the provided data and interpretation rule, which of the following statements are correct interpretations of the regression results? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 214, "Question": "### Background\n\n**Research Question.** How can a parsimonious stochastic model capture the primary dynamics of a population's mortality curve over time, and how sensitive is this model to its underlying assumptions?\n\n**Setting.** The Cairns two-factor model is used to describe the evolution of single-year death probabilities. The model's factors, `A(t)`, are assumed to follow a bivariate random walk with drift.\n\n### Data / Model Specification\n\nThe Cairns model specifies the logit of the death probability as a linear function of age:\n\n  \n\\ln\\left(\\frac{q_{x,t}}{1-q_{x,t}}\\right) = A_{1}(t)+A_{2}(t)x \\quad \\text{(Eq. (1))}\n \n\nThe dynamics of the mortality factors are modeled as a bivariate random walk with drift:\n\n  \nA(t+1) = A(t) + \\upmu + C Z(t+1) \\quad \\text{(Eq. (2))}\n \n\nThis implies that the annual changes, `D(t+1) = A(t+1) - A(t)`, are i.i.d. multivariate normal with mean `μ`. The Maximum Likelihood Estimator (MLE) for the drift vector `μ` simplifies to `μ_hat = (A(n) - A(0)) / n`, where `n` is the number of years in the sample.\n\n---\n\nBased on the model specification and its properties, which of the following statements are valid?\n", "Options": {"A": "The random-walk-with-drift specification makes long-term forecasts highly sensitive to anomalies or structural breaks near the end of the estimation sample.", "B": "The estimator for the drift `μ` depends only on the first and last observations of the factor `A(t)` in the sample period.", "C": "A decrease in the `A_1(t)` factor corresponds to a parallel downward shift in the mortality curve on a logit scale, reducing mortality at all ages.", "D": "An increase in the `A_2(t)` factor makes the mortality curve less steep, indicating that mortality improvements are faster at higher ages than at lower ages."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the Cairns two-factor mortality model, including the interpretation of its factors and the critical flaw in its standard estimation method. Depth Strategy: Scenario Application. The user must mentally apply changes to the model's factors and parameters to determine the consequences. Distractor Logic: Option A is a correct interpretation of the level factor. Option B is a correct statement about the MLE for the drift, as shown in the paper. Option C is the correct implication of the flaw identified in B, which is a central point of the paper's model risk section. Option D is a 'Conceptual Opposite' distractor; an increase in `A_2(t)` (the slope) makes the curve steeper, not less steep.", "qid": "214", "question": "### Background\n\n**Research Question.** How can a parsimonious stochastic model capture the primary dynamics of a population's mortality curve over time, and how sensitive is this model to its underlying assumptions?\n\n**Setting.** The Cairns two-factor model is used to describe the evolution of single-year death probabilities. The model's factors, `A(t)`, are assumed to follow a bivariate random walk with drift.\n\n### Data / Model Specification\n\nThe Cairns model specifies the logit of the death probability as a linear function of age:\n\n  \n\\ln\\left(\\frac{q_{x,t}}{1-q_{x,t}}\\right) = A_{1}(t)+A_{2}(t)x \\quad \\text{(Eq. (1))}\n \n\nThe dynamics of the mortality factors are modeled as a bivariate random walk with drift:\n\n  \nA(t+1) = A(t) + \\upmu + C Z(t+1) \\quad \\text{(Eq. (2))}\n \n\nThis implies that the annual changes, `D(t+1) = A(t+1) - A(t)`, are i.i.d. multivariate normal with mean `μ`. The Maximum Likelihood Estimator (MLE) for the drift vector `μ` simplifies to `μ_hat = (A(n) - A(0)) / n`, where `n` is the number of years in the sample.\n\n---\n\nBased on the model specification and its properties, which of the following statements are valid?\n", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 208, "Question": "### Background\n\nThe analysis compares summary statistics for FHA loan vintages (2007-2009) compiled using two different data structures: a **Borrower-Based** framework and a **Mortgage-Based** framework. The key distinction is the treatment of internal FHA-to-FHA refinances.\n\n*   **Borrower Data:** Tracks a borrower across all their FHA loans. A \"Prepay\" event occurs only when the borrower exits the FHA system entirely (e.g., sells the house). A \"Default\" is recorded for the borrower's original vintage, regardless of intervening refinances.\n*   **Mortgage Data:** Treats each loan as a separate entity. A \"Prepay\" event includes both true exits and internal FHA refinances. A default is attributed to the vintage of the specific loan that defaulted.\n\n### Data / Model Specification\n\n**Table 1: FHA Sustainability—Borrower vs. Mortgage Data (%)**\n\n| Vintage | Borrower Data: Default | Borrower Data: Prepay | Mortgage Data: Default | Mortgage Data: Prepay |\n| :--- | :--- | :--- | :--- | :--- |\n| 2007 | 42.7 | 10.9 | 32.3 | 32.7 |\n| 2008 | 29.6 | 11.2 | 21.0 | 37.0 |\n\n*Notes: Data as of September 2011. Default is ever reaching 90-days delinquent.*\n\n### Question\n\nBased on the data for the **2007 vintage** in Table 1, which of the following statements are correct quantitative deductions?", "Options": {"A": "Approximately two-thirds (66.7%) of the events recorded as \"Prepay\" in the mortgage data are actually internal refinances where credit risk remains with the FHA.", "B": "The rate of internal refinances for the 2007 vintage is 32.7%.", "C": "The mortgage-based framework understates the true default rate attributable to the 2007 cohort by 10.4 percentage points.", "D": "The mortgage-based framework overstates the true prepayment rate by a factor of 3.0."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: The item assesses the ability to perform multi-step calculations and interpret the quantitative discrepancies between the two data frameworks, which is the paper's core empirical finding. Depth Strategy: Computational Judgment. Distractor Logic: Option A requires calculating the ratio `32.7 / 10.9`. Option B requires a two-step calculation: first finding the rate of internal refinances (`32.7 - 10.9 = 21.8`), then finding its proportion of the total recorded prepayments (`21.8 / 32.7`). Option C requires a simple subtraction (`42.7 - 32.3`). Option D is a 'Formula Misuse' distractor, incorrectly equating the total mortgage prepay rate with the internal refinance rate, omitting the subtraction of true prepayments.", "qid": "208", "question": "### Background\n\nThe analysis compares summary statistics for FHA loan vintages (2007-2009) compiled using two different data structures: a **Borrower-Based** framework and a **Mortgage-Based** framework. The key distinction is the treatment of internal FHA-to-FHA refinances.\n\n*   **Borrower Data:** Tracks a borrower across all their FHA loans. A \"Prepay\" event occurs only when the borrower exits the FHA system entirely (e.g., sells the house). A \"Default\" is recorded for the borrower's original vintage, regardless of intervening refinances.\n*   **Mortgage Data:** Treats each loan as a separate entity. A \"Prepay\" event includes both true exits and internal FHA refinances. A default is attributed to the vintage of the specific loan that defaulted.\n\n### Data / Model Specification\n\n**Table 1: FHA Sustainability—Borrower vs. Mortgage Data (%)**\n\n| Vintage | Borrower Data: Default | Borrower Data: Prepay | Mortgage Data: Default | Mortgage Data: Prepay |\n| :--- | :--- | :--- | :--- | :--- |\n| 2007 | 42.7 | 10.9 | 32.3 | 32.7 |\n| 2008 | 29.6 | 11.2 | 21.0 | 37.0 |\n\n*Notes: Data as of September 2011. Default is ever reaching 90-days delinquent.*\n\n### Question\n\nBased on the data for the **2007 vintage** in Table 1, which of the following statements are correct quantitative deductions?", "model_evaluation_result": {"model_raw_response": "\"A,C,D\"", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 256, "Question": "### Background\n\n**Research Question.** Does a fundamental institutional preference for glamour stocks (and aversion to value stocks) persist after accounting for well-known drivers of institutional demand such as firm size and momentum?\n\n**Setting / Data-Generating Environment.** The study employs Fama-MacBeth style regressions. For each quarter `t`, a cross-sectional regression of the `Adjusted TR(i,t)` is run on lagged firm characteristics. The final reported coefficients are the time-series averages of the quarterly regression coefficients.\n\n### Data / Model Specification\n\nThe multivariate regression model for each quarter `t` is:\n  \n\\text{Adjusted } TR_{i,t} = \\alpha_t + \\beta_{1,t}(\\text{Lag size}_{i,t-1}) + \\beta_{2,t}(\\text{Past 6-month return}_{i,t-1}) + \\epsilon_{i,t} \n \n**Table 1. Time-Series Averages of Coefficients from Quarterly Regressions**\n| | **Panel A. Glamour Stocks** | **Panel B. Value Stocks** |\n| :--- | :--- | :--- |\n| Intercept | 0.3571*** | -0.1141*** |\n| Lag size | -0.0404*** | 0.0254** |\n| Past 6-month return | -0.0260 | 0.2374*** |\n\n*Note: *** denotes significance at 1%, ** at 5%.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following statements accurately describe institutional trading preferences?\n", "Options": {"A": "Within the universe of value stocks, institutions show a statistically significant preference for those with stronger past 6-month returns.", "B": "After controlling for size and momentum, a significant baseline institutional preference for buying glamour stocks and selling value stocks remains.", "C": "Across both glamour and value stocks, institutions consistently prefer larger firms, as indicated by positive and significant 'Lag size' coefficients in both panels.", "D": "Within the universe of glamour stocks, institutions show a statistically significant preference for smaller firms over larger firms."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item tests the ability to interpret multiple coefficients from a multivariate regression table and compare results across different sample groups (Glamour vs. Value).\nDepth Strategy: Scenario Application. The general regression model is provided, and the user must apply it to interpret the specific coefficient estimates for different stock styles.\nDistractor Logic:\n- A (Correct): This is a correct interpretation of the significant positive intercept in Panel A and the significant negative intercept in Panel B.\n- B (Correct): This is a correct interpretation of the significant positive coefficient on 'Past 6-month return' in Panel B.\n- C (Correct): This is a correct interpretation of the significant negative coefficient on 'Lag size' in Panel A.\n- D (Incorrect): This is a Conceptual Opposite / Factual Error. The 'Lag size' coefficient is significantly negative for glamour stocks, indicating a preference for smaller firms in that category, which contradicts the statement.", "qid": "256", "question": "### Background\n\n**Research Question.** Does a fundamental institutional preference for glamour stocks (and aversion to value stocks) persist after accounting for well-known drivers of institutional demand such as firm size and momentum?\n\n**Setting / Data-Generating Environment.** The study employs Fama-MacBeth style regressions. For each quarter `t`, a cross-sectional regression of the `Adjusted TR(i,t)` is run on lagged firm characteristics. The final reported coefficients are the time-series averages of the quarterly regression coefficients.\n\n### Data / Model Specification\n\nThe multivariate regression model for each quarter `t` is:\n  \n\\text{Adjusted } TR_{i,t} = \\alpha_t + \\beta_{1,t}(\\text{Lag size}_{i,t-1}) + \\beta_{2,t}(\\text{Past 6-month return}_{i,t-1}) + \\epsilon_{i,t} \n \n**Table 1. Time-Series Averages of Coefficients from Quarterly Regressions**\n| | **Panel A. Glamour Stocks** | **Panel B. Value Stocks** |\n| :--- | :--- | :--- |\n| Intercept | 0.3571*** | -0.1141*** |\n| Lag size | -0.0404*** | 0.0254** |\n| Past 6-month return | -0.0260 | 0.2374*** |\n\n*Note: *** denotes significance at 1%, ** at 5%.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following statements accurately describe institutional trading preferences?\n", "model_evaluation_result": {"model_raw_response": "\"A,B,D\"", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 242, "Question": "### Background\n\n**Research Question.** How do the market prices of local vs. global macroeconomic risks vary over the business cycle, and what are the implications for investors in small, open economies like Finland and Sweden?\n\n**Setting.** This study uses a GMM framework to test if the sensitivity of asset returns to industrial production growth and volatility is state-dependent. The paper finds that for Finland, the positive link between expected stock returns and expected local production growth is strongest when the economy is weak. It also finds that higher global production risk lowers bond yields, an effect that is most pronounced during weak global business conditions.\n\n### Data / Model Specification\n\nThe core model for conditional expected stock returns (`r_m`) is:\n\n  \nE[r_{mt} | F_{t-1}] \\propto (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) E[YP_t | F_{t-1}] + (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) E[e_{1t}^2 | F_{t-1}] \\quad \\text{(Eq. 1)}\n \n\nThe table below summarizes key hypothesis test results for stock returns.\n\n**Table 1: Summary of Hypothesis Tests for State-Dependency of Stock Returns (p-values)**\n\n| Factor Origin | Country | H₀: Price of Growth Risk is Constant (`λ₁s = 0`) | H₀: Price of Volatility Risk is Constant (`κ₁s = 0`) |\n| :--- | :--- | :--- | :--- |\n| **Local** | Finland | 0.009 | 0.000 |\n| **Local** | Sweden | 0.287 | 0.000 |\n| **Global (OECD)** | Finland & Sweden | 0.000 | 0.412 |\n\n### Question\n\nBased on the paper's empirical findings and theoretical framework, which of the following interpretations or actions are justified?", "Options": {"A": "A portfolio manager should consider increasing their allocation to Finnish stocks when the local Finnish economy is weak, as this is when the expected return premium for bearing local growth risk is highest.", "B": "The finding that higher global risk lowers bond yields, especially in a downturn, is a classic example of a \"flight to safety\" driven by a precautionary savings motive.", "C": "The results in Table 1 suggest that the price of global volatility risk for stocks is highly sensitive to the state of the business cycle.", "D": "The state-dependency of risk prices is likely driven by investors becoming *less* risk-averse during economic downturns as they seek higher returns."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a 'Scenario Application' strategy to assess whether a user can connect the paper's empirical results to both its theoretical underpinnings and practical investment actions. Correct options (A, B) require synthesizing a finding with its theoretical name or its strategic implication. Distractor (C) is a 'Conceptual Opposite' of the paper's core theory (counter-cyclical risk aversion). Distractor (D) tests for careful reading of the statistical results in the provided table.", "qid": "242", "question": "### Background\n\n**Research Question.** How do the market prices of local vs. global macroeconomic risks vary over the business cycle, and what are the implications for investors in small, open economies like Finland and Sweden?\n\n**Setting.** This study uses a GMM framework to test if the sensitivity of asset returns to industrial production growth and volatility is state-dependent. The paper finds that for Finland, the positive link between expected stock returns and expected local production growth is strongest when the economy is weak. It also finds that higher global production risk lowers bond yields, an effect that is most pronounced during weak global business conditions.\n\n### Data / Model Specification\n\nThe core model for conditional expected stock returns (`r_m`) is:\n\n  \nE[r_{mt} | F_{t-1}] \\propto (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) E[YP_t | F_{t-1}] + (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) E[e_{1t}^2 | F_{t-1}] \\quad \\text{(Eq. 1)}\n \n\nThe table below summarizes key hypothesis test results for stock returns.\n\n**Table 1: Summary of Hypothesis Tests for State-Dependency of Stock Returns (p-values)**\n\n| Factor Origin | Country | H₀: Price of Growth Risk is Constant (`λ₁s = 0`) | H₀: Price of Volatility Risk is Constant (`κ₁s = 0`) |\n| :--- | :--- | :--- | :--- |\n| **Local** | Finland | 0.009 | 0.000 |\n| **Local** | Sweden | 0.287 | 0.000 |\n| **Global (OECD)** | Finland & Sweden | 0.000 | 0.412 |\n\n### Question\n\nBased on the paper's empirical findings and theoretical framework, which of the following interpretations or actions are justified?", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 11, "Question": "### Background\n\nA Canadian life insurance company offers segregated fund products, which are similar to mutual funds but include a capital guarantee upon death or maturity (e.g., a return of at least 75% of premiums paid). The cost of this guarantee is highly sensitive to stock market and interest rate movements. A regulatory task force has recommended a stochastic, simulation-based approach to determine the necessary financial resources.\n\n### Data / Model Specification\n\nThe regulatory framework for segregated fund guarantees is based on the output of an internal stochastic simulation model that generates a probability distribution for the future cost, `C`. The capital requirement, `K`, is determined by the following rule:\n\n  \nK = \\mathrm{CTE}(95) - L \n \n\nwhere `L` is the policy liability and `CTE(95)` is the Conditional Tail Expectation of the cost `C` at the 95th percentile, i.e., the average cost conditional on being in the worst 5% of simulated scenarios. The total financial resource requirement is `L + K = CTE(95)`.\n\nAn insurer's simulation model produces the following outputs for a block of guarantees:\n\n*   Mean of the simulated cost distribution: $50 million\n*   95th percentile of the cost distribution: $120 million\n*   Average of all costs exceeding the 95th percentile: $180 million\n\nThe insurer's accounting policy sets the policy liability `L` equal to the mean of the simulated costs.\n\n### Question\n\nBased on the provided data and model specification, which of the following statements are correct?\n", "Options": {"A": "The policy liability, `L`, for this block of business is $120 million.", "B": "The total financial resource the regulator requires the firm to hold, `L + K`, is $180 million.", "C": "The required capital, `K`, represents the average loss in the worst 5% of scenarios.", "D": "The required capital, `K`, for this block of business is $130 million."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to correctly apply the regulatory capital formula by identifying the correct inputs from a set of distributional statistics and interpreting the components of the formula.\nDepth Strategy: Computational Judgment. The candidate must perform a multi-step calculation (identify L, identify CTE(95), then calculate K) and interpret the meaning of the total financial resource.\nDistractor Logic:\n- C (Step-Omission Error): Incorrectly uses the 95th percentile value for the liability `L` instead of the specified mean.\n- D (Conceptual Distractor - Almost Right): Incorrectly defines capital `K` as being equal to CTE(95), when it is actually the amount *in excess* of the liability `L` needed to reach CTE(95).", "qid": "11", "question": "### Background\n\nA Canadian life insurance company offers segregated fund products, which are similar to mutual funds but include a capital guarantee upon death or maturity (e.g., a return of at least 75% of premiums paid). The cost of this guarantee is highly sensitive to stock market and interest rate movements. A regulatory task force has recommended a stochastic, simulation-based approach to determine the necessary financial resources.\n\n### Data / Model Specification\n\nThe regulatory framework for segregated fund guarantees is based on the output of an internal stochastic simulation model that generates a probability distribution for the future cost, `C`. The capital requirement, `K`, is determined by the following rule:\n\n  \nK = \\mathrm{CTE}(95) - L \n \n\nwhere `L` is the policy liability and `CTE(95)` is the Conditional Tail Expectation of the cost `C` at the 95th percentile, i.e., the average cost conditional on being in the worst 5% of simulated scenarios. The total financial resource requirement is `L + K = CTE(95)`.\n\nAn insurer's simulation model produces the following outputs for a block of guarantees:\n\n*   Mean of the simulated cost distribution: $50 million\n*   95th percentile of the cost distribution: $120 million\n*   Average of all costs exceeding the 95th percentile: $180 million\n\nThe insurer's accounting policy sets the policy liability `L` equal to the mean of the simulated costs.\n\n### Question\n\nBased on the provided data and model specification, which of the following statements are correct?\n", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: print(180 - 50)\n\n\nB,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 260, "Question": "### Background\n\nThis case examines the attitudes of corporate directors in an emerging market towards global corporate governance standards. The analysis is based on a survey of directors of companies listed on the Taiwan Security Exchange (TSE). Taiwan's corporate governance system is characterized as \"insider-oriented,\" with ownership and control concentrated in founding families, contrasting with \"outsider systems\" where ownership is dispersed. Director attitudes are measured on a 7-point Likert scale, where 1 = strongly disagree and 7 = strongly agree.\n\n### Data / Model Specification\n\nThe central proposition to be evaluated is:\n\n**Hypothesis One:** Company directors want corporate governance reform in Taiwan and want to adopt internationally acceptable corporate governance standards.\n\nSurvey evidence regarding this hypothesis is presented in Table 1.\n\n**Table 1: General Attitudes towards Corporate Governance**\n\n| Statement | N | Mean | Median | Mode | SD |\n| :--- | :-: | :-: | :-: | :-: | :-: |\n| (a) I believe that there should be increased international harmonisation of corporate governance standards | 55 | 5.05 | 5 | 4 | 1.35 |\n| (c) I believe that Taiwan companies should adopt a more Anglo-Saxon model of corporate governance | 53 | 4.04 | 4 | 3 | 1.22 |\n| (d) I believe that international harmonisation of corporate governance standards will not encourage foreign investors to invest in Taiwan companies | 55 | 2.76 | 3 | 3 | 1.05 |\n\n*Note: Scores are on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).* \n\n### Question\n\nBased on the survey data in Table 1 and the background information, which of the following conclusions can be validly drawn about the Taiwanese directors' stance on corporate governance reform? Select all that apply.", "Options": {"A": "Directors show a clear preference for a selective adaptation of global principles rather than a wholesale adoption of the Anglo-Saxon model.", "B": "Directors are enthusiastic about fully replacing their current insider-oriented system with a US/UK-style outsider model.", "C": "Directors strongly believe that adopting international governance standards is a key mechanism for attracting foreign capital.", "D": "Directors are largely indifferent to the idea of international corporate governance reform, viewing it as irrelevant to their firms."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to synthesize multiple data points to form a nuanced conclusion, a key objective of the original QA problem. It uses a Reverse-Reasoning strategy, asking for valid conclusions from the provided data. Option A is correct because the strong disagreement with statement (d) (mean 2.76) implies strong agreement with its opposite. Option B is correct because the high mean for harmonization (5.05) contrasts with the neutral mean for the specific Anglo-Saxon model (4.04). Distractor C is a conceptual opposite, directly contradicted by the high mean for statement (a). Distractor D is an overstatement and a conceptual opposite of the nuanced conclusion in B, representing a plausible misinterpretation of the data.", "qid": "260", "question": "### Background\n\nThis case examines the attitudes of corporate directors in an emerging market towards global corporate governance standards. The analysis is based on a survey of directors of companies listed on the Taiwan Security Exchange (TSE). Taiwan's corporate governance system is characterized as \"insider-oriented,\" with ownership and control concentrated in founding families, contrasting with \"outsider systems\" where ownership is dispersed. Director attitudes are measured on a 7-point Likert scale, where 1 = strongly disagree and 7 = strongly agree.\n\n### Data / Model Specification\n\nThe central proposition to be evaluated is:\n\n**Hypothesis One:** Company directors want corporate governance reform in Taiwan and want to adopt internationally acceptable corporate governance standards.\n\nSurvey evidence regarding this hypothesis is presented in Table 1.\n\n**Table 1: General Attitudes towards Corporate Governance**\n\n| Statement | N | Mean | Median | Mode | SD |\n| :--- | :-: | :-: | :-: | :-: | :-: |\n| (a) I believe that there should be increased international harmonisation of corporate governance standards | 55 | 5.05 | 5 | 4 | 1.35 |\n| (c) I believe that Taiwan companies should adopt a more Anglo-Saxon model of corporate governance | 53 | 4.04 | 4 | 3 | 1.22 |\n| (d) I believe that international harmonisation of corporate governance standards will not encourage foreign investors to invest in Taiwan companies | 55 | 2.76 | 3 | 3 | 1.05 |\n\n*Note: Scores are on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).* \n\n### Question\n\nBased on the survey data in Table 1 and the background information, which of the following conclusions can be validly drawn about the Taiwanese directors' stance on corporate governance reform? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A", "model_answer": ["A"], "error": null}}
{"ID": 254, "Question": "### Background\n\n**Research Question.** How do different tail-focused premium principles comparatively rank two risks with identical mean and variance but different tail thickness?\n\n**Setting.** A numerical example comparing premiums for a heavy-tailed Pareto risk (`X`) and a less heavy-tailed Weibull risk (`Y`). Both risks have been constructed to have the same mean (1.0) and variance (3.0). The goal is to see if the premium principles can correctly identify the Pareto as the more dangerous risk.\n\n**Variables and Parameters.**\n\n*   `X`: A Pareto-distributed loss (heavy-tailed).\n*   `Y`: A Weibull-distributed loss (thinner-tailed).\n*   `p`: The probability level defining the tail.\n*   `T_{1,1,1}`: Premium principle based on cumulative residual entropy.\n*   `T_{1,2,1}`: Premium principle based on Gini Mean Difference.\n*   `T_{1,0.5,1}`: Premium principle based on Wang's right tail deviation.\n\n---\n\n### Data / Model Specification\n\nTwo risks, `X` (Pareto) and `Y` (Weibull), have `E[X] = E[Y] = 1` and `Var(X) = Var(Y) = 3`. The Pareto distribution is known to have a heavier tail than this specific Weibull distribution.\n\n**Table 1: Premiums for Pareto Risk `X` at p=0.99**\n\n| T<sub>1,1,1</sub>(X,0.99) (Entropy) | T<sub>1,2,1</sub>(X,0.99) (Gini) | T<sub>1,0.5,1</sub>(X,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 18.887 | 14.710 | 39.774 |\n\n**Table 2: Premiums for Weibull Risk `Y` at p=0.99**\n\n| T<sub>1,1,1</sub>(Y,0.99) (Entropy) | T<sub>1,2,1</sub>(Y,0.99) (Gini) | T<sub>1,0.5,1</sub>(Y,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 15.476 | 13.510 | 19.911 |\n\n---\n\n### Question\n\nAn actuary and a regulator are evaluating the data in **Table 1** and **Table 2**. Based on this evidence, select all valid conclusions.", "Options": {"A": "An actuary who is uncertain about the true tail shape and wants to set a price that is least dependent on this specific assumption would favor the Gini-based principle (`T_{1,2,1}`).", "B": "A regulator whose primary goal is to ensure solvency by heavily penalizing potential model misspecification toward heavy tails would prefer the Wang-based principle (`T_{1,0.5,1}`).", "C": "All three principles are equally suitable for setting regulatory capital, as they all correctly identify the Pareto risk as the more dangerous one.", "D": "For regulatory purposes, the Gini-based principle (`T_{1,2,1}`) is superior because it results in the highest premium for the heavy-tailed Pareto risk, ensuring maximum safety."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to apply empirical findings to a practical scenario involving risk management and regulation. It uses a Scenario Application strategy. The correct options require connecting the observed sensitivity of the Wang principle to regulatory goals and the observed robustness of the Gini principle to pricing stability goals. Distractors are designed to be plausible but incorrect: (C) contains a direct Factual Error from the tables (Wang's premium is highest), and (D) is a 'True but Irrelevant' statement that ignores the critical differences in magnitude and sensitivity, which are paramount in this context.", "qid": "254", "question": "### Background\n\n**Research Question.** How do different tail-focused premium principles comparatively rank two risks with identical mean and variance but different tail thickness?\n\n**Setting.** A numerical example comparing premiums for a heavy-tailed Pareto risk (`X`) and a less heavy-tailed Weibull risk (`Y`). Both risks have been constructed to have the same mean (1.0) and variance (3.0). The goal is to see if the premium principles can correctly identify the Pareto as the more dangerous risk.\n\n**Variables and Parameters.**\n\n*   `X`: A Pareto-distributed loss (heavy-tailed).\n*   `Y`: A Weibull-distributed loss (thinner-tailed).\n*   `p`: The probability level defining the tail.\n*   `T_{1,1,1}`: Premium principle based on cumulative residual entropy.\n*   `T_{1,2,1}`: Premium principle based on Gini Mean Difference.\n*   `T_{1,0.5,1}`: Premium principle based on Wang's right tail deviation.\n\n---\n\n### Data / Model Specification\n\nTwo risks, `X` (Pareto) and `Y` (Weibull), have `E[X] = E[Y] = 1` and `Var(X) = Var(Y) = 3`. The Pareto distribution is known to have a heavier tail than this specific Weibull distribution.\n\n**Table 1: Premiums for Pareto Risk `X` at p=0.99**\n\n| T<sub>1,1,1</sub>(X,0.99) (Entropy) | T<sub>1,2,1</sub>(X,0.99) (Gini) | T<sub>1,0.5,1</sub>(X,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 18.887 | 14.710 | 39.774 |\n\n**Table 2: Premiums for Weibull Risk `Y` at p=0.99**\n\n| T<sub>1,1,1</sub>(Y,0.99) (Entropy) | T<sub>1,2,1</sub>(Y,0.99) (Gini) | T<sub>1,0.5,1</sub>(Y,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 15.476 | 13.510 | 19.911 |\n\n---\n\n### Question\n\nAn actuary and a regulator are evaluating the data in **Table 1** and **Table 2**. Based on this evidence, select all valid conclusions.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 265, "Question": "### Background\n\n**Research Question.** This study seeks to determine if international investment restrictions are binding by examining their effect on closed-end country fund premiums. The simple hypothesis is that countries with more severe restrictions should exhibit higher premiums.\n\n**Setting / Data-Generating Environment.** The analysis uses weekly data for a sample of closed-end country funds from 1981 to 1989. The paper notes that a simple cross-sectional comparison of premium *levels* is problematic due to confounding factors, motivating a shift to an event study of premium *changes*.\n\n---\n\n### Data / Model Specification\n\n**Table 1** below presents descriptive statistics for the average percentage premiums of selected country funds.\n\n**Table 1: Average Percentage Premiums for Selected Closed-End Country Funds**\n\n| Fund (Country) | Mean Premium (%) |\n| :--- | :---: |\n| **High Restriction Markets** | |\n| Korea | 44.35 |\n| Taiwan | 40.96 |\n| Mexico | -7.46 |\n| Brazil | -28.82 |\n| **Low Restriction Markets** | |\n| U.K. | -21.37 |\n| Germany | -4.32 |\n\n*Source: Adapted from Table II, Panel A of the source paper.*\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the paper's methodological reasoning, select all of the following statements that are accurate.", "Options": {"A": "The large premium for the Korea Fund (+44.35%) is definitive proof that its investment restrictions were the most binding among all countries in the sample.", "B": "The data reveal a non-monotonic relationship between presumed restriction severity and premium levels, as high-restriction countries like Brazil and Mexico show large average discounts.", "C": "The average discount on the U.K. Fund (-21.37%) is primarily explained by the country's low level of investment restrictions.", "D": "The puzzle presented by the premium levels motivates the paper's use of an event study on premium *changes*, which can isolate the effect of a policy change from time-invariant, country-specific confounding factors."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the understanding of the core methodological motivation for the entire paper. It uses a 'Premise/Assumption Packaging' strategy. Option A requires correct interpretation of the descriptive data in the table. Option B requires understanding why the pattern in Option A leads to the choice of an event-study methodology. Distractor C is a 'Conceptual' error (overstatement), as the paper argues one cannot make this conclusion from levels alone. Distractor D is also a 'Conceptual' error, as it makes a causal claim the paper explicitly avoids, noting that the cause of domestic fund discounts is an unsolved puzzle.", "qid": "265", "question": "### Background\n\n**Research Question.** This study seeks to determine if international investment restrictions are binding by examining their effect on closed-end country fund premiums. The simple hypothesis is that countries with more severe restrictions should exhibit higher premiums.\n\n**Setting / Data-Generating Environment.** The analysis uses weekly data for a sample of closed-end country funds from 1981 to 1989. The paper notes that a simple cross-sectional comparison of premium *levels* is problematic due to confounding factors, motivating a shift to an event study of premium *changes*.\n\n---\n\n### Data / Model Specification\n\n**Table 1** below presents descriptive statistics for the average percentage premiums of selected country funds.\n\n**Table 1: Average Percentage Premiums for Selected Closed-End Country Funds**\n\n| Fund (Country) | Mean Premium (%) |\n| :--- | :---: |\n| **High Restriction Markets** | |\n| Korea | 44.35 |\n| Taiwan | 40.96 |\n| Mexico | -7.46 |\n| Brazil | -28.82 |\n| **Low Restriction Markets** | |\n| U.K. | -21.37 |\n| Germany | -4.32 |\n\n*Source: Adapted from Table II, Panel A of the source paper.*\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the paper's methodological reasoning, select all of the following statements that are accurate.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 266, "Question": "### Background\n\n**Research Question.** This study investigates the effects of international investment restrictions on asset prices. The premium on a closed-end country fund can be affected by both investment barriers and the ratio of the domestic (U.S.) to foreign market price of risk.\n\n**Setting / Data-Generating Environment.** The paper discusses the October 1987 stock market crash as a natural experiment. This event is viewed as a large, sudden shock that increased the U.S. market price of risk, making U.S. investors more risk-averse and demanding of higher returns on domestic assets.\n\n---\n\n### Question\n\nBased on the paper's logic regarding the effect of relative market prices of risk on fund premiums, select all predicted outcomes of a large, sudden increase in the U.S. market price of risk (such as the October 1987 crash).", "Options": {"A": "The price-to-NAV ratios of funds that initially traded at a large premium (e.g., Korea Fund) and funds that traded at a discount (e.g., U.K. Fund) would both be expected to fall.", "B": "The net asset values (NAVs) of the funds would fall while their U.S. share prices would remain relatively stable, causing the premium to decrease.", "C": "Only funds from countries with high investment restrictions would be affected, as market segmentation is required for relative risk prices to matter.", "D": "An increase in U.S. risk aversion would generally drive down country fund share prices relative to their net asset values, as U.S. investors become less willing to pay a premium to access foreign assets."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a 'Scenario Application' strategy, asking students to apply the paper's theoretical logic about a confounding factor (relative risk prices) to a specific event (the 1987 crash). Option A correctly identifies the mechanism. Option B correctly applies this mechanism to different types of funds. Distractor C is a 'Conceptual Opposite' regarding the mechanics of the price change (the shock is to U.S. investors, so the U.S. price is the primary mover). Distractor D is 'Almost Right' but incorrect; while segmentation is the backdrop, the shock to relative risk prices would affect all funds, not just those from restricted markets.", "qid": "266", "question": "### Background\n\n**Research Question.** This study investigates the effects of international investment restrictions on asset prices. The premium on a closed-end country fund can be affected by both investment barriers and the ratio of the domestic (U.S.) to foreign market price of risk.\n\n**Setting / Data-Generating Environment.** The paper discusses the October 1987 stock market crash as a natural experiment. This event is viewed as a large, sudden shock that increased the U.S. market price of risk, making U.S. investors more risk-averse and demanding of higher returns on domestic assets.\n\n---\n\n### Question\n\nBased on the paper's logic regarding the effect of relative market prices of risk on fund premiums, select all predicted outcomes of a large, sudden increase in the U.S. market price of risk (such as the October 1987 crash).", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 237, "Question": "### Background\n\nA researcher estimates a Vector Error Correction Model (VECM) for one-minute midquotes for Expedia in 2007, originating from three groups of exchanges: NYSE/ARCA, NASD, and all others (REST). The goal is to assess the relative importance of each exchange group in price discovery using the standard Hasbrouck methodology.\n\n### Data / Model Specification\n\n**Table 1: VECM Estimation Results for Expedia (from paper's Table 7)**\n\n| Dealer | Residual Covariances (Correlations) | Info Shares | |\n|:---|:---|:---|---:|\n| | NYSE | NASD | REST | min | max |\n| NYSE | 1.00 | *0.90* | *0.94* | 0.024 | 0.974 |\n| NASD | 0.96 | 1.15 | *0.87* | 0.001 | 0.824 |\n| REST | 0.96 | 0.95 | 1.05 | 0.024 | 0.964 |\n\n---\n\nBased on the provided results and the context of the Hasbrouck methodology, which of the following statements are valid conclusions or interpretations?", "Options": {"A": "The extremely high residual correlations (e.g., 0.94 between NYSE and REST) are the primary cause of the uninformatively wide information share bounds.", "B": "The results clearly indicate that NASD is the least important market for price discovery, as its minimum information share is the lowest at 0.1%.", "C": "Despite the wide bounds, a definitive conclusion can be made that NYSE is more important than NASD because its minimum bound (2.4%) is higher than NASD's minimum bound (0.1%).", "D": "The wide bounds demonstrate a failure of the Hasbrouck methodology in this context, as the Cholesky decomposition arbitrarily allocates the large shared variance component to whichever market is ordered first."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the user's understanding of the fundamental limitations of the Hasbrouck information share methodology, particularly in highly correlated modern markets. It uses a Reverse-Reasoning strategy (given the outcome of wide bounds, identify the causes). Option A correctly identifies the high residual correlation as the root cause. Option C correctly explains the mechanism of failure (arbitrary allocation of shared variance by Cholesky decomposition). Options B and D are conceptual distractors that represent a naive or incorrect interpretation of the min/max bounds; the wide and overlapping nature of the bounds makes any such ranking impossible.", "qid": "237", "question": "### Background\n\nA researcher estimates a Vector Error Correction Model (VECM) for one-minute midquotes for Expedia in 2007, originating from three groups of exchanges: NYSE/ARCA, NASD, and all others (REST). The goal is to assess the relative importance of each exchange group in price discovery using the standard Hasbrouck methodology.\n\n### Data / Model Specification\n\n**Table 1: VECM Estimation Results for Expedia (from paper's Table 7)**\n\n| Dealer | Residual Covariances (Correlations) | Info Shares | |\n|:---|:---|:---|---:|\n| | NYSE | NASD | REST | min | max |\n| NYSE | 1.00 | *0.90* | *0.94* | 0.024 | 0.974 |\n| NASD | 0.96 | 1.15 | *0.87* | 0.001 | 0.824 |\n| REST | 0.96 | 0.95 | 1.05 | 0.024 | 0.964 |\n\n---\n\nBased on the provided results and the context of the Hasbrouck methodology, which of the following statements are valid conclusions or interpretations?", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 249, "Question": "### Background\n\nA key point of contention in the proposed Volvo-Renault merger was the \"Golden Share,\" a special control right retained by the French government in the new entity, Renault-Volvo Automotive (RVA). This provision was a major concern for institutional investors, who felt it undermined Volvo's position as a partner in the venture.\n\n### Data / Model Specification\n\nThe Golden Share provision granted the French government, already the 65% majority owner of RVA, several powers beyond its ordinary shareholding. These included:\n\n1.  The right to limit any single shareholder's stake to a maximum of 20%.\n2.  The right to force Volvo to reduce its 35% stake down to 20% if the holding company structure were ever dissolved.\n\nIn response to investor concerns, the French Prime Minister offered an assurance that the Golden Share would not be used against Volvo *as long as Volvo did not acquire more than 35% of RVA*.\n\nFrom a financial perspective, such control rights can be valued as a portfolio of contingent claims (options) held by the government at the expense of other shareholders, thereby reducing the *ex-ante* value of those shareholders' stake.\n\n---\n\nBased on the description of the Golden Share, select all of the following statements that accurately characterize its economic implications for Volvo's shareholders.\n", "Options": {"A": "The right to force Volvo to sell down its stake from 35% to 20% upon dissolution of the holding company creates a contingent liability for Volvo, similar to Volvo having written a put option to the French government.", "B": "The existence of the Golden Share meant that Volvo's 35% equity stake did not carry pro-rata control rights, giving the deal what one investor called the \"character of a takeover.\"", "C": "The provision to cap any shareholder's stake at 20% is economically equivalent to the French government holding a call option on Volvo's control premium, which truncates the potential upside value of Volvo's investment.", "D": "The French Prime Minister's assurance meaningfully mitigated the governance risk by guaranteeing that Volvo's 35% stake was secure under all future circumstances."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to translate complex legal/governance terms (a 'Golden Share') into their precise economic and financial equivalents, specifically using the concept of embedded options and contingent claims.\nDepth Strategy: Reverse-Reasoning. Given the terms of the Golden Share, the candidate must identify the plausible economic consequences and valuations.\nDistractor Logic:\n- A: Correct. This accurately frames the control cap as a call option held by the government on the value of Volvo's stake above a certain threshold, which limits Volvo's potential gains.\n- B: This is a Conceptual Distractor (Almost Right). The assurance was economically hollow because it only promised not to challenge the 35% stake as long as it remained at 35%, offering no new protection against the other clauses (e.g., the forced sale upon dissolution).\n- C: Correct. This correctly identifies the forced-sale provision as a put option. The government has the right, but not the obligation, to force Volvo to sell its shares under a specific condition (dissolution), which is a classic put structure written by Volvo.\n- D: Correct. This accurately synthesizes the overall impact of the Golden Share. The non-pro-rata rights and the ultimate, non-negotiable control resting with the French state justify the \"takeover\" characterization, as Volvo was a subordinate party.", "qid": "249", "question": "### Background\n\nA key point of contention in the proposed Volvo-Renault merger was the \"Golden Share,\" a special control right retained by the French government in the new entity, Renault-Volvo Automotive (RVA). This provision was a major concern for institutional investors, who felt it undermined Volvo's position as a partner in the venture.\n\n### Data / Model Specification\n\nThe Golden Share provision granted the French government, already the 65% majority owner of RVA, several powers beyond its ordinary shareholding. These included:\n\n1.  The right to limit any single shareholder's stake to a maximum of 20%.\n2.  The right to force Volvo to reduce its 35% stake down to 20% if the holding company structure were ever dissolved.\n\nIn response to investor concerns, the French Prime Minister offered an assurance that the Golden Share would not be used against Volvo *as long as Volvo did not acquire more than 35% of RVA*.\n\nFrom a financial perspective, such control rights can be valued as a portfolio of contingent claims (options) held by the government at the expense of other shareholders, thereby reducing the *ex-ante* value of those shareholders' stake.\n\n---\n\nBased on the description of the Golden Share, select all of the following statements that accurately characterize its economic implications for Volvo's shareholders.\n", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 257, "Question": "### Background\n\n**Research Question.** How can one construct a standardized measure of net institutional trading that isolates active buying or selling decisions from confounding price effects and aggregate fund flows?\n\n### Data / Model Specification\n\nThe unadjusted Trade Ratio is defined as:\n  \nTR(i,t) = \\frac{P(i,t) - S(i,t)}{P(i,t) + S(i,t)} \\quad \\text{(Eq. 1)}\n \nwhere `P(i,t)` and `S(i,t)` are non-negative share counts of purchases and sales, respectively.\n\nTo control for the influence of aggregate fund flows, an adjusted measure is constructed:\n  \n\\text{Adjusted } TR(i,t) = TR(i,t) - \\overline{TR(t)} \\quad \\text{(Eq. 2)}\n \nwhere `\\overline{TR(t)}` is the cross-sectional average of `TR(i,t)` across all stocks in quarter `t`.\n\n---\n\nWhich of the following statements about the Trade Ratio (`TR`) and its adjustment are mathematically or conceptually correct?\n", "Options": {"A": "The `TR(i,t)` for a stock will be exactly zero only if institutions did not trade the stock at all during the quarter.", "B": "If institutions experience a large net fund inflow in quarter `t`, `\\overline{TR(t)}` will likely be positive, and a stock with an `Adjusted TR(i,t)` near zero was likely bought at a rate similar to the market average.", "C": "The unadjusted `TR(i,t)` is mathematically bounded between -1 and +1, inclusive.", "D": "The adjustment in Eq. (2) is designed to isolate active stock selection decisions from the mechanical trading effects of market-wide net fund inflows or outflows to institutions."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's understanding of the construction and interpretation of the paper's core metric, the Adjusted Trade Ratio. It combines mathematical properties with conceptual purpose.\nDepth Strategy: Computational Judgment. The user must evaluate the mathematical properties and conceptual underpinnings of the provided equations.\nDistractor Logic:\n- A (Correct): This is a provable mathematical property of the ratio, as `P-S` cannot be larger in magnitude than `P+S`.\n- B (Correct): This is the stated conceptual purpose of the adjustment, as explained in the paper.\n- C (Correct): This correctly interprets the meaning of the adjustment term `\\overline{TR(t)}` and the resulting `Adjusted TR` in a specific scenario (net fund inflow).\n- D (Incorrect): This is a Step-Omission Error. `TR(i,t)` is zero if the numerator is zero, which occurs when `P(i,t) = S(i,t)`. This means net trading is zero, but total trading (`P+S`) could be very large. The ratio is undefined, not zero, if no trading occurs (`P=S=0`).", "qid": "257", "question": "### Background\n\n**Research Question.** How can one construct a standardized measure of net institutional trading that isolates active buying or selling decisions from confounding price effects and aggregate fund flows?\n\n### Data / Model Specification\n\nThe unadjusted Trade Ratio is defined as:\n  \nTR(i,t) = \\frac{P(i,t) - S(i,t)}{P(i,t) + S(i,t)} \\quad \\text{(Eq. 1)}\n \nwhere `P(i,t)` and `S(i,t)` are non-negative share counts of purchases and sales, respectively.\n\nTo control for the influence of aggregate fund flows, an adjusted measure is constructed:\n  \n\\text{Adjusted } TR(i,t) = TR(i,t) - \\overline{TR(t)} \\quad \\text{(Eq. 2)}\n \nwhere `\\overline{TR(t)}` is the cross-sectional average of `TR(i,t)` across all stocks in quarter `t`.\n\n---\n\nWhich of the following statements about the Trade Ratio (`TR`) and its adjustment are mathematically or conceptually correct?\n", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 239, "Question": "### Background\n\nThe multivariate random walk plus noise model provides a structural framework for analyzing price discovery. A key identification challenge is that the vector of reaction parameters, `α`, is not uniquely identified from the data.\n\n### Data / Model Specification\n\nThe asymmetry in the first-lag autocovariance matrix of price changes, `Γ_1`, is linked to `α` by the moment condition:\n\n  \n\\Gamma_{1}-\\Gamma_{1}^{\\prime}=\\sigma^{2}(\\iota\\alpha^{\\prime}-\\alpha\\iota^{\\prime}) \\quad \\text{(Eq. (1))}\n \n\nWhere `ι` is a vector of ones. The `(i,j)` element of the matrix on the right is `σ^2(α_j - α_i)`. This implies that if `α` is a valid solution, any vector `α̃ = α - wι` (where `w` is an arbitrary scalar) is also a solution, as it produces the same observable differences `α̃_j - α̃_i = α_j - α_i`.\n\n---\n\nWhich of the following are valid methods for achieving full identification of the `α` vector, as discussed in the paper?", "Options": {"A": "The 'Beveridge-Nelson (BN) normalization', which is derived from the reduced-form VMA representation and provides one admissible solution for `α`.", "B": "The 'Watson normalization', which designates one market `i` as a central market and sets its reaction parameter `α_i` to zero.", "C": "Assuming that the reduced-form VMA error covariance matrix, `Σ`, is diagonal.", "D": "Imposing the restriction that the idiosyncratic noise covariance matrix, `Ω`, is diagonal, which provides testable over-identifying restrictions for systems with more than two markets (`N > 2`)."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses the user's comprehensive knowledge of the different identification strategies discussed in the paper to resolve the under-identification of the `α` vector. It is a direct test of factual recall and conceptual understanding. Options A, B, and C describe three distinct and valid identification schemes discussed in the text (Watson, diagonal `Ω`, and BN). Option D is a conceptual distractor; the paper explicitly states that diagonality of the reduced-form covariance matrix `Σ` is 'violated in any empirical application', distinguishing it from the plausible assumption of a diagonal structural noise matrix `Ω`.", "qid": "239", "question": "### Background\n\nThe multivariate random walk plus noise model provides a structural framework for analyzing price discovery. A key identification challenge is that the vector of reaction parameters, `α`, is not uniquely identified from the data.\n\n### Data / Model Specification\n\nThe asymmetry in the first-lag autocovariance matrix of price changes, `Γ_1`, is linked to `α` by the moment condition:\n\n  \n\\Gamma_{1}-\\Gamma_{1}^{\\prime}=\\sigma^{2}(\\iota\\alpha^{\\prime}-\\alpha\\iota^{\\prime}) \\quad \\text{(Eq. (1))}\n \n\nWhere `ι` is a vector of ones. The `(i,j)` element of the matrix on the right is `σ^2(α_j - α_i)`. This implies that if `α` is a valid solution, any vector `α̃ = α - wι` (where `w` is an arbitrary scalar) is also a solution, as it produces the same observable differences `α̃_j - α̃_i = α_j - α_i`.\n\n---\n\nWhich of the following are valid methods for achieving full identification of the `α` vector, as discussed in the paper?", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 245, "Question": "### Background\n\n**Research Question.** How do different reinsurance pricing principles behave under increasing risk aversion, and what are the practical implications for contract negotiation when the underlying loss distribution is heavy-tailed, characteristic of catastrophic events?\n\n**Setting.** We analyze a limited stop-loss reinsurance contract, `I(X) = (X-a)_+ ∧ b`, where the underlying loss `X` follows a heavy-tailed Pareto distribution. This scenario is designed to have the same expected underlying loss (`E[X]=100`) and expected ceded loss (`E[I(X)]=60.65`) as a corresponding case with an exponential distribution, allowing for a direct comparison of the impact of tail risk.\n\n### Data / Model Specification\n\nThe following table presents calculated reinsurance premiums for the specified contract under a Pareto loss distribution. The weight factor `α` represents the reinsurer's risk aversion or share of the loss function weight.\n\n**Table 1. Reinsurance premiums for `I(X)=(X-30)_+ ∧ 287.07` and Pareto Risk `X`**\n\n| α | `~e_{X,I(X),α}` | `e_α(I(X))` | Modified Quantile | Classical Quantile | Risk-Adjusted |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n| 0.82 | 80.95 | 113.22 | 101.55 | 124.22 | 108.14 |\n| 0.85 | 93.06 | 124.45 | 126.55 | 146.41 | 122.13 |\n| 0.88 | 107.88 | 138.00 | 158.53 | 175.48 | 140.44 |\n| 0.90 | 119.95 | 148.90 | 186.03 | 200.89 | 143.53 |\n| 0.92 | 134.61 | 161.97 | 221.36 | 234.16 | 143.53 |\n| 0.95 | 164.57 | 188.16 | 287.07 | 287.07 | 143.53 |\n| 0.98 | 215.43 | 231.00 | 287.07 | 287.07 | 143.53 |\n\n*Note: The policy limit `b` is 287.07. The expected ceded loss `E[I(X)]` is 60.65.*\n\n### Question\n\nBased on the data in Table 1 and the principles discussed in the paper, select all of the following statements that are valid conclusions or interpretations.", "Options": {"A": "The modified expectile premium `~e_{X,I(X),α}` is consistently higher than the classical expectile premium `e_α(I(X))` for all values of `α` shown, reflecting its more accurate modeling of the insurer's total risk exposure.", "B": "For a reinsurer with a high risk aversion (`α ≥ 0.95`), the choice between the Modified Quantile and Classical Quantile principles is immaterial as both result in a premium equal to the policy limit of 287.07.", "C": "The Risk-Adjusted premium principle is less sensitive to increases in the reinsurer's risk aversion `α` in the high-aversion range (e.g., `α > 0.90`) compared to the modified expectile premium `~e_{X,I(X),α}`.", "D": "At `α = 0.90`, the premium loading for the modified expectile premium (`~e_{X,I(X),α} - E[I(X)]`) is less than the premium loading for the classical expectile premium (`e_α(I(X)) - E[I(X)]`)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: Synthesize and interpret data from a table to compare the behavior of different reinsurance premium principles under a heavy-tailed risk distribution. Strategy: Computational Judgment. Distractor Logic: Option C is a Conceptual Opposite; the paper (and table) shows the modified premium is *lower* than the classical one, as stated in Proposition 3.3 and Remark 3.2. Options A, B, and D require direct data extraction and simple calculations (e.g., calculating premium loading) from the table, confirming their validity.", "qid": "245", "question": "### Background\n\n**Research Question.** How do different reinsurance pricing principles behave under increasing risk aversion, and what are the practical implications for contract negotiation when the underlying loss distribution is heavy-tailed, characteristic of catastrophic events?\n\n**Setting.** We analyze a limited stop-loss reinsurance contract, `I(X) = (X-a)_+ ∧ b`, where the underlying loss `X` follows a heavy-tailed Pareto distribution. This scenario is designed to have the same expected underlying loss (`E[X]=100`) and expected ceded loss (`E[I(X)]=60.65`) as a corresponding case with an exponential distribution, allowing for a direct comparison of the impact of tail risk.\n\n### Data / Model Specification\n\nThe following table presents calculated reinsurance premiums for the specified contract under a Pareto loss distribution. The weight factor `α` represents the reinsurer's risk aversion or share of the loss function weight.\n\n**Table 1. Reinsurance premiums for `I(X)=(X-30)_+ ∧ 287.07` and Pareto Risk `X`**\n\n| α | `~e_{X,I(X),α}` | `e_α(I(X))` | Modified Quantile | Classical Quantile | Risk-Adjusted |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n| 0.82 | 80.95 | 113.22 | 101.55 | 124.22 | 108.14 |\n| 0.85 | 93.06 | 124.45 | 126.55 | 146.41 | 122.13 |\n| 0.88 | 107.88 | 138.00 | 158.53 | 175.48 | 140.44 |\n| 0.90 | 119.95 | 148.90 | 186.03 | 200.89 | 143.53 |\n| 0.92 | 134.61 | 161.97 | 221.36 | 234.16 | 143.53 |\n| 0.95 | 164.57 | 188.16 | 287.07 | 287.07 | 143.53 |\n| 0.98 | 215.43 | 231.00 | 287.07 | 287.07 | 143.53 |\n\n*Note: The policy limit `b` is 287.07. The expected ceded loss `E[I(X)]` is 60.65.*\n\n### Question\n\nBased on the data in Table 1 and the principles discussed in the paper, select all of the following statements that are valid conclusions or interpretations.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 270, "Question": "### Background\n\nStandard methods for insurance claims reserving, such as the Payments Per Claim Finalized (PPCF) method, often assume that the expected inflation-adjusted payment per claim is a stable function of the development year. However, this assumption can be violated if the speed at which claims are finalized changes over time. To address this, the paper proposes a 'See-Saw' method that models expected PPCF as a function of both the portfolio's maturity (measured in 'operational time') and the speed of finalization within a given period.\n\n### Data / Model Specification\n\nThe proposed 'See-Saw' model specifies the expected PPCF, denoted `E[P_ij]`, for claims from origin year `i` in development year `j` as a continuous, piecewise linear function of average operational time (`T̄_ij`) and speed of finalization (`F_ij`). The model is estimated via weighted regression.\n\nThe core model is given by:\n\n  \nE[P_{ij}] = \\alpha + \\sum_{k} \\beta_{k} \\bar{T}_{ij}^{(k)} + \\sum_{k} \\gamma_{k} F_{ij}^{(k)} \\quad \\text{(Eq. (1))}\n \n\nwhere `k` indexes distinct intervals of operational time. The coefficients `β_k` represent changes in the slope of the relationship with operational time, and `γ_k` represents the sensitivity of PPCF to finalization speed `F_ij` within the `k`-th operational time interval.\n\n- **Average Operational Time (`T̄_ij`)**: A measure of maturity, ranging from 0 (no claims finalized) to 1 (all claims finalized).\n- **Speed of Finalization (`F_ij`)**: The proportion of total claims finalized in the period `(i, j)`.\n\nEmpirical data and regression results from the paper are provided in Table 1 and Table 2 below.\n\n**Table 1: Operational Times and Speeds of Finalization (Excerpt for 1972 & 1973)**\n\n| Accident Year | | Dev Year 0 | Dev Year 1 | Dev Year 2 | Dev Year 3 | Dev Year 4 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **1972** | (1) `T̄_ij` | 0.035 | 0.190 | 0.370 | 0.565 | 0.805 |\n| | (2) `F_ij` | 0.07 | 0.24 | 0.12 | 0.27 | 0.21 |\n| **1973** | (1) `T̄_ij` | 0.030 | 0.135 | 0.365 | 0.670 | 0.870 |\n| | (2) `F_ij` | 0.06 | 0.15 | 0.31 | 0.30 | 0.10 |\n\n**Table 2: Regression Coefficients for the See-Saw Model**\n\n| Range k | From Op. Time | To Op. Time | βk | γk | α |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1 | 0 | 0.15 | -2191 | -33950 | |\n| 2 | 0.15 | 0.35 | +10410 | -22090 | |\n| 3 | 0.35 | 0.55 | -1194 | -30380 | -188100 |\n| 4 | 0.55 | 0.75 | -16720 | -30870 | |\n| 5 | 0.75 | 0.85 | -45.09 | -35260 | |\n| 6 | 0.85 | 0.95 | +14660 | -38010 | |\n| 7 | 0.95 | 1.00 | +208100 | -145200 | |\n\nBased on the See-Saw model and the provided data, which of the following statements are correct? (Select all that apply)", "Options": {"A": "The consistently negative `γ_k` coefficients in Table 2 imply that, at any given stage of maturity, an increase in the speed of finalization is associated with a decrease in the average payment per finalized claim.", "B": "For the 1973 accident year in development year 3, the average operational time of 0.670 falls into the `k=4` range, which has an associated `γ_4` coefficient of -30870.", "C": "For the 1973 accident year in development year 3, the average operational time of 0.670 falls into the `k=5` operational time range.", "D": "The `β_k` coefficients represent the direct impact of finalization speed on PPCF, while the `γ_k` coefficients capture the effect of operational time."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses interpretation and application of the paper's core model. It uses the 'Atomic Decomposition' strategy to create two independently verifiable correct statements from the original QA's parts 1 and 2. Option A tests the economic interpretation of the key 'see-saw' coefficient (`γ_k`). Option B tests the ability to correctly apply the model's piecewise structure by mapping a data point to the correct parameter set. Distractor C is a 'Conceptual Opposite' error, swapping the meaning of `β_k` and `γ_k`. Distractor D is a 'Lookup Error', testing careful reading of the coefficient table's ranges.", "qid": "270", "question": "### Background\n\nStandard methods for insurance claims reserving, such as the Payments Per Claim Finalized (PPCF) method, often assume that the expected inflation-adjusted payment per claim is a stable function of the development year. However, this assumption can be violated if the speed at which claims are finalized changes over time. To address this, the paper proposes a 'See-Saw' method that models expected PPCF as a function of both the portfolio's maturity (measured in 'operational time') and the speed of finalization within a given period.\n\n### Data / Model Specification\n\nThe proposed 'See-Saw' model specifies the expected PPCF, denoted `E[P_ij]`, for claims from origin year `i` in development year `j` as a continuous, piecewise linear function of average operational time (`T̄_ij`) and speed of finalization (`F_ij`). The model is estimated via weighted regression.\n\nThe core model is given by:\n\n  \nE[P_{ij}] = \\alpha + \\sum_{k} \\beta_{k} \\bar{T}_{ij}^{(k)} + \\sum_{k} \\gamma_{k} F_{ij}^{(k)} \\quad \\text{(Eq. (1))}\n \n\nwhere `k` indexes distinct intervals of operational time. The coefficients `β_k` represent changes in the slope of the relationship with operational time, and `γ_k` represents the sensitivity of PPCF to finalization speed `F_ij` within the `k`-th operational time interval.\n\n- **Average Operational Time (`T̄_ij`)**: A measure of maturity, ranging from 0 (no claims finalized) to 1 (all claims finalized).\n- **Speed of Finalization (`F_ij`)**: The proportion of total claims finalized in the period `(i, j)`.\n\nEmpirical data and regression results from the paper are provided in Table 1 and Table 2 below.\n\n**Table 1: Operational Times and Speeds of Finalization (Excerpt for 1972 & 1973)**\n\n| Accident Year | | Dev Year 0 | Dev Year 1 | Dev Year 2 | Dev Year 3 | Dev Year 4 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **1972** | (1) `T̄_ij` | 0.035 | 0.190 | 0.370 | 0.565 | 0.805 |\n| | (2) `F_ij` | 0.07 | 0.24 | 0.12 | 0.27 | 0.21 |\n| **1973** | (1) `T̄_ij` | 0.030 | 0.135 | 0.365 | 0.670 | 0.870 |\n| | (2) `F_ij` | 0.06 | 0.15 | 0.31 | 0.30 | 0.10 |\n\n**Table 2: Regression Coefficients for the See-Saw Model**\n\n| Range k | From Op. Time | To Op. Time | βk | γk | α |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1 | 0 | 0.15 | -2191 | -33950 | |\n| 2 | 0.15 | 0.35 | +10410 | -22090 | |\n| 3 | 0.35 | 0.55 | -1194 | -30380 | -188100 |\n| 4 | 0.55 | 0.75 | -16720 | -30870 | |\n| 5 | 0.75 | 0.85 | -45.09 | -35260 | |\n| 6 | 0.85 | 0.95 | +14660 | -38010 | |\n| 7 | 0.95 | 1.00 | +208100 | -145200 | |\n\nBased on the See-Saw model and the provided data, which of the following statements are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 261, "Question": "### Background\n\nThis case explores the attitudes of corporate directors towards performance-based remuneration in an insider-dominated governance system. The analysis is based on a survey of 55 directors of listed companies in Taiwan, a market where the \"political risk level... is extremely high.\" Responses are measured on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).\n\n### Data / Model Specification\n\n**Table 1: Factors Affecting Corporate Performance**\n\n| Statement | N | Mean | Median | Mode | SD |\n| :--- | :-: | :-: | :-: | :-: | :-: |\n| (c) The success of companies does not depend on directors' performance. | 55 | 3.20 | 3 | 2 | 1.39 |\n| (d) Directors' remuneration should be linked to share price performance. | 55 | 2.38 | 2 | 2 | 1.19 |\n\n*Note: Scores are on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).* \n\n### Question\n\nAccording to the data in Table 1 and the background context, which of the following statements accurately reflect the directors' views on performance and compensation? Select all that apply.", "Options": {"A": "Directors collectively believe that their own performance is a critical determinant of their company's success.", "B": "Directors strongly oppose linking their personal remuneration to the company's share price performance.", "C": "Directors strongly advocate for stock-based compensation to align their interests with those of shareholders.", "D": "Directors believe that their performance has no significant impact on the success of their companies."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the ability to interpret survey data, including a negatively worded item, to identify a key contradiction in director attitudes. It uses a Reverse-Reasoning strategy. Option A is correct because disagreement with the negative statement (c) (mean 3.20) implies agreement with its positive counterpart. Option B is correct based on the very low mean score for statement (d) (2.38). Distractor C is a conceptual opposite of the correct interpretation of statement (c). Distractor D is a conceptual opposite of the correct interpretation of statement (d). The item forces the candidate to resolve the apparent contradiction that directors accept responsibility for performance but reject financial accountability for it.", "qid": "261", "question": "### Background\n\nThis case explores the attitudes of corporate directors towards performance-based remuneration in an insider-dominated governance system. The analysis is based on a survey of 55 directors of listed companies in Taiwan, a market where the \"political risk level... is extremely high.\" Responses are measured on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).\n\n### Data / Model Specification\n\n**Table 1: Factors Affecting Corporate Performance**\n\n| Statement | N | Mean | Median | Mode | SD |\n| :--- | :-: | :-: | :-: | :-: | :-: |\n| (c) The success of companies does not depend on directors' performance. | 55 | 3.20 | 3 | 2 | 1.39 |\n| (d) Directors' remuneration should be linked to share price performance. | 55 | 2.38 | 2 | 2 | 1.19 |\n\n*Note: Scores are on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).* \n\n### Question\n\nAccording to the data in Table 1 and the background context, which of the following statements accurately reflect the directors' views on performance and compensation? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": ["B"], "error": null}}
{"ID": 243, "Question": "### Background\n\n**Research Question.** How can one construct a robust econometric framework to test the hypothesis that the market prices of macroeconomic risks are not constant, but vary predictably over the business cycle?\n\n**Setting.** The study's empirical core is a Generalized Method of Moments (GMM) framework designed to jointly model and estimate the relationship between asset returns and the conditional moments of industrial production.\n\n### Data / Model Specification\n\n1.  **Variable Construction:** The model uses two key industrial production variables:\n    *   The realized production growth to be priced, `YP_t`, is the 12-month log growth of *seasonally adjusted* industrial production: `YP_t = ln(IP(sa)_t) - ln(IP(sa)_{t-12})`.\n    *   The business cycle proxy, `IPL_t`, is the 12-month log growth of *seasonally unadjusted* industrial production: `IPL_t = ln(IP_t) - ln(IP_{t-12})`. To ensure it is in the investors' information set, it is lagged two months (`IPL_{t-2}`) in the pricing equations.\n\n2.  **GMM System:** The model is estimated as a system of equations:\n\n      \n    e_{1t} = YP_t - \\phi_0 - \\sum_{j} \\phi_j IPL_{t-j} \\quad \\text{(Eq. 1)}\n     \n\n      \n    e_{2t} = r_{mt} - \\eta_{0s} - (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) YP_t - (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) e_{1t}^2 \\quad \\text{(Eq. 2)}\n     \n\n    where `r_{mt}` is the stock return, `e_{1t}` is the forecast error for production growth, and `e_{1t}^2` proxies for production volatility.\n\n### Question\n\nBased on the provided description of the study's methodology, which of the following statements about the data construction and model interpretation are correct?", "Options": {"A": "The two-month lag on the business cycle proxy (`IPL_{t-2}`) is primarily to account for the statistical problem of time-aggregation in flow variables.", "B": "A statistical rejection of the null hypothesis `H₀: λ₁s = 0` would imply that the price of growth risk is state-dependent, supporting the paper's central thesis over standard unconditional models.", "C": "The business cycle proxy `IPL_t` is based on seasonally *unadjusted* data to avoid look-ahead bias, as seasonal adjustment filters often use future information.", "D": "The target variable `YP_t` is based on seasonally *adjusted* data because it provides a noisier, more realistic measure of production growth available to investors."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 10.0). This item assesses understanding of the critical data construction choices and hypothesis-testing logic in the paper. It uses an 'atomic decomposition' strategy. The correct options (A, C) test the core rationale for variable choice and the interpretation of the central hypothesis. Distractor (B) points to a real but secondary issue (time-aggregation) to create a plausible 'True but Irrelevant' distractor for the specific question of the two-month lag. Distractor (D) is a 'Conceptual Opposite', inverting the reason for seasonal adjustment.", "qid": "243", "question": "### Background\n\n**Research Question.** How can one construct a robust econometric framework to test the hypothesis that the market prices of macroeconomic risks are not constant, but vary predictably over the business cycle?\n\n**Setting.** The study's empirical core is a Generalized Method of Moments (GMM) framework designed to jointly model and estimate the relationship between asset returns and the conditional moments of industrial production.\n\n### Data / Model Specification\n\n1.  **Variable Construction:** The model uses two key industrial production variables:\n    *   The realized production growth to be priced, `YP_t`, is the 12-month log growth of *seasonally adjusted* industrial production: `YP_t = ln(IP(sa)_t) - ln(IP(sa)_{t-12})`.\n    *   The business cycle proxy, `IPL_t`, is the 12-month log growth of *seasonally unadjusted* industrial production: `IPL_t = ln(IP_t) - ln(IP_{t-12})`. To ensure it is in the investors' information set, it is lagged two months (`IPL_{t-2}`) in the pricing equations.\n\n2.  **GMM System:** The model is estimated as a system of equations:\n\n      \n    e_{1t} = YP_t - \\phi_0 - \\sum_{j} \\phi_j IPL_{t-j} \\quad \\text{(Eq. 1)}\n     \n\n      \n    e_{2t} = r_{mt} - \\eta_{0s} - (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) YP_t - (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) e_{1t}^2 \\quad \\text{(Eq. 2)}\n     \n\n    where `r_{mt}` is the stock return, `e_{1t}` is the forecast error for production growth, and `e_{1t}^2` proxies for production volatility.\n\n### Question\n\nBased on the provided description of the study's methodology, which of the following statements about the data construction and model interpretation are correct?", "model_evaluation_result": {"model_raw_response": "\"B,C\"", "model_answer": ["B", "C"], "error": null}}
{"ID": 236, "Question": "### Background\n\nA researcher estimates an Unobserved Components (UC) model for the two-minute midquotes of the five most active dealers in Intel stock in 1999. The goal is to obtain precise estimates of each dealer's contribution to price discovery.\n\n### Data / Model Specification\n\nThe UC model is estimated under two different identifying assumptions, yielding the results below.\n\n**Table 1: UC Model Estimation Results for Intel (from paper's Table 5)**\n\n| Panel | Identification Assumption | Dealer | `α` | `IS` | J-stat (df) |\n|:---|:---|:---|---:|---:|:---|\n| B | Approx. Diagonal `Ω` | ISLD | 0.003 | 0.251 | 103.21 (20) |\n| | | INCA | -0.044 | 0.461 | |\n| | | SLKC | -0.079 | 0.030 | |\n| C | Diagonal `Ω` (Imposed) | ISLD | 0.000 | 0.187 | 162.03 (29) |\n| | | INCA | -0.008 | 0.446 | |\n| | | SLKC | 0.087 | 0.155 | |\n\n*Note: Panel B is observationally equivalent to a VMA model. Panel C imposes 9 additional restrictions (the off-diagonal elements of the noise covariance matrix `Ω` are zero). The 5% critical value for a `χ^2` distribution with 9 degrees of freedom is approximately 16.92.*\n\n---\n\nBased on the provided results, select all of the following statements that are valid conclusions.", "Options": {"A": "The large increase in SLKC's information share from Panel B to Panel C is primarily driven by its `α` parameter changing from negative to positive, a shift forced by the GMM estimator to better fit contemporaneous covariances after the diagonality constraint was imposed.", "B": "The test statistic for the validity of the diagonal `Ω` restriction is calculated as `ΔJ = 162.03 - 103.21 = 58.82`.", "C": "The information share (IS) estimates under the diagonal `Ω` assumption (Panel C) provide a clear ranking, with INCA being the most informative dealer, followed by ISLD, and then SLKC.", "D": "A formal specification test fails to reject the null hypothesis that the `Ω` matrix is diagonal at the 5% significance level."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret empirical results, conduct a formal hypothesis test, and understand the underlying mechanics of GMM estimation. It uses a Scenario Application strategy. Option A tests direct interpretation of the table. Option D tests the calculation of the test statistic. Option C assesses a deep understanding of the trade-offs in constrained estimation, as explained in the paper. Option B is a conceptual distractor; the test statistic (58.82) is far greater than the critical value (16.92), leading to a strong rejection of the null hypothesis.", "qid": "236", "question": "### Background\n\nA researcher estimates an Unobserved Components (UC) model for the two-minute midquotes of the five most active dealers in Intel stock in 1999. The goal is to obtain precise estimates of each dealer's contribution to price discovery.\n\n### Data / Model Specification\n\nThe UC model is estimated under two different identifying assumptions, yielding the results below.\n\n**Table 1: UC Model Estimation Results for Intel (from paper's Table 5)**\n\n| Panel | Identification Assumption | Dealer | `α` | `IS` | J-stat (df) |\n|:---|:---|:---|---:|---:|:---|\n| B | Approx. Diagonal `Ω` | ISLD | 0.003 | 0.251 | 103.21 (20) |\n| | | INCA | -0.044 | 0.461 | |\n| | | SLKC | -0.079 | 0.030 | |\n| C | Diagonal `Ω` (Imposed) | ISLD | 0.000 | 0.187 | 162.03 (29) |\n| | | INCA | -0.008 | 0.446 | |\n| | | SLKC | 0.087 | 0.155 | |\n\n*Note: Panel B is observationally equivalent to a VMA model. Panel C imposes 9 additional restrictions (the off-diagonal elements of the noise covariance matrix `Ω` are zero). The 5% critical value for a `χ^2` distribution with 9 degrees of freedom is approximately 16.92.*\n\n---\n\nBased on the provided results, select all of the following statements that are valid conclusions.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 253, "Question": "### Background\n\n**Research Question.** How do different tail-focused premium principles comparatively rank two risks with identical mean and variance but different tail thickness?\n\n**Setting.** A numerical example comparing premiums for a heavy-tailed Pareto risk (`X`) and a less heavy-tailed Weibull risk (`Y`). Both risks have been constructed to have the same mean (1.0) and variance (3.0). The goal is to see if the premium principles can correctly identify the Pareto as the more dangerous risk.\n\n**Variables and Parameters.**\n\n*   `X`: A Pareto-distributed loss (heavy-tailed).\n*   `Y`: A Weibull-distributed loss (thinner-tailed).\n*   `p`: The probability level defining the tail.\n*   `T_{1,1,1}`: Premium principle based on cumulative residual entropy.\n*   `T_{1,2,1}`: Premium principle based on Gini Mean Difference.\n*   `T_{1,0.5,1}`: Premium principle based on Wang's right tail deviation.\n\n---\n\n### Data / Model Specification\n\nTwo risks, `X` (Pareto) and `Y` (Weibull), have `E[X] = E[Y] = 1` and `Var(X) = Var(Y) = 3`. The Pareto distribution is known to have a heavier tail than this specific Weibull distribution.\n\n**Table 1: Premiums for Pareto Risk `X` at p=0.99**\n\n| T<sub>1,1,1</sub>(X,0.99) (Entropy) | T<sub>1,2,1</sub>(X,0.99) (Gini) | T<sub>1,0.5,1</sub>(X,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 18.887 | 14.710 | 39.774 |\n\n**Table 2: Premiums for Weibull Risk `Y` at p=0.99**\n\n| T<sub>1,1,1</sub>(Y,0.99) (Entropy) | T<sub>1,2,1</sub>(Y,0.99) (Gini) | T<sub>1,0.5,1</sub>(Y,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 15.476 | 13.510 | 19.911 |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and **Table 2** for the `p=0.99` case, select all statements that are factually correct.", "Options": {"A": "All three premium principles correctly rank the Pareto risk (X) as more costly than the Weibull risk (Y).", "B": "The Wang-based principle (`T_{1,0.5,1}`) exhibits the greatest sensitivity to tail thickness, as measured by the absolute premium difference between risk X and risk Y.", "C": "The Entropy-based principle (`T_{1,1,1}`) is the most robust to model choice, showing the smallest absolute premium difference between the two risks.", "D": "The Gini-based principle (`T_{1,2,1}`) is the only principle that fails to rank the risks correctly according to their tail thickness."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret empirical results from tables. It uses an Atomic Decomposition strategy, breaking down the complex original question into independently verifiable statements about ranking and sensitivity. Distractors are high-fidelity: (C) is a Conceptual Opposite, claiming an incorrect ranking when the ranking is correct, and (D) is an 'Almost Right' error, incorrectly identifying the Entropy-based principle as most robust when it is the Gini-based one.", "qid": "253", "question": "### Background\n\n**Research Question.** How do different tail-focused premium principles comparatively rank two risks with identical mean and variance but different tail thickness?\n\n**Setting.** A numerical example comparing premiums for a heavy-tailed Pareto risk (`X`) and a less heavy-tailed Weibull risk (`Y`). Both risks have been constructed to have the same mean (1.0) and variance (3.0). The goal is to see if the premium principles can correctly identify the Pareto as the more dangerous risk.\n\n**Variables and Parameters.**\n\n*   `X`: A Pareto-distributed loss (heavy-tailed).\n*   `Y`: A Weibull-distributed loss (thinner-tailed).\n*   `p`: The probability level defining the tail.\n*   `T_{1,1,1}`: Premium principle based on cumulative residual entropy.\n*   `T_{1,2,1}`: Premium principle based on Gini Mean Difference.\n*   `T_{1,0.5,1}`: Premium principle based on Wang's right tail deviation.\n\n---\n\n### Data / Model Specification\n\nTwo risks, `X` (Pareto) and `Y` (Weibull), have `E[X] = E[Y] = 1` and `Var(X) = Var(Y) = 3`. The Pareto distribution is known to have a heavier tail than this specific Weibull distribution.\n\n**Table 1: Premiums for Pareto Risk `X` at p=0.99**\n\n| T<sub>1,1,1</sub>(X,0.99) (Entropy) | T<sub>1,2,1</sub>(X,0.99) (Gini) | T<sub>1,0.5,1</sub>(X,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 18.887 | 14.710 | 39.774 |\n\n**Table 2: Premiums for Weibull Risk `Y` at p=0.99**\n\n| T<sub>1,1,1</sub>(Y,0.99) (Entropy) | T<sub>1,2,1</sub>(Y,0.99) (Gini) | T<sub>1,0.5,1</sub>(Y,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 15.476 | 13.510 | 19.911 |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and **Table 2** for the `p=0.99` case, select all statements that are factually correct.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 272, "Question": "### Background\n\n**Research Question.** This case evaluates the empirical performance of the Empirical Average Cost of Capital (EACC) model against conventional WACC measures by testing their ability to forecast future corporate profitability.\n\n**Setting.** The analysis focuses on McDonald's and its industry (SIC 58) from 2000-Q4 to 2016-Q1. The EACC is estimated using a rolling 40-quarter regression, and these estimates are used to produce one-quarter-ahead forecasts of Net Operating Profit After Taxes (NOPAT).\n\n**Variables & Parameters.**\n- `EACC`: Empirical Average Cost of Capital, estimated using three different measures of `TOTAL CAPITAL`: Book Value, Market Value, and the average of the two (dimensionless).\n- `WACC`: Conventional Weighted Average Cost of Capital, including a textbook CAPM-based estimate and various Ibbotson estimates (dimensionless).\n- `MAE`: Mean Absolute Error of the quarterly NOPAT forecasts, expressed as a percentage of average NOPAT (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe underlying model is the EACC regression:\n\n  \n\\text{NOPAT}_{i,t} = K_i (\\text{TOTAL CAPITAL}_{i,t-1}) + e_{i,t}\n \n\nForecasts are generated as `\\widehat{NOPAT}_{i,t} = \\hat{K}_{i,t-1} \\times \\text{TOTAL CAPITAL}_{i,t-1}`, where `\\hat{K}_{i,t-1}` is estimated using a rolling window of data up to `t-1`. The performance of these forecasts is evaluated using the Mean Absolute Error (MAE).\n\n**Table 1: EACC and WACC Estimates for McDonald's and its Industry (SIC 58)**\n\n| Variable                                      | Mean (%) | MAE (%) |\n| :-------------------------------------------- | :------- | :------ |\n| **McDonald's Estimates (2000 Q4-2016 Q1):**   |          |         |\n| Required EACC - Book Value                    | 15.62    | 15.2    |\n| Required EACC - Market Value                  | 5.81     | 12.8    |\n| Required EACC - Avg. of Book and Market Values| 8.59     | 11.2    |\n| Conventional WACC based on CAPM               | 5.27     | 38.4    |\n| **SIC 58's Estimates (2000 Q4-2012 Q4):**     |          |         |\n| Required EACC - Book Value                    | 17.76    | 17.4    |\n| Required EACC - Market Value                  | 7.52     | 16.0    |\n| Required EACC - Avg. of Book and Market Values| 10.66    | 15.3    |\n| Conventional WACC based on CAPM               | 7.28     | 29.5    |\n| Ibbotson Average CAPM                         | 9.42     | 31.9    |\n\n---\n\nBased on the data in Table 1 and the paper's discussion of measurement biases, select all of the following statements that are valid conclusions or interpretations.", "Options": {"A": "For McDonald's, the EACC estimate derived from the book value of capital (15.62%) is higher than the estimate from the market value of capital (5.81%), which is consistent with the paper's hypothesis that book values may understate the true capital base and market values may overstate it by including future growth opportunities.", "B": "The EACC model using the average of book and market values for McDonald's has a Mean Absolute Error (MAE) of 11.2%, which is more than three times lower than the MAE of the conventional WACC (38.4%), indicating superior forecasting accuracy for NOPAT.", "C": "For the industry SIC 58, the conventional WACC based on CAPM (MAE of 29.5%) provides more accurate NOPAT forecasts than the Ibbotson Average CAPM (MAE of 31.9%).", "D": "The paper suggests that for a privately held company, the EACC model using market values would be the most appropriate choice due to its lower MAE for McDonald's."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret empirical results from a table and connect them to the theoretical arguments presented in the paper. It uses a 'Computational Judgment' strategy, requiring the user to compare values and calculate relative differences (e.g., MAE comparison). Option A tests the understanding of the hypothesized biases. Option B tests the ability to compare forecasting performance quantitatively. Option C requires a careful reading of the industry-level data. The distractor (D) is a 'Conceptual Distractor' (Almost Right) because while the market-value-based EACC has a low MAE, the paper explicitly states that for privately held firms, the book value approach is appropriate since market values are unavailable.", "qid": "272", "question": "### Background\n\n**Research Question.** This case evaluates the empirical performance of the Empirical Average Cost of Capital (EACC) model against conventional WACC measures by testing their ability to forecast future corporate profitability.\n\n**Setting.** The analysis focuses on McDonald's and its industry (SIC 58) from 2000-Q4 to 2016-Q1. The EACC is estimated using a rolling 40-quarter regression, and these estimates are used to produce one-quarter-ahead forecasts of Net Operating Profit After Taxes (NOPAT).\n\n**Variables & Parameters.**\n- `EACC`: Empirical Average Cost of Capital, estimated using three different measures of `TOTAL CAPITAL`: Book Value, Market Value, and the average of the two (dimensionless).\n- `WACC`: Conventional Weighted Average Cost of Capital, including a textbook CAPM-based estimate and various Ibbotson estimates (dimensionless).\n- `MAE`: Mean Absolute Error of the quarterly NOPAT forecasts, expressed as a percentage of average NOPAT (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe underlying model is the EACC regression:\n\n  \n\\text{NOPAT}_{i,t} = K_i (\\text{TOTAL CAPITAL}_{i,t-1}) + e_{i,t}\n \n\nForecasts are generated as `\\widehat{NOPAT}_{i,t} = \\hat{K}_{i,t-1} \\times \\text{TOTAL CAPITAL}_{i,t-1}`, where `\\hat{K}_{i,t-1}` is estimated using a rolling window of data up to `t-1`. The performance of these forecasts is evaluated using the Mean Absolute Error (MAE).\n\n**Table 1: EACC and WACC Estimates for McDonald's and its Industry (SIC 58)**\n\n| Variable                                      | Mean (%) | MAE (%) |\n| :-------------------------------------------- | :------- | :------ |\n| **McDonald's Estimates (2000 Q4-2016 Q1):**   |          |         |\n| Required EACC - Book Value                    | 15.62    | 15.2    |\n| Required EACC - Market Value                  | 5.81     | 12.8    |\n| Required EACC - Avg. of Book and Market Values| 8.59     | 11.2    |\n| Conventional WACC based on CAPM               | 5.27     | 38.4    |\n| **SIC 58's Estimates (2000 Q4-2012 Q4):**     |          |         |\n| Required EACC - Book Value                    | 17.76    | 17.4    |\n| Required EACC - Market Value                  | 7.52     | 16.0    |\n| Required EACC - Avg. of Book and Market Values| 10.66    | 15.3    |\n| Conventional WACC based on CAPM               | 7.28     | 29.5    |\n| Ibbotson Average CAPM                         | 9.42     | 31.9    |\n\n---\n\nBased on the data in Table 1 and the paper's discussion of measurement biases, select all of the following statements that are valid conclusions or interpretations.", "model_evaluation_result": {"model_raw_response": "\"A,B,C\"", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 285, "Question": "### Background\n\nThe paper argues that organizational capabilities like manufacturing flexibility are akin to real options. Traditional Discounted Cash Flow (DCF) analysis, which relies on a single expected path of future cash flows, systematically undervalues such investments because it fails to capture the value of managerial flexibility to adapt to future uncertainty.\n\n### Data / Model Specification\n\nAt `t=0`, a firm can build one of two plants:\n- **Dedicated Plant:** Cost `I_D = 90`. Can only produce Product A.\n- **Flexible Plant:** Cost `I_F = 100`. Can produce Product A or B.\n\nAt `t=1`, the value of producing each product, `V_A` and `V_B`, is revealed. The flexible plant, initially set to make A, can be switched to make B at a cost of `C_S = 10`. The risk-free discount factor is 0.9 (i.e., `1/(1+r)`).\n\n### Question\n\nAccording to a real options framework, which of the following statements are valid critiques or conclusions regarding the valuation of these investments?", "Options": {"A": "If at t=1, `V_A = 120` and `V_B = 125`, the firm should exercise its option and switch production to B.", "B": "An increase in the future uncertainty (volatility) of `V_A` and `V_B` decreases the value of the flexible plant, as it makes planning more difficult.", "C": "The value of the flexibility option embedded in the flexible plant is equivalent to the present value of a European option to exchange asset A for asset B with a strike price of 10.", "D": "A traditional DCF analysis, focused on the expected value of the primary product (A), would favor the dedicated plant over the flexible plant."}, "Answer": ["C", "D"], "pi_justification": "This item uses a 'Scenario Application' strategy to test the paper's core argument about valuing flexibility as a real option. Option A is correct: a static DCF sees the same expected revenue stream but a higher cost for the flexible plant (`100 > 90`), so it would always choose the dedicated plant. Option B is correct: the option's payoff at `t=1` is `max(0, V_B - V_A - C_S)`, which is the definition of an exchange option with a strike price equal to the switching cost (`C_S=10`). Distractor C is a 'Step-Omission Error'; the firm only switches if `V_B - C_S > V_A`. Here, `125 - 10 = 115`, which is less than `V_A = 120`, so the firm should *not* switch. Distractor D is a 'Conceptual Opposite' error; option value increases with volatility, so higher uncertainty makes the flexibility more, not less, valuable.", "qid": "285", "question": "### Background\n\nThe paper argues that organizational capabilities like manufacturing flexibility are akin to real options. Traditional Discounted Cash Flow (DCF) analysis, which relies on a single expected path of future cash flows, systematically undervalues such investments because it fails to capture the value of managerial flexibility to adapt to future uncertainty.\n\n### Data / Model Specification\n\nAt `t=0`, a firm can build one of two plants:\n- **Dedicated Plant:** Cost `I_D = 90`. Can only produce Product A.\n- **Flexible Plant:** Cost `I_F = 100`. Can produce Product A or B.\n\nAt `t=1`, the value of producing each product, `V_A` and `V_B`, is revealed. The flexible plant, initially set to make A, can be switched to make B at a cost of `C_S = 10`. The risk-free discount factor is 0.9 (i.e., `1/(1+r)`).\n\n### Question\n\nAccording to a real options framework, which of the following statements are valid critiques or conclusions regarding the valuation of these investments?", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 46, "Question": "### Background\n\n**Research Question.** How can one construct a measure of the unexpected component of analysts' earnings forecast revisions that properly accounts for known empirical regularities in forecast data, such as systematic optimism and infrequent updating (staleness)?\n\n**Setting / Data-Generating Environment.** The analysis uses monthly median earnings per share (EPS) forecasts. It is known that analysts tend to be overly optimistic in initial forecasts and update them infrequently, leading to serial correlation in forecast revisions.\n\n**Variables & Parameters.**\n- `F_{i,t}`: Median EPS forecast for firm `i` in month `t`.\n- `P_{i}^{*}`: Price per share for firm `i` six months prior to a rating revision.\n- `FR_{i,t}`: Raw forecast revision for firm `i` in month `t`, scaled by price.\n- `E(FR_{i,t})`: Expected forecast revision for firm `i` in month `t`.\n- `AFR_{i,t}`: Abnormal forecast revision for firm `i` in month `t`.\n\n---\n\n### Data / Model Specification\n\nThe raw forecast revision is defined as:\n  \nFR_{i,t} = \\frac{(F_{i,t} - F_{i,t-1})}{P_{i}^{*}} \\times 100 \\quad \\text{(Eq. (1))}\n \n\nTo model the expected forecast revision, `E(FR_{i,t})`, and correct for serial correlation, a six-month distributed lag model is estimated on a separate random sample of firms:\n  \nFR_{i,t} = -0.093 + 0.099 FR_{i,t-1} + 0.098 FR_{i,t-2} + 0.091 FR_{i,t-3} + 0.076 FR_{i,t-4} + 0.055 FR_{i,t-5} + 0.026 FR_{i,t-6} + u_{i,t} \\quad \\text{(Eq. (2))}\n \n\nThe abnormal forecast revision (`AFR`) is the difference between the raw revision and the expectation from **Eq. (2)**:\n  \nAFR_{i,t} = FR_{i,t} - E(FR_{i,t}) \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the methodology for calculating Abnormal Forecast Revisions (AFR), select all correct statements regarding the model's structure and potential limitations.", "Options": {"A": "The positive coefficients on the six lagged forecast revisions (`FR_{i,t-j}`) are included to correct for the systematic optimism bias.", "B": "The model's negative intercept (-0.093) is intended to capture and correct for a systematic optimism bias, where analysts' initial forecasts tend to be too high.", "C": "The purpose of subtracting the expected forecast revision `E(FR_{i,t})` is to isolate the component of the revision that was predictable based on past trends.", "D": "If this 1984-1990 model were applied to a 2008 recession where analysts became systematically pessimistic (true intercept < -0.093), the model would systematically overstate the negative impact of a downgrade."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). Total Suitability Score = 9.5 (Conceptual Clarity = 9/10, Discriminability = 10/10). The question's core tasks—interpreting model coefficients and analyzing a misspecification scenario—are highly structured with convergent answers. The potential for high-fidelity distractors based on common econometric reasoning errors (e.g., sign errors, confusing which term corrects which bias) is extremely high, making it ideal for a multiple-choice format. This item uses a 'premise packaging' strategy to test the rationale behind the model's specification (Option A) and a 'Scenario Application' strategy to test for model misspecification bias (Option B). The distractors are designed to catch common conceptual errors: Option C confuses the roles of different model components, and Option D misstates the fundamental goal of calculating an 'abnormal' measure.", "qid": "46", "question": "### Background\n\n**Research Question.** How can one construct a measure of the unexpected component of analysts' earnings forecast revisions that properly accounts for known empirical regularities in forecast data, such as systematic optimism and infrequent updating (staleness)?\n\n**Setting / Data-Generating Environment.** The analysis uses monthly median earnings per share (EPS) forecasts. It is known that analysts tend to be overly optimistic in initial forecasts and update them infrequently, leading to serial correlation in forecast revisions.\n\n**Variables & Parameters.**\n- `F_{i,t}`: Median EPS forecast for firm `i` in month `t`.\n- `P_{i}^{*}`: Price per share for firm `i` six months prior to a rating revision.\n- `FR_{i,t}`: Raw forecast revision for firm `i` in month `t`, scaled by price.\n- `E(FR_{i,t})`: Expected forecast revision for firm `i` in month `t`.\n- `AFR_{i,t}`: Abnormal forecast revision for firm `i` in month `t`.\n\n---\n\n### Data / Model Specification\n\nThe raw forecast revision is defined as:\n  \nFR_{i,t} = \\frac{(F_{i,t} - F_{i,t-1})}{P_{i}^{*}} \\times 100 \\quad \\text{(Eq. (1))}\n \n\nTo model the expected forecast revision, `E(FR_{i,t})`, and correct for serial correlation, a six-month distributed lag model is estimated on a separate random sample of firms:\n  \nFR_{i,t} = -0.093 + 0.099 FR_{i,t-1} + 0.098 FR_{i,t-2} + 0.091 FR_{i,t-3} + 0.076 FR_{i,t-4} + 0.055 FR_{i,t-5} + 0.026 FR_{i,t-6} + u_{i,t} \\quad \\text{(Eq. (2))}\n \n\nThe abnormal forecast revision (`AFR`) is the difference between the raw revision and the expectation from **Eq. (2)**:\n  \nAFR_{i,t} = FR_{i,t} - E(FR_{i,t}) \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the methodology for calculating Abnormal Forecast Revisions (AFR), select all correct statements regarding the model's structure and potential limitations.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"abnormal forecast revisions model with distributed lag for serial correlation in analysts forecasts\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"analysts forecast revisions model intercept negative optimism bias\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"limitations of distributed lag model in earnings forecast revisions\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Analyzing financial model**\n> ***- The model uses a six-month distributed lag to estimate expected forecast revisions.***\n\n> 🔍 **10 results found**\n\n> [🔗 \"The Complete Guide to Time Series Forecasting Models | by Peter Wainaina | Medium\" Medium](https://medium.com/%2540wainaina.pierre/the-complete-guide-to-time-series-forecasting-models-ef9c8cd40037)\n> [🔗 \"3.3 Residual diagnostics | Forecasting: Principles and Practice (2nd ed)\" Otexts](https://otexts.com/fpp2/residuals.html)\n> [🔗 \"8.1 Stationarity and differencing | Forecasting: Principles and Practice (2nd ed)\" Otexts](https://otexts.com/fpp2/stationarity.html)\n> [🔗 \"Autocorrelation structure of forecast errors from time-series models: Alternative assessments of the causes of post-earnings announcement drift - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0165410100000069)\n> [🔗 \"forecast: Compute forecasts for distributed lag models in dLagM: Time Series Regression Models with Distributed Lag Models\" Rdrr.io](https://rdrr.io/cran/dLagM/man/forecast.html)\n> [🔗 \"Distributed Lags and Dynamic Models | SpringerLink\" Springer](https://link.springer.com/chapter/10.1007/978-3-662-00516-3_6)\n> [🔗 \"Serial Correlation: Definition, How to Determine, and Analysis\" Investopedia](https://www.investopedia.com/terms/s/serial-correlation.asp)\n> [🔗 \"Time Series Forecasting Methods, Techniques & Models | InfluxData\" InfluxData](https://www.influxdata.com/time-series-forecasting-methods/)\n> [🔗 \"Chapter 9 Regression | Time Series Analysis With R\" Nicolarighetti](https://nicolarighetti.github.io/Time-Series-Analysis-With-R/regression.html)\n> [🔗 \"Testing for Serial Correlation in Fixed-effects Panel Models\" Sage Journals](https://journals.sagepub.com/doi/pdf/10.1177/1536867X1801800106)\n\n> 🔍 **10 results found**\n\n> [🔗 \"Analysts’ forecast optimism: The effects of managers’ incentives on analysts’ forecasts - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S2214635022000508)\n> [🔗 \"Financial analysts’ forecast revisions and managers’ reporting behavior - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0165410108000517)\n> [🔗 \"Overreaction and underreaction in analysts' forecasts - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0167268198000924)\n> [🔗 \"Where’s The Recession? – The Failures Of Economic Forecasting\" Forbes](https://www.forbes.com/sites/georgecalhoun/2025/07/30/the-false-promises-of-economic-forecasting-newly-exposed-by-the-tariff-whiplash/)\n> [🔗 \"The Influence of Cognitive Biases and Financial Factors on Forecast Accuracy of Analysts - PMC\" PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC8764190/)\n> [🔗 \"Social learning and analyst behavior - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304405X21002774)\n> [🔗 \"The Predictability of Analyst Forecast Revisions - Michael J. Jung, Jessica H. Keeley, Joshua Ronen, 2019\" Sage Journals](https://journals.sagepub.com/doi/10.1177/0148558X17722710)\n> [🔗 \"The Ability of Analysts' Recommendations to Predict Optimistic and Pessimistic Forecasts - PMC\" PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC3798608/)\n> [🔗 \"Stock market reactions and optimism bias in analysts’ earnings forecasts: An analysis of China's stock markets - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1544612323011947)\n> [🔗 \"Further evidence on optimism and underreaction in analysts' forecasts - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1058330001000258)\n\n> 🔍 **10 results found**\n\n> [🔗 \"Distributed-Lag Models\" Reed College](https://www.reed.edu/economics/parker/312/tschapters/S13_Ch_3.pdf)\n> [🔗 \"Distributed lag - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Distributed_lag)\n> [🔗 \"The predictability of market-wide earnings revisions | Macrosynergy\" Macrosynergy](https://macrosynergy.com/research/the-predictability-of-market-wide-earnings-revisions/)\n> [🔗 \"Lag Model - an overview | ScienceDirect Topics\" ScienceDirect](https://www.sciencedirect.com/topics/economics-econometrics-and-finance/lag-model)\n> [🔗 \"A Real Guide to Lag Models in Econometrics\" Number Analytics](https://www.numberanalytics.com/blog/real-guide-lag-models-econometrics)\n> [🔗 \"Distributed Lags and Dynamic Models | SpringerLink\" Springer](https://link.springer.com/chapter/10.1007/978-3-540-76516-5_6)\n> [🔗 \"Distributed lag non-linear models - PMC\" PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC2998707/)\n> [🔗 \"The Ins and Outs of Lagged Dependent Variables\" Utk](https://nathanjkelly.utk.edu/wp-content/uploads/2017/10/Keele-kelly-2006.pdf)\n> [🔗 \"Stata Basics: Distributed Lag (DL) and Autoregressive (AR ...\" YouTube](https://www.youtube.com/watch?v=xK3uUx61ueM)\n> [🔗 \"Best Top Finite Lag Tips for Econometric Models\" Number Analytics](https://www.numberanalytics.com/blog/finite-lag-tips-econometric-models)\n\n> ***- Let's check how the model handles systematic optimism in analysts' forecasts.***\n\n> 🔍 **Searching for:** \"abnormal forecast revisions model with distributed lag for serial correlation in analysts forecasts\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"analysts forecast revisions model intercept negative optimism bias\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"limitations of distributed lag model in earnings forecast revisions\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Found 10 search results**\n\n🔗 **[The Complete Guide to Time Series Forecasting Models | by Peter Wainaina | Medium](https://medium.com/%2540wainaina.pierre/the-complete-guide-to-time-series-forecasting-models-ef9c8cd40037)**\n   📄 The Complete Guide to Time Series Forecasting Models Buckle up, because this is a very detailed overview of Time Series Forecasting Models and by the ...\n   🌐 *Medium*\n\n🔗 **[3.3 Residual diagnostics | Forecasting: Principles and Practice (2nd ed)](https://otexts.com/fpp2/residuals.html)**\n   📄 Fixing the correlation problem is harder, and we will not address it until Chapter 9. In addition to these essential properties, it is useful (but not...\n   🌐 *Otexts*\n\n🔗 **[8.1 Stationarity and differencing | Forecasting: Principles and Practice (2nd ed)](https://otexts.com/fpp2/stationarity.html)**\n   📄 In the former case, we could have decided that the data were not sufficiently stationary and taken an extra round of differencing. Some formal tests f...\n   🌐 *Otexts*\n\n🔗 **[Autocorrelation structure of forecast errors from time-series models: Alternative assessments of the causes of post-earnings announcement drift - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0165410100000069)**\n   📄 For each sample firm, we compute the probability of a seasonal unit root at lag four using the procedure outlined in Dickey et al. (1984).1 This proba...\n   🌐 *ScienceDirect*\n\n🔗 **[forecast: Compute forecasts for distributed lag models in dLagM: Time Series Regression Models with Distributed Lag Models](https://rdrr.io/cran/dLagM/man/forecast.html)**\n   📄 View source: R/<strong>forecast</strong>.R · Computes <strong>forecasts</strong> <strong>for</strong> the finite <strong>distributed</strong> <strong>...\n   🌐 *Rdrr.io*\n\n🔗 **[Distributed Lags and Dynamic Models | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-662-00516-3_6)**\n   📄 Durbin, J. (1970), “Testing for Serial Correlation in Least Squares Regression when Some of the Regressors are Lagged Dependent Variables,” Econometri...\n   🌐 *Springer*\n\n🔗 **[Serial Correlation: Definition, How to Determine, and Analysis](https://www.investopedia.com/terms/s/serial-correlation.asp)**\n   📄 These financial trading analysts use technical analysis and other statistical inferences to analyze and predict the stock market. These modelers attem...\n   🌐 *Investopedia*\n\n🔗 **[Time Series Forecasting Methods, Techniques & Models | InfluxData](https://www.influxdata.com/time-series-forecasting-methods/)**\n   📄 Prediction problems involving a time component require time series <strong>forecasting</strong> and use <strong>models</strong> fit on historical data...\n   🌐 *InfluxData*\n\n🔗 **[Chapter 9 Regression | Time Series Analysis With R](https://nicolarighetti.github.io/Time-Series-Analysis-With-R/regression.html)**\n   📄 ## ## Augmented Dickey-Fuller Test ## ## data: diff(y_I1) ## Dickey-Fuller = -7.8913, Lag order = 7, p-value = 0.01 ## alternative hypothesis: station...\n   🌐 *Nicolarighetti*\n\n🔗 **[Testing for Serial Correlation in Fixed-effects Panel Models](https://journals.sagepub.com/doi/pdf/10.1177/1536867X1801800106)**\n   📄 Subscription and open access journals from Sage, the world&#x27;s leading independent academic publisher.\n   🌐 *Sage Journals*\n\n\n> 🔍 **Found 10 search results**\n\n🔗 **[Analysts’ forecast optimism: The effects of managers’ incentives on analysts’ forecasts - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S2214635022000508)**\n   📄 Our primary results rely on the most recent forecast before the earnings announcement date (i.e., the latest forecast) in order to calculate forecast ...\n   🌐 *ScienceDirect*\n\n🔗 **[Financial analysts’ forecast revisions and managers’ reporting behavior - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0165410108000517)**\n   📄 These first two predictions about the analyst's equilibrium forecasts show that non-zero average forecast errors and negative average forecast revisio...\n   🌐 *ScienceDirect*\n\n🔗 **[Overreaction and underreaction in analysts' forecasts - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0167268198000924)**\n   📄 We present a model for the concurrent influence of these heuristics on forecast errors, and examine three predictions of this model: (1) that there is...\n   🌐 *ScienceDirect*\n\n🔗 **[Where’s The Recession? – The Failures Of Economic Forecasting](https://www.forbes.com/sites/georgecalhoun/2025/07/30/the-false-promises-of-economic-forecasting-newly-exposed-by-the-tariff-whiplash/)**\n   📄 The Wall Street Journal’s survey of around 60 professional economists found them basking in optimism.\n   🌐 *Forbes*\n\n🔗 **[The Influence of Cognitive Biases and Financial Factors on Forecast Accuracy of Analysts - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC8764190/)**\n   📄 The results indicated that, among cognitive biases, optimism had a negative relationship with forecasting accuracy while anchoring bias had a positive...\n   🌐 *PubMed Central*\n\n🔗 **[Social learning and analyst behavior - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304405X21002774)**\n   📄 For example, if the forecasts of other analysts on other firms have been systematically higher (lower) than actual earnings, an analyst may update his...\n   🌐 *ScienceDirect*\n\n🔗 **[The Predictability of Analyst Forecast Revisions - Michael J. Jung, Jessica H. Keeley, Joshua Ronen, 2019](https://journals.sagepub.com/doi/10.1177/0148558X17722710)**\n   📄 Consistent with investors’ fixation on unscaled EPS, the strategy earns positive abnormal returns using unscaled EPS revisions but not when revisions ...\n   🌐 *Sage Journals*\n\n🔗 **[The Ability of Analysts' Recommendations to Predict Optimistic and Pessimistic Forecasts - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC3798608/)**\n   📄 Dutta and Gigler [2] suggest that companies have strong incentives to avoid negative Forecast Errors (FEs) or/and generate positive FEs. They propose ...\n   🌐 *PubMed Central*\n\n🔗 **[Stock market reactions and optimism bias in analysts’ earnings forecasts: An analysis of China's stock markets - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1544612323011947)**\n   📄 Such a conceptualization relates to Lehmer et al.'s (2022) conception of the informativeness of future analysts’ reports of changes in volume in respo...\n   🌐 *ScienceDirect*\n\n🔗 **[Further evidence on optimism and underreaction in analysts' forecasts - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1058330001000258)**\n   📄 Thus, our tests are joint tests of analysts' ability to distinguish between the bankrupt and turnaround firms and the argument that optimism will be r...\n   🌐 *ScienceDirect*\n\n\n> 🔍 **Found 10 search results**\n\n🔗 **[Distributed-Lag Models](https://www.reed.edu/economics/parker/312/tschapters/S13_Ch_3.pdf)**\n   📄 3203 Southeast Woodstock Boulevard Portland, Oregon 97202-8199 503-771-1112 · Reed College prohibits unlawful discrimination on the basis <strong>of</...\n   🌐 *Reed College*\n\n🔗 **[Distributed lag - Wikipedia](https://en.wikipedia.org/wiki/Distributed_lag)**\n   📄 In statistics and econometrics, a distributed lag model is a model for time series data in which a regression equation is used to predict current valu...\n   🌐 *Wikipedia*\n\n🔗 **[The predictability of market-wide earnings revisions | Macrosynergy](https://macrosynergy.com/research/the-predictability-of-market-wide-earnings-revisions/)**\n   📄 Forward earnings yields are a key metric for the valuation of an equity market. Helpfully, I/B/E/S and DataStream publish forward earnings forecasts o...\n   🌐 *Macrosynergy*\n\n🔗 **[Lag Model - an overview | ScienceDirect Topics](https://www.sciencedirect.com/topics/economics-econometrics-and-finance/lag-model)**\n   📄 It is now standard to estimate this kind of model with maximum likelihood. An important limitation to this approach is that the model is identified as...\n   🌐 *ScienceDirect*\n\n🔗 **[A Real Guide to Lag Models in Econometrics](https://www.numberanalytics.com/blog/real-guide-lag-models-econometrics)**\n   📄 Whether you are evaluating policy effects, forecasting economic trends, or analyzing time series data, understanding distributed lag models can offer ...\n   🌐 *Number Analytics*\n\n🔗 **[Distributed Lags and Dynamic Models | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-540-76516-5_6)**\n   📄 In turn, one’s life time income may be guessed by looking at past as well as current earnings. In other words, the regression relationship would look ...\n   🌐 *Springer*\n\n🔗 **[Distributed lag non-linear models - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC2998707/)**\n   📄 Environmental stressors often show effects that are delayed in time, requiring the use of statistical models that are flexible enough to describe the ...\n   🌐 *PubMed Central*\n\n🔗 **[The Ins and Outs of Lagged Dependent Variables](https://nathanjkelly.utk.edu/wp-content/uploads/2017/10/Keele-kelly-2006.pdf)**\n   📄 Professor <strong>of</strong> Political Science studying inequality, political institutions, American political economy, and public policy\n   🌐 *Utk*\n\n🔗 **[Stata Basics: Distributed Lag (DL) and Autoregressive (AR ...](https://www.youtube.com/watch?v=xK3uUx61ueM)**\n   📄 Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.\n   🌐 *YouTube*\n\n🔗 **[Best Top Finite Lag Tips for Econometric Models](https://www.numberanalytics.com/blog/finite-lag-tips-econometric-models)**\n   📄 A recent study by The National Bureau of Economic Research (NBER) showcases the integration of machine learning with lag models, providing improved fo...\n   🌐 *Number Analytics*\n\n\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 281, "Question": "### Background\n\n**Research Question.** The study seeks to explain why the aggregate exchange rate pass-through to investment good prices is modest, despite being very high in certain key industrial sectors.\n\n**Setting / Data-Generating Environment.** The analysis uses panel data for 22 industrial sectors. The paper's key insight is that the aggregate effect is a composite of highly heterogeneous sector-level effects.\n\n---\n\n### Data / Model Specification\n\nThe paper reports the following statistically significant pass-through elasticities from its baseline model:\n\n**Table 1: Selected Pass-Through Elasticities (`\\hat{\\alpha}_{i1}`)**\n\n| Industry                               | Pass-Through Elasticity |\n| :------------------------------------- | :---------------------- |\n| Total Industries                       | 0.1225                  |\n| Transport, storage and communication   | 0.3151                  |\n| Finance, insurance, real estate, etc.  | 0.0242 (insignificant)  |\n\nAdditionally, the paper notes that the 'Finance' sector has a very large share of total investment in the economy (34.1%).\n\n---\n\n### Question\n\nGiven these findings, which of the following conclusions or explanations are valid?", "Options": {"A": "The aggregate pass-through for 'Total Industries' can be formally expressed as an investment-weighted average of the individual sector elasticities.", "B": "The 'Transport' sector's high pass-through is the primary driver of the aggregate 'Total Industries' result.", "C": "A policy-induced currency depreciation is unlikely to act as an industrial strategy because the aggregate effect on investment costs is small (1.23% for a 10% depreciation).", "D": "The low aggregate pass-through is heavily influenced by the 'Finance' sector, which combines a very low pass-through elasticity with a very large weight in total economy-wide investment."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize micro-level results to explain a macro-level finding, a key reasoning step in the original QA problem. It uses a 'Reverse-Reasoning' strategy. Option A correctly states the aggregation principle. Option B correctly applies this principle to explain the specific empirical puzzle presented in the stem. Distractor C is a 'Conceptual Opposite', as the Transport sector's high pass-through is offset by other sectors, it does not drive the low aggregate result. Distractor D contradicts the paper's main policy conclusion; the small aggregate effect masks large, heterogeneous sectoral effects, which is precisely why it *does* act as an industrial strategy.", "qid": "281", "question": "### Background\n\n**Research Question.** The study seeks to explain why the aggregate exchange rate pass-through to investment good prices is modest, despite being very high in certain key industrial sectors.\n\n**Setting / Data-Generating Environment.** The analysis uses panel data for 22 industrial sectors. The paper's key insight is that the aggregate effect is a composite of highly heterogeneous sector-level effects.\n\n---\n\n### Data / Model Specification\n\nThe paper reports the following statistically significant pass-through elasticities from its baseline model:\n\n**Table 1: Selected Pass-Through Elasticities (`\\hat{\\alpha}_{i1}`)**\n\n| Industry                               | Pass-Through Elasticity |\n| :------------------------------------- | :---------------------- |\n| Total Industries                       | 0.1225                  |\n| Transport, storage and communication   | 0.3151                  |\n| Finance, insurance, real estate, etc.  | 0.0242 (insignificant)  |\n\nAdditionally, the paper notes that the 'Finance' sector has a very large share of total investment in the economy (34.1%).\n\n---\n\n### Question\n\nGiven these findings, which of the following conclusions or explanations are valid?", "model_evaluation_result": {"model_raw_response": "\"A,C,D\"", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 276, "Question": "### Background\n\n**Research Question.** What is the joint and incremental value relevance of book value (a stock measure from the balance sheet) and earnings (a flow measure from the income statement) for valuing international banks?\n\n**Setting / Data-Generating Environment.** The analysis uses a pooled sample of 28,786 quarterly observations for 813 international banking institutions during the 1993–2004 period.\n\n### Data / Model Specification\n\nThree pooled OLS regressions are estimated to assess the value relevance of book value per share (`BV`) and earnings per share (`EPS`) for explaining market value per share (`MV`):\n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + e_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} EPS_i + e_{i} \\quad \\text{(Eq. 2)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + \\beta_{2} EPS_i + e_{i} \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Pooled OLS Regressions of Value Relevance**\n\n| Variable          | Model 1 (Eq. 1) | Model 2 (Eq. 2) | Model 3 (Eq. 3) |\n| :---------------- | :-------------- | :-------------- | :-------------- |\n| Intercept         | 18.452***       | 17.160***       | 14.340***       |\n| Book Value (`β₁`) | 0.559***        |                 | 0.462***        |\n| Earnings (`β₂`)   |                 | 0.376***        | 0.241***        |\n| R-squared         | 0.291           | 0.250           | 0.357           |\n\n*Note: *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the regression results in Table 1, select all of the following statements that represent a correct interpretation of the models.", "Options": {"A": "The statistical significance of the Earnings coefficient (0.241) in Model 3 demonstrates that earnings provide incremental value-relevant information beyond what is already contained in book value.", "B": "Because Model 1 (R²=0.291) has a higher R-squared than Model 2 (R²=0.250), book value is a sufficient statistic for firm value, and earnings are redundant.", "C": "In Model 3, the coefficient on Book Value (0.462) is larger than on Earnings (0.241), which proves that the balance sheet is more important than the income statement for valuation.", "D": "The decrease in the Book Value coefficient from 0.559 in Model 1 to 0.462 in Model 3 is consistent with positive omitted variable bias in Model 1, where Book Value was likely capturing some of the positive effect of the omitted Earnings variable."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses core econometric interpretation skills: understanding incremental information content and diagnosing omitted variable bias by comparing nested models. It uses an Atomic Decomposition strategy. Correct options test the concept of incremental explanatory power (A) and the specific mechanics of OVB (B). Distractor C misinterprets R-squared to incorrectly conclude redundancy, while D makes a common error of comparing coefficients across variables with different scales to infer 'importance'.", "qid": "276", "question": "### Background\n\n**Research Question.** What is the joint and incremental value relevance of book value (a stock measure from the balance sheet) and earnings (a flow measure from the income statement) for valuing international banks?\n\n**Setting / Data-Generating Environment.** The analysis uses a pooled sample of 28,786 quarterly observations for 813 international banking institutions during the 1993–2004 period.\n\n### Data / Model Specification\n\nThree pooled OLS regressions are estimated to assess the value relevance of book value per share (`BV`) and earnings per share (`EPS`) for explaining market value per share (`MV`):\n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + e_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} EPS_i + e_{i} \\quad \\text{(Eq. 2)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + \\beta_{2} EPS_i + e_{i} \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Pooled OLS Regressions of Value Relevance**\n\n| Variable          | Model 1 (Eq. 1) | Model 2 (Eq. 2) | Model 3 (Eq. 3) |\n| :---------------- | :-------------- | :-------------- | :-------------- |\n| Intercept         | 18.452***       | 17.160***       | 14.340***       |\n| Book Value (`β₁`) | 0.559***        |                 | 0.462***        |\n| Earnings (`β₂`)   |                 | 0.376***        | 0.241***        |\n| R-squared         | 0.291           | 0.250           | 0.357           |\n\n*Note: *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the regression results in Table 1, select all of the following statements that represent a correct interpretation of the models.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 274, "Question": "### Background\n\n**Research Question.** What institutional factors explain the cross-country variation in the value relevance of bank financial statements, and can these empirical relationships be interpreted causally?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage regression approach using a sample of 813 banks from 38 countries over the period 1993-2004. In the first stage, bank-specific value relevance is estimated. In the second stage, a single cross-sectional regression is run to explain the variation in these estimates.\n\n### Data / Model Specification\n\nIn the second stage, the Extent of Value Relevance (`EVR`), proxied by the coefficient on book value from the first-stage regressions, is modeled as a function of country-level and bank-specific factors:\n\n  \n\\mathrm{EVR}_{i} = a_{0} + b_{1} \\text{Transparency}_i + b_{2} \\text{Legal Environment}_i + b_{3} \\text{Accounting Cluster}_i + ... + \\varepsilon_{i}\n \n\nKey independent variables are defined as:\n*   `EVR`: The bank-specific coefficient on book value from a first-stage regression of market value on book value.\n*   `Legal Environment`: An indicator variable equal to 1 for common law systems and 0 for code law systems.\n*   `Transparency`: A country-level index (CIFAR) measuring the extent of mandatory financial disclosure.\n*   `Accounting Cluster`: An indicator variable equal to 1 for the British-American accounting cluster and 0 otherwise.\n\n**Table 1: Second-Stage Regression Results (Dependent Variable = EVR)**\n\n| Correlated Factor     | Coefficient | P-value |\n| :-------------------- | :---------- | :------ |\n| Transparency          | 0.045       | 0.001   |\n| Corporate environment | 0.042       | 0.011   |\n| Legal environment     | 0.041       | 0.037   |\n| Accounting cluster    | 0.114       | 0.001   |\n\n*Source: Adapted from Table 5, Panel A of the paper.*\n\n**Table 2: Correlation Matrix Snippet**\n\n|                     | Legal Env. | Transparency | Acct. Cluster |\n| :------------------ | :--------- | :----------- | :------------ |\n| **Legal Env.**      | 1.00       |              |               |\n| **Transparency**    | 0.40*      | 1.00         |               |\n| **Acct. Cluster**   | 0.48*      | 0.57*        | 1.00          |\n\n*Source: Adapted from Table 4 of the paper. * denotes significant correlation.*\n\n### Question\n\nBased on the provided regression and correlation results, select all of the following statements that are valid interpretations or policy conclusions.", "Options": {"A": "Since both `Legal Environment` and `Transparency` are statistically significant, a regulator should prioritize changing the legal system as it is a more fundamental institutional feature.", "B": "Holding other factors constant, banks in common law countries have a book value coefficient that is, on average, 0.041 higher than banks in code law countries, a difference that is statistically significant at the 5% level.", "C": "The coefficient on `Accounting Cluster` (0.114) is more than double that of `Legal Environment` (0.041), implying that accounting standards have a more than twice as large causal effect on value relevance.", "D": "For a regulator aiming to improve value relevance, focusing on increasing `Transparency` is a more practical policy lever than changing the `Legal Environment` because disclosure requirements are more readily modifiable than a country's entire legal tradition."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret regression coefficients in a policy context. It uses an Atomic Decomposition strategy, breaking down the original QA's interpretation and policy application tasks into distinct, verifiable statements. Correct options require accurate reading of coefficient magnitude/significance (A) and sound reasoning about policy implementability (B). Distractor C tempts a common error of equating coefficient magnitude with causal importance, while D presents a conceptually opposite and impractical policy conclusion.", "qid": "274", "question": "### Background\n\n**Research Question.** What institutional factors explain the cross-country variation in the value relevance of bank financial statements, and can these empirical relationships be interpreted causally?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage regression approach using a sample of 813 banks from 38 countries over the period 1993-2004. In the first stage, bank-specific value relevance is estimated. In the second stage, a single cross-sectional regression is run to explain the variation in these estimates.\n\n### Data / Model Specification\n\nIn the second stage, the Extent of Value Relevance (`EVR`), proxied by the coefficient on book value from the first-stage regressions, is modeled as a function of country-level and bank-specific factors:\n\n  \n\\mathrm{EVR}_{i} = a_{0} + b_{1} \\text{Transparency}_i + b_{2} \\text{Legal Environment}_i + b_{3} \\text{Accounting Cluster}_i + ... + \\varepsilon_{i}\n \n\nKey independent variables are defined as:\n*   `EVR`: The bank-specific coefficient on book value from a first-stage regression of market value on book value.\n*   `Legal Environment`: An indicator variable equal to 1 for common law systems and 0 for code law systems.\n*   `Transparency`: A country-level index (CIFAR) measuring the extent of mandatory financial disclosure.\n*   `Accounting Cluster`: An indicator variable equal to 1 for the British-American accounting cluster and 0 otherwise.\n\n**Table 1: Second-Stage Regression Results (Dependent Variable = EVR)**\n\n| Correlated Factor     | Coefficient | P-value |\n| :-------------------- | :---------- | :------ |\n| Transparency          | 0.045       | 0.001   |\n| Corporate environment | 0.042       | 0.011   |\n| Legal environment     | 0.041       | 0.037   |\n| Accounting cluster    | 0.114       | 0.001   |\n\n*Source: Adapted from Table 5, Panel A of the paper.*\n\n**Table 2: Correlation Matrix Snippet**\n\n|                     | Legal Env. | Transparency | Acct. Cluster |\n| :------------------ | :--------- | :----------- | :------------ |\n| **Legal Env.**      | 1.00       |              |               |\n| **Transparency**    | 0.40*      | 1.00         |               |\n| **Acct. Cluster**   | 0.48*      | 0.57*        | 1.00          |\n\n*Source: Adapted from Table 4 of the paper. * denotes significant correlation.*\n\n### Question\n\nBased on the provided regression and correlation results, select all of the following statements that are valid interpretations or policy conclusions.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 284, "Question": "### Background\n\nThe paper posits that an incumbent firm's willingness to introduce a new product that cannibalizes an existing one can be a powerful strategy to deter market entry. However, this threat is often 'time-inconsistent' and not credible unless the firm makes a prior, irreversible commitment that changes its own incentives.\n\n### Data / Model Specification\n\nAn Incumbent (I) faces a potential Entrant (E). The payoffs are `(Π_I, Π_E)`.\n- If E Stays Out: `(100, 0)`\n- If E Enters & I Accommodates: `(60, 20)`\n- If E Enters & I Fights (cannibalizes): `(40, -10)`\n\nBefore the game begins, the Incumbent can make an irreversible investment `K` in the 'capacity to cannibalize'. This investment changes the Incumbent's payoff from fighting from 40 to `40 + B`, where `B` is a 'boldness' payoff from the investment.\n\n### Question\n\nUsing backward induction and the concept of Subgame Perfect Nash Equilibrium (SPNE), select all of the following statements that are correct.", "Options": {"A": "In the absence of the commitment investment (`K`), the SPNE outcome is that the Entrant enters and the Incumbent accommodates.", "B": "The maximum investment `K` the Incumbent would be willing to make to achieve credibility is 40.", "C": "For the threat to 'Fight' to become credible, the boldness payoff `B` must be greater than 20.", "D": "If the Incumbent makes the threat to fight credible, its net gain compared to the no-investment outcome is 60."}, "Answer": ["A", "B", "C"], "pi_justification": "This item uses a 'Computational Judgment' strategy, applying game theory to formalize the paper's argument about credible commitment and cannibalization. Option A is correct: without investment, if E enters, I compares Accommodate (60) vs. Fight (40) and chooses Accommodate. E anticipates this, compares Enter (20) vs. Stay Out (0), and chooses Enter. Option B is correct: for the threat to be credible, the payoff from fighting must exceed accommodating: `40 + B > 60`, which implies `B > 20`. Option D is correct: the value of successful deterrence is the difference between the monopoly payoff (100) and the duopoly payoff (60), which is 40. The firm would pay up to this amount for the investment `K`. Distractor C is a 'Formula Misuse' error; the net gain is 100 - 60 = 40, not 60.", "qid": "284", "question": "### Background\n\nThe paper posits that an incumbent firm's willingness to introduce a new product that cannibalizes an existing one can be a powerful strategy to deter market entry. However, this threat is often 'time-inconsistent' and not credible unless the firm makes a prior, irreversible commitment that changes its own incentives.\n\n### Data / Model Specification\n\nAn Incumbent (I) faces a potential Entrant (E). The payoffs are `(Π_I, Π_E)`.\n- If E Stays Out: `(100, 0)`\n- If E Enters & I Accommodates: `(60, 20)`\n- If E Enters & I Fights (cannibalizes): `(40, -10)`\n\nBefore the game begins, the Incumbent can make an irreversible investment `K` in the 'capacity to cannibalize'. This investment changes the Incumbent's payoff from fighting from 40 to `40 + B`, where `B` is a 'boldness' payoff from the investment.\n\n### Question\n\nUsing backward induction and the concept of Subgame Perfect Nash Equilibrium (SPNE), select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 279, "Question": "### Background\n\n**Research Question.** The study investigates whether the effect of exchange rate changes on investment good prices is linear or if it depends on the economic context, specifically the degree of currency misalignment.\n\n**Variables & Parameters.**\n- `\\Delta\\mathrm{PI}_{i m t}`: Percentage change in the investment price index.\n- `(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}})`: Percentage change in the domestic currency cost of imported investment goods.\n- `V_{mt}`: The currency misalignment for country `m` at time `t`, defined as the ratio of the actual exchange rate to its Purchasing Power Parity (PPP) level. A value `V > 1` indicates the domestic currency is undervalued.\n\n---\n\n### Data / Model Specification\n\nTo test for non-linearities, the study specifies a model where the pass-through effect is interacted with the currency misalignment `V_{mt}`. The estimated equation for 'Total Industries' is given below.\n\n  \n\\Delta\\mathrm{PI}_{i m t}=\\delta_{i0}+ 0.1132 \\times \\left[V_{mt} \\times \\left(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}}\\right)\\right]+\\text{controls} + \\text{Fixed Effects} + \\nu_{i m t} \\quad \\text{(Eq. (1))}\n \n\nThe estimated coefficient on the interaction term, 0.1132, has a t-statistic of 4.97.\n\n---\n\n### Question\n\nBased on the non-linear model specification and results in **Eq. (1)**, which of the following conclusions are supported by the evidence?", "Options": {"A": "The pass-through elasticity is greater when the domestic currency is undervalued (e.g., V=1.2) than when it is at its PPP value (V=1.0).", "B": "The pass-through elasticity for 'Total Industries' is constant and equal to 0.1132, regardless of the currency's valuation.", "C": "If the currency is 20% undervalued (`V=1.2`), a 10% currency depreciation is predicted to increase investment good prices by approximately 1.36%.", "D": "A currency depreciation has a smaller impact on investment good prices when the currency is already undervalued, as firms are less sensitive to further price changes."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret and apply a non-linear interaction model, a core skill tested in the original QA problem. It uses a 'Computational Judgment' and 'Atomic Decomposition' strategy. Option A tests the conceptual interpretation of the positive interaction coefficient. Option B requires a precise calculation based on the model. Distractor C is a 'Conceptual Opposite' error. Distractor D is a 'Step-Omission Error', as it incorrectly interprets the coefficient from an interaction model as a constant marginal effect.", "qid": "279", "question": "### Background\n\n**Research Question.** The study investigates whether the effect of exchange rate changes on investment good prices is linear or if it depends on the economic context, specifically the degree of currency misalignment.\n\n**Variables & Parameters.**\n- `\\Delta\\mathrm{PI}_{i m t}`: Percentage change in the investment price index.\n- `(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}})`: Percentage change in the domestic currency cost of imported investment goods.\n- `V_{mt}`: The currency misalignment for country `m` at time `t`, defined as the ratio of the actual exchange rate to its Purchasing Power Parity (PPP) level. A value `V > 1` indicates the domestic currency is undervalued.\n\n---\n\n### Data / Model Specification\n\nTo test for non-linearities, the study specifies a model where the pass-through effect is interacted with the currency misalignment `V_{mt}`. The estimated equation for 'Total Industries' is given below.\n\n  \n\\Delta\\mathrm{PI}_{i m t}=\\delta_{i0}+ 0.1132 \\times \\left[V_{mt} \\times \\left(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}}\\right)\\right]+\\text{controls} + \\text{Fixed Effects} + \\nu_{i m t} \\quad \\text{(Eq. (1))}\n \n\nThe estimated coefficient on the interaction term, 0.1132, has a t-statistic of 4.97.\n\n---\n\n### Question\n\nBased on the non-linear model specification and results in **Eq. (1)**, which of the following conclusions are supported by the evidence?", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 289, "Question": "### Background\n\n**Research Question.** Do insiders trade on advance knowledge of negative corporate events, such as an involuntary exchange delisting?\n\n**Setting.** The study analyzes insider trading for firms that are delisted from an exchange, an event associated with significant negative stock returns. Delisting is typically involuntary. A key challenge is distinguishing trading based on specific delisting information from trading based on the general poor performance that leads to delisting.\n\n**Variables & Parameters.**\n- `MANP_t`: Mean abnormal number of net purchasers in event month `t`.\n\n---\n\n### Data / Model Specification\n\nThe table below shows the `MANP_t` for firms approaching delisting. A negative value indicates net selling activity above historical norms.\n\n**Table 1. Mean Abnormal Net Insider Purchasing Activity (Delisting Firms)**\n\n| Event Month(s) | `MANP_t` | t-statistic |\n|:---------------|:-----------|:------------|\n| -9             | -0.460     | -2.616***   |\n| -7             | -0.358     | -1.892*     |\n| -5             | -0.403     | -2.116**    |\n| -3             | -0.510     | -2.439**    |\n| **[-12, -1]**  | **-3.446** | **-5.214*** |\n\n*Note: Table is an abridged version of the original. ***p<0.01, **p<0.05, *p<0.10.*\n\n---\n\nBased on the data and the context of involuntary delistings, which of the following statements are valid conclusions or arguments? Select all that apply.", "Options": {"A": "The data show a persistent pattern of statistically significant abnormal net selling by insiders in the months leading up to the delisting event.", "B": "The involuntary nature of delisting makes it less likely that insider selling is merely a reaction to general poor performance, as the specific timing of the delisting event is the key piece of private information.", "C": "The `MANP_t` values are consistently positive, indicating that insiders attempt to support their firm's stock price by buying shares before a delisting.", "D": "The evidence of anticipatory selling before a negative delisting event strengthens the paper's overall conclusion by demonstrating symmetric insider behavior around both good and bad news."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret statistical results for the delisting sample and synthesize them with the institutional context (involuntary nature of delisting) to evaluate the strength of the paper's argument. Depth Strategy: Reverse-Reasoning. Given the results and context, the user must identify all plausible conclusions. Distractor Logic: Option D is a 'Conceptual Opposite' distractor, directly contradicting the negative `MANP_t` values in the table. The correct options require recognizing the symmetry argument (A), understanding the nuance of involuntary events for identification (B), and accurately reading the statistical pattern in the table (C).", "qid": "289", "question": "### Background\n\n**Research Question.** Do insiders trade on advance knowledge of negative corporate events, such as an involuntary exchange delisting?\n\n**Setting.** The study analyzes insider trading for firms that are delisted from an exchange, an event associated with significant negative stock returns. Delisting is typically involuntary. A key challenge is distinguishing trading based on specific delisting information from trading based on the general poor performance that leads to delisting.\n\n**Variables & Parameters.**\n- `MANP_t`: Mean abnormal number of net purchasers in event month `t`.\n\n---\n\n### Data / Model Specification\n\nThe table below shows the `MANP_t` for firms approaching delisting. A negative value indicates net selling activity above historical norms.\n\n**Table 1. Mean Abnormal Net Insider Purchasing Activity (Delisting Firms)**\n\n| Event Month(s) | `MANP_t` | t-statistic |\n|:---------------|:-----------|:------------|\n| -9             | -0.460     | -2.616***   |\n| -7             | -0.358     | -1.892*     |\n| -5             | -0.403     | -2.116**    |\n| -3             | -0.510     | -2.439**    |\n| **[-12, -1]**  | **-3.446** | **-5.214*** |\n\n*Note: Table is an abridged version of the original. ***p<0.01, **p<0.05, *p<0.10.*\n\n---\n\nBased on the data and the context of involuntary delistings, which of the following statements are valid conclusions or arguments? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 286, "Question": "### Background\n\nOption pricing models require a volatility input. This can be estimated from past asset returns (historical volatility) or backed out from current option prices (implied volatility).\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the differences between historical and implied volatility? Select all that apply.", "Options": {"A": "Historical volatility is a statistical measure calculated as the standard deviation of an asset's past returns.", "B": "Implied volatility is considered forward-looking because it reflects the market's current expectation of future price dispersion.", "C": "Historical volatility is derived from current option prices, making it a real-time measure of market sentiment.", "D": "Implied volatility is always a more stable and less noisy measure than historical volatility."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the fundamental definitions distinguishing the two core volatility concepts in the paper. It replaces Q1 of the source QA. Distractors are designed by swapping the definitions (B) or introducing plausible but incorrect properties (D).", "qid": "286", "question": "### Background\n\nOption pricing models require a volatility input. This can be estimated from past asset returns (historical volatility) or backed out from current option prices (implied volatility).\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the differences between historical and implied volatility? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 277, "Question": "### Background\n\n**Research Question.** What is the joint and incremental value relevance of book value (a stock measure from the balance sheet) and earnings (a flow measure from the income statement) for valuing international banks?\n\n**Setting / Data-Generating Environment.** The analysis uses a pooled sample of 28,786 quarterly observations for 813 international banking institutions during the 1993–2004 period.\n\n### Data / Model Specification\n\nThree pooled OLS regressions are estimated to assess the value relevance of book value per share (`BV`) and earnings per share (`EPS`) for explaining market value per share (`MV`):\n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + e_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} EPS_i + e_{i} \\quad \\text{(Eq. 2)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + \\beta_{2} EPS_i + e_{i} \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Pooled OLS Regressions of Value Relevance**\n\n| Variable          | Model 1 (Eq. 1) | Model 2 (Eq. 2) | Model 3 (Eq. 3) |\n| :---------------- | :-------------- | :-------------- | :-------------- |\n| Intercept         | 18.452***       | 17.160***       | 14.340***       |\n| Book Value (`β₁`) | 0.559***        |                 | 0.462***        |\n| Earnings (`β₂`)   |                 | 0.376***        | 0.241***        |\n| R-squared         | 0.291           | 0.250           | 0.357           |\n\n*Note: *** denotes significance at the 1% level.*\n\n### Question\n\nTo test the hypothesis that the value relevance of earnings is weaker for loss-making firms, a researcher wants to modify Model 3. Select all of the following that correctly describe a valid approach.", "Options": {"A": "The researcher should split the sample into profitable and loss-making firms and test if the R-squared of the regression for the loss-making sample is lower.", "B": "To test if the market's response to losses is weaker, the researcher should test the one-sided alternative hypothesis `Hₐ: δ < 0` against the null `H₀: δ = 0`.", "C": "The researcher should add an interaction term, `δ * (D_loss * EPS)`, where `D_loss` is a dummy variable equal to 1 for firms with negative EPS. The coefficient for loss-making firms would then be `β₂ + δ`.", "D": "If the hypothesis is true, the main effect of earnings, `β₂`, should become statistically insignificant in the modified model."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses advanced modeling skills, specifically how to specify and test for asymmetric effects (a non-linearity). It uses a Reverse-Reasoning strategy, asking for the correct setup to test a given hypothesis. Correct options require specifying the correct interaction term model (A) and the corresponding one-sided hypothesis test (B). Distractor C misidentifies which coefficient is the target of the test, while D proposes an alternative but less direct and formally inferior testing procedure (comparing R-squared).", "qid": "277", "question": "### Background\n\n**Research Question.** What is the joint and incremental value relevance of book value (a stock measure from the balance sheet) and earnings (a flow measure from the income statement) for valuing international banks?\n\n**Setting / Data-Generating Environment.** The analysis uses a pooled sample of 28,786 quarterly observations for 813 international banking institutions during the 1993–2004 period.\n\n### Data / Model Specification\n\nThree pooled OLS regressions are estimated to assess the value relevance of book value per share (`BV`) and earnings per share (`EPS`) for explaining market value per share (`MV`):\n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + e_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} EPS_i + e_{i} \\quad \\text{(Eq. 2)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + \\beta_{2} EPS_i + e_{i} \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Pooled OLS Regressions of Value Relevance**\n\n| Variable          | Model 1 (Eq. 1) | Model 2 (Eq. 2) | Model 3 (Eq. 3) |\n| :---------------- | :-------------- | :-------------- | :-------------- |\n| Intercept         | 18.452***       | 17.160***       | 14.340***       |\n| Book Value (`β₁`) | 0.559***        |                 | 0.462***        |\n| Earnings (`β₂`)   |                 | 0.376***        | 0.241***        |\n| R-squared         | 0.291           | 0.250           | 0.357           |\n\n*Note: *** denotes significance at the 1% level.*\n\n### Question\n\nTo test the hypothesis that the value relevance of earnings is weaker for loss-making firms, a researcher wants to modify Model 3. Select all of the following that correctly describe a valid approach.", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": ["C"], "error": null}}
{"ID": 295, "Question": "### Background\n\n**Research Question.** Does the statistical out-of-sample (OOS) predictability from combination forecasts translate into economically significant, risk-adjusted, net-of-cost returns for an investor?\n\n**Setting.** The paper evaluates the real-time performance of an industry-rotation strategy based on combination forecasts. First, the statistical accuracy of the forecasts is measured using the out-of-sample R-squared (`R_OS^2`). Second, the economic performance of a long-short trading strategy based on these forecasts is evaluated using the Fama-French-Carhart four-factor model, with a focus on net-of-costs alpha (`\\alpha_{net}`). Finally, the benefit to a mean-variance investor is assessed by calculating the maximum attainable Sharpe Ratio (SR).\n\n**Variables and Parameters.**\n- `R_OS^2`: The out-of-sample R-squared, measuring the percentage reduction in mean squared forecast error relative to a historical mean forecast.\n- `\\alpha_{net}`: The annualized abnormal return of a strategy after accounting for transaction costs and exposure to the MKT, SMB, HML, and MOM factors.\n- `\\beta_{MKT}, \\beta_{SMB}, \\beta_{HML}, \\beta_{MOM}`: Loadings on the Fama-French-Carhart factors.\n- `SR`: The annualized maximum net-of-costs Sharpe Ratio of a portfolio.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the key out-of-sample results for the 44 industry portfolios from 1980:1 to 2015:4.\n\n**Table 1. Average Out-of-Sample `R_OS^2` Statistics (%)**\n\n| Forecast Method | ABMA | BMi (Own BM) |\n| :--- | :--- | :--- |\n| Avg `R_OS^2`    | 2.10 | -3.27        |\n\n*Source: Table III in the original paper. ABMA is a combination forecast method. BMi is the benchmark forecast using only an industry's own BM ratio.*\n\n**Table 2. Performance of Long-Short Industry-Rotation Strategies**\n\n| Strategy | `\\alpha_{net}` (%) | `\\beta_{MKT}` | `\\beta_{SMB}` | `\\beta_{HML}` | `\\beta_{MOM}` |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| L-S ABMA | 8.15*** | 0.15* | -0.36*** | -0.05 | 0.62*** |\n| L-S BMi | -0.31 | 0.11 | -0.17* | 0.34*** | -0.10** |\n\n*Source: Table IV in the original paper. L-S ABMA is the long-short strategy based on the ABMA combination forecast. L-S BMi is the benchmark strategy. `***` denotes statistical significance.*\n\n**Table 3. Ex-Post Mean-Variance Efficient Portfolios (Net of Costs)**\n\n| Universe | MKT | SMB | HML | MOM | L-S ABMA | SR |\n| :--- | :-: | :-: | :-: | :-: | :---: | :-: |\n| FF4      | 0.38| 0.00| 0.41| 0.21|       | 0.71|\n| FF4+ABMA | 0.29| 0.03| 0.44| 0.01| 0.23  | 0.86|\n\n*Source: Table V in the original paper. FF4 refers to the universe of the four Fama-French-Carhart factors. Weights represent the tangency portfolio composition.*\n\n---\n\n### Question\n\nBased on the evidence presented in Tables 1, 2, and 3, which of the following conclusions about the ABMA combination forecast strategy are supported?", "Options": {"A": "Despite a significant loading on the momentum (MOM) factor, the `L-S ABMA` strategy is not redundant, as it generates significant alpha *after* accounting for momentum exposure.", "B": "Adding the `L-S ABMA` strategy to the investment universe expands the efficient frontier, increasing the maximum attainable Sharpe Ratio and making the MOM factor largely redundant in the optimal portfolio.", "C": "The `L-S ABMA` strategy's profitability is primarily driven by a traditional value premium, as shown by its large, positive loading on the HML factor.", "D": "The statistical out-of-sample predictability of the ABMA forecast, evidenced by a positive average `R_OS^2`, translates into a statistically significant, positive net-of-costs alpha (`\\alpha_{net}`)."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the paper's core findings across statistical, risk-adjusted, and portfolio-level evidence. It uses an **Atomic Decomposition** strategy, breaking down the paper's main conclusion into three independently verifiable statements. Option B serves as a high-fidelity 'Conceptual Opposite' distractor, testing a common misinterpretation of a strategy based on book-to-market ratios (expecting a value effect) against the actual factor loadings provided.", "qid": "295", "question": "### Background\n\n**Research Question.** Does the statistical out-of-sample (OOS) predictability from combination forecasts translate into economically significant, risk-adjusted, net-of-cost returns for an investor?\n\n**Setting.** The paper evaluates the real-time performance of an industry-rotation strategy based on combination forecasts. First, the statistical accuracy of the forecasts is measured using the out-of-sample R-squared (`R_OS^2`). Second, the economic performance of a long-short trading strategy based on these forecasts is evaluated using the Fama-French-Carhart four-factor model, with a focus on net-of-costs alpha (`\\alpha_{net}`). Finally, the benefit to a mean-variance investor is assessed by calculating the maximum attainable Sharpe Ratio (SR).\n\n**Variables and Parameters.**\n- `R_OS^2`: The out-of-sample R-squared, measuring the percentage reduction in mean squared forecast error relative to a historical mean forecast.\n- `\\alpha_{net}`: The annualized abnormal return of a strategy after accounting for transaction costs and exposure to the MKT, SMB, HML, and MOM factors.\n- `\\beta_{MKT}, \\beta_{SMB}, \\beta_{HML}, \\beta_{MOM}`: Loadings on the Fama-French-Carhart factors.\n- `SR`: The annualized maximum net-of-costs Sharpe Ratio of a portfolio.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the key out-of-sample results for the 44 industry portfolios from 1980:1 to 2015:4.\n\n**Table 1. Average Out-of-Sample `R_OS^2` Statistics (%)**\n\n| Forecast Method | ABMA | BMi (Own BM) |\n| :--- | :--- | :--- |\n| Avg `R_OS^2`    | 2.10 | -3.27        |\n\n*Source: Table III in the original paper. ABMA is a combination forecast method. BMi is the benchmark forecast using only an industry's own BM ratio.*\n\n**Table 2. Performance of Long-Short Industry-Rotation Strategies**\n\n| Strategy | `\\alpha_{net}` (%) | `\\beta_{MKT}` | `\\beta_{SMB}` | `\\beta_{HML}` | `\\beta_{MOM}` |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| L-S ABMA | 8.15*** | 0.15* | -0.36*** | -0.05 | 0.62*** |\n| L-S BMi | -0.31 | 0.11 | -0.17* | 0.34*** | -0.10** |\n\n*Source: Table IV in the original paper. L-S ABMA is the long-short strategy based on the ABMA combination forecast. L-S BMi is the benchmark strategy. `***` denotes statistical significance.*\n\n**Table 3. Ex-Post Mean-Variance Efficient Portfolios (Net of Costs)**\n\n| Universe | MKT | SMB | HML | MOM | L-S ABMA | SR |\n| :--- | :-: | :-: | :-: | :-: | :---: | :-: |\n| FF4      | 0.38| 0.00| 0.41| 0.21|       | 0.71|\n| FF4+ABMA | 0.29| 0.03| 0.44| 0.01| 0.23  | 0.86|\n\n*Source: Table V in the original paper. FF4 refers to the universe of the four Fama-French-Carhart factors. Weights represent the tangency portfolio composition.*\n\n---\n\n### Question\n\nBased on the evidence presented in Tables 1, 2, and 3, which of the following conclusions about the ABMA combination forecast strategy are supported?", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 298, "Question": "### Background\n\n**Research Question.** This case examines a modern econometric technique for isolating business cycle components of time series, which is a prerequisite for analyzing the cyclical comovement between macroeconomic variables like bank capital and real GDP.\n\n**Setting / Data-Generating Environment.** The setting involves a stationary or non-stationary time series, `y_t`, observed over a sample period. The goal is to separate its long-run trend from its cyclical component. The specific application mentioned is for quarterly macroeconomic data, where business cycle frequencies are of primary interest.\n\n**Variables & Parameters.**\n- `y_t`: The value of a time series at time `t` (e.g., log Real GDP).\n- `h`: The forecast horizon, set to 8 for quarterly data (2 years).\n- `\\beta_0, \\ldots, \\beta_4`: OLS regression coefficients.\n- `v_{t+h}`: The population error term of the predictive regression.\n- `\\hat{v}_{t+h}`: The estimated residual from the OLS regression, defined as the cyclical component of the series at time `t+h`.\n- `t`: Time index for observations (e.g., quarters).\n\n---\n\n### Data / Model Specification\n\nThe Hamilton (2018) method for detrending a time series `y_t` is based on the following linear projection:\n  \ny_{t+h} = \\beta_0 + \\beta_1 y_t + \\beta_2 y_{t-1} + \\beta_3 y_{t-2} + \\beta_4 y_{t-3} + v_{t+h} \\quad \\text{(Eq. 1)}\n \nThe trend component is the predicted value from this regression, `\\hat{y}_{t+h}`. The cyclical component is defined as the residual from this regression:\n  \n\\hat{v}_{t+h} = y_{t+h} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 y_t + \\hat{\\beta}_2 y_{t-1} + \\hat{\\beta}_3 y_{t-2} + \\hat{\\beta}_4 y_{t-3}) \\quad \\text{(Eq. 2)}\n \nFor business cycle analysis of quarterly data, the horizon `h` is set to 8 quarters.\n\n---\n\n### The Question\n\nCompared to the widely used Hodrick-Prescott (H-P) filter, what are the key methodological advantages of the Hamilton (2018) method for business cycle analysis? (Select all that apply)", "Options": {"A": "It is a two-sided filter that produces a visually smoother trend component, making it easier to interpret.", "B": "It avoids imposing a fixed, ad-hoc smoothness assumption on the trend, allowing the data to determine the trend's properties.", "C": "It is a one-sided filter that uses only past and current information, which prevents spurious cyclical patterns caused by future data.", "D": "It is guaranteed to produce a stationary cyclical component, regardless of whether the original series has a unit root."}, "Answer": ["B", "C"], "pi_justification": "This Multiple Choice item tests the student's ability to identify the key methodological advantages of the Hamilton filter over the commonly used H-P filter. It uses a 'select-all-valid' strategy. The correct options (A, B) highlight its one-sided nature and data-driven flexibility. Distractors are designed to be plausible but incorrect: (C) incorrectly attributes a property of the H-P filter to the Hamilton filter, and (D) makes an overly strong claim about guaranteed stationarity, a common point of confusion.", "qid": "298", "question": "### Background\n\n**Research Question.** This case examines a modern econometric technique for isolating business cycle components of time series, which is a prerequisite for analyzing the cyclical comovement between macroeconomic variables like bank capital and real GDP.\n\n**Setting / Data-Generating Environment.** The setting involves a stationary or non-stationary time series, `y_t`, observed over a sample period. The goal is to separate its long-run trend from its cyclical component. The specific application mentioned is for quarterly macroeconomic data, where business cycle frequencies are of primary interest.\n\n**Variables & Parameters.**\n- `y_t`: The value of a time series at time `t` (e.g., log Real GDP).\n- `h`: The forecast horizon, set to 8 for quarterly data (2 years).\n- `\\beta_0, \\ldots, \\beta_4`: OLS regression coefficients.\n- `v_{t+h}`: The population error term of the predictive regression.\n- `\\hat{v}_{t+h}`: The estimated residual from the OLS regression, defined as the cyclical component of the series at time `t+h`.\n- `t`: Time index for observations (e.g., quarters).\n\n---\n\n### Data / Model Specification\n\nThe Hamilton (2018) method for detrending a time series `y_t` is based on the following linear projection:\n  \ny_{t+h} = \\beta_0 + \\beta_1 y_t + \\beta_2 y_{t-1} + \\beta_3 y_{t-2} + \\beta_4 y_{t-3} + v_{t+h} \\quad \\text{(Eq. 1)}\n \nThe trend component is the predicted value from this regression, `\\hat{y}_{t+h}`. The cyclical component is defined as the residual from this regression:\n  \n\\hat{v}_{t+h} = y_{t+h} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 y_t + \\hat{\\beta}_2 y_{t-1} + \\hat{\\beta}_3 y_{t-2} + \\hat{\\beta}_4 y_{t-3}) \\quad \\text{(Eq. 2)}\n \nFor business cycle analysis of quarterly data, the horizon `h` is set to 8 quarters.\n\n---\n\n### The Question\n\nCompared to the widely used Hodrick-Prescott (H-P) filter, what are the key methodological advantages of the Hamilton (2018) method for business cycle analysis? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 290, "Question": "### Background\n\n**Research Question.** How can 'normal' insider trading be modeled to establish a benchmark against which to measure abnormal, information-driven trading activity?\n\n**Setting.** The study models the monthly number of net insider purchasers (`NP_{jt}`) for each firm `j` using a time-series process. The parameters are estimated during a non-event period to capture typical behavior.\n\n---\n\n### Data / Model Specification\n\nThe empirical model for 'normal' trading is an AR(1) process:\n\n  \nNP_{jt} = \\alpha_j + \\beta_j NP_{jt-1} + \\epsilon_{jt} \\quad \\text{(Eq. 1)}\n \n\nFrom the estimated coefficients `α̂_j` and `β̂_j`, the long-run mean number of net purchasers, `γ̂_j`, is calculated as:\n\n  \n\\hat{\\gamma}_j = \\frac{\\hat{\\alpha}_j}{1 - \\hat{\\beta}_j} \\quad \\text{(Eq. 2)}\n \n\nFor firms that are listing, the parameters of Eq. (1) are estimated over a post-listing window (months +7 to +60). A successful listing may represent a permanent structural break, improving the firm's prospects.\n\n---\n\nConsidering the model and the choice of a post-listing estimation window for listing firms, which of the following statements are valid? Select all that apply.", "Options": {"A": "The parameter `β_j` represents the persistence of insider trading; a value close to 1 implies that periods of net buying tend to be followed by more net buying.", "B": "Using a post-listing estimation window could create a conservative test; if listing improves firm prospects and raises the true `γ_j`, the benchmark for 'normal' buying will be set high, potentially underestimating the degree of abnormal buying in the pre-listing period.", "C": "If a firm's estimation yields `α̂ = 0.2` and `β̂ = 0.6`, its long-run mean number of net purchasers `γ̂` is 0.12.", "D": "If a firm's estimation yields `α̂ = 0.2` and `β̂ = 0.6`, its long-run mean number of net purchasers `γ̂` is 0.5, indicating a baseline tendency for net buying."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses understanding of the paper's core econometric model, including parameter interpretation, calculation, and the ability to critique the research design for potential bias (structural breaks). Depth Strategy: Computational Judgment and Scenario Application. The user must interpret parameters, perform a calculation, and evaluate the methodological choice of the estimation window. Distractor Logic: Option D is a 'Formula Misuse' distractor, resulting from incorrectly multiplying `α̂` and `β̂` instead of applying the correct formula from Eq. (2). The correct options require correct interpretation of `β` (A), correct calculation of `γ` (B), and a sophisticated understanding of the potential bias from a structural break (C).", "qid": "290", "question": "### Background\n\n**Research Question.** How can 'normal' insider trading be modeled to establish a benchmark against which to measure abnormal, information-driven trading activity?\n\n**Setting.** The study models the monthly number of net insider purchasers (`NP_{jt}`) for each firm `j` using a time-series process. The parameters are estimated during a non-event period to capture typical behavior.\n\n---\n\n### Data / Model Specification\n\nThe empirical model for 'normal' trading is an AR(1) process:\n\n  \nNP_{jt} = \\alpha_j + \\beta_j NP_{jt-1} + \\epsilon_{jt} \\quad \\text{(Eq. 1)}\n \n\nFrom the estimated coefficients `α̂_j` and `β̂_j`, the long-run mean number of net purchasers, `γ̂_j`, is calculated as:\n\n  \n\\hat{\\gamma}_j = \\frac{\\hat{\\alpha}_j}{1 - \\hat{\\beta}_j} \\quad \\text{(Eq. 2)}\n \n\nFor firms that are listing, the parameters of Eq. (1) are estimated over a post-listing window (months +7 to +60). A successful listing may represent a permanent structural break, improving the firm's prospects.\n\n---\n\nConsidering the model and the choice of a post-listing estimation window for listing firms, which of the following statements are valid? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 84, "Question": "### Background\n\n**Research Question.** How can Principal Component Regression (PCR) resolve the problem of theoretically incorrect coefficient signs that plagues OLS models of housing prices when multicollinearity is present?\n\n**Setting.** A regression of standardized housing prices is run on 16 orthogonal factors derived from Principal Component Analysis (PCA). These factors represent dimensions like \"quantity\" and \"quality\". In prior OLS models, variables like number of bedrooms (`BDRMS`) had counter-intuitive negative coefficients.\n\n---\n\n### Data / Model Specification\n\nIn contrast to OLS, PCR regresses price on orthogonal factors. The paper identifies Factor 2 as a \"quantity\" dimension, which is a linear combination of variables including `SQFT`, `BDRMS`, and `BATHRMS`. The results for the PCR model with 16 factors (PCR16) show a regression coefficient for Factor 2 of 0.6861, which is positive and highly significant.\n\n---\n\nWhich of the following statements accurately describe how the PCR methodology addresses the multicollinearity problem observed in the OLS model?", "Options": {"A": "The positive coefficient on Factor 2 (the \"quantity\" dimension) correctly captures the aggregate positive impact of house size on price, resolving the nonsensical negative signs seen on individual size-related variables in the OLS model.", "B": "PCR provides direct, unbiased estimates of the marginal dollar impact for each original variable (e.g., `BDRMS`), which is its primary advantage over OLS for structural analysis.", "C": "PCR transforms the original correlated predictor variables into a new set of factors that are, by mathematical construction, orthogonal (uncorrelated) to each other.", "D": "By regressing the outcome on the orthogonal factors, the resulting factor coefficients are not distorted by inter-factor correlations, thus satisfying the linear independence assumption."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's understanding of the fundamental mechanism of Principal Component Regression (PCR) and how it solves the multicollinearity problem. Depth Strategy: Reverse-Reasoning. The user is given the solution (PCR) and must identify all the correct mechanistic steps and interpretive conclusions that make it effective. Distractor Logic: Option D is a conceptual opposite and a key trade-off mentioned in the paper. PCR's main disadvantage for structural analysis is that it *loses* the direct, interpretable coefficients for the original variables, providing coefficients for abstract factors instead.", "qid": "84", "question": "### Background\n\n**Research Question.** How can Principal Component Regression (PCR) resolve the problem of theoretically incorrect coefficient signs that plagues OLS models of housing prices when multicollinearity is present?\n\n**Setting.** A regression of standardized housing prices is run on 16 orthogonal factors derived from Principal Component Analysis (PCA). These factors represent dimensions like \"quantity\" and \"quality\". In prior OLS models, variables like number of bedrooms (`BDRMS`) had counter-intuitive negative coefficients.\n\n---\n\n### Data / Model Specification\n\nIn contrast to OLS, PCR regresses price on orthogonal factors. The paper identifies Factor 2 as a \"quantity\" dimension, which is a linear combination of variables including `SQFT`, `BDRMS`, and `BATHRMS`. The results for the PCR model with 16 factors (PCR16) show a regression coefficient for Factor 2 of 0.6861, which is positive and highly significant.\n\n---\n\nWhich of the following statements accurately describe how the PCR methodology addresses the multicollinearity problem observed in the OLS model?", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"Principal Component Regression housing prices multicollinearity negative coefficients\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Multiple Regression, Multicollinearity, and Model Building - Housing Prices - JMP User Community\" JMP User Community](https://community.jmp.com/t5/Learning-Center/Multiple-Regression-Multicollinearity-and-Model-Building-Housing/ta-p/276833)\n> [🔗 \"Multicollinearity in Regression Analysis: Problems, Detection, and Solutions - Statistics By Jim\" Statistics By Jim](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/)\n> [🔗 \"Principal Component Analysis to Address Multicollinearity\" Whitman College](https://www.whitman.edu/Documents/Academics/Mathematics/2017/Perez.pdf)\n> [🔗 \"Principal Components Regression\" NCSS](https://www.ncss.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Principal_Components_Regression.pdf)\n> [🔗 \"Understanding Multicollinearity: Detection and Remedies\" Statology](https://www.statology.org/understanding-multicollinearity-detection-and-remedies/)\n> [🔗 \"r/datascience on Reddit: Can we use PCA (Principal Component analysis) to reduce dimension for a linear regression problem and at the same time control the multi-collinearity issue?\" Reddit](https://www.reddit.com/r/datascience/comments/z8175q/can_we_use_pca_principal_component_analysis_to/)\n> [🔗 \"Analysis on Boston Housing Data – SayuriBlog\" Sayuri2333](https://sayuri2333.github.io/post/analysis_on_boston_housing/)\n> [🔗 \"Principal Component Analysis with Lasso Regression on Kaggle’s Ames Housing Dataset | by Dimitri Linde | Medium\" Medium](https://medium.com/%2540dimitrilinde/principal-component-analysis-with-lasso-regression-on-kaggles-ames-housing-dataset-21198ae5d788)\n> [🔗 \"Multiple Linear Regression and Housing Prices | by Seong Minh Jang | Medium\" Medium](https://seongminhjang.medium.com/multiple-linear-regression-on-housing-prices-8837841502ed)\n> [🔗 \"Housing Price Prediction Based on Multiple Linear Regression - Zhang - 2021 - Scientific Programming - Wiley Online Library\" Wiley Online Library](https://onlinelibrary.wiley.com/doi/10.1155/2021/7678931)\n\n\nA,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 187, "Question": "### Background\n\nAn actuarial model is developed for the pure risk premium (`x`) per dollar of guaranteed mortgage. The model explicitly accounts for the statistical dependence between the frequency of mortgage defaults (`z`) and the severity of those defaults (`y`).\n\n### Data / Model Specification\n\nThe mean pure risk premium is given by:\n\n  \nE(x) = E(z)E(y) + Cov(z,y) \\quad \\text{(Eq. (1))}\n \n\nThe paper argues that for mortgage portfolios, `Cov(z,y)` is positive, especially during economic downturns, because a single macroeconomic shock (e.g., a recession) drives both default frequency and loss severity upwards simultaneously.\n\n### Question\n\nAccording to the model in **Eq. (1)** and the paper's reasoning, which of the following scenarios would likely exhibit a strongly positive covariance between claim frequency (`z`) and claim severity (`y`), leading to an underestimation of expected losses if the covariance term were ignored? Select all that apply.", "Options": {"A": "A portfolio of mortgage guarantees concentrated in a region heavily dependent on a single industry, such as oil and gas.", "B": "A portfolio of automobile insurance policies spread across a large, stable country with uncorrelated, idiosyncratic accident causes.", "C": "A large, geographically diversified portfolio of individual life insurance policies with fixed death benefits.", "D": "A portfolio of property insurance policies concentrated in a single coastal region prone to hurricanes."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item assesses the ability to generalize the core concept of correlated frequency and severity to novel scenarios. Depth Strategy: Scenario Application. The user must apply the principle from the paper's mortgage model to different types of insurance portfolios. Distractor Logic: Options B and D represent classic examples of insurance portfolios where risks are idiosyncratic and largely uncorrelated, meaning `Cov(z,y)` would be near zero. These are conceptual opposites to the paper's central argument. Options A and C are analogous to the paper's Southwest 'stress case,' where a single systemic risk factor (a hurricane, a regional recession) drives both the number and size of losses, making them correct applications of the concept.", "qid": "187", "question": "### Background\n\nAn actuarial model is developed for the pure risk premium (`x`) per dollar of guaranteed mortgage. The model explicitly accounts for the statistical dependence between the frequency of mortgage defaults (`z`) and the severity of those defaults (`y`).\n\n### Data / Model Specification\n\nThe mean pure risk premium is given by:\n\n  \nE(x) = E(z)E(y) + Cov(z,y) \\quad \\text{(Eq. (1))}\n \n\nThe paper argues that for mortgage portfolios, `Cov(z,y)` is positive, especially during economic downturns, because a single macroeconomic shock (e.g., a recession) drives both default frequency and loss severity upwards simultaneously.\n\n### Question\n\nAccording to the model in **Eq. (1)** and the paper's reasoning, which of the following scenarios would likely exhibit a strongly positive covariance between claim frequency (`z`) and claim severity (`y`), leading to an underestimation of expected losses if the covariance term were ignored? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 186, "Question": "### Background\n\nAn analysis uses an option-pricing framework to value the U.S. government's implicit guarantee on Freddie Mac's mortgage portfolio. The guarantee is modeled as a put option written by the government, with an exercise price that is reduced by Freddie Mac's own loss-absorbing capital. The model is calibrated under two scenarios: a 'Normal' scenario (20% of mortgages are 'at-risk') and a 'Stress' scenario (50% of mortgages are 'at-risk', with higher loss rates). The analysis then simulates the effect of increasing Freddie Mac's capital to 3.0% of its total mortgage book.\n\n### Data / Model Specification\n\nThe value of the government guarantee, expressed in basis points (b.p.) of the total mortgage portfolio, is calculated for different levels of underlying credit risk (Total Mean Pure Premium, or MPP) and under different capital and economic scenarios.\n\n**Table 1: Value of the Implicit Government Guarantee (in basis points)**\n| Total MPP (b.p.) | Baseline Capital, Normal Scenario | Baseline Capital, Stress Scenario | 3.0% Capital, Normal Scenario | 3.0% Capital, Stress Scenario |\n| :---: | :---: | :---: | :---: | :---: |\n| 35.0 | 16.2 | 22.6 | 4.1 | 11.3 |\n| 27.1 | 11.3 | 16.7 | 2.3 | 7.8 |\n| 15.0 | 4.8 | 8.3 | 0.6 | 3.2 |\n\n*Source: Synthesized from Tables 6, 6A, 7, and 7A in the paper.*\n\n### Question\n\nBased on the data in **Table 1** for the 27.1 b.p. MPP case, which of the following statements are correct interpretations of the effect of increasing Freddie Mac's capital from the baseline to 3.0%?", "Options": {"A": "The additional capital is relatively less effective during the Stress Scenario, reducing the guarantee by only 53.3% compared to a 79.6% reduction in the Normal Scenario.", "B": "The absolute reduction in the government guarantee is greater under the Normal Scenario (9.0 b.p.) than under the Stress Scenario (8.9 b.p.).", "C": "After the capital increase, the remaining government guarantee under the Stress Scenario (7.8 b.p.) is more than triple the remaining guarantee under the Normal Scenario (2.3 b.p.).", "D": "The percentage reduction in the government guarantee is greater under the Stress Scenario than under the Normal Scenario."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to perform multi-step calculations (absolute and percentage changes) from tabular data and draw nuanced conclusions about policy effectiveness under different conditions. Depth Strategy: Computational Judgment. The user must calculate and compare values to evaluate each statement. Distractor Logic: Option B is a conceptual opposite; the percentage reduction is actually smaller in the stress scenario, a key finding of the paper. Options A, C, and D are all arithmetically verifiable and correct interpretations of the data, requiring careful calculation and comparison.", "qid": "186", "question": "### Background\n\nAn analysis uses an option-pricing framework to value the U.S. government's implicit guarantee on Freddie Mac's mortgage portfolio. The guarantee is modeled as a put option written by the government, with an exercise price that is reduced by Freddie Mac's own loss-absorbing capital. The model is calibrated under two scenarios: a 'Normal' scenario (20% of mortgages are 'at-risk') and a 'Stress' scenario (50% of mortgages are 'at-risk', with higher loss rates). The analysis then simulates the effect of increasing Freddie Mac's capital to 3.0% of its total mortgage book.\n\n### Data / Model Specification\n\nThe value of the government guarantee, expressed in basis points (b.p.) of the total mortgage portfolio, is calculated for different levels of underlying credit risk (Total Mean Pure Premium, or MPP) and under different capital and economic scenarios.\n\n**Table 1: Value of the Implicit Government Guarantee (in basis points)**\n| Total MPP (b.p.) | Baseline Capital, Normal Scenario | Baseline Capital, Stress Scenario | 3.0% Capital, Normal Scenario | 3.0% Capital, Stress Scenario |\n| :---: | :---: | :---: | :---: | :---: |\n| 35.0 | 16.2 | 22.6 | 4.1 | 11.3 |\n| 27.1 | 11.3 | 16.7 | 2.3 | 7.8 |\n| 15.0 | 4.8 | 8.3 | 0.6 | 3.2 |\n\n*Source: Synthesized from Tables 6, 6A, 7, and 7A in the paper.*\n\n### Question\n\nBased on the data in **Table 1** for the 27.1 b.p. MPP case, which of the following statements are correct interpretations of the effect of increasing Freddie Mac's capital from the baseline to 3.0%?", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 153, "Question": "### Background\n\nThe study uses an instrumental variables (IV) approach to estimate the causal effect of an employee's total spending account balance (`Total_Account`) on their health care spending. The primary instrument is the minimum account balance within the employee's firm and coverage tier (`Min_Account`). The validity of this strategy rests on two key assumptions: relevance and the exclusion restriction.\n\n### Data / Model Specification\n\n**Second Stage (Structural Equation):**\n  \nSpending_{igt} = \\beta_0 + \\beta_1 Total\\_Account_{igt} + \\dots + \\varepsilon_{igt} \\quad \\text{(Eq. (1))}\n \n**First Stage Equation:**\n  \nTotal\\_Account_{igt} = \\alpha_0 + \\alpha_1 Min\\_Account_{gt} + \\dots + u_{igt} \\quad \\text{(Eq. (2))}\n \n\n---\n\nAccording to the paper's description of its IV strategy, which of the following statements are valid claims or assumptions?\n", "Options": {"A": "The exclusion restriction assumes that the firm's contribution policy (proxied by `Min_Account`) does not directly respond to a single employee's idiosyncratic health shocks.", "B": "The relevance of the `Min_Account` instrument stems from the fact that the firm's contribution is a common component in all employees' total account balances.", "C": "The exclusion restriction can be formally tested using the first-stage F-statistic.", "D": "The exclusion restriction is more likely to be violated in larger firms, where individual health shocks are more likely to influence aggregate benefit design."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the core assumptions underlying the paper's instrumental variables identification strategy. Depth Strategy: Reverse-Reasoning (identifying necessary pre-conditions for the model's validity). Distractor Logic: Option A correctly states the core logic of the exclusion restriction in this context. Option B correctly explains the economic mechanism that makes the instrument relevant. Option C is a Conceptual Opposite; the paper explicitly argues the opposite, stating that the assumption is *more* plausible in larger firms and for this reason they exclude very small firms. Option D is a Conceptual Distractor (Formula Misuse); the F-statistic tests for instrument relevance (strength), not the exclusion restriction, which is untestable.", "qid": "153", "question": "### Background\n\nThe study uses an instrumental variables (IV) approach to estimate the causal effect of an employee's total spending account balance (`Total_Account`) on their health care spending. The primary instrument is the minimum account balance within the employee's firm and coverage tier (`Min_Account`). The validity of this strategy rests on two key assumptions: relevance and the exclusion restriction.\n\n### Data / Model Specification\n\n**Second Stage (Structural Equation):**\n  \nSpending_{igt} = \\beta_0 + \\beta_1 Total\\_Account_{igt} + \\dots + \\varepsilon_{igt} \\quad \\text{(Eq. (1))}\n \n**First Stage Equation:**\n  \nTotal\\_Account_{igt} = \\alpha_0 + \\alpha_1 Min\\_Account_{gt} + \\dots + u_{igt} \\quad \\text{(Eq. (2))}\n \n\n---\n\nAccording to the paper's description of its IV strategy, which of the following statements are valid claims or assumptions?\n", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 230, "Question": "### Background\n\nThe analysis in the paper compares the cost-effectiveness of different Executive Stock Option (ESO) designs. A key finding is that for executives who are highly risk-averse and whose portfolios are heavily concentrated in company stock, shortening the vesting period is a particularly efficient way to increase their perceived (subjective) value of the option grant.\n\n### Data / Model Specification\n\n**Table 1** shows the 'unit cost of vesting period delta' for a hurdle rate ESO under different scenarios. This metric represents the firm's objective cost for each dollar of subjective value delivered to the executive by shortening the vesting period by one year. A lower unit cost indicates higher cost-effectiveness.\n\n**Table 1: Unit cost of vesting period delta ($)**\n| Risk Aversion (`α`) | Excess Holdings (`θ`) | Unit Cost of Vesting Period Delta ($) |\n| :--- | :--- | :--- |\n| 4 | 75% | 20.55 |\n| 5 | 25% | 96.68 |\n| 5 | 50% | 36.07 |\n| 5 | 75% | 15.23 |\n| 7 | 50% | 23.87 |\n| 7 | 75% | 10.06 |\n\n*Base case parameters: S=K=$100, T=10 years, r=5%, σ=30%, h=10%, q=2%, ν=20%. Deltas are based on decreasing the vesting period from 2 years to 1 year.*\n\n---\n\nBased on the data in **Table 1** and the paper's findings, select all of the following statements that are valid conclusions.\n", "Options": {"A": "For an executive with a fixed level of risk aversion (e.g., α=5), shortening the vesting period becomes a more cost-effective way to deliver value as their portfolio becomes more concentrated in company stock.", "B": "For an executive with a fixed, high level of portfolio concentration (e.g., θ=75%), shortening the vesting period becomes a more cost-effective way to deliver value as they become more risk-averse.", "C": "The data suggest that the subjective value an executive places on the flexibility to exercise early increases at an accelerating rate as their risk aversion and portfolio constraints tighten.", "D": "Shortening the vesting period is most cost-effective for executives with low risk aversion and a diversified portfolio (e.g., α=5, θ=25%)."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret trends from tabular data and connect them to the underlying economic intuition presented in the paper. It uses a Scenario Application strategy. Options A and B require direct trend analysis from the table (across a row and down a column, respectively). Option C requires synthesizing these trends into a higher-level economic conclusion about the non-linear utility cost of constraints. Option D is a conceptual opposite, directly contradicted by the data showing the highest unit cost for this group.", "qid": "230", "question": "### Background\n\nThe analysis in the paper compares the cost-effectiveness of different Executive Stock Option (ESO) designs. A key finding is that for executives who are highly risk-averse and whose portfolios are heavily concentrated in company stock, shortening the vesting period is a particularly efficient way to increase their perceived (subjective) value of the option grant.\n\n### Data / Model Specification\n\n**Table 1** shows the 'unit cost of vesting period delta' for a hurdle rate ESO under different scenarios. This metric represents the firm's objective cost for each dollar of subjective value delivered to the executive by shortening the vesting period by one year. A lower unit cost indicates higher cost-effectiveness.\n\n**Table 1: Unit cost of vesting period delta ($)**\n| Risk Aversion (`α`) | Excess Holdings (`θ`) | Unit Cost of Vesting Period Delta ($) |\n| :--- | :--- | :--- |\n| 4 | 75% | 20.55 |\n| 5 | 25% | 96.68 |\n| 5 | 50% | 36.07 |\n| 5 | 75% | 15.23 |\n| 7 | 50% | 23.87 |\n| 7 | 75% | 10.06 |\n\n*Base case parameters: S=K=$100, T=10 years, r=5%, σ=30%, h=10%, q=2%, ν=20%. Deltas are based on decreasing the vesting period from 2 years to 1 year.*\n\n---\n\nBased on the data in **Table 1** and the paper's findings, select all of the following statements that are valid conclusions.\n", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 209, "Question": "### Background\n\nA proportional hazard model was used to estimate the determinants of default and prepayment for FHA borrowers. The results are presented as exponentiated coefficients (hazard ratios). A hazard ratio above 1 indicates an increased risk relative to a baseline borrower, while a ratio below 1 indicates a decreased risk.\n\n**Baseline Borrower Profile:** A borrower with a current Loan-to-Value (LTV) below 80%, a FICO score above 720, and a Debt-to-Income (DTI) ratio below 28.\n\n### Data / Model Specification\n\n**Table 1: Selected Borrower-Based Hazard Ratio Estimates**\n\n| Variable | Default Hazard Ratio | Prepayment Hazard Ratio |\n| :--- | :--- | :--- |\n| **Loan-to-Value (LTV):** | | |\n| 120 or higher | 3.72 | 0.50 |\n| **Credit Score (FICO):** | | |\n| Less than 580 | 10.82 | 0.43 |\n\n*Note: Coefficients are relative to the baseline borrower profile.*\n\n### Question\n\nA high-risk borrower is defined by having *both* a current LTV of 120% or higher *and* a FICO score below 580. Based on the data in Table 1, which of the following statements accurately describe this borrower's risk profile relative to the baseline borrower?", "Options": {"A": "The borrower's monthly probability of default is more than 40 times higher.", "B": "The borrower's monthly probability of prepaying (a sustainable exit) is less than one-quarter (21.5%) of the baseline.", "C": "The borrower's monthly probability of default is approximately 14.5 times higher.", "D": "The combination of high default risk and low prepayment risk implies a prolonged period of high-risk exposure for the FHA."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to synthesize multiple risk factors from a hazard model to create a composite risk profile. It requires both calculation and conceptual interpretation. Depth Strategy: Computational Judgment and Scenario Application. Distractor Logic: Option A requires multiplying the two default hazard ratios (`3.72 * 10.82 = 40.25`). Option B requires multiplying the two prepayment hazard ratios (`0.50 * 0.43 = 0.215`). Option C requires a correct conceptual interpretation of the combined effect. Option D is a 'Formula Misuse' distractor, created by incorrectly adding the hazard ratios (`3.72 + 10.82 = 14.54`) instead of multiplying them, a common error.", "qid": "209", "question": "### Background\n\nA proportional hazard model was used to estimate the determinants of default and prepayment for FHA borrowers. The results are presented as exponentiated coefficients (hazard ratios). A hazard ratio above 1 indicates an increased risk relative to a baseline borrower, while a ratio below 1 indicates a decreased risk.\n\n**Baseline Borrower Profile:** A borrower with a current Loan-to-Value (LTV) below 80%, a FICO score above 720, and a Debt-to-Income (DTI) ratio below 28.\n\n### Data / Model Specification\n\n**Table 1: Selected Borrower-Based Hazard Ratio Estimates**\n\n| Variable | Default Hazard Ratio | Prepayment Hazard Ratio |\n| :--- | :--- | :--- |\n| **Loan-to-Value (LTV):** | | |\n| 120 or higher | 3.72 | 0.50 |\n| **Credit Score (FICO):** | | |\n| Less than 580 | 10.82 | 0.43 |\n\n*Note: Coefficients are relative to the baseline borrower profile.*\n\n### Question\n\nA high-risk borrower is defined by having *both* a current LTV of 120% or higher *and* a FICO score below 580. Based on the data in Table 1, which of the following statements accurately describe this borrower's risk profile relative to the baseline borrower?", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 227, "Question": "### Background\n\n**Research Question.** This case examines the core hypotheses of public finance regarding the capitalization of local fiscal policies. Specifically, it investigates whether residential and business property owners have different preferences for public services versus tax burdens, leading to differential impacts on their respective property values. The analysis tests two primary hypotheses:\n\n- **Hypothesis 1 (H1):** Residential property values are more responsive to changes in school spending than to changes in property taxes.\n- **Hypothesis 2 (H2):** Business property values are more responsive to changes in property taxes than to changes in school spending.\n\n**Setting / Data-Generating Environment.** The study uses a panel of 152 communities in southeast Michigan from 1983-2002. In this setting, public schools are primarily financed by property taxes. The Tiebout model suggests that mobile agents (both residents and firms) sort into communities that offer their preferred tax-service package. Residents, who directly benefit from schools, may value them highly. Firms, modeled as profit-maximizers, may view property taxes as a direct cost of business while receiving little direct benefit from K-12 education.\n\n### Data / Model Specification\n\nThe study estimates a dynamic first-difference 2SLS model to determine the long-run elasticities of property values with respect to fiscal policy. The model includes contemporaneous and three-year lagged values of policy variables to capture the full capitalization effect over time. Table 1 below presents the estimated coefficients for both residential and business property value growth.\n\n**Table 1: Selected 2SLS Regression Coefficients for Property Value Growth**\n\n| Dependent Variable | Regressor | Coefficient | Std. Error |\n| :--- | :--- | :--- | :--- |\n| **ΔLog Residential Property Value** | | | |\n| | Log Property Tax | -0.256*** | (0.040) |\n| | Log Property Tax t-1 | -0.123*** | (0.020) |\n| | Log Property Tax t-2 | -0.029*** | (0.010) |\n| | Log Property Tax t-3 | 0.004 | (0.012) |\n| | Log School Spending | 0.623*** | (0.236) |\n| | Log School Spending t-1 | -0.029 | (0.186) |\n| | Log School Spending t-2 | -0.156 | (0.162) |\n| | Log School Spending t-3 | 0.157 | (0.111) |\n| **ΔLog Business Property Value** | | | |\n| | Log Property Tax | -0.194 | (0.136) |\n| | Log Property Tax t-1 | -0.403** | (0.185) |\n| | Log Property Tax t-2 | -0.601** | (0.275) |\n| | Log Property Tax t-3 | 0.216 | (0.172) |\n| | Log School Spending | -1.458* | (0.810) |\n| | Log School Spending t-1 | 0.057 | (0.428) |\n| | Log School Spending t-2 | -0.231 | (0.194) |\n| | Log School Spending t-3 | 0.494 | (0.471) |\n\n*Note: *, **, *** indicate significance at the 10%, 5%, and 1% levels, respectively. The long-run elasticity is the sum of the contemporaneous and lagged coefficients.*\n\n### Question\n\nA city council proposes a budget-neutral policy: raise property taxes by 10% to fund a 10% increase in school spending. Based on the long-run elasticities derived from Table 1, select all of the following statements that accurately describe the predicted outcomes of this policy.", "Options": {"A": "The policy is predicted to be substantially more harmful to business properties than to residential properties, reflecting the higher tax sensitivity of business capital.", "B": "The policy is predicted to increase the value of business properties by approximately 1.6%, as businesses respond positively to higher school spending.", "C": "The policy is predicted to increase the value of residential properties, as the positive impact from increased school spending outweighs the negative impact from higher taxes.", "D": "The policy is Pareto-improving because it benefits residents without making business owners significantly worse off."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize multiple empirical results to evaluate a policy trade-off. It uses a Scenario Application strategy. Distractors are designed to trap common errors: (C) a Sign Error, where the negative long-run spending elasticity for businesses is incorrectly treated as positive, and (D) a Conceptual error, misinterpreting the definition of a Pareto-improving policy.", "qid": "227", "question": "### Background\n\n**Research Question.** This case examines the core hypotheses of public finance regarding the capitalization of local fiscal policies. Specifically, it investigates whether residential and business property owners have different preferences for public services versus tax burdens, leading to differential impacts on their respective property values. The analysis tests two primary hypotheses:\n\n- **Hypothesis 1 (H1):** Residential property values are more responsive to changes in school spending than to changes in property taxes.\n- **Hypothesis 2 (H2):** Business property values are more responsive to changes in property taxes than to changes in school spending.\n\n**Setting / Data-Generating Environment.** The study uses a panel of 152 communities in southeast Michigan from 1983-2002. In this setting, public schools are primarily financed by property taxes. The Tiebout model suggests that mobile agents (both residents and firms) sort into communities that offer their preferred tax-service package. Residents, who directly benefit from schools, may value them highly. Firms, modeled as profit-maximizers, may view property taxes as a direct cost of business while receiving little direct benefit from K-12 education.\n\n### Data / Model Specification\n\nThe study estimates a dynamic first-difference 2SLS model to determine the long-run elasticities of property values with respect to fiscal policy. The model includes contemporaneous and three-year lagged values of policy variables to capture the full capitalization effect over time. Table 1 below presents the estimated coefficients for both residential and business property value growth.\n\n**Table 1: Selected 2SLS Regression Coefficients for Property Value Growth**\n\n| Dependent Variable | Regressor | Coefficient | Std. Error |\n| :--- | :--- | :--- | :--- |\n| **ΔLog Residential Property Value** | | | |\n| | Log Property Tax | -0.256*** | (0.040) |\n| | Log Property Tax t-1 | -0.123*** | (0.020) |\n| | Log Property Tax t-2 | -0.029*** | (0.010) |\n| | Log Property Tax t-3 | 0.004 | (0.012) |\n| | Log School Spending | 0.623*** | (0.236) |\n| | Log School Spending t-1 | -0.029 | (0.186) |\n| | Log School Spending t-2 | -0.156 | (0.162) |\n| | Log School Spending t-3 | 0.157 | (0.111) |\n| **ΔLog Business Property Value** | | | |\n| | Log Property Tax | -0.194 | (0.136) |\n| | Log Property Tax t-1 | -0.403** | (0.185) |\n| | Log Property Tax t-2 | -0.601** | (0.275) |\n| | Log Property Tax t-3 | 0.216 | (0.172) |\n| | Log School Spending | -1.458* | (0.810) |\n| | Log School Spending t-1 | 0.057 | (0.428) |\n| | Log School Spending t-2 | -0.231 | (0.194) |\n| | Log School Spending t-3 | 0.494 | (0.471) |\n\n*Note: *, **, *** indicate significance at the 10%, 5%, and 1% levels, respectively. The long-run elasticity is the sum of the contemporaneous and lagged coefficients.*\n\n### Question\n\nA city council proposes a budget-neutral policy: raise property taxes by 10% to fund a 10% increase in school spending. Based on the long-run elasticities derived from Table 1, select all of the following statements that accurately describe the predicted outcomes of this policy.", "model_evaluation_result": {"model_raw_response": "C", "model_answer": ["C"], "error": null}}
{"ID": 194, "Question": "### Background\n\n**Research Question.** This case investigates whether a higher proportion of managing directors (insiders) on the supervisory board impairs firm performance by weakening monitoring effectiveness.\n\n**Setting / Data-Generating Environment.** The study uses a cross-section of 116 Danish listed firms (1998–2001). The Danish “semi-two-tier” system allows managing directors to serve on the supervisory board, though this practice is empirically rare.\n\n---\n\n### Data / Model Specification\n\nThe empirical test is based on the following cross-sectional OLS regression:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot \\text{MANAGERS}_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       | Std. Deviation |\n| :---------- | :--------- | :------------- |\n| MANAGERS (MAN)| 0.0591     | 0.0975         |\n\n**Table 2: Regression of Tobin's Q on Manager Proportion**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Managers               | -0.466      | (0.531)   |\n\n---\n\n### Question\n\nBased on the provided data and regression results, select all correct interpretations of the findings for the `Managers` variable.", "Options": {"A": "The point estimate of the coefficient is so small (-0.466) that it implies a negligible economic effect on firm performance.", "B": "A likely reason for the statistical insignificance is low statistical power, stemming from the fact that the proportion of managers on boards is very low and does not vary much across firms.", "C": "The regression coefficient on `Managers` is statistically insignificant, meaning the data do not provide strong evidence to reject the null hypothesis of no effect.", "D": "The statistically insignificant result definitively proves that having managers on the supervisory board has no effect on firm value."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the nuanced skill of interpreting a statistically insignificant result, particularly the distinction between statistical and economic significance and the role of statistical power. It uses an **Atomic Decomposition** strategy to break the interpretation into testable propositions. The **Reverse-Reasoning** strategy is employed by asking for the cause of the insignificance. Distractors target common fallacies: (A) misinterpreting a failure to reject the null as proof of the null (a classic conceptual error), and (C) misjudging economic significance by looking only at the coefficient's raw value without scaling (a numerical comprehension error).", "qid": "194", "question": "### Background\n\n**Research Question.** This case investigates whether a higher proportion of managing directors (insiders) on the supervisory board impairs firm performance by weakening monitoring effectiveness.\n\n**Setting / Data-Generating Environment.** The study uses a cross-section of 116 Danish listed firms (1998–2001). The Danish “semi-two-tier” system allows managing directors to serve on the supervisory board, though this practice is empirically rare.\n\n---\n\n### Data / Model Specification\n\nThe empirical test is based on the following cross-sectional OLS regression:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot \\text{MANAGERS}_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       | Std. Deviation |\n| :---------- | :--------- | :------------- |\n| MANAGERS (MAN)| 0.0591     | 0.0975         |\n\n**Table 2: Regression of Tobin's Q on Manager Proportion**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Managers               | -0.466      | (0.531)   |\n\n---\n\n### Question\n\nBased on the provided data and regression results, select all correct interpretations of the findings for the `Managers` variable.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 94, "Question": "### Background\n\nThe author argues that mandating central clearing for Over-the-Counter (OTC) derivatives does not solve the Too-Big-To-Fail (TBTF) problem but rather transfers it to a new, potentially more dangerous entity: the Central Counterparty (CCP) itself. The implicit government guarantee to a TBTF institution is economically equivalent to providing its shareholders with a put option on its assets, where the value of this subsidy, `S`, increases with the institution's asset volatility `σ`.\n\n### Scenario\n\nA regulator is evaluating two distinct policy proposals to mitigate the moral hazard associated with TBTF institutions in the derivatives market:\n\n*   **Policy 1 (Clearing Mandate)**: Mandate that all standardized derivatives be cleared through a single, immense CCP. The goal is to eliminate the TBTF status of individual dealer banks.\n*   **Policy 2 (Direct Capital Regulation)**: Directly address TBTF by imposing significantly higher, risk-adjusted capital requirements on all large dealer banks, forcing them to reduce their leverage (debt relative to assets).\n\n### Question\n\nBased on the author's critique and the economic logic of TBTF subsidies, which of the following statements accurately describe the likely consequences of these policies?\n\nSelect all that apply.", "Options": {"A": "Policy 1 (Clearing Mandate) is likely to create a new TBTF entity in the form of the CCP, as its failure would be catastrophic for the entire financial system.", "B": "Policy 1 (Clearing Mandate) effectively eliminates the TBTF problem by mutualizing default risk across all clearing members, thereby removing the need for any government backstop.", "C": "The TBTF subsidy for the CCP created under Policy 1 is likely to be larger than the sum of the individual subsidies of the dealer banks it replaces, due to the extreme concentration of systemic risk.", "D": "Policy 2 (Direct Capital Regulation) directly mitigates the moral hazard of TBTF by reducing the value of the implicit government subsidy for dealer banks."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses the student's ability to synthesize the paper's core argument against clearing mandates as a solution for the TBTF problem. It uses a Scenario Application strategy, asking the user to evaluate the consequences of two competing policies. Correct options (A, B, D) reflect the author's central thesis: clearing mandates transfer and concentrate TBTF risk (A, D), while direct capital regulation is a more effective tool (B). The distractor (C) presents the flawed rationale for clearing mandates that the author explicitly refutes, making it a strong 'Conceptual Opposite' error.", "qid": "94", "question": "### Background\n\nThe author argues that mandating central clearing for Over-the-Counter (OTC) derivatives does not solve the Too-Big-To-Fail (TBTF) problem but rather transfers it to a new, potentially more dangerous entity: the Central Counterparty (CCP) itself. The implicit government guarantee to a TBTF institution is economically equivalent to providing its shareholders with a put option on its assets, where the value of this subsidy, `S`, increases with the institution's asset volatility `σ`.\n\n### Scenario\n\nA regulator is evaluating two distinct policy proposals to mitigate the moral hazard associated with TBTF institutions in the derivatives market:\n\n*   **Policy 1 (Clearing Mandate)**: Mandate that all standardized derivatives be cleared through a single, immense CCP. The goal is to eliminate the TBTF status of individual dealer banks.\n*   **Policy 2 (Direct Capital Regulation)**: Directly address TBTF by imposing significantly higher, risk-adjusted capital requirements on all large dealer banks, forcing them to reduce their leverage (debt relative to assets).\n\n### Question\n\nBased on the author's critique and the economic logic of TBTF subsidies, which of the following statements accurately describe the likely consequences of these policies?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 255, "Question": "### Background\n\n**Research Question.** Is institutional trading in glamour and value stocks driven by superior information about future stock performance?\n\n**Setting / Data-Generating Environment.** At the end of each quarter, stocks are sorted into portfolios based on institutional trading activity. Glamour stocks (lowest book-to-market decile) with positive Trade Ratios (`TR > 0`) form \"bought\" portfolios, and value stocks (highest book-to-market decile) with negative Trade Ratios (`TR < 0`) form \"sold\" portfolios. These portfolios are further sorted into terciles based on the intensity of trading. The study then tracks the subsequent quarterly characteristic-adjusted abnormal returns (`AR`) of these portfolios for four quarters.\n\n### Data / Model Specification\n\nIf institutions are \"smart money,\" their purchases should predict positive abnormal returns, and their sales should predict negative abnormal returns.\n\n**Table 1. Subsequent Quarterly Abnormal Returns (%) of Portfolios Formed on Institutional Trading**\n| Portfolio | AR1 | AR2 | AR3 | AR4 |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A. Glamour Stocks Bought** | | |\n| Most heavily bought group | -0.185 | -1.144*** | -1.468** | -1.616*** |\n| **Panel B. Value Stocks Sold** | | |\n| Most heavily sold group | 3.001*** | 1.186 | 1.523* | 2.325*** |\n\n*Note: ***, **, * denote significance at the 1%, 5%, and 10% levels, respectively.*\n\n---\n\nBased on the data in **Table 1**, which of the following conclusions are supported by the evidence?\n", "Options": {"A": "The glamour stocks that institutions bought most aggressively showed statistically significant underperformance in multiple quarters following the purchase.", "B": "The evidence strongly refutes the hypothesis that institutions' most intense style-level trades (i.e., buying glamour, selling value) are based on superior information about future performance.", "C": "The glamour stocks most heavily bought by institutions immediately and significantly outperformed their benchmarks in the first quarter after being purchased.", "D": "The value stocks that institutions sold most aggressively went on to significantly outperform their benchmarks in the quarter immediately following the sale."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret the key empirical results regarding the informational content of institutional trades. It requires synthesizing data from both panels of the table to form a conclusion about the 'smart money' hypothesis.\nDepth Strategy: Reverse-Reasoning. Given the empirical results, the user must identify all valid conclusions that can be drawn.\nDistractor Logic:\n- A, B, C (Correct): These are direct, accurate interpretations of the data presented in the table. 'A' comes from Panel B (AR1 = 3.001***). 'B' comes from Panel A (AR2, AR3, AR4 are significant and negative). 'C' is the overall conclusion derived from A and B.\n- D (Incorrect): This is a Conceptual Opposite. The AR1 for heavily bought glamour stocks is -0.185 and statistically insignificant, directly contradicting the claim of significant outperformance.", "qid": "255", "question": "### Background\n\n**Research Question.** Is institutional trading in glamour and value stocks driven by superior information about future stock performance?\n\n**Setting / Data-Generating Environment.** At the end of each quarter, stocks are sorted into portfolios based on institutional trading activity. Glamour stocks (lowest book-to-market decile) with positive Trade Ratios (`TR > 0`) form \"bought\" portfolios, and value stocks (highest book-to-market decile) with negative Trade Ratios (`TR < 0`) form \"sold\" portfolios. These portfolios are further sorted into terciles based on the intensity of trading. The study then tracks the subsequent quarterly characteristic-adjusted abnormal returns (`AR`) of these portfolios for four quarters.\n\n### Data / Model Specification\n\nIf institutions are \"smart money,\" their purchases should predict positive abnormal returns, and their sales should predict negative abnormal returns.\n\n**Table 1. Subsequent Quarterly Abnormal Returns (%) of Portfolios Formed on Institutional Trading**\n| Portfolio | AR1 | AR2 | AR3 | AR4 |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A. Glamour Stocks Bought** | | |\n| Most heavily bought group | -0.185 | -1.144*** | -1.468** | -1.616*** |\n| **Panel B. Value Stocks Sold** | | |\n| Most heavily sold group | 3.001*** | 1.186 | 1.523* | 2.325*** |\n\n*Note: ***, **, * denote significance at the 1%, 5%, and 10% levels, respectively.*\n\n---\n\nBased on the data in **Table 1**, which of the following conclusions are supported by the evidence?\n", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 268, "Question": "### Background\n\nThe analysis uses a cross-sectional regression to estimate the impact of flow risk on mutual fund performance while controlling for the endogeneity of investor flows.\n\n### Data / Model Specification\n\nThe baseline model is:\n  \n\\alpha_{i}=\\beta_{0}+\\beta_{1} \\cdot inflow_{i}+\\beta_{2} \\cdot outflow_{i}+\\beta_{3} \\cdot flow\\_risk_{i}+\\dots+\\varepsilon_{i}\n \nwhere `flow_risk` is the mean of monthly absolute net flows. The model is estimated using two-stage least squares (2SLS) with lagged instruments.\n\n**Table 1: Selected Summary Statistics and Regression Results**\n\n| Variable | Mean | Std. Dev. | Coefficient (2SLS) |\n| :--- | :--- | :--- | :--- |\n| `flow_risk` | 0.0294 | 0.0187 | -0.0795 |\n| `inflow` | 0.0376 | 0.0410 | 0.0796 |\n| `outflow` | 0.0310 | 0.0377 | -0.0576 |\n\n---\n\nBased on the model and data in **Table 1**, which of the following conclusions are supported?\n", "Options": {"A": "The negative and significant coefficient on `flow_risk` supports the paper's flow risk hypothesis, indicating that the magnitude of investor flows, regardless of direction, is detrimental to performance.", "B": "The positive and significant coefficient on `inflow` is consistent with a \"smart money\" effect, where investors direct capital to better-performing funds.", "C": "The estimated annualized performance drag for a fund with `flow_risk` one standard deviation above the mean is approximately -1.78%.", "D": "The estimated monthly performance drag for a fund with `flow_risk` one standard deviation above the mean is approximately -0.0795%."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to interpret regression coefficients, calculate their economic significance, and connect them to the underlying economic hypotheses (flow risk and smart money effects). It uses a Computational Judgment strategy.\n\nDistractor Logic:\n- A (Correct): Requires a multi-step calculation: `StdDev(flow_risk) × Coeff(flow_risk) × 12 = 0.0187 × (-0.0795) × 12 ≈ -0.0178` or -1.78%.\n- B (Correct): Requires the correct economic interpretation of the `inflow` coefficient in the context of performance-chasing literature.\n- C (Correct): Requires the correct interpretation of the main coefficient of interest (`flow_risk`) and linking it to the paper's primary hypothesis.\n- D (Incorrect): This is a Unit/Scale Error. It confuses the coefficient's value (the effect of a one-unit change) with the effect of a one-standard-deviation change and also misinterprets the scale (it's -0.149%, not -0.0795%).", "qid": "268", "question": "### Background\n\nThe analysis uses a cross-sectional regression to estimate the impact of flow risk on mutual fund performance while controlling for the endogeneity of investor flows.\n\n### Data / Model Specification\n\nThe baseline model is:\n  \n\\alpha_{i}=\\beta_{0}+\\beta_{1} \\cdot inflow_{i}+\\beta_{2} \\cdot outflow_{i}+\\beta_{3} \\cdot flow\\_risk_{i}+\\dots+\\varepsilon_{i}\n \nwhere `flow_risk` is the mean of monthly absolute net flows. The model is estimated using two-stage least squares (2SLS) with lagged instruments.\n\n**Table 1: Selected Summary Statistics and Regression Results**\n\n| Variable | Mean | Std. Dev. | Coefficient (2SLS) |\n| :--- | :--- | :--- | :--- |\n| `flow_risk` | 0.0294 | 0.0187 | -0.0795 |\n| `inflow` | 0.0376 | 0.0410 | 0.0796 |\n| `outflow` | 0.0310 | 0.0377 | -0.0576 |\n\n---\n\nBased on the model and data in **Table 1**, which of the following conclusions are supported?\n", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 129, "Question": "### Background\n\n**Research Question.** How has the market for corporate directors valued the experience of CEOs from regulated versus non-regulated industries over time? The paper hypothesizes that CEOs in regulated industries (e.g., utilities, banks) historically accumulated human capital that was less relevant to competitive markets, leading to a \"directorship penalty.\" However, a wave of deregulation in the U.S. during the 1990s may have altered this perception.\n\n**Setting and Sample.** The study compares the mean number of outside directorships held by retired CEOs from regulated and non-regulated industries across three time periods.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Mean Difference Between Regulated and Non-Regulated Industries**\n\n| Variable | 1989-1993 | 2001-2005 |\n|:---|---:|---:|\n| **Mean Outside Directorships** | |\n| Non-Regulated Firms | 2.06 | 1.68 |\n| Regulated Firms | 1.07 | 1.78 |\n| **Difference (Non-Regulated - Regulated)** | **0.987*** | **-0.103** |\n\n*Note: *** indicates the difference is significant at the 1% level.*\n\n**Table 2: Mean Outside Directorships within Regulated Industries**\n\n| Regulated Industry | 1989-1993 | 1995-1999 |\n|:---|---:|---:|\n| Depository institution | 1.11 | 2.86 |\n| **Total** | **1.07** | **2.21*** |\n\n*Note: *** indicates the total for 1995-1999 is significantly different from 1989-1993 at the 1% level.*\n\n---\n\n### Question\n\nThe paper documents a significant change in the market's valuation of retired CEOs from regulated industries. Based on the data and the paper's hypothesis, select all statements that accurately describe this phenomenon.", "Options": {"A": "The \"directorship penalty\" observed in the 1989-1993 period, where CEOs from regulated firms held significantly fewer outside directorships than their non-regulated peers, had completely disappeared by the 2001-2005 period.", "B": "The Sarbanes-Oxley Act was the primary driver of this change, as it increased the demand for directors with experience in navigating complex regulatory environments.", "C": "The paper's primary explanation for this trend is that deregulation in sectors like banking forced firms to become more competitive, allowing their CEOs to develop and signal generalizable management skills valued by the broader director market.", "D": "The increase in directorships for regulated-industry CEOs was driven primarily by a surge in demand for leaders from the utility sector, which faced the most significant restructuring."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect a long-term empirical trend with its underlying economic cause (deregulation). It uses an **Atomic Decomposition** strategy, with one option stating the main empirical finding and the other stating the proposed mechanism. The depth strategy is **Scenario Application**, linking a broad economic event to specific data patterns. Distractors test for causal misattribution (C, blaming SOX instead of deregulation, which occurred earlier) and factual inaccuracy based on the provided data (D, as the full paper's Table 6, partially represented here, shows the largest increase was in depository institutions, not utilities).", "qid": "129", "question": "### Background\n\n**Research Question.** How has the market for corporate directors valued the experience of CEOs from regulated versus non-regulated industries over time? The paper hypothesizes that CEOs in regulated industries (e.g., utilities, banks) historically accumulated human capital that was less relevant to competitive markets, leading to a \"directorship penalty.\" However, a wave of deregulation in the U.S. during the 1990s may have altered this perception.\n\n**Setting and Sample.** The study compares the mean number of outside directorships held by retired CEOs from regulated and non-regulated industries across three time periods.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Mean Difference Between Regulated and Non-Regulated Industries**\n\n| Variable | 1989-1993 | 2001-2005 |\n|:---|---:|---:|\n| **Mean Outside Directorships** | |\n| Non-Regulated Firms | 2.06 | 1.68 |\n| Regulated Firms | 1.07 | 1.78 |\n| **Difference (Non-Regulated - Regulated)** | **0.987*** | **-0.103** |\n\n*Note: *** indicates the difference is significant at the 1% level.*\n\n**Table 2: Mean Outside Directorships within Regulated Industries**\n\n| Regulated Industry | 1989-1993 | 1995-1999 |\n|:---|---:|---:|\n| Depository institution | 1.11 | 2.86 |\n| **Total** | **1.07** | **2.21*** |\n\n*Note: *** indicates the total for 1995-1999 is significantly different from 1989-1993 at the 1% level.*\n\n---\n\n### Question\n\nThe paper documents a significant change in the market's valuation of retired CEOs from regulated industries. Based on the data and the paper's hypothesis, select all statements that accurately describe this phenomenon.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 203, "Question": "### Background\n\n**Research Question.** How can the Random Walk Hypothesis be tested in a manner that is robust to heteroscedasticity, and how can this test be adapted to measure changes in market efficiency over time?\n\n**Setting / Data-Generating Environment.** The analysis uses the natural logarithm of daily closing prices for UK financial futures. The Lo-MacKinlay Variance-Ratio (VR) test is employed to assess whether the variance of multi-period returns scales linearly with the time horizon, a key property of a random walk. The sample is split to compare market characteristics before and after the introduction of an electronic trading (ET) system in 1999.\n\n**Variables & Parameters.**\n\n*   `VR(q)`: The variance ratio, comparing the variance of `q`-period returns to `q` times the variance of one-period returns.\n*   `Z(q)`: The standardized test statistic for `VR(q)` under the assumption of homoscedasticity.\n*   `Z*(q)`: The standardized test statistic for `VR(q)` that is robust to heteroscedasticity.\n*   `|VR(q) - 1|`: The absolute deviation of the variance ratio from one, used as a proxy for the degree of market inefficiency.\n\n---\n\n### Data / Model Specification\n\nThe null hypothesis of the Variance-Ratio test is `H₀: VR(q) = 1`, which corresponds to a random walk. The test statistics are asymptotically standard normal, with a 5% critical value of approximately 1.96 for a two-sided test.\n\n**Table 1. Variance-Ratio Test Results for UK Futures**\n\n| Panel | Contract | Lag (q) | Statistic | Value | 5% Critical Value |\n|---|---|---|---|---|---|\n| A | Short Sterling (Before ET) | 2 | `Z(q)` | -4.32 | 1.96 |\n| A | Short Sterling (Before ET) | 2 | `Z*(q)` | -0.64 | 1.96 |\n| B | FTSE100 Futures | 2 | `|VR(q) - 1|` (Before ET) | 0.0067 | N/A |\n| B | FTSE100 Futures | 2 | `|VR(q) - 1|` (Incl. ET) | 0.0013 | N/A |\n\n*Source: Adapted from Table 4 of the source paper. Panel A highlights the difference between standard and robust statistics. Panel B shows the change in the inefficiency metric before and after the introduction of electronic trading (ET).* \n\n---\n\n### Question\n\nBased on the data presented in **Table 1**, select ALL of the following statements that are correct interpretations of the results.", "Options": {"A": "The heteroscedasticity-robust Z*-statistic for Short Sterling (-0.64) provides strong evidence to reject the random walk hypothesis, confirming the market's inefficiency.", "B": "For the Short Sterling contract, the standard Z-statistic (-4.32) would lead to a rejection of the random walk hypothesis at the 5% significance level, while the heteroscedasticity-robust Z*-statistic (-0.64) would not.", "C": "For FTSE100 futures, the increase in the inefficiency metric `|VR(2) - 1|` after the introduction of electronic trading suggests a decline in market efficiency.", "D": "The measure of market inefficiency for FTSE100 futures, `|VR(2) - 1|`, decreased from 0.0067 to 0.0013 in the period including electronic trading, indicating the market became more efficient."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to interpret statistical test results in the context of financial data. It uses an 'Atomic Decomposition' strategy, breaking down the complex source QA into two independently verifiable conclusions from the table. Option A tests the core concept of robust inference (Z vs Z*), while Option B tests the application of the paper's novel efficiency metric. Distractors C and D are designed as 'Conceptual Opposites' that represent common misinterpretations of the test statistics and the efficiency metric, respectively.", "qid": "203", "question": "### Background\n\n**Research Question.** How can the Random Walk Hypothesis be tested in a manner that is robust to heteroscedasticity, and how can this test be adapted to measure changes in market efficiency over time?\n\n**Setting / Data-Generating Environment.** The analysis uses the natural logarithm of daily closing prices for UK financial futures. The Lo-MacKinlay Variance-Ratio (VR) test is employed to assess whether the variance of multi-period returns scales linearly with the time horizon, a key property of a random walk. The sample is split to compare market characteristics before and after the introduction of an electronic trading (ET) system in 1999.\n\n**Variables & Parameters.**\n\n*   `VR(q)`: The variance ratio, comparing the variance of `q`-period returns to `q` times the variance of one-period returns.\n*   `Z(q)`: The standardized test statistic for `VR(q)` under the assumption of homoscedasticity.\n*   `Z*(q)`: The standardized test statistic for `VR(q)` that is robust to heteroscedasticity.\n*   `|VR(q) - 1|`: The absolute deviation of the variance ratio from one, used as a proxy for the degree of market inefficiency.\n\n---\n\n### Data / Model Specification\n\nThe null hypothesis of the Variance-Ratio test is `H₀: VR(q) = 1`, which corresponds to a random walk. The test statistics are asymptotically standard normal, with a 5% critical value of approximately 1.96 for a two-sided test.\n\n**Table 1. Variance-Ratio Test Results for UK Futures**\n\n| Panel | Contract | Lag (q) | Statistic | Value | 5% Critical Value |\n|---|---|---|---|---|---|\n| A | Short Sterling (Before ET) | 2 | `Z(q)` | -4.32 | 1.96 |\n| A | Short Sterling (Before ET) | 2 | `Z*(q)` | -0.64 | 1.96 |\n| B | FTSE100 Futures | 2 | `|VR(q) - 1|` (Before ET) | 0.0067 | N/A |\n| B | FTSE100 Futures | 2 | `|VR(q) - 1|` (Incl. ET) | 0.0013 | N/A |\n\n*Source: Adapted from Table 4 of the source paper. Panel A highlights the difference between standard and robust statistics. Panel B shows the change in the inefficiency metric before and after the introduction of electronic trading (ET).* \n\n---\n\n### Question\n\nBased on the data presented in **Table 1**, select ALL of the following statements that are correct interpretations of the results.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 313, "Question": "### Background\n\n**Research Question.** This study evaluates the evidence for money endogeneity versus exogeneity in the United States, paying close attention to the effects of a major monetary policy regime shift in the mid-1980s. The analysis seeks to synthesize results from multiple econometric tests to form a robust conclusion.\n\n**Setting / Data-Generating Environment.** The analysis uses quarterly time-series data for the US, split into two sub-periods to account for a structural break: US 1 (1975:3–1986:4), a period characterized by monetary aggregate targeting, and US 2 (1987:1–2007:1), a period characterized by interest rate targeting. The study employs Vector Error-Correction Models (VECM) for cointegrated series and trivariate Vector Autoregressions (VAR) for robustness checks.\n\n**Variables & Parameters.**\n- `BL`: Bank Loans\n- `MS`: Money Supply\n- `DEP`: Bank Deposits\n- `ECT`: Error-Correction Term. Its coefficient indicates the speed of adjustment to long-run equilibrium.\n- `→`: Unidirectional Granger causality.\n- `↔`: Bidirectional Granger causality.\n- `EXO`: Conclusion that money supply is Exogenous.\n- `ENDO`: Conclusion that money supply is Endogenous.\n\n---\n\n### Data / Model Specification\n\nEvidence is drawn from three sources: a summary of primary causality tests, detailed VECM estimates, and a trivariate VAR robustness check.\n\n**Table 1: Summary of Causality Test Results for BL and MS**\n\n| Period | Long Run | Conclusion | Monetary Policy |\n| :--- | :--- | :--- | :--- |\n| US 1 | MS → BL | EXO | Monetary Targeting |\n| US 2 | BL ↔ MS | ENDO | Interest Rate Targeting |\n\n**Table 2: VECM Results for US 2 (1987:1–2007:1)**\n\n| DV | INDV | ECT Coefficient | t-Statistic | Long Run Conclusion |\n| :--- | :--- | :--- | :--- | :--- |\n| BL | MS | -0.170 | [-5.825]*** | MS → BL |\n| MS | BL | -0.123 | [-6.919]*** | BL → MS |\n\n*Note: DV = Dependent Variable, INDV = Independent Variable. *** denotes significance at 1%.*\n\n**Table 3: Trivariate VAR Causality Results for US 2 (1987:1–2007:1)**\n\n| DV | INDV | Causal Link |\n| :--- | :--- | :--- |\n| DEP | BL | BL → DEP |\n| DEP | MS | MS ↔ DEP |\n| MS | BL & DEP | BL&DEP → MS |\n\n---\n\n### Question\n\nBased on the provided evidence for the United States, select **all** statements that are correctly supported by the data and analysis.", "Options": {"A": "The VECM results for US 2 show a statistically significant error-correction mechanism where the money supply adjusts to deviations from its long-run equilibrium with bank loans at a speed of 12.3% per quarter.", "B": "In the US 1 period, the long-run causality ran from Bank Loans to Money Supply (BL → MS), consistent with the Post-Keynesian view.", "C": "The trivariate VAR analysis for US 2 weakens the case for endogeneity by showing that bank deposits are an omitted variable that explains the relationship between bank loans and money supply.", "D": "The findings suggest a structural break in the nature of money supply determination, with the US 1 period (monetary targeting) showing evidence for exogenous money and the US 2 period (interest rate targeting) showing evidence for endogenous money."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize findings across different time periods and econometric models for the US case. It uses an Atomic Decomposition strategy, breaking the original QA's conclusions into distinct, verifiable statements. Correct options (A, B) test the core contrast between policy regimes and the interpretation of a key VECM parameter. Distractors are based on common conceptual opposites (C: misinterpreting the robustness check's implication; D: reversing the causality for the US 1 period).", "qid": "313", "question": "### Background\n\n**Research Question.** This study evaluates the evidence for money endogeneity versus exogeneity in the United States, paying close attention to the effects of a major monetary policy regime shift in the mid-1980s. The analysis seeks to synthesize results from multiple econometric tests to form a robust conclusion.\n\n**Setting / Data-Generating Environment.** The analysis uses quarterly time-series data for the US, split into two sub-periods to account for a structural break: US 1 (1975:3–1986:4), a period characterized by monetary aggregate targeting, and US 2 (1987:1–2007:1), a period characterized by interest rate targeting. The study employs Vector Error-Correction Models (VECM) for cointegrated series and trivariate Vector Autoregressions (VAR) for robustness checks.\n\n**Variables & Parameters.**\n- `BL`: Bank Loans\n- `MS`: Money Supply\n- `DEP`: Bank Deposits\n- `ECT`: Error-Correction Term. Its coefficient indicates the speed of adjustment to long-run equilibrium.\n- `→`: Unidirectional Granger causality.\n- `↔`: Bidirectional Granger causality.\n- `EXO`: Conclusion that money supply is Exogenous.\n- `ENDO`: Conclusion that money supply is Endogenous.\n\n---\n\n### Data / Model Specification\n\nEvidence is drawn from three sources: a summary of primary causality tests, detailed VECM estimates, and a trivariate VAR robustness check.\n\n**Table 1: Summary of Causality Test Results for BL and MS**\n\n| Period | Long Run | Conclusion | Monetary Policy |\n| :--- | :--- | :--- | :--- |\n| US 1 | MS → BL | EXO | Monetary Targeting |\n| US 2 | BL ↔ MS | ENDO | Interest Rate Targeting |\n\n**Table 2: VECM Results for US 2 (1987:1–2007:1)**\n\n| DV | INDV | ECT Coefficient | t-Statistic | Long Run Conclusion |\n| :--- | :--- | :--- | :--- | :--- |\n| BL | MS | -0.170 | [-5.825]*** | MS → BL |\n| MS | BL | -0.123 | [-6.919]*** | BL → MS |\n\n*Note: DV = Dependent Variable, INDV = Independent Variable. *** denotes significance at 1%.*\n\n**Table 3: Trivariate VAR Causality Results for US 2 (1987:1–2007:1)**\n\n| DV | INDV | Causal Link |\n| :--- | :--- | :--- |\n| DEP | BL | BL → DEP |\n| DEP | MS | MS ↔ DEP |\n| MS | BL & DEP | BL&DEP → MS |\n\n---\n\n### Question\n\nBased on the provided evidence for the United States, select **all** statements that are correctly supported by the data and analysis.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 220, "Question": "### Background\n\nThe paper argues that the goal of optimal international policy cooperation is often misunderstood. It is not necessarily to eliminate or minimize spillovers. In fact, under certain conditions, a cooperative arrangement would intentionally propagate shocks across borders to achieve a better global outcome.\n\n### Data / Model Specification\n\nThe analysis distinguishes between different types of shocks and the objectives of a global planner.\n\n*   **Risk-Sharing Motive:** In response to a negative **distortionary shock** (e.g., a domestic financial crisis) in one country, a key goal of cooperative policy is to act as a substitute for missing insurance markets. This involves spreading the negative welfare consequences across all countries.\n*   **Efficient Allocation Motive:** In response to a negative **productivity shock** in one country, global efficiency requires that production should fall primarily in that country, as it has become the less efficient place to produce.\n\n### Question\n\nConsider a scenario where the Home country is hit by a severe, negative idiosyncratic shock. According to the paper's principles, which of the following statements accurately describe the optimal cooperative policy response and its effect on spillovers to the Foreign country?\n", "Options": {"A": "If the shock is a negative productivity shock, the risk-sharing motive is irrelevant, and the cooperative policy should aim to fully insulate the Foreign country's consumption from the shock.", "B": "If the shock is a negative productivity shock, the efficient allocation motive implies that the cooperative policy should aim to contain the decline in production primarily within the Home country.", "C": "If the shock is a financial crisis (a distortionary shock), the risk-sharing motive dictates that the cooperative policy should aim to spread the consumption loss to the Foreign country.", "D": "If the shock is a financial crisis (a distortionary shock), the optimal cooperative policy will likely result in a smaller negative spillover to the Foreign country compared to a non-cooperative scenario where the Foreign country actively insulates itself."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the candidate's understanding of the paper's nuanced argument about the role of spillovers in optimal policy cooperation. It uses a Scenario Application strategy, asking the user to apply the paper's principles to specific shock types.\n- **Correct Option A:** This is a direct application of the risk-sharing motive for distortionary shocks. The goal is to share the welfare pain, which means spreading the consumption loss.\n- **Correct Option B:** This is a direct application of the efficient allocation motive for productivity shocks. It is globally efficient for the less productive country to produce less.\n- **Distractor C:** This is a Conceptual Opposite and a key point of the paper. To achieve risk-sharing, the cooperative policy must *transmit* the shock, likely leading to a *larger* negative spillover than a non-cooperative, self-insulating policy.\n- **Distractor D:** This is an Almost Right distractor. While the efficient allocation motive is primary for production, the risk-sharing motive still applies to consumption. Optimal policy would still aim to share some of the consumption risk, not fully insulate the Foreign country.", "qid": "220", "question": "### Background\n\nThe paper argues that the goal of optimal international policy cooperation is often misunderstood. It is not necessarily to eliminate or minimize spillovers. In fact, under certain conditions, a cooperative arrangement would intentionally propagate shocks across borders to achieve a better global outcome.\n\n### Data / Model Specification\n\nThe analysis distinguishes between different types of shocks and the objectives of a global planner.\n\n*   **Risk-Sharing Motive:** In response to a negative **distortionary shock** (e.g., a domestic financial crisis) in one country, a key goal of cooperative policy is to act as a substitute for missing insurance markets. This involves spreading the negative welfare consequences across all countries.\n*   **Efficient Allocation Motive:** In response to a negative **productivity shock** in one country, global efficiency requires that production should fall primarily in that country, as it has become the less efficient place to produce.\n\n### Question\n\nConsider a scenario where the Home country is hit by a severe, negative idiosyncratic shock. According to the paper's principles, which of the following statements accurately describe the optimal cooperative policy response and its effect on spillovers to the Foreign country?\n", "model_evaluation_result": {"model_raw_response": "\n\nB,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 200, "Question": "### Background\n\nAn insurance company's management aims to translate a single, clear operational metric—the combined ratio—into superior shareholder value, defined as achieving a return on equity (ROE) that exceeds the cost of equity. Progressive Insurance, a large U.S. auto insurer, has established a core corporate strategy of maximizing growth subject to maintaining specific targets for its underwriting margin, leverage, and investment policies.\n\n**Key Terms:**\n- `CR`: Combined Ratio, the sum of losses and expenses as a percentage of premium revenue.\n- `P`: Premium Revenue, the total revenue from insurance policies sold.\n- `S`: Surplus (or Capital), the capital base of the insurer.\n- `D`: Debt.\n- `E`: Equity, where `S = E + D`.\n- `A_inv`: Investable Assets, the portfolio of assets generating investment returns.\n- `r_inv`: Rate of return on the investment portfolio.\n- `ROE`: Return on Equity.\n\n---\n\n### Data / Model Specification\n\nProgressive's corporate strategy is defined by four explicit financial policies, summarized in Table 1.\n\n**Table 1: Progressive's Stated Financial Policies**\n\n| Policy Metric | Target Value |\n| :--- | :--- |\n| Combined Ratio (`CR`) | `≤ 96%` |\n| Sales-to-Capital (`P/S`) | `3-to-1` |\n| Debt-to-Capital (`D/S`) | `25%` |\n| Investment Allocation | `85% Bonds / 15% Equities` |\n\nThe Return on Equity for an insurer can be decomposed into contributions from its underwriting and investment activities. A simplified (pre-tax) model is:\n\n  \nROE = \\frac{\\text{Underwriting Income} + \\text{Investment Income}}{E} = \\frac{P \\times (1 - CR)}{E} + \\frac{A_{inv} \\times r_{inv}}{E} \\quad \\text{(Eq. (1))}\n \n\nFor the purpose of this analysis, assume that investable assets (`A_inv`) are composed of the firm's capital (surplus) plus the float generated by its premium collections. A reasonable approximation is `A_inv ≈ P + S`.\n\n---\n\n### Question\n\nBased on the provided data, model, and additional return assumptions (4% pre-tax return on bonds, 10% pre-tax return on equities), select all of the following statements that are mathematically correct.", "Options": {"A": "The component of ROE derived from underwriting profit is 16%.", "B": "The total expected pre-tax ROE is approximately 26.1%.", "C": "The expected return on the investment portfolio (`r_inv`) is 7.0%.", "D": "The ratio of Premiums to Equity (`P/E`) for the firm is 4-to-1."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to execute a multi-step financial derivation based on stated corporate policies. It uses a Computational Judgment strategy via atomic decomposition. The correct options (A, B) are key intermediate steps in the derivation of ROE. Distractor C is based on a common error of taking a simple average instead of a weighted average for the portfolio return. Distractor D is a step-omission error, representing only the investment component of ROE, not the total.", "qid": "200", "question": "### Background\n\nAn insurance company's management aims to translate a single, clear operational metric—the combined ratio—into superior shareholder value, defined as achieving a return on equity (ROE) that exceeds the cost of equity. Progressive Insurance, a large U.S. auto insurer, has established a core corporate strategy of maximizing growth subject to maintaining specific targets for its underwriting margin, leverage, and investment policies.\n\n**Key Terms:**\n- `CR`: Combined Ratio, the sum of losses and expenses as a percentage of premium revenue.\n- `P`: Premium Revenue, the total revenue from insurance policies sold.\n- `S`: Surplus (or Capital), the capital base of the insurer.\n- `D`: Debt.\n- `E`: Equity, where `S = E + D`.\n- `A_inv`: Investable Assets, the portfolio of assets generating investment returns.\n- `r_inv`: Rate of return on the investment portfolio.\n- `ROE`: Return on Equity.\n\n---\n\n### Data / Model Specification\n\nProgressive's corporate strategy is defined by four explicit financial policies, summarized in Table 1.\n\n**Table 1: Progressive's Stated Financial Policies**\n\n| Policy Metric | Target Value |\n| :--- | :--- |\n| Combined Ratio (`CR`) | `≤ 96%` |\n| Sales-to-Capital (`P/S`) | `3-to-1` |\n| Debt-to-Capital (`D/S`) | `25%` |\n| Investment Allocation | `85% Bonds / 15% Equities` |\n\nThe Return on Equity for an insurer can be decomposed into contributions from its underwriting and investment activities. A simplified (pre-tax) model is:\n\n  \nROE = \\frac{\\text{Underwriting Income} + \\text{Investment Income}}{E} = \\frac{P \\times (1 - CR)}{E} + \\frac{A_{inv} \\times r_{inv}}{E} \\quad \\text{(Eq. (1))}\n \n\nFor the purpose of this analysis, assume that investable assets (`A_inv`) are composed of the firm's capital (surplus) plus the float generated by its premium collections. A reasonable approximation is `A_inv ≈ P + S`.\n\n---\n\n### Question\n\nBased on the provided data, model, and additional return assumptions (4% pre-tax return on bonds, 10% pre-tax return on equities), select all of the following statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 232, "Question": "### Background\n\nThe paper's methodology integrates the Ingersoll model for subjective valuation with the Longstaff-Schwartz Least Squares Monte Carlo (LSM) algorithm. This allows for the calculation of three distinct option values: Risk-Neutral, Subjective, and Objective.\n\n### Data / Model Specification\n\nThese values are defined by combining a stochastic process (how the stock price evolves) with an exercise strategy (the rule for when to exercise):\n\n*   `P_RN`: Risk-neutral stock price process.\n*   `P_SUB`: Subjective stock price process (perceived by the executive).\n*   `S_RN`: Optimal exercise strategy for an unconstrained, value-maximizing investor.\n*   `S_SUB`: Optimal exercise strategy for the constrained executive.\n\nLet `Value(Process, Strategy)` be the value of cash flows from a `Strategy` evaluated under a given `Process`.\n\n1.  **Risk-Neutral Value (`V_RN`):** `Value(P_RN, S_RN)`\n2.  **Subjective Value (`V_SUB`):** `Value(P_SUB, S_SUB)`\n3.  **Objective Value (`V_OBJ`):** `Value(P_RN, S_SUB)`\n\nTypically, a constrained executive exercises earlier (suboptimally from a value-maximization perspective) than an unconstrained investor, so `S_SUB ≠ S_RN`.\n\n---\n\nBased on these definitions, select all of the following statements that are correct.\n", "Options": {"A": "The Objective Value (`V_OBJ`) represents the firm's economic cost and is calculated by valuing the cash flows from the executive's actual (suboptimal) exercise strategy using the market's risk-neutral pricing measure.", "B": "The inequality `V_OBJ ≤ V_RN` holds because the risk-neutral value is, by definition, maximized by the optimal exercise strategy `S_RN`, and any other strategy (`S_SUB`) must result in an equal or lower value under the same pricing measure `P_RN`.", "C": "To compute the Objective Value via LSM, one must simulate stock paths under the subjective process (`P_SUB`) and then use those same paths to determine cash flows and discount them.", "D": "The difference between `V_OBJ` and `V_SUB` arises because the firm and the executive apply different pricing measures (`P_RN` vs. `P_SUB`) to the exact same stream of expected cash flows generated by the executive's strategy `S_SUB`."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses the deep conceptual understanding of the paper's core valuation methodology. It uses a Reverse-Reasoning strategy, asking for the principles that lead to the observed valuation hierarchy. Options A, B, and C correctly define the Objective Value, explain the `V_OBJ ≤ V_RN` relationship, and explain the source of the `V_OBJ > V_SUB` wedge, respectively. These are the three central pillars of the valuation framework. Option D is a critical distractor describing an incorrect algorithmic implementation; the correct method is to determine the strategy with `P_SUB` but value the resulting cash flows along parallel paths simulated with `P_RN`.", "qid": "232", "question": "### Background\n\nThe paper's methodology integrates the Ingersoll model for subjective valuation with the Longstaff-Schwartz Least Squares Monte Carlo (LSM) algorithm. This allows for the calculation of three distinct option values: Risk-Neutral, Subjective, and Objective.\n\n### Data / Model Specification\n\nThese values are defined by combining a stochastic process (how the stock price evolves) with an exercise strategy (the rule for when to exercise):\n\n*   `P_RN`: Risk-neutral stock price process.\n*   `P_SUB`: Subjective stock price process (perceived by the executive).\n*   `S_RN`: Optimal exercise strategy for an unconstrained, value-maximizing investor.\n*   `S_SUB`: Optimal exercise strategy for the constrained executive.\n\nLet `Value(Process, Strategy)` be the value of cash flows from a `Strategy` evaluated under a given `Process`.\n\n1.  **Risk-Neutral Value (`V_RN`):** `Value(P_RN, S_RN)`\n2.  **Subjective Value (`V_SUB`):** `Value(P_SUB, S_SUB)`\n3.  **Objective Value (`V_OBJ`):** `Value(P_RN, S_SUB)`\n\nTypically, a constrained executive exercises earlier (suboptimally from a value-maximization perspective) than an unconstrained investor, so `S_SUB ≠ S_RN`.\n\n---\n\nBased on these definitions, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 320, "Question": "### Background\n\n**Research Question.** This study investigates the causal pathway through which board gender diversity affects board effectiveness, testing whether board processes—the internal dynamics and working structures of the board—act as a mediating mechanism.\n\n**Setting / Data-Generating Environment.** The analysis uses cross-sectional survey data from CEOs of 201 Norwegian firms, collected in 2003. A hierarchical multiple regression analysis is used to test for mediation.\n\n**Variables & Parameters.**\n- `Ratio of Women Directors` (X): The independent variable, measured as the percentage of female directors on the board.\n- `Board Processes` (M): A set of mediating variables measured via survey. The two key mediators found to be significant are `Board Development Activities` (formal structures like evaluations and instructions) and `Board Conflict` (perceived disagreements and interpersonal friction).\n- `Board Strategic Control` (Y): The dependent variable, a survey measure of the board's effectiveness in monitoring long-term, qualitative issues like corporate strategy, CSR, and human resources.\n\n---\n\n### Data / Model Specification\n\nThe study's central finding is that the effect of the `Ratio of Women Directors` on `Board Strategic Control` is fully mediated by `Board Processes`. This is established via a three-step hierarchical regression procedure summarized in the stylized table below.\n\n**Table 1: Stylized Hierarchical Regression for Mediation on Strategic Control**\n| | (1) Strategic Control | (2) Strategic Control |\n|:---|:---:|:---:|\n| **Ratio of Women Directors (X)** | **0.45*** | 0.05 |\n| | (0.18) | (0.17) |\n| **Board Processes (M)** | | **0.70*** |\n| | | (0.15) |\n| Control Variables | Included | Included |\n| N | 201 | 201 |\n\n*Note: * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the provided background and **Table 1**, select all statements that represent valid interpretations or critiques of the study's mediation analysis.", "Options": {"A": "The results in Table 1 suggest that the positive effect of a higher ratio of women directors on strategic control is not direct, but rather operates indirectly through improvements in board processes (e.g., reduced conflict, more development activities).", "B": "The study's use of a large sample size (N=201) and control variables effectively mitigates the risk of common method bias, which is primarily a small-sample problem.", "C": "The decrease of the coefficient on 'Ratio of Women Directors' from 0.45 to 0.05 indicates that board processes are a confounding variable that invalidates the initial relationship.", "D": "A major limitation of the study is potential omitted variable bias; for instance, an unobserved factor like a progressive corporate culture could independently drive both the appointment of women directors and the adoption of better board processes."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret a full mediation result from a regression table and identify a key econometric threat to its causal validity. It uses an 'atomic decomposition' strategy, breaking down the original QA's insights into distinct, testable propositions. Option A tests the core interpretation of mediation. Option B tests the understanding of omitted variable bias. Distractor C presents a common misinterpretation of a mediator as a confounder. Distractor D targets a misconception about how to solve common method bias (it is a data collection issue, not a sample size issue).", "qid": "320", "question": "### Background\n\n**Research Question.** This study investigates the causal pathway through which board gender diversity affects board effectiveness, testing whether board processes—the internal dynamics and working structures of the board—act as a mediating mechanism.\n\n**Setting / Data-Generating Environment.** The analysis uses cross-sectional survey data from CEOs of 201 Norwegian firms, collected in 2003. A hierarchical multiple regression analysis is used to test for mediation.\n\n**Variables & Parameters.**\n- `Ratio of Women Directors` (X): The independent variable, measured as the percentage of female directors on the board.\n- `Board Processes` (M): A set of mediating variables measured via survey. The two key mediators found to be significant are `Board Development Activities` (formal structures like evaluations and instructions) and `Board Conflict` (perceived disagreements and interpersonal friction).\n- `Board Strategic Control` (Y): The dependent variable, a survey measure of the board's effectiveness in monitoring long-term, qualitative issues like corporate strategy, CSR, and human resources.\n\n---\n\n### Data / Model Specification\n\nThe study's central finding is that the effect of the `Ratio of Women Directors` on `Board Strategic Control` is fully mediated by `Board Processes`. This is established via a three-step hierarchical regression procedure summarized in the stylized table below.\n\n**Table 1: Stylized Hierarchical Regression for Mediation on Strategic Control**\n| | (1) Strategic Control | (2) Strategic Control |\n|:---|:---:|:---:|\n| **Ratio of Women Directors (X)** | **0.45*** | 0.05 |\n| | (0.18) | (0.17) |\n| **Board Processes (M)** | | **0.70*** |\n| | | (0.15) |\n| Control Variables | Included | Included |\n| N | 201 | 201 |\n\n*Note: * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the provided background and **Table 1**, select all statements that represent valid interpretations or critiques of the study's mediation analysis.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 301, "Question": "### Background\n\n**Research Question.** This case evaluates the potential impact of sample selection criteria on the validity of empirical findings concerning the relationship between CEO compensation and analyst forecasts.\n\n**Setting / Data-Generating Environment.** The study constructs its sample by merging ExecuComp, I/B/E/S, and COMPUSTAT data for U.S. firms from 1993–2003. A key filter is requiring a firm to have at least three analyst forecasts.\n\n**Variables & Parameters.**\n- `ACCURACY_it`: Analyst forecast accuracy for firm `i` in year `t`.\n- `OPTIONS_it`: Level of CEO options pay for firm `i` in year `t`.\n- `IN_SAMPLE_it`: A binary variable, equal to 1 if firm `i` is in the final sample, and 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nTo correct for potential selection bias arising from the requirement of having at least three analysts, a researcher proposes using a Heckman two-stage model. The goal is to get an unbiased estimate of the effect of `OPTIONS` on `ACCURACY`.\n\n---\n\n### Question\n\nWhich of the following are necessary and correct steps in specifying and interpreting this Heckman model? Select all that apply.", "Options": {"A": "The first stage should be a probit or logit model where the dependent variable is `IN_SAMPLE_it`.", "B": "The second-stage regression of `ACCURACY_it` on `OPTIONS_it` and other controls must include the inverse Mills ratio, calculated from the first stage, as an additional regressor.", "C": "The first-stage model must include at least one instrumental variable that is correlated with being in the sample but does not have a direct causal effect on `ACCURACY_it`.", "D": "The second-stage regression of `ACCURACY_it` on `OPTIONS_it` and other controls must include the predicted probability of being in the sample, `P(IN_SAMPLE_it=1)`, as an additional regressor."}, "Answer": ["A", "B", "C"], "pi_justification": "This item tests the procedural knowledge of specifying the Heckman two-stage correction for selection bias, a form of Scenario Application. Options A, B, and C correctly describe the three core components: the first-stage selection model, the need for a valid instrument, and the inclusion of the inverse Mills ratio in the second stage. Option D is a high-fidelity distractor representing a common procedural error (Step-Omission Error), where the predicted probability is incorrectly used instead of the inverse Mills ratio.", "qid": "301", "question": "### Background\n\n**Research Question.** This case evaluates the potential impact of sample selection criteria on the validity of empirical findings concerning the relationship between CEO compensation and analyst forecasts.\n\n**Setting / Data-Generating Environment.** The study constructs its sample by merging ExecuComp, I/B/E/S, and COMPUSTAT data for U.S. firms from 1993–2003. A key filter is requiring a firm to have at least three analyst forecasts.\n\n**Variables & Parameters.**\n- `ACCURACY_it`: Analyst forecast accuracy for firm `i` in year `t`.\n- `OPTIONS_it`: Level of CEO options pay for firm `i` in year `t`.\n- `IN_SAMPLE_it`: A binary variable, equal to 1 if firm `i` is in the final sample, and 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nTo correct for potential selection bias arising from the requirement of having at least three analysts, a researcher proposes using a Heckman two-stage model. The goal is to get an unbiased estimate of the effect of `OPTIONS` on `ACCURACY`.\n\n---\n\n### Question\n\nWhich of the following are necessary and correct steps in specifying and interpreting this Heckman model? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 305, "Question": "### Background\n\n**Research Question.** Does the presence of institutional investors mitigate the geographic diversification discount in REITs, and if so, is this effect driven by their ability to select superior firms ('selection hypothesis') or their power to influence management ('monitoring hypothesis')?\n\n**Setting.** The analysis uses two complementary empirical strategies. First, an interaction model tests whether the diversification discount varies with the level of ownership by different investor types. Second, a subsample analysis examines the discount across firms sorted by characteristics of their institutional ownership base.\n\n**Variables & Parameters.**\n*   `Tobin's q`: The dependent variable, measuring firm value (standardized).\n*   `GeographicDiversification` (`GeoDiv`): Measure of geographic diversification (standardized).\n*   `ActiveInstl Ownership`: Ownership by active institutions (e.g., investment advisors, mutual funds) (standardized).\n*   `Active Instl Number`: The number of distinct active institutions holding shares.\n*   `Instl Ownership Herf`: The Herfindahl Index of institutional ownership concentration. A higher value means more concentrated ownership.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Valuation with Ownership Interactions (Region-based)**\n| Dependent variable: Tobin's q | (1) |\n| :--- | :---: |\n| GeographicDiversification | **-0.168** |\n| | (2.49)** |\n| GeoDiv * Active Instl Ownership | **0.090** |\n| | (2.33)** |\n| Geo Div * Passive Instl Ownership | 0.023 |\n| | (0.52) |\n\n*Note: Table presents selected coefficients from a regression of Tobin's q on diversification, ownership types, and their interactions. All variables are standardized.* \n\n**Table 2. Diversification Discount Across Subgroups (Region-based)**\n| | Low Subsample | High Subsample | F-test p-value (Low vs. High) |\n| :--- | :---: | :---: | :---: |\n| **Panel A: Sort by Active Instl Number** | | |\n| `Geographic Diversification` Coeff. | -0.154 | -0.145* | 0.745 |\n| **Panel B: Sort by Instl Ownership Herf** | | |\n| `Geographic Diversification` Coeff. | -0.233*** | 0.019 | 0.013 |\n\n*Note: Table shows the coefficient on `Geographic Diversification` from separate regressions on subsamples split by the median of the sorting variable.* \n\n---\n\n### Question\n\nBased on the evidence presented in Table 1 and Table 2, which of the following conclusions about the 'monitoring' and 'selection' hypotheses are supported by the data? Select all that apply.", "Options": {"A": "The diversification discount is statistically significant only in the subsample of firms with low institutional ownership concentration, which supports the monitoring hypothesis.", "B": "The magnitude of the diversification discount does not significantly differ based on the number of active institutional investors, which weakens the selection hypothesis.", "C": "The interaction effect in Table 1 (GeoDiv * Active Instl Ownership) conclusively proves the monitoring hypothesis by itself, as active investors are by definition monitors.", "D": "Firms with highly concentrated institutional ownership (high Herfindahl) exhibit a larger diversification discount, suggesting that concentrated owners are risk-averse."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize evidence from both interaction-term and subsample-split regressions to distinguish between the 'monitoring' and 'selection' hypotheses. It uses an Atomic Decomposition strategy, breaking the paper's core causal argument into distinct, verifiable claims. Distractor (B) is an 'Almost Right' overstatement, while distractor (D) is a 'Conceptual Opposite' that directly contradicts the findings in Table 2, Panel B.", "qid": "305", "question": "### Background\n\n**Research Question.** Does the presence of institutional investors mitigate the geographic diversification discount in REITs, and if so, is this effect driven by their ability to select superior firms ('selection hypothesis') or their power to influence management ('monitoring hypothesis')?\n\n**Setting.** The analysis uses two complementary empirical strategies. First, an interaction model tests whether the diversification discount varies with the level of ownership by different investor types. Second, a subsample analysis examines the discount across firms sorted by characteristics of their institutional ownership base.\n\n**Variables & Parameters.**\n*   `Tobin's q`: The dependent variable, measuring firm value (standardized).\n*   `GeographicDiversification` (`GeoDiv`): Measure of geographic diversification (standardized).\n*   `ActiveInstl Ownership`: Ownership by active institutions (e.g., investment advisors, mutual funds) (standardized).\n*   `Active Instl Number`: The number of distinct active institutions holding shares.\n*   `Instl Ownership Herf`: The Herfindahl Index of institutional ownership concentration. A higher value means more concentrated ownership.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Valuation with Ownership Interactions (Region-based)**\n| Dependent variable: Tobin's q | (1) |\n| :--- | :---: |\n| GeographicDiversification | **-0.168** |\n| | (2.49)** |\n| GeoDiv * Active Instl Ownership | **0.090** |\n| | (2.33)** |\n| Geo Div * Passive Instl Ownership | 0.023 |\n| | (0.52) |\n\n*Note: Table presents selected coefficients from a regression of Tobin's q on diversification, ownership types, and their interactions. All variables are standardized.* \n\n**Table 2. Diversification Discount Across Subgroups (Region-based)**\n| | Low Subsample | High Subsample | F-test p-value (Low vs. High) |\n| :--- | :---: | :---: | :---: |\n| **Panel A: Sort by Active Instl Number** | | |\n| `Geographic Diversification` Coeff. | -0.154 | -0.145* | 0.745 |\n| **Panel B: Sort by Instl Ownership Herf** | | |\n| `Geographic Diversification` Coeff. | -0.233*** | 0.019 | 0.013 |\n\n*Note: Table shows the coefficient on `Geographic Diversification` from separate regressions on subsamples split by the median of the sorting variable.* \n\n---\n\n### Question\n\nBased on the evidence presented in Table 1 and Table 2, which of the following conclusions about the 'monitoring' and 'selection' hypotheses are supported by the data? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 309, "Question": "### Background\nAn insurer is analyzing the risk profile of a specific fleet of vehicles. The fleet constitutes 50% of the traffic ($\\rho^{\\Phi}=0.5$) and operates under a moderately aggressive driving configuration, denoted $\\xi^{2a}$. The insurer uses a sophisticated microscopic traffic simulation model to project the aggregate annual loss, $L$, for this fleet. The model accounts for non-uniform accident occurrences, where the likelihood of an accident depends on local traffic conditions.\n\nThe analysis compares different assumptions for the distribution of individual accident losses (Gamma vs. Log-normal) and for the dispersion of those losses, measured by the coefficient of variation ($c_v$). The goal is to understand how these modeling choices impact key statistical functionals and risk measures used for pricing and capital allocation.\n\n### Data / Model Specification\nThe following risk measures are used:\n- **Value-at-Risk (VaR)**: $\\operatorname{VaR}_{p}(L)=\\operatorname*{inf}\\{x\\in\\mathbb{R}\\colon P(L\\leq x)\\geq p\\}$\n- **Expected Shortfall (ES)**: $\\mathrm{ES}_{p}(L)=\\frac{1}{1-p}\\int_{p}^{1}\\mathrm{VaR}_{q}(L)\\mathrm{d}q$\n\nTable 1 presents the simulation results for the aggregate annual loss $L$ under various model specifications. All values are based on 10,000 independent samples of $L$.\n\n**Table 1: Statistical Functionals of L for $\\rho^{\\Phi}=0.5$ and $\\xi^{2a}$**\n| | **Gamma** | | | **Log-normal** | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** |\n| **E(L)** | 1577.8 | 1571.5 | 1578.4 | 1581.7 | 1576.2 | 1582.7 |\n| **Var(L)** | 160,179.5 | 247,943.9 | 626,190.6 | 162,067.5 | 247,069.8 | 614,713.4 |\n| **Skewness** | 0.333 | 0.561 | 0.993 | 0.377 | 0.660 | 2.152 |\n| **VaR$_{0.90}$(L)** | 2111.9 | 2219.1 | 2634.2 | 2112.9 | 2242.4 | 2526.6 |\n| **ES$_{0.90}$(L)** | 2331.7 | 2538.8 | 3233.3 | 2342.4 | 2557.5 | 3280.3 |\n| **VaR$_{0.95}$(L)** | 2275.9 | 2471.3 | 3070.5 | 2288.4 | 2468.1 | 2996.2 |\n| **ES$_{0.95}$(L)** | 2468.9 | 2748.0 | 3628.9 | 2491.4 | 2772.5 | 3838.4 |\n| **VaR$_{0.99}$(L)** | 2587.3 | 2891.3 | 3999.6 | 2608.8 | 2943.8 | 4228.6 |\n| **ES$_{0.99}$(L)** | 2755.9 | 3188.6 | 4490.2 | 2809.0 | 3234.4 | 5424.2 |\n\n### Question\nBased on the data in Table 1, which of the following statements are correct?", "Options": {"A": "Across all scenarios presented, the expected annual loss, E(L), is highly sensitive to the choice of the coefficient of variation ($c_v$).", "B": "For the Gamma loss model, increasing the coefficient of variation ($c_v$) from 0.5 to 2.0 results in a more than 250% increase in the variance of the aggregate loss, Var(L).", "C": "When assuming the highest dispersion ($c_v=2.0$), switching the loss model from Gamma to Log-normal increases the required capital to cover the worst 1% of outcomes (as measured by ES$_{0.99}$) by more than 900.", "D": "For the Log-normal distribution with $c_v=1.0$, the 95% Value-at-Risk (VaR$_{0.95}$) is greater than the 90% Expected Shortfall (ES$_{0.90}$)."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret tabular data, perform comparative calculations, and draw valid conclusions about risk drivers. It uses an 'Atomic Decomposition' strategy, breaking down the insights from the original multi-part question into distinct, verifiable statements. Distractors are designed based on conceptual misinterpretation (confusing E(L) sensitivity) and misreading of table values (comparing VaR and ES incorrectly).", "qid": "309", "question": "### Background\nAn insurer is analyzing the risk profile of a specific fleet of vehicles. The fleet constitutes 50% of the traffic ($\\rho^{\\Phi}=0.5$) and operates under a moderately aggressive driving configuration, denoted $\\xi^{2a}$. The insurer uses a sophisticated microscopic traffic simulation model to project the aggregate annual loss, $L$, for this fleet. The model accounts for non-uniform accident occurrences, where the likelihood of an accident depends on local traffic conditions.\n\nThe analysis compares different assumptions for the distribution of individual accident losses (Gamma vs. Log-normal) and for the dispersion of those losses, measured by the coefficient of variation ($c_v$). The goal is to understand how these modeling choices impact key statistical functionals and risk measures used for pricing and capital allocation.\n\n### Data / Model Specification\nThe following risk measures are used:\n- **Value-at-Risk (VaR)**: $\\operatorname{VaR}_{p}(L)=\\operatorname*{inf}\\{x\\in\\mathbb{R}\\colon P(L\\leq x)\\geq p\\}$\n- **Expected Shortfall (ES)**: $\\mathrm{ES}_{p}(L)=\\frac{1}{1-p}\\int_{p}^{1}\\mathrm{VaR}_{q}(L)\\mathrm{d}q$\n\nTable 1 presents the simulation results for the aggregate annual loss $L$ under various model specifications. All values are based on 10,000 independent samples of $L$.\n\n**Table 1: Statistical Functionals of L for $\\rho^{\\Phi}=0.5$ and $\\xi^{2a}$**\n| | **Gamma** | | | **Log-normal** | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** |\n| **E(L)** | 1577.8 | 1571.5 | 1578.4 | 1581.7 | 1576.2 | 1582.7 |\n| **Var(L)** | 160,179.5 | 247,943.9 | 626,190.6 | 162,067.5 | 247,069.8 | 614,713.4 |\n| **Skewness** | 0.333 | 0.561 | 0.993 | 0.377 | 0.660 | 2.152 |\n| **VaR$_{0.90}$(L)** | 2111.9 | 2219.1 | 2634.2 | 2112.9 | 2242.4 | 2526.6 |\n| **ES$_{0.90}$(L)** | 2331.7 | 2538.8 | 3233.3 | 2342.4 | 2557.5 | 3280.3 |\n| **VaR$_{0.95}$(L)** | 2275.9 | 2471.3 | 3070.5 | 2288.4 | 2468.1 | 2996.2 |\n| **ES$_{0.95}$(L)** | 2468.9 | 2748.0 | 3628.9 | 2491.4 | 2772.5 | 3838.4 |\n| **VaR$_{0.99}$(L)** | 2587.3 | 2891.3 | 3999.6 | 2608.8 | 2943.8 | 4228.6 |\n| **ES$_{0.99}$(L)** | 2755.9 | 3188.6 | 4490.2 | 2809.0 | 3234.4 | 5424.2 |\n\n### Question\nBased on the data in Table 1, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 319, "Question": "### Background\n\nThe study investigates the factors driving a remuneration committee's choice between different long-term incentive instruments, primarily executive stock options versus Long-Term Incentive Plans (LTIPs) based on relative Total Shareholder Return (TSR).\n\n### Data / Model Specification\n\n> **Executive Share Option:** Payoff is `max(S_T - K, 0)`, where `S_T` is the stock price at maturity and `K` is the strike price. The reward is based on the absolute increase in share price.\n\n> **Relative TSR LTIP:** Payoff depends on the company's TSR ranking against a peer group. This filters out common market-wide or industry-wide stock price movements.\n\n> **Evidence (Consultant):** \"...option schemes amongst public companies are now back in favour, as it were.\"\n\n> **Evidence (CEO & NXD):** Committee members' choices were influenced by their \"personal wishes\" and whether a scheme was the \"flavour of the moment.\"\n\n### Question\n\nWhich of the following statements represent valid conclusions or applications of the principles discussed regarding the choice between executive stock options and relative TSR LTIPs?\n\nSelect all that apply.", "Options": {"A": "An executive stock option effectively filters out broad market trends, rewarding managers only for firm-specific performance.", "B": "A grant of at-the-money stock options is economically equivalent to a grant of restricted shares under an LTIP, provided their grant-date fair values are identical.", "C": "The evidence suggests that mimetic isomorphism—adopting practices that are currently 'in favour'—is a significant factor in the choice of incentive schemes.", "D": "A relative TSR LTIP is designed to reward managers for outperforming peers, meaning it can pay out even if the absolute share price falls, as long as it falls less than the peer group average."}, "Answer": ["C", "D"], "pi_justification": "This item assesses understanding of the different incentive properties of options vs. LTIPs and the institutional factors influencing their selection. It uses a 'Reverse-Reasoning' strategy, asking for valid conclusions from the provided facts. Option A correctly describes the function of relative performance evaluation in an LTIP. Option C correctly identifies the institutional theory concept (isomorphism) described by the interviewees. Distractor B is a 'Conceptual Opposite', as options reward absolute performance, not relative. Distractor D is a sophisticated 'Conceptual Distractor' that equates accounting value with economic substance, ignoring the fundamental differences in payoff structure and risk exposure, a key distinction.", "qid": "319", "question": "### Background\n\nThe study investigates the factors driving a remuneration committee's choice between different long-term incentive instruments, primarily executive stock options versus Long-Term Incentive Plans (LTIPs) based on relative Total Shareholder Return (TSR).\n\n### Data / Model Specification\n\n> **Executive Share Option:** Payoff is `max(S_T - K, 0)`, where `S_T` is the stock price at maturity and `K` is the strike price. The reward is based on the absolute increase in share price.\n\n> **Relative TSR LTIP:** Payoff depends on the company's TSR ranking against a peer group. This filters out common market-wide or industry-wide stock price movements.\n\n> **Evidence (Consultant):** \"...option schemes amongst public companies are now back in favour, as it were.\"\n\n> **Evidence (CEO & NXD):** Committee members' choices were influenced by their \"personal wishes\" and whether a scheme was the \"flavour of the moment.\"\n\n### Question\n\nWhich of the following statements represent valid conclusions or applications of the principles discussed regarding the choice between executive stock options and relative TSR LTIPs?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 307, "Question": "### Background\n\n**Research Question.** Does diversification affect the market valuation of Real Estate Investment Trusts (REITs)? Answering this requires first understanding the typical diversification strategy of REITs and then estimating the relationship between diversification and value.\n\n**Setting.** The analysis uses summary statistics to profile the sample and a baseline regression model to estimate the diversification discount. All variables in the regression are standardized to have a mean of zero and a standard deviation of one.\n\n**Variables & Parameters.**\n*   `PropertyTypeDiversification`: Measure of diversification across property types (Negative Herfindahl Index).\n*   `GeographicDiversification(Region)`: Measure of diversification across geographic regions (Negative Herfindahl Index).\n*   `Tobin's q`: The dependent variable in the regression, measuring firm value.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Summary Statistics (Selected Variables)**\n| Variable | Mean | Median |\n| :--- | :--- | :--- |\n| PropertyTypeDiversification | -0.837 | -0.965 |\n| GeographicDiversification(Region) | -0.451 | -0.357 |\n\n*Note: A Herfindahl Index of 1.0 (perfect concentration) corresponds to a diversification score of -1.0.*\n\n**Table 2. The Relation Between Diversification and Firm Value**\n| Dependent variable: Tobin's q | (1) Regions |\n| :--- | :---: |\n| **Geographic Diversification** | **-0.159** |\n| | (2.35)** |\n| **PropType Diversification** | **-0.032** |\n| | (0.72) |\n\n*Note: t-statistics in parentheses. ** denotes significance at the 5% level.* \n\n---\n\n### Question\n\nBased on the descriptive statistics in Table 1 and regression results in Table 2, which of the following statements about REIT diversification strategies and their valuation impact are correct? Select all that apply.", "Options": {"A": "The coefficient of -0.159 on Geographic Diversification indicates a diversification premium, as firms become more focused (less diversified), their value increases.", "B": "The typical REIT in the sample is highly diversified across both property types and geographic regions, as shown in Table 1.", "C": "The data shows a statistically significant geographic diversification discount, where a one standard deviation increase in diversification is associated with a 0.159 standard deviation decrease in Tobin's q.", "D": "The lack of a significant valuation effect for property type diversification is likely attributable to the high concentration (low variation) in this dimension, as shown by the median PropertyTypeDiversification score of -0.965 in Table 1."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize descriptive statistics with baseline regression results to understand both a key finding (the geographic discount) and a key null result. It uses an Atomic Decomposition strategy. Distractor (B) is a 'Conceptual Opposite' regarding property type diversification. Distractor (C) is an 'Almost Right' conceptual error that mislabels the nature of the discount, a common point of confusion.", "qid": "307", "question": "### Background\n\n**Research Question.** Does diversification affect the market valuation of Real Estate Investment Trusts (REITs)? Answering this requires first understanding the typical diversification strategy of REITs and then estimating the relationship between diversification and value.\n\n**Setting.** The analysis uses summary statistics to profile the sample and a baseline regression model to estimate the diversification discount. All variables in the regression are standardized to have a mean of zero and a standard deviation of one.\n\n**Variables & Parameters.**\n*   `PropertyTypeDiversification`: Measure of diversification across property types (Negative Herfindahl Index).\n*   `GeographicDiversification(Region)`: Measure of diversification across geographic regions (Negative Herfindahl Index).\n*   `Tobin's q`: The dependent variable in the regression, measuring firm value.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Summary Statistics (Selected Variables)**\n| Variable | Mean | Median |\n| :--- | :--- | :--- |\n| PropertyTypeDiversification | -0.837 | -0.965 |\n| GeographicDiversification(Region) | -0.451 | -0.357 |\n\n*Note: A Herfindahl Index of 1.0 (perfect concentration) corresponds to a diversification score of -1.0.*\n\n**Table 2. The Relation Between Diversification and Firm Value**\n| Dependent variable: Tobin's q | (1) Regions |\n| :--- | :---: |\n| **Geographic Diversification** | **-0.159** |\n| | (2.35)** |\n| **PropType Diversification** | **-0.032** |\n| | (0.72) |\n\n*Note: t-statistics in parentheses. ** denotes significance at the 5% level.* \n\n---\n\n### Question\n\nBased on the descriptive statistics in Table 1 and regression results in Table 2, which of the following statements about REIT diversification strategies and their valuation impact are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C", "model_answer": ["C"], "error": null}}
{"ID": 4, "Question": "### Background\n\n**Research Question.** This case critically evaluates whether the reported returns for different anomaly strategies under different weather states are consistent with the absence of arbitrage, a cornerstone of financial theory.\n\n**Setting / Data-Generating Environment.** Consider a simplified two-state economy where the state is either 'Hot' (`H`) or 'Cold' (`L`), occurring with equal probability (`P(H) = P(L) = 0.5`). The no-arbitrage principle implies the existence of a positive Stochastic Discount Factor (SDF), `m = [m_H, m_L]`, that prices all excess returns, meaning `E[m R^e] = 0.5 m_H E[R^e|H] + 0.5 m_L E[R^e|L] = 0` for any strategy.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Conditional Monthly Excess Returns (%)**\n\n| Strategy | `E[R^e|H]` (Return in Hot state) | `E[R^e|L]` (Return in Cold state) |\n| :--- | :--- | :--- |\n| Size | -0.60 | 1.05 |\n| Return on Assets (ROA) | 1.62 | -0.05 |\n\n---\n\n### Question\n\nBased on the no-arbitrage condition and the data in Table 1, select all of the following conclusions that are correct.", "Options": {"A": "It is impossible to find a single, non-trivial SDF that prices both the Size and ROA strategies simultaneously, implying an arbitrage opportunity exists.", "B": "The ratio of SDF states (`m_H / m_L`) required to price the Return on Assets (ROA) strategy is -32.4.", "C": "The ratio of SDF states (`m_H / m_L`) required to price the Size strategy is 1.75.", "D": "A valid SDF exists where `m_H = 1.0` and `m_L = 1.0`, because the unconditional expected returns of both strategies are positive."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Computational Judgment strategy to test the high-difficulty arbitrage question from the source QA. It requires students to set up and solve the SDF pricing equation for each asset and then compare the results. Option A is a correct calculation for one asset. Option B is the correct final conclusion that the system is inconsistent. Distractor C is a Sign Error in the calculation for the ROA strategy. Distractor D represents a fundamental Conceptual Error, ignoring the conditional information and the core pricing equation.", "qid": "4", "question": "### Background\n\n**Research Question.** This case critically evaluates whether the reported returns for different anomaly strategies under different weather states are consistent with the absence of arbitrage, a cornerstone of financial theory.\n\n**Setting / Data-Generating Environment.** Consider a simplified two-state economy where the state is either 'Hot' (`H`) or 'Cold' (`L`), occurring with equal probability (`P(H) = P(L) = 0.5`). The no-arbitrage principle implies the existence of a positive Stochastic Discount Factor (SDF), `m = [m_H, m_L]`, that prices all excess returns, meaning `E[m R^e] = 0.5 m_H E[R^e|H] + 0.5 m_L E[R^e|L] = 0` for any strategy.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Conditional Monthly Excess Returns (%)**\n\n| Strategy | `E[R^e|H]` (Return in Hot state) | `E[R^e|L]` (Return in Cold state) |\n| :--- | :--- | :--- |\n| Size | -0.60 | 1.05 |\n| Return on Assets (ROA) | 1.62 | -0.05 |\n\n---\n\n### Question\n\nBased on the no-arbitrage condition and the data in Table 1, select all of the following conclusions that are correct.", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: print(\"For Size: ratio = 1.05 / -0.60\")\nprint(1.05...\n\n\nC", "model_answer": ["C"], "error": null}}
{"ID": 262, "Question": "### Background\n\nThis case investigates internal tensions within family-controlled firms in Taiwan's \"insider-oriented\" corporate governance system. In such systems, the primary agency conflict (Type II) is between controlling shareholders (the family) and minority shareholders. This contrasts with \"outsider systems\" where the main conflict (Type I) is between professional managers and dispersed shareholders. As Taiwanese firms hire professional managers, a hybrid conflict emerges.\n\n### Data / Model Specification\n\n**Hypothesis Two:** Company directors view the level of family control in Taiwan as a hindrance to good corporate governance and as an area where improvements and controls are necessary.\n\nRelevant empirical findings from the survey include:\n- 65% of responding directors believe that directors are selected by family members.\n- Directors strongly disagree with the statement \"Outside directors should have family ties to other directors, chairman, and CEO\" (mean score of 2.11 on a 7-point scale where 1=strongly disagree).\n\n### Question\n\nBased on the provided context and empirical findings, which of the following are valid interpretations supporting Hypothesis Two? Select all that apply.", "Options": {"A": "The data reveals a paradox where directors, though often appointed by families, recognize the need for professional checks and balances on family power.", "B": "The findings indicate that directors believe the primary agency problem in Taiwan is the Type I conflict between managers and shareholders, not the Type II conflict involving family control.", "C": "The strong opposition to family ties for outside directors suggests a desire for objective, independent oversight to counterbalance the influence of the controlling family.", "D": "The fact that 65% of directors are selected by families implies that directors are fully aligned with family interests and see no need for governance reform."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to synthesize qualitative and quantitative information to interpret an apparent paradox, which was the core of the original QA problem. It uses a Reverse-Reasoning strategy. Option A is a direct, valid interpretation of the low mean score on the 'family ties' question. Option B correctly identifies and explains the paradox that directors appointed by families still want independent oversight, which strongly supports the hypothesis. Distractor C is a conceptual error, as the text explicitly states the Type II conflict is primary in insider systems. Distractor D is a plausible but incorrect inference; it ignores the countervailing evidence from the 'family ties' question and misinterprets the central tension.", "qid": "262", "question": "### Background\n\nThis case investigates internal tensions within family-controlled firms in Taiwan's \"insider-oriented\" corporate governance system. In such systems, the primary agency conflict (Type II) is between controlling shareholders (the family) and minority shareholders. This contrasts with \"outsider systems\" where the main conflict (Type I) is between professional managers and dispersed shareholders. As Taiwanese firms hire professional managers, a hybrid conflict emerges.\n\n### Data / Model Specification\n\n**Hypothesis Two:** Company directors view the level of family control in Taiwan as a hindrance to good corporate governance and as an area where improvements and controls are necessary.\n\nRelevant empirical findings from the survey include:\n- 65% of responding directors believe that directors are selected by family members.\n- Directors strongly disagree with the statement \"Outside directors should have family ties to other directors, chairman, and CEO\" (mean score of 2.11 on a 7-point scale where 1=strongly disagree).\n\n### Question\n\nBased on the provided context and empirical findings, which of the following are valid interpretations supporting Hypothesis Two? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 114, "Question": "### Background\n\nA model of entrepreneurial finance examines how government credit guarantees affect entrepreneurial effort in a market with moral hazard. An entrepreneur with insufficient collateral seeks financing for a project. The interaction between the entrepreneur's incentive compatibility (IC) and the bank's participation constraint (PC) determines the equilibrium contract and effort level.\n\n### Data / Model Specification\n\nThe model identifies a critical threshold for the guarantee strength, `φ`, defined as:\n\n  \n\\hat{\\phi}(Q) \\equiv 1 - \\frac{A}{I-Q} \n \nwhere `A` is the entrepreneur's collateral and `I-Q` is the loan amount.\n\n- If `φ < hat(φ)(Q)`, the entrepreneur pledges full collateral (`β=1`) and effort `e` is *increasing* in `φ`.\n- If `φ ≥ hat(φ)(Q)`, the entrepreneur pledges partial collateral (`β < 1`) and effort `e` is *decreasing* in `φ`.\n\nCredit rationing occurs if `φ` is below a minimum level `underline(φ)`.\n\n### Question\n\nAccording to the model, which of the following statements accurately describe the mechanisms driving credit rationing and the effects of credit guarantees?\n\nSelect all that apply.", "Options": {"A": "For low levels of guarantee (`φ < hat(φ)(Q)`), an increase in `φ` allows the bank to offer better terms (a lower `R`), which strengthens the entrepreneur's incentive to exert effort.", "B": "For high levels of guarantee (`φ ≥ hat(φ)(Q)`), the guarantee is so effective that the entrepreneur substitutes this 'public collateral' for their own private collateral, which weakens their personal incentive to succeed and causes effort to fall.", "C": "Entrepreneurial effort is monotonically increasing in the guarantee level `φ`, as a stronger guarantee always reduces the entrepreneur's cost of capital.", "D": "Credit rationing occurs because a bank's demand for a high success-state repayment `R` (to cover its risk from low collateral) reduces the entrepreneur's incentive to exert effort, making it impossible to satisfy both parties' constraints simultaneously."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the student's understanding of the paper's core theoretical mechanism: the cause of credit rationing and the non-monotonic effect of guarantees on entrepreneurial effort. Depth Strategy: Reverse-Reasoning. The student must identify the multiple valid causal explanations for the model's key outcomes. Distractor Logic: Option D is a conceptual opposite, contradicting the central non-monotonic finding of the paper. It represents a plausible but incorrect simplification of the model's results. The correct options (A, B, C) each describe a distinct and correct part of the model's logic presented in the original QA problem.", "qid": "114", "question": "### Background\n\nA model of entrepreneurial finance examines how government credit guarantees affect entrepreneurial effort in a market with moral hazard. An entrepreneur with insufficient collateral seeks financing for a project. The interaction between the entrepreneur's incentive compatibility (IC) and the bank's participation constraint (PC) determines the equilibrium contract and effort level.\n\n### Data / Model Specification\n\nThe model identifies a critical threshold for the guarantee strength, `φ`, defined as:\n\n  \n\\hat{\\phi}(Q) \\equiv 1 - \\frac{A}{I-Q} \n \nwhere `A` is the entrepreneur's collateral and `I-Q` is the loan amount.\n\n- If `φ < hat(φ)(Q)`, the entrepreneur pledges full collateral (`β=1`) and effort `e` is *increasing* in `φ`.\n- If `φ ≥ hat(φ)(Q)`, the entrepreneur pledges partial collateral (`β < 1`) and effort `e` is *decreasing* in `φ`.\n\nCredit rationing occurs if `φ` is below a minimum level `underline(φ)`.\n\n### Question\n\nAccording to the model, which of the following statements accurately describe the mechanisms driving credit rationing and the effects of credit guarantees?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 224, "Question": "### Background\nA central bank's monetary policy objectives may not be constant over time. A key hypothesis is that the Bank of Canada (BoC) first focused on establishing credibility by bringing inflation down (the \"disinflation period\") and then, once inflation was stable, shifted its focus toward stabilizing the real economy (the \"post-disinflation period\"). This suggests a structural change in its policy reaction function.\n\n### Data / Model Specification\nThe baseline forward-looking policy rule is given by:\n\n  \ni_{t}=c+\\rho i_{t-1}+\\phi_{r}r_{t}+\\phi_{\\pi}E_{t}(\\pi_{t+h_{\\pi}}-\\pi_{t+h_{\\pi}}^{*})+\\phi_{x}E_{t}(x_{t+h_{x}})+\\epsilon_{t}\n \n\nwhere $\\rho$ is the interest rate smoothing parameter, $\\phi_{\\pi}$ is the short-run response to the expected inflation gap, and $\\phi_x$ is the short-run response to the expected output gap. The long-run response to inflation, which determines if the Taylor principle is met, is $\\frac{\\phi_{\\pi}}{1-\\rho}$.\n\nTo test for a structural shift, the model is estimated over two distinct sub-periods. Table 1 presents the results for the disinflation period (1991Q1–1994Q4) and the post-disinflation period (1995Q1–2015Q4).\n\n**Table 1: Disinflation vs. Post-disinflation Period Estimates**\n| Variables | 1991Q1-1994Q4 | 1995Q1-2015Q4 |\n| :--- | :--- | :--- |\n| $\\rho$: Lagged Int. Rate | 0.554** | 0.899*** |\n| | (0.179) | (0.027) |\n| $\\phi_{\\pi}$: Exp. Infl. Gap | 0.927** | 0.329 |\n| | (0.414) | (0.219) |\n| $\\phi_x$: Exp. Output Gap | -0.493 | 0.204*** |\n| | (0.328) | (0.045) |\n*Note: Newey-West HAC standard errors in parentheses. ***, ** denote significance at 1% and 5% levels.*\n\nBased on the evidence in Table 1, which of the following conclusions about the structural shift in the BoC's monetary policy are supported by the data?\n", "Options": {"A": "The response to the expected inflation gap was statistically significant in the disinflation period but became statistically insignificant in the post-disinflation period.", "B": "In the post-disinflation period, the BoC's focus shifted towards stabilizing the real economy, as evidenced by the positive and highly statistically significant coefficient on the expected output gap.", "C": "Policy inertia, as measured by the coefficient on the lagged interest rate ($\\rho$), decreased significantly from the disinflation to the post-disinflation period.", "D": "During the disinflation period, the BoC's policy satisfied the Taylor principle, with a long-run response to inflation of approximately 2.08."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to analyze and synthesize regression results from two different time periods to identify a structural change in policy. It requires calculation, interpretation of statistical significance, and understanding of the economic narrative presented in the paper. Depth Strategy: Reverse-Reasoning. Given the empirical results (the outcome), the user must identify the plausible conclusions about the underlying policy shift. Distractor Logic: Option D is a conceptual distractor (Conceptual Opposite). The coefficient on the lagged interest rate, $\\rho$, *increased* from 0.554 to 0.899, indicating that policy inertia became substantially higher, not lower, in the later period. This is a plausible error for a user who misinterprets the meaning of the coefficient.", "qid": "224", "question": "### Background\nA central bank's monetary policy objectives may not be constant over time. A key hypothesis is that the Bank of Canada (BoC) first focused on establishing credibility by bringing inflation down (the \"disinflation period\") and then, once inflation was stable, shifted its focus toward stabilizing the real economy (the \"post-disinflation period\"). This suggests a structural change in its policy reaction function.\n\n### Data / Model Specification\nThe baseline forward-looking policy rule is given by:\n\n  \ni_{t}=c+\\rho i_{t-1}+\\phi_{r}r_{t}+\\phi_{\\pi}E_{t}(\\pi_{t+h_{\\pi}}-\\pi_{t+h_{\\pi}}^{*})+\\phi_{x}E_{t}(x_{t+h_{x}})+\\epsilon_{t}\n \n\nwhere $\\rho$ is the interest rate smoothing parameter, $\\phi_{\\pi}$ is the short-run response to the expected inflation gap, and $\\phi_x$ is the short-run response to the expected output gap. The long-run response to inflation, which determines if the Taylor principle is met, is $\\frac{\\phi_{\\pi}}{1-\\rho}$.\n\nTo test for a structural shift, the model is estimated over two distinct sub-periods. Table 1 presents the results for the disinflation period (1991Q1–1994Q4) and the post-disinflation period (1995Q1–2015Q4).\n\n**Table 1: Disinflation vs. Post-disinflation Period Estimates**\n| Variables | 1991Q1-1994Q4 | 1995Q1-2015Q4 |\n| :--- | :--- | :--- |\n| $\\rho$: Lagged Int. Rate | 0.554** | 0.899*** |\n| | (0.179) | (0.027) |\n| $\\phi_{\\pi}$: Exp. Infl. Gap | 0.927** | 0.329 |\n| | (0.414) | (0.219) |\n| $\\phi_x$: Exp. Output Gap | -0.493 | 0.204*** |\n| | (0.328) | (0.045) |\n*Note: Newey-West HAC standard errors in parentheses. ***, ** denote significance at 1% and 5% levels.*\n\nBased on the evidence in Table 1, which of the following conclusions about the structural shift in the BoC's monetary policy are supported by the data?\n", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 308, "Question": "### Background\n\n**Research Question.** To analyze the economic consequences of diversification, one must first have a reliable way to quantify it. The Herfindahl Index (HI) is a standard measure of concentration used in economics and finance.\n\n**Setting.** The analysis considers diversification for a sample of equity REITs along two dimensions: property type and geographic location. The study transforms the standard HI to create a more intuitive measure for its regression analysis.\n\n---\n\n### Data / Model Specification\n\nThe Herfindahl Index is defined as the sum of the squared proportions of assets in each category:\n  \n\\mathrm{Herfindahl Index} \\, (HI) = \\sum_{i=1}^{I} P_{i}^{2} \n \n**Eq. (1)**\n\nwhere `P_i` is the proportion of assets in category `i`.\n\nIn the subsequent regression analysis, the measure for diversification is constructed as the negative of the Herfindahl Index:\n  \n\\text{Geographic Diversification} = -HI_{\\text{geographic}}\n \n**Eq. (2)**\n\n---\n\n### Question\n\nA hypothetical REIT has its assets distributed across three geographic regions: 50% in the Northeast, 30% in the Pacific, and 20% in the Southeast. Based on the provided definitions, which of the following statements are correct? Select all that apply.", "Options": {"A": "A positive and significant coefficient on the `Geographic Diversification` variable would indicate that more diversified firms have lower value.", "B": "To find evidence of a 'diversification discount,' the regression analysis would need to find a statistically significant negative coefficient on the `Geographic Diversification` variable.", "C": "The value of the `Geographic Diversification` variable used in the regression for this REIT is -0.38.", "D": "The geographic Herfindahl Index (HI) for this REIT is 1.00."}, "Answer": ["B", "C"], "pi_justification": "Score_A: 9. Score_B: 10. Total_Score: 9.5. Judgment: REPLACE with Multiple Choice (Score: 9.5). This item uses a Computational Judgment strategy to test the understanding of how a key variable is constructed and interpreted. It requires a multi-step process: calculate the Herfindahl Index, transform it into the regression variable, and then correctly reason about the expected sign of its coefficient under a specific hypothesis. Distractor (A) is a 'Step-Omission Error' (summing proportions without squaring). Distractor (D) is a 'Conceptual Opposite' / 'Sign Error' that reflects a common misunderstanding of the transformed variable's meaning.", "qid": "308", "question": "### Background\n\n**Research Question.** To analyze the economic consequences of diversification, one must first have a reliable way to quantify it. The Herfindahl Index (HI) is a standard measure of concentration used in economics and finance.\n\n**Setting.** The analysis considers diversification for a sample of equity REITs along two dimensions: property type and geographic location. The study transforms the standard HI to create a more intuitive measure for its regression analysis.\n\n---\n\n### Data / Model Specification\n\nThe Herfindahl Index is defined as the sum of the squared proportions of assets in each category:\n  \n\\mathrm{Herfindahl Index} \\, (HI) = \\sum_{i=1}^{I} P_{i}^{2} \n \n**Eq. (1)**\n\nwhere `P_i` is the proportion of assets in category `i`.\n\nIn the subsequent regression analysis, the measure for diversification is constructed as the negative of the Herfindahl Index:\n  \n\\text{Geographic Diversification} = -HI_{\\text{geographic}}\n \n**Eq. (2)**\n\n---\n\n### Question\n\nA hypothetical REIT has its assets distributed across three geographic regions: 50% in the Northeast, 30% in the Pacific, and 20% in the Southeast. Based on the provided definitions, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 312, "Question": "### Background\n\n**Research Question.** How did the composition of foreign direct investment (FDI) into Argentina after its 1991 reforms drive a fundamental shift in corporate ownership, and what were the implications for corporate governance?\n\n**Setting / Data-Generating Environment.** The analysis focuses on Argentina during its decade of intense economic liberalization (1990-1999). Sweeping reforms attracted a massive inflow of foreign capital, leading to a dramatic change in the ownership structure of the country's top corporations, which had previously been dominated by domestic families and the state. This shift from concentrated family/state ownership to control by foreign corporations and investment funds had profound effects on prevailing corporate governance practices.\n\n### Data / Model Specification\n\nThe composition of foreign investment in Argentina from 1990 to 1999 is detailed in Table 1.\n\n**Table 1: Foreign Investment in Argentina from 1990–1999 (in millions of dollars)**\n\n| Category      | 1990-1996 | 1997   | 1998   | 1999    | Total      |\n|:--------------|:----------|:-------|:-------|:--------|:-----------|\n| M&A           | 8,131.3   | 11,792.5 | 9,120.7  | 19,507.5  | 48,552.0   |\n| Privatizations| 8,763.0   | 1,111.2  | 442.5    | 5,926.7   | 16,243.3   |\n| Greenfield    | 5,552.4   | 4,143.3  | 6,263.9  | 5,194.7   | 21,154.3   |\n| Expansion     | 18,258.0  | 7,453.5  | 7,670.6  | 7,272.6   | 40,654.7   |\n| **Total**     | 40,704.7  | 24,500.5 | 23,497.6 | 37,901.5  | 126,604.3  |\n\nThis influx of capital resulted in a significant ownership transition, as exemplified by the cases in Table 2.\n\n**Table 2: Examples of Ownership Transition in Top Argentine Corporations**\n\n| Corporation     | Sector       | Pre-1991 Controlling Shareholders | Post-1991 Controlling Shareholders |\n|:----------------|:-------------|:----------------------------------|:-----------------------------------|\n| YPF             | Oil & Gas    | State                             | Repsol (Spain)                     |\n| Banco Francés   | Banking      | Domestic Families                 | Banco Bilbao Vizcaya (Spain)       |\n| Norte           | Supermarkets | Domestic                          | Promodes (Carrefour France)        |\n\n### Question\n\nThe shift in ownership from domestic family-owned firms to foreign corporations, driven by the investment patterns shown, had profound implications for corporate governance. Select ALL statements that correctly describe these theoretical implications.", "Options": {"A": "The corporate restructuring in Argentina in the 1990s was primarily financed by leveraged buy-outs and junk bonds, mirroring the US experience in the 1980s.", "B": "The ownership transition would be expected to replace the dominant Type 2 agency problem (majority vs. minority shareholders) with the classic Type 1 agency problem (managers vs. dispersed owners).", "C": "From a real options perspective, an M&A strategy is valuable because it allows an acquirer to resolve a target firm's governance problems, thereby increasing the value of embedded options like the option to expand or restructure.", "D": "The primary agency conflict likely shifted from manager-shareholder conflicts (Type 1) to conflicts between controlling and minority shareholders (Type 2)."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect empirical patterns to core corporate finance theories (agency theory, real options). It uses a Scenario Application strategy. The correct options (B, C) require applying these theories to the specific context of Argentina's reforms. Distractor A is a direct Conceptual Opposite, testing for understanding of agency problem types. Distractor D introduces a plausible but factually incorrect comparison (the paper explicitly states LBOs/junk bonds were not used), testing for close reading of the paper's specific details.", "qid": "312", "question": "### Background\n\n**Research Question.** How did the composition of foreign direct investment (FDI) into Argentina after its 1991 reforms drive a fundamental shift in corporate ownership, and what were the implications for corporate governance?\n\n**Setting / Data-Generating Environment.** The analysis focuses on Argentina during its decade of intense economic liberalization (1990-1999). Sweeping reforms attracted a massive inflow of foreign capital, leading to a dramatic change in the ownership structure of the country's top corporations, which had previously been dominated by domestic families and the state. This shift from concentrated family/state ownership to control by foreign corporations and investment funds had profound effects on prevailing corporate governance practices.\n\n### Data / Model Specification\n\nThe composition of foreign investment in Argentina from 1990 to 1999 is detailed in Table 1.\n\n**Table 1: Foreign Investment in Argentina from 1990–1999 (in millions of dollars)**\n\n| Category      | 1990-1996 | 1997   | 1998   | 1999    | Total      |\n|:--------------|:----------|:-------|:-------|:--------|:-----------|\n| M&A           | 8,131.3   | 11,792.5 | 9,120.7  | 19,507.5  | 48,552.0   |\n| Privatizations| 8,763.0   | 1,111.2  | 442.5    | 5,926.7   | 16,243.3   |\n| Greenfield    | 5,552.4   | 4,143.3  | 6,263.9  | 5,194.7   | 21,154.3   |\n| Expansion     | 18,258.0  | 7,453.5  | 7,670.6  | 7,272.6   | 40,654.7   |\n| **Total**     | 40,704.7  | 24,500.5 | 23,497.6 | 37,901.5  | 126,604.3  |\n\nThis influx of capital resulted in a significant ownership transition, as exemplified by the cases in Table 2.\n\n**Table 2: Examples of Ownership Transition in Top Argentine Corporations**\n\n| Corporation     | Sector       | Pre-1991 Controlling Shareholders | Post-1991 Controlling Shareholders |\n|:----------------|:-------------|:----------------------------------|:-----------------------------------|\n| YPF             | Oil & Gas    | State                             | Repsol (Spain)                     |\n| Banco Francés   | Banking      | Domestic Families                 | Banco Bilbao Vizcaya (Spain)       |\n| Norte           | Supermarkets | Domestic                          | Promodes (Carrefour France)        |\n\n### Question\n\nThe shift in ownership from domestic family-owned firms to foreign corporations, driven by the investment patterns shown, had profound implications for corporate governance. Select ALL statements that correctly describe these theoretical implications.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 23, "Question": "### Background\n\n**Research Question.** How can corporations exploit segmentation within developed debt markets arising from regulatory restrictions through financial engineering?\n\n**Setting / Data-Generating Environment.** In 1986, the Kingdom of Denmark issued “Bull and Bear” notes in the French market. The Bear notes' redemption value was inversely related to a stock market index, while the Bull notes' redemption value was directly related to the index. For the issuer, the average redemption value of a pair of notes was fixed, making the total issuance riskless from its perspective.\n\n### Question\n\nSelect all statements that correctly describe the financial engineering behind this issuance.", "Options": {"A": "The structure allowed Denmark to sell substitutes for prohibited derivative instruments at a premium.", "B": "The Bull notes were equivalent to a risk-free bond, as their coupon was guaranteed.", "C": "The issuer (Denmark) was left with a significant, unhedged net long position in the French stock market.", "D": "The Bear notes synthetically replicated a long put option on the stock index, providing portfolio insurance for investors."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: Tests the understanding of how structured products can replicate derivative payoffs to circumvent regulatory barriers. Strategy: Scenario Application. Students must map the described features of the notes to core financial concepts. Distractor Logic: (A) Correct. (B) Correct. (C) Conceptual Opposite: The paper explicitly states the combined issue was riskless for the issuer. (D) Almost Right: The coupon was guaranteed, but the principal redemption was variable, making it a risky, not risk-free, instrument when held in isolation.", "qid": "23", "question": "### Background\n\n**Research Question.** How can corporations exploit segmentation within developed debt markets arising from regulatory restrictions through financial engineering?\n\n**Setting / Data-Generating Environment.** In 1986, the Kingdom of Denmark issued “Bull and Bear” notes in the French market. The Bear notes' redemption value was inversely related to a stock market index, while the Bull notes' redemption value was directly related to the index. For the issuer, the average redemption value of a pair of notes was fixed, making the total issuance riskless from its perspective.\n\n### Question\n\nSelect all statements that correctly describe the financial engineering behind this issuance.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"Denmark Bull and Bear notes 1986 French market\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Analyzing financial engineering**\n> 🔍 **10 results found**\n\n> [🔗 \"Bull and bear markets over time\" Vanguard UK](https://www.vanguard.co.uk/content/dam/intl/europe/documents/en/bear-and-bull-chart-uk-en.pdf)\n> [🔗 \"Bull Vs Bear Neon Sign, Bear and Bull Neon Sign Lamp, Stock Market Neon Sign, Trading Neon Sign - Etsy Denmark\" Etsy](https://www.etsy.com/dk-en/listing/1697613894/bull-vs-bear-neon-sign-bear-and-bull)\n> [🔗 \"History of U.S. Bear & Bull Markets Since 1926\" Raymond James](https://www.raymondjames.com/neunuebelbarrantes/pdfs/history-of-market-corrections.pdf)\n> [🔗 \"Bull and Bear Markets Since 1932\" Stifel](https://www.stifel.com/newsletters/AdGraphics/InSight/Market-Volatility/Bull-and-bear-Markets-since-1932.pdf)\n> [🔗 \"History of US Bear & Bull Markets\" First Trust](https://www.ftportfolios.com/COMMON/CONTENTFILELOADER.ASPX?CONTENTGUID=4ECFA978-D0BB-4924-92C8-628FF9BFE12D)\n> [🔗 \"r/Bogleheads on Reddit: Historical Bull VS Bear Markets: 1942-2024 (First Trust)\" Reddit](https://www.reddit.com/r/Bogleheads/comments/1j992te/historical_bull_vs_bear_markets_19422024_first/)\n> [🔗 \"The History of Bull and Bear Markets - Rockefeller Capital Management\" Rockefeller Capital Management](https://www.rockco.com/strategic-insights/bull-and-bear-markets/)\n> [🔗 \"Stock Market Historical Tables: Bull & Bear Markets\" Yardeni Research](https://yardeni.com/wp-content/uploads/BullBearTables.pdf)\n> [🔗 \"r/investing on Reddit: History of U.S. Bear vs Bull Markets\" Reddit](https://www.reddit.com/r/investing/comments/1jo5vl0/history_of_us_bear_vs_bull_markets/)\n> [🔗 \"Bear, Bull, Wolf, Eagle Markets and Multi-Method Investing\" 4tfg](https://www.4tfg.com/bear-bull-wolf-eagle-markets)\n\n> ***- The issuance involved Bull and Bear notes with redemption values tied to a stock market index.***\n\n> 🔍 **Searching for:** \"\"Denmark\" \"Bull and Bear\" notes 1986\"\n> 📊 **Results requested:** 10\n\n> ***- Denmark's structure allowed selling substitutes for prohibited derivatives at a premium, as per option A.***\n\n> 🔍 **Searching for:** \"Denmark Bull and Bear notes 1986 French market\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Found 10 search results**\n\n🔗 **[Bull and bear markets over time](https://www.vanguard.co.uk/content/dam/intl/europe/documents/en/bear-and-bull-chart-uk-en.pdf)**\n   📄 We cannot provide a description for this page right now\n   🌐 *Vanguard UK*\n\n🔗 **[Bull Vs Bear Neon Sign, Bear and Bull Neon Sign Lamp, Stock Market Neon Sign, Trading Neon Sign - Etsy Denmark](https://www.etsy.com/dk-en/listing/1697613894/bull-vs-bear-neon-sign-bear-and-bull)**\n   📄 🤝 Hello my dear friend, it`s listing called bull vs bear neon sign, neon light decor for trading office. You can change the colors, combine colors or...\n   🌐 *Etsy*\n\n🔗 **[History of U.S. Bear & Bull Markets Since 1926](https://www.raymondjames.com/neunuebelbarrantes/pdfs/history-of-market-corrections.pdf)**\n   📄 We make a personal connection <strong>and</strong> commitment to you – to create a financial plan that uniquely reflects you <strong>and</strong> the ...\n   🌐 *Raymond James*\n\n🔗 **[Bull and Bear Markets Since 1932](https://www.stifel.com/newsletters/AdGraphics/InSight/Market-Volatility/Bull-and-bear-Markets-since-1932.pdf)**\n   📄 Stifel is a full service brokerage <strong>and</strong> investment banking firm. The Company provides securities brokerage, investment banking, tradin...\n   🌐 *Stifel*\n\n🔗 **[History of US Bear & Bull Markets](https://www.ftportfolios.com/COMMON/CONTENTFILELOADER.ASPX?CONTENTGUID=4ECFA978-D0BB-4924-92C8-628FF9BFE12D)**\n   📄 Visit First Trust for investment products <strong>and</strong> economic <strong>and</strong> investment insight....\n   🌐 *First Trust*\n\n🔗 **[r/Bogleheads on Reddit: Historical Bull VS Bear Markets: 1942-2024 (First Trust)](https://www.reddit.com/r/Bogleheads/comments/1j992te/historical_bull_vs_bear_markets_19422024_first/)**\n   📄 Posted by u/Godkun007 - 829 votes and 102 comments\n   🌐 *Reddit*\n\n🔗 **[The History of Bull and Bear Markets - Rockefeller Capital Management](https://www.rockco.com/strategic-insights/bull-and-bear-markets/)**\n   📄 Once inflation was under control, the Fed was able to lower interest rates, which helped fuel economic growth and the upward valuation of the stock ma...\n   🌐 *Rockefeller Capital Management*\n\n🔗 **[Stock Market Historical Tables: Bull & Bear Markets](https://yardeni.com/wp-content/uploads/BullBearTables.pdf)**\n   📄 Stay ahead of the game with unparalleled insights into <strong>market</strong> movements <strong>and</strong> trends, backed by thorough analysis <str...\n   🌐 *Yardeni Research*\n\n🔗 **[r/investing on Reddit: History of U.S. Bear vs Bull Markets](https://www.reddit.com/r/investing/comments/1jo5vl0/history_of_us_bear_vs_bull_markets/)**\n   📄 Posted by u/artiom_baloian - 127 votes and 76 comments\n   🌐 *Reddit*\n\n🔗 **[Bear, Bull, Wolf, Eagle Markets and Multi-Method Investing](https://www.4tfg.com/bear-bull-wolf-eagle-markets)**\n   📄 It is important to note that based on these quantitative definitions of the Bear, Bull, Wolf, and Eagle environments, markets can overlap with each ot...\n   🌐 *4tfg*\n\n\n> 🔍 **Searching for:** \"Denmark Bull Bear bonds 1986\"\n> 📊 **Results requested:** 10\n\n> ***- The Bear notes synthetically replicated a long put option on the stock index, matching option D.***\n\n> 🔍 **Searching for:** \"\"Denmark\" \"Bull and Bear\" notes 1986\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Interest Rates: Long-Term Government Bond Yields: 10-Year: Main (Including Benchmark) for Denmark (IRLTLT01DKM156N) | FRED | St. Louis Fed\" FRED](https://fred.stlouisfed.org/series/IRLTLT01DKM156N)\n> [🔗 \"List of Danish government bonds\" Danmarks Nationalbank](https://www.nationalbanken.dk/en/government-debt/trading-and-data/list-of-danish-government-bonds)\n> [🔗 \"Denmark 10-Year Bond Historical Data - Investing.com\" Investing.com](https://www.investing.com/rates-bonds/denmark-10-year-bond-yield-historical-data)\n> [🔗 \"Denmark 10-Year Government Bond Yield - Quote - Chart - Historical Data - News\" TRADING ECONOMICS](https://tradingeconomics.com/denmark/government-bond-yield)\n> [🔗 \"One Year of Rate Hikes Ends Decades of Bond Market Prosperity\" Investopedia](https://www.investopedia.com/bonds-bear-market-7370474)\n> [🔗 \"Stock Returns And Bond Yields In Denmark, 1922-99\" IDEAS/RePEc](https://ideas.repec.org/p/hhs/cbsnow/2001_003.html)\n> [🔗 \"Denmark Government Bonds - Yields Curve\" World Government Bonds](http://www.worldgovernmentbonds.com/country/denmark/)\n> [🔗 \"Denmark Government Bonds - Investing.com\" Investing.com](https://www.investing.com/rates-bonds/denmark-government-bonds)\n> [🔗 \"Bull or Bear? Emerging Markets Bonds Outperform in Both | VanEck\" VanEck](https://www.vaneck.com/us/en/blogs/emerging-markets-bonds/bull-or-bear-emerging-markets-bonds-outperform-in-both/)\n> [🔗 \"S&P Denmark Sovereign Bond Index | S&P Dow Jones Indices\" S&P Dow Jones Indices](https://www.spglobal.com/spdji/en/indices/fixed-income/sp-denmark-sovereign-bond-index/)\n\n> 🔍 **Searching for:** \"Denmark \"bull and bear\" notes financial engineering 1986\"\n> 📊 **Results requested:** 10\n\n> ***- Bull notes weren't risk-free bonds; their value depended on the stock index, contradicting option B.***\n\n> 🔍 **Searching for:** \"Denmark Bull Bear bonds 1986\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Found 10 search results**\n\n🔗 **[Interest Rates: Long-Term Government Bond Yields: 10-Year: Main (Including Benchmark) for Denmark (IRLTLT01DKM156N) | FRED | St. Louis Fed](https://fred.stlouisfed.org/series/IRLTLT01DKM156N)**\n   📄 Organization for Economic Co-operation and Development, Interest Rates: Long-Term Government Bond Yields: 10-Year: Main (Including Benchmark) for Denm...\n   🌐 *FRED*\n\n🔗 **[List of Danish government bonds](https://www.nationalbanken.dk/en/government-debt/trading-and-data/list-of-danish-government-bonds)**\n   📄 The Kingdom of <strong>Denmark</strong> offers a broad pallet of government <strong>bonds</strong> with a focus on 2- and 10-year nominal <strong>bond...\n   🌐 *Danmarks Nationalbank*\n\n🔗 **[Denmark 10-Year Bond Historical Data - Investing.com](https://www.investing.com/rates-bonds/denmark-10-year-bond-yield-historical-data)**\n   📄 Get free historical data for Denmark 10-Year Bond Yield. You'll find the closing yield, open, high, low, change and %change for the selected range of ...\n   🌐 *Investing.com*\n\n🔗 **[Denmark 10-Year Government Bond Yield - Quote - Chart - Historical Data - News](https://tradingeconomics.com/denmark/government-bond-yield)**\n   📄 The yield on Denmark 10Y Bond Yield eased to 2.50% on September 10, 2025, marking a 0 percentage point decrease from the previous session. Over the pa...\n   🌐 *TRADING ECONOMICS*\n\n🔗 **[One Year of Rate Hikes Ends Decades of Bond Market Prosperity](https://www.investopedia.com/bonds-bear-market-7370474)**\n   📄 Instead, bond bull markets merely reflect a prolonged downward trend in interest rates, and vice-versa for a bear market. Bond prices move inversely w...\n   🌐 *Investopedia*\n\n🔗 **[Stock Returns And Bond Yields In Denmark, 1922-99](https://ideas.repec.org/p/hhs/cbsnow/2001_003.html)**\n   📄 Kleidon, Allan W, 1986. \"Variance Bounds Tests and Stock Price Valuation Models,\" Journal of Political Economy, University of Chicago Press, vol.\n   🌐 *IDEAS/RePEc*\n\n🔗 **[Denmark Government Bonds - Yields Curve](http://www.worldgovernmentbonds.com/country/denmark/)**\n   📄 The 10-Year bond yield reflects the return investors can expect if they hold the bond until maturity. Government bond yields are critical indicators o...\n   🌐 *World Government Bonds*\n\n🔗 **[Denmark Government Bonds - Investing.com](https://www.investing.com/rates-bonds/denmark-government-bonds)**\n   📄 Stay on top of current data on government bond yields in Denmark, including the yield, daily high, low and change% for each bond. The yield on a Treas...\n   🌐 *Investing.com*\n\n🔗 **[Bull or Bear? Emerging Markets Bonds Outperform in Both | VanEck](https://www.vaneck.com/us/en/blogs/emerging-markets-bonds/bull-or-bear-emerging-markets-bonds-outperform-in-both/)**\n   📄 United States Australia Austria Belgium Brazil Canada Chile China Denmark Finland France Germany Ireland Iceland Italy Liechtenstein Luxembourg Mexico...\n   🌐 *VanEck*\n\n🔗 **[S&P Denmark Sovereign Bond Index | S&P Dow Jones Indices](https://www.spglobal.com/spdji/en/indices/fixed-income/sp-denmark-sovereign-bond-index/)**\n   📄 S&P U.S. Aggregate Bond Index ... The S&P Denmark Sovereign Bond Index is a comprehensive, market-value-weighted index designed to track the performan...\n   🌐 *S&P Dow Jones Indices*\n\n\n> ***- Option C is incorrect; Denmark had a riskless position, not a net long position, as the average redemption value was fixed.***\n\n> 🔍 **Searching for:** \"Denmark \"bull and bear\" notes financial engineering 1986\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Bear, Bull, Wolf, Eagle Markets and Multi-Method Investing\" 4tfg](https://www.4tfg.com/bear-bull-wolf-eagle-markets)\n> [🔗 \"Where Did the Bull and Bear Market Get Their Names?\" Investopedia](https://www.investopedia.com/ask/answers/bull-bear-market-names/)\n> [🔗 \"Bull vs Bear | What to Know About Market Trends in 2025 | Citizens Wealth Management\" Citizens Bank](https://www.citizensbank.com/learning/bull-market-vs-bear-market.aspx)\n> [🔗 \"The History of Bull and Bear Markets - Rockefeller Capital Management\" Rockefeller Capital Management](https://www.rockco.com/strategic-insights/bull-and-bear-markets/)\n> [🔗 \"Bear vs bull market: How to tell the difference | Fidelity\" Fidelity](https://www.fidelity.com/learning-center/smart-money/bear-vs-bull-market)\n> [🔗 \"What Is a Bull Market? Characteristics and Historic Bull Markets\" Investopedia](https://www.investopedia.com/terms/b/bullmarket.asp)\n> [🔗 \"Bull and Bear Markets Since 1932\" Stifel](https://www.stifel.com/newsletters/AdGraphics/InSight/Market-Volatility/Bull-and-bear-Markets-since-1932.pdf)\n> [🔗 \"Bull and bear markets over time\" Vanguard UK](https://www.vanguard.co.uk/content/dam/intl/europe/documents/en/bear-and-bull-chart-uk-en.pdf)\n> [🔗 \"(PDF) Intermediate Financial Theory\" Academia.edu](https://www.academia.edu/40575978/Intermediate_Financial_Theory)\n> [🔗 \"98 Years of S&P 500 Bull and Bear Markets - Sensible Financial Planning\" Sensiblefinancial](https://www.sensiblefinancial.com/how-has-the-sp-500-performed-over-the-last-98-years/)\n\n> 🔍 **Found 10 search results**\n\n🔗 **[Bear, Bull, Wolf, Eagle Markets and Multi-Method Investing](https://www.4tfg.com/bear-bull-wolf-eagle-markets)**\n   📄 Our hypotheses and findings appear to be (at least in part) corroborated by related previous data studies conducted by Modern Portfolio Theorists. For...\n   🌐 *4tfg*\n\n🔗 **[Where Did the Bull and Bear Market Get Their Names?](https://www.investopedia.com/ask/answers/bull-bear-market-names/)**\n   📄 The use of bull and bear to label financial markets has several different possible origins. However, the terms could come from how these animals attac...\n   🌐 *Investopedia*\n\n🔗 **[Bull vs Bear | What to Know About Market Trends in 2025 | Citizens Wealth Management](https://www.citizensbank.com/learning/bull-market-vs-bear-market.aspx)**\n   📄 Market uncertainty? Get the facts about the difference between a Bull vs Bear Market, their effects on your investments, and strategies to manage risk...\n   🌐 *Citizens Bank*\n\n🔗 **[The History of Bull and Bear Markets - Rockefeller Capital Management](https://www.rockco.com/strategic-insights/bull-and-bear-markets/)**\n   📄 The bull, charging upward, embodies investor confidence and economic growth, while the slower, lumbering bear indicates a dreaded economic downturn. T...\n   🌐 *Rockefeller Capital Management*\n\n🔗 **[Bear vs bull market: How to tell the difference | Fidelity](https://www.fidelity.com/learning-center/smart-money/bear-vs-bull-market)**\n   📄 Bull markets, on the other hand, can trigger a sense of euphoria as you see stock prices surge. But rushing to invest in something simply because it s...\n   🌐 *Fidelity*\n\n🔗 **[What Is a Bull Market? Characteristics and Historic Bull Markets](https://www.investopedia.com/terms/b/bullmarket.asp)**\n   📄 A bull market is a trend in a financial market characterized by rising prices and investor optimism. It can occur in the stock market as well as the b...\n   🌐 *Investopedia*\n\n🔗 **[Bull and Bear Markets Since 1932](https://www.stifel.com/newsletters/AdGraphics/InSight/Market-Volatility/Bull-and-bear-Markets-since-1932.pdf)**\n   📄 Stifel is a full service brokerage <strong>and</strong> investment banking firm. The Company provides securities brokerage, investment banking, tradin...\n   🌐 *Stifel*\n\n🔗 **[Bull and bear markets over time](https://www.vanguard.co.uk/content/dam/intl/europe/documents/en/bear-and-bull-chart-uk-en.pdf)**\n   📄 We cannot provide a description for this page right now\n   🌐 *Vanguard UK*\n\n🔗 **[(PDF) Intermediate Financial Theory](https://www.academia.edu/40575978/Intermediate_Financial_Theory)**\n   📄 Wagenaar, W., Keren, G., 1986. Does the expect know? the reliability of predictions and confidence ratios of experts. In: Hollnagel, E., Mancini, G., ...\n   🌐 *Academia.edu*\n\n🔗 **[98 Years of S&P 500 Bull and Bear Markets - Sensible Financial Planning](https://www.sensiblefinancial.com/how-has-the-sp-500-performed-over-the-last-98-years/)**\n   📄 Our analysis begins in 1926 amidst the Roaring Twenties bull market, followed by the most significant stock market decline of the century during the G...\n   🌐 *Sensiblefinancial*\n\n\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 324, "Question": "## Background\n\n**Research Question.** This case examines the strategic decision-making of a large Chinese firm, Huaneng Power International (HPI), in choosing a venue for its international equity offering, highlighting the trade-offs between market depth, valuation, and strategic goals.\n\n**Setting.** In 1994, HPI, a recently privatized power company, needed to raise $750 million to fund a massive expansion plan. Management was considering a Depositary Receipt offering on the New York Stock Exchange (NYSE) versus the more common route for Chinese firms at the time, the Hong Kong Stock Exchange (HKEX).\n\n**Variables & Parameters.**\n- **P/E Multiple:** Price-to-Earnings ratio, a valuation metric.\n- **Offering Size:** The total amount of capital raised in an equity issue (in USD).\n- **EPS (Earnings Per Share):** HPI's earnings per share, given as $1.45.\n\n---\n\n## Data / Model Specification\n\nData on comparable Chinese company IPOs from 1993-1994 are presented in Table 1. HPI's sister company, Shandong Huaneng, had previously listed on the NYSE and was trading at a P/E of 14x. HPI's planned offering of $750 million was substantially larger than any of the recent HKEX listings.\n\n**Table 1: Chinese Company IPOs in 1993 and 1994**\n\n| Company | Exchange | Offering Size | P/E Multiple |\n| :--- | :--- | :--- | :--- |\n| Dongfang Electrical | Hong Kong | $62m | 12.0x |\n| Tianjin Bohai Chem. | Hong Kong | $53m | 11.2x |\n| Yizheng Chemical | Hong Kong | $308m | 13.5x |\n| Kunming Machine | Hong Kong | $17m | 5.4x |\n| Beiren Printing | Hong Kong | $27m | 15.2x |\n| Shanghai Petrochem. | Hong Kong | $343m | 11.3x |\n| Tsingtao Brewery | Hong Kong | n/a | 14.0x |\n| Shandong Huaneng | NYSE | $333m | 14.0x (trading) |\n\n---\n\n## Question\n\nBased on the provided background and data in Table 1, select all statements that accurately describe the strategic rationale and quantitative trade-offs for Huaneng Power International (HPI) choosing an NYSE listing over a Hong Kong listing.", "Options": {"A": "The total capital raised by HPI on the NYSE ($625 million) exceeded its initial target of $750 million, indicating strong U.S. investor demand.", "B": "A valuation analysis suggested a potential premium on the NYSE, where HPI's sister company traded at a 14.0x P/E multiple, compared to a simple average of approximately 11.8x for recent Hong Kong IPOs.", "C": "Listing in Hong Kong at a price implied by the average local P/E multiple would have allowed HPI to meet its $750 million funding goal.", "D": "The NYSE was favored for its market capacity, as HPI's $750 million target was more than double the size of the largest recent Chinese IPOs in Hong Kong."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize qualitative strategic factors (market capacity) and quantitative valuation metrics (P/E multiples) from case data. It uses an Atomic Decomposition strategy to break down the core decision into two distinct, verifiable propositions. Distractor C is a factual error from the text (under-raising capital), while D is a computational error that reverses the conclusion of the market capacity analysis.", "qid": "324", "question": "## Background\n\n**Research Question.** This case examines the strategic decision-making of a large Chinese firm, Huaneng Power International (HPI), in choosing a venue for its international equity offering, highlighting the trade-offs between market depth, valuation, and strategic goals.\n\n**Setting.** In 1994, HPI, a recently privatized power company, needed to raise $750 million to fund a massive expansion plan. Management was considering a Depositary Receipt offering on the New York Stock Exchange (NYSE) versus the more common route for Chinese firms at the time, the Hong Kong Stock Exchange (HKEX).\n\n**Variables & Parameters.**\n- **P/E Multiple:** Price-to-Earnings ratio, a valuation metric.\n- **Offering Size:** The total amount of capital raised in an equity issue (in USD).\n- **EPS (Earnings Per Share):** HPI's earnings per share, given as $1.45.\n\n---\n\n## Data / Model Specification\n\nData on comparable Chinese company IPOs from 1993-1994 are presented in Table 1. HPI's sister company, Shandong Huaneng, had previously listed on the NYSE and was trading at a P/E of 14x. HPI's planned offering of $750 million was substantially larger than any of the recent HKEX listings.\n\n**Table 1: Chinese Company IPOs in 1993 and 1994**\n\n| Company | Exchange | Offering Size | P/E Multiple |\n| :--- | :--- | :--- | :--- |\n| Dongfang Electrical | Hong Kong | $62m | 12.0x |\n| Tianjin Bohai Chem. | Hong Kong | $53m | 11.2x |\n| Yizheng Chemical | Hong Kong | $308m | 13.5x |\n| Kunming Machine | Hong Kong | $17m | 5.4x |\n| Beiren Printing | Hong Kong | $27m | 15.2x |\n| Shanghai Petrochem. | Hong Kong | $343m | 11.3x |\n| Tsingtao Brewery | Hong Kong | n/a | 14.0x |\n| Shandong Huaneng | NYSE | $333m | 14.0x (trading) |\n\n---\n\n## Question\n\nBased on the provided background and data in Table 1, select all statements that accurately describe the strategic rationale and quantitative trade-offs for Huaneng Power International (HPI) choosing an NYSE listing over a Hong Kong listing.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 180, "Question": "### Background\n\n**Research Question.** How can the risk-adjusted performance of different equity styles be evaluated to test the behavioral proposition that investor return-chasing leads to predictable cycles of outperformance for 'in-favor' styles and underperformance for their 'twin' styles?\n\n**Setting and Data.** The analysis uses monthly returns for four Fama-French (FF) style benchmarks (Large Growth, Large Value, Small Growth, Small Value) and the Wilshire 5000 market index from January 1979 to December 2004. Performance is assessed using metrics for both total risk and systematic risk.\n\n---\n\n### Data / Model Specification\n\nPerformance is evaluated using two primary methods:\n\n1.  **Total Risk-Adjusted Performance:** The Sharpe Ratio (`SHP`) measures excess return per unit of total risk (standard deviation). The Jobson and Korkie `Z`-statistic tests the null hypothesis that a portfolio's Sharpe Ratio is equal to the market's Sharpe Ratio.\n\n2.  **Systematic Risk-Adjusted Performance:** A four-factor model is used to measure performance after accounting for exposure to known risk factors. The intercept, `a_i0` (alpha), captures the abnormal return.\n\n  \nR_{it} = a_{i0} + b_{i1}R_{mt} + b_{i2}SMB_t + b_{i3}HML_t + b_{i4}UMD_t + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nThe paper's proposition is that 'in-favor' styles (identified as Large Growth and Small Value) will exhibit superior performance, while their 'twin' styles (Large Value and Small Growth) will underperform.\n\n**Table 1: Performance Summary of Fama-French Style Benchmarks (1979-2004)**\n\n| Benchmark | Annualized Return (%) | Annualized S.D. (%) | Sharpe Ratio (SHP) | Z-Stat vs. Market | Four-Factor Alpha (`a_i0`, %/month) | t-stat for Alpha |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Wilshire 5000 (Market) | 13.59 | 15.64 | 0.461 | - | - | - |\n| FF Large Growth (FFLG) | 13.08 | 16.93 | 0.392 | -1.530 | 0.164 | 3.358 |\n| FF Large Value (FFLV) | 15.25 | 14.71 | 0.601 | 2.462 | -0.147 | -2.294 |\n| FF Small Growth (FFSG) | 9.69 | 24.47 | 0.130 | -5.415 | -0.220 | -3.450 |\n| FF Small Value (FFSV) | 20.18 | 16.70 | 0.831 | 4.201 | 0.091 | 2.286 |\n\n---\n\n### Question\n\nBased on the performance metrics in **Table 1**, select all statements that are supported by the data.", "Options": {"A": "Although the FF Small Value (FFSV) portfolio's Sharpe Ratio was higher than the market's, the Z-statistic indicates this difference was not statistically significant at the 1% level.", "B": "The value premium, measured by the difference in Sharpe Ratios (Value SHP - Growth SHP), was substantially more pronounced in the small-cap segment than in the large-cap segment.", "C": "The pattern of systematic risk-adjusted returns (alphas) strongly supports the paper's performance proposition: 'in-favor' styles (FFLG, FFSV) earned significant positive alphas, while their 'twin' styles (FFLV, FFSG) earned significant negative alphas.", "D": "The FF Large Value (FFLV) portfolio underperformed the market on a systematic risk-adjusted basis (alpha) but outperformed the market on a total risk-adjusted basis (Sharpe Ratio)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item tests the ability to synthesize multiple empirical results from a performance table. It uses an Atomic Decomposition strategy, breaking down the original QA into distinct, verifiable claims about the value premium, alpha patterns, and risk-adjusted returns. Distractor C is a Conceptual Opposite, directly contradicting the high Z-statistic. Option D is a nuanced correct answer that requires comparing two different performance metrics against the market benchmark, making it a good discriminator.", "qid": "180", "question": "### Background\n\n**Research Question.** How can the risk-adjusted performance of different equity styles be evaluated to test the behavioral proposition that investor return-chasing leads to predictable cycles of outperformance for 'in-favor' styles and underperformance for their 'twin' styles?\n\n**Setting and Data.** The analysis uses monthly returns for four Fama-French (FF) style benchmarks (Large Growth, Large Value, Small Growth, Small Value) and the Wilshire 5000 market index from January 1979 to December 2004. Performance is assessed using metrics for both total risk and systematic risk.\n\n---\n\n### Data / Model Specification\n\nPerformance is evaluated using two primary methods:\n\n1.  **Total Risk-Adjusted Performance:** The Sharpe Ratio (`SHP`) measures excess return per unit of total risk (standard deviation). The Jobson and Korkie `Z`-statistic tests the null hypothesis that a portfolio's Sharpe Ratio is equal to the market's Sharpe Ratio.\n\n2.  **Systematic Risk-Adjusted Performance:** A four-factor model is used to measure performance after accounting for exposure to known risk factors. The intercept, `a_i0` (alpha), captures the abnormal return.\n\n  \nR_{it} = a_{i0} + b_{i1}R_{mt} + b_{i2}SMB_t + b_{i3}HML_t + b_{i4}UMD_t + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nThe paper's proposition is that 'in-favor' styles (identified as Large Growth and Small Value) will exhibit superior performance, while their 'twin' styles (Large Value and Small Growth) will underperform.\n\n**Table 1: Performance Summary of Fama-French Style Benchmarks (1979-2004)**\n\n| Benchmark | Annualized Return (%) | Annualized S.D. (%) | Sharpe Ratio (SHP) | Z-Stat vs. Market | Four-Factor Alpha (`a_i0`, %/month) | t-stat for Alpha |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Wilshire 5000 (Market) | 13.59 | 15.64 | 0.461 | - | - | - |\n| FF Large Growth (FFLG) | 13.08 | 16.93 | 0.392 | -1.530 | 0.164 | 3.358 |\n| FF Large Value (FFLV) | 15.25 | 14.71 | 0.601 | 2.462 | -0.147 | -2.294 |\n| FF Small Growth (FFSG) | 9.69 | 24.47 | 0.130 | -5.415 | -0.220 | -3.450 |\n| FF Small Value (FFSV) | 20.18 | 16.70 | 0.831 | 4.201 | 0.091 | 2.286 |\n\n---\n\n### Question\n\nBased on the performance metrics in **Table 1**, select all statements that are supported by the data.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 280, "Question": "### Background\n\n**Research Question.** This study empirically estimates the magnitude of exchange rate pass-through to investment good prices and investigates how this effect varies across different industries in a panel of OECD countries.\n\n**Setting / Data-Generating Environment.** The analysis uses an unbalanced panel of annual data for various industrial sectors across 12 OECD countries from 1971 to 1997.\n\n---\n\n### Data / Model Specification\n\nThe pass-through elasticity `\\alpha_{i1}` is estimated for each industry `i` using the following panel regression model, which includes country (`D_m`) and year (`D_t`) fixed effects.\n\n  \n\\Delta\\mathrm{PI}_{i m t}=\\alpha_{i0}+\\alpha_{i1}\\left(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}}\\right)+\\alpha_{i2}\\Delta\\mathrm{PI}_{m t}^{\\mathrm{N}}+\\alpha_{i3}\\Delta c_{m t}+\\sum D_{t}+\\sum D_{m}+\\nu_{i m t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Selected Regression Results for Pass-Through Elasticity (`\\hat{\\alpha}_{i1}`)**\n\n| Industry                                                  | Coefficient (`\\hat{\\alpha}_{i1}`) | t-statistic |\n| :-------------------------------------------------------- | :-------------------------------- | :---------- |\n| Transport, storage and communication                      | 0.3151                            | (2.37)      |\n| Finance, insurance, real estate and business services     | 0.0242                            | (0.49)      |\n\n*Source: Adapted from Table 1 in the source paper. t-statistics are heteroscedasticity-robust.*\n\n---\n\n### Question\n\nBased on the model specification in **Eq. (1)** and the results in **Table 1**, which of the following statements are correct interpretations of the empirical strategy and findings?", "Options": {"A": "The results indicate that a 1% depreciation of the domestic currency leads to a statistically significant increase of approximately 0.315% in the investment good prices for the 'Transport' sector.", "B": "The inclusion of year fixed effects (`D_t`) helps to control for unobserved global shocks, such as a worldwide change in commodity prices, that could affect investment prices in all sample countries simultaneously.", "C": "The coefficient for the 'Finance' sector (0.0242) implies that exchange rate changes have a small but statistically significant positive effect on its investment good prices.", "D": "The country fixed effects (`D_m`) are included to account for country-specific shocks that vary over time, such as a national recession in a single country."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the baseline model's identification strategy and the interpretation of its results, which were key parts of the original QA problem. It uses an 'Atomic Decomposition' strategy, breaking the interpretation into distinct claims. Option A correctly describes the role of year fixed effects. Option B correctly interprets a coefficient and its statistical significance. Distractor C misinterprets country fixed effects, which capture time-invariant factors ('Conceptual Opposite'). Distractor D incorrectly assesses statistical significance based on the t-statistic ('Misinterpretation of Significance').", "qid": "280", "question": "### Background\n\n**Research Question.** This study empirically estimates the magnitude of exchange rate pass-through to investment good prices and investigates how this effect varies across different industries in a panel of OECD countries.\n\n**Setting / Data-Generating Environment.** The analysis uses an unbalanced panel of annual data for various industrial sectors across 12 OECD countries from 1971 to 1997.\n\n---\n\n### Data / Model Specification\n\nThe pass-through elasticity `\\alpha_{i1}` is estimated for each industry `i` using the following panel regression model, which includes country (`D_m`) and year (`D_t`) fixed effects.\n\n  \n\\Delta\\mathrm{PI}_{i m t}=\\alpha_{i0}+\\alpha_{i1}\\left(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}}\\right)+\\alpha_{i2}\\Delta\\mathrm{PI}_{m t}^{\\mathrm{N}}+\\alpha_{i3}\\Delta c_{m t}+\\sum D_{t}+\\sum D_{m}+\\nu_{i m t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Selected Regression Results for Pass-Through Elasticity (`\\hat{\\alpha}_{i1}`)**\n\n| Industry                                                  | Coefficient (`\\hat{\\alpha}_{i1}`) | t-statistic |\n| :-------------------------------------------------------- | :-------------------------------- | :---------- |\n| Transport, storage and communication                      | 0.3151                            | (2.37)      |\n| Finance, insurance, real estate and business services     | 0.0242                            | (0.49)      |\n\n*Source: Adapted from Table 1 in the source paper. t-statistics are heteroscedasticity-robust.*\n\n---\n\n### Question\n\nBased on the model specification in **Eq. (1)** and the results in **Table 1**, which of the following statements are correct interpretations of the empirical strategy and findings?", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 174, "Question": "### Background\n\nA market maker's optimal strategy depends on the balance between information asymmetry and liquidity demand. In a simplified market with discrete outcomes, a dealer faces a risk-neutral informed trader and a liquidity trader. The dealer must choose an ask price `a` and an ask depth `z`.\n\n**Variables & Parameters.**\n- `a`: Ask price.\n- `z`: Ask depth (quantity limit).\n- `\\bar{v}`: The informed trader's positive valuation, a measure of information asymmetry.\n- `\\bar{\\eta}`: The positive component of the liquidity shock, a measure of potential liquidity demand.\n\n---\n\n### Data / Model Specification\n\nThe dealer's optimal strategy falls into different regimes based on the ratio `\\bar{v}/\\bar{\\eta}`.\n\n**Table 1: Equilibrium Price and Depth under Discrete Distributions**\n\n| Condition | `1/2 \\bar{\\eta} < \\bar{v} \\le 3/5 \\bar{\\eta}` | `3/5 \\bar{\\eta} < \\bar{v} \\le 3 \\bar{\\eta}` | `\\bar{v} > 3 \\bar{\\eta}` |\n| :--- | :--- | :--- | :--- |\n| **Optimal `a`** | `\\bar{v}` | `\\frac{1}{2}\\bar{\\eta}+\\frac{1}{6}\\bar{v}` | n.a. |\n| **Optimal `z`** | n.a. (effectively infinite) | `\\frac{1}{2}\\bar{\\eta}-\\frac{1}{6}\\bar{v}` | 0 |\n\n---\n\nConsider a scenario where `\\bar{\\eta} = 10`. According to the model specifications in Table 1, which of the following statements accurately describe the dealer's optimal strategy under the given market conditions?", "Options": {"A": "If information asymmetry is extreme (`\\bar{v} = 35`), the dealer exits the market by setting the depth `z = 0`.", "B": "If information asymmetry is high (`\\bar{v} = 24`), the dealer sets the ask price `a = 8` and the depth `z = 1`.", "C": "If information asymmetry is low (`\\bar{v} = 5`), the dealer sets the ask price `a = 5` and does not impose a quantity limit.", "D": "If information asymmetry is moderate (`\\bar{v} = 9`), the dealer sets the ask price `a = 6.5` and the depth `z = 3.5`."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to apply the regime-dependent formulas from the provided table to specific numerical scenarios. It requires identifying the correct regime for a given set of parameters and then performing the calculations for optimal price and depth. \nDepth Strategy: Computational Judgment. The user must perform a series of calculations for different scenarios to determine the validity of each option. \nDistractor Logic: Option C is a distractor based on a Formula Misuse error. The user correctly identifies the regime (`3/5 \\bar{\\eta} < \\bar{v} \\le 3 \\bar{\\eta}`), but then misapplies the formulas for `a` and `z`, leading to an incorrect result. Options A, B, and D are all correct applications of the table's logic to their respective regimes.", "qid": "174", "question": "### Background\n\nA market maker's optimal strategy depends on the balance between information asymmetry and liquidity demand. In a simplified market with discrete outcomes, a dealer faces a risk-neutral informed trader and a liquidity trader. The dealer must choose an ask price `a` and an ask depth `z`.\n\n**Variables & Parameters.**\n- `a`: Ask price.\n- `z`: Ask depth (quantity limit).\n- `\\bar{v}`: The informed trader's positive valuation, a measure of information asymmetry.\n- `\\bar{\\eta}`: The positive component of the liquidity shock, a measure of potential liquidity demand.\n\n---\n\n### Data / Model Specification\n\nThe dealer's optimal strategy falls into different regimes based on the ratio `\\bar{v}/\\bar{\\eta}`.\n\n**Table 1: Equilibrium Price and Depth under Discrete Distributions**\n\n| Condition | `1/2 \\bar{\\eta} < \\bar{v} \\le 3/5 \\bar{\\eta}` | `3/5 \\bar{\\eta} < \\bar{v} \\le 3 \\bar{\\eta}` | `\\bar{v} > 3 \\bar{\\eta}` |\n| :--- | :--- | :--- | :--- |\n| **Optimal `a`** | `\\bar{v}` | `\\frac{1}{2}\\bar{\\eta}+\\frac{1}{6}\\bar{v}` | n.a. |\n| **Optimal `z`** | n.a. (effectively infinite) | `\\frac{1}{2}\\bar{\\eta}-\\frac{1}{6}\\bar{v}` | 0 |\n\n---\n\nConsider a scenario where `\\bar{\\eta} = 10`. According to the model specifications in Table 1, which of the following statements accurately describe the dealer's optimal strategy under the given market conditions?", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 216, "Question": "### Background\n\n**Research Question.** Can circuit breakers effectively stabilize financial markets by taming endogenous volatility, and how does this conclusion hold up against the efficient-market critique that such interventions merely delay necessary price discovery, especially when the fundamental value is non-stationary?\n\n**Setting.** The paper's agent-based model is used as a laboratory to study the effects of circuit breakers. The analysis is made more realistic by allowing the log fundamental value `F_t` to evolve as a random walk, which provides a framework for evaluating Fama's critique that circuit breakers hinder the market's ability to track new information.\n\n**Variables and Parameters.**\n- `P_t`: Log price of the stock at time `t`.\n- `F_t`: Log fundamental value at time `t`.\n- `n_t`: A normally distributed fundamental shock, `n_t \\sim N(0, (\\sigma^F)^2)`.\n- `s`: The circuit breaker threshold, i.e., the maximum allowed absolute log price change.\n- `V_t`: Market volatility, an EWMA of squared price changes.\n- `\\sigma_t^2`: Trading intensity (variance of speculator demand).\n- `\\rho_t`: Correlation of speculator demands (herding).\n- `C_t`: A condition measuring sharp price reversals.\n- Distortion: A measure of the deviation between `P_t` and `F_t`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental value evolves as a random walk:\n  \nF_t = F_{t-1} + n_t \\quad \\text{(Eq. 1)}\n \nThe potential price `P_{t+1}^*` is calculated based on aggregate demand, and the circuit breaker rule determines the actual price `P_{t+1}`:\n  \nP_{t+1} = \\begin{cases} P_t + s & \\text{if } (P_{t+1}^* - P_t) > s \\\\ P_t - s & \\text{if } (P_{t+1}^* - P_t) < -s \\\\ P_{t+1}^* & \\text{otherwise} \\end{cases} \\quad \\text{(Eq. 2)}\n \nKey endogenous feedback variables are defined as:\n  \nV_{t} = d V_{t-1} + (1-d)(P_{t}-P_{t-1})^{2} \\quad \\text{(Eq. 3)}\n \n  \n\\sigma_{t}^{2} = e^{l} + \\frac{e^{h}-e^{l}}{1+\\exp[-e^{s}(V_{t}-\\bar{V})]} \\quad \\text{(Eq. 4)}\n \n  \nC_{t} = ((P_{t}-P_{t-1})-(P_{t-1}-P_{t-2}))^{2} \\quad \\text{(Eq. 5)}\n \n  \n\\rho_{t} = f^{l} + \\frac{f^{h}-f^{l}}{1+\\exp[-f^{s}(C_{t}-\\bar{C})]} \\quad \\text{(Eq. 6)}\n \n\n---\n\n### Question\n\nAccording to the model presented, circuit breakers stabilize the market through both direct and indirect effects. Select all statements that accurately describe an **indirect** mechanism by which circuit breakers reduce market volatility.", "Options": {"A": "They cause unfulfilled orders to be executed in the next period with higher priority, leading to a faster resolution of price discovery.", "B": "By capping the magnitude of price changes, they prevent the volatility measure `V_t` from rising excessively, which in turn suppresses speculators' trading intensity `\\sigma_t^2`.", "C": "They directly reduce the fundamental trading parameter `c`, making speculators less responsive to mispricing.", "D": "By limiting the size of consecutive price swings, they cap the value of the reversal condition `C_t`, which prevents the correlation `\\rho_t` from jumping to high levels associated with herding."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the understanding of the specific indirect feedback mechanisms through which circuit breakers operate in the model. It uses an **Atomic Decomposition** strategy, breaking down the complex answer from the original QA into distinct, verifiable propositions. **Correct options (A, C)** describe the two primary indirect channels: dampening the volatility-intensity loop and preventing the reversal-herding loop. **Distractors** are designed to be high-fidelity: (B) presents a plausible but incorrect mechanism by confusing variables with parameters (Conceptual Error), while (D) introduces a false premise about order execution that aligns with the 'volatility spillover' critique the paper argues against (Conceptual Opposite).", "qid": "216", "question": "### Background\n\n**Research Question.** Can circuit breakers effectively stabilize financial markets by taming endogenous volatility, and how does this conclusion hold up against the efficient-market critique that such interventions merely delay necessary price discovery, especially when the fundamental value is non-stationary?\n\n**Setting.** The paper's agent-based model is used as a laboratory to study the effects of circuit breakers. The analysis is made more realistic by allowing the log fundamental value `F_t` to evolve as a random walk, which provides a framework for evaluating Fama's critique that circuit breakers hinder the market's ability to track new information.\n\n**Variables and Parameters.**\n- `P_t`: Log price of the stock at time `t`.\n- `F_t`: Log fundamental value at time `t`.\n- `n_t`: A normally distributed fundamental shock, `n_t \\sim N(0, (\\sigma^F)^2)`.\n- `s`: The circuit breaker threshold, i.e., the maximum allowed absolute log price change.\n- `V_t`: Market volatility, an EWMA of squared price changes.\n- `\\sigma_t^2`: Trading intensity (variance of speculator demand).\n- `\\rho_t`: Correlation of speculator demands (herding).\n- `C_t`: A condition measuring sharp price reversals.\n- Distortion: A measure of the deviation between `P_t` and `F_t`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental value evolves as a random walk:\n  \nF_t = F_{t-1} + n_t \\quad \\text{(Eq. 1)}\n \nThe potential price `P_{t+1}^*` is calculated based on aggregate demand, and the circuit breaker rule determines the actual price `P_{t+1}`:\n  \nP_{t+1} = \\begin{cases} P_t + s & \\text{if } (P_{t+1}^* - P_t) > s \\\\ P_t - s & \\text{if } (P_{t+1}^* - P_t) < -s \\\\ P_{t+1}^* & \\text{otherwise} \\end{cases} \\quad \\text{(Eq. 2)}\n \nKey endogenous feedback variables are defined as:\n  \nV_{t} = d V_{t-1} + (1-d)(P_{t}-P_{t-1})^{2} \\quad \\text{(Eq. 3)}\n \n  \n\\sigma_{t}^{2} = e^{l} + \\frac{e^{h}-e^{l}}{1+\\exp[-e^{s}(V_{t}-\\bar{V})]} \\quad \\text{(Eq. 4)}\n \n  \nC_{t} = ((P_{t}-P_{t-1})-(P_{t-1}-P_{t-2}))^{2} \\quad \\text{(Eq. 5)}\n \n  \n\\rho_{t} = f^{l} + \\frac{f^{h}-f^{l}}{1+\\exp[-f^{s}(C_{t}-\\bar{C})]} \\quad \\text{(Eq. 6)}\n \n\n---\n\n### Question\n\nAccording to the model presented, circuit breakers stabilize the market through both direct and indirect effects. Select all statements that accurately describe an **indirect** mechanism by which circuit breakers reduce market volatility.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 287, "Question": "### Background\n\n**Research Question.** Do corporate insiders systematically engage in abnormal trading ahead of value-relevant corporate events, such as an exchange listing announcement, and how can this be tested empirically?\n\n**Setting.** The study examines insider trading around the month of a firm's application for exchange listing (event month 0). It calculates the mean abnormal number of net purchasers (`MANP_t`) for a sample of firms in each month relative to the announcement.\n\n**Variables & Parameters.**\n- `MANP_t`: Mean abnormal number of net purchasers in event month `t`.\n- `CUMNP_t`: Cumulated mean abnormal number of net purchasers over a specified window.\n- Event Month 0: The month of the first public announcement of a formal listing application.\n\n---\n\n### Data / Model Specification\n\nAbnormal insider trading is measured using `MANP_t`, the cross-sectional average of firm-level abnormal net purchasers. A positive `MANP_t` indicates net buying activity above historical firm-specific norms. The statistical significance of this activity is assessed with a t-statistic.\n\n**Table 1. Mean Abnormal Net Insider Purchasing Activity (NYSE & AMEX Listers)**\n\n| Event Month(s) | `MANP_t` | t-statistic |\n|:---------------|:-----------|:------------|\n| -12            | 0.405      | 2.480**     |\n| -11            | 0.426      | 2.624***    |\n| -6             | 0.436      | 2.803***    |\n| -1             | 0.071      | 0.476       |\n| 0              | 0.435      | 2.941***    |\n| +1             | 0.616      | 4.231***    |\n| **[-12, -1]**  | **3.082**  | **5.692***  |\n\n*Note: Table is an abridged version of the original. `CUMNP` is reported for summary rows. ***p<0.01, **p<0.05.*\n\n---\n\nBased on the data in Table 1 and the study's context, which of the following statements are valid interpretations or conclusions?", "Options": {"A": "The cumulative measure `CUMNP` for the period [-12, -1] provides stronger statistical evidence of systematic pre-announcement buying than any single month's `MANP` in that period.", "B": "The pattern of significant `MANP` values dispersed over the pre-announcement year suggests that insiders may possess and act on information about a potential listing long before it is publicly announced.", "C": "Significant abnormal insider purchasing activity is observed both in the month of the listing announcement (month 0) and the month immediately following it (month +1).", "D": "The data show that abnormal insider buying is concentrated in the month immediately prior to the listing announcement (month -1)."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to interpret statistical results from a table (`MANP` and `CUMNP`), synthesize patterns over time, and draw valid conclusions about insider behavior. Depth Strategy: Reverse-Reasoning. Given the empirical results, the user must identify the plausible conclusions. Distractor Logic: Option B is a 'Conceptual Opposite' distractor. The table explicitly shows that `MANP` in month -1 is small and statistically insignificant, contradicting the idea of concentrated buying. The correct options require recognizing the power of cumulative statistics (A), identifying significant point estimates (C), and interpreting the overall temporal pattern (D).", "qid": "287", "question": "### Background\n\n**Research Question.** Do corporate insiders systematically engage in abnormal trading ahead of value-relevant corporate events, such as an exchange listing announcement, and how can this be tested empirically?\n\n**Setting.** The study examines insider trading around the month of a firm's application for exchange listing (event month 0). It calculates the mean abnormal number of net purchasers (`MANP_t`) for a sample of firms in each month relative to the announcement.\n\n**Variables & Parameters.**\n- `MANP_t`: Mean abnormal number of net purchasers in event month `t`.\n- `CUMNP_t`: Cumulated mean abnormal number of net purchasers over a specified window.\n- Event Month 0: The month of the first public announcement of a formal listing application.\n\n---\n\n### Data / Model Specification\n\nAbnormal insider trading is measured using `MANP_t`, the cross-sectional average of firm-level abnormal net purchasers. A positive `MANP_t` indicates net buying activity above historical firm-specific norms. The statistical significance of this activity is assessed with a t-statistic.\n\n**Table 1. Mean Abnormal Net Insider Purchasing Activity (NYSE & AMEX Listers)**\n\n| Event Month(s) | `MANP_t` | t-statistic |\n|:---------------|:-----------|:------------|\n| -12            | 0.405      | 2.480**     |\n| -11            | 0.426      | 2.624***    |\n| -6             | 0.436      | 2.803***    |\n| -1             | 0.071      | 0.476       |\n| 0              | 0.435      | 2.941***    |\n| +1             | 0.616      | 4.231***    |\n| **[-12, -1]**  | **3.082**  | **5.692***  |\n\n*Note: Table is an abridged version of the original. `CUMNP` is reported for summary rows. ***p<0.01, **p<0.05.*\n\n---\n\nBased on the data in Table 1 and the study's context, which of the following statements are valid interpretations or conclusions?", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 233, "Question": "### Background\n\nThe Ingersoll model provides a utility-theoretic framework for the subjective valuation of ESOs. It modifies the standard Black-Scholes-Merton formula by transforming the risk-free rate (`r`) and dividend yield (`q`) to reflect an executive's risk aversion and portfolio constraints.\n\n### Data / Model Specification\n\nThe transformed rates, `r'` and `q'`, are given by:\n\n  \nq^{\\prime} = q + \\alpha(1-\\theta)\\theta\\nu^{2} \\quad \\text{(Eq. (1))}\n \n  \nr^{\\prime} = r - \\alpha\\theta^{2}\\nu^{2} \\quad \\text{(Eq. (2))}\n \n\nwhere `α` is risk aversion, `θ` is excess stock holdings, and `ν` is residual stock risk. For a constrained executive, `α > 0`, `0 < θ < 1`, and `ν > 0`.\n\n---\n\nBased on these equations, which of the following conclusions about the Ingersoll model are valid?\n", "Options": {"A": "The subjective risk-free rate `r'` is lower than the market rate `r`, reflecting a utility cost or drag on wealth growth due to the executive's forced holding of undiversified risk.", "B": "In the boundary case where an executive's wealth is almost entirely concentrated in the firm's stock (`θ` approaches 1), the adjustment to the dividend yield (`q' - q`) is maximized.", "C": "The subjective dividend yield `q'` is higher than the market yield `q`, reflecting a utility benefit from any event (like a dividend) that reduces the executive's painful risk exposure.", "D": "In the boundary case where an executive's portfolio is perfectly diversified (`θ` approaches 0), the subjective rates `r'` and `q'` converge to the market rates `r` and `q`, and the subjective value equals the risk-neutral value."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses the economic intuition and boundary-case analysis of the Ingersoll model, a core theoretical component of the paper. It uses a Reverse-Reasoning strategy, asking for valid conclusions derived from the model's equations. Options A and B correctly state the economic interpretation of the transformed rates. Option C correctly analyzes the `θ → 0` boundary case, where the model collapses to the standard risk-neutral framework. Option D is a subtle distractor based on a misinterpretation of the `(1-θ)θ` term in Eq. (1); this term is maximized at `θ=0.5` and approaches zero as `θ → 1`, meaning the dividend yield adjustment disappears at full concentration.", "qid": "233", "question": "### Background\n\nThe Ingersoll model provides a utility-theoretic framework for the subjective valuation of ESOs. It modifies the standard Black-Scholes-Merton formula by transforming the risk-free rate (`r`) and dividend yield (`q`) to reflect an executive's risk aversion and portfolio constraints.\n\n### Data / Model Specification\n\nThe transformed rates, `r'` and `q'`, are given by:\n\n  \nq^{\\prime} = q + \\alpha(1-\\theta)\\theta\\nu^{2} \\quad \\text{(Eq. (1))}\n \n  \nr^{\\prime} = r - \\alpha\\theta^{2}\\nu^{2} \\quad \\text{(Eq. (2))}\n \n\nwhere `α` is risk aversion, `θ` is excess stock holdings, and `ν` is residual stock risk. For a constrained executive, `α > 0`, `0 < θ < 1`, and `ν > 0`.\n\n---\n\nBased on these equations, which of the following conclusions about the Ingersoll model are valid?\n", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 196, "Question": "### Background\n\nAn assistant professor is developing a submission strategy for a new working paper. The professor is facing a fixed tenure deadline and needs to secure acceptances efficiently. The analysis focuses on articles published between 2012 and 2015 in the top five general-interest finance journals.\n\n### Data / Model Specification\n\nThe study's findings for the median acceptance times of these journals are summarized below.\n\n**Table 1. Median Acceptance Times for Top Five General-Interest Finance Journals (2012-2015)**\n\n| Journal | Median Acceptance Time (Months) |\n| :--- | :--- |\n| Journal of Financial Economics (JFE) | 9.9 |\n| Journal of Financial and Quantitative Analysis (JFQA) | 10.6 |\n| Review of Finance (RF) | 11.7 |\n| Review of Financial Studies (RFS) | 15.5 |\n| Journal of Finance (JF) | 19.8 |\n\n---\n\nBased on the data in **Table 1**, which of the following statements are valid conclusions or strategically sound applications of the findings?\n\nSelect all that apply.", "Options": {"A": "For a high-risk, controversial paper, submitting to JFE or JFQA first is a strategically sound approach to get a quick, high-quality signal on its viability.", "B": "For a methodologically 'safe' paper with a high probability of eventual acceptance, submitting to RFS or JF first is a reasonable strategy to maximize journal prestige, as the time-cost risk is lower.", "C": "The median time to acceptance at the Journal of Finance (JF) is approximately twice as long as the median time at the Journal of Financial Economics (JFE).", "D": "The difference in median acceptance time between the slowest journal (JF) and the fastest journal (JFE) is 9.9 months."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to perform a simple calculation, interpret its magnitude, and apply the findings to solve the 'journal ordering problem' under different scenarios. It combines computational judgment and scenario application.\n\nChosen Strategy: Scenario Application. The question asks the user to apply the data to different strategic situations faced by a researcher.\n\nDistractor Logic: All options are designed to be correct to test comprehensive understanding. This is a less common but valid use of the Multiple Response format, requiring the user to validate every single statement against the data and the strategic context provided in the original QA problem. It forces a thorough evaluation of all aspects of the original problem's solution.", "qid": "196", "question": "### Background\n\nAn assistant professor is developing a submission strategy for a new working paper. The professor is facing a fixed tenure deadline and needs to secure acceptances efficiently. The analysis focuses on articles published between 2012 and 2015 in the top five general-interest finance journals.\n\n### Data / Model Specification\n\nThe study's findings for the median acceptance times of these journals are summarized below.\n\n**Table 1. Median Acceptance Times for Top Five General-Interest Finance Journals (2012-2015)**\n\n| Journal | Median Acceptance Time (Months) |\n| :--- | :--- |\n| Journal of Financial Economics (JFE) | 9.9 |\n| Journal of Financial and Quantitative Analysis (JFQA) | 10.6 |\n| Review of Finance (RF) | 11.7 |\n| Review of Financial Studies (RFS) | 15.5 |\n| Journal of Finance (JF) | 19.8 |\n\n---\n\nBased on the data in **Table 1**, which of the following statements are valid conclusions or strategically sound applications of the findings?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 252, "Question": "### Background\n\n**Research Question.** What are the fundamental conditions that ensure valuation equivalence between accounting-based (e.g., Residual Income, RI) and cash-flow-based (e.g., Free Cash Flow, FCF) models?\n\n**Setting.** The paper argues that both RI and FCF models can be viewed as discounted dividend models. Therefore, for their valuations to be equivalent, both the numerators (expected dividends) and the denominators (the product of discount factors) in their respective valuation formulas must be identical.\n\n---\n\n### Question\n\nAccording to the paper, which of the following statements correctly describe the jointly sufficient conditions for ensuring valuation equivalence between Residual Income (RI) and Free Cash Flow (FCF) models? (Select all that apply)", "Options": {"A": "Imposing the Modigliani-Miller (MM) condition on discount rates is necessary to align the denominators (discount factors) of the valuation formulas.", "B": "Imposing the Modigliani-Miller (MM) condition on discount rates is necessary to align the numerators (expected dividends) of the valuation formulas.", "C": "Enforcing the income statement and balance sheet identities on forecasts is necessary to align the numerators (expected dividends) of the valuation formulas.", "D": "Using the exact same pro-forma spreadsheet for both models is the only way to ensure the accounting identities hold."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This question tests the recall and understanding of the paper's central thesis—the two core conditions for valuation equivalence. It specifically assesses whether the student can correctly map each condition to its role in the valuation formula (numerator vs. denominator).\nStrategy: This is a select-all-valid task.\nDistractor Logic:\n- A & B are the two correct conditions and their correct mappings.\n- C (Conceptual Mismatch): This distractor incorrectly maps the MM condition to the numerator, testing for a precise understanding of its role.\n- D (Overly Restrictive): This distractor presents a sufficient but not necessary condition. The paper explicitly notes that other forecasting methods (like a constrained VAR) can also enforce the identities, so stating pro-formas are the *only* way is incorrect.", "qid": "252", "question": "### Background\n\n**Research Question.** What are the fundamental conditions that ensure valuation equivalence between accounting-based (e.g., Residual Income, RI) and cash-flow-based (e.g., Free Cash Flow, FCF) models?\n\n**Setting.** The paper argues that both RI and FCF models can be viewed as discounted dividend models. Therefore, for their valuations to be equivalent, both the numerators (expected dividends) and the denominators (the product of discount factors) in their respective valuation formulas must be identical.\n\n---\n\n### Question\n\nAccording to the paper, which of the following statements correctly describe the jointly sufficient conditions for ensuring valuation equivalence between Residual Income (RI) and Free Cash Flow (FCF) models? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 278, "Question": "### Background\n\n**Research Question.** This study investigates why the pass-through of exchange rate changes to investment good prices varies so significantly across industries. A key empirical challenge is to distinguish between competing explanations for this variation.\n\n**Setting / Data-Generating Environment.** The analysis uses panel data for various industrial sectors across 12 OECD countries. The paper documents a strong positive correlation between an industry's estimated exchange rate pass-through and its reliance on imported capital goods.\n\n---\n\n### Data / Model Specification\n\nThe paper reports a simple correlation of 0.87 between an industry's exchange rate pass-through and its share of imported investment goods (`S_i`). However, it also notes that industries with high pass-through are often classified as 'high-technology' (`H_i`). This raises a question of whether the import share channel is causal or merely correlated with the true driver (technology level).\n\n**Table 1: Pass-Through and Import Shares for Selected Industries**\n\n| Industry                                    | Predicted Price Impact of 10% Depreciation | Percent of Investment Goods Imported (`S_i`) |\n| :------------------------------------------ | :----------------------------------------- | :------------------------------------------- |\n| Transport, storage and communication        | 3.15%                                      | 51.0%                                        |\n| Finance, insurance, real estate & business  | 0.24%                                      | 2.3%                                         |\n\n*Source: Adapted from the paper's Table 2.*\n\n---\n\n### Question\n\nBased on the principles of causal inference and the context provided, which of the following statements accurately describe the challenge of identifying the true causal mechanism behind high pass-through?", "Options": {"A": "The high correlation of 0.87 is sufficient evidence to conclude that a higher import share (`S_i`) causes higher pass-through, making the role of technology level (`H_i`) secondary.", "B": "A plausible economic reason for technology level (`H_i`) to independently cause higher pass-through is that specialized high-tech capital goods often lack domestic substitutes, making demand for them less price-elastic.", "C": "For technology level (`H_i`) to be a valid confounder of the relationship between import share (`S_i`) and pass-through, high-technology industries must systematically have different import shares than low-technology industries.", "D": "In a regression of pass-through on both `S_i` and `H_i`, a statistically significant coefficient on `H_i` would prove that the import share channel is entirely spurious."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of causal inference, specifically the concept of confounding, which was central to the original QA problem. It uses the 'Premise/assumption packaging' strategy by asking for the necessary conditions for confounding. Options A and B correctly state the two conditions for confounding in this context. Distractor C represents a classic 'correlation is not causation' error. Distractor D presents an overly strong conclusion, a common misinterpretation of regression results where multiple factors can have partial effects.", "qid": "278", "question": "### Background\n\n**Research Question.** This study investigates why the pass-through of exchange rate changes to investment good prices varies so significantly across industries. A key empirical challenge is to distinguish between competing explanations for this variation.\n\n**Setting / Data-Generating Environment.** The analysis uses panel data for various industrial sectors across 12 OECD countries. The paper documents a strong positive correlation between an industry's estimated exchange rate pass-through and its reliance on imported capital goods.\n\n---\n\n### Data / Model Specification\n\nThe paper reports a simple correlation of 0.87 between an industry's exchange rate pass-through and its share of imported investment goods (`S_i`). However, it also notes that industries with high pass-through are often classified as 'high-technology' (`H_i`). This raises a question of whether the import share channel is causal or merely correlated with the true driver (technology level).\n\n**Table 1: Pass-Through and Import Shares for Selected Industries**\n\n| Industry                                    | Predicted Price Impact of 10% Depreciation | Percent of Investment Goods Imported (`S_i`) |\n| :------------------------------------------ | :----------------------------------------- | :------------------------------------------- |\n| Transport, storage and communication        | 3.15%                                      | 51.0%                                        |\n| Finance, insurance, real estate & business  | 0.24%                                      | 2.3%                                         |\n\n*Source: Adapted from the paper's Table 2.*\n\n---\n\n### Question\n\nBased on the principles of causal inference and the context provided, which of the following statements accurately describe the challenge of identifying the true causal mechanism behind high pass-through?", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 154, "Question": "### Background\n\n**Research Question.** This case investigates whether an unexpected decline in asset market liquidity was a primary, causal factor in the financial distress of firms that had undergone highly leveraged recapitalizations (recaps). The analysis aims to distinguish between two possibilities: 1) distressed firms were forced to conduct 'fire sales' of assets at unexpectedly low prices due to an illiquid market, or 2) these firms simply sold undesirable assets, leading to poor outcomes.\n\n**Setting / Data-Generating Environment.** The study examines the stock market's reaction to asset sale announcements made by recap firms. A key test partitions the sample based on the selling firm's financial condition (Coverage Ratio) and the health of its industry.\n\n### Data / Model Specification\n\nThe market's perception of asset sales is measured by the 3-day cumulative abnormal return (CAR) around the sale announcement.\n\n**Table 1: Average 3-Day CARs for Asset Sale Announcements**\n| Firm Condition | Industry Condition: Healthy | Industry Condition: Troubled |\n|---|:---:|:---:|\n| **Coverage Ratio > 1** (Financially Healthy) | 0.88% | 0.65% |\n| **Coverage Ratio < 1** (Financially Weak) | 0.59% | -1.82% |\n\n*Source: Adapted from Table 3 of the study. 'Troubled' industries are those where peer firms had negative earnings changes.* \n\n### Question\n\nBased on the data in **Table 1** and the study's logic, which of the following statements are correct interpretations of the evidence?\n", "Options": {"A": "The results indicate that any asset sale conducted within a troubled industry is perceived negatively by the market, regardless of the seller's financial health.", "B": "The large negative CAR (-1.82%) is concentrated in the group of financially weak firms selling assets in troubled industries, supporting the 'fire sale' hypothesis that constrained sellers in illiquid markets receive unexpectedly low prices.", "C": "The positive CAR (0.65%) for financially healthy firms in troubled industries suggests that the market views these firms as strategically divesting non-core assets at favorable prices.", "D": "The positive CAR (0.59%) for financially weak firms in healthy industries contradicts the 'poor asset' hypothesis, which would predict a negative reaction regardless of industry condition."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item tests the ability to interpret a 2x2 difference-in-differences research design used to distinguish between competing hypotheses ('fire sale' vs. 'poor asset').\nStrategy: Atomic Decomposition. The core conclusion is broken down into independently verifiable statements about the table's cells.\nDistractor Logic:\n- C is a Conceptual Opposite; it ignores the positive CAR (0.65%) for healthy firms in troubled industries.\n- D is True but Irrelevant/Misleading. While the CAR is positive, the key insight comes from comparing it to the -1.82% for weak firms in the same industry, which isolates the effect of financial constraint. The option offers a plausible but incomplete interpretation.", "qid": "154", "question": "### Background\n\n**Research Question.** This case investigates whether an unexpected decline in asset market liquidity was a primary, causal factor in the financial distress of firms that had undergone highly leveraged recapitalizations (recaps). The analysis aims to distinguish between two possibilities: 1) distressed firms were forced to conduct 'fire sales' of assets at unexpectedly low prices due to an illiquid market, or 2) these firms simply sold undesirable assets, leading to poor outcomes.\n\n**Setting / Data-Generating Environment.** The study examines the stock market's reaction to asset sale announcements made by recap firms. A key test partitions the sample based on the selling firm's financial condition (Coverage Ratio) and the health of its industry.\n\n### Data / Model Specification\n\nThe market's perception of asset sales is measured by the 3-day cumulative abnormal return (CAR) around the sale announcement.\n\n**Table 1: Average 3-Day CARs for Asset Sale Announcements**\n| Firm Condition | Industry Condition: Healthy | Industry Condition: Troubled |\n|---|:---:|:---:|\n| **Coverage Ratio > 1** (Financially Healthy) | 0.88% | 0.65% |\n| **Coverage Ratio < 1** (Financially Weak) | 0.59% | -1.82% |\n\n*Source: Adapted from Table 3 of the study. 'Troubled' industries are those where peer firms had negative earnings changes.* \n\n### Question\n\nBased on the data in **Table 1** and the study's logic, which of the following statements are correct interpretations of the evidence?\n", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 327, "Question": "### Background\n\n**Research Question.** This case provides a comprehensive empirical evaluation of the consumption-based asset pricing model (CCAPM), testing whether incorporating taxes and tax-timing options can resolve its well-known empirical failures.\n\n**Setting.** The analysis compares the benchmark tax-free CCAPM (Model 1) against the paper's preferred specification, an asymmetric tax model where losses are realized immediately and gains are realized after one year at a preferential rate (Model 4). Both models are estimated using the Generalized Method of Moments (GMM) on monthly U.S. data from 1959-1990 for an equity portfolio and a one-month T-bill.\n\n**Variables and Parameters.**\n\n*   `β`: Subjective time discount factor.\n*   `γ`: Utility parameter, where `1-γ` is the coefficient of relative risk aversion (RRA).\n*   `τ_c`: Short-term capital gains tax rate.\n*   `τ_d`: Tax rate on dividend/interest income.\n*   `J-statistic (χ²)`: GMM test for a model's overidentifying restrictions. A high value (low p-value) indicates model rejection.\n\n---\n\n### Data / Model Specification\n\nThe GMM estimation results for the tax-free and asymmetric tax models are summarized in Table 1.\n\n**Table 1: GMM Estimation Results for Competing CCAPM Specifications**\n\n| Parameter / Statistic | Model 1 (Tax-Free) | Model 4 (Asymmetric Tax) |\n| :--- | :--- | :--- |\n| `β` (Time Preference) | 0.998 (0.001) | 1.000 (0.001) |\n| `γ` (Utility Parameter) | 1.176 (0.371) | 0.982 (0.114) |\n| `τ_c` (Capital Gains Tax) | - | 0.099 (0.022) |\n| `τ_d` (Dividend Tax) | - | 0.320 (0.113) |\n| **Goodness-of-fit** | | |\n| `J-statistic (χ²)` | 33.424 | 11.154 |\n| Degrees of Freedom | 6 | 4 |\n| p-value | (< 0.001) | (0.025) |\n\n*Standard errors are in parentheses.* \n\n---\n\n### Question\n\nBased on the GMM estimation results in Table 1, select ALL statements that correctly interpret the parameter estimates and their implications for the models.", "Options": {"A": "Model 1 implies a negative coefficient of relative risk aversion (1 - 1.176 = -0.176), which contradicts the standard economic assumption of risk-averse agents.", "B": "The utility parameter γ is statistically indistinguishable between Model 1 (1.176) and Model 4 (0.982), indicating that taxes do not resolve the risk aversion puzzle.", "C": "In Model 4, the dividend tax rate (τ_d = 32.0%) is not statistically significant at the 5% level, weakening the case for including taxes.", "D": "Model 4 produces a statistically significant and positive estimate for the short-term capital gains tax rate (τ_c = 9.9%), a major improvement in economic plausibility over the tax-free model."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret GMM parameter estimates and their statistical and economic significance. It uses an **Atomic Decomposition** strategy, breaking down the multifaceted comparison from the original QA into discrete, testable propositions. The depth strategy is **Computational Judgment**, as it requires calculating the RRA and t-statistics to evaluate the options. Distractors are high-fidelity: Option C is a **Numerical (Comprehension Slip)** error, as the t-statistic (0.320/0.113 ≈ 2.83) is significant. Option D is a **Conceptual (Misinterpretation)** error, as the point estimates imply qualitatively different economic behavior (risk-loving vs. risk-averse) and are statistically different.", "qid": "327", "question": "### Background\n\n**Research Question.** This case provides a comprehensive empirical evaluation of the consumption-based asset pricing model (CCAPM), testing whether incorporating taxes and tax-timing options can resolve its well-known empirical failures.\n\n**Setting.** The analysis compares the benchmark tax-free CCAPM (Model 1) against the paper's preferred specification, an asymmetric tax model where losses are realized immediately and gains are realized after one year at a preferential rate (Model 4). Both models are estimated using the Generalized Method of Moments (GMM) on monthly U.S. data from 1959-1990 for an equity portfolio and a one-month T-bill.\n\n**Variables and Parameters.**\n\n*   `β`: Subjective time discount factor.\n*   `γ`: Utility parameter, where `1-γ` is the coefficient of relative risk aversion (RRA).\n*   `τ_c`: Short-term capital gains tax rate.\n*   `τ_d`: Tax rate on dividend/interest income.\n*   `J-statistic (χ²)`: GMM test for a model's overidentifying restrictions. A high value (low p-value) indicates model rejection.\n\n---\n\n### Data / Model Specification\n\nThe GMM estimation results for the tax-free and asymmetric tax models are summarized in Table 1.\n\n**Table 1: GMM Estimation Results for Competing CCAPM Specifications**\n\n| Parameter / Statistic | Model 1 (Tax-Free) | Model 4 (Asymmetric Tax) |\n| :--- | :--- | :--- |\n| `β` (Time Preference) | 0.998 (0.001) | 1.000 (0.001) |\n| `γ` (Utility Parameter) | 1.176 (0.371) | 0.982 (0.114) |\n| `τ_c` (Capital Gains Tax) | - | 0.099 (0.022) |\n| `τ_d` (Dividend Tax) | - | 0.320 (0.113) |\n| **Goodness-of-fit** | | |\n| `J-statistic (χ²)` | 33.424 | 11.154 |\n| Degrees of Freedom | 6 | 4 |\n| p-value | (< 0.001) | (0.025) |\n\n*Standard errors are in parentheses.* \n\n---\n\n### Question\n\nBased on the GMM estimation results in Table 1, select ALL statements that correctly interpret the parameter estimates and their implications for the models.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 334, "Question": "### Background\n\n**Research Question.** A central puzzle in international finance is that different stationary regressions testing the forward exchange rate unbiasedness hypothesis yield conflicting results. The standard forward premium regression typically rejects unbiasedness, while a seemingly minor modification appears to support it. Can this conflict be explained by differences in the magnitude of simultaneity bias across specifications?\n\n**Setting.** An econometric analysis of two stationary models. The first is the standard forward premium regression of the spot rate change `S_t - S_{t-1}` on the forward premium `F_{t-1} - S_{t-1}`. The second is a modified version, regressing `S_t - S_{t-2}` on `F_{t-1} - S_{t-2}`. Under the null hypothesis of unbiasedness, the true slope coefficient `\\beta` is 1 in both models.\n\n**Variables & Parameters.**\n- `S_t`, `S_{t-1}`, `S_{t-2}`: Log spot exchange rates.\n- `F_{t-1}`: Log forward exchange rate.\n- `\\eta_t`: Innovation to the spot rate process, `\\eta_t = S_t - S_{t-1}`.\n- `e_{t-1}`: Innovation to the forward rate process, `e_{t-1} = F_{t-1} - S_{t-1}`.\n- `\\beta`: The slope coefficient in the regression.\n\n---\n\n### Data / Model Specification\n\nThe analysis hinges on the statistical properties of the spot and forward rate innovations, particularly the \"relative error variance characteristic\"—the empirical fact that the variance of the spot rate innovation is vastly larger than that of the forward rate innovation.\n\n**Table 1. Summary Statistics of DGP Residuals (UK)**\n| Statistic | Spot Rate Innovation (`\\eta_t`) | Forward Rate Innovation (`e_{t-1}`) |\n| :--- | :--- | :--- |\n| Variance | 0.0010692 | 0.0000078 |\n| Covariance `cov(e_{t-1}, \\eta_t)` | | -0.000013 |\n\n**Asymptotic Bias of OLS Estimator `\\hat{\\beta}`:**\n- **Standard Regression (`S_t - S_{t-1}` on `F_{t-1} - S_{t-1}`):**\n      \n    \\text{Bias} = \\frac{\\mathrm{cov}(e_{t-1}, \\eta_t)}{\\mathrm{var}(e_{t-1})} - 1 \\quad \\text{(Eq. (1))}\n     \n- **Modified Regression (`S_t - S_{t-2}` on `F_{t-1} - S_{t-2}`):**\n      \n    \\text{Bias} = \\frac{\\mathrm{cov}(e_{t-1}, \\eta_t) - \\mathrm{var}(e_{t-1})}{\\mathrm{var}(\\eta_{t-1}) + \\mathrm{var}(e_{t-1})} \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nUsing the provided data for the UK and the asymptotic bias formulas (where `plim(\\hat{\\beta}) = 1 + \\text{Bias}`), which of the following statements are correct? Select all that apply.", "Options": {"A": "Both regression specifications suffer from equally severe simultaneity bias, as the numerators of their respective bias formulas are nearly identical.", "B": "The asymptotic bias in the standard forward premium regression (Eq. (1)) is approximately -1.67.", "C": "The modified regression (Eq. (2)) exhibits negligible bias because the large spot rate innovation variance, `var(\\eta_{t-1})`, inflates the denominator of the bias formula, mitigating the endogeneity problem.", "D": "The probability limit of the slope coefficient in the standard forward premium regression (Eq. (1)) is approximately -1.67, indicating a bias strong enough to flip the coefficient's sign."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to calculate and compare asymptotic biases for two different stationary regression models. It employs a Computational Judgment strategy with atomic decomposition. Correct options A and B require accurate calculation for the standard model and a correct conceptual explanation for the modified model's result. Distractor C targets a common step-omission error (confusing the probability limit with the bias term, which is -2.67). Distractor D is a 'True but Irrelevant' statement that leads to a false conclusion by ignoring the crucial role of the denominator.", "qid": "334", "question": "### Background\n\n**Research Question.** A central puzzle in international finance is that different stationary regressions testing the forward exchange rate unbiasedness hypothesis yield conflicting results. The standard forward premium regression typically rejects unbiasedness, while a seemingly minor modification appears to support it. Can this conflict be explained by differences in the magnitude of simultaneity bias across specifications?\n\n**Setting.** An econometric analysis of two stationary models. The first is the standard forward premium regression of the spot rate change `S_t - S_{t-1}` on the forward premium `F_{t-1} - S_{t-1}`. The second is a modified version, regressing `S_t - S_{t-2}` on `F_{t-1} - S_{t-2}`. Under the null hypothesis of unbiasedness, the true slope coefficient `\\beta` is 1 in both models.\n\n**Variables & Parameters.**\n- `S_t`, `S_{t-1}`, `S_{t-2}`: Log spot exchange rates.\n- `F_{t-1}`: Log forward exchange rate.\n- `\\eta_t`: Innovation to the spot rate process, `\\eta_t = S_t - S_{t-1}`.\n- `e_{t-1}`: Innovation to the forward rate process, `e_{t-1} = F_{t-1} - S_{t-1}`.\n- `\\beta`: The slope coefficient in the regression.\n\n---\n\n### Data / Model Specification\n\nThe analysis hinges on the statistical properties of the spot and forward rate innovations, particularly the \"relative error variance characteristic\"—the empirical fact that the variance of the spot rate innovation is vastly larger than that of the forward rate innovation.\n\n**Table 1. Summary Statistics of DGP Residuals (UK)**\n| Statistic | Spot Rate Innovation (`\\eta_t`) | Forward Rate Innovation (`e_{t-1}`) |\n| :--- | :--- | :--- |\n| Variance | 0.0010692 | 0.0000078 |\n| Covariance `cov(e_{t-1}, \\eta_t)` | | -0.000013 |\n\n**Asymptotic Bias of OLS Estimator `\\hat{\\beta}`:**\n- **Standard Regression (`S_t - S_{t-1}` on `F_{t-1} - S_{t-1}`):**\n      \n    \\text{Bias} = \\frac{\\mathrm{cov}(e_{t-1}, \\eta_t)}{\\mathrm{var}(e_{t-1})} - 1 \\quad \\text{(Eq. (1))}\n     \n- **Modified Regression (`S_t - S_{t-2}` on `F_{t-1} - S_{t-2}`):**\n      \n    \\text{Bias} = \\frac{\\mathrm{cov}(e_{t-1}, \\eta_t) - \\mathrm{var}(e_{t-1})}{\\mathrm{var}(\\eta_{t-1}) + \\mathrm{var}(e_{t-1})} \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nUsing the provided data for the UK and the asymptotic bias formulas (where `plim(\\hat{\\beta}) = 1 + \\text{Bias}`), which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 64, "Question": "### Background\n\n**Research Question.** How sensitive are an insurer's optimal dividend and reinsurance policies to the magnitude of the penalty associated with financial ruin?\n\n**Setting.** A numerical example illustrating the theoretical predictions that a higher ruin penalty `P` makes the insurer more conservative. The table shows the optimal policy thresholds (`x_1`, `x_b`) and the initial risk retention `q*(0)` for a range of ruin penalty values `P`.\n\n**Variables and Parameters.**\n- `P`: The penalty for ruin. `P < 0` indicates a salvage value (units: currency).\n- `x_1`: The surplus threshold separating partial and full reinsurance (units: currency).\n- `x_b`: The optimal dividend barrier (units: currency).\n- `q*(0)`: The optimal retention proportion at infinitesimally small surplus (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe following table presents numerical solutions for the optimal policy under the parameterization: `a=1`, `b=0.2`, `θ=0.1`, `β=0.05`, and `η=0.15`.\n\n**Table 1: The effect of `P` on `x_1`, `x_b`, and `q*(0)`**\n\n| P    | x_1   | x_b   | q*(0) |\n|:-----|:------|:------|:------|\n| -1.0 | 0.080 | 0.651 | 0.863 |\n| 10.0 | 0.606 | 1.177 | 0.574 |\n\nTheoretically, in the full retention region `x ∈ [x_1, x_b)`, the value function solves a second-order ODE whose characteristic roots `R_1 > 0` and `R_2 < 0` are given by:\n\n  \nR_{1,2}=\\frac{-\\theta a\\pm\\sqrt{(\\theta a)^{2}+2b^{2}\\beta}}{b^{2}} \\quad \\text{(Eq. (1))}\n \n\nThe size of this full-retention region is constant and given by:\n\n  \nx_{b}-x_{1}=\\frac{1}{R_{1}-R_{2}}\\ln\\left(\\frac{2\\beta-\\eta a R_{2}}{2\\beta-\\eta a R_{1}}\\right) \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nUsing the provided parameters and equations, which of the following statements about the theoretical model are correct? Select all that apply.", "Options": {"A": "The positive characteristic root `R_1` of the value function's ODE in the full retention region is approximately 0.458.", "B": "The theoretically derived size of the full-retention region, `x_b - x_1`, is approximately 0.571, which is consistent with the data in Table 1.", "C": "The size of the full-retention region `x_b - x_1` is approximately 3.38, based on the logarithm term in Eq. (2).", "D": "The negative characteristic root `R_2` is approximately 5.458."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform a multi-step calculation and verify a theoretical result against data, a 'Computational Judgment' strategy. Options A and B are correct intermediate and final results of the calculation. Distractor C (Sign Error) presents `R_2` with the wrong sign. Distractor D (Step-Omission Error) presents an intermediate value from the calculation as the final answer, a common error.", "qid": "64", "question": "### Background\n\n**Research Question.** How sensitive are an insurer's optimal dividend and reinsurance policies to the magnitude of the penalty associated with financial ruin?\n\n**Setting.** A numerical example illustrating the theoretical predictions that a higher ruin penalty `P` makes the insurer more conservative. The table shows the optimal policy thresholds (`x_1`, `x_b`) and the initial risk retention `q*(0)` for a range of ruin penalty values `P`.\n\n**Variables and Parameters.**\n- `P`: The penalty for ruin. `P < 0` indicates a salvage value (units: currency).\n- `x_1`: The surplus threshold separating partial and full reinsurance (units: currency).\n- `x_b`: The optimal dividend barrier (units: currency).\n- `q*(0)`: The optimal retention proportion at infinitesimally small surplus (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe following table presents numerical solutions for the optimal policy under the parameterization: `a=1`, `b=0.2`, `θ=0.1`, `β=0.05`, and `η=0.15`.\n\n**Table 1: The effect of `P` on `x_1`, `x_b`, and `q*(0)`**\n\n| P    | x_1   | x_b   | q*(0) |\n|:-----|:------|:------|:------|\n| -1.0 | 0.080 | 0.651 | 0.863 |\n| 10.0 | 0.606 | 1.177 | 0.574 |\n\nTheoretically, in the full retention region `x ∈ [x_1, x_b)`, the value function solves a second-order ODE whose characteristic roots `R_1 > 0` and `R_2 < 0` are given by:\n\n  \nR_{1,2}=\\frac{-\\theta a\\pm\\sqrt{(\\theta a)^{2}+2b^{2}\\beta}}{b^{2}} \\quad \\text{(Eq. (1))}\n \n\nThe size of this full-retention region is constant and given by:\n\n  \nx_{b}-x_{1}=\\frac{1}{R_{1}-R_{2}}\\ln\\left(\\frac{2\\beta-\\eta a R_{2}}{2\\beta-\\eta a R_{1}}\\right) \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nUsing the provided parameters and equations, which of the following statements about the theoretical model are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: import math\n\na = 1\ntheta = 0.1\nb = 0.2\nbeta = 0.05...\n\n\nA,B\n> 🔍 **Analyzing financial model**\n> ***- The question asks how sensitive an insurer's policies are to the ruin penalty magnitude.***\n", "model_answer": ["A", "B"], "error": null}}
{"ID": 332, "Question": "### Background\n\n**Research Question.** How does the number of long-run equilibrium relationships (cointegrating vectors) among a set of international stock prices relate to the number of common driving trends, the degree of market integration, and long-term portfolio diversification benefits?\n\n**Setting.** Consider a system of `p` international stock markets. The log stock price in each market is integrated of order one, I(1), meaning its first difference is stationary. The system is characterized by its cointegrating rank, `r`.\n\n**Variables and Parameters.**\n- `X_t`: A `p x 1` vector of log stock prices `(x_{1t}, ..., x_{pt})'` at time `t`.\n- `p`: The number of stock markets in the system.\n- `r`: The cointegrating rank, representing the number of stationary long-run relationships among the `p` stock prices (`0 ≤ r ≤ p-1`).\n- `p-r`: The number of common stochastic trends driving the system.\n\n---\n\n### Data / Model Specification\n\nThe core concepts are:\n1.  **Cointegration:** A set of `p` variables, `X_t`, all individually I(1), are cointegrated with rank `r` if there exist `r` linearly independent combinations of these variables that are stationary, I(0).\n2.  **Common Trends:** If the cointegrating rank is `r`, the system of `p` non-stationary prices is driven by a smaller number, `p-r`, of common stochastic trends.\n\n---\n\n### Question\n\nIn a system of `p` international stock markets, a finding of maximum cointegration (`r = p-1`) has several important implications. Which of the following statements are correct implications of `r = p-1`? (Select all that apply)", "Options": {"A": "The markets exhibit a high degree of long-run integration.", "B": "The system is driven by a single common stochastic trend.", "C": "The system is driven by `p` distinct stochastic trends.", "D": "The markets offer significant long-term diversification benefits."}, "Answer": ["A", "B"], "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.5). This item replaces part of the original QA problem. It assesses the core financial and statistical interpretation of a key cointegration scenario (`r=p-1`). The use of Multiple Choice allows for testing multiple facets of the concept simultaneously. Distractors are designed based on classic conceptual opposites: Distractor B (`p` trends) describes the `r=0` case, and Distractor D (high diversification benefits) is the financial opposite of high integration.", "qid": "332", "question": "### Background\n\n**Research Question.** How does the number of long-run equilibrium relationships (cointegrating vectors) among a set of international stock prices relate to the number of common driving trends, the degree of market integration, and long-term portfolio diversification benefits?\n\n**Setting.** Consider a system of `p` international stock markets. The log stock price in each market is integrated of order one, I(1), meaning its first difference is stationary. The system is characterized by its cointegrating rank, `r`.\n\n**Variables and Parameters.**\n- `X_t`: A `p x 1` vector of log stock prices `(x_{1t}, ..., x_{pt})'` at time `t`.\n- `p`: The number of stock markets in the system.\n- `r`: The cointegrating rank, representing the number of stationary long-run relationships among the `p` stock prices (`0 ≤ r ≤ p-1`).\n- `p-r`: The number of common stochastic trends driving the system.\n\n---\n\n### Data / Model Specification\n\nThe core concepts are:\n1.  **Cointegration:** A set of `p` variables, `X_t`, all individually I(1), are cointegrated with rank `r` if there exist `r` linearly independent combinations of these variables that are stationary, I(0).\n2.  **Common Trends:** If the cointegrating rank is `r`, the system of `p` non-stationary prices is driven by a smaller number, `p-r`, of common stochastic trends.\n\n---\n\n### Question\n\nIn a system of `p` international stock markets, a finding of maximum cointegration (`r = p-1`) has several important implications. Which of the following statements are correct implications of `r = p-1`? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 246, "Question": "### Background\n\n**Research Question.** Under what technical conditions does an optimal reinsurance premium exist as the solution to a joint loss-minimization problem, and what first-order condition must this premium satisfy?\n\n**Setting.** An optimal reinsurance premium, `c*`, is sought to minimize a joint expected loss function for an insurer and a reinsurer.\n\n### Data / Model Specification\n\nThe joint expected loss function is:\n  \nf_{1}(c)=\\mathbb{E}[g(X)\\varphi_{1}((I(X)-c)_{+})]+\\mathbb{E}[h(X)\\varphi_{2}(X-I(X)+c)] \n \nwhere `φ_1` and `φ_2` are quantifying functions for the reinsurer's and insurer's risks, respectively. The optimal premium `c*` minimizes `f_1(c)`. If `φ_1` and `φ_2` are differentiable and `φ_1'(0) = 0`, the optimal premium `c*` is the solution to the first-order condition (FOC):\n  \n\\mathbb{E}[h(X)\\varphi_{2}^{\\prime}(X-I(X)+c)]=\\mathbb{E}[g(X)\\varphi_{1}^{\\prime}((I(X)-c)_{+})] \\quad \\text{(Eq. 1)}\n \n\n### Question\n\nAccording to the paper's framework, which of the following are valid interpretations or consequences of the first-order condition in Eq. (1) and its underlying assumptions? Select all that apply.", "Options": {"A": "If the insurer's quantifying function `φ_2` were non-convex (e.g., S-shaped as in Prospect Theory), the objective function `f_1(c)` would remain convex, ensuring a unique optimal premium.", "B": "The condition `φ_1'(0) = 0` implies that if the premium `c` is already sufficient to cover the ceded loss `I(X)`, the reinsurer gains no marginal risk-reduction benefit from a small additional increase in `c`.", "C": "The right-hand side of Eq. (1) represents the reinsurer's expected marginal benefit (i.e., reduction in disutility) from an incremental increase in the premium `c`.", "D": "The left-hand side of Eq. (1) represents the insurer's expected marginal disutility from an incremental increase in the premium `c`."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: Assess understanding of the economic interpretation of the central first-order condition (FOC) derived in the paper. Strategy: Reverse-Reasoning (interpreting the components of a given equation). Distractor Logic: Option D is a Conceptual Opposite. The paper's proof of a unique optimum relies on the convexity of `φ_1` and `φ_2`. Violating this assumption for `φ_2` would break the convexity of the overall objective function `f_1(c)`, potentially leading to multiple local minima, not a unique optimum. Options A, B, and C are correct interpretations of the FOC as an economic equilibrium, as explained in the original QA's detailed answer.", "qid": "246", "question": "### Background\n\n**Research Question.** Under what technical conditions does an optimal reinsurance premium exist as the solution to a joint loss-minimization problem, and what first-order condition must this premium satisfy?\n\n**Setting.** An optimal reinsurance premium, `c*`, is sought to minimize a joint expected loss function for an insurer and a reinsurer.\n\n### Data / Model Specification\n\nThe joint expected loss function is:\n  \nf_{1}(c)=\\mathbb{E}[g(X)\\varphi_{1}((I(X)-c)_{+})]+\\mathbb{E}[h(X)\\varphi_{2}(X-I(X)+c)] \n \nwhere `φ_1` and `φ_2` are quantifying functions for the reinsurer's and insurer's risks, respectively. The optimal premium `c*` minimizes `f_1(c)`. If `φ_1` and `φ_2` are differentiable and `φ_1'(0) = 0`, the optimal premium `c*` is the solution to the first-order condition (FOC):\n  \n\\mathbb{E}[h(X)\\varphi_{2}^{\\prime}(X-I(X)+c)]=\\mathbb{E}[g(X)\\varphi_{1}^{\\prime}((I(X)-c)_{+})] \\quad \\text{(Eq. 1)}\n \n\n### Question\n\nAccording to the paper's framework, which of the following are valid interpretations or consequences of the first-order condition in Eq. (1) and its underlying assumptions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 218, "Question": "### Background\n\n**Research Question.** This case explores the empirical strategy used to isolate the causal effect of a bank's monitoring effort on a borrower's loan announcement return, focusing on the role of robustness checks in addressing threats to identification.\n\n**Setting.** The analysis employs a cross-sectional regression where the dependent variable is the borrower's standardized cumulative abnormal return (`SCAR`). The main independent variable is `MONITORING EFFORT`. The main result is subjected to sensitivity analysis by adding more control variables.\n\n**Variables & Parameters.**\n- `SCAR_i`: Standardized cumulative abnormal return for borrower `i`.\n- `MONITORING_EFFORT_i`: Proxy for the monitoring ability of borrower `i`'s lead bank.\n- `LOAN LOSS RATIO`: A proxy for the bank's portfolio risk.\n- `BETA`: A measure of the borrower's systematic market risk.\n\n---\n\n### Data / Model Specification\n\nThe conceptual regression model is:\n  \nSCAR_i = \\beta_0 + \\beta_1 MONITORING\\_EFFORT_i + Controls_i + \\epsilon_i \\quad \\text{(Eq. (1))}\n \n\n---\n\n### The Question\n\nTo strengthen the causal interpretation of their findings, the authors perform robustness checks by adding more variables to the `Controls` vector in Eq. (1). Which of the following statements correctly identify the purpose of these specific checks? (Select all that apply.)", "Options": {"A": "Including the bank's `LOAN LOSS RATIO` helps to rule out the alternative explanation that `MONITORING EFFORT` is simply capturing the bank's general risk-taking behavior.", "B": "Including the bank's `LOAN LOSS RATIO` directly tests whether the `MONITORING EFFORT` proxy is biased due to the bank's involvement in investment banking.", "C": "Including the borrower's `BETA` is primarily intended to increase the regression's R-squared value.", "D": "Including the borrower's `BETA` helps to address the concern that high-`MONITORING EFFORT` banks appear to add more value merely because they select borrowers with different levels of systematic risk."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the user's understanding of the role of robustness checks in causal inference. It uses a Reverse-Reasoning strategy. Options A and B are correct as they accurately describe how controlling for a specific risk measure (bank-level or borrower-level) helps to isolate the effect of the variable of interest from a potential confounder. Distractor C (Conceptual Confusion) misattributes the purpose of the control; `LOAN LOSS RATIO` measures portfolio risk, not business mix bias. Distractor D (True but Irrelevant) states a potential side effect (higher R-squared) but misses the primary purpose of a robustness check, which is to test the stability of a key coefficient against alternative explanations.", "qid": "218", "question": "### Background\n\n**Research Question.** This case explores the empirical strategy used to isolate the causal effect of a bank's monitoring effort on a borrower's loan announcement return, focusing on the role of robustness checks in addressing threats to identification.\n\n**Setting.** The analysis employs a cross-sectional regression where the dependent variable is the borrower's standardized cumulative abnormal return (`SCAR`). The main independent variable is `MONITORING EFFORT`. The main result is subjected to sensitivity analysis by adding more control variables.\n\n**Variables & Parameters.**\n- `SCAR_i`: Standardized cumulative abnormal return for borrower `i`.\n- `MONITORING_EFFORT_i`: Proxy for the monitoring ability of borrower `i`'s lead bank.\n- `LOAN LOSS RATIO`: A proxy for the bank's portfolio risk.\n- `BETA`: A measure of the borrower's systematic market risk.\n\n---\n\n### Data / Model Specification\n\nThe conceptual regression model is:\n  \nSCAR_i = \\beta_0 + \\beta_1 MONITORING\\_EFFORT_i + Controls_i + \\epsilon_i \\quad \\text{(Eq. (1))}\n \n\n---\n\n### The Question\n\nTo strengthen the causal interpretation of their findings, the authors perform robustness checks by adding more variables to the `Controls` vector in Eq. (1). Which of the following statements correctly identify the purpose of these specific checks? (Select all that apply.)", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 124, "Question": "### Background\n\n**Research Question.** This case examines the characteristics that distinguish firms choosing to undertake an initial public debt offering (debt IPO) from those that do not.\n\n**Setting.** The study employs a matched-sample research design. For each of the 143 firms that issued a debt IPO, a control firm is randomly selected from the same industry group with no long-term debt in the year of the IPO. Univariate and multivariate (probit) analyses are used to compare the characteristics of the two groups in the year prior to the offering.\n\n---\n\n### Data / Model Specification\n\nTo identify the determinants of the debt IPO decision, the paper first compares the median characteristics of the sample firms with their matched controls (Table 1), and then estimates a multivariate probit model to control for interdependencies (Table 2).\n\n**Table 1: Comparison of Median Characteristics for Debt IPO Firms and Control Firms**\n\n| Characteristic | Sample Median | Control Median | p-Value for Difference |\n| :--- | :--- | :--- | :--- |\n| Market-to-book ratio | 1.35 | 1.62 | 0.089 |\n\n**Table 2: Probit Model Estimates for the Determinants of a Debt IPO**\n\n| Independent Variable | Coefficient | p-value |\n| :--- | :--- | :--- |\n| Intercept | -2.7363 | (0.00) |\n| Asset size | 0.4876 | (0.00) |\n| Capex | 6.3868 | (0.00) |\n| Market-to-book | 0.0275 | (0.66) |\n\n---\n\nBased on the provided data, which of the following statements represent valid conclusions or critiques of the research design? Select all that apply.", "Options": {"A": "The probit model in Table 2 indicates that, after controlling for other factors, a firm's growth opportunities (Market-to-book ratio) are a primary and significant driver of the decision to issue public debt.", "B": "The probit model results imply that a firm's size is a statistically significant predictor of its likelihood to undertake a debt IPO.", "C": "The univariate results in Table 1 suggest that firms with lower growth opportunities are more likely to undertake a debt IPO, but this conclusion is likely confounded by the omission of firm size.", "D": "A key limitation of the study's research design is that by comparing debt IPO firms to firms with no long-term debt, it cannot distinguish between the decision to use leverage versus the specific choice of public debt over private debt."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to synthesize findings from univariate and multivariate analyses, identify omitted variable bias, and critically evaluate a research design's choice of control group. Depth Strategy: Reverse-Reasoning (identifying plausible causes for conflicting results and flaws in design). Distractor Logic: Option A is correct; it accurately describes the univariate finding and correctly identifies the likely source of bias (omitted variable of firm size). Option C is a correct and sophisticated critique of the control group choice. Option D is a correct interpretation of the highly significant coefficient on `Asset size` in Table 2. Option B is a Conceptual Opposite; the probit model shows that the Market-to-book ratio is statistically insignificant (p=0.66), not a primary driver.", "qid": "124", "question": "### Background\n\n**Research Question.** This case examines the characteristics that distinguish firms choosing to undertake an initial public debt offering (debt IPO) from those that do not.\n\n**Setting.** The study employs a matched-sample research design. For each of the 143 firms that issued a debt IPO, a control firm is randomly selected from the same industry group with no long-term debt in the year of the IPO. Univariate and multivariate (probit) analyses are used to compare the characteristics of the two groups in the year prior to the offering.\n\n---\n\n### Data / Model Specification\n\nTo identify the determinants of the debt IPO decision, the paper first compares the median characteristics of the sample firms with their matched controls (Table 1), and then estimates a multivariate probit model to control for interdependencies (Table 2).\n\n**Table 1: Comparison of Median Characteristics for Debt IPO Firms and Control Firms**\n\n| Characteristic | Sample Median | Control Median | p-Value for Difference |\n| :--- | :--- | :--- | :--- |\n| Market-to-book ratio | 1.35 | 1.62 | 0.089 |\n\n**Table 2: Probit Model Estimates for the Determinants of a Debt IPO**\n\n| Independent Variable | Coefficient | p-value |\n| :--- | :--- | :--- |\n| Intercept | -2.7363 | (0.00) |\n| Asset size | 0.4876 | (0.00) |\n| Capex | 6.3868 | (0.00) |\n| Market-to-book | 0.0275 | (0.66) |\n\n---\n\nBased on the provided data, which of the following statements represent valid conclusions or critiques of the research design? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 273, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical foundation and empirical implementation of the Empirical Average Cost of Capital (EACC), a novel measure derived from accounting data based on a fundamental microeconomic equilibrium condition.\n\n**Setting.** The model considers a firm in a long-run equilibrium where the marginal benefit of capital equals its marginal cost. The analysis links this economic principle to observable financial statement variables and operationalizes it via a simple time-series regression.\n\n---\n\n### Data / Model Specification\n\nThe model is founded on the Marshallian equilibrium condition, which in dollar terms is:\n\n  \n\\text{Realized Return on Capital} = \\text{Required Return on Capital} \n \n\nThe EACC model proposes that the realized dollar return on capital can be proxied by `NOPAT` and that the required return can be expressed as `K` times `Total Capital`. The model assumes the firm operates in an equilibrium where economic profit is zero. Empirically, the EACC is estimated via the following bivariate regression model:\n\n  \n\\text{NOPAT}_{i,t} = K_i (\\text{TOTAL CAPITAL}_{i,t-1}) + e_{i,t} \n\\quad \\text{(Eq. 1)}\n \n\n---\n\nAccording to the paper's theoretical framework, which of the following statements are valid assumptions or direct implications of the EACC model?", "Options": {"A": "If a firm consistently generates positive economic profit (e.g., due to a sustainable competitive advantage), its EACC estimate `K` will be an upwardly biased estimate of its true WACC.", "B": "The model's use of lagged total capital (`TOTAL CAPITAL_{i,t-1}`) as the regressor completely eliminates the possibility of endogeneity in the OLS estimation of `K_i`.", "C": "The assumption of zero economic profit is necessary to interpret the estimated coefficient `K` as being theoretically equivalent to the firm's WACC.", "D": "The EACC model requires fewer subjective analyst inputs (e.g., risk-free rate, equity beta, market portfolio return) compared to a conventional WACC calculation using the CAPM."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses the user's understanding of the core theoretical underpinnings and assumptions of the EACC model. It uses a 'Reverse-Reasoning' strategy by asking for the necessary pre-conditions and implications of the model. Option A tests the critical link between zero economic profit and the interpretation of K. Option B tests the ability to reason about what happens when a key assumption is violated. Option C tests the user's knowledge of the model's practical advantages as described in the paper and appendix. The distractor (D) is a 'Conceptual Distractor' (Almost Right); while using lagged variables can mitigate some endogeneity issues, it does not eliminate all sources, such as omitted variable bias where the omitted variable is serially correlated, a common issue in finance.", "qid": "273", "question": "### Background\n\n**Research Question.** This problem explores the theoretical foundation and empirical implementation of the Empirical Average Cost of Capital (EACC), a novel measure derived from accounting data based on a fundamental microeconomic equilibrium condition.\n\n**Setting.** The model considers a firm in a long-run equilibrium where the marginal benefit of capital equals its marginal cost. The analysis links this economic principle to observable financial statement variables and operationalizes it via a simple time-series regression.\n\n---\n\n### Data / Model Specification\n\nThe model is founded on the Marshallian equilibrium condition, which in dollar terms is:\n\n  \n\\text{Realized Return on Capital} = \\text{Required Return on Capital} \n \n\nThe EACC model proposes that the realized dollar return on capital can be proxied by `NOPAT` and that the required return can be expressed as `K` times `Total Capital`. The model assumes the firm operates in an equilibrium where economic profit is zero. Empirically, the EACC is estimated via the following bivariate regression model:\n\n  \n\\text{NOPAT}_{i,t} = K_i (\\text{TOTAL CAPITAL}_{i,t-1}) + e_{i,t} \n\\quad \\text{(Eq. 1)}\n \n\n---\n\nAccording to the paper's theoretical framework, which of the following statements are valid assumptions or direct implications of the EACC model?", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 205, "Question": "### Background\n\n**Research Question.** How can statistical tests be combined to build a robust case for the non-stationarity of financial price series, a key property implied by the weak-form Efficient Market Hypothesis?\n\n**Setting / Data-Generating Environment.** The analysis uses daily closing prices for UK financial futures. To test for non-stationarity, the paper employs two complementary methods: the Augmented Dickey-Fuller (ADF) test, which has a null hypothesis of a unit root (non-stationarity), and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, which has a null hypothesis of stationarity.\n\n---\n\n### Data / Model Specification\n\nThe Augmented Dickey-Fuller (ADF) test has a null hypothesis `H₀: The series has a unit root`.\nThe Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test has a null hypothesis `H₀: The series is stationary`.\n\n**Table 1. Stationarity Test Results for FTSE100 Futures**\n\n| Panel | Test | Data Series | Lag | Test Statistic | 5% Critical Value |\n|---|---|---|---|---|---|\n| A | ADF | Price Level | 1 | -0.882 | -3.4139 |\n| A | ADF | First Difference | 1 | -43.03 | -3.4139 |\n| B | KPSS | Price Level | 8 | 3.4577 | 0.146 |\n\n*Source: Adapted from Tables 2 and 3 of the source paper.*\n\n---\n\n### Question\n\nBased on the data presented in **Table 1**, select ALL of the following statements that correctly interpret and synthesize the test results.", "Options": {"A": "The KPSS test statistic (3.4577) is significantly larger than its 5% critical value (0.146), leading to the rejection of the null hypothesis of stationarity.", "B": "The ADF test fails to reject the unit root null hypothesis for the price level series but rejects it for the first-difference series, indicating the series is integrated of order one, I(1).", "C": "The ADF and KPSS tests provide conflicting conclusions, as the ADF test supports non-stationarity while the KPSS test supports stationarity for the price level series.", "D": "The ADF test on the price level series (statistic = -0.882) provides strong evidence to reject the null hypothesis of a unit root at the 5% significance level."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to synthesize evidence from two complementary statistical tests. It uses an 'Atomic Decomposition' strategy. Options A and B test the correct, standalone interpretation of the ADF and KPSS results, respectively. The distractors target common, high-frequency misconceptions. Distractor C presents the classic error of misinterpreting the KPSS test's rejection of stationarity as evidence *for* stationarity. Distractor D is a 'Conceptual Opposite' that tests for a basic misreading of the ADF test outcome. The item effectively captures the core inferential logic of the paper's stationarity analysis.", "qid": "205", "question": "### Background\n\n**Research Question.** How can statistical tests be combined to build a robust case for the non-stationarity of financial price series, a key property implied by the weak-form Efficient Market Hypothesis?\n\n**Setting / Data-Generating Environment.** The analysis uses daily closing prices for UK financial futures. To test for non-stationarity, the paper employs two complementary methods: the Augmented Dickey-Fuller (ADF) test, which has a null hypothesis of a unit root (non-stationarity), and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, which has a null hypothesis of stationarity.\n\n---\n\n### Data / Model Specification\n\nThe Augmented Dickey-Fuller (ADF) test has a null hypothesis `H₀: The series has a unit root`.\nThe Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test has a null hypothesis `H₀: The series is stationary`.\n\n**Table 1. Stationarity Test Results for FTSE100 Futures**\n\n| Panel | Test | Data Series | Lag | Test Statistic | 5% Critical Value |\n|---|---|---|---|---|---|\n| A | ADF | Price Level | 1 | -0.882 | -3.4139 |\n| A | ADF | First Difference | 1 | -43.03 | -3.4139 |\n| B | KPSS | Price Level | 8 | 3.4577 | 0.146 |\n\n*Source: Adapted from Tables 2 and 3 of the source paper.*\n\n---\n\n### Question\n\nBased on the data presented in **Table 1**, select ALL of the following statements that correctly interpret and synthesize the test results.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 158, "Question": "### Background\n\nA study investigates whether incumbent banks use branch network expansion as a strategy to deter the entry of competitors, particularly multimarket (MM) banks. The analysis uses a two-stage model. The first stage models the entry decisions of firms, while the second stage models the number of branches an incumbent firm operates. To address the endogeneity of market structure (i.e., the number of competitors), the second-stage model is estimated in two ways: an \"Unadjusted\" version that ignores endogeneity and an \"Adjusted\" version that includes correction terms derived from the first stage.\n\n### Data / Model Specification\n\nThe key output from the second-stage branching model for incumbent multimarket banks is presented below.\n\n**Table 1: Second-Stage Marginal Effects on Number of Branches per Multimarket Bank**\n\n| Variable | Unadjusted Model (Biased) | Adjusted Model (Corrected) |\n| :--- | :---: | :---: |\n| Effect of one additional MM Competitor | -0.048 | 0.159 |\n| | (-3.79) | (5.85) |\n| Coefficient on MM market presence unobservable | --- | -0.100 |\n| | | (-8.57) |\n\n*Note: t-statistics are in parentheses.* \n\n---\n\n### Question\n\nBased on the results in **Table 1**, select all statements that are supported by the paper's analysis.", "Options": {"A": "The negative coefficient on the 'MM market presence unobservable' is consistent with a story of strategic deterrence, where incumbents preemptively build more branches in markets that are otherwise prime targets for MM bank entry.", "B": "The sign reversal for the 'Effect of one additional MM Competitor' from negative to positive after adjustment suggests that a naive analysis would incorrectly conclude that MM competition suppresses branch expansion.", "C": "The negative coefficient on the 'MM market presence unobservable' in the adjusted model indicates that markets inherently attractive to MM banks are also inherently unattractive for building branches.", "D": "The positive adjusted effect of MM competitors implies that MM banks likely collude to mutually expand their branch networks when present in the same market."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the student's ability to interpret the core results of the endogeneity correction. It uses an 'Atomic Decomposition' strategy, breaking the complex finding into two key, independently verifiable interpretations. Distractor (B) presents a conceptual opposite, misinterpreting the strategic interaction as a simple negative correlation in market characteristics. Distractor (D) offers an alternative, incorrect strategic explanation (collusion vs. competition) for the observed positive coefficient.", "qid": "158", "question": "### Background\n\nA study investigates whether incumbent banks use branch network expansion as a strategy to deter the entry of competitors, particularly multimarket (MM) banks. The analysis uses a two-stage model. The first stage models the entry decisions of firms, while the second stage models the number of branches an incumbent firm operates. To address the endogeneity of market structure (i.e., the number of competitors), the second-stage model is estimated in two ways: an \"Unadjusted\" version that ignores endogeneity and an \"Adjusted\" version that includes correction terms derived from the first stage.\n\n### Data / Model Specification\n\nThe key output from the second-stage branching model for incumbent multimarket banks is presented below.\n\n**Table 1: Second-Stage Marginal Effects on Number of Branches per Multimarket Bank**\n\n| Variable | Unadjusted Model (Biased) | Adjusted Model (Corrected) |\n| :--- | :---: | :---: |\n| Effect of one additional MM Competitor | -0.048 | 0.159 |\n| | (-3.79) | (5.85) |\n| Coefficient on MM market presence unobservable | --- | -0.100 |\n| | | (-8.57) |\n\n*Note: t-statistics are in parentheses.* \n\n---\n\n### Question\n\nBased on the results in **Table 1**, select all statements that are supported by the paper's analysis.", "model_evaluation_result": {"model_raw_response": "\n\nA,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 219, "Question": "### Background\n\nThe paper contrasts two models of international monetary policy cooperation: a theoretical \"Global Planner\" model and a more realistic \"Bargaining Game\" model. The New Keynesian literature, which the paper endorses as a useful framework, assumes the ultimate objective of policy is to maximize the welfare of households.\n\n### Data / Model Specification\n\nThe two views of cooperation can be formalized with distinct objective functions.\n\n**Model 1 (Academic View - Global Planner):** A single planner chooses policies `{ \\pi_i }` for all countries `i=1,...,N` to solve:\n\n  \n\\max_{\\{\\pi_i\\}} \\sum_{i=1}^{N} \\omega_i W_i(\\pi_1, ..., \\pi_N)\n\\quad \\text{(Eq. 1)}\n \nwhere `W_i` is the welfare of country `i` and `\\omega_i` is its weight in the global objective.\n\n**Model 2 (Practice - Nash Bargaining Game):** The negotiated policy outcome `{ \\pi_i }` is the solution to:\n\n  \n\\max_{\\{\\pi_i\\}} \\prod_{i=1}^{N} (W_i(\\pi_1, ..., \\pi_N) - W_i^{Nash})^{\\beta_i}\n\\quad \\text{(Eq. 2)}\n \nwhere `W_i^{Nash}` is the welfare country `i` achieves in the non-cooperative (Nash) equilibrium (the \"disagreement point\"), and `\\beta_i` is its bargaining power.\n\n### Question\n\nBased on the comparison between the Global Planner (Eq. 1) and Nash Bargaining (Eq. 2) models of cooperation, which of the following statements are valid conclusions?\n", "Options": {"A": "In the Nash Bargaining model (Eq. 2), a country that is relatively self-sufficient and suffers little in the non-cooperative equilibrium (i.e., has a high `W_i^{Nash}`) possesses greater bargaining leverage.", "B": "A country's ability to inflict large negative spillovers on other countries in a non-cooperative scenario would decrease its leverage in the bargaining process, as it would be seen as a destabilizing actor.", "C": "The Nash Bargaining model (Eq. 2) inherently guarantees that the cooperative outcome is a Pareto improvement over the non-cooperative (Nash) equilibrium for all participating countries.", "D": "The Global Planner model (Eq. 1) may produce a globally optimal policy set that leaves at least one country worse off than it would be in the non-cooperative (Nash) equilibrium."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses the candidate's ability to interpret and contrast two formal models of policy cooperation. It uses a Reverse-Reasoning strategy, asking for valid conclusions derived from the models' structures. \n- **Correct Option A:** This is correct because the objective in Eq. 2 maximizes the product of gains from cooperation `(W_i - W_i^{Nash})`. Any solution must ensure this term is non-negative for all countries, which is the definition of a Pareto improvement.\n- **Correct Option C:** This is correct because the global planner maximizes a sum, which could involve making one country worse off if the gains to others are sufficiently large to increase the total.\n- **Correct Option D:** This is correct. A high disagreement point `W_i^{Nash}` means the country has a strong outside option and is less desperate for a deal, thus increasing its leverage.\n- **Distractor B:** This is a Conceptual Opposite. The ability to inflict negative spillovers *lowers the disagreement point of other countries*, making them more desperate for a deal and thus *increasing* the threatening country's leverage.", "qid": "219", "question": "### Background\n\nThe paper contrasts two models of international monetary policy cooperation: a theoretical \"Global Planner\" model and a more realistic \"Bargaining Game\" model. The New Keynesian literature, which the paper endorses as a useful framework, assumes the ultimate objective of policy is to maximize the welfare of households.\n\n### Data / Model Specification\n\nThe two views of cooperation can be formalized with distinct objective functions.\n\n**Model 1 (Academic View - Global Planner):** A single planner chooses policies `{ \\pi_i }` for all countries `i=1,...,N` to solve:\n\n  \n\\max_{\\{\\pi_i\\}} \\sum_{i=1}^{N} \\omega_i W_i(\\pi_1, ..., \\pi_N)\n\\quad \\text{(Eq. 1)}\n \nwhere `W_i` is the welfare of country `i` and `\\omega_i` is its weight in the global objective.\n\n**Model 2 (Practice - Nash Bargaining Game):** The negotiated policy outcome `{ \\pi_i }` is the solution to:\n\n  \n\\max_{\\{\\pi_i\\}} \\prod_{i=1}^{N} (W_i(\\pi_1, ..., \\pi_N) - W_i^{Nash})^{\\beta_i}\n\\quad \\text{(Eq. 2)}\n \nwhere `W_i^{Nash}` is the welfare country `i` achieves in the non-cooperative (Nash) equilibrium (the \"disagreement point\"), and `\\beta_i` is its bargaining power.\n\n### Question\n\nBased on the comparison between the Global Planner (Eq. 1) and Nash Bargaining (Eq. 2) models of cooperation, which of the following statements are valid conclusions?\n", "model_evaluation_result": {"model_raw_response": "\n\nA,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 335, "Question": "### Background\n\n**Research Question.** A study establishes a causal link suggesting that higher local air pollution leads to less corporate tax avoidance (i.e., a higher effective tax rate, ETR). This question explores the economic significance of this finding, the mechanisms driving it, and how the effect varies across different types of firms.\n\n**Setting / Data-Generating Environment.** The analysis uses a panel of Chinese A-share firms. The main effect is estimated via a fixed-effects model. The mechanisms and heterogeneous effects are explored using interaction models.\n\n**Variables & Parameters.**\n- `ETR_it`: Cash effective tax rate for firm `i` in year `t`. A higher `ETR` indicates less tax avoidance.\n- `AQI_it`: City-level Air Quality Index, scaled by 1000. Higher `AQI` means worse pollution.\n- `PC_it`: An indicator variable = 1 if the firm has politically connected executives, 0 otherwise.\n- `DUAL_it`: An indicator variable = 1 if the CEO is also the board chair (proxy for managerial entrenchment).\n- `SOE_it`: An indicator variable = 1 if the firm is a state-owned enterprise.\n\n### Data / Model Specification\n\nThe baseline model is:\n  \nETR_{it} = \\beta_{0} + \\beta_{1} \\mathrm{AQI}_{it} + \\text{Controls}_{it} + \\mu_{i} + \\eta_{t} + \\varepsilon_{it}\n \n\nThe interaction model is:\n  \nETR_{it} = \\alpha_{0} + \\alpha_{1} (AQI_{it} \\times Moderator_{it}) + \\alpha_{2} AQI_{it} + \\alpha_{3} Moderator_{it} + ... + \\nu_{it}\n \n\n**Table 1: Summary Statistics**\n\n| Variable | Mean  | Std. Dev. |\n| :---     | :---- | :---      |\n| ETR      | 0.207 | 0.126     |\n| AQI      | 0.082 | 0.023     |\n\n**Table 2: Baseline Regression Result**\n\n| Variable | Coefficient (β₁) |\n| :---     | :---        |\n| AQI      | 0.084**     |\n|          | (0.038)     |\n\n**Table 3: Channel Test (Governmental Pressure)**\n\n| Variables | Coefficient (ETR) |\n| :---      | :---: |\n| AQI       | 0.024** |\n|           | (0.008) |\n| AQI*PC    | -0.036*** |\n|           | (0.011) |\n\n**Table 4: Heterogeneous Effects Tests**\n\n| Variables | (Col 2) CEO Duality | (Col 3) State-Owned |\n| :---      | :---: | :---: |\n| AQI       | 0.133*** | 0.117** |\n|           | (0.044) | (0.057) |\n| AQI*DUAL  | -0.108*** | | \n|           | (0.009) | | \n| AQI*SOE   | | 0.357*** |\n|           | | (0.106) |\n\n*Note: Standard errors in parentheses. **, *** denote significance at 5% and 1% levels.*\n\n### The Question\n\nBased on the provided data and regression results, which of the following statements are quantitatively supported? (Select all that apply)", "Options": {"A": "A one-standard-deviation increase in AQI is associated with an increase in ETR of approximately 0.19 percentage points.", "B": "For firms with political connections (PC=1), the marginal effect of AQI on ETR is negative (-0.012).", "C": "For firms with CEO duality (DUAL=1), the marginal effect of AQI on ETR is 0.133.", "D": "For state-owned enterprises (SOE=1), the positive effect of AQI on ETR is weaker than for non-SOEs."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an 'atomic decomposition' strategy to test multiple quantitative conclusions from the paper. The assessment target is the ability to perform multi-step calculations (economic magnitude, marginal effects with interactions) and interpret the results correctly. This is a Computational Judgment task. Distractor C is a conceptual error (misinterpreting the sign of an interaction term, which indicates amplification, not weakening). Distractor D is a step-omission error (failing to include the interaction term in the marginal effect calculation, reporting only the baseline coefficient).", "qid": "335", "question": "### Background\n\n**Research Question.** A study establishes a causal link suggesting that higher local air pollution leads to less corporate tax avoidance (i.e., a higher effective tax rate, ETR). This question explores the economic significance of this finding, the mechanisms driving it, and how the effect varies across different types of firms.\n\n**Setting / Data-Generating Environment.** The analysis uses a panel of Chinese A-share firms. The main effect is estimated via a fixed-effects model. The mechanisms and heterogeneous effects are explored using interaction models.\n\n**Variables & Parameters.**\n- `ETR_it`: Cash effective tax rate for firm `i` in year `t`. A higher `ETR` indicates less tax avoidance.\n- `AQI_it`: City-level Air Quality Index, scaled by 1000. Higher `AQI` means worse pollution.\n- `PC_it`: An indicator variable = 1 if the firm has politically connected executives, 0 otherwise.\n- `DUAL_it`: An indicator variable = 1 if the CEO is also the board chair (proxy for managerial entrenchment).\n- `SOE_it`: An indicator variable = 1 if the firm is a state-owned enterprise.\n\n### Data / Model Specification\n\nThe baseline model is:\n  \nETR_{it} = \\beta_{0} + \\beta_{1} \\mathrm{AQI}_{it} + \\text{Controls}_{it} + \\mu_{i} + \\eta_{t} + \\varepsilon_{it}\n \n\nThe interaction model is:\n  \nETR_{it} = \\alpha_{0} + \\alpha_{1} (AQI_{it} \\times Moderator_{it}) + \\alpha_{2} AQI_{it} + \\alpha_{3} Moderator_{it} + ... + \\nu_{it}\n \n\n**Table 1: Summary Statistics**\n\n| Variable | Mean  | Std. Dev. |\n| :---     | :---- | :---      |\n| ETR      | 0.207 | 0.126     |\n| AQI      | 0.082 | 0.023     |\n\n**Table 2: Baseline Regression Result**\n\n| Variable | Coefficient (β₁) |\n| :---     | :---        |\n| AQI      | 0.084**     |\n|          | (0.038)     |\n\n**Table 3: Channel Test (Governmental Pressure)**\n\n| Variables | Coefficient (ETR) |\n| :---      | :---: |\n| AQI       | 0.024** |\n|           | (0.008) |\n| AQI*PC    | -0.036*** |\n|           | (0.011) |\n\n**Table 4: Heterogeneous Effects Tests**\n\n| Variables | (Col 2) CEO Duality | (Col 3) State-Owned |\n| :---      | :---: | :---: |\n| AQI       | 0.133*** | 0.117** |\n|           | (0.044) | (0.057) |\n| AQI*DUAL  | -0.108*** | | \n|           | (0.009) | | \n| AQI*SOE   | | 0.357*** |\n|           | | (0.106) |\n\n*Note: Standard errors in parentheses. **, *** denote significance at 5% and 1% levels.*\n\n### The Question\n\nBased on the provided data and regression results, which of the following statements are quantitatively supported? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": ["B"], "error": null}}
{"ID": 259, "Question": "### Background\n\n**Research Question.** What is the empirical journey from a raw correlation to a final, causal estimate of the net economic impact of M&A rumors on deal outcomes?\n\n**Setting / Data-Generating Environment.** The analysis progresses through several stages: (1) examining raw descriptive data, (2) estimating a naive model that ignores endogeneity and selection bias, and (3) estimating a full structural model using Indirect Inference (II) to correct for these biases, and (4) synthesizing the final estimates to calculate the overall economic impact.\n\n**Variables & Parameters.**\n- `Closing`: Binary variable; 1 if a deal is completed, 0 otherwise.\n- `Ln(Price)`: The natural logarithm of the deal's transaction multiple (e.g., EV/Sales).\n- `Leak`: Binary variable; 1 if a deal was subject to a pre-announcement rumor, 0 otherwise.\n- `\\gamma`: The causal effect of a `Leak` on the propensity for `Closing`.\n- `\\kappa`: The causal effect of a `Leak` on `Ln(Price)` for completed deals.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Naive OLS Model for Deal Value, Ln(Price)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.066*** |\n| | (0.005) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 8, estimated on observed prices only. `***` denotes p < 0.01.*\n\n**Table 2: Indirect Inference (II) Structural Model Estimates**\n\n| Equation | Variable | Coefficient (Std. Err.) |\n| :--- | :--- | :--- |\n| **Closing** | **Leak** (`\\hat{\\gamma}`) | **-1.734*** (0.596) |\n| **Ln(Price)** | **Leak** (`\\hat{\\kappa}`) | **0.160*** (0.031) |\n*Source: Abridged from original paper's Table 9. `***` denotes p < 0.01.*\n\n**Table 3: Final Economic Impact Calculation**\n\n| Metric | Value |\n| :--- | :--- |\n| **Relative Rumor Price (Damage)** | **-32.42%*** |\n*Source: Original paper's Table 10. `***` denotes p < 0.01.*\n\n---\n\n### Question\n\nThe paper's main contribution comes from contrasting the naive estimates with corrected estimates from a structural model. Based on the full set of results, select all of the following conclusions that are supported by the paper.", "Options": {"A": "A key finding from the structural model (Table 2) is that rumors *increase* the premium for completed deals by 16.0%, a direct reversal of the conclusion from the naive model (Table 1).", "B": "The structural model's estimate for the effect of a leak on closing (`\\hat{\\gamma}` = -1.734) is smaller in magnitude than the naive probit estimate of -0.904 (not shown), indicating the naive model overstated the negative impact.", "C": "The final net economic impact of -32.42% (Table 3) is driven by the fact that the large negative effect of a rumor on deal completion probability outweighs its positive effect on the price of deals that do close.", "D": "The paper's final conclusion is that the 16.0% price premium for rumored deals makes leaking a profitable strategy for sellers on average, despite the risks."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the paper's core causal findings, contrasting naive vs. corrected estimates and interpreting the ultimate net economic impact. It uses an 'Atomic Decomposition' strategy to test the key takeaways. Distractor C is a 'Numerical' error, requiring careful comparison of coefficient magnitudes. Distractor D is a 'Conceptual Opposite' that presents a conclusion directly refuted by the paper's final -32.42% damage estimate.", "qid": "259", "question": "### Background\n\n**Research Question.** What is the empirical journey from a raw correlation to a final, causal estimate of the net economic impact of M&A rumors on deal outcomes?\n\n**Setting / Data-Generating Environment.** The analysis progresses through several stages: (1) examining raw descriptive data, (2) estimating a naive model that ignores endogeneity and selection bias, and (3) estimating a full structural model using Indirect Inference (II) to correct for these biases, and (4) synthesizing the final estimates to calculate the overall economic impact.\n\n**Variables & Parameters.**\n- `Closing`: Binary variable; 1 if a deal is completed, 0 otherwise.\n- `Ln(Price)`: The natural logarithm of the deal's transaction multiple (e.g., EV/Sales).\n- `Leak`: Binary variable; 1 if a deal was subject to a pre-announcement rumor, 0 otherwise.\n- `\\gamma`: The causal effect of a `Leak` on the propensity for `Closing`.\n- `\\kappa`: The causal effect of a `Leak` on `Ln(Price)` for completed deals.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Naive OLS Model for Deal Value, Ln(Price)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.066*** |\n| | (0.005) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 8, estimated on observed prices only. `***` denotes p < 0.01.*\n\n**Table 2: Indirect Inference (II) Structural Model Estimates**\n\n| Equation | Variable | Coefficient (Std. Err.) |\n| :--- | :--- | :--- |\n| **Closing** | **Leak** (`\\hat{\\gamma}`) | **-1.734*** (0.596) |\n| **Ln(Price)** | **Leak** (`\\hat{\\kappa}`) | **0.160*** (0.031) |\n*Source: Abridged from original paper's Table 9. `***` denotes p < 0.01.*\n\n**Table 3: Final Economic Impact Calculation**\n\n| Metric | Value |\n| :--- | :--- |\n| **Relative Rumor Price (Damage)** | **-32.42%*** |\n*Source: Original paper's Table 10. `***` denotes p < 0.01.*\n\n---\n\n### Question\n\nThe paper's main contribution comes from contrasting the naive estimates with corrected estimates from a structural model. Based on the full set of results, select all of the following conclusions that are supported by the paper.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 231, "Question": "### Background\n\nA firm is designing a hurdle rate ESO for a senior executive characterized by risk aversion `α=7` and excess stock holdings `θ=75%`. The board wants to grant an option package with a target subjective value of approximately $4.95. They are considering two designs:\n\n*   **Design A ('Standard'):** 2-year vesting period, 10% hurdle rate `h`.\n*   **Design B ('Lenient Vesting'):** 1-year vesting period, but with a hurdle rate `h'` adjusted upwards to keep the subjective value at $4.95.\n\n### Data / Model Specification\n\nFor an executive with `α=7, θ=75%`, the following sensitivities apply:\n\n*   **Subjective vesting period delta:** The subjective value increases by **$1.19** for each 1-year *decrease* in the vesting period.\n*   **Subjective hurdle rate delta:** The subjective value decreases by **$0.50** for each 1 percentage point *increase* in the hurdle rate `h`.\n\nThe paper argues that the superior design is the one that delivers the target subjective value to the executive at the lowest objective cost to the firm.\n\n---\n\nBased on the information provided, which of the following statements are correct?\n", "Options": {"A": "To offset the value gained from shortening the vesting period, the hurdle rate in Design B must be set to 12.38%.", "B": "Design B is superior from a shareholder's perspective because it achieves the same incentive effect for the executive at a lower objective cost to the firm.", "C": "The increase in subjective value from shortening the vesting period from 2 years to 1 year is $1.19.", "D": "To keep the subjective value constant, the hurdle rate in Design B must be decreased to 7.62%."}, "Answer": ["A", "B", "C"], "pi_justification": "This item uses a Computational Judgment strategy, requiring a multi-step calculation to evaluate a contract design trade-off. Option C is a direct data extraction. Option A requires calculating the necessary hurdle rate increase: Required value offset = $1.19; Required % increase = $1.19 / $0.50 = 2.38%; New rate = 10% + 2.38% = 12.38%. Option B requires the strategic conclusion that since both shortening the vesting period and increasing the hurdle rate tend to lower the firm's objective cost, Design B is unambiguously cheaper. Option D represents a Sign Error, incorrectly subtracting the required adjustment.", "qid": "231", "question": "### Background\n\nA firm is designing a hurdle rate ESO for a senior executive characterized by risk aversion `α=7` and excess stock holdings `θ=75%`. The board wants to grant an option package with a target subjective value of approximately $4.95. They are considering two designs:\n\n*   **Design A ('Standard'):** 2-year vesting period, 10% hurdle rate `h`.\n*   **Design B ('Lenient Vesting'):** 1-year vesting period, but with a hurdle rate `h'` adjusted upwards to keep the subjective value at $4.95.\n\n### Data / Model Specification\n\nFor an executive with `α=7, θ=75%`, the following sensitivities apply:\n\n*   **Subjective vesting period delta:** The subjective value increases by **$1.19** for each 1-year *decrease* in the vesting period.\n*   **Subjective hurdle rate delta:** The subjective value decreases by **$0.50** for each 1 percentage point *increase* in the hurdle rate `h`.\n\nThe paper argues that the superior design is the one that delivers the target subjective value to the executive at the lowest objective cost to the firm.\n\n---\n\nBased on the information provided, which of the following statements are correct?\n", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 340, "Question": "### Background\n\n**Research Question.** How does the market's valuation of a firm's current operations (assets in place) and future opportunities (real options) depend on the interaction between the firm's profitability and its growth stage?\n\n**Setting.** A valuation model is estimated on distinct subsamples of firms, partitioned by profitability (profit vs. loss) and sales growth (low vs. high). The analysis focuses on the valuation coefficients of Net Income (`NI`), a proxy for assets in place, and R&D expenditures (`RD`), a proxy for real options. The persistence of abnormal earnings is also estimated as a foundational input for the valuation model and as a justification for the valuation patterns.\n\n**Variables and Parameters.**\n\n*   `P_{i,t}`: Price per share of firm `i` at the end of year `t`.\n*   `NI_{i,t}`: Net income per share, a proxy for the value of assets in place.\n*   `RD_{i,t}`: R&D expenditures per share, a proxy for investment in real options.\n*   `NI_{i,t}^a`: Abnormal earnings for firm `i` in period `t`.\n*   `\\omega_1`: The persistence parameter of abnormal earnings.\n*   `\\beta_3`, `\\beta_4`: Regression coefficients on `NI` and `RD`, respectively.\n\n---\n\n### Data / Model Specification\n\nThe following valuation model is estimated:\n\n  \nP_{i,t} = \\beta_0 + \\beta_1 BV_{i,t} + \\beta_2 DIV_{i,t} + \\beta_3 NI_{i,t} + \\beta_4 RD_{i,t} + \\beta_5 OTHER_{i,t} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nThe key hypotheses regarding profitability and growth are:\n*   **H2a:** For firms reporting a loss, the market places a lower weight on net income and positively values R&D expenditures.\n*   **H2b:** For firms reporting a loss, the market places a higher value on R&D expenditures when the firm is experiencing high sales growth.\n*   **H2c:** For firms reporting a profit, the market positively values R&D expenditures when the firm is experiencing high sales growth, but not when the firm is experiencing low sales growth.\n\nTable 1 below presents the estimated persistence parameters (`\\hat{\\omega}_1`) for various subsamples. Table 2 presents the key coefficient estimates (`\\hat{\\beta}_3` and `\\hat{\\beta}_4`) from estimating Eq. (1) on four subsamples.\n\n**Table 1: Abnormal Earnings Persistence Parameters (`\\hat{\\omega}_1`)**\n\n| | Entire sample | Low sales growth firms | High sales growth firms |\n| :--- | :---: | :---: | :---: |\n| **Entire sample** | 0.271 | 0.350 | 0.229 |\n| **Loss firms** | 0.133 | 0.207 | 0.081 |\n| **Profit firms** | 0.339 | 0.391 | 0.288 |\n\n*Note: Differences between high and low sales growth firms are statistically significant at p < 0.05.*\n\n**Table 2: Valuation Regression Results for `NI` and `RD`**\n\n| Variable | Subsample | Low Growth | High Growth |\n| :--- | :--- | :---: | :---: |\n| **NI** (`\\hat{\\beta}_3`) | Loss Firms | 0.440 | 0.371 |\n| | | (1.38) | (0.60) |\n| | Profit Firms | **3.608** | **1.252** |\n| | | (19.26) | (4.31) |\n| **RD** (`\\hat{\\beta}_4`) | Loss Firms | **2.100** | **4.081** |\n| | | (5.09) | (5.21) |\n| | Profit Firms | **-3.285** | **1.880** |\n| | | (10.48) | (4.23) |\n\n*Note: t-statistics are in parentheses. Bolded coefficients are significant at p < 0.01. Differences between low and high growth are statistically significant.*\n\n---\n\n### Question\n\nBased on the regression results in Table 2, which of the following statements accurately interpret the valuation of R&D expenditures (`RD`)?", "Options": {"A": "The positive valuation of `RD` for high-growth profitable firms is inconsistent with Hypothesis 2c.", "B": "For profitable firms, `RD` is valued positively regardless of the growth stage, but the valuation is higher in high-growth environments.", "C": "For loss-making firms, the market values `RD` more highly when they are in a high-growth stage compared to a low-growth stage.", "D": "The negative valuation of `RD` for low-growth profitable firms is consistent with the view that, in a stable environment, R&D may be seen as a risky diversion of cash from a successful core business."}, "Answer": ["C", "D"], "pi_justification": "This item replaces a portion of a multi-part Table QA problem, assessing the ability to interpret conditional regression coefficients in line with the paper's core hypotheses. It uses an **Atomic Decomposition** strategy, breaking down the complex findings on R&D valuation into distinct, verifiable statements. Distractors are designed to trap common misinterpretations, such as ignoring the sign change (B) or misreading the hypothesis (D).", "qid": "340", "question": "### Background\n\n**Research Question.** How does the market's valuation of a firm's current operations (assets in place) and future opportunities (real options) depend on the interaction between the firm's profitability and its growth stage?\n\n**Setting.** A valuation model is estimated on distinct subsamples of firms, partitioned by profitability (profit vs. loss) and sales growth (low vs. high). The analysis focuses on the valuation coefficients of Net Income (`NI`), a proxy for assets in place, and R&D expenditures (`RD`), a proxy for real options. The persistence of abnormal earnings is also estimated as a foundational input for the valuation model and as a justification for the valuation patterns.\n\n**Variables and Parameters.**\n\n*   `P_{i,t}`: Price per share of firm `i` at the end of year `t`.\n*   `NI_{i,t}`: Net income per share, a proxy for the value of assets in place.\n*   `RD_{i,t}`: R&D expenditures per share, a proxy for investment in real options.\n*   `NI_{i,t}^a`: Abnormal earnings for firm `i` in period `t`.\n*   `\\omega_1`: The persistence parameter of abnormal earnings.\n*   `\\beta_3`, `\\beta_4`: Regression coefficients on `NI` and `RD`, respectively.\n\n---\n\n### Data / Model Specification\n\nThe following valuation model is estimated:\n\n  \nP_{i,t} = \\beta_0 + \\beta_1 BV_{i,t} + \\beta_2 DIV_{i,t} + \\beta_3 NI_{i,t} + \\beta_4 RD_{i,t} + \\beta_5 OTHER_{i,t} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nThe key hypotheses regarding profitability and growth are:\n*   **H2a:** For firms reporting a loss, the market places a lower weight on net income and positively values R&D expenditures.\n*   **H2b:** For firms reporting a loss, the market places a higher value on R&D expenditures when the firm is experiencing high sales growth.\n*   **H2c:** For firms reporting a profit, the market positively values R&D expenditures when the firm is experiencing high sales growth, but not when the firm is experiencing low sales growth.\n\nTable 1 below presents the estimated persistence parameters (`\\hat{\\omega}_1`) for various subsamples. Table 2 presents the key coefficient estimates (`\\hat{\\beta}_3` and `\\hat{\\beta}_4`) from estimating Eq. (1) on four subsamples.\n\n**Table 1: Abnormal Earnings Persistence Parameters (`\\hat{\\omega}_1`)**\n\n| | Entire sample | Low sales growth firms | High sales growth firms |\n| :--- | :---: | :---: | :---: |\n| **Entire sample** | 0.271 | 0.350 | 0.229 |\n| **Loss firms** | 0.133 | 0.207 | 0.081 |\n| **Profit firms** | 0.339 | 0.391 | 0.288 |\n\n*Note: Differences between high and low sales growth firms are statistically significant at p < 0.05.*\n\n**Table 2: Valuation Regression Results for `NI` and `RD`**\n\n| Variable | Subsample | Low Growth | High Growth |\n| :--- | :--- | :---: | :---: |\n| **NI** (`\\hat{\\beta}_3`) | Loss Firms | 0.440 | 0.371 |\n| | | (1.38) | (0.60) |\n| | Profit Firms | **3.608** | **1.252** |\n| | | (19.26) | (4.31) |\n| **RD** (`\\hat{\\beta}_4`) | Loss Firms | **2.100** | **4.081** |\n| | | (5.09) | (5.21) |\n| | Profit Firms | **-3.285** | **1.880** |\n| | | (10.48) | (4.23) |\n\n*Note: t-statistics are in parentheses. Bolded coefficients are significant at p < 0.01. Differences between low and high growth are statistically significant.*\n\n---\n\n### Question\n\nBased on the regression results in Table 2, which of the following statements accurately interpret the valuation of R&D expenditures (`RD`)?", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 212, "Question": "### Background\n\n**Research Question.** How do different sources of uncertainty—specifically parameter risk and model risk—contribute to the total uncertainty in pricing a longevity-linked derivative?\n\n**Setting.** The fair spread `σ` for a 10-year QxX index swap is estimated using a two-factor stochastic mortality model. The analysis first quantifies **parameter risk**—the uncertainty in model parameters `μ` and `V` estimated from a fixed data sample (1971-2005)—by generating a 95% confidence interval (CI) for the spread. It then quantifies **model risk** by re-estimating the central value of the spread using different data samples, reflecting uncertainty about a potential structural break in mortality trends around the year 2000.\n\n### Data / Model Specification\n\n**Table 1.** 95% Confidence Intervals for `σ` from Parameter Risk Analysis (in basis points).\nThis table reflects uncertainty assuming the model is correctly specified and fitted to the 1971-2005 data.\n\n| λ_1   | λ_2   | 95% CI for σ |\n|-------|-------|--------------|\n| 0     | 0.316 | (581, 668)   |\n| 0.375 | 0     | (587, 673)   |\n| 0.175 | 0.175 | (583, 671)   |\n\n**Table 2.** Central Estimates of `σ` from Model Risk Analysis (in basis points).\nThis table shows how the central estimate of `σ` changes based on the estimation period.\n- **Scenario 1:** Full sample (1971-2005).\n- **Scenario 2:** Pre-break sample (1971-2000), assuming old trends revert.\n- **Scenario 3:** Post-break sample (2000-2005), assuming the new, slower-improvement trend is permanent.\n\n| (λ_1, λ_2)    | Scenario 1 | Scenario 2 | Scenario 3 |\n|---------------|------------|------------|------------|\n| (0, 0.316)    | 625        | 634        | 589        |\n| (0.375, 0)    | 631        | 638        | 594        |\n| (0.175, 0.175)| 627        | 635        | 591        |\n\n---\n\nBased on the provided data for the case where `(λ_1, λ_2) = (0.375, 0)`, select all of the following statements that are correct.\n", "Options": {"A": "The lowest central estimate from model risk (Scenario 3) falls within the 95% confidence interval for parameter risk, indicating that parameter risk fully accounts for model risk.", "B": "A heuristic 95% uncertainty range that combines parameter risk with the model risk from Scenario 3 (permanent trend change) is (551, 637) basis points.", "C": "The uncertainty in the swap spread `σ` attributable to model risk, as measured by the range of central estimates across scenarios, is 44 basis points.", "D": "The central estimate of the swap spread `σ` under the baseline model (Scenario 1) is 630 basis points."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to quantify and synthesize two distinct types of risk (parameter and model risk) from tabular data. It requires calculation, comparison, and the construction of a combined risk measure. Depth Strategy: Computational Judgment. The user must perform several calculations: finding the midpoint of a CI, calculating the range of central estimates, and constructing a new CI by shifting the parameter risk interval. Distractor Logic: Option A is correct (midpoint of (587, 673) is 630). Option B is correct (range of 638 to 594 is 44). Option C is correct (the parameter risk uncertainty is ±43 bps; applying this to the Scenario 3 central estimate of 594 gives 594 ± 43 = (551, 637)). Option D is a plausible but incorrect conceptual conclusion (a 'True but Irrelevant' distractor), as the two risks are distinct and additive, not nested.", "qid": "212", "question": "### Background\n\n**Research Question.** How do different sources of uncertainty—specifically parameter risk and model risk—contribute to the total uncertainty in pricing a longevity-linked derivative?\n\n**Setting.** The fair spread `σ` for a 10-year QxX index swap is estimated using a two-factor stochastic mortality model. The analysis first quantifies **parameter risk**—the uncertainty in model parameters `μ` and `V` estimated from a fixed data sample (1971-2005)—by generating a 95% confidence interval (CI) for the spread. It then quantifies **model risk** by re-estimating the central value of the spread using different data samples, reflecting uncertainty about a potential structural break in mortality trends around the year 2000.\n\n### Data / Model Specification\n\n**Table 1.** 95% Confidence Intervals for `σ` from Parameter Risk Analysis (in basis points).\nThis table reflects uncertainty assuming the model is correctly specified and fitted to the 1971-2005 data.\n\n| λ_1   | λ_2   | 95% CI for σ |\n|-------|-------|--------------|\n| 0     | 0.316 | (581, 668)   |\n| 0.375 | 0     | (587, 673)   |\n| 0.175 | 0.175 | (583, 671)   |\n\n**Table 2.** Central Estimates of `σ` from Model Risk Analysis (in basis points).\nThis table shows how the central estimate of `σ` changes based on the estimation period.\n- **Scenario 1:** Full sample (1971-2005).\n- **Scenario 2:** Pre-break sample (1971-2000), assuming old trends revert.\n- **Scenario 3:** Post-break sample (2000-2005), assuming the new, slower-improvement trend is permanent.\n\n| (λ_1, λ_2)    | Scenario 1 | Scenario 2 | Scenario 3 |\n|---------------|------------|------------|------------|\n| (0, 0.316)    | 625        | 634        | 589        |\n| (0.375, 0)    | 631        | 638        | 594        |\n| (0.175, 0.175)| 627        | 635        | 591        |\n\n---\n\nBased on the provided data for the case where `(λ_1, λ_2) = (0.375, 0)`, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "C", "model_answer": ["C"], "error": null}}
{"ID": 172, "Question": "### Background\n\n**Research Question.** What is the appropriate econometric framework for modeling the dynamic relationships between house prices, liquidity, and market tightness in a short panel dataset where variables may have different time-series properties?\n\n**Setting.** The study uses a quarterly panel of 403 Dutch municipalities from 2011 to 2013 (`T=12`, `N=403`). A panel Vector Autoregression (VAR) is proposed, but its specification and estimation must account for non-stationarity and the biases inherent in short dynamic panels.\n\n**Variables & Parameters.**\n- `pr`: Log house price index.\n- `ros`: Rate of sale (liquidity measure).\n- `cph`: Log clicks per house (market tightness measure).\n- `Δ`: The first-difference operator.\n- `I(1)` / `I(0)`: Integrated of order one (non-stationary) / zero (stationary).\n\n### Data / Model Specification\n\nTo determine the correct specification, panel unit root tests were conducted. The results are in Table 1. The proposed empirical model is a panel VAR, estimated using the Generalized Method of Moments (GMM).\n\n**Table 1. Panel Unit Root Test Results**\n\n| Variable | Lags | p-value | Conclusion |\n| :--- | :--- | :--- | :--- |\n| `pr` | 1 | 1.0000 | Non-stationary |\n| `ros` | 1 | 0.0020 | Stationary |\n| `cph` | 1 | 0.0000 | Stationary |\n| `Δpr` | 1 | 0.0000 | Stationary |\n\n*Notes: H₀: All panels contain unit roots. A low p-value rejects H₀.*\n\n### Question\n\nBased on the provided methodological details, which of the following statements accurately describe the econometric specification and estimation strategy used in the paper? (Select all that apply)", "Options": {"A": "The log house price series (`pr`) is included in the VAR in first-differences because unit root tests show it is non-stationary (`I(1)`), and this avoids the problem of spurious regression.", "B": "According to the unit root tests, the rate of sale (`ros`) and clicks per house (`cph`) are non-stationary and must also be first-differenced.", "C": "Nickell's bias is a concern in panels with a large time dimension (`large T`) and is addressed by using standard fixed effects estimation.", "D": "The choice of GMM for estimation is necessary to correct for Nickell's bias, which occurs when including lagged dependent variables in a panel with a small time dimension (`T`)."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the paper's core econometric methodology. It uses a **Premise/Assumption Packaging** strategy, combining key justifications for the model specification and estimation choices. Correct options (A, B) accurately state the reason for differencing the price series (non-stationarity) and for using GMM (Nickell's bias in short panels). Distractors are based on classic econometric misconceptions: (C) misreads the unit root test results for the other variables, and (D) incorrectly defines the conditions for Nickell's bias (`small T`, not `large T`) and its solution (GMM, not standard fixed effects).", "qid": "172", "question": "### Background\n\n**Research Question.** What is the appropriate econometric framework for modeling the dynamic relationships between house prices, liquidity, and market tightness in a short panel dataset where variables may have different time-series properties?\n\n**Setting.** The study uses a quarterly panel of 403 Dutch municipalities from 2011 to 2013 (`T=12`, `N=403`). A panel Vector Autoregression (VAR) is proposed, but its specification and estimation must account for non-stationarity and the biases inherent in short dynamic panels.\n\n**Variables & Parameters.**\n- `pr`: Log house price index.\n- `ros`: Rate of sale (liquidity measure).\n- `cph`: Log clicks per house (market tightness measure).\n- `Δ`: The first-difference operator.\n- `I(1)` / `I(0)`: Integrated of order one (non-stationary) / zero (stationary).\n\n### Data / Model Specification\n\nTo determine the correct specification, panel unit root tests were conducted. The results are in Table 1. The proposed empirical model is a panel VAR, estimated using the Generalized Method of Moments (GMM).\n\n**Table 1. Panel Unit Root Test Results**\n\n| Variable | Lags | p-value | Conclusion |\n| :--- | :--- | :--- | :--- |\n| `pr` | 1 | 1.0000 | Non-stationary |\n| `ros` | 1 | 0.0020 | Stationary |\n| `cph` | 1 | 0.0000 | Stationary |\n| `Δpr` | 1 | 0.0000 | Stationary |\n\n*Notes: H₀: All panels contain unit roots. A low p-value rejects H₀.*\n\n### Question\n\nBased on the provided methodological details, which of the following statements accurately describe the econometric specification and estimation strategy used in the paper? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "\n\nA,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 251, "Question": "### Background\n\nThe paper proposes that a key feature of a derivative is its embedded leverage (`Ω`), which is the return magnification it offers relative to its underlying asset. The central hypothesis is that leverage-constrained investors bid up the prices of securities with high `Ω`, leading to lower subsequent risk-adjusted returns.\n\n### Data / Model Specification\n\n**Table 1: Summary Statistics for Embedded Leverage (1996-2018)**\n\n| Sample | Mean `Ω` |\n| :--- | :--- |\n| Equity Options | 7.02 |\n| Index Options | 14.61 |\n\n**Table 2: Embedded Leverage and Returns for Extreme Index Option Portfolios**\n\n| Portfolio Category | Maturity | `|Δ|` Range | Avg. `Ω` | Avg. Delta-Hedged Excess Return (%) |\n| :--- | :--- | :--- | :--- | :--- |\n| High-Leverage | < 1 month | 0.00 - 0.20 | 42.16 | -29.23% |\n| Low-Leverage | > 12 months | 0.80 - 1.00 | 3.27 | +0.01% |\n\n### Question\n\nBased on the empirical evidence presented in Table 1 and Table 2, select all of the following statements that are true.", "Options": {"A": "On average, options on broad market indexes tend to have higher embedded leverage than options on individual equities.", "B": "Securities with higher embedded leverage are associated with lower subsequent risk-adjusted returns.", "C": "Securities with higher embedded leverage are associated with higher subsequent risk-adjusted returns.", "D": "On average, options on individual equities tend to have higher embedded leverage than options on broad market indexes."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the student's ability to interpret empirical results from tables, replacing the 'interpretation' part of the original QA. It is a 'select-all-valid' task. The correct options (A, B) are direct conclusions from Table 2 and Table 1, respectively. The distractors (C, D) are the direct conceptual opposites of the correct statements, designed to catch misinterpretations of the data or the paper's main hypothesis.", "qid": "251", "question": "### Background\n\nThe paper proposes that a key feature of a derivative is its embedded leverage (`Ω`), which is the return magnification it offers relative to its underlying asset. The central hypothesis is that leverage-constrained investors bid up the prices of securities with high `Ω`, leading to lower subsequent risk-adjusted returns.\n\n### Data / Model Specification\n\n**Table 1: Summary Statistics for Embedded Leverage (1996-2018)**\n\n| Sample | Mean `Ω` |\n| :--- | :--- |\n| Equity Options | 7.02 |\n| Index Options | 14.61 |\n\n**Table 2: Embedded Leverage and Returns for Extreme Index Option Portfolios**\n\n| Portfolio Category | Maturity | `|Δ|` Range | Avg. `Ω` | Avg. Delta-Hedged Excess Return (%) |\n| :--- | :--- | :--- | :--- | :--- |\n| High-Leverage | < 1 month | 0.00 - 0.20 | 42.16 | -29.23% |\n| Low-Leverage | > 12 months | 0.80 - 1.00 | 3.27 | +0.01% |\n\n### Question\n\nBased on the empirical evidence presented in Table 1 and Table 2, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 358, "Question": "### Background\n\n**Research Question.** How do investment banks' valuations respond to deregulation events that simultaneously increase competition and the likelihood of being an acquisition target?\n\n**Setting and Sample.** An event study of U.S. investment banks. The paper posits two competing forces: a negative 'competition effect' as commercial banks enter their market, and a positive 'acquisition target effect' as those same commercial banks become potential buyers. The paper also notes that rumors preceded some announcements, suggesting potential information leakage.\n\n### Data / Model Specification\n\n**Table 1. Two-Day Cumulative Abnormal Returns `CAR(-1,0)` for Investment Banks**\n\n| Event | Description                       | `CAR(-1,0)` (%) | t-statistic |\n| :---- | :-------------------------------- | :-------------- | :---------- |\n| 2     | Increasing Revenue Limits         | 0.53            | (1.07)      |\n| 3     | Bankers Trust Acquires Alex Brown | 3.89            | (7.93)**    |\n| 4     | FSMA Passage                      | 6.18            | (4.19)**    |\n\n*Note: Adapted from Table 3 of the source paper. ** indicates significance at the 0.01 level.*\n\n---\n\n### Question\n\nBased on the paper's hypotheses and the results in **Table 1**, select all statements that represent a valid conclusion.", "Options": {"A": "The highly significant positive returns for Events 3 and 4 suggest that for major, transformative announcements, the market perceived the 'acquisition target effect' to be much stronger than the negative 'competition effect' for investment banks.", "B": "The paper's observation of pre-announcement rumors for Event 3 implies that the reported `CAR(-1,0)` of 3.89% may understate the total valuation impact, as some price adjustment could have occurred before day -1.", "C": "The results indicate that the 'competition effect' from commercial banks entering their market consistently dominated any potential benefits for investment banks across all events.", "D": "The insignificant result for Event 2 proves that increasing revenue limits for commercial banks had no economic impact whatsoever on the investment banking sector."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize empirical results with competing theoretical hypotheses and understand the methodological implications of information leakage. It uses the 'Atomic Decomposition' strategy. Option A tests the core synthesis of the competing effects theory with the data. Option B tests the understanding of how information leakage affects event study interpretation. Distractor C is a 'Conceptual Opposite' of the paper's main finding. Distractor D represents a common but incorrect interpretation of statistical insignificance ('absence of evidence is not evidence of absence').", "qid": "358", "question": "### Background\n\n**Research Question.** How do investment banks' valuations respond to deregulation events that simultaneously increase competition and the likelihood of being an acquisition target?\n\n**Setting and Sample.** An event study of U.S. investment banks. The paper posits two competing forces: a negative 'competition effect' as commercial banks enter their market, and a positive 'acquisition target effect' as those same commercial banks become potential buyers. The paper also notes that rumors preceded some announcements, suggesting potential information leakage.\n\n### Data / Model Specification\n\n**Table 1. Two-Day Cumulative Abnormal Returns `CAR(-1,0)` for Investment Banks**\n\n| Event | Description                       | `CAR(-1,0)` (%) | t-statistic |\n| :---- | :-------------------------------- | :-------------- | :---------- |\n| 2     | Increasing Revenue Limits         | 0.53            | (1.07)      |\n| 3     | Bankers Trust Acquires Alex Brown | 3.89            | (7.93)**    |\n| 4     | FSMA Passage                      | 6.18            | (4.19)**    |\n\n*Note: Adapted from Table 3 of the source paper. ** indicates significance at the 0.01 level.*\n\n---\n\n### Question\n\nBased on the paper's hypotheses and the results in **Table 1**, select all statements that represent a valid conclusion.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 353, "Question": "### Background\nA study models the joint effect of private anti-self-dealing regulations and ownership structure on corporate valuation. A key challenge in such models is endogeneity, where the ownership structure may be correlated with unobserved factors that also affect firm value.\n\n### Data / Model Specification\nThe core empirical model is:\n\n  \n\\text{Tobin's Q}_{it} = \\alpha + \\beta_{1}\\mathrm{WEDGE}_{it} + \\beta_{2}\\mathrm{PRIVATE}_{ct} + \\beta_{3}(\\mathrm{WEDGE}_{it} * \\mathrm{PRIVATE}_{ct}) + \\dots + \\varepsilon_{it} \\quad \\text{(Eq. 1)}\n \n\nThe paper argues that the ownership wedge (`WEDGE`) can be treated as exogenous. However, a critic suggests `WEDGE` is endogenous because it is correlated with an unobserved firm-level governance quality (`U_i`) that also affects `Tobin's Q`. To address this, an instrumental variable (`Z_i`) is proposed for `WEDGE`.\n\n---\n\nWhich of the following conditions are necessary for `Z_i` to be a valid instrument in a Generalized Method of Moments (GMM) framework designed to obtain consistent estimates of the coefficients in Eq. (1)? Select all that apply.", "Options": {"A": "The instrument `Z_i` must be uncorrelated with the other exogenous variables in the model, such as `PRIVATE_ct`.", "B": "The instrument `Z_i` must be uncorrelated with the dependent variable, `Tobin's Q_it`.", "C": "The instrument `Z_i` must be strongly correlated with the endogenous variable, `WEDGE_it`.", "D": "The instrument `Z_i` must be uncorrelated with the model's error term, `ε_it`."}, "Answer": ["C", "D"], "pi_justification": "This item assesses foundational knowledge of causal inference and instrumental variables (IV/GMM), a key methodological challenge discussed in the paper. It uses a Depth Strategy of applying general econometric principles to the specific model in the paper. Options A (the exclusion restriction) and B (instrument relevance) are the two core, defining conditions for a valid instrument. Option C is a distractor; instruments do not need to be uncorrelated with other exogenous regressors. Option D is a Conceptual Opposite; a valid instrument *must* be correlated with the dependent variable, but only *through* its effect on the endogenous variable.", "qid": "353", "question": "### Background\nA study models the joint effect of private anti-self-dealing regulations and ownership structure on corporate valuation. A key challenge in such models is endogeneity, where the ownership structure may be correlated with unobserved factors that also affect firm value.\n\n### Data / Model Specification\nThe core empirical model is:\n\n  \n\\text{Tobin's Q}_{it} = \\alpha + \\beta_{1}\\mathrm{WEDGE}_{it} + \\beta_{2}\\mathrm{PRIVATE}_{ct} + \\beta_{3}(\\mathrm{WEDGE}_{it} * \\mathrm{PRIVATE}_{ct}) + \\dots + \\varepsilon_{it} \\quad \\text{(Eq. 1)}\n \n\nThe paper argues that the ownership wedge (`WEDGE`) can be treated as exogenous. However, a critic suggests `WEDGE` is endogenous because it is correlated with an unobserved firm-level governance quality (`U_i`) that also affects `Tobin's Q`. To address this, an instrumental variable (`Z_i`) is proposed for `WEDGE`.\n\n---\n\nWhich of the following conditions are necessary for `Z_i` to be a valid instrument in a Generalized Method of Moments (GMM) framework designed to obtain consistent estimates of the coefficients in Eq. (1)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 345, "Question": "### Background\n\n**Research Question.** This case assesses the robustness of a trading strategy's profitability to transaction costs.\n\n**Setting and Sample.** A portfolio manager analyzes the `MADP(3)` strategy. The reduction in a portfolio's average return from trading is modeled as `CostDrag = c × Turnover`, where `c` is the transaction cost and `Turnover` is the average annual trade volume. This cost drag is the sole reason for the difference between the no-cost and 50bps-cost `Δ(cer)`.\n\n### Data / Model Specification\n\nThe economic value of a model is measured by the CER gain, `Δ(cer) = CER_{model} - CER_{benchmark}`.\n\n**Table 1: Economic Evaluation of the MADP(3) Strategy**\n\n| Strategy | `Δ(cer)` (%) (No cost) | `Δ(cer)` (%) (50bps cost) |\n|:---|---:|---:|\n| MADP(3) | 1.64 | 1.39 |\n\n\n### Question\n\nBased on the provided model and data for the `MADP(3)` strategy, select all of the following conclusions that are arithmetically correct.", "Options": {"A": "The breakeven transaction cost `c^*` that would make the net CER gain of the `MADP(3)` strategy equal to zero is 328 basis points.", "B": "If the strategy's turnover were 100% annually, its net `Δ(cer)` with a 50bps cost would be 0.82%.", "C": "The annual cost drag for the `MADP(3)` strategy at a 50bps cost is 1.39%.", "D": "The implied annual turnover of the `MADP(3)` strategy is 50%."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This is a **Computational Judgment** item. It requires the user to perform a multi-step calculation based on the provided data and model to verify quantitative statements. Option A requires calculating turnover from the cost drag (`(0.0164 - 0.0139) / 0.005`). Option B requires using the result from A to find the breakeven cost (`0.0164 / 0.5`). Distractors are based on misreading the table (C mistakes the net gain for the drag) and plausible calculation errors (D uses a simple but incorrect heuristic of halving the gross gain).", "qid": "345", "question": "### Background\n\n**Research Question.** This case assesses the robustness of a trading strategy's profitability to transaction costs.\n\n**Setting and Sample.** A portfolio manager analyzes the `MADP(3)` strategy. The reduction in a portfolio's average return from trading is modeled as `CostDrag = c × Turnover`, where `c` is the transaction cost and `Turnover` is the average annual trade volume. This cost drag is the sole reason for the difference between the no-cost and 50bps-cost `Δ(cer)`.\n\n### Data / Model Specification\n\nThe economic value of a model is measured by the CER gain, `Δ(cer) = CER_{model} - CER_{benchmark}`.\n\n**Table 1: Economic Evaluation of the MADP(3) Strategy**\n\n| Strategy | `Δ(cer)` (%) (No cost) | `Δ(cer)` (%) (50bps cost) |\n|:---|---:|---:|\n| MADP(3) | 1.64 | 1.39 |\n\n\n### Question\n\nBased on the provided model and data for the `MADP(3)` strategy, select all of the following conclusions that are arithmetically correct.", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": ["A"], "error": null}}
{"ID": 354, "Question": "### Background\nA study models the effect of country-level public anti-self-dealing regulations (`PUBLIC_c`) on firm-level corporate valuation (`Tobin's Q_it`). The model includes country fixed effects (`δ_c`) to control for unobserved, time-invariant country characteristics.\n\n### Data / Model Specification\nThe model is specified as:\n\n  \n\\text{Tobin's Q}_{it} = \\alpha + \\beta_{1}\\mathrm{WEDGE}_{it} + \\beta_{2}\\mathrm{PUBLIC}_{c} + \\beta_{3}(\\mathrm{WEDGE}_{it} * \\mathrm{PUBLIC}_{c}) + \\dots + \\delta_c + \\varepsilon_{it} \\quad \\text{(Eq. 1)}\n \n\nWhere `i` is firm, `c` is country, and `t` is year. The `PUBLIC_c` index is time-invariant within the sample period. In a standard 'within-estimator' fixed-effects model, the coefficient `β₂` on the time-invariant `PUBLIC_c` variable would be unidentified due to perfect collinearity with the country fixed effects `δ_c`.\n\n---\n\nGiven this specification, which of the following statements about the identification of the model's coefficients are correct? Select all that apply.", "Options": {"A": "The coefficient `β₂` is unidentified in this model due to perfect collinearity between `PUBLIC_c` and the set of country fixed effects `δ_c`.", "B": "The coefficient on the firm-level variable `WEDGE_it`, `β₁`, is identified primarily from the variation of `WEDGE` over time for each individual firm.", "C": "The coefficient on the interaction term, `β₃`, is identified primarily from the variation of `WEDGE_it` across firms within the same country.", "D": "The main effect of public regulation, `β₂`, is identified by assuming a linear effect of `PUBLIC_c` and using the cross-country variation that remains after omitting one country's fixed effect as a baseline."}, "Answer": ["C", "D"], "pi_justification": "This item assesses a sophisticated understanding of econometric identification in panel data models, a key methodological issue for the paper's empirical strategy. It uses a Depth Strategy focusing on the technical underpinnings of the model. Option A is correct because the interaction term varies within countries (due to firm-level `WEDGE`), allowing its coefficient to be estimated separately from the country fixed effects. Option B correctly describes how the coefficient on a time-invariant regressor is identified in a Least Squares Dummy Variable (LSDV) model. Option C is a distractor (Conceptual Opposite) that incorrectly applies the logic of a 'within-estimator' to this specific model. Option D is a distractor (Almost Right); while time-series variation could contribute, the primary source of identification for `β₁` in this pooled OLS setup is the cross-sectional variation between firms.", "qid": "354", "question": "### Background\nA study models the effect of country-level public anti-self-dealing regulations (`PUBLIC_c`) on firm-level corporate valuation (`Tobin's Q_it`). The model includes country fixed effects (`δ_c`) to control for unobserved, time-invariant country characteristics.\n\n### Data / Model Specification\nThe model is specified as:\n\n  \n\\text{Tobin's Q}_{it} = \\alpha + \\beta_{1}\\mathrm{WEDGE}_{it} + \\beta_{2}\\mathrm{PUBLIC}_{c} + \\beta_{3}(\\mathrm{WEDGE}_{it} * \\mathrm{PUBLIC}_{c}) + \\dots + \\delta_c + \\varepsilon_{it} \\quad \\text{(Eq. 1)}\n \n\nWhere `i` is firm, `c` is country, and `t` is year. The `PUBLIC_c` index is time-invariant within the sample period. In a standard 'within-estimator' fixed-effects model, the coefficient `β₂` on the time-invariant `PUBLIC_c` variable would be unidentified due to perfect collinearity with the country fixed effects `δ_c`.\n\n---\n\nGiven this specification, which of the following statements about the identification of the model's coefficients are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 222, "Question": "### Background\n\n**Research Question.** How can dynamic panel GMM estimators be used to obtain consistent estimates in corporate finance settings when regressors are endogenous and persistent, and how are the validity of these estimators tested?\n\n**Setting.** A dynamic panel data model is used to analyze bank behavior. The model includes potentially endogenous regressors like a corporate governance index (`CGI`) and accounts for unobserved bank-specific effects. The paper uses a two-step system GMM estimator as a robustness check for its main findings.\n\n### Data / Model Specification\n\nConsider the canonical dynamic panel model:\n\n  \ny_{it} = \\beta' x_{it} + \\mu_i + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nTo address endogeneity, the system GMM estimator uses lagged levels of variables as instruments for the first-differenced equation and lagged differences as instruments for the levels equation.\n\n**Table 1: GMM Diagnostic Statistics (from paper's Table 4, Column 6)**\n| Test | Null Hypothesis | Reported p-value |\n| :--- | :--- | :---: |\n| AR(2) Test | No second-order serial correlation in differenced residuals | Insignificant* |\n| Hansen J-Test | Instruments are jointly valid (exogenous) | 0.224 |\n*Notes: The paper states the AR(2) test is insignificant without reporting the exact p-value.*\n\n### Question\n\nThe authors use a two-step system GMM estimator as a robustness check. Based on the diagnostic statistics provided in **Table 1**, select all of the following statements that represent a correct interpretation of the tests and their implications.", "Options": {"A": "The insignificant AR(2) test is a necessary condition for the validity of instruments lagged two or more periods, as it supports the core assumption that the original idiosyncratic errors are not serially correlated.", "B": "The Hansen J-test p-value of 0.224 provides strong evidence that the corporate governance index (`CGI`) is exogenous.", "C": "The Hansen J-test p-value of 0.224 indicates that the null hypothesis of joint instrument validity cannot be rejected, supporting the overall model specification.", "D": "A significant AR(2) test (p < 0.05) would be required to confirm the validity of the GMM model's instruments."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the user's understanding of the specification tests for dynamic panel GMM estimators, a critical part of evaluating empirical research. The strategy is Atomic Decomposition of diagnostic interpretations. Correct options (A, B) require a precise understanding of the null hypotheses and purpose of the AR(2) and Hansen tests. Distractor C is a Conceptual Opposite regarding the AR(2) test's interpretation. Distractor D reflects a subtle but common confusion between instrument validity and regressor exogeneity (Conceptual Confusion).", "qid": "222", "question": "### Background\n\n**Research Question.** How can dynamic panel GMM estimators be used to obtain consistent estimates in corporate finance settings when regressors are endogenous and persistent, and how are the validity of these estimators tested?\n\n**Setting.** A dynamic panel data model is used to analyze bank behavior. The model includes potentially endogenous regressors like a corporate governance index (`CGI`) and accounts for unobserved bank-specific effects. The paper uses a two-step system GMM estimator as a robustness check for its main findings.\n\n### Data / Model Specification\n\nConsider the canonical dynamic panel model:\n\n  \ny_{it} = \\beta' x_{it} + \\mu_i + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nTo address endogeneity, the system GMM estimator uses lagged levels of variables as instruments for the first-differenced equation and lagged differences as instruments for the levels equation.\n\n**Table 1: GMM Diagnostic Statistics (from paper's Table 4, Column 6)**\n| Test | Null Hypothesis | Reported p-value |\n| :--- | :--- | :---: |\n| AR(2) Test | No second-order serial correlation in differenced residuals | Insignificant* |\n| Hansen J-Test | Instruments are jointly valid (exogenous) | 0.224 |\n*Notes: The paper states the AR(2) test is insignificant without reporting the exact p-value.*\n\n### Question\n\nThe authors use a two-step system GMM estimator as a robustness check. Based on the diagnostic statistics provided in **Table 1**, select all of the following statements that represent a correct interpretation of the tests and their implications.", "model_evaluation_result": {"model_raw_response": "\n\nA,C\n\n## Explanation\n\nThe provided answer, \"A,C\", is derived from evaluating each option against the properties of dynamic panel GMM estimators, particularly the system GMM approach, and the diagnostic statistics in Table 1.\n\nFirst, consider the AR(2) test, which examines second-order serial correlation in differenced residuals. Option A correctly interprets the insignificant result as supporting the assumption of no serial correlation in the original idiosyncratic errors, a necessary condition for the validity of instruments lagged two or more periods, as these instruments rely on errors being serially uncorrelated.\n\nOption B is incorrect because the Hansen J-test assesses the joint exogeneity of all instruments, not the exogeneity of specific regressors like the corporate governance index (CGI). A p-value of 0.224 does not provide strong evidence for CGI's exogeneity alone.\n\nOption C is correct, as the Hansen J-test's p-value of 0.224 (greater than typical significance levels like 0.05 or 0.10) fails to reject the null hypothesis of joint instrument validity, thereby supporting the overall model specification.\n\nOption D is incorrect because a significant AR(2) test (p < 0.05) would reject\n> 🔍 **Analyzing financial question**\n> ***- The task involves selecting correct interpretations of GMM diagnostic tests from given options.***\n the null of no second-order serial correlation, invalidating the instruments; thus, insignificance is required for confirmation, not significance.\n\nThis step-by-step evaluation confirms options A and C as the accurate interpretations.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 350, "Question": "### Background\n\n**Research Question.** This case examines the empirical evidence for traditional fiscal theories of inflation, which posit that inflation is driven by government financing needs. The \"efficient government\" or \"tax-smoothing\" model predicts that all tax instruments, including the inflation tax, should be used in tandem to finance government spending, implying specific correlations between inflation and fiscal variables.\n\n**Variables & Parameters.**\n- `π`: Annual inflation rate.\n- `Budget surplus/GDP`: Ratio of government budget surplus to GDP.\n- `Expenditure/GDP`: Ratio of government expenditure to GDP.\n- `Revenue/GDP`: Ratio of government revenue to GDP.\n\n---\n\n### Data / Model Specification\n\nThe efficient government model predicts:\n1.  A positive correlation between `π` and `Expenditure/GDP`.\n2.  A positive correlation between `π` and `Revenue/GDP`.\n3.  A negative correlation between `π` and `Budget surplus/GDP`.\n\n**Table 1. Time-Series Correlations: Annual `π` with Fiscal Variables (Excerpt)**\n\n| Country | (a) Budget surplus/GDP | (b) Expenditure/GDP | (c) Revenue/GDP |\n| :------ | :--------------------- | :------------------ | :-------------- |\n| Mexico  | -0.879                 | 0.880               | 0.736           |\n| Ghana   | -0.060                 | -0.621              | -0.763          |\n\n**Table 2. Cross-Country Correlation Matrix (using sample averages)**\n\n|                     | `π`    | Expenditure/GDP | Revenue/GDP |\n| :------------------ | :----- | :-------------- | :---------- |\n| `π`                 | 1.0    | -0.019          | -0.210      |\n| Expenditure/GDP     | -0.019 | 1.0             | 0.929       |\n| Revenue/GDP         | -0.210 | 0.929           | 1.0         |\n\n---\n\n### Question\n\nAccording to the provided tables and the paper's analysis, which of the following statements accurately describe the empirical evidence regarding traditional fiscal theories of inflation?", "Options": {"A": "The time-series data present a mixed picture: Mexico's fiscal correlations are consistent with the efficient government model's predictions, whereas Ghana's are not.", "B": "The paper concludes that the time-series evidence, exemplified by countries like Mexico, provides universal support for the tax-smoothing model of public finance.", "C": "The cross-country data in Table 2 show a strong positive correlation between inflation and government expenditure, supporting the idea that high-spending countries inflate more.", "D": "The cross-country data in Table 2 contradict the efficient government model's key predictions, showing a near-zero correlation between inflation and expenditure and a negative correlation between inflation and revenue."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize evidence from both time-series and cross-sectional data to evaluate a theory. It uses an Atomic Decomposition strategy, with correct options summarizing the findings from each data source. Distractors are designed to trap common errors: a direct misreading of the data (C) and an overgeneralization from a single case (D).", "qid": "350", "question": "### Background\n\n**Research Question.** This case examines the empirical evidence for traditional fiscal theories of inflation, which posit that inflation is driven by government financing needs. The \"efficient government\" or \"tax-smoothing\" model predicts that all tax instruments, including the inflation tax, should be used in tandem to finance government spending, implying specific correlations between inflation and fiscal variables.\n\n**Variables & Parameters.**\n- `π`: Annual inflation rate.\n- `Budget surplus/GDP`: Ratio of government budget surplus to GDP.\n- `Expenditure/GDP`: Ratio of government expenditure to GDP.\n- `Revenue/GDP`: Ratio of government revenue to GDP.\n\n---\n\n### Data / Model Specification\n\nThe efficient government model predicts:\n1.  A positive correlation between `π` and `Expenditure/GDP`.\n2.  A positive correlation between `π` and `Revenue/GDP`.\n3.  A negative correlation between `π` and `Budget surplus/GDP`.\n\n**Table 1. Time-Series Correlations: Annual `π` with Fiscal Variables (Excerpt)**\n\n| Country | (a) Budget surplus/GDP | (b) Expenditure/GDP | (c) Revenue/GDP |\n| :------ | :--------------------- | :------------------ | :-------------- |\n| Mexico  | -0.879                 | 0.880               | 0.736           |\n| Ghana   | -0.060                 | -0.621              | -0.763          |\n\n**Table 2. Cross-Country Correlation Matrix (using sample averages)**\n\n|                     | `π`    | Expenditure/GDP | Revenue/GDP |\n| :------------------ | :----- | :-------------- | :---------- |\n| `π`                 | 1.0    | -0.019          | -0.210      |\n| Expenditure/GDP     | -0.019 | 1.0             | 0.929       |\n| Revenue/GDP         | -0.210 | 0.929           | 1.0         |\n\n---\n\n### Question\n\nAccording to the provided tables and the paper's analysis, which of the following statements accurately describe the empirical evidence regarding traditional fiscal theories of inflation?", "model_evaluation_result": {"model_raw_response": "\"A,D\"", "model_answer": ["A", "D"], "error": null}}
{"ID": 348, "Question": "### Background\n\n**Research Question.** This case investigates the paper's central empirical claim: that a country's underlying \"financial structure\"—specifically, the sensitivity of money demand to inflation—is a more powerful determinant of its average inflation rate than traditional fiscal variables. The analysis traces the proposed causal chain from estimated money demand parameters to a country's theoretical inflation-tax capacity, and finally to its observed inflation outcomes.\n\n**Variables & Parameters.**\n- `α`: The estimated semi-elasticity of currency demand with respect to inflation. A higher `α` means demand is more sensitive to inflation.\n- `π*`: The estimated revenue-maximizing inflation rate, a measure of a country's inflation-tax capacity.\n- `π`: The observed average inflation rate.\n- `ln(·)`: The natural logarithm transformation of a variable.\n\n---\n\n### Data / Model Specification\n\nThe paper's central hypothesis is a causal chain: low `α` (inelastic demand) ⇒ high `π*` (high tax capacity) ⇒ high observed average inflation `π`. This chain is tested by examining correlations between these variables.\n\n**Table 1. Correlation Coefficients (in Natural Logarithms)**\n\n|                     | `ln(α)` | `ln(π*)` | `ln(π)` |\n| :------------------ | :---------- | :--------- | :------------------- |\n| `ln(α)`             | 1.0         |            |                      |\n| `ln(π*)`            | -0.580      | 1.0        |                      |\n| `ln(π)`             | -0.802      | 0.816      | 1.0                  |\n\n---\n\n### Question\n\nBased on the provided data and the paper's financial structure hypothesis, select all statements that are supported by the evidence in **Table 1**.", "Options": {"A": "The data indicate that a higher sensitivity of currency demand to inflation (higher `α`) is associated with a greater capacity to levy the inflation tax (higher `π*`).", "B": "The financial structure hypothesis is primarily supported by the strong positive correlation between the semi-elasticity of currency demand (`ln(α)`) and the observed inflation rate (`ln(π)`).", "C": "The strong positive correlation (0.816) between `ln(π*)` and `ln(π)` suggests that countries with a higher inflation-tax capacity systematically exhibit higher average inflation rates in practice.", "D": "The strong negative correlation (-0.580) between `ln(α)` and `ln(π*)` indicates that countries where currency demand is less sensitive to inflation (lower `α`) have a greater theoretical capacity to raise revenue through inflation (higher `π*`)."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the user's ability to interpret the paper's core empirical evidence from correlation coefficients. It uses an Atomic Decomposition strategy, breaking the paper's central causal argument into two independently verifiable statements. Distractors are based on common interpretation errors: a Sign Error (C) and a Conceptual Opposite (D), ensuring high discriminability.", "qid": "348", "question": "### Background\n\n**Research Question.** This case investigates the paper's central empirical claim: that a country's underlying \"financial structure\"—specifically, the sensitivity of money demand to inflation—is a more powerful determinant of its average inflation rate than traditional fiscal variables. The analysis traces the proposed causal chain from estimated money demand parameters to a country's theoretical inflation-tax capacity, and finally to its observed inflation outcomes.\n\n**Variables & Parameters.**\n- `α`: The estimated semi-elasticity of currency demand with respect to inflation. A higher `α` means demand is more sensitive to inflation.\n- `π*`: The estimated revenue-maximizing inflation rate, a measure of a country's inflation-tax capacity.\n- `π`: The observed average inflation rate.\n- `ln(·)`: The natural logarithm transformation of a variable.\n\n---\n\n### Data / Model Specification\n\nThe paper's central hypothesis is a causal chain: low `α` (inelastic demand) ⇒ high `π*` (high tax capacity) ⇒ high observed average inflation `π`. This chain is tested by examining correlations between these variables.\n\n**Table 1. Correlation Coefficients (in Natural Logarithms)**\n\n|                     | `ln(α)` | `ln(π*)` | `ln(π)` |\n| :------------------ | :---------- | :--------- | :------------------- |\n| `ln(α)`             | 1.0         |            |                      |\n| `ln(π*)`            | -0.580      | 1.0        |                      |\n| `ln(π)`             | -0.802      | 0.816      | 1.0                  |\n\n---\n\n### Question\n\nBased on the provided data and the paper's financial structure hypothesis, select all statements that are supported by the evidence in **Table 1**.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 351, "Question": "### Background\nAn international study investigates how private anti-self-dealing regulations affect firm value, and whether this effect is conditional on a firm's ownership structure. The analysis is based on OLS regression results from a sample of 4,634 international firms.\n\n### Data / Model Specification\nThe estimated OLS model is:\n\n  \n\\widehat{\\text{Tobin's Q}} = \\hat{\\alpha} + \\hat{\\beta}_{1}\\mathrm{WEDGE} + \\hat{\\beta}_{2}\\mathrm{PRIVATE} + \\hat{\\beta}_{3}(\\mathrm{WEDGE} * \\mathrm{PRIVATE}) + \\dots \\quad \\text{(Eq. 1)}\n \n\nWhere `Tobin's Q` is a proxy for corporate valuation, `PRIVATE` is a country-level index for the strictness of private control of self-dealing, and `WEDGE` is the firm-level ownership wedge (the difference between control rights and cash-flow ownership of the largest shareholder, in percent).\n\n**Table 1: Key Regression Results**\n\n| Variable                | Coefficient | t-statistic |\n| :---------------------- | :---------- | :---------- |\n| `PRIVATE` (`β₂`)        | 0.64        | 3.80        |\n| `PRIVATE*WEDGE` (`β₃`)  | -0.02       | -2.07       |\n\n*Source: Table 5, Model 3 of the source paper.*\n\n---\n\nBased on the provided model and results, which of the following statements are valid interpretations or calculations? Select all that apply.", "Options": {"A": "The results suggest that as a controlling shareholder's incentive to expropriate wealth increases, the effectiveness of shareholder-led protective regulations diminishes.", "B": "The positive valuation effect of private control regulations is completely neutralized when the ownership wedge reaches 32%.", "C": "The marginal effect of stricter private regulation on firm value is positive for a firm with an ownership wedge of 10%.", "D": "For a firm with a zero ownership wedge, a one-unit increase in the `PRIVATE` index is associated with a 0.62 increase in Tobin's Q."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret an interaction model and perform multi-step calculations based on it. It uses a Computational Judgment strategy. Option A requires calculating the marginal effect at a specific wedge value. Option B requires solving for the wedge that makes the marginal effect zero. Option D tests the conceptual interpretation of the negative interaction term. Option C is a numerical distractor (Step-Omission Error), as it incorrectly subtracts the interaction coefficient from the main effect (0.64 - 0.02 = 0.62) instead of interpreting the main effect directly for WEDGE=0.", "qid": "351", "question": "### Background\nAn international study investigates how private anti-self-dealing regulations affect firm value, and whether this effect is conditional on a firm's ownership structure. The analysis is based on OLS regression results from a sample of 4,634 international firms.\n\n### Data / Model Specification\nThe estimated OLS model is:\n\n  \n\\widehat{\\text{Tobin's Q}} = \\hat{\\alpha} + \\hat{\\beta}_{1}\\mathrm{WEDGE} + \\hat{\\beta}_{2}\\mathrm{PRIVATE} + \\hat{\\beta}_{3}(\\mathrm{WEDGE} * \\mathrm{PRIVATE}) + \\dots \\quad \\text{(Eq. 1)}\n \n\nWhere `Tobin's Q` is a proxy for corporate valuation, `PRIVATE` is a country-level index for the strictness of private control of self-dealing, and `WEDGE` is the firm-level ownership wedge (the difference between control rights and cash-flow ownership of the largest shareholder, in percent).\n\n**Table 1: Key Regression Results**\n\n| Variable                | Coefficient | t-statistic |\n| :---------------------- | :---------- | :---------- |\n| `PRIVATE` (`β₂`)        | 0.64        | 3.80        |\n| `PRIVATE*WEDGE` (`β₃`)  | -0.02       | -2.07       |\n\n*Source: Table 5, Model 3 of the source paper.*\n\n---\n\nBased on the provided model and results, which of the following statements are valid interpretations or calculations? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 352, "Question": "### Background\nA study investigates the counterintuitive hypothesis that stricter public (i.e., government-enforced) regulations against self-dealing may be associated with lower corporate valuations. The analysis uses a sample of 4,634 firms from 22 countries.\n\n### Data / Model Specification\nThe hypothesis is tested using the following OLS regression model:\n\n  \n\\text{Tobin's Q} = \\alpha + \\dots + \\beta_{2}\\mathrm{PUBLIC} + \\dots + \\text{Controls} + \\varepsilon \\quad \\text{(Eq. 1)}\n \n\nWhere `Tobin's Q` is a proxy for corporate valuation and `PUBLIC` is a country-level index measuring the strictness of public control of self-dealing (e.g., fines, prison terms).\n\n**Table 1: Regression Results for Public Control**\n\n| Variable | Coefficient (Model 3) | t-statistic |\n| :------- | :-------------------- | :---------- |\n| `PUBLIC` | -0.42                 | -3.60       |\n\n*Source: Table 6, Model 3 of the source paper.*\n\n---\n\nGiven the significant negative coefficient on `PUBLIC`, which of the following are plausible economic rationales for why investors might perceive stricter government-enforced anti-self-dealing regulations as value-destroying? Select all that apply.", "Options": {"A": "Stricter public regulations empower minority shareholders to file lawsuits, increasing litigation risk and associated legal costs for the firm.", "B": "Empowering government agencies with the ability to levy fines or initiate proceedings creates opportunities for corruption or political extraction by officials.", "C": "Government agencies may lack industry-specific expertise, leading to inefficient, overly broad rules that stifle legitimate, value-creating transactions.", "D": "Public control regulations are a strong complement to an already efficient judicial system, creating redundant layers of costly oversight."}, "Answer": ["B", "C"], "pi_justification": "This item uses a Reverse-Reasoning strategy. Given the empirical result (a negative effect of public control), the candidate must identify plausible underlying causes. Options B and C represent two distinct, valid economic arguments for this negative effect: bureaucratic inefficiency/compliance costs and the risk of political extraction. Option A is a distractor (Almost Right) that confuses public control (government enforcement) with private control (shareholder lawsuits). Option D is a Conceptual Opposite; in a country with an efficient judiciary, public control is more likely a costly *substitute*, not a complement, making its net effect negative.", "qid": "352", "question": "### Background\nA study investigates the counterintuitive hypothesis that stricter public (i.e., government-enforced) regulations against self-dealing may be associated with lower corporate valuations. The analysis uses a sample of 4,634 firms from 22 countries.\n\n### Data / Model Specification\nThe hypothesis is tested using the following OLS regression model:\n\n  \n\\text{Tobin's Q} = \\alpha + \\dots + \\beta_{2}\\mathrm{PUBLIC} + \\dots + \\text{Controls} + \\varepsilon \\quad \\text{(Eq. 1)}\n \n\nWhere `Tobin's Q` is a proxy for corporate valuation and `PUBLIC` is a country-level index measuring the strictness of public control of self-dealing (e.g., fines, prison terms).\n\n**Table 1: Regression Results for Public Control**\n\n| Variable | Coefficient (Model 3) | t-statistic |\n| :------- | :-------------------- | :---------- |\n| `PUBLIC` | -0.42                 | -3.60       |\n\n*Source: Table 6, Model 3 of the source paper.*\n\n---\n\nGiven the significant negative coefficient on `PUBLIC`, which of the following are plausible economic rationales for why investors might perceive stricter government-enforced anti-self-dealing regulations as value-destroying? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\"A,B,C\"", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 359, "Question": "### Background\n\n**Research Question.** How robust are event study findings to the specification of the model for expected returns?\n\n**Setting and Sample.** An event study using daily stock returns for commercial banks. The study uses a single-factor market model to estimate expected returns but acknowledges that a two-factor model including an interest rate factor is a common alternative for bank stocks, which are known to be sensitive to interest rate movements.\n\n### Data / Model Specification\n\nThe study employs a single-factor market model to define abnormal returns (`AR`):\n\n  \nR_{i,t} - R_{f,t} = \\alpha_i + \\beta_i (R_{m,t} - R_{f,t}) + \\epsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nAn alternative, potentially more accurate, two-factor model for bank stocks would include an interest rate factor, `Δr_t`:\n\n  \nR_{i,t} - R_{f,t} = \\alpha_i^* + \\beta_i^* (R_{m,t} - R_{f,t}) + \\delta_i \\Delta r_t + \\nu_{i,t} \\quad \\text{(Eq. (2))}\n \n\nIf the single-factor model (**Eq. (1)**) is used when the true process is the two-factor model (**Eq. (2)**), the calculated abnormal return may be biased.\n\n---\n\n### Question\n\nRegarding the potential misspecification of the single-factor model used in the study, select all correct statements.", "Options": {"A": "A rejection of the null hypothesis in a GMM J-test of the single-factor model's overidentifying restrictions would indicate that the model is misspecified, thus challenging the validity of the abnormal returns calculated from it.", "B": "A GMM J-test is used to confirm that the model's parameters (`α_i`, `β_i`) are statistically significant.", "C": "The omission of an interest rate factor would cause the `CAR` to be understated, regardless of the direction of interest rate movements on the event day.", "D": "If bank stocks have a positive sensitivity to interest rates (`δ_i > 0`), and an unexpected drop in interest rates occurred during an event window, the reported positive `CAR` would likely be an overestimation of the true effect of the deregulation announcement."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of omitted variable bias in the context of event study methodology and knowledge of advanced specification tests. It uses the 'Premise/Assumption Packaging' strategy. Option A requires applying the logic of OVB to a specific scenario to determine the direction of bias. Option B tests the correct interpretation of a GMM J-test as a tool for assessing model validity. Distractor C is a 'Conceptual Opposite' that incorrectly specifies the bias direction. Distractor D is 'Almost Right' but confuses a specification test (J-test) with a test of parameter significance (t-test).", "qid": "359", "question": "### Background\n\n**Research Question.** How robust are event study findings to the specification of the model for expected returns?\n\n**Setting and Sample.** An event study using daily stock returns for commercial banks. The study uses a single-factor market model to estimate expected returns but acknowledges that a two-factor model including an interest rate factor is a common alternative for bank stocks, which are known to be sensitive to interest rate movements.\n\n### Data / Model Specification\n\nThe study employs a single-factor market model to define abnormal returns (`AR`):\n\n  \nR_{i,t} - R_{f,t} = \\alpha_i + \\beta_i (R_{m,t} - R_{f,t}) + \\epsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nAn alternative, potentially more accurate, two-factor model for bank stocks would include an interest rate factor, `Δr_t`:\n\n  \nR_{i,t} - R_{f,t} = \\alpha_i^* + \\beta_i^* (R_{m,t} - R_{f,t}) + \\delta_i \\Delta r_t + \\nu_{i,t} \\quad \\text{(Eq. (2))}\n \n\nIf the single-factor model (**Eq. (1)**) is used when the true process is the two-factor model (**Eq. (2)**), the calculated abnormal return may be biased.\n\n---\n\n### Question\n\nRegarding the potential misspecification of the single-factor model used in the study, select all correct statements.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 144, "Question": "### Background\n\nA study of Italian savings banks (1982-1989) used a translog cost function with a time trend (`T`) to measure the rate of technological change (`TECH`) and its effect on economies of scale (`Δξ`). The paper notes that savings banks were technological leaders, introducing ATMs and computer systems during this period.\n\n### Data / Model Specification\n\nThe rate of technological change is the partial derivative of log cost with respect to time:\n  \nTECH = \\frac{\\partial \\mathrm{Ln}(TC)}{\\partial T} \\quad \\text{(Eq. (1))}\n \nA negative value indicates cost savings over time.\n\nThe effect of technological change on economies of scale is the cross-partial derivative:\n  \n\\Delta\\xi = \\frac{\\partial^2 \\mathrm{Ln}(TC)}{\\partial \\mathrm{Ln}(Y_1) \\partial T} = \\zeta_5 \\quad \\text{(Eq. (2))}\n \nwhere `ζ₅` is the coefficient on the interaction term `T * Ln(Y₁)`. A negative `Δξ` means technology increases scale economies.\n\nThe paper reports the following estimates:\n- `TECH` = -0.0293 (statistically significant)\n- `Δξ` = `ζ₅` = -0.0003 (statistically insignificant)\n\n---\n\nWhich of the following statements are **INCORRECT** interpretations or critiques of these findings?", "Options": {"A": "The significant `TECH` estimate of -0.0293 suggests that, holding output and input prices constant, bank costs were increasing by approximately 2.93% per year.", "B": "A statistically significant, negative value for `Δξ` (i.e., `ζ₅` < 0) would have implied that technological change made economies of scale more pronounced over time.", "C": "The insignificant `Δξ` estimate suggests that the technological progress observed was neutral with respect to scale, meaning it did not disproportionately benefit larger or smaller banks.", "D": "If unmodeled financial deregulation during the 1980s also forced banks to cut costs, the `TECH` estimate of -0.0293 likely understates the true rate of cost savings from purely technological innovation."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item assesses the ability to correctly interpret regression coefficients for technological change, understand the concept of scale-biased innovation, and critically evaluate the identification strategy of using a time trend. It requires identifying flawed reasoning.\nStrategy: Single-Correct-Answer Inversion Protocol. The core concepts (interpretation of TECH, confounding variables, scale-bias) each have one correct interpretation. To create a multiple-response item, the stem is inverted to ask for incorrect statements. This forces the user to evaluate the validity of each option against the correct interpretation.\nDistractor Logic:\n- A (Correct, as in 'Incorrect Statement'): This is a Sign Error. A negative `TECH` value means costs were *decreasing* by 2.93% per year.\n- B (Incorrect, as in 'Correct Statement'): This is a valid interpretation. An insignificant `Δξ` means there is no statistical evidence that the effect of technology on marginal cost depends on the scale of output (`Y₁`).\n- C (Correct, as in 'Incorrect Statement'): This is a Conceptual Opposite. If deregulation also pushed costs down, the time trend `T` captures both effects. Therefore, the `TECH` estimate of -0.0293 *overstates* the effect of pure technology by confounding it with the effect of deregulation.\n- D (Incorrect, as in 'Correct Statement'): This is a valid interpretation. `Δξ` is `∂ξ/∂T`. If `Δξ` < 0, it means that as time (`T`) increases, the cost elasticity of output (`ξ`) decreases, which means economies of scale (1-`ξ`) become larger or more pronounced.", "qid": "144", "question": "### Background\n\nA study of Italian savings banks (1982-1989) used a translog cost function with a time trend (`T`) to measure the rate of technological change (`TECH`) and its effect on economies of scale (`Δξ`). The paper notes that savings banks were technological leaders, introducing ATMs and computer systems during this period.\n\n### Data / Model Specification\n\nThe rate of technological change is the partial derivative of log cost with respect to time:\n  \nTECH = \\frac{\\partial \\mathrm{Ln}(TC)}{\\partial T} \\quad \\text{(Eq. (1))}\n \nA negative value indicates cost savings over time.\n\nThe effect of technological change on economies of scale is the cross-partial derivative:\n  \n\\Delta\\xi = \\frac{\\partial^2 \\mathrm{Ln}(TC)}{\\partial \\mathrm{Ln}(Y_1) \\partial T} = \\zeta_5 \\quad \\text{(Eq. (2))}\n \nwhere `ζ₅` is the coefficient on the interaction term `T * Ln(Y₁)`. A negative `Δξ` means technology increases scale economies.\n\nThe paper reports the following estimates:\n- `TECH` = -0.0293 (statistically significant)\n- `Δξ` = `ζ₅` = -0.0003 (statistically insignificant)\n\n---\n\nWhich of the following statements are **INCORRECT** interpretations or critiques of these findings?", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 163, "Question": "### Background\n\n**Research Question.** How can the empirical superiority of a new volatility model be rigorously established? This involves assessing its performance against strong benchmarks using both statistical loss functions and practical, economic criteria.\n\n**Setting.** A comparative evaluation of the proposed Fractionally Integrated Generalized Autoregressive Score (FIGAS) model against the short-memory GAS model and the HEAVY model. The HEAVY model assumes a Wishart distribution for realized covariances, which is not robust to outliers, whereas the FIGAS and GAS models use a fat-tailed matrix-F distribution.\n\n**Key Concepts.**\n- **Economic Evaluation:** A Global Minimum Variance Portfolio (GMVP) strategy is used to assess the economic value of covariance forecasts. A superior model should produce a portfolio with lower ex-post realized volatility and more desirable characteristics (e.g., lower turnover, less concentration, fewer short positions).\n\n---\n\n### Data / Model Specification\n\n**Table 1: GMVP Performance Statistics (`k=15` assets, 1-step ahead)**\n| Model | Ex-post Std Dev | Concentration (CO) | Short Positions (SP) |\n| :--- | :--- | :--- | :--- |\n| FIGAS | 0.688 | 0.483 | -0.173 |\n| GAS | 0.689 | 0.486 | -0.175 |\n| HEAVY | 0.737 | 0.578 | -0.337 |\n\n---\n\n### Question\n\nBased on the provided background and data, select all statements that accurately describe the link between model specification and economic performance.", "Options": {"A": "The less robust forecasts from the HEAVY model result in more extreme portfolio allocations, evidenced by both higher portfolio concentration and substantially larger short positions compared to the FIGAS model.", "B": "The primary economic advantage of FIGAS over GAS is a large reduction in short positions (SP), as seen in Table 1.", "C": "The HEAVY model produces portfolios with lower concentration (CO) than the FIGAS model, indicating superior diversification.", "D": "The HEAVY model's lack of a fat-tailed distributional assumption contributes to its statistically inferior forecasts, which in turn leads to portfolios with significantly higher realized risk (ex-post standard deviation)."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\n\nThis item uses a **Reverse-Reasoning** and **Atomic Decomposition** strategy to test the connection between a model's theoretical assumptions and its tangible economic outcomes. It isolates the second part of the synthesis question from the original QA.\n\n- **Correct Options (A, B):** These options correctly link the HEAVY model's thin-tailed Wishart assumption to its poor statistical performance and, consequently, its poor economic outcomes (higher risk, concentration, and shorting), as shown in Table 1.\n- **Distractor C (Numerical Error):** This option makes a claim that is directly contradicted by the data in Table 1 (0.578 > 0.483), testing for careful data extraction.\n- **Distractor D (True but Irrelevant / Misinterpretation):** This option misattributes the source of performance differences. While FIGAS is slightly better than GAS, the economic differences between them are marginal compared to the massive gap between the fat-tailed models (FIGAS/GAS) and the thin-tailed HEAVY model. This distracts by focusing on a minor comparison.", "qid": "163", "question": "### Background\n\n**Research Question.** How can the empirical superiority of a new volatility model be rigorously established? This involves assessing its performance against strong benchmarks using both statistical loss functions and practical, economic criteria.\n\n**Setting.** A comparative evaluation of the proposed Fractionally Integrated Generalized Autoregressive Score (FIGAS) model against the short-memory GAS model and the HEAVY model. The HEAVY model assumes a Wishart distribution for realized covariances, which is not robust to outliers, whereas the FIGAS and GAS models use a fat-tailed matrix-F distribution.\n\n**Key Concepts.**\n- **Economic Evaluation:** A Global Minimum Variance Portfolio (GMVP) strategy is used to assess the economic value of covariance forecasts. A superior model should produce a portfolio with lower ex-post realized volatility and more desirable characteristics (e.g., lower turnover, less concentration, fewer short positions).\n\n---\n\n### Data / Model Specification\n\n**Table 1: GMVP Performance Statistics (`k=15` assets, 1-step ahead)**\n| Model | Ex-post Std Dev | Concentration (CO) | Short Positions (SP) |\n| :--- | :--- | :--- | :--- |\n| FIGAS | 0.688 | 0.483 | -0.173 |\n| GAS | 0.689 | 0.486 | -0.175 |\n| HEAVY | 0.737 | 0.578 | -0.337 |\n\n---\n\n### Question\n\nBased on the provided background and data, select all statements that accurately describe the link between model specification and economic performance.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 162, "Question": "### Background\n\n**Research Question.** How can the empirical superiority of a new volatility model be rigorously established? This involves assessing its performance against strong benchmarks using both statistical loss functions and practical, economic criteria across different market regimes.\n\n**Setting.** A comparative evaluation of the proposed Fractionally Integrated Generalized Autoregressive Score (FIGAS) model against the short-memory GAS model. The evaluation uses daily realized kernel data for 15 U.S. equities.\n\n**Key Concepts.**\n- **Statistical Evaluation:** The Quasi-Likelihood (QLIK) loss function is used to measure the statistical fit of a forecast. A lower QLIK value indicates a better fit.\n- **Positive Definiteness:** For a covariance matrix `V_t` to be valid, it must be positive definite. The FIGAS(0,d,1) model guarantees this if its scalar parameters `B` and `d` satisfy one of two conditions. Case 1: `0 < B < 1` and `d - B ≥ 0`. Case 2: `ζ-1 < B < 0` and `(d - sqrt(2(2-d)))/2 ≤ B` for some constant `ζ`.\n\n---\n\n### Data / Model Specification\n\n**Table 1: In-Sample FIGAS Parameter Estimates**\n| Parameter | `k=5` assets | `k=15` assets |\n| :--- | :--- | :--- |\n| `d` | 0.663 | 0.653 |\n| `B` | -0.086 | 0.157 |\n\n**Table 2: Average Out-of-Sample QLIK Loss (`k=15` assets)**\n| Period | Model | 1-day ahead | 22-days ahead |\n| :--- | :--- | :--- | :--- |\n| **Crisis** | FIGAS | 27.65 | 32.71 |\n| (2007-2009) | GAS | 27.70 | 32.91 |\n| | *DM t-stat* | *(-1.1)* | *(-0.5)* |\n| **Non-Crisis**| FIGAS | 12.76 | 13.93 |\n| (calm) | GAS | 12.85 | 14.41 |\n| | *DM t-stat* | *(-6.3)* | *(-3.4)* |\n\n---\n\n### Question\n\nBased on the provided data, select all statements that are supported by the empirical evidence.", "Options": {"A": "During non-crisis periods, the FIGAS model's long-memory component provides a statistically significant forecasting advantage over the short-memory GAS model.", "B": "The positive definiteness conditions for the FIGAS model are violated for the `k=5` specification because the parameter `B` is negative, as shown in Table 1.", "C": "The long-memory component of the FIGAS model is most valuable during crisis periods, as shown by the large negative DM t-statistics in the top panel of Table 2.", "D": "During the crisis period, the performance of the FIGAS and GAS models is statistically indistinguishable, indicating that the robust score dynamics common to both are the primary driver of performance in volatile markets."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\n\nThis item assesses the ability to synthesize the paper's central empirical findings from multiple tables. It uses an **Atomic Decomposition** strategy to break down the complex synthesis question from the original QA into distinct, verifiable propositions.\n\n- **Correct Options (A, B):** These two options capture the core nuance of the paper's findings regarding state-dependent performance, directly supported by the QLIK values and DM t-statistics in Table 2.\n- **Distractor C (Conceptual Opposite):** This option presents the exact opposite conclusion from the data, testing for careful reading of the table which shows insignificant DM stats during the crisis.\n- **Distractor D (Formula Misuse):** This option tests the validation conditions. It presents a plausible but incorrect conclusion by ignoring 'Case 2' of the positive definiteness rule, which applies when B is negative. This maps to a specific comprehension slip.", "qid": "162", "question": "### Background\n\n**Research Question.** How can the empirical superiority of a new volatility model be rigorously established? This involves assessing its performance against strong benchmarks using both statistical loss functions and practical, economic criteria across different market regimes.\n\n**Setting.** A comparative evaluation of the proposed Fractionally Integrated Generalized Autoregressive Score (FIGAS) model against the short-memory GAS model. The evaluation uses daily realized kernel data for 15 U.S. equities.\n\n**Key Concepts.**\n- **Statistical Evaluation:** The Quasi-Likelihood (QLIK) loss function is used to measure the statistical fit of a forecast. A lower QLIK value indicates a better fit.\n- **Positive Definiteness:** For a covariance matrix `V_t` to be valid, it must be positive definite. The FIGAS(0,d,1) model guarantees this if its scalar parameters `B` and `d` satisfy one of two conditions. Case 1: `0 < B < 1` and `d - B ≥ 0`. Case 2: `ζ-1 < B < 0` and `(d - sqrt(2(2-d)))/2 ≤ B` for some constant `ζ`.\n\n---\n\n### Data / Model Specification\n\n**Table 1: In-Sample FIGAS Parameter Estimates**\n| Parameter | `k=5` assets | `k=15` assets |\n| :--- | :--- | :--- |\n| `d` | 0.663 | 0.653 |\n| `B` | -0.086 | 0.157 |\n\n**Table 2: Average Out-of-Sample QLIK Loss (`k=15` assets)**\n| Period | Model | 1-day ahead | 22-days ahead |\n| :--- | :--- | :--- | :--- |\n| **Crisis** | FIGAS | 27.65 | 32.71 |\n| (2007-2009) | GAS | 27.70 | 32.91 |\n| | *DM t-stat* | *(-1.1)* | *(-0.5)* |\n| **Non-Crisis**| FIGAS | 12.76 | 13.93 |\n| (calm) | GAS | 12.85 | 14.41 |\n| | *DM t-stat* | *(-6.3)* | *(-3.4)* |\n\n---\n\n### Question\n\nBased on the provided data, select all statements that are supported by the empirical evidence.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"Diebold-Mariano test t-statistic sign for forecasting superiority\"\n> 📊 **Results requested:** 5\n\n> 🔍 **5 results found**\n\n> [🔗 \"Tests of forecast accuracy and forecast encompassing - The Stata Blog\" The Stata Blog](https://blog.stata.com/2016/06/01/tests-of-forecast-accuracy-and-forecast-encompassing/)\n> [🔗 \"rms - Diebold-Mariano in the context of volatility forecasting: What is the ultimate aim of this test? - Cross Validated\" Stack Exchange](https://stats.stackexchange.com/questions/272792/diebold-mariano-in-the-context-of-volatility-forecasting-what-is-the-ultimate-a)\n> [🔗 \"Diebold-Mariano test for predictive accuracy — dm.test • forecast\" Robjhyndman](https://pkg.robjhyndman.com/forecast/reference/dm.test.html)\n> [🔗 \"Diebold-Mariano Test\" Maggima](https://maggima.github.io/pages/stats/tests/forecasts/dm.html)\n> [🔗 \"The Diebold-Mariano Test\" Sssup](https://www.lem.sssup.it/phd/documents/Lesson19.pdf)\n\n\nA,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 282, "Question": "### Background\n\nThe paper investigates why U.S. industries experienced starkly different competitive outcomes between 1960 and 1986, despite facing similar macroeconomic conditions. It challenges the theory that a high cost of capital was the primary cause of competitive decline, proposing instead that firm-specific organizational capabilities are the key differentiating factor.\n\n### Data / Model Specification\n\nConsider two competing stylized models for firm value (`V`):\n\n  \nV_1 = f(I, r) \\quad \\text{(Eq. 1)}\n \n\n  \nV_2 = g(I, r, k) \\quad \\text{with} \\quad \\frac{\\partial g}{\\partial k} > 0 \\quad \\text{and} \\quad \\frac{\\partial^2 g}{\\partial I \\partial k} > 0 \\quad \\text{(Eq. 2)}\n \n\nEq. (1) represents the view that value is driven by investment levels (`I`) and a common cost of capital (`r`). Eq. (2) introduces a firm-specific organizational capability (`k`) that is complementary with capital investment.\n\n**Table 1: Changes in “World Market Share” of U.S. Companies by Major Industry, 1960-1986**\n\n| Industry                             | Percentage Change |\n| :----------------------------------- | :---------------- |\n| Iron and Steel                       | -58%              |\n| Autos and Trucks                     | -33%              |\n| Paper and Paper Products             | -4%               |\n| Aerospace                            | +6%               |\n\n### Question\n\nBased on the paper's argument, the provided models, and the data in Table 1, select all of the following statements that represent valid conclusions or interpretations.", "Options": {"A": "The data for the Aerospace and Iron and Steel industries, which both have long-lived capital assets, provides strong evidence against the sufficiency of the model in Eq. (1) to explain competitive outcomes.", "B": "The significant market share loss in the Autos and Trucks industry is consistent with a failure to develop capabilities like 'internal integration' (speed and efficiency in product development), as described in the paper.", "C": "The model in Eq. (2) implies that firms with higher levels of organizational capability (`k`) will have a lower optimal level of investment (`I*`), as capabilities substitute for capital.", "D": "According to the model in Eq. (2), the marginal productivity of capital investment (`∂g/∂I`) is independent of the level of organizational capability (`k`)."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the student's ability to synthesize the paper's core empirical puzzle (Table 1) with its central theoretical argument (the importance of capabilities, formalized in Eq. 2). It uses a 'Scenario Application' strategy. Option A is correct because the divergent outcomes of two industries with similar capital horizons refutes the simple cost-of-capital theory (Eq. 1). Option C is correct as it applies the paper's specific example of 'internal integration' to a relevant industry from the table. Distractor B is a 'Conceptual Opposite' error; the complementarity in Eq. 2 (`∂²g/∂I∂k > 0`) means capability *increases* the optimal investment level. Distractor D is a 'Formula Misuse' error, directly contradicting the complementarity assumption in Eq. 2.", "qid": "282", "question": "### Background\n\nThe paper investigates why U.S. industries experienced starkly different competitive outcomes between 1960 and 1986, despite facing similar macroeconomic conditions. It challenges the theory that a high cost of capital was the primary cause of competitive decline, proposing instead that firm-specific organizational capabilities are the key differentiating factor.\n\n### Data / Model Specification\n\nConsider two competing stylized models for firm value (`V`):\n\n  \nV_1 = f(I, r) \\quad \\text{(Eq. 1)}\n \n\n  \nV_2 = g(I, r, k) \\quad \\text{with} \\quad \\frac{\\partial g}{\\partial k} > 0 \\quad \\text{and} \\quad \\frac{\\partial^2 g}{\\partial I \\partial k} > 0 \\quad \\text{(Eq. 2)}\n \n\nEq. (1) represents the view that value is driven by investment levels (`I`) and a common cost of capital (`r`). Eq. (2) introduces a firm-specific organizational capability (`k`) that is complementary with capital investment.\n\n**Table 1: Changes in “World Market Share” of U.S. Companies by Major Industry, 1960-1986**\n\n| Industry                             | Percentage Change |\n| :----------------------------------- | :---------------- |\n| Iron and Steel                       | -58%              |\n| Autos and Trucks                     | -33%              |\n| Paper and Paper Products             | -4%               |\n| Aerospace                            | +6%               |\n\n### Question\n\nBased on the paper's argument, the provided models, and the data in Table 1, select all of the following statements that represent valid conclusions or interpretations.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 170, "Question": "### Background\n\n**Research Question.** What are the dynamic effects of a shock to housing market tightness on market liquidity and prices, and do these empirical effects align with predictions from search and matching theory?\n\n**Setting.** The analysis uses a panel VAR model estimated on quarterly data for 403 Dutch municipalities from 2011-2013. The model examines the interrelationships between market tightness, liquidity, and prices.\n\n**Theoretical Prediction.** Search and matching models with information asymmetries predict that a positive shock to market tightness causes an immediate increase in liquidity (matching probability), followed by a more gradual increase in prices as sellers slowly update their reservation prices. This is because sellers' reservation prices are based on *perceived* market tightness (`λ*`), which adjusts slowly, while the probability of meeting a buyer depends on *true* market tightness (`λ`), which changes instantly.\n\n**Empirical Finding.** A shock to market tightness leads to a temporary increase in the rate of sale (`ros`) but a gradual and permanent increase in the price level (`pr`).\n\n### Question\n\nThe paper's empirical findings show a key difference in the dynamic response of liquidity and prices to a market tightness shock. According to the theoretical framework presented, which of the following mechanisms correctly explain this observed pattern? (Select all that apply)", "Options": {"A": "The immediate increase in liquidity is driven by the seller's meeting probability, which is a function of the true, contemporaneous market tightness (`λ`).", "B": "Prices adjust slowly because buyers, not sellers, are slow to update their expectations about market conditions.", "C": "The permanent effect on prices and temporary effect on liquidity are inconsistent with the predictions of search and matching theory.", "D": "The gradual price increase is caused by sellers' reservation prices being based on perceived market tightness (`λ*`), which adapts slowly to changes in true market tightness."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a **Reverse-Reasoning** strategy, asking for the theoretical causes of an observed empirical pattern. It tests the core theoretical contribution of the paper. Correct options (A, B) use **Atomic Decomposition** to separate the explanations for the fast liquidity response (tied to true tightness `λ`) and the slow price response (tied to perceived tightness `λ*`). Distractors are based on conceptual errors: (C) incorrectly assigns the information friction to buyers instead of sellers, and (D) claims the findings contradict the theory when they actually provide strong support for it.", "qid": "170", "question": "### Background\n\n**Research Question.** What are the dynamic effects of a shock to housing market tightness on market liquidity and prices, and do these empirical effects align with predictions from search and matching theory?\n\n**Setting.** The analysis uses a panel VAR model estimated on quarterly data for 403 Dutch municipalities from 2011-2013. The model examines the interrelationships between market tightness, liquidity, and prices.\n\n**Theoretical Prediction.** Search and matching models with information asymmetries predict that a positive shock to market tightness causes an immediate increase in liquidity (matching probability), followed by a more gradual increase in prices as sellers slowly update their reservation prices. This is because sellers' reservation prices are based on *perceived* market tightness (`λ*`), which adjusts slowly, while the probability of meeting a buyer depends on *true* market tightness (`λ`), which changes instantly.\n\n**Empirical Finding.** A shock to market tightness leads to a temporary increase in the rate of sale (`ros`) but a gradual and permanent increase in the price level (`pr`).\n\n### Question\n\nThe paper's empirical findings show a key difference in the dynamic response of liquidity and prices to a market tightness shock. According to the theoretical framework presented, which of the following mechanisms correctly explain this observed pattern? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "\n\nA,DA,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 369, "Question": "### Background\n\n**Research Question.** How does the choice of payment method in M&A deals, particularly the use of earnouts and stock consideration, serve as a mechanism to mitigate information asymmetry problems like adverse selection and moral hazard, especially in the context of cross-border acquisitions (CBAs)?\n\n**Setting.** M&A transactions are fraught with information asymmetry, where the target firm's managers have superior knowledge about its true value and prospects. Acquirers must design contracts to protect themselves from overpaying (adverse selection) and to ensure sellers remain motivated post-acquisition (moral hazard).\n\n**Variables and Concepts.**\n- **Adverse Selection:** The pre-contractual risk that a seller with private information will exploit this advantage, leading the buyer to purchase an overvalued asset (a 'lemon').\n- **Moral Hazard:** The post-contractual risk that one party will change their behavior (e.g., shirk) after the contract is signed, to the detriment of the other party.\n- **Earnout:** A contract where a portion of the payment is deferred and made conditional on the target's post-merger performance.\n- **CBA (Cross-Border Acquisition):** An acquisition where the target is domiciled in a different country than the acquirer, assumed to involve higher information asymmetry.\n\n---\n\n### The Question\n\nIn the context of M&A, an earnout's deferred, performance-contingent payment is a contractual mechanism designed to mitigate information asymmetry. Which of the following problems does this mechanism directly address? (Select all that apply)", "Options": {"A": "Moral Hazard, by incentivizing key personnel to remain and perform well post-acquisition to meet performance targets.", "B": "Adverse Selection, by forcing sellers who claim high future value to have a portion of their payment depend on achieving that value.", "C": "Acquirer's Liquidity Constraints, by allowing the acquirer to pay a significant portion of the deal price later using future cash flows.", "D": "Principal-Agent Problems within the Acquirer's Firm, by ensuring the acquiring managers do not overpay for empire-building purposes."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the foundational understanding of the dual role of an earnout in mitigating information asymmetry, directly testing concepts from Q1 and Q2 of the source QA. The distractors are designed to be high-fidelity: Option C is a true but irrelevant benefit of deferred payments, testing if the user can distinguish between financing and informational roles. Option D misapplies agency theory to the wrong party in the transaction.", "qid": "369", "question": "### Background\n\n**Research Question.** How does the choice of payment method in M&A deals, particularly the use of earnouts and stock consideration, serve as a mechanism to mitigate information asymmetry problems like adverse selection and moral hazard, especially in the context of cross-border acquisitions (CBAs)?\n\n**Setting.** M&A transactions are fraught with information asymmetry, where the target firm's managers have superior knowledge about its true value and prospects. Acquirers must design contracts to protect themselves from overpaying (adverse selection) and to ensure sellers remain motivated post-acquisition (moral hazard).\n\n**Variables and Concepts.**\n- **Adverse Selection:** The pre-contractual risk that a seller with private information will exploit this advantage, leading the buyer to purchase an overvalued asset (a 'lemon').\n- **Moral Hazard:** The post-contractual risk that one party will change their behavior (e.g., shirk) after the contract is signed, to the detriment of the other party.\n- **Earnout:** A contract where a portion of the payment is deferred and made conditional on the target's post-merger performance.\n- **CBA (Cross-Border Acquisition):** An acquisition where the target is domiciled in a different country than the acquirer, assumed to involve higher information asymmetry.\n\n---\n\n### The Question\n\nIn the context of M&A, an earnout's deferred, performance-contingent payment is a contractual mechanism designed to mitigate information asymmetry. Which of the following problems does this mechanism directly address? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 355, "Question": "### Background\n\nThe aggregate claims process in insurance, `X(t)`, starts at zero and increases with discrete jumps as claims occur. In contrast, stock prices are often modeled with geometric Brownian motion (GBM).\n\n### Question\n\nWhich of the following are valid reasons why geometric Brownian motion (GBM) is a fundamentally unsuitable model for the aggregate claims process `X(t)`? Select all that apply.", "Options": {"A": "The claims process starts at `X(0) = 0`, while a GBM models an asset with a positive starting price `S(0) > 0`.", "B": "A GBM process can decrease in value, whereas the aggregate claims process is non-decreasing.", "C": "The paths of a GBM are continuous, whereas the claims process consists of discrete jumps.", "D": "GBM assumes the volatility of the process is constant, which is unrealistic for claims."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: Understanding the fundamental properties of stochastic processes used in finance and insurance. Strategy: Conceptual Differentiation. Distractor Logic: (A), (B), and (D) are all correct and fundamental mismatches between the process properties. (C) is a distractor of the 'True but Irrelevant' type. While constant volatility might be an unrealistic assumption for some applications, it is not a *fundamental* reason GBM is unsuitable for claims; more advanced stochastic volatility models are still diffusion processes and share the other, more critical flaws (continuity, ability to decrease).", "qid": "355", "question": "### Background\n\nThe aggregate claims process in insurance, `X(t)`, starts at zero and increases with discrete jumps as claims occur. In contrast, stock prices are often modeled with geometric Brownian motion (GBM).\n\n### Question\n\nWhich of the following are valid reasons why geometric Brownian motion (GBM) is a fundamentally unsuitable model for the aggregate claims process `X(t)`? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 365, "Question": "### Background\n\n**Research Question.** Is the immediate market reaction to bad earnings news more severe for growth firms than for value firms (the \"earnings torpedo\" effect), and does this asymmetry depend on whether the news is routine or non-routine?\n\n**Setting and Sample.** The study examines the 3-day market-adjusted abnormal returns (`BH_3`) around management earnings forecast (MEF) announcements for Australian firms. Firms are categorized as 'Growth' or 'Value' based on book-to-market quintiles. MEFs are categorized as 'Good', 'Bad', or 'Neutral' news. Announcements are also classified as 'Routine' (e.g., at an Annual General Meeting) or 'Non-routine' (arising from continuous disclosure obligations).\n\n**Variables and Parameters.**\n- `BH_3`: 3-day market-adjusted buy-and-hold abnormal return.\n- `Growth`, `Value`: Dummy variables for firm classification. The omitted category is intermediate B/M firms.\n- `D_BadA`, `D_GoodA`: Dummy variables for news classification.\n- Control variables include `LSize` (log firm size), `NAN` (number of analysts), and `LMeanrba` (log bid-ask spread).\n- `β_k`: Regression coefficients.\n\n---\n\n### Data / Model Specification\n\nThe study tests for an asymmetric market reaction using a regression model with interaction terms. The omitted baseline category is an intermediate B/M firm issuing neutral news.\n\n  \n\\mathrm{BH}_{3} = \\alpha + \\beta_{1}(\\text{Growth} \\cdot D_{\\mathrm{BadA}}) + \\beta_{3}(\\text{Value} \\cdot D_{\\mathrm{BadA}}) + \\beta_{6}(\\text{Growth} \\cdot D_{\\mathrm{GoodA}}) + \\beta_{8}(\\text{Value} \\cdot D_{\\mathrm{GoodA}}) + \\dots + \\text{Controls} + \\varepsilon\n \n\nHypothesis H1A posits that the reaction to bad news is more negative for growth firms (`|β1| > |β3|`). Hypothesis H1B posits no difference in reaction to good news (`β6 = β8`).\n\n**Table 1: Regression of 3-Day Abnormal Returns (`BH_3`)**\n\n| Variable | Overall | Non-routine |\n| :--- | :--- | :--- |\n| `Growth * D_BadA` (β1) | -0.053*** | -0.074*** |\n| `Value * D_BadA` (β3) | -0.027*** | -0.023 |\n| `Growth * D_GoodA` (β6) | 0.013** | 0.018* |\n| `Value * D_GoodA` (β8) | 0.019** | 0.034* |\n| **Wald Test H1A: |β1|=|β3|** | p = 0.029 | p = 0.007 |\n| **Wald Test H1B: β6=β8** | p = 0.522 | p = 0.344 |\n\n*Controls included but not shown. *** p<0.01, ** p<0.05, * p<0.1. Source: Table 3 of the paper.* \n\n---\n\n### Question\n\nBased on the regression results presented in **Table 1**, which of the following conclusions about the immediate 3-day market reaction (`BH_3`) to management earnings forecasts are supported by the data?", "Options": {"A": "For good news announcements, there is no statistically significant difference in the market's reaction between growth and value firms in either the 'Overall' or 'Non-routine' sample.", "B": "When considering only 'Non-routine' announcements, the market's negative reaction to bad news is statistically significant for growth firms but not for value firms.", "C": "The asymmetric negative reaction to bad news, where growth firms are penalized more severely than value firms, is statistically significant in both the 'Overall' and 'Non-routine' samples.", "D": "The positive reaction to good news for value firms is statistically significant in the 'Overall' sample but not in the 'Non-routine' sample."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize and compare regression results across different subsamples ('Overall' vs. 'Non-routine'). It uses an Atomic Decomposition strategy, breaking down the paper's main findings on the 'earnings torpedo' effect into distinct, verifiable statements. The options require careful interpretation of coefficient estimates, significance levels from Wald tests, and individual coefficient significance from stars. Distractor D is a high-fidelity error based on misreading the significance levels across columns.", "qid": "365", "question": "### Background\n\n**Research Question.** Is the immediate market reaction to bad earnings news more severe for growth firms than for value firms (the \"earnings torpedo\" effect), and does this asymmetry depend on whether the news is routine or non-routine?\n\n**Setting and Sample.** The study examines the 3-day market-adjusted abnormal returns (`BH_3`) around management earnings forecast (MEF) announcements for Australian firms. Firms are categorized as 'Growth' or 'Value' based on book-to-market quintiles. MEFs are categorized as 'Good', 'Bad', or 'Neutral' news. Announcements are also classified as 'Routine' (e.g., at an Annual General Meeting) or 'Non-routine' (arising from continuous disclosure obligations).\n\n**Variables and Parameters.**\n- `BH_3`: 3-day market-adjusted buy-and-hold abnormal return.\n- `Growth`, `Value`: Dummy variables for firm classification. The omitted category is intermediate B/M firms.\n- `D_BadA`, `D_GoodA`: Dummy variables for news classification.\n- Control variables include `LSize` (log firm size), `NAN` (number of analysts), and `LMeanrba` (log bid-ask spread).\n- `β_k`: Regression coefficients.\n\n---\n\n### Data / Model Specification\n\nThe study tests for an asymmetric market reaction using a regression model with interaction terms. The omitted baseline category is an intermediate B/M firm issuing neutral news.\n\n  \n\\mathrm{BH}_{3} = \\alpha + \\beta_{1}(\\text{Growth} \\cdot D_{\\mathrm{BadA}}) + \\beta_{3}(\\text{Value} \\cdot D_{\\mathrm{BadA}}) + \\beta_{6}(\\text{Growth} \\cdot D_{\\mathrm{GoodA}}) + \\beta_{8}(\\text{Value} \\cdot D_{\\mathrm{GoodA}}) + \\dots + \\text{Controls} + \\varepsilon\n \n\nHypothesis H1A posits that the reaction to bad news is more negative for growth firms (`|β1| > |β3|`). Hypothesis H1B posits no difference in reaction to good news (`β6 = β8`).\n\n**Table 1: Regression of 3-Day Abnormal Returns (`BH_3`)**\n\n| Variable | Overall | Non-routine |\n| :--- | :--- | :--- |\n| `Growth * D_BadA` (β1) | -0.053*** | -0.074*** |\n| `Value * D_BadA` (β3) | -0.027*** | -0.023 |\n| `Growth * D_GoodA` (β6) | 0.013** | 0.018* |\n| `Value * D_GoodA` (β8) | 0.019** | 0.034* |\n| **Wald Test H1A: |β1|=|β3|** | p = 0.029 | p = 0.007 |\n| **Wald Test H1B: β6=β8** | p = 0.522 | p = 0.344 |\n\n*Controls included but not shown. *** p<0.01, ** p<0.05, * p<0.1. Source: Table 3 of the paper.* \n\n---\n\n### Question\n\nBased on the regression results presented in **Table 1**, which of the following conclusions about the immediate 3-day market reaction (`BH_3`) to management earnings forecasts are supported by the data?", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 344, "Question": "### Background\n\n**Research Question.** This case assesses the economic significance of return predictability by evaluating the performance of trading strategies based on moving average signals.\n\n**Setting and Sample.** The out-of-sample performance of dynamic trading strategies is evaluated for a mean-variance investor. Performance is measured by the Certainty Equivalent Return (CER) gain relative to a benchmark.\n\n### Data / Model Specification\n\nThe Certainty Equivalent Return (CER) for a portfolio `p` is the risk-free return that provides an investor with the same utility as the risky portfolio:\n  \nCER_p = \\hat{\\mu}_p - \\frac{\\gamma}{2} \\hat{\\sigma}_p^2 \\quad \\text{(Eq. 1)}\n \nwhere `\\hat{\\mu}_p` is the average return, `\\hat{\\sigma}_p^2` is the return variance, and `γ` is the risk aversion coefficient. The economic value of a model is measured by the CER gain, `Δ(cer) = CER_{model} - CER_{benchmark}`.\n\n**Table 1: Economic Evaluation of Predictability (Selected Results)**\n\n| Strategy | `Δ(cer)` (%) (No cost) | `Δ(cer)` (%) (50bps cost) |\n|:---|---:|---:|\n| MADP(3) | 1.64 | 1.39 |\n| PC-long | 0.80 | 0.45 |\n\n*Note: `PC-long` is a strategy based on the first principal component of long-term (20-200 day) MADP predictors.*\n\n### Question\n\nBased on the definition of Certainty Equivalent Return (CER) and the results in Table 1, select all of the following statements that are correct.", "Options": {"A": "After accounting for transaction costs, the strategy based on the short-term `MADP(3)` signal delivers more than three times the economic value (`Δ(cer)`) of the strategy based on the long-term `PC-long` signal.", "B": "The CER gain, `Δ(cer)`, provides a more complete measure of economic value than average return alone because it incorporates a penalty for portfolio variance.", "C": "A positive `Δ(cer)` indicates that the strategy's average return is higher than the benchmark, regardless of its risk.", "D": "The `PC-long` strategy loses a smaller absolute amount of its CER gain (0.35%) to transaction costs than the `MADP(3)` strategy (0.25%), implying it involves less frequent trading."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses **Atomic Decomposition** to test the conceptual understanding of CER and the ability to interpret the table's results comparatively. Option A tests the definition. Option B requires a simple calculation and comparison. Distractor C presents a plausible but incorrect inference (a larger cost drag suggests *more* trading, not less). Distractor D represents a key conceptual error, ignoring the risk-adjustment inherent in CER.", "qid": "344", "question": "### Background\n\n**Research Question.** This case assesses the economic significance of return predictability by evaluating the performance of trading strategies based on moving average signals.\n\n**Setting and Sample.** The out-of-sample performance of dynamic trading strategies is evaluated for a mean-variance investor. Performance is measured by the Certainty Equivalent Return (CER) gain relative to a benchmark.\n\n### Data / Model Specification\n\nThe Certainty Equivalent Return (CER) for a portfolio `p` is the risk-free return that provides an investor with the same utility as the risky portfolio:\n  \nCER_p = \\hat{\\mu}_p - \\frac{\\gamma}{2} \\hat{\\sigma}_p^2 \\quad \\text{(Eq. 1)}\n \nwhere `\\hat{\\mu}_p` is the average return, `\\hat{\\sigma}_p^2` is the return variance, and `γ` is the risk aversion coefficient. The economic value of a model is measured by the CER gain, `Δ(cer) = CER_{model} - CER_{benchmark}`.\n\n**Table 1: Economic Evaluation of Predictability (Selected Results)**\n\n| Strategy | `Δ(cer)` (%) (No cost) | `Δ(cer)` (%) (50bps cost) |\n|:---|---:|---:|\n| MADP(3) | 1.64 | 1.39 |\n| PC-long | 0.80 | 0.45 |\n\n*Note: `PC-long` is a strategy based on the first principal component of long-term (20-200 day) MADP predictors.*\n\n### Question\n\nBased on the definition of Certainty Equivalent Return (CER) and the results in Table 1, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 357, "Question": "### Background\n\n**Research Question.** How can econometric methods address concerns about causality in the relationship between firm characteristics and stock returns following deregulation?\n\n**Setting and Sample.** The study uses a cross-sectional OLS regression to link commercial bank characteristics (like size and `SEC20` status) to abnormal returns. However, this approach is vulnerable to critiques of endogeneity and omitted variable bias, which could be addressed by more advanced methods like Instrumental Variables (IV) or panel data with Fixed Effects (FE).\n\n### Data / Model Specification\n\nThe paper estimates the following cross-sectional model via OLS:\n\n  \n\\mathrm{CAR}_{i} = \\beta_{0} + \\beta_{1}\\mathrm{Ln}(\\mathrm{SIZE}_{i}) + \\beta_{2}\\mathrm{SEC}20_{i} + \\text{controls}_i + e_{i} \\quad \\text{(Eq. (1))}\n \n\nConcerns exist that `SEC20_i` is endogenous (correlated with the error term `e_i`) and that time-invariant unobserved variables (e.g., managerial quality) bias the estimates.\n\n---\n\n### Question\n\nRegarding the potential endogeneity of `SEC20` status and the use of alternative estimation methods, select all correct statements.", "Options": {"A": "A valid instrumental variable for `SEC20` status must be correlated with a bank's choice to have a subsidiary but must not be independently correlated with the bank's abnormal returns from deregulation.", "B": "The main advantage of a firm fixed-effects model is its ability to provide unbiased estimates for time-invariant characteristics like `SEC20` status.", "C": "A primary limitation of using a firm fixed-effects panel model for this analysis is that it cannot estimate the coefficient on a time-invariant variable like `SEC20` status.", "D": "The exclusion restriction for an instrumental variable requires it to be highly correlated with the abnormal return `CAR_i`."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses knowledge of advanced causal inference techniques (IV and FE) as they apply to the paper's research design. It uses the 'Premise/Assumption Packaging' strategy, asking the user to identify correct statements about the conditions and limitations of these methods. Options A and B correctly state the exclusion restriction for IV and a key limitation of FE, respectively. Distractor C is a 'Conceptual Opposite' of option B. Distractor D is 'Almost Right' but confuses the relevance condition with the exclusion restriction for an IV.", "qid": "357", "question": "### Background\n\n**Research Question.** How can econometric methods address concerns about causality in the relationship between firm characteristics and stock returns following deregulation?\n\n**Setting and Sample.** The study uses a cross-sectional OLS regression to link commercial bank characteristics (like size and `SEC20` status) to abnormal returns. However, this approach is vulnerable to critiques of endogeneity and omitted variable bias, which could be addressed by more advanced methods like Instrumental Variables (IV) or panel data with Fixed Effects (FE).\n\n### Data / Model Specification\n\nThe paper estimates the following cross-sectional model via OLS:\n\n  \n\\mathrm{CAR}_{i} = \\beta_{0} + \\beta_{1}\\mathrm{Ln}(\\mathrm{SIZE}_{i}) + \\beta_{2}\\mathrm{SEC}20_{i} + \\text{controls}_i + e_{i} \\quad \\text{(Eq. (1))}\n \n\nConcerns exist that `SEC20_i` is endogenous (correlated with the error term `e_i`) and that time-invariant unobserved variables (e.g., managerial quality) bias the estimates.\n\n---\n\n### Question\n\nRegarding the potential endogeneity of `SEC20` status and the use of alternative estimation methods, select all correct statements.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 202, "Question": "### Background\n\nThis paper proposes that life insurance regulation should be viewed primarily as a mechanism for enforcing long-term, often incomplete, contracts. This approach is designed to solve specific market failures inherent to the life insurance industry, which differ significantly from those in banking.\n\n### Question\n\nAccording to the paper's analysis of the life insurance market, which of the following statements are correct? Select all that apply.", "Options": {"A": "The illiquidity of life insurance policies creates a captive pool of existing policyholders, which can weaken incentives for information disclosure.", "B": "Regulators should aim to minimize the probability of any insurer failing, for example, by setting a universal failure probability target of 0.5%.", "C": "The primary justification for life insurance regulation is to serve as a third-party enforcement mechanism for long-term, incomplete contracts.", "D": "The main regulatory concern for life insurers, similar to banks, is preventing systemic risk from 'insurance runs'."}, "Answer": ["A", "C"], "pi_justification": "This item assesses a synthetic understanding of the paper's core arguments. It requires students to identify the central thesis (A), the key market failure (B), and reject incorrect rationales that the paper explicitly refutes (C and D). Distractor C is a direct Conceptual Opposite. Distractor D (True but Irrelevant/Misleading) represents the type of one-size-fits-all regulation the paper criticizes as potentially inefficient and not aligned with the contract enforcement view.", "qid": "202", "question": "### Background\n\nThis paper proposes that life insurance regulation should be viewed primarily as a mechanism for enforcing long-term, often incomplete, contracts. This approach is designed to solve specific market failures inherent to the life insurance industry, which differ significantly from those in banking.\n\n### Question\n\nAccording to the paper's analysis of the life insurance market, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 377, "Question": "### Background\n\nTo control for unobserved differences between firms, a study estimates the following panel regression model for corporate investment (`INV`):\n\n  \nINV_{it} = \\beta_0 + \\beta_1 Q_{it-1} + \\beta_2 CF_{it-1} + \\text{FirmFE}_i + \\text{YearFE}_t + \\varepsilon_{it}\n \n\nWhere `FirmFE_i` represents firm fixed effects.\n\n### Question\n\nWhich of the following statements are valid implications of including firm fixed effects (`FirmFE_i`) in this model? (Select all that apply)", "Options": {"A": "The model controls for time-invariant firm characteristics, such as a firm's foundational corporate culture or its long-term strategic industry position.", "B": "The model fails to control for time-varying omitted variables, such as a sudden change in CEO effectiveness, that could affect both investment and cash flow.", "C": "The model effectively eliminates all potential sources of endogeneity bias in the estimation of β₁ and β₂.", "D": "The model's coefficients are identified based on how a firm's investment changes over time as its own Q and cash flow change."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses a nuanced understanding of the power and limitations of firm fixed effects, a cornerstone identification strategy. It uses a Conceptual Judgment strategy. Correct options (A), (B), and (D) accurately describe what fixed effects do (identify from within-variation), what they control for (time-invariant heterogeneity), and what they do not control for (time-varying heterogeneity). Distractor (C) represents a classic, high-frequency misconception: the belief that fixed effects are a cure-all for endogeneity. This format forces the user to evaluate each statement's validity independently.", "qid": "377", "question": "### Background\n\nTo control for unobserved differences between firms, a study estimates the following panel regression model for corporate investment (`INV`):\n\n  \nINV_{it} = \\beta_0 + \\beta_1 Q_{it-1} + \\beta_2 CF_{it-1} + \\text{FirmFE}_i + \\text{YearFE}_t + \\varepsilon_{it}\n \n\nWhere `FirmFE_i` represents firm fixed effects.\n\n### Question\n\nWhich of the following statements are valid implications of including firm fixed effects (`FirmFE_i`) in this model? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 193, "Question": "### Background\n\n**Research Question.** This case examines the relationship between the average age of a firm's supervisory board and its financial performance. The paper's main empirical finding is a robust negative correlation between average board age and firm value (Tobin's Q) in a cross-sectional analysis.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish listed firms, with data averaged over the 1998–2001 period.\n\n---\n\n### Data / Model Specification\n\nThe baseline cross-sectional OLS model is:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot AGE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\nThe paper interprets the negative estimate of `β₁` as “older boards cause underperformance.” However, this causal claim is threatened by potential endogeneity.\n\n---\n\n### Question\n\nSelect all statements that correctly describe a key causal inference challenge for **Eq. (1)** and a valid econometric solution.", "Options": {"A": "A key threat to a causal interpretation is reverse causality, where poorly performing firms struggle to attract younger directors, creating a spurious correlation.", "B": "A panel data fixed-effects model helps mitigate the reverse causality threat by using only within-firm variation, controlling for time-invariant unobserved firm characteristics like 'prestige'.", "C": "The cross-sectional OLS estimate is reliable for causal inference because the model includes several control variables, which solves the endogeneity problem.", "D": "A fixed-effects model identifies the coefficient on `Age` by comparing high-age firms to low-age firms at a single point in time."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses understanding of a core causal inference problem (endogeneity via reverse causality) and the logic of its solution (panel data fixed effects). It uses a **Premise/Assumption Packaging** strategy, asking the user to identify valid statements about the problem and solution. Distractors are designed to be high-fidelity: (B) is an 'Almost Right' statement that overstates the power of control variables, and (C) is a 'Conceptual Opposite' that incorrectly describes how a fixed-effects model works.", "qid": "193", "question": "### Background\n\n**Research Question.** This case examines the relationship between the average age of a firm's supervisory board and its financial performance. The paper's main empirical finding is a robust negative correlation between average board age and firm value (Tobin's Q) in a cross-sectional analysis.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish listed firms, with data averaged over the 1998–2001 period.\n\n---\n\n### Data / Model Specification\n\nThe baseline cross-sectional OLS model is:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot AGE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\nThe paper interprets the negative estimate of `β₁` as “older boards cause underperformance.” However, this causal claim is threatened by potential endogeneity.\n\n---\n\n### Question\n\nSelect all statements that correctly describe a key causal inference challenge for **Eq. (1)** and a valid econometric solution.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 267, "Question": "### Background\n\nThe analysis interprets coefficients from a regression of mutual fund alpha on flow risk, a dummy for derivative use, and the interaction between the two to test the \"flow management hypothesis.\"\n\n### Data / Model Specification\n\nThe estimated interaction model is:\n  \n\\alpha_{i}=\\beta_{0}+\\dots+\\beta_{3}flow\\_risk_{i}+\\beta_{4}flow\\_mgmt_{i}+\\beta_{5}derivatives_{i} +\\dots+\\varepsilon_{i}\n \nwhere `flow_mgmt_i` is the interaction term `derivatives_i × flow_risk_i`. This model is estimated using a two-stage least squares (2SLS) approach.\n\n**Table 1: Key Coefficient Estimates (2SLS, from Table 7, Col. 6)**\n\n| Variable | Coefficient (`\\hat{\\beta}`) |\n| :--- | :--- |\n| `flow_risk` | -0.0871 |\n| `flow_mgmt` | 0.0529 |\n| `derivatives` | -0.0011 |\n\n---\n\nBased on the model and data in **Table 1**, select all of the following statements that are correct.\n", "Options": {"A": "The total marginal effect of a one-unit increase in `flow_risk` on monthly alpha for a fund that uses derivatives is -0.0342.", "B": "The results suggest that using derivatives mitigates approximately 61% of the negative performance impact from flow risk.", "C": "For a fund that does not use derivatives, the marginal effect of a one-unit increase in `flow_risk` on monthly alpha is -0.0342.", "D": "The negative coefficient on the `derivatives` variable implies that, after accounting for flow management benefits, the net effect of using derivatives is a performance drag, possibly due to implementation costs."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item tests the ability to correctly interpret an interaction model, calculate marginal effects for different groups, and synthesize these calculations into a quantitative and conceptual understanding of the paper's central finding (the flow management hypothesis). It uses a Computational Judgment strategy.\n\nDistractor Logic:\n- A (Correct): Requires calculating the total marginal effect for a user fund: `β_flow_risk + β_flow_mgmt = -0.0871 + 0.0529 = -0.0342`.\n- B (Correct): Requires calculating the percentage mitigation: `|β_flow_mgmt / β_flow_risk| = |0.0529 / -0.0871| ≈ 0.607` or 60.7%.\n- C (Correct): Requires correct interpretation of the main effect (`derivatives`) in an interaction model, which represents the effect when the interacting variable (`flow_risk`) is zero.\n- D (Incorrect): This is a Step-Omission Error. It incorrectly applies the user fund's marginal effect to the non-user fund. The correct marginal effect for a non-user is simply the coefficient on `flow_risk`, which is -0.0871.", "qid": "267", "question": "### Background\n\nThe analysis interprets coefficients from a regression of mutual fund alpha on flow risk, a dummy for derivative use, and the interaction between the two to test the \"flow management hypothesis.\"\n\n### Data / Model Specification\n\nThe estimated interaction model is:\n  \n\\alpha_{i}=\\beta_{0}+\\dots+\\beta_{3}flow\\_risk_{i}+\\beta_{4}flow\\_mgmt_{i}+\\beta_{5}derivatives_{i} +\\dots+\\varepsilon_{i}\n \nwhere `flow_mgmt_i` is the interaction term `derivatives_i × flow_risk_i`. This model is estimated using a two-stage least squares (2SLS) approach.\n\n**Table 1: Key Coefficient Estimates (2SLS, from Table 7, Col. 6)**\n\n| Variable | Coefficient (`\\hat{\\beta}`) |\n| :--- | :--- |\n| `flow_risk` | -0.0871 |\n| `flow_mgmt` | 0.0529 |\n| `derivatives` | -0.0011 |\n\n---\n\nBased on the model and data in **Table 1**, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 166, "Question": "### Background\n\nA study analyzed the performance and strategic outcomes of *de novo* banks three years after their entry into a market. The analysis used a multivariate regression framework where the key explanatory variables were indicators for different founding motivations reported in a survey.\n\n### Data / Model Specification\n\nThe following table presents selected results from bank-level regressions where the dependent variables are measures of performance or strategy in Year +3 post-entry.\n\n**Table 1: Selected Bank-Level Regression Results (Year +3)**\n\n| Dependent Variable | `RESP1` (Underserved Small Business Market) | `RESP2` (Provide Competition) | `RESP5` (M&A Eliminated Community Banks) |\n| :--- | :--- | :--- | :--- |\n| ROA (%) | -0.0113 | 0.0179 | **-0.2970** (p<0.10) |\n| Profit efficiency | -0.0016 | **0.0158** (p<0.05) | -0.0073 |\n| Amount of Small Bus. loans-to-assets | **0.0645** (p<0.01) | 0.0025 | -0.0114 |\n\n*Note: Table shows estimated coefficients. Significance is based on p-values reported in the original paper.* \n\n---\n\nBased on the statistically significant results in **Table 1**, which of the following conclusions about the strategic paths and performance of *de novo* banks are supported by the evidence? Select all that apply.", "Options": {"A": "Banks founded with the stated goal of serving an underserved small-business market (`RESP1`) successfully execute this strategy, dedicating a significantly larger portion of their assets to small-business loans compared to other *de novo* banks.", "B": "Banks founded with the intent to provide competition (`RESP2`) achieve superior operational excellence, as evidenced by their significantly higher profit efficiency three years after entry.", "C": "Entering a market where M&A activity has eliminated community banks (`RESP5`) is associated with significantly lower bank profitability (ROA) three years post-entry.", "D": "Banks motivated by providing competition (`RESP2`) also demonstrate a significantly higher commitment to small-business lending, as shown by the `Amount of Small Bus. loans-to-assets`."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the candidate's ability to correctly interpret multiple, statistically significant regression coefficients from a table and synthesize them into valid conclusions about distinct business strategies. The depth strategy is 'Computational Judgment,' requiring the user to filter for significance and then translate the numerical results into conceptual statements. Option A is correct based on the significant positive coefficient for RESP1 on small business loans. Option B is correct based on the significant positive coefficient for RESP2 on profit efficiency. Option C is correct based on the significant negative coefficient for RESP5 on ROA. Option D is a 'True but Irrelevant' distractor, as the coefficient for RESP2 on small business loans is positive but statistically insignificant, making the conclusion invalid.", "qid": "166", "question": "### Background\n\nA study analyzed the performance and strategic outcomes of *de novo* banks three years after their entry into a market. The analysis used a multivariate regression framework where the key explanatory variables were indicators for different founding motivations reported in a survey.\n\n### Data / Model Specification\n\nThe following table presents selected results from bank-level regressions where the dependent variables are measures of performance or strategy in Year +3 post-entry.\n\n**Table 1: Selected Bank-Level Regression Results (Year +3)**\n\n| Dependent Variable | `RESP1` (Underserved Small Business Market) | `RESP2` (Provide Competition) | `RESP5` (M&A Eliminated Community Banks) |\n| :--- | :--- | :--- | :--- |\n| ROA (%) | -0.0113 | 0.0179 | **-0.2970** (p<0.10) |\n| Profit efficiency | -0.0016 | **0.0158** (p<0.05) | -0.0073 |\n| Amount of Small Bus. loans-to-assets | **0.0645** (p<0.01) | 0.0025 | -0.0114 |\n\n*Note: Table shows estimated coefficients. Significance is based on p-values reported in the original paper.* \n\n---\n\nBased on the statistically significant results in **Table 1**, which of the following conclusions about the strategic paths and performance of *de novo* banks are supported by the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 168, "Question": "### Background\n\nA study investigated the conventional wisdom that bank managers displaced by mergers and acquisitions (M&A) have a competitive advantage when starting a *de novo* bank. The study used survey data to distinguish between two M&A-related entry motivations and analyzed their impact on bank performance three years post-entry.\n\n### Data / Model Specification\n\nThe study's multivariate regressions of bank performance (from Table 5 in the paper) yielded the following key results:\n\n1.  **`RESP3` (Personal Displacement):** The coefficient for founders motivated by their own prior bank being acquired was statistically insignificant in regressions explaining ROA, profit efficiency, and small-business lending.\n2.  **`RESP5` (Market Structure Change):** The coefficient for founders motivated by M&A eliminating community banks in the market was negative and statistically significant in the regression explaining ROA.\n\n---\n\nBased on these findings, which of the following statements are valid conclusions or plausible economic interpretations presented in the paper? Select all that apply.", "Options": {"A": "The negative performance impact of `RESP5` suggests that markets that have undergone significant M&A consolidation are inherently more challenging environments, potentially due to aggressive competition from large surviving incumbents.", "B": "The study's results challenge the conventional wisdom, finding no statistical evidence that managers personally displaced by M&A (`RESP3`) achieve superior (or inferior) performance compared to other *de novo* founders.", "C": "A plausible reason for the null result for `RESP3` is that any advantage from portable client relationships may be offset because non-displaced founders also possess strong, transferable client networks.", "D": "The results demonstrate that being personally displaced by a merger (`RESP3`) is the primary cause of lower profitability for *de novo* banks."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to synthesize and interpret a mix of significant and null findings to understand a nuanced argument. The depth strategy is 'Reverse-Reasoning,' asking for plausible causes and valid interpretations of observed results. Option A is a direct, correct summary of the finding for RESP3. Option B provides the economic rationale offered by the authors for the null result. Option C provides a coherent economic explanation for the negative RESP5 result, contrasting a market-level effect with the personal-level effect. Option D is a 'Conceptual Opposite' distractor; it incorrectly attributes the negative performance effect to RESP3, which had a null result, rather than RESP5.", "qid": "168", "question": "### Background\n\nA study investigated the conventional wisdom that bank managers displaced by mergers and acquisitions (M&A) have a competitive advantage when starting a *de novo* bank. The study used survey data to distinguish between two M&A-related entry motivations and analyzed their impact on bank performance three years post-entry.\n\n### Data / Model Specification\n\nThe study's multivariate regressions of bank performance (from Table 5 in the paper) yielded the following key results:\n\n1.  **`RESP3` (Personal Displacement):** The coefficient for founders motivated by their own prior bank being acquired was statistically insignificant in regressions explaining ROA, profit efficiency, and small-business lending.\n2.  **`RESP5` (Market Structure Change):** The coefficient for founders motivated by M&A eliminating community banks in the market was negative and statistically significant in the regression explaining ROA.\n\n---\n\nBased on these findings, which of the following statements are valid conclusions or plausible economic interpretations presented in the paper? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 178, "Question": "### Background\n\n**Research Question.** This case examines the link between executive stock option compensation and incentives to meet short-term earnings targets. The primary hypothesis (H1) posits that a higher proportion of stock option compensation (`OPT`) increases the likelihood that a firm will meet or beat analyst earnings forecasts (`MEET=1`).\n\n**Causal Inference Challenge.** A key challenge is endogeneity. An unobserved factor, such as superior firm quality, could drive both higher option-based pay and stronger performance, creating a spurious correlation. To address this, the paper employs an instrumental variable (IV) approach, using the industry-average `OPT` as an instrument for a firm's own `OPT`.\n\n### Data / Model Specification\n\nThe probability of meeting or beating analyst forecasts (`MEET=1`) is modeled via logistic regression. The percentage change in odds for an indicator variable `j` is `100[exp(β_j) - 1]`. The paper also reports that a one-standard-deviation increase in `OPT` increases the odds of meeting the forecast by 5.3% in the baseline model.\n\n**Table 1: Logistic Regression Results for P(MEET=1)**\n\n| Specification | Variable | Coefficient | z-statistic |\n| :--- | :--- | :---: | :---: |\n| Baseline (OLS) | `OPT` | 0.199 | (2.82)** |\n| | `LIT` | 0.185 | (4.09)** |\n| Instrumental Variable (IV) | `OPT` | 0.042 | (2.65)** |\n\n*N=40,661. **p<0.01. `LIT` is an indicator for high-litigation risk industry.*\n\n### Question\n\nBased on the provided regression results and econometric context, select all statements that represent a correct quantitative interpretation or a valid critique of the identification strategy.", "Options": {"A": "The instrumental variable (IV) estimate for `OPT` (0.042) is smaller than the OLS estimate (0.199), suggesting the OLS estimate was biased downwards due to unobserved managerial talent.", "B": "A key threat to the OLS estimate's causal interpretation is reverse causality: boards may grant more options to managers who consistently meet forecasts, meaning `MEET` causes `OPT`.", "C": "The economic significance of being in a high-litigation industry (`LIT`) is substantially larger than that of a one-standard-deviation increase in `OPT`, with `LIT` increasing the odds of meeting a forecast by approximately 20.3%.", "D": "The IV's exclusion restriction is potentially violated if an unobserved factor, like 'hot industry' status, both increases industry-wide use of options (the instrument) and independently increases pressure on firms to meet earnings targets."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests a comprehensive set of econometrics skills: quantitative interpretation of logistic regression output (Computational Judgment), identification of endogeneity threats (Reverse-Reasoning), and critical evaluation of an instrumental variable strategy. It uses Atomic Decomposition. Option A requires a calculation. Option C identifies reverse causality. Option D critiques the IV's exclusion restriction. Option B is a Conceptual Opposite distractor, incorrectly stating the direction of OLS bias.", "qid": "178", "question": "### Background\n\n**Research Question.** This case examines the link between executive stock option compensation and incentives to meet short-term earnings targets. The primary hypothesis (H1) posits that a higher proportion of stock option compensation (`OPT`) increases the likelihood that a firm will meet or beat analyst earnings forecasts (`MEET=1`).\n\n**Causal Inference Challenge.** A key challenge is endogeneity. An unobserved factor, such as superior firm quality, could drive both higher option-based pay and stronger performance, creating a spurious correlation. To address this, the paper employs an instrumental variable (IV) approach, using the industry-average `OPT` as an instrument for a firm's own `OPT`.\n\n### Data / Model Specification\n\nThe probability of meeting or beating analyst forecasts (`MEET=1`) is modeled via logistic regression. The percentage change in odds for an indicator variable `j` is `100[exp(β_j) - 1]`. The paper also reports that a one-standard-deviation increase in `OPT` increases the odds of meeting the forecast by 5.3% in the baseline model.\n\n**Table 1: Logistic Regression Results for P(MEET=1)**\n\n| Specification | Variable | Coefficient | z-statistic |\n| :--- | :--- | :---: | :---: |\n| Baseline (OLS) | `OPT` | 0.199 | (2.82)** |\n| | `LIT` | 0.185 | (4.09)** |\n| Instrumental Variable (IV) | `OPT` | 0.042 | (2.65)** |\n\n*N=40,661. **p<0.01. `LIT` is an indicator for high-litigation risk industry.*\n\n### Question\n\nBased on the provided regression results and econometric context, select all statements that represent a correct quantitative interpretation or a valid critique of the identification strategy.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 390, "Question": "### Background\n\n**Research Question.** Does the degree of information asymmetry affect the magnitude of Seasoned Equity Offering (SEO) underpricing? The paper's central hypothesis is that SEOs by REITs that went public in or after 1990 (\"post-1990\") are more underpriced than those by REITs that went public before 1990 (\"pre-1990\").\n\n**Setting.** The study posits that post-1990 REITs operate in a high-information-asymmetry environment, while pre-1990 REITs operate in a lower-asymmetry environment. The effect of variables like offer size may differ depending on which economic force—price pressure vs. information production—dominates in a given environment.\n\n**Variables & Parameters.**\n- `Underpricing (UR)`: The offer-to-close return `(P_1 / P_0 - 1) * 100` (percentage).\n- `LNOFFSIZE`: Natural log of the offer size.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a multivariate OLS regression for each subsample. Key results for the offer size variable are summarized below.\n\n**Table 1. Multivariate OLS Regression Estimates of Underpricing (UR)**\n\n| Variable | Pre-1990 IPO (n=57) | Post-1990 IPO (n=114) |\n| :--- | :--- | :--- |\n| LNOFFSIZE | 1.154 (1.87)* | -0.671 (-2.49)* |\n\n*Source: Synthesized from Model 2 results in Table 5 of the source document. t-statistics are in parentheses. An asterisk (*) denotes significance at the 10% level or lower.*\n\n---\n\n### Question\n\nThe study finds a significant relationship between offer size (`LNOFFSIZE`) and underpricing that differs between the two subsamples. Select all statements that accurately describe these findings and their proposed economic interpretations.", "Options": {"A": "For the pre-1990 (low-asymmetry) sample, larger offers were linked to *more* underpricing, consistent with a 'price pressure' effect dominating.", "B": "For the post-1990 sample, the positive coefficient on `LNOFFSIZE` suggests that price pressure from large new supply was the dominant factor.", "C": "For the post-1990 (high-asymmetry) sample, larger offers were linked to *less* underpricing, consistent with an 'information production' or 'certification' effect dominating.", "D": "The study found that offer size was an insignificant predictor of underpricing in both the pre-1990 and post-1990 environments."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses a nuanced finding: the reversal of a key coefficient across subsamples. It uses a **Scenario Application** strategy, where the 'scenarios' are the low- vs. high-asymmetry environments. The correct options (A, B) correctly link the empirical finding (coefficient sign) to the corresponding economic theory (price pressure vs. information production) for each environment. Distractor C is a **Factual Error** (the coefficient was negative for post-1990), and Distractor D is also a **Factual Error** (the coefficient was significant in both samples).", "qid": "390", "question": "### Background\n\n**Research Question.** Does the degree of information asymmetry affect the magnitude of Seasoned Equity Offering (SEO) underpricing? The paper's central hypothesis is that SEOs by REITs that went public in or after 1990 (\"post-1990\") are more underpriced than those by REITs that went public before 1990 (\"pre-1990\").\n\n**Setting.** The study posits that post-1990 REITs operate in a high-information-asymmetry environment, while pre-1990 REITs operate in a lower-asymmetry environment. The effect of variables like offer size may differ depending on which economic force—price pressure vs. information production—dominates in a given environment.\n\n**Variables & Parameters.**\n- `Underpricing (UR)`: The offer-to-close return `(P_1 / P_0 - 1) * 100` (percentage).\n- `LNOFFSIZE`: Natural log of the offer size.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a multivariate OLS regression for each subsample. Key results for the offer size variable are summarized below.\n\n**Table 1. Multivariate OLS Regression Estimates of Underpricing (UR)**\n\n| Variable | Pre-1990 IPO (n=57) | Post-1990 IPO (n=114) |\n| :--- | :--- | :--- |\n| LNOFFSIZE | 1.154 (1.87)* | -0.671 (-2.49)* |\n\n*Source: Synthesized from Model 2 results in Table 5 of the source document. t-statistics are in parentheses. An asterisk (*) denotes significance at the 10% level or lower.*\n\n---\n\n### Question\n\nThe study finds a significant relationship between offer size (`LNOFFSIZE`) and underpricing that differs between the two subsamples. Select all statements that accurately describe these findings and their proposed economic interpretations.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 383, "Question": "### Background\n\n**Research Question.** How can the causal effect of foreign official inflows on U.S. Treasury yields be estimated when these inflows are endogenous to market conditions, and are the findings robust across different measures of risk premium?\n\n**Setting.** A two-stage least squares (2SLS) regression framework is used to estimate the short-run causal effect of foreign official inflows on two different measures of the bond risk premium: the model-based 5-year term premium (`\\Delta TP_t^5`) and the model-free realized one-year excess return (`XR_6t`). A key challenge is that unobserved “flight-to-safety” episodes can simultaneously lower Treasury yields and reduce foreign official inflows, biasing ordinary least squares (OLS) estimates.\n\n### Data / Model Specification\n\nThe general regression model is:\n\n  \n\\text{Risk Premium}_t = \\alpha + \\beta_1 (\\text{Foreign Inflows}_t) + \\mathbf{X}_t'\\gamma + u_t\n \n\nwhere `\\mathbf{X}_t` is a vector of exogenous controls. To address the endogeneity of `\\text{Foreign Inflows}_t`, instruments such as Japanese foreign exchange interventions (`JPYFXINT_t`) are used.\n\n**Table 1. The Impact of Foreign Official Inflows on U.S. Bond Risk Premia**\n\n| | (1) OLS | (2) IV / 2SLS | (3) IV / 2SLS | (4) IV / 2SLS |\n| :--- | :---: | :---: | :---: | :---: |\n| **Dependent Variable:** | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `XR_6t` |\n| | | | | |\n| `\\Delta FOIL/DEBT_{t-1}` | 0.052 | -0.135** | | |\n| (Monthly Inflows) | (0.030) | (0.061) | | |\n| | | | | |\n| `\\Delta FOIL_JAPAN/DEBT_{t-1}` | | | -0.147** | |\n| (Japanese Monthly Inflows) | | | (0.059) | | |\n| | | | | |\n| `\\sum_{12} FOI_t / DEBT_{t-12}` | | | | 0.595*** |\n| (Annual Inflows) | | | | (0.184) |\n| | | | | |\n| **Specification Tests** | | | | |\n| Cragg-Donald F-Stat | | 15.72 | 97.59 | 92.79 |\n| *Weak Inst. Critical Value* | | *11.59* | *8.96* | *11.59* |\n| Hansen J Test (p-value) | | 0.3498 | n.a. | 0.3563 |\n\n*Notes: Standard errors in parentheses. *p<0.1, **p<0.05, ***p<0.01. `\\Delta TP_t^5` is the monthly change in the 5-year term premium. `XR_6t` is the one-year realized excess return. `FOIL/DEBT` denotes foreign official inflows scaled by outstanding marketable debt.*\n\n### Question\n\nBased on the provided theory and regression results in Table 1, select all of the following statements that are correct interpretations of the findings regarding endogeneity and the main instrumental variable (IV) estimate.", "Options": {"A": "The 2SLS estimate in Column (2) indicates that a $100 billion foreign official inflow would lower the 5-year term premium by approximately 46 basis points, and this effect is statistically significant at the 5% level.", "B": "The diagnostic tests for the specification in Column (2) suggest the instruments are weak, as the Cragg-Donald F-statistic (15.72) is low, and invalid, as the Hansen J-test p-value (0.3498) is high.", "C": "The positive OLS coefficient (0.052) in Column (1) is consistent with an upward bias caused by \"flight-to-safety\" episodes, where unobserved risk aversion simultaneously lowers yields and reduces foreign official inflows.", "D": "The 2SLS coefficient of -0.135 implies that after correcting for endogeneity, increased foreign official inflows are associated with a statistically significant *increase* in the 5-year term premium."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the core concepts of endogeneity bias and IV estimate interpretation, converted from a Table QA problem. It uses an **Atomic Decomposition** strategy, breaking the original QA's parts 1 and 2 into distinct, verifiable statements. **Correct options (A, B)** test the user's ability to explain the OLS bias mechanism and to correctly interpret the magnitude and significance of the 2SLS result. **Distractor C (Sign Error)** targets a fundamental misreading of the coefficient's sign. **Distractor D (Instrument Misinterpretation)** targets common errors in interpreting diagnostic statistics: confusing a high F-statistic with weakness and a high p-value on the Hansen J-test with invalidity.", "qid": "383", "question": "### Background\n\n**Research Question.** How can the causal effect of foreign official inflows on U.S. Treasury yields be estimated when these inflows are endogenous to market conditions, and are the findings robust across different measures of risk premium?\n\n**Setting.** A two-stage least squares (2SLS) regression framework is used to estimate the short-run causal effect of foreign official inflows on two different measures of the bond risk premium: the model-based 5-year term premium (`\\Delta TP_t^5`) and the model-free realized one-year excess return (`XR_6t`). A key challenge is that unobserved “flight-to-safety” episodes can simultaneously lower Treasury yields and reduce foreign official inflows, biasing ordinary least squares (OLS) estimates.\n\n### Data / Model Specification\n\nThe general regression model is:\n\n  \n\\text{Risk Premium}_t = \\alpha + \\beta_1 (\\text{Foreign Inflows}_t) + \\mathbf{X}_t'\\gamma + u_t\n \n\nwhere `\\mathbf{X}_t` is a vector of exogenous controls. To address the endogeneity of `\\text{Foreign Inflows}_t`, instruments such as Japanese foreign exchange interventions (`JPYFXINT_t`) are used.\n\n**Table 1. The Impact of Foreign Official Inflows on U.S. Bond Risk Premia**\n\n| | (1) OLS | (2) IV / 2SLS | (3) IV / 2SLS | (4) IV / 2SLS |\n| :--- | :---: | :---: | :---: | :---: |\n| **Dependent Variable:** | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `XR_6t` |\n| | | | | |\n| `\\Delta FOIL/DEBT_{t-1}` | 0.052 | -0.135** | | |\n| (Monthly Inflows) | (0.030) | (0.061) | | |\n| | | | | |\n| `\\Delta FOIL_JAPAN/DEBT_{t-1}` | | | -0.147** | |\n| (Japanese Monthly Inflows) | | | (0.059) | | |\n| | | | | |\n| `\\sum_{12} FOI_t / DEBT_{t-12}` | | | | 0.595*** |\n| (Annual Inflows) | | | | (0.184) |\n| | | | | |\n| **Specification Tests** | | | | |\n| Cragg-Donald F-Stat | | 15.72 | 97.59 | 92.79 |\n| *Weak Inst. Critical Value* | | *11.59* | *8.96* | *11.59* |\n| Hansen J Test (p-value) | | 0.3498 | n.a. | 0.3563 |\n\n*Notes: Standard errors in parentheses. *p<0.1, **p<0.05, ***p<0.01. `\\Delta TP_t^5` is the monthly change in the 5-year term premium. `XR_6t` is the one-year realized excess return. `FOIL/DEBT` denotes foreign official inflows scaled by outstanding marketable debt.*\n\n### Question\n\nBased on the provided theory and regression results in Table 1, select all of the following statements that are correct interpretations of the findings regarding endogeneity and the main instrumental variable (IV) estimate.", "model_evaluation_result": {"model_raw_response": "C", "model_answer": ["C"], "error": null}}
{"ID": 376, "Question": "### Background\n\n**Research Question.** How do modern financial innovations in the syndicated loan market, such as the \"alphabet loan\" structure, allow LBO sponsors to increase leverage while managing different lender appetites, and what new risks do these structures create?\n\n**Setting.** This case examines the financial engineering behind tranched senior secured loans used in LBOs. This structure partitions a single senior loan into multiple classes (A, B, C) with different terms, attracting both traditional banks and non-bank lenders like prime rate funds.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Typical Characteristics of Alphabet Loan Tranches**\n\n| Characteristic | Tranche A (Term Loan A) | Tranche B (Term Loan B) |\n| :--- | :--- | :--- |\n| Lenders | Commercial Banks | Prime Rate Funds, Institutional Investors |\n| Maturity (`M`) | 6-7 years | up to 9 years |\n| Spread (`S`) | LIBOR + 200-250 bps | LIBOR + 275-375 bps |\n| Amortization (`Amort`) | Significant, scheduled | Nominal or none (bullet) |\n\n**Proposition 1.** Tranches B, C, and D have the same legal priority and security as Tranche A.\n\n---\n\n### Question\n\nBased on the provided information about alphabet loan structures, select all of the following statements that are correct.", "Options": {"A": "Tranche B lenders receive a higher spread because their claims have a lower legal priority in a bankruptcy liquidation compared to Tranche A lenders.", "B": "The primary benefit of the alphabet structure is that it allows the borrower to increase total leverage by replacing senior bank debt with subordinated debt.", "C": "Tranche B is considered riskier than Tranche A primarily due to its effective subordination in the cash flow waterfall, where principal is repaid only after Tranche A is fully amortized.", "D": "The structure segments the lender market by offering a shorter-maturity, amortizing Tranche A for risk-averse commercial banks and a higher-yield, longer-maturity Tranche B for yield-driven institutional investors."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of a specific financial innovation. It uses a 'Scenario Application' strategy with 'Atomic Decomposition' to test comprehension of market segmentation, risk pricing, and the crucial distinction between legal and effective priority. Distractor C is a 'Conceptual Opposite' that directly contradicts Proposition 1. Distractor D is a subtle 'Conceptual Error' because Tranche B is legally senior, not subordinated, debt.", "qid": "376", "question": "### Background\n\n**Research Question.** How do modern financial innovations in the syndicated loan market, such as the \"alphabet loan\" structure, allow LBO sponsors to increase leverage while managing different lender appetites, and what new risks do these structures create?\n\n**Setting.** This case examines the financial engineering behind tranched senior secured loans used in LBOs. This structure partitions a single senior loan into multiple classes (A, B, C) with different terms, attracting both traditional banks and non-bank lenders like prime rate funds.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Typical Characteristics of Alphabet Loan Tranches**\n\n| Characteristic | Tranche A (Term Loan A) | Tranche B (Term Loan B) |\n| :--- | :--- | :--- |\n| Lenders | Commercial Banks | Prime Rate Funds, Institutional Investors |\n| Maturity (`M`) | 6-7 years | up to 9 years |\n| Spread (`S`) | LIBOR + 200-250 bps | LIBOR + 275-375 bps |\n| Amortization (`Amort`) | Significant, scheduled | Nominal or none (bullet) |\n\n**Proposition 1.** Tranches B, C, and D have the same legal priority and security as Tranche A.\n\n---\n\n### Question\n\nBased on the provided information about alphabet loan structures, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 388, "Question": "### Background\n\n**Research Question.** How do flawed internal performance metrics and incentive schemes lead to systematic capital misallocation, creating vulnerabilities that activists can exploit through strategic proposals like spin-offs or redesigned compensation contracts?\n\n**Setting.** A multi-division corporation where capital allocation decisions are decentralized. Divisional managers are compensated based on their division's performance, but the chosen metric may not align with overall shareholder value creation. The firm operates an internal capital market to allocate a fixed capital budget between its divisions.\n\n**Variables & Parameters.**\n*   `ROIC`: Return on Invested Capital (Net Operating Profit After Tax / Invested Capital).\n*   `WACC`: Weighted Average Cost of Capital, the firm's hurdle rate.\n*   `V_A(K_A)`, `V_B(K_B)`: The standalone value of Division A and B, respectively, as a function of the capital `K` allocated to them. Assume these functions exhibit positive but diminishing returns to capital (i.e., `V' > 0`, `V'' < 0`).\n\n---\n\n### Data / Model Specification\n\nThe paper argues that activists often push for the breakup of conglomerates due to \"poor capital allocation decisions as one business subsidizes the other.\" This can happen when a conglomerate systematically overallocates capital to its less productive divisions, resulting in an allocation where the marginal return to capital is unequal across divisions (`V_A'(K_A) > V_B'(K_B)`). Activists may propose structural changes like spin-offs to resolve these inefficiencies.\n\n---\n\n### Question\n\nAccording to the paper's logic on conglomerate structure and capital allocation, select all statements that are TRUE.", "Options": {"A": "A spin-off can create value by dismantling an inefficient internal capital market, forcing the newly independent businesses to raise capital externally based on their standalone prospects.", "B": "In a conglomerate with inefficient capital allocation, value is destroyed because one business unit effectively subsidizes another, preventing capital from flowing to its highest-return use.", "C": "The primary value created by a spin-off is the reduction of corporate-level taxes, which is more significant than any gains from improved capital allocation.", "D": "A conglomerate creates maximum value when it allocates capital such that the average ROIC is equal across all divisions."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the understanding of a key strategic vulnerability in conglomerates—inefficient internal capital markets—and the mechanism by which a spin-off acts as a structural remedy. It uses a premise-packaging strategy. The correct options (B, C) identify the core problem (cross-subsidization) and the solution (dismantling the internal market). Distractor A targets the common economic error of confusing average vs. marginal returns in optimization problems. Distractor D presents an alternative, but secondary, rationale for spin-offs to test for focus on the paper's central argument.", "qid": "388", "question": "### Background\n\n**Research Question.** How do flawed internal performance metrics and incentive schemes lead to systematic capital misallocation, creating vulnerabilities that activists can exploit through strategic proposals like spin-offs or redesigned compensation contracts?\n\n**Setting.** A multi-division corporation where capital allocation decisions are decentralized. Divisional managers are compensated based on their division's performance, but the chosen metric may not align with overall shareholder value creation. The firm operates an internal capital market to allocate a fixed capital budget between its divisions.\n\n**Variables & Parameters.**\n*   `ROIC`: Return on Invested Capital (Net Operating Profit After Tax / Invested Capital).\n*   `WACC`: Weighted Average Cost of Capital, the firm's hurdle rate.\n*   `V_A(K_A)`, `V_B(K_B)`: The standalone value of Division A and B, respectively, as a function of the capital `K` allocated to them. Assume these functions exhibit positive but diminishing returns to capital (i.e., `V' > 0`, `V'' < 0`).\n\n---\n\n### Data / Model Specification\n\nThe paper argues that activists often push for the breakup of conglomerates due to \"poor capital allocation decisions as one business subsidizes the other.\" This can happen when a conglomerate systematically overallocates capital to its less productive divisions, resulting in an allocation where the marginal return to capital is unequal across divisions (`V_A'(K_A) > V_B'(K_B)`). Activists may propose structural changes like spin-offs to resolve these inefficiencies.\n\n---\n\n### Question\n\nAccording to the paper's logic on conglomerate structure and capital allocation, select all statements that are TRUE.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 371, "Question": "### Background\n\n**Research Question.** To test the causal mechanisms through which government industrial policy influences corporate financialization, distinguishing between two primary theoretical motivations.\n\n**Theoretical Framework.** Corporate financialization is driven by two main motives:\n1.  **Prevention Motivation:** Firms hold liquid financial assets as a precautionary measure to manage future operating risks and ease financial constraints.\n2.  **Crowding-Out Effect:** Agency conflicts lead managers to favor short-term financial investments over long-term real investments.\n\nThe study tests four potential channels. It finds that the two channels related to the **prevention motivation** (operational uncertainty, financial constraints) are statistically significant mediators of the policy's effect. In contrast, it finds that the two channels related to the **crowding-out effect** (information transparency, investment opportunities) are not statistically significant mediators.\n\n### Question\n\nConsidering the full set of mechanism findings—that the prevention-related channels are significant mediators while the agency-related channels are not—select all conclusions that are directly supported by this pattern of evidence.", "Options": {"A": "The insignificance of the investment opportunities channel as a mediator means that industrial policy does not create new real investment opportunities for supported firms.", "B": "Industrial policy appears to curb financialization mainly by providing an external substitute for firms' internal need to self-insure against risk.", "C": "The evidence suggests that financialization in this context is primarily a defensive, risk-management strategy for firms.", "D": "The findings imply that industrial policy is an effective tool for improving corporate governance and reducing agency conflicts between managers and shareholders."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Reverse-Reasoning strategy, asking students to infer the underlying driver of corporate behavior from a pattern of empirical results. It tests the synthesis of multiple findings. Correct options (A, B) represent the core theoretical takeaway from the mechanism analysis. Distractors are based on misinterpreting the implications of null results for the agency channel (C) and misunderstanding the logic of mediation, where a variable can be affected by the treatment without being a mediator (D).", "qid": "371", "question": "### Background\n\n**Research Question.** To test the causal mechanisms through which government industrial policy influences corporate financialization, distinguishing between two primary theoretical motivations.\n\n**Theoretical Framework.** Corporate financialization is driven by two main motives:\n1.  **Prevention Motivation:** Firms hold liquid financial assets as a precautionary measure to manage future operating risks and ease financial constraints.\n2.  **Crowding-Out Effect:** Agency conflicts lead managers to favor short-term financial investments over long-term real investments.\n\nThe study tests four potential channels. It finds that the two channels related to the **prevention motivation** (operational uncertainty, financial constraints) are statistically significant mediators of the policy's effect. In contrast, it finds that the two channels related to the **crowding-out effect** (information transparency, investment opportunities) are not statistically significant mediators.\n\n### Question\n\nConsidering the full set of mechanism findings—that the prevention-related channels are significant mediators while the agency-related channels are not—select all conclusions that are directly supported by this pattern of evidence.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 380, "Question": "### Background\n\n**Research Question:** How are the parameters of a MAR-GARCH model estimated, and how can the model's specification be validated, especially when standard tests for the number of regimes are invalid?\n\n**Setting / Data-Generating Environment:** A researcher has fitted a MAR-GARCH model via Maximum Likelihood (ML) and needs to perform diagnostic checks to assess its adequacy. This involves understanding both the conditions for standard inference and the methods for testing for remaining misspecification in the conditional mean and variance.\n\n### Data / Model Specification\n\nDiagnostic checks are based on the standardized residual, `\\hat{e}_t`, which under the null of correct specification is a martingale difference sequence with unit variance.\n  \ne_{t}=\\frac{y_{t}-E_{t-1}(y_{t})}{\\sqrt{\\operatorname{Var}_{t-1}(y_{t})}} \\quad \\text{(Eq. (1))}\n \nWithin the GARCH specification, `u_{i,t}` refers to the regime-specific unscaled residual, `y_t - E(y_t | regime=i)`.\n\n### Question\n\nA researcher has estimated a MAR-GARCH model and is performing diagnostic checks. Which of the following statements about the diagnostic process are correct? Select all that apply.", "Options": {"A": "To test for an asymmetric volatility response (leverage effect), one could add the term `γ * u_{i,t-1} * I(u_{i,t-1}<0)` to the conditional variance equation and test the null `H_0: γ = 0`.", "B": "Significant serial correlation in the standardized residuals, `\\hat{e}_t`, would suggest that the autoregressive order (`p`) of the conditional mean is too low.", "C": "Even if the model is correctly specified, the standardized residuals `e_t` are not expected to be normally distributed.", "D": "Significant serial correlation in the squared standardized residuals, `\\hat{e}_t^2`, would suggest that the GARCH component of the model is misspecified."}, "Answer": ["B", "C", "D"], "pi_justification": "This Multiple Choice item assesses the practical application of diagnostic testing for MAR-GARCH models. It combines key insights from the paper regarding residual analysis and test formulation. Option A tests the standard interpretation of GARCH diagnostics. Option B is a high-fidelity distractor with a subtle error in the test specification (omitting the square on the residual), testing close reading. Option C tests a key property of the model's residuals. Option D tests the standard interpretation of conditional mean diagnostics. This format efficiently covers multiple distinct but related concepts.", "qid": "380", "question": "### Background\n\n**Research Question:** How are the parameters of a MAR-GARCH model estimated, and how can the model's specification be validated, especially when standard tests for the number of regimes are invalid?\n\n**Setting / Data-Generating Environment:** A researcher has fitted a MAR-GARCH model via Maximum Likelihood (ML) and needs to perform diagnostic checks to assess its adequacy. This involves understanding both the conditions for standard inference and the methods for testing for remaining misspecification in the conditional mean and variance.\n\n### Data / Model Specification\n\nDiagnostic checks are based on the standardized residual, `\\hat{e}_t`, which under the null of correct specification is a martingale difference sequence with unit variance.\n  \ne_{t}=\\frac{y_{t}-E_{t-1}(y_{t})}{\\sqrt{\\operatorname{Var}_{t-1}(y_{t})}} \\quad \\text{(Eq. (1))}\n \nWithin the GARCH specification, `u_{i,t}` refers to the regime-specific unscaled residual, `y_t - E(y_t | regime=i)`.\n\n### Question\n\nA researcher has estimated a MAR-GARCH model and is performing diagnostic checks. Which of the following statements about the diagnostic process are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 378, "Question": "### Background\n\n**Research Question.** What are the welfare and financial stability trade-offs of implementing an optimal macroprudential LTV policy, particularly regarding its impact on quantity volatility versus price volatility?\n\n**Setting.** The analysis compares three regimes in a DSGE model: no macroprudential policy, a non-reciprocal policy, and the optimal reciprocal policy. A key finding is that in the presence of financial frictions, the relevant terms for welfare are not the levels and volatilities of total consumption or housing but the consumption and housing gaps between constrained borrowers and unconstrained savers. Policies that manage to effectively close these gaps (or their variance) will be welfare enhancing.\n\n**Variables & Parameters.**\n- `φ_H`, `φ_F`: Policy response coefficients for domestic and foreign LTV rules.\n- `σ_bT`: Standard deviation of total borrowing (quantity stability).\n- `σ_q`: Standard deviation of house prices (price stability).\n- `σ_c,gap`, `σ_h,gap`: Standard deviation of the consumption and housing gaps between borrowers and savers (welfare indicators).\n- **Welfare Gain**: The consumption equivalent (CE) percentage gain from a policy relative to the no-policy baseline.\n\n---\n\n### Data / Model Specification\n\nThe paper models macroprudential policy as a set of counter-cyclical rules where loan-to-value (LTV) ratios are tightened in response to rising domestic house prices:\n  \nm_{H t}=m_{H}(q_{t})^{-\\phi_{H}}, \\quad m_{F t}=m_{F}(q_{t})^{-\\phi_{F}}\n \nReciprocity implies that `φ_F > 0`. The social planner optimizes the choice of `φ_H` and `φ_F` to maximize social welfare. The results of this optimization and the resulting model moments are presented below.\n\n**Table 1: Optimal Macroprudential Policy**\n\n| φ_H (Optimal Domestic) | φ_F (Optimal Foreign) | Welfare Gain (CE) |\n| :--- | :--- | :--- |\n| 8.5 | 0.2 | 1.32% |\n\n**Table 2: Financial Stability and Welfare Moments**\n\n| Policy Regime | σ_bT (Total Borrowing) | σ_c,gap (Consumption Gap) | σ_h,gap (Housing Gap) | σ_q (House Price) | Welfare Gain (CE) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| No Macropru | 5.160 | 2.315 | 7.961 | 1.753 | — |\n| Macropru - No Reciprocity | 3.818 | 0.352 | 1.890 | 1.775 | 0.97% |\n| Optimal Macropru (Reciprocal) | 0.104 | 0.337 | 0.815 | 2.235 | 1.32% |\n\n---\n\n### Question\n\nBased on the provided context and data in Table 1 and Table 2, which of the following statements accurately describe the effects and trade-offs of the \"Optimal Macropru\" policy compared to the \"No Macropru\" baseline?", "Options": {"A": "The optimal policy enhances financial stability by reducing the volatility of both credit quantities (`σ_bT`) and house prices (`σ_q`).", "B": "The policy successfully stabilizes the quantity of total borrowing (`σ_bT`) but does so at the cost of significantly increasing house price volatility (`σ_q`).", "C": "The primary channel for the welfare gain is the reduction in house price volatility, which makes housing more accessible to constrained borrowers.", "D": "The policy achieves its welfare gain primarily by dramatically reducing the volatility of the housing gap (`σ_h,gap`) between borrowers and savers."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize quantitative results from a table with the paper's theoretical framework. It uses an 'atomic decomposition' strategy, breaking down the complex policy evaluation into distinct, verifiable claims about the welfare mechanism and a key policy trade-off. Distractors are designed as 'Conceptual Opposites' that target common misinterpretations of the results, such as assuming stability must apply to both prices and quantities, or misidentifying the source of welfare gains.", "qid": "378", "question": "### Background\n\n**Research Question.** What are the welfare and financial stability trade-offs of implementing an optimal macroprudential LTV policy, particularly regarding its impact on quantity volatility versus price volatility?\n\n**Setting.** The analysis compares three regimes in a DSGE model: no macroprudential policy, a non-reciprocal policy, and the optimal reciprocal policy. A key finding is that in the presence of financial frictions, the relevant terms for welfare are not the levels and volatilities of total consumption or housing but the consumption and housing gaps between constrained borrowers and unconstrained savers. Policies that manage to effectively close these gaps (or their variance) will be welfare enhancing.\n\n**Variables & Parameters.**\n- `φ_H`, `φ_F`: Policy response coefficients for domestic and foreign LTV rules.\n- `σ_bT`: Standard deviation of total borrowing (quantity stability).\n- `σ_q`: Standard deviation of house prices (price stability).\n- `σ_c,gap`, `σ_h,gap`: Standard deviation of the consumption and housing gaps between borrowers and savers (welfare indicators).\n- **Welfare Gain**: The consumption equivalent (CE) percentage gain from a policy relative to the no-policy baseline.\n\n---\n\n### Data / Model Specification\n\nThe paper models macroprudential policy as a set of counter-cyclical rules where loan-to-value (LTV) ratios are tightened in response to rising domestic house prices:\n  \nm_{H t}=m_{H}(q_{t})^{-\\phi_{H}}, \\quad m_{F t}=m_{F}(q_{t})^{-\\phi_{F}}\n \nReciprocity implies that `φ_F > 0`. The social planner optimizes the choice of `φ_H` and `φ_F` to maximize social welfare. The results of this optimization and the resulting model moments are presented below.\n\n**Table 1: Optimal Macroprudential Policy**\n\n| φ_H (Optimal Domestic) | φ_F (Optimal Foreign) | Welfare Gain (CE) |\n| :--- | :--- | :--- |\n| 8.5 | 0.2 | 1.32% |\n\n**Table 2: Financial Stability and Welfare Moments**\n\n| Policy Regime | σ_bT (Total Borrowing) | σ_c,gap (Consumption Gap) | σ_h,gap (Housing Gap) | σ_q (House Price) | Welfare Gain (CE) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| No Macropru | 5.160 | 2.315 | 7.961 | 1.753 | — |\n| Macropru - No Reciprocity | 3.818 | 0.352 | 1.890 | 1.775 | 0.97% |\n| Optimal Macropru (Reciprocal) | 0.104 | 0.337 | 0.815 | 2.235 | 1.32% |\n\n---\n\n### Question\n\nBased on the provided context and data in Table 1 and Table 2, which of the following statements accurately describe the effects and trade-offs of the \"Optimal Macropru\" policy compared to the \"No Macropru\" baseline?", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 234, "Question": "### Background\n\n**Research Question.** How does the choice of a specific distribution within the Generalised Hyperbolic (GH) family, even when all choices provide a good statistical fit, quantitatively impact the estimation of extreme tail risk?\n\n**Setting.** A risk manager has fitted two different univariate distributions from the skewed GH family—the Student-t and the Variance Gamma (VG)—to the daily loss data of Goldman Sachs stock. Both models pass standard goodness-of-fit tests, but the manager needs to select one to calculate the Tail Conditional Expectation (TCE) for setting regulatory capital against extreme losses.\n\n**Variables and Parameters.**\n- `X`: A univariate random variable representing the daily loss of Goldman Sachs stock.\n- `TCE_q(X)`: The Tail Conditional Expectation of `X` at quantile level `q`.\n- `μ, γ, σ²`: Location, skewness, and scale parameters of `X`.\n- `λ, χ, ψ`: Parameters of the GIG mixing distribution.\n- `q`: The quantile level, set to 0.999 for an extreme risk scenario.\n- `k_λ`: A constant derived from the GIG mixing distribution.\n\n---\n\n### Data / Model Specification\n\nThe Tail Conditional Expectation for a `GH_1` random variable `X` is given by:\n  \nTCE_q(X) = \\mu + \\frac{\\gamma}{1-q}k_{\\lambda}\\bar{F}_{GH_{1}}(x_{q};\\lambda+1) + \\frac{\\sigma^{2}}{1-q}k_{\\lambda}f_{GH_{1}}(x_{q};\\lambda+1) \\quad \\text{(Eq. (1))}\n \nwhere `f` and `\\bar{F}` are the pdf and survival function, respectively, of a related `GH_1` distribution with parameter `λ+1`.\n\nFor the limiting cases, the coefficient `k_λ` simplifies:\n- For the Student-t distribution: `k_λ = 1`.\n- For the Variance Gamma (VG) distribution: `k_λ = 2λ / ψ`.\n\nTable 1 below provides the estimated parameters for Goldman Sachs losses under the two models, extracted from the paper's empirical analysis.\n\n**Table 1: Univariate Parameter Estimates for Goldman Sachs Losses**\n| Parameter | Student-t | Variance Gamma (VG) |\n|:----------|:----------|:--------------------|\n| `λ`       | -2.88775  | 1.87933             |\n| `χ`       | 3.77551   | 0.00000             |\n| `ψ`       | 0.00000   | 3.75866             |\n| `μ`       | -0.28039  | -0.25329            |\n| `σ`       | 1.55206   | 1.54198             |\n| `γ`       | 0.22783   | 0.19946             |\n\nTo facilitate the calculation, assume that for the `q=0.999` quantile, the following intermediate values for the `GH_1(λ+1, ...)` distribution have been computed:\n- For the Student-t model: `\\bar{F}_{GH_{1}}(x_{0.999}; λ+1) = 0.0021` and `f_{GH_{1}}(x_{0.999}; λ+1) = 0.0015`.\n- For the VG model: `\\bar{F}_{GH_{1}}(x_{0.999}; λ+1) = 0.0018` and `f_{GH_{1}}(x_{0.999}; λ+1) = 0.0011`.\n\n---\n\n### Question\n\nBased on the provided data and formulas, select all of the following statements that are correct.", "Options": {"A": "If the term `1/(1-q)` were omitted from Eq. (1), the calculated `TCE_{0.999}` for the Student-t model would be approximately 0.198.", "B": "For the VG model, the coefficient `k_λ` is approximately 0.5, which explains its lower TCE value compared to the Student-t model.", "C": "The calculated `TCE_{0.999}` for the Student-t model is approximately 3.81.", "D": "The calculated `TCE_{0.999}` for the Variance Gamma (VG) model is approximately 2.72."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to apply the complex TCE formula from the paper to empirical data for two different models. It uses a Computational Judgment strategy with atomic decomposition. The correct options (A, B) are derived from direct calculation. Distractor C tests a key intermediate step (`k_λ` calculation) using a Formula Misuse error (`k_λ` is 1.0, not 0.5). Distractor D is based on a Step-Omission Error (forgetting the `1/(1-q)` term), a common slip in applying such formulas.", "qid": "234", "question": "### Background\n\n**Research Question.** How does the choice of a specific distribution within the Generalised Hyperbolic (GH) family, even when all choices provide a good statistical fit, quantitatively impact the estimation of extreme tail risk?\n\n**Setting.** A risk manager has fitted two different univariate distributions from the skewed GH family—the Student-t and the Variance Gamma (VG)—to the daily loss data of Goldman Sachs stock. Both models pass standard goodness-of-fit tests, but the manager needs to select one to calculate the Tail Conditional Expectation (TCE) for setting regulatory capital against extreme losses.\n\n**Variables and Parameters.**\n- `X`: A univariate random variable representing the daily loss of Goldman Sachs stock.\n- `TCE_q(X)`: The Tail Conditional Expectation of `X` at quantile level `q`.\n- `μ, γ, σ²`: Location, skewness, and scale parameters of `X`.\n- `λ, χ, ψ`: Parameters of the GIG mixing distribution.\n- `q`: The quantile level, set to 0.999 for an extreme risk scenario.\n- `k_λ`: A constant derived from the GIG mixing distribution.\n\n---\n\n### Data / Model Specification\n\nThe Tail Conditional Expectation for a `GH_1` random variable `X` is given by:\n  \nTCE_q(X) = \\mu + \\frac{\\gamma}{1-q}k_{\\lambda}\\bar{F}_{GH_{1}}(x_{q};\\lambda+1) + \\frac{\\sigma^{2}}{1-q}k_{\\lambda}f_{GH_{1}}(x_{q};\\lambda+1) \\quad \\text{(Eq. (1))}\n \nwhere `f` and `\\bar{F}` are the pdf and survival function, respectively, of a related `GH_1` distribution with parameter `λ+1`.\n\nFor the limiting cases, the coefficient `k_λ` simplifies:\n- For the Student-t distribution: `k_λ = 1`.\n- For the Variance Gamma (VG) distribution: `k_λ = 2λ / ψ`.\n\nTable 1 below provides the estimated parameters for Goldman Sachs losses under the two models, extracted from the paper's empirical analysis.\n\n**Table 1: Univariate Parameter Estimates for Goldman Sachs Losses**\n| Parameter | Student-t | Variance Gamma (VG) |\n|:----------|:----------|:--------------------|\n| `λ`       | -2.88775  | 1.87933             |\n| `χ`       | 3.77551   | 0.00000             |\n| `ψ`       | 0.00000   | 3.75866             |\n| `μ`       | -0.28039  | -0.25329            |\n| `σ`       | 1.55206   | 1.54198             |\n| `γ`       | 0.22783   | 0.19946             |\n\nTo facilitate the calculation, assume that for the `q=0.999` quantile, the following intermediate values for the `GH_1(λ+1, ...)` distribution have been computed:\n- For the Student-t model: `\\bar{F}_{GH_{1}}(x_{0.999}; λ+1) = 0.0021` and `f_{GH_{1}}(x_{0.999}; λ+1) = 0.0015`.\n- For the VG model: `\\bar{F}_{GH_{1}}(x_{0.999}; λ+1) = 0.0018` and `f_{GH_{1}}(x_{0.999}; λ+1) = 0.0011`.\n\n---\n\n### Question\n\nBased on the provided data and formulas, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 389, "Question": "### Background\n\n**Research Question.** Does the degree of information asymmetry affect the magnitude of Seasoned Equity Offering (SEO) underpricing? The paper's central hypothesis is that SEOs by REITs that went public in or after 1990 (\"post-1990\") are more underpriced than those by REITs that went public before 1990 (\"pre-1990\").\n\n**Setting.** The study posits that post-1990 REITs are characterized by greater valuation uncertainty because they are actively managed \"management plays\" with valuable growth options, often use complex UPREIT organizational structures with potential conflicts of interest, and have significantly higher ownership by better-informed institutional investors.\n\n**Variables & Parameters.**\n- `Underpricing (UR)`: The offer-to-close return `(P_1 / P_0 - 1) * 100` (percentage).\n- `LNOFFSIZE`: Natural log of the offer size.\n- `LNINST`: Natural log of (1 + institutional ownership fraction).\n- `UNDRANK`: Underwriter rank (higher rank = better reputation).\n- `UPREIT`: Dummy variable (1 if UPREIT structure).\n\n---\n\n### Data / Model Specification\n\nThe study estimates a multivariate OLS regression for each subsample to test the asymmetric information hypothesis. A representative model is:\n\n  \nUR_{j}=\\beta_{1}+\\beta_{2} UNDRANK_{j} + \\beta_{3} UPREIT_{j} +\\beta_{4}LNINST_{j}+\\beta_{5}LNOFFSIZE_{j}+\\varepsilon_{j} \\quad \\text{(Eq. (1))}\n \n\nKey empirical results are summarized below.\n\n**Table 1. Multivariate OLS Regression Estimates of Underpricing (UR)**\n\n| Variable | Pre-1990 IPO (n=57) | Post-1990 IPO (n=114) |\n| :--- | :--- | :--- |\n| LNOFFSIZE | 1.154 (1.87)* | -0.671 (-2.49)* |\n| LNINST | -0.573 (-1.46) | 0.690 (1.75)* |\n| UNDRANK | -0.207 (-0.88) | -0.221 (-1.84)* |\n| UPREIT | -0.123 (-0.14) | 0.715 (1.98)* |\n\n*Source: Synthesized from Model 2 results in Table 5 of the source document. t-statistics are in parentheses. An asterisk (*) denotes significance at the 10% level or lower.*\n\n---\n\n### Question\n\nBased on the multivariate regression results for the **post-1990 sample** in **Table 1**, select all findings that support the asymmetric information hypothesis.", "Options": {"A": "REITs with the more complex UPREIT structure were associated with significantly more underpricing.", "B": "The size of the offer (`LNOFFSIZE`) was not a statistically significant predictor of underpricing for this group.", "C": "Higher institutional ownership (`LNINST`) was associated with significantly *less* underpricing.", "D": "SEOs with higher-ranked underwriters (`UNDRANK`) were associated with significantly less underpricing."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the core multivariate findings supporting the paper's central hypothesis. It uses an **Atomic Decomposition** strategy, breaking the overall conclusion into testable propositions about individual regression coefficients. Correct options (A, B) accurately state significant findings consistent with the theory (reputable underwriters and simpler structures reduce asymmetry and thus underpricing). Distractor C is a **Conceptual Opposite** (reversing the sign of the LNINST coefficient), while Distractor D is a **Factual Error** (LNOFFSIZE was significant).", "qid": "389", "question": "### Background\n\n**Research Question.** Does the degree of information asymmetry affect the magnitude of Seasoned Equity Offering (SEO) underpricing? The paper's central hypothesis is that SEOs by REITs that went public in or after 1990 (\"post-1990\") are more underpriced than those by REITs that went public before 1990 (\"pre-1990\").\n\n**Setting.** The study posits that post-1990 REITs are characterized by greater valuation uncertainty because they are actively managed \"management plays\" with valuable growth options, often use complex UPREIT organizational structures with potential conflicts of interest, and have significantly higher ownership by better-informed institutional investors.\n\n**Variables & Parameters.**\n- `Underpricing (UR)`: The offer-to-close return `(P_1 / P_0 - 1) * 100` (percentage).\n- `LNOFFSIZE`: Natural log of the offer size.\n- `LNINST`: Natural log of (1 + institutional ownership fraction).\n- `UNDRANK`: Underwriter rank (higher rank = better reputation).\n- `UPREIT`: Dummy variable (1 if UPREIT structure).\n\n---\n\n### Data / Model Specification\n\nThe study estimates a multivariate OLS regression for each subsample to test the asymmetric information hypothesis. A representative model is:\n\n  \nUR_{j}=\\beta_{1}+\\beta_{2} UNDRANK_{j} + \\beta_{3} UPREIT_{j} +\\beta_{4}LNINST_{j}+\\beta_{5}LNOFFSIZE_{j}+\\varepsilon_{j} \\quad \\text{(Eq. (1))}\n \n\nKey empirical results are summarized below.\n\n**Table 1. Multivariate OLS Regression Estimates of Underpricing (UR)**\n\n| Variable | Pre-1990 IPO (n=57) | Post-1990 IPO (n=114) |\n| :--- | :--- | :--- |\n| LNOFFSIZE | 1.154 (1.87)* | -0.671 (-2.49)* |\n| LNINST | -0.573 (-1.46) | 0.690 (1.75)* |\n| UNDRANK | -0.207 (-0.88) | -0.221 (-1.84)* |\n| UPREIT | -0.123 (-0.14) | 0.715 (1.98)* |\n\n*Source: Synthesized from Model 2 results in Table 5 of the source document. t-statistics are in parentheses. An asterisk (*) denotes significance at the 10% level or lower.*\n\n---\n\n### Question\n\nBased on the multivariate regression results for the **post-1990 sample** in **Table 1**, select all findings that support the asymmetric information hypothesis.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 397, "Question": "### Background\n\nThe well-posedness of the Thiele PDE model for a surplus-linked insurance reserve depends on a set of technical and economic assumptions about the coefficient functions.\n\n### Data / Model Specification\n\nThe paper presents Hypothesis 1, which includes the following core assumptions:\n\n*   **(i)(a) Uniform Ellipticity:** There exists a `\\pi_0 > 0` such that `\\pi(t,x) \\ge \\pi_0`, where `\\pi(t,x)` is the share of surplus invested in a risky asset. This ensures the coefficient of the `\\partial_x^2` term in the PDE, `(1/2)\\pi(t,x)^2\\sigma^2x^2`, is strictly positive.\n*   **(iii)(a) Linear Dividend Bound:** There exists a `k > 0` such that `0 \\le \\delta^j(t,x) \\le kx`, where `\\delta^j(t,x)` are dividend payments.\n*   **(iii)(c) Surplus Non-negativity:** `x + c^{jk}(t) - \\delta^{jk}(t,x) \\ge 0`, ensuring the surplus remains non-negative after cash flows from state transitions.\n\nThe paper notes that the uniform ellipticity assumption can be relaxed to `\\pi(x) \\ge 0`, which makes the PDE degenerate where `\\pi(x) = 0`.\n\n---\n\nWhich of the following statements represent valid mathematical or economic consequences of these assumptions, or valid critiques of their realism?\n", "Options": {"A": "A Constant Proportion Portfolio Insurance (CPPI) strategy, where risky asset allocation can become zero if the surplus hits a floor, would violate the uniform ellipticity assumption.", "B": "The surplus non-negativity assumption (iii)(c) ensures that the arguments of the reserve function `V` remain within its domain (`\\mathbb{R}_+`) after state transitions.", "C": "A dividend policy that pays a fixed amount (e.g., $1000) as long as the surplus `x` is above a threshold would violate the linear dividend bound assumption (iii)(a).", "D": "The uniform ellipticity assumption (i)(a) ensures the PDE is non-degenerate parabolic, which is a standard condition for the existence of a smooth, classical solution."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to connect the paper's formal mathematical assumptions to their technical purpose (e.g., ensuring non-degeneracy) and their economic realism. Strategy: Scenario Application. The student must apply the abstract assumptions to concrete financial strategies (CPPI, fixed dividends) to evaluate their validity. All four options are correct statements drawn from the original QA problem, requiring the student to understand the role of each assumption and identify realistic scenarios where they might fail.", "qid": "397", "question": "### Background\n\nThe well-posedness of the Thiele PDE model for a surplus-linked insurance reserve depends on a set of technical and economic assumptions about the coefficient functions.\n\n### Data / Model Specification\n\nThe paper presents Hypothesis 1, which includes the following core assumptions:\n\n*   **(i)(a) Uniform Ellipticity:** There exists a `\\pi_0 > 0` such that `\\pi(t,x) \\ge \\pi_0`, where `\\pi(t,x)` is the share of surplus invested in a risky asset. This ensures the coefficient of the `\\partial_x^2` term in the PDE, `(1/2)\\pi(t,x)^2\\sigma^2x^2`, is strictly positive.\n*   **(iii)(a) Linear Dividend Bound:** There exists a `k > 0` such that `0 \\le \\delta^j(t,x) \\le kx`, where `\\delta^j(t,x)` are dividend payments.\n*   **(iii)(c) Surplus Non-negativity:** `x + c^{jk}(t) - \\delta^{jk}(t,x) \\ge 0`, ensuring the surplus remains non-negative after cash flows from state transitions.\n\nThe paper notes that the uniform ellipticity assumption can be relaxed to `\\pi(x) \\ge 0`, which makes the PDE degenerate where `\\pi(x) = 0`.\n\n---\n\nWhich of the following statements represent valid mathematical or economic consequences of these assumptions, or valid critiques of their realism?\n", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 165, "Question": "### Background\nThe paper analyzes a scenario where an insurer offers premium rebates for a deductible that are partially risk-rated based on age and gender. This creates 18 distinct premium risk groups. The simulation parameters are fixed at `m=0.05` (moral hazard reduction) and `r=0.003` (risk aversion).\n\n### Data / Model Specification\nThe adverse selection spiral can now occur *within* each premium risk group. A spiral is avoided within a group only if the group-specific offered premium rebate is high enough to attract the highest-risk members of that specific group.\n\n**Table 1: Percentage of Insured Women Taking a Deductible, by Age Group**\n(`m=0.05`, `r=0.003`)\n\n| Age Group | Year 1 | Year 10 |\n| :--- | :--- | :--- |\n| 0-9 | 97% | 0% |\n| 10-19 | 97% | 0% |\n| 20-29 | 97% | 0% |\n| 30-39 | 96% | 0% |\n| 40-49 | 94% | 0% |\n| 50-59 | 94% | 0% |\n| 60-69 | 98% | 57% |\n| 70-79 | 100% | 100% |\n| 80+ | 100% | 100% |\n\n---\n\nBased on the data in Table 1 and the model of partial risk-rating, which of the following statements are valid inferences?\n", "Options": {"A": "The initial offered premium rebate for the \"80+\" age group must have been greater than the highest demanded premium rebate from any individual within that same group.", "B": "Partial risk-rating successfully eliminates the adverse selection spiral for the insurance pool as a whole.", "C": "The initial offered premium rebate for the \"20-29\" age group was lower than the initial offered premium rebate for the \"70-79\" age group.", "D": "For the \"60-69\" age group, the offered premium rebate in Year 2 was lower than in Year 1, but the rebate stabilized at a level sufficient to retain a majority of the group by Year 10."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to apply the general theory of the adverse selection spiral to the specific case of partial risk-rating, using the provided table as evidence. It uses a 'Reverse-Reasoning' strategy, requiring the user to infer the underlying conditions that must have produced the observed outcomes.\n- **A (Correct):** The only way for participation to be 100% in Year 1 and remain stable is if the offered rebate exceeds the demanded rebate of every single member, including the one with the highest demand (`DPR_max`).\n- **B (Distractor - Conceptual Opposite):** The table clearly shows the spiral is *not* eliminated; it is merely localized. Six of the nine groups shown collapse to 0% participation. This is a key finding of the paper.\n- **C (Correct):** The offered rebate is based on the average expected costs of the group. Since older age groups have much higher average health costs, the rebate offered to the \"70-79\" group must have been significantly higher than the one offered to the much healthier \"20-29\" group.\n- **D (Distractor - Almost Right):** The first part is correct; the drop from 98% to 57% implies the rebate must have decreased. However, the paper states that the spiral continues until participation is 0% or 100% within a group. The 57% is a snapshot in Year 10; the spiral is still ongoing and will eventually lead to 0% participation for this group as well, so it has not 'stabilized'.", "qid": "165", "question": "### Background\nThe paper analyzes a scenario where an insurer offers premium rebates for a deductible that are partially risk-rated based on age and gender. This creates 18 distinct premium risk groups. The simulation parameters are fixed at `m=0.05` (moral hazard reduction) and `r=0.003` (risk aversion).\n\n### Data / Model Specification\nThe adverse selection spiral can now occur *within* each premium risk group. A spiral is avoided within a group only if the group-specific offered premium rebate is high enough to attract the highest-risk members of that specific group.\n\n**Table 1: Percentage of Insured Women Taking a Deductible, by Age Group**\n(`m=0.05`, `r=0.003`)\n\n| Age Group | Year 1 | Year 10 |\n| :--- | :--- | :--- |\n| 0-9 | 97% | 0% |\n| 10-19 | 97% | 0% |\n| 20-29 | 97% | 0% |\n| 30-39 | 96% | 0% |\n| 40-49 | 94% | 0% |\n| 50-59 | 94% | 0% |\n| 60-69 | 98% | 57% |\n| 70-79 | 100% | 100% |\n| 80+ | 100% | 100% |\n\n---\n\nBased on the data in Table 1 and the model of partial risk-rating, which of the following statements are valid inferences?\n", "model_evaluation_result": {"model_raw_response": "\n\nA,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 264, "Question": "### Background\n\n**Research Question.** This study tests whether government-imposed barriers to international investment are effective in segmenting capital markets by analyzing the price behavior of closed-end country funds around announcements of changes in these restrictions.\n\n**Setting / Data-Generating Environment.** The study uses an event-study methodology with weekly data for five country funds. The central hypothesis is that an announcement of a liberalization of investment restrictions will cause a fund's premium to decrease as the barrier to investment is removed.\n\n---\n\n### Data / Model Specification\n\nThe core empirical test is the following regression model, estimated separately for each fund `j`:\n\n  \n\\pi_{j t}=\\delta_{0j}+\\delta_{1j}D_{1j t}+\\delta_{2j}D_{2j t}+\\delta_{3j}D_{3j t}+\\epsilon_{j t} \\quad \\text{(Eq. 1)}\n \n\nwhere `π_jt` is the weekly premium change and `D_kjt` are dummies for windows around a liberalization announcement. The key coefficient is `δ_2j`, measuring the effect during the announcement window.\n\n**Table 1: Regression Results for Changes in Fund Premiums**\n\n| Fund | `δ_1j` | `δ_2j` | `δ_3j` |\n| :--- | :--- | :--- | :--- |\n| France | -1.32 (0.09) | -3.85 (0.03) | 0.41 (0.67) |\n| Japan | -0.85 (0.02) | -0.46 (0.45) | 0.19 (0.66) |\n| Korea | -2.74 (0.01) | -3.11 (0.02) | 0.06 (0.62) |\n| Mexico | -1.13 (0.21) | -5.41 (0.00) | 0.71 (0.94) |\n| Taiwan | -0.42 (0.56) | 0.82 (0.56) | -0.10 (0.39) |\n\n*Source: Adapted from Table IV of the source paper. One-sided p-values are in parentheses.*\n\n---\n\n### Question\n\nBased on the findings in **Table 1** and the paper's overall argument, select all conclusions that are strongly supported by the evidence.", "Options": {"A": "The results imply that for countries with previously binding restrictions, the cost of capital for local projects would likely change as markets become more integrated post-liberalization.", "B": "The positive `δ_3j` coefficient for France (0.41) indicates a statistically significant price reversal in the post-announcement period, questioning the long-term impact of the liberalization.", "C": "The significant premium declines for funds like Mexico and Korea following liberalization announcements suggest that government-imposed barriers were effective in segmenting international capital markets.", "D": "The lack of a significant effect for the Japan Fund during the event window (`δ_2j` p-value = 0.45) proves that its capital markets were fully integrated with global markets."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect the specific empirical results to the paper's main theoretical conclusions. It uses a 'Premise/Assumption Packaging' strategy. Options A and B are the two central, correct conclusions drawn from the significant negative coefficients. Distractor C is a 'Conceptual' error, asking the student to correctly interpret statistical insignificance (p=0.67). Distractor D is an 'Almost Right' conceptual error; it makes too strong a claim ('proves') and ignores the significant pre-announcement effect for Japan, which suggests anticipation rather than perfect integration.", "qid": "264", "question": "### Background\n\n**Research Question.** This study tests whether government-imposed barriers to international investment are effective in segmenting capital markets by analyzing the price behavior of closed-end country funds around announcements of changes in these restrictions.\n\n**Setting / Data-Generating Environment.** The study uses an event-study methodology with weekly data for five country funds. The central hypothesis is that an announcement of a liberalization of investment restrictions will cause a fund's premium to decrease as the barrier to investment is removed.\n\n---\n\n### Data / Model Specification\n\nThe core empirical test is the following regression model, estimated separately for each fund `j`:\n\n  \n\\pi_{j t}=\\delta_{0j}+\\delta_{1j}D_{1j t}+\\delta_{2j}D_{2j t}+\\delta_{3j}D_{3j t}+\\epsilon_{j t} \\quad \\text{(Eq. 1)}\n \n\nwhere `π_jt` is the weekly premium change and `D_kjt` are dummies for windows around a liberalization announcement. The key coefficient is `δ_2j`, measuring the effect during the announcement window.\n\n**Table 1: Regression Results for Changes in Fund Premiums**\n\n| Fund | `δ_1j` | `δ_2j` | `δ_3j` |\n| :--- | :--- | :--- | :--- |\n| France | -1.32 (0.09) | -3.85 (0.03) | 0.41 (0.67) |\n| Japan | -0.85 (0.02) | -0.46 (0.45) | 0.19 (0.66) |\n| Korea | -2.74 (0.01) | -3.11 (0.02) | 0.06 (0.62) |\n| Mexico | -1.13 (0.21) | -5.41 (0.00) | 0.71 (0.94) |\n| Taiwan | -0.42 (0.56) | 0.82 (0.56) | -0.10 (0.39) |\n\n*Source: Adapted from Table IV of the source paper. One-sided p-values are in parentheses.*\n\n---\n\n### Question\n\nBased on the findings in **Table 1** and the paper's overall argument, select all conclusions that are strongly supported by the evidence.", "model_evaluation_result": {"model_raw_response": "\n\nA,C\n\n## Explanation\n\nTo evaluate the conclusions, the reasoning follows these steps based on the regression results in Table 1 and the study's hypothesis that liberalization announcements reduce fund premiums by removing investment barriers.\n\n1. **Assess Option A**: The significant negative δ₂j coefficients (e.g., -5.41 for Mexico, p=0.00; -3.11 for Korea, p=0.02) indicate premium declines for funds in restricted markets, supporting market integration. This aligns with financial theory, where reduced segmentation lowers risk premia and thus the cost of capital for local projects, making the conclusion strongly supported.\n\n2. **Assess Option B**: The δ₃j coefficient for France is 0.41 with a p-value of 0.67, which exceeds typical significance thresholds (e.g., 0.05 or 0.10), indicating no statistical significance. Therefore, it does not support a price reversal, and the conclusion is not strongly supported.\n\n3. **Assess Option C**: The large negative and significant δ₂j values for Mexico and Korea demonstrate premium reductions during announcement windows, consistent with barriers previously causing market segmentation. This directly validates\n> 🔍 **Analyzing regression results**\n> ***- Table 1 shows regression results for fund premiums around liberalization announcements.***\n the hypothesis, making the conclusion strongly supported.\n\n4. **Assess Option D**: The non-significant δ₂j for Japan (-0.46, p=0.45) fails to reject the null hypothesis of no effect but does not prove full integration, as it could reflect other factors (e.g., market anticipation). Thus, the conclusion is not strongly supported.\n\nThis analysis identifies A and C as the conclusions backed by the evidence.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 181, "Question": "### Background\n\n**Research Question.** How can the risk-adjusted performance of different equity styles be evaluated to test the behavioral proposition that investor return-chasing leads to predictable cycles of outperformance for 'in-favor' styles and underperformance for their 'twin' styles?\n\n**Setting and Data.** The analysis uses monthly returns for four Fama-French (FF) style benchmarks and the Wilshire 5000 market index from January 1979 to December 2004.\n\n---\n\n### Data / Model Specification\n\nThe Hansen-Jagannathan (HJ) bound is a theoretical condition in asset pricing which states that for any asset `i`, its Sharpe Ratio must be less than or equal to the maximum possible Sharpe Ratio in the economy, `SHP_max`. If the Capital Asset Pricing Model (CAPM) holds, the market portfolio is assumed to be mean-variance efficient, implying its Sharpe Ratio is this maximum (`SHP_M = SHP_max`).\n\n**Table 1: Performance Summary of Fama-French Style Benchmarks (1979-2004)**\n\n| Benchmark | Sharpe Ratio (SHP) | Z-Stat vs. Market |\n| :--- | :--- | :--- |\n| Wilshire 5000 (Market) | 0.461 | - |\n| FF Small Value (FFSV) | 0.831 | 4.201 |\n\n---\n\n### Question\n\nThe finding that the FF Small Value (FFSV) portfolio has a Sharpe Ratio statistically superior to the market proxy has significant implications for asset pricing theory. Based on the data in **Table 1** and the principles of the Hansen-Jagannathan (HJ) bound, select all valid conclusions.", "Options": {"A": "The result directly contradicts the premise that the market portfolio, as proxied by the Wilshire 5000, is mean-variance efficient.", "B": "The FFSV portfolio must be perfectly correlated with the true Stochastic Discount Factor (SDF).", "C": "The existence of the FFSV portfolio implies that the true Stochastic Discount Factor (SDF) cannot be represented solely as a linear function of the market portfolio's return.", "D": "The FFSV portfolio's high Sharpe Ratio confirms that the single-factor CAPM is a sufficient model for explaining returns in this dataset."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item tests deep theoretical knowledge by connecting an empirical result to fundamental asset pricing theory (HJ bound, SDF). It uses a Reverse-Reasoning strategy, asking for the theoretical implications of an observed outcome. Correct options A and B are the two key steps in the proof that the CAPM is violated. Distractor C is a Conceptual Opposite. Distractor D is a sophisticated 'Almost Right' error; the portfolio that *achieves* the maximum Sharpe ratio is perfectly correlated with the SDF, but FFSV merely proves the market is not that portfolio—we don't know if FFSV itself is.", "qid": "181", "question": "### Background\n\n**Research Question.** How can the risk-adjusted performance of different equity styles be evaluated to test the behavioral proposition that investor return-chasing leads to predictable cycles of outperformance for 'in-favor' styles and underperformance for their 'twin' styles?\n\n**Setting and Data.** The analysis uses monthly returns for four Fama-French (FF) style benchmarks and the Wilshire 5000 market index from January 1979 to December 2004.\n\n---\n\n### Data / Model Specification\n\nThe Hansen-Jagannathan (HJ) bound is a theoretical condition in asset pricing which states that for any asset `i`, its Sharpe Ratio must be less than or equal to the maximum possible Sharpe Ratio in the economy, `SHP_max`. If the Capital Asset Pricing Model (CAPM) holds, the market portfolio is assumed to be mean-variance efficient, implying its Sharpe Ratio is this maximum (`SHP_M = SHP_max`).\n\n**Table 1: Performance Summary of Fama-French Style Benchmarks (1979-2004)**\n\n| Benchmark | Sharpe Ratio (SHP) | Z-Stat vs. Market |\n| :--- | :--- | :--- |\n| Wilshire 5000 (Market) | 0.461 | - |\n| FF Small Value (FFSV) | 0.831 | 4.201 |\n\n---\n\n### Question\n\nThe finding that the FF Small Value (FFSV) portfolio has a Sharpe Ratio statistically superior to the market proxy has significant implications for asset pricing theory. Based on the data in **Table 1** and the principles of the Hansen-Jagannathan (HJ) bound, select all valid conclusions.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"implications of portfolio with higher Sharpe ratio than market for CAPM and Hansen-Jagannathan bound\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Hansen–Jagannathan bound: Explained | TIOmarkets\" TIOmarkets](https://tiomarkets.com/en/article/hansen-jagannathan-bound-guide)\n> [🔗 \"sharpe ratio - Intuitive explanation of the Hansen-Jagannathan bound - Quantitative Finance Stack Exchange\" Stack Exchange](https://quant.stackexchange.com/questions/9340/intuitive-explanation-of-the-hansen-jagannathan-bound)\n> [🔗 \"Hansen–Jagannathan bound - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Hansen%25E2%2580%2593Jagannathan_bound)\n> [🔗 \"Sharpe Ratio: Definition, Formula, and Examples\" Investopedia](https://www.investopedia.com/terms/s/sharperatio.asp)\n> [🔗 \"Sharpe ratio - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Sharpe_ratio)\n> [🔗 \"Lecture 03: Sharpe Ratio, Bounds and the Equity Premium ...\" Princeton University](https://www.princeton.edu/~markus/teaching/Eco525/03%252520Sharpe%252520Ratio_Bounds_Equity%252520Premium_a.pdf)\n> [🔗 \"equities - Existence of an upper bound for risk-factor betas/coefficients - Quantitative Finance Stack Exchange\" Stack Exchange](https://quant.stackexchange.com/questions/74070/existence-of-an-upper-bound-for-risk-factor-betas-coefficients)\n> [🔗 \"Measuring a Portfolio's Performance\" Investopedia](https://www.investopedia.com/articles/08/performance-measure.asp)\n> [🔗 \"What is Hansen–Jagannathan bound | Capital.com\" Capital](https://capital.com/hansen-jagannathan-bound-definition)\n> [🔗 \"In-sample and out-of-sample Sharpe ratios of multi-factor asset pricing models - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304405X24000606)\n\n\nA,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 406, "Question": "### Background\n\n**Research Question.** Beyond empirical costs, the optimal design of sovereign debt issuance requires navigating theoretical risks and institutional constraints. The choice between a sealed-bid auction and a fixed-price offering is state-dependent, with each mechanism having advantages in different market environments.\n\n**Setting / Data-Generating Environment.** The Treasury has raised several institutional objections to extending auctions to long-term securities, including: (i) vulnerability in weak markets due to investor uncertainty, (ii) the potential to encourage undesirable speculation, and (iii) tax complications that could harm market liquidity.\n\n### Data / Model Specification\n\nUnder the OID (Original Issue Discount) rule, if a bond is issued at a price `P_i` below a certain threshold, a secondary buyer's future tax liability depends on `P_i`. The taxable ordinary income is calculated pro-rata:\n\n  \n\\text{Taxable Ordinary Income} = \\frac{m}{M}(100-P_i) \n\\quad \\text{(Eq. (1))}\n \n\nwhere `M` is the total months to maturity and `m` is the holding period of the secondary buyer.\n\n### Question\n\nAccording to the paper's analysis of institutional arguments, which of the following are valid concerns or counter-arguments regarding the use of sealed-bid auctions for long-term securities? Select all that apply.", "Options": {"A": "The author argues that auctions can curtail speculation by eliminating the time lag, forcing competitors to bid away potential profits and transfer them to the Treasury.", "B": "The Original Issue Discount (OID) tax rule, when applied to a multi-price auction, would create non-fungible securities with different tax implications for secondary buyers, harming market liquidity.", "C": "Auctions may increase speculation because the time lag between the price announcement and the sale creates arbitrage opportunities for non-professional investors.", "D": "In weak markets with high uncertainty, the winner's curse is amplified, potentially leading rational bidders to bid timidly or abstain, thus raising Treasury's costs."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the understanding of three key institutional arguments discussed in the paper. It uses a 'select-all-that-apply' format to test comprehension of multiple distinct concepts from the text. Distractor B is a high-fidelity 'Conceptual Opposite,' as it correctly describes a mechanism for speculation but attributes it to the wrong issuance type (auctions instead of fixed-price offerings).", "qid": "406", "question": "### Background\n\n**Research Question.** Beyond empirical costs, the optimal design of sovereign debt issuance requires navigating theoretical risks and institutional constraints. The choice between a sealed-bid auction and a fixed-price offering is state-dependent, with each mechanism having advantages in different market environments.\n\n**Setting / Data-Generating Environment.** The Treasury has raised several institutional objections to extending auctions to long-term securities, including: (i) vulnerability in weak markets due to investor uncertainty, (ii) the potential to encourage undesirable speculation, and (iii) tax complications that could harm market liquidity.\n\n### Data / Model Specification\n\nUnder the OID (Original Issue Discount) rule, if a bond is issued at a price `P_i` below a certain threshold, a secondary buyer's future tax liability depends on `P_i`. The taxable ordinary income is calculated pro-rata:\n\n  \n\\text{Taxable Ordinary Income} = \\frac{m}{M}(100-P_i) \n\\quad \\text{(Eq. (1))}\n \n\nwhere `M` is the total months to maturity and `m` is the holding period of the secondary buyer.\n\n### Question\n\nAccording to the paper's analysis of institutional arguments, which of the following are valid concerns or counter-arguments regarding the use of sealed-bid auctions for long-term securities? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 413, "Question": "### Background\n\n**Research Question:** This case requires an interpretation of diagnostic tests for a BEKK-MVGARCH model to assess its adequacy. Model diagnostics are crucial for determining if the model's assumptions are met and its conclusions are reliable.\n\n### Data / Model Specification\n\nThe model consists of a VAR(1) mean equation and a BEKK(1,1) variance equation. After estimation, Ljung-Box Q-statistics are calculated for the standardized residuals (`Q(12)`) and squared standardized residuals (`Q²(12)`) to test for remaining serial correlation.\n\n- A significant `Q(12)` statistic suggests the mean equation is misspecified.\n- A significant `Q²(12)` statistic suggests the variance equation is misspecified.\n\n**Table 1: BEKK-MVGARCH Diagnostic Results**\n\n| Market | Test | Statistic | p-value |\n| :--- | :--- | :--- | :--- |\n| Shanghai | `Q(12)` | 48.73 | 0.00 |\n| | `Q²(12)` | 7.08 | 0.93 |\n| Hong Kong | `Q(12)` | 24.51 | 0.22 |\n| | `Q²(12)` | 41.10 | 0.01 |\n\n### Question\n\nBased on the diagnostic results in **Table 1**, select all of the following statements that are correct interpretations of model misspecification.\n", "Options": {"A": "The mean equation for the Shanghai market is misspecified.", "B": "The mean equation for the Hong Kong market is misspecified.", "C": "The variance equation for the Shanghai market is misspecified.", "D": "The variance equation for the Hong Kong market is misspecified."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the ability to correctly interpret standard diagnostic tests for a time series model. It uses a select-all-that-apply format to test for comprehensive understanding. The correct answers (A, D) correspond to the statistically significant p-values (0.00 and 0.01). The distractors (B, C) correspond to the insignificant p-values and represent a failure to correctly apply the decision rule for hypothesis testing.", "qid": "413", "question": "### Background\n\n**Research Question:** This case requires an interpretation of diagnostic tests for a BEKK-MVGARCH model to assess its adequacy. Model diagnostics are crucial for determining if the model's assumptions are met and its conclusions are reliable.\n\n### Data / Model Specification\n\nThe model consists of a VAR(1) mean equation and a BEKK(1,1) variance equation. After estimation, Ljung-Box Q-statistics are calculated for the standardized residuals (`Q(12)`) and squared standardized residuals (`Q²(12)`) to test for remaining serial correlation.\n\n- A significant `Q(12)` statistic suggests the mean equation is misspecified.\n- A significant `Q²(12)` statistic suggests the variance equation is misspecified.\n\n**Table 1: BEKK-MVGARCH Diagnostic Results**\n\n| Market | Test | Statistic | p-value |\n| :--- | :--- | :--- | :--- |\n| Shanghai | `Q(12)` | 48.73 | 0.00 |\n| | `Q²(12)` | 7.08 | 0.93 |\n| Hong Kong | `Q(12)` | 24.51 | 0.22 |\n| | `Q²(12)` | 41.10 | 0.01 |\n\n### Question\n\nBased on the diagnostic results in **Table 1**, select all of the following statements that are correct interpretations of model misspecification.\n", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 374, "Question": "### Background\n\n**Research Question.** What combination of misaligned incentives and fragile financial engineering led to the high rate of LBO defaults in the late 1980s, and how did the market correct these flaws in the early 1990s?\n\n**Setting.** The case examines the structural evolution of LBOs, contrasting the aggressive, high-failure deals of 1985-1989 with the more conservative, restructured deals of the early 1990s. The analysis focuses on the role of purchase prices, capital structure, and promoter incentives as documented by Kaplan and Stein.\n\n**Variables & Parameters.**\n- `P`: Purchase Price (monetary units).\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization (monetary units).\n- `D`: Total Debt (monetary units).\n- `E`: Total Equity (monetary units).\n- `I`: Interest Expense (monetary units).\n- `C_p`: Purchase Multiple, `P / EBITDA` (dimensionless).\n- `E_pct`: Equity Percentage, `E / (D+E)` (dimensionless).\n- `ICR`: Interest Coverage Ratio, `EBITDA / I` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Evolution of LBO Financial Structures**\n\n| Metric | Late 1980s (1986-88) | Early 1990s (1992-95) |\n| :--- | :--- | :--- |\n| Purchase Multiple (`C_p`) | 7.0x - 10.0x | 5.0x - 6.0x |\n| Equity Percentage (`E_pct`) | 5% - 10% | 20% - 30% |\n| Interest Coverage (`ICR`) | Often < 1.0x | Minimum of 2.0x |\n\n---\n\n### Question\n\nConsider a company with a stable `EBITDA` of $200 million and an average interest rate on debt of 10%. Based on the data in **Table 1**, select all of the following statements that are correct when comparing a representative \"late-1980s\" deal (8.0x purchase multiple, 10% equity) with a representative \"early-1990s\" deal (5.5x purchase multiple, 25% equity).", "Options": {"A": "The total debt in the late-1980s deal is $1,440 million.", "B": "The Interest Coverage Ratio (ICR) for the early-1990s deal is approximately 2.42x.", "C": "The early-1990s deal structure has a higher debt-to-equity ratio than the late-1980s deal.", "D": "The annual interest expense for the late-1980s deal is $160 million."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform multi-step calculations based on table data and draw accurate comparisons, a core quantitative skill. It uses a 'Computational Judgment' strategy with 'Atomic Decomposition' to create multiple correct, independently verifiable statements. Distractor C is a 'Step-Omission Error' (confusing equity with interest expense), and Distractor D is a 'Conceptual Opposite' regarding leverage trends.", "qid": "374", "question": "### Background\n\n**Research Question.** What combination of misaligned incentives and fragile financial engineering led to the high rate of LBO defaults in the late 1980s, and how did the market correct these flaws in the early 1990s?\n\n**Setting.** The case examines the structural evolution of LBOs, contrasting the aggressive, high-failure deals of 1985-1989 with the more conservative, restructured deals of the early 1990s. The analysis focuses on the role of purchase prices, capital structure, and promoter incentives as documented by Kaplan and Stein.\n\n**Variables & Parameters.**\n- `P`: Purchase Price (monetary units).\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization (monetary units).\n- `D`: Total Debt (monetary units).\n- `E`: Total Equity (monetary units).\n- `I`: Interest Expense (monetary units).\n- `C_p`: Purchase Multiple, `P / EBITDA` (dimensionless).\n- `E_pct`: Equity Percentage, `E / (D+E)` (dimensionless).\n- `ICR`: Interest Coverage Ratio, `EBITDA / I` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Evolution of LBO Financial Structures**\n\n| Metric | Late 1980s (1986-88) | Early 1990s (1992-95) |\n| :--- | :--- | :--- |\n| Purchase Multiple (`C_p`) | 7.0x - 10.0x | 5.0x - 6.0x |\n| Equity Percentage (`E_pct`) | 5% - 10% | 20% - 30% |\n| Interest Coverage (`ICR`) | Often < 1.0x | Minimum of 2.0x |\n\n---\n\n### Question\n\nConsider a company with a stable `EBITDA` of $200 million and an average interest rate on debt of 10%. Based on the data in **Table 1**, select all of the following statements that are correct when comparing a representative \"late-1980s\" deal (8.0x purchase multiple, 10% equity) with a representative \"early-1990s\" deal (5.5x purchase multiple, 25% equity).", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 407, "Question": "### Background\n\n**Research Question.** This case dissects the economic mechanism behind the finding that increased board independence leads to a reduction in Corporate Social Responsibility (CSR). It investigates which specific types of CSR are cut and their impact on firm value to build a cohesive narrative supporting the agency cost hypothesis.\n\n**Setting.** The analysis builds on a primary finding from a difference-in-differences (DiD) study showing that the Sarbanes-Oxley Act (SOX), by forcing some firms to increase board independence, caused a reduction in their overall CSR engagement. This case examines a series of follow-on tests to understand the nuances of this effect.\n\n**Variables & Parameters.**\n\n*   `CSR score`: The composite measure, defined as `CSR strengths - CSR concerns`.\n*   `CSR strengths`: A count of a firm's positive, proactive CSR activities (e.g., community development, environmental initiatives).\n*   `CSR concerns`: A count of a firm's negative CSR-related incidents (e.g., environmental fines, employee lawsuits).\n*   `Tobin's Q`: A proxy for firm value.\n\n---\n\n### Data / Model Specification\n\nThe analysis relies on results from two separate panel regressions, presented below.\n\n**Table 1: DiD Effect on CSR Strengths and Concerns**\n\nThis table shows the results of the DiD model run separately on the two components of the CSR score.\n\n| Dependent Variable | `Noncompliant × post-SOX` Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| CSR strengths | -0.587 | (-2.98) |\n| CSR concerns | 0.080 | (0.53) |\n\n**Table 2: The Effects of CSR Strengths and Concerns on Firm Value**\n\nThis table shows results from a regression of Tobin's Q on the CSR components, including firm and year fixed effects.\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `CSR strengths` | -0.044 | (-2.89) |\n| `CSR concerns` | 0.009 | (0.47) |\n\n---\n\n### Question\n\nBased on the evidence provided, which of the following statements accurately synthesize the findings regarding the impact of increased board independence on CSR components and firm value? Select all that apply.", "Options": {"A": "The reduction in the composite CSR score is statistically attributable to a significant decrease in CSR strengths.", "B": "CSR strengths are associated with a statistically significant reduction in firm value, as measured by Tobin's Q.", "C": "The findings suggest that CSR concerns, unlike CSR strengths, are positively and significantly associated with firm value.", "D": "The analysis shows that independent boards cause a statistically significant increase in CSR concerns."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the ability to synthesize findings from two separate analyses (CSR component decomposition and valuation). It uses an 'Atomic Decomposition' strategy, breaking the paper's narrative into independently verifiable claims. Distractors are based on misinterpreting statistical significance (C) and the sign of the valuation effect (D), representing common comprehension errors.", "qid": "407", "question": "### Background\n\n**Research Question.** This case dissects the economic mechanism behind the finding that increased board independence leads to a reduction in Corporate Social Responsibility (CSR). It investigates which specific types of CSR are cut and their impact on firm value to build a cohesive narrative supporting the agency cost hypothesis.\n\n**Setting.** The analysis builds on a primary finding from a difference-in-differences (DiD) study showing that the Sarbanes-Oxley Act (SOX), by forcing some firms to increase board independence, caused a reduction in their overall CSR engagement. This case examines a series of follow-on tests to understand the nuances of this effect.\n\n**Variables & Parameters.**\n\n*   `CSR score`: The composite measure, defined as `CSR strengths - CSR concerns`.\n*   `CSR strengths`: A count of a firm's positive, proactive CSR activities (e.g., community development, environmental initiatives).\n*   `CSR concerns`: A count of a firm's negative CSR-related incidents (e.g., environmental fines, employee lawsuits).\n*   `Tobin's Q`: A proxy for firm value.\n\n---\n\n### Data / Model Specification\n\nThe analysis relies on results from two separate panel regressions, presented below.\n\n**Table 1: DiD Effect on CSR Strengths and Concerns**\n\nThis table shows the results of the DiD model run separately on the two components of the CSR score.\n\n| Dependent Variable | `Noncompliant × post-SOX` Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| CSR strengths | -0.587 | (-2.98) |\n| CSR concerns | 0.080 | (0.53) |\n\n**Table 2: The Effects of CSR Strengths and Concerns on Firm Value**\n\nThis table shows results from a regression of Tobin's Q on the CSR components, including firm and year fixed effects.\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `CSR strengths` | -0.044 | (-2.89) |\n| `CSR concerns` | 0.009 | (0.47) |\n\n---\n\n### Question\n\nBased on the evidence provided, which of the following statements accurately synthesize the findings regarding the impact of increased board independence on CSR components and firm value? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 401, "Question": "### Background\n\n**Research Question.** This case investigates the relative importance of three investor characteristics—sophistication, experience (proxied by age), and the use of stop-loss orders—in mitigating the disposition effect. The analysis aims to determine which factors remain significant when all are considered simultaneously.\n\n**Setting / Data-Generating Environment.** The analysis uses a full Cox proportional hazard model including all three characteristics and their interactions with the trading gain/loss indicators. This allows for a direct comparison of the marginal contribution of each factor, controlling for the others.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator, a dummy variable equal to 1 if a position is at a loss.\n- `Soph`: A time-invariant dummy variable equal to 1 for sophisticated investors (warrant traders).\n- `Age`: A continuous variable defined as `(investor's actual age - 18) / 10`.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who have used a stop-loss order at least once.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a comprehensive model with multiple interaction terms. The key results from this full specification are summarized in Table 1.\n\n**Table 1: Combined Model of Predictive Variables**\n\n| Variable | Hazard Ratio (Loss Model) |\n| :--- | :--- |\n| TLI | 0.3948*** |\n| TLI × Sophistication | 1.039 |\n| TLI × Age | 1.1054*** |\n| TLI × SL_User | 1.1700*** |\n\n*Notes: *** denotes significance at the 1% level.*\n\n---\n\n### Question\n\nIn the full model presented in Table 1, the interaction `TLI × Sophistication` is statistically insignificant. However, in a simpler model that excluded `Age` and `SL_User`, this same interaction was found to be significant. Select all statistically valid explanations for this change in significance.", "Options": {"A": "The significance in the simpler model was likely influenced by omitted variable bias, as `Sophistication` may have been capturing the correlated effect of `Age` (experience), which is now controlled for directly.", "B": "The paper notes that sophisticated investors tend to be older, suggesting a correlation between the `Sophistication` and `Age` variables. Including both in the model can cause multicollinearity, inflating standard errors and potentially rendering one of the correlated terms insignificant.", "C": "The Proportional Hazards assumption was violated for the `Sophistication` variable in the full model but not the simple model, invalidating the result.", "D": "The change in significance indicates that the effect of sophistication on selling losers is entirely mediated by stop-loss use, meaning sophisticated investors only sell losers more often because they use stop-losses."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret a common statistical artifact: a change in coefficient significance when new control variables are added. It uses a 'Reverse-Reasoning' strategy. Options A and B represent the two canonical explanations for this phenomenon (omitted variable bias and multicollinearity), both of which are correct. Distractor C is a 'Factual Error' distractor, as the paper states the assumption was violated for `Age`, not `Sophistication`. Distractor D presents a plausible but unsupported causal story ('True but Irrelevant/Unsupported'), a common error in interpretation.", "qid": "401", "question": "### Background\n\n**Research Question.** This case investigates the relative importance of three investor characteristics—sophistication, experience (proxied by age), and the use of stop-loss orders—in mitigating the disposition effect. The analysis aims to determine which factors remain significant when all are considered simultaneously.\n\n**Setting / Data-Generating Environment.** The analysis uses a full Cox proportional hazard model including all three characteristics and their interactions with the trading gain/loss indicators. This allows for a direct comparison of the marginal contribution of each factor, controlling for the others.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator, a dummy variable equal to 1 if a position is at a loss.\n- `Soph`: A time-invariant dummy variable equal to 1 for sophisticated investors (warrant traders).\n- `Age`: A continuous variable defined as `(investor's actual age - 18) / 10`.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who have used a stop-loss order at least once.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a comprehensive model with multiple interaction terms. The key results from this full specification are summarized in Table 1.\n\n**Table 1: Combined Model of Predictive Variables**\n\n| Variable | Hazard Ratio (Loss Model) |\n| :--- | :--- |\n| TLI | 0.3948*** |\n| TLI × Sophistication | 1.039 |\n| TLI × Age | 1.1054*** |\n| TLI × SL_User | 1.1700*** |\n\n*Notes: *** denotes significance at the 1% level.*\n\n---\n\n### Question\n\nIn the full model presented in Table 1, the interaction `TLI × Sophistication` is statistically insignificant. However, in a simpler model that excluded `Age` and `SL_User`, this same interaction was found to be significant. Select all statistically valid explanations for this change in significance.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 367, "Question": "### Background\n\n**Research Question.** Are news narratives a better source of information for measuring ICAPM state variables than traditional macroeconomic data?\n\n**Setting / Data-Generating Environment.** The paper's model is estimated using two different sets of inputs: (1) 180 narrative attention series derived from daily *Wall Street Journal* articles, and (2) 129 standard numerical macroeconomic data series from the FRED-MD database (e.g., quarterly GDP, monthly inflation). The model using narrative inputs achieves an out-of-sample MVE Sharpe ratio of 1.3, while the model using FRED-MD inputs achieves a Sharpe ratio of only 0.7.\n\n---\n\n### The Question\n\nBased on the paper's premise and results, select all of the following that represent plausible advantages of using high-frequency news narratives over traditional, low-frequency macroeconomic data for measuring ICAPM state variables.", "Options": {"A": "Timeliness: News is released daily, allowing for a more synchronous measurement of asset covariance with fundamental shocks compared to lagged monthly or quarterly macro data.", "B": "Forward-Looking Content: News narratives synthesize complex information and report on expectations and sentiment, which are better proxies for 'revisions to future investment opportunities' than backward-looking macro data.", "C": "Objectivity: News articles from reputable sources are less prone to measurement error and revisions than official government statistics.", "D": "Simplicity: The 'bag-of-words' representation of text is a lower-dimensional and less noisy data format than numerical time-series."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the core argument for why news text is a valuable data source in asset pricing. It uses a Scenario Application strategy, asking the user to apply the paper's logic to compare two data types. The correct options (A, C) identify the key advantages of timeliness and forward-looking content. Distractor B is an 'Almost Right' but incorrect claim, as news has its own significant biases. Distractor D is a 'Conceptual Opposite', as raw text data is extremely high-dimensional and noisy.", "qid": "367", "question": "### Background\n\n**Research Question.** Are news narratives a better source of information for measuring ICAPM state variables than traditional macroeconomic data?\n\n**Setting / Data-Generating Environment.** The paper's model is estimated using two different sets of inputs: (1) 180 narrative attention series derived from daily *Wall Street Journal* articles, and (2) 129 standard numerical macroeconomic data series from the FRED-MD database (e.g., quarterly GDP, monthly inflation). The model using narrative inputs achieves an out-of-sample MVE Sharpe ratio of 1.3, while the model using FRED-MD inputs achieves a Sharpe ratio of only 0.7.\n\n---\n\n### The Question\n\nBased on the paper's premise and results, select all of the following that represent plausible advantages of using high-frequency news narratives over traditional, low-frequency macroeconomic data for measuring ICAPM state variables.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 381, "Question": "### Background\n\nA risk-averse insured chooses an indemnity function `I(·)` to maximize their expected utility of final wealth. The problem is characterized by belief heterogeneity, where the insured's belief `\\mathbb{P}` differs from the insurer's `\\mathbb{Q}`.\n\n---\n\n### Data / Model Specification\n\nThe set of admissible insurance contracts, `\\mathfrak{C}`, is defined to be incentive-compatible and to follow the principle of indemnity:\n  \n\\mathfrak{C} = \\{ I(x) : 0 \\le I(x) \\le x, \\text{ and both } I(x) \\text{ and } x-I(x) \\text{ are increasing functions} \\} \\quad \\text{(Eq. (1))}\n \nFor a differentiable `I(x)`, the condition that `I(x)` and `x-I(x)` are increasing is equivalent to `0 \\le I'(x) \\le 1`.\n\n---\n\n### Question\n\nAccording to the economic principles discussed in the paper, which of the following statements correctly identify the rationale for the constraints imposed on admissible insurance contracts `I(x)` in the set `\\mathfrak{C}`? Select all that apply.", "Options": {"A": "The principle of indemnity (`0 \\le I(x) \\le x`) is primarily to ensure the insurer's premium calculation is actuarially fair.", "B": "The constraint `I'(x) \\ge 0` is imposed to prevent the insured from profiting on a loss by receiving an indemnity greater than the loss itself.", "C": "The condition that `I(x)` is increasing (i.e., `I'(x) \\ge 0`) is designed to prevent the insured from having an incentive to under-report a loss.", "D": "The condition that `x - I(x)` is increasing (i.e., `I'(x) \\le 1`) is designed to prevent the insured from having an incentive to exaggerate a loss."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item assesses understanding of the foundational model constraints. It uses a 'premise packaging' strategy to test the distinct economic rationales for different parts of the incentive-compatibility conditions. Distractors are designed by swapping the rationales for the constraints (D is a swap of the rationale for `I(x) <= x`) and by introducing a true but irrelevant concept (C confuses moral hazard with premium principles).", "qid": "381", "question": "### Background\n\nA risk-averse insured chooses an indemnity function `I(·)` to maximize their expected utility of final wealth. The problem is characterized by belief heterogeneity, where the insured's belief `\\mathbb{P}` differs from the insurer's `\\mathbb{Q}`.\n\n---\n\n### Data / Model Specification\n\nThe set of admissible insurance contracts, `\\mathfrak{C}`, is defined to be incentive-compatible and to follow the principle of indemnity:\n  \n\\mathfrak{C} = \\{ I(x) : 0 \\le I(x) \\le x, \\text{ and both } I(x) \\text{ and } x-I(x) \\text{ are increasing functions} \\} \\quad \\text{(Eq. (1))}\n \nFor a differentiable `I(x)`, the condition that `I(x)` and `x-I(x)` are increasing is equivalent to `0 \\le I'(x) \\le 1`.\n\n---\n\n### Question\n\nAccording to the economic principles discussed in the paper, which of the following statements correctly identify the rationale for the constraints imposed on admissible insurance contracts `I(x)` in the set `\\mathfrak{C}`? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 361, "Question": "### Background\n\n**Research Question.** How do rational investors infer a fund manager's unobserved quality from a single return realization when the manager can strategically choose the portfolio's risk level?\n\n**Setting and Environment.** We consider a model with an actively managed fund and a passive index fund. The active manager has a private quality type—'good' (`μ_g`) or 'bad' (`μ_b`)—which determines the mean of the fund's return. Risk-neutral investors observe the fund's realized return `R` and must decide whether to invest. Critically, the manager's choice of portfolio risk `σ` is unobservable to investors.\n\n**Variables and Parameters.**\n\n*   `R`: Realized net-of-fee return of the active fund.\n*   `μ`: The manager's unobserved quality (`μ_g` or `μ_b`).\n*   `σ`: The manager's strategic choice of portfolio risk.\n*   `ψ`: The prior probability that the manager is of good quality.\n*   `μ_o`: The known expected return of the passive index fund.\n*   The model assumes `μ_b < μ_o < μ_g`.\n\n---\n\n### Data / Model Specification\n\nThe return-generating process for the active fund is given by:\n  \nR = \\mu + \\sigma\\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0,1) \\quad \\text{(Eq. 1)}\n \nInvestors are rational and risk-neutral, investing in the active fund if its posterior expected return exceeds that of the index fund:\n  \nP(\\text{good}|R)\\mu_{g} + P(\\text{bad}|R)\\mu_{b} > \\mu_{o} \\quad \\text{(Eq. 2)}\n \nInvestors use Bayes' rule to update their beliefs. The posterior probability can be expressed using the prior `ψ` and the likelihood ratio `L(R) = P(R|bad) / P(R|good)` as:\n  \nP(\\text{good}|R) = \\frac{\\psi}{\\psi + (1-\\psi)L(R)} \\quad \\text{(Eq. 3)}\n \nIn a 'gambling' equilibrium, the good manager chooses minimum risk `σ_g^*` and the bad manager chooses higher risk `σ_b^* > σ_g^*`.\n\n---\n\n### Question\n\nBased on the model of investor behavior, select **all** of the following statements that are mathematically correct.", "Options": {"A": "In a 'gambling' equilibrium where `σ_b^* > σ_g^*`, the log-likelihood ratio `log(L(R))` is a U-shaped quadratic function of the return `R`, which is why the investment region for `R` can be a finite interval.", "B": "A rational, risk-neutral investor will invest in the active fund only if their posterior belief `P(good|R)` exceeds the threshold `τ = (μ_o - μ_b) / (μ_g - μ_b)`.", "C": "In a 'gambling' equilibrium, the investment region is a finite interval because the log-likelihood ratio `log(L(R))` is a linear function of `R`.", "D": "The investment threshold `τ` is derived as `(μ_g - μ_b) / (μ_o - μ_b)`."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 10.0). The original QA problem's core components (deriving the investment threshold and identifying the mathematical driver of the interval outcome) are highly suitable for conversion. This item uses the **Atomic Decomposition** strategy to test these two sharp, factual insights. It assesses **Computational Judgment** (deriving τ) and **Reverse-Reasoning** (understanding the mathematical cause of the interval outcome). Distractor B is a **Formula Misuse** error for the threshold τ. Distractor C is a **Conceptual Opposite** error regarding the functional form of the log-likelihood ratio.", "qid": "361", "question": "### Background\n\n**Research Question.** How do rational investors infer a fund manager's unobserved quality from a single return realization when the manager can strategically choose the portfolio's risk level?\n\n**Setting and Environment.** We consider a model with an actively managed fund and a passive index fund. The active manager has a private quality type—'good' (`μ_g`) or 'bad' (`μ_b`)—which determines the mean of the fund's return. Risk-neutral investors observe the fund's realized return `R` and must decide whether to invest. Critically, the manager's choice of portfolio risk `σ` is unobservable to investors.\n\n**Variables and Parameters.**\n\n*   `R`: Realized net-of-fee return of the active fund.\n*   `μ`: The manager's unobserved quality (`μ_g` or `μ_b`).\n*   `σ`: The manager's strategic choice of portfolio risk.\n*   `ψ`: The prior probability that the manager is of good quality.\n*   `μ_o`: The known expected return of the passive index fund.\n*   The model assumes `μ_b < μ_o < μ_g`.\n\n---\n\n### Data / Model Specification\n\nThe return-generating process for the active fund is given by:\n  \nR = \\mu + \\sigma\\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0,1) \\quad \\text{(Eq. 1)}\n \nInvestors are rational and risk-neutral, investing in the active fund if its posterior expected return exceeds that of the index fund:\n  \nP(\\text{good}|R)\\mu_{g} + P(\\text{bad}|R)\\mu_{b} > \\mu_{o} \\quad \\text{(Eq. 2)}\n \nInvestors use Bayes' rule to update their beliefs. The posterior probability can be expressed using the prior `ψ` and the likelihood ratio `L(R) = P(R|bad) / P(R|good)` as:\n  \nP(\\text{good}|R) = \\frac{\\psi}{\\psi + (1-\\psi)L(R)} \\quad \\text{(Eq. 3)}\n \nIn a 'gambling' equilibrium, the good manager chooses minimum risk `σ_g^*` and the bad manager chooses higher risk `σ_b^* > σ_g^*`.\n\n---\n\n### Question\n\nBased on the model of investor behavior, select **all** of the following statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 125, "Question": "### Background\n\n**Research Question.** This case examines whether the market's negative reaction to an initial public debt offering (debt IPO) is driven by the default risk of the issuing firm.\n\n**Setting.** The analysis is based on a sample of 143 debt IPOs from 1971-1994. The study uses an event study to measure the announcement-period stock returns for subsamples based on credit quality.\n\n**Variables & Parameters.**\n*   `SER`: Standardized daily stock excess return on the announcement day.\n*   `Z-statistic`: The test statistic for the null hypothesis that the `SER` is equal to zero.\n*   Investment Grade: Bonds with a Standard & Poor's rating of BBB or higher.\n*   Junk Grade: Bonds with a Standard & Poor's rating of BB or lower, including non-rated issues.\n\n---\n\n### Data / Model Specification\n\nTo test if the negative market reaction is simply a response to high default risk, the announcement day `SER` is calculated separately for Investment Grade and Junk Grade subsamples. The results are in Table 1.\n\n**Table 1: Announcement Day SER by Bond Rating**\n\n| Subsample | Number | SER (%) | Z-statistic |\n| :--- | :--- | :--- | :--- |\n| Investment grade | 39 | -1.07*** | -3.82 |\n| Junk grade | 91 | -0.64** | -2.29 |\n\n*Note: *** and ** denote significance at the 1% and 5% levels, respectively.* \n\n---\n\nBased on the data in Table 1, which of the following statements are **NOT** supported by the evidence? Select all that apply.", "Options": {"A": "A formal statistical test for the difference in means would show that the SER for investment grade issues is significantly more negative than the SER for junk grade issues at the 1% level.", "B": "The negative announcement effect is primarily driven by the high default risk of junk bonds, as these issues show a more negative stock price reaction than investment grade bonds.", "C": "The negative stock price reaction to debt IPOs is statistically significant for both investment grade and junk grade issues.", "D": "The stock price reaction for investment grade issues is economically and statistically more negative than the reaction for junk grade issues."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret and compare statistical results across subsamples and to recognize incorrect conclusions. Depth Strategy: Reverse-Reasoning. The Single-Correct-Answer Inversion Protocol was applied because the core assessment objective—identifying the main takeaway from the table—yields only one correct affirmative statement. The stem was inverted to 'NOT supported'. Distractor Logic: Option A is the single true statement supported by the data (both Z-statistics are significant), making it the distractor in this inverted question. Option B is incorrect; the data show the opposite, with investment grade having a more negative SER. Option C is incorrect; a formal test for the difference in means (Z ≈ -1.09) fails to reject the null of no difference. Option D is incorrect; while the point estimate for investment grade is more negative, the difference is not statistically significant, so one cannot conclude it is 'statistically more negative'.", "qid": "125", "question": "### Background\n\n**Research Question.** This case examines whether the market's negative reaction to an initial public debt offering (debt IPO) is driven by the default risk of the issuing firm.\n\n**Setting.** The analysis is based on a sample of 143 debt IPOs from 1971-1994. The study uses an event study to measure the announcement-period stock returns for subsamples based on credit quality.\n\n**Variables & Parameters.**\n*   `SER`: Standardized daily stock excess return on the announcement day.\n*   `Z-statistic`: The test statistic for the null hypothesis that the `SER` is equal to zero.\n*   Investment Grade: Bonds with a Standard & Poor's rating of BBB or higher.\n*   Junk Grade: Bonds with a Standard & Poor's rating of BB or lower, including non-rated issues.\n\n---\n\n### Data / Model Specification\n\nTo test if the negative market reaction is simply a response to high default risk, the announcement day `SER` is calculated separately for Investment Grade and Junk Grade subsamples. The results are in Table 1.\n\n**Table 1: Announcement Day SER by Bond Rating**\n\n| Subsample | Number | SER (%) | Z-statistic |\n| :--- | :--- | :--- | :--- |\n| Investment grade | 39 | -1.07*** | -3.82 |\n| Junk grade | 91 | -0.64** | -2.29 |\n\n*Note: *** and ** denote significance at the 1% and 5% levels, respectively.* \n\n---\n\nBased on the data in Table 1, which of the following statements are **NOT** supported by the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: from scipy import stats\n\n# Sample sizes\nn1 = 39  #...\n\n> 🔍 **Analyzing financial data**\n> ***- Table 1 shows investment grade bonds have a -1.07% SER, significant at 1%, while junk grade bonds show -0.64%, significant at 5%.***\n\n\nA,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 271, "Question": "### Background\n\nA theoretical argument in the paper suggests that a sufficient condition for increased finalization speed to *decrease* Payments Per Claim Finalized (PPCF) is that the underlying PPCF curve is a strictly concave function of time. This theoretical condition would explain the consistently negative `γ_k` coefficients found in the empirical 'See-Saw' model, which indicate that higher finalization speed is associated with lower average payments.\n\n### Data / Model Specification\n\nTo test this, the author examines the fitted PPCF values from the model for a typical runoff pattern and calculates their second differences. A persistently concave function would exhibit consistently negative second differences. The results are shown in Table 1.\n\n**Table 1: PPCF for a Typical Runoff of Claim Numbers**\n\n| Development Year | Fitted PPCF | Second Difference of Fitted PPCF |\n| :--- | :--- | :--- |\n| 0 | 12,568 | |\n| 1 | 8,106 | +2,981 |\n| 2 | 9,587 | -2,593 |\n| 3 | 8,475 | -865 |\n| 4 | 6,498 | +5,921 |\n| 5 | 10,442 | -1,837 |\n| 6 | 12,549 | -1,814 |\n| 7 | 12,842 | +1,725 |\n| 8 | 14,860 | +2,556 |\n| 9 | 19,434 | -2,493 |\n| 10 | 21,515 | |\n\nThe paper investigates the apparent contradiction between the theoretical condition (concavity) and the model's empirical results. Based on the provided data and the paper's analysis, which of the following conclusions are valid? (Select all that apply)", "Options": {"A": "The author suggests the negative `γ_k` coefficients might be explained by other factors, such as a secular trend of increasing claim sizes over accident years that is negatively correlated with finalization speeds.", "B": "The data in Table 1 strongly supports the theory that the PPCF curve is a strictly concave function of development year, as evidenced by the predominantly negative second differences.", "C": "The fluctuating signs in the 'Second Difference' column of Table 1 show that the fitted PPCF curve is not persistently concave, which appears to contradict the sufficient condition that would theoretically guarantee the observed negative `γ_k` coefficients.", "D": "The paper concludes that because the concavity condition is violated, the 'See-Saw' model is fundamentally flawed and its results are unreliable."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize theoretical arguments with empirical evidence, a key part of the original QA's part 3. It uses the 'Atomic Decomposition' strategy to test two distinct parts of the paper's conclusion. Option A tests the correct interpretation of the second-difference data in Table 1. Option B tests the understanding of the author's proposed resolution to the apparent contradiction. Distractor C is a 'Conceptual Opposite' error, misreading the evidence in the table. Distractor D presents an 'Incorrect Conclusion'; the author explicitly argues the model remains useful as a linear approximation even if the underlying theory is incomplete.", "qid": "271", "question": "### Background\n\nA theoretical argument in the paper suggests that a sufficient condition for increased finalization speed to *decrease* Payments Per Claim Finalized (PPCF) is that the underlying PPCF curve is a strictly concave function of time. This theoretical condition would explain the consistently negative `γ_k` coefficients found in the empirical 'See-Saw' model, which indicate that higher finalization speed is associated with lower average payments.\n\n### Data / Model Specification\n\nTo test this, the author examines the fitted PPCF values from the model for a typical runoff pattern and calculates their second differences. A persistently concave function would exhibit consistently negative second differences. The results are shown in Table 1.\n\n**Table 1: PPCF for a Typical Runoff of Claim Numbers**\n\n| Development Year | Fitted PPCF | Second Difference of Fitted PPCF |\n| :--- | :--- | :--- |\n| 0 | 12,568 | |\n| 1 | 8,106 | +2,981 |\n| 2 | 9,587 | -2,593 |\n| 3 | 8,475 | -865 |\n| 4 | 6,498 | +5,921 |\n| 5 | 10,442 | -1,837 |\n| 6 | 12,549 | -1,814 |\n| 7 | 12,842 | +1,725 |\n| 8 | 14,860 | +2,556 |\n| 9 | 19,434 | -2,493 |\n| 10 | 21,515 | |\n\nThe paper investigates the apparent contradiction between the theoretical condition (concavity) and the model's empirical results. Based on the provided data and the paper's analysis, which of the following conclusions are valid? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: ppcfs = [12568, 8106, 9587, 8475, 6498, 10442, 125...\n\n\nA,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 244, "Question": "### Background\n\n**Research Question.** How can one construct a robust econometric framework to test the hypothesis that the market prices of macroeconomic risks are not constant, but vary predictably over the business cycle?\n\n**Setting.** The study's empirical core is a Generalized Method of Moments (GMM) framework designed to jointly model and estimate the relationship between asset returns and the conditional moments of industrial production.\n\n### Data / Model Specification\n\n1.  **Variable Construction:** The model uses two key industrial production variables:\n    *   The realized production growth to be priced, `YP_t`, is the 12-month log growth of *seasonally adjusted* industrial production.\n    *   The business cycle proxy, `IPL_t`, is the 12-month log growth of *seasonally unadjusted* industrial production, lagged two months (`IPL_{t-2}`).\n\n2.  **GMM System:** The model is estimated as a system of equations:\n\n      \n    e_{1t} = YP_t - \\phi_0 - \\sum_{j} \\phi_j IPL_{t-j} \\quad \\text{(Eq. 1)}\n     \n\n      \n    e_{2t} = r_{mt} - \\eta_{0s} - (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) YP_t - (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) e_{1t}^2 \\quad \\text{(Eq. 2)}\n     \n\n    where `r_{mt}` is the stock return and `e_{1t}^2` proxies for production volatility.\n\n### Question\n\nRegarding the study's GMM estimation framework and its underlying assumptions, which of the following statements accurately describe potential statistical problems and the methodological choices made to address them?", "Options": {"A": "Estimating the system jointly via GMM is preferred to a two-step OLS approach primarily to avoid the \"generated regressors problem,\" which would lead to underestimated standard errors.", "B": "The joint GMM estimation is necessary because stock returns (`r_mt`) are endogenous with respect to production growth (`YP_t`).", "C": "If the production growth series `YP_t` were non-stationary (contained a unit root), the GMM estimates would be invalid due to the risk of spurious regression.", "D": "The main consequence of using a \"weak instrument\" (`IPL_{t-2}`) would be heteroskedasticity in the pricing errors, which GMM corrects automatically."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 10.0). This item assesses knowledge of advanced econometric issues central to the paper's methodology. It uses a 'Reverse-Reasoning' strategy, asking for the problems that justify the methods used. Correct options (A, B) identify two critical issues: the generated regressors problem and the stationarity assumption. Distractor (C) misidentifies the consequence of a classic econometric problem (weak instruments), a 'Formula Misuse' type of error. Distractor (D) offers a plausible but incorrect primary motivation for the specific GMM vs. two-step OLS choice, making it a strong 'Almost Right' distractor.", "qid": "244", "question": "### Background\n\n**Research Question.** How can one construct a robust econometric framework to test the hypothesis that the market prices of macroeconomic risks are not constant, but vary predictably over the business cycle?\n\n**Setting.** The study's empirical core is a Generalized Method of Moments (GMM) framework designed to jointly model and estimate the relationship between asset returns and the conditional moments of industrial production.\n\n### Data / Model Specification\n\n1.  **Variable Construction:** The model uses two key industrial production variables:\n    *   The realized production growth to be priced, `YP_t`, is the 12-month log growth of *seasonally adjusted* industrial production.\n    *   The business cycle proxy, `IPL_t`, is the 12-month log growth of *seasonally unadjusted* industrial production, lagged two months (`IPL_{t-2}`).\n\n2.  **GMM System:** The model is estimated as a system of equations:\n\n      \n    e_{1t} = YP_t - \\phi_0 - \\sum_{j} \\phi_j IPL_{t-j} \\quad \\text{(Eq. 1)}\n     \n\n      \n    e_{2t} = r_{mt} - \\eta_{0s} - (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) YP_t - (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) e_{1t}^2 \\quad \\text{(Eq. 2)}\n     \n\n    where `r_{mt}` is the stock return and `e_{1t}^2` proxies for production volatility.\n\n### Question\n\nRegarding the study's GMM estimation framework and its underlying assumptions, which of the following statements accurately describe potential statistical problems and the methodological choices made to address them?", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"GMM estimation generated regressors problem in asset pricing models\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"endogeneity of stock returns to macroeconomic variables in GMM\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"unit root non-stationarity in GMM estimation asset pricing\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"weak instruments in GMM consequences heteroskedasticity\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Analyzing financial research**\n> ***- The task involves constructing an econometric framework to test varying market prices of macroeconomic risks over the business cycle.***\n\n> 🔍 **10 results found**\n\n> [🔗 \"Regression-based estimation of dynamic asset pricing models - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304405X15001397)\n> [🔗 \"SciELO Brazil - Um Tutorial sobre o Método Generalizado dos Momentos (GMM) em Finanças Um Tutorial sobre o Método Generalizado dos Momentos (GMM) em Finanças\" SciELO Brazil](https://www.scielo.br/j/rac/a/PmCb55tHdKJ3B53RnftXtNd/?lang=en)\n> [🔗 \"Profile GMM estimation of panel data models with interactive fixed effects - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304407622001592)\n> [🔗 \"Regularized Gmm For Time‐Varying Models With Applications To Asset Pricing\" IDEAS/RePEc](https://ideas.repec.org/a/wly/iecrev/v65y2024i2p851-883.html)\n> [🔗 \"A GMM approach for estimation of volatility and regression models when daily prices are subject to price limits - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0927538X04000022)\n> [🔗 \"GMM tests of stochastic discount factor models with useless factors - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304405X99000331)\n> [🔗 \"Regularized GMM for Time-Varying Models with Applications to Asset Pricing by Liyuan Cui, Guanhao Feng, Yongmiao Hong :: SSRN\" SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3814520)\n> [🔗 \"GMM weighting matrices in cross-sectional asset pricing tests - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0378426624000438)\n> [🔗 \"Asset pricing models with measurement error problems: A new framework with Compact Genetic Algorithms | Cairn.info\" Cairn.info](https://shs.cairn.info/journal-finance-2022-2-page-1?lang=en&tab=bibliographie)\n> [🔗 \"REGULARIZED GMM FOR TIME‐VARYING MODELS WITH APPLICATIONS TO ASSET PRICING\" ResearchGate](https://www.researchgate.net/publication/374771107_REGULARIZED_GMM_FOR_TIME-VARYING_MODELS_WITH_APPLICATIONS_TO_ASSET_PRICING)\n\n> 🔍 **10 results found**\n\n> [🔗 \"Endogeneity in panel data regressions: methodological guidance for corporate finance researchers\" Redalyc](https://www.redalyc.org/journal/947/94768442003/html/)\n> [🔗 \"Do the macroeconomic factors influence the firm's investment decisions? A generalized method of moments (GMM) approach\" IDEAS/RePEc](https://ideas.repec.org/a/wly/ijfiec/v26y2021i1p790-801.html)\n> [🔗 \"Dealing with endogeneity bias: The generalized method of moments (GMM) for panel data - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0019850117308532)\n> [🔗 \"Dealing with dynamic endogeneity in international business research | Journal of International Business Studies\" Springer](https://link.springer.com/article/10.1057/s41267-020-00398-8)\n> [🔗 \"Endogeneity and the Dynamics of Internal Corporate ...\" University of Georgia](https://www.terry.uga.edu/wp-content/uploads/Board_Endogeneity_JFE_2012_Final.pdf)\n> [🔗 \"(PDF) Dealing with Endogeneity Bias: The Generalized Method of Moments (GMM) for Panel Data\" ResearchGate](https://www.researchgate.net/publication/321267050_Dealing_with_Endogeneity_Bias_The_Generalized_Method_of_Moments_GMM_for_Panel_Data)\n> [🔗 \"Addressing endogeneity in the relationship between early entry and performance: The case of foreign market expansion - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0024630124000657)\n> [🔗 \"GMM in Econometrics: Your Quick and Clear Guide\" Number Analytics](https://www.numberanalytics.com/blog/gmm-econometrics-guide)\n> [🔗 \"The Ultimate Guide to GMM for Econometric Analysis\" Number Analytics](https://www.numberanalytics.com/blog/ultimate-guide-gmm-econometric-analysis)\n> [🔗 \"Macroeconomic attention and stock market return predictability\" University of Portsmouth](https://researchportal.port.ac.uk/files/52836875/Macroeconomic_atention_and_stock_market_return_predictability.pdf)\n\n> 🔍 **10 results found**\n\n> [🔗 \"generalized moments - Unit roots and GMM estimation for panel data ...\" Stack Exchange](https://stats.stackexchange.com/questions/112431/unit-roots-and-gmm-estimation)\n> [🔗 \"Unit Root Tests\" University of Washington](https://faculty.washington.edu/ezivot/econ584/notes/unitroot.pdf)\n> [🔗 \"How can I deal with non-stationary regressors in a System-GMM estimation ? | ResearchGate\" ResearchGate](https://www.researchgate.net/post/How_can_I_deal_with_non-stationary_regressors_in_a_System-GMM_estimation)\n> [🔗 \"stata - For system GMM, do we need stationarity in levels or first-differences? - Cross Validated\" Stack Exchange](https://stats.stackexchange.com/questions/256929/for-system-gmm-do-we-need-stationarity-in-levels-or-first-differences)\n> [🔗 \"Are unit root tests (stationarity) necessary for the use of DIF GMM (Arellano and Bond, 1991) or SYS GMM (Blundell and Bond, 1998)? | ResearchGate\" ResearchGate](https://www.researchgate.net/post/Are-unit-root-tests-stationarity-necessary-for-the-use-of-DIF-GMM-Arellano-and-Bond-1991-or-SYS-GMM-Blundell-and-Bond-1998)\n> [🔗 \"Stationarity, Nonstationarity, and Unit Roots | Quantitative Methods | CFA Level 2 | FinancialAnalystGuide.com\" Financialanalystguide](https://financialanalystguide.com/cfa-level-2/volume-1-quantitative-methods/6/1/)\n> [🔗 \"GMM Linear Dynamic panel data model (non stationarity) - Statalist\" Statalist](https://www.statalist.org/forums/forum/general-stata-discussion/general/1679069-gmm-linear-dynamic-panel-data-model-non-stationarity)\n> [🔗 \"Are real interest rates really nonstationary? New evidence from tests with good size and power - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0164070404000278)\n> [🔗 \"Controlling for Non-Stationarity in a Time Series Difference-GMM Estimation ? | ResearchGate\" ResearchGate](https://www.researchgate.net/post/Controlling_for_Non-Stationarity_in_a_Time_Series_Difference-GMM_Estimation)\n> [🔗 \"GMM weighting matrices in cross-sectional asset pricing tests - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0378426624000438)\n\n> 🔍 **10 results found**\n\n> [🔗 \"[2208.01967] Weak Instruments, First-Stage Heteroskedasticity, the Robust F-Test and a GMM Estimator with the Weight Matrix Based on First-Stage Residuals\" arXiv](https://arxiv.org/abs/2208.01967)\n> [🔗 \"Instrumental variables and GMM: Estimation and testing\" University of Washington](https://faculty.washington.edu/ezivot/econ583/ivreg2_bcwp545.pdf)\n> [🔗 \"gmm — Generalized method of moments estimation\" Stata](https://www.stata.com/manuals13/rgmm.pdf)\n> [🔗 \"Need solution for a weak instrument in the system GMM | ResearchGate\" ResearchGate](https://www.researchgate.net/post/Need_solution_for_a_weak_instrument_in_the_system_GMM)\n> [🔗 \"Misspecified heteroskedasticity in the panel probit model: A small sample comparison of GMM and SML estimators - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304407699000706)\n> [🔗 \"Weak Instruments, First-Stage Heteroskedasticity and the ...\" University of Bristol](https://www.bristol.ac.uk/efm/media/workingpapers/working_papers/pdffiles/dp19708.pdf)\n> [🔗 \"Instrumental Variables and GMM: Estimation and Testing\" Stata](https://www.stata.com/meeting/9uk/IVGMM3316.pdf)\n> [🔗 \"Instrumental variable estimation with heteroskedasticity and many instruments - Hausman - 2012 - Quantitative Economics - Wiley Online Library\" Wiley Online Library](https://onlinelibrary.wiley.com/doi/abs/10.3982/QE89)\n> [🔗 \"IV and IV-GMM\" FMB Church](http://fmwww.bc.edu/EC-C/S2014/823/EC823.S2014.nn02.slides.pdf)\n> [🔗 \"Heteroscedasticity - an overview | ScienceDirect Topics\" ScienceDirect](https://www.sciencedirect.com/topics/engineering/heteroscedasticity)\n\n\nA,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 349, "Question": "### Background\n\n**Research Question.** This case investigates the paper's central empirical claim: that a country's underlying \"financial structure\" determines its average inflation rate. The analysis compares country-specific estimates of inflation-tax capacity and welfare costs. A key caveat discussed in the paper is that governments might engage in \"financial repression\" (e.g., capital controls) to deliberately make money demand less elastic, thereby enhancing their inflation-tax capacity.\n\n**Variables & Parameters.**\n- `π*`: The estimated revenue-maximizing inflation rate.\n- `S*`: The estimated maximum seigniorage revenue as a share of GDP.\n- `Ω(μ)`: The marginal welfare cost of seigniorage (`dW/dS`).\n- `α`: The semi-elasticity of currency demand with respect to inflation.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Basic Laffer Surface Calculations (Excerpt for select countries)**\n\n| Country   | `π*`   | `S*`   | `Ω(μ)` |\n| :-------- | :----- | :----- | :--------- |\n| Argentina | 2.3166 | 0.1723 | 0.4959     |\n| Japan     | 0.3562 | 0.0137 | 0.1580     |\n\n---\n\n### Question\n\nConsidering the country-specific data in **Table 1** and the paper's discussion of financial repression, select all correct interpretations.", "Options": {"A": "The paper notes that if governments actively engage in financial repression to make money demand less elastic (lower `α`), the calculated shoe leather costs would represent an underestimate of the true total social cost of raising revenue.", "B": "The marginal welfare cost (`Ω(μ)`) in Argentina (0.4959) is lower than in Japan (0.1580), indicating that Argentina can raise inflation tax revenue more efficiently.", "C": "Argentina's estimated revenue-maximizing inflation rate (`π*` = 2.3166) is more than six times that of Japan (`π*` = 0.3562), which is consistent with the financial structure hypothesis explaining their divergent inflation histories.", "D": "The data show that Japan's maximum potential seigniorage as a share of GDP (`S*`) is significantly higher than Argentina's, reflecting a more developed financial system."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the application of the paper's theory to specific country data and assesses understanding of a key theoretical caveat (financial repression). It uses an Atomic Decomposition strategy to create two correct statements, one empirical and one conceptual. Distractors are based on a Numerical Error/Conceptual Opposite (C) and a direct data misreading (D).", "qid": "349", "question": "### Background\n\n**Research Question.** This case investigates the paper's central empirical claim: that a country's underlying \"financial structure\" determines its average inflation rate. The analysis compares country-specific estimates of inflation-tax capacity and welfare costs. A key caveat discussed in the paper is that governments might engage in \"financial repression\" (e.g., capital controls) to deliberately make money demand less elastic, thereby enhancing their inflation-tax capacity.\n\n**Variables & Parameters.**\n- `π*`: The estimated revenue-maximizing inflation rate.\n- `S*`: The estimated maximum seigniorage revenue as a share of GDP.\n- `Ω(μ)`: The marginal welfare cost of seigniorage (`dW/dS`).\n- `α`: The semi-elasticity of currency demand with respect to inflation.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Basic Laffer Surface Calculations (Excerpt for select countries)**\n\n| Country   | `π*`   | `S*`   | `Ω(μ)` |\n| :-------- | :----- | :----- | :--------- |\n| Argentina | 2.3166 | 0.1723 | 0.4959     |\n| Japan     | 0.3562 | 0.0137 | 0.1580     |\n\n---\n\n### Question\n\nConsidering the country-specific data in **Table 1** and the paper's discussion of financial repression, select all correct interpretations.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 116, "Question": "### Background\n\nThe model is extended to include costly bank monitoring. The bank must be incentivized to monitor at a private cost `P`. This introduces a second moral hazard problem, as credit guarantees can weaken the bank's incentive to monitor.\n\n### Data / Model Specification\n\nThe need to incentivize bank monitoring lowers the maximum useful guarantee level to:\n\n  \n\\tilde{\\phi}(Q) = 1 - \\frac{A+P}{I-Q} \n \nwhere `A` is collateral, `P` is monitoring cost, and `I-Q` is the loan amount. This new cap is strictly less than the cap without monitoring costs. As a result, the optimal policy shifts towards co-funding at a lower budget threshold.\n\n### Question\n\nIn the extended model with costly bank monitoring, which of the following statements are **INCORRECT** descriptions of the policy trade-offs and outcomes?\n\nSelect all that apply.", "Options": {"A": "Credit guarantees and co-funding are perfect substitutes, and the introduction of monitoring costs does not change their relative effectiveness.", "B": "The fundamental 'pecking order' of support is completely reversed; co-funding should always be provided before any guarantees are considered.", "C": "The maximum useful guarantee level, `tilde(φ)(Q)`, is strictly increasing in the bank's monitoring cost `P`.", "D": "A higher monitoring cost `P` makes co-funding a relatively more attractive policy tool because guarantees directly undermine the bank's incentive to incur that cost, while co-funding does not."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to analyze how an additional constraint (bank monitoring) alters the model's main conclusions. It tests the understanding of the new trade-off between incentivizing the entrepreneur and the bank. Depth Strategy: Reverse-Reasoning. The student must identify multiple invalid conclusions about the model's extension. Single-Correct-Answer Inversion Protocol: This protocol was applied. The core assessment objective—understanding how monitoring costs shift the optimal policy—has a single, nuanced correct answer (D). To create a valid Multiple Response item, the stem was inverted to ask for INCORRECT statements. The original distractors (A, B, C), which represent plausible misinterpretations, now become the correct answers. Distractor Logic: The new (and only) distractor, D, is the correct summary of the extension's main finding. Options A, B, and C are incorrect because: (A) `tilde(φ)` is decreasing in `P`; (B) the tools are not perfect substitutes and monitoring costs change their relative appeal; (C) the pecking order is modified, not reversed.", "qid": "116", "question": "### Background\n\nThe model is extended to include costly bank monitoring. The bank must be incentivized to monitor at a private cost `P`. This introduces a second moral hazard problem, as credit guarantees can weaken the bank's incentive to monitor.\n\n### Data / Model Specification\n\nThe need to incentivize bank monitoring lowers the maximum useful guarantee level to:\n\n  \n\\tilde{\\phi}(Q) = 1 - \\frac{A+P}{I-Q} \n \nwhere `A` is collateral, `P` is monitoring cost, and `I-Q` is the loan amount. This new cap is strictly less than the cap without monitoring costs. As a result, the optimal policy shifts towards co-funding at a lower budget threshold.\n\n### Question\n\nIn the extended model with costly bank monitoring, which of the following statements are **INCORRECT** descriptions of the policy trade-offs and outcomes?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 391, "Question": "### Background\n\n**Research Question.** Is the initial underpricing of a Seasoned Equity Offering (SEO) a credible signal of the issuing firm's superior future performance?\n\n**Setting.** The signaling hypothesis of underpricing posits that high-quality firms are willing to underprice their shares to signal private information about strong future prospects. This implies a positive relationship between initial underpricing and subsequent stock performance.\n\n**Variables & Parameters.**\n- `R_1`: Offer-to-close return, a measure of initial underpricing (percentage).\n- `Adjusted BHR_{+60}`: Market-adjusted buy-and-hold return for 60 trading days after the offer (percentage).\n- `CRETP60`: The cumulative raw return from the offer to 60 trading days subsequent to the offer.\n\n---\n\n### Data / Model Specification\n\nThe study examines the link between underpricing and performance using both cross-sectional comparisons and regression analysis.\n\n**Table 1. SEO Underpricing and Performance by REIT Property Type**\n\n| REIT Type | Initial Underpricing (`R_1`, %) | 60-Day Adjusted BHR (%) |\n| :--- | :--- | :--- |\n| Office | 1.35* | 2.47* |\n| Malls | 0.25 | -0.10 |\n\n*Source: Synthesized from Table 2 and Table 7 in the source document. An asterisk (*) denotes statistical significance.*\n\n**Table 2. Regression of Underpricing on Post-Offer Returns**\n\n| Dependent Variable | Independent Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- |\n| Underpricing (`R_1`) | CRETP60 | 0.077 | (4.85)* |\n\n*Source: Synthesized from Model 4, Table 8 in the source document. An asterisk (*) denotes statistical significance.*\n\n---\n\n### Question\n\nThe signaling hypothesis suggests that underpricing is a credible signal of a firm's superior future performance. Select all empirical findings presented that are consistent with this hypothesis.", "Options": {"A": "The study finds that all REIT property types with significant underpricing had negative subsequent market-adjusted returns, contradicting the signaling hypothesis.", "B": "The regression coefficient linking underpricing to future returns was positive but not statistically significant, providing only weak support for the signaling hypothesis.", "C": "Regression analysis for the full sample shows a statistically significant positive relationship between the initial underpricing (`R_1`) and the subsequent 60-day raw return (`CRETP60`).", "D": "In a cross-sectional comparison, Office REITs exhibited high initial underpricing and strong positive 60-day market-adjusted returns, while Malls REITs showed low underpricing and negative returns."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to synthesize two different forms of evidence (cross-sectional and regression) supporting the signaling hypothesis. It uses an **Atomic Decomposition** strategy, presenting each piece of evidence as a separate, verifiable statement. Correct options (A, B) accurately describe the results from Table 1 and Table 2. Distractors C and D are **Factual Errors** that directly contradict the data presented in the tables, testing for careful reading and interpretation.", "qid": "391", "question": "### Background\n\n**Research Question.** Is the initial underpricing of a Seasoned Equity Offering (SEO) a credible signal of the issuing firm's superior future performance?\n\n**Setting.** The signaling hypothesis of underpricing posits that high-quality firms are willing to underprice their shares to signal private information about strong future prospects. This implies a positive relationship between initial underpricing and subsequent stock performance.\n\n**Variables & Parameters.**\n- `R_1`: Offer-to-close return, a measure of initial underpricing (percentage).\n- `Adjusted BHR_{+60}`: Market-adjusted buy-and-hold return for 60 trading days after the offer (percentage).\n- `CRETP60`: The cumulative raw return from the offer to 60 trading days subsequent to the offer.\n\n---\n\n### Data / Model Specification\n\nThe study examines the link between underpricing and performance using both cross-sectional comparisons and regression analysis.\n\n**Table 1. SEO Underpricing and Performance by REIT Property Type**\n\n| REIT Type | Initial Underpricing (`R_1`, %) | 60-Day Adjusted BHR (%) |\n| :--- | :--- | :--- |\n| Office | 1.35* | 2.47* |\n| Malls | 0.25 | -0.10 |\n\n*Source: Synthesized from Table 2 and Table 7 in the source document. An asterisk (*) denotes statistical significance.*\n\n**Table 2. Regression of Underpricing on Post-Offer Returns**\n\n| Dependent Variable | Independent Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- |\n| Underpricing (`R_1`) | CRETP60 | 0.077 | (4.85)* |\n\n*Source: Synthesized from Model 4, Table 8 in the source document. An asterisk (*) denotes statistical significance.*\n\n---\n\n### Question\n\nThe signaling hypothesis suggests that underpricing is a credible signal of a firm's superior future performance. Select all empirical findings presented that are consistent with this hypothesis.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 164, "Question": "### Background\n\nA simulated health insurance market evolves over multiple years. An insurer offers a single, community-rated premium rebate for a €500 deductible. Individuals are heterogeneous in health risk and are characterized by their risk aversion (`r`) and their potential for moral hazard reduction (`m`). The market can experience an \"adverse selection spiral,\" where the market for the deductible option collapses over time.\n\n### Data / Model Specification\n\nThe market evolves according to two rules:\n1.  **Consumer Choice:** Individuals `i` take the deductible in year `t` if the Offered Premium Rebate (`OPR_t`) exceeds their Demanded Premium Rebate (`DPR_i`).\n2.  **Insurer Pricing:** The insurer sets `OPR_{t+1}` equal to the average realized cost savings from the pool of individuals who took the deductible in year `t`.\n\nAdverse selection occurs because individuals with lower health risks have lower `DPR_i` and are thus more likely to opt-in.\n\n**Table 1: Percentage of Insured Taking a Deductible (Scenario I)**\n\n| Year | Level of Risk Aversion | m=0 | m = 0.05 | m = 0.1 |\n| :--- | :--- | :-- | :--- | :--- |\n| 1 | r=0.000 | 56 | 93 | 100 |\n| | r=0.003 | 28 | 84 | 99 |\n| | r=0.005 | 18 | 76 | 99 |\n| 2 | r=0.000 | 0 | 79 | 100 |\n| | r=0.003 | 0 | 22 | 98 |\n| | r=0.005 | 0 | 0 | 96 |\n| 3 | r=0.000 | 0 | 23 | 100 |\n| | r=0.003 | 0 | 0 | 97 |\n| | r=0.005 | 0 | 0 | 88 |\n| 4 | r=0.000 | 0 | 0 | 100 |\n| | r=0.003 | 0 | 0 | 95 |\n| | r=0.005 | 0 | 0 | 55 |\n\n---\n\nBased on the data in Table 1 and the model description, which of the following statements are valid conclusions about the dynamics of the adverse selection spiral?\n", "Options": {"A": "The market is stable (i.e., avoids a spiral) only in the scenario where `m=0.1` and `r=0.000`, because this is the only case where the initial offered rebate was high enough to attract even the unhealthiest individuals.", "B": "If the moral hazard reduction (`m`) is zero, the market for deductibles is guaranteed to collapse to zero participation by Year 2, regardless of the level of risk aversion.", "C": "A higher level of consumer risk aversion (`r`) consistently leads to a lower initial participation rate (Year 1) for any given level of moral hazard reduction (`m`).", "D": "In the scenario with `m=0.05` and `r=0.003`, the insurer's offered premium rebate in Year 2 (`OPR_2`) must have been lower than the rebate offered in Year 1 (`OPR_1`)."}, "Answer": ["B", "C", "D"], "pi_justification": "This item assesses the ability to interpret the provided data table in the context of the described dynamic model of adverse selection. It uses a 'Reverse-Reasoning' strategy, asking the user to identify valid conclusions that must be true given the observed outcomes. \n- **A (Correct):** The drop in participation from 84% to 22% implies the pool of participants in Year 1 was healthier than average, leading to lower-than-expected savings and thus a lower `OPR_2`. \n- **B (Correct):** A direct observation from the table. For `m=0`, `m=0.05`, and `m=0.1`, as `r` increases, the Year 1 participation rate decreases. This is because higher `r` increases the demanded premium rebate. \n- **C (Distractor - Almost Right):** The market is stable, but the reason is that the initial rebate attracts *all* individuals (100% participation), which prevents adverse selection. The term 'unhealthiest' is correct, but the key is *full* participation. The phrasing is subtly less precise than the underlying mechanism. More importantly, the paper notes that even with `r>0` and `m=0.1`, the spiral is very slow, so stability isn't *exclusive* to the `r=0` case, although it is the most stable. \n- **D (Correct):** A direct observation from the table. For `m=0`, participation is 56%, 28%, and 18% in Year 1, but drops to 0% for all `r` values in Year 2. This demonstrates the spiral's power when moral hazard savings are absent.", "qid": "164", "question": "### Background\n\nA simulated health insurance market evolves over multiple years. An insurer offers a single, community-rated premium rebate for a €500 deductible. Individuals are heterogeneous in health risk and are characterized by their risk aversion (`r`) and their potential for moral hazard reduction (`m`). The market can experience an \"adverse selection spiral,\" where the market for the deductible option collapses over time.\n\n### Data / Model Specification\n\nThe market evolves according to two rules:\n1.  **Consumer Choice:** Individuals `i` take the deductible in year `t` if the Offered Premium Rebate (`OPR_t`) exceeds their Demanded Premium Rebate (`DPR_i`).\n2.  **Insurer Pricing:** The insurer sets `OPR_{t+1}` equal to the average realized cost savings from the pool of individuals who took the deductible in year `t`.\n\nAdverse selection occurs because individuals with lower health risks have lower `DPR_i` and are thus more likely to opt-in.\n\n**Table 1: Percentage of Insured Taking a Deductible (Scenario I)**\n\n| Year | Level of Risk Aversion | m=0 | m = 0.05 | m = 0.1 |\n| :--- | :--- | :-- | :--- | :--- |\n| 1 | r=0.000 | 56 | 93 | 100 |\n| | r=0.003 | 28 | 84 | 99 |\n| | r=0.005 | 18 | 76 | 99 |\n| 2 | r=0.000 | 0 | 79 | 100 |\n| | r=0.003 | 0 | 22 | 98 |\n| | r=0.005 | 0 | 0 | 96 |\n| 3 | r=0.000 | 0 | 23 | 100 |\n| | r=0.003 | 0 | 0 | 97 |\n| | r=0.005 | 0 | 0 | 88 |\n| 4 | r=0.000 | 0 | 0 | 100 |\n| | r=0.003 | 0 | 0 | 95 |\n| | r=0.005 | 0 | 0 | 55 |\n\n---\n\nBased on the data in Table 1 and the model description, which of the following statements are valid conclusions about the dynamics of the adverse selection spiral?\n", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: import pandas as pd\n\ndata = {\n    'Year': [1,1,1,2...\n\n> 🔍 **Analyzing options**\n> ***- The task is to identify valid conclusions about the adverse selection spiral from Table 1 data.***\n\n\nB,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 339, "Question": "### Background\n\n**Research Question.** How can the theoretical properties of the mortgage risk model be used to justify and assess the validity of using a simpler infinite-horizon calculation to approximate a more complex finite-term risk?\n\n**Setting.** The paper proposes using the infinite-horizon survival probability, `\\psi^\\infty(u)`, which is often analytically tractable, as an approximation for the finite-time survival probability, `\\psi(u,t)`, which is relevant for fixed-term loans but harder to compute. The validity of this approximation depends on the rate of convergence between the two.\n\n**Variables and Parameters.**\n\n*   `\\psi(u,t)`: Probability of repaying loan `u` before time `t`.\n*   `\\psi^\\infty(u)`: Probability of ever repaying loan `u` (`\\lim_{t\\to\\infty} \\psi(u,t)`).\n*   `W`: Random variable for the inter-arrival time between disasters.\n*   `M_W(\\theta) = \\mathbf{E}[e^{\\theta W}]`: The moment generating function (MGF) of `W`.\n*   `X`: Random variable for disaster severity.\n\n---\n\n### Data / Model Specification\n\nThe paper's key theoretical result on convergence is:\n\n**Theorem:** Assume there exists `\\theta_0 > 0` such that the MGF `M_W(\\theta)` exists for `\\theta < \\theta_0`. Then, the convergence is exponentially fast. For any `u>0`, there exists `C(u)>0` such that:\n\n  \n0 \\le \\psi^{\\infty}(u) - \\psi(u,t) \\le C(u)e^{-\\xi t} \\quad \\text{(Eq. 1)}\n \n\nfor some decay rate `\\xi > 0`.\n\n---\n\n### Question\n\nSuppose the assumption that the MGF of `W` exists for some `\\theta > 0` is violated because the inter-arrival time `W` follows a heavy-tailed distribution (e.g., Pareto). Select all of the following statements that are true consequences of this violation.\n", "Options": {"A": "The convergence of `\\psi(u,t)` to `\\psi^\\infty(u)` would be polynomial, not exponential.", "B": "The survival probability `\\psi(u,t)` would fail to converge to `\\psi^\\infty(u)` as `t` goes to infinity.", "C": "The expected time between disasters, `E[W]`, would be infinite.", "D": "Using the infinite-horizon default probability would likely lead to a significant underestimation of the true 30-year default risk."}, "Answer": ["A", "D"], "pi_justification": "This item assesses understanding of the theoretical underpinnings of the paper's approximation method. It uses the **Scenario Application** strategy by asking for the consequences of violating a key assumption. The options are designed to discriminate between precise understanding and common misconceptions. Correct options (A, C) state the true consequences (slower convergence, risk underestimation). Distractors include a conceptual opposite (B: 'no convergence' vs. 'slower convergence') and a subtle confusion (D: equating heavy tails with an infinite mean, which is not necessarily true or the core issue).", "qid": "339", "question": "### Background\n\n**Research Question.** How can the theoretical properties of the mortgage risk model be used to justify and assess the validity of using a simpler infinite-horizon calculation to approximate a more complex finite-term risk?\n\n**Setting.** The paper proposes using the infinite-horizon survival probability, `\\psi^\\infty(u)`, which is often analytically tractable, as an approximation for the finite-time survival probability, `\\psi(u,t)`, which is relevant for fixed-term loans but harder to compute. The validity of this approximation depends on the rate of convergence between the two.\n\n**Variables and Parameters.**\n\n*   `\\psi(u,t)`: Probability of repaying loan `u` before time `t`.\n*   `\\psi^\\infty(u)`: Probability of ever repaying loan `u` (`\\lim_{t\\to\\infty} \\psi(u,t)`).\n*   `W`: Random variable for the inter-arrival time between disasters.\n*   `M_W(\\theta) = \\mathbf{E}[e^{\\theta W}]`: The moment generating function (MGF) of `W`.\n*   `X`: Random variable for disaster severity.\n\n---\n\n### Data / Model Specification\n\nThe paper's key theoretical result on convergence is:\n\n**Theorem:** Assume there exists `\\theta_0 > 0` such that the MGF `M_W(\\theta)` exists for `\\theta < \\theta_0`. Then, the convergence is exponentially fast. For any `u>0`, there exists `C(u)>0` such that:\n\n  \n0 \\le \\psi^{\\infty}(u) - \\psi(u,t) \\le C(u)e^{-\\xi t} \\quad \\text{(Eq. 1)}\n \n\nfor some decay rate `\\xi > 0`.\n\n---\n\n### Question\n\nSuppose the assumption that the MGF of `W` exists for some `\\theta > 0` is violated because the inter-arrival time `W` follows a heavy-tailed distribution (e.g., Pareto). Select all of the following statements that are true consequences of this violation.\n", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 409, "Question": "### Background\n\n**Research Question.** This case examines the methodology for estimating the causal effect of a mandatory increase in corporate board independence on a firm's Corporate Social Responsibility (CSR) engagement, focusing on the difference-in-differences (DiD) research design.\n\n**Setting.** The analysis centers on the Sarbanes-Oxley (SOX) Act of 2002, which mandated that listed firms have a majority of independent directors. This regulatory shock created a quasi-natural experiment, dividing firms into a \"treatment\" group (those forced to change) and a \"control\" group (those already compliant).\n\n**Variables & Parameters.**\n\n*   `CSR_{it}`: The CSR score for firm `i` in year `t`.\n*   `post-SOX_t`: A binary variable equal to 1 for years after 2002, and 0 otherwise.\n*   `noncompliant_i`: A time-invariant binary variable equal to 1 if firm `i` did not have a majority of independent directors in 2002 (the \"treatment group\"), and 0 otherwise (the \"control group\").\n\n---\n\n### Data / Model Specification\n\nThe following difference-in-differences (DiD) model is estimated with firm fixed effects (`α_i`):\n\n  \nCSR_{it} = \\alpha_i + \\beta_1 \\text{post-SOX}_t + \\beta_3 (\\text{post-SOX}_t \\times \\text{noncompliant}_i) + \\gamma'\\text{controls}_{it} + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Summary Statistics for Compliant and Noncompliant Firms (Pre-SOX)**\n\n| Variable | Compliant | Noncompliant | Difference | t-statistic |\n| :--- | :--- | :--- | :--- | :--- |\n| CSR score | 0.397 | 0.137 | 0.260 | 2.142 |\n| R&D expense/total assets | 0.028 | 0.021 | 0.007 | 3.078 |\n\n**Table 2: Main DiD Regression Results (Firm Fixed Effects)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `Noncompliant × post-SOX` (β₃) | -0.591 | (-4.90) |\n| `post-SOX` (β₁) | 0.414 | (6.74) |\n\n---\n\n### Question\n\nThe tables below present key data and the main difference-in-differences (DiD) results. Based on the model specification and the data, which of the following statements are valid interpretations or conclusions? Select all that apply.", "Options": {"A": "The total net change in CSR for a typical noncompliant firm following the SOX mandate is an estimated decrease of 0.177 points.", "B": "The coefficient on `post-SOX` (β₁ = 0.414) shows that compliant firms, on average, increased their CSR score by 0.414 points after SOX, representing the secular trend in the absence of the treatment.", "C": "The significant pre-SOX differences between compliant and noncompliant firms shown in Table 1 strongly validate the parallel trends assumption required for the DiD model.", "D": "The coefficient on the interaction term (β₃ = -0.591) represents the estimated causal effect of the SOX mandate, indicating that noncompliant firms reduced their CSR score by 0.591 points relative to the change observed in compliant firms."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses a deep understanding of the difference-in-differences methodology, from interpreting coefficients to performing calculations. It uses a 'Computational Judgment' strategy for one option and 'Atomic Decomposition' for others. The distractor (D) targets a fundamental and common misconception about the parallel trends assumption, making it highly diagnostic.", "qid": "409", "question": "### Background\n\n**Research Question.** This case examines the methodology for estimating the causal effect of a mandatory increase in corporate board independence on a firm's Corporate Social Responsibility (CSR) engagement, focusing on the difference-in-differences (DiD) research design.\n\n**Setting.** The analysis centers on the Sarbanes-Oxley (SOX) Act of 2002, which mandated that listed firms have a majority of independent directors. This regulatory shock created a quasi-natural experiment, dividing firms into a \"treatment\" group (those forced to change) and a \"control\" group (those already compliant).\n\n**Variables & Parameters.**\n\n*   `CSR_{it}`: The CSR score for firm `i` in year `t`.\n*   `post-SOX_t`: A binary variable equal to 1 for years after 2002, and 0 otherwise.\n*   `noncompliant_i`: A time-invariant binary variable equal to 1 if firm `i` did not have a majority of independent directors in 2002 (the \"treatment group\"), and 0 otherwise (the \"control group\").\n\n---\n\n### Data / Model Specification\n\nThe following difference-in-differences (DiD) model is estimated with firm fixed effects (`α_i`):\n\n  \nCSR_{it} = \\alpha_i + \\beta_1 \\text{post-SOX}_t + \\beta_3 (\\text{post-SOX}_t \\times \\text{noncompliant}_i) + \\gamma'\\text{controls}_{it} + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Summary Statistics for Compliant and Noncompliant Firms (Pre-SOX)**\n\n| Variable | Compliant | Noncompliant | Difference | t-statistic |\n| :--- | :--- | :--- | :--- | :--- |\n| CSR score | 0.397 | 0.137 | 0.260 | 2.142 |\n| R&D expense/total assets | 0.028 | 0.021 | 0.007 | 3.078 |\n\n**Table 2: Main DiD Regression Results (Firm Fixed Effects)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `Noncompliant × post-SOX` (β₃) | -0.591 | (-4.90) |\n| `post-SOX` (β₁) | 0.414 | (6.74) |\n\n---\n\n### Question\n\nThe tables below present key data and the main difference-in-differences (DiD) results. Based on the model specification and the data, which of the following statements are valid interpretations or conclusions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 403, "Question": "### Background\n\n**Research Question.** This paper investigates two distinct facets of market efficiency. The first is the deviation of prices from a random walk, often attributed to trading-related noise. The second is *informational efficiency*, which concerns the degree to which prices incorporate private information. This question explores how a secular increase in market liquidity, driven by reductions in the minimum tick size, affected both types of efficiency.\n\n**Setting and Sample.** The study examines size-sorted portfolios of NYSE stocks across three tick-size regimes: Eighths (1993-1997), Sixteenths (1997-2001), and Decimal (2001-2002).\n\n### Data / Model Specification\n\n**Metric 1: Random Walk Deviation.** The deviation from a random walk is measured by a variance ratio:\n  \nVR = \\frac{q \\cdot \\text{Var}(\\text{5-minute returns})}{\\text{Var}(\\text{open-to-close returns})} \\quad \\text{(Eq. 1)}\n \nwhere `q` is the number of 5-minute intervals in a trading day. For a perfect random walk, `VR = 1`. A `VR > 1` suggests negative serial correlation in short-horizon returns, often caused by microstructure noise like bid-ask bounce.\n\n**Metric 2: Informational Efficiency.** Following French and Roll (1986), informational efficiency is assessed by jointly analyzing two statistics:\n1.  The **Informational Variance Ratio** (`VR_Info`), defined as the ratio of per-hour open-to-close return variance to per-hour close-to-open return variance. An increase in `VR_Info` can be caused by either (i) more private information being incorporated during trading hours, or (ii) more mispricing (e.g., noise trading, overreactions) during trading hours.\n2.  The **First-Order Daily Return Autocorrelation** (`ρ_1`), which serves as a proxy for mispricing. Positive autocorrelation is consistent with investor underreaction or slow price adjustment.\n\n**Table 1. Efficiency Metrics by Tick Regime**\n\n| | Eighths | Sixteenths | Decimals |\n| :--- | :--- | :--- | :--- |\n| **Panel A: Random Walk Variance Ratio (`VR`)** | | | |\n| Large firms | 1.21 | 1.21 | 1.10 |\n| Mid-cap firms | 1.81 | 1.82 | 1.36* |\n| Small firms | 2.28 | 2.38 | 1.79* |\n| **Panel B: Informational Variance Ratio (`VR_Info`)** | | | |\n| Small firms | 8.95 | 15.33 | 22.16* |\n| **Panel C: Daily Return Autocorrelation (`ρ_1`)** | | | |\n| Small firms | 0.2121 | 0.0814* | 0.0339* |\n| (p-value for `ρ_1=0`) | (0.000) | (0.014) | (0.458) |\n\n*An asterisk (*) indicates the value in the Decimals/Sixteenths regime is statistically different from the Eighths regime at the 5% level.*\n\n### Question\n\nBased on the provided data for Small Firms and the paper's reasoning, which of the following conclusions are supported?", "Options": {"A": "The significant decline in daily return autocorrelation (`ρ_1`) from a highly significant 0.2121 to an insignificant 0.0339 suggests that market mispricing diminished in the more liquid decimal regime.", "B": "The decline in the random walk variance ratio (`VR`) from 2.28 to 1.79 indicates that prices for small firms moved further away from a random walk benchmark.", "C": "The sharp increase in the informational variance ratio (`VR_Info`) from 8.95 to 22.16 provides unambiguous evidence of increased mispricing during trading hours.", "D": "The joint observation of a rising informational variance ratio (`VR_Info`) and a falling autocorrelation (`ρ_1`) supports the conclusion that increased incorporation of private information, rather than increased mispricing, drove the rise in trading-hour volatility."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize multiple empirical findings to support a nuanced conclusion about informational efficiency. It uses an Atomic Decomposition strategy, breaking the paper's complex argument into verifiable statements. Distractor C is a Conceptual Opposite (interpreting a move towards the benchmark of 1 as a move away). Distractor D represents a Step-Omission Error (failing to use the autocorrelation data from Panel C to resolve the ambiguity of the variance ratio in Panel B).", "qid": "403", "question": "### Background\n\n**Research Question.** This paper investigates two distinct facets of market efficiency. The first is the deviation of prices from a random walk, often attributed to trading-related noise. The second is *informational efficiency*, which concerns the degree to which prices incorporate private information. This question explores how a secular increase in market liquidity, driven by reductions in the minimum tick size, affected both types of efficiency.\n\n**Setting and Sample.** The study examines size-sorted portfolios of NYSE stocks across three tick-size regimes: Eighths (1993-1997), Sixteenths (1997-2001), and Decimal (2001-2002).\n\n### Data / Model Specification\n\n**Metric 1: Random Walk Deviation.** The deviation from a random walk is measured by a variance ratio:\n  \nVR = \\frac{q \\cdot \\text{Var}(\\text{5-minute returns})}{\\text{Var}(\\text{open-to-close returns})} \\quad \\text{(Eq. 1)}\n \nwhere `q` is the number of 5-minute intervals in a trading day. For a perfect random walk, `VR = 1`. A `VR > 1` suggests negative serial correlation in short-horizon returns, often caused by microstructure noise like bid-ask bounce.\n\n**Metric 2: Informational Efficiency.** Following French and Roll (1986), informational efficiency is assessed by jointly analyzing two statistics:\n1.  The **Informational Variance Ratio** (`VR_Info`), defined as the ratio of per-hour open-to-close return variance to per-hour close-to-open return variance. An increase in `VR_Info` can be caused by either (i) more private information being incorporated during trading hours, or (ii) more mispricing (e.g., noise trading, overreactions) during trading hours.\n2.  The **First-Order Daily Return Autocorrelation** (`ρ_1`), which serves as a proxy for mispricing. Positive autocorrelation is consistent with investor underreaction or slow price adjustment.\n\n**Table 1. Efficiency Metrics by Tick Regime**\n\n| | Eighths | Sixteenths | Decimals |\n| :--- | :--- | :--- | :--- |\n| **Panel A: Random Walk Variance Ratio (`VR`)** | | | |\n| Large firms | 1.21 | 1.21 | 1.10 |\n| Mid-cap firms | 1.81 | 1.82 | 1.36* |\n| Small firms | 2.28 | 2.38 | 1.79* |\n| **Panel B: Informational Variance Ratio (`VR_Info`)** | | | |\n| Small firms | 8.95 | 15.33 | 22.16* |\n| **Panel C: Daily Return Autocorrelation (`ρ_1`)** | | | |\n| Small firms | 0.2121 | 0.0814* | 0.0339* |\n| (p-value for `ρ_1=0`) | (0.000) | (0.014) | (0.458) |\n\n*An asterisk (*) indicates the value in the Decimals/Sixteenths regime is statistically different from the Eighths regime at the 5% level.*\n\n### Question\n\nBased on the provided data for Small Firms and the paper's reasoning, which of the following conclusions are supported?", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 362, "Question": "### Background\n\n**Research Question.** How can Monte Carlo simulation be used to price options under stochastic volatility, particularly when the correlation `ρ` between the stock price and volatility is non-zero?\n\n**Setting / Data-Generating Environment.** To price an option when `ρ≠0`, one must simulate the joint path of the stock price `S` and variance `V` in a risk-neutral world. The simulation can be improved with variance reduction techniques.\n\n---\n\n### Data / Model Specification\n\nThe joint processes are discretized using two independent standard normal random variates, `uᵢ` and `vᵢ`:\n  \nS_{i} = S_{i-1} \\exp\\left[ (r - V_{i-1}/2)\\Delta t + \\sqrt{V_{i-1}\\Delta t} \\cdot u_i \\right] \\quad \\text{(Eq. (1))}\n \n  \nV_{i} = V_{i-1} \\exp\\left[ (\\mu - \\xi^2/2)\\Delta t + \\xi\\sqrt{\\Delta t} \\cdot (\\rho u_i + \\sqrt{1-\\rho^2}v_i) \\right] \\quad \\text{(Eq. (2))}\n \nTwo variance reduction techniques mentioned are:\n1.  **Antithetic Variates:** Using the opposite random shocks (e.g., `-uᵢ`, `-vᵢ`) to generate a second, negatively correlated price estimate.\n2.  **Control Variates:** Using a related variable with a known mean (like a standard B-S option price) that is highly correlated with the variable of interest to reduce variance.\n\n---\n\n### Question\n\nWhen implementing the Monte Carlo simulation for the `ρ≠0` case, which of the following statements about the procedure and its refinement are correct? (Select all that apply)", "Options": {"A": "The shock to the variance process in Eq. (2) is constructed using two independent normal variates (`uᵢ`, `vᵢ`) to ensure the final shocks to `S` and `V` have the specified correlation `ρ`.", "B": "The control variate technique is effective because the primary driver of the option's value (the stock price path, driven by `uᵢ`) is shared between the stochastic volatility price and the standard B-S price, inducing high positive correlation.", "C": "The antithetic variate technique involves using shocks `-uᵢ` and `-vᵢ` because this generates a new price estimate that is independent of the original, reducing variance when averaged.", "D": "The discretized stock price `Sᵢ` in Eq. (1) depends on the correlated shock `ρuᵢ + sqrt(1-ρ²)vᵢ`."}, "Answer": ["A", "B"], "pi_justification": "This item tests the procedural details of the advanced simulation. It is a 'select-all-valid' task. Option A tests the construction of correlated shocks. Option B tests the statistical intuition for the control variate method. Distractors are based on a 'Conceptual Opposite' error for variance reduction (C) and a 'Formula Misuse' error in applying the shocks (D).", "qid": "362", "question": "### Background\n\n**Research Question.** How can Monte Carlo simulation be used to price options under stochastic volatility, particularly when the correlation `ρ` between the stock price and volatility is non-zero?\n\n**Setting / Data-Generating Environment.** To price an option when `ρ≠0`, one must simulate the joint path of the stock price `S` and variance `V` in a risk-neutral world. The simulation can be improved with variance reduction techniques.\n\n---\n\n### Data / Model Specification\n\nThe joint processes are discretized using two independent standard normal random variates, `uᵢ` and `vᵢ`:\n  \nS_{i} = S_{i-1} \\exp\\left[ (r - V_{i-1}/2)\\Delta t + \\sqrt{V_{i-1}\\Delta t} \\cdot u_i \\right] \\quad \\text{(Eq. (1))}\n \n  \nV_{i} = V_{i-1} \\exp\\left[ (\\mu - \\xi^2/2)\\Delta t + \\xi\\sqrt{\\Delta t} \\cdot (\\rho u_i + \\sqrt{1-\\rho^2}v_i) \\right] \\quad \\text{(Eq. (2))}\n \nTwo variance reduction techniques mentioned are:\n1.  **Antithetic Variates:** Using the opposite random shocks (e.g., `-uᵢ`, `-vᵢ`) to generate a second, negatively correlated price estimate.\n2.  **Control Variates:** Using a related variable with a known mean (like a standard B-S option price) that is highly correlated with the variable of interest to reduce variance.\n\n---\n\n### Question\n\nWhen implementing the Monte Carlo simulation for the `ρ≠0` case, which of the following statements about the procedure and its refinement are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 347, "Question": "### Background\n\n**Research Question.** This case explores potential econometric biases in predictive regressions and appropriate methods to address them.\n\n**Setting and Sample.** An econometrician is concerned about potential biases when estimating the predictive regression `r_{t+1} = α + β_1 F_{1,t}^{\\text{short}} + ε_{t+1}` using Ordinary Least Squares (OLS), where `F_{1,t}^{\\text{short}}` is a persistent financial time series predictor.\n\n### Data / Model Specification\n\nThe predictive regression model is:\n  \nr_{t+1} = \\alpha + \\beta_1 F_{1,t}^{\\text{short}} + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \nThe predictor `F_{1,t}^{\\text{short}}` is known to be persistent (highly autocorrelated).\n\n### Question\n\nSelect all statements that accurately describe a potential econometric problem in this setting and a valid methodological response.", "Options": {"A": "A valid approach to mitigate this bias is to use an instrumental variables (IV) framework, for instance, by using the lagged predictor `F_{1,t-1}^{\\text{short}}` as an instrument for `F_{1,t}^{\\text{short}}`.", "B": "A key concern is the Stambaugh (1999) bias, which can affect the OLS estimate of `β_1` if the predictor is persistent and its innovations are contemporaneously correlated with the return innovations.", "C": "The Stambaugh bias can be completely eliminated by ensuring the sample size is sufficiently large (e.g., > 1000 observations), as the bias is only a small-sample problem.", "D": "The only significant issue is potential autocorrelation in the errors `ε_{t+1}`, which can be fixed by using Newey-West standard errors, leaving the OLS coefficient `β_1` unbiased."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a **Reverse-Reasoning** and **Premise/Assumption Packaging** strategy to assess deep econometric knowledge. It requires identifying the specific bias relevant to predictive regressions (Option A) and its corresponding solution (Option B). Distractors offer common but incorrect or incomplete solutions: C confuses a fix for standard errors with a fix for a biased coefficient (True but Irrelevant/Incomplete), and D mischaracterizes the nature of the bias, which persists in any finite sample (Conceptual Error).", "qid": "347", "question": "### Background\n\n**Research Question.** This case explores potential econometric biases in predictive regressions and appropriate methods to address them.\n\n**Setting and Sample.** An econometrician is concerned about potential biases when estimating the predictive regression `r_{t+1} = α + β_1 F_{1,t}^{\\text{short}} + ε_{t+1}` using Ordinary Least Squares (OLS), where `F_{1,t}^{\\text{short}}` is a persistent financial time series predictor.\n\n### Data / Model Specification\n\nThe predictive regression model is:\n  \nr_{t+1} = \\alpha + \\beta_1 F_{1,t}^{\\text{short}} + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \nThe predictor `F_{1,t}^{\\text{short}}` is known to be persistent (highly autocorrelated).\n\n### Question\n\nSelect all statements that accurately describe a potential econometric problem in this setting and a valid methodological response.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 120, "Question": "### Background\n\n**Research Question.** Has the efficiency of Indonesian banks converged over time, and how did the Asian financial crisis and subsequent reforms affect the speed of this convergence?\n\n**Setting / Data-Generating Environment.** Panel data analysis of Indonesian bank efficiency is used to test for `β`-convergence (laggards catching up) before and after the 1997-1999 financial crisis.\n\n**Variables & Parameters.**\n\n*   `$\\ln(\\hat{\\hat{\\rho}}_{j,t})$`: Log of the bias-corrected efficiency score for bank `j` at time `t`.\n*   `$\\theta$`: The `β`-convergence parameter. A negative value indicates catch-up.\n\n---\n\n### Data / Model Specification\n\nThe model for `β`-convergence is given by:\n\n  \n\\ln\\hat{\\hat{\\rho}}_{j,t} - \\ln\\hat{\\hat{\\rho}}_{j,t-1} = \\phi + \\theta \\ln\\hat{\\hat{\\rho}}_{j,t} + \\nu_{j,t} \\quad \\text{(Eq. 1)}\n \n\nThis implies an autoregressive process for log efficiency: `$\\ln(\\hat{\\hat{\\rho}}_{j,t}) = c + \\lambda_1 \\ln(\\hat{\\hat{\\rho}}_{j,t-1})$`, where `$\\lambda_1 = 1/(1-\\theta)$`.\n\n**Table 1. `β`-Convergence Results (Model 2)**\n\n| Period | Parameter | Coefficient | P-value |\n| :--- | :--- | :--- | :--- |\n| 2000-2007 | `$\\theta$` | -0.8249 | 0.0000 |\n\n---\n\n### Question\n\nThe speed of `β`-convergence can be measured by the half-life (`$T_{1/2}$`), the time it takes for half of an efficiency gap to dissipate. The formula is `$T_{1/2} = -\\ln(2) / \\ln(\\lambda_1)$`, where `$\\lambda_1 = 1/(1-\\theta)$`. Using the estimated `$\\theta = -0.8249$` for the post-crisis period from Table 1, select all true statements.", "Options": {"A": "A smaller absolute value of `$\\theta$` (e.g., -0.5 instead of -0.8249) would imply a shorter half-life and faster convergence.", "B": "The value of `$\\lambda_1$` implies that, on average, an inefficient bank closes less than 30% of its efficiency gap each year.", "C": "The implied coefficient on the lagged efficiency term, `$\\lambda_1$`, is approximately 0.548.", "D": "The calculated half-life of an efficiency gap is approximately 1.15 years."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Computational Judgment strategy to test the quantitative implications of the convergence parameter. It breaks the calculation into an intermediate step (calculating `$\\lambda_1$`) and a final step (calculating the half-life), allowing for multiple correct assertions. Distractor C is a step-omission error (misinterpreting `$\\lambda_1$`). Distractor D is a conceptual opposite regarding the relationship between the parameter's magnitude and the speed of convergence.", "qid": "120", "question": "### Background\n\n**Research Question.** Has the efficiency of Indonesian banks converged over time, and how did the Asian financial crisis and subsequent reforms affect the speed of this convergence?\n\n**Setting / Data-Generating Environment.** Panel data analysis of Indonesian bank efficiency is used to test for `β`-convergence (laggards catching up) before and after the 1997-1999 financial crisis.\n\n**Variables & Parameters.**\n\n*   `$\\ln(\\hat{\\hat{\\rho}}_{j,t})$`: Log of the bias-corrected efficiency score for bank `j` at time `t`.\n*   `$\\theta$`: The `β`-convergence parameter. A negative value indicates catch-up.\n\n---\n\n### Data / Model Specification\n\nThe model for `β`-convergence is given by:\n\n  \n\\ln\\hat{\\hat{\\rho}}_{j,t} - \\ln\\hat{\\hat{\\rho}}_{j,t-1} = \\phi + \\theta \\ln\\hat{\\hat{\\rho}}_{j,t} + \\nu_{j,t} \\quad \\text{(Eq. 1)}\n \n\nThis implies an autoregressive process for log efficiency: `$\\ln(\\hat{\\hat{\\rho}}_{j,t}) = c + \\lambda_1 \\ln(\\hat{\\hat{\\rho}}_{j,t-1})$`, where `$\\lambda_1 = 1/(1-\\theta)$`.\n\n**Table 1. `β`-Convergence Results (Model 2)**\n\n| Period | Parameter | Coefficient | P-value |\n| :--- | :--- | :--- | :--- |\n| 2000-2007 | `$\\theta$` | -0.8249 | 0.0000 |\n\n---\n\n### Question\n\nThe speed of `β`-convergence can be measured by the half-life (`$T_{1/2}$`), the time it takes for half of an efficiency gap to dissipate. The formula is `$T_{1/2} = -\\ln(2) / \\ln(\\lambda_1)$`, where `$\\lambda_1 = 1/(1-\\theta)$`. Using the estimated `$\\theta = -0.8249$` for the post-crisis period from Table 1, select all true statements.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 310, "Question": "### Background\nAn insurer is analyzing the risk profile of a specific fleet of vehicles. The fleet constitutes 50% of the traffic ($\\rho^{\\Phi}=0.5$) and operates under a moderately aggressive driving configuration, denoted $\\xi^{2a}$. The insurer uses a sophisticated microscopic traffic simulation model to project the aggregate annual loss, $L$, for this fleet.\n\n### Data / Model Specification\nAn insurer is deciding between two premium principles to price this fleet's policy:\n\n(a) **Variance Principle:** Premium = E(L) + $\\alpha \\cdot \\sqrt{\\text{Var(L)}}$, with a risk loading factor $\\alpha = 0.25$.\n(b) **Capital Cost Principle:** Premium = E(L) + $\\beta \\cdot (\\text{ES}_{0.95}(\\text{L}) - \\text{E(L)})$, with a cost-of-capital factor $\\beta = 0.10$.\n\nTable 1 presents the simulation results for the aggregate annual loss $L$ under various model specifications. All values are based on 10,000 independent samples of $L$.\n\n**Table 1: Statistical Functionals of L for $\\rho^{\\Phi}=0.5$ and $\\xi^{2a}$**\n| | **Gamma** | | | **Log-normal** | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** |\n| **E(L)** | 1577.8 | 1571.5 | 1578.4 | 1581.7 | 1576.2 | 1582.7 |\n| **Var(L)** | 160,179.5 | 247,943.9 | 626,190.6 | 162,067.5 | 247,069.8 | 614,713.4 |\n| **ES$_{0.95}$(L)** | 2468.9 | 2748.0 | 3628.9 | 2491.4 | 2772.5 | 3838.4 |\n\n### Question\nAn insurer uses the data for the Gamma distribution with $c_v=1.0$ from Table 1 to calculate the annual premium. Based on the two pricing principles provided, which of the following conclusions are valid?", "Options": {"A": "Using the Capital Cost Principle with a cost-of-capital factor $\\beta = 0.10$, the risk loading component of the premium (the amount added to E(L)) is greater than 115.", "B": "The premium calculated using the Capital Cost Principle ($\\beta=0.10$) is higher than the premium calculated using the Variance Principle ($\\alpha=0.25$).", "C": "Using the Variance Principle with a risk loading factor $\\alpha = 0.25$, the calculated premium is less than 1700.", "D": "If the insurer switched to the Log-normal model ($c_v=1.0$), the premium from the Variance Principle would decrease because Var(L) is slightly lower."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses multi-step computational judgment and comparison of actuarial pricing models. It uses a 'Computational Judgment' strategy, requiring the user to calculate premiums under two different frameworks and compare the results. Distractors are based on common errors: a direct comparison error (C) and a conceptual error where a correct premise leads to a false conclusion (D).", "qid": "310", "question": "### Background\nAn insurer is analyzing the risk profile of a specific fleet of vehicles. The fleet constitutes 50% of the traffic ($\\rho^{\\Phi}=0.5$) and operates under a moderately aggressive driving configuration, denoted $\\xi^{2a}$. The insurer uses a sophisticated microscopic traffic simulation model to project the aggregate annual loss, $L$, for this fleet.\n\n### Data / Model Specification\nAn insurer is deciding between two premium principles to price this fleet's policy:\n\n(a) **Variance Principle:** Premium = E(L) + $\\alpha \\cdot \\sqrt{\\text{Var(L)}}$, with a risk loading factor $\\alpha = 0.25$.\n(b) **Capital Cost Principle:** Premium = E(L) + $\\beta \\cdot (\\text{ES}_{0.95}(\\text{L}) - \\text{E(L)})$, with a cost-of-capital factor $\\beta = 0.10$.\n\nTable 1 presents the simulation results for the aggregate annual loss $L$ under various model specifications. All values are based on 10,000 independent samples of $L$.\n\n**Table 1: Statistical Functionals of L for $\\rho^{\\Phi}=0.5$ and $\\xi^{2a}$**\n| | **Gamma** | | | **Log-normal** | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** |\n| **E(L)** | 1577.8 | 1571.5 | 1578.4 | 1581.7 | 1576.2 | 1582.7 |\n| **Var(L)** | 160,179.5 | 247,943.9 | 626,190.6 | 162,067.5 | 247,069.8 | 614,713.4 |\n| **ES$_{0.95}$(L)** | 2468.9 | 2748.0 | 3628.9 | 2491.4 | 2772.5 | 3838.4 |\n\n### Question\nAn insurer uses the data for the Gamma distribution with $c_v=1.0$ from Table 1 to calculate the annual premium. Based on the two pricing principles provided, which of the following conclusions are valid?", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 317, "Question": "### Background\n\n**Research Question.** Do recent German legal reforms genuinely empower shareholders, or do they create an illusion of enhanced rights while imposing restrictions that maintain the status quo? \n\n**Setting.** The analysis focuses on two distinct proposals in a draft German bill on \"Corporate Integrity and the Right of Avoidance.\" One proposal addresses shareholder participation in annual meetings, while the other reforms the mechanism for enforcing director liability. The paper suggests that despite their stated aims, these reforms exhibit a \"restrictive tendency.\"\n\n**Variables & Parameters.**\n- **Institutional Investors:** Large investment managers (e.g., mutual funds, pension funds) whose participation is crucial for effective shareholder oversight.\n- **Liquidity:** The ability to trade an asset quickly without affecting its price. A perceived lack of liquidity is a major deterrent for institutional investors.\n- **Derivative Suit:** A lawsuit brought by a shareholder on behalf of the corporation against directors for damages caused to the company.\n- **Duty of Care:** A director's obligation to act on an informed basis. A breach can be caused by ordinary negligence, recklessness, or fraud.\n\n---\n\n### Data / Model Specification\n\nThe draft bill contains two key reforms concerning shareholder rights:\n\n1.  **Voting Participation Reform:** To encourage participation by foreign institutional investors, the ambiguous \"deposit of the shares\" requirement is to be replaced. The new system allows proof of ownership via a certificate from a depositary bank that refers to a **record date** 14 days before the shareholders’ meeting. This clarifies that shares are not subject to a trading \"lock-up.\"\n\n2.  **Director Liability Reform:** The law governing derivative suits is amended:\n    *   **Threshold Lowered:** The ownership threshold for shareholders to initiate a suit is lowered from 5% of capital (or €500,000) to 1% (or €100,000).\n    *   **Procedural Hurdle Added:** Under the proposed §148 AktG, a suit is admissible only with the leave of a district court. The court will only grant leave if the plaintiff presents facts justifying the suspicion that the damage was caused by **\"untruth or recklessness\"** (i.e., fraud or severe negligence).\n\n---\n\n### The Question\n\nBased on the analysis of the two proposed reforms, select all statements that accurately characterize their effects on shareholder rights and corporate governance in Germany.", "Options": {"A": "The 'record date' reform encourages institutional investors to vote by locking up their shares for 14 days, ensuring they have a vested interest in the meeting's outcome.", "B": "The derivative suit reform represents a clear net empowerment of shareholders, as the reduction in the capital threshold to 1% makes it significantly easier to hold directors accountable for any breach of their duty of care.", "C": "The proposed derivative suit reform, despite lowering ownership thresholds, effectively restricts shareholder power by requiring court leave based on a standard of 'untruth or recklessness', thereby shielding directors from suits based on ordinary negligence.", "D": "The 'record date' reform is designed to increase shareholder participation primarily by eliminating a perceived liquidity constraint, which lowers the economic cost of voting for institutional investors."}, "Answer": ["C", "D"], "pi_justification": "This item replaces a QA problem deemed highly suitable for conversion (Score: 9.5). It uses an **Atomic Decomposition** strategy to test the core analytical tasks of the original question: interpreting the net effect of two distinct legal reforms. The item functions as a **Computational Judgment** task, requiring the user to weigh the empowering and restrictive elements of each reform. **Correct options (A, B)** accurately describe the mechanisms of the voting reform and the restrictive nature of the derivative suit reform. **Distractor (C)** represents a common comprehension error, focusing only on the lowered threshold while ignoring the restrictive procedural hurdle. **Distractor (D)** presents a conceptual opposite, claiming the reform creates a lock-up when its entire purpose is to clarify that no lock-up exists.", "qid": "317", "question": "### Background\n\n**Research Question.** Do recent German legal reforms genuinely empower shareholders, or do they create an illusion of enhanced rights while imposing restrictions that maintain the status quo? \n\n**Setting.** The analysis focuses on two distinct proposals in a draft German bill on \"Corporate Integrity and the Right of Avoidance.\" One proposal addresses shareholder participation in annual meetings, while the other reforms the mechanism for enforcing director liability. The paper suggests that despite their stated aims, these reforms exhibit a \"restrictive tendency.\"\n\n**Variables & Parameters.**\n- **Institutional Investors:** Large investment managers (e.g., mutual funds, pension funds) whose participation is crucial for effective shareholder oversight.\n- **Liquidity:** The ability to trade an asset quickly without affecting its price. A perceived lack of liquidity is a major deterrent for institutional investors.\n- **Derivative Suit:** A lawsuit brought by a shareholder on behalf of the corporation against directors for damages caused to the company.\n- **Duty of Care:** A director's obligation to act on an informed basis. A breach can be caused by ordinary negligence, recklessness, or fraud.\n\n---\n\n### Data / Model Specification\n\nThe draft bill contains two key reforms concerning shareholder rights:\n\n1.  **Voting Participation Reform:** To encourage participation by foreign institutional investors, the ambiguous \"deposit of the shares\" requirement is to be replaced. The new system allows proof of ownership via a certificate from a depositary bank that refers to a **record date** 14 days before the shareholders’ meeting. This clarifies that shares are not subject to a trading \"lock-up.\"\n\n2.  **Director Liability Reform:** The law governing derivative suits is amended:\n    *   **Threshold Lowered:** The ownership threshold for shareholders to initiate a suit is lowered from 5% of capital (or €500,000) to 1% (or €100,000).\n    *   **Procedural Hurdle Added:** Under the proposed §148 AktG, a suit is admissible only with the leave of a district court. The court will only grant leave if the plaintiff presents facts justifying the suspicion that the damage was caused by **\"untruth or recklessness\"** (i.e., fraud or severe negligence).\n\n---\n\n### The Question\n\nBased on the analysis of the two proposed reforms, select all statements that accurately characterize their effects on shareholder rights and corporate governance in Germany.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 149, "Question": "### Background\n\nA model of futures pricing is developed where speculator participation is limited by a fixed transaction cost, making the number of traders endogenous. Speculators enter the market until the marginal speculator is indifferent between paying the cost to trade versus not trading at all. This equilibrium condition determines the futures risk premium.\n\n### Data / Model Specification\n\nThe equilibrium percentage futures risk premium (`π`) is determined by the indifference condition of the marginal speculator, leading to the following pricing relation from Proposition 3:\n\n  \n\\pi = \\beta_{\\pi m} \\bar{R}_m \\pm \\sigma_{\\pi} \\sqrt{2\\alpha t(1-\\rho^2)} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `π`: Percentage futures risk premium.\n- `t`: Fixed transaction/setup cost for trading futures.\n- `α`: Coefficient of absolute risk aversion.\n- `β_πm`: Market beta of the percentage futures return.\n- `R̄_m`: Expected net return on the stock market.\n- `σ_π`: Standard deviation of the percentage futures return.\n- `ρ`: Correlation between the futures return `π̃` and the market return `R_m`.\n\n### Question\n\nConsider a scenario where the model's assumption of identical speculators is relaxed, and potential speculators instead have a distribution of setup costs (`t`). Which of the following outcomes would be plausible consequences of this more realistic assumption?\n\nSelect all that apply.", "Options": {"A": "The equilibrium risk premium would become sensitive to the intensity of producer hedging pressure (e.g., the covariance of futures with nonmarketable wealth).", "B": "The supply of speculative services would become perfectly elastic at a positive premium determined by the speculator with the lowest setup cost.", "C": "An increase in producer hedging demand would lead to a higher equilibrium risk premium, as higher-cost speculators would need to be induced to enter the market.", "D": "The equilibrium risk premium would be determined solely by the speculator with the median setup cost, regardless of hedging demand."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to critique and extend the model's core assumptions, specifically by reasoning about the effects of heterogeneous agents. Depth Strategy: Scenario Application. The candidate must apply the logic of the model to a new, more complex scenario (heterogeneous costs) not explicitly modeled in the paper's equations but discussed qualitatively. Distractor Logic: Option B is a Conceptual Opposite; with heterogeneous costs, the supply curve for speculation becomes upward-sloping, not perfectly elastic. Option D is a Conceptual Distractor; the premium is set by the *marginal* entrant, whose identity depends on the level of hedging demand, not a fixed statistical measure like the median. Options A and C are correct and represent the key consequences: an upward-sloping supply of speculation means that higher demand (from more intense hedging) requires a higher price (premium) to attract the next, higher-cost speculator, thus re-establishing the link between hedging intensity and the premium.", "qid": "149", "question": "### Background\n\nA model of futures pricing is developed where speculator participation is limited by a fixed transaction cost, making the number of traders endogenous. Speculators enter the market until the marginal speculator is indifferent between paying the cost to trade versus not trading at all. This equilibrium condition determines the futures risk premium.\n\n### Data / Model Specification\n\nThe equilibrium percentage futures risk premium (`π`) is determined by the indifference condition of the marginal speculator, leading to the following pricing relation from Proposition 3:\n\n  \n\\pi = \\beta_{\\pi m} \\bar{R}_m \\pm \\sigma_{\\pi} \\sqrt{2\\alpha t(1-\\rho^2)} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `π`: Percentage futures risk premium.\n- `t`: Fixed transaction/setup cost for trading futures.\n- `α`: Coefficient of absolute risk aversion.\n- `β_πm`: Market beta of the percentage futures return.\n- `R̄_m`: Expected net return on the stock market.\n- `σ_π`: Standard deviation of the percentage futures return.\n- `ρ`: Correlation between the futures return `π̃` and the market return `R_m`.\n\n### Question\n\nConsider a scenario where the model's assumption of identical speculators is relaxed, and potential speculators instead have a distribution of setup costs (`t`). Which of the following outcomes would be plausible consequences of this more realistic assumption?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 333, "Question": "### Background\n\n**Research Question.** How can two different non-stationary regression specifications, both designed to test for forward rate unbiasedness and using cointegrated regressors, produce diametrically opposed results? This question explores how simultaneity bias can persist in non-stationary settings and how its magnitude is critically dependent on the choice of regressors.\n\n**Setting.** An econometric analysis of two non-stationary models. The first, used by Barnhart and Szakmary, regresses the spot rate `S_t` on the lagged forward rate `F_{t-1}` and the lagged spot rate `S_{t-1}`. The second, used by Frenkel, regresses `S_t` on `F_{t-1}` and the second lag of the forward rate, `F_{t-2}`. Under the null hypothesis of unbiasedness, the true coefficients `[\\beta_1, \\beta_2]` are `[1, 0]`.\n\n**Variables & Parameters.**\n- `S_t`, `S_{t-1}`: Log spot exchange rates.\n- `F_{t-1}`, `F_{t-2}`: Log forward exchange rates.\n- `\\eta_t`: Innovation to the spot rate process, `S_t - S_{t-1}`.\n- `e_{t-1}`: Innovation to the forward rate process, `F_{t-1} - S_{t-1}`.\n- `\\beta_1`, `\\beta_2`: Regression coefficients.\n\n---\n\n### Data / Model Specification\n\nThe analysis relies on the properties of the underlying innovations to the spot and forward rate data generating processes (DGPs).\n\n**Table 1. Summary Statistics of DGP Residuals (Japan)**\n| Statistic | Spot Rate Innovation (`\\eta_t`) | Forward Rate Innovation (`e_{t-1}`) |\n| :--- | :--- | :--- |\n| Variance | 0.0011155 | 0.0000115 |\n| Covariance `cov(e_{t-1}, \\eta_t)` | | -0.000014 |\n\n**Asymptotic Limits of OLS Estimators:**\n- **Barnhart/Szakmary/McCallum Specification (`S_t` on `F_{t-1}`, `S_{t-1}`):**\n      \n    [\\hat{\\beta}_1, \\hat{\\beta}_2] \\to [1, 0] + \\frac{\\mathrm{cov}(\\eta_{t},e_{t-1})-\\mathrm{var}(e_{t-1})}{\\mathrm{var}(e_{t-1})}[1, -1] \\quad \\text{(Eq. (1))}\n     \n- **Frenkel Specification (`S_t` on `F_{t-1}`, `F_{t-2}`):**\n      \n    [\\hat{\\beta}_1, \\hat{\\beta}_2] \\to [1, 0] + \\frac{\\mathrm{cov}(\\eta_{t},e_{t-1})-\\mathrm{var}(e_{t-1})}{2\\mathrm{var}(e_{t-1})+\\mathrm{var}(\\eta_{t})-2\\mathrm{cov}(\\eta_{t},e_{t-1})}[1, -1] \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nBased on the provided data and formulas for the Japanese Yen, which of the following statements are correct? Select all that apply.", "Options": {"A": "The theoretical asymptotic limit for `\\hat{\\beta}_2` in the Barnhart/Szakmary/McCallum specification (Eq. (1)) is approximately -2.22.", "B": "The theoretical asymptotic limit for `\\hat{\\beta}_1` in the Barnhart/Szakmary/McCallum specification (Eq. (1)) is approximately -1.22, indicating severe bias.", "C": "The negligible bias in the Frenkel specification (Eq. (2)) is due to its bias formula's denominator being dominated by the large spot rate error variance, `var(\\eta_t)`.", "D": "The large bias in the Barnhart/Szakmary/McCallum specification (Eq. (1)) is primarily caused by the large variance of the spot rate innovation (`var(\\eta_t)`) appearing in the denominator of its bias formula."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform calculations based on theoretical formulas and interpret the results in the context of the paper's argument. It uses a Computational Judgment strategy with atomic decomposition. Correct options A and B test the calculation for one model and the conceptual reason for the result in the other. Distractor C is based on a common sign error in calculation (the correct value is approx +2.22). Distractor D presents a conceptual opposite regarding the source of the bias.", "qid": "333", "question": "### Background\n\n**Research Question.** How can two different non-stationary regression specifications, both designed to test for forward rate unbiasedness and using cointegrated regressors, produce diametrically opposed results? This question explores how simultaneity bias can persist in non-stationary settings and how its magnitude is critically dependent on the choice of regressors.\n\n**Setting.** An econometric analysis of two non-stationary models. The first, used by Barnhart and Szakmary, regresses the spot rate `S_t` on the lagged forward rate `F_{t-1}` and the lagged spot rate `S_{t-1}`. The second, used by Frenkel, regresses `S_t` on `F_{t-1}` and the second lag of the forward rate, `F_{t-2}`. Under the null hypothesis of unbiasedness, the true coefficients `[\\beta_1, \\beta_2]` are `[1, 0]`.\n\n**Variables & Parameters.**\n- `S_t`, `S_{t-1}`: Log spot exchange rates.\n- `F_{t-1}`, `F_{t-2}`: Log forward exchange rates.\n- `\\eta_t`: Innovation to the spot rate process, `S_t - S_{t-1}`.\n- `e_{t-1}`: Innovation to the forward rate process, `F_{t-1} - S_{t-1}`.\n- `\\beta_1`, `\\beta_2`: Regression coefficients.\n\n---\n\n### Data / Model Specification\n\nThe analysis relies on the properties of the underlying innovations to the spot and forward rate data generating processes (DGPs).\n\n**Table 1. Summary Statistics of DGP Residuals (Japan)**\n| Statistic | Spot Rate Innovation (`\\eta_t`) | Forward Rate Innovation (`e_{t-1}`) |\n| :--- | :--- | :--- |\n| Variance | 0.0011155 | 0.0000115 |\n| Covariance `cov(e_{t-1}, \\eta_t)` | | -0.000014 |\n\n**Asymptotic Limits of OLS Estimators:**\n- **Barnhart/Szakmary/McCallum Specification (`S_t` on `F_{t-1}`, `S_{t-1}`):**\n      \n    [\\hat{\\beta}_1, \\hat{\\beta}_2] \\to [1, 0] + \\frac{\\mathrm{cov}(\\eta_{t},e_{t-1})-\\mathrm{var}(e_{t-1})}{\\mathrm{var}(e_{t-1})}[1, -1] \\quad \\text{(Eq. (1))}\n     \n- **Frenkel Specification (`S_t` on `F_{t-1}`, `F_{t-2}`):**\n      \n    [\\hat{\\beta}_1, \\hat{\\beta}_2] \\to [1, 0] + \\frac{\\mathrm{cov}(\\eta_{t},e_{t-1})-\\mathrm{var}(e_{t-1})}{2\\mathrm{var}(e_{t-1})+\\mathrm{var}(\\eta_{t})-2\\mathrm{cov}(\\eta_{t},e_{t-1})}[1, -1] \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nBased on the provided data and formulas for the Japanese Yen, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 416, "Question": "### Background\n\n**Research Question.** Are the earnings releases that precede a seasoned equity offering (SEO) systematically different in their information content and news valence compared to other earnings releases?\n\n**Setting / Data-Generating Environment.** The study analyzes abnormal returns and return volatility around quarterly earnings releases for firms that conduct an SEO, comparing the quarters before the issue to the quarters after. The theory predicts that firms with good news will delay their issue until after the news is released, and this incentive is strongest when the news is most informative.\n\n**Variables & Parameters.**\n- `Average abnormal return`: The average two-day cumulative abnormal return for firms around an earnings release.\n- `Average excess variance ratio`: A measure of the informativeness of an earnings release, where higher values indicate abnormally high volatility and information content.\n\n### Data / Model Specification\n\n**Table 1. Abnormal Returns and Informativeness at Earnings Release Dates**\n\n| Quarters relative to issue day | -2 | -1 | 1 | 2 |\n|:------------------------------|:-----|:-----|:------|:------|\n| Average abnormal return (%)   | 1.20 | 0.33 | 0.81  | -0.10 |\n| (t-statistic)                 | (6.10)| (1.85)| (4.77)|(-0.52)|\n| Average excess variance ratio | 0.61 | 0.79 | 0.53  | 0.52  |\n| (t-statistic)                 | (4.06)| (5.06)| (5.02)| (4.96)|\n\n*Source: Adapted from the original paper's Table 5.*\n\n### Question\n\nThe theory of time-varying adverse selection predicts that firms self-select when to issue equity based on their private information. Based on the results in Table 1, select all conclusions that are consistent with this self-selection mechanism.", "Options": {"A": "The significantly positive 'Average abnormal return' in quarters -2 and -1 suggests that the sample of issuing firms is disproportionately composed of those that chose to wait for good news to become public before issuing.", "B": "The high 'Average excess variance ratio' across all quarters indicates that earnings releases for issuing firms are generally uninformative.", "C": "The results for quarters 1 and 2 (post-issue) show significantly negative abnormal returns, suggesting the market views the equity issue as a negative long-term signal.", "D": "The high 'Average excess variance ratio' in quarters -2 and -1 indicates that the earnings releases preceding an issue are not only positive but also more informative than typical releases, consistent with firms having the greatest incentive to delay for high-impact news."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses comprehension of the self-selection mechanism underlying Hypothesis 2. It uses an 'atomic decomposition' strategy, testing the interpretation of both the first moment (returns) and second moment (variance) of the data. Options A and B correctly link the data to the theory of self-selection based on news valence and informativeness. Distractor C is a factual error based on the table (post-issue returns are not significantly negative). Distractor D is a conceptual opposite, misinterpreting the meaning of a high variance ratio.", "qid": "416", "question": "### Background\n\n**Research Question.** Are the earnings releases that precede a seasoned equity offering (SEO) systematically different in their information content and news valence compared to other earnings releases?\n\n**Setting / Data-Generating Environment.** The study analyzes abnormal returns and return volatility around quarterly earnings releases for firms that conduct an SEO, comparing the quarters before the issue to the quarters after. The theory predicts that firms with good news will delay their issue until after the news is released, and this incentive is strongest when the news is most informative.\n\n**Variables & Parameters.**\n- `Average abnormal return`: The average two-day cumulative abnormal return for firms around an earnings release.\n- `Average excess variance ratio`: A measure of the informativeness of an earnings release, where higher values indicate abnormally high volatility and information content.\n\n### Data / Model Specification\n\n**Table 1. Abnormal Returns and Informativeness at Earnings Release Dates**\n\n| Quarters relative to issue day | -2 | -1 | 1 | 2 |\n|:------------------------------|:-----|:-----|:------|:------|\n| Average abnormal return (%)   | 1.20 | 0.33 | 0.81  | -0.10 |\n| (t-statistic)                 | (6.10)| (1.85)| (4.77)|(-0.52)|\n| Average excess variance ratio | 0.61 | 0.79 | 0.53  | 0.52  |\n| (t-statistic)                 | (4.06)| (5.06)| (5.02)| (4.96)|\n\n*Source: Adapted from the original paper's Table 5.*\n\n### Question\n\nThe theory of time-varying adverse selection predicts that firms self-select when to issue equity based on their private information. Based on the results in Table 1, select all conclusions that are consistent with this self-selection mechanism.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 300, "Question": "### Background\n\n**Research Question.** Under what specific conditions involving binding order limits can a firm increase its total trading profit by allowing a less-informed (rookie) trader to trade competitively alongside an expert, effectively creating a negative “training cost”?\n\n**Setting.** A firm employs an expert trader (H) with perfect information and a rookie trader (L) whose signal about an asset's value `\\tilde{\\nu}` is corrupted by noise. The firm imposes a binding aggregate order limit `X` and allocates capital between the traders via individual limits `X_H` and `X_L`. A rational market maker sets prices based on the aggregate order flow.\n\n**Key Concepts.**\n- **Competition Effect**: In a duopoly, traders act more aggressively than a monopolist, which reveals information faster and tends to lower total profits.\n- **Information-Hiding Effect**: The rookie's noisy trades obscure the expert's trades, increasing information asymmetry, which can lead to a higher price impact and potentially higher profits for the informed traders as a group.\n\n---\n\n### Data / Model Specification\n\n**Lemma 2** from the paper states that in a constrained duopoly, the aggregate expected profit can be higher than in a constrained monopoly if the parameters fall within a specific range. There is no closed-form solution, but numerical analysis provides the following key results.\n\n**Table 1: Numerical Results for Noisy Duopoly vs. Constrained Monopoly**\n\n| Case | Aggregate Limit (X) | Expert's Limit (X_H) | Change in E[π] vs. Monopoly | Effect of increasing Rookie Noise (η_L) on E[π] |\n| :--- | :--- | :--- | :--- | :--- |\n| 1    | 2.0                 | 1.5 or 1.0           | Higher                       | Positive                            |\n| 2    | 1.0 or 2.0          | 0.5                  | Lower                        | Negative                            |\n\n---\n\n### Question\n\nBased on the provided theory and numerical results in Table 1, select all statements that correctly describe the conditions under which allowing a rookie trader to trade alongside an expert can increase the firm's aggregate expected profit.", "Options": {"A": "In all cases where the aggregate order limit X is constrained, adding a rookie trader is guaranteed to increase firm profits because of the information-hiding effect.", "B": "The profit gains in the successful noisy duopoly case are primarily driven by the rookie's trades being more profitable on a standalone basis than the expert's.", "C": "A high aggregate order limit (e.g., X=2) combined with a high allocation to the expert (e.g., X_H=1.5) can result in higher expected profit compared to a constrained monopoly.", "D": "Under the profitable scenario identified in the table (X=2, X_H=1.5), the 'information-hiding effect' is strong enough that increasing the rookie's signal noise (η_L) is beneficial to the firm."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the paper's core theoretical concepts (competition vs. information-hiding) with specific numerical evidence from a table. It uses an **Atomic Decomposition** strategy, breaking down the conditions for negative training costs into distinct, verifiable statements. Option A tests direct interpretation of Case 1. Option B tests the interpretation of the marginal effect of noise in Case 1. Distractor C presents an overgeneralization (a 'Conceptual: Almost Right' error), while Distractor D posits a 'Conceptual Opposite' explanation for the source of profit gains.", "qid": "300", "question": "### Background\n\n**Research Question.** Under what specific conditions involving binding order limits can a firm increase its total trading profit by allowing a less-informed (rookie) trader to trade competitively alongside an expert, effectively creating a negative “training cost”?\n\n**Setting.** A firm employs an expert trader (H) with perfect information and a rookie trader (L) whose signal about an asset's value `\\tilde{\\nu}` is corrupted by noise. The firm imposes a binding aggregate order limit `X` and allocates capital between the traders via individual limits `X_H` and `X_L`. A rational market maker sets prices based on the aggregate order flow.\n\n**Key Concepts.**\n- **Competition Effect**: In a duopoly, traders act more aggressively than a monopolist, which reveals information faster and tends to lower total profits.\n- **Information-Hiding Effect**: The rookie's noisy trades obscure the expert's trades, increasing information asymmetry, which can lead to a higher price impact and potentially higher profits for the informed traders as a group.\n\n---\n\n### Data / Model Specification\n\n**Lemma 2** from the paper states that in a constrained duopoly, the aggregate expected profit can be higher than in a constrained monopoly if the parameters fall within a specific range. There is no closed-form solution, but numerical analysis provides the following key results.\n\n**Table 1: Numerical Results for Noisy Duopoly vs. Constrained Monopoly**\n\n| Case | Aggregate Limit (X) | Expert's Limit (X_H) | Change in E[π] vs. Monopoly | Effect of increasing Rookie Noise (η_L) on E[π] |\n| :--- | :--- | :--- | :--- | :--- |\n| 1    | 2.0                 | 1.5 or 1.0           | Higher                       | Positive                            |\n| 2    | 1.0 or 2.0          | 0.5                  | Lower                        | Negative                            |\n\n---\n\n### Question\n\nBased on the provided theory and numerical results in Table 1, select all statements that correctly describe the conditions under which allowing a rookie trader to trade alongside an expert can increase the firm's aggregate expected profit.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 356, "Question": "### Background\n\n**Research Question.** Which firm characteristics explain the cross-sectional variation in commercial bank stock returns following deregulation announcements?\n\n**Setting and Sample.** The study analyzes the stock market reaction of U.S. commercial banks to a series of deregulation events in the 1990s. It investigates why some banks benefited more than others by regressing their event-day abnormal returns on firm characteristics like size and prior experience with investment banking activities.\n\n### Data / Model Specification\n\nThe paper estimates the following cross-sectional model via OLS for each of four events:\n\n  \n\\mathrm{CAR}_{i} = \\beta_{0} + \\beta_{1}\\mathrm{Ln}(\\mathrm{SIZE}_{i}) + \\beta_{2}\\mathrm{SEC}20_{i} + \\text{controls}_i + e_{i} \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Cross-Sectional Regression Results for Commercial Banks: Event 1 (Easing of Firewall Restrictions)**\n\n| Variable    | Coefficient | t-statistic |\n| :---------- | :---------- | :---------- |\n| Intercept   | -6.44       | (-2.82)**   |\n| Ln(SIZE)    | 0.36        | (2.27)*     |\n| SEC 20      | 0.98        | (2.47)*     |\n| Adj. R2     | 0.28        |             |\n\n*Note: Adapted from Table 5, Panel A of the source paper. ** indicates significance at the 0.01 level, * at the 0.05 level. The dependent variable is the two-day CAR in percent.*\n\n---\n\n### Question\n\nBased on the model and results in **Table 1**, select all statements that are correct.", "Options": {"A": "The coefficient on `Ln(SIZE)` implies that smaller banks were expected to benefit more from the easing of firewalls.", "B": "The `SEC20` coefficient is not statistically significant at the 5% level, indicating that prior experience in investment banking was irrelevant.", "C": "If unobserved managerial quality is positively correlated with both bank size and abnormal returns, the estimated coefficient on `Ln(SIZE)` is likely biased upwards, potentially overstating the true effect of size.", "D": "The results suggest that, holding size constant, banks with a Section 20 subsidiary experienced a two-day abnormal return that was, on average, 0.98 percentage points higher than banks without one."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret regression coefficients and understand the direction of omitted variable bias. It uses the 'Atomic Decomposition' strategy, breaking down the original QA's interpretation and bias analysis into distinct, verifiable statements. Option A tests direct interpretation of a coefficient's magnitude and meaning. Option B tests the application of OVB theory. Distractor C is a 'Conceptual Opposite' (sign error). Distractor D is a 'Comprehension Slip' (misreading statistical significance).", "qid": "356", "question": "### Background\n\n**Research Question.** Which firm characteristics explain the cross-sectional variation in commercial bank stock returns following deregulation announcements?\n\n**Setting and Sample.** The study analyzes the stock market reaction of U.S. commercial banks to a series of deregulation events in the 1990s. It investigates why some banks benefited more than others by regressing their event-day abnormal returns on firm characteristics like size and prior experience with investment banking activities.\n\n### Data / Model Specification\n\nThe paper estimates the following cross-sectional model via OLS for each of four events:\n\n  \n\\mathrm{CAR}_{i} = \\beta_{0} + \\beta_{1}\\mathrm{Ln}(\\mathrm{SIZE}_{i}) + \\beta_{2}\\mathrm{SEC}20_{i} + \\text{controls}_i + e_{i} \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Cross-Sectional Regression Results for Commercial Banks: Event 1 (Easing of Firewall Restrictions)**\n\n| Variable    | Coefficient | t-statistic |\n| :---------- | :---------- | :---------- |\n| Intercept   | -6.44       | (-2.82)**   |\n| Ln(SIZE)    | 0.36        | (2.27)*     |\n| SEC 20      | 0.98        | (2.47)*     |\n| Adj. R2     | 0.28        |             |\n\n*Note: Adapted from Table 5, Panel A of the source paper. ** indicates significance at the 0.01 level, * at the 0.05 level. The dependent variable is the two-day CAR in percent.*\n\n---\n\n### Question\n\nBased on the model and results in **Table 1**, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 419, "Question": "### Background\n\n**Research Question.** How do solvency capital requirements that explicitly model both process and systematic risk compare to a simpler standard formula that only captures systematic risk, and how does this comparison depend on portfolio size?\n\n**Setting and Environment.** The analysis considers a portfolio of immediate life annuities issued to a cohort of retirees aged `s=65`. Capital is calculated using two methods: (1) a full stochastic approach that models the entire liability distribution, and (2) the Solvency II QIS5 standard approach, which uses a deterministic shock. The comparison is made for portfolios of varying cohort sizes at different valuation times `t` (years since retirement).\n\n**Variables and Parameters.**\n\n*   `SR`: Solvency Ratio, defined as `SC_t / V_t`.\n*   `l(s+t)`: Cohort size (number of policies).\n*   `t`: Time of valuation (years since retirement age `s=65`).\n*   `SC_t`: Solvency capital at time `t`.\n*   `V_t`: Actuarial reserve (best estimate liability) at time `t`.\n*   `SCR_t`: Solvency Capital Requirement from the QIS5 standard formula.\n*   `k_t^Δ`: Coefficient of variation of the portfolio's random liability under a systematic shock.\n\n---\n\n### Data / Model Specification\n\nThe numerical experiment compares the VaR Solvency Ratio (`VaR SR`) from a stochastic internal model against the limiting ratio from the Solvency II standard approach (`Limiting SR = SCR_t / V_t`). The stochastic model accounts for both process risk (random fluctuations) and systematic risk (a permanent 20% decrease in mortality rates), while the standard approach only accounts for the systematic risk shock.\n\nFor large portfolios, the liability distribution is approximated by a normal distribution. The VaR Solvency Ratio at a confidence level of `1-ε` is calculated as:\n\n  \nSR_t^{VaR} = \\frac{SCR_t + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot V_t^\\Delta}{V_t} = \\left( \\frac{SCR_t}{V_t} \\right) + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot \\left( \\frac{V_t^\\Delta}{V_t} \\right) \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ⁻¹` is the inverse of the standard normal CDF. The confidence level is 99.5%, for which `Φ⁻¹(0.995) = 2.576`.\n\n**Table 1: Simulation Results for Longevity Risk**\n\n| Cohort Size `l(s+t)` | Valuation Time `t` | Coeff. of Variation `k_t^Δ` | `V_t^Δ / V_t` Ratio | Limiting SR (QIS5) | Calculated VaR SR |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 100 | 0 | 1.977% | 1.0551 | 5.51% | 10.88% |\n| 1,000 | 0 | 0.625% | 1.0551 | 5.51% | 7.20% |\n| 100,000 | 0 | 0.063% | 1.0551 | 5.51% | 5.68% |\n| 100 | 10 | 2.760% | 1.0834 | 8.34% | 16.04% |\n| 1,000 | 10 | 0.873% | 1.0834 | 8.34% | 10.78% |\n| 100,000 | 10 | 0.087% | 1.0834 | 8.34% | 8.58% |\n\n---\n\n### Question\n\nUsing the provided data, formulas, and the value `Φ⁻¹(0.995) = 2.576`, select all of the following statements that are mathematically correct.", "Options": {"A": "The VaR Solvency Ratio for the cohort of 1,000 at `t=0` is 7.12%, calculated as `5.51% + 2.576 * 0.00625`.", "B": "The component of the VaR Solvency Ratio attributable to process risk for the cohort of 100 at `t=10` is approximately 7.70%.", "C": "As the portfolio size grows infinitely large, the total VaR Solvency Ratio approaches zero.", "D": "For a hypothetical portfolio of 2,500 annuitants at `t=10` with `k_t^Δ = 0.552%`, the total VaR Solvency Ratio is 9.88%."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Computational Judgment strategy to test calculations from the original QA problem's second and third questions. Options A and B are both correct, independently verifiable calculations based on the provided data and formula. Distractor C is designed based on a Formula Misuse error (omitting the `V_t^Δ / V_t` term). Distractor D is a Conceptual error, incorrectly stating that total risk vanishes rather than converging to the systematic risk component.", "qid": "419", "question": "### Background\n\n**Research Question.** How do solvency capital requirements that explicitly model both process and systematic risk compare to a simpler standard formula that only captures systematic risk, and how does this comparison depend on portfolio size?\n\n**Setting and Environment.** The analysis considers a portfolio of immediate life annuities issued to a cohort of retirees aged `s=65`. Capital is calculated using two methods: (1) a full stochastic approach that models the entire liability distribution, and (2) the Solvency II QIS5 standard approach, which uses a deterministic shock. The comparison is made for portfolios of varying cohort sizes at different valuation times `t` (years since retirement).\n\n**Variables and Parameters.**\n\n*   `SR`: Solvency Ratio, defined as `SC_t / V_t`.\n*   `l(s+t)`: Cohort size (number of policies).\n*   `t`: Time of valuation (years since retirement age `s=65`).\n*   `SC_t`: Solvency capital at time `t`.\n*   `V_t`: Actuarial reserve (best estimate liability) at time `t`.\n*   `SCR_t`: Solvency Capital Requirement from the QIS5 standard formula.\n*   `k_t^Δ`: Coefficient of variation of the portfolio's random liability under a systematic shock.\n\n---\n\n### Data / Model Specification\n\nThe numerical experiment compares the VaR Solvency Ratio (`VaR SR`) from a stochastic internal model against the limiting ratio from the Solvency II standard approach (`Limiting SR = SCR_t / V_t`). The stochastic model accounts for both process risk (random fluctuations) and systematic risk (a permanent 20% decrease in mortality rates), while the standard approach only accounts for the systematic risk shock.\n\nFor large portfolios, the liability distribution is approximated by a normal distribution. The VaR Solvency Ratio at a confidence level of `1-ε` is calculated as:\n\n  \nSR_t^{VaR} = \\frac{SCR_t + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot V_t^\\Delta}{V_t} = \\left( \\frac{SCR_t}{V_t} \\right) + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot \\left( \\frac{V_t^\\Delta}{V_t} \\right) \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ⁻¹` is the inverse of the standard normal CDF. The confidence level is 99.5%, for which `Φ⁻¹(0.995) = 2.576`.\n\n**Table 1: Simulation Results for Longevity Risk**\n\n| Cohort Size `l(s+t)` | Valuation Time `t` | Coeff. of Variation `k_t^Δ` | `V_t^Δ / V_t` Ratio | Limiting SR (QIS5) | Calculated VaR SR |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 100 | 0 | 1.977% | 1.0551 | 5.51% | 10.88% |\n| 1,000 | 0 | 0.625% | 1.0551 | 5.51% | 7.20% |\n| 100,000 | 0 | 0.063% | 1.0551 | 5.51% | 5.68% |\n| 100 | 10 | 2.760% | 1.0834 | 8.34% | 16.04% |\n| 1,000 | 10 | 0.873% | 1.0834 | 8.34% | 10.78% |\n| 100,000 | 10 | 0.087% | 1.0834 | 8.34% | 8.58% |\n\n---\n\n### Question\n\nUsing the provided data, formulas, and the value `Φ⁻¹(0.995) = 2.576`, select all of the following statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 418, "Question": "### Background\n\n**Research Question.** How do solvency capital requirements that explicitly model both process and systematic risk compare to a simpler standard formula that only captures systematic risk, and how does this comparison depend on portfolio size?\n\n**Setting and Environment.** The analysis considers a portfolio of immediate life annuities issued to a cohort of retirees aged `s=65`. Capital is calculated using two methods: (1) a full stochastic approach that models the entire liability distribution, and (2) the Solvency II QIS5 standard approach, which uses a deterministic shock. The comparison is made for portfolios of varying cohort sizes at different valuation times `t` (years since retirement).\n\n**Variables and Parameters.**\n\n*   `SR`: Solvency Ratio, defined as `SC_t / V_t`.\n*   `l(s+t)`: Cohort size (number of policies).\n*   `t`: Time of valuation (years since retirement age `s=65`).\n*   `SC_t`: Solvency capital at time `t`.\n*   `V_t`: Actuarial reserve (best estimate liability) at time `t`.\n*   `SCR_t`: Solvency Capital Requirement from the QIS5 standard formula.\n*   `k_t^Δ`: Coefficient of variation of the portfolio's random liability under a systematic shock.\n\n---\n\n### Data / Model Specification\n\nThe numerical experiment compares the VaR Solvency Ratio (`VaR SR`) from a stochastic internal model against the limiting ratio from the Solvency II standard approach (`Limiting SR = SCR_t / V_t`). The stochastic model accounts for both process risk (random fluctuations) and systematic risk (a permanent 20% decrease in mortality rates), while the standard approach only accounts for the systematic risk shock.\n\nFor large portfolios, the liability distribution is approximated by a normal distribution. The VaR Solvency Ratio at a confidence level of `1-ε` is calculated as:\n\n  \nSR_t^{VaR} = \\frac{SCR_t + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot V_t^\\Delta}{V_t} = \\left( \\frac{SCR_t}{V_t} \\right) + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot \\left( \\frac{V_t^\\Delta}{V_t} \\right) \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ⁻¹` is the inverse of the standard normal CDF. The confidence level is 99.5%, for which `Φ⁻¹(0.995) = 2.576`.\n\n**Table 1: Simulation Results for Longevity Risk**\n\n| Cohort Size `l(s+t)` | Valuation Time `t` | Coeff. of Variation `k_t^Δ` | `V_t^Δ / V_t` Ratio | Limiting SR (QIS5) | Calculated VaR SR |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 100 | 0 | 1.977% | 1.0551 | 5.51% | 10.88% |\n| 1,000 | 0 | 0.625% | 1.0551 | 5.51% | 7.20% |\n| 100,000 | 0 | 0.063% | 1.0551 | 5.51% | 5.68% |\n| 100 | 10 | 2.760% | 1.0834 | 8.34% | 16.04% |\n| 1,000 | 10 | 0.873% | 1.0834 | 8.34% | 10.78% |\n| 100,000 | 10 | 0.087% | 1.0834 | 8.34% | 8.58% |\n\n---\n\n### Question\n\nBased on the data for valuation time `t=0` in **Table 1**, select all of the following statements that are correct interpretations of the relationship between portfolio size, risk, and capital requirements.", "Options": {"A": "An increase in cohort size from 1,000 to 100,000 primarily reduces systematic risk.", "B": "For the smallest portfolio (100 lives), the capital required for process risk (the difference between VaR SR and Limiting SR) is a significant component of the total capital requirement.", "C": "As the cohort size increases, the coefficient of variation `k_t^Δ` decreases, reflecting the diversification of process risk.", "D": "The Solvency II standard formula (Limiting SR) provides a conservative capital estimate for portfolios of all sizes."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test the core interpretations from the original QA problem's first question. Options A and B are independently correct statements derived from the table and the provided context. Distractor C (Conceptual Opposite) targets the misunderstanding that the standard formula is always sufficient. Distractor D (Conceptual Opposite) targets the common confusion between diversifiable process risk and non-diversifiable systematic risk.", "qid": "418", "question": "### Background\n\n**Research Question.** How do solvency capital requirements that explicitly model both process and systematic risk compare to a simpler standard formula that only captures systematic risk, and how does this comparison depend on portfolio size?\n\n**Setting and Environment.** The analysis considers a portfolio of immediate life annuities issued to a cohort of retirees aged `s=65`. Capital is calculated using two methods: (1) a full stochastic approach that models the entire liability distribution, and (2) the Solvency II QIS5 standard approach, which uses a deterministic shock. The comparison is made for portfolios of varying cohort sizes at different valuation times `t` (years since retirement).\n\n**Variables and Parameters.**\n\n*   `SR`: Solvency Ratio, defined as `SC_t / V_t`.\n*   `l(s+t)`: Cohort size (number of policies).\n*   `t`: Time of valuation (years since retirement age `s=65`).\n*   `SC_t`: Solvency capital at time `t`.\n*   `V_t`: Actuarial reserve (best estimate liability) at time `t`.\n*   `SCR_t`: Solvency Capital Requirement from the QIS5 standard formula.\n*   `k_t^Δ`: Coefficient of variation of the portfolio's random liability under a systematic shock.\n\n---\n\n### Data / Model Specification\n\nThe numerical experiment compares the VaR Solvency Ratio (`VaR SR`) from a stochastic internal model against the limiting ratio from the Solvency II standard approach (`Limiting SR = SCR_t / V_t`). The stochastic model accounts for both process risk (random fluctuations) and systematic risk (a permanent 20% decrease in mortality rates), while the standard approach only accounts for the systematic risk shock.\n\nFor large portfolios, the liability distribution is approximated by a normal distribution. The VaR Solvency Ratio at a confidence level of `1-ε` is calculated as:\n\n  \nSR_t^{VaR} = \\frac{SCR_t + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot V_t^\\Delta}{V_t} = \\left( \\frac{SCR_t}{V_t} \\right) + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot \\left( \\frac{V_t^\\Delta}{V_t} \\right) \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ⁻¹` is the inverse of the standard normal CDF. The confidence level is 99.5%, for which `Φ⁻¹(0.995) = 2.576`.\n\n**Table 1: Simulation Results for Longevity Risk**\n\n| Cohort Size `l(s+t)` | Valuation Time `t` | Coeff. of Variation `k_t^Δ` | `V_t^Δ / V_t` Ratio | Limiting SR (QIS5) | Calculated VaR SR |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 100 | 0 | 1.977% | 1.0551 | 5.51% | 10.88% |\n| 1,000 | 0 | 0.625% | 1.0551 | 5.51% | 7.20% |\n| 100,000 | 0 | 0.063% | 1.0551 | 5.51% | 5.68% |\n| 100 | 10 | 2.760% | 1.0834 | 8.34% | 16.04% |\n| 1,000 | 10 | 0.873% | 1.0834 | 8.34% | 10.78% |\n| 100,000 | 10 | 0.087% | 1.0834 | 8.34% | 8.58% |\n\n---\n\n### Question\n\nBased on the data for valuation time `t=0` in **Table 1**, select all of the following statements that are correct interpretations of the relationship between portfolio size, risk, and capital requirements.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 423, "Question": "### Background\n\nThe empirical finding that an external factor (stock market volatility) predicts bond market volatility, even after controlling for the yield curve itself, challenges standard Affine Term Structure Models (ATSMs). This suggests the need for a modified theoretical framework.\n\n### Question\n\nThe empirical findings in the paper challenge standard ATSMs. Which of the following describe valid theoretical modifications or properties of an extended model that could reconcile theory with this evidence? Select all that apply.", "Options": {"A": "The puzzle can be resolved within a standard ATSM by simply adding more 'spanned' factors, such as a fourth or fifth principal component of the yield curve.", "B": "The evidence can be accommodated by assuming that the innovations to the yield curve factors are driven by a GARCH process that is independent of the equity market.", "C": "Introduce an 'unspanned' stochastic volatility factor that governs the variance of the yield curve's state variables but does not directly enter the bond pricing equation itself.", "D": "In the extended model, the innovations to the new unspanned volatility factor must be allowed to be correlated with innovations to equity market volatility."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the 'Apex' knowledge from the original QA, focusing on how to theoretically resolve the empirical puzzle. It uses a Reverse-Reasoning strategy, asking for model properties that would generate the observed results. Correct options A and B together describe the standard solution: an unspanned stochastic volatility factor correlated with the external variable. Distractor C misses the core issue, which is about an *external* (unspanned) factor, not a lack of internal (spanned) factors. Distractor D describes a model that would generate volatility clustering but would fail to explain why an *external* variable like stock volatility has predictive power.", "qid": "423", "question": "### Background\n\nThe empirical finding that an external factor (stock market volatility) predicts bond market volatility, even after controlling for the yield curve itself, challenges standard Affine Term Structure Models (ATSMs). This suggests the need for a modified theoretical framework.\n\n### Question\n\nThe empirical findings in the paper challenge standard ATSMs. Which of the following describe valid theoretical modifications or properties of an extended model that could reconcile theory with this evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 398, "Question": "### Background\n\n**Research Question.** The efficiency of the S&P 500 futures market can be tested under two competing hypotheses: the simple unbiasedness hypothesis, which posits a direct cointegrating relationship between spot and futures prices, and the cost-of-carry model, which argues for a trivariate cointegrating relationship that includes a measure of financing costs. This investigation seeks to determine which model is empirically supported by analyzing the stability of the estimated relationships and the validity of theoretical parameter restrictions.\n\n**Setting / Data-Generating Environment.** A Vector Error Correction Model (VECM) framework is applied to daily S&P 500 spot prices (`s_t`), futures prices (`f_t`), and the 3-month Treasury bill rate (`r_t`) from April 1982 to June 2003. The analysis proceeds in three stages: (1) estimating a bivariate model for `(s_t, f_t)`, (2) estimating a trivariate model for `(s_t, f_t, r_t)`, and (3) testing for weak exogeneity within the preferred model to determine the locus of price discovery.\n\n### Data / Model Specification\n\nThe core of the analysis rests on the VECM representation of a system of `p` variables `X_t`:\n  \n\\Delta X_{t}=\\alpha \\beta' X_{t-1} + \\sum_{j=1}^{k-1} \\Gamma_j \\Delta X_{t-j} + \\mu + \\delta t + \\varepsilon_t \\quad \\text{(Eq. (1))}\n \nwhere `\\beta` contains the `r` long-run cointegrating vectors and `\\alpha` contains the adjustment coefficients. A variable `i` is weakly exogenous if its corresponding adjustment coefficient, `\\alpha_i`, is zero.\n\n**Table 1: Summary of Empirical Findings for Bivariate and Trivariate Models**\n\n| Feature | Bivariate Model (`s_t`, `f_t`) | Trivariate Model (`s_t`, `f_t`, `r_t`) |\n| :--- | :--- | :--- |\n| **Cointegrating Rank** | `r=1` is found (Trace stat = 44.59 > 25.47 crit.) | `r=1` is found (Trace stat = 25.08 ≈ 25.47 crit.) |\n| **Parameter Stability** | **Rejected**. The relationship is unstable over the sample. | **Not Rejected**. The relationship is stable over the sample. |\n| **Symmetry Test (`H_0: \\beta` contains `[1, -1]`)** | **Rejected**. (LR stat = 5.44, `\\chi^2(1)`). | **Not Rejected**. (LR stat = 0.27, `\\chi^2(1)`). |\n| **Weak Exogeneity of Spot Price (`H_0: \\alpha_s=0`)** | Not tested (model misspecified). | **Not Rejected**. (LR stat = 1.36, `\\chi^2(1)`). |\n| **Weak Exogeneity of Futures Price (`H_0: \\alpha_f=0`)** | Not tested (model misspecified). | **Rejected**. (LR stat = 31.47, `\\chi^2(1)`). |\n\n### Question\n\nBased on the findings for the Bivariate Model in **Table 1**, select all statements that correctly diagnose why this model is likely misspecified.", "Options": {"A": "The trace statistic of 44.59 is excessively high, which points to data contamination rather than a genuine economic relationship.", "B": "The successful identification of one cointegrating vector (r=1) is sufficient evidence to confirm the validity of the simple unbiasedness hypothesis, despite other contradictory findings.", "C": "The rejection of parameter stability indicates that the estimated long-run relationship is not consistent over time, a primary symptom of an omitted variable.", "D": "The rejection of the `[1, -1]` symmetry restriction contradicts a core theoretical expectation of asset pricing, suggesting the model is capturing a spurious relationship."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to diagnose model misspecification by synthesizing multiple, seemingly contradictory statistical results. It uses the **Atomic Decomposition** strategy, breaking the complex conclusion of the original QA problem into independently verifiable statements about the evidence. Options A and B are the two key pillars of the argument against the bivariate model. Distractor C represents a common error: focusing on a single confirmatory result while ignoring contradictory evidence ('confirmation bias'). Distractor D is a plausible but incorrect interpretation of a large test statistic, misattributing it to data error instead of model error.", "qid": "398", "question": "### Background\n\n**Research Question.** The efficiency of the S&P 500 futures market can be tested under two competing hypotheses: the simple unbiasedness hypothesis, which posits a direct cointegrating relationship between spot and futures prices, and the cost-of-carry model, which argues for a trivariate cointegrating relationship that includes a measure of financing costs. This investigation seeks to determine which model is empirically supported by analyzing the stability of the estimated relationships and the validity of theoretical parameter restrictions.\n\n**Setting / Data-Generating Environment.** A Vector Error Correction Model (VECM) framework is applied to daily S&P 500 spot prices (`s_t`), futures prices (`f_t`), and the 3-month Treasury bill rate (`r_t`) from April 1982 to June 2003. The analysis proceeds in three stages: (1) estimating a bivariate model for `(s_t, f_t)`, (2) estimating a trivariate model for `(s_t, f_t, r_t)`, and (3) testing for weak exogeneity within the preferred model to determine the locus of price discovery.\n\n### Data / Model Specification\n\nThe core of the analysis rests on the VECM representation of a system of `p` variables `X_t`:\n  \n\\Delta X_{t}=\\alpha \\beta' X_{t-1} + \\sum_{j=1}^{k-1} \\Gamma_j \\Delta X_{t-j} + \\mu + \\delta t + \\varepsilon_t \\quad \\text{(Eq. (1))}\n \nwhere `\\beta` contains the `r` long-run cointegrating vectors and `\\alpha` contains the adjustment coefficients. A variable `i` is weakly exogenous if its corresponding adjustment coefficient, `\\alpha_i`, is zero.\n\n**Table 1: Summary of Empirical Findings for Bivariate and Trivariate Models**\n\n| Feature | Bivariate Model (`s_t`, `f_t`) | Trivariate Model (`s_t`, `f_t`, `r_t`) |\n| :--- | :--- | :--- |\n| **Cointegrating Rank** | `r=1` is found (Trace stat = 44.59 > 25.47 crit.) | `r=1` is found (Trace stat = 25.08 ≈ 25.47 crit.) |\n| **Parameter Stability** | **Rejected**. The relationship is unstable over the sample. | **Not Rejected**. The relationship is stable over the sample. |\n| **Symmetry Test (`H_0: \\beta` contains `[1, -1]`)** | **Rejected**. (LR stat = 5.44, `\\chi^2(1)`). | **Not Rejected**. (LR stat = 0.27, `\\chi^2(1)`). |\n| **Weak Exogeneity of Spot Price (`H_0: \\alpha_s=0`)** | Not tested (model misspecified). | **Not Rejected**. (LR stat = 1.36, `\\chi^2(1)`). |\n| **Weak Exogeneity of Futures Price (`H_0: \\alpha_f=0`)** | Not tested (model misspecified). | **Rejected**. (LR stat = 31.47, `\\chi^2(1)`). |\n\n### Question\n\nBased on the findings for the Bivariate Model in **Table 1**, select all statements that correctly diagnose why this model is likely misspecified.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 315, "Question": "### Background\n\n**Research Question.** How can a Central Clearing Party (CCP) identify the specific sources of \"crowded risk\" in real time, and what do these sources reveal about the nature of systemic risk?\n\n**Setting.** An empirical analysis of a European equity CCP focuses on member behavior during high-stress periods. The study uses a theoretical tool, the elasticity of CCP exposure (`e_{σ^f}^{ExpCCP}`), to measure how sensitive the CCP's total risk is to changes in the volatility of specific risk factors.\n\n**Variables and Parameters.**\n- `ExpCCP`: The CCP's total exposure to aggregate member loss.\n- `e_{σ^f}^{ExpCCP}`: The elasticity of `ExpCCP` with respect to the volatility of a candidate risk factor `f`.\n- **Candidate risk factors:** Market (STOXXNordic30), Nokia stock.\n- **Key Dates:** \"Nokia day\" (Apr 26, 2010), driven by a firm-specific earnings shock; \"Bailout day\" (May 10, 2010), driven by a macroeconomic announcement.\n\n---\n\n### Data / Model Specification\n\n**Table 1: CCP Exposure Elasticity to Various Risk Factors**\n\nThis table reports the elasticity of total CCP exposure (`ExpCCP`) with respect to a 1% change in the daily volatility of three candidate risk factors on three different days.\n\n| | Date | Risk factor | Elasticity |\n| :--- | :--- | :--- | :--- |\n| Median-CrowdIx day | Jul 29, 2010 | Market | 0.91 |\n| | | Nokia | 0.15 |\n| Bailout program | May 10, 2010 | Market | 0.98 |\n| | | Nokia | 0.14 |\n| Nokia reports Q1 | Apr 26, 2010 | Market | 0.19 |\n| | | Nokia | 1.05 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, select all statements that are valid conclusions supported by the paper's analysis.", "Options": {"A": "On a typical day (Median-CrowdIx day), the market is the dominant risk factor, which proves that idiosyncratic risks are generally not systemic.", "B": "On the \"Bailout day\" (May 10), the Nokia stock was a more significant source of systemic risk than the overall market.", "C": "The findings demonstrate that a purely firm-specific (idiosyncratic) event can become a source of systemic risk for the CCP if members' positions are crowded on that single factor.", "D": "On the \"Nokia day\" (Apr 26), the Nokia stock was the primary crowded risk factor, as indicated by its elasticity of 1.05."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret elasticity data to identify crowded risk factors and draw correct high-level conclusions. It uses the 'Atomic Decomposition' strategy, breaking down the original QA question's first part into independently verifiable statements. Options A and B are correct statements directly derivable from the table and the paper's core argument. Distractor C is a 'Conceptual Opposite' error, reversing the finding for the Bailout day. Distractor D is a 'Misleading Generalization,' using a correct premise (market dominance on a typical day) to draw a conclusion that is explicitly contradicted by the Nokia day evidence.", "qid": "315", "question": "### Background\n\n**Research Question.** How can a Central Clearing Party (CCP) identify the specific sources of \"crowded risk\" in real time, and what do these sources reveal about the nature of systemic risk?\n\n**Setting.** An empirical analysis of a European equity CCP focuses on member behavior during high-stress periods. The study uses a theoretical tool, the elasticity of CCP exposure (`e_{σ^f}^{ExpCCP}`), to measure how sensitive the CCP's total risk is to changes in the volatility of specific risk factors.\n\n**Variables and Parameters.**\n- `ExpCCP`: The CCP's total exposure to aggregate member loss.\n- `e_{σ^f}^{ExpCCP}`: The elasticity of `ExpCCP` with respect to the volatility of a candidate risk factor `f`.\n- **Candidate risk factors:** Market (STOXXNordic30), Nokia stock.\n- **Key Dates:** \"Nokia day\" (Apr 26, 2010), driven by a firm-specific earnings shock; \"Bailout day\" (May 10, 2010), driven by a macroeconomic announcement.\n\n---\n\n### Data / Model Specification\n\n**Table 1: CCP Exposure Elasticity to Various Risk Factors**\n\nThis table reports the elasticity of total CCP exposure (`ExpCCP`) with respect to a 1% change in the daily volatility of three candidate risk factors on three different days.\n\n| | Date | Risk factor | Elasticity |\n| :--- | :--- | :--- | :--- |\n| Median-CrowdIx day | Jul 29, 2010 | Market | 0.91 |\n| | | Nokia | 0.15 |\n| Bailout program | May 10, 2010 | Market | 0.98 |\n| | | Nokia | 0.14 |\n| Nokia reports Q1 | Apr 26, 2010 | Market | 0.19 |\n| | | Nokia | 1.05 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, select all statements that are valid conclusions supported by the paper's analysis.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 123, "Question": "### Background\n\n**Research Question.** This case examines the economic channels that explain the cross-sectional variation in stock price reactions to initial public debt offerings (debt IPOs).\n\n**Setting.** The study moves beyond the average negative announcement return to a multivariate OLS regression framework. The dependent variable is the two-day stock excess return (`SER`). The analysis tests several core hypotheses from corporate finance theory.\n\n**Variables & Parameters.**\n*   `SER`: Two-day stock excess return (dependent variable).\n*   `Age`: ln(1 + years since stock IPO), proxy for reputation/low information asymmetry.\n*   `Maturity`: Years to maturity of the bond, proxy for negative signal about growth options.\n*   `Creditchg`: Dummy = 1 if bank loan commitment increases, proxy for continued bank monitoring.\n*   `Mkt/bk`: Market-to-book ratio, proxy for growth opportunities.\n\n---\n\n### Data / Model Specification\n\nThe event study found an average `SER` of -0.86% over two days. To understand what drives this effect, the following cross-sectional regression is estimated:\n\n  \nSER_i = \\beta_0 + \\beta_1 \\text{Age}_i + \\beta_2 \\text{Maturity}_i + \\beta_3 \\text{Creditchg}_i + \\beta_4 \\text{Mkt/bk}_i + \\text{Controls}_i + \\epsilon_i \n \n\nKey hypotheses and their predictions for the coefficients:\n*   **Hypothesis 1B (Monitoring):** Reducing bank debt is bad news. Predicts `β₃ > 0`.\n*   **Hypothesis 2 (Maturity Signal):** Longer maturity is bad news. Predicts `β₂ < 0`.\n*   **Hypothesis 3 (Reputation):** Older firms are penalized less. Predicts `β₁ > 0`.\n*   **Hypothesis 4 (Growth/Flexibility):** High-growth firms benefit more. Predicts `β₄ > 0`.\n\nEstimation results for a representative model are presented in Table 1.\n\n**Table 1: OLS Regression Explaining Announcement Period SER**\n\n| Independent Variable | Coefficient | p-value |\n| :--- | :--- | :--- |\n| Intercept | -0.004 | (0.74) |\n| Age | 0.004 | (0.04) |\n| Maturity | -0.001 | (0.07) |\n| Creditchg | 0.006 | (0.10) |\n| Mkt/bk | 0.004 | (0.05) |\n\n---\n\nBased on the regression results in Table 1, which of the following statements are valid interpretations or conclusions? Select all that apply.", "Options": {"A": "The results are consistent with the theory that longer debt maturity sends a negative signal to the market, leading to a more negative stock price reaction.", "B": "The empirical evidence supports the hypothesis that firms with stronger growth prospects (higher `Mkt/bk`) experience a less adverse stock price reaction to a debt IPO.", "C": "The model predicts that a firm with `Age`=ln(11), `Maturity`=10, `Creditchg`=1, and `Mkt/bk`=2.0 would have a predicted SER (ignoring the intercept) of +1.36%.", "D": "The evidence refutes the hypothesis that maintaining a relationship with a monitoring bank (as proxied by `Creditchg`) mitigates the negative announcement effect."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to interpret multiple regression coefficients in the context of specific economic hypotheses and to perform a multi-step calculation to predict an outcome. Depth Strategy: Computational Judgment. Distractor Logic: Option A is a correct interpretation of the positive and significant coefficient on `Mkt/bk`. Option B is a correct interpretation of the negative and significant coefficient on `Maturity`. Option C requires a correct calculation: (0.004 * ln(11)) + (-0.001 * 10) + (0.006 * 1) + (0.004 * 2.0) = (0.004 * 2.4) - 0.01 + 0.006 + 0.008 = 0.0096 - 0.01 + 0.014 = 0.0136 or +1.36%, so it is correct. Option D is a Conceptual Opposite; the positive coefficient on `Creditchg` supports, not refutes, the hypothesis.", "qid": "123", "question": "### Background\n\n**Research Question.** This case examines the economic channels that explain the cross-sectional variation in stock price reactions to initial public debt offerings (debt IPOs).\n\n**Setting.** The study moves beyond the average negative announcement return to a multivariate OLS regression framework. The dependent variable is the two-day stock excess return (`SER`). The analysis tests several core hypotheses from corporate finance theory.\n\n**Variables & Parameters.**\n*   `SER`: Two-day stock excess return (dependent variable).\n*   `Age`: ln(1 + years since stock IPO), proxy for reputation/low information asymmetry.\n*   `Maturity`: Years to maturity of the bond, proxy for negative signal about growth options.\n*   `Creditchg`: Dummy = 1 if bank loan commitment increases, proxy for continued bank monitoring.\n*   `Mkt/bk`: Market-to-book ratio, proxy for growth opportunities.\n\n---\n\n### Data / Model Specification\n\nThe event study found an average `SER` of -0.86% over two days. To understand what drives this effect, the following cross-sectional regression is estimated:\n\n  \nSER_i = \\beta_0 + \\beta_1 \\text{Age}_i + \\beta_2 \\text{Maturity}_i + \\beta_3 \\text{Creditchg}_i + \\beta_4 \\text{Mkt/bk}_i + \\text{Controls}_i + \\epsilon_i \n \n\nKey hypotheses and their predictions for the coefficients:\n*   **Hypothesis 1B (Monitoring):** Reducing bank debt is bad news. Predicts `β₃ > 0`.\n*   **Hypothesis 2 (Maturity Signal):** Longer maturity is bad news. Predicts `β₂ < 0`.\n*   **Hypothesis 3 (Reputation):** Older firms are penalized less. Predicts `β₁ > 0`.\n*   **Hypothesis 4 (Growth/Flexibility):** High-growth firms benefit more. Predicts `β₄ > 0`.\n\nEstimation results for a representative model are presented in Table 1.\n\n**Table 1: OLS Regression Explaining Announcement Period SER**\n\n| Independent Variable | Coefficient | p-value |\n| :--- | :--- | :--- |\n| Intercept | -0.004 | (0.74) |\n| Age | 0.004 | (0.04) |\n| Maturity | -0.001 | (0.07) |\n| Creditchg | 0.006 | (0.10) |\n| Mkt/bk | 0.004 | (0.05) |\n\n---\n\nBased on the regression results in Table 1, which of the following statements are valid interpretations or conclusions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 366, "Question": "### Background\n\n**Research Question:** The paper's central theory predicts that in a competitive innovation race, firms with a higher relative innovation output (i.e., 'leaders') will have lower systematic risk (beta) than their less successful rivals ('laggards'). This question assesses the empirical evidence for this prediction using both portfolio-level and firm-level tests.\n\n**Empirical Strategy:** The authors use patent data to construct a measure of a firm's relative innovation output—its share of patents within a specific technology field. They first test the theory using simple portfolio sorts and then move to more formal firm-level panel regressions, controlling for a range of known determinants of beta.\n\n### Data / Model Specification\n\nThe primary firm-level regression specification is:\n\n  \n\\beta_{i t}=\\lambda_{0}+\\lambda_{1}\\bar{y}_{i t}+\\lambda_{2}X_{i t}+FE+\\epsilon_{i t}\n\\quad \\text{(Eq. (1))}\n \n\nwhere `β_it` is the market beta of firm `i` at time `t`, `y_it` is the firm's normalized share of innovation output (z-score), `X_it` is a vector of firm-level controls (e.g., size, leverage, book-to-market), and `FE` represents fixed effects.\n\n**Table 1** below presents a summary of key empirical results from the paper. Panel A shows results from sorting innovating firms into portfolios based on their patent share. Panel B shows results from the firm-level regression in Eq. (1).\n\n**Table 1: Empirical Results on Innovation Output and Systematic Risk**\n\n| Panel / Specification | Dependent Variable | Key Metric | Leaders | Middle | M - L Diff. |\n| :--- | :--- | :--- | :---: | :---: | :---: |\n| **A: Portfolio Sorts** | Portfolio Returns | Market Beta (`β_MKT`) | 0.937*** | 1.196*** | 0.259*** |\n| | | Std. Error | (0.0126) | (0.0373) | (0.0366) |\n| | | | | | |\n| **B: Firm-Level Regressions** | Firm Beta (`β_it`) | Coefficient on `y_it` (`λ_1`) | | | |\n| Baseline (Patent Counts) | `β_it` | `λ_1` | -0.0713*** | | |\n| | | Std. Error | (0.0110) | | |\n| Robustness (Citation-Weighted) | `β_it` | `λ_1` | -0.0469*** | | |\n| | | Std. Error | (0.00749) | | |\n\n*Notes: Results are based on Tables 2 and 3 in the original paper. *** denotes significance at the 1% level. The regression results in Panel B control for firm characteristics and industry × time fixed effects.*\n\n### Question\n\nBased on the provided data and the paper's analysis, which of the following statements are supported by the evidence?", "Options": {"A": "Using a citation-weighted measure of innovation output reveals a much larger economic effect on beta compared to using simple patent counts, indicating that patent quality is the primary driver of the risk reduction.", "B": "The difference in market beta between the 'Middle' and 'Leader' innovation portfolios is approximately 0.26, and this difference is statistically significant at the 1% level.", "C": "The portfolio of 'Leader' firms exhibits a higher cost of capital than the 'Middle' portfolio, reflecting the higher risk associated with maintaining a dominant innovation position.", "D": "The firm-level regression results suggest that a one-standard-deviation increase in a firm's normalized share of patents is associated with a decrease in its market beta of approximately 0.07."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item replaces a Table QA problem, converting its core quantitative interpretations into a multiple-choice format. It uses an **atomic decomposition** strategy, breaking down the original multi-part question into distinct, verifiable statements. The assessment target is the user's ability to accurately interpret both portfolio-level and regression-level evidence from the provided table (**Computational Judgment**). Distractor C is a **Conceptual Opposite**, directly contradicting the paper's main finding about the relationship between leadership and cost of capital. Distractor D is a **Numerical Misinterpretation**, forcing a careful comparison of the coefficients in Panel B and correctly identifying that the citation-weighted effect is smaller in magnitude (-0.0469) not larger.", "qid": "366", "question": "### Background\n\n**Research Question:** The paper's central theory predicts that in a competitive innovation race, firms with a higher relative innovation output (i.e., 'leaders') will have lower systematic risk (beta) than their less successful rivals ('laggards'). This question assesses the empirical evidence for this prediction using both portfolio-level and firm-level tests.\n\n**Empirical Strategy:** The authors use patent data to construct a measure of a firm's relative innovation output—its share of patents within a specific technology field. They first test the theory using simple portfolio sorts and then move to more formal firm-level panel regressions, controlling for a range of known determinants of beta.\n\n### Data / Model Specification\n\nThe primary firm-level regression specification is:\n\n  \n\\beta_{i t}=\\lambda_{0}+\\lambda_{1}\\bar{y}_{i t}+\\lambda_{2}X_{i t}+FE+\\epsilon_{i t}\n\\quad \\text{(Eq. (1))}\n \n\nwhere `β_it` is the market beta of firm `i` at time `t`, `y_it` is the firm's normalized share of innovation output (z-score), `X_it` is a vector of firm-level controls (e.g., size, leverage, book-to-market), and `FE` represents fixed effects.\n\n**Table 1** below presents a summary of key empirical results from the paper. Panel A shows results from sorting innovating firms into portfolios based on their patent share. Panel B shows results from the firm-level regression in Eq. (1).\n\n**Table 1: Empirical Results on Innovation Output and Systematic Risk**\n\n| Panel / Specification | Dependent Variable | Key Metric | Leaders | Middle | M - L Diff. |\n| :--- | :--- | :--- | :---: | :---: | :---: |\n| **A: Portfolio Sorts** | Portfolio Returns | Market Beta (`β_MKT`) | 0.937*** | 1.196*** | 0.259*** |\n| | | Std. Error | (0.0126) | (0.0373) | (0.0366) |\n| | | | | | |\n| **B: Firm-Level Regressions** | Firm Beta (`β_it`) | Coefficient on `y_it` (`λ_1`) | | | |\n| Baseline (Patent Counts) | `β_it` | `λ_1` | -0.0713*** | | |\n| | | Std. Error | (0.0110) | | |\n| Robustness (Citation-Weighted) | `β_it` | `λ_1` | -0.0469*** | | |\n| | | Std. Error | (0.00749) | | |\n\n*Notes: Results are based on Tables 2 and 3 in the original paper. *** denotes significance at the 1% level. The regression results in Panel B control for firm characteristics and industry × time fixed effects.*\n\n### Question\n\nBased on the provided data and the paper's analysis, which of the following statements are supported by the evidence?", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 429, "Question": "### Background\n\nThe paper proposes a heuristic test to assess the substitutability of different credit sources for financing specific expenditures, with implications for the effectiveness of selective credit controls. The test compares the t-statistic of an aggregate financial variable (`t_agg`) to that of the best single component (`t_comp`).\n\n### Data / Model Specification\n\nThe decision rule is as follows:\n- If `t_agg / t_comp > 1`, it suggests high substitutability, making selective controls less effective.\n- If `t_agg / t_comp < 1`, it suggests low substitutability (a \"one-to-one relation\"), making selective controls more effective.\n\n**Table 1: Tests of Substitutability for Household Expenditures (based on Table 8, Annual Data)**\n\n| Expenditure Category | Best Component Instrument | Test Ratio (`t_agg / t_comp`) | Author's Interpretation |\n| :--- | :--- | :---: | :--- |\n| Residential Construction | 1-to-4 family mortgages | 0.7304 | Low Substitutability |\n| Consumer Durables | Consumer credit | 1.007 | High Substitutability |\n\n---\n\nBased on the data and logic presented, which of the following conclusions or policy implications are valid?\n\nSelect all that apply.", "Options": {"A": "The results suggest that a selective credit control policy targeting mortgage origination would be a relatively direct and effective tool for influencing residential construction expenditures.", "B": "The test ratio for Residential Construction being less than 1 implies that adding non-mortgage credit flows to the regression model adds more statistical noise than explanatory power.", "C": "The test ratio for Consumer Durables being greater than 1 proves that consumer credit and mortgage credit are perfect substitutes in a formal econometric sense.", "D": "A policy restricting only consumer credit would likely be ineffective at curbing durable goods spending, as households could substitute towards other financing like mortgage credit."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret the paper's heuristic test for credit substitutability and draw correct policy conclusions from it.\nDepth Strategy: Scenario Application. The user must apply the general findings from the table to specific, hypothetical policy scenarios.\nDistractor Logic:\n- A (Correct): This is the correct policy implication for the Consumer Durables result (ratio > 1), indicating high substitutability means controls can be circumvented.\n- B (Correct): This is the correct policy implication for the Residential Construction result (ratio < 1), indicating a dedicated financing channel that is vulnerable to targeted controls.\n- C (Correct): This correctly interprets the statistical logic behind the heuristic. A ratio < 1 means the aggregate variable's t-statistic is lower, which happens when the added variables (other credit flows) are noisy and irrelevant for explaining the specific expenditure.\n- D (Conceptual Distractor - Overstatement): This distractor uses the term 'proves' and 'perfect substitutes in a formal econometric sense'. The paper's test is explicitly a heuristic, not a formal F-test for coefficient equality. This option is incorrect because it overstates the power and formality of the method used.", "qid": "429", "question": "### Background\n\nThe paper proposes a heuristic test to assess the substitutability of different credit sources for financing specific expenditures, with implications for the effectiveness of selective credit controls. The test compares the t-statistic of an aggregate financial variable (`t_agg`) to that of the best single component (`t_comp`).\n\n### Data / Model Specification\n\nThe decision rule is as follows:\n- If `t_agg / t_comp > 1`, it suggests high substitutability, making selective controls less effective.\n- If `t_agg / t_comp < 1`, it suggests low substitutability (a \"one-to-one relation\"), making selective controls more effective.\n\n**Table 1: Tests of Substitutability for Household Expenditures (based on Table 8, Annual Data)**\n\n| Expenditure Category | Best Component Instrument | Test Ratio (`t_agg / t_comp`) | Author's Interpretation |\n| :--- | :--- | :---: | :--- |\n| Residential Construction | 1-to-4 family mortgages | 0.7304 | Low Substitutability |\n| Consumer Durables | Consumer credit | 1.007 | High Substitutability |\n\n---\n\nBased on the data and logic presented, which of the following conclusions or policy implications are valid?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 306, "Question": "### Background\n\n**Research Question.** To further distinguish between the 'monitoring' and 'selection' hypotheses, the analysis turns to the dynamics of diversification decisions. Do institutions proactively influence firms' decisions to diversify ('ex-ante' monitoring), or do they reactively trade shares after a decision has been made ('ex-post' selection)?\n\n**Setting.** The analysis uses two predictive regressions. The first models the firm's decision to change its diversification level. The second models the subsequent change in institutional ownership.\n\n**Variables & Parameters.**\n*   `Change in Geo Div`: Change in geographic diversification.\n*   `Change in Instl Ownership`: Change in institutional ownership.\n*   `Ln(Relative q Ratio)_{t-1}`: A lagged measure of the firm's market valuation relative to its peers. Higher values indicate higher perceived quality.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Predicting Changes in Diversification (MSA-based)**\n| Dependent var: Change in Geo Div | (1) |\n| :--- | :---: |\n| Ln(Relative q Ratio)<sub>t-1</sub> | **0.065** |\n| | (2.36)** |\n| Ln(Relative q Ratio)<sub>t-1</sub> * | **-0.049** |\n| Change in Instl Ownership<sub>t-1</sub> | (1.94)* |\n\n**Table 2. Predicting Changes in Institutional Ownership (Region-based)**\n| Dependent var: Change in Instl Ownership | (2) |\n| :--- | :---: |\n| Change in Geo Div<sub>t-1</sub> | **-0.008** |\n| | (0.29) |\n\n*Note: Tables show selected coefficients from regressions with controls. t-statistics are in parentheses.* \n\n---\n\n### Question\n\nThe provided tables analyze the dynamics of diversification decisions and investor responses. Which of the following statements accurately describe the chronological evidence for proactive monitoring? Select all that apply.", "Options": {"A": "There is no statistically significant relationship between a firm's past changes in diversification and subsequent changes in institutional ownership, which is inconsistent with a reactive 'voting with their feet' selection model.", "B": "Firms with higher relative valuations (`Ln(Relative q Ratio)`) are significantly less likely to diversify, suggesting they focus on their core strengths.", "C": "The interaction effect in Table 1 shows that institutional investors encourage high-quality firms to diversify more, likely to leverage their superior management skills.", "D": "For firms with high relative valuations, an increase in institutional ownership is associated with a lower likelihood of subsequent geographic diversification, suggesting an ex-ante moderating influence."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to construct a chronological narrative of cause and effect from two dynamic regressions. It uses an Atomic Decomposition strategy to separate the 'ex-ante' influence on firm decisions from the 'ex-post' investor reaction. Distractor (A) is a 'Conceptual Opposite' of the main effect in Table 1, while distractor (C) represents a 'Sign Error' in interpreting the crucial interaction term.", "qid": "306", "question": "### Background\n\n**Research Question.** To further distinguish between the 'monitoring' and 'selection' hypotheses, the analysis turns to the dynamics of diversification decisions. Do institutions proactively influence firms' decisions to diversify ('ex-ante' monitoring), or do they reactively trade shares after a decision has been made ('ex-post' selection)?\n\n**Setting.** The analysis uses two predictive regressions. The first models the firm's decision to change its diversification level. The second models the subsequent change in institutional ownership.\n\n**Variables & Parameters.**\n*   `Change in Geo Div`: Change in geographic diversification.\n*   `Change in Instl Ownership`: Change in institutional ownership.\n*   `Ln(Relative q Ratio)_{t-1}`: A lagged measure of the firm's market valuation relative to its peers. Higher values indicate higher perceived quality.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Predicting Changes in Diversification (MSA-based)**\n| Dependent var: Change in Geo Div | (1) |\n| :--- | :---: |\n| Ln(Relative q Ratio)<sub>t-1</sub> | **0.065** |\n| | (2.36)** |\n| Ln(Relative q Ratio)<sub>t-1</sub> * | **-0.049** |\n| Change in Instl Ownership<sub>t-1</sub> | (1.94)* |\n\n**Table 2. Predicting Changes in Institutional Ownership (Region-based)**\n| Dependent var: Change in Instl Ownership | (2) |\n| :--- | :---: |\n| Change in Geo Div<sub>t-1</sub> | **-0.008** |\n| | (0.29) |\n\n*Note: Tables show selected coefficients from regressions with controls. t-statistics are in parentheses.* \n\n---\n\n### Question\n\nThe provided tables analyze the dynamics of diversification decisions and investor responses. Which of the following statements accurately describe the chronological evidence for proactive monitoring? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n\nD", "model_answer": ["D"], "error": null}}
{"ID": 151, "Question": "### Background\n\nThe study uses OLS and Instrumental Variables (IV) regressions to estimate the causal effect of Consumer-Directed Health Plan (CDHP) features on total health care spending. The IV strategy is intended to correct for endogeneity bias stemming from employees' decisions about how much to contribute to their spending accounts.\n\n### Data / Model Specification\n\nThe core regression model is:\n\n  \nTotal\\_Spending_{igt} = \\beta_0 + \\beta_{Account} \\text{Spending Account}_{igt} + \\beta_{Deductible} \\text{Deductible}_{gt} + \\dots + \\eta_g + \\varepsilon_{igt} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Estimated Effects on Total Health Care Spending**\n\n| Regressor        | OLS Estimate   | IV Estimate    |\n| :--------------- | :------------: | :------------: |\n| Spending Account | 1.23** (0.17)  | 0.92 (0.84)    |\n| Deductible       | -0.55** (0.13) | -0.56** (0.13) |\n\n*Notes: Robust standard errors in parentheses. ** significant at 1%. The first-stage partial F-statistic for the IV model is 18.8. The J-test of overidentifying restrictions has a p-value of 0.42.*\n\n---\n\nBased on the results in **Table 1** and the paper's discussion, select all of the following statements that are valid interpretations or conclusions.\n", "Options": {"A": "The IV estimate for the Spending Account coefficient (0.92) is not statistically significant at the 5% level.", "B": "The reported J-test p-value of 0.42 provides strong evidence that the instruments are invalid because they are correlated with the error term.", "C": "A policy change that increases the spending account by $500 and the deductible by $1,000 is predicted by the IV model to decrease total annual spending by $100.", "D": "The fact that the OLS estimate for the Spending Account (1.23) is larger than the IV estimate (0.92) suggests that, on net, adverse selection is a more dominant force than advantageous selection in this sample."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret regression coefficients, standard errors, and the implications of comparing OLS and IV estimates. It also tests the ability to perform a multi-step calculation based on the model. Depth Strategy: Computational Judgment and Reverse-Reasoning. Distractor Logic: Option A is a correct interpretation of statistical significance (t-stat = 0.92/0.84 ≈ 1.095 < 1.96). Option B correctly interprets the direction of the OLS-IV difference as evidence for the direction of net selection bias. Option C requires a correct calculation: (0.92 * 500) + (-0.56 * 1000) = 460 - 560 = -100. Option D is a Conceptual Opposite; a high p-value for the J-test indicates a failure to reject the null of valid instruments, supporting the IV strategy.", "qid": "151", "question": "### Background\n\nThe study uses OLS and Instrumental Variables (IV) regressions to estimate the causal effect of Consumer-Directed Health Plan (CDHP) features on total health care spending. The IV strategy is intended to correct for endogeneity bias stemming from employees' decisions about how much to contribute to their spending accounts.\n\n### Data / Model Specification\n\nThe core regression model is:\n\n  \nTotal\\_Spending_{igt} = \\beta_0 + \\beta_{Account} \\text{Spending Account}_{igt} + \\beta_{Deductible} \\text{Deductible}_{gt} + \\dots + \\eta_g + \\varepsilon_{igt} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Estimated Effects on Total Health Care Spending**\n\n| Regressor        | OLS Estimate   | IV Estimate    |\n| :--------------- | :------------: | :------------: |\n| Spending Account | 1.23** (0.17)  | 0.92 (0.84)    |\n| Deductible       | -0.55** (0.13) | -0.56** (0.13) |\n\n*Notes: Robust standard errors in parentheses. ** significant at 1%. The first-stage partial F-statistic for the IV model is 18.8. The J-test of overidentifying restrictions has a p-value of 0.42.*\n\n---\n\nBased on the results in **Table 1** and the paper's discussion, select all of the following statements that are valid interpretations or conclusions.\n", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 157, "Question": "### Background\n\n**Research Question.** This case investigates the core innovation of the paper: the use of a deep neural network (NN) as a computationally efficient surrogate for the bond pricing function in complex, non-linear term structure models.\n\n**Setting / Data-Generating Environment.** The paper proposes a two-stage solution: first, use a slow but accurate Monte Carlo (MC) simulation to generate a large training dataset of model parameter inputs and corresponding bond yield outputs. Second, train a deep NN to learn this mapping. The trained NN is a function `ŷ = ŷ(θ)` composed of alternating linear transformations (via weight matrices `β` and bias vectors `α`) and non-linear activation functions `κ(.)`. The trained NN can then be used as a rapid pricing engine inside a Bayesian MCMC estimation loop.\n\n### Question\n\nSelect all statements that accurately describe the paper's proposed methodology.", "Options": {"A": "The computationally intensive MC simulation is performed only once (offline) to create the training data, making the millions of subsequent (online) price calculations required by the MCMC algorithm fast and feasible.", "B": "The non-linear activation function `κ(.)` is critical because, without it, the entire multi-layer network would collapse into a single, less expressive linear function.", "C": "The primary role of the bias vectors `α` is to learn the complex interactions between inputs, while the weight matrices `β` scale the outputs of the activation functions.", "D": "The MC estimator is not used directly within the MCMC loop because it is less accurate than the trained neural network."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: Tests understanding of the NN architecture and the logic of the two-stage training/estimation process. Strategy: Select-all-valid task. Distractor Logic: Options A and B are correct statements summarizing key aspects of the methodology. Option C is incorrect; it reverses the roles of weights (interactions) and biases (shifts/translations). Option D is incorrect; it misidentifies the reason for not using MC directly. The paper shows MC can be highly accurate (by increasing simulations), but it is prohibitively slow, which is the actual reason it's replaced by the NN for inference.", "qid": "157", "question": "### Background\n\n**Research Question.** This case investigates the core innovation of the paper: the use of a deep neural network (NN) as a computationally efficient surrogate for the bond pricing function in complex, non-linear term structure models.\n\n**Setting / Data-Generating Environment.** The paper proposes a two-stage solution: first, use a slow but accurate Monte Carlo (MC) simulation to generate a large training dataset of model parameter inputs and corresponding bond yield outputs. Second, train a deep NN to learn this mapping. The trained NN is a function `ŷ = ŷ(θ)` composed of alternating linear transformations (via weight matrices `β` and bias vectors `α`) and non-linear activation functions `κ(.)`. The trained NN can then be used as a rapid pricing engine inside a Bayesian MCMC estimation loop.\n\n### Question\n\nSelect all statements that accurately describe the paper's proposed methodology.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"deep neural network surrogate bond pricing term structure models paper\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Cmu\" Carnegie Mellon University](https://www.ml.cmu.edu/research/dap-papers/f18/dap-martin-daniel.pdf)\n> [🔗 \"Pricing the Term Structure with Linear Regressions\" Federal Reserve Bank of New York](https://www.newyorkfed.org/medialibrary/media/research/staff_reports/sr340.pdf)\n> [🔗 \"Deep Learning Neural Networks for Bond Pricing | by gaurav singh | Medium\" Medium](https://medium.com/%2540gaurav1086/deep-learning-neural-networks-for-bond-pricing-59c720f19f8a)\n> [🔗 \"Improving CAT bond pricing models via machine learning | Journal of Asset Management\" Springer](https://link.springer.com/article/10.1057/s41260-020-00167-0)\n> [🔗 \"Full article: Predicting Corporate Bond Illiquidity via Machine Learning\" Taylor & Francis Online](https://www.tandfonline.com/doi/full/10.1080/0015198X.2024.2350952)\n> [🔗 \"Bond Risk Premiums with Machine Learning | The Review of Financial Studies | Oxford Academic\" Oxford Academic](https://academic.oup.com/rfs/article-abstract/34/2/1046/5843806)\n> [🔗 \"Predicting bond risk premiums with machine learning: Evidence from China - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0927538X25002197)\n> [🔗 \"Deep Learning in Asset Pricing\" Yale Department of Economics](https://economics.yale.edu/sites/default/files/deep_learning_in_asset_pricing.pdf)\n> [🔗 \"Affine Term Structure Models\" Stanford University](https://web.stanford.edu/~piazzesi/s.pdf)\n> [🔗 \"[1910.13205] Deep reinforcement learning for market making in corporate bonds: beating the curse of dimensionality\" arXiv](https://arxiv.org/abs/1910.13205)\n\n> 🔍 **Analyzing methodology**\n> ***- The paper uses a two-stage approach with Monte Carlo simulation for initial data generation.***\n\n> 🔍 **Searching for:** \"\"deep neural network\" \"monte carlo\" \"bond pricing\" \"mcmc\" term structure\"\n> 📊 **Results requested:** 10\n\n> ***- The Monte Carlo simulation creates a large training dataset offline for the neural network.***\n\n> 🔍 **Searching for:** \"deep neural network surrogate bond pricing term structure models paper\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Found 10 search results**\n\n🔗 **[Cmu](https://www.ml.cmu.edu/research/dap-papers/f18/dap-martin-daniel.pdf)**\n   📄 Index of /research/dap-<strong>papers</strong>\n   🌐 *Carnegie Mellon University*\n\n🔗 **[Pricing the Term Structure with Linear Regressions](https://www.newyorkfed.org/medialibrary/media/research/staff_reports/sr340.pdf)**\n   📄 At the New York Fed, our mission is to make the U.S. economy stronger and the financial system more stable for all segments of society. We do this by ...\n   🌐 *Federal Reserve Bank of New York*\n\n🔗 **[Deep Learning Neural Networks for Bond Pricing | by gaurav singh | Medium](https://medium.com/%2540gaurav1086/deep-learning-neural-networks-for-bond-pricing-59c720f19f8a)**\n   📄 In this post, I will try to train an ANN (Artificial Neural Network) to identify a geometrical figure, let’s say a circle to highlight its…\n   🌐 *Medium*\n\n🔗 **[Improving CAT bond pricing models via machine learning | Journal of Asset Management](https://link.springer.com/article/10.1057/s41260-020-00167-0)**\n   📄 Enhanced machine learning methods provide an encouraging alternative to forecast asset prices by extending or generalizing the possible model specific...\n   🌐 *Springer*\n\n🔗 **[Full article: Predicting Corporate Bond Illiquidity via Machine Learning](https://www.tandfonline.com/doi/full/10.1080/0015198X.2024.2350952)**\n   📄 This paper tests the predictive performance of machine learning methods in estimating the illiquidity of US corporate bonds. Machine learning techniqu...\n   🌐 *Taylor & Francis Online*\n\n🔗 **[Bond Risk Premiums with Machine Learning | The Review of Financial Studies | Oxford Academic](https://academic.oup.com/rfs/article-abstract/34/2/1046/5843806)**\n   📄 We show that machine learning methods, in particular, extreme trees and neural networks (NNs), provide strong statistical evidence in favor of bond re...\n   🌐 *Oxford Academic*\n\n🔗 **[Predicting bond risk premiums with machine learning: Evidence from China - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0927538X25002197)**\n   📄 This study evaluates the ability of machine-learning algorithms to forecast bond risk premiums in the Chinese market. Using a comprehensive set of mac...\n   🌐 *ScienceDirect*\n\n🔗 **[Deep Learning in Asset Pricing](https://economics.yale.edu/sites/default/files/deep_learning_in_asset_pricing.pdf)**\n   📄 A world-class institution for learning and research in economics · The Economics Department organizes dozens of events, workshops and seminars through...\n   🌐 *Yale Department of Economics*\n\n🔗 **[Affine Term Structure Models](https://web.stanford.edu/~piazzesi/s.pdf)**\n   📄 Click here on how to pronounce my last name · Department of Economics\n   🌐 *Stanford University*\n\n🔗 **[[1910.13205] Deep reinforcement learning for market making in corporate bonds: beating the curse of dimensionality](https://arxiv.org/abs/1910.13205)**\n   📄 In corporate bond markets, which are mainly OTC markets, market makers play a central role by providing bid and ask prices for a large number of bonds...\n   🌐 *arXiv*\n\n\n> 🔍 **Searching for:** \"neural network surrogate monte carlo bond pricing bayesian mcmc term structure\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"MCMC-Net: Accelerating Markov Chain Monte Carlo with Neural Networks for Inverse Problems\" arXiv](https://arxiv.org/html/2412.16883v1)\n> [🔗 \"Neural Networks in finance?: Markov Chain Monte Carlo (MCMC) and Stochastic Volatility Modelling | by Aroa Gomez | Analytics Vidhya | Medium\" Medium](https://medium.com/analytics-vidhya/neural-networks-in-finance-markov-chain-monte-carlo-mcmc-and-stochastic-volatility-modelling-3f4f148c3046)\n> [🔗 \"(PDF) Neural Classifiers based Monte Carlo simulation\" ResearchGate](https://www.researchgate.net/publication/372784924_Neural_Classifiers_based_Monte_Carlo_simulation)\n> [🔗 \"Neural Surrogate HMC: Accelerated Hamiltonian Monte Carlo with a Neural Network Surrogate Likelihood\" arXiv](https://arxiv.org/html/2407.20432)\n> [🔗 \"[2304.02902] Towards Efficient MCMC Sampling in Bayesian Neural Networks by Exploiting Symmetry\" arXiv](https://arxiv.org/abs/2304.02902)\n> [🔗 \"Efficient Geometric Markov Chain Monte Carlo for Nonlinear Bayesian Inversion Enabled by Derivative-Informed Neural Operators\" arXiv](https://arxiv.org/html/2403.08220v1)\n> [🔗 \"[2403.08220] Derivative-informed neural operator acceleration of geometric MCMC for infinite-dimensional Bayesian inverse problems\" arXiv](https://arxiv.org/abs/2403.08220)\n> [🔗 \"Hamiltonian Monte Carlo acceleration using surrogate functions with random bases - PMC\" PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC5624739/)\n> [🔗 \"Towards Efficient MCMC Sampling in Bayesian Neural Networks by Exploiting Symmetry | SpringerLink\" Springer](https://link.springer.com/chapter/10.1007/978-3-031-43412-9_27)\n> [🔗 \"Challenges in Markov chain Monte Carlo for Bayesian neural ...\" University of Manchester](https://research.manchester.ac.uk/files/196953151/STS840_double_column.pdf)\n\n> ***- The trained neural network acts as a fast pricing engine within the Bayesian MCMC estimation loop.***\n\n> 🔍 **Searching for:** \"\"deep neural network\" \"monte carlo\" \"bond pricing\" \"mcmc\" term structure\"\n> 📊 **Results requested:** 10\n\n\nA,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 405, "Question": "### Background\n\n**Research Question.** Having established that liquidity enhances market efficiency, this question explores two deeper issues: (1) Did this effect vary by firm size? and (2) What is the underlying market-maker behavior that generates this predictability in the first place? The paper contrasts two hypotheses for predictability: it arises either from rational but capacity-constrained market makers, or from cognitively limited (behaviorally biased) market makers.\n\n**Setting and Sample.** The analysis uses 5-minute data for NYSE stocks, stratified by firm size. Order imbalance (`OIB$`) is decomposed into an expected component (`OIBP$`) and an unexpected innovation (`OIBI$`) using an AR(12) model.\n\n### Data / Model Specification\n\n**Model 1: Size-Stratified Regressions.** The main interaction model is run separately for Large, Mid-Cap, and Small firm portfolios.\n  \nReturn_t = \\alpha + \\beta_1 OIB\\$_{t-1} + \\beta_2 (OIB\\$_{t-1} \\cdot ILD_t) + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n**Model 2: OIB Decomposition.** The predictive regression is re-run using the decomposed components of order flow.\n  \nReturn_t = \\alpha + \\beta OIBI\\$_{t-1} + \\gamma OIBP\\$_{t-1} + ... + \\epsilon_t \\quad \\text{(Eq. 2)}\n \n\n**Table 1. Regression Results for Heterogeneity and Mechanism**\n\n| Panel A: Baseline Predictability (`β₁` from Eq. 1) by Firm Size and Tick Regime | Large Firms | Small Firms |\n| :--- | :--- | :--- |\n| Eighths Regime | 0.0293 | 0.0115 |\n| Decimal Regime | 0.0029 | 0.0245 |\n\n| Panel B: OIB Decomposition Results (Eq. 2, Sixteenths Regime) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| Unexpected OIB (`OIBI$t-1`) | 0.0131 | 7.51 |\n| Expected OIB (`OIBP$t-1`) | 0.0012 | 0.16 |\n\n### Question\n\nBased on the provided results regarding heterogeneity and market-maker behavior, which of the following conclusions are supported by the paper?", "Options": {"A": "The evidence from Panel A suggests that the reduction in minimum tick size had a much stronger positive impact on the market efficiency of Large Firms compared to Small Firms.", "B": "For Small Firms, the baseline predictability coefficient (`β₁`) decreased from the Eighths to the Decimal regime, indicating an improvement in market efficiency.", "C": "The results in Panel B, where only the unexpected component of order flow (`OIBI$`) predicts returns, are consistent with a market where predictability arises from the limited risk-bearing capacity of rational market makers, not their cognitive biases.", "D": "The results in Panel B support the hypothesis of cognitively limited market makers, as both expected and unexpected order flow components significantly predict returns."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret findings on both subgroup heterogeneity and the underlying economic mechanism. It uses an Atomic Decomposition strategy, testing two distinct conclusions from the paper. Distractor C is a Conceptual Opposite, reversing the observed trend for small firms shown in the table. Distractor D misinterprets the results of the mechanism test, supporting the rejected hypothesis.", "qid": "405", "question": "### Background\n\n**Research Question.** Having established that liquidity enhances market efficiency, this question explores two deeper issues: (1) Did this effect vary by firm size? and (2) What is the underlying market-maker behavior that generates this predictability in the first place? The paper contrasts two hypotheses for predictability: it arises either from rational but capacity-constrained market makers, or from cognitively limited (behaviorally biased) market makers.\n\n**Setting and Sample.** The analysis uses 5-minute data for NYSE stocks, stratified by firm size. Order imbalance (`OIB$`) is decomposed into an expected component (`OIBP$`) and an unexpected innovation (`OIBI$`) using an AR(12) model.\n\n### Data / Model Specification\n\n**Model 1: Size-Stratified Regressions.** The main interaction model is run separately for Large, Mid-Cap, and Small firm portfolios.\n  \nReturn_t = \\alpha + \\beta_1 OIB\\$_{t-1} + \\beta_2 (OIB\\$_{t-1} \\cdot ILD_t) + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n**Model 2: OIB Decomposition.** The predictive regression is re-run using the decomposed components of order flow.\n  \nReturn_t = \\alpha + \\beta OIBI\\$_{t-1} + \\gamma OIBP\\$_{t-1} + ... + \\epsilon_t \\quad \\text{(Eq. 2)}\n \n\n**Table 1. Regression Results for Heterogeneity and Mechanism**\n\n| Panel A: Baseline Predictability (`β₁` from Eq. 1) by Firm Size and Tick Regime | Large Firms | Small Firms |\n| :--- | :--- | :--- |\n| Eighths Regime | 0.0293 | 0.0115 |\n| Decimal Regime | 0.0029 | 0.0245 |\n\n| Panel B: OIB Decomposition Results (Eq. 2, Sixteenths Regime) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| Unexpected OIB (`OIBI$t-1`) | 0.0131 | 7.51 |\n| Expected OIB (`OIBP$t-1`) | 0.0012 | 0.16 |\n\n### Question\n\nBased on the provided results regarding heterogeneity and market-maker behavior, which of the following conclusions are supported by the paper?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": ["C"], "error": null}}
{"ID": 428, "Question": "### Background\n\nThe study uses time-series regressions on U.S. annual data (1946-64) to analyze the link between corporate plant and equipment (P&E) expenditures, external financing, and interest rates. The author highlights a puzzle where interest rates are positively correlated with investment, contrary to traditional theory.\n\n### Data / Model Specification\n\nThe author argues that a positive correlation between interest rates and expenditures may reflect shifts in credit demand rather than movements along a stable demand curve: \"higher interest rates may...denote increased finance and increased expenditures if the demand for credit shifts to the right.\"\n\n**Table 1: Selected Regressions for P&E Expenditures (based on Table 6 of the paper)**\n\n| Eq. | Constant | `SAVE` | `BONDS` | `STOCK` | `RATE_govt` | `RATE_corp` | R² |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| (1) | 1.1430 | 0.4946** | 0.9561* | | | | 0.945 |\n| | (1.64) | (5.56) | (2.18) | | | | |\n| (2) | 0.4524 | 0.5148** | | 1.5292* | | | 0.941 |\n| | (0.54) | (6.88) | | (2.20) | | | |\n| (3) | -2.8148** | 0.8555** | | | 3.7115* | | 0.941 |\n| | (-2.23) | (17.05) | | | (2.03) | | |\n| (4) | -0.6286 | 0.7522** | | | | 3.0392** | 0.942 |\n*Notes: t-statistics in parentheses. ** significant at 1%, * significant at 5%. `SAVE` is gross saving, `BONDS` is corporate bonds issued, `STOCK` is corporate stock issued, `RATE_govt` is long-term government bond yield, and `RATE_corp` is long-term corporate bond yield.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following statements are valid interpretations supporting the paper's central thesis?\n\nSelect all that apply.", "Options": {"A": "The statistically significant positive coefficients on `BONDS` and `STOCK` in Eq. (1) and Eq. (2) demonstrate that external financial flows are a key determinant of P&E expenditures.", "B": "The significant positive coefficients on `RATE_govt` and `RATE_corp` in Eq. (3) and Eq. (4) suggest that periods of high investment demand are shifting the credit demand curve rightward, increasing both equilibrium investment and interest rates.", "C": "The consistently high significance of the `SAVE` variable across all equations indicates that internal funds are the only truly important driver of P&E spending, making external finance secondary.", "D": "The results in Eq. (3) and Eq. (4) imply that the true relationship between interest rates and investment is positive, contradicting the traditional theory of interest rates as a cost of capital."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret regression results in the context of the paper's core argument. It tests understanding of both the 'financial flow' linkage and the 'interest rate puzzle'.\nDepth Strategy: Reverse-Reasoning. Given the empirical results (the regression table), the user must identify the theoretical conclusions that are plausibly supported by them.\nDistractor Logic:\n- A (Correct): Directly tests the interpretation of the financial flow variables as supporting the paper's thesis.\n- B (Correct): Correctly interprets the positive interest rate coefficient as evidence of demand shifts, which is the author's central explanation for the puzzle.\n- C (Conceptual Distractor - Almost Right): This is a subtle misinterpretation. The author does not claim the *true* structural relationship is positive, but that the *observed* relationship is positive due to an identification problem (simultaneous shifts in supply and demand). The underlying demand curve is still presumed to be downward sloping.\n- D (Conceptual Distractor - Conceptual Opposite): While `SAVE` is significant, the significance of `BONDS` and `STOCK` directly contradicts the idea that external finance is secondary. This distractor forces the user to evaluate the importance of multiple variables, not just the most significant one.", "qid": "428", "question": "### Background\n\nThe study uses time-series regressions on U.S. annual data (1946-64) to analyze the link between corporate plant and equipment (P&E) expenditures, external financing, and interest rates. The author highlights a puzzle where interest rates are positively correlated with investment, contrary to traditional theory.\n\n### Data / Model Specification\n\nThe author argues that a positive correlation between interest rates and expenditures may reflect shifts in credit demand rather than movements along a stable demand curve: \"higher interest rates may...denote increased finance and increased expenditures if the demand for credit shifts to the right.\"\n\n**Table 1: Selected Regressions for P&E Expenditures (based on Table 6 of the paper)**\n\n| Eq. | Constant | `SAVE` | `BONDS` | `STOCK` | `RATE_govt` | `RATE_corp` | R² |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| (1) | 1.1430 | 0.4946** | 0.9561* | | | | 0.945 |\n| | (1.64) | (5.56) | (2.18) | | | | |\n| (2) | 0.4524 | 0.5148** | | 1.5292* | | | 0.941 |\n| | (0.54) | (6.88) | | (2.20) | | | |\n| (3) | -2.8148** | 0.8555** | | | 3.7115* | | 0.941 |\n| | (-2.23) | (17.05) | | | (2.03) | | |\n| (4) | -0.6286 | 0.7522** | | | | 3.0392** | 0.942 |\n*Notes: t-statistics in parentheses. ** significant at 1%, * significant at 5%. `SAVE` is gross saving, `BONDS` is corporate bonds issued, `STOCK` is corporate stock issued, `RATE_govt` is long-term government bond yield, and `RATE_corp` is long-term corporate bond yield.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following statements are valid interpretations supporting the paper's central thesis?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 427, "Question": "### Background\n\n**Research Question.** How precise are the theoretical bounds on lifetime functionals when applied to both a theoretical mortality law and a real-world life table, and what do the results reveal about the practical severity of model risk from fractional age assumptions?\n\n**Setting.** An actuary has developed a theoretical framework for bounding the value of life-contingent claims for any lifetime `X` that has an Increasing Force of Mortality (IFR) and is consistent with a given life table. The framework produces a stochastic lower bound `L` (based on a Constant Force of Mortality assumption) and a stochastic upper bound `U` (the supremum bound). The key result is that for any valid lifetime `X`, `L ≤_st X ≤_st U`.\n\n**Variables and Parameters.**\n\n*   `e_x`: Expected future lifetime for a person aged `x` (units: years).\n*   `Ā_x`: Present value of a continuous life annuity for a person aged `x`, with interest rate `i=0.06` (dimensionless value).\n*   `E[h(L)]`: The value of a functional calculated using the stochastic lower bound `L`.\n*   `E[h(U)]`: The value of a functional calculated using the stochastic upper bound `U`.\n*   `E[h(U_μ*)]`: The value of a functional calculated using a tighter upper bound that requires optimizing over possible mortality rates at integer ages.\n\n---\n\n### Data / Model Specification\n\nThe following tables present calculated bounds for two scenarios: a theoretical lifetime distribution from Makeham’s law (where the true value is known) and a real-world German life table from 1999/2001.\n\n**Table 1.** Bounds for `e_30` and `Ā_30` under Makeham's Law (`i=0.06`)\n\n| | `e_30` (years) | `Ā_30` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 45.05944407 | 0.10554796 |\n| **True Value** | **45.06687713** | **0.10550557** |\n| `E[h(U_μ*)]` | 45.07059946 | 0.10548412 |\n| `E[h(U)]` | 45.08173562 | 0.10542090 |\n\n**Table 2.** Bounds for `e_50` and `Ā_50` using German Life Table 1999/2001 (`i=0.06`)\n\n| | `e_50` (years) | `Ā_50` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 27.27856559 | 0.24740336 |\n| `E[h(U_μ*)]` | 27.28957960 | 0.24723758 |\n| `E[h(U)]` | 27.29933908 | 0.24708628 |\n\nFor any increasing function `f(X)` (like `e_x`), the stochastic ordering `L ≤_st X ≤_st U` implies `E[f(L)] ≤ E[f(X)] ≤ E[f(U)]`. For any decreasing function `g(X)`, the implication is reversed: `E[g(L)] ≥ E[g(X)] ≥ E[g(U)]`.\n\n---\n\n### Question\n\nThe numerical results for the annuity `Ā_x` in both tables show `E[Ā_x(L)] > E[Ā_x(U)]`. This contradicts the theoretical expectation from the stochastic order `L ≤_st U`, which implies `E[f(L)] ≤ E[f(U)]` for any increasing function `f(X)`. Select all of the following statements that correctly diagnose this contradiction.", "Options": {"A": "The discrepancy is minor and likely stems from a numerical precision error in the integration routine used to calculate the annuity values, rather than a fundamental conceptual mistake.", "B": "The observed numerical pattern (`E[value(L)] > E[value(U)]`) would be correct if the annuity were a decreasing function of lifetime. A plausible error is that the authors misclassified the annuity's functional form when interpreting the results.", "C": "The present value of a continuous life annuity is an increasing function of lifetime `X`, meaning the theoretical relationship `E[Ā_x(L)] ≤ E[Ā_x(U)]` must hold. The reported numbers violate this property.", "D": "The paper's core crossing criterion is likely invalid for financial instruments, as non-linear discounting can break the conditions required for stochastic ordering."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses deep conceptual understanding by asking the user to diagnose a contradiction between theory and reported data. It uses a **Reverse-Reasoning** strategy. The correct options require identifying the correct theoretical property of the annuity and pinpointing the likely source of the error in the paper's application. Distractor C is a **Conceptual (Almost Right)** error that incorrectly blames the paper's core theorem. Distractor D is a **Conceptual** error that mischaracterizes a fundamental sign error as a minor numerical issue.", "qid": "427", "question": "### Background\n\n**Research Question.** How precise are the theoretical bounds on lifetime functionals when applied to both a theoretical mortality law and a real-world life table, and what do the results reveal about the practical severity of model risk from fractional age assumptions?\n\n**Setting.** An actuary has developed a theoretical framework for bounding the value of life-contingent claims for any lifetime `X` that has an Increasing Force of Mortality (IFR) and is consistent with a given life table. The framework produces a stochastic lower bound `L` (based on a Constant Force of Mortality assumption) and a stochastic upper bound `U` (the supremum bound). The key result is that for any valid lifetime `X`, `L ≤_st X ≤_st U`.\n\n**Variables and Parameters.**\n\n*   `e_x`: Expected future lifetime for a person aged `x` (units: years).\n*   `Ā_x`: Present value of a continuous life annuity for a person aged `x`, with interest rate `i=0.06` (dimensionless value).\n*   `E[h(L)]`: The value of a functional calculated using the stochastic lower bound `L`.\n*   `E[h(U)]`: The value of a functional calculated using the stochastic upper bound `U`.\n*   `E[h(U_μ*)]`: The value of a functional calculated using a tighter upper bound that requires optimizing over possible mortality rates at integer ages.\n\n---\n\n### Data / Model Specification\n\nThe following tables present calculated bounds for two scenarios: a theoretical lifetime distribution from Makeham’s law (where the true value is known) and a real-world German life table from 1999/2001.\n\n**Table 1.** Bounds for `e_30` and `Ā_30` under Makeham's Law (`i=0.06`)\n\n| | `e_30` (years) | `Ā_30` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 45.05944407 | 0.10554796 |\n| **True Value** | **45.06687713** | **0.10550557** |\n| `E[h(U_μ*)]` | 45.07059946 | 0.10548412 |\n| `E[h(U)]` | 45.08173562 | 0.10542090 |\n\n**Table 2.** Bounds for `e_50` and `Ā_50` using German Life Table 1999/2001 (`i=0.06`)\n\n| | `e_50` (years) | `Ā_50` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 27.27856559 | 0.24740336 |\n| `E[h(U_μ*)]` | 27.28957960 | 0.24723758 |\n| `E[h(U)]` | 27.29933908 | 0.24708628 |\n\nFor any increasing function `f(X)` (like `e_x`), the stochastic ordering `L ≤_st X ≤_st U` implies `E[f(L)] ≤ E[f(X)] ≤ E[f(U)]`. For any decreasing function `g(X)`, the implication is reversed: `E[g(L)] ≥ E[g(X)] ≥ E[g(U)]`.\n\n---\n\n### Question\n\nThe numerical results for the annuity `Ā_x` in both tables show `E[Ā_x(L)] > E[Ā_x(U)]`. This contradicts the theoretical expectation from the stochastic order `L ≤_st U`, which implies `E[f(L)] ≤ E[f(U)]` for any increasing function `f(X)`. Select all of the following statements that correctly diagnose this contradiction.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 302, "Question": "### Background\n\n**Research Question.** How can the theoretical results of joint credibility forecasting be applied and verified using a specific numerical example where the forecasts are known to be exact?\n\n**Setting.** The paper analyzes a Normal likelihood `p(x|\\mu, \\omega) = \\text{Normal}(\\mu, \\omega^{-1})` with a conjugate Normal-Gamma prior. The chosen hyperparameters are `x_{01}=10`, `n_{01}=10`, `x_{02}=21`, and `\\alpha=6.5`. This specific choice satisfies the condition `2\\alpha = n_{01}+3`, which ensures that the credibility forecasts for the first and second moments, `f_1(\\mathcal{D})` and `f_2(\\mathcal{D})`, are exactly equal to the true Bayesian predictive means, `m_1(\\mathcal{D})` and `m_2(\\mathcal{D})`.\n\n**Variables and Parameters.**\n- `n`: The number of observations.\n- `\\mathbf{Z}`: The 3x3 joint credibility matrix.\n- `\\mathbf{f}(\\mathcal{D})`: The vector of joint forecasts for `[m_1, m_2, m_{11}]'`.\n- `\\mathbf{f}^*(\\mathcal{D})`: The vector of independent forecasts.\n- `\\mathbf{mse}(f_i)`: The mean-squared approximation error for the forecast of moment `i` using the joint predictor.\n- `\\mathbf{mse}(f_i^*)`: The mean-squared approximation error for the forecast of moment `i` using the independent predictor.\n- `n_{01}, n_{02}`: The time constants for the independent forecasts of the first and second moments, respectively. For this example, `n_{01}=10` and `n_{02}=10`.\n- `z_1, z_2`: The scalar credibility factors for the independent forecasts.\n\n---\n\n### Data / Model Specification\n\nThe prior moments for this example are `\\mathbf{m} = [m_1, m_2, m_{11}]' = [1, 2.1, 1.1]'`.\nThe paper provides the following numerical results for the joint and independent predictors for a sample size of `n=10`.\n\n**Table 1: Prediction Results for Example A (Normal-Gamma) at n=10**\n| Predictor Type | Credibility Matrix (Z) | mse f(D) `[f_1, f_2, f_{11}]` |\n|:---:|:---:|:---:|\n| **Joint** | `[[0.50000, 0.00000, 0.00000],` | `[0.05000,` |\n| | `[0.00000, 0.50000, 0.00000],` | `0.34667,` |\n| | `[0.47619, 0.04762, 0.21429]]` | `0.21862]` |\n| **Independent** | `[[0.50000, 0.00000, 0.00000],` | `[0.05000,` |\n| | `[0.00000, 0.50000, 0.00000],` | `0.34667,` |\n| | `[0.00000, 0.00000, 0.47265]]` | `0.22500]` |\n\n---\n\n### Question\n\nBased on the information provided, select all of the following statements that are true.", "Options": {"A": "Given observed sample statistics `\\mathbf{t}(\\mathcal{D}) = [1.5, 3.0, 1.2]'` for `n=10`, the joint credibility forecast for the second moment, `f_2(\\mathcal{D})`, is 2.05.", "B": "Given observed sample statistics `\\mathbf{t}(\\mathcal{D}) = [1.5, 3.0, 1.2]'` for `n=10`, the joint credibility forecast for the mean, `f_1(\\mathcal{D})`, is 1.25.", "C": "For `n=10`, the mean-squared error of the independent forecast `f_{11}^*` is approximately 2.9% higher than that of the joint forecast `f_{11}`.", "D": "For a sample size of `n=10`, the independent credibility factors for the first and second moments (`z_1` and `z_2`) are both equal to 0.50."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to verify credibility factor calculations, compare mean-squared errors, and apply the joint forecast formula. The strategy is atomic decomposition, combining verification, comparison, and calculation into a single multi-select item (Computational Judgment). Option D is a numerical distractor based on a plausible calculation error (using `m_{11}` instead of `m_2` in the forecast formula), testing for careful application of the model.", "qid": "302", "question": "### Background\n\n**Research Question.** How can the theoretical results of joint credibility forecasting be applied and verified using a specific numerical example where the forecasts are known to be exact?\n\n**Setting.** The paper analyzes a Normal likelihood `p(x|\\mu, \\omega) = \\text{Normal}(\\mu, \\omega^{-1})` with a conjugate Normal-Gamma prior. The chosen hyperparameters are `x_{01}=10`, `n_{01}=10`, `x_{02}=21`, and `\\alpha=6.5`. This specific choice satisfies the condition `2\\alpha = n_{01}+3`, which ensures that the credibility forecasts for the first and second moments, `f_1(\\mathcal{D})` and `f_2(\\mathcal{D})`, are exactly equal to the true Bayesian predictive means, `m_1(\\mathcal{D})` and `m_2(\\mathcal{D})`.\n\n**Variables and Parameters.**\n- `n`: The number of observations.\n- `\\mathbf{Z}`: The 3x3 joint credibility matrix.\n- `\\mathbf{f}(\\mathcal{D})`: The vector of joint forecasts for `[m_1, m_2, m_{11}]'`.\n- `\\mathbf{f}^*(\\mathcal{D})`: The vector of independent forecasts.\n- `\\mathbf{mse}(f_i)`: The mean-squared approximation error for the forecast of moment `i` using the joint predictor.\n- `\\mathbf{mse}(f_i^*)`: The mean-squared approximation error for the forecast of moment `i` using the independent predictor.\n- `n_{01}, n_{02}`: The time constants for the independent forecasts of the first and second moments, respectively. For this example, `n_{01}=10` and `n_{02}=10`.\n- `z_1, z_2`: The scalar credibility factors for the independent forecasts.\n\n---\n\n### Data / Model Specification\n\nThe prior moments for this example are `\\mathbf{m} = [m_1, m_2, m_{11}]' = [1, 2.1, 1.1]'`.\nThe paper provides the following numerical results for the joint and independent predictors for a sample size of `n=10`.\n\n**Table 1: Prediction Results for Example A (Normal-Gamma) at n=10**\n| Predictor Type | Credibility Matrix (Z) | mse f(D) `[f_1, f_2, f_{11}]` |\n|:---:|:---:|:---:|\n| **Joint** | `[[0.50000, 0.00000, 0.00000],` | `[0.05000,` |\n| | `[0.00000, 0.50000, 0.00000],` | `0.34667,` |\n| | `[0.47619, 0.04762, 0.21429]]` | `0.21862]` |\n| **Independent** | `[[0.50000, 0.00000, 0.00000],` | `[0.05000,` |\n| | `[0.00000, 0.50000, 0.00000],` | `0.34667,` |\n| | `[0.00000, 0.00000, 0.47265]]` | `0.22500]` |\n\n---\n\n### Question\n\nBased on the information provided, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 321, "Question": "### Background\n\n**Research Question.** This study examines why prior research has found mixed evidence on the link between board gender diversity and firm performance, proposing that the impact of female directors is task-dependent.\n\n**Setting / Data-Generating Environment.** The analysis uses survey data from CEOs of 201 Norwegian firms to measure board effectiveness in different domains.\n\n**Variables & Parameters.**\n- `Ratio of Women Directors`: The number of female directors as a percentage of total board size.\n- `Board Strategic Control`: A survey-based measure of the board's effectiveness in monitoring long-term, qualitative, and ex-ante strategic issues. The paper's theory links this to `Communal Characteristics` (e.g., interpersonal sensitivity, concern for stakeholders), which are more strongly ascribed to women.\n- `Board Operational Control`: A survey-based measure of the board's effectiveness in monitoring short-term, quantitative, and ex-post financial issues (e.g., budgets, investments).\n\n---\n\n### Data / Model Specification\n\nThe study's main empirical finding is that the effect of gender diversity is differential, as summarized in the stylized regression results below.\n\n**Table 1: Stylized Regression Results for Board Control Tasks**\n| | (1) Strategic Control | (2) Operational Control |\n|:---|:---:|:---:|\n| **Ratio of Women Directors** | **0.45*** | 0.08 |\n| | (0.18) | (0.21) |\n| Control Variables | Included | Included |\n| N | 201 | 201 |\n\n*Note: * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the provided background and **Table 1**, select all statements that accurately interpret the study's findings regarding the differential impact of women directors and their implications.", "Options": {"A": "The lack of a significant effect on operational control implies that female directors are less skilled in the quantitative tasks associated with financial monitoring.", "B": "The results indicate that the contributions of female directors are specialized, providing a comparative advantage for long-term strategic oversight but not for routine, short-term operational monitoring.", "C": "The study demonstrates that boards with more women are generally more effective across all major board functions, including both strategic and operational control.", "D": "This differential impact helps explain prior 'mixed evidence' on firm performance, as the net effect of gender diversity becomes context-dependent on which board task is more critical for a given firm's success."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the paper's core finding of a 'differential impact' and understand its implications for the broader literature. It uses an 'atomic decomposition' strategy. Option A tests the direct interpretation of the regression results in the context of the paper's theory. Option B tests the ability to connect this finding to resolving a puzzle in prior research. Distractor C is a conceptual opposite, representing a complete misreading of the table. Distractor D is a subtle misinterpretation, confusing a null differential effect with a negative performance judgment.", "qid": "321", "question": "### Background\n\n**Research Question.** This study examines why prior research has found mixed evidence on the link between board gender diversity and firm performance, proposing that the impact of female directors is task-dependent.\n\n**Setting / Data-Generating Environment.** The analysis uses survey data from CEOs of 201 Norwegian firms to measure board effectiveness in different domains.\n\n**Variables & Parameters.**\n- `Ratio of Women Directors`: The number of female directors as a percentage of total board size.\n- `Board Strategic Control`: A survey-based measure of the board's effectiveness in monitoring long-term, qualitative, and ex-ante strategic issues. The paper's theory links this to `Communal Characteristics` (e.g., interpersonal sensitivity, concern for stakeholders), which are more strongly ascribed to women.\n- `Board Operational Control`: A survey-based measure of the board's effectiveness in monitoring short-term, quantitative, and ex-post financial issues (e.g., budgets, investments).\n\n---\n\n### Data / Model Specification\n\nThe study's main empirical finding is that the effect of gender diversity is differential, as summarized in the stylized regression results below.\n\n**Table 1: Stylized Regression Results for Board Control Tasks**\n| | (1) Strategic Control | (2) Operational Control |\n|:---|:---:|:---:|\n| **Ratio of Women Directors** | **0.45*** | 0.08 |\n| | (0.18) | (0.21) |\n| Control Variables | Included | Included |\n| N | 201 | 201 |\n\n*Note: * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the provided background and **Table 1**, select all statements that accurately interpret the study's findings regarding the differential impact of women directors and their implications.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 104, "Question": "### Background\n\n**Research Question.** This case investigates the characteristics of firms involved in Reverse Takeovers (RTs) and their stated motivations to understand the underlying economic logic of these transactions.\n\n**Setting.** The analysis uses data on the financial condition of the public 'vehicle' firms, the stated reasons for the merger from proxy statements, and the industrial relatedness of the merging parties for a sample of 121 RTs.\n\n**Variables & Parameters.**\n- `ROA`: Return on Assets, a measure of profitability.\n- `V_ops`: The value of a firm's ongoing operations.\n- `V_listing`: The value of a firm's public stock exchange listing.\n\n---\n\n### Data / Model Specification\n\nThe characteristics of the firms involved in an RT reveal its true purpose. The public firm is often a poorly performing entity, while the private firm has a viable business seeking a public listing.\n\n**Table 1: Summary Characteristics of Public Firms (Year Prior to RT)**\n\n| Variable | Mean | Median |\n| :--- | :--- | :--- |\n| Total Assets (US$ millions) | 4.95 | 0.16 |\n| Return on Assets (ROA, %) | -21.87 | -10.75 |\n| Return on Equity (ROE, %) | -42.44 | -6.70 |\n\n*Note: Table is adapted from Panel A of Table 4 in the source.*\n\n**Table 2: Reasons Cited for Reverse Takeovers**\n\n| Reason(s) Cited in Filing Documents | Cited by Public Firm | Cited by Private Firm | Total Citations |\n| :--- | :--- | :--- | :--- |\n| Solid financial position of private parent | 43 | 16 | 63 |\n| Growth | 15 | 25 | 47 |\n| Going public issues | 7 | 17 | 27 |\n\n*Note: Table is a selection from Panel B of Table 2 in the source.*\n\n**Table 3: Distribution by Degree of Industry Similarity**\n\n| SIC Industry Classification | Before the Takeover | After the Takeover |\n| :--- | :--- | :--- |\n| From the same industry | 27.3% | 52.1% |\n| From complementary industries | 31.4% | 33.1% |\n| From different industries | 41.3% | 14.9% |\n\n*Note: Table is adapted from Panel B of Table 3 in the source. 'After the takeover' same industry is the sum of same as private, public, or both.*\n\n---\n\n### Question\n\nBased on the provided tables characterizing the firms and transactions, which of the following statements are accurate? Select all that apply.", "Options": {"A": "The most frequent motivation cited by the public firm was a desire to access capital markets or achieve other 'going public' objectives.", "B": "The typical public 'vehicle' firm was unprofitable in the year prior to the RT, as measured by median Return on Assets.", "C": "The most common pre-takeover arrangement involved two firms from the same industry seeking to achieve economies of scale.", "D": "Following the RT, the combined entity was most commonly structured to operate within a single industry (that of one or both parents), rather than across different or complementary industries."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to assess the student's ability to synthesize descriptive statistics from multiple tables to build a coherent profile of a typical RT. It tests the understanding of the firm characteristics, motivations, and industrial logic of the transaction. Distractors are based on high-fidelity errors: C (Conceptual Error) confuses the distinct motivations of the public and private firms, and D (Conceptual Opposite) presents the opposite of the empirical finding about pre-merger industry pairings.", "qid": "104", "question": "### Background\n\n**Research Question.** This case investigates the characteristics of firms involved in Reverse Takeovers (RTs) and their stated motivations to understand the underlying economic logic of these transactions.\n\n**Setting.** The analysis uses data on the financial condition of the public 'vehicle' firms, the stated reasons for the merger from proxy statements, and the industrial relatedness of the merging parties for a sample of 121 RTs.\n\n**Variables & Parameters.**\n- `ROA`: Return on Assets, a measure of profitability.\n- `V_ops`: The value of a firm's ongoing operations.\n- `V_listing`: The value of a firm's public stock exchange listing.\n\n---\n\n### Data / Model Specification\n\nThe characteristics of the firms involved in an RT reveal its true purpose. The public firm is often a poorly performing entity, while the private firm has a viable business seeking a public listing.\n\n**Table 1: Summary Characteristics of Public Firms (Year Prior to RT)**\n\n| Variable | Mean | Median |\n| :--- | :--- | :--- |\n| Total Assets (US$ millions) | 4.95 | 0.16 |\n| Return on Assets (ROA, %) | -21.87 | -10.75 |\n| Return on Equity (ROE, %) | -42.44 | -6.70 |\n\n*Note: Table is adapted from Panel A of Table 4 in the source.*\n\n**Table 2: Reasons Cited for Reverse Takeovers**\n\n| Reason(s) Cited in Filing Documents | Cited by Public Firm | Cited by Private Firm | Total Citations |\n| :--- | :--- | :--- | :--- |\n| Solid financial position of private parent | 43 | 16 | 63 |\n| Growth | 15 | 25 | 47 |\n| Going public issues | 7 | 17 | 27 |\n\n*Note: Table is a selection from Panel B of Table 2 in the source.*\n\n**Table 3: Distribution by Degree of Industry Similarity**\n\n| SIC Industry Classification | Before the Takeover | After the Takeover |\n| :--- | :--- | :--- |\n| From the same industry | 27.3% | 52.1% |\n| From complementary industries | 31.4% | 33.1% |\n| From different industries | 41.3% | 14.9% |\n\n*Note: Table is adapted from Panel B of Table 3 in the source. 'After the takeover' same industry is the sum of same as private, public, or both.*\n\n---\n\n### Question\n\nBased on the provided tables characterizing the firms and transactions, which of the following statements are accurate? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 432, "Question": "### Background\n\n**Research Question.** What is the causal effect of an underwriter's reputation on the degree of IPO underpricing, after accounting for the non-random matching between issuers and underwriters?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage least-squares (2SLS) regression to analyze IPO underpricing. The key challenge is that high-quality firms may preferentially match with high-reputation underwriters, creating an endogeneity problem.\n\n**Variables & Parameters.**\n\n*   `Underpricing_i`: The percentage difference between the first-day closing price and the offer price for IPO `i`.\n*   `Reputation_i`: The endogenous measure of underwriter reputation. The primary measure is the `Fragmented ranking Equally weighted` score.\n*   `X_i`: A vector of exogenous control variables for firm `i` (e.g., firm age, offer size, VC-backing).\n*   `Z_i`: The instrumental variable for `Reputation_i`, defined as the number of underwriters that have underwritten European IPOs in the same 2-digit industry as firm `i` in the past five years.\n\n---\n\n### Data / Model Specification\n\nThe study estimates the following structural model:\n\n  \n\\text{Underpricing}_i = \\beta_0 + \\beta_1 \\text{Reputation}_i + \\Gamma' X_i + \\epsilon_i\n \n\nDue to endogeneity, `Reputation_i` is instrumented using `Z_i` in a 2SLS framework.\n\n**Table 1: 2SLS Regression of Underpricing on Underwriter Reputation (Model 1)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| **Second Stage** | | |\n| Underwriter Reputation | | |\n| (Fragmented ranking Equally weighted) | -0.04 | (-6.08) |\n\n*Source: Adapted from Table 7, Model 1 in the paper.*\n\n---\n\n### The Question\n\nSuppose a critic asserts that the true causal effect of underwriter reputation on underpricing is zero (i.e., `β_1 = 0`). Given the study's finding of a highly significant negative coefficient (-0.04, t=-6.08) from the 2SLS estimation, which of the following conclusions logically follow from the critic's premise?", "Options": {"A": "The significant 2SLS result implies that the key identifying assumption—the exclusion restriction—is violated.", "B": "The finding proves that the instrument is irrelevant and fails the first-stage regression.", "C": "The instrument (`Z_i`) must have a direct negative causal effect on `Underpricing_i`, likely through a channel like increased pre-IPO competition and information discovery.", "D": "This outcome suggests that a simple OLS regression would have provided a less biased estimate of the effect."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Reverse-Reasoning strategy to test a deep understanding of instrumental variable assumptions. By positing that the study's conclusion is wrong (true effect is zero), it forces the candidate to deduce the logical implications for the model's validity, specifically the violation of the exclusion restriction. Distractors target common confusions between IV relevance and exogeneity (C) and the nature of endogeneity bias (D).", "qid": "432", "question": "### Background\n\n**Research Question.** What is the causal effect of an underwriter's reputation on the degree of IPO underpricing, after accounting for the non-random matching between issuers and underwriters?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage least-squares (2SLS) regression to analyze IPO underpricing. The key challenge is that high-quality firms may preferentially match with high-reputation underwriters, creating an endogeneity problem.\n\n**Variables & Parameters.**\n\n*   `Underpricing_i`: The percentage difference between the first-day closing price and the offer price for IPO `i`.\n*   `Reputation_i`: The endogenous measure of underwriter reputation. The primary measure is the `Fragmented ranking Equally weighted` score.\n*   `X_i`: A vector of exogenous control variables for firm `i` (e.g., firm age, offer size, VC-backing).\n*   `Z_i`: The instrumental variable for `Reputation_i`, defined as the number of underwriters that have underwritten European IPOs in the same 2-digit industry as firm `i` in the past five years.\n\n---\n\n### Data / Model Specification\n\nThe study estimates the following structural model:\n\n  \n\\text{Underpricing}_i = \\beta_0 + \\beta_1 \\text{Reputation}_i + \\Gamma' X_i + \\epsilon_i\n \n\nDue to endogeneity, `Reputation_i` is instrumented using `Z_i` in a 2SLS framework.\n\n**Table 1: 2SLS Regression of Underpricing on Underwriter Reputation (Model 1)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| **Second Stage** | | |\n| Underwriter Reputation | | |\n| (Fragmented ranking Equally weighted) | -0.04 | (-6.08) |\n\n*Source: Adapted from Table 7, Model 1 in the paper.*\n\n---\n\n### The Question\n\nSuppose a critic asserts that the true causal effect of underwriter reputation on underpricing is zero (i.e., `β_1 = 0`). Given the study's finding of a highly significant negative coefficient (-0.04, t=-6.08) from the 2SLS estimation, which of the following conclusions logically follow from the critic's premise?", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 304, "Question": "### Background\n\n**Research Question.** What determines the long-run behavior of credibility forecasts, and under what conditions might a forecast fail to become fully reliant on observed data, even with a large sample size?\n\n**Setting.** In the multidimensional credibility framework, the forecast `\\mathbf{f}(\\mathcal{D})` depends on the credibility matrix `\\mathbf{Z}`. The asymptotic behavior of `\\mathbf{Z}` as the sample size `n \\to \\infty` determines whether the forecast becomes \"fully credible\" (i.e., depends only on the sample statistics `\\mathbf{t}(\\mathcal{D})`).\n\n**Variables and Parameters.**\n- `\\mathbf{Z}`: The 3x3 credibility matrix.\n- `\\mathbf{D}`: The 3x3 between-risk covariance matrix, `\\mathbf{D} = \\mathcal{C}\\langle\\mathbf{m}(\\tilde{\\theta}); \\mathbf{m}(\\tilde{\\theta})\\rangle`.\n- `\\mathbf{E}(n)`: The 3x3 within-risk covariance matrix.\n- `\\mathbf{m}(\\theta) = [m_1(\\theta), m_2(\\theta), m_1(\\theta)^2]'`: The vector of conditional moments.\n\n---\n\n### Data / Model Specification\n\nAssuming the between-risk matrix `\\mathbf{D}` is invertible, the credibility matrix `\\mathbf{Z}` can be written as:\n\n  \n\\mathbf{Z} = n(n\\mathbf{I} + \\mathbf{N}(n))^{-1}, \\quad \\text{where } \\mathbf{N}(n) = \\mathbf{E}(n)\\mathbf{D}^{-1} \\quad \\text{(Eq. (1))}\n \n\nHowever, if `\\mathbf{D}` is singular (not invertible), this expression is invalid. This occurs if there is a linear dependency among the components of the conditional mean vector `\\mathbf{m}(\\theta)`. For example, for a Poisson(`\\pi`) likelihood, `m_2(\\pi) = m_1(\\pi) + m_1(\\pi)^2`.\n\n---\n\n### Question\n\nBased on the information provided about the asymptotic properties of credibility forecasts, select all of the following statements that are true.", "Options": {"A": "If the between-risk matrix `\\mathbf{D}` is invertible, the credibility matrix `\\mathbf{Z}` converges to the identity matrix `\\mathbf{I}` as `n \\to \\infty`, meaning the forecast becomes \"fully credible\" and depends only on the sample statistics `\\mathbf{t}(\\mathcal{D})`.", "B": "When `\\mathbf{D}` is singular, the forecast `\\mathbf{f}(\\mathcal{D})` fails to become fully credible because it remains dependent on the prior mean vector `\\mathbf{m}` even as `n \\to \\infty`.", "C": "The `\\mathbf{D}` matrix becomes singular if there is a perfect linear relationship among the conditional moments (e.g., `m_2(\\theta) = m_1(\\theta) + m_{11}(\\theta)`), because this implies a linear combination of the components of the random vector `\\mathbf{m}(\\tilde{\\theta})` has zero variance.", "D": "A \"fully credible\" forecast is one where the mean-squared forecast error matrix `\\mathbf{\\Psi}` becomes zero, indicating a perfect prediction of the future observation with no uncertainty."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item tests understanding of the conditions for and implications of the asymptotic behavior of credibility forecasts. The strategy is premise/assumption packaging, presenting complex statements about the model's long-run properties. Option C is a sophisticated 'Almost Right' distractor that misidentifies why the forecast fails full credibility in the singular case. Option D is a 'Conceptual' distractor that confuses the elimination of estimation risk with the elimination of total forecast error (process risk).", "qid": "304", "question": "### Background\n\n**Research Question.** What determines the long-run behavior of credibility forecasts, and under what conditions might a forecast fail to become fully reliant on observed data, even with a large sample size?\n\n**Setting.** In the multidimensional credibility framework, the forecast `\\mathbf{f}(\\mathcal{D})` depends on the credibility matrix `\\mathbf{Z}`. The asymptotic behavior of `\\mathbf{Z}` as the sample size `n \\to \\infty` determines whether the forecast becomes \"fully credible\" (i.e., depends only on the sample statistics `\\mathbf{t}(\\mathcal{D})`).\n\n**Variables and Parameters.**\n- `\\mathbf{Z}`: The 3x3 credibility matrix.\n- `\\mathbf{D}`: The 3x3 between-risk covariance matrix, `\\mathbf{D} = \\mathcal{C}\\langle\\mathbf{m}(\\tilde{\\theta}); \\mathbf{m}(\\tilde{\\theta})\\rangle`.\n- `\\mathbf{E}(n)`: The 3x3 within-risk covariance matrix.\n- `\\mathbf{m}(\\theta) = [m_1(\\theta), m_2(\\theta), m_1(\\theta)^2]'`: The vector of conditional moments.\n\n---\n\n### Data / Model Specification\n\nAssuming the between-risk matrix `\\mathbf{D}` is invertible, the credibility matrix `\\mathbf{Z}` can be written as:\n\n  \n\\mathbf{Z} = n(n\\mathbf{I} + \\mathbf{N}(n))^{-1}, \\quad \\text{where } \\mathbf{N}(n) = \\mathbf{E}(n)\\mathbf{D}^{-1} \\quad \\text{(Eq. (1))}\n \n\nHowever, if `\\mathbf{D}` is singular (not invertible), this expression is invalid. This occurs if there is a linear dependency among the components of the conditional mean vector `\\mathbf{m}(\\theta)`. For example, for a Poisson(`\\pi`) likelihood, `m_2(\\pi) = m_1(\\pi) + m_1(\\pi)^2`.\n\n---\n\n### Question\n\nBased on the information provided about the asymptotic properties of credibility forecasts, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 225, "Question": "### Background\nIn the later part of its sample, the paper finds that the Bank of Canada's (BoC) response to the standard one-year-ahead inflation gap becomes statistically insignificant. This raises the question of whether the BoC stopped responding to inflation, or if it shifted its focus to a different measure. The paper proposes that the BoC may respond to *persistent* inflation deviations (the average expected deviation over an extended future period) and that this response could be non-linear and asymmetric.\n\n### Data / Model Specification\nTo test this, the baseline monetary policy rule is augmented with a new inflation term, $\\tilde{\\pi}_{t}|\\tilde{\\pi}_{t}|$, which captures a non-linear response to the average expected future inflation deviation, $\\tilde{\\pi}_{t}$. The model allows for an asymmetric response by estimating separate coefficients for positive deviations (overshoots, $\\phi_{id}^{pos}$) and negative deviations (undershoots, $\\phi_{id}^{neg}$). A larger coefficient implies a stronger policy reaction.\n\nAccording to the paper's analysis, which of the following are valid characterizations of the BoC's policy response to persistent inflation deviations?\n", "Options": {"A": "The BoC's response is asymmetric and time-varying, with evidence suggesting a stronger reaction to positive deviations (overshoots) early in the post-disinflation period, and a stronger reaction to negative deviations (undershoots) later in the period, especially after the Global Financial Crisis.", "B": "The BoC's response is non-linear, meaning that as expected persistent deviations from the target grow larger, the policy interest rate response becomes disproportionately stronger.", "C": "The paper finds no evidence that the BoC responds to *past* persistent inflation deviations, suggesting a forward-looking \"bygone\" strategy rather than a backward-looking \"makeup\" strategy.", "D": "The paper finds that the BoC responds more aggressively to positive deviations (overshoots) than negative deviations (undershoots) throughout the entire post-disinflation period (1995-2015)."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the comprehension of the paper's most novel and complex finding: the shift to a non-linear, asymmetric, and time-varying response to persistent inflation deviations. Depth Strategy: Scenario Application. The question asks the user to apply the paper's general findings to characterize the BoC's behavior under different conditions (e.g., small vs. large deviations, early vs. late periods). Distractor Logic: Option D is a conceptual distractor (Almost Right). While the paper does find a strong response to positive deviations, it explicitly states this response was concentrated in the *first half* of the post-disinflation period and that the response to negative deviations became more prominent later. The claim that this holds for the *entire* period is incorrect and tests for a nuanced understanding of the time-varying nature of the asymmetry.", "qid": "225", "question": "### Background\nIn the later part of its sample, the paper finds that the Bank of Canada's (BoC) response to the standard one-year-ahead inflation gap becomes statistically insignificant. This raises the question of whether the BoC stopped responding to inflation, or if it shifted its focus to a different measure. The paper proposes that the BoC may respond to *persistent* inflation deviations (the average expected deviation over an extended future period) and that this response could be non-linear and asymmetric.\n\n### Data / Model Specification\nTo test this, the baseline monetary policy rule is augmented with a new inflation term, $\\tilde{\\pi}_{t}|\\tilde{\\pi}_{t}|$, which captures a non-linear response to the average expected future inflation deviation, $\\tilde{\\pi}_{t}$. The model allows for an asymmetric response by estimating separate coefficients for positive deviations (overshoots, $\\phi_{id}^{pos}$) and negative deviations (undershoots, $\\phi_{id}^{neg}$). A larger coefficient implies a stronger policy reaction.\n\nAccording to the paper's analysis, which of the following are valid characterizations of the BoC's policy response to persistent inflation deviations?\n", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 143, "Question": "### Background\n\nAn analysis of Italian savings banks (1982-1989) distinguished between standard and augmented economies of scale. The standard measure (`ξ`) captures cost changes from output growth within a fixed branch network. The augmented measure (`Augξ`) also includes the indirect cost effects of expanding the branch network itself to support that output growth. During this period, regulations severely restricted new branch openings.\n\n### Data / Model Specification\n\nThe augmented measure of economies of scale is defined as:\n  \n\\mathrm{Aug}\\xi = \\xi + \\left(\\frac{\\partial{\\mathrm{Ln}}(TC)}{\\partial{\\mathrm{Ln}}(B)}\\right) \\left(\\frac{d\\mathrm{Ln}(B)}{d\\mathrm{Ln}(Y_{1})}\\right) \\quad \\text{(Eq. (1))}\n \nwhere `TC` is total cost, `Y₁` is output (loans), and `B` is the number of branches.\n\nThe paper reports the following estimates for the full sample period:\n- `ξ` (Standard OES) = 0.7845\n- `Augξ` (Augmented OES) = 0.7978\n\n---\n\nBased on the model and empirical results, which of the following statements are valid conclusions?", "Options": {"A": "The empirical finding that `Augξ` > `ξ` implies that, on average, both the cost elasticity of branches (`∂Ln(TC)/∂Ln(B)`) and the branch elasticity of output (`dLn(B)/dLn(Y₁)`) were positive during the sample period.", "B": "The small difference between `Augξ` and `ξ` is consistent with the institutional context of strict branching restrictions, which would suppress the magnitude of `dLn(B)/dLn(Y₁) `.", "C": "The term `∂Ln(TC)/∂Ln(B)` represents the total elasticity of cost with respect to branches, allowing output to adjust simultaneously.", "D": "If branching regulations were completely lifted, one would expect the estimated value of `dLn(B)/dLn(Y₁)` to increase in a subsequent study period."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to deconstruct a composite formula, relate its components to empirical results and institutional context, and predict how a policy change would affect a model parameter. It tests deep reasoning about the interplay between a model, its estimates, and the real-world environment it represents.\nStrategy: Reverse-Reasoning. The user must work backward from the empirical result (`Augξ > ξ`) to infer the properties of the underlying components and connect them to the regulatory setting.\nDistractor Logic:\n- A (Correct): Since `Augξ` > `ξ`, the adjustment term must be positive. Assuming more branches increase costs (`∂Ln(TC)/∂Ln(B)` > 0), it must be that `dLn(B)/dLn(Y₁)` is also positive.\n- B (Correct): Strict regulations would make it difficult for banks to expand their branch network in response to output growth, leading to a small, though positive, elasticity `dLn(B)/dLn(Y₁)`. This would result in `Augξ` being only slightly larger than `ξ`.\n- C (Correct): Deregulation would remove the constraint, allowing banks to more freely adjust their branch network to match output growth, thus increasing the observed elasticity `dLn(B)/dLn(Y₁)`.\n- D (Incorrect - Conceptual Distractor): This misinterprets the partial derivative. `∂Ln(TC)/∂Ln(B)` is the partial elasticity, which measures the effect of branches on cost *holding output and other variables constant*, not allowing them to adjust.", "qid": "143", "question": "### Background\n\nAn analysis of Italian savings banks (1982-1989) distinguished between standard and augmented economies of scale. The standard measure (`ξ`) captures cost changes from output growth within a fixed branch network. The augmented measure (`Augξ`) also includes the indirect cost effects of expanding the branch network itself to support that output growth. During this period, regulations severely restricted new branch openings.\n\n### Data / Model Specification\n\nThe augmented measure of economies of scale is defined as:\n  \n\\mathrm{Aug}\\xi = \\xi + \\left(\\frac{\\partial{\\mathrm{Ln}}(TC)}{\\partial{\\mathrm{Ln}}(B)}\\right) \\left(\\frac{d\\mathrm{Ln}(B)}{d\\mathrm{Ln}(Y_{1})}\\right) \\quad \\text{(Eq. (1))}\n \nwhere `TC` is total cost, `Y₁` is output (loans), and `B` is the number of branches.\n\nThe paper reports the following estimates for the full sample period:\n- `ξ` (Standard OES) = 0.7845\n- `Augξ` (Augmented OES) = 0.7978\n\n---\n\nBased on the model and empirical results, which of the following statements are valid conclusions?", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 103, "Question": "### Background\n\n**Research Question.** This case investigates the apparent contradiction between the large positive market reaction to Reverse Takeover (RT) announcements and the poor long-term performance and high failure rate of the surviving firms.\n\n**Setting.** The analysis contrasts the short-term announcement returns (CARs) with the medium-term survival and performance of the newly combined public entities. The sample consists of 121 RTs from 1987-2001. A key puzzle is why the market reacts positively to events that so often lead to failure.\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return at announcement (dimensionless, as %).\n- `Survival_i`: A binary indicator, =1 if firm `i` survives for 2 years, 0 otherwise.\n- `Prior_Distress_i`: Dummy variable; =1 if the public vehicle was in financial distress, 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nThe data present a sharp contrast between the market's initial reaction and subsequent firm survival. A logistic regression is used to model the probability of survival based on pre-merger characteristics.\n\n**Table 1: Post-Reverse Takeover Survival Outcomes (Full Sample, N=121)**\n\n| Outcome | Number of Firms | Percent of Total Sample |\n| :--- | :--- | :--- |\n| **Survival Status** | | |\n| Survived 2 Years | 56 | 46.3% |\n| Did Not Survive 2 Years | 65 | 53.7% |\n| **Details for Non-Surviving Firms** | | |\n| Delisted/moved to Bulletin Board | 33 | 27.3% |\n| Bankruptcy | 24 | 19.8% |\n| Acquired | 25 | 20.7% |\n\n*Note: Table adapted from Panels A and E of Table 9 in the source.*\n\n**Table 2: Announcement CARs by Post-Takeover Survival Outcome (All Firms)**\n\n| Outcome | No. | CAR (-1,+1) | CAR (0) |\n| :--- | :-: | :--- | :--- |\n| Survived | 56 | 18.37%*** | 3.18%*** |\n| Failure | 65 | 29.58%*** | 16.28%*** |\n\n*Note: Table adapted from Panel B of Table 10 in the source. `***` denotes significance at the 1% level.*\n\n**Table 3: Logistic Regression Modeling the Probability of Survival**\n\n| Variable | Model 1 Coefficient (Wald Chi-sq) |\n| :--- | :--- |\n| Intercept | 1.512 (5.38)** |\n| Prior Distress | -1.158 (6.70)*** |\n| Log of Public Firm Size | -0.077 (0.36) |\n| Cash to Total Assets | -1.656 (3.20)* |\n\n*Note: Table adapted from Model 1 of Table 11 in the source. `*`, `**`, `***` denote significance at the 10%, 5%, and 1% levels, respectively.*\n\n---\n\n### Question\n\nBased on the provided data, which of the following conclusions about Reverse Takeovers (RTs) are factually supported? Select all that apply.", "Options": {"A": "The market's initial reaction, measured by 3-day CARs, was significantly more positive for firms that ultimately failed than for those that survived.", "B": "The two-year survival rate for firms created via RT is less than 50%.", "C": "The announcement-day CAR (CAR (0)) was statistically insignificant for firms that ultimately survived.", "D": "Among firms that did not survive for two years, bankruptcy was the most frequent outcome."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test the ability to accurately interpret key quantitative findings from multiple tables regarding RT outcomes and market reactions. It assesses core data comprehension. Distractors are designed based on common misreadings: C (Step-Omission Error) tests for careful comparison within a category, and D (Conceptual Error) tests for correct interpretation of statistical significance symbols.", "qid": "103", "question": "### Background\n\n**Research Question.** This case investigates the apparent contradiction between the large positive market reaction to Reverse Takeover (RT) announcements and the poor long-term performance and high failure rate of the surviving firms.\n\n**Setting.** The analysis contrasts the short-term announcement returns (CARs) with the medium-term survival and performance of the newly combined public entities. The sample consists of 121 RTs from 1987-2001. A key puzzle is why the market reacts positively to events that so often lead to failure.\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return at announcement (dimensionless, as %).\n- `Survival_i`: A binary indicator, =1 if firm `i` survives for 2 years, 0 otherwise.\n- `Prior_Distress_i`: Dummy variable; =1 if the public vehicle was in financial distress, 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nThe data present a sharp contrast between the market's initial reaction and subsequent firm survival. A logistic regression is used to model the probability of survival based on pre-merger characteristics.\n\n**Table 1: Post-Reverse Takeover Survival Outcomes (Full Sample, N=121)**\n\n| Outcome | Number of Firms | Percent of Total Sample |\n| :--- | :--- | :--- |\n| **Survival Status** | | |\n| Survived 2 Years | 56 | 46.3% |\n| Did Not Survive 2 Years | 65 | 53.7% |\n| **Details for Non-Surviving Firms** | | |\n| Delisted/moved to Bulletin Board | 33 | 27.3% |\n| Bankruptcy | 24 | 19.8% |\n| Acquired | 25 | 20.7% |\n\n*Note: Table adapted from Panels A and E of Table 9 in the source.*\n\n**Table 2: Announcement CARs by Post-Takeover Survival Outcome (All Firms)**\n\n| Outcome | No. | CAR (-1,+1) | CAR (0) |\n| :--- | :-: | :--- | :--- |\n| Survived | 56 | 18.37%*** | 3.18%*** |\n| Failure | 65 | 29.58%*** | 16.28%*** |\n\n*Note: Table adapted from Panel B of Table 10 in the source. `***` denotes significance at the 1% level.*\n\n**Table 3: Logistic Regression Modeling the Probability of Survival**\n\n| Variable | Model 1 Coefficient (Wald Chi-sq) |\n| :--- | :--- |\n| Intercept | 1.512 (5.38)** |\n| Prior Distress | -1.158 (6.70)*** |\n| Log of Public Firm Size | -0.077 (0.36) |\n| Cash to Total Assets | -1.656 (3.20)* |\n\n*Note: Table adapted from Model 1 of Table 11 in the source. `*`, `**`, `***` denote significance at the 10%, 5%, and 1% levels, respectively.*\n\n---\n\n### Question\n\nBased on the provided data, which of the following conclusions about Reverse Takeovers (RTs) are factually supported? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 101, "Question": "### Background\n\n**Research Question.** How can the theoretical predictions about inflation-driven wealth transfers be empirically tested, and what are the main findings from a cross-sectional analysis of stock returns?\n\n**Setting.** The study uses a multivariate cross-sectional regression to test its hypotheses over three distinct time periods with varying levels of inflation. The key hypotheses are that a firm's stock price is higher the higher its net debtor position (`a_1 > 0`), the lower its proportion of fixed assets (`a_2 < 0`), and the lower its inventory undercosting (`a_3 < 0`).\n\n### Data / Model Specification\n\nThe theoretical model is tested using the following cross-sectional regression of average monthly returns (`y_i`) on firm characteristics:\n  \ny_{i}=\\alpha+a_{0}\\beta_{i}+a_{1}X_{1i}+a_{2}X_{2i}+a_{3}X_{3i}+u_{i}\n \nwhere `y` is the average monthly return, `β` is systematic risk, `X_1` is the net debtor position, `X_2` is fixed assets (plant), and `X_3` is the inventory cost understatement, all scaled by book value.\n\n**Table 1: Cross-Sectional Regression Results (Full Specification)**\n\n| Period | Total Inflation | Const. `α` | Sys. Risk `a_0` | Debt `a_1` | Plant `a_2` | Inv. `a_3` | R² |\n|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| 1964-68 | 17% | .0083 (2.75) | .0068 (3.18) | .0006 (0.18) | -.0143 (-3.86) | -.0099 (-1.57) | 0.73 |\n| 1959-63 | 5% | .0087 (2.00) | -.0043 (-1.18) | -.0027 (-0.66) | -.0047 (-2.32) | -.0061 (-1.71) | 0.28 |\n| 1954-58 | 8% | .0180 (2.54) | .0047 (2.54) | .0014 (0.34) | -.0122 (-3.14) | -.0042 (-0.63) | 0.44 |\n*Note: t-values are in parentheses. A one-tailed t-test at 5% significance requires a t-value of approximately ±1.7.* \n\n### Question\n\nBased on the regression results in **Table 1**, select all statements that represent a correct interpretation of the findings.", "Options": {"A": "The systematic risk coefficient (`a_0`) is positive and statistically significant in all three periods, consistent with standard asset pricing theory.", "B": "The pattern of the plant coefficient (`a_2`) across the three periods is consistent with the theory that the tax-shield loss from fixed assets is more severe during periods of higher inflation.", "C": "The Debtor-Creditor hypothesis (`a_1 > 0`) is not statistically supported in any of the three periods, as the t-statistics for the debt coefficient are consistently small.", "D": "The inventory tax effect (`a_3 < 0`) is statistically significant at the 5% level in the high-inflation 1964-68 period."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret multivariate regression results in the context of economic hypotheses. It uses an Atomic Decomposition strategy, breaking down the paper's main findings into distinct, verifiable claims about statistical significance and comparative statics. Distractors are designed based on common misinterpretations: confusing significance levels (the paper notes 10% significance for `a_3`, not 5%) and misreading results for a specific sub-period (the risk coefficient `a_0` is insignificant in 1959-63).", "qid": "101", "question": "### Background\n\n**Research Question.** How can the theoretical predictions about inflation-driven wealth transfers be empirically tested, and what are the main findings from a cross-sectional analysis of stock returns?\n\n**Setting.** The study uses a multivariate cross-sectional regression to test its hypotheses over three distinct time periods with varying levels of inflation. The key hypotheses are that a firm's stock price is higher the higher its net debtor position (`a_1 > 0`), the lower its proportion of fixed assets (`a_2 < 0`), and the lower its inventory undercosting (`a_3 < 0`).\n\n### Data / Model Specification\n\nThe theoretical model is tested using the following cross-sectional regression of average monthly returns (`y_i`) on firm characteristics:\n  \ny_{i}=\\alpha+a_{0}\\beta_{i}+a_{1}X_{1i}+a_{2}X_{2i}+a_{3}X_{3i}+u_{i}\n \nwhere `y` is the average monthly return, `β` is systematic risk, `X_1` is the net debtor position, `X_2` is fixed assets (plant), and `X_3` is the inventory cost understatement, all scaled by book value.\n\n**Table 1: Cross-Sectional Regression Results (Full Specification)**\n\n| Period | Total Inflation | Const. `α` | Sys. Risk `a_0` | Debt `a_1` | Plant `a_2` | Inv. `a_3` | R² |\n|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| 1964-68 | 17% | .0083 (2.75) | .0068 (3.18) | .0006 (0.18) | -.0143 (-3.86) | -.0099 (-1.57) | 0.73 |\n| 1959-63 | 5% | .0087 (2.00) | -.0043 (-1.18) | -.0027 (-0.66) | -.0047 (-2.32) | -.0061 (-1.71) | 0.28 |\n| 1954-58 | 8% | .0180 (2.54) | .0047 (2.54) | .0014 (0.34) | -.0122 (-3.14) | -.0042 (-0.63) | 0.44 |\n*Note: t-values are in parentheses. A one-tailed t-test at 5% significance requires a t-value of approximately ±1.7.* \n\n### Question\n\nBased on the regression results in **Table 1**, select all statements that represent a correct interpretation of the findings.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 190, "Question": "### Background\n\nIn a one-period pure exchange economy, aggregate demand for `N` risky assets from two investor types (absolute and benchmark-aware) must equal the fixed supply, `S x_M`. This market clearing condition leads to an equilibrium relationship for expected excess returns (`μ - r**1**`).\n\n### Data / Model Specification\n\nThe optimal portfolios for the two investor types are:\n  \nω_{a}=\\frac{1}{π}Ω^{-1}(μ-R{\\bf1}) \\quad \\text{(Eq. (1))}\n \n  \nω_{b}=Φ+\\frac{1}{π}Ω^{-1}(μ-(A/C){\\bf1}) \\quad \\text{(Eq. (2))}\n \nwhere `ω_a` and `ω_b` are the optimal portfolios, `Φ` is the benchmark portfolio, `R` is the risk-free rate, and `A/C` is the expected return on the global minimum variance portfolio.\n\nThe market clearing condition `W_a ω_a + W_b ω_b = S x_M` can be rearranged into:\n  \nμ-r{\\bf1}= H Ω (S x_M - W_b Φ) \\quad \\text{(Eq. (3))}\n \nwhere `H = π / (W_a + W_b)` is aggregate risk aversion and `r = (W_a R + W_b A/C) / (W_a + W_b)` is a composite hurdle rate.\n\n---\n\nBased on the model's structure, which of the following statements are correct descriptions of the equilibrium?\n", "Options": {"A": "If the benchmark portfolio `Φ` were identical to the market portfolio `x_M`, the equilibrium expected excess returns would still be affected by the presence of benchmark investors through the `W_b` term.", "B": "The aggregate risk aversion `H` is independent of the relative wealth of benchmark vs. absolute investors (`W_b / W_a`), depending only on their total wealth (`W_a + W_b`).", "C": "The model collapses to a standard CAPM framework, with the effective zero-beta rate `r` equaling the risk-free rate `R`, if the wealth of benchmark investors is zero (`W_b = 0`).", "D": "In an economy populated only by benchmark investors (`W_a = 0`), the effective zero-beta rate `r` becomes the expected return on the global minimum variance portfolio, `A/C`."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the theoretical model's equilibrium conditions, its limiting cases, and the distinct roles of its parameters. Depth Strategy: Scenario Application. The candidate must evaluate the model under several specific micro-cases (e.g., `W_a=0`, `W_b=0`, `Φ=x_M`) and interpret the definitions of the composite parameters (`r`, `H`). Distractor Logic: This is a difficult item with four correct answers, requiring comprehensive knowledge. A common error would be to assume that if `Φ=x_M`, the benchmark effect vanishes entirely, but Eq. (3) shows the term becomes `(S - W_b)Ωx_M`, so the magnitude of the market risk premium is still affected by `W_b`. Another plausible error is to misunderstand the definition of `H`, thinking it depends on the wealth *distribution* rather than the total.", "qid": "190", "question": "### Background\n\nIn a one-period pure exchange economy, aggregate demand for `N` risky assets from two investor types (absolute and benchmark-aware) must equal the fixed supply, `S x_M`. This market clearing condition leads to an equilibrium relationship for expected excess returns (`μ - r**1**`).\n\n### Data / Model Specification\n\nThe optimal portfolios for the two investor types are:\n  \nω_{a}=\\frac{1}{π}Ω^{-1}(μ-R{\\bf1}) \\quad \\text{(Eq. (1))}\n \n  \nω_{b}=Φ+\\frac{1}{π}Ω^{-1}(μ-(A/C){\\bf1}) \\quad \\text{(Eq. (2))}\n \nwhere `ω_a` and `ω_b` are the optimal portfolios, `Φ` is the benchmark portfolio, `R` is the risk-free rate, and `A/C` is the expected return on the global minimum variance portfolio.\n\nThe market clearing condition `W_a ω_a + W_b ω_b = S x_M` can be rearranged into:\n  \nμ-r{\\bf1}= H Ω (S x_M - W_b Φ) \\quad \\text{(Eq. (3))}\n \nwhere `H = π / (W_a + W_b)` is aggregate risk aversion and `r = (W_a R + W_b A/C) / (W_a + W_b)` is a composite hurdle rate.\n\n---\n\nBased on the model's structure, which of the following statements are correct descriptions of the equilibrium?\n", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 240, "Question": "### Background\n\n**Research Question.** How should regulators use stress test results to set capital requirements?\n\n**Setting and Data-Generating Environment.** A bank regulator is assessing a portfolio whose borrower default probabilities match the distribution from a stress test. The regulator must decide if the change in Expected Loss (EL) is a sufficient basis for setting a capital buffer to cover unexpected losses.\n\n---\n\n### Data / Model Specification\n\nThe distribution of firm-level bankruptcy probabilities before and after the stress test is given below:\n\n**Table 1: Distribution of Bankruptcy Probabilities (`p_it`, as %) Before and After Stress**\n| Scenario                | Mean  | P99   |\n|:------------------------|------:|------:|\n| Before stress           | 1.550 | 11.07 |\n| After stress (GAP only) | 1.732 | 12.18 |\n\n---\n\n### The Question\n\nWhy is the change in a portfolio's Expected Loss (EL) an insufficient metric for setting a capital buffer against unexpected losses (e.g., a 99% Value-at-Risk)? Select all valid reasons.", "Options": {"A": "The stress test shows that the 99th percentile (P99) default rate increases significantly, indicating that tail risk measures like VaR will increase more substantially than the EL.", "B": "Expected Loss already includes a buffer for unexpected losses, making additional capital redundant.", "C": "Capital is designed to cover unexpected losses in the tail of the loss distribution, whereas EL only measures the mean of the distribution.", "D": "Expected Loss cannot be calculated after a stress test because the probabilities are uncertain."}, "Answer": ["A", "C"], "pi_justification": "This question assesses the core conceptual distinction between Expected Loss and Unexpected Loss (or tail risk). It targets a key concept from the original QA's risk management application. Options A and B are both correct statements that highlight why EL is insufficient. Distractor C is factually incorrect (EL is directly calculable from the new mean PD). Distractor D is the Conceptual Opposite of the definition of EL.", "qid": "240", "question": "### Background\n\n**Research Question.** How should regulators use stress test results to set capital requirements?\n\n**Setting and Data-Generating Environment.** A bank regulator is assessing a portfolio whose borrower default probabilities match the distribution from a stress test. The regulator must decide if the change in Expected Loss (EL) is a sufficient basis for setting a capital buffer to cover unexpected losses.\n\n---\n\n### Data / Model Specification\n\nThe distribution of firm-level bankruptcy probabilities before and after the stress test is given below:\n\n**Table 1: Distribution of Bankruptcy Probabilities (`p_it`, as %) Before and After Stress**\n| Scenario                | Mean  | P99   |\n|:------------------------|------:|------:|\n| Before stress           | 1.550 | 11.07 |\n| After stress (GAP only) | 1.732 | 12.18 |\n\n---\n\n### The Question\n\nWhy is the change in a portfolio's Expected Loss (EL) an insufficient metric for setting a capital buffer against unexpected losses (e.g., a 99% Value-at-Risk)? Select all valid reasons.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 417, "Question": "### Background\n\n**Research Question.** This case examines the empirical validity of the Basel II regulatory framework for Small and Medium Enterprise (SME) credit risk, focusing on how misspecified asset correlation assumptions lead to a misallocation of bank capital.\n\n**Setting.** The analysis contrasts the asset correlation (AC) patterns prescribed by the Basel II rules with those empirically estimated from a large sample of Italian SMEs.\n\n**Variables & Parameters.**\n- `PD`: Probability of Default.\n- `S`: Annual sales of the SME, in millions of euros.\n\n---\n\n### Data / Model Specification\n\nThe Basel II framework for SMEs assumes that asset correlation (AC) is a decreasing function of `PD` and an increasing function of firm size `S`.\n\n**Table 1. Empirical Asset Correlation by Size and Risk Classes (1994–2008)**\n\n| Risk Classes | Total Sample | <1 | 1-5 | 5-7.5 | 7.5-10 | 10-25 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **Total Sample** | | **0.0647** | **0.0539** | **0.0686** | **0.1054** | **0.1172** |\n| A | 0.0410 | 0.0588 | 0.0521 | 0.0860 | 0.0910 | 0.0501 |\n| B | 0.0470 | 0.0805 | 0.0457 | 0.0591 | 0.0713 | 0.0555 |\n| C | 0.0518 | 0.0931 | 0.0493 | 0.0678 | 0.0690 | 0.0471 |\n| D | 0.0592 | 0.0835 | 0.0606 | 0.0638 | 0.0719 | 0.0797 |\n| E | 0.0549 | 0.0698 | 0.0773 | 0.1339 | 0.2019 | 0.2440 |\n| F | 0.0664 | 0.0766 | 0.2995 | 0.5167 | 0.5737 | 0.7148 |\n*Source: Adapted from Table 6 of the paper. Size classes are in millions of euros.* \n\n---\n\n### The Question\n\nAccording to the empirical results presented in **Table 1**, which of the following findings about asset correlation (AC) for Italian SMEs contradict the standard Basel II assumptions? (Select all that apply)", "Options": {"A": "The AC for the smallest firms (<1 million euros) is the highest among all size classes in the 'Total Sample'.", "B": "The AC for the safest firms (Risk Class A) is consistently negative across all size classes.", "C": "The relationship between AC and firm size is J-shaped, not monotonically positive.", "D": "The relationship between AC and credit risk (i.e., moving from risk class A to F) is positive, not negative."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to interpret tabular data and contrast it with a stated theoretical framework (Basel II assumptions). It is a Multiple Choice question testing the two core empirical findings of the paper. Correct options A and B capture the central contradictions identified in the original QA problem. Distractors are based on plausible misinterpretations of the table (C, a 'Total Sample' row misreading) or simple factual errors (D).", "qid": "417", "question": "### Background\n\n**Research Question.** This case examines the empirical validity of the Basel II regulatory framework for Small and Medium Enterprise (SME) credit risk, focusing on how misspecified asset correlation assumptions lead to a misallocation of bank capital.\n\n**Setting.** The analysis contrasts the asset correlation (AC) patterns prescribed by the Basel II rules with those empirically estimated from a large sample of Italian SMEs.\n\n**Variables & Parameters.**\n- `PD`: Probability of Default.\n- `S`: Annual sales of the SME, in millions of euros.\n\n---\n\n### Data / Model Specification\n\nThe Basel II framework for SMEs assumes that asset correlation (AC) is a decreasing function of `PD` and an increasing function of firm size `S`.\n\n**Table 1. Empirical Asset Correlation by Size and Risk Classes (1994–2008)**\n\n| Risk Classes | Total Sample | <1 | 1-5 | 5-7.5 | 7.5-10 | 10-25 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **Total Sample** | | **0.0647** | **0.0539** | **0.0686** | **0.1054** | **0.1172** |\n| A | 0.0410 | 0.0588 | 0.0521 | 0.0860 | 0.0910 | 0.0501 |\n| B | 0.0470 | 0.0805 | 0.0457 | 0.0591 | 0.0713 | 0.0555 |\n| C | 0.0518 | 0.0931 | 0.0493 | 0.0678 | 0.0690 | 0.0471 |\n| D | 0.0592 | 0.0835 | 0.0606 | 0.0638 | 0.0719 | 0.0797 |\n| E | 0.0549 | 0.0698 | 0.0773 | 0.1339 | 0.2019 | 0.2440 |\n| F | 0.0664 | 0.0766 | 0.2995 | 0.5167 | 0.5737 | 0.7148 |\n*Source: Adapted from Table 6 of the paper. Size classes are in millions of euros.* \n\n---\n\n### The Question\n\nAccording to the empirical results presented in **Table 1**, which of the following findings about asset correlation (AC) for Italian SMEs contradict the standard Basel II assumptions? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 395, "Question": "### Background\n\nThe analysis of Thiele's PDE system for a surplus-linked insurance contract is facilitated by converting it from a terminal-value problem into an initial-value problem. This requires a change of variables for both time (`t`) and the surplus value (`x`).\n\n### Data / Model Specification\n\nThe original Thiele's PDE for a single state `j` includes the following terms related to the evolution of the surplus:\n  \n\\partial_{t}V^{j} + \\frac{1}{2}\\pi^{2}\\sigma^{2}x^{2}\\partial_{x}^{2}V^j + \\left(r x+c^{j}-\\delta^{j}\\right)\\partial_{x}V^j\n \nThis is transformed using the change of variables `\\tau = T-t` and `y = \\log x`. The transformed system is an initial-value problem:\n  \n\\partial_{\\tau}{\\bf V} = \\left({\\mathcal{A}}(\\tau)+{\\bf T}-r\\right){\\bf V}+e^{r\\tau}\\beta, \\quad {\\bf V}(0)=0\n \nThe `j`-th diagonal element of the operator `\\mathcal{A}` is given by:\n  \n\\mathcal{A}^{j}=\\frac{1}{2}\\pi^{2}\\sigma^{2}\\partial_{y}^{2}+\\left(r+\\left(c^{j}-\\delta^{j}\\right)e^{-y}-\\frac{1}{2}\\pi^{2}\\sigma^{2}\\right)\\partial_{y} \\quad \\text{(Eq. 1)}\n \n\n---\n\nBased on the provided transformation, which of the following statements about the operators and their financial interpretation are correct?\n", "Options": {"A": "The off-diagonal operator `\\mathbf{T}` governs the evolution of the reserve *between* different insurance states, capturing insurance risks like mortality and morbidity.", "B": "The diagonal operator `\\mathcal{A}` governs the evolution of the reserve *within* a given insurance state, capturing the financial risk from the surplus's diffusion and drift.", "C": "The transformation `y = \\log x` converts the `x^2 \\partial_x^2` term into a term proportional to `\\partial_y^2 - \\partial_y`, which is a key step in deriving Eq. (1).", "D": "If contributions and dividends upon transition are zero (`c^{jk} - \\delta^{jk} \\equiv 0`), the operators `\\mathcal{A}` and `\\mathbf{T}` commute because `\\mathbf{T}` becomes independent of the spatial variable `y`."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the student's understanding of the core mathematical transformation in the paper and their ability to connect the structure of the resulting operators (`\\mathcal{A}`, `\\mathbf{T}`) to their distinct financial roles (financial vs. insurance risk). It also tests a key special case (commutativity). Strategy: This is a direct application of the paper's central model setup. All four options are correct statements derived from the original QA problem, requiring a comprehensive understanding of the derivation and interpretation. The item is designed to have multiple correct answers to ensure a thorough check of knowledge.", "qid": "395", "question": "### Background\n\nThe analysis of Thiele's PDE system for a surplus-linked insurance contract is facilitated by converting it from a terminal-value problem into an initial-value problem. This requires a change of variables for both time (`t`) and the surplus value (`x`).\n\n### Data / Model Specification\n\nThe original Thiele's PDE for a single state `j` includes the following terms related to the evolution of the surplus:\n  \n\\partial_{t}V^{j} + \\frac{1}{2}\\pi^{2}\\sigma^{2}x^{2}\\partial_{x}^{2}V^j + \\left(r x+c^{j}-\\delta^{j}\\right)\\partial_{x}V^j\n \nThis is transformed using the change of variables `\\tau = T-t` and `y = \\log x`. The transformed system is an initial-value problem:\n  \n\\partial_{\\tau}{\\bf V} = \\left({\\mathcal{A}}(\\tau)+{\\bf T}-r\\right){\\bf V}+e^{r\\tau}\\beta, \\quad {\\bf V}(0)=0\n \nThe `j`-th diagonal element of the operator `\\mathcal{A}` is given by:\n  \n\\mathcal{A}^{j}=\\frac{1}{2}\\pi^{2}\\sigma^{2}\\partial_{y}^{2}+\\left(r+\\left(c^{j}-\\delta^{j}\\right)e^{-y}-\\frac{1}{2}\\pi^{2}\\sigma^{2}\\right)\\partial_{y} \\quad \\text{(Eq. 1)}\n \n\n---\n\nBased on the provided transformation, which of the following statements about the operators and their financial interpretation are correct?\n", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 184, "Question": "### Background\n\n**Research Question.** Do institutional investors exhibit return-chasing behavior in specific equity styles, and do their subsequent fund flows influence the performance of those styles?\n\n**Setting and Data.** The study uses a two-equation system to analyze the relationship between investor fund flows and style portfolio returns, using monthly data from 1979-2004. The `INDE` portfolio is an equal-weighted portfolio of Large Growth and Small Value styles.\n\n---\n\n### Data / Model Specification\n\n1.  A regression to test for return-chasing, where current fund flows (`DIFF_t`, proxying net institutional flows) are regressed on past style returns:\n      \n    \\mathrm{DIFF}_{t} = b_0 + b_{1}R_{\\mathrm{FFLG},t-1} + b_{2}R_{\\mathrm{FFLV},t-1} + ... + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n     \n\n2.  An augmented five-factor model to test the performance impact of flows, where portfolio returns (`R_pt`) are regressed on standard factors plus the contemporaneous flow variable:\n      \n    R_{pt} = a_{i0} + ... + b_{i5}\\mathrm{DIFF}_t + \\upsilon_t \\quad \\text{(Eq. (2))}\n     \n\n**Table 1: Investor Class Test Results**\n\n| Regression | Coefficient | Estimate | t-statistic |\n| :--- | :--- | :--- | :--- |\n| **Panel A: Fund Flow Allocation (Eq. 1)** | | |\n| | `b_1` on `R_{FFLG,t-1}` | 0.216 | 2.453 |\n| | `b_2` on `R_{FFLV,t-1}` | -0.292 | -3.106 |\n| **Panel B: Performance Impact (Eq. 2)** | | |\n| For INDE Portfolio | `b_{i5}` on `DIFF_t` | 0.041 | 3.616 |\n\n---\n\n### Question\n\nThe paper uses this two-equation system to test a behavioral narrative about institutional investors. Based on the results in **Table 1**, select all conclusions that are supported by the evidence.", "Options": {"A": "The evidence from Eq. (1) suggests that retail investors, not institutional investors, tend to chase positive returns in the Large Value style.", "B": "The results from Eq. (1) indicate that institutional investors exhibit return-chasing behavior in the Large Growth style, as shown by the positive and significant coefficient on lagged FFLG returns.", "C": "The results from Eq. (2) show that net institutional fund flows (`DIFF_t`) have a positive and statistically significant contemporaneous association with the returns of the `INDE` portfolio, even after controlling for standard risk factors.", "D": "The significant coefficient on `DIFF_t` in Eq. (2) definitively proves that institutional fund flows are the primary cause of momentum in the `INDE` portfolio."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item tests the ability to interpret and synthesize the results of a two-stage regression analysis to build a coherent behavioral story. It uses an Atomic Decomposition strategy. Options A and B are direct interpretations of the two key regression results. Option C requires a slightly deeper inference: since `DIFF` is Institutional-Retail flows, a negative coefficient on past returns means positive returns are associated with lower net institutional flows, implying retail flows are dominant. Distractor D is an 'Almost Right' error that overstates causality, a critical distinction in econometrics.", "qid": "184", "question": "### Background\n\n**Research Question.** Do institutional investors exhibit return-chasing behavior in specific equity styles, and do their subsequent fund flows influence the performance of those styles?\n\n**Setting and Data.** The study uses a two-equation system to analyze the relationship between investor fund flows and style portfolio returns, using monthly data from 1979-2004. The `INDE` portfolio is an equal-weighted portfolio of Large Growth and Small Value styles.\n\n---\n\n### Data / Model Specification\n\n1.  A regression to test for return-chasing, where current fund flows (`DIFF_t`, proxying net institutional flows) are regressed on past style returns:\n      \n    \\mathrm{DIFF}_{t} = b_0 + b_{1}R_{\\mathrm{FFLG},t-1} + b_{2}R_{\\mathrm{FFLV},t-1} + ... + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n     \n\n2.  An augmented five-factor model to test the performance impact of flows, where portfolio returns (`R_pt`) are regressed on standard factors plus the contemporaneous flow variable:\n      \n    R_{pt} = a_{i0} + ... + b_{i5}\\mathrm{DIFF}_t + \\upsilon_t \\quad \\text{(Eq. (2))}\n     \n\n**Table 1: Investor Class Test Results**\n\n| Regression | Coefficient | Estimate | t-statistic |\n| :--- | :--- | :--- | :--- |\n| **Panel A: Fund Flow Allocation (Eq. 1)** | | |\n| | `b_1` on `R_{FFLG,t-1}` | 0.216 | 2.453 |\n| | `b_2` on `R_{FFLV,t-1}` | -0.292 | -3.106 |\n| **Panel B: Performance Impact (Eq. 2)** | | |\n| For INDE Portfolio | `b_{i5}` on `DIFF_t` | 0.041 | 3.616 |\n\n---\n\n### Question\n\nThe paper uses this two-equation system to test a behavioral narrative about institutional investors. Based on the results in **Table 1**, select all conclusions that are supported by the evidence.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 250, "Question": "### Background\n\nLate in the campaign against the Volvo-Renault merger, the Swedish Small Shareholders' Association (Aktiespararna) introduced a novel legal threat. This tactic was designed to shift the risk-reward calculation for Volvo's Board of Directors, moving the potential consequences from the realm of business judgment to legal and personal liability.\n\n### Data / Model Specification\n\n1.  **The Legal Argument:** The Aktiespararna claimed the merger violated Volvo's by-laws, which allegedly required the company to maintain at least a 50% majority ownership of its core business. The proposed deal would give Volvo only a 35% stake.\n2.  **The Threat:** The association publicly announced it would sue to overturn the merger if the shareholder vote passed.\n3.  **The Consequence:** A successful lawsuit would be \"disastrous for the board,\" potentially exposing them to personal financial liability for knowingly violating the company's by-laws, in addition to the immense corporate cost of unwinding a consummated merger.\n\n---\n\nBased on this information, select all of the following statements that are valid assessments of the Aktiespararna's legal strategy.\n", "Options": {"A": "The strategy's primary goal was to increase the potential upside for shareholders by securing a better merger price from Renault.", "B": "The threat fundamentally altered the board's decision calculus by introducing a low-probability but catastrophic-cost event (losing the lawsuit), which could make the expected value of proceeding with the vote negative.", "C": "The legal argument shifted the debate from a subjective question of financial merit (which is debatable) to an objective question of legal compliance with the corporate charter.", "D": "The legal threat was powerful because it sought to pierce the protections of the business judgment rule, which typically shields directors from liability for decisions made in good faith."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of corporate governance and shareholder activism, specifically the strategic use of litigation threats to influence board decisions.\nDepth Strategy: Scenario Application. The candidate must apply principles of corporate law (business judgment rule, fiduciary duty) and decision theory (expected value under uncertainty) to the specific scenario described.\nDistractor Logic:\n- A: Correct. A core concept in corporate law is that the business judgment rule does not protect directors who act illegally or violate the corporate charter. The lawsuit was designed to frame the board's action this way, exposing them to personal liability.\n- B: This is a Conceptual Distractor (True but Irrelevant/Incorrect Goal). The stated goal of the Aktiespararna was to kill the deal entirely, not to negotiate a better price. Their actions were aimed at making the deal too risky to proceed with at all.\n- C: Correct. This accurately describes the risk from a decision theory perspective. Even if the board felt they would win the shareholder vote and likely win the lawsuit, the sheer magnitude of the cost of losing (`C_L`) could make the risk unacceptable, driving the expected value of proceeding below the expected value of withdrawing.\n- D: Correct. This captures the strategic brilliance of the move. Financial arguments are matters of opinion. A legal argument about by-laws is a matter of fact and law, making it a much more potent and less ambiguous threat for a risk-averse board.", "qid": "250", "question": "### Background\n\nLate in the campaign against the Volvo-Renault merger, the Swedish Small Shareholders' Association (Aktiespararna) introduced a novel legal threat. This tactic was designed to shift the risk-reward calculation for Volvo's Board of Directors, moving the potential consequences from the realm of business judgment to legal and personal liability.\n\n### Data / Model Specification\n\n1.  **The Legal Argument:** The Aktiespararna claimed the merger violated Volvo's by-laws, which allegedly required the company to maintain at least a 50% majority ownership of its core business. The proposed deal would give Volvo only a 35% stake.\n2.  **The Threat:** The association publicly announced it would sue to overturn the merger if the shareholder vote passed.\n3.  **The Consequence:** A successful lawsuit would be \"disastrous for the board,\" potentially exposing them to personal financial liability for knowingly violating the company's by-laws, in addition to the immense corporate cost of unwinding a consummated merger.\n\n---\n\nBased on this information, select all of the following statements that are valid assessments of the Aktiespararna's legal strategy.\n", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 368, "Question": "### Background\n\nThis case examines the conceptual foundations of the Z-score, a widely used measure of bank solvency. The Z-score is defined as a measure of the number of standard deviations that a bank's asset returns must fall to cause insolvency. It is calculated as:\n\n  \nZ \\cong \\frac{ROAA + K}{\\sigma}\n \n\nwhere `ROAA` is the return on average assets, `K` is the equity-to-assets ratio, and `σ` is the standard deviation of `ROAA`. The Z-score can be additively decomposed into two components, `Z = Z_1 + Z_2`.\n\n### Question\n\nBased on the definition and standard interpretation of the Z-score, which of the following statements are correct? Select all that apply.", "Options": {"A": "Using a short rolling window to calculate `σ` can create an artificially high Z-score if a bank has a period of unusually stable returns, as `σ` would be close to zero.", "B": "The component `Z_1 = ROAA / σ` can be interpreted as a 'performance buffer' that measures how many standard deviations of losses can be absorbed by current earnings.", "C": "The component `Z_2 = K / σ` measures the bank's risk-adjusted profitability.", "D": "The two main components of the Z-score are `ROAA` and `K`, representing the two primary sources of solvency."}, "Answer": ["A", "B"], "pi_justification": "This item tests foundational understanding of the Z-score, the paper's key dependent variable. It assesses knowledge of the components' interpretation, a key methodological nuance in its calculation, and the precise formula. Distractors are based on common conceptual errors: C swaps the interpretations of the two components, and D confuses the raw inputs (`ROAA`, `K`) with the risk-adjusted component ratios (`ROAA/σ`, `K/σ`).", "qid": "368", "question": "### Background\n\nThis case examines the conceptual foundations of the Z-score, a widely used measure of bank solvency. The Z-score is defined as a measure of the number of standard deviations that a bank's asset returns must fall to cause insolvency. It is calculated as:\n\n  \nZ \\cong \\frac{ROAA + K}{\\sigma}\n \n\nwhere `ROAA` is the return on average assets, `K` is the equity-to-assets ratio, and `σ` is the standard deviation of `ROAA`. The Z-score can be additively decomposed into two components, `Z = Z_1 + Z_2`.\n\n### Question\n\nBased on the definition and standard interpretation of the Z-score, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 176, "Question": "### Background\n\nThe demand of a risk-averse informed trader is finite and depends on their perceived profit opportunity, their aversion to risk, and their uncertainty about the asset's value. This contrasts with a risk-neutral trader, whose demand would be infinite if they perceive any positive expected profit.\n\n**Variables & Parameters.**\n- `q(a)`: The informed trader's quantity demanded at price `a`.\n- `a`: The dealer's ask price.\n- `v`: The trader's conditional valuation of the asset, `v = E[x|G]`.\n- `\\gamma`: The trader's coefficient of absolute risk aversion (`\\gamma > 0`).\n- `var(x|G)`: The conditional variance of the asset's value, given the signal `G`.\n\n---\n\n### Data / Model Specification\n\nWhen the asset value `x` is normally distributed, a risk-averse informed trader's demand function is:\n  \nq(a)=\\operatorname*{max}\\bigl[0,\\bigl(\\gamma\\mathrm{var}(x|G)\\bigr)^{-1}(v-a)\\bigr] \\quad \\text{(Eq. (1))}\n \nWhen the asset value `x` is lognormally distributed, the demand function is:\n  \nq(a,v)=\\frac{1}{\\theta}\\operatorname*{max}\\bigg[0,\\frac{1}{v}-\\frac{a}{v^{2}}\\bigg] \\quad \\text{(Eq. (2))}\n \nwhere `\\theta` is a parameter incorporating risk aversion and conditional variance. In the lognormal case, the conditional variance `var(x|G)` is proportional to `v^2`.\n\n---\n\nWhich of the following statements are valid conclusions derived from these demand functions?", "Options": {"A": "In the normal distribution model (Eq. 1), the trader's demand is finite because the marginal benefit of trading, `(v-a)`, diminishes as the quantity `q` increases.", "B": "The economic reason for non-monotonic demand in the lognormal model (Eq. 2) is that as `v` increases, the conditional variance of the asset also increases, making the trade riskier and eventually outweighing the benefit of a higher expected return.", "C": "In the normal distribution model (Eq. 1), a trader with higher risk aversion (larger `\\gamma`) will demand a smaller quantity for the same perceived mispricing `(v-a)`.", "D": "In the lognormal distribution model (Eq. 2), demand is non-monotonic in the trader's valuation `v`; for a fixed price `a`, demand is maximized when the valuation is `v = 2a`."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret and analyze the mathematical forms of a risk-averse trader's demand under different distributional assumptions. It requires understanding the role of each parameter and the economic intuition behind the functional forms. \nDepth Strategy: Scenario Application. The item asks the user to apply their understanding of the models to interpret parameters (Option A), perform a novel optimization (Option B), and explain the underlying economic trade-offs (Option D). \nDistractor Logic: Option C is an Almost Right distractor. The trader's demand is indeed finite, but the reason provided is incorrect. The marginal benefit `(v-a)` is constant with respect to `q`; it is the marginal *cost* of risk that increases with `q`, which is what ultimately caps the demand. This tests for a precise understanding of the mean-variance trade-off.", "qid": "176", "question": "### Background\n\nThe demand of a risk-averse informed trader is finite and depends on their perceived profit opportunity, their aversion to risk, and their uncertainty about the asset's value. This contrasts with a risk-neutral trader, whose demand would be infinite if they perceive any positive expected profit.\n\n**Variables & Parameters.**\n- `q(a)`: The informed trader's quantity demanded at price `a`.\n- `a`: The dealer's ask price.\n- `v`: The trader's conditional valuation of the asset, `v = E[x|G]`.\n- `\\gamma`: The trader's coefficient of absolute risk aversion (`\\gamma > 0`).\n- `var(x|G)`: The conditional variance of the asset's value, given the signal `G`.\n\n---\n\n### Data / Model Specification\n\nWhen the asset value `x` is normally distributed, a risk-averse informed trader's demand function is:\n  \nq(a)=\\operatorname*{max}\\bigl[0,\\bigl(\\gamma\\mathrm{var}(x|G)\\bigr)^{-1}(v-a)\\bigr] \\quad \\text{(Eq. (1))}\n \nWhen the asset value `x` is lognormally distributed, the demand function is:\n  \nq(a,v)=\\frac{1}{\\theta}\\operatorname*{max}\\bigg[0,\\frac{1}{v}-\\frac{a}{v^{2}}\\bigg] \\quad \\text{(Eq. (2))}\n \nwhere `\\theta` is a parameter incorporating risk aversion and conditional variance. In the lognormal case, the conditional variance `var(x|G)` is proportional to `v^2`.\n\n---\n\nWhich of the following statements are valid conclusions derived from these demand functions?", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 415, "Question": "### Background\n\n**Research Question.** Do firms strategically time their equity issues to occur after public disclosures, and do firm characteristics associated with information asymmetry predict this timing?\n\n**Setting / Data-Generating Environment.** The study examines the timing of seasoned equity issues relative to quarterly earnings releases. Two competing hypotheses are considered:\n- **Hypothesis 1 (Adverse Selection Timing)**: Firms issue after releases to minimize information asymmetry.\n- **Alternative Hypothesis (Operating Cycle)**: Issue clustering is driven by institutional factors (e.g., board meetings) that align with the quarterly reporting cycle, not by information asymmetry.\n\n**Variables & Parameters.**\n- `D_I`: The number of days between the equity issue and the *preceding* earnings release.\n- `ln(size)`: Log of the market value of the firm's equity.\n- `ln(issue size)`: Log of the dollar value of the primary share issue.\n\n### Data / Model Specification\n\nA one-sided Wilcoxon rank-sum test confirms that issues cluster after earnings releases (results not shown). To distinguish between the competing hypotheses, the following regression is estimated:\n  \nD_I = \\beta_0 + \\beta_1 \\ln(\\text{size}) + \\beta_2 \\ln(\\text{issue size}) + \\epsilon \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Regression of Issue Timing on Firm and Issue Size**\n\n| Dependent Variable | Constant | ln(size) | ln(issue size) | N |\n|:-------------------|:---------|:---------|:---------------|:--|\n| `D_I`              | 40.263*  | 0.321*   | -0.397*        | 563|\n|                    | (1.273)  | (0.060)  | (0.030)        |   |\n\n*White heteroskedasticity-robust standard errors in parentheses. *Significant at 1% level.*\n\n### Question\n\nThe study aims to distinguish the 'Adverse Selection Timing' hypothesis from the 'Operating Cycle' hypothesis. Based on the provided results, select all statements that accurately reflect the evidence and the logic of the identification strategy.", "Options": {"A": "The negative coefficient on `ln(issue size)` supports the adverse selection hypothesis by suggesting that for larger issues, where potential 'lemons' costs are higher, firms have a stronger incentive to issue quickly after a release.", "B": "The 'Operating Cycle' hypothesis predicts that the coefficient on `ln(size)` should be positive and the coefficient on `ln(issue size)` should be negative.", "C": "The positive coefficient on `ln(size)` supports the adverse selection hypothesis by suggesting that larger firms, which face less information asymmetry, have less incentive to issue immediately after an earnings release.", "D": "The fact that issues cluster after earnings releases is sufficient on its own to reject the 'Operating Cycle' hypothesis."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the identification strategy used to distinguish the adverse selection hypothesis from the operating cycle alternative. It uses a 'premise packaging' strategy. Options A and B correctly state the nuanced predictions of the adverse selection model. Distractor C tests a common logical flaw, confusing correlation with causation (clustering is consistent with both hypotheses). Distractor D presents a conceptual opposite for the alternative hypothesis's prediction (it predicts zero coefficients).", "qid": "415", "question": "### Background\n\n**Research Question.** Do firms strategically time their equity issues to occur after public disclosures, and do firm characteristics associated with information asymmetry predict this timing?\n\n**Setting / Data-Generating Environment.** The study examines the timing of seasoned equity issues relative to quarterly earnings releases. Two competing hypotheses are considered:\n- **Hypothesis 1 (Adverse Selection Timing)**: Firms issue after releases to minimize information asymmetry.\n- **Alternative Hypothesis (Operating Cycle)**: Issue clustering is driven by institutional factors (e.g., board meetings) that align with the quarterly reporting cycle, not by information asymmetry.\n\n**Variables & Parameters.**\n- `D_I`: The number of days between the equity issue and the *preceding* earnings release.\n- `ln(size)`: Log of the market value of the firm's equity.\n- `ln(issue size)`: Log of the dollar value of the primary share issue.\n\n### Data / Model Specification\n\nA one-sided Wilcoxon rank-sum test confirms that issues cluster after earnings releases (results not shown). To distinguish between the competing hypotheses, the following regression is estimated:\n  \nD_I = \\beta_0 + \\beta_1 \\ln(\\text{size}) + \\beta_2 \\ln(\\text{issue size}) + \\epsilon \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Regression of Issue Timing on Firm and Issue Size**\n\n| Dependent Variable | Constant | ln(size) | ln(issue size) | N |\n|:-------------------|:---------|:---------|:---------------|:--|\n| `D_I`              | 40.263*  | 0.321*   | -0.397*        | 563|\n|                    | (1.273)  | (0.060)  | (0.030)        |   |\n\n*White heteroskedasticity-robust standard errors in parentheses. *Significant at 1% level.*\n\n### Question\n\nThe study aims to distinguish the 'Adverse Selection Timing' hypothesis from the 'Operating Cycle' hypothesis. Based on the provided results, select all statements that accurately reflect the evidence and the logic of the identification strategy.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 275, "Question": "### Background\n\n**Research Question.** What institutional factors explain the cross-country variation in the value relevance of bank financial statements, and can these empirical relationships be interpreted causally?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage regression approach using a sample of 813 banks from 38 countries over the period 1993-2004. In the first stage, bank-specific value relevance is estimated. In the second stage, a single cross-sectional regression is run to explain the variation in these estimates.\n\n### Data / Model Specification\n\nIn the second stage, the Extent of Value Relevance (`EVR`), proxied by the coefficient on book value from the first-stage regressions, is modeled as a function of country-level and bank-specific factors:\n\n  \n\\mathrm{EVR}_{i} = a_{0} + b_{1} \\text{Transparency}_i + b_{2} \\text{Legal Environment}_i + b_{3} \\text{Accounting Cluster}_i + ... + \\varepsilon_{i}\n \n\nKey independent variables are defined as:\n*   `EVR`: The bank-specific coefficient on book value from a first-stage regression of market value on book value.\n*   `Legal Environment`: An indicator variable equal to 1 for common law systems and 0 for code law systems.\n*   `Transparency`: A country-level index (CIFAR) measuring the extent of mandatory financial disclosure.\n*   `Accounting Cluster`: An indicator variable equal to 1 for the British-American accounting cluster and 0 otherwise.\n\n**Table 1: Second-Stage Regression Results (Dependent Variable = EVR)**\n\n| Correlated Factor     | Coefficient | P-value |\n| :-------------------- | :---------- | :------ |\n| Transparency          | 0.045       | 0.001   |\n| Corporate environment | 0.042       | 0.011   |\n| Legal environment     | 0.041       | 0.037   |\n| Accounting cluster    | 0.114       | 0.001   |\n\n*Source: Adapted from Table 5, Panel A of the paper.*\n\n**Table 2: Correlation Matrix Snippet**\n\n|                     | Legal Env. | Transparency | Acct. Cluster |\n| :------------------ | :--------- | :----------- | :------------ |\n| **Legal Env.**      | 1.00       |              |               |\n| **Transparency**    | 0.40*      | 1.00         |               |\n| **Acct. Cluster**   | 0.48*      | 0.57*        | 1.00          |\n\n*Source: Adapted from Table 4 of the paper. * denotes significant correlation.*\n\n### Question\n\nA researcher argues that the significant coefficient on `Legal Environment` in Table 1 cannot be interpreted as a clean causal effect. Based on the provided tables, select all statements that correctly identify an econometric problem supporting this argument.", "Options": {"A": "If `Transparency` were omitted from the regression in Table 1, the estimated coefficient on `Legal Environment` would likely be biased upwards, as `Legal Environment` would incorrectly capture some of the positive effect of the correlated `Transparency` variable.", "B": "The strong positive correlations between `Legal Environment`, `Transparency`, and `Accounting Cluster` (as shown in Table 2) make it difficult to statistically isolate the unique effect of the legal system from the effects of correlated institutional features.", "C": "The p-value for `Legal Environment` (0.037) is not low enough to rule out that the finding is due to random chance, which prevents a causal interpretation.", "D": "If `Transparency` were omitted from the regression, the coefficient on `Legal Environment` would be biased towards zero due to the confounding effect."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to critique causal claims using evidence of multicollinearity and the theory of omitted variable bias. It uses a Premise/Assumption Packaging strategy, asking students to select valid econometric arguments. Correct options require identifying multicollinearity from a correlation matrix (A) and correctly reasoning about the direction of omitted variable bias (B). Distractor C presents the opposite direction for the bias (a Sign Error), and D confuses statistical significance with the separate issue of causal identification (a Conceptual Error).", "qid": "275", "question": "### Background\n\n**Research Question.** What institutional factors explain the cross-country variation in the value relevance of bank financial statements, and can these empirical relationships be interpreted causally?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage regression approach using a sample of 813 banks from 38 countries over the period 1993-2004. In the first stage, bank-specific value relevance is estimated. In the second stage, a single cross-sectional regression is run to explain the variation in these estimates.\n\n### Data / Model Specification\n\nIn the second stage, the Extent of Value Relevance (`EVR`), proxied by the coefficient on book value from the first-stage regressions, is modeled as a function of country-level and bank-specific factors:\n\n  \n\\mathrm{EVR}_{i} = a_{0} + b_{1} \\text{Transparency}_i + b_{2} \\text{Legal Environment}_i + b_{3} \\text{Accounting Cluster}_i + ... + \\varepsilon_{i}\n \n\nKey independent variables are defined as:\n*   `EVR`: The bank-specific coefficient on book value from a first-stage regression of market value on book value.\n*   `Legal Environment`: An indicator variable equal to 1 for common law systems and 0 for code law systems.\n*   `Transparency`: A country-level index (CIFAR) measuring the extent of mandatory financial disclosure.\n*   `Accounting Cluster`: An indicator variable equal to 1 for the British-American accounting cluster and 0 otherwise.\n\n**Table 1: Second-Stage Regression Results (Dependent Variable = EVR)**\n\n| Correlated Factor     | Coefficient | P-value |\n| :-------------------- | :---------- | :------ |\n| Transparency          | 0.045       | 0.001   |\n| Corporate environment | 0.042       | 0.011   |\n| Legal environment     | 0.041       | 0.037   |\n| Accounting cluster    | 0.114       | 0.001   |\n\n*Source: Adapted from Table 5, Panel A of the paper.*\n\n**Table 2: Correlation Matrix Snippet**\n\n|                     | Legal Env. | Transparency | Acct. Cluster |\n| :------------------ | :--------- | :----------- | :------------ |\n| **Legal Env.**      | 1.00       |              |               |\n| **Transparency**    | 0.40*      | 1.00         |               |\n| **Acct. Cluster**   | 0.48*      | 0.57*        | 1.00          |\n\n*Source: Adapted from Table 4 of the paper. * denotes significant correlation.*\n\n### Question\n\nA researcher argues that the significant coefficient on `Legal Environment` in Table 1 cannot be interpreted as a clean causal effect. Based on the provided tables, select all statements that correctly identify an econometric problem supporting this argument.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 161, "Question": "### Background\n\nIn the 1950s, Canadian stock exchanges operated under a significantly different market structure compared to their U.S. counterparts. A primary structural difference was the absence of a \"specialist\" system on Canadian exchanges like the Toronto Stock Exchange (TSE). In the U.S., specialists on the New York Stock Exchange (NYSE) had an affirmative obligation to maintain a fair and orderly market by trading for their own account to provide liquidity and dampen volatility. In contrast, the TSE utilized a free auction system where any market-making was left to the voluntary, proprietary trading activities of member brokerage firms.\n\n### Scenario\n\nA TSE member firm is considering whether to voluntarily provide liquidity (i.e., consistently post buy and sell orders) for two different securities: 'StableCo,' a large, established industrial firm, and 'ProspectorCo,' a speculative mining venture with highly uncertain drilling prospects.\n\nAccording to the economic reasoning presented in the paper, which of the following are valid concerns or likely outcomes influencing the firm's decision? Select all that apply.", "Options": {"A": "For ProspectorCo, the capital required to absorb selling pressure and maintain an orderly market would be exceptionally high due to the stock's inherent volatility, making the specialist role economically unviable.", "B": "For StableCo, the firm has no affirmative obligation to trade against market trends and may choose to withdraw liquidity during a market-wide downturn, potentially worsening price declines.", "C": "For StableCo, the firm could still face a 'free-rider' problem, where it bears the costs and risks of providing liquidity while other firms benefit from the resulting market stability without contributing.", "D": "For ProspectorCo, the firm faces a severe risk of adverse selection, where it would consistently lose money trading against insiders who have private information about drilling results."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the economic implications of the specialist system (or lack thereof) as discussed in the paper. It requires applying the general principles from the text to a specific, comparative scenario.\nStrategy: Scenario Application. The question presents a micro-case (a firm deciding to make markets in two different stocks) and asks the user to apply the paper's reasoning about adverse selection, volatility, and lack of obligation to this specific context.\nDistractor Logic: This item is constructed with four correct options, each representing a distinct economic principle discussed or implied in the original QA answer. An incorrect response would indicate a failure to grasp a key concept: A) misunderstanding adverse selection in speculative stocks; B) not recognizing the public good/free-rider nature of liquidity; C) failing to connect volatility to capital risk for market makers; D) confusing the voluntary nature of TSE trading with the obligatory nature of the NYSE specialist system.", "qid": "161", "question": "### Background\n\nIn the 1950s, Canadian stock exchanges operated under a significantly different market structure compared to their U.S. counterparts. A primary structural difference was the absence of a \"specialist\" system on Canadian exchanges like the Toronto Stock Exchange (TSE). In the U.S., specialists on the New York Stock Exchange (NYSE) had an affirmative obligation to maintain a fair and orderly market by trading for their own account to provide liquidity and dampen volatility. In contrast, the TSE utilized a free auction system where any market-making was left to the voluntary, proprietary trading activities of member brokerage firms.\n\n### Scenario\n\nA TSE member firm is considering whether to voluntarily provide liquidity (i.e., consistently post buy and sell orders) for two different securities: 'StableCo,' a large, established industrial firm, and 'ProspectorCo,' a speculative mining venture with highly uncertain drilling prospects.\n\nAccording to the economic reasoning presented in the paper, which of the following are valid concerns or likely outcomes influencing the firm's decision? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 263, "Question": "### Background\n\n**Research Question.** This study tests whether government-imposed barriers to international investment are effective in segmenting capital markets by analyzing the price behavior of closed-end country funds around announcements of changes in these restrictions.\n\n**Setting / Data-Generating Environment.** The study uses an event-study methodology with weekly data for five country funds (France, Japan, Korea, Mexico, Taiwan) that experienced announced changes in their home country's investment restrictions. The central hypothesis is that an announcement of a liberalization (tightening) of investment restrictions will cause a fund's premium to decrease (increase).\n\n**Variables & Parameters.**\n- `π_jt`: The weekly percentage change in the premium of fund `j` in week `t`.\n- `D_1jt`: Dummy variable for the pre-announcement window. `D_1jt = 1` for a loosening (`-1` for a tightening) if `t` is between 2 and 7 weeks *before* the announcement; 0 otherwise.\n- `D_2jt`: Dummy variable for the event window. `D_2jt = 1` for a loosening (`-1` for a tightening) if `t` is between 1 week *before* and 1 week *after* the announcement; 0 otherwise.\n- `D_3jt`: Dummy variable for the post-announcement window. `D_3jt = 1` for a loosening (`-1` for a tightening) if `t` is between 2 and 7 weeks *after* the announcement; 0 otherwise.\n- `δ_1j, δ_2j, δ_3j`: Regression coefficients measuring the average weekly premium change during the respective windows.\n\n---\n\n### Data / Model Specification\n\nThe core empirical test is the following regression model, estimated separately for each fund `j`:\n\n  \n\\pi_{j t}=\\delta_{0j}+\\delta_{1j}D_{1j t}+\\delta_{2j}D_{2j t}+\\delta_{3j}D_{3j t}+\\epsilon_{j t} \\quad \\text{(Eq. 1)}\n \n\nThe alternative hypothesis is that liberalizations reduce premiums, implying `δ_kj < 0`.\n\n**Table 1: Regression Results for Changes in Fund Premiums**\n\n| Fund | `δ_1j` | `δ_2j` | `δ_3j` | N | Events |\n| :--- | :--- | :--- | :--- | :-: | :---: |\n| France | -1.32 (0.09) | -3.85 (0.03) | 0.41 (0.67) | 132 | 2 |\n| Japan | -0.85 (0.02) | -0.46 (0.45) | 0.19 (0.66) | 292 | 8 |\n| Korea | -2.74 (0.01) | -3.11 (0.02) | 0.06 (0.62) | 208 | 5 |\n| Mexico | -5.41 (0.00) | 0.71 (0.94) | 314 | 6 |\n| Taiwan | -0.42 (0.56) | 0.82 (0.56) | -0.10 (0.39) | 68 | 2 |\n\n*Source: Adapted from Table IV of the source paper. One-sided p-values are in parentheses.*\n\n---\n\n### Question\n\nBased on the regression results in **Table 1**, select all of the following statements that represent a correct interpretation or calculation.", "Options": {"A": "The statistically significant coefficient on `δ_1j` for the Korea Fund suggests that information about the liberalization was likely anticipated by the market before the official announcement.", "B": "The total cumulative decline in the Mexico Fund's premium associated with a liberalization announcement during the three-week event window is 16.23%.", "C": "For the France Fund, the total impact of a liberalization announcement is a premium decline of 3.85% during the event window.", "D": "The results for the Taiwan Fund (`δ_2j` = 0.82) indicate that its investment restrictions were not binding, as the premium change during the event window is not statistically significant."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform calculations and interpret coefficients directly from the regression table. It uses an 'Atomic Decomposition' strategy, breaking down the findings into distinct, verifiable statements. Option A is a 'Computational Judgment' task. Option B tests interpretation of the pre-announcement effect. Distractor C makes an inferential leap (insignificance doesn't prove non-binding, could be anticipated/unimportant event). Distractor D is a 'Step-Omission Error', confusing the average weekly effect with the total cumulative effect.", "qid": "263", "question": "### Background\n\n**Research Question.** This study tests whether government-imposed barriers to international investment are effective in segmenting capital markets by analyzing the price behavior of closed-end country funds around announcements of changes in these restrictions.\n\n**Setting / Data-Generating Environment.** The study uses an event-study methodology with weekly data for five country funds (France, Japan, Korea, Mexico, Taiwan) that experienced announced changes in their home country's investment restrictions. The central hypothesis is that an announcement of a liberalization (tightening) of investment restrictions will cause a fund's premium to decrease (increase).\n\n**Variables & Parameters.**\n- `π_jt`: The weekly percentage change in the premium of fund `j` in week `t`.\n- `D_1jt`: Dummy variable for the pre-announcement window. `D_1jt = 1` for a loosening (`-1` for a tightening) if `t` is between 2 and 7 weeks *before* the announcement; 0 otherwise.\n- `D_2jt`: Dummy variable for the event window. `D_2jt = 1` for a loosening (`-1` for a tightening) if `t` is between 1 week *before* and 1 week *after* the announcement; 0 otherwise.\n- `D_3jt`: Dummy variable for the post-announcement window. `D_3jt = 1` for a loosening (`-1` for a tightening) if `t` is between 2 and 7 weeks *after* the announcement; 0 otherwise.\n- `δ_1j, δ_2j, δ_3j`: Regression coefficients measuring the average weekly premium change during the respective windows.\n\n---\n\n### Data / Model Specification\n\nThe core empirical test is the following regression model, estimated separately for each fund `j`:\n\n  \n\\pi_{j t}=\\delta_{0j}+\\delta_{1j}D_{1j t}+\\delta_{2j}D_{2j t}+\\delta_{3j}D_{3j t}+\\epsilon_{j t} \\quad \\text{(Eq. 1)}\n \n\nThe alternative hypothesis is that liberalizations reduce premiums, implying `δ_kj < 0`.\n\n**Table 1: Regression Results for Changes in Fund Premiums**\n\n| Fund | `δ_1j` | `δ_2j` | `δ_3j` | N | Events |\n| :--- | :--- | :--- | :--- | :-: | :---: |\n| France | -1.32 (0.09) | -3.85 (0.03) | 0.41 (0.67) | 132 | 2 |\n| Japan | -0.85 (0.02) | -0.46 (0.45) | 0.19 (0.66) | 292 | 8 |\n| Korea | -2.74 (0.01) | -3.11 (0.02) | 0.06 (0.62) | 208 | 5 |\n| Mexico | -5.41 (0.00) | 0.71 (0.94) | 314 | 6 |\n| Taiwan | -0.42 (0.56) | 0.82 (0.56) | -0.10 (0.39) | 68 | 2 |\n\n*Source: Adapted from Table IV of the source paper. One-sided p-values are in parentheses.*\n\n---\n\n### Question\n\nBased on the regression results in **Table 1**, select all of the following statements that represent a correct interpretation or calculation.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 392, "Question": "### Background\n\n**Research Question.** How can one estimate the *causal* effect of SEO underpricing on a firm's subsequent stock performance, avoiding biases from unobserved confounding factors (e.g., 'market hype') that might drive both?\n\n**Setting.** A standard OLS regression of future returns on underpricing may be biased. An instrumental variable (IV) approach can potentially solve this problem. A proposed instrument for `Underpricing` is the rank of the lead underwriter (`UNDRANK`).\n\n---\n\n### Question\n\nTo estimate the causal effect of `Underpricing` on future returns, one could use underwriter rank (`UNDRANK`) as an instrumental variable (IV). For an IV strategy to be valid, the instrument must satisfy two key conditions. Select all statements that correctly describe these conditions as they apply to `UNDRANK`.", "Options": {"A": "Orthogonality: `Underpricing` must be statistically independent of `UNDRANK`.", "B": "Exclusion Restriction: `UNDRANK` must be uncorrelated with any unobserved factors that determine future returns, affecting returns *only* through its influence on `Underpricing`.", "C": "Exogeneity: `UNDRANK` must be highly correlated with the future stock returns it is trying to explain.", "D": "Relevance: `UNDRANK` must have a statistically significant causal effect on the level of `Underpricing`."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item converts a difficult, open-ended question about research design into a focused test of the underlying theoretical principles. It uses a **Premise/Assumption Packaging** strategy, asking for the necessary conditions for a valid IV. Correct options (A, B) state the relevance and exclusion restriction conditions accurately. Distractor C is a **Conceptual Error** that describes a violation of the exclusion restriction. Distractor D is a **Conceptual Opposite** of the relevance condition.", "qid": "392", "question": "### Background\n\n**Research Question.** How can one estimate the *causal* effect of SEO underpricing on a firm's subsequent stock performance, avoiding biases from unobserved confounding factors (e.g., 'market hype') that might drive both?\n\n**Setting.** A standard OLS regression of future returns on underpricing may be biased. An instrumental variable (IV) approach can potentially solve this problem. A proposed instrument for `Underpricing` is the rank of the lead underwriter (`UNDRANK`).\n\n---\n\n### Question\n\nTo estimate the causal effect of `Underpricing` on future returns, one could use underwriter rank (`UNDRANK`) as an instrumental variable (IV). For an IV strategy to be valid, the instrument must satisfy two key conditions. Select all statements that correctly describe these conditions as they apply to `UNDRANK`.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 433, "Question": "### Background\n\n**Research Question.** What are the key empirical differences between firms listing on main versus second-tier European stock markets, and what do these differences imply about market structure and the role of financial intermediaries?\n\n**Setting / Data-Generating Environment.** The analysis covers 3,776 Initial Public Offerings (IPOs) from 1995-2010 across four major European economies. The data distinguishes between listings on main markets (e.g., London's Official List) and various second-tier markets (e.g., London's Alternative Investment Market, or AIM).\n\n**Variables & Parameters.**\n\n*   `Age`: The age of the issuing firm at the time of the IPO (years).\n*   `Offer size`: The total proceeds from the IPO, adjusted for inflation to 2010 purchasing power (€ millions).\n*   `Secondary offer`: The ratio of shares sold by existing shareholders to the total shares offered in the IPO (dimensionless, expressed as %).\n*   `VC`: An indicator variable equal to 1 if the firm is venture-capital backed, and 0 otherwise (dimensionless, expressed as % of firms in a given market).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Descriptive Statistics of IPOs in Europe, by Listing Market (Selected Markets)**\n\n| Market | IPOs (No.) | Age (median, yrs) | Offer size (median, €m) | Secondary offer (median, %) | VC (% backed) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **London** | | | | | |\n| Official List | 459 | 6.0 | 70.0 | 11.2 | 65.7 |\n| AIM | 1,666 | 2.0 | 7.7 | 0.0 | 61.9 |\n| **Milan** | | | | | |\n| MTA | 149 | 29.0 | 103.0 | 17.4 | 23.5 |\n\n*Source: Adapted from the paper's Table 1.* The paper posits that underwriters act as certifying agents, producing information to alleviate asymmetries between firm insiders and outsiders.\n\n---\n\n### The Question\n\nBased on the data in Table 1 and the concept of underwriter certification, select all statements that are supported by the evidence.", "Options": {"A": "Due to the large volume of IPOs on AIM, economies of scale likely lead to lower average underwriter fees (as a percentage of proceeds) compared to Milan's MTA.", "B": "Firms listing on London's AIM are typically at an earlier life-cycle stage than those on the Official List, as evidenced by their lower median age, smaller offer sizes, and near-zero median secondary offerings.", "C": "The underwriter's certification role is likely more critical for AIM-listed firms because their characteristics (e.g., younger age) are associated with greater information asymmetry for potential investors.", "D": "The high proportion of VC-backed firms on AIM (61.9%) reduces information asymmetry, making the underwriter's certification role less important than on Milan's MTA (23.5% VC-backed)."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize descriptive statistics with financial theory. It uses an Atomic Decomposition strategy, breaking down the original QA into testable propositions about firm characteristics on second-tier markets and the resulting implications for the underwriter's certification role. Distractors are based on a Conceptual Opposite argument regarding fees (C) and a plausible but incorrect theoretical inference about VC backing (D).", "qid": "433", "question": "### Background\n\n**Research Question.** What are the key empirical differences between firms listing on main versus second-tier European stock markets, and what do these differences imply about market structure and the role of financial intermediaries?\n\n**Setting / Data-Generating Environment.** The analysis covers 3,776 Initial Public Offerings (IPOs) from 1995-2010 across four major European economies. The data distinguishes between listings on main markets (e.g., London's Official List) and various second-tier markets (e.g., London's Alternative Investment Market, or AIM).\n\n**Variables & Parameters.**\n\n*   `Age`: The age of the issuing firm at the time of the IPO (years).\n*   `Offer size`: The total proceeds from the IPO, adjusted for inflation to 2010 purchasing power (€ millions).\n*   `Secondary offer`: The ratio of shares sold by existing shareholders to the total shares offered in the IPO (dimensionless, expressed as %).\n*   `VC`: An indicator variable equal to 1 if the firm is venture-capital backed, and 0 otherwise (dimensionless, expressed as % of firms in a given market).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Descriptive Statistics of IPOs in Europe, by Listing Market (Selected Markets)**\n\n| Market | IPOs (No.) | Age (median, yrs) | Offer size (median, €m) | Secondary offer (median, %) | VC (% backed) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **London** | | | | | |\n| Official List | 459 | 6.0 | 70.0 | 11.2 | 65.7 |\n| AIM | 1,666 | 2.0 | 7.7 | 0.0 | 61.9 |\n| **Milan** | | | | | |\n| MTA | 149 | 29.0 | 103.0 | 17.4 | 23.5 |\n\n*Source: Adapted from the paper's Table 1.* The paper posits that underwriters act as certifying agents, producing information to alleviate asymmetries between firm insiders and outsiders.\n\n---\n\n### The Question\n\nBased on the data in Table 1 and the concept of underwriter certification, select all statements that are supported by the evidence.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 421, "Question": "### Background\n\nA portfolio manager, observing the paper's findings, considers a dynamic hedging strategy: they will significantly increase their portfolio's allocation to T-bonds whenever lagged stock volatility crosses into an 'Above Median' state, anticipating that T-bonds will provide a strong hedge during the more frequent stock market crashes that occur in such regimes.\n\n**Table 1. Average Daily T-bond Returns During Extreme Stock Declines (<5th percentile)**\n\n| Lagged Volatility was: | # of Obs. | Avg Ret 30-yr T-bond (%) |\n| :--- | :--- | :--- |\n| Below Median | 31 | 0.59 |\n| Above Median | 167 | 0.61 |\n\n### Question\n\nWhich of the following represent significant, real-world risks of implementing this dynamic hedging strategy that are not directly apparent from the data in Table 1? Select all that apply.", "Options": {"A": "Execution Risk: Attempting to buy T-bonds when the FTQ mechanism is most active means competing with massive, one-sided demand, leading to high market impact costs and adverse price movements.", "B": "Basis Risk: During a crisis, the stable price relationship between cash Treasury bonds and T-bond futures can break down, making a futures-based hedge unreliable and imperfect.", "C": "Signal Risk: The signal to hedge (lagged stock volatility) is only available with a significant delay, making it impossible to implement the strategy in a timely manner.", "D": "Hedging Ineffectiveness: The strategy is flawed because Table 1 shows that T-bonds provide a weaker hedge (0.61% vs 0.59%) during high-volatility regimes."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to critique a practical application of the paper's findings, converting the 'Apex' part of the original QA. It uses a Premise/assumption packaging strategy, asking the user to select valid risks of a proposed strategy. The correct options capture the core concepts of execution risk and basis risk. Distractor C misinterprets the data in the table (the hedge is effective, and the difference is minimal). Distractor D introduces an irrelevant constraint, as the paper's premise is that lagged volatility is a timely and available signal.", "qid": "421", "question": "### Background\n\nA portfolio manager, observing the paper's findings, considers a dynamic hedging strategy: they will significantly increase their portfolio's allocation to T-bonds whenever lagged stock volatility crosses into an 'Above Median' state, anticipating that T-bonds will provide a strong hedge during the more frequent stock market crashes that occur in such regimes.\n\n**Table 1. Average Daily T-bond Returns During Extreme Stock Declines (<5th percentile)**\n\n| Lagged Volatility was: | # of Obs. | Avg Ret 30-yr T-bond (%) |\n| :--- | :--- | :--- |\n| Below Median | 31 | 0.59 |\n| Above Median | 167 | 0.61 |\n\n### Question\n\nWhich of the following represent significant, real-world risks of implementing this dynamic hedging strategy that are not directly apparent from the data in Table 1? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 386, "Question": "### Background\n\n**Research Question.** To study the impact of capital flows on bond yields, one must first isolate the component of the yield that reflects risk compensation from the component that reflects expectations of future monetary policy. This requires precise definitions of bond risk premia.\n\n**Setting.** The analysis considers two alternative measures of the risk premium on an `n`-period zero-coupon U.S. Treasury bond: an ex-ante, model-based measure (the term premium) and an ex-post, model-free measure (realized excess return).\n\n### Data / Model Specification\n\nThe **term premium** (`TP`) is defined as the difference between the long-term yield and the average of expected future short rates:\n\n  \nTP_{t}^{(n)} \\equiv R_{t}^{(n)} - \\frac{1}{n}\\sum_{i=0}^{n-1}E_{t}(r_{t+i}) \\quad \\text{(Eq. 1)}\n \n\nwhere `R_t^{(n)}` is the `n`-period yield and `r_t` is the one-period short rate.\n\nThe **realized one-period excess return** (`D`) from buying an `n`-period bond and selling it one period later is:\n\n  \nD_{t+1}^{(n)} = \\ln(P_{t+1}^{(n-1)}) - \\ln(P_t^{(n)}) - r_t \\quad \\text{(Eq. 2)}\n \n\nwhere `P_t^{(n)}` is the bond's price.\n\nThe theoretical link between the two measures is:\n\n  \nTP_{t}^{(n)} = \\frac{1}{n} \\sum_{i=1}^{n-1} E_{t}(D_{t+i}^{(n-i+1)}) \\quad \\text{(Eq. 3)}\n \n\n### Question\n\nBased on the provided definitions and theoretical relationships between the term premium (`TP`) and realized excess return (`D`), select all of the following statements that are correct.", "Options": {"A": "The term premium (`TP`) is a \"model-free\" measure because it is directly calculated from bond yields, while the realized excess return (`D`) is \"model-dependent\" because it relies on the log-price transformation.", "B": "The realized excess return (`D`) is an \"ex-post\" and \"model-free\" measure because it is calculated after the fact using only observable market prices, making it a valuable tool for checking the robustness of findings based on the term premium.", "C": "The term premium (`TP`) is considered an \"ex-ante\" and \"model-dependent\" measure because it reflects future expectations and requires a theoretical model to estimate the unobservable path of future short rates.", "D": "The theoretical link in Eq. (3) implies that the Expectations Hypothesis (`TP = 0`) holds if and only if all expected one-period excess returns are zero (`E_t[D_{t+i}^{(k)}] = 0`), reflecting the economic principle of risk-neutrality towards duration risk."}, "Answer": ["B", "C", "D"], "pi_justification": "This item was converted from a General QA problem based on a high suitability score (9.0), reflecting its highly structured concepts and potential for high-fidelity distractors. The item uses an **Atomic Decomposition** strategy to test the theoretical definitions underpinning the paper's dependent variables. **Correct options (A, B, C)** cover the distinct properties of the two risk premium measures and the core theoretical result that unifies them (the conclusion of the original QA's proof). This structure efficiently assesses a broad range of foundational knowledge. **Distractor D (Conceptual Swap)** is a classic high-fidelity distractor that directly swaps the key attributes (\"model-dependent\" vs. \"model-free\") of the two concepts.", "qid": "386", "question": "### Background\n\n**Research Question.** To study the impact of capital flows on bond yields, one must first isolate the component of the yield that reflects risk compensation from the component that reflects expectations of future monetary policy. This requires precise definitions of bond risk premia.\n\n**Setting.** The analysis considers two alternative measures of the risk premium on an `n`-period zero-coupon U.S. Treasury bond: an ex-ante, model-based measure (the term premium) and an ex-post, model-free measure (realized excess return).\n\n### Data / Model Specification\n\nThe **term premium** (`TP`) is defined as the difference between the long-term yield and the average of expected future short rates:\n\n  \nTP_{t}^{(n)} \\equiv R_{t}^{(n)} - \\frac{1}{n}\\sum_{i=0}^{n-1}E_{t}(r_{t+i}) \\quad \\text{(Eq. 1)}\n \n\nwhere `R_t^{(n)}` is the `n`-period yield and `r_t` is the one-period short rate.\n\nThe **realized one-period excess return** (`D`) from buying an `n`-period bond and selling it one period later is:\n\n  \nD_{t+1}^{(n)} = \\ln(P_{t+1}^{(n-1)}) - \\ln(P_t^{(n)}) - r_t \\quad \\text{(Eq. 2)}\n \n\nwhere `P_t^{(n)}` is the bond's price.\n\nThe theoretical link between the two measures is:\n\n  \nTP_{t}^{(n)} = \\frac{1}{n} \\sum_{i=1}^{n-1} E_{t}(D_{t+i}^{(n-i+1)}) \\quad \\text{(Eq. 3)}\n \n\n### Question\n\nBased on the provided definitions and theoretical relationships between the term premium (`TP`) and realized excess return (`D`), select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 396, "Question": "### Background\n\nThe solution to the transformed Thiele PDE system can be expressed as a 'mild solution' using the Duhamel formula. This integral representation is crucial for the paper's analysis as it does not require the reserve function to be classically differentiable.\n\n### Data / Model Specification\n\nThe transformed Thiele PDE system is:\n  \n\\partial_{\\tau}{\\bf V}(\\tau) = \\left({\\mathcal{A}}(\\tau)-r\\right){\\bf V}(\\tau) + {\\bf T}(\\tau){\\bf V}(\\tau) + e^{r\\tau}\\beta(\tau), \\quad {\\bf V}(0)=0\n \nThe mild solution is given by the Duhamel formula:\n  \n\\mathbf{V}(\\tau)=\\int_{0}^{\\tau}\\mathbf{G}(\\tau,s)\\left[\\mathbf{T}(s)\\mathbf{V}(s)+e^{-r(\\tau-s)}\\beta(s)\\right]d s \\quad \\text{(Eq. 1)}\n \nHere, `\\mathbf{G}(\\tau, s)` is the evolution system (propagator) generated by the operator `\\mathcal{A}(\\tau) - rI`.\n\n---\n\nWhich of the following statements accurately describe the properties and interpretation of the Duhamel formula (Eq. 1) and its components?\n", "Options": {"A": "The term `\\mathbf{T}(s)\\mathbf{V}(s)` represents the instantaneous liability generated at time `s` due to insurance risk (state transitions).", "B": "The evolution operator `\\mathbf{G}(\\tau,s)` propagates the liability generated at time `s` forward to time `\\tau`, accounting for financial risk and the time value of money.", "C": "The solution is termed a 'mild solution' because it satisfies the integral equation (Eq. 1) but does not require the reserve `\\mathbf{V}` to be differentiable, unlike a 'classical solution'.", "D": "The operator `\\mathbf{G}(\\tau,s)` is generated by the full operator `\\mathcal{A}(\\tau) + \\mathbf{T}(\\tau) - rI`, combining both financial and insurance risks into a single propagator."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the student's ability to interpret the economic meaning of each component of the Duhamel formula, which is the paper's primary analytical tool. It also tests the distinction between 'mild' and 'classical' solutions. Strategy: Reverse-Reasoning. Given the solution formula, the student must identify the correct roles of its constituent parts. Distractor Logic: Option D is a 'Conceptual Distractor (Almost Right)'. It incorrectly combines `\\mathcal{A}` and `\\mathbf{T}` into the generator for `\\mathbf{G}`, whereas the paper's formulation treats `\\mathbf{T}` as part of the inhomogeneity term inside the integral, which is a crucial structural feature of the solution method.", "qid": "396", "question": "### Background\n\nThe solution to the transformed Thiele PDE system can be expressed as a 'mild solution' using the Duhamel formula. This integral representation is crucial for the paper's analysis as it does not require the reserve function to be classically differentiable.\n\n### Data / Model Specification\n\nThe transformed Thiele PDE system is:\n  \n\\partial_{\\tau}{\\bf V}(\\tau) = \\left({\\mathcal{A}}(\\tau)-r\\right){\\bf V}(\\tau) + {\\bf T}(\\tau){\\bf V}(\\tau) + e^{r\\tau}\\beta(\tau), \\quad {\\bf V}(0)=0\n \nThe mild solution is given by the Duhamel formula:\n  \n\\mathbf{V}(\\tau)=\\int_{0}^{\\tau}\\mathbf{G}(\\tau,s)\\left[\\mathbf{T}(s)\\mathbf{V}(s)+e^{-r(\\tau-s)}\\beta(s)\\right]d s \\quad \\text{(Eq. 1)}\n \nHere, `\\mathbf{G}(\\tau, s)` is the evolution system (propagator) generated by the operator `\\mathcal{A}(\\tau) - rI`.\n\n---\n\nWhich of the following statements accurately describe the properties and interpretation of the Duhamel formula (Eq. 1) and its components?\n", "model_evaluation_result": {"model_raw_response": "\n\nA,B,C\n\n## Explanation\n\nThe reasoning process evaluates each option against the provided equation and background to determine alignment with the Duhamel formula's properties.\n\nFor option A, the term \\(\\mathbf{T}(s)\\mathbf{V}(s)\\) in Eq. 1 directly corresponds to the insurance risk component in the PDE, representing state transitions that generate instantaneous liabilities at time \\(s\\), as specified in the model.\n\nFor option B, the evolution operator \\(\\mathbf{G}(\\tau,s)\\) is defined as the propagator generated by \\(\\mathcal{A}(\\tau) - rI\\), which accounts for financial risk (via \\(\\mathcal{A}\\)) and the time value of money (via the \\(-r\\) discount), propagating values from \\(s\\) to \\(\\tau\\).\n\nFor option C, the background explicitly states that the mild solution uses the Duhamel formula and does not require classical differentiability of \\(\\mathbf{V}\\), distinguishing it from a classical solution that would satisfy the PDE directly.\n\nFor option D, the description is incorrect because \\(\\mathbf{G}(\\tau,s)\\) is generated solely by \\(\\mathcal{A}(\\tau) - rI\\), excluding\n> 🔍 **Analyzing options**\n> ***- The term `\\mathbf{T}(s)\\mathbf{V}(s)` likely shows liability from insurance risk at time `s`.***\n \\(\\mathbf{T}(\\tau)\\), which is treated as a forcing term in the integral rather than part of the propagator.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 436, "Question": "### Background\n\n**Research Question.** A central challenge in empirical asset pricing is accurately measuring a firm's exposure to risk factors, such as exchange rate fluctuations. Standard models, like the augmented CAPM proposed by Jorion, often find surprisingly weak evidence of such exposure. This paper argues that this weak evidence may be a statistical artifact stemming from the model's failure to account for the correlation between exchange rate movements and overall stock market returns.\n\n**Setting / Data-Generating Environment.** The analysis compares two methods for estimating firm-level currency exposure for European firms from 1999-2011: the standard Jorion model and a three-step orthogonalization procedure. The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Model 1: The Standard Jorion Model**\nThis model estimates the currency exposure coefficient `\\beta_{1i}`.\n\n**Model 2: The Three-Step Orthogonalization Procedure**\nThis model estimates the currency exposure coefficient `\\beta_{is}` using orthogonal risk factors.\n\n**Table 1. Comparison of Final Estimated Currency Exposure (Euro Stoxx TMI)**\n\n| Currency | Model | Avg. `\\beta` (significant firms) | Total Significant (%) |\n|:---|:---|:---:|:---:|\n| **CHF** | Orthogonalized | 1.88 | 52.74% |\n| | Jorion (Augmented) | 1.05 | 11.34% |\n\n### Question\n\nUsing the results for the CHF/Euro exchange rate in Table 1, select all statements that correctly quantify and interpret the impact of using the Orthogonalized model compared to the standard Jorion model.", "Options": {"A": "The average exposure coefficient (`\\beta`) for significantly affected firms is substantially larger in the Orthogonalized model (1.88) than in the Jorion model (1.05), suggesting the Jorion model understates not just the prevalence but also the magnitude of the exposure.", "B": "The results suggest that the Jorion model is more conservative and likely provides a more reliable estimate of currency exposure by avoiding the complexities of the three-step procedure.", "C": "The Orthogonalized model identifies a proportion of significantly exposed firms (52.74%) that is more than four times greater than the proportion identified by the Jorion model (11.34%).", "D": "The Jorion model finds that over 20% of firms are significantly exposed to CHF risk, which is only moderately lower than the Orthogonalized model's finding."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Computational Judgment and Atomic Decomposition strategies. It tests the ability to extract specific data from a table, perform a simple calculation (ratio), and draw the correct conclusion about the methodological comparison. Distractors are based on a Numerical Error (C) and a Conceptual Opposite to the paper's findings (D).", "qid": "436", "question": "### Background\n\n**Research Question.** A central challenge in empirical asset pricing is accurately measuring a firm's exposure to risk factors, such as exchange rate fluctuations. Standard models, like the augmented CAPM proposed by Jorion, often find surprisingly weak evidence of such exposure. This paper argues that this weak evidence may be a statistical artifact stemming from the model's failure to account for the correlation between exchange rate movements and overall stock market returns.\n\n**Setting / Data-Generating Environment.** The analysis compares two methods for estimating firm-level currency exposure for European firms from 1999-2011: the standard Jorion model and a three-step orthogonalization procedure. The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Model 1: The Standard Jorion Model**\nThis model estimates the currency exposure coefficient `\\beta_{1i}`.\n\n**Model 2: The Three-Step Orthogonalization Procedure**\nThis model estimates the currency exposure coefficient `\\beta_{is}` using orthogonal risk factors.\n\n**Table 1. Comparison of Final Estimated Currency Exposure (Euro Stoxx TMI)**\n\n| Currency | Model | Avg. `\\beta` (significant firms) | Total Significant (%) |\n|:---|:---|:---:|:---:|\n| **CHF** | Orthogonalized | 1.88 | 52.74% |\n| | Jorion (Augmented) | 1.05 | 11.34% |\n\n### Question\n\nUsing the results for the CHF/Euro exchange rate in Table 1, select all statements that correctly quantify and interpret the impact of using the Orthogonalized model compared to the standard Jorion model.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 248, "Question": "### Background\n\nThe public debate surrounding the 1993 Volvo-Renault merger pitted two competing frameworks against each other. The pro-merger rationale, often summarized as the \"Big is Beautiful\" argument, focused on the strategic imperative of achieving economies of scale in the global automotive industry. This view was contrasted with a critical shareholder value analysis, championed by journalist Peter Malmqvist, which argued the deal was financially detrimental to Volvo's owners.\n\n### Data / Model Specification\n\nLet `V_V` be the standalone market value of Volvo, `V_R` be the standalone market value of Renault, and `S` be the present value of all future synergies from the merger, estimated at over $5 billion. In the proposed deal, Volvo shareholders contribute their entire firm (value `V_V`) and in return receive a 35% stake in the new, combined firm (RVA).\n\nThe Net Present Value (NPV) of the merger to Volvo's shareholders can be expressed as the value of what they receive minus the value of what they give up:\n\n  \nNPV_V = (0.35 \\times (V_V + V_R + S)) - V_V\n \n\nThis can be rearranged to:\n\n  \nNPV_V = 0.35 \\times (V_R + S) - 0.65 \\times V_V\n \n\nMalmqvist's critique asserted that Volvo's truck division was \"world-class\" while Renault's was \"weak,\" implying that Volvo's standalone value (`V_V`) was disproportionately high relative to Renault's (`V_R`).\n\n---\n\nBased on the provided model and context, select all of the following statements that are valid conclusions.\n", "Options": {"A": "Even if the projected synergies (`S`) are positive and substantial, the deal can still destroy value for Volvo shareholders (`NPV_V < 0`).", "B": "According to the NPV formula, the deal is value-destructive for Volvo shareholders if the value of the assets they contribute (`0.65 \\times V_V`) exceeds the value of the assets they effectively acquire (`0.35 \\times (V_R + S)`).", "C": "The merger would create positive value for Volvo shareholders (`NPV_V > 0`) if the value of Renault plus synergies is greater than the value of Volvo (`V_R + S > V_V`).", "D": "Malmqvist's argument implies that the ratio of `V_V` to `V_R` is high, which increases the likelihood that `0.65 \\times V_V` will be larger than `0.35 \\times (V_R + S)`."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: The item assesses the candidate's ability to interpret a formal M&A valuation model (NPV to one party's shareholders) and connect it to the qualitative critiques presented in the case. It requires understanding how value is created versus how it is distributed.\nDepth Strategy: Computational Judgment. The user must analyze the provided NPV formula and evaluate the conditions under which the NPV is positive or negative, linking this formal analysis to the narrative arguments.\nDistractor Logic:\n- A: This is a common conceptual error (Formula Misuse). It simplifies the condition incorrectly. The correct condition is `0.35 * (V_R + S) > 0.65 * V_V`, not `V_R + S > V_V`.\n- B: Correct. This is the central thesis of the critique. If `V_V` is sufficiently large relative to `V_R + S`, the NPV will be negative.\n- C: Correct. This is a direct and correct interpretation of the rearranged NPV formula, framing it as a trade-off between the value contributed and the value acquired.\n- D: Correct. This correctly translates Malmqvist's qualitative critique ('world-class' vs. 'weak') into its quantitative implication for the variables in the NPV model, demonstrating a synthesis of narrative and formula.", "qid": "248", "question": "### Background\n\nThe public debate surrounding the 1993 Volvo-Renault merger pitted two competing frameworks against each other. The pro-merger rationale, often summarized as the \"Big is Beautiful\" argument, focused on the strategic imperative of achieving economies of scale in the global automotive industry. This view was contrasted with a critical shareholder value analysis, championed by journalist Peter Malmqvist, which argued the deal was financially detrimental to Volvo's owners.\n\n### Data / Model Specification\n\nLet `V_V` be the standalone market value of Volvo, `V_R` be the standalone market value of Renault, and `S` be the present value of all future synergies from the merger, estimated at over $5 billion. In the proposed deal, Volvo shareholders contribute their entire firm (value `V_V`) and in return receive a 35% stake in the new, combined firm (RVA).\n\nThe Net Present Value (NPV) of the merger to Volvo's shareholders can be expressed as the value of what they receive minus the value of what they give up:\n\n  \nNPV_V = (0.35 \\times (V_V + V_R + S)) - V_V\n \n\nThis can be rearranged to:\n\n  \nNPV_V = 0.35 \\times (V_R + S) - 0.65 \\times V_V\n \n\nMalmqvist's critique asserted that Volvo's truck division was \"world-class\" while Renault's was \"weak,\" implying that Volvo's standalone value (`V_V`) was disproportionately high relative to Renault's (`V_R`).\n\n---\n\nBased on the provided model and context, select all of the following statements that are valid conclusions.\n", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: from sympy import symbols, solve\nV_R, S, V_V = sym...\n\n> 🔍 **Analyzing merger value**\n> ***- The NPV formula shows Volvo shareholders' gain as 0.35 times Renault's value plus synergies, minus 0.65 times Volvo's value.***\n\n\nA,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 291, "Question": "### Background\n\n**Research Question.** How can the classical exponential bound on aggregate claims be refined for different classes of claim severity distributions, particularly those exhibiting opposing 'aging' characteristics?\n\n**Setting / Data-Generating Environment.** We are in the classical collective risk model. The goal is to refine the standard Lundberg exponential bound by incorporating information about the shape of the claim size distribution `F(x)`, specifically whether it belongs to the New Worse than Used in Convex ordering (NWUC) class or the Decreasing Mean Residual Lifetime (DMRL) class.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `\\overline{F}(x)`: The survival function of individual claim sizes.\n- `p_0, \\phi`: Parameters of the claim frequency distribution.\n- `\\kappa`: The adjustment coefficient, `\\kappa > 0`.\n\n---\n\n### Data / Model Specification\n\nThe classical Lundberg inequality provides a general upper bound:\n  \n\\overline{G}(x) \\leq \\frac{1-p_{0}}{\\phi} \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (1))}\n \nIf `F(x)` is **NWUC** (a class of distributions with no 'wear-out' effects), a tighter bound (Theorem 5) is available:\n  \n\\overline{G}(x) \\leq (1-p_{0}) \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (2))}\n \nIf `F(x)` is **DMRL** (characteristic of risks with 'wear-out' effects), a different refined bound (Theorem 6) is available.\n\n---\n\n### Question\n\nBased on the paper's analysis of refined exponential bounds, select all of the following statements that are correct.", "Options": {"A": "DMRL distributions characterize risks with 'positive aging' or wear-out, where the expected remaining lifetime decreases as the current age or loss amount increases.", "B": "The refined bound for NWUC distributions in Eq. (2) is tighter than the classical Lundberg bound in Eq. (1) by a factor of `1-p_0`.", "C": "The refined bound for NWUC distributions in Eq. (2) is tighter than the classical Lundberg bound in Eq. (1) by a constant factor of `\\phi`.", "D": "NWUC distributions characterize risks with 'positive aging' or wear-out, a property exemplified by equipment breakdown risk where failure becomes more likely over time."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the conceptual understanding of the paper's key refinements to the exponential bound. It uses an Atomic Decomposition strategy, breaking down the original QA's comparison into distinct, testable propositions. Correct options (A, B) accurately state the improvement factor and the definition of a DMRL risk profile. Distractor (C) presents an incorrect improvement factor (Formula Misuse). Distractor (D) swaps the properties of NWUC and DMRL risks (Conceptual Opposite), targeting a common point of confusion.", "qid": "291", "question": "### Background\n\n**Research Question.** How can the classical exponential bound on aggregate claims be refined for different classes of claim severity distributions, particularly those exhibiting opposing 'aging' characteristics?\n\n**Setting / Data-Generating Environment.** We are in the classical collective risk model. The goal is to refine the standard Lundberg exponential bound by incorporating information about the shape of the claim size distribution `F(x)`, specifically whether it belongs to the New Worse than Used in Convex ordering (NWUC) class or the Decreasing Mean Residual Lifetime (DMRL) class.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `\\overline{F}(x)`: The survival function of individual claim sizes.\n- `p_0, \\phi`: Parameters of the claim frequency distribution.\n- `\\kappa`: The adjustment coefficient, `\\kappa > 0`.\n\n---\n\n### Data / Model Specification\n\nThe classical Lundberg inequality provides a general upper bound:\n  \n\\overline{G}(x) \\leq \\frac{1-p_{0}}{\\phi} \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (1))}\n \nIf `F(x)` is **NWUC** (a class of distributions with no 'wear-out' effects), a tighter bound (Theorem 5) is available:\n  \n\\overline{G}(x) \\leq (1-p_{0}) \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (2))}\n \nIf `F(x)` is **DMRL** (characteristic of risks with 'wear-out' effects), a different refined bound (Theorem 6) is available.\n\n---\n\n### Question\n\nBased on the paper's analysis of refined exponential bounds, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 331, "Question": "### Background\n\n**Research Question.** How should an analyst select comparable companies when using a market multiple to estimate the terminal value of a firm, and why are valuation multiples expected to change as a firm matures?\n\n**Setting.** An illustrative valuation of a company that transitions from a high-growth phase (25% revenue growth in Year 1) to a stable, mature phase (2% growth from Year 4 onwards).\n\n**Variables and Parameters.**\n- `EV_t`: Enterprise Value at the beginning of Year `t`.\n- `EBIT_t`: EBIT for Year `t`.\n\n---\n\n### Data / Model Specification\n\nThe constant growth perpetuity formula for firm value is `EV_t = FCF_{t+1} / (r - g)`. This shows that multiples are an increasing function of the growth rate `g`.\n\n**Table 1: Market Multiples for a High-Growth Firm Over Time**\n| Metric | Year 1 | Year 2 | Year 3 | Year 4 | Year 5 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Revenue Growth Rate | 25.0% | 20.0% | 8.0% | 2.0% | 2.0% |\n| EBIT Multiple (EV/EBIT) | 10.9x | 9.6x | 8.4x | 8.0x | 8.0x |\n\n---\n\n### Question\n\nAn analyst is valuing a young, high-growth company. The valuation involves a 5-year explicit forecast period followed by a terminal value calculation, at which point the company is expected to be a mature, stable-growth firm. Based on the principles and data presented, select all appropriate practices for this valuation.", "Options": {"A": "The peer group used to determine the terminal value multiple should consist of companies that are currently mature and exhibit stable, low-growth characteristics.", "B": "The terminal EV/EBIT multiple should be cross-checked for consistency with an implied DCF perpetuity formula, such as `EV/EBIT = (FCF/EBIT) * (1+g) / (r-g)`.", "C": "To ensure consistency, the terminal multiple should be based on the company's own calculated multiple in Year 1 (e.g., 10.9x EV/EBIT).", "D": "The valuation multiples of the company being valued are expected to decline over the forecast period as its growth rate slows."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Premise/Assumption Packaging strategy to assess the practical application of the paper's findings to terminal value calculation, a critical valuation step. It tests for understanding of the 'characteristic matching' principle. Options A and B are direct applications of the paper's argument. Option C packages the logic of the original QA's quantitative derivation into a conceptual check, adding depth. Distractor (D) represents the primary valuation error discussed in the paper.", "qid": "331", "question": "### Background\n\n**Research Question.** How should an analyst select comparable companies when using a market multiple to estimate the terminal value of a firm, and why are valuation multiples expected to change as a firm matures?\n\n**Setting.** An illustrative valuation of a company that transitions from a high-growth phase (25% revenue growth in Year 1) to a stable, mature phase (2% growth from Year 4 onwards).\n\n**Variables and Parameters.**\n- `EV_t`: Enterprise Value at the beginning of Year `t`.\n- `EBIT_t`: EBIT for Year `t`.\n\n---\n\n### Data / Model Specification\n\nThe constant growth perpetuity formula for firm value is `EV_t = FCF_{t+1} / (r - g)`. This shows that multiples are an increasing function of the growth rate `g`.\n\n**Table 1: Market Multiples for a High-Growth Firm Over Time**\n| Metric | Year 1 | Year 2 | Year 3 | Year 4 | Year 5 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Revenue Growth Rate | 25.0% | 20.0% | 8.0% | 2.0% | 2.0% |\n| EBIT Multiple (EV/EBIT) | 10.9x | 9.6x | 8.4x | 8.0x | 8.0x |\n\n---\n\n### Question\n\nAn analyst is valuing a young, high-growth company. The valuation involves a 5-year explicit forecast period followed by a terminal value calculation, at which point the company is expected to be a mature, stable-growth firm. Based on the principles and data presented, select all appropriate practices for this valuation.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 326, "Question": "## Background\n\n**Research Question.** This case explores the theoretical mechanism through which a firm can lower its cost of capital by cross-listing its shares on a foreign exchange, thereby moving from a segmented home market to an integrated global capital market, and the resulting change in its risk profile.\n\n**Setting.** Consider a firm from a segmented home market (`H`) that cross-lists its shares in the U.S., which is part of the integrated global market (`G`). Before listing, the firm's cost of capital is determined by a local CAPM. After listing, its cost of capital is determined by a global CAPM.\n\n**Variables & Parameters.**\n- `E[R_i]`: Expected return on asset `i`.\n- `R_f`: Risk-free rate.\n- `β_{i,H}`: Beta of asset `i` with respect to the home market portfolio, `H`.\n- `β_{i,G}`: Beta of asset `i` with respect to the global market portfolio, `G`.\n- `λ_H`: Market price of risk in the segmented home market.\n- `λ_G`: Market price of risk in the integrated global market.\n\n---\n\n## Data / Model Specification\n\nIn a **segmented** home market, the firm's cost of capital is determined by a local CAPM:\n  \nE[R_i] = R_f + \\beta_{i,H} \\lambda_H \\quad \\text{(Eq. (1))}\n \nAfter a successful cross-listing **integrates** the firm into the global market, its cost of capital is determined by a global CAPM:\n  \nE[R_i] = R_f + \\beta_{i,G} \\lambda_G \\quad \\text{(Eq. (2))}\n \nIt is generally assumed that `λ_H > λ_G`. Empirical evidence (**Finding #2** in the paper) suggests that after listing, a firm's measured home market beta (`β_{i,H}`) declines.\n\n---\n\n## Question\n\nAccording to the asset pricing framework and empirical findings described, select all correct statements regarding how a cross-listing in the U.S. can lower a non-U.S. firm's cost of capital.", "Options": {"A": "The model assumes that the market price of risk is higher in the integrated global market than in the segmented home market (λ_G > λ_H).", "B": "The cost of capital can fall because the 'price of risk' is lower in the integrated global market (λ_G) than in the segmented home market (λ_H), as risk is shared among a broader base of investors.", "C": "The cost of capital can also fall due to a change in the relevant 'quantity of risk', as the firm's risk is measured by its global beta (β_i,G) rather than its home market beta (β_i,H).", "D": "The primary benefit of cross-listing comes from an increase in the firm's home market beta (β_i,H), which signals higher quality to global investors."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). Conceptual Clarity = 9/10, Discriminability = 9/10. This item assesses the core theoretical mechanism for cost of capital reduction via market integration. It uses an Atomic Decomposition strategy, breaking the concept into the 'price of risk' and 'quantity of risk' channels. Distractors C and D are designed as conceptual opposites, directly testing for common misunderstandings of the theory by reversing the direction of the effects or the core assumption.", "qid": "326", "question": "## Background\n\n**Research Question.** This case explores the theoretical mechanism through which a firm can lower its cost of capital by cross-listing its shares on a foreign exchange, thereby moving from a segmented home market to an integrated global capital market, and the resulting change in its risk profile.\n\n**Setting.** Consider a firm from a segmented home market (`H`) that cross-lists its shares in the U.S., which is part of the integrated global market (`G`). Before listing, the firm's cost of capital is determined by a local CAPM. After listing, its cost of capital is determined by a global CAPM.\n\n**Variables & Parameters.**\n- `E[R_i]`: Expected return on asset `i`.\n- `R_f`: Risk-free rate.\n- `β_{i,H}`: Beta of asset `i` with respect to the home market portfolio, `H`.\n- `β_{i,G}`: Beta of asset `i` with respect to the global market portfolio, `G`.\n- `λ_H`: Market price of risk in the segmented home market.\n- `λ_G`: Market price of risk in the integrated global market.\n\n---\n\n## Data / Model Specification\n\nIn a **segmented** home market, the firm's cost of capital is determined by a local CAPM:\n  \nE[R_i] = R_f + \\beta_{i,H} \\lambda_H \\quad \\text{(Eq. (1))}\n \nAfter a successful cross-listing **integrates** the firm into the global market, its cost of capital is determined by a global CAPM:\n  \nE[R_i] = R_f + \\beta_{i,G} \\lambda_G \\quad \\text{(Eq. (2))}\n \nIt is generally assumed that `λ_H > λ_G`. Empirical evidence (**Finding #2** in the paper) suggests that after listing, a firm's measured home market beta (`β_{i,H}`) declines.\n\n---\n\n## Question\n\nAccording to the asset pricing framework and empirical findings described, select all correct statements regarding how a cross-listing in the U.S. can lower a non-U.S. firm's cost of capital.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 438, "Question": "### Background\n\n**Research Question.** After establishing a robust method for measuring currency exposure, the paper investigates how this exposure varies across different types of firms, including by industry and country of origin.\n\n**Setting / Data-Generating Environment.** The analysis uses firm-level exchange rate exposure coefficients (`\\beta_{is}`) estimated from the third stage of the orthogonalized model for firms in the Euro Stoxx TMI index (1999-2011). The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Table 1. Exchange Rate Exposure (`\\beta_{is}`) by Industry (for CHF Shocks)**\n\n| | Avg. `\\beta_{is}` (significant firms) | Total Significant (%) |\n|:---|:---:|:---:|\n| Financials | 1.90 | 72% |\n| Non-Financials | 1.88 | 49% |\n\n*Notes: `\\beta_{is}` is the average of significant coefficients.* \n\n### Question\n\nA portfolio manager notes that the Spanish stock market is composed of approximately 28% financial firms and 72% non-financial firms. Using the average significant `\\beta_{is}` coefficients for the CHF/Euro rate from Table 1, which of the following conclusions are arithmetically and conceptually correct?", "Options": {"A": "The expected value-weighted average exposure for the Spanish market is simply the average of the two coefficients, (1.90 + 1.88) / 2 = 1.89, because the weights are nearly equal.", "B": "This bottom-up calculation, showing a high aggregate exposure, supports the paper's finding that country-level risk is heavily influenced by industrial composition, particularly the concentration of high-exposure financial firms.", "C": "The calculation shows that the high exposure of financial firms is largely offset by the lower exposure of non-financials, leading to a moderate overall market exposure.", "D": "The expected value-weighted average exposure (`\\bar{\\beta}_{is}`) for the Spanish market is approximately 1.89."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Computational Judgment strategy. It requires the user to perform a multi-step calculation (weighted average) based on data from the table and then draw a conclusion that synthesizes this calculation with the paper's broader argument about country-level heterogeneity. Distractors are based on a Formula Misuse error (C) and a Conceptual Opposite interpretation of the result (D).", "qid": "438", "question": "### Background\n\n**Research Question.** After establishing a robust method for measuring currency exposure, the paper investigates how this exposure varies across different types of firms, including by industry and country of origin.\n\n**Setting / Data-Generating Environment.** The analysis uses firm-level exchange rate exposure coefficients (`\\beta_{is}`) estimated from the third stage of the orthogonalized model for firms in the Euro Stoxx TMI index (1999-2011). The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Table 1. Exchange Rate Exposure (`\\beta_{is}`) by Industry (for CHF Shocks)**\n\n| | Avg. `\\beta_{is}` (significant firms) | Total Significant (%) |\n|:---|:---:|:---:|\n| Financials | 1.90 | 72% |\n| Non-Financials | 1.88 | 49% |\n\n*Notes: `\\beta_{is}` is the average of significant coefficients.* \n\n### Question\n\nA portfolio manager notes that the Spanish stock market is composed of approximately 28% financial firms and 72% non-financial firms. Using the average significant `\\beta_{is}` coefficients for the CHF/Euro rate from Table 1, which of the following conclusions are arithmetically and conceptually correct?", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 430, "Question": "### Background\n\nThe paper develops an integrated expenditure equation that combines financing from internal, non-financial receipts with financing from external, market-determined funds. This model is designed to provide a more complete picture of spending decisions.\n\n### Data / Model Specification\n\nThe proposed integrated expenditure equation is:\n\n  \nN^{ij} = {}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j} + DN^{ij} + LN^{ij} + CN^{ij} + MN^{ij} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `N^{ij}`: Expenditure on product `i` by sector `j`.\n- `{}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j}`: Spending financed by internal receipts (determined by the fixed-coefficient circular flow model).\n- `DN^{ij} + LN^{ij} + CN^{ij} + MN^{ij}`: Spending financed by external funds from equity (D), other asset (L), bank loan (C), and money (M) markets (determined by the financial market model).\n\n---\n\nConsider a scenario where a central bank implements a contractionary monetary policy by selling government securities, which drains reserves from the banking system. According to the logic of the paper's integrated model, which of the following are likely consequences of this policy action?\n\nSelect all that apply.", "Options": {"A": "The overall real expenditure, `N^{ij}`, would be guaranteed to fall by the exact amount that `CN^{ij}` decreases, as all sources of funds are perfect substitutes.", "B": "The bank loan flow term, `CN^{ij}`, would decrease as the supply of bank credit is constrained by the lack of reserves.", "C": "Firms and households seeking to maintain spending levels might attempt to compensate for the reduction in bank loans by increasing their reliance on other financial sources, potentially increasing flows like `LN^{ij}` (selling other assets) or `MN^{ij}` (dishoarding).", "D": "The internal funds component, `{}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j}`, would remain unaffected in the short run, as it depends on past receipts, not current financial market conditions."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item tests the user's understanding of the structure of the integrated expenditure model and their ability to trace the transmission of a monetary policy shock through its components.\nDepth Strategy: Scenario Application. The user must apply the theoretical model to a novel micro-case (a specific monetary policy action) and predict its effects.\nDistractor Logic:\n- A (Correct): This identifies the most direct and primary transmission channel of a reserve-draining operation, which is the constraint on bank lending.\n- B (Correct): This correctly distinguishes between the external finance component, which is immediately affected by market conditions, and the internal finance component, which is based on a sector's own receipts and is therefore more backward-looking.\n- C (Correct): This demonstrates a deeper understanding of the model by acknowledging that agents will attempt to substitute between different sources of external finance in response to a shock in one market.\n- D (Conceptual Distractor - Overstatement/Conceptual Opposite): This is incorrect because it makes two extreme claims: a guaranteed 1-for-1 effect and perfect substitutability. The paper's empirical section suggests substitutability is imperfect and varies by expenditure type. The attempt to substitute (as in option C) does not mean the substitution will be complete or costless, so total expenditure is still expected to fall, but not necessarily by the exact amount of the initial shock to bank loans.", "qid": "430", "question": "### Background\n\nThe paper develops an integrated expenditure equation that combines financing from internal, non-financial receipts with financing from external, market-determined funds. This model is designed to provide a more complete picture of spending decisions.\n\n### Data / Model Specification\n\nThe proposed integrated expenditure equation is:\n\n  \nN^{ij} = {}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j} + DN^{ij} + LN^{ij} + CN^{ij} + MN^{ij} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `N^{ij}`: Expenditure on product `i` by sector `j`.\n- `{}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j}`: Spending financed by internal receipts (determined by the fixed-coefficient circular flow model).\n- `DN^{ij} + LN^{ij} + CN^{ij} + MN^{ij}`: Spending financed by external funds from equity (D), other asset (L), bank loan (C), and money (M) markets (determined by the financial market model).\n\n---\n\nConsider a scenario where a central bank implements a contractionary monetary policy by selling government securities, which drains reserves from the banking system. According to the logic of the paper's integrated model, which of the following are likely consequences of this policy action?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 408, "Question": "### Background\n\n**Research Question.** This case dissects the economic mechanism behind the finding that increased board independence leads to a reduction in Corporate Social Responsibility (CSR). It investigates the relationship between CSR and firm risk to build a cohesive narrative supporting the agency cost hypothesis.\n\n**Setting.** The 'risk-mitigation/agency cost hypothesis' posits that managers, who are more risk-averse than well-diversified shareholders, may over-invest in CSR activities to reduce firm-specific risk and protect their personal human and financial capital. An independent board, representing shareholders, would act to curtail such value-destroying over-investment.\n\n**Variables & Parameters.**\n\n*   `CSR score`: The composite measure, defined as `CSR strengths - CSR concerns`.\n*   `Total risk`: Standard deviation of daily stock returns.\n*   `Idiosyncratic risk`: Standard deviation of residuals from a market model regression.\n*   `Systematic risk`: The market beta (`β`) from a market model regression.\n\n---\n\n### Data / Model Specification\n\n**Table 1: The Effect of CSR on Firm Risk**\n\nThis table shows results from regressing three different measures of firm risk on the composite CSR score, including firm and year fixed effects.\n\n| Dependent Variable | `CSR` Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| Total risk | -0.004 | (-4.48) |\n| Idiosyncratic risk | -0.003 | (-3.25) |\n| Systematic risk | -0.019 | (-6.13) |\n\n---\n\n### Question\n\nThe 'risk-mitigation/agency cost hypothesis' provides a rationale for why managers might over-invest in CSR. Based on the theory and the empirical evidence in Table 1, which of the following statements are correct? Select all that apply.", "Options": {"A": "According to the agency cost hypothesis, independent boards cut CSR because any form of risk reduction, including systematic risk reduction, is inherently value-destroying for shareholders.", "B": "The agency conflict motivating CSR over-investment stems from managers' aversion to firm-specific (idiosyncratic) risk, which well-diversified shareholders are insulated from.", "C": "The data shows that higher CSR scores are associated with statistically significant reductions in total, idiosyncratic, and systematic firm risk.", "D": "The evidence suggests that CSR activities primarily reduce systematic (market) risk, with no significant effect on idiosyncratic risk."}, "Answer": ["B", "C"], "pi_justification": "This item tests the understanding of the core theoretical mechanism (agency cost of risk aversion) and the ability to link it to supporting empirical evidence. It combines 'Premise Packaging' (testing the theory) and 'Atomic Decomposition' (testing the data). Distractors test for incomplete reading of the table (C) and a misunderstanding of the nuance of shareholder value (D).", "qid": "408", "question": "### Background\n\n**Research Question.** This case dissects the economic mechanism behind the finding that increased board independence leads to a reduction in Corporate Social Responsibility (CSR). It investigates the relationship between CSR and firm risk to build a cohesive narrative supporting the agency cost hypothesis.\n\n**Setting.** The 'risk-mitigation/agency cost hypothesis' posits that managers, who are more risk-averse than well-diversified shareholders, may over-invest in CSR activities to reduce firm-specific risk and protect their personal human and financial capital. An independent board, representing shareholders, would act to curtail such value-destroying over-investment.\n\n**Variables & Parameters.**\n\n*   `CSR score`: The composite measure, defined as `CSR strengths - CSR concerns`.\n*   `Total risk`: Standard deviation of daily stock returns.\n*   `Idiosyncratic risk`: Standard deviation of residuals from a market model regression.\n*   `Systematic risk`: The market beta (`β`) from a market model regression.\n\n---\n\n### Data / Model Specification\n\n**Table 1: The Effect of CSR on Firm Risk**\n\nThis table shows results from regressing three different measures of firm risk on the composite CSR score, including firm and year fixed effects.\n\n| Dependent Variable | `CSR` Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| Total risk | -0.004 | (-4.48) |\n| Idiosyncratic risk | -0.003 | (-3.25) |\n| Systematic risk | -0.019 | (-6.13) |\n\n---\n\n### Question\n\nThe 'risk-mitigation/agency cost hypothesis' provides a rationale for why managers might over-invest in CSR. Based on the theory and the empirical evidence in Table 1, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 370, "Question": "### Background\n\n**Research Question.** To test the causal mechanisms through which government industrial policy influences corporate financialization, distinguishing between two primary theoretical motivations.\n\n**Theoretical Framework.** Corporate financialization—the strategy of non-financial corporations (NFCs) allocating more resources to financial rather than real investments—is driven by two main motives:\n1.  **Prevention Motivation:** Firms hold liquid financial assets as a precautionary measure to manage future operating risks and ease financial constraints. This is a risk-management strategy.\n2.  **Crowding-Out Effect:** Agency conflicts lead managers to favor short-term financial investments over long-term real investments. This is a corporate governance problem.\n\nThe study uses a three-step mediation framework to test four potential channels: two related to the prevention motive (operational uncertainty, financial constraints) and two related to the crowding-out effect (information transparency, investment opportunities).\n\n### Data / Model Specification\n\nThe analysis finds that the two channels related to the prevention motivation are statistically significant, while the two channels related to the crowding-out effect are not. The key results for the two significant channels are presented below.\n\n**Table 1. Mediation Effect Test of Operational Uncertainty**\n\n| | (1) Path a | (2) Path b | (3) Path c |\n| :--- | :--- | :--- | :--- |\n| **Dependent Var.** | `FINRATIO` | `UNCERTAINTY` | `FINRATIO` |\n| `IP1` | -0.0126*** | -0.0977*** | -0.0113*** |\n| `UNCERTAINTY` | | | 0.0112*** |\n\n*Note: `FINRATIO` is the ratio of financial assets to total assets. `IP1` is a dummy for industrial policy support. `UNCERTAINTY` is a proxy for firm-level operational risk. *** denotes p < .01.*\n\n**Table 2. Mediation Effect Test of Easing Financial Constraints**\n\n| | (1) Path a | (2) Path b | (3) Path c |\n| :--- | :--- | :--- | :--- |\n| **Dependent Var.** | `FINRATIO` | `KZ` | `FINRATIO` |\n| `IP1` | -0.0126*** | -0.0794*** | -0.0122*** |\n| `KZ` | | | 0.0022* |\n\n*Note: `KZ` is the Kaplan-Zingales index of financial constraints (higher value = more constrained). * denotes p < .10, *** denotes p < .01.*\n\n### Question\n\nBased on the mediation analysis results in Table 1 and Table 2, select all of the following statements that are correct.", "Options": {"A": "Industrial policy is shown to significantly ease firms' financial constraints, as measured by the KZ index.", "B": "The indirect effect of industrial policy on financialization transmitted through the operational uncertainty channel is approximately -0.0011.", "C": "The results indicate that higher operational uncertainty leads to lower levels of corporate financialization.", "D": "The total effect of industrial policy on financialization (-0.0126) is fully explained by the two significant mediation channels combined."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses Computational Judgment and Atomic Decomposition to test the student's ability to interpret and calculate mediation effects from regression tables. Option A tests the interpretation of Path b for the financial constraints channel. Option B requires calculation of the indirect effect for the uncertainty channel. Distractors are based on a sign error in interpreting Path c (C) and a conceptual error misinterpreting partial vs. total mediation effects (D).", "qid": "370", "question": "### Background\n\n**Research Question.** To test the causal mechanisms through which government industrial policy influences corporate financialization, distinguishing between two primary theoretical motivations.\n\n**Theoretical Framework.** Corporate financialization—the strategy of non-financial corporations (NFCs) allocating more resources to financial rather than real investments—is driven by two main motives:\n1.  **Prevention Motivation:** Firms hold liquid financial assets as a precautionary measure to manage future operating risks and ease financial constraints. This is a risk-management strategy.\n2.  **Crowding-Out Effect:** Agency conflicts lead managers to favor short-term financial investments over long-term real investments. This is a corporate governance problem.\n\nThe study uses a three-step mediation framework to test four potential channels: two related to the prevention motive (operational uncertainty, financial constraints) and two related to the crowding-out effect (information transparency, investment opportunities).\n\n### Data / Model Specification\n\nThe analysis finds that the two channels related to the prevention motivation are statistically significant, while the two channels related to the crowding-out effect are not. The key results for the two significant channels are presented below.\n\n**Table 1. Mediation Effect Test of Operational Uncertainty**\n\n| | (1) Path a | (2) Path b | (3) Path c |\n| :--- | :--- | :--- | :--- |\n| **Dependent Var.** | `FINRATIO` | `UNCERTAINTY` | `FINRATIO` |\n| `IP1` | -0.0126*** | -0.0977*** | -0.0113*** |\n| `UNCERTAINTY` | | | 0.0112*** |\n\n*Note: `FINRATIO` is the ratio of financial assets to total assets. `IP1` is a dummy for industrial policy support. `UNCERTAINTY` is a proxy for firm-level operational risk. *** denotes p < .01.*\n\n**Table 2. Mediation Effect Test of Easing Financial Constraints**\n\n| | (1) Path a | (2) Path b | (3) Path c |\n| :--- | :--- | :--- | :--- |\n| **Dependent Var.** | `FINRATIO` | `KZ` | `FINRATIO` |\n| `IP1` | -0.0126*** | -0.0794*** | -0.0122*** |\n| `KZ` | | | 0.0022* |\n\n*Note: `KZ` is the Kaplan-Zingales index of financial constraints (higher value = more constrained). * denotes p < .10, *** denotes p < .01.*\n\n### Question\n\nBased on the mediation analysis results in Table 1 and Table 2, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 288, "Question": "### Background\n\n**Research Question.** How can a research design be improved to distinguish between insiders trading on specific foreknowledge of a listing versus trading on the firm's general good performance that makes it *eligible* for listing?\n\n**Setting.** A key challenge in the study is this identification problem. The observed pre-listing buying could be a reaction to the firm's strong underlying performance, which is also the reason the firm decides to list.\n\n---\n\nTo address the alternative hypothesis that insider buying is driven solely by general 'good performance' rather than specific listing information, a researcher proposes a difference-in-differences approach. This involves creating a control group of high-performing OTC firms that did *not* apply for listing and comparing their insider trading to the listing firms (the treatment group).\n\nWhich of the following are valid components or predictions of such a refined research design? Select all that apply.", "Options": {"A": "The key test statistic would be the difference in Mean Abnormal Net Purchasers between the listing firms and the control firms (`MANP_Listing - MANP_Control`).", "B": "If the 'good performance' hypothesis were the sole driver of trading, the test statistic (`MANP_Listing - MANP_Control`) would be expected to be statistically indistinguishable from zero.", "C": "The control group should be matched to the treatment group on characteristics like industry, firm size, and especially recent stock performance.", "D": "If the 'good performance' hypothesis were the sole driver of trading, the test statistic (`MANP_Listing - MANP_Control`) would be expected to be significantly negative."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the user's understanding of econometric research design, specifically the logic of a difference-in-differences approach used to solve an identification problem (confounding variables). Depth Strategy: Scenario Application. The user must apply the principles of experimental design to the specific scenario of disentangling two competing hypotheses from the paper. Distractor Logic: Option D is a 'Conceptual Opposite' distractor. If good performance were the only driver, both groups would have similar abnormal buying, leading to a difference near zero, not a negative one. The correct options correctly identify the test statistic (A), the null hypothesis prediction (B), and the proper construction of a control group (C).", "qid": "288", "question": "### Background\n\n**Research Question.** How can a research design be improved to distinguish between insiders trading on specific foreknowledge of a listing versus trading on the firm's general good performance that makes it *eligible* for listing?\n\n**Setting.** A key challenge in the study is this identification problem. The observed pre-listing buying could be a reaction to the firm's strong underlying performance, which is also the reason the firm decides to list.\n\n---\n\nTo address the alternative hypothesis that insider buying is driven solely by general 'good performance' rather than specific listing information, a researcher proposes a difference-in-differences approach. This involves creating a control group of high-performing OTC firms that did *not* apply for listing and comparing their insider trading to the listing firms (the treatment group).\n\nWhich of the following are valid components or predictions of such a refined research design? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 422, "Question": "### Background\n\n**Research Question.** This case examines the paper's challenge to a central tenet of standard Affine Term Structure Models (ATSMs): the idea that the volatility of the yield curve should be fully explained ('spanned') by information within the yield curve itself. The analysis tests whether an external variable—lagged equity market volatility—has incremental predictive power.\n\n### Data / Model Specification\n\nThe full predictive regression model for the volatility of a principal component (`PCk`) is:\n\n  \n\\sigma_{t,t+21}^{PCk} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{PCk} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\nHere, `\\sigma^{ST}` is the lagged log realized stock volatility, and `PrComp_{j,t-1}` are the lagged values of the principal components themselves, which serve as proxies for the state of the yield curve.\n\n**Table 1. Predictive Power of Stock Volatility for Yield Curve Component Volatility (Full Period 1997:10-2013:06)**\n\n| Dependent Variable | `\\hat{\\gamma}_{2}` (t-stat) |\n| :--- | :--- |\n| `\\sigma^{PC1}` (Level Volatility) | 0.222 (5.99) |\n| `\\sigma^{PC2}` (Slope Volatility) | 0.352 (8.36) |\n\n*Note: Results are for the full specification in Eq. (1).*\n\n### Question\n\nBased on the results in Table 1 and the principles of Affine Term Structure Models (ATSMs), which of the following conclusions are supported? Select all that apply.", "Options": {"A": "The evidence indicates that stock market volatility primarily affects the level of the yield curve (PC1), with no significant spillover to the yield curve's slope (PC2).", "B": "The statistical significance of `\\hat{\\gamma}_{2}` for an external variable (`\\sigma^{ST}`) after controlling for yield curve factors (`PrComp`) violates the 'spanning condition' implied by standard ATSMs.", "C": "The fact that stock volatility predicts volatility in two orthogonal components (level and slope) suggests that equity market risk is a pervasive, systemic factor for the bond market, not a narrow one.", "D": "The results confirm the core prediction of standard ATSMs that the bond market is informationally self-contained, with its volatility fully explained by its own past state."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the paper's main theoretical challenge to Affine Term Structure Models. It uses an Atomic Decomposition strategy, converting the first two questions of the source QA into distinct, verifiable statements. Correct option A defines the violation of the spanning condition. Correct option B provides the economic interpretation of the effect on orthogonal components. Distractor C is a direct conceptual opposite of the paper's conclusion. Distractor D is factually incorrect according to Table 1.", "qid": "422", "question": "### Background\n\n**Research Question.** This case examines the paper's challenge to a central tenet of standard Affine Term Structure Models (ATSMs): the idea that the volatility of the yield curve should be fully explained ('spanned') by information within the yield curve itself. The analysis tests whether an external variable—lagged equity market volatility—has incremental predictive power.\n\n### Data / Model Specification\n\nThe full predictive regression model for the volatility of a principal component (`PCk`) is:\n\n  \n\\sigma_{t,t+21}^{PCk} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{PCk} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\nHere, `\\sigma^{ST}` is the lagged log realized stock volatility, and `PrComp_{j,t-1}` are the lagged values of the principal components themselves, which serve as proxies for the state of the yield curve.\n\n**Table 1. Predictive Power of Stock Volatility for Yield Curve Component Volatility (Full Period 1997:10-2013:06)**\n\n| Dependent Variable | `\\hat{\\gamma}_{2}` (t-stat) |\n| :--- | :--- |\n| `\\sigma^{PC1}` (Level Volatility) | 0.222 (5.99) |\n| `\\sigma^{PC2}` (Slope Volatility) | 0.352 (8.36) |\n\n*Note: Results are for the full specification in Eq. (1).*\n\n### Question\n\nBased on the results in Table 1 and the principles of Affine Term Structure Models (ATSMs), which of the following conclusions are supported? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 99, "Question": "### Background\n\n**Research Question.** What is the theoretical and econometric basis for modeling long memory in financial volatility, and how can standard GARCH models be generalized to incorporate both long-run persistence and asymmetric news impacts?\n\n**Setting.** This problem explores the theoretical underpinnings of long memory, its estimation, and its incorporation into parametric volatility models like FIGARCH and FIEGARCH.\n\n**Variables and Parameters.**\n*   `ρ(j)`: The autocorrelation function (ACF) at lag `j`.\n*   `d`: The long-memory parameter, or degree of fractional integration.\n*   `I(ω_j)`: The sample periodogram at Fourier frequency `ω_j`.\n*   `β_1`: The slope coefficient in a log-periodogram regression.\n*   `σ_t^2`: Conditional variance at time `t`.\n*   `ε_t`: Innovation (return shock) at time `t`.\n*   `ξ_t`: Standardized innovation, `ξ_t = ε_t / σ_t`.\n*   `θ`, `γ`: Parameters governing the asymmetry of news impact.\n\n---\n\n### Data / Model Specification\n\nA weakly stationary process has long memory if its ACF decays hyperbolically:\n\n  \n\\rho(j) \\sim C j^{2d-1} \\quad \\text{as } j \\rightarrow \\infty, \\text{ with } 0 < d < \\frac{1}{2} \\quad \\text{(Eq. (1))}\n \n\nThe Geweke and Porter-Hudak (GPH) method estimates `d` via the regression:\n\n  \n\\log[I(\\omega_{j})] = \\beta_{0} + \\beta_{1}\\log(\\omega_{j}) + U_{j} \\quad \\text{(Eq. (2))}\n \n\nThe standard GARCH(p,q) process can be written as an ARMA process for squared innovations `ε_t^2`:\n\n  \n\\{1-\\alpha(L)-\\beta(L)\\}\\varepsilon_{t}^{2} = \\omega + \\{1-\\beta(L)\\}\\nu_{t} \\quad \\text{(Eq. (3))}\n \n\nwhere `ν_t = ε_t^2 - σ_t^2`. The FIGARCH model generalizes this by introducing the fractional differencing operator `(1-L)^d`. The FIEGARCH model further generalizes to allow for asymmetry via a news impact function `g(ξ_t)`:\n\n  \ng(\\xi_{t}) = \\theta\\xi_{t} + \\gamma(|\\xi_{t}| - E|\\xi_{t}|) \\quad \\text{(Eq. (4))}\n \n\n---\n\nConsider the FIEGARCH model's news impact function `g(ξ_t)` in Eq. (4). Assuming a typical leverage effect where `γ > 0`, which of the following statements are correct interpretations of the model? Select all that apply.", "Options": {"A": "The presence of a significant leverage effect (`γ > 0`) is a substitute for the long-memory parameter `d`, meaning if one is present, the other is not needed.", "B": "The model implies that the elevated volatility caused by a large negative shock will be highly persistent, decaying at a hyperbolic rate determined by `d`.", "C": "The impact of a negative shock (`ξ_t < 0`) on log-variance has a slope of `θ - γ`.", "D": "The impact of a positive shock (`ξ_t > 0`) on log-variance has a slope of `θ - γ`."}, "Answer": ["B", "C"], "pi_justification": "This item assesses both the mechanical derivation of the FIEGARCH news impact slopes and the conceptual synthesis of combining asymmetry and persistence. Option A is a correct derivation. Option B correctly interprets the combined economic meaning of `d > 0` and `γ > 0`. Distractor C is a Sign Error in the derivation for positive shocks. Distractor D presents a common Conceptual Confusion, suggesting the two effects are substitutes when they are distinct, complementary features of volatility dynamics.", "qid": "99", "question": "### Background\n\n**Research Question.** What is the theoretical and econometric basis for modeling long memory in financial volatility, and how can standard GARCH models be generalized to incorporate both long-run persistence and asymmetric news impacts?\n\n**Setting.** This problem explores the theoretical underpinnings of long memory, its estimation, and its incorporation into parametric volatility models like FIGARCH and FIEGARCH.\n\n**Variables and Parameters.**\n*   `ρ(j)`: The autocorrelation function (ACF) at lag `j`.\n*   `d`: The long-memory parameter, or degree of fractional integration.\n*   `I(ω_j)`: The sample periodogram at Fourier frequency `ω_j`.\n*   `β_1`: The slope coefficient in a log-periodogram regression.\n*   `σ_t^2`: Conditional variance at time `t`.\n*   `ε_t`: Innovation (return shock) at time `t`.\n*   `ξ_t`: Standardized innovation, `ξ_t = ε_t / σ_t`.\n*   `θ`, `γ`: Parameters governing the asymmetry of news impact.\n\n---\n\n### Data / Model Specification\n\nA weakly stationary process has long memory if its ACF decays hyperbolically:\n\n  \n\\rho(j) \\sim C j^{2d-1} \\quad \\text{as } j \\rightarrow \\infty, \\text{ with } 0 < d < \\frac{1}{2} \\quad \\text{(Eq. (1))}\n \n\nThe Geweke and Porter-Hudak (GPH) method estimates `d` via the regression:\n\n  \n\\log[I(\\omega_{j})] = \\beta_{0} + \\beta_{1}\\log(\\omega_{j}) + U_{j} \\quad \\text{(Eq. (2))}\n \n\nThe standard GARCH(p,q) process can be written as an ARMA process for squared innovations `ε_t^2`:\n\n  \n\\{1-\\alpha(L)-\\beta(L)\\}\\varepsilon_{t}^{2} = \\omega + \\{1-\\beta(L)\\}\\nu_{t} \\quad \\text{(Eq. (3))}\n \n\nwhere `ν_t = ε_t^2 - σ_t^2`. The FIGARCH model generalizes this by introducing the fractional differencing operator `(1-L)^d`. The FIEGARCH model further generalizes to allow for asymmetry via a news impact function `g(ξ_t)`:\n\n  \ng(\\xi_{t}) = \\theta\\xi_{t} + \\gamma(|\\xi_{t}| - E|\\xi_{t}|) \\quad \\text{(Eq. (4))}\n \n\n---\n\nConsider the FIEGARCH model's news impact function `g(ξ_t)` in Eq. (4). Assuming a typical leverage effect where `γ > 0`, which of the following statements are correct interpretations of the model? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n\nB,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 448, "Question": "### Background\n\n**Research Question.** This case examines whether the behavior of hedgers and speculators in commodity markets is stable over time or if it shifts between distinct regimes. It investigates the econometric methods used to model such shifts and explores the implications for market dynamics.\n\n**Setting.** A model for futures returns is extended to a two-state Markov-Switching (MS) framework. The parameters governing the influence of hedging and speculative risk factors, as well as speculator risk preferences, are allowed to differ depending on an unobserved, latent market state, `s_t`, which captures shifts between market regimes.\n\n### Data / Model Specification\n\nThe Markov-Switching model for futures returns is:\n  \nr_{f,t}=e_{0s_{t}}-(b_{s_{t}}^{H}/d_{s_{t}}^{S})\\sigma_{r_H,t}^2+(e_{s_{t}}^{S}/d_{s_{t}}^{S})h_{r_{f},t}^{2}+c_{1s_{t}}r_{f,t-1}+u_{r_{f},s_{t}t} \n \nwhere `\\sigma_{r_H,t}^2` is the variance of the optimally hedged portfolio, `h_{r_f,t}^2` is the conditional variance of futures returns, and `u_{r_{f},s_t,t} \\sim N(0, \\sigma_{s_t}^2)`. The unobserved state `s_t` follows a first-order Markov chain.\n\nSpeculator demand is modeled such that the sign of the parameter `e^S` reflects risk preferences: `e^S > 0` for risk-averse speculators and `e^S < 0` for risk-seeking speculators. The parameter `d^S` is assumed to be positive.\n\n**Table 1. Identification and Estimation of Market Regimes**\n\n| Panel A: Correlation of Regime 1 Probability with Market Variables ||\n|:---|:---:|\n| Correlated Variable | Silver |\n| `r_{f,t}` (Futures Return) | 0.114 |\n| `\\sigma_{r_{f,t}}` (Return Std. Dev.) | -0.718 |\n\n| Panel B: Markov-Switching Estimates for Copper and Soybeans |||||\n| Parameter | Copper (St=1) | Copper (St=2) | Soybeans (St=1) | Soybeans (St=2) |\n| `P(st != st-1)` (Transition Prob.) | 0.021 | 0.056 | 0.034 | 0.084 |\n| `(e_{s_t}^S/d_{s_t}^S)` | 2.979 | 11.315 | 4.368 | -8.901 |\n\n*Source: Adapted from Tables 6 and 7 in the source paper. State 1 is identified as the low-volatility state and State 2 as the high-volatility state.*\n\n### Question\n\nBased on the data and model specification provided, select all of the following statements that are correct interpretations of the results.", "Options": {"A": "The estimates for Copper in Panel B show that speculators are risk-averse in the low-volatility state but become risk-seeking in the high-volatility state.", "B": "The estimates for Soybeans in Panel B indicate that speculators switch from risk-averse behavior in the low-volatility state to risk-seeking behavior in the high-volatility state.", "C": "For Silver, the correlations in Panel A suggest that Regime 1 is best characterized as a 'bearish, low-volatility' state.", "D": "For Copper, the transition probabilities in Panel B imply that the expected duration of the high-volatility regime (State 2) is approximately 18 working days."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the 'atomic decomposition' strategy to convert a multi-part QA problem into a single, high-fidelity multi-select question. It tests three distinct skills: interpreting correlation coefficients for regime identification (Option A), interpreting model parameters for state-dependent behavior (Options B & D), and performing a simple calculation based on transition probabilities (Option C). This makes it a 'Computational Judgment' item. Distractors are designed based on common errors: A misinterprets the sign of the return correlation (Conceptual Opposite); D misinterprets the sign of the coefficient for Copper in State 2, which remains positive (Almost Right).", "qid": "448", "question": "### Background\n\n**Research Question.** This case examines whether the behavior of hedgers and speculators in commodity markets is stable over time or if it shifts between distinct regimes. It investigates the econometric methods used to model such shifts and explores the implications for market dynamics.\n\n**Setting.** A model for futures returns is extended to a two-state Markov-Switching (MS) framework. The parameters governing the influence of hedging and speculative risk factors, as well as speculator risk preferences, are allowed to differ depending on an unobserved, latent market state, `s_t`, which captures shifts between market regimes.\n\n### Data / Model Specification\n\nThe Markov-Switching model for futures returns is:\n  \nr_{f,t}=e_{0s_{t}}-(b_{s_{t}}^{H}/d_{s_{t}}^{S})\\sigma_{r_H,t}^2+(e_{s_{t}}^{S}/d_{s_{t}}^{S})h_{r_{f},t}^{2}+c_{1s_{t}}r_{f,t-1}+u_{r_{f},s_{t}t} \n \nwhere `\\sigma_{r_H,t}^2` is the variance of the optimally hedged portfolio, `h_{r_f,t}^2` is the conditional variance of futures returns, and `u_{r_{f},s_t,t} \\sim N(0, \\sigma_{s_t}^2)`. The unobserved state `s_t` follows a first-order Markov chain.\n\nSpeculator demand is modeled such that the sign of the parameter `e^S` reflects risk preferences: `e^S > 0` for risk-averse speculators and `e^S < 0` for risk-seeking speculators. The parameter `d^S` is assumed to be positive.\n\n**Table 1. Identification and Estimation of Market Regimes**\n\n| Panel A: Correlation of Regime 1 Probability with Market Variables ||\n|:---|:---:|\n| Correlated Variable | Silver |\n| `r_{f,t}` (Futures Return) | 0.114 |\n| `\\sigma_{r_{f,t}}` (Return Std. Dev.) | -0.718 |\n\n| Panel B: Markov-Switching Estimates for Copper and Soybeans |||||\n| Parameter | Copper (St=1) | Copper (St=2) | Soybeans (St=1) | Soybeans (St=2) |\n| `P(st != st-1)` (Transition Prob.) | 0.021 | 0.056 | 0.034 | 0.084 |\n| `(e_{s_t}^S/d_{s_t}^S)` | 2.979 | 11.315 | 4.368 | -8.901 |\n\n*Source: Adapted from Tables 6 and 7 in the source paper. State 1 is identified as the low-volatility state and State 2 as the high-volatility state.*\n\n### Question\n\nBased on the data and model specification provided, select all of the following statements that are correct interpretations of the results.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 258, "Question": "### Background\n\n**Research Question.** What is the empirical journey from a raw correlation to a final, causal estimate of the net economic impact of M&A rumors on deal outcomes?\n\n**Setting / Data-Generating Environment.** The analysis progresses through several stages: (1) examining raw descriptive data, (2) estimating a naive model that ignores endogeneity and selection bias, and (3) estimating a full structural model using Indirect Inference (II) to correct for these biases.\n\n**Variables & Parameters.**\n- `Closing`: Binary variable; 1 if a deal is completed, 0 otherwise.\n- `Ln(Price)`: The natural logarithm of the deal's transaction multiple (e.g., EV/Sales).\n- `Leak`: Binary variable; 1 if a deal was subject to a pre-announcement rumor, 0 otherwise.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Tabulation of Deal Closings and Rumors**\n\n| | Not Closed | Closed | Totals |\n| :--- | :--- | :--- | :--- |\n| **Not Leaked** | 11,977 | 38,627 | 50,604 |\n| **Leaked** | 11,377 | 6,063 | 17,440 |\n| **Totals** | 23,354 | 44,690 | 68,044 |\n\n**Table 2: Naive Probit Model for Deal Completion, Pr(Closing = 1)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.904*** |\n| | (0.019) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 7, Specification (3). `***` denotes p < 0.01.*\n\n**Table 3: Naive OLS Model for Deal Value, Ln(Price)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.066*** |\n| | (0.005) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 8, estimated on observed prices only. `***` denotes p < 0.01.*\n\n---\n\n### Question\n\nBased on the provided raw data and naive model estimates, select all of the following statements that are correct.", "Options": {"A": "The naive OLS model in Table 3 suggests that, for deals with observed prices, a rumor is associated with a 6.6% lower transaction value.", "B": "The naive probit model in Table 2 implies that a rumor causes a 90.4 percentage point decrease in the probability of deal completion.", "C": "The naive models (Table 2 and Table 3) collectively suggest that rumors increase deal value but decrease the probability of closing.", "D": "According to the raw data in Table 1, rumored deals are approximately 41.5 percentage points less likely to close than non-rumored deals."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret both raw descriptive data and naive regression outputs, which is the first step in the paper's empirical narrative. It uses an 'Atomic Decomposition' strategy, breaking down the initial findings into distinct, verifiable statements. Distractor C tests for the common error of misinterpreting a probit coefficient as a marginal effect (Formula Misuse). Distractor D presents a 'Conceptual Opposite' of the naive price finding to test careful reading of the tables.", "qid": "258", "question": "### Background\n\n**Research Question.** What is the empirical journey from a raw correlation to a final, causal estimate of the net economic impact of M&A rumors on deal outcomes?\n\n**Setting / Data-Generating Environment.** The analysis progresses through several stages: (1) examining raw descriptive data, (2) estimating a naive model that ignores endogeneity and selection bias, and (3) estimating a full structural model using Indirect Inference (II) to correct for these biases.\n\n**Variables & Parameters.**\n- `Closing`: Binary variable; 1 if a deal is completed, 0 otherwise.\n- `Ln(Price)`: The natural logarithm of the deal's transaction multiple (e.g., EV/Sales).\n- `Leak`: Binary variable; 1 if a deal was subject to a pre-announcement rumor, 0 otherwise.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Tabulation of Deal Closings and Rumors**\n\n| | Not Closed | Closed | Totals |\n| :--- | :--- | :--- | :--- |\n| **Not Leaked** | 11,977 | 38,627 | 50,604 |\n| **Leaked** | 11,377 | 6,063 | 17,440 |\n| **Totals** | 23,354 | 44,690 | 68,044 |\n\n**Table 2: Naive Probit Model for Deal Completion, Pr(Closing = 1)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.904*** |\n| | (0.019) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 7, Specification (3). `***` denotes p < 0.01.*\n\n**Table 3: Naive OLS Model for Deal Value, Ln(Price)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.066*** |\n| | (0.005) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 8, estimated on observed prices only. `***` denotes p < 0.01.*\n\n---\n\n### Question\n\nBased on the provided raw data and naive model estimates, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: import numpy as np\nfrom scipy.stats import norm\n\n#...\n\n\nA,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 451, "Question": "Background\n\nStandard risk models, such as 'Normal VaR', assume that asset returns follow a normal distribution. A normal distribution is symmetric (skewness = 0) and has a kurtosis of 3.\n\nData / Model Specification\n\nThe paper analyzes daily returns for Canadian crude oil and reports two key statistical properties:\n1.  The sample kurtosis is 16.2.\n2.  The return distribution 'skews to the left' (i.e., has negative skewness).\n\n---\n\nQuestion\n\nGiven these two empirical findings, which of the following are valid critiques of using a standard Normal VaR model for this market? (Select all that apply)", "Options": {"A": "The model would systematically overestimate the average daily return of crude oil.", "B": "The model would be overly conservative, systematically overstating the potential for extreme losses.", "C": "The model would fail to capture the asymmetric nature of risk, where large losses are more likely or more severe than large gains.", "D": "The model would underestimate the frequency and magnitude of extreme price movements because the actual distribution has 'fat tails' (leptokurtosis)."}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: Tests the ability to interpret kurtosis and skewness and critique a model based on violations of its assumptions. Strategy: Select-all-valid consequences. Distractor Logic: (A) Correct: Kurtosis > 3 implies fat tails (leptokurtosis), meaning a normal model underestimates extreme events. (B) Correct: Negative skewness means the left (loss) tail is fatter/longer than the right, an asymmetry a normal model ignores. (C) Conceptual Confusion: Incorrectly links higher-order moments (kurtosis, skew) to the first moment (mean/average return). (D) Conceptual Opposite: The normal model would be insufficiently conservative, underestimating risk, not overestimating it.", "qid": "451", "question": "Background\n\nStandard risk models, such as 'Normal VaR', assume that asset returns follow a normal distribution. A normal distribution is symmetric (skewness = 0) and has a kurtosis of 3.\n\nData / Model Specification\n\nThe paper analyzes daily returns for Canadian crude oil and reports two key statistical properties:\n1.  The sample kurtosis is 16.2.\n2.  The return distribution 'skews to the left' (i.e., has negative skewness).\n\n---\n\nQuestion\n\nGiven these two empirical findings, which of the following are valid critiques of using a standard Normal VaR model for this market? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 440, "Question": "### Background\n\nA large investor with CRRA utility (`R>1`) operates in a two-state market (good state 0, bad state 1). The investor's portfolio `\\pi` and consumption `c` choices influence the intensity of switching between states via a floored affine function:\n  \n\\vartheta^{i,1-i}(\\pi,c) = \\max\\{A^{i}+\\pi^{\\top}B_{\\pi}^{i}+B_{c}^{i}c,C^{i}\\} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Data / Model Specification\n\nConsider the specific case where the investor gains no direct utility from intermediate consumption (`\\varepsilon=0`). However, consumption can still influence the market regime. Assume that in the good state (`i=0`), consumption reduces the intensity of switching to the bad state (i.e., `B_c^0 < 0`).\n\n---\n\n### Question\n\nUnder these circumstances, it can be optimal for the investor to consume a positive amount of wealth, a behavior interpreted as 'market manipulation'. Select all of the following conditions that must be met for this seemingly paradoxical positive consumption to be optimal.", "Options": {"A": "The marginal benefit of consumption on reducing the switching intensity is sufficiently high (i.e., `B_c^0` is sufficiently negative).", "B": "The investor's current portfolio and consumption choices do not already result in the minimum possible switching intensity, `C^0`.", "C": "The perceived difference in value between the good and bad market regimes is sufficiently large.", "D": "The investor's coefficient of relative risk aversion, `R`, is greater than 1."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: Tests the understanding of the specific conditions required for the paper's novel 'consumption as manipulation' result. Strategy: Scenario Application. The user must apply the theory to identify the set of necessary conditions. Distractor Logic: Options A, B, and D are the three necessary conditions outlined in the paper's analysis. Option C is True but Irrelevant; `R>1` is a standing assumption of the model setup, not a specific trigger condition for this particular effect.", "qid": "440", "question": "### Background\n\nA large investor with CRRA utility (`R>1`) operates in a two-state market (good state 0, bad state 1). The investor's portfolio `\\pi` and consumption `c` choices influence the intensity of switching between states via a floored affine function:\n  \n\\vartheta^{i,1-i}(\\pi,c) = \\max\\{A^{i}+\\pi^{\\top}B_{\\pi}^{i}+B_{c}^{i}c,C^{i}\\} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Data / Model Specification\n\nConsider the specific case where the investor gains no direct utility from intermediate consumption (`\\varepsilon=0`). However, consumption can still influence the market regime. Assume that in the good state (`i=0`), consumption reduces the intensity of switching to the bad state (i.e., `B_c^0 < 0`).\n\n---\n\n### Question\n\nUnder these circumstances, it can be optimal for the investor to consume a positive amount of wealth, a behavior interpreted as 'market manipulation'. Select all of the following conditions that must be met for this seemingly paradoxical positive consumption to be optimal.", "model_evaluation_result": {"model_raw_response": "\"A,B,C\"", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 463, "Question": "### Background\n\n**Research Question.** How can we test for 'leakage' in global climate policy, where divestment from carbon-intensive firms by investors in one region is offset by increased investment from another?\n\n**Setting.** A study finds that after the Paris Agreement (PA), overall investor participation in High-Carbon (HC) firms decreased. However, the researchers note that \"investors from BRIC countries increased their participation in HC firms,\" while anecdotal evidence suggests EU investors decreased their participation.\n\n**Data / Model Specification.**\nThe original model was:\n\n  \nY_{it} = \\alpha_i + \\delta_t + \\beta_1 (HC_i \\times PostPA_t) + \\dots + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nwhere `Y_{it}` is total institutional ownership in firm `i`.\n\n---\n\n### Question\n\nTo formally test the 'leakage' hypothesis, which of the following are valid and necessary modifications to the research design? Select ALL that apply.", "Options": {"A": "Disaggregate the dependent variable by investor domicile, creating separate ownership variables such as `Y_{it,EU}` (ownership by EU investors) and `Y_{it,BRIC}` (ownership by BRIC investors).", "B": "Run separate regressions for subsamples of investors (e.g., one for EU investors, one for BRIC investors) and test if the coefficient on `HC x PostPA` is negative for the EU group and positive for the BRIC group.", "C": "Add firm fixed effects for BRIC-domiciled firms and EU-domiciled firms to the original model to control for regional differences.", "D": "Add a new interaction term, `BRIC_i × PostPA_t`, to the original regression, where `BRIC_i` is a dummy for firms domiciled in BRIC countries."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item assesses the ability to extend an econometric model to test a more nuanced hypothesis. It uses a Premise/Assumption Packaging strategy, asking for the correct modeling steps. Distractors (B and D) represent common but incorrect specifications that misidentify the source of heterogeneity (investor vs. firm), making them high-fidelity 'Formula Misuse' errors.", "qid": "463", "question": "### Background\n\n**Research Question.** How can we test for 'leakage' in global climate policy, where divestment from carbon-intensive firms by investors in one region is offset by increased investment from another?\n\n**Setting.** A study finds that after the Paris Agreement (PA), overall investor participation in High-Carbon (HC) firms decreased. However, the researchers note that \"investors from BRIC countries increased their participation in HC firms,\" while anecdotal evidence suggests EU investors decreased their participation.\n\n**Data / Model Specification.**\nThe original model was:\n\n  \nY_{it} = \\alpha_i + \\delta_t + \\beta_1 (HC_i \\times PostPA_t) + \\dots + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nwhere `Y_{it}` is total institutional ownership in firm `i`.\n\n---\n\n### Question\n\nTo formally test the 'leakage' hypothesis, which of the following are valid and necessary modifications to the research design? Select ALL that apply.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 443, "Question": "### Background\n\n**Research Question.** How does an investor optimally liquidate inventory when market liquidity switches between high and low states?\n\n**Setting.** The market liquidity, parameterized by `\\lambda`, follows a two-state Markov chain: an \"active\" state (0, high `\\lambda_0`) and a \"slow\" state (1, low `\\lambda_1`), with `\\lambda_0 > \\lambda_1`. The investor observes the current state and adjusts their strategy. The analysis is in the continuous, infinite-horizon fluid limit for a power-law LOB where `\\Lambda(s) = \\lambda s^{-\\alpha}`.\n\n**Result.** A key finding is that the optimal spread is *higher* in the active (high liquidity) state than in the slow state. This seems paradoxical.\n\n---\n\n### Question\n\nSelect all of the following statements that provide a valid economic justification for why the investor optimally posts a higher, less aggressive spread precisely when the market is most liquid (the active state).", "Options": {"A": "The continuation value of holding inventory is higher in the active state, as the potential for future high-rate revenue generation is greater.", "B": "The higher spread is needed to offset the greater risk of the market switching from the active to the slow state.", "C": "The investor becomes more risk-averse in the active state and therefore demands a higher premium for each share sold.", "D": "Selling a share in the active state means forfeiting a valuable option to exploit favorable market conditions, thus requiring higher immediate compensation (a higher spread)."}, "Answer": ["A", "D"], "pi_justification": "REPLACE with Multiple Choice (Score: 9.0). This item assesses the deep economic reasoning behind the paradoxical result from the regime-switching model. It uses a Premise Packaging strategy, asking for the components of the correct 'option value of liquidity' argument. Distractor C is a 'True but Irrelevant' or 'Conceptual Misattribution' error, as risk aversion is not the primary driver in this model. Distractor D is a plausible but incorrect causal link, misinterpreting the role of the transition rates.", "qid": "443", "question": "### Background\n\n**Research Question.** How does an investor optimally liquidate inventory when market liquidity switches between high and low states?\n\n**Setting.** The market liquidity, parameterized by `\\lambda`, follows a two-state Markov chain: an \"active\" state (0, high `\\lambda_0`) and a \"slow\" state (1, low `\\lambda_1`), with `\\lambda_0 > \\lambda_1`. The investor observes the current state and adjusts their strategy. The analysis is in the continuous, infinite-horizon fluid limit for a power-law LOB where `\\Lambda(s) = \\lambda s^{-\\alpha}`.\n\n**Result.** A key finding is that the optimal spread is *higher* in the active (high liquidity) state than in the slow state. This seems paradoxical.\n\n---\n\n### Question\n\nSelect all of the following statements that provide a valid economic justification for why the investor optimally posts a higher, less aggressive spread precisely when the market is most liquid (the active state).", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 437, "Question": "### Background\n\n**Research Question.** After establishing a robust method for measuring currency exposure, the paper investigates how this exposure varies across different types of firms. The analysis explores heterogeneity based on firms' international activities and their industry sector, revealing several counter-intuitive and economically significant patterns.\n\n**Setting / Data-Generating Environment.** The analysis uses firm-level exchange rate exposure coefficients (`\\beta_{is}`) estimated from the third stage of the orthogonalized model for firms in the Euro Stoxx TMI index (1999-2011). The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Table 1. Exchange Rate Exposure (`\\beta_{is}`) by Firm Characteristics**\n\n| | Avg. `\\beta_{is}` (significant firms) | Total Significant (%) |\n|:---|:---:|:---:|\n| **Panel A: Overall Results** | | |\n| JPY | 0.82 | 27.60% |\n| USD | 0.68 | 13.04% |\n| **Panel B: By International Involvement (for USD Shocks)** | | |\n| MNCs (High Exporters) | 0.47 | 10% |\n| Domestic Firms | 0.76 | 13% |\n| **Panel C: By Industry (for CHF Shocks)** | | |\n| Financials | 1.90 | 72% |\n| Non-Financials | 1.88 | 49% |\n\n*Notes: MNCs are Multinational Corporations. `\\beta_{is}` is the average of significant coefficients.* \n\n### Question\n\nBased on the results in Table 1, select all statements that correctly link an empirical finding to its proposed theoretical explanation in the paper.", "Options": {"A": "The finding in Panel B that MNCs have lower average USD exposure (`\\beta_{is}`=0.47) than domestic firms (`\\beta_{is}`=0.76) is explained by the argument that MNCs engage in more effective corporate hedging, reducing their net stock price sensitivity.", "B": "The predominantly positive `\\beta_{is}` coefficients in Panel A suggest that the typical European firm benefits from a stronger Euro, consistent with being a net importer or having net foreign currency liabilities.", "C": "The higher percentage of financial firms with significant CHF exposure (Panel C) is because they are primarily net exporters of financial services, making them vulnerable to a stronger Euro.", "D": "The results in Panel B imply that domestic firms have greater underlying gross economic exposure to currency fluctuations than MNCs."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition and Reverse-Reasoning strategies. It tests the ability to connect distinct empirical patterns from the table with their corresponding theoretical explanations (net importer hypothesis, corporate hedging). Distractors are designed using a Conceptual Opposite explanation (C) and a subtle misinterpretation of the hedging argument (D).", "qid": "437", "question": "### Background\n\n**Research Question.** After establishing a robust method for measuring currency exposure, the paper investigates how this exposure varies across different types of firms. The analysis explores heterogeneity based on firms' international activities and their industry sector, revealing several counter-intuitive and economically significant patterns.\n\n**Setting / Data-Generating Environment.** The analysis uses firm-level exchange rate exposure coefficients (`\\beta_{is}`) estimated from the third stage of the orthogonalized model for firms in the Euro Stoxx TMI index (1999-2011). The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Table 1. Exchange Rate Exposure (`\\beta_{is}`) by Firm Characteristics**\n\n| | Avg. `\\beta_{is}` (significant firms) | Total Significant (%) |\n|:---|:---:|:---:|\n| **Panel A: Overall Results** | | |\n| JPY | 0.82 | 27.60% |\n| USD | 0.68 | 13.04% |\n| **Panel B: By International Involvement (for USD Shocks)** | | |\n| MNCs (High Exporters) | 0.47 | 10% |\n| Domestic Firms | 0.76 | 13% |\n| **Panel C: By Industry (for CHF Shocks)** | | |\n| Financials | 1.90 | 72% |\n| Non-Financials | 1.88 | 49% |\n\n*Notes: MNCs are Multinational Corporations. `\\beta_{is}` is the average of significant coefficients.* \n\n### Question\n\nBased on the results in Table 1, select all statements that correctly link an empirical finding to its proposed theoretical explanation in the paper.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 445, "Question": "### Background\n\n**Research Question.** A key empirical challenge is to distinguish the paper's proposed 'Maturity Mismatch' (MM) channel from an alternative 'Demand Uncertainty' (DU) channel. The DU hypothesis posits that capital flow volatility merely proxies for uncertainty about future demand for the country's exports, which would also dampen investment.\n\n**Setting & Data.** To distinguish these channels, the analysis disaggregates total portfolio flows into debt and equity flows for the emerging market sample.\n\n### Data / Model Specification\n\n**Key Empirical Results.**\nThe paper estimates the impact of the volatility of debt flows and equity flows separately on investment. A summary of these results is provided in Table 1.\n\n**Table 1: Disaggregated Regression Results on Investment (Emerging Markets)**\n| Specification | Key Regressor | Estimated Coefficient |\n| :--- | :--- | :--- |\n| 1 | Volatility (Debt Flows) | -3.162*** |\n| 2 | Volatility (Equity Flows) | 0.315 |\n*Note: `***` denotes statistical significance at the 1% level.* \n\n### Question\n\nBased on the competing hypotheses and the results in Table 1, which of the following conclusions are supported?", "Options": {"A": "The positive coefficient on equity flow volatility (0.315), though insignificant, suggests that equity market uncertainty might be beneficial for investment.", "B": "The empirical findings provide strong evidence against the Demand Uncertainty channel being the primary driver, as the volatility of both flow types would be expected to capture such uncertainty.", "C": "The Maturity Mismatch hypothesis predicts that only the volatility of debt flows should negatively impact investment, as equity financing does not create rollover risk.", "D": "The results imply that both financial (debt) and real (demand) uncertainty channels are at play, as the test is unable to definitively separate their effects."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the ability to interpret a 'horse race' regression designed to distinguish between competing causal mechanisms, a core skill in empirical analysis. It uses a Reverse-Reasoning strategy where the student must infer which hypothesis is supported by the evidence. Options A and B correctly state the prediction of the target hypothesis and the conclusion from the evidence. Distractor C (Step-Omission Error) tests for the common error of over-interpreting insignificant coefficients. Distractor D (Conceptual Opposite) presents a false conclusion of equivalence between channels, contrary to the differentiating evidence.", "qid": "445", "question": "### Background\n\n**Research Question.** A key empirical challenge is to distinguish the paper's proposed 'Maturity Mismatch' (MM) channel from an alternative 'Demand Uncertainty' (DU) channel. The DU hypothesis posits that capital flow volatility merely proxies for uncertainty about future demand for the country's exports, which would also dampen investment.\n\n**Setting & Data.** To distinguish these channels, the analysis disaggregates total portfolio flows into debt and equity flows for the emerging market sample.\n\n### Data / Model Specification\n\n**Key Empirical Results.**\nThe paper estimates the impact of the volatility of debt flows and equity flows separately on investment. A summary of these results is provided in Table 1.\n\n**Table 1: Disaggregated Regression Results on Investment (Emerging Markets)**\n| Specification | Key Regressor | Estimated Coefficient |\n| :--- | :--- | :--- |\n| 1 | Volatility (Debt Flows) | -3.162*** |\n| 2 | Volatility (Equity Flows) | 0.315 |\n*Note: `***` denotes statistical significance at the 1% level.* \n\n### Question\n\nBased on the competing hypotheses and the results in Table 1, which of the following conclusions are supported?", "model_evaluation_result": {"model_raw_response": "\"A,C\"", "model_answer": ["A", "C"], "error": null}}
{"ID": 447, "Question": "### Background\n\nA central theme in household finance is understanding how consumers choose between fixed-rate mortgages (FRMs) and adjustable-rate mortgages (ARMs). A baseline model proposed by Koijen, Van Hemert, and Van Nieuwerburgh (KHN) posits a simple linear relationship between the aggregate ARM share and the \"Household Decision Rule\" (HDR), a proxy for the long-term bond risk premium. This paper extends that model by hypothesizing that borrowers' sensitivity to the HDR is not constant, but varies over time with their perceived risk. This perceived risk is driven by two main factors: the current level of short-term interest rates and the state of the housing market.\n\n### Data / Model Specification\n\nTo capture this time-varying sensitivity, the coefficients of the baseline model are made conditional on the one-year treasury yield (`short_t`) and a dummy variable for a declining housing market (`down_t`, which equals 1 if the Case-Shiller home price index fell in the previous month, and 0 otherwise). This results in the following full interaction model:\n\n  \nARM_{t}=\\alpha_{0}+\\alpha_{1}short_{t}+\\alpha_{2}down_{t}+\\beta_{0}HDR_{t}+\\beta_{1}(HDR_{t} \\times short_{t}) +\\beta_{2}(HDR_{t} \\times down_{t})+\\varepsilon_{t}\n \nEq. (1)\n\nwhere `ARM_t` is the percentage share of new mortgages that are ARMs. The model is estimated using monthly data from the Federal Housing Finance Agency's Monthly Interest Rate Survey (MIRS) from February 1987 to September 2008. Summary statistics and regression results for this sample are provided in Table 1 and Table 2 below.\n\n**Table 1: Summary Statistics & Correlations (MIRS Sample, 1987-2008)**\n\n| Variable | Mean | Std. Dev. | Min | Max |\n| :--- | :--- | :--- | :--- | :--- |\n| ARM (%) | 25.75 | 13.76 | 4.00 | 69.00 |\n| HDR (%) | 1.11 | 0.98 | -0.86 | 3.73 |\n| short_t (%) | 4.90 | 2.02 | 1.00 | 9.56 |\n| down_t | 0.25 | 0.43 | 0 | 1 |\n\n**Table 2: ARM Choice Model Regression Results (MIRS Sample)**\n\n| Variable | (1) Simple Model | (2) Full Model |\n| :--- | :--- | :--- |\n| Intercept | 15.28*** | 5.13* |\n| | (1.74) | (2.68) |\n| HDR_t | 9.41*** | 5.09** |\n| | (1.54) | (2.55) |\n| short_t | | 2.16** |\n| | | (0.59) |\n| down_t | | -0.30 |\n| | | (2.25) |\n| HDR_t x short_t | | 1.01* |\n| | | (0.53) |\n| HDR_t x down_t | | -5.06** |\n| | | (2.06) |\n| | | |\n| N | 260 | 260 |\n| Adj. R² | 0.448 | 0.736 |\n\n*Notes: Newey-West standard errors (12 lags) in parentheses. Significance codes: *p<0.1; **p<0.05; ***p<0.01.*\n\n### Question\n\nUsing the results from the full model (Table 2) and the mean `short_t` from Table 1, which of the following conclusions about the marginal effect of `HDR_t` on the ARM share are correct? Select all that apply.", "Options": {"A": "In a normal housing market (`down_t` = 0), with the short-term rate at its sample mean, a one-percentage-point increase in `HDR_t` is associated with an approximate 10.04 percentage point increase in the ARM share.", "B": "The analysis shows that the sensitivity of ARM choice to the HDR is more than halved during a declining housing market compared to a normal one (at the mean short-term rate).", "C": "The marginal effect of `HDR_t` on the ARM share is constant at its direct coefficient of 5.09, as this is the primary effect listed in the regression table.", "D": "During a declining housing market (`down_t` = 1), the marginal effect of `HDR_t` on the ARM share increases to approximately 15.10 at the mean short-term rate."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to calculate and interpret a conditional marginal effect from an interaction model, a key quantitative skill. The strategy is Computational Judgment. Correct options (A, B) require performing the calculation correctly for one scenario (A: 5.09 + 1.01*4.90 ≈ 10.04) and then comparing the results across two scenarios (B: 10.04 vs. 10.04 - 5.06 = 4.98). Distractor (C) is a Step-Omission Error, where the student fails to include the interaction terms in the marginal effect calculation. Distractor (D) is a Sign Error, where the student incorrectly adds instead of subtracts the effect of the `down_t` interaction term.", "qid": "447", "question": "### Background\n\nA central theme in household finance is understanding how consumers choose between fixed-rate mortgages (FRMs) and adjustable-rate mortgages (ARMs). A baseline model proposed by Koijen, Van Hemert, and Van Nieuwerburgh (KHN) posits a simple linear relationship between the aggregate ARM share and the \"Household Decision Rule\" (HDR), a proxy for the long-term bond risk premium. This paper extends that model by hypothesizing that borrowers' sensitivity to the HDR is not constant, but varies over time with their perceived risk. This perceived risk is driven by two main factors: the current level of short-term interest rates and the state of the housing market.\n\n### Data / Model Specification\n\nTo capture this time-varying sensitivity, the coefficients of the baseline model are made conditional on the one-year treasury yield (`short_t`) and a dummy variable for a declining housing market (`down_t`, which equals 1 if the Case-Shiller home price index fell in the previous month, and 0 otherwise). This results in the following full interaction model:\n\n  \nARM_{t}=\\alpha_{0}+\\alpha_{1}short_{t}+\\alpha_{2}down_{t}+\\beta_{0}HDR_{t}+\\beta_{1}(HDR_{t} \\times short_{t}) +\\beta_{2}(HDR_{t} \\times down_{t})+\\varepsilon_{t}\n \nEq. (1)\n\nwhere `ARM_t` is the percentage share of new mortgages that are ARMs. The model is estimated using monthly data from the Federal Housing Finance Agency's Monthly Interest Rate Survey (MIRS) from February 1987 to September 2008. Summary statistics and regression results for this sample are provided in Table 1 and Table 2 below.\n\n**Table 1: Summary Statistics & Correlations (MIRS Sample, 1987-2008)**\n\n| Variable | Mean | Std. Dev. | Min | Max |\n| :--- | :--- | :--- | :--- | :--- |\n| ARM (%) | 25.75 | 13.76 | 4.00 | 69.00 |\n| HDR (%) | 1.11 | 0.98 | -0.86 | 3.73 |\n| short_t (%) | 4.90 | 2.02 | 1.00 | 9.56 |\n| down_t | 0.25 | 0.43 | 0 | 1 |\n\n**Table 2: ARM Choice Model Regression Results (MIRS Sample)**\n\n| Variable | (1) Simple Model | (2) Full Model |\n| :--- | :--- | :--- |\n| Intercept | 15.28*** | 5.13* |\n| | (1.74) | (2.68) |\n| HDR_t | 9.41*** | 5.09** |\n| | (1.54) | (2.55) |\n| short_t | | 2.16** |\n| | | (0.59) |\n| down_t | | -0.30 |\n| | | (2.25) |\n| HDR_t x short_t | | 1.01* |\n| | | (0.53) |\n| HDR_t x down_t | | -5.06** |\n| | | (2.06) |\n| | | |\n| N | 260 | 260 |\n| Adj. R² | 0.448 | 0.736 |\n\n*Notes: Newey-West standard errors (12 lags) in parentheses. Significance codes: *p<0.1; **p<0.05; ***p<0.01.*\n\n### Question\n\nUsing the results from the full model (Table 2) and the mean `short_t` from Table 1, which of the following conclusions about the marginal effect of `HDR_t` on the ARM share are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\"A,B\"", "model_answer": ["A", "B"], "error": null}}
{"ID": 293, "Question": "### Background\n\n**Research Question.** How can a tail risk bound be constructed for aggregate claims when the severity distribution is heavy-tailed, possessing only a finite number of moments and no moment generating function?\n\n**Setting / Data-Generating Environment.** We consider a collective risk model where the claim size distribution `F(y)` is potentially heavy-tailed. The exponential Lundberg-type bounds are not applicable. An alternative framework based on Pareto-type bounds is needed.\n\n**Variables & Parameters.**\n- `F(y)`: The CDF of claim sizes.\n- `\\phi`: A parameter of the claim frequency distribution.\n- `r`: A positive real number indicating the highest finite moment of the claim size distribution.\n- `\\kappa`: A parameter determining the scale of the Pareto bound.\n\n---\n\n### Data / Model Specification\n\nThis approach uses a Pareto-type function for the bound, `\\overline{B}(x) = (1+\\kappa x)^{-r}`. The parameter `\\kappa > 0` is chosen to satisfy the integral constraint:\n  \n\\phi^{-1} = \\int_{0}^{\\infty} (1+\\kappa y)^{r} \\mathrm{d}F(y) \\quad \\text{(Eq. (1))}\n \nThe resulting Pareto bound on the aggregate claims tail is:\n  \n\\overline{G}(x) \\leq \\phi^{-1}(1-p_{0})(1+\\kappa x)^{-r} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the paper's framework for Pareto bounds, select all of the following statements that are correct.", "Options": {"A": "For the case where `r=2`, the parameter `\\kappa` is given by the positive root of the quadratic equation `(E[Y^2])\\kappa^2 + (2E[Y])\\kappa + (1 - \\phi^{-1}) = 0`.", "B": "The Pareto bound is suitable for heavy-tailed claim distributions that may lack a moment generating function, whereas the classical exponential bound requires the existence of one.", "C": "For the case where `r=2`, the parameter `\\kappa` is given by the positive root of the linear equation `(2E[Y])\\kappa + (1 - \\phi^{-1}) = 0`.", "D": "For extreme events (large `x`), the Pareto bound `(1+\\kappa x)^{-r}` decays faster than the exponential bound `e^{-\\kappa x}`, implying lower capital requirements for heavy-tailed risks."}, "Answer": ["A", "B"], "pi_justification": "This item assesses understanding of the conceptual basis and parameter derivation for the Pareto bound framework, using an Atomic Decomposition strategy. Correct option (A) accurately contrasts the assumptions of Pareto and exponential bounds. Correct option (B) correctly identifies the quadratic equation for `\\kappa` when `r=2`. Distractor (C) makes a false claim about the relative decay rates and capital implications (Conceptual Opposite). Distractor (D) presents an incorrect equation for `\\kappa` that would arise from ignoring the `y^2` term in the expansion (Step-Omission Error).", "qid": "293", "question": "### Background\n\n**Research Question.** How can a tail risk bound be constructed for aggregate claims when the severity distribution is heavy-tailed, possessing only a finite number of moments and no moment generating function?\n\n**Setting / Data-Generating Environment.** We consider a collective risk model where the claim size distribution `F(y)` is potentially heavy-tailed. The exponential Lundberg-type bounds are not applicable. An alternative framework based on Pareto-type bounds is needed.\n\n**Variables & Parameters.**\n- `F(y)`: The CDF of claim sizes.\n- `\\phi`: A parameter of the claim frequency distribution.\n- `r`: A positive real number indicating the highest finite moment of the claim size distribution.\n- `\\kappa`: A parameter determining the scale of the Pareto bound.\n\n---\n\n### Data / Model Specification\n\nThis approach uses a Pareto-type function for the bound, `\\overline{B}(x) = (1+\\kappa x)^{-r}`. The parameter `\\kappa > 0` is chosen to satisfy the integral constraint:\n  \n\\phi^{-1} = \\int_{0}^{\\infty} (1+\\kappa y)^{r} \\mathrm{d}F(y) \\quad \\text{(Eq. (1))}\n \nThe resulting Pareto bound on the aggregate claims tail is:\n  \n\\overline{G}(x) \\leq \\phi^{-1}(1-p_{0})(1+\\kappa x)^{-r} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the paper's framework for Pareto bounds, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 323, "Question": "### Background\n\n**Research Question.** This case explores the formal modeling of dynamic relationships between cointegrated time series, building to the Vector Error Correction (VEC) model.\n\n**Setting.** Consider a bivariate system of two log stock price indices, `P_t = [X_t, Y_t]'`, which are I(1) and share a single cointegrating relationship.\n\n---\n\n### Data / Model Specification\n\nA Vector Autoregression (VAR) of I(1) variables can be written in the Vector Error Correction (VEC) form:\n\n  \n\\Delta P_t = \\Pi P_{t-1} + \\sum_{i=1}^{k-1} \\Gamma_i \\Delta P_{t-i} + \\epsilon_t\n\\quad \\text{(Eq. (1))}\n \n\nThe Granger Representation Theorem states that the long-run impact matrix `Π` can be factored as `Π = αβ'`, where `α` contains adjustment coefficients and `β` is the cointegrating vector.\n\n---\n\n### Question\n\nSelect all statements that are TRUE regarding the VEC model in **Eq. (1)** and its relationship to standard Granger causality tests for cointegrated I(1) series.", "Options": {"A": "The `α` vector defines the long-run equilibrium relationship, while the `β` vector measures the speed of adjustment to equilibrium.", "B": "A non-zero element in the `α` vector for a given variable is evidence of long-run causality, as it shows the variable adjusts to past equilibrium errors.", "C": "The `β` vector defines the long-run equilibrium relationship between the levels of the series in `P_t`.", "D": "A standard Granger causality test on the levels of the series in `P_t` is misspecified because it omits the error correction term, `αβ'P_{t-1}`."}, "Answer": ["B", "C", "D"], "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.5). This item assesses the conceptual understanding of the VEC model's components and its superiority over simpler models for cointegrated series. Options A and B are correct definitions of `β` and `α`. Option D correctly identifies the misspecification of a standard Granger causality test in levels, a key point in the original QA. Distractor C represents the most common conceptual error: swapping the roles of `α` and `β`.", "qid": "323", "question": "### Background\n\n**Research Question.** This case explores the formal modeling of dynamic relationships between cointegrated time series, building to the Vector Error Correction (VEC) model.\n\n**Setting.** Consider a bivariate system of two log stock price indices, `P_t = [X_t, Y_t]'`, which are I(1) and share a single cointegrating relationship.\n\n---\n\n### Data / Model Specification\n\nA Vector Autoregression (VAR) of I(1) variables can be written in the Vector Error Correction (VEC) form:\n\n  \n\\Delta P_t = \\Pi P_{t-1} + \\sum_{i=1}^{k-1} \\Gamma_i \\Delta P_{t-i} + \\epsilon_t\n\\quad \\text{(Eq. (1))}\n \n\nThe Granger Representation Theorem states that the long-run impact matrix `Π` can be factored as `Π = αβ'`, where `α` contains adjustment coefficients and `β` is the cointegrating vector.\n\n---\n\n### Question\n\nSelect all statements that are TRUE regarding the VEC model in **Eq. (1)** and its relationship to standard Granger causality tests for cointegrated I(1) series.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 341, "Question": "### Background\n\n**Research Question.** How does the market's valuation of a firm's current operations (assets in place) and future opportunities (real options) depend on the interaction between the firm's profitability and its growth stage?\n\n**Setting.** A valuation model is estimated on distinct subsamples of firms, partitioned by profitability (profit vs. loss) and sales growth (low vs. high). The analysis focuses on the valuation coefficients of Net Income (`NI`), a proxy for assets in place, and R&D expenditures (`RD`), a proxy for real options. The persistence of abnormal earnings is also estimated as a foundational input for the valuation model and as a justification for the valuation patterns.\n\n**Variables and Parameters.**\n\n*   `P_{i,t}`: Price per share of firm `i` at the end of year `t`.\n*   `NI_{i,t}`: Net income per share, a proxy for the value of assets in place.\n*   `RD_{i,t}`: R&D expenditures per share, a proxy for investment in real options.\n*   `NI_{i,t}^a`: Abnormal earnings for firm `i` in period `t`.\n*   `\\omega_1`: The persistence parameter of abnormal earnings.\n*   `\\beta_3`, `\\beta_4`: Regression coefficients on `NI` and `RD`, respectively.\n\n---\n\n### Data / Model Specification\n\nThe following valuation model is estimated:\n\n  \nP_{i,t} = \\beta_0 + \\beta_1 BV_{i,t} + \\beta_2 DIV_{i,t} + \\beta_3 NI_{i,t} + \\beta_4 RD_{i,t} + \\beta_5 OTHER_{i,t} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nThe key hypotheses regarding profitability and growth are:\n*   **H2a:** For firms reporting a loss, the market places a lower weight on net income and positively values R&D expenditures.\n*   **H2b:** For firms reporting a loss, the market places a higher value on R&D expenditures when the firm is experiencing high sales growth.\n*   **H2c:** For firms reporting a profit, the market positively values R&D expenditures when the firm is experiencing high sales growth, but not when the firm is experiencing low sales growth.\n\nTable 1 below presents the estimated persistence parameters (`\\hat{\\omega}_1`) for various subsamples. Table 2 presents the key coefficient estimates (`\\hat{\\beta}_3` and `\\hat{\\beta}_4`) from estimating Eq. (1) on four subsamples.\n\n**Table 1: Abnormal Earnings Persistence Parameters (`\\hat{\\omega}_1`)**\n\n| | Entire sample | Low sales growth firms | High sales growth firms |\n| :--- | :---: | :---: | :---: |\n| **Entire sample** | 0.271 | 0.350 | 0.229 |\n| **Loss firms** | 0.133 | 0.207 | 0.081 |\n| **Profit firms** | 0.339 | 0.391 | 0.288 |\n\n*Note: Differences between high and low sales growth firms are statistically significant at p < 0.05.*\n\n**Table 2: Valuation Regression Results for `NI` and `RD`**\n\n| Variable | Subsample | Low Growth | High Growth |\n| :--- | :--- | :---: | :---: |\n| **NI** (`\\hat{\\beta}_3`) | Loss Firms | 0.440 | 0.371 |\n| | | (1.38) | (0.60) |\n| | Profit Firms | **3.608** | **1.252** |\n| | | (19.26) | (4.31) |\n| **RD** (`\\hat{\\beta}_4`) | Loss Firms | **2.100** | **4.081** |\n| | | (5.09) | (5.21) |\n| | Profit Firms | **-3.285** | **1.880** |\n| | | (10.48) | (4.23) |\n\n*Note: t-statistics are in parentheses. Bolded coefficients are significant at p < 0.01. Differences between low and high growth are statistically significant.*\n\n---\n\n### Question\n\nBy synthesizing the findings on abnormal earnings persistence (Table 1) and net income valuation (Table 2), which of the following conclusions are supported by the data?", "Options": {"A": "The abnormal earnings of loss-making firms are more persistent than those of profitable firms, justifying the higher valuation of Book Value (`BV`) for loss firms.", "B": "High-growth firms have higher `NI` valuation coefficients because their higher R&D spending signals greater future profitability.", "C": "The market places a lower valuation multiple on the Net Income (`NI`) of high-growth firms in part because their abnormal earnings are empirically less persistent than those of low-growth firms.", "D": "The statistically insignificant valuation of Net Income (`NI`) for loss-making firms is consistent with their low abnormal earnings persistence."}, "Answer": ["C", "D"], "pi_justification": "This item captures the synthesis component of the original Table QA problem, requiring students to connect results from two different tables (earnings persistence and valuation). The strategy is **Reverse-Reasoning**, where the observed valuation pattern for Net Income is explained by the underlying persistence data. Distractors test for incorrect causal links (D) or factual errors in reading the persistence table (B).", "qid": "341", "question": "### Background\n\n**Research Question.** How does the market's valuation of a firm's current operations (assets in place) and future opportunities (real options) depend on the interaction between the firm's profitability and its growth stage?\n\n**Setting.** A valuation model is estimated on distinct subsamples of firms, partitioned by profitability (profit vs. loss) and sales growth (low vs. high). The analysis focuses on the valuation coefficients of Net Income (`NI`), a proxy for assets in place, and R&D expenditures (`RD`), a proxy for real options. The persistence of abnormal earnings is also estimated as a foundational input for the valuation model and as a justification for the valuation patterns.\n\n**Variables and Parameters.**\n\n*   `P_{i,t}`: Price per share of firm `i` at the end of year `t`.\n*   `NI_{i,t}`: Net income per share, a proxy for the value of assets in place.\n*   `RD_{i,t}`: R&D expenditures per share, a proxy for investment in real options.\n*   `NI_{i,t}^a`: Abnormal earnings for firm `i` in period `t`.\n*   `\\omega_1`: The persistence parameter of abnormal earnings.\n*   `\\beta_3`, `\\beta_4`: Regression coefficients on `NI` and `RD`, respectively.\n\n---\n\n### Data / Model Specification\n\nThe following valuation model is estimated:\n\n  \nP_{i,t} = \\beta_0 + \\beta_1 BV_{i,t} + \\beta_2 DIV_{i,t} + \\beta_3 NI_{i,t} + \\beta_4 RD_{i,t} + \\beta_5 OTHER_{i,t} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nThe key hypotheses regarding profitability and growth are:\n*   **H2a:** For firms reporting a loss, the market places a lower weight on net income and positively values R&D expenditures.\n*   **H2b:** For firms reporting a loss, the market places a higher value on R&D expenditures when the firm is experiencing high sales growth.\n*   **H2c:** For firms reporting a profit, the market positively values R&D expenditures when the firm is experiencing high sales growth, but not when the firm is experiencing low sales growth.\n\nTable 1 below presents the estimated persistence parameters (`\\hat{\\omega}_1`) for various subsamples. Table 2 presents the key coefficient estimates (`\\hat{\\beta}_3` and `\\hat{\\beta}_4`) from estimating Eq. (1) on four subsamples.\n\n**Table 1: Abnormal Earnings Persistence Parameters (`\\hat{\\omega}_1`)**\n\n| | Entire sample | Low sales growth firms | High sales growth firms |\n| :--- | :---: | :---: | :---: |\n| **Entire sample** | 0.271 | 0.350 | 0.229 |\n| **Loss firms** | 0.133 | 0.207 | 0.081 |\n| **Profit firms** | 0.339 | 0.391 | 0.288 |\n\n*Note: Differences between high and low sales growth firms are statistically significant at p < 0.05.*\n\n**Table 2: Valuation Regression Results for `NI` and `RD`**\n\n| Variable | Subsample | Low Growth | High Growth |\n| :--- | :--- | :---: | :---: |\n| **NI** (`\\hat{\\beta}_3`) | Loss Firms | 0.440 | 0.371 |\n| | | (1.38) | (0.60) |\n| | Profit Firms | **3.608** | **1.252** |\n| | | (19.26) | (4.31) |\n| **RD** (`\\hat{\\beta}_4`) | Loss Firms | **2.100** | **4.081** |\n| | | (5.09) | (5.21) |\n| | Profit Firms | **-3.285** | **1.880** |\n| | | (10.48) | (4.23) |\n\n*Note: t-statistics are in parentheses. Bolded coefficients are significant at p < 0.01. Differences between low and high growth are statistically significant.*\n\n---\n\n### Question\n\nBy synthesizing the findings on abnormal earnings persistence (Table 1) and net income valuation (Table 2), which of the following conclusions are supported by the data?", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 468, "Question": "### Background\n\nAn insurer seeks to minimize the Value-at-Risk (`VaR`) of its total risk exposure, `T_I(X)`, by choosing a ceded loss function `I(x)`. The reinsurance premium includes a safety loading `θ`. A key finding (Theorem 3.2(i)) is that under certain conditions, the optimal strategy is to purchase no reinsurance at all, i.e., `I*(x) = 0`.\n\n### Data / Model Specification\n\nThe condition for forgoing reinsurance is:\n\n  \np \\leq \\theta^{*} \\quad \\Leftrightarrow \\quad \\theta \\geq \\frac{p}{1-p} \\quad \\text{(Eq. 1)}\n \n\nWhere `p` is the VaR confidence level and `θ* = θ / (1+θ)`.\n\nThe objective function being minimized is:\n\n  \n\\mathsf{VaR}_{T_{I}(X)}(p) = \\mathsf{VaR}_{X}(p) - I(\\mathsf{VaR}_{X}(p)) + (1+\\theta)\\mathbb{E}[I(X)] \\quad \\text{(Eq. 2)}\n \n\n### Question\n\nAccording to the model and its underlying logic, which of the following statements are valid criticisms or interpretations of the recommendation to purchase no reinsurance (`I*(x)=0`) when the condition in Eq. 1 is met? Select all that apply.", "Options": {"A": "The term `p/(1-p)` in the condition represents the statistical odds of a loss *exceeding* the `p`-quantile, meaning reinsurance is unattractive if its markup exceeds these odds.", "B": "This 'optimal' strategy can be dangerously misleading in practice because the VaR-minimization framework is blind to the magnitude of catastrophic losses in the `(1-p)` tail, potentially exposing the insurer to insolvency risk.", "C": "Switching to a TVaR-minimization framework would likely lead to the same 'no reinsurance' conclusion under the same condition, as both risk measures are based on the `p`-quantile.", "D": "The recommendation is economically sound within the model because when `θ` is sufficiently high, the premium cost `(1+θ)E[I(X)]` systematically outweighs the benefit of reducing the loss at the single point `VaR_X(p)`."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the ability to interpret a key theorem and critically evaluate its practical implications. It uses a 'Reverse-Reasoning' strategy by asking for valid interpretations and critiques of a given result. Option A correctly explains the economic logic *within* the VaR model. Option B provides a valid and crucial critique of the model's limitations, highlighting the myopia of VaR. Option C is a 'Conceptual Distractor' (Almost Right); `p/(1-p)` represents the odds of the loss *not* exceeding the quantile. Option D is a 'Conceptual Opposite' distractor; a TVaR framework is sensitive to the tail and would likely lead to a different, more prudent conclusion.", "qid": "468", "question": "### Background\n\nAn insurer seeks to minimize the Value-at-Risk (`VaR`) of its total risk exposure, `T_I(X)`, by choosing a ceded loss function `I(x)`. The reinsurance premium includes a safety loading `θ`. A key finding (Theorem 3.2(i)) is that under certain conditions, the optimal strategy is to purchase no reinsurance at all, i.e., `I*(x) = 0`.\n\n### Data / Model Specification\n\nThe condition for forgoing reinsurance is:\n\n  \np \\leq \\theta^{*} \\quad \\Leftrightarrow \\quad \\theta \\geq \\frac{p}{1-p} \\quad \\text{(Eq. 1)}\n \n\nWhere `p` is the VaR confidence level and `θ* = θ / (1+θ)`.\n\nThe objective function being minimized is:\n\n  \n\\mathsf{VaR}_{T_{I}(X)}(p) = \\mathsf{VaR}_{X}(p) - I(\\mathsf{VaR}_{X}(p)) + (1+\\theta)\\mathbb{E}[I(X)] \\quad \\text{(Eq. 2)}\n \n\n### Question\n\nAccording to the model and its underlying logic, which of the following statements are valid criticisms or interpretations of the recommendation to purchase no reinsurance (`I*(x)=0`) when the condition in Eq. 1 is met? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 228, "Question": "### Background\n\n**Research Question.** How can the theoretical framework for solving partial integro-differential equations (PIDEs) be applied to the practical problem of pricing and hedging a catastrophe (CAT) bond, an instrument exposed to both non-hedgeable insurance risk and hedgeable market risk?\n\n**Setting / Data-Generating Environment.** The price of a CAT bond depends on a three-dimensional state process `X = (Z^1, Z^2, L)`, where `Z^1` is a factor driving the loss intensity, `Z^2` is a factor driving the interest rate, and `L` is the cumulative loss process. The goal is to price the bond and construct a mean-variance optimal hedge for its interest rate exposure using a traded zero-coupon bond.\n\n### Data / Model Specification\n\nThe state process `X_t = (Z^1_t, Z^2_t, L_t)` follows the SDE system under a risk-neutral measure `Q`:\n  \n\\mathrm{d}Z_{t}^{1} = a_{1}(b_{1}-Z_{t}^{1})\\mathrm{d}t+\\sqrt{1-\\rho^{2}}\\sigma_{1}\\mathrm{d}W_{t}^{1}+\\rho\\sigma_{1}\\mathrm{d}W_{t}^{2}+\\gamma^{Z}\\mathrm{d}L_{t} \\quad \\text{(Eq. (1))}\n \n  \n\\mathrm{d}Z_{t}^{2} = a_{2}(b_{2}-Z_{t}^{2})\\mathrm{d}t+\\sigma_{2}\\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (2))}\n \nwhere `W^1, W^2` are independent Brownian motions. The cumulative loss `L_t` is a compound Poisson process with intensity `λ(t,Z_t^1)` and jump size distribution `μ(du)`. The short rate is `r_t = r(Z^2_t)`. The CAT bond has a terminal payoff at time `T` given by:\n  \ng(l) = F - \\left( (l-K_1)^+ - (l-K_2)^+ \\right) \\quad \\text{(Eq. (3))}\n \nwhere `F` is the face value and `K_1`, `K_2` are the attachment and exhaustion points.\n\n### Question\n\nBased on the model specification, select ALL of the following statements that are mathematically correct descriptions of the system's dynamics or risk factors.", "Options": {"A": "A value of `ρ=0` would imply that the CAT bond has no exposure to financial market risk, as the loss intensity factor `Z^1` would be independent of the interest rate factor `Z^2`.", "B": "The parameter `γ^Z` models a self-excitation effect, where a loss event directly increases the loss intensity factor `Z^1`, making subsequent losses more probable.", "C": "The cross-diffusion term in the generator `L_t` for the state process `X`, which captures the instantaneous covariance between the diffusion parts of `Z^1` and `Z^2`, is `ρσ₁σ₂φ_{z₁z₂}`.", "D": "The diffusion term (second-order derivative term) for the `Z^1` component in the generator `L_t` is `(1/2)(1-ρ²)σ₁²φ_{z₁z₁}`."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item tests the user's ability to correctly interpret the model's SDEs to derive components of the associated PIDE generator and to understand the financial meaning of key risk parameters. Strategy: Atomic Decomposition. The original QA question's parts on generator derivation and parameter interpretation are broken down into independent, verifiable statements. Distractor Logic: (C) is a Step-Omission Error, as it incorrectly calculates the variance of `dZ^1` by ignoring the `dW^2` component. (D) is a Conceptual: Almost Right error; `ρ=0` removes correlation between the *shocks* but does not remove the bond's fundamental exposure to interest rates via discounting.", "qid": "228", "question": "### Background\n\n**Research Question.** How can the theoretical framework for solving partial integro-differential equations (PIDEs) be applied to the practical problem of pricing and hedging a catastrophe (CAT) bond, an instrument exposed to both non-hedgeable insurance risk and hedgeable market risk?\n\n**Setting / Data-Generating Environment.** The price of a CAT bond depends on a three-dimensional state process `X = (Z^1, Z^2, L)`, where `Z^1` is a factor driving the loss intensity, `Z^2` is a factor driving the interest rate, and `L` is the cumulative loss process. The goal is to price the bond and construct a mean-variance optimal hedge for its interest rate exposure using a traded zero-coupon bond.\n\n### Data / Model Specification\n\nThe state process `X_t = (Z^1_t, Z^2_t, L_t)` follows the SDE system under a risk-neutral measure `Q`:\n  \n\\mathrm{d}Z_{t}^{1} = a_{1}(b_{1}-Z_{t}^{1})\\mathrm{d}t+\\sqrt{1-\\rho^{2}}\\sigma_{1}\\mathrm{d}W_{t}^{1}+\\rho\\sigma_{1}\\mathrm{d}W_{t}^{2}+\\gamma^{Z}\\mathrm{d}L_{t} \\quad \\text{(Eq. (1))}\n \n  \n\\mathrm{d}Z_{t}^{2} = a_{2}(b_{2}-Z_{t}^{2})\\mathrm{d}t+\\sigma_{2}\\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (2))}\n \nwhere `W^1, W^2` are independent Brownian motions. The cumulative loss `L_t` is a compound Poisson process with intensity `λ(t,Z_t^1)` and jump size distribution `μ(du)`. The short rate is `r_t = r(Z^2_t)`. The CAT bond has a terminal payoff at time `T` given by:\n  \ng(l) = F - \\left( (l-K_1)^+ - (l-K_2)^+ \\right) \\quad \\text{(Eq. (3))}\n \nwhere `F` is the face value and `K_1`, `K_2` are the attachment and exhaustion points.\n\n### Question\n\nBased on the model specification, select ALL of the following statements that are mathematically correct descriptions of the system's dynamics or risk factors.", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: import sympy as sp\n\n# Define symbols\nrho, sigma1, ...\n\n\nB,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 434, "Question": "### Background\n\n**Research Question.** How do different methods for ranking underwriter reputation—by deal count versus deal value—reveal fundamental differences in the structure of US and European IPO markets?\n\n**Setting / Data-Generating Environment.** The study constructs underwriter rankings for IPOs in major European markets and the US. It compares two primary ranking methods and analyzes the correlation of these rankings across different market segments.\n\n**Variables & Parameters.**\n\n*   `Equally-weighted ranking`: A ranking based on the number of IPOs an underwriter has completed. This metric emphasizes deal frequency.\n*   `Proceeds-weighted ranking`: A ranking based on the total capital raised across all IPOs an underwriter has completed. This metric emphasizes deal size.\n*   `Rank correlation coefficient`: A statistical measure of the association between two sets of rankings. A value near 1 indicates strong agreement, while a value near 0 indicates no association.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Top Underwriters on London AIM (Panel A: Equally-weighted)**\n\n| Rank | Name | IPOs (No.) |\n| :--- | :--- | :--- |\n| 1 | Evolution Sec | 123 |\n| 2 | Collins Stewart | 90 |\n| 3 | WH Ireland | 62 |\n| ... | ... | ... |\n\n*Note: The paper states that proceeds-weighted rankings for AIM are dominated by different firms.* \n\n**Table 2: Selected Rank Correlation Coefficients**\n\n| Markets Compared | Correlation Coefficient |\n| :--- | :--- |\n| London: AIM vs. Official List | 0.170* |\n| Paris: Marché Libre vs. Premier Marché | 0.023 |\n| US: NYSE vs. NASDAQ | 0.929*** |\n\n*Significance levels: * p<0.10, ** p<0.05, *** p<0.01*\n\nThe paper notes that in the US, the top underwriters are largely the same regardless of the ranking metric used (equally- vs. proceeds-weighted) and the exchange (NYSE vs. NASDAQ).\n\n---\n\n### The Question\n\nBased on the provided data and the paper's arguments about market structure, select all statements that are correct.", "Options": {"A": "The low correlation (0.170) between AIM and Official List rankings suggests that underwriter reputation is not an important factor in the UK market, unlike in the highly correlated US market.", "B": "An underwriter's high rank on a 'proceeds-weighted' list, such as for Evolution Securities on the AIM, signifies a business model focused on a high volume of small deals.", "C": "A hypothetical regulation forcing main-market banks to co-lead large AIM IPOs would likely increase the AIM-Official List rank correlation while simultaneously strengthening the paper's core argument that a single 'integrated' reputation measure is misleading for Europe.", "D": "The stark contrast between the high NYSE-NASDAQ rank correlation (0.929) and the low AIM-Official List correlation (0.170) indicates that the US IPO underwriting market is highly integrated, while the UK market is fragmented with specialized underwriters for different segments."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item assesses understanding of the paper's central thesis on market fragmentation by testing interpretation of statistical evidence (rank correlations) and the logic of a hypothetical scenario. It uses an Atomic Decomposition strategy. Distractors are designed to target a direct conceptual error in ranking definitions (C) and a faulty inference from the statistical evidence (D).", "qid": "434", "question": "### Background\n\n**Research Question.** How do different methods for ranking underwriter reputation—by deal count versus deal value—reveal fundamental differences in the structure of US and European IPO markets?\n\n**Setting / Data-Generating Environment.** The study constructs underwriter rankings for IPOs in major European markets and the US. It compares two primary ranking methods and analyzes the correlation of these rankings across different market segments.\n\n**Variables & Parameters.**\n\n*   `Equally-weighted ranking`: A ranking based on the number of IPOs an underwriter has completed. This metric emphasizes deal frequency.\n*   `Proceeds-weighted ranking`: A ranking based on the total capital raised across all IPOs an underwriter has completed. This metric emphasizes deal size.\n*   `Rank correlation coefficient`: A statistical measure of the association between two sets of rankings. A value near 1 indicates strong agreement, while a value near 0 indicates no association.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Top Underwriters on London AIM (Panel A: Equally-weighted)**\n\n| Rank | Name | IPOs (No.) |\n| :--- | :--- | :--- |\n| 1 | Evolution Sec | 123 |\n| 2 | Collins Stewart | 90 |\n| 3 | WH Ireland | 62 |\n| ... | ... | ... |\n\n*Note: The paper states that proceeds-weighted rankings for AIM are dominated by different firms.* \n\n**Table 2: Selected Rank Correlation Coefficients**\n\n| Markets Compared | Correlation Coefficient |\n| :--- | :--- |\n| London: AIM vs. Official List | 0.170* |\n| Paris: Marché Libre vs. Premier Marché | 0.023 |\n| US: NYSE vs. NASDAQ | 0.929*** |\n\n*Significance levels: * p<0.10, ** p<0.05, *** p<0.01*\n\nThe paper notes that in the US, the top underwriters are largely the same regardless of the ranking metric used (equally- vs. proceeds-weighted) and the exchange (NYSE vs. NASDAQ).\n\n---\n\n### The Question\n\nBased on the provided data and the paper's arguments about market structure, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 458, "Question": "### Background\n\n**Research Question.** This study investigates whether market-assessed sovereign risk (bond spreads) and agency-assessed sovereign risk (credit ratings) are driven by the same factors, specifically testing the differential impact of global investor risk appetite.\n\n**Setting / Data-Generating Environment.** The analysis uses a quarterly panel dataset for five emerging market countries from 1992-1997. The core methodology is a dynamic panel regression with country fixed effects, estimated for two different dependent variables: market-based Brady bond spreads and agency-based credit ratings.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t` (basis points).\n- `IIR`\\u208_it\\u208_: Institutional Investor Sovereign Ratings for country `i` at quarter `t` (ratings scale).\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t` (dimensionless index).\n- `X`\\u208_it\\u208_: Vector of country-specific economic fundamentals for country `i` at quarter `t`.\n\n---\n\n### Data / Model Specification\n\nThe core regression model is specified as:\n\n  \nY_{it} = \\beta_{0i} + \\beta_{1}Y_{it-1} + \\beta_{2} \\mathrm{RAI}_{t} + \\Gamma' X_{it} + \\varepsilon_{it} \n \n\nwhere `Y`\\u208_it\\u208_ is the measure of country risk (`BBY`\\u208_it\\u208_ or `IIR`\\u208_it\\u208_). The estimated coefficients for this model are presented in Table 1 below.\n\n**Table 1: Estimated Country Risk Measures**\n\n| | (1) | (2) | (3) |\n| :--- | :--- | :--- | :--- |\n| **Dependent Variable** | **BBY** | **BBY** | **IIR** |\n| One-lagged dep. Var. | 0.6694 (8.859)** | 0.7344 (10.017)** | 0.9568 (65.623)** |\n| Risk appetite index (RAI) | | **-1.9785 (-3.630)**** | -0.0002 (-0.150) |\n| Real GDP growth | -14.6289 (-2.525)** | -11.7498 (-2.133)** | 0.0856 (6.718)** |\n| International reserves | -1.3429 (-3.158)** | -0.8628 (-2.048)** | 0.0036 (3.774)** |\n| Change in real exchange rate | -23.8800 (-2.862)** | -22.7968 (-2.902)** | -0.0660 (-3.125)** |\n| Inflation rate | 0.0963 (2.435)** | 0.9711 (2.610)** | -0.0002 (-2.104)** |\n\n*Notes: t-statistics in parentheses. ** denotes significance at the 5% level or better. BBY is Brady bond stripped yield spread. IIR is Institutional Investor Sovereign Ratings. Column (1) excludes RAI, Column (2) includes it.* \n\n---\n\nBased on the model specification and the estimation results in Table 1, which of the following statements are correct interpretations of the findings?", "Options": {"A": "The estimated effect of the Risk Appetite Index (RAI) on Institutional Investor Sovereign Ratings (IIR) is statistically significant at conventional levels (e.g., 5%).", "B": "A 10-point increase in the Risk Appetite Index (RAI) is associated with a decrease in the Brady bond yield spread (BBY) of approximately 19.8 basis points.", "C": "An increase in investor risk appetite (a higher RAI) is associated with an increase in market-assessed sovereign risk (a higher BBY).", "D": "The estimated effect of the Risk Appetite Index (RAI) on the Brady bond yield spread (BBY) is statistically significant at conventional levels (e.g., 5%)."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret regression coefficients from a table, covering magnitude, statistical significance, and direction. It uses an Atomic Decomposition strategy to create two correct options from the core findings. Distractors are based on common errors: misinterpreting statistical significance (C) and a sign error in the economic interpretation (D).", "qid": "458", "question": "### Background\n\n**Research Question.** This study investigates whether market-assessed sovereign risk (bond spreads) and agency-assessed sovereign risk (credit ratings) are driven by the same factors, specifically testing the differential impact of global investor risk appetite.\n\n**Setting / Data-Generating Environment.** The analysis uses a quarterly panel dataset for five emerging market countries from 1992-1997. The core methodology is a dynamic panel regression with country fixed effects, estimated for two different dependent variables: market-based Brady bond spreads and agency-based credit ratings.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t` (basis points).\n- `IIR`\\u208_it\\u208_: Institutional Investor Sovereign Ratings for country `i` at quarter `t` (ratings scale).\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t` (dimensionless index).\n- `X`\\u208_it\\u208_: Vector of country-specific economic fundamentals for country `i` at quarter `t`.\n\n---\n\n### Data / Model Specification\n\nThe core regression model is specified as:\n\n  \nY_{it} = \\beta_{0i} + \\beta_{1}Y_{it-1} + \\beta_{2} \\mathrm{RAI}_{t} + \\Gamma' X_{it} + \\varepsilon_{it} \n \n\nwhere `Y`\\u208_it\\u208_ is the measure of country risk (`BBY`\\u208_it\\u208_ or `IIR`\\u208_it\\u208_). The estimated coefficients for this model are presented in Table 1 below.\n\n**Table 1: Estimated Country Risk Measures**\n\n| | (1) | (2) | (3) |\n| :--- | :--- | :--- | :--- |\n| **Dependent Variable** | **BBY** | **BBY** | **IIR** |\n| One-lagged dep. Var. | 0.6694 (8.859)** | 0.7344 (10.017)** | 0.9568 (65.623)** |\n| Risk appetite index (RAI) | | **-1.9785 (-3.630)**** | -0.0002 (-0.150) |\n| Real GDP growth | -14.6289 (-2.525)** | -11.7498 (-2.133)** | 0.0856 (6.718)** |\n| International reserves | -1.3429 (-3.158)** | -0.8628 (-2.048)** | 0.0036 (3.774)** |\n| Change in real exchange rate | -23.8800 (-2.862)** | -22.7968 (-2.902)** | -0.0660 (-3.125)** |\n| Inflation rate | 0.0963 (2.435)** | 0.9711 (2.610)** | -0.0002 (-2.104)** |\n\n*Notes: t-statistics in parentheses. ** denotes significance at the 5% level or better. BBY is Brady bond stripped yield spread. IIR is Institutional Investor Sovereign Ratings. Column (1) excludes RAI, Column (2) includes it.* \n\n---\n\nBased on the model specification and the estimation results in Table 1, which of the following statements are correct interpretations of the findings?", "model_evaluation_result": {"model_raw_response": "\"B,D\"", "model_answer": ["B", "D"], "error": null}}
{"ID": 382, "Question": "### Background\n\nA risk-averse insured, with utility function `U(·)`, chooses an indemnity function `I(·)` to maximize their expected utility. The insured's belief is `\\mathbb{P}` (with density `f_\\mathbb{P}(x)`) and the insurer's is `\\mathbb{Q}` (with density `f_\\mathbb{Q}(x)`).\n\n---\n\n### Data / Model Specification\n\nThe insured's optimization problem is:\n  \n\\operatorname*{max}_{I(\\cdot)\\in\\mathfrak{C}}\\mathbb{E}^{\\mathbb{P}}\\left[U(w_{I}(X))\\right] \\quad \\text{(Eq. (1))}\n \nFor an interior solution where there is partial insurance coverage (`0 < I^{*'}(x) < 1`), the optimal contract `I^*(x)` must satisfy the following pointwise first-order condition:\n  \nU'(w_{I^*}(x)) f_\\mathbb{P}(x) = \\lambda f_\\mathbb{Q}(x) \\quad \\text{(Eq. (2))}\n \nHere, `w_{I^*}(x)` is the insured's final wealth when the loss is `x`, and `\\lambda` is a positive constant representing the shadow price of insurance.\n\n---\n\n### Question\n\nBased on the optimality condition in **Eq. (2)**, which of the following interpretations are correct? Select all that apply.", "Options": {"A": "The constant `\\lambda` is equal to the insurer's safety loading factor, `1+\\rho`.", "B": "The left-hand side, `U'(w_{I^*}(x)) f_\\mathbb{P}(x)`, represents the insured's state-price density: the marginal utility of wealth in state `x` multiplied by the insured's subjective probability density of that state.", "C": "The condition implies that the insured's marginal utility of wealth, `U'(w_{I^*}(x))`, must be highest in states `x` that the insurer considers most probable (i.e., where `f_\\mathbb{Q}(x)` is highest).", "D": "The right-hand side, `\\lambda f_\\mathbb{Q}(x)`, represents the effective marginal cost of providing indemnity in state `x`, as determined by the insurer's beliefs and the overall cost of the insurance contract."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item assesses the interpretation of the paper's general first-order condition for an optimal contract, using an 'atomic decomposition' strategy. The correct options (A, B) break the condition into its left-hand side (insured's valuation) and right-hand side (insurer's effective price). Distractors are based on common errors: C incorrectly relates marginal utility directly to the insurer's density without accounting for the insured's own beliefs, and D provides a simplistic and incorrect definition for the complex shadow price `lambda`.", "qid": "382", "question": "### Background\n\nA risk-averse insured, with utility function `U(·)`, chooses an indemnity function `I(·)` to maximize their expected utility. The insured's belief is `\\mathbb{P}` (with density `f_\\mathbb{P}(x)`) and the insurer's is `\\mathbb{Q}` (with density `f_\\mathbb{Q}(x)`).\n\n---\n\n### Data / Model Specification\n\nThe insured's optimization problem is:\n  \n\\operatorname*{max}_{I(\\cdot)\\in\\mathfrak{C}}\\mathbb{E}^{\\mathbb{P}}\\left[U(w_{I}(X))\\right] \\quad \\text{(Eq. (1))}\n \nFor an interior solution where there is partial insurance coverage (`0 < I^{*'}(x) < 1`), the optimal contract `I^*(x)` must satisfy the following pointwise first-order condition:\n  \nU'(w_{I^*}(x)) f_\\mathbb{P}(x) = \\lambda f_\\mathbb{Q}(x) \\quad \\text{(Eq. (2))}\n \nHere, `w_{I^*}(x)` is the insured's final wealth when the loss is `x`, and `\\lambda` is a positive constant representing the shadow price of insurance.\n\n---\n\n### Question\n\nBased on the optimality condition in **Eq. (2)**, which of the following interpretations are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n\nB,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 137, "Question": "### Background\n\n**Research Question.** Leslie Perlow argues that culture is a system of three interconnected layers (artifacts, values, assumptions) and that changing only one layer often fails. This case formalizes this concept using a coordination game to model cultural equilibria and analyze the difficulty and cost of engineering a cultural shift.\n\n**Setting.** Consider a stylized firm where employees' collective behavior determines the dominant work norm. We model this as a symmetric 2-player game where each player must choose a work style. The payoff to each player depends on their own choice and the choice of the other player, reflecting the need for coordination.\n\n**Variables and Parameters.**\n- `a_i`: Action of player `i`, `a_i ∈ {H, F}`.\n- `H`: “High-Effort” work style (e.g., Perlow's “client comes first”).\n- `F`: “Flexible” work style (e.g., work-life balance).\n- `π(a_i, a_j)`: Payoff to player `i` given their action `a_i` and player `j`'s action `a_j`.\n\n---\n\n### Data / Model Specification\n\nThe strategic interaction is represented by the following payoff matrix. The entries are (Player 1's payoff, Player 2's payoff).\n\n**Table 1: Baseline Payoff Matrix**\n| | Player 2 Chooses H | Player 2 Chooses F |\n| :--- | :---: | :---: |\n| **Player 1 Chooses H** | (5, 5) | (1, 2) |\n| **Player 1 Chooses F** | (2, 1) | (4, 4) |\n\nThis game structure allows for multiple, self-sustaining cultural norms.\n\n---\n\n### The Question\n\nBased on the payoff matrix in Table 1, which of the following outcomes represent a pure-strategy Nash Equilibrium? Select all that apply.", "Options": {"A": "Player 1 chooses H, Player 2 chooses F", "B": "Player 1 chooses H, Player 2 chooses H", "C": "Player 1 chooses F, Player 2 chooses H", "D": "Player 1 chooses F, Player 2 chooses F"}, "Answer": ["B", "D"], "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item tests the fundamental skill of identifying Nash equilibria in a matrix game, which is the foundation of the original QA problem. It uses a Computational Judgment strategy. The Multiple Choice format is ideal for a 'select-all-that-apply' task. The correct answers are the two cells where neither player has a unilateral incentive to deviate. The distractors are the off-diagonal cells, which are not equilibria.", "qid": "137", "question": "### Background\n\n**Research Question.** Leslie Perlow argues that culture is a system of three interconnected layers (artifacts, values, assumptions) and that changing only one layer often fails. This case formalizes this concept using a coordination game to model cultural equilibria and analyze the difficulty and cost of engineering a cultural shift.\n\n**Setting.** Consider a stylized firm where employees' collective behavior determines the dominant work norm. We model this as a symmetric 2-player game where each player must choose a work style. The payoff to each player depends on their own choice and the choice of the other player, reflecting the need for coordination.\n\n**Variables and Parameters.**\n- `a_i`: Action of player `i`, `a_i ∈ {H, F}`.\n- `H`: “High-Effort” work style (e.g., Perlow's “client comes first”).\n- `F`: “Flexible” work style (e.g., work-life balance).\n- `π(a_i, a_j)`: Payoff to player `i` given their action `a_i` and player `j`'s action `a_j`.\n\n---\n\n### Data / Model Specification\n\nThe strategic interaction is represented by the following payoff matrix. The entries are (Player 1's payoff, Player 2's payoff).\n\n**Table 1: Baseline Payoff Matrix**\n| | Player 2 Chooses H | Player 2 Chooses F |\n| :--- | :---: | :---: |\n| **Player 1 Chooses H** | (5, 5) | (1, 2) |\n| **Player 1 Chooses F** | (2, 1) | (4, 4) |\n\nThis game structure allows for multiple, self-sustaining cultural norms.\n\n---\n\n### The Question\n\nBased on the payoff matrix in Table 1, which of the following outcomes represent a pure-strategy Nash Equilibrium? Select all that apply.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 426, "Question": "### Background\n\n**Research Question.** How precise are the theoretical bounds on lifetime functionals when applied to both a theoretical mortality law and a real-world life table, and what do the results reveal about the practical severity of model risk from fractional age assumptions?\n\n**Setting.** An actuary has developed a theoretical framework for bounding the value of life-contingent claims for any lifetime `X` that has an Increasing Force of Mortality (IFR) and is consistent with a given life table. The framework produces a stochastic lower bound `L` (based on a Constant Force of Mortality assumption) and a stochastic upper bound `U` (the supremum bound). The key result is that for any valid lifetime `X`, `L ≤_st X ≤_st U`.\n\n**Variables and Parameters.**\n\n*   `e_x`: Expected future lifetime for a person aged `x` (units: years).\n*   `Ā_x`: Present value of a continuous life annuity for a person aged `x`, with interest rate `i=0.06` (dimensionless value).\n*   `E[h(L)]`: The value of a functional calculated using the stochastic lower bound `L`.\n*   `E[h(U)]`: The value of a functional calculated using the stochastic upper bound `U`.\n*   `E[h(U_μ*)]`: The value of a functional calculated using a tighter upper bound that requires optimizing over possible mortality rates at integer ages.\n\n---\n\n### Data / Model Specification\n\nThe following tables present calculated bounds for two scenarios: a theoretical lifetime distribution from Makeham’s law (where the true value is known) and a real-world German life table from 1999/2001.\n\n**Table 1.** Bounds for `e_30` and `Ā_30` under Makeham's Law (`i=0.06`)\n\n| | `e_30` (years) | `Ā_30` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 45.05944407 | 0.10554796 |\n| **True Value** | **45.06687713** | **0.10550557** |\n| `E[h(U_μ*)]` | 45.07059946 | 0.10548412 |\n| `E[h(U)]` | 45.08173562 | 0.10542090 |\n\n**Table 2.** Bounds for `e_50` and `Ā_50` using German Life Table 1999/2001 (`i=0.06`)\n\n| | `e_50` (years) | `Ā_50` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 27.27856559 | 0.24740336 |\n| `E[h(U_μ*)]` | 27.28957960 | 0.24723758 |\n| `E[h(U)]` | 27.29933908 | 0.24708628 |\n\nFor any increasing function `f(X)` (like `e_x`), the stochastic ordering `L ≤_st X ≤_st U` implies `E[f(L)] ≤ E[f(X)] ≤ E[f(U)]`. For any decreasing function `g(X)`, the implication is reversed: `E[g(L)] ≥ E[g(X)] ≥ E[g(U)]`.\n\n---\n\n### Question\n\nBased on the provided data and the theoretical framework, which of the following statements are correct?", "Options": {"A": "The width of the `[E[h(L)], E[h(U)]]` bounding interval for the expected future lifetime of a 30-year-old (`e_30`) under Makeham's law is less than 0.025 years.", "B": "According to the German Life Table results in Table 2, the lower bound `E[h(L)]` for the annuity value `Ā_50` is 0.24723758.", "C": "The uncertainty in the expected future lifetime for a 50-year-old (`e_50`) based on the German Life Table, as captured by the `[E[h(L)], E[h(U)]]` interval, is approximately one week, representing a financially insignificant risk for large-scale pension fund management.", "D": "The bounding interval for the annuity value `Ā_30` under Makeham's law is wider than the interval for the expected lifetime `e_30`, indicating greater model risk for financial valuations than for longevity projections."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform precise calculations from tabular data and interpret the results in a practical context. It uses an **Atomic Decomposition** strategy, breaking down the original problem into two distinct, verifiable claims. Distractor C is a **Numerical (Step-Omission Error)** type, testing careful reading of the table by using a value from the wrong row. Distractor D is a **Conceptual** error based on a faulty numerical comparison, testing whether the user can correctly calculate and compare the relative widths of the bounds.", "qid": "426", "question": "### Background\n\n**Research Question.** How precise are the theoretical bounds on lifetime functionals when applied to both a theoretical mortality law and a real-world life table, and what do the results reveal about the practical severity of model risk from fractional age assumptions?\n\n**Setting.** An actuary has developed a theoretical framework for bounding the value of life-contingent claims for any lifetime `X` that has an Increasing Force of Mortality (IFR) and is consistent with a given life table. The framework produces a stochastic lower bound `L` (based on a Constant Force of Mortality assumption) and a stochastic upper bound `U` (the supremum bound). The key result is that for any valid lifetime `X`, `L ≤_st X ≤_st U`.\n\n**Variables and Parameters.**\n\n*   `e_x`: Expected future lifetime for a person aged `x` (units: years).\n*   `Ā_x`: Present value of a continuous life annuity for a person aged `x`, with interest rate `i=0.06` (dimensionless value).\n*   `E[h(L)]`: The value of a functional calculated using the stochastic lower bound `L`.\n*   `E[h(U)]`: The value of a functional calculated using the stochastic upper bound `U`.\n*   `E[h(U_μ*)]`: The value of a functional calculated using a tighter upper bound that requires optimizing over possible mortality rates at integer ages.\n\n---\n\n### Data / Model Specification\n\nThe following tables present calculated bounds for two scenarios: a theoretical lifetime distribution from Makeham’s law (where the true value is known) and a real-world German life table from 1999/2001.\n\n**Table 1.** Bounds for `e_30` and `Ā_30` under Makeham's Law (`i=0.06`)\n\n| | `e_30` (years) | `Ā_30` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 45.05944407 | 0.10554796 |\n| **True Value** | **45.06687713** | **0.10550557** |\n| `E[h(U_μ*)]` | 45.07059946 | 0.10548412 |\n| `E[h(U)]` | 45.08173562 | 0.10542090 |\n\n**Table 2.** Bounds for `e_50` and `Ā_50` using German Life Table 1999/2001 (`i=0.06`)\n\n| | `e_50` (years) | `Ā_50` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 27.27856559 | 0.24740336 |\n| `E[h(U_μ*)]` | 27.28957960 | 0.24723758 |\n| `E[h(U)]` | 27.29933908 | 0.24708628 |\n\nFor any increasing function `f(X)` (like `e_x`), the stochastic ordering `L ≤_st X ≤_st U` implies `E[f(L)] ≤ E[f(X)] ≤ E[f(U)]`. For any decreasing function `g(X)`, the implication is reversed: `E[g(L)] ≥ E[g(X)] ≥ E[g(U)]`.\n\n---\n\n### Question\n\nBased on the provided data and the theoretical framework, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 402, "Question": "### Background\n\n**Research Question.** This case examines the behavioral motivation behind the use of stop-loss orders, distinguishing between the 'learning hypothesis' (stop-loss use teaches better habits) and the 'self-control hypothesis' (biased investors use stop losses as a pre-commitment device).\n\n**Setting / Data-Generating Environment.** The analysis uses two distinct empirical tests. The first is a transaction-level analysis comparing trades closed via stop-loss to manual trades. The second is a user-level analysis on a subsample of data that *excludes all stop-loss transactions*, comparing the underlying manual trading behavior of stop-loss users versus non-users.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator (dummy=1 for loss).\n- `TGI(t)`: Trading Gain Indicator (dummy=1 for gain).\n- `SL_Transaction`: A dummy variable equal to 1 for a roundtrip position closed by a stop-loss order.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who use stop losses.\n\n---\n\n### Data / Model Specification\n\nResults from the two key analyses are presented in Table 1 and Table 2.\n\n**Table 1: Transaction-Level Analysis of Stop-Loss Executions**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.5502*** | | \n| TLI × SL_Transaction | 2.1546*** | | \n| TGI | | 1.7695*** |\n| TGI × SL_Transaction | | 0.5225*** |\n\n**Table 2: User-Level Analysis on Manual (Non-Stop-Loss) Transactions**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.5610*** | | \n| TLI × SL_User | 0.9555** | | \n| TGI | | 1.8918*** |\n| TGI × SL_User | | 0.8419*** |\n\n*Notes: *** denotes significance at 1%, ** at 5%.*\n\n---\n\n### Question\n\nBased on the results in Table 1 and Table 2, select all of the following statements that are correct.", "Options": {"A": "The results in Table 2 support the 'self-control hypothesis' by showing that in their manual trades, stop-loss users are even more reluctant to sell losers than non-users are.", "B": "The results in Table 1 show that while stop-loss transactions significantly increase the propensity to sell losers, the combined hazard ratio for selling a loser remains below 1.", "C": "The results in Table 2 support the 'learning hypothesis' by showing that stop-loss users are less prone to the disposition effect in their manual trades.", "D": "The results in Table 1 demonstrate that for transactions closed via a stop-loss, the disposition effect is inverted, with the conditional probability of selling a loser becoming greater than the conditional probability of selling a winner."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item synthesizes results from two different tables to test a core theoretical question. It uses a 'Computational Judgment' and 'Scenario Application' strategy. Option A requires calculating the combined hazard ratios from Table 1 (`0.5502 * 2.1546 = 1.185` for losers; `1.7695 * 0.5225 = 0.925` for winners) and correctly interpreting the inversion. Option B requires interpreting the significant interaction term in Table 2 and connecting it to the correct behavioral theory. Distractor C is the 'Conceptual Opposite' of B. Distractor D is a 'Step-Omission Error', as it correctly notes the increase but fails to perform the multiplication which shows the combined hazard is > 1.", "qid": "402", "question": "### Background\n\n**Research Question.** This case examines the behavioral motivation behind the use of stop-loss orders, distinguishing between the 'learning hypothesis' (stop-loss use teaches better habits) and the 'self-control hypothesis' (biased investors use stop losses as a pre-commitment device).\n\n**Setting / Data-Generating Environment.** The analysis uses two distinct empirical tests. The first is a transaction-level analysis comparing trades closed via stop-loss to manual trades. The second is a user-level analysis on a subsample of data that *excludes all stop-loss transactions*, comparing the underlying manual trading behavior of stop-loss users versus non-users.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator (dummy=1 for loss).\n- `TGI(t)`: Trading Gain Indicator (dummy=1 for gain).\n- `SL_Transaction`: A dummy variable equal to 1 for a roundtrip position closed by a stop-loss order.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who use stop losses.\n\n---\n\n### Data / Model Specification\n\nResults from the two key analyses are presented in Table 1 and Table 2.\n\n**Table 1: Transaction-Level Analysis of Stop-Loss Executions**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.5502*** | | \n| TLI × SL_Transaction | 2.1546*** | | \n| TGI | | 1.7695*** |\n| TGI × SL_Transaction | | 0.5225*** |\n\n**Table 2: User-Level Analysis on Manual (Non-Stop-Loss) Transactions**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.5610*** | | \n| TLI × SL_User | 0.9555** | | \n| TGI | | 1.8918*** |\n| TGI × SL_User | | 0.8419*** |\n\n*Notes: *** denotes significance at 1%, ** at 5%.*\n\n---\n\n### Question\n\nBased on the results in Table 1 and Table 2, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 303, "Question": "### Background\n\n**Research Question.** In a Bayesian setting, how is the total uncertainty about a future observation decomposed into process risk versus parameter uncertainty, and how does this decomposition inform the optimal linear forecast of the mean?\n\n**Setting.** Consider a random observable, `\\tilde{x}` (e.g., an insurance claim), whose likelihood `p(x|\\theta)` depends on an unknown risk parameter `\\tilde{\\theta}`. The parameter `\\tilde{\\theta}` is itself a random variable with a prior density `p(\\theta)`. The goal is to forecast the next observation, `\\tilde{x}_{n+1}`, using a linear function of the mean of `n` previous observations, `\\bar{x}`.\n\n**Variables and Parameters.**\n- `\\tilde{x}`: A random observable.\n- `\\tilde{\\theta}`: An unobservable random parameter.\n- `m_1 = \\mathcal{E}[\\tilde{x}]`: The prior mean of `\\tilde{x}`.\n- `e`: Expected process variance, or \"within-risk\" variance.\n- `d`: Variance of hypothetical means, or \"between-risk\" variance.\n- `c`: Total variance of the observable `\\tilde{x}`.\n- `f_1^*(\\mathcal{D})`: The linear credibility forecast for the mean.\n- `z_1`: The credibility factor.\n\n---\n\n### Data / Model Specification\n\nThe total variance of `\\tilde{x}` can be decomposed according to the law of total variance:\n\n  \nc = \\mathcal{V}\\{\\tilde{x}\\} = e + d \\quad \\text{(Eq. (1))}\n \n\nwhere `e` and `d` are defined as:\n\n  \ne = \\mathcal{E} [\\mathcal{V}\\{\\tilde{x}|\\tilde{\\theta}\\}] \\quad \\text{(Eq. (2))}\n \n\n  \nd = \\mathcal{V} [\\mathcal{E}\\{\\tilde{x}|\\tilde{\\theta}\\}] \\quad \\text{(Eq. (3))}\n \n\nThe optimal linear forecast (credibility formula) for the mean is a weighted average of the prior mean `m_1` and the sample mean `\\bar{x}`:\n\n  \nf_{1}^{*}(\\mathcal{D})=(1-z_{1})m_{1}+z_{1}\\bar{x} \\quad \\text{(Eq. (4))}\n \n\n---\n\n### Question\n\nBased on the credibility framework provided, select all of the following statements that are true.", "Options": {"A": "The component `e`, the \"within-risk\" variance, represents the variation in average risk levels across the entire population of policyholders.", "B": "The \"credibility time constant\" `n_{01}`, which determines how much weight is given to new data, is defined by the ratio of the expected process variance to the variance of hypothetical means (`e/d`).", "C": "In an actuarial context, the component `d`, the \"between-risk\" variance, represents parameter uncertainty about a policyholder's true underlying risk type.", "D": "The credibility factor `z_1` is given by `d / (e/n + d)`. In the limit as `n \\to \\infty`, `z_1` approaches `d/e`."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item tests understanding of the fundamental decomposition of variance in credibility theory (`e` vs. `d`) and the resulting formula for the optimal credibility weight. The strategy is atomic decomposition, breaking down core concepts into distinct, verifiable statements. Option C is a 'Conceptual Opposite' distractor, swapping the interpretations of `e` and `d`, a common error. Option D presents a correct intermediate formula for `z_1` but draws an incorrect conclusion about its asymptotic limit, testing for a deeper conceptual understanding.", "qid": "303", "question": "### Background\n\n**Research Question.** In a Bayesian setting, how is the total uncertainty about a future observation decomposed into process risk versus parameter uncertainty, and how does this decomposition inform the optimal linear forecast of the mean?\n\n**Setting.** Consider a random observable, `\\tilde{x}` (e.g., an insurance claim), whose likelihood `p(x|\\theta)` depends on an unknown risk parameter `\\tilde{\\theta}`. The parameter `\\tilde{\\theta}` is itself a random variable with a prior density `p(\\theta)`. The goal is to forecast the next observation, `\\tilde{x}_{n+1}`, using a linear function of the mean of `n` previous observations, `\\bar{x}`.\n\n**Variables and Parameters.**\n- `\\tilde{x}`: A random observable.\n- `\\tilde{\\theta}`: An unobservable random parameter.\n- `m_1 = \\mathcal{E}[\\tilde{x}]`: The prior mean of `\\tilde{x}`.\n- `e`: Expected process variance, or \"within-risk\" variance.\n- `d`: Variance of hypothetical means, or \"between-risk\" variance.\n- `c`: Total variance of the observable `\\tilde{x}`.\n- `f_1^*(\\mathcal{D})`: The linear credibility forecast for the mean.\n- `z_1`: The credibility factor.\n\n---\n\n### Data / Model Specification\n\nThe total variance of `\\tilde{x}` can be decomposed according to the law of total variance:\n\n  \nc = \\mathcal{V}\\{\\tilde{x}\\} = e + d \\quad \\text{(Eq. (1))}\n \n\nwhere `e` and `d` are defined as:\n\n  \ne = \\mathcal{E} [\\mathcal{V}\\{\\tilde{x}|\\tilde{\\theta}\\}] \\quad \\text{(Eq. (2))}\n \n\n  \nd = \\mathcal{V} [\\mathcal{E}\\{\\tilde{x}|\\tilde{\\theta}\\}] \\quad \\text{(Eq. (3))}\n \n\nThe optimal linear forecast (credibility formula) for the mean is a weighted average of the prior mean `m_1` and the sample mean `\\bar{x}`:\n\n  \nf_{1}^{*}(\\mathcal{D})=(1-z_{1})m_{1}+z_{1}\\bar{x} \\quad \\text{(Eq. (4))}\n \n\n---\n\n### Question\n\nBased on the credibility framework provided, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"credibility theory in actuarial science between-risk within-risk variance\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"credibility factor z formula e d n\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"credibility time constant n0 in credibility theory\"\n> 📊 **Results requested:** 5\n\n> 🔍 **10 results found**\n\n> [🔗 \"Credible risk measures with applications in actuarial sciences and finance - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0167668716302748)\n> [🔗 \"Credibility theory - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Credibility_theory)\n> [🔗 \"Credibility Theory: Definition & Examples | Vaia\" Vaia](https://www.vaia.com/en-us/explanations/business-studies/actuarial-science-in-business/credibility-theory/)\n> [🔗 \"Chapter 9 Experience Rating Using Credibility Theory | Loss Data Analytics\" Openacttexts](https://openacttexts.github.io/Loss-Data-Analytics/ChapCredibility.html)\n> [🔗 \"Risk Theory and Credibility | EBSCO Research Starters\" EBSCO](https://www.ebsco.com/research-starters/business-and-management/risk-theory-and-credibility)\n> [🔗 \"(PDF) Credibility theory for variance premium principle\" ResearchGate](https://www.researchgate.net/publication/376798715_Credibility_theory_for_variance_premium_principle)\n> [🔗 \"Credibility theory using fuzzy numbers | Published in Variance\" Variancejournal](https://variancejournal.org/article/94735-credibility-theory-using-fuzzy-numbers)\n> [🔗 \"Actuarial Credibility: Beyond the Numbers | by Alex Filiakov, ACAS | Medium\" Medium](https://medium.com/%2540alexfiliakov/actuarial-credibility-beyond-the-numbers-c1bd465d7339)\n> [🔗 \"AN INTRODUCTION TO CREDIBILITY THEORY\" Casualty Actuarial Society](https://www.casact.org/sites/default/files/database/proceed_proceed62_62194.pdf)\n> [🔗 \"A longitudinal data analysis interpretation of credibility models - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0167668798000559)\n\n> 🔍 **10 results found**\n\n> [🔗 \"Credibility theory - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Credibility_theory)\n> [🔗 \"Chapter 9 Experience Rating Using Credibility Theory | Loss Data Analytics\" Openacttexts](https://openacttexts.github.io/Loss-Data-Analytics/ChapCredibility.html)\n> [🔗 \"An Introduction to Credibility\" Casualty Actuarial Society](https://www.casact.org/sites/default/files/database/forum_97wforum_97wf055.pdf)\n> [🔗 \"Mastering Buhlmann Credibility\" Number Analytics](https://www.numberanalytics.com/blog/mastering-buhlmann-credibility-stat-476)\n> [🔗 \"Chapter 8 CREDIBILITY\" Society of Actuaries](https://www.soa.org/493903/globalassets/assets/files/edu/c-21-01.pdf)\n> [🔗 \"Chapter 6 Classical Credibility - Nonlife Actuarial Models\" Mysmu](http://www.mysmu.edu/faculty/yktse/NAM/NAM_S6.pdf)\n> [🔗 \"AN INTRODUCTION TO CREDIBILITY THEORY\" Casualty Actuarial Society](https://www.casact.org/sites/default/files/database/proceed_proceed62_62194.pdf)\n> [🔗 \"Z-Score: Meaning and Formula\" Investopedia](https://www.investopedia.com/terms/z/zscore.asp)\n> [🔗 \"Credibility Methods Applied to Life, Health, and Pensions\" Society of Actuaries](https://www.soa.org/globalassets/assets/files/resources/tables-calcs-tools/credibility-methods-life-health-pensions.pdf)\n> [🔗 \"[Exam C] Is the credibility factor, Z, the same regardless if I calculate it using the claim count, exposure count, or aggregate? : r/actuary\" Reddit](https://www.reddit.com/r/actuary/comments/4k2usv/exam_c_is_the_credibility_factor_z_the_same/)\n\n> 🔍 **5 results found**\n\n> [🔗 \"Credibility theory - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Credibility_theory)\n> [🔗 \"Chapter 9 Experience Rating Using Credibility Theory | Loss Data Analytics\" Openacttexts](https://openacttexts.github.io/Loss-Data-Analytics/ChapCredibility.html)\n> [🔗 \"AN INTRODUCTION TO CREDIBILITY THEORY\" Casualty Actuarial Society](https://www.casact.org/sites/default/files/database/proceed_proceed62_62194.pdf)\n> [🔗 \"Principles and Application of Credibility Theory\" University of Nebraska-Lincoln](https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1092&context=joap)\n> [🔗 \"A general optimal approach to Bühlmann credibility theory - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0167668722000245)\n\n> 🔍 **Analyzing financial question**\n> ***- The task involves selecting true statements about Bayesian uncertainty decomposition.***\n\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 411, "Question": "### Background\n\n**Research Question.** Under what conditions does an optimal portfolio exist for a robust utility maximization problem in a one-period market?\n\n**Setting and Environment.** We consider a one-period market with `d` stocks and initial capital `x`. An agent with utility function `U` faces model uncertainty represented by a set of measures `\\mathcal{P}`. The existence of an optimal portfolio hinges on the geometric properties of the market, induced by a no-arbitrage principle.\n\n**Variables and Parameters.**\n- `\\Delta S`: Random `\\mathbb{R}^d`-valued vector of stock price changes.\n- `h`: A deterministic portfolio vector in `\\mathbb{R}^d`.\n- `x`: Initial capital.\n- `\\mathcal{P}`: A convex set of probability measures.\n- `\\mathcal{P}\\text{-q.s.}`: A property holds `\\mathcal{P}`-quasi-surely, i.e., outside a set of measure zero for all `P \\in \\mathcal{P}`.\n- `\\mathrm{supp}_{\\mathcal P}(\\Delta S)`: The smallest closed set `A` such that `P(\\Delta S \\in A) = 1` for all `P \\in \\mathcal{P}`.\n- `L`: The linear subspace `\\mathrm{span~supp}_{\\mathcal P}(\\Delta S)`.\n\n---\n\n### Data / Model Specification\n\nThe set of admissible portfolios is defined as `D_{x}:=\\{h\\in\\mathbb{R}^{d}:x+h\\Delta S\\geq0 \\; \\mathcal{P}\\mathrm{-q.s.}\\}`. The set of non-redundant admissible portfolios is `K_x = D_x \\cap L`.\nThe market is assumed to satisfy the no-arbitrage condition `NA(\\mathcal{P})`: for any `h \\in \\mathbb{R}^d`,\n  \nh\\Delta S\\geq0 \\; \\mathcal{P}\\mathrm{-q.s.} \\quad \\implies \\quad h\\Delta S=0 \\; \\mathcal{P}\\mathrm{-q.s.} \\quad \\text{(Eq. (1))}\n \nThe existence of an optimal portfolio `\\hat{h}` for the problem `\\sup_{h\\in D_{x}}\\inf_{P\\in{\\mathcal{P}}}E_{P}[U(x+h\\Delta S)]` is guaranteed by showing the objective function attains its supremum on the compact set `K_x`.\n\n---\n\n### Question\n\nSelect all conditions or properties that are essential for the argument that an optimal portfolio exists.\n", "Options": {"A": "The integrability condition `E_{P}[U^{+}(x+h\\Delta S)] < \\infty` is used to establish that the objective function `h \\mapsto \\inf_{P\\in\\mathcal{P}}E_{P}[U(x+h\\Delta S)]` is upper semicontinuous.", "B": "The set of measures `\\mathcal{P}` must be dominated by a single reference measure `P^*` to allow for the use of standard integration theory.", "C": "The no-arbitrage condition `NA(\\mathcal{P})` is used to prove that the set `K_x` is bounded, and therefore compact.", "D": "The concavity of the utility function `U` is required to ensure the objective function is well-defined and finite."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests the student's understanding of the complete logical chain of the existence proof in Theorem 2.2. It uses a 'Premise/assumption packaging' strategy to isolate the key components of the argument. Distractor C is a subtle 'True but Irrelevant' fact for this specific part of the proof (concavity is important, but not for the compactness/semicontinuity argument itself), while Distractor D presents a 'Conceptual Opposite' to the paper's main setting, which is explicitly non-dominated.", "qid": "411", "question": "### Background\n\n**Research Question.** Under what conditions does an optimal portfolio exist for a robust utility maximization problem in a one-period market?\n\n**Setting and Environment.** We consider a one-period market with `d` stocks and initial capital `x`. An agent with utility function `U` faces model uncertainty represented by a set of measures `\\mathcal{P}`. The existence of an optimal portfolio hinges on the geometric properties of the market, induced by a no-arbitrage principle.\n\n**Variables and Parameters.**\n- `\\Delta S`: Random `\\mathbb{R}^d`-valued vector of stock price changes.\n- `h`: A deterministic portfolio vector in `\\mathbb{R}^d`.\n- `x`: Initial capital.\n- `\\mathcal{P}`: A convex set of probability measures.\n- `\\mathcal{P}\\text{-q.s.}`: A property holds `\\mathcal{P}`-quasi-surely, i.e., outside a set of measure zero for all `P \\in \\mathcal{P}`.\n- `\\mathrm{supp}_{\\mathcal P}(\\Delta S)`: The smallest closed set `A` such that `P(\\Delta S \\in A) = 1` for all `P \\in \\mathcal{P}`.\n- `L`: The linear subspace `\\mathrm{span~supp}_{\\mathcal P}(\\Delta S)`.\n\n---\n\n### Data / Model Specification\n\nThe set of admissible portfolios is defined as `D_{x}:=\\{h\\in\\mathbb{R}^{d}:x+h\\Delta S\\geq0 \\; \\mathcal{P}\\mathrm{-q.s.}\\}`. The set of non-redundant admissible portfolios is `K_x = D_x \\cap L`.\nThe market is assumed to satisfy the no-arbitrage condition `NA(\\mathcal{P})`: for any `h \\in \\mathbb{R}^d`,\n  \nh\\Delta S\\geq0 \\; \\mathcal{P}\\mathrm{-q.s.} \\quad \\implies \\quad h\\Delta S=0 \\; \\mathcal{P}\\mathrm{-q.s.} \\quad \\text{(Eq. (1))}\n \nThe existence of an optimal portfolio `\\hat{h}` for the problem `\\sup_{h\\in D_{x}}\\inf_{P\\in{\\mathcal{P}}}E_{P}[U(x+h\\Delta S)]` is guaranteed by showing the objective function attains its supremum on the compact set `K_x`.\n\n---\n\n### Question\n\nSelect all conditions or properties that are essential for the argument that an optimal portfolio exists.\n", "model_evaluation_result": {"model_raw_response": "\n\nA,C\n\n## Explanation\n\nThe reasoning for selecting options A and C involves analyzing the conditions necessary for proving the existence of an optimal portfolio through the compactness of the set \\( K_x \\) and the properties of the objective function.\n\nFirst, recall that the existence argument relies on the objective function \\( h \\mapsto \\inf_{P \\in \\mathcal{P}} E_P[U(x + h \\Delta S)] \\) attaining its supremum on the compact set \\( K_x \\). For this, \\( K_x \\) must be compact, and the objective function must be upper semicontinuous.\n\n- For option A: Upper semicontinuity of the objective function requires that it is continuous from above, which is established using the integrability condition \\( E_P[U^+(x + h \\Delta S)] < \\infty \\). This condition ensures that the expectations are well-behaved under weak convergence of measures, allowing the infimum over \\( \\mathcal{P} \\) to preserve upper semicontinuity, as needed for the supremum to be attained on a compact set.\n\n- For option B: While a dominating measure \\( P^* \\) might simplify integration in some settings, it is not essential here. The problem operates under \\( \\mathcal{P} \\)-quasi-sure properties and the support structure of \\( \\Delta S \\), which do not require domination for the existence proof.\n\n- For option C: The no-arbitrage condition \\( NA(\\mathcal{P}) \\) implies that there\n> 🔍 **Analyzing conditions**\n> ***- The no-arbitrage condition NA(\\mathcal{P}) helps show the set K_x is bounded and compact.***\n are no free lunch opportunities, which bounds the set of admissible portfolios. Specifically, it ensures \\( K_x = D_x \\cap L \\) is closed (as the intersection of closed sets) and bounded (due to the absence of arbitrage directions in \\( L = \\span \\supp_{\\mathcal{P}}(\\Delta S) \\)), making \\( K_x \\) compact in the finite-dimensional space \\( \\mathbb{R}^d \\).\n\n- For option D: Concavity of \\( U \\) is useful for concavity of the objective but not required for existence via the direct method. The argument depends on compactness and upper semicontinuity, not on concavity to ensure finiteness, as the admissibility set \\( D_x \\) already enforces \\( x + h \\Delta S \\geq 0 \\) quasi-surely, and integrability (from A) handles the rest.\n\nThus, only A and C are essential for the compactness and semicontinuity arguments that guarantee the optimum.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 384, "Question": "### Background\n\n**Research Question.** How can the causal effect of foreign official inflows on U.S. Treasury yields be estimated when these inflows are endogenous to market conditions, and are the findings robust across different measures of risk premium?\n\n**Setting.** A two-stage least squares (2SLS) regression framework is used to estimate the short-run causal effect of foreign official inflows on two different measures of the bond risk premium: the model-based 5-year term premium (`\\Delta TP_t^5`) and the model-free realized one-year excess return (`XR_6t`). A key challenge is that unobserved “flight-to-safety” episodes can simultaneously lower Treasury yields and reduce foreign official inflows, biasing ordinary least squares (OLS) estimates.\n\n### Data / Model Specification\n\nThe general regression model is:\n\n  \n\\text{Risk Premium}_t = \\alpha + \\beta_1 (\\text{Foreign Inflows}_t) + \\mathbf{X}_t'\\gamma + u_t\n \n\nwhere `\\mathbf{X}_t` is a vector of exogenous controls. To address the endogeneity of `\\text{Foreign Inflows}_t`, instruments such as Japanese foreign exchange interventions (`JPYFXINT_t`) are used.\n\n**Table 1. The Impact of Foreign Official Inflows on U.S. Bond Risk Premia**\n\n| | (1) OLS | (2) IV / 2SLS | (3) IV / 2SLS | (4) IV / 2SLS |\n| :--- | :---: | :---: | :---: | :---: |\n| **Dependent Variable:** | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `XR_6t` |\n| | | | | |\n| `\\Delta FOIL/DEBT_{t-1}` | 0.052 | -0.135** | | |\n| (Monthly Inflows) | (0.030) | (0.061) | | |\n| | | | | |\n| `\\Delta FOIL_JAPAN/DEBT_{t-1}` | | | -0.147** | |\n| (Japanese Monthly Inflows) | | | (0.059) | | |\n| | | | | |\n| `\\sum_{12} FOI_t / DEBT_{t-12}` | | | | 0.595*** |\n| (Annual Inflows) | | | | (0.184) |\n| | | | | |\n| **Specification Tests** | | | | |\n| Cragg-Donald F-Stat | | 15.72 | 97.59 | 92.79 |\n| *Weak Inst. Critical Value* | | *11.59* | *8.96* | *11.59* |\n| Hansen J Test (p-value) | | 0.3498 | n.a. | 0.3563 |\n\n*Notes: Standard errors in parentheses. *p<0.1, **p<0.05, ***p<0.01. `\\Delta TP_t^5` is the monthly change in the 5-year term premium. `XR_6t` is the one-year realized excess return. `FOIL/DEBT` denotes foreign official inflows scaled by outstanding marketable debt.*\n\n### Question\n\nThe study uses several methods to validate its main finding. Based on the results in Table 1, select all statements that accurately describe these validation and robustness checks.", "Options": {"A": "The result in Column (4) validates the main finding using a model-free measure of risk premium (realized excess returns), showing that foreign inflows have a statistically significant effect that is directionally consistent with the term premium analysis.", "B": "The positive coefficient (0.595) in the excess returns regression (Column 4) contradicts the negative coefficient (-0.135) in the term premium regression (Column 2), suggesting the paper's main conclusion is not robust.", "C": "The analysis of Japanese inflows in Column (3) serves as a strong robustness check because its result (-0.147) is quantitatively similar to the main estimate (-0.135) and is identified by an exceptionally strong instrument (F-stat ≈ 98).", "D": "The coefficient of 0.595 in Column (4) is much larger in absolute value than the coefficient in Column (2), indicating that the economic impact of foreign inflows on realized excess returns is far greater than the impact on the term premium."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to synthesize results from multiple robustness checks, converted from a Table QA problem. It uses an **Atomic Decomposition** strategy, transforming parts 3 and 4 of the original QA into testable propositions. **Correct options (A, B)** require understanding why the Japan-specific result is a powerful check and why the excess return regression provides model-free validation. **Distractor C (Magnitude Misinterpretation)** tempts a superficial comparison of coefficients that have different units and meanings. **Distractor D (Conceptual Error)** targets a critical misunderstanding: a positive effect on excess returns is mechanically linked to a negative effect on yields, so the results are consistent, not contradictory.", "qid": "384", "question": "### Background\n\n**Research Question.** How can the causal effect of foreign official inflows on U.S. Treasury yields be estimated when these inflows are endogenous to market conditions, and are the findings robust across different measures of risk premium?\n\n**Setting.** A two-stage least squares (2SLS) regression framework is used to estimate the short-run causal effect of foreign official inflows on two different measures of the bond risk premium: the model-based 5-year term premium (`\\Delta TP_t^5`) and the model-free realized one-year excess return (`XR_6t`). A key challenge is that unobserved “flight-to-safety” episodes can simultaneously lower Treasury yields and reduce foreign official inflows, biasing ordinary least squares (OLS) estimates.\n\n### Data / Model Specification\n\nThe general regression model is:\n\n  \n\\text{Risk Premium}_t = \\alpha + \\beta_1 (\\text{Foreign Inflows}_t) + \\mathbf{X}_t'\\gamma + u_t\n \n\nwhere `\\mathbf{X}_t` is a vector of exogenous controls. To address the endogeneity of `\\text{Foreign Inflows}_t`, instruments such as Japanese foreign exchange interventions (`JPYFXINT_t`) are used.\n\n**Table 1. The Impact of Foreign Official Inflows on U.S. Bond Risk Premia**\n\n| | (1) OLS | (2) IV / 2SLS | (3) IV / 2SLS | (4) IV / 2SLS |\n| :--- | :---: | :---: | :---: | :---: |\n| **Dependent Variable:** | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `XR_6t` |\n| | | | | |\n| `\\Delta FOIL/DEBT_{t-1}` | 0.052 | -0.135** | | |\n| (Monthly Inflows) | (0.030) | (0.061) | | |\n| | | | | |\n| `\\Delta FOIL_JAPAN/DEBT_{t-1}` | | | -0.147** | |\n| (Japanese Monthly Inflows) | | | (0.059) | | |\n| | | | | |\n| `\\sum_{12} FOI_t / DEBT_{t-12}` | | | | 0.595*** |\n| (Annual Inflows) | | | | (0.184) |\n| | | | | |\n| **Specification Tests** | | | | |\n| Cragg-Donald F-Stat | | 15.72 | 97.59 | 92.79 |\n| *Weak Inst. Critical Value* | | *11.59* | *8.96* | *11.59* |\n| Hansen J Test (p-value) | | 0.3498 | n.a. | 0.3563 |\n\n*Notes: Standard errors in parentheses. *p<0.1, **p<0.05, ***p<0.01. `\\Delta TP_t^5` is the monthly change in the 5-year term premium. `XR_6t` is the one-year realized excess return. `FOIL/DEBT` denotes foreign official inflows scaled by outstanding marketable debt.*\n\n### Question\n\nThe study uses several methods to validate its main finding. Based on the results in Table 1, select all statements that accurately describe these validation and robustness checks.", "model_evaluation_result": {"model_raw_response": "\n\nA,C\n\n## Explanation\n\nThe reasoning process evaluates each option against the results in Table 1 to determine which statements accurately describe the validation and robustness checks for the study's main finding that foreign official inflows negatively affect U.S. Treasury yields (implying a negative effect on risk premia).\n\n- For option A: The main finding from Columns (2) and (3) shows a negative and statistically significant effect of inflows on the model-based term premium (ΔTP_t^5). Column (4) uses a model-free measure (XR_6t, realized excess returns) and reports a positive coefficient (0.595***), which is statistically significant. In bond pricing, a positive\n> 🔍 **Analyzing research question**\n> ***- The study examines how foreign official inflows affect U.S. Treasury yields, addressing endogeneity with a 2SLS framework.***\n excess return corresponds to a downward shift in yields, aligning directionally with the negative term premium effect. Thus, this validates robustness across measures.\n\n- For option B: The coefficients differ in sign (negative in Column 2 vs. positive in Column 4), but this does not contradict the main conclusion. The positive sign in the excess returns specification is consistent with the expected negative yield impact, as excess returns measure investor gains from lower yields. The statement's claim of contradiction is inaccurate.\n\n- For option C: The main estimate in Column (2) is -0.135**, and the Japanese-specific estimate in Column (3) is -0.147**, which are quantitatively similar (both around -0.14 in magnitude). The Cragg-Donald F-statistic of 97.59 exceeds the weak instrument critical value (8.96), confirming a strong instrument. This similarity and instrument strength make it a valid robustness check.\n\n- For option D: The absolute value of the coefficient in Column (4) (0.595) is larger than in Column (2) (0.135), but the statement incorrectly equates this to a greater economic impact. The variables differ: Column (2) uses monthly flows (ΔFOIL/DEBT_{t-1}), while Column (4) uses annual cumulative flows (∑_{12} FOI_t / DEBT_{t-12}), making direct comparisons of magnitude inappropriate without scaling.\n\nBased on this analysis, options A and C accurately describe the checks, while B and D do not.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 338, "Question": "### Background\n\n**Research Question.** How are the equilibrium trading strategies and market liquidity determined when one versus two informed central banks intervene in a foreign exchange market?\n\n**Setting.** A Kyle-style microstructure model where informed central banks strategically trade. We compare the linear Nash equilibrium under unilateral intervention (one bank) with the one under coordinated intervention (two identical banks).\n\n**Variables and Parameters.**\n- `x_k`, `x_i`: Central bank order (coordinated, unilateral).\n- `f`: Fundamental value.\n- `t`: Central bank target.\n- `q`: Weight on stabilization objective.\n- `λ`, `λ_1`: Price impact parameter (coordinated, unilateral). A higher `λ` means lower liquidity.\n- `β`, `β_1`: Coefficients measuring the intensity of trading on fundamentals in the linear strategy `x = α + βf + γt`.\n\n---\n\n### Data / Model Specification\n\nIn equilibrium, the central bank's trading intensity on fundamentals is given by:\n-   Coordinated: `β = 1 / (λ(4qλ+3))`\n-   Unilateral: `β_1 = 1 / (2λ_1(qλ_1+1))`\n\nThe paper proves that market liquidity is higher in the coordinated case, which means the price impact parameter is lower (`λ < λ_1`).\n\n---\n\n### Question\n\nBased on the model specifications provided, which of the following statements about a central bank's trading intensity on fundamentals (`β`) are correct?", "Options": {"A": "In the coordinated case, each central bank trades *less* aggressively on its fundamental information than a bank acting alone (`β < β_1`).", "B": "The higher trading aggressiveness in the coordinated case (`β`) is a strategic response to the *greater* market liquidity (lower price impact, `λ < λ_1`) that prevails under coordination.", "C": "The higher trading aggressiveness in the coordinated case (`β`) is necessary to overcome the *lower* market liquidity (higher price impact, `λ > λ_1`) that prevails under coordination.", "D": "In the coordinated case, each central bank trades *more* aggressively on its fundamental information than a bank acting alone (`β > β_1`)."}, "Answer": ["B", "D"], "pi_justification": "Replaced QA (Suitability Score: 9.5). The original QA's core analytical insight—a counter-intuitive result about trading intensity—is an atomic, indisputable conclusion ideal for a choice format. This item uses Atomic Decomposition to test both the directional result and its underlying economic cause, ensuring a genuine multi-select question. Distractors are high-fidelity: (C) is a strong conceptual opposite based on flawed intuition, and (D) presents a causal error, testing for a complete understanding of the mechanism.", "qid": "338", "question": "### Background\n\n**Research Question.** How are the equilibrium trading strategies and market liquidity determined when one versus two informed central banks intervene in a foreign exchange market?\n\n**Setting.** A Kyle-style microstructure model where informed central banks strategically trade. We compare the linear Nash equilibrium under unilateral intervention (one bank) with the one under coordinated intervention (two identical banks).\n\n**Variables and Parameters.**\n- `x_k`, `x_i`: Central bank order (coordinated, unilateral).\n- `f`: Fundamental value.\n- `t`: Central bank target.\n- `q`: Weight on stabilization objective.\n- `λ`, `λ_1`: Price impact parameter (coordinated, unilateral). A higher `λ` means lower liquidity.\n- `β`, `β_1`: Coefficients measuring the intensity of trading on fundamentals in the linear strategy `x = α + βf + γt`.\n\n---\n\n### Data / Model Specification\n\nIn equilibrium, the central bank's trading intensity on fundamentals is given by:\n-   Coordinated: `β = 1 / (λ(4qλ+3))`\n-   Unilateral: `β_1 = 1 / (2λ_1(qλ_1+1))`\n\nThe paper proves that market liquidity is higher in the coordinated case, which means the price impact parameter is lower (`λ < λ_1`).\n\n---\n\n### Question\n\nBased on the model specifications provided, which of the following statements about a central bank's trading intensity on fundamentals (`β`) are correct?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": ["A"], "error": null}}
{"ID": 330, "Question": "### Background\n\n**Research Question.** How do changes in a firm's growth prospects and capital intensity differentially affect its cash flow-based versus its earnings-based valuation multiples?\n\n**Setting.** A simulation using the Staples, Inc. financial model where different value drivers are changed one at a time, each calibrated to produce a 10% increase in the firm's enterprise value (EV). The model assumes that investments in working capital and property, plant, and equipment (PPE) scale with revenue.\n\n**Variables and Parameters.**\n- `Enterprise Value (EV)`: The market value of the firm's operating assets.\n- `Unlevered Free Cash Flow (UFCF)`: Cash flow available to all capital providers after all operating expenses and investments.\n- `EBIT`: Earnings Before Interest and Taxes.\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization.\n\n---\n\n### Data / Model Specification\n\nThe percentage change in a multiple is calculated as:\n\n  \n(1 + \\% \\Delta \\text{Multiple}) = \\frac{(1 + \\% \\Delta \\text{Numerator})}{(1 + \\% \\Delta \\text{Denominator})} \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Simulation of Changes in Growth and Capital Expenditure**\n| Scenario | Multiple Type | % Change in Numerator (EV) | % Change in Denominator | % Change in Multiple |\n| :--- | :--- | :--- | :--- | :--- |\n| **Increase Initial Revenue Growth** | Unlevered Free Cash Flow | +10.0% | -4.3% | +14.9% |\n| | EBIT | +10.0% | +2.3% | +7.5% |\n| **Reduce Net PPE Growth (CapEx)** | Unlevered Free Cash Flow | +10.0% | +10.4% | -0.3% |\n| | EBITDA | +10.0% | 0.0% | +10.0% |\n\n---\n\n### Question\n\nBased on the simulation results in **Table 1**, which of the following conclusions about the situational reliability of different valuation multiples are supported?", "Options": {"A": "For comparing two firms with similar capital intensity but different near-term growth rates, an earnings-based multiple like EV/EBIT is likely more stable and reliable than the EV/UFCF multiple.", "B": "The EV/Unlevered Free Cash Flow multiple is a reliable metric for comparing firms that differ primarily in their capital intensity, as changes in CapEx affect both the numerator (EV) and the denominator (UFCF) by similar percentages.", "C": "The EV/EBITDA multiple is the most robust metric for comparing firms with different capital expenditure requirements because its denominator is unaffected by depreciation policies.", "D": "The EV/Unlevered Free Cash Flow multiple is highly sensitive to changes in revenue growth, as the necessary investments in working capital and PPE can cause the UFCF denominator to decrease even as firm value increases."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Premise/Assumption Packaging strategy to test the paper's nuanced conclusion that no single multiple is always superior. It requires synthesizing contrasting simulation results to understand the trade-offs. Options A, B, and C are atomic decompositions of the core argument. Distractor (D) targets a common practitioner misconception (confusing an unaffected denominator with a robust multiple), making it a high-fidelity distractor.", "qid": "330", "question": "### Background\n\n**Research Question.** How do changes in a firm's growth prospects and capital intensity differentially affect its cash flow-based versus its earnings-based valuation multiples?\n\n**Setting.** A simulation using the Staples, Inc. financial model where different value drivers are changed one at a time, each calibrated to produce a 10% increase in the firm's enterprise value (EV). The model assumes that investments in working capital and property, plant, and equipment (PPE) scale with revenue.\n\n**Variables and Parameters.**\n- `Enterprise Value (EV)`: The market value of the firm's operating assets.\n- `Unlevered Free Cash Flow (UFCF)`: Cash flow available to all capital providers after all operating expenses and investments.\n- `EBIT`: Earnings Before Interest and Taxes.\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization.\n\n---\n\n### Data / Model Specification\n\nThe percentage change in a multiple is calculated as:\n\n  \n(1 + \\% \\Delta \\text{Multiple}) = \\frac{(1 + \\% \\Delta \\text{Numerator})}{(1 + \\% \\Delta \\text{Denominator})} \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Simulation of Changes in Growth and Capital Expenditure**\n| Scenario | Multiple Type | % Change in Numerator (EV) | % Change in Denominator | % Change in Multiple |\n| :--- | :--- | :--- | :--- | :--- |\n| **Increase Initial Revenue Growth** | Unlevered Free Cash Flow | +10.0% | -4.3% | +14.9% |\n| | EBIT | +10.0% | +2.3% | +7.5% |\n| **Reduce Net PPE Growth (CapEx)** | Unlevered Free Cash Flow | +10.0% | +10.4% | -0.3% |\n| | EBITDA | +10.0% | 0.0% | +10.0% |\n\n---\n\n### Question\n\nBased on the simulation results in **Table 1**, which of the following conclusions about the situational reliability of different valuation multiples are supported?", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 322, "Question": "### Background\n\nA study on board governance finds that a higher ratio of women directors significantly improves a board's effectiveness in `Strategic Control` (monitoring long-term, qualitative issues like brand reputation, CSR, and innovation) but has no statistically significant effect on `Board Operational Control` (monitoring short-term, quantitative issues like budgets and cost management).\n\n---\n\n### Question\n\nConsider two firms: Firm A, a high-end consumer technology company whose value depends heavily on long-term innovation and brand image, and Firm B, a regulated utility whose value depends primarily on efficient capital allocation and cost control. Based on the study's findings, select all likely consequences of increasing the ratio of women directors on each firm's board.", "Options": {"A": "In Firm B, where operational control is the primary driver of value, the effect of a higher ratio of women directors on overall firm performance is likely to be statistically insignificant.", "B": "In Firm B, a higher ratio of women directors would likely harm firm performance by shifting the board's focus away from critical operational details.", "C": "In Firm A, where strategic control is a key driver of value, a higher ratio of women directors is likely to translate into a measurable improvement in overall firm performance.", "D": "The impact of increasing the ratio of women directors on firm performance would be equally positive for both Firm A and Firm B, as good governance practices are universally beneficial."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to apply a core theoretical finding to novel business scenarios, a key 'Scenario Application' skill. It tests whether the student can connect the specific type of board effectiveness improved by gender diversity (strategic control) to the business model where that effectiveness is most critical. Options A and B represent the two correct applications of this logic. Distractor C introduces a common error of interpreting a null effect as a negative one (Sign Error). Distractor D directly contradicts the paper's central premise of task-dependency (Conceptual Opposite).", "qid": "322", "question": "### Background\n\nA study on board governance finds that a higher ratio of women directors significantly improves a board's effectiveness in `Strategic Control` (monitoring long-term, qualitative issues like brand reputation, CSR, and innovation) but has no statistically significant effect on `Board Operational Control` (monitoring short-term, quantitative issues like budgets and cost management).\n\n---\n\n### Question\n\nConsider two firms: Firm A, a high-end consumer technology company whose value depends heavily on long-term innovation and brand image, and Firm B, a regulated utility whose value depends primarily on efficient capital allocation and cost control. Based on the study's findings, select all likely consequences of increasing the ratio of women directors on each firm's board.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 410, "Question": "### Background\n\n**Research Question.** This case examines two competing theories regarding the role of Corporate Social Responsibility (CSR) and its relationship with corporate governance. Does effective governance, proxied by board independence, encourage or curtail CSR activities?\n\n**Setting.** The analysis is framed by a fundamental conflict in corporate finance: the potential divergence of interests between a firm's managers and its shareholders. CSR investments are a key area where this conflict may manifest.\n\n---\n\n### Data / Model Specification\n\nThe study evaluates two competing hypotheses regarding the link between board independence and CSR:\n\n1.  **The Conflict Resolution Hypothesis.** This hypothesis posits that CSR is a value-maximizing tool used to resolve costly conflicts among various stakeholders (e.g., employees, customers, community). By mitigating these conflicts, CSR ultimately enhances firm performance and shareholder wealth. Effective governance should therefore promote value-enhancing CSR.\n\n2.  **The Risk-Mitigation / Agency Cost Hypothesis.** This hypothesis argues that managers, whose human and financial capital are poorly diversified and tied to the firm, are more risk-averse than the firm's well-diversified shareholders. To the extent that CSR activities reduce firm-specific risk, managers may over-invest in them to protect their personal wealth, even if such investments are not optimal for shareholders. This over-investment is an agency cost that effective governance should curtail.\n\n---\n\n### Question\n\nThe paper evaluates two competing hypotheses to explain the relationship between corporate governance and CSR. Which of the following statements accurately describe the logic and predictions of these hypotheses? Select all that apply.", "Options": {"A": "The agency conflict described in the paper arises because well-diversified shareholders are more averse to firm-specific risk than undiversified managers.", "B": "According to the logic of Jensen's inequality as applied to managerial risk aversion, a risk-averse manager's expected utility is maximized by maximizing the firm's expected value, regardless of risk.", "C": "The Risk-Mitigation/Agency Cost Hypothesis predicts a negative relationship between board independence and CSR, viewing CSR as a potential over-investment driven by managers' personal risk aversion.", "D": "The Conflict Resolution Hypothesis predicts a positive relationship between board independence and CSR, viewing CSR as a tool to maximize shareholder wealth."}, "Answer": ["C", "D"], "pi_justification": "REPLACED General QA (Suitability Score: 9.0; A=8, B=10). The original question tested recall of two opposing theories, making it highly suitable for conversion. This item uses a 'Premise Packaging' strategy to test the core logic of each hypothesis. Distractors are designed as 'Conceptual Opposites' (C) or misapplications of underlying theory (D), targeting high-frequency errors in corporate finance.", "qid": "410", "question": "### Background\n\n**Research Question.** This case examines two competing theories regarding the role of Corporate Social Responsibility (CSR) and its relationship with corporate governance. Does effective governance, proxied by board independence, encourage or curtail CSR activities?\n\n**Setting.** The analysis is framed by a fundamental conflict in corporate finance: the potential divergence of interests between a firm's managers and its shareholders. CSR investments are a key area where this conflict may manifest.\n\n---\n\n### Data / Model Specification\n\nThe study evaluates two competing hypotheses regarding the link between board independence and CSR:\n\n1.  **The Conflict Resolution Hypothesis.** This hypothesis posits that CSR is a value-maximizing tool used to resolve costly conflicts among various stakeholders (e.g., employees, customers, community). By mitigating these conflicts, CSR ultimately enhances firm performance and shareholder wealth. Effective governance should therefore promote value-enhancing CSR.\n\n2.  **The Risk-Mitigation / Agency Cost Hypothesis.** This hypothesis argues that managers, whose human and financial capital are poorly diversified and tied to the firm, are more risk-averse than the firm's well-diversified shareholders. To the extent that CSR activities reduce firm-specific risk, managers may over-invest in them to protect their personal wealth, even if such investments are not optimal for shareholders. This over-investment is an agency cost that effective governance should curtail.\n\n---\n\n### Question\n\nThe paper evaluates two competing hypotheses to explain the relationship between corporate governance and CSR. Which of the following statements accurately describe the logic and predictions of these hypotheses? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 414, "Question": "### Background\n\n**Research Question.** Is the magnitude of the stock price drop at a seasoned equity offering (SEO) announcement related to the timing of the announcement relative to the firm's last public information release?\n\n**Setting / Data-Generating Environment.** The analysis begins with an event study to establish the baseline price reaction to SEOs. It then uses a weighted least squares (WLS) regression to test if the magnitude of this reaction varies with issue timing. The sample is split into two scenarios, with this question focusing on the primary scenario:\n- **Case 1:** An earnings release is followed by an issue announcement, which is followed by the issue (no intervening release).\n\n**Variables & Parameters.**\n- `Announcement Return`: Two-day abnormal return around the SEO announcement, calculated as the firm's return minus the return on an equal-weighted market portfolio.\n- `Days_Announce`: The number of calendar days between the issue announcement and the previous earnings release.\n\n### Data / Model Specification\n\n**Table 1. Average Abnormal Returns Around SEO Announcement Day (%)**\n\n| Day relative to event | Announcement Abnormal Return | Announcement t-statistic |\n|:---------------------|:-----------------------------|:-------------------------|\n| -2                   | -0.19                        | -1.84                    |\n| -1                   | -2.26                        | -20.64                   |\n| 0                    | -0.43                        | -3.74                    |\n| 1                    | 0.11                         | 1.00                     |\n\n*Source: Adapted from the original paper's Table 2. Day 0 is the date of the announcement in the Wall Street Journal.*\n\nThe following regression model is estimated for the **Case 1** subsample:\n  \n\\text{Announcement Return}_{i} = \\beta_0 + \\beta_1 \\times \\text{Days\\_Announce}_{i} + \\epsilon_i \\quad \\text{(Eq. (1))}\n \n\n**Table 2. WLS Regression of Announcement Return on Timing (Case 1 only)**\n\n| Dependent Variable | Explanatory Variable | Constant `(x10^-2)` | Timing Coeff. `(x10^-3)` | N |\n|:-------------------|:---------------------|:--------------------|:-------------------------|:--|\n| Announcement Return| `Days_Announce`      | -2.251** (0.306)    | -0.147* (0.085)          | 435|\n\n*White heteroskedasticity-robust standard errors in parentheses. *Significant at 10% level. **Significant at 1% level.*\n\n### Question\n\nBased on the provided data and the theory of time-varying adverse selection, select all of the following statements that are supported by the evidence.", "Options": {"A": "The regression results indicate that while the timing of an announcement is statistically significant at the 10% level, its economic impact on the price drop is negligible.", "B": "The potential for errors-in-variables bias in the regression suggests that the reported coefficient `β̂₁` likely overstates the true negative relationship between announcement timing and price reaction.", "C": "The regression coefficient on `Days_Announce` implies that a 30-day delay in announcing an issue is associated with an additional price drop of approximately 0.44%.", "D": "The two-day cumulative abnormal return (days -1 and 0) associated with an SEO announcement is approximately -2.69%, consistent with a 'lemons market' interpretation."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the paper's core pricing result (Hypothesis 3). It uses an 'atomic decomposition' strategy, breaking the finding into testable statements about the baseline price drop (A), the economic magnitude of the timing effect (B), and the implication of an econometric critique (C/D). Distractor C presents the conceptual opposite of attenuation bias (which biases toward zero, thus understating the effect). Distractor D misinterprets the paper's conclusion on economic significance.", "qid": "414", "question": "### Background\n\n**Research Question.** Is the magnitude of the stock price drop at a seasoned equity offering (SEO) announcement related to the timing of the announcement relative to the firm's last public information release?\n\n**Setting / Data-Generating Environment.** The analysis begins with an event study to establish the baseline price reaction to SEOs. It then uses a weighted least squares (WLS) regression to test if the magnitude of this reaction varies with issue timing. The sample is split into two scenarios, with this question focusing on the primary scenario:\n- **Case 1:** An earnings release is followed by an issue announcement, which is followed by the issue (no intervening release).\n\n**Variables & Parameters.**\n- `Announcement Return`: Two-day abnormal return around the SEO announcement, calculated as the firm's return minus the return on an equal-weighted market portfolio.\n- `Days_Announce`: The number of calendar days between the issue announcement and the previous earnings release.\n\n### Data / Model Specification\n\n**Table 1. Average Abnormal Returns Around SEO Announcement Day (%)**\n\n| Day relative to event | Announcement Abnormal Return | Announcement t-statistic |\n|:---------------------|:-----------------------------|:-------------------------|\n| -2                   | -0.19                        | -1.84                    |\n| -1                   | -2.26                        | -20.64                   |\n| 0                    | -0.43                        | -3.74                    |\n| 1                    | 0.11                         | 1.00                     |\n\n*Source: Adapted from the original paper's Table 2. Day 0 is the date of the announcement in the Wall Street Journal.*\n\nThe following regression model is estimated for the **Case 1** subsample:\n  \n\\text{Announcement Return}_{i} = \\beta_0 + \\beta_1 \\times \\text{Days\\_Announce}_{i} + \\epsilon_i \\quad \\text{(Eq. (1))}\n \n\n**Table 2. WLS Regression of Announcement Return on Timing (Case 1 only)**\n\n| Dependent Variable | Explanatory Variable | Constant `(x10^-2)` | Timing Coeff. `(x10^-3)` | N |\n|:-------------------|:---------------------|:--------------------|:-------------------------|:--|\n| Announcement Return| `Days_Announce`      | -2.251** (0.306)    | -0.147* (0.085)          | 435|\n\n*White heteroskedasticity-robust standard errors in parentheses. *Significant at 10% level. **Significant at 1% level.*\n\n### Question\n\nBased on the provided data and the theory of time-varying adverse selection, select all of the following statements that are supported by the evidence.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 206, "Question": "### Background\n\n**Research Question.** Why is it necessary to use multiple, complementary tests like the Augmented Dickey-Fuller (ADF) and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) to establish the non-stationarity of a time series?\n\n**Setting / Data-Generating Environment.** The ADF test has a null hypothesis of a unit root (non-stationarity), while the KPSS test has a null hypothesis of stationarity. The paper notes that standard unit root tests like ADF have \"low power against stable autoregressive alternatives with roots near unity.\" The KPSS test statistic `η_u` is also known to be sensitive to the choice of a lag parameter `L`, which is used to estimate the long-run variance `S²(L)` in the denominator of the test statistic.\n\n---\n\n### Question\n\nSelect ALL of the following statements that correctly describe the properties, limitations, and mechanics of the ADF and KPSS tests.", "Options": {"A": "The KPSS test statistic, `η_u`, is sensitive to the choice of the lag parameter `L`; it typically decreases as `L` increases because the long-run variance estimator in the denominator, `S²(L)`, grows larger by incorporating more positive autocorrelation terms.", "B": "Choosing a larger lag parameter `L` in the KPSS test is always better as it increases the test statistic `η_u`, making it more likely to correctly detect non-stationarity.", "C": "The ADF test is preferred for its high power in distinguishing between a true unit root and a stationary process with a root close to one, a weakness of the KPSS test.", "D": "The KPSS test is used to complement the ADF test because ADF tests often have low power, meaning they may fail to reject the unit root null hypothesis even when the series is a highly persistent but stationary process."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses a deep conceptual understanding of the statistical theory behind unit root testing. It uses a 'Premise/Assumption Packaging' strategy to test knowledge of the tests' properties. Option A addresses the critical concept of statistical power, which motivates the use of the KPSS test. Option B tests the mechanical understanding of how the KPSS statistic is constructed and why it is sensitive to the lag parameter. Distractors C and D are 'Conceptual Opposites' that directly contradict the correct statements, targeting fundamental misunderstandings of power and test construction.", "qid": "206", "question": "### Background\n\n**Research Question.** Why is it necessary to use multiple, complementary tests like the Augmented Dickey-Fuller (ADF) and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) to establish the non-stationarity of a time series?\n\n**Setting / Data-Generating Environment.** The ADF test has a null hypothesis of a unit root (non-stationarity), while the KPSS test has a null hypothesis of stationarity. The paper notes that standard unit root tests like ADF have \"low power against stable autoregressive alternatives with roots near unity.\" The KPSS test statistic `η_u` is also known to be sensitive to the choice of a lag parameter `L`, which is used to estimate the long-run variance `S²(L)` in the denominator of the test statistic.\n\n---\n\n### Question\n\nSelect ALL of the following statements that correctly describe the properties, limitations, and mechanics of the ADF and KPSS tests.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"KPSS test statistic sensitivity to lag parameter L\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"ADF test low power against stable autoregressive alternatives near unity\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"why use both ADF and KPSS tests for stationarity\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"kpsstest - KPSS test for stationarity - MATLAB\" MathWorks](https://www.mathworks.com/help/econ/kpsstest.html)\n> [🔗 \"statsmodels.tsa.stattools.kpss - statsmodels 0.14.4\" statsmodels](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html)\n> [🔗 \"forecasting - R (auto.arima function): number of lags in KPSS test - Cross Validated\" Stack Exchange](https://stats.stackexchange.com/questions/350260/r-auto-arima-function-number-of-lags-in-kpss-test)\n> [🔗 \"statsmodels.tsa.stattools.kpss - statsmodels 0.15.0 (+706)\" statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.kpss.html)\n> [🔗 \"Kwiatkowski-Phillips-Schmidt-Shin (KPSS) - GeeksforGeeks\" GeeksforGeeks](https://www.geeksforgeeks.org/python/kwiatkowski-phillips-schmidt-shin-kpss/)\n> [🔗 \"KPSS Test for Stationarity - Machine Learning Plus\" Machine Learning Plus](https://www.machinelearningplus.com/time-series/kpss-test-for-stationarity/)\n> [🔗 \"stata - Is there an optmal lag choice in the KPSS test? - Cross Validated\" Stack Exchange](https://stats.stackexchange.com/questions/151080/is-there-an-optmal-lag-choice-in-the-kpss-test)\n> [🔗 \"Sensitivity Analysis of Stationarity Tests’ Outcome to Time Series Facets and Test Parameters\" Hindawi](https://www.hindawi.com/journals/mpe/2022/2402989/)\n> [🔗 \"5.4 KPSS test | Applied Time Series Analysis for Fisheries and Environmental Sciences\" Atsa-es](https://atsa-es.github.io/atsa-labs/sec-boxjenkins-kpss.html)\n> [🔗 \"R: KPSS tests for stationarity\" R Project](https://search.r-project.org/CRAN/refmans/VGAMextra/html/KPSS.test.html)\n\n> 🔍 **10 results found**\n\n> [🔗 \"Augmented Dickey-Fuller (ADF) Test\" Rtmath](https://rtmath.net/assets/docs/finmath/html/93a7b7b9-e3c3-4f19-8a57-49c3938d607d.htm)\n> [🔗 \"Augmented Dickey–Fuller test - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Augmented_Dickey%25E2%2580%2593Fuller_test)\n> [🔗 \"Dickey–Fuller test - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Dickey%25E2%2580%2593Fuller_test)\n> [🔗 \"The power of the ADF test - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0165176597818721)\n> [🔗 \"ADF Test: The ADF Test: A Beacon for Stationarity in Time Series Analysis - FasterCapital\" FasterCapital](https://www.fastercapital.com/content/ADF-Test--The-ADF-Test--A-Beacon-for-Stationarity-in-Time-Series-Analysis.html)\n> [🔗 \"Mastering the Augmented Dickey-Fuller Test for Time Series Analysis\" Number Analytics](https://www.numberanalytics.com/blog/augmented-dickey-fuller-test-time-series)\n> [🔗 \"On the distributions of Augmented Dickey–Fuller statistics in processes with moving average components - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304407698000979)\n> [🔗 \"time series - Unit root test (e.g. ADF) vs Autocorrelation test (e.g. Breusch-Godfrey) - Cross Validated\" Stack Exchange](https://stats.stackexchange.com/questions/466818/unit-root-test-e-g-adf-vs-autocorrelation-test-e-g-breusch-godfrey)\n> [🔗 \"adftest - Augmented Dickey-Fuller test - MATLAB\" MathWorks](https://www.mathworks.com/help/econ/adftest.html)\n> [🔗 \"Augmented Dickey Fuller Test (ADF Test) – Must Read Guide\" Machine Learning Plus](https://www.machinelearningplus.com/time-series/augmented-dickey-fuller-test/)\n\n> 🔍 **10 results found**\n\n> [🔗 \"Stationarity and detrending (ADF/KPSS) - statsmodels 0.15.0 (+710)\" statsmodels](https://www.statsmodels.org/dev/examples/notebooks/generated/stationarity_detrending_adf_kpss.html)\n> [🔗 \"COMPARISION STUDY OF ADF vs KPSS TEST | by Tannyasharma | Medium\" Medium](https://medium.com/%2540tannyasharma21/comparision-study-of-adf-vs-kpss-test-c9d8dec4f62a)\n> [🔗 \"Stationarity and detrending (ADF/KPSS) - statsmodels 0.14.4\" statsmodels](https://www.statsmodels.org/stable/examples/notebooks/generated/stationarity_detrending_adf_kpss.html)\n> [🔗 \"python - Interpret the results of ADF and KPSS tests - Stack Overflow\" Stack Overflow](https://stackoverflow.com/questions/73554240/interpret-the-results-of-adf-and-kpss-tests)\n> [🔗 \"Stationarity and detrending (ADF/KPSS) - statsmodels 0.15.0 (+649)\" Statsmodels](https://www.statsmodels.org/devel/examples/notebooks/generated/stationarity_detrending_adf_kpss.html)\n> [🔗 \"time series - Contradictory results of ADF and KPSS unit root tests - Cross Validated\" Stack Exchange](https://stats.stackexchange.com/questions/239360/contradictory-results-of-adf-and-kpss-unit-root-tests)\n> [🔗 \"KPSS Test for Stationarity - Machine Learning Plus\" Machine Learning Plus](https://www.machinelearningplus.com/time-series/kpss-test-for-stationarity/)\n> [🔗 \"Stationarity in Time Series: The Power of ADF and KPSS Tests for ARIMA Models\" Developer-service](https://developer-service.blog/stationarity-in-time-series-the-power-of-adf-and-kpss-tests-for-arima-models/)\n> [🔗 \"\"How to Test for Stationarity in Time Series: A Deep Dive into ...\" LinkedIn](https://www.linkedin.com/pulse/how-test-stationarity-time-series-deep-dive-adf-pp-ravichandran-usy1c#:~:text=ADF%2520:%2520Tests%2520for%2520the%2520presence%252Cand%2520testing%2520against%2520non-stationarity.)\n> [🔗 \"Unit root tests (ADF, KPSS) | Intro to Time Series Class Notes | Fiveable | Fiveable\" Fiveable](https://library.fiveable.me/intro-time-series/unit-3/unit-root-tests-adf-kpss/study-guide/psGQzOPIpDJwW3lx)\n\n> 🔍 **Analyzing test properties**\n> ***- The KPSS test statistic `η_u` is sensitive to the lag parameter `L`; it typically decreases as `L` increases due to a larger long-run variance estimator.***\n\n\nA,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 328, "Question": "### Background\n\n**Research Question.** This case provides a comprehensive empirical evaluation of the consumption-based asset pricing model (CCAPM), testing whether incorporating taxes and tax-timing options can resolve its well-known empirical failures.\n\n**Setting.** The analysis compares the benchmark tax-free CCAPM (Model 1) against the paper's preferred specification, an asymmetric tax model (Model 4). Both models are estimated using the Generalized Method of Moments (GMM).\n\n**Variables and Parameters.**\n\n*   `J-statistic (χ²)`: GMM test for a model's overidentifying restrictions. A high value (low p-value) indicates model rejection.\n*   `u_{j,t+1}`: The model's pricing error for asset `j` (risk-adjusted after-tax return).\n\n---\n\n### Data / Model Specification\n\nThe GMM estimation results for the tax-free and asymmetric tax models are summarized in Table 1.\n\n**Table 1: GMM Estimation Results for Competing CCAPM Specifications**\n\n| Parameter / Statistic | Model 1 (Tax-Free) | Model 4 (Asymmetric Tax) |\n| :--- | :--- | :--- |\n| `J-statistic (χ²)` | 33.424 | 11.154 |\n| p-value | (< 0.001) | (0.025) |\n\nTo diagnose the rejection of Model 4, diagnostic OLS regressions were run to test if the model's pricing errors (`u_{j,t+1}`) are predictable by the GMM instruments. Key results for the one-month T-bill are in Table 2.\n\n**Table 2: Diagnostic OLS Regression for T-Bill Returns**\n\n| Dependent Variable | `R²` |\n| :--- | :--- |\n| Before-Tax Real Return | 0.221 |\n| Model 4 Pricing Error (`u_{f,t+1}`) | 0.104 |\n\nFinally, a formal Newey-West test of the null hypothesis that the tax-free model (Model 1) is correct against the alternative that the asymmetric tax model (Model 4) is correct yields a `χ²` statistic of 34.611 (p < 0.001).\n\n---\n\n### Question\n\nBased on the full set of statistical evidence provided, select ALL statements that correctly synthesize the findings regarding the models' performance and comparison.", "Options": {"A": "The diagnostic regression in Table 2 shows that Model 4 explains only 10.4% of the variation in T-bill returns, which is why it is rejected by the J-test.", "B": "The fact that both Model 1 and Model 4 are rejected by the J-test implies that incorporating taxes offers no meaningful improvement to the consumption-based asset pricing model.", "C": "The rejection of Model 4 by the J-test is partly explained by its failure to fully account for the predictability in T-bill returns, as the model's pricing errors still have a significant R² of 10.4%.", "D": "The Newey-West test result strongly indicates that the asymmetric tax model (Model 4) provides a statistically significant improvement in fit over the tax-free model (Model 1)."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize results from multiple, distinct statistical tests (J-test, OLS diagnostic, Newey-West comparison). It uses an **Atomic Decomposition** strategy to isolate key conclusions from the original QA's apex question. The depth strategy is **Reverse-Reasoning**, requiring the user to infer the reasons for model failure and the logic behind model comparison. Distractors are high-fidelity: Option C is a **Conceptual Opposite** that represents a common misinterpretation of nested vs. non-nested tests. Option D is a **Numerical (Misinterpretation)** error, confusing the R² of the pricing error with the R² of the return itself.", "qid": "328", "question": "### Background\n\n**Research Question.** This case provides a comprehensive empirical evaluation of the consumption-based asset pricing model (CCAPM), testing whether incorporating taxes and tax-timing options can resolve its well-known empirical failures.\n\n**Setting.** The analysis compares the benchmark tax-free CCAPM (Model 1) against the paper's preferred specification, an asymmetric tax model (Model 4). Both models are estimated using the Generalized Method of Moments (GMM).\n\n**Variables and Parameters.**\n\n*   `J-statistic (χ²)`: GMM test for a model's overidentifying restrictions. A high value (low p-value) indicates model rejection.\n*   `u_{j,t+1}`: The model's pricing error for asset `j` (risk-adjusted after-tax return).\n\n---\n\n### Data / Model Specification\n\nThe GMM estimation results for the tax-free and asymmetric tax models are summarized in Table 1.\n\n**Table 1: GMM Estimation Results for Competing CCAPM Specifications**\n\n| Parameter / Statistic | Model 1 (Tax-Free) | Model 4 (Asymmetric Tax) |\n| :--- | :--- | :--- |\n| `J-statistic (χ²)` | 33.424 | 11.154 |\n| p-value | (< 0.001) | (0.025) |\n\nTo diagnose the rejection of Model 4, diagnostic OLS regressions were run to test if the model's pricing errors (`u_{j,t+1}`) are predictable by the GMM instruments. Key results for the one-month T-bill are in Table 2.\n\n**Table 2: Diagnostic OLS Regression for T-Bill Returns**\n\n| Dependent Variable | `R²` |\n| :--- | :--- |\n| Before-Tax Real Return | 0.221 |\n| Model 4 Pricing Error (`u_{f,t+1}`) | 0.104 |\n\nFinally, a formal Newey-West test of the null hypothesis that the tax-free model (Model 1) is correct against the alternative that the asymmetric tax model (Model 4) is correct yields a `χ²` statistic of 34.611 (p < 0.001).\n\n---\n\n### Question\n\nBased on the full set of statistical evidence provided, select ALL statements that correctly synthesize the findings regarding the models' performance and comparison.", "model_evaluation_result": {"model_raw_response": "C", "model_answer": ["C"], "error": null}}
{"ID": 337, "Question": "### Background\n\n**Research Question.** What is the relationship between factor risk premia estimated using multivariate betas versus univariate betas, and are the two specifications economically equivalent?\n\n**Setting.** An econometrician is considering two alternative specifications for a `K`-factor model. One uses multivariate betas (`B`) from a multiple regression of returns on all factors simultaneously. The other uses univariate betas (`B*`) from separate simple regressions of returns on each factor.\n\n**Variables and Parameters.**\n- `b`: `K`x1 vector of multivariate risk premia.\n- `b*`: `K`x1 vector of univariate risk premia.\n- `Ω`: `K`x`K` covariance matrix of factors, `Var(y)`.\n- `D`: `K`x`K` diagonal matrix where `Dₖₖ = Ωₖₖ = Var(yₖ)`.\n\n---\n\n### Data / Model Specification\n\nThe risk premia from the two models are related by the following transformation:\n  \nb = \\Omega D^{-1} b^* \\quad \\text{(Eq. (1))}\n \nConsider a two-factor model where the factor covariance matrix `Ω` and the univariate risk premia `b*` are given by:\n  \n\\Omega = \\begin{pmatrix} 1 & -0.5 \\\\ -0.5 & 1 \\end{pmatrix}, \\quad b^* = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n \n\n---\n\n### Question\n\nGiven the model specifications and the specific numerical example, select all of the following statements that are correct.", "Options": {"A": "In the multivariate model, the risk premium for the first factor (`b₁`) is 0.", "B": "This example demonstrates that a factor's multivariate risk premium (`bₖ`) can be zero even if its univariate premium (`b*ₖ`) is non-zero, because the multivariate specification accounts for the factor's correlation with other priced factors.", "C": "The finding that `b₁ = 0` must be a numerical error, because a factor with a positive univariate risk premium (`b*₁ = 1`) cannot have a zero multivariate risk premium.", "D": "In the multivariate model, the risk premium for the second factor (`b₂`) is 2.0."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: Tests the ability to calculate the transformation between univariate and multivariate risk premia and understand the economic non-equivalence. Strategy: Scenario Application & Computational Judgment. A specific numerical scenario is provided, requiring both calculation and conceptual interpretation. Distractor Logic: Option B is a Numerical Error (specifically, a Formula Misuse error where `b₂` is incorrectly assumed to equal `b*₂`). Option D is a classic Conceptual error, stating the core misconception that the two premia specifications are economically equivalent.", "qid": "337", "question": "### Background\n\n**Research Question.** What is the relationship between factor risk premia estimated using multivariate betas versus univariate betas, and are the two specifications economically equivalent?\n\n**Setting.** An econometrician is considering two alternative specifications for a `K`-factor model. One uses multivariate betas (`B`) from a multiple regression of returns on all factors simultaneously. The other uses univariate betas (`B*`) from separate simple regressions of returns on each factor.\n\n**Variables and Parameters.**\n- `b`: `K`x1 vector of multivariate risk premia.\n- `b*`: `K`x1 vector of univariate risk premia.\n- `Ω`: `K`x`K` covariance matrix of factors, `Var(y)`.\n- `D`: `K`x`K` diagonal matrix where `Dₖₖ = Ωₖₖ = Var(yₖ)`.\n\n---\n\n### Data / Model Specification\n\nThe risk premia from the two models are related by the following transformation:\n  \nb = \\Omega D^{-1} b^* \\quad \\text{(Eq. (1))}\n \nConsider a two-factor model where the factor covariance matrix `Ω` and the univariate risk premia `b*` are given by:\n  \n\\Omega = \\begin{pmatrix} 1 & -0.5 \\\\ -0.5 & 1 \\end{pmatrix}, \\quad b^* = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n \n\n---\n\n### Question\n\nGiven the model specifications and the specific numerical example, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 336, "Question": "### Background\n\n**Research Question.** What is the correct asymptotic distribution for estimators from a two-stage cross-sectional regression, and how does it differ under assumptions of conditional heteroskedasticity versus homoskedasticity?\n\n**Setting.** We analyze the asymptotic properties of the GLS estimator `ĉ` from a two-stage cross-sectional regression as the number of time-series observations `T` goes to infinity. The analysis contrasts the simple Fama-MacBeth (FM) variance estimator with more general forms.\n\n**Variables and Parameters.**\n- `ĉ`: The `(1+L+K)`x1 GLS estimate of the cross-sectional parameters.\n- `c`: The true parameter vector.\n- `uₜ`: `N`x1 vector of residuals from the time-series regressions `Rₜ = A + B yₜ + uₜ`.\n- `Δ`: An `N`x`N` constant covariance matrix of `uₜ` under homoskedasticity.\n\n---\n\n### Data / Model Specification\n\nThe full asymptotic variance of `√T(ĉ - c)` under general (conditionally heteroskedastic) conditions is given by:\n  \nS = V + W - G \\quad \\text{(Eq. (1))}\n \nwhere `V` is the variance from using sample average returns, `W` is the variance from using estimated betas, and `G` is twice the covariance between these two error sources. The standard Fama-MacBeth (FM) procedure produces an estimate of `V` but ignores `W` and `G`.\n\nUnder the special assumption of **conditional homoskedasticity** (i.e., `E[uₜuₜ'|Y] = Δ`), the `G` term becomes zero and the variance simplifies to:\n  \nS_{homo} = V + \\tilde{W} \\quad \\text{(Eq. (2))}\n \nwhere `W̃` is a specific positive semi-definite form of `W`.\n\n---\n\n### Question\n\nBased on the analysis of the asymptotic variance of the cross-sectional regression estimator, select all of the following statements that are correct.", "Options": {"A": "The `W` term in the general variance formula `S = V + W - G` captures the additional variance arising from the errors-in-variables problem of using estimated betas (`B̂`) instead of true betas (`B`).", "B": "When asset returns are conditionally homoskedastic, the Fama-MacBeth procedure systematically understates the true sampling variance because it correctly estimates `V` but omits the positive semi-definite term `W̃`.", "C": "Under the assumption of conditional homoskedasticity, the Fama-MacBeth variance estimator (`V`) is a consistent estimator of the true asymptotic variance (`S_homo`).", "D": "In the general case (conditional heteroskedasticity), the Fama-MacBeth procedure is guaranteed to overstate the precision of the estimates because the omitted term `W - G` is always positive semi-definite."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: Tests understanding of the components of the asymptotic variance formula and the impact of conditional homoskedasticity on the bias of the Fama-MacBeth procedure. Strategy: Atomic Decomposition. The question asks students to evaluate four distinct propositions about the variance components. Distractor Logic: Option B is a Step-Omission Error, as it correctly identifies that FM estimates V but incorrectly concludes this makes it consistent for S_homo (it omits W̃). Option C is a Conceptual Opposite, incorrectly stating the direction of bias is known in the general case, which is a key finding of the paper.", "qid": "336", "question": "### Background\n\n**Research Question.** What is the correct asymptotic distribution for estimators from a two-stage cross-sectional regression, and how does it differ under assumptions of conditional heteroskedasticity versus homoskedasticity?\n\n**Setting.** We analyze the asymptotic properties of the GLS estimator `ĉ` from a two-stage cross-sectional regression as the number of time-series observations `T` goes to infinity. The analysis contrasts the simple Fama-MacBeth (FM) variance estimator with more general forms.\n\n**Variables and Parameters.**\n- `ĉ`: The `(1+L+K)`x1 GLS estimate of the cross-sectional parameters.\n- `c`: The true parameter vector.\n- `uₜ`: `N`x1 vector of residuals from the time-series regressions `Rₜ = A + B yₜ + uₜ`.\n- `Δ`: An `N`x`N` constant covariance matrix of `uₜ` under homoskedasticity.\n\n---\n\n### Data / Model Specification\n\nThe full asymptotic variance of `√T(ĉ - c)` under general (conditionally heteroskedastic) conditions is given by:\n  \nS = V + W - G \\quad \\text{(Eq. (1))}\n \nwhere `V` is the variance from using sample average returns, `W` is the variance from using estimated betas, and `G` is twice the covariance between these two error sources. The standard Fama-MacBeth (FM) procedure produces an estimate of `V` but ignores `W` and `G`.\n\nUnder the special assumption of **conditional homoskedasticity** (i.e., `E[uₜuₜ'|Y] = Δ`), the `G` term becomes zero and the variance simplifies to:\n  \nS_{homo} = V + \\tilde{W} \\quad \\text{(Eq. (2))}\n \nwhere `W̃` is a specific positive semi-definite form of `W`.\n\n---\n\n### Question\n\nBased on the analysis of the asymptotic variance of the cross-sectional regression estimator, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 318, "Question": "### Background\n\nThe study examines two FTSE 350 UK utilities operating in a sector where a five-yearly regulatory review constrains profit potential in the core regulated business. This context forces firms to make distinct strategic choices about whether to focus on the regulated core or diversify into higher-risk, unregulated businesses.\n\n### Data / Model Specification\n\nAccording to contingency theory, remuneration policy should be tailored to corporate strategy. The following evidence was gathered:\n\n> **Fact 1:** The two utilities studied, despite being in the same industry, adopted very different remuneration policies that aligned with their distinct strategic aims.\n\n> **Evidence (HR Director 1):** \"He pointed out that the core business, being a utility, was relatively riskfree... and so the remuneration packages had to reflect that. However, he noted that the risk was much higher in some of the group’s unregulated activities, and this part of the business merited a different reward structure.\"\n\n### Question\n\nBased on contingency theory and the provided evidence, which of the following remuneration design choices would be considered rational and strategically aligned for a utility?\n\nSelect all that apply.", "Options": {"A": "For a utility focused on diversification into high-risk unregulated markets, a high-leverage pay structure with a large stock option component is appropriate to incentivize entrepreneurial risk-taking.", "B": "For a utility focused exclusively on its stable, price-capped regulated business, a high-leverage pay structure with a large stock option component is appropriate to maximize shareholder returns.", "C": "For a utility focused on diversification into high-risk unregulated markets, a low-leverage pay structure with a high proportion of fixed salary is appropriate to ensure executive retention.", "D": "For a utility focused exclusively on its stable, price-capped regulated business, a low-leverage pay structure with a high proportion of fixed salary and bonuses tied to operational efficiency is appropriate."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the ability to apply contingency theory to the specific scenarios described in the paper. It uses a 'Scenario Application' strategy. The correct options (A, C) represent the two distinct, logically consistent pairings of strategy and remuneration structure discussed in the source material (high-risk/high-leverage and low-risk/low-leverage). The distractors (B, D) present mismatched pairings that violate the logic of contingency theory and the HR Director's statement, representing plausible 'Formula Misuse' or conceptual errors.", "qid": "318", "question": "### Background\n\nThe study examines two FTSE 350 UK utilities operating in a sector where a five-yearly regulatory review constrains profit potential in the core regulated business. This context forces firms to make distinct strategic choices about whether to focus on the regulated core or diversify into higher-risk, unregulated businesses.\n\n### Data / Model Specification\n\nAccording to contingency theory, remuneration policy should be tailored to corporate strategy. The following evidence was gathered:\n\n> **Fact 1:** The two utilities studied, despite being in the same industry, adopted very different remuneration policies that aligned with their distinct strategic aims.\n\n> **Evidence (HR Director 1):** \"He pointed out that the core business, being a utility, was relatively riskfree... and so the remuneration packages had to reflect that. However, he noted that the risk was much higher in some of the group’s unregulated activities, and this part of the business merited a different reward structure.\"\n\n### Question\n\nBased on contingency theory and the provided evidence, which of the following remuneration design choices would be considered rational and strategically aligned for a utility?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 373, "Question": "### Background\n\n**Research Question.** To determine if the effect of national industrial policy on corporate financialization is conditional on local institutional factors, specifically regional market development and local government financial capacity.\n\n**Theoretical Framework.** The effectiveness of a top-down national policy may depend on the capacity of local institutions to implement it.\n\n### Data / Model Specification\n\nThe analysis uses a regression model with an interaction term:\n  \nFINRATIO_{i,t+1} = \\beta_{0} + \\beta_{1} IP1_{it} + \\beta_{2} (IP1_{it} \\times Moderator_{it}) + \\dots + e_{i,t+1} \\quad \\text{(Eq. 1)}\n \n-   `FINRATIO`: The ratio of financial assets to total assets.\n-   `IP1`: A dummy variable for industrial policy support.\n-   `MARKET`: A dummy variable = 1 for firms in high-marketization regions.\n-   `DEFICIT`: A measure of local government financial capacity (higher value = greater capacity).\n\n**Table 1. Moderating Effect of Marketization (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | 0.0131 | (1.63) |\n| `IP1 × MARKET` | -0.0035*** | (-3.24) |\n\n**Table 2. Moderating Effect of Local Gov't Financial Capacity (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | -0.0182*** | (-9.82) |\n| `IP1 × DEFICIT` | -0.0076*** | (-4.36) |\n\n*Note: *** denotes p < .01.*\n\n### Question\n\nBased on the regression results for moderating effects, select all of the following statements that are correct.", "Options": {"A": "In high-marketization regions (`MARKET=1`), industrial policy has a statistically significant negative effect on `FINRATIO`.", "B": "The effect of industrial policy on financialization is significantly more pronounced (i.e., more negative) in regions with greater local government financial capacity.", "C": "The results in Table 2 imply that the total effect of industrial policy is `Total Effect = -0.0182 + (-0.0076 × DEFICIT)`.", "D": "For firms in low-marketization regions (`MARKET=0`), the estimated effect of industrial policy on `FINRATIO` is positive but not statistically significant."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Scenario Application strategy to test the interpretation of interaction terms. Students must calculate and interpret conditional effects. Correct options A and B require correct interpretation of an interaction term and a main effect's significance, respectively. Distractors are designed to catch common errors: C involves a sign error in calculating the total effect for a subgroup, and D involves a sign error when constructing the total effect equation from the regression coefficients.", "qid": "373", "question": "### Background\n\n**Research Question.** To determine if the effect of national industrial policy on corporate financialization is conditional on local institutional factors, specifically regional market development and local government financial capacity.\n\n**Theoretical Framework.** The effectiveness of a top-down national policy may depend on the capacity of local institutions to implement it.\n\n### Data / Model Specification\n\nThe analysis uses a regression model with an interaction term:\n  \nFINRATIO_{i,t+1} = \\beta_{0} + \\beta_{1} IP1_{it} + \\beta_{2} (IP1_{it} \\times Moderator_{it}) + \\dots + e_{i,t+1} \\quad \\text{(Eq. 1)}\n \n-   `FINRATIO`: The ratio of financial assets to total assets.\n-   `IP1`: A dummy variable for industrial policy support.\n-   `MARKET`: A dummy variable = 1 for firms in high-marketization regions.\n-   `DEFICIT`: A measure of local government financial capacity (higher value = greater capacity).\n\n**Table 1. Moderating Effect of Marketization (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | 0.0131 | (1.63) |\n| `IP1 × MARKET` | -0.0035*** | (-3.24) |\n\n**Table 2. Moderating Effect of Local Gov't Financial Capacity (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | -0.0182*** | (-9.82) |\n| `IP1 × DEFICIT` | -0.0076*** | (-4.36) |\n\n*Note: *** denotes p < .01.*\n\n### Question\n\nBased on the regression results for moderating effects, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 325, "Question": "## Background\n\n**Research Question.** This case examines the valuation of a large-scale privatization, Deutsche Telekom (DT), using the method of comparables, with a focus on the choice of valuation multiple and the limitations of the approach in a dynamic industry.\n\n**Setting.** In 1996, investment bankers are tasked with pricing the landmark $10 billion privatization of Deutsche Telekom. The company is a state-owned monopoly facing imminent deregulation and major capital expenditures. The valuation relies on comparing DT to other global telecommunications firms.\n\n---\n\n## Data / Model Specification\n\nThe valuation was based on applying a range of peer-group multiples to DT's financial metrics. DT's estimated cash flow per share (CFPS) was $2.60. The paper states that applying a PCF multiple range of 5x to 8x to DT's CFPS yielded a price range of $13 to $21. It also notes that Daimler Benz, another German firm, had 1995 earnings per share of -$11.05 but cash flow per share of $18.64.\n\n**Table 1: Comparables Valuation Data for Deutsche Telekom**\n\n| Company | Year | PCF Multiple |\n| :--- | :--- | :--- |\n| AT&T (U.S.) | 1995 | 6.4x |\n| BCE, Inc (Canada) | 1995 | 5.3x |\n| British Telecom (U.K.) | 1995 | 5.4x |\n| Sprint (U.S.) | 1995 | 5.1x |\n| PTT Nederland (Neth.) | 1996e | 5.1x |\n| Telmex (Mexico) | 1995 | 6.5x |\n\n---\n\n## Question\n\nBased on the provided background and data for the Deutsche Telekom (DT) privatization, select all statements that are correct regarding the valuation methodology and its rationale.", "Options": {"A": "The case of Daimler Benz, which had negative earnings but positive cash flow, illustrates that P/E multiples are generally more reliable than PCF multiples for firms undergoing restructuring.", "B": "The upper end of the valuation range (using an 8.0x PCF multiple) was directly supported by the median PCF multiple of the comparable firms listed in Table 1.", "C": "The valuation range of $13 to $21 per share for DT was calculated by applying a PCF multiple range of 5.0x to 8.0x to the company's estimated cash flow per share of $2.60.", "D": "Analysts likely preferred Price-to-Cash-Flow (PCF) multiples over Price-to-Earnings (P/E) multiples because DT's earnings were expected to be distorted by large non-cash charges and restructuring costs typical of a privatizing state-owned enterprise."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of valuation using comparables, specifically the choice of an appropriate multiple (PCF vs. P/E) in a special situation (privatization/restructuring). It uses a combination of Atomic Decomposition and Computational Judgment. Option A tests the conceptual rationale. Option B tests the ability to verify the calculation. Distractor C targets misinterpretation of the provided table data, while D presents a conceptual opposite regarding the lesson from the Daimler Benz example.", "qid": "325", "question": "## Background\n\n**Research Question.** This case examines the valuation of a large-scale privatization, Deutsche Telekom (DT), using the method of comparables, with a focus on the choice of valuation multiple and the limitations of the approach in a dynamic industry.\n\n**Setting.** In 1996, investment bankers are tasked with pricing the landmark $10 billion privatization of Deutsche Telekom. The company is a state-owned monopoly facing imminent deregulation and major capital expenditures. The valuation relies on comparing DT to other global telecommunications firms.\n\n---\n\n## Data / Model Specification\n\nThe valuation was based on applying a range of peer-group multiples to DT's financial metrics. DT's estimated cash flow per share (CFPS) was $2.60. The paper states that applying a PCF multiple range of 5x to 8x to DT's CFPS yielded a price range of $13 to $21. It also notes that Daimler Benz, another German firm, had 1995 earnings per share of -$11.05 but cash flow per share of $18.64.\n\n**Table 1: Comparables Valuation Data for Deutsche Telekom**\n\n| Company | Year | PCF Multiple |\n| :--- | :--- | :--- |\n| AT&T (U.S.) | 1995 | 6.4x |\n| BCE, Inc (Canada) | 1995 | 5.3x |\n| British Telecom (U.K.) | 1995 | 5.4x |\n| Sprint (U.S.) | 1995 | 5.1x |\n| PTT Nederland (Neth.) | 1996e | 5.1x |\n| Telmex (Mexico) | 1995 | 6.5x |\n\n---\n\n## Question\n\nBased on the provided background and data for the Deutsche Telekom (DT) privatization, select all statements that are correct regarding the valuation methodology and its rationale.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 364, "Question": "### Background\n\n**Research Question.** Is there a post-earnings announcement drift (PEAD) following management earnings forecasts (MEFs), and does its magnitude differ between growth and value firms? How does the pattern of drift relate to the immediate market reaction to the news?\n\n**Setting and Sample.** The study analyzes abnormal returns for Australian firms following an MEF. It distinguishes between the immediate 3-day reaction and the subsequent 60-day drift.\n\n**Variables and Parameters.**\n- `BH_3`: The 3-day market-adjusted abnormal return centered on the MEF announcement.\n- `BH_60`: The 60-day size-adjusted buy-and-hold abnormal return, calculated for the window beginning 2 days after the MEF announcement and ending 61 days after (`[+2, +61]`).\n- `Growth`: A dummy variable equal to unity if a firm falls within the lowest two quintiles of the book-to-market ratio.\n- `Value`: A dummy variable equal to unity if a firm falls within the highest two quintiles of the book-to-market ratio.\n- `D_BadA`: A dummy variable equal to unity for MEFs classified as bad news relative to the mean analyst earnings benchmark.\n\n---\n\n### Data / Model Specification\n\nThe study estimates an interaction model to analyze both the immediate return and the post-announcement drift. The key results for bad news announcements are summarized from the 'Overall' sample analysis in the paper's Tables 3 and 4.\n\n  \n\\text{Abnormal Return} = \\alpha + \\beta_{1}(\\text{Growth} \\cdot D_{\\mathrm{BadA}}) + \\beta_{3}(\\text{Value} \\cdot D_{\\mathrm{BadA}}) + \\dots + \\text{Controls} + \\varepsilon\n \n\n**Table 1: Summary of Market Reactions to Bad News MEFs**\n\n| Variable | Immediate Return (`BH_3`) | Post-Announcement Drift (`BH_60`) |\n| :--- | :--- | :--- |\n| `Growth * D_BadA` (β1) | -0.053*** | -0.0422*** |\n| `Value * D_BadA` (β3) | -0.027*** | -0.0504*** |\n| **Wald Test: |β1|=|β3|** | p = 0.029 | p = 0.696 |\n\n*Source: 'Overall' columns of Tables 3 and 4 in the source paper. *** indicates p<0.01.* \n\n---\n\n### Question\n\nBased on the model specification and empirical results in **Table 1**, which of the following statements accurately describe the market reaction to bad news MEFs?", "Options": {"A": "The post-announcement drift (`BH_60`) is significantly more negative for value firms than for growth firms at the 5% level.", "B": "The total expected abnormal return over the 63-day period (combining immediate reaction and post-announcement drift) for a growth firm is approximately -9.52%.", "C": "The immediate return (`BH_3`) accounts for the majority of the total 63-day negative return for value firms.", "D": "The immediate negative return (`BH_3`) is significantly more severe for growth firms than for value firms at the 5% level."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret statistical significance from p-values and perform multi-step calculations using regression coefficients from a table. It uses an Atomic Decomposition strategy to create a genuine multi-select question. Distractor C tests the common confusion between magnitude and statistical significance. Distractor D targets a core conclusion of the paper regarding the *timing* of the reaction for value firms, making it a high-fidelity misconception.", "qid": "364", "question": "### Background\n\n**Research Question.** Is there a post-earnings announcement drift (PEAD) following management earnings forecasts (MEFs), and does its magnitude differ between growth and value firms? How does the pattern of drift relate to the immediate market reaction to the news?\n\n**Setting and Sample.** The study analyzes abnormal returns for Australian firms following an MEF. It distinguishes between the immediate 3-day reaction and the subsequent 60-day drift.\n\n**Variables and Parameters.**\n- `BH_3`: The 3-day market-adjusted abnormal return centered on the MEF announcement.\n- `BH_60`: The 60-day size-adjusted buy-and-hold abnormal return, calculated for the window beginning 2 days after the MEF announcement and ending 61 days after (`[+2, +61]`).\n- `Growth`: A dummy variable equal to unity if a firm falls within the lowest two quintiles of the book-to-market ratio.\n- `Value`: A dummy variable equal to unity if a firm falls within the highest two quintiles of the book-to-market ratio.\n- `D_BadA`: A dummy variable equal to unity for MEFs classified as bad news relative to the mean analyst earnings benchmark.\n\n---\n\n### Data / Model Specification\n\nThe study estimates an interaction model to analyze both the immediate return and the post-announcement drift. The key results for bad news announcements are summarized from the 'Overall' sample analysis in the paper's Tables 3 and 4.\n\n  \n\\text{Abnormal Return} = \\alpha + \\beta_{1}(\\text{Growth} \\cdot D_{\\mathrm{BadA}}) + \\beta_{3}(\\text{Value} \\cdot D_{\\mathrm{BadA}}) + \\dots + \\text{Controls} + \\varepsilon\n \n\n**Table 1: Summary of Market Reactions to Bad News MEFs**\n\n| Variable | Immediate Return (`BH_3`) | Post-Announcement Drift (`BH_60`) |\n| :--- | :--- | :--- |\n| `Growth * D_BadA` (β1) | -0.053*** | -0.0422*** |\n| `Value * D_BadA` (β3) | -0.027*** | -0.0504*** |\n| **Wald Test: |β1|=|β3|** | p = 0.029 | p = 0.696 |\n\n*Source: 'Overall' columns of Tables 3 and 4 in the source paper. *** indicates p<0.01.* \n\n---\n\n### Question\n\nBased on the model specification and empirical results in **Table 1**, which of the following statements accurately describe the market reaction to bad news MEFs?", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 424, "Question": "### Background\n\n**Research Question.** This case examines the paper's core empirical framework for establishing an intertemporal stock-to-bond volatility (ISBV) relation. It tests whether lagged equity volatility has incremental predictive power for future bond futures volatility, after controlling for known predictors from within the bond market.\n\n### Data / Model Specification\n\nThe primary predictive regression model is:\n\n  \n\\sigma_{t,t+21}^{TmSt} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TmSt} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\nwhere `\\sigma^{TmSt}` is the log realized volatility of a Treasury futures contract, `\\sigma^{ST}` is the lagged log realized stock volatility, and `\\sigma_{t-1,t-22}^{TmSt}` and `PrComp_{j,t-1}` are control variables.\n\n**Table 1. Predictive Regression Results for 30-yr T-bond Futures Volatility (Full Period 1997:10-2013:06)**\n\n| Panel | Stock Volatility Measure | `\\hat{\\gamma}_{2}` (t-stat) | `R^2` |\n| :--- | :--- | :--- | :--- |\n| A | Daily S&P 500 Returns | 0.189 (5.92) | 58.8% |\n| B | 5-minute SPY Returns | 0.230 (7.01) | 60.3% |\n\n### Question\n\nBased on the regression specification and the results in Table 1, which of the following statements are valid econometric interpretations? Select all that apply.", "Options": {"A": "The larger coefficient `\\hat{\\gamma}_{2}` and higher `R^2` in Panel B versus Panel A is consistent with the idea that using a higher-frequency volatility measure reduces attenuation bias from measurement error.", "B": "The high `R^2` value of 58.8% in Panel A indicates that multicollinearity between the regressors is not a concern for the validity of the model.", "C": "The use of a higher-frequency (5-minute) stock volatility measure in Panel B introduces additional noise, which biases the `\\hat{\\gamma}_{2}` coefficient downward compared to Panel A.", "D": "The significance of `\\hat{\\gamma}_{2}` in the full model (Eq. 1) supports the claim that stock volatility offers 'incremental' information beyond that contained in the bond market's own past volatility and the state of the yield curve."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the core regression design and interpretation. It uses an Atomic Decomposition strategy, testing the concept of 'incremental information' (from Q1 of the source) and 'measurement error' (from Q2 of the source). Correct option A defines the key takeaway from the full specification. Correct option B correctly applies the concept of attenuation bias. Distractor C makes a false claim about the relationship between R-squared and multicollinearity. Distractor D presents the conceptual opposite of the paper's argument about measurement error.", "qid": "424", "question": "### Background\n\n**Research Question.** This case examines the paper's core empirical framework for establishing an intertemporal stock-to-bond volatility (ISBV) relation. It tests whether lagged equity volatility has incremental predictive power for future bond futures volatility, after controlling for known predictors from within the bond market.\n\n### Data / Model Specification\n\nThe primary predictive regression model is:\n\n  \n\\sigma_{t,t+21}^{TmSt} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TmSt} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\nwhere `\\sigma^{TmSt}` is the log realized volatility of a Treasury futures contract, `\\sigma^{ST}` is the lagged log realized stock volatility, and `\\sigma_{t-1,t-22}^{TmSt}` and `PrComp_{j,t-1}` are control variables.\n\n**Table 1. Predictive Regression Results for 30-yr T-bond Futures Volatility (Full Period 1997:10-2013:06)**\n\n| Panel | Stock Volatility Measure | `\\hat{\\gamma}_{2}` (t-stat) | `R^2` |\n| :--- | :--- | :--- | :--- |\n| A | Daily S&P 500 Returns | 0.189 (5.92) | 58.8% |\n| B | 5-minute SPY Returns | 0.230 (7.01) | 60.3% |\n\n### Question\n\nBased on the regression specification and the results in Table 1, which of the following statements are valid econometric interpretations? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 316, "Question": "### Background\n\n**Research Question.** How can a CCP's risk management framework be designed to account for the fact that some members persistently contribute more to systemic \"crowded risk\" than others?\n\n**Setting.** The paper proposes allocating total CCP risk (`ExpCCP`) back to individual members (`ExpCCP_j`). The difference between a member's allocated risk and the margin they posted is their \"funding gap.\" A key policy challenge is whether to use this allocation for high-frequency (daily) margin calls, which could create trading uncertainty, or for lower-frequency capital charges.\n\n**Variables and Parameters.**\n- `ExpCCP_j`: The portion of total CCP exposure allocated to member `j`.\n- **Member-specific funding gap:** The difference between a member's allocated risk (`ExpCCP_j`) and the actual margin they posted.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Correlation of Member Funding Gaps Across High-Stress Days**\n\nThis table shows the pairwise correlation of member-specific funding gaps across ten different high-stress days. A positive correlation indicates that members who under-contribute to collateral on one stress day tend to do so on other stress days as well. Asterisks (*) denote statistical significance at the 5% level.\n\n| | May 10 | Apr 23 | Apr 26 | May 24 | May 11 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| May 10, 2010 | 1.00 | 0.36* | 0.35* | 0.63* | 0.73* |\n| Apr 23, 2010 | | 1.00 | 0.71* | 0.41* | 0.35* |\n| Apr 26, 2010 | | | 1.00 | 0.24 | 0.62* |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the policy context described, select all statements that represent valid interpretations or policy recommendations supported by the paper.", "Options": {"A": "The significant positive correlations in the table support the hypothesis of \"type persistence,\" where the same members tend to be the primary contributors to crowded risk across different stress events.", "B": "Given the evidence of persistence, using a member's average `ExpCCP_j` to set their quarterly default fund contribution is a policy supported by the paper to make polluters pay without creating daily trading uncertainty.", "C": "The fact that no correlation is 1.00 (except with the same day) indicates that polluter behavior is essentially random, making long-term surcharges based on past behavior ineffective.", "D": "The data justifies implementing immediate, ad-hoc daily margin calls based on `ExpCCP_j` for all members during stress events to ensure polluters pay."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect an empirical finding ('polluter persistence') to a specific policy recommendation. It uses the 'Premise/Assumption Packaging' strategy. Options A and B are correct interpretations and policy conclusions drawn directly from the paper's text and the original QA answer. Distractor C proposes a policy ('ad-hoc daily margin calls') that the paper explicitly argues against due to the uncertainty it creates for traders. Distractor D represents a 'Conceptual Opposite' misinterpretation of statistical evidence, confusing 'not perfectly correlated' with 'random'.", "qid": "316", "question": "### Background\n\n**Research Question.** How can a CCP's risk management framework be designed to account for the fact that some members persistently contribute more to systemic \"crowded risk\" than others?\n\n**Setting.** The paper proposes allocating total CCP risk (`ExpCCP`) back to individual members (`ExpCCP_j`). The difference between a member's allocated risk and the margin they posted is their \"funding gap.\" A key policy challenge is whether to use this allocation for high-frequency (daily) margin calls, which could create trading uncertainty, or for lower-frequency capital charges.\n\n**Variables and Parameters.**\n- `ExpCCP_j`: The portion of total CCP exposure allocated to member `j`.\n- **Member-specific funding gap:** The difference between a member's allocated risk (`ExpCCP_j`) and the actual margin they posted.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Correlation of Member Funding Gaps Across High-Stress Days**\n\nThis table shows the pairwise correlation of member-specific funding gaps across ten different high-stress days. A positive correlation indicates that members who under-contribute to collateral on one stress day tend to do so on other stress days as well. Asterisks (*) denote statistical significance at the 5% level.\n\n| | May 10 | Apr 23 | Apr 26 | May 24 | May 11 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| May 10, 2010 | 1.00 | 0.36* | 0.35* | 0.63* | 0.73* |\n| Apr 23, 2010 | | 1.00 | 0.71* | 0.41* | 0.35* |\n| Apr 26, 2010 | | | 1.00 | 0.24 | 0.62* |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the policy context described, select all statements that represent valid interpretations or policy recommendations supported by the paper.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 241, "Question": "### Background\n\n**Research Question.** How do the market prices of local vs. global macroeconomic risks vary over the business cycle, and what are the implications for investors in small, open economies like Finland and Sweden?\n\n**Setting.** This study uses a GMM framework to test if the sensitivity of asset returns to industrial production growth and volatility is state-dependent. The analysis is conducted using both local (country-specific) and global (OECD-wide) production data as risk factors. The state of the business cycle is proxied by the lagged 12-month growth in the relevant industrial production series (`IPL`).\n\n### Data / Model Specification\n\nThe core model for conditional expected stock returns (`r_m`) is:\n\n  \nE[r_{mt} | F_{t-1}] \\propto (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) E[YP_t | F_{t-1}] + (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) E[e_{1t}^2 | F_{t-1}] \\quad \\text{(Eq. 1)}\n \n\nwhere `YP_t` is production growth and `e_{1t}^2` is its volatility. The key hypothesis is that the interaction terms `\\lambda_{1s}` and `\\kappa_{1s}` are non-zero, indicating that the prices of growth and volatility risk are state-dependent. The table below summarizes the key hypothesis test results from the paper for stock returns.\n\n**Table 1: Summary of Hypothesis Tests for State-Dependency of Stock Returns (p-values)**\n\n| Factor Origin | Country | H₀: Price of Growth Risk is Constant (`λ₁s = 0`) | H₀: Price of Volatility Risk is Constant (`κ₁s = 0`) |\n| :--- | :--- | :--- | :--- |\n| **Local** | Finland | 0.009 | 0.000 |\n| **Local** | Sweden | 0.287 | 0.000 |\n| **Global (OECD)** | Finland & Sweden | 0.000 | 0.412 |\n\n*Source: Synthesized from Tables 2, 3, and 4 of the paper. For the Global factor, the test for `κ₁s=0` is for stocks only, while the test for `λ₁s=0` is a joint test across stocks and bonds.*\n\n### Question\n\nBased on the provided model specification and hypothesis test results in Table 1, which of the following conclusions about the state-dependency of risk pricing for *stocks* are statistically supported at the 5% significance level?", "Options": {"A": "The price of local *growth* risk is state-dependent in Finland, but not in Sweden.", "B": "The price of local *growth* risk is state-dependent in both Finland and Sweden.", "C": "The price of local *volatility* risk is state-dependent in both Finland and Sweden.", "D": "The price of global *growth* risk is state-dependent, but the price of global *volatility* risk is not."}, "Answer": ["A", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to accurately interpret statistical evidence from a summary table. It uses the 'atomic decomposition' strategy to break down the complex findings of the original QA into distinct, verifiable propositions. The distractor (D) is designed to catch a common error of overgeneralization, where a finding for one country (Finland) is incorrectly applied to another (Sweden).", "qid": "241", "question": "### Background\n\n**Research Question.** How do the market prices of local vs. global macroeconomic risks vary over the business cycle, and what are the implications for investors in small, open economies like Finland and Sweden?\n\n**Setting.** This study uses a GMM framework to test if the sensitivity of asset returns to industrial production growth and volatility is state-dependent. The analysis is conducted using both local (country-specific) and global (OECD-wide) production data as risk factors. The state of the business cycle is proxied by the lagged 12-month growth in the relevant industrial production series (`IPL`).\n\n### Data / Model Specification\n\nThe core model for conditional expected stock returns (`r_m`) is:\n\n  \nE[r_{mt} | F_{t-1}] \\propto (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) E[YP_t | F_{t-1}] + (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) E[e_{1t}^2 | F_{t-1}] \\quad \\text{(Eq. 1)}\n \n\nwhere `YP_t` is production growth and `e_{1t}^2` is its volatility. The key hypothesis is that the interaction terms `\\lambda_{1s}` and `\\kappa_{1s}` are non-zero, indicating that the prices of growth and volatility risk are state-dependent. The table below summarizes the key hypothesis test results from the paper for stock returns.\n\n**Table 1: Summary of Hypothesis Tests for State-Dependency of Stock Returns (p-values)**\n\n| Factor Origin | Country | H₀: Price of Growth Risk is Constant (`λ₁s = 0`) | H₀: Price of Volatility Risk is Constant (`κ₁s = 0`) |\n| :--- | :--- | :--- | :--- |\n| **Local** | Finland | 0.009 | 0.000 |\n| **Local** | Sweden | 0.287 | 0.000 |\n| **Global (OECD)** | Finland & Sweden | 0.000 | 0.412 |\n\n*Source: Synthesized from Tables 2, 3, and 4 of the paper. For the Global factor, the test for `κ₁s=0` is for stocks only, while the test for `λ₁s=0` is a joint test across stocks and bonds.*\n\n### Question\n\nBased on the provided model specification and hypothesis test results in Table 1, which of the following conclusions about the state-dependency of risk pricing for *stocks* are statistically supported at the 5% significance level?", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 247, "Question": "### Background\n\n**Research Question.** What are the key economic properties of the modified expectile reinsurance premium, and what do they imply about its suitability for practical applications in insurance risk management?\n\n**Setting.** The modified expectile premium `c = ~e_{X,I(X),α}` is the unique solution to:\n  \n(1-\\alpha)c=\\alpha\\mathbb{E}[(I(X)-c)_{+}]-(1-\\alpha)\\mathbb{E}[X-I(X)] \n \n\n### Data / Model Specification\n\nThis premium has several important properties, including:\n\n**Subadditivity:** For two risks `X` and `Y`, if `I(X+Y) ≤ I_1(X) + I_2(Y)`, then `~e_{X+Y,I(X+Y),α} ≤ ~e_{X,I_1(X),α} + ~e_{Y,I_2(Y),α}`.\n\n**Risk Loading:** The premium includes a non-negative risk loading (`~e_{X,I(X),α} ≥ E[I(X)]`) if the reinsurer's weight `α` exceeds a certain threshold:\n  \n\\alpha \\ge \\alpha_{X,I(X)}^{*} = 1-\\frac{\\mathbb{E}[(I(X)-\\mathbb{E}[I(X)])_{+}]}{\\mathbb{E}[(I(X)-E[I(X)])_{+}]+\\mathbb{E}[X]} \n \n\n### Question\n\nBased on the specified properties of the modified expectile premium, select all of the following statements that are **INCORRECT**.", "Options": {"A": "If the reinsurer's weight `α` is less than the threshold `α*`, the resulting premium `~e_{X,I(X),α}` will be less than the expected ceded loss `E[I(X)]`.", "B": "The risk loading threshold `α*` will be higher for a ceded loss `I_B(X)` with high tail risk compared to a ceded loss `I_A(X)` with lower tail risk, assuming both have the same expected value `E[I(X)]`.", "C": "The risk loading property ensures the premium is always greater than the expected ceded loss, regardless of the value of `α`.", "D": "The subadditivity property implies that an insurer can potentially lower its total reinsurance cost by bundling different risk portfolios (e.g., life and property) into a single contract rather than reinsuring them separately."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: Evaluate understanding of the economic properties (subadditivity, risk loading) of the modified expectile premium. Strategy: Scenario Application and Computational Judgment. The Single-Correct-Answer Inversion Protocol was used. The original assessment objective was to identify correct properties. However, only one option (A) was unambiguously correct based on direct interpretation. To create a valid Multiple Response item, the stem was inverted to 'select all that are INCORRECT'. Distractor Logic: Option A is a correct interpretation of subadditivity and is now a distractor. Option C is a correct deduction from the risk loading property and is also a distractor. The new correct answers are B and D. Option B is incorrect because, as shown in the original QA, a contract with higher tail risk has a *lower* `α*` threshold. Option D is incorrect because the risk loading is conditional on `α ≥ α*`; it is not guaranteed for all `α`.", "qid": "247", "question": "### Background\n\n**Research Question.** What are the key economic properties of the modified expectile reinsurance premium, and what do they imply about its suitability for practical applications in insurance risk management?\n\n**Setting.** The modified expectile premium `c = ~e_{X,I(X),α}` is the unique solution to:\n  \n(1-\\alpha)c=\\alpha\\mathbb{E}[(I(X)-c)_{+}]-(1-\\alpha)\\mathbb{E}[X-I(X)] \n \n\n### Data / Model Specification\n\nThis premium has several important properties, including:\n\n**Subadditivity:** For two risks `X` and `Y`, if `I(X+Y) ≤ I_1(X) + I_2(Y)`, then `~e_{X+Y,I(X+Y),α} ≤ ~e_{X,I_1(X),α} + ~e_{Y,I_2(Y),α}`.\n\n**Risk Loading:** The premium includes a non-negative risk loading (`~e_{X,I(X),α} ≥ E[I(X)]`) if the reinsurer's weight `α` exceeds a certain threshold:\n  \n\\alpha \\ge \\alpha_{X,I(X)}^{*} = 1-\\frac{\\mathbb{E}[(I(X)-\\mathbb{E}[I(X)])_{+}]}{\\mathbb{E}[(I(X)-E[I(X)])_{+}]+\\mathbb{E}[X]} \n \n\n### Question\n\nBased on the specified properties of the modified expectile premium, select all of the following statements that are **INCORRECT**.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 192, "Question": "### Background\n\n**Research Question.** This case examines the relationship between the average age of a firm's supervisory board and its financial performance, testing the hypothesis that older boards are associated with lower firm value.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish listed firms, with data averaged over the 1998–2001 period. The study is motivated by recommendations in Denmark's “Nørby report” on corporate governance concerning board member age.\n\n**Variables & Parameters.**\n\n*   `TOBINQ_i`: Tobin’s Q for firm `i`.\n*   `AGE_i`: The average age of the supervisory board for firm `i`.\n\n---\n\n### Data / Model Specification\n\nThe baseline cross-sectional OLS model is:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot AGE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       | Std. Deviation |\n| :---------- | :--------- | :------------- |\n| Tobin's Q   | 1.3786     | 1.0444         |\n| AGE         | 56.8190    | 4.1902         |\n\n**Table 2: Regression of Tobin's Q on Board Age and Controls**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Constant               | 2.575       | (0.005)** |\n| Age                    | -0.029      | (0.053)*  |\n\n*Note: **Significant at 5%, *Significant at 10%.*\n\n---\n\n### Question\n\nBased on the provided data and regression results, select all statements that correctly interpret the findings.", "Options": {"A": "The coefficient on `Age` is statistically significant at the 10% level.", "B": "The economic magnitude of the effect is negligible, representing less than a 1% change in Tobin's Q relative to its mean for a one-standard-deviation change in age.", "C": "The coefficient on `Age` is statistically significant at the 5% level.", "D": "A one-standard-deviation increase in average board age is associated with a predicted decrease in Tobin's Q of approximately 0.12."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to interpret regression output, distinguishing between statistical and economic significance. It uses an **Atomic Decomposition** strategy, breaking down the interpretation into distinct claims about significance levels and magnitude. The **Computational Judgment** strategy is used for the economic significance calculation. Distractors are designed to trap common errors: (B) misinterpreting the p-value threshold (a classic misconception), and (D) misjudging the economic magnitude (a conceptual opposite).", "qid": "192", "question": "### Background\n\n**Research Question.** This case examines the relationship between the average age of a firm's supervisory board and its financial performance, testing the hypothesis that older boards are associated with lower firm value.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish listed firms, with data averaged over the 1998–2001 period. The study is motivated by recommendations in Denmark's “Nørby report” on corporate governance concerning board member age.\n\n**Variables & Parameters.**\n\n*   `TOBINQ_i`: Tobin’s Q for firm `i`.\n*   `AGE_i`: The average age of the supervisory board for firm `i`.\n\n---\n\n### Data / Model Specification\n\nThe baseline cross-sectional OLS model is:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot AGE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       | Std. Deviation |\n| :---------- | :--------- | :------------- |\n| Tobin's Q   | 1.3786     | 1.0444         |\n| AGE         | 56.8190    | 4.1902         |\n\n**Table 2: Regression of Tobin's Q on Board Age and Controls**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Constant               | 2.575       | (0.005)** |\n| Age                    | -0.029      | (0.053)*  |\n\n*Note: **Significant at 5%, *Significant at 10%.*\n\n---\n\n### Question\n\nBased on the provided data and regression results, select all statements that correctly interpret the findings.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 173, "Question": "### Background\n\n**Research Question.** How can the fair value of an insurer's policy reserve be calculated in a closed form when its accumulation depends on a series of embedded options related to market performance?\n\n**Setting and Environment.** We operate in a frictionless Black-Scholes market with a constant risk-free rate `r` and an equity fund `A` following a geometric Brownian motion under the risk-neutral measure `\\hat{\\mathbb{P}}`. The objective is to value the policy reserve, `V_P(t)`, for a participating contract.\n\n**Variables and Parameters.**\n- `V_P(t)`: Market value of the policy reserve at time `t`.\n- `P(t)`: Policyholder's account value at time `t`.\n- `r_P(k)`: Annual crediting rate for the policyholder's account.\n- `r_A(k)`: Annual return on the equity fund `A`.\n- `r_G`: Guaranteed annual crediting rate.\n- `\\beta`: Participation rate in the fund's returns.\n- `r`: Continuously compounded risk-free rate.\n- `\\sigma`: Volatility of the equity fund `A`.\n- `T`: Maturity of the contract in years.\n- `\\hat{\\mathbb{E}}`: Expectation operator under the risk-neutral measure.\n\n---\n\n### Data / Model Specification\n\nThe policyholder's account accumulates annually with a crediting rate given by:\n\n  \nr_{P}(k) = \\max\\{r_{G}, \\beta r_{A}(k)\\} \\quad \\text{where} \\quad r_{A}(k) = \\frac{A(k)-A(k-1)}{A(k-1)} \\quad \\text{(Eq. (1))}\n \n\nThis structure allows for the derivation of a closed-form solution for the policy reserve:\n\n  \nV_P(t) = P(t) \\left[ \\mathrm{e}^{-r}(1+r_{G})+\\beta N(d_{1})-\\mathrm{e}^{-r}(\\beta+r_{G})N(d_{2}) \\right]^{T-t} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### The Question\n\nThe ability to simplify the valuation of `V_P(t)` into the closed-form expression shown in Eq. (2) depends critically on which of the following assumptions? Select all that apply.", "Options": {"A": "The annual crediting rate, `r_P(k)`, depends only on the return in the current year `k`.", "B": "The annual returns on the equity fund, `r_A(k)`, are independent and identically distributed (i.i.d.).", "C": "The risk-free interest rate `r` is stochastic.", "D": "The contract includes a path-dependent feature, such as a high-water mark guarantee."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the understanding of the critical assumptions that enable the closed-form solution. It uses a Reverse-Reasoning strategy. The correct options (A, B) state the necessary conditions: i.i.d. returns (a consequence of the GBM model) and a memoryless contract design. The distractors represent conditions that would explicitly violate the model's requirements: path-dependency (C) and stochastic interest rates (D), both of which would prevent the simple product-of-expectations simplification.", "qid": "173", "question": "### Background\n\n**Research Question.** How can the fair value of an insurer's policy reserve be calculated in a closed form when its accumulation depends on a series of embedded options related to market performance?\n\n**Setting and Environment.** We operate in a frictionless Black-Scholes market with a constant risk-free rate `r` and an equity fund `A` following a geometric Brownian motion under the risk-neutral measure `\\hat{\\mathbb{P}}`. The objective is to value the policy reserve, `V_P(t)`, for a participating contract.\n\n**Variables and Parameters.**\n- `V_P(t)`: Market value of the policy reserve at time `t`.\n- `P(t)`: Policyholder's account value at time `t`.\n- `r_P(k)`: Annual crediting rate for the policyholder's account.\n- `r_A(k)`: Annual return on the equity fund `A`.\n- `r_G`: Guaranteed annual crediting rate.\n- `\\beta`: Participation rate in the fund's returns.\n- `r`: Continuously compounded risk-free rate.\n- `\\sigma`: Volatility of the equity fund `A`.\n- `T`: Maturity of the contract in years.\n- `\\hat{\\mathbb{E}}`: Expectation operator under the risk-neutral measure.\n\n---\n\n### Data / Model Specification\n\nThe policyholder's account accumulates annually with a crediting rate given by:\n\n  \nr_{P}(k) = \\max\\{r_{G}, \\beta r_{A}(k)\\} \\quad \\text{where} \\quad r_{A}(k) = \\frac{A(k)-A(k-1)}{A(k-1)} \\quad \\text{(Eq. (1))}\n \n\nThis structure allows for the derivation of a closed-form solution for the policy reserve:\n\n  \nV_P(t) = P(t) \\left[ \\mathrm{e}^{-r}(1+r_{G})+\\beta N(d_{1})-\\mathrm{e}^{-r}(\\beta+r_{G})N(d_{2}) \\right]^{T-t} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### The Question\n\nThe ability to simplify the valuation of `V_P(t)` into the closed-form expression shown in Eq. (2) depends critically on which of the following assumptions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n\nA,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 145, "Question": "### Background\n\n**Research Question.** What is the valuation effect of security issuance announcements by Bank Holding Companies (BHCs), and what do these effects imply about the competing hypotheses regarding information asymmetry and bank capital adequacy?\n\n**Setting.** The study analyzes the stock market's reaction to announcements of common stock and straight debt offerings by BHCs. The findings are evaluated in light of two competing theories.\n\n**Hypotheses.**\n1.  **Regulation/Asymmetry Reduction Hypothesis:** Predicts a negative announcement effect for equity, but one that is less severe than for industrial firms due to regulatory oversight reducing information asymmetry. The effect for straight debt is expected to be insignificant, similar to industrial firms.\n2.  **Bank Capital Hypothesis:** Predicts a significant negative announcement effect for both equity and subordinated debt (which can qualify as regulatory capital), possibly even larger than for industrial firms, as the issuance signals capital inadequacy and heightened bankruptcy risk.\n\n---\n\n### Data / Model Specification\n\nAn event study was conducted to measure the abnormal stock returns around the announcement day (`t=0`). The key results for common stock and straight debt offerings are summarized in Table 1. For context, prior studies find a typical two-day announcement effect for industrial firms' common stock offerings of -3.17%.\n\n**Table 1: Two-Day (t=0 to t+1) Cumulative Average Excess Returns (CAER) for BHC Security Offerings**\n\n| Security Type | N | CAER (%) | z-Statistic |\n| :--- | :-: | :--- | :--- |\n| Common Stock | 19 | -1.507 | -3.065 |\n| Straight Debt (Nonshelf) | 50 | 0.095 | 0.294 |\n\n*Source: Synthesized from Table 3 and Table 4 in the paper.*\n\n---\n\n### The Question\n\nBased on the provided data and context, which of the following conclusions are supported by the event study results?", "Options": {"A": "The announcement of a straight debt offering leads to a statistically significant positive stock price reaction, supporting the idea of positive leverage effects.", "B": "The results for both common stock and straight debt offerings are consistent with the bank capital hypothesis, which predicts negative reactions for any form of capital raising.", "C": "The announcement of a common stock offering is associated with a statistically significant negative stock price reaction at the 1% level.", "D": "The magnitude of the negative reaction to common stock offerings (-1.507%) is more consistent with the regulation/asymmetry reduction hypothesis than the bank capital hypothesis, given the industrial firm benchmark of -3.17%."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the 'Atomic Decomposition' strategy to test the core empirical findings and their theoretical interpretations. The assessment combines 'Computational Judgment' (interpreting the z-statistic for option A) and 'Scenario Application' (evaluating hypotheses against data for option B). Distractor C presents a plausible but incorrect interpretation (sign and significance error). Distractor D tests a misunderstanding of how the evidence adjudicates between the two main hypotheses (conceptual opposite).", "qid": "145", "question": "### Background\n\n**Research Question.** What is the valuation effect of security issuance announcements by Bank Holding Companies (BHCs), and what do these effects imply about the competing hypotheses regarding information asymmetry and bank capital adequacy?\n\n**Setting.** The study analyzes the stock market's reaction to announcements of common stock and straight debt offerings by BHCs. The findings are evaluated in light of two competing theories.\n\n**Hypotheses.**\n1.  **Regulation/Asymmetry Reduction Hypothesis:** Predicts a negative announcement effect for equity, but one that is less severe than for industrial firms due to regulatory oversight reducing information asymmetry. The effect for straight debt is expected to be insignificant, similar to industrial firms.\n2.  **Bank Capital Hypothesis:** Predicts a significant negative announcement effect for both equity and subordinated debt (which can qualify as regulatory capital), possibly even larger than for industrial firms, as the issuance signals capital inadequacy and heightened bankruptcy risk.\n\n---\n\n### Data / Model Specification\n\nAn event study was conducted to measure the abnormal stock returns around the announcement day (`t=0`). The key results for common stock and straight debt offerings are summarized in Table 1. For context, prior studies find a typical two-day announcement effect for industrial firms' common stock offerings of -3.17%.\n\n**Table 1: Two-Day (t=0 to t+1) Cumulative Average Excess Returns (CAER) for BHC Security Offerings**\n\n| Security Type | N | CAER (%) | z-Statistic |\n| :--- | :-: | :--- | :--- |\n| Common Stock | 19 | -1.507 | -3.065 |\n| Straight Debt (Nonshelf) | 50 | 0.095 | 0.294 |\n\n*Source: Synthesized from Table 3 and Table 4 in the paper.*\n\n---\n\n### The Question\n\nBased on the provided data and context, which of the following conclusions are supported by the event study results?", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 294, "Question": "### Background\n\n**Research Question.** How can a tail risk bound be constructed for aggregate claims when the severity distribution is heavy-tailed, possessing only a finite number of moments and no moment generating function?\n\n**Setting / Data-Generating Environment.** We consider a collective risk model where the claim size distribution `F(y)` is heavy-tailed. The paper develops a Pareto-type bound.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `E[X]`: The mean of aggregate claims.\n- `E[Y]`: The mean of individual claim sizes.\n- `E[N]`: The mean number of claims.\n- `\\phi`: A parameter of the claim frequency distribution.\n\n---\n\n### Data / Model Specification\n\nThe general Pareto bound with `r=1` is:\n  \n\\overline{G}(x) \\leq \\frac{(1-p_{0})E[Y]}{\\phi E[Y] + (1-\\phi)x} \\quad \\text{(Eq. (1))}\n \nFor the special case of a compound geometric distribution (`p_n=(1-\\phi)\\phi^n`), this simplifies to:\n  \n\\overline{G}(x) \\leq \\frac{E[X]}{E[X]+x}, \\quad x \\geq 0 \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nIn the derivation that simplifies the general Pareto bound in **Eq. (1)** to the specific bound for a compound geometric distribution in **Eq. (2)**, which of the following statements represent **INCORRECT** steps, assumptions, or intermediate formulas?", "Options": {"A": "After substituting `p_0 = 1-\\phi`, the bound in Eq. (1) simplifies to `\\frac{\\phi E[Y]}{\\phi E[Y] + (1-\\phi)E[X]}`.", "B": "A key property of the compound geometric distribution is that `p_0 = 1-\\phi`.", "C": "For a compound geometric distribution, the mean number of claims is `E[N] = \\phi`.", "D": "A key relationship used in the derivation is Wald's identity, `E[X] = E[N]E[Y]`."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the user's ability to follow a multi-step algebraic derivation by identifying flawed steps. It uses a Complement-set Selection strategy. The correct answers are the incorrect statements (A, B). Statement (A) provides an incorrect formula for the mean of a geometric distribution (Formula Misuse). Statement (B) contains an algebraic error in the denominator during an intermediate step (Step-Omission/Algebraic Error). The distractors (C, D) are correct and necessary premises for the derivation, forcing the user to check the full logical chain.", "qid": "294", "question": "### Background\n\n**Research Question.** How can a tail risk bound be constructed for aggregate claims when the severity distribution is heavy-tailed, possessing only a finite number of moments and no moment generating function?\n\n**Setting / Data-Generating Environment.** We consider a collective risk model where the claim size distribution `F(y)` is heavy-tailed. The paper develops a Pareto-type bound.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `E[X]`: The mean of aggregate claims.\n- `E[Y]`: The mean of individual claim sizes.\n- `E[N]`: The mean number of claims.\n- `\\phi`: A parameter of the claim frequency distribution.\n\n---\n\n### Data / Model Specification\n\nThe general Pareto bound with `r=1` is:\n  \n\\overline{G}(x) \\leq \\frac{(1-p_{0})E[Y]}{\\phi E[Y] + (1-\\phi)x} \\quad \\text{(Eq. (1))}\n \nFor the special case of a compound geometric distribution (`p_n=(1-\\phi)\\phi^n`), this simplifies to:\n  \n\\overline{G}(x) \\leq \\frac{E[X]}{E[X]+x}, \\quad x \\geq 0 \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nIn the derivation that simplifies the general Pareto bound in **Eq. (1)** to the specific bound for a compound geometric distribution in **Eq. (2)**, which of the following statements represent **INCORRECT** steps, assumptions, or intermediate formulas?", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 314, "Question": "### Background\n\n**Research Question.** A researcher aims to test for money endogeneity using time-series data. However, valid causal inference requires addressing two key econometric challenges: potential structural breaks in the data due to shifts in policy regimes, and the non-stationarity of the economic variables.\n\n**Setting / Data-Generating Environment.** The analysis uses quarterly data for the United Kingdom from 1975 to 2006. The researcher suspects a structural break in 1992:Q4. The key variables, Bank Loans (`BL`) and Money Supply (`MS`), are found to be integrated of order one, I(1).\n\n**Variables & Parameters.**\n- `I(1)`: Integrated of order one; a non-stationary time series whose first difference is stationary.\n- `UK 1`: The sub-period from 1975:3 to 1992:3.\n- `UK 2`: The sub-period from 1992:4 to 2006:2.\n\n---\n\n### Data / Model Specification\n\n**1. Structural Break Test:** The Chow test is used to check for a structural break at 1992:Q4. The null hypothesis is parameter stability.\n\n**Table 1: Chow Breakpoint Test Results for UK**\n\n| Test | Statistic | Significance |\n| :--- | :--- | :--- |\n| F-statistic | 152.28 | *** |\n| Log Likelihood Ratio | 156.68 | *** |\n\n*Note: *** indicates significance at the 1% level.*\n\n**2. Cointegration Test:** After splitting the sample, the Johansen cointegration test is run on each sub-period to check for a stable long-run relationship between `BL` and `MS`. The null hypothesis is no cointegration.\n\n**Table 2: Johansen Cointegration Test Results for BL and MS**\n\n| Sample | Trace Test | Max Eigenvalue Test | Conclusion |\n| :--- | :--- | :--- | :--- |\n| UK 1 | 12.01 | 8.15 | No Cointegration |\n| UK 2 | 28.58*** | 24.88*** | Cointegration |\n\n*Note: *** indicates significance at the 1% level.*\n\n---\n\n### Question\n\nBased on the diagnostic tests for the UK time-series data, select **all** statements that represent a correct methodological conclusion or procedural step.", "Options": {"A": "The Johansen test results (Table 2) imply that a Vector Error-Correction Model (VECM) is the appropriate specification for the UK 2 period, while a VAR in first differences should be used for the UK 1 period.", "B": "The highly significant Chow test statistics (Table 1) provide strong evidence of a structural break, making it necessary to split the sample to avoid biased inferences from pooling two distinct data-generating processes.", "C": "For the UK 2 period, the absence of cointegration means that any observed relationship between Bank Loans and Money Supply is likely spurious.", "D": "According to the results, a single VECM model applied to the full 1975-2006 sample is the most powerful way to test for a long-run relationship."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the user's understanding of the correct econometric workflow for time-series analysis with structural breaks. It uses a Premise Packaging strategy, asking the user to select all valid methodological steps. Correct options (A, B) confirm the interpretation of diagnostic tests and the subsequent choice of model. Distractors are based on clear methodological errors: a Conceptual Opposite (C: misreading the cointegration result for UK 2) and a Step-Omission Error (D: ignoring the structural break test result).", "qid": "314", "question": "### Background\n\n**Research Question.** A researcher aims to test for money endogeneity using time-series data. However, valid causal inference requires addressing two key econometric challenges: potential structural breaks in the data due to shifts in policy regimes, and the non-stationarity of the economic variables.\n\n**Setting / Data-Generating Environment.** The analysis uses quarterly data for the United Kingdom from 1975 to 2006. The researcher suspects a structural break in 1992:Q4. The key variables, Bank Loans (`BL`) and Money Supply (`MS`), are found to be integrated of order one, I(1).\n\n**Variables & Parameters.**\n- `I(1)`: Integrated of order one; a non-stationary time series whose first difference is stationary.\n- `UK 1`: The sub-period from 1975:3 to 1992:3.\n- `UK 2`: The sub-period from 1992:4 to 2006:2.\n\n---\n\n### Data / Model Specification\n\n**1. Structural Break Test:** The Chow test is used to check for a structural break at 1992:Q4. The null hypothesis is parameter stability.\n\n**Table 1: Chow Breakpoint Test Results for UK**\n\n| Test | Statistic | Significance |\n| :--- | :--- | :--- |\n| F-statistic | 152.28 | *** |\n| Log Likelihood Ratio | 156.68 | *** |\n\n*Note: *** indicates significance at the 1% level.*\n\n**2. Cointegration Test:** After splitting the sample, the Johansen cointegration test is run on each sub-period to check for a stable long-run relationship between `BL` and `MS`. The null hypothesis is no cointegration.\n\n**Table 2: Johansen Cointegration Test Results for BL and MS**\n\n| Sample | Trace Test | Max Eigenvalue Test | Conclusion |\n| :--- | :--- | :--- | :--- |\n| UK 1 | 12.01 | 8.15 | No Cointegration |\n| UK 2 | 28.58*** | 24.88*** | Cointegration |\n\n*Note: *** indicates significance at the 1% level.*\n\n---\n\n### Question\n\nBased on the diagnostic tests for the UK time-series data, select **all** statements that represent a correct methodological conclusion or procedural step.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 269, "Question": "### Background\n\nThe paper tests the \"flow management hypothesis\" using an interaction model to distinguish whether derivatives improve performance by mitigating flow risk or through other channels like market timing.\n\n### Data / Model Specification\n\nThe model is:\n  \n\\alpha_{i}=\\dots+\\beta_{3}flow\\_risk_{i}+\\beta_{4}flow\\_mgmt_{i}+\\beta_{5}derivatives_{i} +\\dots+\\varepsilon_{i}\n \nwhere `flow_mgmt_i = derivatives_i × flow_risk_i`.\n\n---\n\nAccording to the paper's theoretical framework and empirical strategy for testing the flow management hypothesis, which of the following statements are correct interpretations or predictions?\n", "Options": {"A": "The marginal effect of `flow_risk` on alpha for a derivative-using fund is given by the sum of two coefficients: `β_3 + β_4`.", "B": "The flow management hypothesis predicts that the coefficient on the interaction term, `β_4`, should be positive, indicating that derivatives reduce the negative impact of flow risk.", "C": "The flow management hypothesis predicts that the coefficient on `flow_risk`, `β_3`, should be positive, as higher risk requires more skillful management.", "D": "The coefficient `β_5` on the main `derivatives` term isolates the performance impact of using derivatives that is unrelated to flow management, such as fixed implementation costs or fees."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the econometric specification of an interaction model and how it's used to test a specific economic hypothesis. It requires interpreting the roles of the main and interaction effects in the context of the paper's core argument. It uses a Reverse-Reasoning strategy (identifying correct theoretical predictions).\n\nDistractor Logic:\n- A (Correct): Correctly derives the marginal effect for the treatment group (`derivatives_i = 1`) in an interaction model.\n- B (Correct): Correctly states the central prediction of the flow management hypothesis for the interaction term's coefficient.\n- C (Correct): Correctly interprets the main effect's coefficient (`β_5`) in an interaction model as the effect when the interacting variable (`flow_risk`) is zero.\n- D (Incorrect): This is a Conceptual Opposite. The hypothesis predicts `β_3` will be negative, as it represents the baseline performance drag from flow risk for funds that *do not* use derivatives.", "qid": "269", "question": "### Background\n\nThe paper tests the \"flow management hypothesis\" using an interaction model to distinguish whether derivatives improve performance by mitigating flow risk or through other channels like market timing.\n\n### Data / Model Specification\n\nThe model is:\n  \n\\alpha_{i}=\\dots+\\beta_{3}flow\\_risk_{i}+\\beta_{4}flow\\_mgmt_{i}+\\beta_{5}derivatives_{i} +\\dots+\\varepsilon_{i}\n \nwhere `flow_mgmt_i = derivatives_i × flow_risk_i`.\n\n---\n\nAccording to the paper's theoretical framework and empirical strategy for testing the flow management hypothesis, which of the following statements are correct interpretations or predictions?\n", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 431, "Question": "### Background\n\n**Research Question.** What is the causal effect of an underwriter's reputation on the degree of IPO underpricing, after accounting for the non-random matching between issuers and underwriters?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage least-squares (2SLS) regression to analyze IPO underpricing. The key challenge is that high-quality firms may preferentially match with high-reputation underwriters, creating an endogeneity problem.\n\n**Variables & Parameters.**\n\n*   `Underpricing_i`: The percentage difference between the first-day closing price and the offer price for IPO `i`.\n*   `Reputation_i`: The endogenous measure of underwriter reputation. The primary measure is the `Fragmented ranking Equally weighted` score.\n*   `X_i`: A vector of exogenous control variables for firm `i` (e.g., firm age, offer size, VC-backing).\n*   `Z_i`: The instrumental variable for `Reputation_i`, defined as the number of underwriters that have underwritten European IPOs in the same 2-digit industry as firm `i` in the past five years.\n\n---\n\n### Data / Model Specification\n\nThe study estimates the following structural model:\n\n  \n\\text{Underpricing}_i = \\beta_0 + \\beta_1 \\text{Reputation}_i + \\Gamma' X_i + \\epsilon_i\n \n\nDue to endogeneity, `Reputation_i` is instrumented using `Z_i` in a 2SLS framework.\n\n**Table 1: 2SLS Regression of Underpricing on Underwriter Reputation (Model 1)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| **Second Stage** | | |\n| Underwriter Reputation | | |\n| (Fragmented ranking Equally weighted) | -0.04 | (-6.08) |\n| Ln(Offer Size) | -0.03 | (-5.46) |\n| VC-backed | 0.09 | (1.29) |\n| ... (other controls) | ... | ... |\n| **First Stage** | | |\n| Industry Underwriter Av. (Instrument `Z_i`) | 0.04 | (1.73) |\n\n*Source: Adapted from Table 7, Model 1 in the paper. The dependent variable in the first stage is Underwriter Reputation.* The `underwriter certification hypothesis` posits that reputable underwriters reduce information asymmetry, leading to lower underpricing.\n\n---\n\n### The Question\n\nBased on the provided information, select all statements that correctly describe the 2SLS model's setup, assumptions, and results.", "Options": {"A": "The instrument `Z_i` violates the exclusion restriction because a higher availability of underwriters directly reduces underpricing through increased competition, independent of the chosen underwriter's reputation.", "B": "The primary endogeneity concern is that high-reputation underwriters might take on riskier IPOs to earn higher fees, leading to an upward bias (i.e., less negative) in an OLS estimate of the `Reputation` coefficient.", "C": "The instrument `Z_i` (availability of industry-experienced underwriters) shows evidence of relevance, as its positive first-stage coefficient (0.04) is economically intuitive and at least marginally statistically significant (t=1.73).", "D": "If an OLS regression of `Underpricing` on `Reputation` were run, the coefficient on `Reputation` would likely be biased downwards (i.e., more negative than the true causal effect) due to unobserved issuer quality being positively correlated with `Reputation` and negatively correlated with `Underpricing`."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the core components of a 2SLS estimation in the context of IPO underpricing. It uses an Atomic Decomposition strategy to test distinct concepts: the direction of omitted variable bias and the criteria for instrument relevance. Distractors are designed to represent a plausible alternative theory of bias (C) and a misapplication of an advanced critique as a statement of fact (D).", "qid": "431", "question": "### Background\n\n**Research Question.** What is the causal effect of an underwriter's reputation on the degree of IPO underpricing, after accounting for the non-random matching between issuers and underwriters?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage least-squares (2SLS) regression to analyze IPO underpricing. The key challenge is that high-quality firms may preferentially match with high-reputation underwriters, creating an endogeneity problem.\n\n**Variables & Parameters.**\n\n*   `Underpricing_i`: The percentage difference between the first-day closing price and the offer price for IPO `i`.\n*   `Reputation_i`: The endogenous measure of underwriter reputation. The primary measure is the `Fragmented ranking Equally weighted` score.\n*   `X_i`: A vector of exogenous control variables for firm `i` (e.g., firm age, offer size, VC-backing).\n*   `Z_i`: The instrumental variable for `Reputation_i`, defined as the number of underwriters that have underwritten European IPOs in the same 2-digit industry as firm `i` in the past five years.\n\n---\n\n### Data / Model Specification\n\nThe study estimates the following structural model:\n\n  \n\\text{Underpricing}_i = \\beta_0 + \\beta_1 \\text{Reputation}_i + \\Gamma' X_i + \\epsilon_i\n \n\nDue to endogeneity, `Reputation_i` is instrumented using `Z_i` in a 2SLS framework.\n\n**Table 1: 2SLS Regression of Underpricing on Underwriter Reputation (Model 1)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| **Second Stage** | | |\n| Underwriter Reputation | | |\n| (Fragmented ranking Equally weighted) | -0.04 | (-6.08) |\n| Ln(Offer Size) | -0.03 | (-5.46) |\n| VC-backed | 0.09 | (1.29) |\n| ... (other controls) | ... | ... |\n| **First Stage** | | |\n| Industry Underwriter Av. (Instrument `Z_i`) | 0.04 | (1.73) |\n\n*Source: Adapted from Table 7, Model 1 in the paper. The dependent variable in the first stage is Underwriter Reputation.* The `underwriter certification hypothesis` posits that reputable underwriters reduce information asymmetry, leading to lower underpricing.\n\n---\n\n### The Question\n\nBased on the provided information, select all statements that correctly describe the 2SLS model's setup, assumptions, and results.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 297, "Question": "### Background\n\n**Research Question.** How do real-world frictions like transaction costs and differences in asset characteristics affect the viability of sophisticated trading strategies, particularly at institutional scale?\n\n**Setting.** A large fund attempts to implement the ABMA size/BM rotation strategy. The paper's reported `\\alpha_{net}` of 2.62% only accounts for effective bid-ask spreads and does not include the price impact of large trades, which can be a significant cost for large asset managers.\n\n---\n\n### Data / Model Specification\n\nConsider a $1 billion fund implementing the ABMA size/BM strategy. The strategy longs/shorts portfolios composed entirely of small-cap stocks. Assume the following parameters:\n- **AUM:** $1,000,000,000\n- **Quarterly Turnover:** 40%\n- **Trade Concentration:** Trades are concentrated in 100 small-cap stocks.\n- **Average Daily Volume (ADV):** $5,000,000 per stock.\n- **Price Impact Model:** The one-way cost per trade is `PI = \\lambda \\cdot (Q / ADV)`, where `Q` is the trade size in dollars and `\\lambda=0.1` is a price impact parameter.\n\n---\n\n### Question\n\nGiven the parameters above, which of the following statements about the additional price impact costs faced by the fund are correct?", "Options": {"A": "The one-way price impact cost for an average trade is 4.0%.", "B": "After accounting for these price impact costs, the strategy's reported net alpha of 2.62% would remain positive.", "C": "The total quarterly price impact cost for the fund amounts to 1.6% of its AUM.", "D": "The total annualized price impact cost is approximately 3.2%."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to apply a theoretical model of transaction costs to a practical scenario, a **Computational Judgment** task. It requires a multi-step calculation to evaluate the viability of a strategy at institutional scale. Distractor C is a 'Step-Omission Error' (incorrect annualization), and Distractor D is a 'Conceptual Opposite' that ignores the calculation's outcome, testing whether the student can connect their computation to a final conclusion.", "qid": "297", "question": "### Background\n\n**Research Question.** How do real-world frictions like transaction costs and differences in asset characteristics affect the viability of sophisticated trading strategies, particularly at institutional scale?\n\n**Setting.** A large fund attempts to implement the ABMA size/BM rotation strategy. The paper's reported `\\alpha_{net}` of 2.62% only accounts for effective bid-ask spreads and does not include the price impact of large trades, which can be a significant cost for large asset managers.\n\n---\n\n### Data / Model Specification\n\nConsider a $1 billion fund implementing the ABMA size/BM strategy. The strategy longs/shorts portfolios composed entirely of small-cap stocks. Assume the following parameters:\n- **AUM:** $1,000,000,000\n- **Quarterly Turnover:** 40%\n- **Trade Concentration:** Trades are concentrated in 100 small-cap stocks.\n- **Average Daily Volume (ADV):** $5,000,000 per stock.\n- **Price Impact Model:** The one-way cost per trade is `PI = \\lambda \\cdot (Q / ADV)`, where `Q` is the trade size in dollars and `\\lambda=0.1` is a price impact parameter.\n\n---\n\n### Question\n\nGiven the parameters above, which of the following statements about the additional price impact costs faced by the fund are correct?", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 393, "Question": "### Background\n\n**Research Question.** How are Seasoned Equity Offerings (SEOs) priced relative to prevailing market prices, and which economic theories best explain the observed patterns?\n\n**Setting.** Two main theories compete to explain SEO pricing. The model of Parsons and Raviv (P&R) suggests that due to rationing risk, the pre-offer market price will be higher than the offer price (`P_{-1} > P_0`), and the post-offer price will also be higher than the offer price (`P_1 > P_0`). The model of Loderer, Sheehan, and Kadlec (LSK) argues that to avoid commissions, the offer price must be higher than the pre-offer price (`P_0 > P_{-1}`). Both models predict `P_1 > P_0`.\n\n**Variables & Parameters.**\n- `R_0`: The close-to-offer return, `(P_0 / P_{-1} - 1) * 100`.\n- `R_1`: The offer-to-close return, `(P_1 / P_0 - 1) * 100`.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Average SEO Returns for REITs (1991-1996)**\n\n| Return Type | Sample Size | Raw Return (%) | t-Statistic |\n| :--- | :--- | :--- | :--- |\n| Close-to-Offer (`R_0`) | 178 | -1.14 | -7.45 |\n| Offer-to-Close (`R_1`) | 178 | 0.74 | 4.57 |\n\n*Source: Synthesized from Table 2 in the source document.*\n\n---\n\n### Question\n\nThe study tests two competing theories of SEO pricing using the data in **Table 1**. Based on these results, select all correct conclusions.", "Options": {"A": "The finding of a significantly negative close-to-offer return (`R_0` = -1.14%) contradicts the Loderer, Sheehan, and Kadlec (LSK) model.", "B": "The finding of a significantly negative close-to-offer return (`R_0` = -1.14%) is consistent with the Parsons and Raviv (P&R) model.", "C": "The significantly positive offer-to-close return (`R_1` = 0.74%) is a unique prediction of the LSK model and contradicts the P&R model.", "D": "The empirical results are inconclusive, as they fail to provide clear support for either the P&R or LSK model."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to adjudicate between competing theories using empirical evidence. It uses an **Atomic Decomposition** strategy, where each option is a discrete statement about the consistency between theory and data. Correct options (A, B) accurately reflect that the negative R_0 supports P&R and refutes LSK. Distractor C is a **Factual Error**, as both models predict a positive R_1. Distractor D is a **Factual Error** because the results clearly favor one model over the other.", "qid": "393", "question": "### Background\n\n**Research Question.** How are Seasoned Equity Offerings (SEOs) priced relative to prevailing market prices, and which economic theories best explain the observed patterns?\n\n**Setting.** Two main theories compete to explain SEO pricing. The model of Parsons and Raviv (P&R) suggests that due to rationing risk, the pre-offer market price will be higher than the offer price (`P_{-1} > P_0`), and the post-offer price will also be higher than the offer price (`P_1 > P_0`). The model of Loderer, Sheehan, and Kadlec (LSK) argues that to avoid commissions, the offer price must be higher than the pre-offer price (`P_0 > P_{-1}`). Both models predict `P_1 > P_0`.\n\n**Variables & Parameters.**\n- `R_0`: The close-to-offer return, `(P_0 / P_{-1} - 1) * 100`.\n- `R_1`: The offer-to-close return, `(P_1 / P_0 - 1) * 100`.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Average SEO Returns for REITs (1991-1996)**\n\n| Return Type | Sample Size | Raw Return (%) | t-Statistic |\n| :--- | :--- | :--- | :--- |\n| Close-to-Offer (`R_0`) | 178 | -1.14 | -7.45 |\n| Offer-to-Close (`R_1`) | 178 | 0.74 | 4.57 |\n\n*Source: Synthesized from Table 2 in the source document.*\n\n---\n\n### Question\n\nThe study tests two competing theories of SEO pricing using the data in **Table 1**. Based on these results, select all correct conclusions.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 146, "Question": "### Background\n\n**Research Question.** How does the interaction between tax policy and a firm's growth opportunities affect its systematic risk?\n\n**Setting / Data-Generating Environment.** The study simulates equity betas for a baseline firm (`g=0.25`) and a high-growth firm (`g=1`), where `g` is the rate of expansion upon exercising a growth option. This allows for a direct test of the interaction between tax asymmetry (`δ`) and growth potential.\n\n**Variables & Parameters.**\n- `β_E`: The firm's equity beta.\n- `δ`: The level of tax asymmetry (tax convexity).\n- `g`: The rate of expansion, a proxy for the magnitude of growth opportunities.\n- `X`: The firm's profit flow, an inverse proxy for leverage.\n\n---\n\n### Data / Model Specification\n\nThe following tables show the simulated equity beta for a baseline firm (`g=0.25`) and a high-growth firm (`g=1`).\n\n**Table 1. Equity Beta for Baseline Firm (g=0.25)**\n\n| `δ` | Low Profit (X=2) | Medium Profit (X=4) | High Profit (X=6) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.4718 | 1.0964 | 0.9800 |\n| 0.35 | 1.6578 | 1.1968 | 1.1072 |\n| % Change | **+12.6%** | **+9.1%** | **+13.0%** |\n\n**Table 2. Equity Beta for High-Growth Firm (g=1)**\n\n| `δ` | Low Profit (X=2) | Medium Profit (X=4) | High Profit (X=6) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.0882 | 0.6092 | 0.4360 |\n| 0.35 | 1.3260 | 0.7419 | 0.5554 |\n| % Change | **+21.9%** | **+21.8%** | **+27.4%** |\n\n---\n\n### Question\n\nBased on the provided data and the model's economic principles, which of the following statements about the interaction between tax asymmetry (`δ`) and growth opportunities (`g`) are correct?", "Options": {"A": "For the high-growth firm, increasing tax asymmetry from `δ=0` to `δ=0.35` raises its equity beta by more than 20% across all profit levels shown.", "B": "The percentage increase in equity beta resulting from a higher tax asymmetry (`δ`) is consistently greater for the high-growth firm (`g=1`) than for the baseline firm (`g=0.25`).", "C": "For any given level of tax asymmetry, the absolute equity beta of the high-growth firm is always higher than that of the baseline firm.", "D": "The model implies that the value of a firm's growth options decreases its sensitivity to changes in tax policy."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to identify and quantify an interaction effect from tabular data. It uses an Atomic Decomposition strategy to create two independently verifiable correct statements from the tables. Distractor C is an 'Almost Right' conceptual error that tests for careful reading of absolute vs. relative values. Distractor D is a 'Conceptual Opposite' error that contradicts the core theoretical mechanism where growth options amplify sensitivity.", "qid": "146", "question": "### Background\n\n**Research Question.** How does the interaction between tax policy and a firm's growth opportunities affect its systematic risk?\n\n**Setting / Data-Generating Environment.** The study simulates equity betas for a baseline firm (`g=0.25`) and a high-growth firm (`g=1`), where `g` is the rate of expansion upon exercising a growth option. This allows for a direct test of the interaction between tax asymmetry (`δ`) and growth potential.\n\n**Variables & Parameters.**\n- `β_E`: The firm's equity beta.\n- `δ`: The level of tax asymmetry (tax convexity).\n- `g`: The rate of expansion, a proxy for the magnitude of growth opportunities.\n- `X`: The firm's profit flow, an inverse proxy for leverage.\n\n---\n\n### Data / Model Specification\n\nThe following tables show the simulated equity beta for a baseline firm (`g=0.25`) and a high-growth firm (`g=1`).\n\n**Table 1. Equity Beta for Baseline Firm (g=0.25)**\n\n| `δ` | Low Profit (X=2) | Medium Profit (X=4) | High Profit (X=6) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.4718 | 1.0964 | 0.9800 |\n| 0.35 | 1.6578 | 1.1968 | 1.1072 |\n| % Change | **+12.6%** | **+9.1%** | **+13.0%** |\n\n**Table 2. Equity Beta for High-Growth Firm (g=1)**\n\n| `δ` | Low Profit (X=2) | Medium Profit (X=4) | High Profit (X=6) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.0882 | 0.6092 | 0.4360 |\n| 0.35 | 1.3260 | 0.7419 | 0.5554 |\n| % Change | **+21.9%** | **+21.8%** | **+27.4%** |\n\n---\n\n### Question\n\nBased on the provided data and the model's economic principles, which of the following statements about the interaction between tax asymmetry (`δ`) and growth opportunities (`g`) are correct?", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 342, "Question": "### Background\n\n**Research Question.** This case investigates the source of stock return predictability from moving averages by comparing predictors constructed from low-frequency (monthly) versus high-frequency (daily) price data.\n\n**Setting and Sample.** The analysis uses monthly excess returns on the S&P 500 index from January 1963 to December 2015. Predictor variables are constructed at the end of each month `t` to forecast the excess return in month `t+1`.\n\n### Data / Model Specification\n\nThe predictive relationship is modeled using a bivariate linear regression:\n  \nr_{t+1} = \\alpha + \\beta Z_t + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \nThe novel daily predictor, `MADP_t(K)`, is constructed as the K-day simple moving average of daily prices divided by the current day's closing price, `P_{t,d}`:\n  \nM A D P_{t}(K) = \\frac{M A D_{t}(K)}{P_{t,d}} \\quad \\text{where} \\quad M A D_{t}(K)=\\frac{1}{K}\\sum_{j=0}^{K-1}P_{t,d-j} \\quad \\text{(Eq. 2)}\n \n\n**Table 1: Bivariate Predictive Regression Results (Selected)**\n\n| Predictor `Z_t` | Slope (`β`) | t-statistic | In-Sample `R^2` (%) | Out-of-Sample `R_{os}^2` (%) |\n|:---|---:|:---:|---:|---:|\n| MAMP(12) | -0.65 | (-0.34) | 0.02 | -1.87 |\n| MADP(3) | 1.28 | (4.24)*** | 2.75 | 1.79** |\n| MADP(20) | 0.40 | (1.27) | 0.25 | -1.14 |\n\n*Note: `MAMP(12)` is a predictor based on a 12-month moving average. ***, ** denote significance at the 1% and 5% levels, respectively.*\n\n### Question\n\nBased on the provided model specifications and empirical results in Table 1, select all statements that are correctly supported by the evidence.", "Options": {"A": "The `MAMP(12)` predictor shows stronger out-of-sample performance than the `MADP(20)` predictor, as indicated by its less negative `R_{os}^2`.", "B": "A value of `MADP_t(3) > 1` indicates that the current price `P_{t,d}` is lower than the average price over the preceding 3 days.", "C": "The combination of a positive slope coefficient (`β > 0`) for `MADP(3)` and its definition implies that the predictor captures a short-term mean-reversion effect.", "D": "The significant t-statistic for `MADP(3)` suggests it captures a short-term momentum effect."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the **Atomic Decomposition** strategy to test the core interpretation of the predictor and the main empirical finding. Option A tests the basic definition of the predictor. Option B tests the economic interpretation of the regression results. The distractors test for common misinterpretations: C requires a careful comparison of negative numbers in the table (a common slip), and D presents a conceptual opposite (momentum vs. mean-reversion).", "qid": "342", "question": "### Background\n\n**Research Question.** This case investigates the source of stock return predictability from moving averages by comparing predictors constructed from low-frequency (monthly) versus high-frequency (daily) price data.\n\n**Setting and Sample.** The analysis uses monthly excess returns on the S&P 500 index from January 1963 to December 2015. Predictor variables are constructed at the end of each month `t` to forecast the excess return in month `t+1`.\n\n### Data / Model Specification\n\nThe predictive relationship is modeled using a bivariate linear regression:\n  \nr_{t+1} = \\alpha + \\beta Z_t + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \nThe novel daily predictor, `MADP_t(K)`, is constructed as the K-day simple moving average of daily prices divided by the current day's closing price, `P_{t,d}`:\n  \nM A D P_{t}(K) = \\frac{M A D_{t}(K)}{P_{t,d}} \\quad \\text{where} \\quad M A D_{t}(K)=\\frac{1}{K}\\sum_{j=0}^{K-1}P_{t,d-j} \\quad \\text{(Eq. 2)}\n \n\n**Table 1: Bivariate Predictive Regression Results (Selected)**\n\n| Predictor `Z_t` | Slope (`β`) | t-statistic | In-Sample `R^2` (%) | Out-of-Sample `R_{os}^2` (%) |\n|:---|---:|:---:|---:|---:|\n| MAMP(12) | -0.65 | (-0.34) | 0.02 | -1.87 |\n| MADP(3) | 1.28 | (4.24)*** | 2.75 | 1.79** |\n| MADP(20) | 0.40 | (1.27) | 0.25 | -1.14 |\n\n*Note: `MAMP(12)` is a predictor based on a 12-month moving average. ***, ** denote significance at the 1% and 5% levels, respectively.*\n\n### Question\n\nBased on the provided model specifications and empirical results in Table 1, select all statements that are correctly supported by the evidence.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 223, "Question": "### Background\nThe behavior of central banks is often modeled using a monetary policy reaction function, or a \"Taylor rule,\" which describes how the policy interest rate is set in response to macroeconomic conditions. A key question is whether a central bank's focus shifts over time. Another critical aspect is the type of data used for estimation; using real-time data available to policymakers at the time of their decisions is considered crucial for accurate inference, as opposed to ex-post revised data which contains information unavailable at the time.\n\n### Data / Model Specification\nThe baseline forward-looking policy rule estimated in this paper is:\n\n  \ni_{t}=c+\\rho i_{t-1}+\\phi_{r}r_{t}+\\phi_{\\pi}E_{t}(\\pi_{t+h_{\\pi}}-\\pi_{t+h_{\\pi}}^{*})+\\phi_{x}E_{t}(x_{t+h_{x}})+\\epsilon_{t} \n \n\nwhere $i_t$ is the nominal policy rate, $i_{t-1}$ is its lag, $r_t$ is a measure of the long-run real interest rate, $E_t(\\pi_{t+h_{\\pi}} - \\pi_{t+h_{\\pi}}^*)$ is the expected future inflation gap, and $E_t(x_{t+h_x})$ is the expected future output gap. The parameter $\\rho$ captures interest rate smoothing, while $\\phi_{\\pi}$ and $\\phi_x$ are the short-run responses to inflation and output gaps, respectively. The long-run response to inflation is $\\frac{\\phi_{\\pi}}{1-\\rho}$. The **Taylor principle** is satisfied if this long-run response is greater than 1.\n\nThe paper estimates this model over the full sample period (1991Q1–2015Q4) using two different datasets. Table 1 below presents key estimation results.\n\n**Table 1: Monetary Policy Reaction Function Estimates (1991Q1 – 2015Q4)**\n| Variables | (1) Baseline (Real-Time Data) | (14) Revised Data |\n| :--- | :--- | :--- |\n| $\\rho$: Lagged Int. Rate | 0.889*** | 0.867*** |\n| | (0.028) | (0.025) |\n| $\\phi_{\\pi}$: Exp. Infl. Gap | 0.304*** | 0.004 |\n| | (0.106) | (0.096) |\n| $\\phi_x$: Exp. Output Gap | 0.098*** | 0.167*** |\n| | (0.036) | (0.049) |\n*Note: Newey-West HAC standard errors in parentheses. *** denotes significance at the 1% level.*\n\nBased on the data provided, which of the following statements are correct interpretations of the estimation results?\n", "Options": {"A": "The analysis using revised data suggests that the Bank of Canada's policy did not satisfy the Taylor principle, as the long-run response to inflation is approximately 0.03.", "B": "The estimation with real-time data finds a statistically insignificant response to the expected output gap, indicating the BoC did not consider the state of the real economy.", "C": "The estimation with revised data finds a statistically insignificant response to the expected inflation gap, suggesting an analysis with this data would incorrectly conclude the BoC was not actively targeting inflation.", "D": "Using real-time data, the calculated long-run response to the expected inflation gap is approximately 2.74, satisfying the Taylor principle."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to perform calculations based on a provided formula, interpret the results in the context of a key economic principle (the Taylor principle), and compare and contrast findings based on different data types (real-time vs. revised). Depth Strategy: Computational Judgment. The user must calculate the long-run response for two scenarios and then evaluate the statistical and economic significance of the findings. Distractor Logic: Option D is a conceptual distractor (Almost Right). While the response to the output gap is smaller in the real-time data model than in the revised data model, it is still highly statistically significant (0.098***), making the statement that the response was insignificant incorrect.", "qid": "223", "question": "### Background\nThe behavior of central banks is often modeled using a monetary policy reaction function, or a \"Taylor rule,\" which describes how the policy interest rate is set in response to macroeconomic conditions. A key question is whether a central bank's focus shifts over time. Another critical aspect is the type of data used for estimation; using real-time data available to policymakers at the time of their decisions is considered crucial for accurate inference, as opposed to ex-post revised data which contains information unavailable at the time.\n\n### Data / Model Specification\nThe baseline forward-looking policy rule estimated in this paper is:\n\n  \ni_{t}=c+\\rho i_{t-1}+\\phi_{r}r_{t}+\\phi_{\\pi}E_{t}(\\pi_{t+h_{\\pi}}-\\pi_{t+h_{\\pi}}^{*})+\\phi_{x}E_{t}(x_{t+h_{x}})+\\epsilon_{t} \n \n\nwhere $i_t$ is the nominal policy rate, $i_{t-1}$ is its lag, $r_t$ is a measure of the long-run real interest rate, $E_t(\\pi_{t+h_{\\pi}} - \\pi_{t+h_{\\pi}}^*)$ is the expected future inflation gap, and $E_t(x_{t+h_x})$ is the expected future output gap. The parameter $\\rho$ captures interest rate smoothing, while $\\phi_{\\pi}$ and $\\phi_x$ are the short-run responses to inflation and output gaps, respectively. The long-run response to inflation is $\\frac{\\phi_{\\pi}}{1-\\rho}$. The **Taylor principle** is satisfied if this long-run response is greater than 1.\n\nThe paper estimates this model over the full sample period (1991Q1–2015Q4) using two different datasets. Table 1 below presents key estimation results.\n\n**Table 1: Monetary Policy Reaction Function Estimates (1991Q1 – 2015Q4)**\n| Variables | (1) Baseline (Real-Time Data) | (14) Revised Data |\n| :--- | :--- | :--- |\n| $\\rho$: Lagged Int. Rate | 0.889*** | 0.867*** |\n| | (0.028) | (0.025) |\n| $\\phi_{\\pi}$: Exp. Infl. Gap | 0.304*** | 0.004 |\n| | (0.106) | (0.096) |\n| $\\phi_x$: Exp. Output Gap | 0.098*** | 0.167*** |\n| | (0.036) | (0.049) |\n*Note: Newey-West HAC standard errors in parentheses. *** denotes significance at the 1% level.*\n\nBased on the data provided, which of the following statements are correct interpretations of the estimation results?\n", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: rho = 0.889\nphi_pi = 0.304\nlong_run = phi_pi / (1 ...\n\n> 🔍 **Analyzing monetary policy**\n> ***- The Taylor principle is met if the long-run inflation response exceeds 1.***\n\n\nA,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 152, "Question": "### Background\n\nThe study's theoretical framework models consumer choice along a kinked budget constraint. The theory predicts that changes to the deductible primarily affect the choices of high-spending individuals who expect to be in \"Region 3\" (post-deductible), while having little effect on low-spenders in \"Region 1\" (spending from the account). To test this, the sample is stratified by age as a proxy for expected health expenditure.\n\n### Data / Model Specification\n\n**Table 1: OLS Estimates of the Effect of Deductible on Total Spending, by Age Group**\n\n| Regressor  | Age < 50       | Age ≥ 50       |\n| :--------- | :------------: | :------------: |\n| Deductible | -0.42** (0.17) | -0.88** (0.21) |\n\n*Notes: OLS estimates are displayed. Robust standard errors in parentheses. ** significant at 1%.*\n\n---\n\nBased on the theoretical framework and the empirical results in **Table 1**, select all of the following statements that are supported by the evidence.\n", "Options": {"A": "According to the theoretical model, a change in the deductible primarily impacts high-spenders through a substitution effect, as they switch to lower-cost providers.", "B": "The empirical results are consistent with the theory that the deductible has a stronger marginal effect on spending for individuals with higher expected health care needs.", "C": "For the \"Age ≥ 50\" group, a $1 increase in the deductible is associated with an $0.88 decrease in total spending.", "D": "The difference in the deductible's effect between the two age groups is statistically significant at the 1% level."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to connect the paper's theoretical predictions to its empirical tests and correctly interpret the stratified regression results. Depth Strategy: Scenario Application (applying the general theory to the specific empirical test). Distractor Logic: Option A is a correct high-level conclusion from the table; the coefficient for the older group (-0.88) is more than double that of the younger group (-0.42). Option C is a direct, correct interpretation of the coefficient for the older group. Option B is a Numerical Distractor (Step-Omission Error); a formal hypothesis test shows the difference is not statistically significant at the 5% level, let alone 1% (t-stat ≈ -1.70). Option D is a Conceptual Distractor; the paper describes the deductible's effect on high-spenders as a negative *income effect* (shifting the budget line down), not a substitution effect.", "qid": "152", "question": "### Background\n\nThe study's theoretical framework models consumer choice along a kinked budget constraint. The theory predicts that changes to the deductible primarily affect the choices of high-spending individuals who expect to be in \"Region 3\" (post-deductible), while having little effect on low-spenders in \"Region 1\" (spending from the account). To test this, the sample is stratified by age as a proxy for expected health expenditure.\n\n### Data / Model Specification\n\n**Table 1: OLS Estimates of the Effect of Deductible on Total Spending, by Age Group**\n\n| Regressor  | Age < 50       | Age ≥ 50       |\n| :--------- | :------------: | :------------: |\n| Deductible | -0.42** (0.17) | -0.88** (0.21) |\n\n*Notes: OLS estimates are displayed. Robust standard errors in parentheses. ** significant at 1%.*\n\n---\n\nBased on the theoretical framework and the empirical results in **Table 1**, select all of the following statements that are supported by the evidence.\n", "model_evaluation_result": {"model_raw_response": "\n\nB,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 346, "Question": "### Background\n\n**Research Question.** This case investigates how to use principal component analysis (PCA) to distill predictive signals from a set of highly correlated moving average predictors.\n\n**Setting and Sample.** The analysis uses monthly excess returns on the S&P 500. Predictors are the first principal components extracted from two distinct sets of `MADP` variables: short-term lags (2-10 days) and long-term lags (20-200 days).\n\n### Data / Model Specification\n\nThe principal component predictive regression using the first component is:\n  \nr_{t+1} = \\alpha + \\beta_1 F_{1,t} + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Principal Component Regression Results**\n\n| Predictor `F_{1,t}` | In-Sample `R^2` (%) | Out-of-Sample `R_{os}^2` (%) |\n|:---|---:|---:|\n| `F_{1,t}^{\\text{short}}` (from short-term MADPs) | 2.75 | 2.62 |\n| `F_{1,t}^{\\text{long}}` (from long-term MADPs) | 0.01 | -1.13 |\n\n\n### Question\n\nRegarding the use of principal component analysis (PCA) in this context, select all statements that are methodologically sound or correctly interpret the empirical results in Table 1.", "Options": {"A": "A negative `R_{os}^2` for `F_{1,t}^{\\text{long}}` indicates that the model has significant power to predict market downturns.", "B": "A primary motivation for using PCA is to create a single, powerful predictor from a set of highly correlated `MADP` variables, thus avoiding multicollinearity issues.", "C": "The stark contrast in `R_{os}^2` between `F_{1,t}^{\\text{short}}` (2.62%) and `F_{1,t}^{\\text{long}}` (-1.13%) provides robust evidence that the predictive signal is concentrated in very recent price movements.", "D": "The first principal component `F_{1,t}^{\\text{short}}` is interpreted as the simple arithmetic average of the `MADP(2)` through `MADP(10)` predictors."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a mix of **Premise/Assumption Packaging** (Option A) and **Atomic Decomposition** (Option B) to test the rationale for the method and the interpretation of its results. Option A assesses understanding of why PCA is used. Option B assesses the ability to draw a clear conclusion from the results table. Distractors target common misunderstandings: C confuses a principal component with a simple average (Conceptual Error), and D misinterprets the meaning of a negative out-of-sample R-squared (it means the model is worse than the historical mean, not that it predicts negative outcomes).", "qid": "346", "question": "### Background\n\n**Research Question.** This case investigates how to use principal component analysis (PCA) to distill predictive signals from a set of highly correlated moving average predictors.\n\n**Setting and Sample.** The analysis uses monthly excess returns on the S&P 500. Predictors are the first principal components extracted from two distinct sets of `MADP` variables: short-term lags (2-10 days) and long-term lags (20-200 days).\n\n### Data / Model Specification\n\nThe principal component predictive regression using the first component is:\n  \nr_{t+1} = \\alpha + \\beta_1 F_{1,t} + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Principal Component Regression Results**\n\n| Predictor `F_{1,t}` | In-Sample `R^2` (%) | Out-of-Sample `R_{os}^2` (%) |\n|:---|---:|---:|\n| `F_{1,t}^{\\text{short}}` (from short-term MADPs) | 2.75 | 2.62 |\n| `F_{1,t}^{\\text{long}}` (from long-term MADPs) | 0.01 | -1.13 |\n\n\n### Question\n\nRegarding the use of principal component analysis (PCA) in this context, select all statements that are methodologically sound or correctly interpret the empirical results in Table 1.", "model_evaluation_result": {"model_raw_response": "B", "model_answer": ["B"], "error": null}}
{"ID": 372, "Question": "### Background\n\n**Research Question.** To establish the baseline empirical relationship between government industrial policy and corporate financialization.\n\n**Theoretical Framework.** The paper's central hypothesis (H1) posits that industrial policy reduces corporate financialization. Financialization is driven by two primary motives: a “prevention motivation” (risk management) and a “crowding-out effect” (agency conflicts).\n\n### Data / Model Specification\n\n-   `FINRATIO`: The ratio of a firm's financial assets to its total assets. The sample average is 0.0435.\n-   `IP1`: A dummy variable equal to 1 if a firm's industry is supported by industrial policy, and 0 otherwise.\n-   `AGENCY`: A proxy for agency costs.\n\nThe baseline multivariate model is:\n  \nFINRATIO_{i,t+1} = \\alpha_{0} + \\alpha_{1} IP1_{it} + \\alpha' ControlVariables_{i,t} + \\text{FixedEffects} + e_{i,t+1} \\quad \\text{(Eq. 1)}\n \n\n**Table 1. Univariate Mean Difference Test**\n\n| Group | `FINRATIO` Mean |\n| :--- | :--- |\n| `IP1` = 1 (Supported) | 0.0356 |\n| `IP1` = 0 (Unsupported) | 0.0547 |\n\n**Table 2. Selected Multivariate Regression Results (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | -0.0126 | (-11.05) |\n| `AGENCY`| 0.0594 | (6.18) |\n\n*Note: All presented coefficients are significant at p < .01.*\n\n### Question\n\nBased on the univariate and multivariate results presented, select all of the following statements that are correct.", "Options": {"A": "After controlling for firm characteristics, industrial policy is associated with a 1.26 percentage point reduction in a firm's financialization ratio.", "B": "The multivariate estimate of the policy effect (-0.0126) is larger in magnitude than the unconditional difference from Table 1, suggesting the presence of positive confounding bias.", "C": "The economic significance of the policy effect is modest, reducing the average firm's `FINRATIO` by approximately 1.26%.", "D": "The positive and significant coefficient on `AGENCY` is consistent with the theory that agency problems can lead to higher financialization (the \"crowding-out effect\")."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses Atomic Decomposition to test interpretation of regression coefficients, both for the main variable and a key control, and the concept of economic significance. Correct options A and B test direct interpretation of the main effect and a theory-linked control. Distractors are based on misinterpreting the comparison between univariate and multivariate results (C, which has both a magnitude and sign error in its logic) and confusing coefficient magnitude with percentage effect (D, a Unit Error).", "qid": "372", "question": "### Background\n\n**Research Question.** To establish the baseline empirical relationship between government industrial policy and corporate financialization.\n\n**Theoretical Framework.** The paper's central hypothesis (H1) posits that industrial policy reduces corporate financialization. Financialization is driven by two primary motives: a “prevention motivation” (risk management) and a “crowding-out effect” (agency conflicts).\n\n### Data / Model Specification\n\n-   `FINRATIO`: The ratio of a firm's financial assets to its total assets. The sample average is 0.0435.\n-   `IP1`: A dummy variable equal to 1 if a firm's industry is supported by industrial policy, and 0 otherwise.\n-   `AGENCY`: A proxy for agency costs.\n\nThe baseline multivariate model is:\n  \nFINRATIO_{i,t+1} = \\alpha_{0} + \\alpha_{1} IP1_{it} + \\alpha' ControlVariables_{i,t} + \\text{FixedEffects} + e_{i,t+1} \\quad \\text{(Eq. 1)}\n \n\n**Table 1. Univariate Mean Difference Test**\n\n| Group | `FINRATIO` Mean |\n| :--- | :--- |\n| `IP1` = 1 (Supported) | 0.0356 |\n| `IP1` = 0 (Unsupported) | 0.0547 |\n\n**Table 2. Selected Multivariate Regression Results (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | -0.0126 | (-11.05) |\n| `AGENCY`| 0.0594 | (6.18) |\n\n*Note: All presented coefficients are significant at p < .01.*\n\n### Question\n\nBased on the univariate and multivariate results presented, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 197, "Question": "### Background\n\nA researcher is analyzing the distribution of publication speeds across the top twenty finance journals to inform submission strategies. The study finds that journals cluster into a central 'pack' with some notable outliers.\n\n### Data / Model Specification\n\nThe paper defines the majority 'pack' of top-twenty finance journals as those having a median acceptance time in the range from 10.0 to 14.0 months. Data for a selection of these journals are provided below.\n\n**Table 1. Distribution of Median Acceptance Times for Select Top Twenty Finance Journals (2012-2015)**\n\n| Journal | Median Acceptance Time (Months) |\n| :--- | :--- |\n| Review of Corporate Finance Studies (RCFS) | 7.4 |\n| Review of Asset Pricing Studies (RAPS) | 7.6 |\n| Journal of Corporate Finance (JCF) | 10.3 |\n| Journal of Money, Credit, and Banking (JMCB) | 13.9 |\n| Review of Financial Studies (RFS) | 15.5 |\n| Journal of Financial Econometrics (JFEconometrics) | 21.0 |\n\n---\n\nBased on the provided definition and data, which of the following journals fall OUTSIDE the defined median acceptance time range of the 'pack' (10.0 to 14.0 months)?\n\nSelect all that apply.", "Options": {"A": "Review of Asset Pricing Studies (RAPS)", "B": "Review of Financial Studies (RFS)", "C": "Journal of Financial Econometrics (JFEconometrics)", "D": "Review of Corporate Finance Studies (RCFS)"}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item tests the ability to apply a specific quantitative rule (the definition of the 'pack') to a dataset to correctly classify items. It is a precise test of data interpretation and application of a definition.\n\nChosen Strategy: Computational Judgment. The user must compare each journal's median time to the 10.0-14.0 month range and make a binary judgment (in/out).\n\nDistractor Logic: The distractors in this case would be the journals that are *inside* the pack (JCF and JMCB). By asking which are OUTSIDE, the question is structured to have multiple correct answers. This requires the user to check every single option against the rule, ensuring a comprehensive assessment rather than just finding one correct example.", "qid": "197", "question": "### Background\n\nA researcher is analyzing the distribution of publication speeds across the top twenty finance journals to inform submission strategies. The study finds that journals cluster into a central 'pack' with some notable outliers.\n\n### Data / Model Specification\n\nThe paper defines the majority 'pack' of top-twenty finance journals as those having a median acceptance time in the range from 10.0 to 14.0 months. Data for a selection of these journals are provided below.\n\n**Table 1. Distribution of Median Acceptance Times for Select Top Twenty Finance Journals (2012-2015)**\n\n| Journal | Median Acceptance Time (Months) |\n| :--- | :--- |\n| Review of Corporate Finance Studies (RCFS) | 7.4 |\n| Review of Asset Pricing Studies (RAPS) | 7.6 |\n| Journal of Corporate Finance (JCF) | 10.3 |\n| Journal of Money, Credit, and Banking (JMCB) | 13.9 |\n| Review of Financial Studies (RFS) | 15.5 |\n| Journal of Financial Econometrics (JFEconometrics) | 21.0 |\n\n---\n\nBased on the provided definition and data, which of the following journals fall OUTSIDE the defined median acceptance time range of the 'pack' (10.0 to 14.0 months)?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: print(7.6 >= 10.0 and 7.6 <= 14.0)\nprint(15.5 >= 1...\n\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 400, "Question": "### Background\n\n**Research Question.** This case investigates the relative importance of three investor characteristics—sophistication, experience (proxied by age), and the use of stop-loss orders—in mitigating the disposition effect. The analysis aims to determine which factors remain significant when all are considered simultaneously.\n\n**Setting / Data-Generating Environment.** The analysis uses a full Cox proportional hazard model including all three characteristics and their interactions with the trading gain/loss indicators. This allows for a direct comparison of the marginal contribution of each factor, controlling for the others.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator, a dummy variable equal to 1 if a position is at a loss.\n- `TGI(t)`: Trading Gain Indicator, a dummy variable equal to 1 if a position is at a gain.\n- `Soph`: A time-invariant dummy variable equal to 1 for sophisticated investors (warrant traders).\n- `Age`: A continuous variable defined as `(investor's actual age - 18) / 10`. Hazard ratios reflect the effect of a 10-year increase in age.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who have used a stop-loss order at least once.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a comprehensive model with multiple interaction terms. The key results from this full specification are summarized in Table 1.\n\n**Table 1: Combined Model of Predictive Variables**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.3948*** | | \n| TGI | | 2.3777*** |\n| TLI × Sophistication | 1.039 | | \n| TGI × Sophistication | | 0.8987*** |\n| TLI × Age | 1.1054*** | | \n| TGI × Age | | 0.9356*** |\n| TLI × SL_User | 1.1700*** | | \n| TGI × SL_User | | 0.7409*** |\n\n*Notes: *** denotes significance at the 1% level.*\n\n---\n\n### Question\n\nConsider a 38-year-old, non-sophisticated investor (for whom the `Age` variable = 2.0 and `Soph` = 0). Based on the model results in Table 1, select all of the following statements that are correct.", "Options": {"A": "The disposition ratio (HR_winner / HR_loser) for this investor, if they are *not* a stop-loss user, is approximately 4.31.", "B": "Becoming a stop-loss user reduces this investor's disposition ratio by more than 35%.", "C": "If this investor becomes a stop-loss user, their conditional probability of selling a losing stock becomes lower than if they were not a user.", "D": "For this investor, being a stop-loss user has a larger percentage impact on their propensity to sell winners than it does on their propensity to sell losers."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform a multi-step calculation using the Cox model's hazard ratios. It uses a 'Computational Judgment' strategy, requiring students to calculate intermediate and final values to verify the provided statements. Options A and B are correct calculations derived from the source QA. Distractor C is a 'Sign Error' misconception, as the hazard ratio for selling losers *increases*. Distractor D is an 'Almost Right' distractor requiring a careful comparison of percentage changes (a 25.9% decrease for winners vs. a 17.0% increase for losers), a common point of confusion.", "qid": "400", "question": "### Background\n\n**Research Question.** This case investigates the relative importance of three investor characteristics—sophistication, experience (proxied by age), and the use of stop-loss orders—in mitigating the disposition effect. The analysis aims to determine which factors remain significant when all are considered simultaneously.\n\n**Setting / Data-Generating Environment.** The analysis uses a full Cox proportional hazard model including all three characteristics and their interactions with the trading gain/loss indicators. This allows for a direct comparison of the marginal contribution of each factor, controlling for the others.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator, a dummy variable equal to 1 if a position is at a loss.\n- `TGI(t)`: Trading Gain Indicator, a dummy variable equal to 1 if a position is at a gain.\n- `Soph`: A time-invariant dummy variable equal to 1 for sophisticated investors (warrant traders).\n- `Age`: A continuous variable defined as `(investor's actual age - 18) / 10`. Hazard ratios reflect the effect of a 10-year increase in age.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who have used a stop-loss order at least once.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a comprehensive model with multiple interaction terms. The key results from this full specification are summarized in Table 1.\n\n**Table 1: Combined Model of Predictive Variables**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.3948*** | | \n| TGI | | 2.3777*** |\n| TLI × Sophistication | 1.039 | | \n| TGI × Sophistication | | 0.8987*** |\n| TLI × Age | 1.1054*** | | \n| TGI × Age | | 0.9356*** |\n| TLI × SL_User | 1.1700*** | | \n| TGI × SL_User | | 0.7409*** |\n\n*Notes: *** denotes significance at the 1% level.*\n\n---\n\n### Question\n\nConsider a 38-year-old, non-sophisticated investor (for whom the `Age` variable = 2.0 and `Soph` = 0). Based on the model results in Table 1, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 394, "Question": "### Background\n\n**Research Question.** Why are Seasoned Equity Offerings (SEOs) typically underpriced, resulting in a positive average return from the offer price to the closing price on the offer day (`R_1 > 0`)?\n\n**Setting.** One prominent explanation is the 'winner's curse' hypothesis, which is based on information asymmetry between informed and uninformed investors. Uninformed investors know they are at an informational disadvantage and will only participate in offerings if they expect to make a non-negative return on average.\n\n---\n\n### Question\n\nThe 'winner's curse' is a key concept in information asymmetry models used to explain SEO underpricing. Select all statements that accurately describe the logic or implications of the winner's curse.", "Options": {"A": "The problem is most severe in markets with no information asymmetry, as all investors face the same risk of overpaying.", "B": "The winner's curse implies that it is the *informed* investors who systematically lose money in SEOs because they are cursed with knowing too much.", "C": "To prevent uninformed investors from systematically losing money and withdrawing from the market, issues must be underpriced on average.", "D": "It arises because uninformed investors tend to receive larger allocations of overpriced ('bad') issues and smaller, rationed allocations of underpriced ('good') issues."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the conceptual understanding of the winner's curse, a core theoretical underpinning of the paper. It uses an **Atomic Decomposition** strategy to break the theory into its core premise and its main implication. Correct options (A, B) accurately describe these two components. Distractor C is a **Conceptual Opposite**, misidentifying who is cursed. Distractor D is also a **Conceptual Opposite**, as the curse requires information asymmetry to exist.", "qid": "394", "question": "### Background\n\n**Research Question.** Why are Seasoned Equity Offerings (SEOs) typically underpriced, resulting in a positive average return from the offer price to the closing price on the offer day (`R_1 > 0`)?\n\n**Setting.** One prominent explanation is the 'winner's curse' hypothesis, which is based on information asymmetry between informed and uninformed investors. Uninformed investors know they are at an informational disadvantage and will only participate in offerings if they expect to make a non-negative return on average.\n\n---\n\n### Question\n\nThe 'winner's curse' is a key concept in information asymmetry models used to explain SEO underpricing. Select all statements that accurately describe the logic or implications of the winner's curse.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"winner's curse in seasoned equity offerings\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Winner's Curse: Definition, How It Works, Causes, and Example\" Investopedia](https://www.investopedia.com/terms/w/winnerscurse.asp)\n> [🔗 \"Winner's curse - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Winner%2527s_curse)\n> [🔗 \"The Pricing of Seasoned Equity Offerings: Evidence from REITs | Request PDF\" ResearchGate](https://www.researchgate.net/publication/23523695_The_Pricing_of_Seasoned_Equity_Offerings_Evidence_from_REITs)\n> [🔗 \"Seasoned equity offerings and the short‐ and long‐run performance of initial public offerings in the UK\" IDEAS/RePEc](https://ideas.repec.org/a/bla/eufman/v1y1995i2p125-146.html)\n> [🔗 \"Winner's Curse: Negotiation Mistakes to Avoid\" PON - Program on Negotiation](https://www.pon.harvard.edu/daily/business-negotiations/how-to-avoid-the-winners-curse/)\n> [🔗 \"Prevalence of The Winner’s Curse in Initial Public Offerings : Networks Course blog for INFO 2040/CS 2850/Econ 2040/SOC 2090\" Cornell Blogs](https://blogs.cornell.edu/info2040/2017/10/18/prevalence-of-the-winners-curse-in-initial-public-offerings/)\n> [🔗 \"[Solved] discuss the winners curse to short run ipo underpricing and is - Law (M100) - Studocu\" Studocu](https://www.studocu.com/en-gb/messages/question/5813454/discuss-the-winners-curse-to-short-run-ipo-underpricing-and-is-this-a-good-explanation-for-short)\n> [🔗 \"The winner’s curse in acquisitions of privately-held firms - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1062976917300406)\n> [🔗 \"The Ultimate Guide to Winner's Curse in Markets\" Number Analytics](https://www.numberanalytics.com/blog/winner-curse-microeconomics-guide)\n> [🔗 \"Full article: Windows of opportunity and seasoned equity offerings: An empirical study\" Taylor & Francis Online](https://www.tandfonline.com/doi/full/10.1080/23322039.2018.1528688)\n\n\nC,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 118, "Question": "### Background\n\nTo model the pass-through between different interest rates, one must have a theoretical framework for how rates are related and how their dynamics should be modeled based on their statistical properties. The analysis considers a conceptual framework for decomposing interest rate spreads and modeling the dynamic adjustment between two rates, `r_t` and `i_t`, based on whether they are stationary or cointegrated non-stationary processes.\n\n### Data / Model Specification\n\nThe market structure of interest rates posits that the spread between a riskier, longer-term rate (`r_t`) and a less-risky, shorter-term rate (`i_t`) can be decomposed as:\n  \nr_{t}=i_{t}+\\gamma+\\phi \\quad \\text{(Eq. 1)}\n \nwhere `\\gamma` is the term premium and `\\phi` is the risk premium.\n\nIf `r_t` and `i_t` are non-stationary (I(1)) but cointegrated, their dynamic relationship can be modeled with an Error Correction Model (ECM) of the form:\n  \n\\Delta{r_{t}}=c+\\beta_{0}\\Delta{i_{t}}+\\gamma({r_{t-1}}-{i_{t-1}}) \\quad \\text{(Eq. 2)}\n \n\n### Question\n\nBased on the theoretical framework presented in Eq. (1) and Eq. (2), select all of the following statements that are correct.", "Options": {"A": "The coefficient `\\beta_0` in Eq. (2) measures the speed at which `r_t` returns to its long-run equilibrium with `i_t` after a shock.", "B": "For the dynamic system in Eq. (2) to be stable, the error correction parameter `\\gamma` must be a positive value between 0 and 1.", "C": "For the two non-stationary rates `r_t` and `i_t` to be cointegrated, the combined premium `\\gamma + \\phi` must be a stationary process.", "D": "The cointegrating vector that makes the spread `r_t - i_t` stationary is `[1, -1]`."}, "Answer": ["C", "D"], "pi_justification": "This item assesses foundational knowledge of cointegration and Error Correction Models. It uses a 'select-all-valid' format to efficiently test multiple concepts. Distractor B confuses the short-run pass-through (`\\beta_0`) with the speed of adjustment (`\\gamma`), a classic misconception. Distractor D introduces a sign error, as the adjustment parameter `\\gamma` must be negative for stability, another common point of confusion.", "qid": "118", "question": "### Background\n\nTo model the pass-through between different interest rates, one must have a theoretical framework for how rates are related and how their dynamics should be modeled based on their statistical properties. The analysis considers a conceptual framework for decomposing interest rate spreads and modeling the dynamic adjustment between two rates, `r_t` and `i_t`, based on whether they are stationary or cointegrated non-stationary processes.\n\n### Data / Model Specification\n\nThe market structure of interest rates posits that the spread between a riskier, longer-term rate (`r_t`) and a less-risky, shorter-term rate (`i_t`) can be decomposed as:\n  \nr_{t}=i_{t}+\\gamma+\\phi \\quad \\text{(Eq. 1)}\n \nwhere `\\gamma` is the term premium and `\\phi` is the risk premium.\n\nIf `r_t` and `i_t` are non-stationary (I(1)) but cointegrated, their dynamic relationship can be modeled with an Error Correction Model (ECM) of the form:\n  \n\\Delta{r_{t}}=c+\\beta_{0}\\Delta{i_{t}}+\\gamma({r_{t-1}}-{i_{t-1}}) \\quad \\text{(Eq. 2)}\n \n\n### Question\n\nBased on the theoretical framework presented in Eq. (1) and Eq. (2), select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "\n\nA: False. The coefficient β₀ measures the contemporaneous impact of Δiₜ on Δrₜ (short-run pass-through), not the speed of adjustment.\n\nB: False. For stability in the ECM, γ must be negative (typically between -1 and 0), indicating mean-reversion.\n\nC: False. Cointegration requires the linear combination (here, rₜ - iₜ - (γ + φ)) to be stationary, but the premium γ + φ is a constant, which is stationary by definition. However, the statement implies the premium's stationarity is the condition for cointegration, which is incorrect; non-cointegration could still occur if the spread doesn't revert.\n\nD: True. The cointegrating vector [1, -1] defines the spread rₜ - iₜ, and under cointegration, this combination is stationary.\n\nCorrect: D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 435, "Question": "### Background\n\n**Research Question.** A central challenge in empirical asset pricing is accurately measuring a firm's exposure to risk factors, such as exchange rate fluctuations. Standard models, like the augmented CAPM proposed by Jorion, often find surprisingly weak evidence of such exposure. This paper argues that this weak evidence may be a statistical artifact stemming from the model's failure to account for the correlation between exchange rate movements and overall stock market returns, which are both driven by common macroeconomic news.\n\n**Setting / Data-Generating Environment.** The analysis compares two methods for estimating firm-level currency exposure for European firms from 1999-2011: the standard Jorion model and a three-step orthogonalization procedure designed to address the correlation issue. The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Model 1: The Standard Jorion Model**\nAn augmented CAPM specification regresses a firm's stock return (`r_{it}`) on the contemporaneous exchange rate return (`r_{st}`) and the market portfolio return (`r_{mt}`).\n  \nr_{\\mathrm{it}}=\\beta_{0i}+\\beta_{1i}r_{\\mathrm{st}}+\\beta_{2i}r_{\\mathrm{mt}}+\\mu_{\\mathrm{it}} \\quad \\text{(Eq. (1))}\n \n\n**Model 2: The Three-Step Orthogonalization Procedure**\nThis procedure first isolates unexpected shocks to the exchange rate and the market, and then estimates exposure using these orthogonal shocks.\n- **Step 1:** Isolate unexpected exchange rate returns (`\\hat{\\varepsilon}_{st}`) by regressing `r_{st}` on lagged macroeconomic control variables.\n- **Step 2:** Isolate unexpected market returns (`\\hat{e}_{mt}^M`) by regressing `r_{mt}` on the same controls *and* the unexpected exchange rate returns `\\hat{\\varepsilon}_{st}` from Step 1.\n- **Step 3:** Estimate firm-level exposure by regressing `r_{it}` on the orthogonal shocks from the first two steps:\n  \nr_{\\mathrm{{it}}}=\\alpha_{i,0}+\\dots+\\beta_{\\mathrm{{im}}}\\widehat{e_{\\mathrm{{mt}}}^{M}}+\\beta_{\\mathrm{{is}}}\\widehat{\\varepsilon_{\\mathrm{{st}}}}+\\gamma_{\\mathrm{{it}}} \\quad \\text{(Eq. (2))}\n \n\n**Table 1. Second-Stage Regression Results (Euro Stoxx TMI)**\nThis table shows the results from regressing the market return (`r_{mt}`) on macroeconomic controls and the unexpected exchange rate shock (`\\hat{\\varepsilon}_{st}`) from Step 1.\n\n| | **Second Stage (for Market Return `r_{mt}` )** | | | |\n|:---|:---:|:---:|:---:|:---:|\n| **Currency Shock Used** | **JPY** | **USD** | **GBP** | **CHF** |\n| `\\hat{\\varepsilon}_{st}` Coefficient | 0.43*** | 0.21 | 0.27 | 1.18*** |\n\n*Notes: Significance levels: *** 1%. Coefficients for control variables are omitted.* \n\n### Question\n\nBased on the provided models and data, select all statements that accurately describe the methodological problem with the standard Jorion model and the logic of the orthogonalization procedure.", "Options": {"A": "The Jorion model's primary flaw is its failure to include lagged macroeconomic control variables, which leads to omitted variable bias.", "B": "The orthogonalization procedure assumes that the raw market return (`r_{mt}`) is a suitable proxy for the market risk factor in the final regression, as it captures all systematic risks.", "C": "The purpose of including `\\hat{\\varepsilon}_{st}` in the second-stage regression is to ensure that the resulting market shock residual (`\\hat{e}_{mt}^M`) is, by construction, statistically orthogonal to the exchange rate shock.", "D": "Based on Table 1, the significant coefficient of 1.18 on the CHF shock (`\\hat{\\varepsilon}_{st}`) in the second-stage regression provides direct evidence that market returns and exchange rate shocks are correlated, violating a key assumption of the Jorion model."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test understanding of the paper's core methodological critique. It assesses the ability to connect empirical evidence from Table 1 to a theoretical flaw (Option A) and understand the mechanics of the proposed solution (Option B). Distractors are designed based on a Conceptual Opposite (C) and a misinterpretation of the paper's main argument (D).", "qid": "435", "question": "### Background\n\n**Research Question.** A central challenge in empirical asset pricing is accurately measuring a firm's exposure to risk factors, such as exchange rate fluctuations. Standard models, like the augmented CAPM proposed by Jorion, often find surprisingly weak evidence of such exposure. This paper argues that this weak evidence may be a statistical artifact stemming from the model's failure to account for the correlation between exchange rate movements and overall stock market returns, which are both driven by common macroeconomic news.\n\n**Setting / Data-Generating Environment.** The analysis compares two methods for estimating firm-level currency exposure for European firms from 1999-2011: the standard Jorion model and a three-step orthogonalization procedure designed to address the correlation issue. The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Model 1: The Standard Jorion Model**\nAn augmented CAPM specification regresses a firm's stock return (`r_{it}`) on the contemporaneous exchange rate return (`r_{st}`) and the market portfolio return (`r_{mt}`).\n  \nr_{\\mathrm{it}}=\\beta_{0i}+\\beta_{1i}r_{\\mathrm{st}}+\\beta_{2i}r_{\\mathrm{mt}}+\\mu_{\\mathrm{it}} \\quad \\text{(Eq. (1))}\n \n\n**Model 2: The Three-Step Orthogonalization Procedure**\nThis procedure first isolates unexpected shocks to the exchange rate and the market, and then estimates exposure using these orthogonal shocks.\n- **Step 1:** Isolate unexpected exchange rate returns (`\\hat{\\varepsilon}_{st}`) by regressing `r_{st}` on lagged macroeconomic control variables.\n- **Step 2:** Isolate unexpected market returns (`\\hat{e}_{mt}^M`) by regressing `r_{mt}` on the same controls *and* the unexpected exchange rate returns `\\hat{\\varepsilon}_{st}` from Step 1.\n- **Step 3:** Estimate firm-level exposure by regressing `r_{it}` on the orthogonal shocks from the first two steps:\n  \nr_{\\mathrm{{it}}}=\\alpha_{i,0}+\\dots+\\beta_{\\mathrm{{im}}}\\widehat{e_{\\mathrm{{mt}}}^{M}}+\\beta_{\\mathrm{{is}}}\\widehat{\\varepsilon_{\\mathrm{{st}}}}+\\gamma_{\\mathrm{{it}}} \\quad \\text{(Eq. (2))}\n \n\n**Table 1. Second-Stage Regression Results (Euro Stoxx TMI)**\nThis table shows the results from regressing the market return (`r_{mt}`) on macroeconomic controls and the unexpected exchange rate shock (`\\hat{\\varepsilon}_{st}`) from Step 1.\n\n| | **Second Stage (for Market Return `r_{mt}` )** | | | |\n|:---|:---:|:---:|:---:|:---:|\n| **Currency Shock Used** | **JPY** | **USD** | **GBP** | **CHF** |\n| `\\hat{\\varepsilon}_{st}` Coefficient | 0.43*** | 0.21 | 0.27 | 1.18*** |\n\n*Notes: Significance levels: *** 1%. Coefficients for control variables are omitted.* \n\n### Question\n\nBased on the provided models and data, select all statements that accurately describe the methodological problem with the standard Jorion model and the logic of the orthogonalization procedure.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"Jorion model currency exposure CAPM\"\n> 📊 **Results requested:** 5\n\n> 🔍 **Searching for:** \"orthogonalization procedure currency exposure Jorion model\"\n> 📊 **Results requested:** 5\n\n> 🔍 **5 results found**\n\n> [🔗 \"JORION-More papers\" UCI](https://merage.uci.edu/~jorion/morep.htm)\n> [🔗 \"VALUE AT RISK: PHILIPPE JORION\" UCI](https://merage.uci.edu/~jorion/answer.pdf)\n> [🔗 \"Value at Risk: The New Benchmark for Managing Financial Risk | Request PDF\" ResearchGate](https://www.researchgate.net/publication/243767965_Value_at_Risk_The_New_Benchmark_for_Managing_Financial_Risk)\n> [🔗 \"International Capital Asset Pricing Model (CAPM) Overview\" Investopedia](https://www.investopedia.com/terms/i/international-capm.asp)\n> [🔗 \"Philippe Jorion: Value at Risk – The New Benchmark for Managing ...\" Springer](https://link.springer.com/content/pdf/10.1007/s11408-007-0057-3.pdf)\n\n> 🔍 **5 results found**\n\n> [🔗 \"Orthogonalized regressors and spurious precision, with an application to currency exposures - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0261560614002010)\n> [🔗 \"Linear and nonlinear exchange rate exposure - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0261560607000605)\n> [🔗 \"How does currency risk impact firms? New evidence from bank loan contracts - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S092911992400004X)\n> [🔗 \"Jorion, P., 1990. The exchange rate exposure of US multinationals. Journal of Business 63, 331-341.\" SciEPub](https://www.sciepub.com/reference/2712)\n> [🔗 \"Orthogonal portfolios to assess estimation risk - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S1059056022000673)\n\n> 🔍 **Analyzing models**\n> ***- The standard Jorion model regresses firm stock returns on exchange rate and market returns.***\n\n\nC,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 329, "Question": "### Background\n\n**Research Question.** Why might the popular EV/EBITDA multiple be a less reliable valuation tool than the EV/Unlevered Earnings multiple, particularly when comparing firms with different tax rates or capital intensities?\n\n**Setting.** The analysis compares two key enterprise value multiples: EV/EBITDA and EV/Unlevered Earnings (also known as NOPAT). The goal is to understand their relative sensitivity to underlying value drivers based on a simulation using a financial model for Staples, Inc.\n\n**Variables and Parameters.**\n- `Enterprise Value (EV)`: The market value of the firm's operating assets.\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization.\n- `Unlevered Earnings (NOPAT)`: Net Operating Profit After Tax, defined as `EBIT * (1 - Tax Rate)`.\n- `CapEx`: Capital Expenditures.\n\n---\n\n### Data / Model Specification\n\nSimulation results from the paper show the sensitivity of multiples to permanent changes in value drivers that are calibrated to cause a 10% increase in EV.\n\n**Table 1: Sensitivity of Multiples to Changes in Tax Rate and CapEx**\n| Scenario | % Change in Unlevered Earnings Multiple | % Change in EBITDA Multiple |\n| :--- | :--- | :--- |\n| Income Tax Rate Reduced | +0.6% | +10.0% |\n| CapEx Requirements Reduced | +9.5% | +10.0% |\n\nKey definitions:\n- `EBITDA = EBIT + Depreciation & Amortization`\n- `Unlevered Earnings = EBIT - Taxes on EBIT = EBIT * (1 - τ_c)`\n\n---\n\n### Question\n\nBased on the provided definitions and simulation results in **Table 1**, select all of the following statements that correctly describe the relative vulnerabilities of the EV/EBITDA and EV/Unlevered Earnings multiples.", "Options": {"A": "When valuing a firm with significant Net Operating Losses (NOLs), using the cash tax rate (0%) to calculate NOPAT provides the most comparable multiple to peer firms without NOLs.", "B": "The EV/Unlevered Earnings multiple is relatively stable against tax rate changes because both its numerator (EV) and denominator (Unlevered Earnings) are after-tax measures that move in the same direction.", "C": "The EV/EBITDA multiple is highly sensitive to a firm's capital intensity because the EBITDA denominator ignores capital expenditures and depreciation, while these investments directly affect firm value (the numerator).", "D": "A change in the corporate tax rate significantly impacts the EV/EBITDA multiple but has a minimal effect on the EV/Unlevered Earnings multiple."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses Atomic Decomposition to test the paper's central conclusion about the relative merits of NOPAT vs. EBITDA multiples. It assesses the user's ability to interpret simulation data in light of accounting definitions. Distractor (D) is a high-fidelity conceptual opposite based on an advanced scenario from the original problem (valuing firms with NOLs), testing for deep understanding rather than simple table lookup.", "qid": "329", "question": "### Background\n\n**Research Question.** Why might the popular EV/EBITDA multiple be a less reliable valuation tool than the EV/Unlevered Earnings multiple, particularly when comparing firms with different tax rates or capital intensities?\n\n**Setting.** The analysis compares two key enterprise value multiples: EV/EBITDA and EV/Unlevered Earnings (also known as NOPAT). The goal is to understand their relative sensitivity to underlying value drivers based on a simulation using a financial model for Staples, Inc.\n\n**Variables and Parameters.**\n- `Enterprise Value (EV)`: The market value of the firm's operating assets.\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization.\n- `Unlevered Earnings (NOPAT)`: Net Operating Profit After Tax, defined as `EBIT * (1 - Tax Rate)`.\n- `CapEx`: Capital Expenditures.\n\n---\n\n### Data / Model Specification\n\nSimulation results from the paper show the sensitivity of multiples to permanent changes in value drivers that are calibrated to cause a 10% increase in EV.\n\n**Table 1: Sensitivity of Multiples to Changes in Tax Rate and CapEx**\n| Scenario | % Change in Unlevered Earnings Multiple | % Change in EBITDA Multiple |\n| :--- | :--- | :--- |\n| Income Tax Rate Reduced | +0.6% | +10.0% |\n| CapEx Requirements Reduced | +9.5% | +10.0% |\n\nKey definitions:\n- `EBITDA = EBIT + Depreciation & Amortization`\n- `Unlevered Earnings = EBIT - Taxes on EBIT = EBIT * (1 - τ_c)`\n\n---\n\n### Question\n\nBased on the provided definitions and simulation results in **Table 1**, select all of the following statements that correctly describe the relative vulnerabilities of the EV/EBITDA and EV/Unlevered Earnings multiples.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"Why might the popular EV/EBITDA multiple be a less reliable valuation tool than the EV/Unlevered Earnings multiple, particularly when comparing firms with different tax rates or capital intensities?\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"EV/EBITDA vs EV/NOPAT sensitivity to tax rates and CapEx Staples Inc simulation\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Should You Use the EV/EBITDA or P/E Multiple?\" Investopedia](https://www.investopedia.com/ask/answers/061515/which-metric-should-i-pay-more-attention-evebitda-or-pe.asp)\n> [🔗 \"What Is Considered a Healthy EV/EBITDA?\" Investopedia](https://www.investopedia.com/ask/answers/072715/what-considered-healthy-evebitda.asp)\n> [🔗 \"Relative valuation conflicts - EV/EBITDA versus P/E | The Footnotes Analyst\" Footnotesanalyst](https://www.footnotesanalyst.com/relative-valuation-conflicts-ev-ebitda-versus-p-e/)\n> [🔗 \"Valuation Multiples: Enterprise vs Equity, P/E, EBITDA, EBIT, Sales - Macabacus\" Macabacus](https://macabacus.com/valuation/multiples)\n> [🔗 \"r/ValueInvesting on Reddit: Confused about EV/EBIT and EV/EBITDA multiples, what do they mean?\" Reddit](https://www.reddit.com/r/ValueInvesting/comments/17e09lz/confused_about_evebit_and_evebitda_multiples_what/)\n> [🔗 \"Understanding Enterprise Multiple (EV/EBITDA): A Financial Valuation Guide\" Investopedia](https://www.investopedia.com/terms/e/ev-ebitda.asp)\n> [🔗 \"The EV/EBITDA Multiple – The Preferred Choice for Professional Investors | eFinancialModels\" eFinancialModels](https://www.efinancialmodels.com/knowledge-base/valuation/valuation-multiples/the-ev-ebitda-multiple-the-preferred-choice-for-professional-investors/)\n> [🔗 \"Valuation Multiples\" Morgan Stanley](https://www.morganstanley.com/im/publication/insights/articles/article_valuationmultiples.pdf)\n> [🔗 \"EV/EBITDA\" Corporate Finance Institute](https://corporatefinanceinstitute.com/resources/valuation/ev-ebitda/)\n> [🔗 \"P/E vs. EV/EBITDA - Advantages/Disadvantages? | Wall Street Oasis\" Wall Street Oasis](https://www.wallstreetoasis.com/forum/investment-banking/pe-vs-evebitda-advantagesdisadvantages)\n\n> 🔍 **10 results found**\n\n> [🔗 \"EBITDA: Definition, Calculation Formulas, History, and Criticisms\" Investopedia](https://www.investopedia.com/terms/e/ebitda.asp)\n> [🔗 \"Net Operating Profit After Tax (NOPAT) Definition and Formula\" Investopedia](https://www.investopedia.com/terms/n/nopat.asp)\n> [🔗 \"Relative valuation conflicts - EV/EBITDA versus P/E | The Footnotes Analyst\" Footnotesanalyst](https://www.footnotesanalyst.com/relative-valuation-conflicts-ev-ebitda-versus-p-e/)\n> [🔗 \"What is NOPAT? How Does it Different from EBITDA? | SoFi\" SoFi](https://www.sofi.com/learn/content/nopat-vs-ebitda/)\n> [🔗 \"What Is Considered a Healthy EV/EBITDA?\" Investopedia](https://www.investopedia.com/ask/answers/072715/what-considered-healthy-evebitda.asp)\n> [🔗 \"Enterprise Value: Enterprise Value: A NOPAT Perspective - FasterCapital\" FasterCapital](https://www.fastercapital.com/content/Enterprise-Value--Enterprise-Value--A-NOPAT-Perspective.html)\n> [🔗 \"NOPAT: Definition, Formula, and Examples\" Career Principles](https://www.careerprinciples.com/resources/nopat-definition-formula)\n> [🔗 \"EV / EBIT vs EV / EBITDA - Capex | Wall Street Oasis\" Wall Street Oasis](https://www.wallstreetoasis.com/forum/investment-banking/ev-ebit-vs-ev-ebitda-capex)\n> [🔗 \"NOPAT - Definition, Formula, Example, Template\" Corporate Finance Institute](https://corporatefinanceinstitute.com/resources/valuation/what-is-nopat/)\n> [🔗 \"EV/EBITDA\" Corporate Finance Institute](https://corporatefinanceinstitute.com/resources/valuation/ev-ebitda/)\n\n> 🔍 **Analyzing financial multiples**\n> ***- The EV/EBITDA multiple shows high sensitivity to tax rate changes, with a 10% increase when the tax rate is reduced.***\n\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 379, "Question": "### Background\n\n**Research Question.** How does the scale and structure of foreign bank presence in a domestic financial system create risks of macroprudential policy “leakage,” thereby motivating the need for international policy reciprocity?\n\n**Setting.** The analysis focuses on European Union countries, where the distinction between foreign bank subsidiaries and branches is critical for understanding regulatory boundaries. Subsidiaries are subject to host country regulation, while branches, in the absence of reciprocity, are typically subject to home country regulation.\n\n**Variables & Parameters.**\n- **Foreign Subsidiaries' Assets/GDP**: The ratio of assets held by foreign-owned, locally incorporated banking subsidiaries to the host country's Gross Domestic Product.\n- **Foreign Branches' Assets/GDP**: The ratio of assets held by branches of foreign banks to the host country's GDP.\n- **Leakage**: The phenomenon where the intended effect of a domestic credit-tightening policy is offset by an increase in lending from less-regulated entities, particularly foreign bank branches.\n\n---\n\n### Data / Model Specification\n\nThe following table provides data on the presence of foreign banks in selected EU countries.\n\n**Table 1: Foreign Bank Presence in Selected EU Countries**\n\n| Country | Share of Foreign Loans (2014) | Foreign Subsidiaries Assets/GDP (2013) | Foreign Branches Assets/GDP (2013) |\n| :--- | :--- | :--- | :--- |\n| IE | 52% | 50% and above | 30% and above |\n| ES | 10% | 0-10% | 10-20% |\n| GB | 44% | 10-25% | 30% and above |\n| FR | 24% | 10-25% | 0-10% |\n| BE | 48% | 50% and above | 30% and above |\n| LU | 80% | 50% and above | 30% and above |\n\n---\n\n### Question\n\nBased on the provided data and regulatory context, which of the following conclusions about macroprudential policy leakage are supported?", "Options": {"A": "According to the table, Spain (ES) and France (FR) face the most severe risk of leakage because their foreign loan shares are the highest among the countries listed.", "B": "Policy leakage is primarily a concern in countries with a large presence of foreign subsidiaries, as these entities are not bound by host-country regulations.", "C": "In countries like the UK (GB) and Ireland (IE), with a large presence of foreign bank branches (assets > 30% of GDP), a unilateral tightening of host-country regulation on LTVs is highly susceptible to leakage.", "D": "The primary mechanism for leakage is that foreign bank branches are typically subject to home-country, not host-country, regulation, allowing them to expand lending when host-country rules tighten for other banks."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to connect empirical data on banking structure with the core theoretical concept of policy leakage. It uses a 'premise packaging' strategy, combining a data-driven conclusion (Option A) with a statement of the underlying mechanism (Option B). Distractors are a 'Conceptual Opposite' (confusing branches with subsidiaries) and a 'Factual Error' (misreading the table) to ensure sharp discrimination.", "qid": "379", "question": "### Background\n\n**Research Question.** How does the scale and structure of foreign bank presence in a domestic financial system create risks of macroprudential policy “leakage,” thereby motivating the need for international policy reciprocity?\n\n**Setting.** The analysis focuses on European Union countries, where the distinction between foreign bank subsidiaries and branches is critical for understanding regulatory boundaries. Subsidiaries are subject to host country regulation, while branches, in the absence of reciprocity, are typically subject to home country regulation.\n\n**Variables & Parameters.**\n- **Foreign Subsidiaries' Assets/GDP**: The ratio of assets held by foreign-owned, locally incorporated banking subsidiaries to the host country's Gross Domestic Product.\n- **Foreign Branches' Assets/GDP**: The ratio of assets held by branches of foreign banks to the host country's GDP.\n- **Leakage**: The phenomenon where the intended effect of a domestic credit-tightening policy is offset by an increase in lending from less-regulated entities, particularly foreign bank branches.\n\n---\n\n### Data / Model Specification\n\nThe following table provides data on the presence of foreign banks in selected EU countries.\n\n**Table 1: Foreign Bank Presence in Selected EU Countries**\n\n| Country | Share of Foreign Loans (2014) | Foreign Subsidiaries Assets/GDP (2013) | Foreign Branches Assets/GDP (2013) |\n| :--- | :--- | :--- | :--- |\n| IE | 52% | 50% and above | 30% and above |\n| ES | 10% | 0-10% | 10-20% |\n| GB | 44% | 10-25% | 30% and above |\n| FR | 24% | 10-25% | 0-10% |\n| BE | 48% | 50% and above | 30% and above |\n| LU | 80% | 50% and above | 30% and above |\n\n---\n\n### Question\n\nBased on the provided data and regulatory context, which of the following conclusions about macroprudential policy leakage are supported?", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"macroprudential policy leakage foreign bank branches EU\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Cross-country spillovers from macroprudential regulation: Reciprocity and leakage - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0261560618302158)\n> [🔗 \"Macroprudential regulation and leakage to the shadow ...\" European Central Bank](https://www.ecb.europa.eu/pub/pdf/scpwps/ecb.wp2406~af673f115a.en.pdf)\n> [🔗 \"Does macroprudential policy leak? Evidence from shadow bank lending in EU countries - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0264999324000130)\n> [🔗 \"Does Macro‐Prudential Regulation Leak? Evidence from a UK Policy Experiment\" IDEAS/RePEc](https://ideas.repec.org/a/wly/jmoncb/v46y2014is1p181-214.html)\n> [🔗 \"Macroprudential regulation and leakage to the shadow banking sector - Publications Office of the EU\" Publications Office of the EU](https://op.europa.eu/en/publication-detail/-/publication/2d09b99e-bffb-11ea-855a-01aa75ed71a1/language-en)\n> [🔗 \"Macroprudential policy implications of foreign branches ...\" Europa](https://www.esrb.europa.eu/pub/pdf/reports/esrb.report_191213_macroprudentialpolicy_implications_of_foreign_branches.en~db9943c11b.docx.pdf)\n> [🔗 \"Does Macroprudential Policy Leak? Evidence from Non-Bank Credit Intermediation in EU Countries\" IDEAS/RePEc](https://ideas.repec.org/p/cnb/wpaper/2021-5.html)\n> [🔗 \"Macroprudential framework for banks - European Commission\" European Commission](https://finance.ec.europa.eu/capital-markets-union-and-financial-markets/financial-markets/macroprudential-policy/macroprudential-framework-banks_en)\n> [🔗 \"Macroprudential regulation - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Macroprudential_regulation)\n> [🔗 \"Macroprudential regulation and leakage to the shadow banking sector\" IDEAS/RePEc](https://ideas.repec.org/p/ecb/ecbwps/20202406.html)\n\n\nC,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 292, "Question": "### Background\n\n**Research Question.** How can the classical exponential bound on aggregate claims be refined for different classes of claim severity distributions, particularly those exhibiting opposing 'aging' characteristics?\n\n**Setting / Data-Generating Environment.** We are in the classical collective risk model. The goal is to refine the standard Lundberg exponential bound by incorporating information about the shape of the claim size distribution `F(x)`.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `\\overline{F}(x)`: The survival function of individual claim sizes.\n- `p_0, \\phi`: Parameters of the claim frequency distribution.\n- `\\kappa`: The adjustment coefficient, `\\kappa > 0`.\n- `r_F(x)`: The mean residual lifetime of `F`, `E[Y-x | Y>x]`.\n\n---\n\n### Data / Model Specification\n\nIf a claim size distribution `F(x)` is **DMRL** (Decreasing Mean Residual Lifetime), a refined bound (Theorem 6) is available:\n  \n\\overline{G}(x) \\leq \\phi^{-1}(1-p_{0}) \\{1+\\kappa\\alpha(x)\\}^{-1} \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (1))}\n \nwhere `\\alpha(x) = (\\int_{x}^{\\infty} e^{\\kappa y} \\overline{F}(y) dy) / (e^{\\kappa x} \\overline{F}(x))`.\n\n---\n\n### Question\n\nConsider a uniform claim severity distribution on `[0, x_0]`, where `\\overline{F}(x) = 1 - x/x_0` for `x \\in [0, x_0]`. Based on an analysis of this distribution, select all of the following statements that are correct.", "Options": {"A": "The improvement factor over the classical Lundberg bound, `\\{1+\\kappa\\alpha(x)\\}^{-1}`, simplifies to `\\frac{e^{\\kappa(x_0-x)} - 1}{\\kappa(x_0-x)}`.", "B": "The mean residual lifetime function is `r_F(x) = (x_0 - x) / 2`, which confirms that the uniform distribution is DMRL.", "C": "The mean residual lifetime function is `r_F(x) = (x_0 - x)^2 / (2x_0)`, which confirms that the uniform distribution is DMRL.", "D": "The improvement factor over the classical Lundberg bound, `\\{1+\\kappa\\alpha(x)\\}^{-1}`, simplifies to `\\frac{\\kappa(x_0-x)}{e^{\\kappa(x_0-x)} - 1}`."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the ability to apply the paper's theoretical results to a concrete example, using a Computational Judgment strategy. It requires performing the multi-step calculation from the original QA problem. Correct options (A, B) represent the correct intermediate result for the mean residual lifetime and the correct final result for the improvement factor. Distractor (C) presents an incorrect formula for `r_F(x)` based on a common calculation error (forgetting to divide by `\\overline{F}(x)`), a Step-Omission Error. Distractor (D) presents the reciprocal of the correct improvement factor, a common algebraic slip (Formula Misuse).", "qid": "292", "question": "### Background\n\n**Research Question.** How can the classical exponential bound on aggregate claims be refined for different classes of claim severity distributions, particularly those exhibiting opposing 'aging' characteristics?\n\n**Setting / Data-Generating Environment.** We are in the classical collective risk model. The goal is to refine the standard Lundberg exponential bound by incorporating information about the shape of the claim size distribution `F(x)`.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `\\overline{F}(x)`: The survival function of individual claim sizes.\n- `p_0, \\phi`: Parameters of the claim frequency distribution.\n- `\\kappa`: The adjustment coefficient, `\\kappa > 0`.\n- `r_F(x)`: The mean residual lifetime of `F`, `E[Y-x | Y>x]`.\n\n---\n\n### Data / Model Specification\n\nIf a claim size distribution `F(x)` is **DMRL** (Decreasing Mean Residual Lifetime), a refined bound (Theorem 6) is available:\n  \n\\overline{G}(x) \\leq \\phi^{-1}(1-p_{0}) \\{1+\\kappa\\alpha(x)\\}^{-1} \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (1))}\n \nwhere `\\alpha(x) = (\\int_{x}^{\\infty} e^{\\kappa y} \\overline{F}(y) dy) / (e^{\\kappa x} \\overline{F}(x))`.\n\n---\n\n### Question\n\nConsider a uniform claim severity distribution on `[0, x_0]`, where `\\overline{F}(x) = 1 - x/x_0` for `x \\in [0, x_0]`. Based on an analysis of this distribution, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 343, "Question": "### Background\n\n**Research Question.** This case investigates the portfolio implications of stock return predictability from moving averages and the likely impact of arbitrage activity on such signals.\n\n**Setting and Sample.** A mean-variance investor uses a predictive regression to form expectations and allocate capital between the S&P 500 and a risk-free asset.\n\n### Data / Model Specification\n\nThe investor uses the `MADP(3)` predictive regression to form expectations about the next period's excess return, `E_t[r_{t+1}]`:\n  \nE_t[r_{t+1}] = \\hat{\\alpha} + \\hat{\\beta} \\cdot MADP_t(3) \\quad \\text{(Eq. 1)}\n \nThe optimal weight, `w_t^*`, allocated to the S&P 500 for an investor with risk aversion `γ` and a constant conditional return variance `σ^2` is given by:\n  \nw_t^* = \\frac{E_t[r_{t+1}]}{\\gamma \\sigma^2} \\quad \\text{(Eq. 2)}\n \nEmpirical results show that the `MADP(3)` predictor had significant out-of-sample predictive power, measured by `R_{os}^2 > 0`.\n\n### Question\n\nConsidering the investor's portfolio choice problem and the principles of market efficiency, select all of the following statements that are true.", "Options": {"A": "If other large funds begin to aggressively trade on this signal, the predictive power of the model, as measured by out-of-sample `R_{os}^2`, is expected to decline over time.", "B": "The optimal weight `w_t^*` is inversely proportional to the estimated intercept `\\hat{\\alpha}` from the predictive regression.", "C": "The optimal weight `w_t^*` is a positive linear function of the predictor's current value, `MADP_t(3)`.", "D": "The optimal weight `w_t^*` is directly proportional to the investor's risk aversion `γ`."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a **Scenario Application** strategy. It tests the application of portfolio choice theory (deriving properties of the optimal weight) and the economic concept of limits to arbitrage. Option A requires substituting Eq. 1 into Eq. 2 and interpreting the result. Option B is a conceptual question about market efficiency. Distractors target common errors in interpreting the portfolio weight formula: C incorrectly states the role of `γ` (Conceptual Opposite), and D incorrectly states the role of `\\hat{\\alpha}` (Formula Misuse).", "qid": "343", "question": "### Background\n\n**Research Question.** This case investigates the portfolio implications of stock return predictability from moving averages and the likely impact of arbitrage activity on such signals.\n\n**Setting and Sample.** A mean-variance investor uses a predictive regression to form expectations and allocate capital between the S&P 500 and a risk-free asset.\n\n### Data / Model Specification\n\nThe investor uses the `MADP(3)` predictive regression to form expectations about the next period's excess return, `E_t[r_{t+1}]`:\n  \nE_t[r_{t+1}] = \\hat{\\alpha} + \\hat{\\beta} \\cdot MADP_t(3) \\quad \\text{(Eq. 1)}\n \nThe optimal weight, `w_t^*`, allocated to the S&P 500 for an investor with risk aversion `γ` and a constant conditional return variance `σ^2` is given by:\n  \nw_t^* = \\frac{E_t[r_{t+1}]}{\\gamma \\sigma^2} \\quad \\text{(Eq. 2)}\n \nEmpirical results show that the `MADP(3)` predictor had significant out-of-sample predictive power, measured by `R_{os}^2 > 0`.\n\n### Question\n\nConsidering the investor's portfolio choice problem and the principles of market efficiency, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 363, "Question": "### Background\n\nThis case examines the effectiveness of traditional corporate governance mechanisms and proposes an evolution in the relationship between corporate management and large institutional investors. The author argues that conventional mechanisms are often reactive and ill-suited to the modern reality of concentrated ownership, necessitating a shift from a 'control' orientation to a more collaborative 'value-added' orientation.\n\n### Data / Model Specification\n\nThe analysis contrasts two models for the investor-management relationship:\n\n| Feature             | Control Orientation                                | Value-Added Orientation                                      |\n| :------------------ | :------------------------------------------------- | :----------------------------------------------------------- |\n| **Core Assumption** | Adversarial; investors must control managers.      | Collaborative; based on mutual dependence.                   |\n| **Primary Goal**    | Discipline managers; react to poor performance.    | Create shared understanding; prevent crises.                 |\n| **Key Tools**       | Incentive contracts, board pressure, hostile threats. | Continuous dialogue, information sharing, strategic counsel. |\n| **Information Flow**| Limited, backward-looking, constrained by legal rules. | Rich, forward-looking, focused on strategic assumptions.     |\n| **Focus**           | Crisis management.                                 | Crisis prevention.                                           |\n\nThis shift is motivated by two empirical realities: (1) External interventions like hostile takeovers are 'lagging indicators' of managerial failure, occurring only after significant value is destroyed. (2) The rise of large, concentrated institutional investors, who cannot easily sell their illiquid stakes, has created a 'mutual dependence' between managers and major shareholders, making the old adversarial model untenable.\n\nTo formalize this strategic shift, consider a one-shot game between a manager (M) and a large institutional investor (I). The manager first decides whether to be 'Transparent' (T) or 'Opaque' (O). The investor then decides whether to be 'Supportive' (S) or 'Adversarial' (A). The payoffs (Manager, Investor) are given in Table 1.\n\n**Table 1: Payoff Matrix**\n\n|         | Investor: Supportive (S) | Investor: Adversarial (A) |\n| :------ | :---------------------- | :------------------------ |\n| **M: T**| (3, 3)                  | (0, 2)                    |\n| **M: O**| (2, 0)                  | (1, 1)                    |\n\n### Question\n\nBased on the one-shot game described in Table 1, select ALL of the following statements that are correct.", "Options": {"A": "The outcome (Opaque, Adversarial) is a pure-strategy Nash Equilibrium.", "B": "The 'Adversarial' strategy is a dominant strategy for the Investor.", "C": "The outcome (Transparent, Supportive) is a pure-strategy Nash Equilibrium.", "D": "The outcome (Opaque, Supportive) is a pure-strategy Nash Equilibrium."}, "Answer": ["A", "C"], "pi_justification": "This item replaces a QA problem (Suitability Score: 9.0) whose core analytical task was a game-theoretic calculation, making it highly suitable for conversion. It uses a 'Computational Judgment' strategy, requiring the user to derive the Nash Equilibria. The 'Atomic Decomposition' method is used to create two independently correct options (the two pure-strategy NE), ensuring a genuine multi-select format. Distractors are based on common, high-fidelity errors in game theory: C incorrectly identifies a dominant strategy, and D identifies a non-equilibrium outcome where one player has a clear incentive to deviate.", "qid": "363", "question": "### Background\n\nThis case examines the effectiveness of traditional corporate governance mechanisms and proposes an evolution in the relationship between corporate management and large institutional investors. The author argues that conventional mechanisms are often reactive and ill-suited to the modern reality of concentrated ownership, necessitating a shift from a 'control' orientation to a more collaborative 'value-added' orientation.\n\n### Data / Model Specification\n\nThe analysis contrasts two models for the investor-management relationship:\n\n| Feature             | Control Orientation                                | Value-Added Orientation                                      |\n| :------------------ | :------------------------------------------------- | :----------------------------------------------------------- |\n| **Core Assumption** | Adversarial; investors must control managers.      | Collaborative; based on mutual dependence.                   |\n| **Primary Goal**    | Discipline managers; react to poor performance.    | Create shared understanding; prevent crises.                 |\n| **Key Tools**       | Incentive contracts, board pressure, hostile threats. | Continuous dialogue, information sharing, strategic counsel. |\n| **Information Flow**| Limited, backward-looking, constrained by legal rules. | Rich, forward-looking, focused on strategic assumptions.     |\n| **Focus**           | Crisis management.                                 | Crisis prevention.                                           |\n\nThis shift is motivated by two empirical realities: (1) External interventions like hostile takeovers are 'lagging indicators' of managerial failure, occurring only after significant value is destroyed. (2) The rise of large, concentrated institutional investors, who cannot easily sell their illiquid stakes, has created a 'mutual dependence' between managers and major shareholders, making the old adversarial model untenable.\n\nTo formalize this strategic shift, consider a one-shot game between a manager (M) and a large institutional investor (I). The manager first decides whether to be 'Transparent' (T) or 'Opaque' (O). The investor then decides whether to be 'Supportive' (S) or 'Adversarial' (A). The payoffs (Manager, Investor) are given in Table 1.\n\n**Table 1: Payoff Matrix**\n\n|         | Investor: Supportive (S) | Investor: Adversarial (A) |\n| :------ | :---------------------- | :------------------------ |\n| **M: T**| (3, 3)                  | (0, 2)                    |\n| **M: O**| (2, 0)                  | (1, 1)                    |\n\n### Question\n\nBased on the one-shot game described in Table 1, select ALL of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 404, "Question": "### Background\n\n**Research Question.** This paper's central hypothesis is that market liquidity enhances efficiency by facilitating arbitrage. This is tested by examining if the predictability of short-horizon returns from past order flow is weaker on more liquid days. The analysis is conducted across three tick-size regimes which induced secular changes in liquidity.\n\n**Setting and Sample.** The study uses 5-minute interval data for a value-weighted portfolio of NYSE stocks from 1993-2002.\n\n### Data / Model Specification\n\n**Variables:**\n- `Return_t`: The 5-minute portfolio return.\n- `OIB$t-1`: The dollar order imbalance from the prior 5-minute interval, calculated as (buyer-initiated volume - seller-initiated volume) / total volume.\n- `ILD_t`: An \"Illiquid Day\" dummy variable. It is constructed for each tick-size regime by first linearly detrending the daily effective spread, then setting `ILD_t = 1` for days when the residual is at least one standard deviation above the mean, and 0 otherwise.\n\n**Baseline Model:** The paper first establishes baseline predictability:\n  \nReturn_t = \\alpha + \\beta \\cdot OIB\\$_{t-1} + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n**Interaction Model:** The main hypothesis is tested with an interaction term:\n  \nReturn_t = \\alpha + \\beta_1 OIB\\$_{t-1} + \\beta_2 (OIB\\$_{t-1} \\cdot ILD_t) + \\epsilon_t \\quad \\text{(Eq. 2)}\n \n\n**Table 1. Summary Statistics and Regression Results**\n\n| Panel A: Summary Statistic (Full Sample) | Value |\n| :--- | :--- |\n| Standard Deviation of `OIB$` | 0.251 |\n\n| Panel B: Baseline Regression (Eq. 1, Full Sample) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| `OIB$t-1` | 0.0259 | 31.40 |\n\n| Panel C: Interaction Regression (Eq. 2, Sixteenths Regime) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| `OIB$t-1` (β₁) | 0.0148 | 7.92 |\n| `OIB$t-1 * ILD_t` (β₂) | 0.0517 | 9.53 |\n\n### Question\n\nBased on the provided model specifications and results, which of the following statements are correct?", "Options": {"A": "In the Sixteenths Regime, the marginal effect of lagged order imbalance (`OIB$t-1`) on returns is more than four times stronger on illiquid days (`ILD_t=1`) compared to liquid days (`ILD_t=0`).", "B": "The highly significant positive coefficient on the interaction term (`β₂`) supports the hypothesis that illiquidity impedes market efficiency by allowing the price impact of order flow to persist longer.", "C": "Based on the full-sample baseline regression, a one-standard-deviation increase in `OIB$t-1` predicts a 0.0259 basis point increase in the five-minute return.", "D": "In the Sixteenths Regime, the marginal effect of lagged order imbalance on returns during an illiquid day is 0.0517."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the interpretation of an interaction model, combining Computational Judgment with conceptual understanding. It uses an Atomic Decomposition strategy. Option A requires a multi-step calculation of marginal effects. Option C tests the conceptual takeaway. Distractor B is a classic Step-Omission Error in interpreting interaction terms (using β₂ instead of β₁+β₂). Distractor D is a Numerical/Unit Error, confusing a regression coefficient with its calculated economic magnitude.", "qid": "404", "question": "### Background\n\n**Research Question.** This paper's central hypothesis is that market liquidity enhances efficiency by facilitating arbitrage. This is tested by examining if the predictability of short-horizon returns from past order flow is weaker on more liquid days. The analysis is conducted across three tick-size regimes which induced secular changes in liquidity.\n\n**Setting and Sample.** The study uses 5-minute interval data for a value-weighted portfolio of NYSE stocks from 1993-2002.\n\n### Data / Model Specification\n\n**Variables:**\n- `Return_t`: The 5-minute portfolio return.\n- `OIB$t-1`: The dollar order imbalance from the prior 5-minute interval, calculated as (buyer-initiated volume - seller-initiated volume) / total volume.\n- `ILD_t`: An \"Illiquid Day\" dummy variable. It is constructed for each tick-size regime by first linearly detrending the daily effective spread, then setting `ILD_t = 1` for days when the residual is at least one standard deviation above the mean, and 0 otherwise.\n\n**Baseline Model:** The paper first establishes baseline predictability:\n  \nReturn_t = \\alpha + \\beta \\cdot OIB\\$_{t-1} + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n**Interaction Model:** The main hypothesis is tested with an interaction term:\n  \nReturn_t = \\alpha + \\beta_1 OIB\\$_{t-1} + \\beta_2 (OIB\\$_{t-1} \\cdot ILD_t) + \\epsilon_t \\quad \\text{(Eq. 2)}\n \n\n**Table 1. Summary Statistics and Regression Results**\n\n| Panel A: Summary Statistic (Full Sample) | Value |\n| :--- | :--- |\n| Standard Deviation of `OIB$` | 0.251 |\n\n| Panel B: Baseline Regression (Eq. 1, Full Sample) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| `OIB$t-1` | 0.0259 | 31.40 |\n\n| Panel C: Interaction Regression (Eq. 2, Sixteenths Regime) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| `OIB$t-1` (β₁) | 0.0148 | 7.92 |\n| `OIB$t-1 * ILD_t` (β₂) | 0.0517 | 9.53 |\n\n### Question\n\nBased on the provided model specifications and results, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": ["B"], "error": null}}
{"ID": 299, "Question": "### Background\n\n**Research Question.** How can one construct a simple, empirically tractable measure of stock illiquidity that captures the price impact of trades, and how should this measure be adjusted to be comparable over long time periods characterized by market-wide price inflation?\n\n**Setting / Data-Generating Environment.** The measures are constructed using daily firm-level stock data and aggregate market capitalization data.\n\n---\n\n### Data / Model Specification\n\nThe raw Amihud illiquidity measure is:\n  \n\\mathsf{Amihud}_{i,t}^{RAW} = 10^{6} \\times \\frac{1}{D_{i,t}} \\sum_{d=1}^{D_{i,t}} \\frac{|R_{i,t,d}|}{DVOL_{i,t,d}} \\quad \\text{(Eq. (1))}\n \nwhere `R` is stock return and `DVOL` is dollar volume.\n\nThe price-inflation adjusted measure is:\n  \nAmihud_{i,t}^{AP} = \\min\\Big(0.25 + 0.30 \\times \\mathsf{Amihud}_{i,t}^{RAW} \\times P_{t-1}^{M}, 71.9\\Big) \\quad \\text{(Eq. (2))}\n \nwhere `P_{t-1}^{M}` is a market-wide price index.\n\n---\n\nBased on the definitions in Eq. (1) and Eq. (2), which of the following statements about the Amihud illiquidity measures are correct? (Select all that apply)", "Options": {"A": "The term `P_{t-1}^{M}` in Eq. (2) is designed to adjust for changes in the number of shares traded, not the price level.", "B": "A higher value of `Amihud_{i,t}^{RAW}` indicates lower stock liquidity, as it implies a larger price movement per dollar traded.", "C": "The ratio `|R_{i,t,d}| / DVOL_{i,t,d}` measures the dollar volume required to move the price by 1%.", "D": "A key reason for the adjustment in Eq. (2) is that sustained market-wide price appreciation would mechanically cause `Amihud_{i,t}^{RAW}` to trend downwards, creating a spurious appearance of increasing liquidity over time."}, "Answer": ["B", "D"], "pi_justification": "This item assesses a comprehensive understanding of the Amihud illiquidity measure, covering its basic intuition, a key time-series bias, and the rationale for its correction. Using a Multiple Choice format allows for efficient testing of multiple related concepts. Distractor C offers an incorrect rationale for the price-inflation adjustment, while Distractor D presents a conceptually inverted definition of the measure's core component.", "qid": "299", "question": "### Background\n\n**Research Question.** How can one construct a simple, empirically tractable measure of stock illiquidity that captures the price impact of trades, and how should this measure be adjusted to be comparable over long time periods characterized by market-wide price inflation?\n\n**Setting / Data-Generating Environment.** The measures are constructed using daily firm-level stock data and aggregate market capitalization data.\n\n---\n\n### Data / Model Specification\n\nThe raw Amihud illiquidity measure is:\n  \n\\mathsf{Amihud}_{i,t}^{RAW} = 10^{6} \\times \\frac{1}{D_{i,t}} \\sum_{d=1}^{D_{i,t}} \\frac{|R_{i,t,d}|}{DVOL_{i,t,d}} \\quad \\text{(Eq. (1))}\n \nwhere `R` is stock return and `DVOL` is dollar volume.\n\nThe price-inflation adjusted measure is:\n  \nAmihud_{i,t}^{AP} = \\min\\Big(0.25 + 0.30 \\times \\mathsf{Amihud}_{i,t}^{RAW} \\times P_{t-1}^{M}, 71.9\\Big) \\quad \\text{(Eq. (2))}\n \nwhere `P_{t-1}^{M}` is a market-wide price index.\n\n---\n\nBased on the definitions in Eq. (1) and Eq. (2), which of the following statements about the Amihud illiquidity measures are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"Amihud illiquidity measure adjustment for price inflation Eq. (2) min(0.25 + 0.30 * Amihud_RAW * P_{t-1}^M, 71.9)\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Why is the Amihud (2002) Illiquidity Measure Priced?\" ICMA](https://www.icmagroup.org/assets/documents/Regulatory/Secondary-markets/Bond-Market-Liquidity-Library/Lou-X_Shu-T---2016---Price-Impact-or-Trading-Volume-Why-is-the-Amihud-2002-Illiquidity-Measure-Priced-290118.pdf)\n> [🔗 \"(PDF) An Analysis of the Amihud Illiquidity Premium\" ResearchGate](https://www.researchgate.net/publication/228315879_An_Analysis_of_the_Amihud_Illiquidity_Premium)\n> [🔗 \"Amihud Illiquidity Measure - Breaking Down Finance\" Breakingdownfinance](https://breakingdownfinance.com/finance-topics/alternative-investments/amihud-illiquidity-measure/)\n> [🔗 \"Price Impact or Trading Volume: Why Is the Amihud (2002) Measure Priced? | The Review of Financial Studies | Oxford Academic\" Oxford Academic](https://academic.oup.com/rfs/article-abstract/30/12/4481/3954040)\n> [🔗 \"Illiquidity and stock returns: cross-section and time-series effects\" University of Pennsylvania](https://www.cis.upenn.edu/~mkearns/finread/amihud.pdf)\n> [🔗 \"Analysis of the Amihud Illiquidity Premium | The Review of Asset Pricing Studies | Oxford Academic\" Oxford Academic](https://academic.oup.com/raps/article-abstract/3/1/133/1574887?redirectedFrom=fulltext&login=false)\n> [🔗 \"Which stock price component drives the Amihud illiquidity premium? - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S106294082200211X)\n> [🔗 \"Price Impact or Trading Volume: Why Is the Amihud (2002) ...\" JSTOR](https://www.jstor.org/stable/48616728)\n> [🔗 \"An extension analysis of Amihud's illiquidity premium: Evidence from the Taiwan stock market - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0927538X2400235X)\n> [🔗 \"The Night and Day of Amihud's (2002) Liquidity Measure\" Chapman University](https://www.chapman.edu/business/_files/faculty-research/2019-conference-money-finance/yashar-barardehi-paper.pdf)\n\n\nB,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 115, "Question": "### Background\n\nA credit support agency with a limited budget `B` must decide how to allocate funds between direct co-funding (`Q`) and credit guarantees (`φ`) to support productive entrepreneurs. The agency faces a population that includes both productive entrepreneurs and a mass `ρ` of unproductive 'copy-cat' entrepreneurs with riskless projects.\n\n### Data / Model Specification\n\nThe model establishes a 'pecking order' for government support based on the agency's budget `B` relative to a threshold `underline(B)`:\n\n1.  If `B < underline(B)`, the optimal policy is **no co-funding (`Q=0`)** and maximum possible guarantees.\n2.  If `B ≥ underline(B)`, the optimal policy involves **both co-funding (`Q > 0`) and guarantees (`φ > 0`)**.\n\nThe maximum incentive-compatible guarantee is `hat(φ)(Q) = 1 - A/(I-Q)`, which is decreasing in `Q`.\n\n### Question\n\nBased on the model's 'pecking order' theory, which of the following statements are valid conclusions about the optimal policy mix?\n\nSelect all that apply.", "Options": {"A": "In the high-budget regime (`B ≥ underline(B)`), as more co-funding `Q` is provided, the optimal guarantee level `φ` is reduced because the entrepreneur's increased equity makes a powerful guarantee less necessary to secure financing.", "B": "Co-funding is always the superior policy tool because it directly increases an entrepreneur's equity, which avoids the incentive-distorting effects of guarantees.", "C": "When the budget is large (`B ≥ underline(B)`), co-funding is introduced because the maximum useful level of guarantees has been reached, and any additional funds are best used to increase entrepreneurs' equity.", "D": "For a tightly constrained budget (`B < underline(B)`), guarantees are preferred because they are targeted; copy-cats with riskless projects cannot benefit from a guarantee that only pays out on failure, preventing resource leakage."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses comprehension of the paper's central policy prescription: the 'pecking order' of support. It requires understanding why guarantees are preferred with low budgets and how the policy mix shifts with high budgets. Depth Strategy: Scenario Application. The question asks the student to apply the pecking order theory to different budget scenarios (low vs. high) and identify the correct rationales. Distractor Logic: Option D is a conceptual opposite and an oversimplification. It ignores the key problem of resource leakage to copy-cats, which is the entire basis for the pecking order, making it an appealing but incorrect choice for students who miss this nuance.", "qid": "115", "question": "### Background\n\nA credit support agency with a limited budget `B` must decide how to allocate funds between direct co-funding (`Q`) and credit guarantees (`φ`) to support productive entrepreneurs. The agency faces a population that includes both productive entrepreneurs and a mass `ρ` of unproductive 'copy-cat' entrepreneurs with riskless projects.\n\n### Data / Model Specification\n\nThe model establishes a 'pecking order' for government support based on the agency's budget `B` relative to a threshold `underline(B)`:\n\n1.  If `B < underline(B)`, the optimal policy is **no co-funding (`Q=0`)** and maximum possible guarantees.\n2.  If `B ≥ underline(B)`, the optimal policy involves **both co-funding (`Q > 0`) and guarantees (`φ > 0`)**.\n\nThe maximum incentive-compatible guarantee is `hat(φ)(Q) = 1 - A/(I-Q)`, which is decreasing in `Q`.\n\n### Question\n\nBased on the model's 'pecking order' theory, which of the following statements are valid conclusions about the optimal policy mix?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 387, "Question": "### Background\n\n**Research Question.** How do flawed internal performance metrics and incentive schemes lead to systematic capital misallocation, creating vulnerabilities that activists can exploit through strategic proposals like spin-offs or redesigned compensation contracts?\n\n**Setting.** A multi-division corporation where capital allocation decisions are decentralized. Divisional managers are compensated based on their division's performance, but the chosen metric may not align with overall shareholder value creation. The firm operates an internal capital market to allocate a fixed capital budget between its divisions.\n\n**Variables & Parameters.**\n*   `ROIC`: Return on Invested Capital (Net Operating Profit After Tax / Invested Capital).\n*   `WACC`: Weighted Average Cost of Capital, the firm's hurdle rate.\n*   `EVA`: Economic Value Added, defined as `(ROIC - WACC) × Invested Capital`.\n\n---\n\n### Data / Model Specification\n\nThe paper argues that operational and strategic vulnerabilities often stem from flawed internal processes. For example, a focus on maximizing average Return on Invested Capital (ROIC) can lead to suboptimal investment decisions. The text provides a specific case:\n\n*   A company has a corporate `WACC` of 10%.\n*   It has two divisions, A and B, with current performance:\n    *   Division A: `ROIC_A` = 30%\n    *   Division B: `ROIC_B` = 6%\n*   Two new, independent projects are available:\n    *   Project A (for Division A): offers `ROIC_new_A` = 20%\n    *   Project B (for Division B): offers `ROIC_new_B` = 8%\n\n---\n\n### Question\n\nBased on the scenario provided, select all statements that are TRUE regarding the investment decisions and outcomes under different incentive schemes.", "Options": {"A": "If incentives are switched to maximizing Economic Value Added (EVA), the manager of Division B will accept the new project offering an 8% ROIC.", "B": "Under an ROIC-maximization scheme, the manager of the low-return Division B (6% ROIC) will reject the new project offering an 8% ROIC.", "C": "If incentives are switched to maximizing Economic Value Added (EVA), both divisions will make investment decisions that align with shareholder value creation.", "D": "Under an ROIC-maximization scheme, the manager of the high-return Division A (30% ROIC) will reject the new project offering a 20% ROIC."}, "Answer": ["C", "D"], "pi_justification": "This item assesses the ability to diagnose capital allocation distortions caused by flawed performance metrics (ROIC) and to evaluate the corrective properties of a superior metric (EVA). It uses a Computational Judgment strategy, requiring the user to apply two different decision rules to a concrete scenario from the paper. The correct options (A, D) are derived by correctly applying both rules. Distractor B incorrectly applies the ROIC rule, while distractor C incorrectly applies the EVA rule, targeting common conceptual errors.", "qid": "387", "question": "### Background\n\n**Research Question.** How do flawed internal performance metrics and incentive schemes lead to systematic capital misallocation, creating vulnerabilities that activists can exploit through strategic proposals like spin-offs or redesigned compensation contracts?\n\n**Setting.** A multi-division corporation where capital allocation decisions are decentralized. Divisional managers are compensated based on their division's performance, but the chosen metric may not align with overall shareholder value creation. The firm operates an internal capital market to allocate a fixed capital budget between its divisions.\n\n**Variables & Parameters.**\n*   `ROIC`: Return on Invested Capital (Net Operating Profit After Tax / Invested Capital).\n*   `WACC`: Weighted Average Cost of Capital, the firm's hurdle rate.\n*   `EVA`: Economic Value Added, defined as `(ROIC - WACC) × Invested Capital`.\n\n---\n\n### Data / Model Specification\n\nThe paper argues that operational and strategic vulnerabilities often stem from flawed internal processes. For example, a focus on maximizing average Return on Invested Capital (ROIC) can lead to suboptimal investment decisions. The text provides a specific case:\n\n*   A company has a corporate `WACC` of 10%.\n*   It has two divisions, A and B, with current performance:\n    *   Division A: `ROIC_A` = 30%\n    *   Division B: `ROIC_B` = 6%\n*   Two new, independent projects are available:\n    *   Project A (for Division A): offers `ROIC_new_A` = 20%\n    *   Project B (for Division B): offers `ROIC_new_B` = 8%\n\n---\n\n### Question\n\nBased on the scenario provided, select all statements that are TRUE regarding the investment decisions and outcomes under different incentive schemes.", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 420, "Question": "### Background\n\n**Research Question.** This case examines whether the documented intertemporal stock-to-bond volatility (ISBV) relation is an episodic phenomenon driven by a Flight-to-Quality (FTQ) mechanism, which predicts the relationship should be strongest during periods of high market stress.\n\n**Setting and Sample.** The analysis uses a predictive regression for bond volatility estimated separately on subperiods classified as 'High Stress' (around NBER recessions) and 'Low Stress' (sustained low VIX). A separate sorting analysis characterizes the market environment following months of extremely high stock volatility.\n\n### Data / Model Specification\n\n**Test 1: State-Dependent Regressions**\nThe model below is estimated on four separate subperiods:\n\n  \n\\sigma_{t,t+21}^{TB} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TB} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\varepsilon_{t}\n \n\nwhere `\\sigma^{TB}` is the log realized volatility of 30-year T-bond futures and `\\sigma^{ST}` is the log realized volatility of S&P 500 futures.\n\n**Table 1. Subperiod Regression Results for 30-year T-bond Futures Volatility**\n\n| Subperiod | Stress Level | `\\hat{\\gamma}_{1}` (t-stat) | `\\hat{\\gamma}_{2}` (t-stat) | `R^2` |\n| :--- | :--- | :--- | :--- | :--- |\n| I. 1993-1996 | Low | 0.447 (4.11) | 0.024 (0.33) | 21.7% |\n| II. 2001-2002 | High | 0.141 (0.82) | 0.304 (3.69) | 23.5% |\n| III. 2004-2007 | Low | 0.579 (5.79) | -0.016 (-0.14) | 36.4% |\n| IV. 2007-2010 | High | 0.325 (1.52) | 0.176 (1.94) | 40.3% |\n\n**Test 2: Sorting Analysis**\nMonths are sorted based on lagged stock volatility. The table below shows the median characteristics of the subsequent month for the top decile (highest preceding volatility).\n\n**Table 2. Market Characteristics Following Highest 10% Realized Stock Volatility**\n\n| Variable | Median Value |\n| :--- | :--- |\n| Subsequent Stock Volatility (`StFt`) | 28.13 |\n| Subsequent T-bond Volatility (`TB`) | 13.30 |\n| Subsequent Stock-Bond Correlation (`pStFt,TB`) | -0.32 |\n\n### Question\n\nBased on the evidence in Table 1 and Table 2, which of the following statements accurately characterize the Intertemporal Stock-to-Bond Volatility (ISBV) relation and the Flight-to-Quality (FTQ) dynamic? Select all that apply.", "Options": {"A": "The market environment following a high stock volatility shock is characterized by the joint occurrence of persistent stock volatility, elevated bond volatility, and a strongly negative stock-bond correlation.", "B": "The FTQ dynamic is characterized by a positive correlation between stock and bond returns as investors flee to the quality of government bonds.", "C": "The predictive power of lagged stock volatility (γ₂) for future bond volatility is statistically significant and economically large during high-stress periods, but insignificant during low-stress periods.", "D": "In high-stress periods, the bond market's own lagged volatility (γ₁) is a more powerful predictor of future bond volatility than lagged stock volatility (γ₂)."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the core finding that the ISBV relation is state-dependent. It uses an Atomic Decomposition strategy, breaking down the original QA into two key verifiable claims: one about the regression coefficients (from Q1 of the source) and one about the market environment (from Q2 of the source). Distractor C misinterprets the relative importance of predictors in Table 1 for high-stress periods. Distractor D presents a conceptual opposite for the stock-bond correlation during FTQ.", "qid": "420", "question": "### Background\n\n**Research Question.** This case examines whether the documented intertemporal stock-to-bond volatility (ISBV) relation is an episodic phenomenon driven by a Flight-to-Quality (FTQ) mechanism, which predicts the relationship should be strongest during periods of high market stress.\n\n**Setting and Sample.** The analysis uses a predictive regression for bond volatility estimated separately on subperiods classified as 'High Stress' (around NBER recessions) and 'Low Stress' (sustained low VIX). A separate sorting analysis characterizes the market environment following months of extremely high stock volatility.\n\n### Data / Model Specification\n\n**Test 1: State-Dependent Regressions**\nThe model below is estimated on four separate subperiods:\n\n  \n\\sigma_{t,t+21}^{TB} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TB} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\varepsilon_{t}\n \n\nwhere `\\sigma^{TB}` is the log realized volatility of 30-year T-bond futures and `\\sigma^{ST}` is the log realized volatility of S&P 500 futures.\n\n**Table 1. Subperiod Regression Results for 30-year T-bond Futures Volatility**\n\n| Subperiod | Stress Level | `\\hat{\\gamma}_{1}` (t-stat) | `\\hat{\\gamma}_{2}` (t-stat) | `R^2` |\n| :--- | :--- | :--- | :--- | :--- |\n| I. 1993-1996 | Low | 0.447 (4.11) | 0.024 (0.33) | 21.7% |\n| II. 2001-2002 | High | 0.141 (0.82) | 0.304 (3.69) | 23.5% |\n| III. 2004-2007 | Low | 0.579 (5.79) | -0.016 (-0.14) | 36.4% |\n| IV. 2007-2010 | High | 0.325 (1.52) | 0.176 (1.94) | 40.3% |\n\n**Test 2: Sorting Analysis**\nMonths are sorted based on lagged stock volatility. The table below shows the median characteristics of the subsequent month for the top decile (highest preceding volatility).\n\n**Table 2. Market Characteristics Following Highest 10% Realized Stock Volatility**\n\n| Variable | Median Value |\n| :--- | :--- |\n| Subsequent Stock Volatility (`StFt`) | 28.13 |\n| Subsequent T-bond Volatility (`TB`) | 13.30 |\n| Subsequent Stock-Bond Correlation (`pStFt,TB`) | -0.32 |\n\n### Question\n\nBased on the evidence in Table 1 and Table 2, which of the following statements accurately characterize the Intertemporal Stock-to-Bond Volatility (ISBV) relation and the Flight-to-Quality (FTQ) dynamic? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 375, "Question": "### Background\n\n**Research Question.** What combination of misaligned incentives and fragile financial engineering led to the high rate of LBO defaults in the late 1980s, and how did the market correct these flaws in the early 1990s?\n\n**Setting.** The case examines the structural evolution of LBOs, contrasting the aggressive, high-failure deals of 1985-1989 with the more conservative, restructured deals of the early 1990s. The analysis focuses on the role of purchase prices, capital structure, and promoter incentives as documented by Kaplan and Stein.\n\n**Propositions from Kaplan and Stein on Late-80s LBOs:**\n1.  **High Prices:** Buyout prices as multiples of cash flow rose sharply.\n2.  **Fragile Structures:** Cash flow coverage of debt obligations fell dramatically as senior banks shortened maturities while total leverage increased.\n3.  **Inflexible Debt:** Public junk bonds (including PIK/zero-coupon) replaced private subordinated debt, increasing reorganization costs.\n4.  **Misaligned Incentives:** Promoters' net investment fell due to smaller equity stakes and substantial upfront fees.\n\n**Model of Promoter's Payoff.** A deal promoter's payoff `Π` can be modeled as the sum of certain upfront fees and a share of the uncertain future equity value:\n  \n\\Pi = F(P) + \\alpha \\cdot E[\\max(0, V - (P-E))] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the provided context regarding the flaws in late-1980s LBOs, select all statements that accurately describe the structural and incentive problems that led to high failure rates.", "Options": {"A": "The primary cause of defaults was the use of public junk bonds, which were inherently riskier than private debt regardless of the deal structure.", "B": "Senior banks in the late-80s protected the firm's stability by extending maturities on their loans to match the long-dated subordinated debt.", "C": "The combination of short-maturity, amortizing senior bank debt with long-dated, non-cash-paying subordinated debt created a severe liquidity squeeze in the early years of the LBO.", "D": "A key incentive flaw was that promoters' large, certain upfront fees could outweigh their share of potential losses from overpaying for a deal, especially when their equity stake was small."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses comprehension of the core conceptual arguments about incentive misalignment and capital structure fragility. It uses a 'Premise/assumption packaging' strategy to test the main conclusions from the original QA. Distractor C is a 'Conceptual Opposite' of the facts presented in the paper. Distractor D is an 'Almost Right' oversimplification that misses the crucial point about the interaction between different debt tranches.", "qid": "375", "question": "### Background\n\n**Research Question.** What combination of misaligned incentives and fragile financial engineering led to the high rate of LBO defaults in the late 1980s, and how did the market correct these flaws in the early 1990s?\n\n**Setting.** The case examines the structural evolution of LBOs, contrasting the aggressive, high-failure deals of 1985-1989 with the more conservative, restructured deals of the early 1990s. The analysis focuses on the role of purchase prices, capital structure, and promoter incentives as documented by Kaplan and Stein.\n\n**Propositions from Kaplan and Stein on Late-80s LBOs:**\n1.  **High Prices:** Buyout prices as multiples of cash flow rose sharply.\n2.  **Fragile Structures:** Cash flow coverage of debt obligations fell dramatically as senior banks shortened maturities while total leverage increased.\n3.  **Inflexible Debt:** Public junk bonds (including PIK/zero-coupon) replaced private subordinated debt, increasing reorganization costs.\n4.  **Misaligned Incentives:** Promoters' net investment fell due to smaller equity stakes and substantial upfront fees.\n\n**Model of Promoter's Payoff.** A deal promoter's payoff `Π` can be modeled as the sum of certain upfront fees and a share of the uncertain future equity value:\n  \n\\Pi = F(P) + \\alpha \\cdot E[\\max(0, V - (P-E))] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the provided context regarding the flaws in late-1980s LBOs, select all statements that accurately describe the structural and incentive problems that led to high failure rates.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 283, "Question": "### Background\n\nThe paper argues that traditional, decentralized capital budgeting systems fail because they ignore system-wide effects like positive externalities and synergies, which are central to the value of organizational capabilities. A division maximizing its own profit will underinvest from the firm's perspective.\n\n### Data / Model Specification\n\nA firm has two divisions, A and B. The total value created by their investment decisions is:\n\n  \nV_{Firm} = (\\pi_A - I_A)1_A + (\\pi_B - I_B)1_B + E_{BA}1_B + E_{AB}1_A + S \\cdot 1_A \\cdot 1_B\n \n\nwhere `I` is investment cost, `π` is standalone profit, `E` represents a positive externality (e.g., `E_AB` is the benefit to B from A's investment), `S` is a synergy from joint investment, and `1` is an indicator for investing.\n\nConsider the following numerical scenario:\n- Costs: `I_A = 20`, `I_B = 15`\n- Standalone Profits: `π_A = 18`, `π_B = 12`\n- Externalities: `E_AB = 4` (from A to B), `E_BA = 5` (from B to A)\n- Synergy: `S = 6`\n\n### Question\n\nBased on the provided model and numerical scenario, select all of the following statements that are correct.", "Options": {"A": "If Division A invests but Division B does not, the total value created for the firm is 2.", "B": "The decentralized decision-making process fails to account for a total of 15 in value from externalities and synergies when both projects are undertaken.", "C": "From a centralized (firm-wide) perspective, the optimal decision is for both divisions to invest, yielding a total firm value of 10.", "D": "If both divisions act independently to maximize their own standalone profit (`π - I`), both will choose not to invest."}, "Answer": ["B", "C", "D"], "pi_justification": "This item uses the 'Computational Judgment' strategy to test understanding of why decentralized capital budgeting fails. It requires students to perform calculations from both a decentralized and a centralized perspective and compare the outcomes. Option A is correct: A's standalone NPV is 18-20=-2; B's is 12-15=-3. Both reject. Option B is correct: The firm-wide value of investing in both is (-2) + (-3) + 4 + 5 + 6 = 10, which is the maximum possible value. Option D is correct: The ignored value is E_AB + E_BA + S = 4 + 5 + 6 = 15. Distractor C is a 'Step-Omission Error'; the value of investing in A only is (π_A - I_A) + E_AB = (18-20) + 4 = 2. Wait, this makes C correct. Let me re-evaluate. A's NPV is -2, B's is -3. Both reject. Correct. Centralized value for both is (-2)+(-3)+4+5+6 = 10. Correct. Ignored value is 4+5+6=15. Correct. Value for A only is (18-20) + E_AB = -2 + 4 = 2. Correct. This means A, B, C, and D are all correct. This is not a good design. I will redesign the item to have fewer correct answers and better distractors. Let's change the numbers. New scenario: I_A=20, π_A=18, I_B=15, π_B=16, E_AB=3, E_BA=5, S=6. A's NPV = -2 (Reject). B's NPV = +1 (Invest). Decentralized outcome is (Reject, Invest). Firm value = (16-15) + E_BA = 1+5=6. Centralized: A only = (18-20)+E_AB = -2+3=1. B only = (16-15)+E_BA = 1+5=6. Both = (-2)+(1)+3+5+6 = 13. Optimal is Both. This is a better scenario. Let's re-write the options based on this. A: B invests, A rejects. Correct. B: Optimal is both, value 13. Correct. C: Value of A only is 1. Correct. D: Ignored value is 3+5+6=14. Correct. Still too many correct answers. I will revert to the original numbers and use the Inversion Protocol. The single best outcome is investing in both. I will ask what is NOT a valid conclusion. The original correct answers A, B, D will become distractors. C will become a distractor. I need to create new options that are incorrect. Let's try again from scratch with the original numbers. A: Both reject. Correct. B: Optimal is both, value 10. Correct. C: Value of A only is (18-20)+E_AB = 2. Correct. D: Ignored value is 15. Correct. This is a flawed question. I will create a new one. Let's focus on the subsidies part of the original QA. This is more complex and allows for better distractors. I will scrap the current item and build a new one based on QA2, part 3. \n\n**REVISED ITEM:**\n\n json\n{\n  \"question_type\": \"Multiple Response\",\n  \"question\": \"### Background\\n\\nThe paper argues that traditional, decentralized capital budgeting systems fail because they ignore system-wide effects. This coordination failure can sometimes be solved if a central planner (HQ) provides subsidies to align divisional incentives with firm-wide goals.\\n\\n### Data / Model Specification\\n\\nA firm has two divisions, A and B. Consider the following numerical scenario:\\n- Costs: `I_A = 12`, `I_B = 11`\\n- Standalone Profits: `π_A = 10`, `π_B = 8`\\n- Externalities: `E_AB = 3` (from A to B), `E_BA = 4` (from B to A)\\n- Synergy: `S = 5`\\n\\nIn this scenario, decentralized divisions will not invest, but the firm-optimal decision is for both to invest, creating a total value of 7. HQ wants to design minimum subsidies `(T_A, T_B)` to make (Invest, Invest) a Nash Equilibrium, meaning each division finds it profitable to invest *given the other is investing*. A division's total payoff is its standalone profit, plus any incoming externality, plus any subsidy.\\n\\n### Question\\n\\nBased on the provided data, select all of the following statements that are correct regarding the design of this subsidy scheme.\",\n  \"options\": [\n    {\n      \"value\": \"A\",\n      \"text\": \"To incentivize Division A to invest (assuming B invests), HQ must provide a subsidy `T_A` that is greater than 2.\"\n    },\n    {\n      \"value\": \"B\",\n      \"text\": \"Division A is willing to invest without a subsidy if it knows Division B is investing.\"\n    },\n    {\n      \"value\": \"C\",\n      \"text\": \"To incentivize Division B to invest (assuming A invests), HQ must provide a subsidy `T_B` that is greater than 0.\"\n    },\n    {\n      \"value\": \"D\",\n      \"text\": \"The primary risk of this scheme is that divisions may overstate their investment costs (`I_A`, `I_B`) to secure larger subsidies from HQ.\"\n    }\n  ],\n  \"answer\": [\n    \"B\",\n    \"C\"\n  ],\n  \"pi_justification\": \"This item uses a 'Computational Judgment' strategy to assess a nuanced concept from the paper: using internal mechanisms to solve coordination failures. It requires students to calculate the conditions for a Nash Equilibrium in a subsidized game. Option B is correct: A's payoff if B invests is `π_A + E_BA = 10 + 4 = 14`, which is greater than its cost `I_A = 12`. No subsidy is needed. Option C is correct: B's payoff if A invests is `π_B + E_AB = 8 + 3 = 11`, which is exactly equal to its cost `I_B = 11`. Any infinitesimally positive subsidy makes investing the strictly better option. Distractor A is a 'Sign Error', as no subsidy is needed for A. Distractor D is a plausible but incorrect 'Conceptual Distractor'; the paper and standard mechanism design theory suggest the key risk is understating standalone payoffs (`π`), not overstating costs, to justify the need for a subsidy.\"\n}\n ", "qid": "283", "question": "### Background\n\nThe paper argues that traditional, decentralized capital budgeting systems fail because they ignore system-wide effects like positive externalities and synergies, which are central to the value of organizational capabilities. A division maximizing its own profit will underinvest from the firm's perspective.\n\n### Data / Model Specification\n\nA firm has two divisions, A and B. The total value created by their investment decisions is:\n\n  \nV_{Firm} = (\\pi_A - I_A)1_A + (\\pi_B - I_B)1_B + E_{BA}1_B + E_{AB}1_A + S \\cdot 1_A \\cdot 1_B\n \n\nwhere `I` is investment cost, `π` is standalone profit, `E` represents a positive externality (e.g., `E_AB` is the benefit to B from A's investment), `S` is a synergy from joint investment, and `1` is an indicator for investing.\n\nConsider the following numerical scenario:\n- Costs: `I_A = 20`, `I_B = 15`\n- Standalone Profits: `π_A = 18`, `π_B = 12`\n- Externalities: `E_AB = 4` (from A to B), `E_BA = 5` (from B to A)\n- Synergy: `S = 6`\n\n### Question\n\nBased on the provided model and numerical scenario, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 412, "Question": "### Background\n\n**Research Question.** How does model uncertainty affect an agent's decision when a potential arbitrage opportunity exists under one model but not another?\n\n**Setting and Environment.** Consider a one-period market where the no-arbitrage condition is violated. There exists a portfolio `h^*` such that `h^*\\Delta S \\ge 0` `\\mathcal{P}`-q.s. The set of models `\\mathcal{P}` contains two specific measures:\n- `P_1`: An 'optimistic' model where `E_{P_1}[h^*\\Delta S] > 0`.\n- `P_2`: A 'pessimistic' model where `E_{P_2}[h^*\\Delta S] < 0`.\n\nAn agent with utility `U(w) = \\log(w)` and initial wealth `x` considers portfolios of the form `h = \\alpha h^*` for `\\alpha \\ge 0`.\n\n---\n\n### Data / Model Specification\n\nThe agent's problem is to choose `\\alpha` to solve the robust optimization problem:\n  \n\\sup_{\\alpha \\ge 0} \\inf_{P \\in \\{P_1, P_2\\}} E_P[\\log(x+\\alpha h^*\\Delta S)]\n \n\n---\n\n### Question\n\nBased on this robust optimization framework, select all correct conclusions about the agent's behavior and the properties of the problem.\n", "Options": {"A": "Under model `P_2`, the agent's expected utility `E_{P_2}[\\log(x+\\alpha h^*\\Delta S)]` is a decreasing function of `\\alpha` for `\\alpha > 0`.", "B": "If the agent only considered model `P_1`, their optimal strategy would be to choose an arbitrarily large `\\alpha` (i.e., `\\alpha \\to \\infty`).", "C": "The optimal strategy is a finite, non-zero `\\alpha^* > 0` that optimally balances the expected gain from `P_1` against the expected loss from `P_2`.", "D": "The presence of the pessimistic measure `P_2` in the robust framework leads to an optimal choice of `\\alpha^* = 0`."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests the application of the robust (max-min) objective to a scenario where the core no-arbitrage assumption is violated. It uses a 'Scenario Application' strategy. The options are designed to decompose the reasoning via 'Atomic Decomposition': analyzing each model separately (A, B), and then synthesizing them under the `inf` operator (C). Distractor D presents a common but incorrect intuition ('Almost Right') about 'balancing' outcomes, which is not how the max-min criterion works, making it a high-fidelity distractor.", "qid": "412", "question": "### Background\n\n**Research Question.** How does model uncertainty affect an agent's decision when a potential arbitrage opportunity exists under one model but not another?\n\n**Setting and Environment.** Consider a one-period market where the no-arbitrage condition is violated. There exists a portfolio `h^*` such that `h^*\\Delta S \\ge 0` `\\mathcal{P}`-q.s. The set of models `\\mathcal{P}` contains two specific measures:\n- `P_1`: An 'optimistic' model where `E_{P_1}[h^*\\Delta S] > 0`.\n- `P_2`: A 'pessimistic' model where `E_{P_2}[h^*\\Delta S] < 0`.\n\nAn agent with utility `U(w) = \\log(w)` and initial wealth `x` considers portfolios of the form `h = \\alpha h^*` for `\\alpha \\ge 0`.\n\n---\n\n### Data / Model Specification\n\nThe agent's problem is to choose `\\alpha` to solve the robust optimization problem:\n  \n\\sup_{\\alpha \\ge 0} \\inf_{P \\in \\{P_1, P_2\\}} E_P[\\log(x+\\alpha h^*\\Delta S)]\n \n\n---\n\n### Question\n\nBased on this robust optimization framework, select all correct conclusions about the agent's behavior and the properties of the problem.\n", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 311, "Question": "### Background\n\n**Research Question.** How did the composition of foreign direct investment (FDI) into Argentina after its 1991 reforms drive a fundamental shift in corporate ownership, and what were the implications for corporate governance?\n\n**Setting / Data-Generating Environment.** The analysis focuses on Argentina during its decade of intense economic liberalization (1990-1999). Sweeping reforms attracted a massive inflow of foreign capital, leading to a dramatic change in the ownership structure of the country's top corporations, which had previously been dominated by domestic families and the state. This shift from concentrated family/state ownership to control by foreign corporations and investment funds had profound effects on prevailing corporate governance practices.\n\n### Data / Model Specification\n\nThe composition of foreign investment in Argentina from 1990 to 1999 is detailed in Table 1.\n\n**Table 1: Foreign Investment in Argentina from 1990–1999 (in millions of dollars)**\n\n| Category      | 1990-1996 | 1997   | 1998   | 1999    | Total      |\n|:--------------|:----------|:-------|:-------|:--------|:-----------|\n| M&A           | 8,131.3   | 11,792.5 | 9,120.7  | 19,507.5  | 48,552.0   |\n| Privatizations| 8,763.0   | 1,111.2  | 442.5    | 5,926.7   | 16,243.3   |\n| Greenfield    | 5,552.4   | 4,143.3  | 6,263.9  | 5,194.7   | 21,154.3   |\n| Expansion     | 18,258.0  | 7,453.5  | 7,670.6  | 7,272.6   | 40,654.7   |\n| **Total**     | 40,704.7  | 24,500.5 | 23,497.6 | 37,901.5  | 126,604.3  |\n\nThis influx of capital resulted in a significant ownership transition, as exemplified by the cases in Table 2.\n\n**Table 2: Examples of Ownership Transition in Top Argentine Corporations**\n\n| Corporation     | Sector       | Pre-1991 Controlling Shareholders | Post-1991 Controlling Shareholders |\n|:----------------|:-------------|:----------------------------------|:-----------------------------------|\n| YPF             | Oil & Gas    | State                             | Repsol (Spain)                     |\n| Banco Francés   | Banking      | Domestic Families                 | Banco Bilbao Vizcaya (Spain)       |\n| Norte           | Supermarkets | Domestic                          | Promodes (Carrefour France)        |\n\n### Question\n\nBased on the provided data and context, select ALL statements that are factually correct and supported by the analysis.", "Options": {"A": "Greenfield investments, which involve building new facilities from scratch, represented over 20% of the total foreign investment.", "B": "Investment in Privatizations was the single largest category of foreign investment, indicating the primary focus was on acquiring state assets.", "C": "The combined investment in Mergers & Acquisitions (M&A) and Expansion accounted for more than two-thirds of the total foreign investment from 1990-1999.", "D": "Investment related to Privatizations constituted less than 15% of the total foreign capital inflow during the period."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform accurate calculations from tabular data and draw direct, first-order conclusions. It uses a Computational Judgment strategy. The correct options (A, C) are derived from straightforward percentage calculations (70.5% and 12.8%, respectively). Distractor B presents a plausible but incorrect interpretation, testing for careful reading against the data. Distractor D is based on a common numerical misjudgment (the correct value is 16.7%), creating a high-fidelity numerical distractor.", "qid": "311", "question": "### Background\n\n**Research Question.** How did the composition of foreign direct investment (FDI) into Argentina after its 1991 reforms drive a fundamental shift in corporate ownership, and what were the implications for corporate governance?\n\n**Setting / Data-Generating Environment.** The analysis focuses on Argentina during its decade of intense economic liberalization (1990-1999). Sweeping reforms attracted a massive inflow of foreign capital, leading to a dramatic change in the ownership structure of the country's top corporations, which had previously been dominated by domestic families and the state. This shift from concentrated family/state ownership to control by foreign corporations and investment funds had profound effects on prevailing corporate governance practices.\n\n### Data / Model Specification\n\nThe composition of foreign investment in Argentina from 1990 to 1999 is detailed in Table 1.\n\n**Table 1: Foreign Investment in Argentina from 1990–1999 (in millions of dollars)**\n\n| Category      | 1990-1996 | 1997   | 1998   | 1999    | Total      |\n|:--------------|:----------|:-------|:-------|:--------|:-----------|\n| M&A           | 8,131.3   | 11,792.5 | 9,120.7  | 19,507.5  | 48,552.0   |\n| Privatizations| 8,763.0   | 1,111.2  | 442.5    | 5,926.7   | 16,243.3   |\n| Greenfield    | 5,552.4   | 4,143.3  | 6,263.9  | 5,194.7   | 21,154.3   |\n| Expansion     | 18,258.0  | 7,453.5  | 7,670.6  | 7,272.6   | 40,654.7   |\n| **Total**     | 40,704.7  | 24,500.5 | 23,497.6 | 37,901.5  | 126,604.3  |\n\nThis influx of capital resulted in a significant ownership transition, as exemplified by the cases in Table 2.\n\n**Table 2: Examples of Ownership Transition in Top Argentine Corporations**\n\n| Corporation     | Sector       | Pre-1991 Controlling Shareholders | Post-1991 Controlling Shareholders |\n|:----------------|:-------------|:----------------------------------|:-----------------------------------|\n| YPF             | Oil & Gas    | State                             | Repsol (Spain)                     |\n| Banco Francés   | Banking      | Domestic Families                 | Banco Bilbao Vizcaya (Spain)       |\n| Norte           | Supermarkets | Domestic                          | Promodes (Carrefour France)        |\n\n### Question\n\nBased on the provided data and context, select ALL statements that are factually correct and supported by the analysis.", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: total = 126604.3\nprint(f\"Total: {total}\")\n\ngreenfi...\n\n> 🔍 **Analyzing investment data**\n> ***- Total foreign investment in Argentina from 1990-1999 was $126,604.3 million.***\n\n\nA,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 142, "Question": "### Background\n\nAn analysis of the Italian savings bank industry from 1982-1989 used a translog cost function to estimate economies of scale. The study calculated Ordinary Economies of Scale (OES), representing the elasticity of total cost with respect to output, holding the number of branches constant. A value less than 1 indicates economies of scale (i.e., a 1% increase in output leads to a less than 1% increase in cost).\n\n### Data / Model Specification\n\n**Table 1. Ordinary Economies of Scale (OES) Estimates**\n\n| Period      | OES Estimate | Standard Error |\n| :---------- | :----------- | :------------- |\n| 1982-1983   | 0.8203       | 0.0455         |\n| 1984-1985   | 0.7945       | 0.0838         |\n| 1986-1987   | 0.7735       | 0.0689         |\n| 1988-1989   | 0.9502       | 0.2742         |\n| **1982-1989** | **0.7845**   | **0.0254**     |\n\nAn alternative fixed-effects (FE) model, which controls for unobserved, time-invariant bank characteristics, produced a much lower OES estimate of **0.2712**. The paper's authors discarded this result, arguing it could be biased.\n\n---\n\nBased on the provided data and context, which of the following statements are valid interpretations or inferences?", "Options": {"A": "If unobserved managerial quality is positively correlated with output and negatively correlated with costs, then the pooled OES estimate of 0.7845 is biased upwards, making the true economies of scale appear smaller than they are.", "B": "The OES estimate for the 1988-1989 subperiod (0.9502) is not statistically different from 1 (constant returns to scale) at the 5% significance level.", "C": "The trend in OES estimates from 1982 to 1989 suggests that the economies of scale available to Italian savings banks were diminishing over the decade.", "D": "The OES estimate for the full 1982-1989 period (0.7845) implies that a 10% increase in a bank's total loans was associated with a 7.845% increase in its total costs, ceteris paribus."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret elasticity coefficients, conduct statistical inference from point estimates and standard errors, identify trends in data, and understand the direction of omitted variable bias. It combines computational judgment with reverse-reasoning about econometric bias.\nStrategy: Scenario Application. The user must apply statistical and econometric principles to the specific results presented in the table.\nDistractor Logic:\n- A (Correct): Correctly scales the elasticity interpretation from 1% to 10%.\n- B (Correct): Requires calculating a t-statistic: (0.9502 - 1) / 0.2742 ≈ -0.18. The absolute value is far less than the critical value of ~1.96, so the null of OES=1 cannot be rejected.\n- C (Correct): The OES estimate rises from ~0.82 to ~0.95, moving closer to 1. This indicates that the magnitude of scale economies (1 - OES) was shrinking.\n- D (Incorrect - Conceptual Opposite): This describes a downward bias. The bias on the OES coefficient would be `Corr(MgrQuality, Ln(Y)) * Corr(MgrQuality, Ln(TC)) = (+) * (-) = (-)`. A downward bias means the pooled estimate (0.7845) is artificially low, making economies of scale appear *larger* than they truly are. The FE estimate of 0.2712 is consistent with a large downward bias in the pooled model.", "qid": "142", "question": "### Background\n\nAn analysis of the Italian savings bank industry from 1982-1989 used a translog cost function to estimate economies of scale. The study calculated Ordinary Economies of Scale (OES), representing the elasticity of total cost with respect to output, holding the number of branches constant. A value less than 1 indicates economies of scale (i.e., a 1% increase in output leads to a less than 1% increase in cost).\n\n### Data / Model Specification\n\n**Table 1. Ordinary Economies of Scale (OES) Estimates**\n\n| Period      | OES Estimate | Standard Error |\n| :---------- | :----------- | :------------- |\n| 1982-1983   | 0.8203       | 0.0455         |\n| 1984-1985   | 0.7945       | 0.0838         |\n| 1986-1987   | 0.7735       | 0.0689         |\n| 1988-1989   | 0.9502       | 0.2742         |\n| **1982-1989** | **0.7845**   | **0.0254**     |\n\nAn alternative fixed-effects (FE) model, which controls for unobserved, time-invariant bank characteristics, produced a much lower OES estimate of **0.2712**. The paper's authors discarded this result, arguing it could be biased.\n\n---\n\nBased on the provided data and context, which of the following statements are valid interpretations or inferences?", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 296, "Question": "### Background\n\n**Research Question.** How do real-world frictions like transaction costs and differences in asset characteristics affect the viability of sophisticated trading strategies?\n\n**Setting.** The paper argues that value-weighted industry portfolios are a better testing ground for its strategy than characteristic-sorted portfolios (e.g., 25 portfolios sorted on size and book-to-market). The rationale involves two key dimensions: the inherent factor structure of the assets and their trading costs. The performance of a strategy on size/BM portfolios is used to test this argument.\n\n**Variables and Parameters.**\n- `\\alpha_{gross}`: Four-factor alpha before transaction costs.\n- `\\alpha_{net}`: Four-factor alpha after transaction costs.\n\n---\n\n### Data / Model Specification\n\nThe paper argues that characteristic-sorted portfolios have a tight factor structure, meaning most of their return variation is explained by common factors. Furthermore, it notes that portfolios with extreme characteristics (e.g., small-cap stocks) have much higher trading costs. **Table 1** below shows the performance of the long-short combo strategies applied to the 25 size/BM portfolios.\n\n**Table 1. Performance of Size/BM-Rotation Strategies (Annualized, %)**\n\n| Strategy | `\\alpha_{gross}` | `\\alpha_{net}` |\n| :--- | :--- | :--- |\n| L-S ABMA | 7.16*** | 2.62** |\n| L-S DMSFE| 7.01*** | 2.67** |\n| L-S MEAN | 5.18** | 0.04 |\n\n*Source: Table VI in the original paper. `**` and `***` denote statistical significance.*\n\n---\n\n### Question\n\nBased on the paper's arguments and the data in Table 1, which of the following statements accurately describe the challenges and performance of applying the combo strategy to characteristic-sorted (size/BM) portfolios?", "Options": {"A": "The paper argues that characteristic-sorted portfolios are less ideal for demonstrating alpha because their returns have a tight factor structure, leaving little idiosyncratic variance to exploit.", "B": "The combo strategies fail to generate statistically significant *gross* alpha when applied to size/BM portfolios, indicating the forecasts are ineffective for these assets.", "C": "The erosion of alpha by transaction costs is so severe for the MEAN strategy that a statistically significant gross alpha is reduced to a statistically insignificant net alpha.", "D": "The strategy's reliance on trading extreme portfolios (e.g., small-cap stocks) leads to high transaction costs, which substantially erode the gross alpha."}, "Answer": ["A", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the paper's rationale for choosing test assets and the impact of real-world frictions. It uses an **Atomic Decomposition** strategy to test the distinct concepts of factor structure, transaction costs, and the resulting gap between gross and net alpha. Option C is a 'Conceptual Opposite' distractor, incorrectly claiming the forecasts fail at the gross level, which tests careful reading of the table.", "qid": "296", "question": "### Background\n\n**Research Question.** How do real-world frictions like transaction costs and differences in asset characteristics affect the viability of sophisticated trading strategies?\n\n**Setting.** The paper argues that value-weighted industry portfolios are a better testing ground for its strategy than characteristic-sorted portfolios (e.g., 25 portfolios sorted on size and book-to-market). The rationale involves two key dimensions: the inherent factor structure of the assets and their trading costs. The performance of a strategy on size/BM portfolios is used to test this argument.\n\n**Variables and Parameters.**\n- `\\alpha_{gross}`: Four-factor alpha before transaction costs.\n- `\\alpha_{net}`: Four-factor alpha after transaction costs.\n\n---\n\n### Data / Model Specification\n\nThe paper argues that characteristic-sorted portfolios have a tight factor structure, meaning most of their return variation is explained by common factors. Furthermore, it notes that portfolios with extreme characteristics (e.g., small-cap stocks) have much higher trading costs. **Table 1** below shows the performance of the long-short combo strategies applied to the 25 size/BM portfolios.\n\n**Table 1. Performance of Size/BM-Rotation Strategies (Annualized, %)**\n\n| Strategy | `\\alpha_{gross}` | `\\alpha_{net}` |\n| :--- | :--- | :--- |\n| L-S ABMA | 7.16*** | 2.62** |\n| L-S DMSFE| 7.01*** | 2.67** |\n| L-S MEAN | 5.18** | 0.04 |\n\n*Source: Table VI in the original paper. `**` and `***` denote statistical significance.*\n\n---\n\n### Question\n\nBased on the paper's arguments and the data in Table 1, which of the following statements accurately describe the challenges and performance of applying the combo strategy to characteristic-sorted (size/BM) portfolios?", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"Table VI Performance of Size/BM-Rotation Strategies ABMA DMSFE MEAN alpha gross net\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"paper combo strategies value-weighted industry portfolios vs characteristic-sorted portfolios size book-to-market trading costs\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Analyzing financial data**\n> ***- The paper suggests value-weighted industry portfolios are better for testing strategies than characteristic-sorted portfolios like size/BM due to factor structure and trading costs.***\n\n> 🔍 **10 results found**\n\n> [🔗 \"6.3 Performance Measures | Portfolio Optimization\" Bookdown](https://bookdown.org/palomar/portfoliooptimizationbook/6.3-performance-measures.html)\n> [🔗 \"Momentum Asset Allocation Strategy - Quantpedia\" QuantPedia](https://quantpedia.com/strategies/asset-class-momentum-rotational-system)\n> [🔗 \"Alpha: Its Meaning in Investing, With Examples\" Investopedia](https://www.investopedia.com/terms/a/alpha.asp)\n> [🔗 \"(PDF) BM(book-to-market ratio) factor: medium-term momentum and long-term reversal\" ResearchGate](https://www.researchgate.net/publication/322711264_BMbook-to-market_ratio_factor_medium-term_momentum_and_long-term_reversal)\n> [🔗 \"(PDF) US Sector Rotation with Five-Factor Fama-French Alphas\" ResearchGate](https://www.researchgate.net/publication/321289535_US_Sector_Rotation_with_Five-Factor_Fama-French_Alphas)\n> [🔗 \"Your Complete Guide to Sector Rotation - YCharts\" YCharts](https://get.ycharts.com/resources/blog/your-complete-guide-to-sector-rotation/)\n> [🔗 \"Data for current year\" Damodaran On-line](https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datacurrent.html)\n> [🔗 \"8 Key Metrics to Measure Success for Your ABM Strategy\" Gartner](https://www.gartner.com/en/digital-markets/insights/key-metrics-measure-abm-strategy)\n> [🔗 \"The myth of business cycle sector rotation - Molchanov - 2024 - International Journal of Finance & Economics - Wiley Online Library\" Wiley Online Library](https://onlinelibrary.wiley.com/doi/full/10.1002/ijfe.2882)\n> [🔗 \"13 Key ABM Metrics Every SaaS Company Should Track [2025]\" Revvgrowth](https://www.revvgrowth.com/abm/13-key-metrics)\n\n> 🔍 **10 results found**\n\n> [🔗 \"Characteristic-Sorted Portfolios: Estimation and Inference\" Federal Reserve Bank of New York](https://www.newyorkfed.org/medialibrary/media/research/staff_reports/sr788.pdf?la=en)\n> [🔗 \"Portfolio Optimization: Simple versus Optimal Methods - ReSolve Asset Management\" Investresolve](https://investresolve.com/portfolio-optimization-simple-optimal-methods/)\n> [🔗 \"How Does Weighting Scheme Impact Systematic Equity Portfolios? - QuantPedia\" QuantPedia](https://quantpedia.com/how-does-weighting-scheme-impacts-systematic-equity-portfolios/)\n> [🔗 \"Why Do Equal-Weighted Portfolios Outperform Value-Weighted Portfolios?\" Academia.edu](https://www.academia.edu/16820265/Why_Do_Equal_Weighted_Portfolios_Outperform_Value_Weighted_Portfolios)\n> [🔗 \"Discovering optimal weights in weighted-scoring stock-picking models: a mixture design approach | Financial Innovation | Full Text\" Financial Innovation](https://jfin-swufe.springeropen.com/articles/10.1186/s40854-020-00209-x)\n> [🔗 \"Portfolio Weight: Meaning, Calculations, and Examples\" Investopedia](https://www.investopedia.com/terms/p/portfolio-weight.asp)\n> [🔗 \"(PDF) Optimal Characteristic Portfolios\" ResearchGate](https://www.researchgate.net/publication/342529936_Optimal_Characteristic_Portfolios)\n> [🔗 \"Investment portfolios: Asset allocation models | Vanguard\" Vanguard](https://investor.vanguard.com/investor-resources-education/education/model-portfolio-allocation)\n> [🔗 \"equities - Value premium analysis - Equal or Value-weighted Portfolios? - Quantitative Finance Stack Exchange\" Stack Exchange](https://quant.stackexchange.com/questions/49780/value-premium-analysis-equal-or-value-weighted-portfolios)\n> [🔗 \"(continued) Panel A: size sorted portfolios | Download Table\" ResearchGate](https://www.researchgate.net/figure/continued-Panel-A-size-sorted-portfolios_tbl3_309138994)\n\n> ***- Characteristic-sorted portfolios have tight factor structures, leaving little room for idiosyncratic variance to exploit.***\n\n> 🔍 **Searching for:** \"Table VI Performance of Size/BM-Rotation Strategies ABMA DMSFE MEAN alpha gross net\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"paper combo strategies value-weighted industry portfolios vs characteristic-sorted portfolios size book-to-market trading costs\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Found 10 search results**\n\n🔗 **[6.3 Performance Measures | Portfolio Optimization](https://bookdown.org/palomar/portfoliooptimizationbook/6.3-performance-measures.html)**\n   📄 This textbook is a comprehensive guide to a wide range of portfolio designs, bridging the gap between mathematical formulations and practical algorith...\n   🌐 *Bookdown*\n\n🔗 **[Momentum Asset Allocation Strategy - Quantpedia](https://quantpedia.com/strategies/asset-class-momentum-rotational-system)**\n   📄 There are various ways how to utilize this anomaly for the profit; a simple one is based on using momentum filter separately in each asset class and t...\n   🌐 *QuantPedia*\n\n🔗 **[Alpha: Its Meaning in Investing, With Examples](https://www.investopedia.com/terms/a/alpha.asp)**\n   📄 Active portfolio managers seek to generate alpha in diversified portfolios, with diversification intended to eliminate unsystematic risk. Because alph...\n   🌐 *Investopedia*\n\n🔗 **[(PDF) BM(book-to-market ratio) factor: medium-term momentum and long-term reversal](https://www.researchgate.net/publication/322711264_BMbook-to-market_ratio_factor_medium-term_momentum_and_long-term_reversal)**\n   📄 This study shows that this anomaly interpretative model can explain stock market perceptions and medium-term momentum. Most importantly, BM is a criti...\n   🌐 *ResearchGate*\n\n🔗 **[(PDF) US Sector Rotation with Five-Factor Fama-French Alphas](https://www.researchgate.net/publication/321289535_US_Sector_Rotation_with_Five-Factor_Fama-French_Alphas)**\n   📄 : Descriptive Statistics This table reports the descriptive statistics (Mean, Standard Deviation, Skewness and Kurtosis) of the 10 Fama- French sector...\n   🌐 *ResearchGate*\n\n🔗 **[Your Complete Guide to Sector Rotation - YCharts](https://get.ycharts.com/resources/blog/your-complete-guide-to-sector-rotation/)**\n   📄 Tie your analysis together with Model Portfolios to validate your sector rotation strategy against any benchmark. Here, you can evaluate the alpha of ...\n   🌐 *YCharts*\n\n🔗 **[Data for current year](https://pages.stern.nyu.edu/~adamodar/New_Home_Page/datacurrent.html)**\n   📄 In computing the statistics, the data used will reflect the most updated numbers I can find for each company, which at the start of each year, will re...\n   🌐 *Damodaran On-line*\n\n🔗 **[8 Key Metrics to Measure Success for Your ABM Strategy](https://www.gartner.com/en/digital-markets/insights/key-metrics-measure-abm-strategy)**\n   📄 Marketers and sales leaders face heavy pressure to justify spending and prove ROI of ABM efforts. Discover ways to establish the value of ABM campaign...\n   🌐 *Gartner*\n\n🔗 **[The myth of business cycle sector rotation - Molchanov - 2024 - International Journal of Finance & Economics - Wiley Online Library](https://onlinelibrary.wiley.com/doi/full/10.1002/ijfe.2882)**\n   📄 Conventional wisdom suggests that sectors/industries provide systematic performance and that business cycle rotation strategies generate excess market...\n   🌐 *Wiley Online Library*\n\n🔗 **[13 Key ABM Metrics Every SaaS Company Should Track [2025]](https://www.revvgrowth.com/abm/13-key-metrics)**\n   📄 Running an ABM campaign without the right ABM metrics is like training for a marathon with the wrong fitness tracker.Imagine spending months preparing...\n   🌐 *Revvgrowth*\n\n\n> 🔍 **Found 10 search results**\n\n🔗 **[Characteristic-Sorted Portfolios: Estimation and Inference](https://www.newyorkfed.org/medialibrary/media/research/staff_reports/sr788.pdf?la=en)**\n   📄 At the New York Fed, our mission is <strong>to</strong> make the U.S. economy stronger and the financial system more stable for all segments of societ...\n   🌐 *Federal Reserve Bank of New York*\n\n🔗 **[Portfolio Optimization: Simple versus Optimal Methods - ReSolve Asset Management](https://investresolve.com/portfolio-optimization-simple-optimal-methods/)**\n   📄 Portfolios formed quarterly based on trailing 252 day returns for industries, factor portfolios, and monthly for asset classes. Results are gross of t...\n   🌐 *Investresolve*\n\n🔗 **[How Does Weighting Scheme Impact Systematic Equity Portfolios? - QuantPedia](https://quantpedia.com/how-does-weighting-scheme-impacts-systematic-equity-portfolios/)**\n   📄 We re-rank the firms according to their average ranks across those three independent sorts, which makes for gradual changes in the weights across firm...\n   🌐 *QuantPedia*\n\n🔗 **[Why Do Equal-Weighted Portfolios Outperform Value-Weighted Portfolios?](https://www.academia.edu/16820265/Why_Do_Equal_Weighted_Portfolios_Outperform_Value_Weighted_Portfolios)**\n   📄 The proposed portfolios' performances were assessed from January 2009 to December 2018 and compared to the performance of BOVA11 (Exchange Traded Fund...\n   🌐 *Academia.edu*\n\n🔗 **[Discovering optimal weights in weighted-scoring stock-picking models: a mixture design approach | Financial Innovation | Full Text](https://jfin-swufe.springeropen.com/articles/10.1186/s40854-020-00209-x)**\n   📄 The last three, PBR*R*MV, ROE*R*MV, and R*beta*MV, are negative and share the common characteristics of a stock-picking concept, that is, large total ...\n   🌐 *Financial Innovation*\n\n🔗 **[Portfolio Weight: Meaning, Calculations, and Examples](https://www.investopedia.com/terms/p/portfolio-weight.asp)**\n   📄 James Chen, CMT is an expert trader, investment adviser, and global market strategist.\n   🌐 *Investopedia*\n\n🔗 **[(PDF) Optimal Characteristic Portfolios](https://www.researchgate.net/publication/342529936_Optimal_Characteristic_Portfolios)**\n   📄 RSEW :The equally-weighted rank, sorted decile portfolio based on the attribute (e.g. for · size, long stocks below the 10 percentile of market capita...\n   🌐 *ResearchGate*\n\n🔗 **[Investment portfolios: Asset allocation models | Vanguard](https://investor.vanguard.com/investor-resources-education/education/model-portfolio-allocation)**\n   📄 Diversification is one of the most fundamental strategies for building an investment portfolio focused on long-term growth. Asset allocation funds are...\n   🌐 *Vanguard*\n\n🔗 **[equities - Value premium analysis - Equal or Value-weighted Portfolios? - Quantitative Finance Stack Exchange](https://quant.stackexchange.com/questions/49780/value-premium-analysis-equal-or-value-weighted-portfolios)**\n   📄 The key word is “microcaps.” Microcaps represent only 3.2% of the aggregate market capitalization but 60.7% of the number of stocks. Microcaps have th...\n   🌐 *Stack Exchange*\n\n🔗 **[(continued) Panel A: size sorted portfolios | Download Table](https://www.researchgate.net/figure/continued-Panel-A-size-sorted-portfolios_tbl3_309138994)**\n   📄 Download Table | (continued) Panel A: size sorted portfolios from publication: Timing the Market with a Combination of Moving Averages: Combination of...\n   🌐 *ResearchGate*\n\n\n> 🔍 **Searching for:** \"\"L-S ABMA\" \"L-S DMSFE\" \"L-S MEAN\" alpha strategy paper\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"combo strategy industry portfolios size/BM portfolios transaction costs alpha\"\n> 📊 **Results requested:** 10\n\n> ***- Table 1 shows L-S ABMA, DMSFE, and MEAN strategies with significant gross alphas, but net alphas drop, especially for MEAN.***\n\n> 🔍 **Searching for:** \"\"L-S ABMA\" \"L-S DMSFE\" \"L-S MEAN\" alpha strategy paper\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Searching for:** \"combo strategy industry portfolios size/BM portfolios transaction costs alpha\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Alpha: Its Meaning in Investing, With Examples\" Investopedia](https://www.investopedia.com/terms/a/alpha.asp)\n> [🔗 \"Markowitz portfolios under transaction costs - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/pii/S1062976925000031)\n> [🔗 \"Bettering Your Portfolio With Alpha and Beta\" Investopedia](https://www.investopedia.com/articles/07/alphabeta.asp)\n> [🔗 \"Alpha - Learn How to Calculate and Use Alpha in Investing\" Corporate Finance Institute](https://corporatefinanceinstitute.com/resources/career-map/sell-side/capital-markets/alpha/)\n> [🔗 \"Portable alpha strategies – Institutional | BlackRock\" BlackRock](https://www.blackrock.com/ca/institutional/en/insights/investment-actions/portable-alpha-strategies)\n> [🔗 \"Alpha (finance) - Wikipedia\" Wikipedia](https://en.wikipedia.org/wiki/Alpha_%2528finance%2529)\n> [🔗 \"Portable alpha strategies | BlackRock\" BlackRock](https://www.blackrock.com/uk/professionals/solutions/systematic-investing/portable-alpha-strategies)\n> [🔗 \"Optimizing Risk-Adjusted Return in Constructing Portfolios of Alphas | Presto Research\" Prestolabs](https://www.prestolabs.io/research/optimizing-risk-adjusted-return-in-constructing-portfolios-of-alphas)\n> [🔗 \"How to Combine Value and Momentum Investing Strategies -\" Alpha Architect](https://alphaarchitect.com/the-best-way-to-combine-value-and-momentum-investing-strategies/)\n> [🔗 \"Understanding Portable Alpha Strategies | PIMCO\" PIMCO](https://www.pimco.com/us/en/resources/education/understanding-portable-alpha-strategies)\n\n> 🔍 **Found 10 search results**\n\n🔗 **[Alpha: Its Meaning in Investing, With Examples](https://www.investopedia.com/terms/a/alpha.asp)**\n   📄 Because beta risk can be isolated by diversifying and hedging various risks (which comes with various transaction costs), some have proposed that alph...\n   🌐 *Investopedia*\n\n🔗 **[Markowitz portfolios under transaction costs - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S1062976925000031)**\n   📄 Most academic studies either ignore transaction costs or account for them in a way that is both unrealistic and suboptimal by (i) assuming transaction...\n   🌐 *ScienceDirect*\n\n🔗 **[Bettering Your Portfolio With Alpha and Beta](https://www.investopedia.com/articles/07/alphabeta.asp)**\n   📄 The disadvantage is that one must choose a settlement date for a futures contract, and this turnover can create higher transaction costs. For an inves...\n   🌐 *Investopedia*\n\n🔗 **[Alpha - Learn How to Calculate and Use Alpha in Investing](https://corporatefinanceinstitute.com/resources/career-map/sell-side/capital-markets/alpha/)**\n   📄 The two ratios are both used in the Capital Assets Pricing Model (CAPM) to analyze a portfolio of investments and assess its theoretical performance. ...\n   🌐 *Corporate Finance Institute*\n\n🔗 **[Portable alpha strategies – Institutional | BlackRock](https://www.blackrock.com/ca/institutional/en/insights/investment-actions/portable-alpha-strategies)**\n   📄 First, the investor chooses a target index for their beta exposure. Second, the target index is replicated using market-linked instruments. This part ...\n   🌐 *BlackRock*\n\n🔗 **[Alpha (finance) - Wikipedia](https://en.wikipedia.org/wiki/Alpha_%2528finance%2529)**\n   📄 Alpha, along with beta, is one of two key coefficients in the capital asset pricing model used in modern portfolio theory and is closely related to ot...\n   🌐 *Wikipedia*\n\n🔗 **[Portable alpha strategies | BlackRock](https://www.blackrock.com/uk/professionals/solutions/systematic-investing/portable-alpha-strategies)**\n   📄 (4) a natural person resident in an EEA State that permits the authorisation of natural persons as qualified investors, who expressly asks to be treat...\n   🌐 *BlackRock*\n\n🔗 **[Optimizing Risk-Adjusted Return in Constructing Portfolios of Alphas | Presto Research](https://www.prestolabs.io/research/optimizing-risk-adjusted-return-in-constructing-portfolios-of-alphas)**\n   📄 Modern Portfolio Theory (MPT), developed by Harry Markowitz in the 1950s, is a foundational concept in investment management and financial theory. It ...\n   🌐 *Prestolabs*\n\n🔗 **[How to Combine Value and Momentum Investing Strategies -](https://alphaarchitect.com/the-best-way-to-combine-value-and-momentum-investing-strategies/)**\n   📄 Why don't you include momentum in your value investing process? This is a great question and one we will answer publicly for the first time.\n   🌐 *Alpha Architect*\n\n🔗 **[Understanding Portable Alpha Strategies | PIMCO](https://www.pimco.com/us/en/resources/education/understanding-portable-alpha-strategies)**\n   📄 Equities may decline in value due to both real and perceived general market, economic and industry conditions. Diversification does not ensure against...\n   🌐 *PIMCO*\n\n\n\nA,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 442, "Question": "### Background\n\n**Research Question.** How does an investor optimally liquidate inventory when market liquidity switches between high and low states?\n\n**Setting.** The market liquidity, parameterized by `\\lambda`, follows a two-state Markov chain: an \"active\" state (0, high `\\lambda_0`) and a \"slow\" state (1, low `\\lambda_1`), with `\\lambda_0 > \\lambda_1`. The investor observes the current state and adjusts their strategy. The analysis is in the continuous, infinite-horizon fluid limit for a power-law LOB where `\\Lambda(s) = \\lambda s^{-\\alpha}`.\n\n**Variables & Parameters.**\n- `u(x)`: Value function when in the active state (0) with inventory `x`.\n- `w(x)`: Value function when in the slow state (1) with inventory `x`.\n- `s_0^*(x), s_1^*(x)`: Optimal spreads in the active and slow states, respectively.\n\n---\n\n### Data / Model Specification\n\nThe value functions `u(x)` and `w(x)` solve a system of coupled ODEs. The solution has the properties that `u(x) > w(x)` and the marginal value is also higher in the active state, `u_x(x) > w_x(x)`. The optimal spread is proportional to the marginal value: `s^* \\propto u_x` or `s^* \\propto w_x`.\n\n---\n\n### Question\n\nBased on the model's specification and results, select all of the following statements that are correct descriptions of the optimal strategy and its underlying values.", "Options": {"A": "The marginal value of inventory is lower in the active state than in the slow state, i.e., `u_x(x) < w_x(x)`.", "B": "To take advantage of high liquidity, the investor posts a lower (more aggressive) spread in the active state.", "C": "The value function in the active state is higher than in the slow state, i.e., `u(x) > w(x)`.", "D": "The optimal spread is higher in the active state than in the slow state, i.e., `s_0^*(x) > s_1^*(x)`."}, "Answer": ["C", "D"], "pi_justification": "REPLACE with Multiple Choice (Score: 9.0). This item assesses the core results of the regime-switching model. It uses an Atomic Decomposition strategy to test understanding of the relationships between value functions, marginal values, and the paradoxical optimal spread. Distractor C is a Conceptual Opposite of a key premise. Distractor D represents the intuitive but incorrect reasoning that the paradox is designed to challenge, making it a high-fidelity misconception.", "qid": "442", "question": "### Background\n\n**Research Question.** How does an investor optimally liquidate inventory when market liquidity switches between high and low states?\n\n**Setting.** The market liquidity, parameterized by `\\lambda`, follows a two-state Markov chain: an \"active\" state (0, high `\\lambda_0`) and a \"slow\" state (1, low `\\lambda_1`), with `\\lambda_0 > \\lambda_1`. The investor observes the current state and adjusts their strategy. The analysis is in the continuous, infinite-horizon fluid limit for a power-law LOB where `\\Lambda(s) = \\lambda s^{-\\alpha}`.\n\n**Variables & Parameters.**\n- `u(x)`: Value function when in the active state (0) with inventory `x`.\n- `w(x)`: Value function when in the slow state (1) with inventory `x`.\n- `s_0^*(x), s_1^*(x)`: Optimal spreads in the active and slow states, respectively.\n\n---\n\n### Data / Model Specification\n\nThe value functions `u(x)` and `w(x)` solve a system of coupled ODEs. The solution has the properties that `u(x) > w(x)` and the marginal value is also higher in the active state, `u_x(x) > w_x(x)`. The optimal spread is proportional to the marginal value: `s^* \\propto u_x` or `s^* \\propto w_x`.\n\n---\n\n### Question\n\nBased on the model's specification and results, select all of the following statements that are correct descriptions of the optimal strategy and its underlying values.", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 449, "Question": "### Background\n\n**Research Question.** This case examines how the equilibrium futures return can be modeled as an outcome of the interaction between two representative agents: hedgers, who are sensitive to portfolio risk, and speculators, who are sensitive to expected returns and risk.\n\n**Setting.** A futures market is populated by hedgers and speculators. Hedger demand for futures depends on the variance of an optimally hedged portfolio. Speculator demand depends on the expected return and variance of futures contracts. The market clears daily.\n\n### Data / Model Specification\n\nThe testable relationship for the realized futures return, `r_{f,t}`, derived from the market clearing condition is:\n  \nr_{f,t} = e_0 - \\frac{b^H}{d^S}\\sigma_{r_{H},t}^{2} + \\frac{e^S}{d^S}\\sigma_{r_{f},t}^{2} + u_{r_{f},t} \n \nwhere `\\sigma_{r_H,t}^2` is the variance of the optimally hedged portfolio and `\\sigma_{r_f,t}^2` is the variance of futures returns.\n\nBehavioral assumptions on parameters:\n*   `b^H`: Hedger sensitivity. `b^H > 0` for consumers; `b^H < 0` for producers.\n*   `d^S`: Speculator sensitivity to expected return (assumed positive).\n*   `e^S`: Speculator sensitivity to futures variance. `e^S > 0` for risk-averse; `e^S < 0` for risk-loving.\n\n**Table 1. Empirical Results for Selected Futures Markets**\n\n| Commodity | Coefficient `-(b^H/d^S)` | Coefficient `(e^S/d^S)` | Relative Importance of Speculative Drivers |\n|:---|:---:|:---:|:---:|\n| Crude Oil | 4.217 | 2.401 | 1.30 |\n| Soybeans | -19.695 | -5.916 | 1.21 |\n\n*Source: Adapted from Tables 2, 3, and 4 in the source paper.*\n\n### Question\n\nBased on the model and empirical results in Table 1, select all of the following statements that are correct characterizations of the markets.", "Options": {"A": "The 'Relative Importance' metric for Crude Oil (1.30) indicates that futures returns in this market are more sensitive to hedging risk (`\\sigma_{r_H,t}^2`) than to speculative risk (`\\sigma_{r_f,t}^2`).", "B": "The negative sign on the `-(b^H/d^S)` coefficient for Soybeans indicates that the market is dominated by producers.", "C": "In the Soybeans market, the results imply that hedgers are predominantly consumers and speculators are, on average, risk-loving.", "D": "In the Crude Oil market, the results imply that hedgers are predominantly producers and speculators are, on average, risk-averse."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the 'atomic decomposition' strategy to test the core interpretation of the paper's behavioral model. It is a 'Reverse-Reasoning' item, requiring the user to infer underlying agent characteristics from estimated coefficients. Options A and B test the correct interpretation for two different markets, providing two independent correct answers. Distractors are based on high-fidelity errors: D makes a sign error in interpreting the `-(b^H/d^S)` coefficient (a negative coefficient implies `-b^H < 0`, so `b^H > 0`, which means consumers dominate). C misinterprets the 'Relative Importance' metric, which shows the opposite (Conceptual Opposite).", "qid": "449", "question": "### Background\n\n**Research Question.** This case examines how the equilibrium futures return can be modeled as an outcome of the interaction between two representative agents: hedgers, who are sensitive to portfolio risk, and speculators, who are sensitive to expected returns and risk.\n\n**Setting.** A futures market is populated by hedgers and speculators. Hedger demand for futures depends on the variance of an optimally hedged portfolio. Speculator demand depends on the expected return and variance of futures contracts. The market clears daily.\n\n### Data / Model Specification\n\nThe testable relationship for the realized futures return, `r_{f,t}`, derived from the market clearing condition is:\n  \nr_{f,t} = e_0 - \\frac{b^H}{d^S}\\sigma_{r_{H},t}^{2} + \\frac{e^S}{d^S}\\sigma_{r_{f},t}^{2} + u_{r_{f},t} \n \nwhere `\\sigma_{r_H,t}^2` is the variance of the optimally hedged portfolio and `\\sigma_{r_f,t}^2` is the variance of futures returns.\n\nBehavioral assumptions on parameters:\n*   `b^H`: Hedger sensitivity. `b^H > 0` for consumers; `b^H < 0` for producers.\n*   `d^S`: Speculator sensitivity to expected return (assumed positive).\n*   `e^S`: Speculator sensitivity to futures variance. `e^S > 0` for risk-averse; `e^S < 0` for risk-loving.\n\n**Table 1. Empirical Results for Selected Futures Markets**\n\n| Commodity | Coefficient `-(b^H/d^S)` | Coefficient `(e^S/d^S)` | Relative Importance of Speculative Drivers |\n|:---|:---:|:---:|:---:|\n| Crude Oil | 4.217 | 2.401 | 1.30 |\n| Soybeans | -19.695 | -5.916 | 1.21 |\n\n*Source: Adapted from Tables 2, 3, and 4 in the source paper.*\n\n### Question\n\nBased on the model and empirical results in Table 1, select all of the following statements that are correct characterizations of the markets.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 446, "Question": "### Background\n\nA central theme in household finance is understanding how consumers choose between fixed-rate mortgages (FRMs) and adjustable-rate mortgages (ARMs). A baseline model proposed by Koijen, Van Hemert, and Van Nieuwerburgh (KHN) posits a simple linear relationship between the aggregate ARM share and the \"Household Decision Rule\" (HDR), a proxy for the long-term bond risk premium. This paper extends that model by hypothesizing that borrowers' sensitivity to the HDR is not constant, but varies over time with their perceived risk. This perceived risk is driven by two main factors: the current level of short-term interest rates and the state of the housing market.\n\n### Data / Model Specification\n\nTo capture this time-varying sensitivity, the coefficients of the baseline model are made conditional on the one-year treasury yield (`short_t`) and a dummy variable for a declining housing market (`down_t`, which equals 1 if the Case-Shiller home price index fell in the previous month, and 0 otherwise). This results in the following full interaction model:\n\n  \nARM_{t}=\\alpha_{0}+\\alpha_{1}short_{t}+\\alpha_{2}down_{t}+\\beta_{0}HDR_{t}+\\beta_{1}(HDR_{t} \\times short_{t}) +\\beta_{2}(HDR_{t} \\times down_{t})+\\varepsilon_{t}\n \nEq. (1)\n\nwhere `ARM_t` is the percentage share of new mortgages that are ARMs. The model is estimated using monthly data from the Federal Housing Finance Agency's Monthly Interest Rate Survey (MIRS) from February 1987 to September 2008. Summary statistics and regression results for this sample are provided in Table 1 and Table 2 below.\n\n**Table 1: Summary Statistics & Correlations (MIRS Sample, 1987-2008)**\n\n| Variable | Mean | Std. Dev. | Min | Max |\n| :--- | :--- | :--- | :--- | :--- |\n| ARM (%) | 25.75 | 13.76 | 4.00 | 69.00 |\n| HDR (%) | 1.11 | 0.98 | -0.86 | 3.73 |\n| short_t (%) | 4.90 | 2.02 | 1.00 | 9.56 |\n| down_t | 0.25 | 0.43 | 0 | 1 |\n\n| | ARM_t | HDR_t | short_t | down_t |\n| :--- | :--- | :--- | :--- | :--- |\n| **ARM_t** | 1.00 | | | |\n| **HDR_t** | 0.67 | 1.00 | | |\n| **short_t** | 0.50 | 0.02 | 1.00 | |\n| **down_t** | -0.27 | -0.17 | -0.06 | 1.00 |\n\n**Table 2: ARM Choice Model Regression Results (MIRS Sample)**\n\n| Variable | (1) Simple Model | (2) Full Model |\n| :--- | :--- | :--- |\n| Intercept | 15.28*** | 5.13* |\n| | (1.74) | (2.68) |\n| HDR_t | 9.41*** | 5.09** |\n| | (1.54) | (2.55) |\n| short_t | | 2.16** |\n| | | (0.59) |\n| down_t | | -0.30 |\n| | | (2.25) |\n| HDR_t x short_t | | 1.01* |\n| | | (0.53) |\n| HDR_t x down_t | | -5.06** |\n| | | (2.06) |\n| | | |\n| N | 260 | 260 |\n| Adj. R² | 0.448 | 0.736 |\n\n*Notes: Newey-West standard errors (12 lags) in parentheses. Significance codes: *p<0.1; **p<0.05; ***p<0.01.*\n\n### Question\n\nBased on the provided data and regression results, which of the following statements are valid interpretations consistent with the paper's theory of time-varying perceived risk? Select all that apply.", "Options": {"A": "The coefficient of -5.06 on `HDR_t x down_t` suggests that during a down market, borrowers become more sensitive to the economic incentives (the risk premium) of choosing an ARM.", "B": "The coefficient of -0.30 on `down_t` indicates that a declining housing market has a direct, statistically significant negative impact on the ARM share.", "C": "The negative correlation of -0.27 between `ARM_t` and `down_t` provides initial, model-free evidence that borrowers are less likely to choose ARMs during housing market downturns.", "D": "The significant positive coefficient of 1.01 on the `HDR_t x short_t` interaction term implies that borrowers' sensitivity to the HDR increases when short-term rates are higher."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret both bivariate correlations and multivariate regression coefficients, particularly interaction terms, in the context of the paper's core theory. The strategy is Atomic Decomposition, breaking down the multifaceted interpretation from the original QA into distinct, verifiable statements. Correct options (A, B) accurately link statistical results to theoretical claims. Distractor (C) targets a common error: ignoring statistical significance (the p-value for the `down_t` coefficient is high, given the standard error of 2.25). Distractor (D) is a Conceptual Opposite, misinterpreting the sign of the interaction term to mean 'more sensitive' instead of 'less sensitive'.", "qid": "446", "question": "### Background\n\nA central theme in household finance is understanding how consumers choose between fixed-rate mortgages (FRMs) and adjustable-rate mortgages (ARMs). A baseline model proposed by Koijen, Van Hemert, and Van Nieuwerburgh (KHN) posits a simple linear relationship between the aggregate ARM share and the \"Household Decision Rule\" (HDR), a proxy for the long-term bond risk premium. This paper extends that model by hypothesizing that borrowers' sensitivity to the HDR is not constant, but varies over time with their perceived risk. This perceived risk is driven by two main factors: the current level of short-term interest rates and the state of the housing market.\n\n### Data / Model Specification\n\nTo capture this time-varying sensitivity, the coefficients of the baseline model are made conditional on the one-year treasury yield (`short_t`) and a dummy variable for a declining housing market (`down_t`, which equals 1 if the Case-Shiller home price index fell in the previous month, and 0 otherwise). This results in the following full interaction model:\n\n  \nARM_{t}=\\alpha_{0}+\\alpha_{1}short_{t}+\\alpha_{2}down_{t}+\\beta_{0}HDR_{t}+\\beta_{1}(HDR_{t} \\times short_{t}) +\\beta_{2}(HDR_{t} \\times down_{t})+\\varepsilon_{t}\n \nEq. (1)\n\nwhere `ARM_t` is the percentage share of new mortgages that are ARMs. The model is estimated using monthly data from the Federal Housing Finance Agency's Monthly Interest Rate Survey (MIRS) from February 1987 to September 2008. Summary statistics and regression results for this sample are provided in Table 1 and Table 2 below.\n\n**Table 1: Summary Statistics & Correlations (MIRS Sample, 1987-2008)**\n\n| Variable | Mean | Std. Dev. | Min | Max |\n| :--- | :--- | :--- | :--- | :--- |\n| ARM (%) | 25.75 | 13.76 | 4.00 | 69.00 |\n| HDR (%) | 1.11 | 0.98 | -0.86 | 3.73 |\n| short_t (%) | 4.90 | 2.02 | 1.00 | 9.56 |\n| down_t | 0.25 | 0.43 | 0 | 1 |\n\n| | ARM_t | HDR_t | short_t | down_t |\n| :--- | :--- | :--- | :--- | :--- |\n| **ARM_t** | 1.00 | | | |\n| **HDR_t** | 0.67 | 1.00 | | |\n| **short_t** | 0.50 | 0.02 | 1.00 | |\n| **down_t** | -0.27 | -0.17 | -0.06 | 1.00 |\n\n**Table 2: ARM Choice Model Regression Results (MIRS Sample)**\n\n| Variable | (1) Simple Model | (2) Full Model |\n| :--- | :--- | :--- |\n| Intercept | 15.28*** | 5.13* |\n| | (1.74) | (2.68) |\n| HDR_t | 9.41*** | 5.09** |\n| | (1.54) | (2.55) |\n| short_t | | 2.16** |\n| | | (0.59) |\n| down_t | | -0.30 |\n| | | (2.25) |\n| HDR_t x short_t | | 1.01* |\n| | | (0.53) |\n| HDR_t x down_t | | -5.06** |\n| | | (2.06) |\n| | | |\n| N | 260 | 260 |\n| Adj. R² | 0.448 | 0.736 |\n\n*Notes: Newey-West standard errors (12 lags) in parentheses. Significance codes: *p<0.1; **p<0.05; ***p<0.01.*\n\n### Question\n\nBased on the provided data and regression results, which of the following statements are valid interpretations consistent with the paper's theory of time-varying perceived risk? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 452, "Question": "### Background\n\n**Research Question.** This question assesses the ability to identify research settings where the paper's \"signaling critique\" is most applicable.\n\n**Setting.** The signaling critique argues that an identification strategy (like IV or RD) may be misleading for a decision-maker if it estimates a partial causal effect while ignoring a plausible signaling channel.\n\n---\n\n### Data / Model Specification\n\nThe signaling critique is deemed applicable if two conditions are met:\n1.  The main explanatory (causal) variable is a choice variable of a unitary decision-maker (e.g., a CFO or board) who may have private information.\n2.  The main dependent (outcome) variable is mediated by the beliefs of a counterparty (e.g., investors, rating agencies, consumers).\n\n---\n\n### Question\n\nSelect all of the following hypothetical research questions where the signaling critique would apply, based on the two criteria provided.\n", "Options": {"A": "What is the causal effect of a firm's board voluntarily destaggering its election cycle on the firm's stock price (Tobin's Q)?", "B": "What is the causal effect of a CEO's decision to initiate a large, unannounced share repurchase program on the firm's credit default swap spreads?", "C": "What is the causal effect of an exogenous, industry-wide shock to input costs on firms' inventory management policies?", "D": "What is the causal effect of a firm's location in an earthquake-prone zone on its insurance premiums?"}, "Answer": ["A", "B"], "pi_justification": "This item assesses the application of the two-part rule for the signaling critique. It is a 'select-all-valid' task. Options (A) and (C) both meet the criteria: they feature a discretionary choice by a firm's leadership (board decision, CEO decision) and an outcome mediated by market beliefs (stock price, CDS spreads). Option (B) fails the first criterion, as location is largely not a discretionary choice intended to signal quality. Option (D) fails the first criterion, as an industry-wide shock is not a unilateral firm choice. The distractors test the ability to distinguish discretionary choices from exogenous states or shocks.", "qid": "452", "question": "### Background\n\n**Research Question.** This question assesses the ability to identify research settings where the paper's \"signaling critique\" is most applicable.\n\n**Setting.** The signaling critique argues that an identification strategy (like IV or RD) may be misleading for a decision-maker if it estimates a partial causal effect while ignoring a plausible signaling channel.\n\n---\n\n### Data / Model Specification\n\nThe signaling critique is deemed applicable if two conditions are met:\n1.  The main explanatory (causal) variable is a choice variable of a unitary decision-maker (e.g., a CFO or board) who may have private information.\n2.  The main dependent (outcome) variable is mediated by the beliefs of a counterparty (e.g., investors, rating agencies, consumers).\n\n---\n\n### Question\n\nSelect all of the following hypothetical research questions where the signaling critique would apply, based on the two criteria provided.\n", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 439, "Question": "### Background\n\n**Research Question.** Beyond estimating the average levels of risk, it is important to understand the relationship between different risk exposures. A key question is whether firms that are highly sensitive to systematic market risk (high \"market beta\") are also highly sensitive to exchange rate risk. A simple linear relationship might be expected, but the reality could be more complex due to factors like corporate risk management.\n\n**Setting / Data-Generating Environment.** A quantile regression framework is used to model the relationship between firms' estimated market betas (`\\hat{\\beta}_{im}^M`) and their estimated exchange rate betas (`\\hat{\\beta}_{is}`), which were generated from a prior regression analysis.\n\n### Data / Model Specification\n\nThe paper investigates the relationship between the two estimated risk exposures. The quantile regression technique models the `\\tau`-th conditional quantile of a dependent variable `y` given an independent variable `x`. Its objective function is:\n\n  \n\\min_{\\theta_0, \\theta_1} \\sum_{i=1}^{N} \\rho_{\\tau}(y_i - \\theta_0 - \\theta_1 x_i), \\quad \\text{where } \\rho_{\\tau}(u) = u(\\tau - I(u<0)) \\quad \\text{(Eq. (1))}\n \n\nHere, `y_i = \\hat{\\beta}_{is}` and `x_i = \\hat{\\beta}_{im}^M`. The term `I(u<0)` is an indicator function that is 1 if `u<0` and 0 otherwise. The paper's key finding is that the relationship is non-monotonic.\n\n### Question\n\nThe paper uses quantile regression to analyze the relationship between market risk (`\\hat{\\beta}_{im}^M`) and exchange rate risk (`\\hat{\\beta}_{is}`). Based on the provided information and the paper's findings, select all statements that are methodologically or empirically correct.", "Options": {"A": "The `\\rho_{\\tau}(u)` function in Eq. (1) works by minimizing the sum of squared residuals, which ensures the regression line fits the conditional mean of the data, similar to OLS.", "B": "The analysis shows a simple monotonic relationship, where firms with the highest market risk (`\\hat{\\beta}_{im}^M`) consistently exhibit the highest exchange rate risk (`\\hat{\\beta}_{is}`).", "C": "The key empirical finding is a non-monotonic, inverted U-shaped relationship: exchange rate sensitivity first increases with market sensitivity for defensive firms, then decreases for the most aggressive firms.", "D": "Quantile regression is preferred over OLS because it can model how the relationship between the two risk factors changes at different points of the conditional distribution, which is necessary to detect the non-monotonic pattern."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). Score A=8, B=10. This item uses Atomic Decomposition to test a comprehensive understanding of the quantile regression analysis. It assesses the ability to recall the main empirical finding (A), understand the methodological rationale (B), and avoid common misconceptions about the technique's mechanics (C) and the empirical results (D). Distractors are based on classic Conceptual Opposites from econometrics and a direct contradiction of the paper's findings.", "qid": "439", "question": "### Background\n\n**Research Question.** Beyond estimating the average levels of risk, it is important to understand the relationship between different risk exposures. A key question is whether firms that are highly sensitive to systematic market risk (high \"market beta\") are also highly sensitive to exchange rate risk. A simple linear relationship might be expected, but the reality could be more complex due to factors like corporate risk management.\n\n**Setting / Data-Generating Environment.** A quantile regression framework is used to model the relationship between firms' estimated market betas (`\\hat{\\beta}_{im}^M`) and their estimated exchange rate betas (`\\hat{\\beta}_{is}`), which were generated from a prior regression analysis.\n\n### Data / Model Specification\n\nThe paper investigates the relationship between the two estimated risk exposures. The quantile regression technique models the `\\tau`-th conditional quantile of a dependent variable `y` given an independent variable `x`. Its objective function is:\n\n  \n\\min_{\\theta_0, \\theta_1} \\sum_{i=1}^{N} \\rho_{\\tau}(y_i - \\theta_0 - \\theta_1 x_i), \\quad \\text{where } \\rho_{\\tau}(u) = u(\\tau - I(u<0)) \\quad \\text{(Eq. (1))}\n \n\nHere, `y_i = \\hat{\\beta}_{is}` and `x_i = \\hat{\\beta}_{im}^M`. The term `I(u<0)` is an indicator function that is 1 if `u<0` and 0 otherwise. The paper's key finding is that the relationship is non-monotonic.\n\n### Question\n\nThe paper uses quantile regression to analyze the relationship between market risk (`\\hat{\\beta}_{im}^M`) and exchange rate risk (`\\hat{\\beta}_{is}`). Based on the provided information and the paper's findings, select all statements that are methodologically or empirically correct.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 467, "Question": "### Background\n\nAn insurer's goal is to minimize the Tail Value-at-Risk (TVaR) of its post-reinsurance total risk, `T_I(X)`. The derivation of the objective function for this optimization problem is a key step in finding the optimal reinsurance contract, `I(X)`.\n\n### Data / Model Specification\n\nThe TVaR of the total risk `T_I(X)` can be expressed as:\n\n  \n\\mathrm{TVaR}_{T_{I}(X)}(p) = C(p) + (1+\\theta)\\int_{0}^{\\mathrm{VaR}_{X}(p)}I(x)dF_{X}(x) + \\delta\\int_{\\mathrm{VaR}_{X}(p)}^{\\infty}I(x)dF_{X}(x) \\quad \\text{(Eq. 1)}\n \n\nWhere:\n- `C(p)` is a term that depends on the confidence level `p` and the loss distribution `F_X(x)`, but not on the choice of `I(x)`.\n- `θ > 0` is the safety loading factor.\n- `VaR_X(p)` is the Value-at-Risk of the initial loss `X`.\n- The parameter `δ` is defined as:\n\n  \n\\delta = 1+\\theta - \\frac{1}{1-p} \\quad \\text{(Eq. 2)}\n \n\nThis derivation relies on the assumption that the ceded loss `I(X)` and the retained loss `X - I(X)` are comonotonic (i.e., they always move in the same direction).\n\n### Question\n\nBased on the structure of the TVaR objective function in Eq. 1 and the definition of `δ` in Eq. 2, which of the following conclusions are valid? Select all that apply.", "Options": {"A": "If `δ < 0`, the insurer has an incentive to cede more risk for losses `x` that fall in the extreme tail (i.e., `x > VaR_X(p)`), because the marginal benefit of TVaR reduction outweighs the marginal premium cost for those losses.", "B": "If a reinsurance contract introduces basis risk (e.g., the payout `I(X,Y)` depends on an external index `Y`), the comonotonicity assumption is violated, and the derivation in Eq. 1 would fail because the TVaR of the retained loss would be greater than the sum of the individual TVaRs.", "C": "The parameter `δ` represents the net marginal cost of ceding one unit of expected loss in the region below the `p`-quantile (i.e., for `x < VaR_X(p)`).", "D": "If the safety loading `θ` is very low and the confidence level `p` is very high (e.g., `p=0.99`), it is more likely that `δ` will be negative."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the student's ability to interpret the mathematical structure of the TVaR objective function, a core analytical result of the paper. It uses a 'Reverse-Reasoning' strategy, asking for valid conclusions from the given formula. Option A correctly interprets the economic trade-off governed by `δ`. Option C requires 'Computational Judgment' to assess how the inputs `θ` and `p` affect the sign of `δ`. Option B is a 'Conceptual Distractor' (Almost Right), as `δ` applies to the tail *above* the quantile, not below. Option D is also 'Almost Right' but contains a sign error; basis risk introduces a diversification benefit, so the TVaR of the retained loss would be *less than* the sum, not greater.", "qid": "467", "question": "### Background\n\nAn insurer's goal is to minimize the Tail Value-at-Risk (TVaR) of its post-reinsurance total risk, `T_I(X)`. The derivation of the objective function for this optimization problem is a key step in finding the optimal reinsurance contract, `I(X)`.\n\n### Data / Model Specification\n\nThe TVaR of the total risk `T_I(X)` can be expressed as:\n\n  \n\\mathrm{TVaR}_{T_{I}(X)}(p) = C(p) + (1+\\theta)\\int_{0}^{\\mathrm{VaR}_{X}(p)}I(x)dF_{X}(x) + \\delta\\int_{\\mathrm{VaR}_{X}(p)}^{\\infty}I(x)dF_{X}(x) \\quad \\text{(Eq. 1)}\n \n\nWhere:\n- `C(p)` is a term that depends on the confidence level `p` and the loss distribution `F_X(x)`, but not on the choice of `I(x)`.\n- `θ > 0` is the safety loading factor.\n- `VaR_X(p)` is the Value-at-Risk of the initial loss `X`.\n- The parameter `δ` is defined as:\n\n  \n\\delta = 1+\\theta - \\frac{1}{1-p} \\quad \\text{(Eq. 2)}\n \n\nThis derivation relies on the assumption that the ceded loss `I(X)` and the retained loss `X - I(X)` are comonotonic (i.e., they always move in the same direction).\n\n### Question\n\nBased on the structure of the TVaR objective function in Eq. 1 and the definition of `δ` in Eq. 2, which of the following conclusions are valid? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,D", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 462, "Question": "### Background\n\n**Research Question.** What is the causal effect of international climate policy announcements on investor participation in carbon-intensive firms?\n\n**Setting.** The study uses a global sample of publicly traded firms to analyze changes in investor holdings around two key events: the signing of the Paris Agreement (PA) and the subsequent announcement of the US withdrawal. Firms are classified into High-Carbon (HC) and Low-Carbon (LC) groups.\n\n**Data / Model Specification.**\nThe study employs a multi-period Difference-in-Differences (DiD) regression model:\n\n  \nY_{it} = \\alpha_i + \\delta_t + \\beta_1 (HC_i \\times PostPA_t) + \\beta_2 (HC_i \\times PostUSW_t) + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\n- `Y_{it}`: Institutional investor ownership in firm `i` at time `t`.\n- `HC_i`: A dummy for High-Carbon firms (treatment group).\n- `PostPA_t`, `PostUSW_t`: Dummies for the periods after the two events.\n\n**Table 1: Stylized DiD Regression Results**\n\n| Variable | Coefficient Estimate |\n| :--- | :--- |\n| `HC x PostPA` (`\\beta_1`) | -0.05*** |\n| `HC x PostUSW` (`\\beta_2`) | +0.04** |\n\n*Note: *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nBased on the provided model and results, select ALL of the following statements that are correct.", "Options": {"A": "The coefficient `β1 = -0.05` implies that after the Paris Agreement, investor ownership in High-Carbon firms fell by 5 percentage points relative to the change in ownership for Low-Carbon firms.", "B": "The study's causal interpretation relies on the 'parallel trends' assumption: that absent the policy events, ownership trends in High-Carbon and Low-Carbon firms would have evolved similarly.", "C": "The results suggest that investors react to the perceived credibility of global climate policy, divesting when the policy seems strong and reinvesting when its enforcement appears weakened.", "D": "The positive coefficient `β2 = +0.04` indicates that the US withdrawal completely erased the initial effect of the Paris Agreement, returning ownership in HC firms to its pre-agreement trend."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests the interpretation of DiD regression results and the understanding of the key identifying assumption. It uses an Atomic Decomposition strategy. The distractor (C) is a subtle 'Almost Right' error, testing for precise interpretation of the coefficient magnitudes rather than a general impression.", "qid": "462", "question": "### Background\n\n**Research Question.** What is the causal effect of international climate policy announcements on investor participation in carbon-intensive firms?\n\n**Setting.** The study uses a global sample of publicly traded firms to analyze changes in investor holdings around two key events: the signing of the Paris Agreement (PA) and the subsequent announcement of the US withdrawal. Firms are classified into High-Carbon (HC) and Low-Carbon (LC) groups.\n\n**Data / Model Specification.**\nThe study employs a multi-period Difference-in-Differences (DiD) regression model:\n\n  \nY_{it} = \\alpha_i + \\delta_t + \\beta_1 (HC_i \\times PostPA_t) + \\beta_2 (HC_i \\times PostUSW_t) + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\n- `Y_{it}`: Institutional investor ownership in firm `i` at time `t`.\n- `HC_i`: A dummy for High-Carbon firms (treatment group).\n- `PostPA_t`, `PostUSW_t`: Dummies for the periods after the two events.\n\n**Table 1: Stylized DiD Regression Results**\n\n| Variable | Coefficient Estimate |\n| :--- | :--- |\n| `HC x PostPA` (`\\beta_1`) | -0.05*** |\n| `HC x PostUSW` (`\\beta_2`) | +0.04** |\n\n*Note: *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nBased on the provided model and results, select ALL of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 191, "Question": "### Background\n\nThe model's central prediction is that the active management risk premium is, on average, positive. This premium arises because benchmark-aware investors create demand for portfolios that deviate from the market, and other investors must be compensated for absorbing this risk.\n\n### Data / Model Specification\n\nThe equilibrium condition for the active management risk premium, constructed to be orthogonal to the market factor, is:\n  \nβ̂μ_{M} - μ_{Φ} = (π w) × [Φ'ΩΦ(1-ρ_{MΦ}^{2})] \\quad \\text{(Eq. (1))}\n \nwhere:\n- The left-hand side is the expected active management risk premium.\n- `π` is the CARA coefficient of risk aversion (`π > 0`).\n- `w` is the wealth share of benchmark investors, `W_b / (W_a + W_b)`.\n- `Φ'ΩΦ` is the variance of the benchmark portfolio's return.\n- `ρ_{MΦ}` is the correlation between the market and benchmark portfolios.\n\n---\n\nAccording to the relationship in **Eq. (1)**, which of the following conditions would lead to a predicted active management risk premium of exactly zero?\n", "Options": {"A": "The benchmark portfolio is the global minimum variance portfolio (`Φ'ΩΦ` is minimized).", "B": "The benchmark portfolio is perfectly correlated with the market portfolio (`ρ_{MΦ}^2 = 1`).", "C": "The economy contains no benchmark-aware investors (`w = 0`).", "D": "Investors are risk-neutral (`π = 0`)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to interpret the economic determinants of the active management risk premium as specified by the model's core theoretical equation. Depth Strategy: Reverse-Reasoning. The question asks for the conditions under which the predicted effect (a positive risk premium) disappears, forcing the user to identify all necessary components of the causal mechanism. Distractor Logic: Option D is a conceptual distractor. While minimizing the benchmark's variance would reduce the premium's magnitude, it would not drive it to exactly zero unless the variance itself was zero (i.e., a risk-free benchmark), which is not what is stated. This tests whether the user understands the difference between reducing and eliminating the premium.", "qid": "191", "question": "### Background\n\nThe model's central prediction is that the active management risk premium is, on average, positive. This premium arises because benchmark-aware investors create demand for portfolios that deviate from the market, and other investors must be compensated for absorbing this risk.\n\n### Data / Model Specification\n\nThe equilibrium condition for the active management risk premium, constructed to be orthogonal to the market factor, is:\n  \nβ̂μ_{M} - μ_{Φ} = (π w) × [Φ'ΩΦ(1-ρ_{MΦ}^{2})] \\quad \\text{(Eq. (1))}\n \nwhere:\n- The left-hand side is the expected active management risk premium.\n- `π` is the CARA coefficient of risk aversion (`π > 0`).\n- `w` is the wealth share of benchmark investors, `W_b / (W_a + W_b)`.\n- `Φ'ΩΦ` is the variance of the benchmark portfolio's return.\n- `ρ_{MΦ}` is the correlation between the market and benchmark portfolios.\n\n---\n\nAccording to the relationship in **Eq. (1)**, which of the following conditions would lead to a predicted active management risk premium of exactly zero?\n", "model_evaluation_result": {"model_raw_response": "B,C,D", "model_answer": ["B", "C", "D"], "error": null}}
{"ID": 455, "Question": "### Background\n\nIn econometric analyses using instrumental variables, such as the Generalized Method of Moments (GMM), it is crucial to validate the model's specification. The Hansen J-test of overidentifying restrictions is a key diagnostic tool for this purpose.\n\n---\n\nWhich of the following statements about the Hansen J-test are correct? (Select all that apply)", "Options": {"A": "Under the null hypothesis, the test statistic follows a standard normal distribution.", "B": "The test is applicable only when the number of instruments is strictly greater than the number of endogenous variables being instrumented.", "C": "The null hypothesis is that the instruments are jointly valid (i.e., uncorrelated with the error term).", "D": "A low p-value (e.g., < 0.05) provides confidence that the instruments are valid."}, "Answer": ["B", "C"], "pi_justification": "This item assesses knowledge of a fundamental econometric specification test. The strategy is factual recall. The distractors are designed based on high-frequency misconceptions: B (Conceptual Opposite: a high p-value is desired), C (Correct: this is the definition of 'overidentifying'), and D (Formula Misuse: the statistic follows a chi-squared, not normal, distribution).", "qid": "455", "question": "### Background\n\nIn econometric analyses using instrumental variables, such as the Generalized Method of Moments (GMM), it is crucial to validate the model's specification. The Hansen J-test of overidentifying restrictions is a key diagnostic tool for this purpose.\n\n---\n\nWhich of the following statements about the Hansen J-test are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "B,C", "model_answer": ["B", "C"], "error": null}}
{"ID": 453, "Question": "### Background\n\nA central question in corporate finance is whether dividend policy contains information about a firm's future prospects. This paper investigates if managerial reluctance to cut dividends, termed \"dividend stickiness,\" serves as a credible signal of future earnings quality. The primary empirical finding is a positive correlation between dividend stickiness and future earnings persistence. However, a key challenge to interpreting this correlation as a causal signal is the issue of self-selection bias: managers of firms with genuinely better (but unobserved) future prospects may be more likely to adopt a sticky dividend policy. The correlation would then be spurious rather than a result of the policy signaling information.\n\nTo strengthen the causal claim, the paper employs two distinct strategies: (1) a cross-sectional analysis to rule out the alternative \"catering theory\" of dividends, and (2) a propensity score matching (PSM) analysis to mitigate selection bias based on observable firm characteristics.\n\n### Data / Model Specification\n\nThe baseline test for the signaling hypothesis uses the following one-year-ahead earnings persistence model:\n\n  \nEPS_{i,t+1}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}DS_{i,t}+\\alpha_{3}EPS_{i,t}*DS_{i,t}+\\alpha_{4}SI_{i,t}+\\alpha_{5}SI_{i,t}*EPS_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n \n**Eq. (1)**\n\nwhere `EPS` is earnings per share, `DS` is a measure of dividend stickiness (`DS_Dummy` = 1 for sticky firms), and `SI` is a dummy for special items. The coefficient of interest is `α3`, which captures the incremental effect of dividend stickiness on earnings persistence.\n\n**Catering Theory Analysis**: This theory posits that managers smooth dividends to cater to investor demand, not to signal. This demand is proxied by the Dividend Premium (`DP`), where `DP=1` indicates high catering incentives. The sample is split into high (`DP=1`) and low (`DP=0`) catering groups, and Eq. (1) is estimated for each.\n\n**Propensity Score Matching (PSM) Analysis**: To address self-selection, a logistic model estimates the probability (propensity score) of a firm having a sticky dividend policy based on observable characteristics. Sticky firms (treatment) are then matched with non-sticky firms (control) that have the closest propensity score. Eq. (1) is then re-estimated on this matched sample.\n\n**Table 1: Baseline OLS Results for Earnings Persistence**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.675*** |\n| **EPS\\*DS** | **0.031*** |\n| Observations | 59,038 |\n\n**Table 2: Cross-Sectional Analysis by Catering Incentives**\n\n| | DP-high (DP=1) | DP-low (DP=0) |\n| :--- | :--- | :--- |\n| EPSt | 0.666*** | 0.688*** |\n| **EPS\\*DS_Dummy** | **0.019** | **0.039*** |\n\n**Table 3: PSM Results for Earnings Persistence**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.751*** |\n| **EPS\\*DS** | **0.045** |\n| Observations | 18,984 |\n\n\n### Question\n\nThe paper investigates whether dividend stickiness is a credible signal of future earnings, using several econometric strategies to address potential biases. Based on the provided models and results, which of the following statements are valid interpretations of the evidence? Select all that apply.", "Options": {"A": "The PSM results in Table 3 reveal that once observable differences are accounted for, the signaling effect of dividend stickiness disappears, suggesting the baseline OLS result was entirely due to self-selection.", "B": "The cross-sectional analysis in Table 2 provides strong support for the catering theory, showing that the reluctance to cut dividends is most informative when investor demand for dividends is high.", "C": "The Propensity Score Matching (PSM) analysis, which controls for selection on observable firm characteristics, confirms that a positive relationship between dividend stickiness and earnings persistence remains.", "D": "The signaling effect of dividend stickiness on earnings persistence is statistically significant in the sub-sample of firms with low catering incentives, but not in the sub-sample with high catering incentives."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize evidence from multiple, advanced econometric analyses (sub-group and PSM) to evaluate a paper's core causal claim. It uses an Atomic Decomposition strategy, breaking down the paper's identification strategy into distinct, verifiable conclusions. Distractors are designed as Conceptual Opposites (C and D), directly contradicting the evidence in the tables to test for careful reading and interpretation of the paper's main efforts to establish causality.", "qid": "453", "question": "### Background\n\nA central question in corporate finance is whether dividend policy contains information about a firm's future prospects. This paper investigates if managerial reluctance to cut dividends, termed \"dividend stickiness,\" serves as a credible signal of future earnings quality. The primary empirical finding is a positive correlation between dividend stickiness and future earnings persistence. However, a key challenge to interpreting this correlation as a causal signal is the issue of self-selection bias: managers of firms with genuinely better (but unobserved) future prospects may be more likely to adopt a sticky dividend policy. The correlation would then be spurious rather than a result of the policy signaling information.\n\nTo strengthen the causal claim, the paper employs two distinct strategies: (1) a cross-sectional analysis to rule out the alternative \"catering theory\" of dividends, and (2) a propensity score matching (PSM) analysis to mitigate selection bias based on observable firm characteristics.\n\n### Data / Model Specification\n\nThe baseline test for the signaling hypothesis uses the following one-year-ahead earnings persistence model:\n\n  \nEPS_{i,t+1}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}DS_{i,t}+\\alpha_{3}EPS_{i,t}*DS_{i,t}+\\alpha_{4}SI_{i,t}+\\alpha_{5}SI_{i,t}*EPS_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n \n**Eq. (1)**\n\nwhere `EPS` is earnings per share, `DS` is a measure of dividend stickiness (`DS_Dummy` = 1 for sticky firms), and `SI` is a dummy for special items. The coefficient of interest is `α3`, which captures the incremental effect of dividend stickiness on earnings persistence.\n\n**Catering Theory Analysis**: This theory posits that managers smooth dividends to cater to investor demand, not to signal. This demand is proxied by the Dividend Premium (`DP`), where `DP=1` indicates high catering incentives. The sample is split into high (`DP=1`) and low (`DP=0`) catering groups, and Eq. (1) is estimated for each.\n\n**Propensity Score Matching (PSM) Analysis**: To address self-selection, a logistic model estimates the probability (propensity score) of a firm having a sticky dividend policy based on observable characteristics. Sticky firms (treatment) are then matched with non-sticky firms (control) that have the closest propensity score. Eq. (1) is then re-estimated on this matched sample.\n\n**Table 1: Baseline OLS Results for Earnings Persistence**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.675*** |\n| **EPS\\*DS** | **0.031*** |\n| Observations | 59,038 |\n\n**Table 2: Cross-Sectional Analysis by Catering Incentives**\n\n| | DP-high (DP=1) | DP-low (DP=0) |\n| :--- | :--- | :--- |\n| EPSt | 0.666*** | 0.688*** |\n| **EPS\\*DS_Dummy** | **0.019** | **0.039*** |\n\n**Table 3: PSM Results for Earnings Persistence**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.751*** |\n| **EPS\\*DS** | **0.045** |\n| Observations | 18,984 |\n\n\n### Question\n\nThe paper investigates whether dividend stickiness is a credible signal of future earnings, using several econometric strategies to address potential biases. Based on the provided models and results, which of the following statements are valid interpretations of the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "C", "model_answer": ["C"], "error": null}}
{"ID": 456, "Question": "### Background\nIn the paper's model, a target firm and an “insider” acquirer form a coalition to maximize their joint surplus in a future takeover contest against N “outsiders.” They achieve this by having the target sell an equity stake of size `α*` to the insider. The optimal stake size, `α*`, is determined by the trade-off between extracting surplus from outsiders and ensuring outsiders are willing to pay an investigation cost, *c*, to participate. A larger stake `α` makes the insider a more aggressive bidder, which helps the coalition extract surplus but reduces the outsiders' expected payoff, potentially violating their participation constraint.\n\n### Data / Model Specification\nThe model yields several key predictions about the optimal stake size:\n1.  The optimal stake size, `α*`, is increasing in the probability of a takeover, *s*, and decreasing in the outsiders’ cost of investigation, *c*.\n2.  The optimal stake size, `α*`, decreases with any positive stochastic shift in the insider’s synergy value distribution, F_I(·).\n3.  Assuming disagreement between the target and insider would result in a standard second-price auction with no stake, the equity stake `α*` will always sell for a discount to the target’s post-trade market price.\n\n### Question\nBased on the model's logic and propositions, select ALL statements that accurately describe the determinants of the optimal stake size, `α*`.", "Options": {"A": "A higher probability of a takeover (*s*) allows the coalition to take a larger optimal stake `α*`.", "B": "If the insider is ex-ante stronger (i.e., has a stochastically dominant synergy distribution), the coalition must take a smaller optimal stake `α*`.", "C": "In a hypothetical scenario where outsiders' investigation costs (*c*) are zero, the optimal stake size `α*` would also be zero.", "D": "A higher investigation cost (*c*) for outsiders requires the coalition to take a larger optimal stake `α*` to compensate."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the understanding of the model's key comparative statics and a critical boundary condition. It uses an Atomic Decomposition strategy to test multiple distinct predictions from the paper. Distractors are designed based on a sign error (B), a common logical slip, and a misunderstanding of a boundary condition (D), which targets a frequent error path where a variable going to zero implies the outcome must also be zero.", "qid": "456", "question": "### Background\nIn the paper's model, a target firm and an “insider” acquirer form a coalition to maximize their joint surplus in a future takeover contest against N “outsiders.” They achieve this by having the target sell an equity stake of size `α*` to the insider. The optimal stake size, `α*`, is determined by the trade-off between extracting surplus from outsiders and ensuring outsiders are willing to pay an investigation cost, *c*, to participate. A larger stake `α` makes the insider a more aggressive bidder, which helps the coalition extract surplus but reduces the outsiders' expected payoff, potentially violating their participation constraint.\n\n### Data / Model Specification\nThe model yields several key predictions about the optimal stake size:\n1.  The optimal stake size, `α*`, is increasing in the probability of a takeover, *s*, and decreasing in the outsiders’ cost of investigation, *c*.\n2.  The optimal stake size, `α*`, decreases with any positive stochastic shift in the insider’s synergy value distribution, F_I(·).\n3.  Assuming disagreement between the target and insider would result in a standard second-price auction with no stake, the equity stake `α*` will always sell for a discount to the target’s post-trade market price.\n\n### Question\nBased on the model's logic and propositions, select ALL statements that accurately describe the determinants of the optimal stake size, `α*`.", "model_evaluation_result": {"model_raw_response": "A,B", "model_answer": ["A", "B"], "error": null}}
{"ID": 460, "Question": "### Background\n\n**Research Question.** After establishing the statistical significance of various factors on sovereign risk, the study seeks to determine their relative economic importance. Specifically, is the impact of global risk appetite larger or smaller than that of traditional country-specific fundamentals?\n\n**Setting / Data-Generating Environment.** The analysis uses standardized coefficients from a dynamic panel model. Standardization involves normalizing each variable (dependent and independent) by subtracting its mean and dividing by its standard deviation. This allows for a unit-free comparison of the explanatory variables' influence.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t`.\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t`.\n- `IR`\\u208_it\\u208_: International reserves / imports ratio for country `i` at quarter `t`.\n- Standardized Coefficient (`\\theta`): The change in the dependent variable, measured in standard deviations, resulting from a one standard deviation change in an independent variable.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents standardized coefficients to assess the relative economic importance of different factors on the Brady bond yield spread (BBY).\n\n**Table 1: Estimated Standardized Coefficients for BBY**\n\n| | Standardized Coefficient |\n| :--- | :--- |\n| **Dependent Variable** | **Brady bond stripped yield spread** |\n| One-lagged dep. Var. | 0.6374 (7.899)** |\n| Risk appetite index (RAI) | **-0.2083 (-3.355)**** |\n| Real GDP growth (GDP) | -0.1599 (-2.271)** |\n| International reserves (IR) | **-0.1688 (-2.325)**** |\n| Change in real exchange rate | -0.1333 (-1.943)** |\n| Inflation rate | 0.0808 (1.016) |\n\n*Notes: t-statistics in parentheses. ** denotes significance at the 5% level or better.* \n\n---\n\nWhich of the following statements are valid conclusions based on these results and the methodology?", "Options": {"A": "A one-standard-deviation increase in International Reserves (IR) is predicted to cause the BBY spread to decrease by 0.1688 basis points.", "B": "The use of standardized coefficients is necessary for a meaningful comparison of economic importance because the explanatory variables are measured in different units and have different natural scales of variation.", "C": "The large coefficient on the lagged dependent variable (0.6374) implies that the short-run impacts shown in the table are identical to the long-run impacts of each variable.", "D": "A one-standard-deviation shock to the Risk Appetite Index (RAI) has a larger immediate impact on the BBY spread than a one-standard-deviation shock to International Reserves (IR)."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the interpretation of standardized coefficients and the understanding of their methodological purpose. It uses an Atomic Decomposition strategy. Distractors include a classic Unit Error (C) confusing standard deviations with basis points, and a Conceptual Opposite (D) that misinterprets the implications of dynamic adjustment in the model, subtly testing a key limitation of the analysis.", "qid": "460", "question": "### Background\n\n**Research Question.** After establishing the statistical significance of various factors on sovereign risk, the study seeks to determine their relative economic importance. Specifically, is the impact of global risk appetite larger or smaller than that of traditional country-specific fundamentals?\n\n**Setting / Data-Generating Environment.** The analysis uses standardized coefficients from a dynamic panel model. Standardization involves normalizing each variable (dependent and independent) by subtracting its mean and dividing by its standard deviation. This allows for a unit-free comparison of the explanatory variables' influence.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t`.\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t`.\n- `IR`\\u208_it\\u208_: International reserves / imports ratio for country `i` at quarter `t`.\n- Standardized Coefficient (`\\theta`): The change in the dependent variable, measured in standard deviations, resulting from a one standard deviation change in an independent variable.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents standardized coefficients to assess the relative economic importance of different factors on the Brady bond yield spread (BBY).\n\n**Table 1: Estimated Standardized Coefficients for BBY**\n\n| | Standardized Coefficient |\n| :--- | :--- |\n| **Dependent Variable** | **Brady bond stripped yield spread** |\n| One-lagged dep. Var. | 0.6374 (7.899)** |\n| Risk appetite index (RAI) | **-0.2083 (-3.355)**** |\n| Real GDP growth (GDP) | -0.1599 (-2.271)** |\n| International reserves (IR) | **-0.1688 (-2.325)**** |\n| Change in real exchange rate | -0.1333 (-1.943)** |\n| Inflation rate | 0.0808 (1.016) |\n\n*Notes: t-statistics in parentheses. ** denotes significance at the 5% level or better.* \n\n---\n\nWhich of the following statements are valid conclusions based on these results and the methodology?", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 464, "Question": "### Background\n\n**Research Question.** In designing early warning systems for financial distress, why might simple, transparent metrics be more effective than complex, statistically sophisticated models?\n\n**Setting.** A financial stability authority compares a simple indicator (`I_S`) with a complex model (`I_C`) based on their historical predictive accuracy for financial crises, measured by the Area Under the Curve (AUC).\n\n**Table 1: Hypothetical Performance of Early Warning Indicators**\n\n| Indicator Type | Number of Parameters | In-Sample AUC (1980-2007) | Out-of-Sample AUC (2008-2020) |\n| :--- | :--- | :--- | :--- |\n| Simple (`I_S`) | 1 | 0.75 | 0.72 |\n| Complex (`I_C`) | 50+ | 0.92 | 0.65 |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the principles of robust policymaking, select ALL of the following statements that are correct.", "Options": {"A": "The complex model's (`I_C`) superior in-sample performance and poor out-of-sample performance is a classic sign of overfitting, where the model has learned sample-specific noise rather than the true underlying signal.", "B": "Beyond statistical performance, a simple indicator is often preferred by regulators because it is easier to communicate to the public and financial industry, which enhances policy legitimacy and predictability.", "C": "The simple model's (`I_S`) better out-of-sample performance suggests it is more robust and has captured a more generalizable relationship for predicting crises.", "D": "In the event of a structural break not seen in the historical data, the complex model (`I_C`) would likely be more reliable because its multiple parameters can adapt more quickly to the new environment."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests core concepts in statistical modeling and the political economy of regulation. It uses an Atomic Decomposition strategy to convert the original QA into verifiable statements. The distractor (D) is a high-fidelity 'Conceptual Opposite' that targets a common misunderstanding about model complexity and robustness to structural change.", "qid": "464", "question": "### Background\n\n**Research Question.** In designing early warning systems for financial distress, why might simple, transparent metrics be more effective than complex, statistically sophisticated models?\n\n**Setting.** A financial stability authority compares a simple indicator (`I_S`) with a complex model (`I_C`) based on their historical predictive accuracy for financial crises, measured by the Area Under the Curve (AUC).\n\n**Table 1: Hypothetical Performance of Early Warning Indicators**\n\n| Indicator Type | Number of Parameters | In-Sample AUC (1980-2007) | Out-of-Sample AUC (2008-2020) |\n| :--- | :--- | :--- | :--- |\n| Simple (`I_S`) | 1 | 0.75 | 0.72 |\n| Complex (`I_C`) | 50+ | 0.92 | 0.65 |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the principles of robust policymaking, select ALL of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "\n\nA,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 399, "Question": "### Background\n\n**Research Question.** The efficiency of the S&P 500 futures market can be tested under two competing hypotheses: the simple unbiasedness hypothesis, which posits a direct cointegrating relationship between spot and futures prices, and the cost-of-carry model, which argues for a trivariate cointegrating relationship that includes a measure of financing costs. This investigation seeks to determine which model is empirically supported by analyzing the stability of the estimated relationships and the validity of theoretical parameter restrictions.\n\n**Setting / Data-Generating Environment.** A Vector Error Correction Model (VECM) framework is applied to daily S&P 500 spot prices (`s_t`), futures prices (`f_t`), and the 3-month Treasury bill rate (`r_t`) from April 1982 to June 2003. The analysis proceeds in three stages: (1) estimating a bivariate model for `(s_t, f_t)`, (2) estimating a trivariate model for `(s_t, f_t, r_t)`, and (3) testing for weak exogeneity within the preferred model to determine the locus of price discovery.\n\n### Data / Model Specification\n\nThe core of the analysis rests on the VECM representation of a system of `p` variables `X_t`:\n  \n\\Delta X_{t}=\\alpha \\beta' X_{t-1} + \\sum_{j=1}^{k-1} \\Gamma_j \\Delta X_{t-j} + \\mu + \\delta t + \\varepsilon_t \\quad \\text{(Eq. (1))}\n \nwhere `\\beta` contains the `r` long-run cointegrating vectors and `\\alpha` contains the adjustment coefficients. A variable `i` is weakly exogenous if its corresponding adjustment coefficient, `\\alpha_i`, is zero.\n\n**Table 1: Summary of Empirical Findings for Bivariate and Trivariate Models**\n\n| Feature | Bivariate Model (`s_t`, `f_t`) | Trivariate Model (`s_t`, `f_t`, `r_t`) |\n| :--- | :--- | :--- |\n| **Cointegrating Rank** | `r=1` is found (Trace stat = 44.59 > 25.47 crit.) | `r=1` is found (Trace stat = 25.08 ≈ 25.47 crit.) |\n| **Parameter Stability** | **Rejected**. The relationship is unstable over the sample. | **Not Rejected**. The relationship is stable over the sample. |\n| **Symmetry Test (`H_0: \\beta` contains `[1, -1]`)** | **Rejected**. (LR stat = 5.44, `\\chi^2(1)`). | **Not Rejected**. (LR stat = 0.27, `\\chi^2(1)`). |\n| **Weak Exogeneity of Spot Price (`H_0: \\alpha_s=0`)** | Not tested (model misspecified). | **Not Rejected**. (LR stat = 1.36, `\\chi^2(1)`). |\n| **Weak Exogeneity of Futures Price (`H_0: \\alpha_f=0`)** | Not tested (model misspecified). | **Rejected**. (LR stat = 31.47, `\\chi^2(1)`). |\n\n### Question\n\nAccording to the evidence for the Trivariate Model in **Table 1**, select all conclusions that are supported by its results.", "Options": {"A": "The model is superior because it yields a stable cointegrating relationship that is also consistent with the theoretically expected `[1, -1]` symmetry between spot and futures prices.", "B": "The weak exogeneity test results imply a 'leader-follower' dynamic where the spot market incorporates new information first, and the futures market adjusts to restore equilibrium.", "C": "The failure to reject the weak exogeneity of both the spot price and the futures price implies that price discovery occurs simultaneously in both markets.", "D": "The futures price is found to be weakly exogenous, indicating it is the primary source of new information and drives the price discovery process."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the evidence supporting the preferred model and interpret its causal implications. It uses the **Premise/Assumption Packaging** strategy, asking for all valid conclusions from the trivariate analysis. Options A and B correctly summarize the model's success and the price discovery finding. Distractor C is a **Conceptual Opposite**, reversing the roles of the spot and futures markets. Distractor D is a factual error, as the weak exogeneity of the futures price is strongly rejected, making the premise of the statement false.", "qid": "399", "question": "### Background\n\n**Research Question.** The efficiency of the S&P 500 futures market can be tested under two competing hypotheses: the simple unbiasedness hypothesis, which posits a direct cointegrating relationship between spot and futures prices, and the cost-of-carry model, which argues for a trivariate cointegrating relationship that includes a measure of financing costs. This investigation seeks to determine which model is empirically supported by analyzing the stability of the estimated relationships and the validity of theoretical parameter restrictions.\n\n**Setting / Data-Generating Environment.** A Vector Error Correction Model (VECM) framework is applied to daily S&P 500 spot prices (`s_t`), futures prices (`f_t`), and the 3-month Treasury bill rate (`r_t`) from April 1982 to June 2003. The analysis proceeds in three stages: (1) estimating a bivariate model for `(s_t, f_t)`, (2) estimating a trivariate model for `(s_t, f_t, r_t)`, and (3) testing for weak exogeneity within the preferred model to determine the locus of price discovery.\n\n### Data / Model Specification\n\nThe core of the analysis rests on the VECM representation of a system of `p` variables `X_t`:\n  \n\\Delta X_{t}=\\alpha \\beta' X_{t-1} + \\sum_{j=1}^{k-1} \\Gamma_j \\Delta X_{t-j} + \\mu + \\delta t + \\varepsilon_t \\quad \\text{(Eq. (1))}\n \nwhere `\\beta` contains the `r` long-run cointegrating vectors and `\\alpha` contains the adjustment coefficients. A variable `i` is weakly exogenous if its corresponding adjustment coefficient, `\\alpha_i`, is zero.\n\n**Table 1: Summary of Empirical Findings for Bivariate and Trivariate Models**\n\n| Feature | Bivariate Model (`s_t`, `f_t`) | Trivariate Model (`s_t`, `f_t`, `r_t`) |\n| :--- | :--- | :--- |\n| **Cointegrating Rank** | `r=1` is found (Trace stat = 44.59 > 25.47 crit.) | `r=1` is found (Trace stat = 25.08 ≈ 25.47 crit.) |\n| **Parameter Stability** | **Rejected**. The relationship is unstable over the sample. | **Not Rejected**. The relationship is stable over the sample. |\n| **Symmetry Test (`H_0: \\beta` contains `[1, -1]`)** | **Rejected**. (LR stat = 5.44, `\\chi^2(1)`). | **Not Rejected**. (LR stat = 0.27, `\\chi^2(1)`). |\n| **Weak Exogeneity of Spot Price (`H_0: \\alpha_s=0`)** | Not tested (model misspecified). | **Not Rejected**. (LR stat = 1.36, `\\chi^2(1)`). |\n| **Weak Exogeneity of Futures Price (`H_0: \\alpha_f=0`)** | Not tested (model misspecified). | **Rejected**. (LR stat = 31.47, `\\chi^2(1)`). |\n\n### Question\n\nAccording to the evidence for the Trivariate Model in **Table 1**, select all conclusions that are supported by its results.", "model_evaluation_result": {"model_raw_response": "\n\nA,B\n\n## Explanation\n\nThe reasoning process evaluates the Trivariate Model results from Table 1 against each option to identify supported conclusions.\n\nFirst, option A is assessed: The table indicates parameter stability is not rejected, confirming a stable cointegrating relationship, and the symmetry test (LR stat = 0.27) fails to reject the [1, -1] relationship, aligning with theoretical expectations. This supports the model's superiority over the bivariate model, which shows instability and rejection of symmetry.\n\nNext, option B is examined: Weak exogeneity of the spot price is not rejected (LR stat = 1.36), while it is rejected for the futures price (LR stat = 31.47). This implies the spot price does not adjust to deviations from equilibrium, positioning it as the leader that incorporates new information first, with the futures price adjusting to restore balance, consistent with a leader-follower dynamic.\n\nOption C is then considered: The table shows weak exogeneity is not rejected for the spot price but rejected for the futures price, contradicting the requirement for both to be weakly exogenous, which would indicate simultaneous price\n> 🔍 **Analyzing financial data**\n> ***- The trivariate model shows a stable cointegrating relationship, unlike the bivariate model.***\n discovery. Thus, this option is not supported.\n\nFinally, option D is reviewed: The results reject weak exogeneity for the futures price, indicating it adjusts to equilibrium errors rather than being the primary source of new information, so this conclusion is unsupported.\n\nBased on this evaluation, only options A and B are supported by the evidence.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 461, "Question": "### Background\n\n**Research Question.** How can stress tests that focus solely on the banking sector misrepresent systemic risk by ignoring contagion from the non-bank financial sector, such as investment funds?\n\n**Setting.** A regulator conducts a stress test on a representative bank. The analysis compares a 'banks-only' scenario with an 'integrated' scenario that also models the behavior of investment funds holding similar assets, specifically government bonds.\n\n**Variables and Parameters.**\n- `A`: Total assets.\n- `L`: Total liabilities.\n- `E`: Equity capital (`E = A - L`).\n- `CR`: Capital Ratio (`CR = E / A`).\n- `A_{GB}`: Assets held in government bonds.\n- `A_{Loans}`: Assets held in loans.\n- `ε_L`: Loss rate on the loan portfolio under stress (dimensionless).\n- `ε_{GB}`: Loss rate on the government bond portfolio due to fire sales (dimensionless).\n\n---\n\n### Data / Model Specification\n\nConsider the stylized balance sheet of a representative bank before the stress test.\n\n**Table 1: Representative Bank Balance Sheet (Pre-Stress)**\n\n| Assets | Value | Liabilities & Equity | Value |\n| :--- | :--- | :--- | :--- |\n| Government Bonds | 400 | Deposits | 920 |\n| Loans | 600 | Equity | 80 |\n| **Total Assets** | **1000** | **Total Liab. & Equity** | **1000** |\n\n**Scenario 1 (Banks-Only):** A severe recession is simulated. The shock is a direct credit loss on the bank's loan portfolio. The loss rate `ε_L` is 10%.\n\n**Scenario 2 (Integrated):** The same recession also causes massive outflows from investment funds. These funds are forced to sell their large holdings of government bonds, the same type held by the bank. This fire sale causes the market price of these bonds to fall. The loss rate `ε_{GB}` is 15%.\n\n---\n\n### Question\n\nBased on the provided data and scenarios, select ALL of the following statements that are true.", "Options": {"A": "The bank's initial capital ratio is 8.0%.", "B": "Under the 'Banks-Only' scenario (Scenario 1), the bank's capital ratio falls to approximately 2.13%, and the bank remains solvent.", "C": "The additional fire-sale shock from investment funds reduces the bank's final capital ratio by approximately one percentage point, a magnitude consistent with the general findings of Sydow et al.", "D": "In the integrated scenario (combining Scenario 1 and 2), the bank becomes insolvent with a final capital ratio of approximately -4.55%."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Computational Judgment strategy to test the core calculations from the original QA problem. The options are designed using atomic decomposition, breaking the original multi-part question into independently verifiable statements. The distractor (C) is a conceptual error that tempts the user to apply a qualitative finding from the paper to a stylized example where the effect is deliberately amplified, testing whether they performed the calculation or are relying on memory.", "qid": "461", "question": "### Background\n\n**Research Question.** How can stress tests that focus solely on the banking sector misrepresent systemic risk by ignoring contagion from the non-bank financial sector, such as investment funds?\n\n**Setting.** A regulator conducts a stress test on a representative bank. The analysis compares a 'banks-only' scenario with an 'integrated' scenario that also models the behavior of investment funds holding similar assets, specifically government bonds.\n\n**Variables and Parameters.**\n- `A`: Total assets.\n- `L`: Total liabilities.\n- `E`: Equity capital (`E = A - L`).\n- `CR`: Capital Ratio (`CR = E / A`).\n- `A_{GB}`: Assets held in government bonds.\n- `A_{Loans}`: Assets held in loans.\n- `ε_L`: Loss rate on the loan portfolio under stress (dimensionless).\n- `ε_{GB}`: Loss rate on the government bond portfolio due to fire sales (dimensionless).\n\n---\n\n### Data / Model Specification\n\nConsider the stylized balance sheet of a representative bank before the stress test.\n\n**Table 1: Representative Bank Balance Sheet (Pre-Stress)**\n\n| Assets | Value | Liabilities & Equity | Value |\n| :--- | :--- | :--- | :--- |\n| Government Bonds | 400 | Deposits | 920 |\n| Loans | 600 | Equity | 80 |\n| **Total Assets** | **1000** | **Total Liab. & Equity** | **1000** |\n\n**Scenario 1 (Banks-Only):** A severe recession is simulated. The shock is a direct credit loss on the bank's loan portfolio. The loss rate `ε_L` is 10%.\n\n**Scenario 2 (Integrated):** The same recession also causes massive outflows from investment funds. These funds are forced to sell their large holdings of government bonds, the same type held by the bank. This fire sale causes the market price of these bonds to fall. The loss rate `ε_{GB}` is 15%.\n\n---\n\n### Question\n\nBased on the provided data and scenarios, select ALL of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "A,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 360, "Question": "### Background\n\n**Research Question.** Does a bad manager's incentive to gamble persist in a more realistic setting with decreasing returns to scale (DRS) and an objective of maximizing assets under management (AUM)?\n\n**Setting and Environment.** The baseline model is extended to incorporate DRS, where a manager's expected return `μ` is a decreasing linear function of the fund's AUM, `s`. The manager's objective is no longer to simply attract capital, but to maximize the expected AUM, `E[s]`. The good manager is assumed to choose the minimum feasible risk level, `σ_g^* = σ_`.\n\n**Variables and Parameters.**\n\n*   `s`: Amount of money (AUM) invested in the active fund (units of capital).\n*   `μ_g(s) = μ_g_bar - (μ_g_bar - μ_o)s`: Expected return for a good manager with AUM `s`.\n*   `μ_b(s) = μ_o - (μ_o - μ_b_bar)s`: Expected return for a bad manager with AUM `s`.\n*   `ψ`: Prior probability of a good manager (dimensionless).\n*   `σ_`: Minimum feasible risk (dimensionless).\n*   `σ_b^*`: Equilibrium risk choice for the bad manager (dimensionless).\n\n---\n\n### Data / Model Specification\n\nNumerical simulations produce the following equilibrium choices for the bad manager's risk, `σ_b^*`:\n\n**Table 1: Equilibrium `σ_b^*` under Decreasing Returns to Scale**\n\n| `ψ` (Prior Good) | `σ_`=0.03 | `σ_`=0.05 | `σ_`=0.07 | `σ_`=0.09 |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A: High Skill Gap (`μ_g_bar`=1.2, `μ_b_bar`=1.05, `μ_o`=1.1)** | | | | |\n| 0.3 | 6.15 | 6.92 | 7.83 | 9.64 |\n| 0.5 | 5.41 | 6.27 | 7.70 | 9.38 |\n| 0.7 | 5.37 | 6.40 | 7.81 | 9.52 |\n| **Panel B: Low Skill Gap (`μ_g_bar`=1.1, `μ_b_bar`=1.0, `μ_o`=1.05)** | | | | |\n| 0.3 | 4.92 | 5.88 | 7.42 | 9.26 |\n| 0.5 | 4.68 | 5.54 | 7.22 | 9.15 |\n| 0.7 | 4.91 | 5.88 | 7.41 | 9.27 |\n\n*Note: In all simulations, the good manager's equilibrium choice is `σ_g^* = σ_`.* \n\n---\n\n### Question\n\nBased on the numerical results in **Table 1**, select **all** statements that are supported by the data.", "Options": {"A": "For any given prior belief `ψ`, the bad manager's optimal risk `σ_b^*` increases as the good manager's minimum risk `σ_` increases.", "B": "For any given minimum risk `σ_`, the bad manager's optimal risk `σ_b^*` is monotonically decreasing as the prior probability of a good manager `ψ` increases.", "C": "In all scenarios presented, the bad manager's equilibrium risk choice `σ_b^*` is substantially larger than the minimum feasible risk `σ_`, supporting the robustness of the 'gambling' incentive under decreasing returns to scale.", "D": "A smaller skill gap (Panel B vs. Panel A) incentivizes the bad manager to take on more risk to compensate for their smaller intrinsic advantage."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the **Atomic Decomposition** strategy to test multiple core findings from the simulation table. It assesses **Computational Judgment** by requiring direct interpretation of the table to verify the robustness of the gambling equilibrium (A) and a key comparative static (C). Distractor B targets a common error of misinterpreting a non-monotonic relationship as monotonic. Distractor D presents a plausible but incorrect economic intuition, serving as a **Conceptual Opposite** error.", "qid": "360", "question": "### Background\n\n**Research Question.** Does a bad manager's incentive to gamble persist in a more realistic setting with decreasing returns to scale (DRS) and an objective of maximizing assets under management (AUM)?\n\n**Setting and Environment.** The baseline model is extended to incorporate DRS, where a manager's expected return `μ` is a decreasing linear function of the fund's AUM, `s`. The manager's objective is no longer to simply attract capital, but to maximize the expected AUM, `E[s]`. The good manager is assumed to choose the minimum feasible risk level, `σ_g^* = σ_`.\n\n**Variables and Parameters.**\n\n*   `s`: Amount of money (AUM) invested in the active fund (units of capital).\n*   `μ_g(s) = μ_g_bar - (μ_g_bar - μ_o)s`: Expected return for a good manager with AUM `s`.\n*   `μ_b(s) = μ_o - (μ_o - μ_b_bar)s`: Expected return for a bad manager with AUM `s`.\n*   `ψ`: Prior probability of a good manager (dimensionless).\n*   `σ_`: Minimum feasible risk (dimensionless).\n*   `σ_b^*`: Equilibrium risk choice for the bad manager (dimensionless).\n\n---\n\n### Data / Model Specification\n\nNumerical simulations produce the following equilibrium choices for the bad manager's risk, `σ_b^*`:\n\n**Table 1: Equilibrium `σ_b^*` under Decreasing Returns to Scale**\n\n| `ψ` (Prior Good) | `σ_`=0.03 | `σ_`=0.05 | `σ_`=0.07 | `σ_`=0.09 |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A: High Skill Gap (`μ_g_bar`=1.2, `μ_b_bar`=1.05, `μ_o`=1.1)** | | | | |\n| 0.3 | 6.15 | 6.92 | 7.83 | 9.64 |\n| 0.5 | 5.41 | 6.27 | 7.70 | 9.38 |\n| 0.7 | 5.37 | 6.40 | 7.81 | 9.52 |\n| **Panel B: Low Skill Gap (`μ_g_bar`=1.1, `μ_b_bar`=1.0, `μ_o`=1.05)** | | | | |\n| 0.3 | 4.92 | 5.88 | 7.42 | 9.26 |\n| 0.5 | 4.68 | 5.54 | 7.22 | 9.15 |\n| 0.7 | 4.91 | 5.88 | 7.41 | 9.27 |\n\n*Note: In all simulations, the good manager's equilibrium choice is `σ_g^* = σ_`.* \n\n---\n\n### Question\n\nBased on the numerical results in **Table 1**, select **all** statements that are supported by the data.", "model_evaluation_result": {"model_raw_response": "A,C", "model_answer": ["A", "C"], "error": null}}
{"ID": 465, "Question": "### Background\n\n**Research Question.** In a financial system vulnerable to fire-sale contagion, what is the minimum amount of capital that a regulator must inject to halt the contagion, and can this amount be determined analytically?\n\n**Setting.** A fire-sale contagion process is modeled in discrete rounds. An initial shock causes some banks to sell assets (`S_1`). These sales depress prices, causing mark-to-market losses (`ΔA_t`) on other banks, which in turn leads to further sales (`S_{t+1}`).\n\n**Data / Model Specification.**\nThe propagation of fire sales is modeled by the linear relationship:\n\n  \nS_{t+1} = \\beta (\\lambda S_t) = \\phi S_t \\quad \\text{(Eq. (1))}\n \n\nwhere `λ` is the price impact of sales, `β` is the sensitivity of sales to losses, and `φ = βλ` is the **contagion coefficient**.\n\nA regulator can make a capital injection `K` to prevent the fire sale from starting. The optimal (minimum) injection `K*` is the amount just sufficient to prevent the first round of sales (`S_1=0`).\n\n---\n\n### Question\n\nBased on the provided fire-sale model, select ALL of the following statements that are correct.", "Options": {"A": "The minimum capital injection `k_i` needed to restore a single distressed bank to the regulatory leverage threshold is equal to the full amount of that bank's asset loss, `ΔA_{i,0}`.", "B": "The fire-sale spiral is self-sustaining and explosive if the contagion coefficient `φ = βλ` is greater than or equal to 1.", "C": "As the contagion coefficient `φ` approaches 1 from below, the total cumulative asset sales `S_total` from an initial shock `S_1` approaches infinity.", "D": "The total optimal injection `K*` is typically smaller than the total initial loss `ΔA_0` because the injection surgically targets only the 'excess leverage' of distressed banks, while well-capitalized banks can absorb their losses without intervention."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests the understanding of a dynamic fire-sale model and the logic of optimal bailouts. It uses an Atomic Decomposition strategy to test insights from all parts of the original QA. The distractor (B) is a high-fidelity 'Conceptual Opposite' that targets a critical and common misconception about the size of required capital injections, making it highly discriminative.", "qid": "465", "question": "### Background\n\n**Research Question.** In a financial system vulnerable to fire-sale contagion, what is the minimum amount of capital that a regulator must inject to halt the contagion, and can this amount be determined analytically?\n\n**Setting.** A fire-sale contagion process is modeled in discrete rounds. An initial shock causes some banks to sell assets (`S_1`). These sales depress prices, causing mark-to-market losses (`ΔA_t`) on other banks, which in turn leads to further sales (`S_{t+1}`).\n\n**Data / Model Specification.**\nThe propagation of fire sales is modeled by the linear relationship:\n\n  \nS_{t+1} = \\beta (\\lambda S_t) = \\phi S_t \\quad \\text{(Eq. (1))}\n \n\nwhere `λ` is the price impact of sales, `β` is the sensitivity of sales to losses, and `φ = βλ` is the **contagion coefficient**.\n\nA regulator can make a capital injection `K` to prevent the fire sale from starting. The optimal (minimum) injection `K*` is the amount just sufficient to prevent the first round of sales (`S_1=0`).\n\n---\n\n### Question\n\nBased on the provided fire-sale model, select ALL of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "A,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 425, "Question": "### Background\n\n**Research Question.** This case examines the structure of the intertemporal stock-to-bond volatility (ISBV) relation across different bond maturities and potential econometric challenges to its identification.\n\n### Data / Model Specification\n\nThe primary predictive regression model is:\n\n  \n\\sigma_{t,t+21}^{TmSt} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TmSt} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Predictive Regression Results for Bond Futures Volatility (Full Period 1997:10-2013:06)**\n\n| Panel | Dependent Var. | `\\hat{\\gamma}_{2}` (t-stat) |\n| :--- | :--- | :--- |\n| A | `\\sigma^{TB}` (30-yr) | 0.189 (5.92) |\n| B | `\\sigma^{TN}` (10-yr) | 0.154 (4.05) |\n\n*Note: All results are for the full specification in Eq. (1).*\n\n### Question\n\nWhich of the following conclusions about the structure of volatility spillovers and potential econometric issues are supported by the evidence and model specification? Select all that apply.", "Options": {"A": "A key threat to identifying the unique effect of `\\gamma_{2}` would be strong multicollinearity between stock volatility (`\\sigma^{ST}`) and the yield curve state variables (`PrComp`), as they may be driven by a common risk factor.", "B": "The use of heteroscedasticity- and autocorrelation-consistent (HAC) standard errors in the estimation of Eq. (1) eliminates any potential problems caused by multicollinearity among the regressors.", "C": "The results in Table 1 imply that volatility spillovers from the stock market are uniform across the entire yield curve.", "D": "The larger `\\hat{\\gamma}_{2}` coefficient for 30-year bonds compared to 10-year bonds suggests that the long end of the yield curve is more sensitive to volatility spillovers from the equity market."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the more advanced interpretations from the 'Apex' part of the original QA. It uses an Atomic Decomposition strategy to test two distinct concepts: the term structure of spillovers (Q3a from source) and a key econometric challenge (Q3b from source). Correct option A correctly interprets the difference in coefficients across maturities. Correct option B identifies multicollinearity as a valid threat to identification. Distractor C is a direct contradiction of the evidence in Table 1. Distractor D confuses the purpose of HAC standard errors (for serial correlation/heteroscedasticity) with the problem of multicollinearity.", "qid": "425", "question": "### Background\n\n**Research Question.** This case examines the structure of the intertemporal stock-to-bond volatility (ISBV) relation across different bond maturities and potential econometric challenges to its identification.\n\n### Data / Model Specification\n\nThe primary predictive regression model is:\n\n  \n\\sigma_{t,t+21}^{TmSt} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TmSt} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Predictive Regression Results for Bond Futures Volatility (Full Period 1997:10-2013:06)**\n\n| Panel | Dependent Var. | `\\hat{\\gamma}_{2}` (t-stat) |\n| :--- | :--- | :--- |\n| A | `\\sigma^{TB}` (30-yr) | 0.189 (5.92) |\n| B | `\\sigma^{TN}` (10-yr) | 0.154 (4.05) |\n\n*Note: All results are for the full specification in Eq. (1).*\n\n### Question\n\nWhich of the following conclusions about the structure of volatility spillovers and potential econometric issues are supported by the evidence and model specification? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 444, "Question": "### Background\n\n**Research Question.** How can the theoretical predictions of the maturity mismatch model—that capital flow volatility negatively impacts investment primarily in economies with underdeveloped financial markets—be tested empirically?\n\n**Setting & Data.** The analysis uses a panel dataset for 16 emerging markets (EMs) and 11 small open advanced economies (AEs) to identify the impact of capital flow volatility on investment.\n\n### Data / Model Specification\n\n**Volatility Measure Construction.**\nThe key independent variable, capital flow volatility, is constructed in a two-step process. First, an AR(1) model is estimated on monthly gross capital inflows (`flow_{i,m}`) to extract the unpredictable component (`e_{i,m}`):\n\n  \nflow_{i,m} = \\rho_{0,i} + \\rho_{1,i} flow_{i,m-1} + e_{i,m} \\quad \\text{(Eq. (1))}\n \n\nSecond, volatility (`Vol_{i,m}`) is calculated as the 12-month trailing standard deviation of the estimated residuals (`\\widehat{e}_{i,m}`), normalized by trend GDP:\n\n  \nVol_{i,m} = \\frac{\\sqrt{\\frac{1}{12}\\sum_{j=0}^{11}\\widehat{e}_{i,m-j}^{2}}}{Trend~GDP_{i,m}} \\quad \\text{(Eq. (2))}\n \n\n**Key Empirical Results.**\nThe paper's core findings come from panel regressions of investment on lagged capital flows and their volatility. A summary of key results is provided in Table 1.\n\n**Table 1: Summary of Panel Regression Results on Investment**\n| Specification | Sample | Key Regressor | Estimated Coefficient |\n| :--- | :--- | :--- | :--- |\n| 1 | Emerging Markets | Volatility (Total Portfolio) | -1.581*** |\n| 2 | Advanced Economies | Volatility (Total Portfolio) | 0.107 |\n*Note: `***` denotes statistical significance at the 1% level.* \n\n### Question\n\nThe paper's empirical strategy relies on a specific construction of the volatility measure and a comparison between emerging and advanced economies. Based on the information provided, which of the following statements accurately describe the rationale or interpretation of this approach?", "Options": {"A": "Normalizing volatility by trend GDP (Eq. 2) is done to amplify the measure's effect in smaller economies, making it easier to achieve statistical significance.", "B": "The results primarily indicate that advanced economies are more resilient to shocks simply because their economies are larger and more diversified.", "C": "The starkly different coefficients for volatility in emerging markets (-1.581***) versus advanced economies (0.107) support the hypothesis that a structural feature, like the prevalence of maturity mismatch, is necessary for volatility to negatively impact investment.", "D": "The use of residuals from an AR(1) model (Eq. 1) is intended to isolate the unpredictable component of capital flows, which represents genuine uncertainty for firms."}, "Answer": ["C", "D"], "pi_justification": "This item assesses the student's understanding of the foundational empirical methodology. It uses an Atomic Decomposition strategy to test two key concepts: the rationale for variable construction (isolating shocks) and the interpretation of the primary cross-country evidence. Distractor C (Conceptual Opposite) presents an incorrect rationale for normalization. Distractor D (True but Irrelevant) offers a plausible but less precise explanation that misses the paper's core mechanism, testing for careful interpretation.", "qid": "444", "question": "### Background\n\n**Research Question.** How can the theoretical predictions of the maturity mismatch model—that capital flow volatility negatively impacts investment primarily in economies with underdeveloped financial markets—be tested empirically?\n\n**Setting & Data.** The analysis uses a panel dataset for 16 emerging markets (EMs) and 11 small open advanced economies (AEs) to identify the impact of capital flow volatility on investment.\n\n### Data / Model Specification\n\n**Volatility Measure Construction.**\nThe key independent variable, capital flow volatility, is constructed in a two-step process. First, an AR(1) model is estimated on monthly gross capital inflows (`flow_{i,m}`) to extract the unpredictable component (`e_{i,m}`):\n\n  \nflow_{i,m} = \\rho_{0,i} + \\rho_{1,i} flow_{i,m-1} + e_{i,m} \\quad \\text{(Eq. (1))}\n \n\nSecond, volatility (`Vol_{i,m}`) is calculated as the 12-month trailing standard deviation of the estimated residuals (`\\widehat{e}_{i,m}`), normalized by trend GDP:\n\n  \nVol_{i,m} = \\frac{\\sqrt{\\frac{1}{12}\\sum_{j=0}^{11}\\widehat{e}_{i,m-j}^{2}}}{Trend~GDP_{i,m}} \\quad \\text{(Eq. (2))}\n \n\n**Key Empirical Results.**\nThe paper's core findings come from panel regressions of investment on lagged capital flows and their volatility. A summary of key results is provided in Table 1.\n\n**Table 1: Summary of Panel Regression Results on Investment**\n| Specification | Sample | Key Regressor | Estimated Coefficient |\n| :--- | :--- | :--- | :--- |\n| 1 | Emerging Markets | Volatility (Total Portfolio) | -1.581*** |\n| 2 | Advanced Economies | Volatility (Total Portfolio) | 0.107 |\n*Note: `***` denotes statistical significance at the 1% level.* \n\n### Question\n\nThe paper's empirical strategy relies on a specific construction of the volatility measure and a comparison between emerging and advanced economies. Based on the information provided, which of the following statements accurately describe the rationale or interpretation of this approach?", "model_evaluation_result": {"model_raw_response": "\n\nC,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 466, "Question": "### Background\n\nAn insurer with an initial random loss `X` chooses a ceded loss function `I(X)` to transfer risk to a reinsurer. The insurer pays a premium `π_I(X)` and is left with a total post-reinsurance risk exposure, `T_I(X)`. The goal is to minimize a risk measure of this final exposure. The insurer is concerned with solvency during catastrophic events and is comparing two different risk management objectives: minimizing Value-at-Risk (VaR) versus minimizing Tail Value-at-Risk (TVaR).\n\n### Data / Model Specification\n\nThe insurer's total risk exposure is its retained loss plus the premium paid:\n\n  \nT_I(X) = X - I(X) + (1+\\theta)\\mathbb{E}[I(X)] \\quad \\text{(Eq. 1)}\n \n\nWhere `θ > 0` is the reinsurer's safety loading factor.\n\nValue-at-Risk (`VaR`) at confidence level `p` is the loss threshold that will not be exceeded with probability `p`.\n\nTail Value-at-Risk (`TVaR`) at confidence level `p` is the expected loss, conditional on the loss exceeding the `VaR` at that same confidence level `p`.\n\n### Question\n\nFrom a risk management perspective, which of the following statements accurately contrast the implications of a VaR-minimizing strategy versus a TVaR-minimizing strategy for the insurer? Select all that apply.", "Options": {"A": "If the insurer's objective switches from minimizing VaR to minimizing TVaR, the optimal upper limit of reinsurance coverage (`b*` in a two-layer contract `I(x) = (x-a)_+ - (x-b)_+`) would tend to increase to provide more protection against extreme tail events.", "B": "A TVaR-minimizing strategy is considered more conservative because it accounts for the average severity of losses in the tail, whereas a VaR-minimizing strategy is indifferent to the magnitude of losses beyond the `p`-quantile.", "C": "A VaR-minimizing strategy might lead to an optimal contract that leaves the insurer exposed to rare but enormous losses, as long as the `p`-quantile of the total risk exposure is minimized.", "D": "The optimal ceded loss function `I*(X)` under VaR minimization will always cede more of the extreme tail losses (for very large `X`) compared to the `I*(X)` under TVaR minimization."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the core conceptual comparison between VaR and TVaR, which is central to the paper's motivation. It uses a 'Scenario Application' strategy by asking for the practical implications of choosing one risk measure over the other. Option A correctly defines the key difference. Option B correctly applies this logic to contract design, predicting the effect on the coverage limit. Option D correctly identifies a major weakness of the VaR framework. Option C is a 'Conceptual Opposite' distractor; TVaR, not VaR, incentivizes ceding more of the extreme tail.", "qid": "466", "question": "### Background\n\nAn insurer with an initial random loss `X` chooses a ceded loss function `I(X)` to transfer risk to a reinsurer. The insurer pays a premium `π_I(X)` and is left with a total post-reinsurance risk exposure, `T_I(X)`. The goal is to minimize a risk measure of this final exposure. The insurer is concerned with solvency during catastrophic events and is comparing two different risk management objectives: minimizing Value-at-Risk (VaR) versus minimizing Tail Value-at-Risk (TVaR).\n\n### Data / Model Specification\n\nThe insurer's total risk exposure is its retained loss plus the premium paid:\n\n  \nT_I(X) = X - I(X) + (1+\\theta)\\mathbb{E}[I(X)] \\quad \\text{(Eq. 1)}\n \n\nWhere `θ > 0` is the reinsurer's safety loading factor.\n\nValue-at-Risk (`VaR`) at confidence level `p` is the loss threshold that will not be exceeded with probability `p`.\n\nTail Value-at-Risk (`TVaR`) at confidence level `p` is the expected loss, conditional on the loss exceeding the `VaR` at that same confidence level `p`.\n\n### Question\n\nFrom a risk management perspective, which of the following statements accurately contrast the implications of a VaR-minimizing strategy versus a TVaR-minimizing strategy for the insurer? Select all that apply.", "model_evaluation_result": {"model_raw_response": "\n\nA,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 454, "Question": "### Background\n\nThis paper investigates whether a firm's reluctance to cut dividends, a behavior termed \"dividend stickiness,\" serves as a credible signal to investors about the firm's future earnings prospects. The study proposes that managers who are confident in their firm's future earnings are more likely to avoid cutting dividends even when current earnings fall. This behavior, if understood by the market, should be associated with higher future earnings persistence, stronger future earnings growth, and a higher valuation multiple placed on the firm's current earnings.\n\n### Data / Model Specification\n\nThe study tests three main hypotheses using regression analysis:\n1.  **Earnings Persistence (H1)**: Tested with the model:\n      \n    EPS_{i,t+1}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}DS_{i,t}+\\alpha_{3}EPS_{i,t}*DS_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n     \n    **Eq. (1)**\n2.  **Earnings Growth (H2)**: Tested with the model:\n      \n    EG_{i,(t+1,t)}=\\alpha_{0}+\\alpha_{1}DS_{i,t}+\\alpha_{2}Size_{i,t}+\\alpha_{3}ROA_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n     \n    **Eq. (2)**\n3.  **Pricing Multiples (H3)**: Tested with an Ohlson valuation model:\n      \n    P_{i,t}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}BV_{i,t}+\\alpha_{3}DS_{i,t-1}+\\alpha_{4}EPS_{i,t}*DS_{i,t-1} + \\text{Controls} + \\varepsilon_{i,t}\n     \n    **Eq. (3)**\n\nIn the models, `DS_Dummy` is a binary variable equal to 1 for firms with sticky dividends.\n\n**Table 1: Asymmetric Response of Dividends to Earnings Change (Total Sample)**\n\n| Earnings change | Median of dividend change (%) |\n| :--- | :--- |\n| Increase (+) | 0.678 |\n| Decrease (-) | -0.389 |\n\n**Table 2: Impact on Earnings Persistence (H1)**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.675*** |\n| DS_Dummyt | 0.207*** |\n| **EPS\\*DS_Dummy** | **0.031*** |\n\n**Table 3: Impact on Earnings Growth (H2)**\n\n| Variable | (1) 1 year |\n| :--- | :--- |\n| **DS_Dummy** | **0.078*** |\n| Size | 0.049*** |\n\n**Table 4: Impact on Pricing Multiples (H3)**\n\n| Variable | (1) DS_dummy |\n| :--- | :--- |\n| **EPSt** | **5.041*** |\n| BVt | 1.582*** |\n| **EPS\\*DSt-1** | **0.834** |\n\n\n### Question\n\nThe paper tests three hypotheses regarding the signaling content of dividend stickiness. Based on the provided definitions and regression results, which of the following empirical findings are directly supported by the tables? Select all that apply.", "Options": {"A": "The evidence supports Hypothesis 1, showing that the earnings persistence for firms with sticky dividends is significantly higher than for firms without.", "B": "The regression results for Hypothesis 3 indicate that the market values the earnings of firms with sticky dividends more highly, with the earnings multiple increasing by approximately 16.5% for such firms.", "C": "The total sample data shows a strong asymmetry: the median dividend increase for a 1% earnings rise (0.678%) is substantially larger in magnitude than the median dividend decrease for a 1% earnings fall (-0.389%).", "D": "The `DS_Dummy` variable has a direct, positive, and significant effect on one-year-ahead earnings (`EPSt+1`), independent of current earnings levels."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the comprehensive understanding of the paper's central empirical results across all three of its main hypotheses. It uses an Atomic Decomposition strategy, presenting key findings as separate statements. The item requires both calculation/estimation (asymmetry in A, percentage increase in B) and careful interpretation of regression coefficients (D). The distractor (C) is a 'True but Misleading' statement; while the coefficient on DS_Dummyt in Table 2 is significant, the core test of H1 relies on the interaction term, assessing a reader's focus on the paper's central argument.", "qid": "454", "question": "### Background\n\nThis paper investigates whether a firm's reluctance to cut dividends, a behavior termed \"dividend stickiness,\" serves as a credible signal to investors about the firm's future earnings prospects. The study proposes that managers who are confident in their firm's future earnings are more likely to avoid cutting dividends even when current earnings fall. This behavior, if understood by the market, should be associated with higher future earnings persistence, stronger future earnings growth, and a higher valuation multiple placed on the firm's current earnings.\n\n### Data / Model Specification\n\nThe study tests three main hypotheses using regression analysis:\n1.  **Earnings Persistence (H1)**: Tested with the model:\n      \n    EPS_{i,t+1}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}DS_{i,t}+\\alpha_{3}EPS_{i,t}*DS_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n     \n    **Eq. (1)**\n2.  **Earnings Growth (H2)**: Tested with the model:\n      \n    EG_{i,(t+1,t)}=\\alpha_{0}+\\alpha_{1}DS_{i,t}+\\alpha_{2}Size_{i,t}+\\alpha_{3}ROA_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n     \n    **Eq. (2)**\n3.  **Pricing Multiples (H3)**: Tested with an Ohlson valuation model:\n      \n    P_{i,t}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}BV_{i,t}+\\alpha_{3}DS_{i,t-1}+\\alpha_{4}EPS_{i,t}*DS_{i,t-1} + \\text{Controls} + \\varepsilon_{i,t}\n     \n    **Eq. (3)**\n\nIn the models, `DS_Dummy` is a binary variable equal to 1 for firms with sticky dividends.\n\n**Table 1: Asymmetric Response of Dividends to Earnings Change (Total Sample)**\n\n| Earnings change | Median of dividend change (%) |\n| :--- | :--- |\n| Increase (+) | 0.678 |\n| Decrease (-) | -0.389 |\n\n**Table 2: Impact on Earnings Persistence (H1)**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.675*** |\n| DS_Dummyt | 0.207*** |\n| **EPS\\*DS_Dummy** | **0.031*** |\n\n**Table 3: Impact on Earnings Growth (H2)**\n\n| Variable | (1) 1 year |\n| :--- | :--- |\n| **DS_Dummy** | **0.078*** |\n| Size | 0.049*** |\n\n**Table 4: Impact on Pricing Multiples (H3)**\n\n| Variable | (1) DS_dummy |\n| :--- | :--- |\n| **EPSt** | **5.041*** |\n| BVt | 1.582*** |\n| **EPS\\*DSt-1** | **0.834** |\n\n\n### Question\n\nThe paper tests three hypotheses regarding the signaling content of dividend stickiness. Based on the provided definitions and regression results, which of the following empirical findings are directly supported by the tables? Select all that apply.", "model_evaluation_result": {"model_raw_response": "A,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 441, "Question": "### Background\n\n**Research Question.** This case evaluates the empirical and economic value of the Hadamard Exponential (HE) extension to Conditional Autoregressive Wishart (CAW) models. It investigates whether the statistical significance of the HE parameter translates into superior out-of-sample performance for a practical risk management application.\n\n**Setting.** Forecasts from various covariance models for 29 DJIA stocks are evaluated. The analysis compares in-sample statistical fit with out-of-sample economic performance, measured by the risk of a Global Minimum Variance Portfolio (GMVP). The COV family of models specifies the conditional covariance matrix `S_t` directly.\n\n**Variables and Parameters.**\n*   `S_t`: The `n x n` conditional covariance matrix.\n*   `S_hat_{t,b}`: The `b`-step ahead forecast of `S_t`.\n*   `w_{t,b}`: The `n x 1` vector of GMVP weights.\n*   `j_n`: An `n x 1` vector of ones.\n*   `h`: The forecast horizon in days (1, 5, or 22).\n*   `φ_A`: The Hadamard Exponential parameter, which makes the impact of covariance shocks time-varying and dependent on past correlations.\n*   `a_i`, `b_i`: Asset-specific parameters for the rank-one (R1) models, grouped into 3 clusters.\n\n---\n\n### Data / Model Specification\n\nThe weight vector for the GMVP is computed using the forecast covariance matrix `S_hat_{t,b}`:\n  \n\\widehat{w}_{t,b} = \\frac{\\widehat{S}_{t,b}^{-1} j_n}{j_n' \\widehat{S}_{t,b}^{-1} j_n}\n\n (Eq. (1))\n \nModel performance is evaluated based on in-sample fit (Table 1) and the annualized standard deviation of the out-of-sample GMVP returns constructed using these weights (Table 2). Lower standard deviation is better.\n\n**Table 1: In-Sample Estimation Results for COV Models (29 stocks)**\n| Parameters | S       | S-Pt    | S-Rt    | R1      | R1-Pt   | R1-Rt   |\n|:-----------|:--------|:--------|:--------|:--------|:--------|:--------|\n| **ΦA**     |         | **0.061**   | **0.072**   |         | **0.055**   | **0.070**   |\n|            |         | **(0.015)** | **(0.012)** |         | **(0.014)** | **(0.013)** |\n| Log-lik    | -93522.1| -93464.4| -93428.6| -93485.7| -93442.2| -93409.4|\n\n*Note: Robust standard errors in parentheses. S-Pt/Rt are scalar HE models; R1-Pt/Rt are rank-one HE models.* \n\n**Table 2: Annualized Standard Deviations of GMVP Returns (%)**\n| Model        | h=1  | h=5  | h=22 |\n|--------------|:----:|:----:|:----:|\n| **COV-S**    | **8.10** | **7.53** | **7.63** |\n| **COV-S-Pt** | **8.08** | **7.51** | **7.60** |\n| **COV-S-Rt** | **8.07** | **7.49** | **7.60** |\n| **EWMA**     | **8.03** | 8.05 | **7.87** |\n\n*Note: Bold values identify models in the 95% Model Confidence Set. EWMA is the Exponentially Weighted Moving Average benchmark.* \n\n---\n\n### Question\n\nBased on the in-sample estimation results in **Table 1** and the out-of-sample economic performance in **Table 2**, select ALL statements that are supported by the data.", "Options": {"A": "The COV-S-Rt model delivers the best economic performance at the 1-day forecast horizon, achieving the lowest GMVP risk of all models shown in Table 2.", "B": "Despite its statistical significance, the HE extension (e.g., COV-S-Rt) fails to improve upon the simple COV-S model in terms of GMVP risk at any of the reported forecast horizons.", "C": "The HE extension in the COV-S-Rt model provides a tangible, though modest, economic benefit over the standard COV-S model at the 5-day forecast horizon, reducing the GMVP standard deviation from 7.53% to 7.49%.", "D": "The Hadamard Exponential parameter (`ΦA`) is highly statistically significant in the COV-S-Rt model, with a t-statistic of 6.0, indicating strong in-sample explanatory power."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect in-sample statistical significance with out-of-sample economic performance. It uses an Atomic Decomposition strategy, breaking down the original QA problem's conclusions into independently verifiable statements. Correct options A and B test direct calculation (t-stat) and data comparison (GMVP risk reduction). Distractor C is a Conceptual Opposite, directly contradicting the data for h=1 and h=5. Distractor D is an 'Almost Right' error, as COV-S-Rt performs well at h=1 but is outperformed by the EWMA benchmark, testing for careful reading.", "qid": "441", "question": "### Background\n\n**Research Question.** This case evaluates the empirical and economic value of the Hadamard Exponential (HE) extension to Conditional Autoregressive Wishart (CAW) models. It investigates whether the statistical significance of the HE parameter translates into superior out-of-sample performance for a practical risk management application.\n\n**Setting.** Forecasts from various covariance models for 29 DJIA stocks are evaluated. The analysis compares in-sample statistical fit with out-of-sample economic performance, measured by the risk of a Global Minimum Variance Portfolio (GMVP). The COV family of models specifies the conditional covariance matrix `S_t` directly.\n\n**Variables and Parameters.**\n*   `S_t`: The `n x n` conditional covariance matrix.\n*   `S_hat_{t,b}`: The `b`-step ahead forecast of `S_t`.\n*   `w_{t,b}`: The `n x 1` vector of GMVP weights.\n*   `j_n`: An `n x 1` vector of ones.\n*   `h`: The forecast horizon in days (1, 5, or 22).\n*   `φ_A`: The Hadamard Exponential parameter, which makes the impact of covariance shocks time-varying and dependent on past correlations.\n*   `a_i`, `b_i`: Asset-specific parameters for the rank-one (R1) models, grouped into 3 clusters.\n\n---\n\n### Data / Model Specification\n\nThe weight vector for the GMVP is computed using the forecast covariance matrix `S_hat_{t,b}`:\n  \n\\widehat{w}_{t,b} = \\frac{\\widehat{S}_{t,b}^{-1} j_n}{j_n' \\widehat{S}_{t,b}^{-1} j_n}\n\n (Eq. (1))\n \nModel performance is evaluated based on in-sample fit (Table 1) and the annualized standard deviation of the out-of-sample GMVP returns constructed using these weights (Table 2). Lower standard deviation is better.\n\n**Table 1: In-Sample Estimation Results for COV Models (29 stocks)**\n| Parameters | S       | S-Pt    | S-Rt    | R1      | R1-Pt   | R1-Rt   |\n|:-----------|:--------|:--------|:--------|:--------|:--------|:--------|\n| **ΦA**     |         | **0.061**   | **0.072**   |         | **0.055**   | **0.070**   |\n|            |         | **(0.015)** | **(0.012)** |         | **(0.014)** | **(0.013)** |\n| Log-lik    | -93522.1| -93464.4| -93428.6| -93485.7| -93442.2| -93409.4|\n\n*Note: Robust standard errors in parentheses. S-Pt/Rt are scalar HE models; R1-Pt/Rt are rank-one HE models.* \n\n**Table 2: Annualized Standard Deviations of GMVP Returns (%)**\n| Model        | h=1  | h=5  | h=22 |\n|--------------|:----:|:----:|:----:|\n| **COV-S**    | **8.10** | **7.53** | **7.63** |\n| **COV-S-Pt** | **8.08** | **7.51** | **7.60** |\n| **COV-S-Rt** | **8.07** | **7.49** | **7.60** |\n| **EWMA**     | **8.03** | 8.05 | **7.87** |\n\n*Note: Bold values identify models in the 95% Model Confidence Set. EWMA is the Exponentially Weighted Moving Average benchmark.* \n\n---\n\n### Question\n\nBased on the in-sample estimation results in **Table 1** and the out-of-sample economic performance in **Table 2**, select ALL statements that are supported by the data.", "model_evaluation_result": {"model_raw_response": "C,D", "model_answer": ["C", "D"], "error": null}}
{"ID": 459, "Question": "### Background\n\n**Research Question.** This study investigates whether market-assessed sovereign risk (bond spreads) and agency-assessed sovereign risk (credit ratings) are driven by the same factors, specifically testing the differential impact of global investor risk appetite.\n\n**Setting / Data-Generating Environment.** The analysis uses a quarterly panel dataset for five emerging market countries from 1992-1997. The core methodology is a dynamic panel regression with country fixed effects, estimated for two different dependent variables: market-based Brady bond spreads and agency-based credit ratings.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t` (basis points).\n- `IIR`\\u208_it\\u208_: Institutional Investor Sovereign Ratings for country `i` at quarter `t` (ratings scale).\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t` (dimensionless index).\n- Pure Contagion: The portion of co-movement in sovereign risk premia that cannot be explained by changes in economic fundamentals or other common shocks.\n\n---\n\n### Data / Model Specification\n\nThe paper's central argument is that the Risk Appetite Index (RAI) explains both the divergence between market and agency risk measures and the phenomenon of \"pure contagion.\" The core finding is that RAI significantly affects market-based Brady bond spreads (BBY) but has no statistically significant effect on agency-based ratings (IIR).\n\n---\n\nBased on this central finding, which of the following statements accurately describe the mechanisms of divergence and contagion as proposed by the paper?", "Options": {"A": "The paper concludes that contagion is primarily a 'spill-over' effect, where a crisis in one country directly worsens the economic fundamentals of another, rather than a 'pure contagion' effect.", "B": "The model supports the idea of 'pure contagion' by showing that a common global factor (RAI) can induce co-movement in the bond spreads of different countries, even if their economic fundamentals are unchanged and uncorrelated.", "C": "The model implies that agency ratings (IIR) are the primary transmission mechanism for market contagion, as they are more stable than bond spreads.", "D": "A divergence between market-based spreads (BBY) and agency ratings (IIR) is explained by their differential sensitivity; a global risk-off shock (fall in RAI) would widen spreads while leaving ratings, which are based on fundamentals, unchanged."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect empirical results to the paper's core theoretical claims about contagion and divergence. It uses a Premise/Assumption Packaging strategy. Distractors test for confusion between the paper's main point and alternative explanations of contagion (D) or a direct misreading of the results' implications (C).", "qid": "459", "question": "### Background\n\n**Research Question.** This study investigates whether market-assessed sovereign risk (bond spreads) and agency-assessed sovereign risk (credit ratings) are driven by the same factors, specifically testing the differential impact of global investor risk appetite.\n\n**Setting / Data-Generating Environment.** The analysis uses a quarterly panel dataset for five emerging market countries from 1992-1997. The core methodology is a dynamic panel regression with country fixed effects, estimated for two different dependent variables: market-based Brady bond spreads and agency-based credit ratings.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t` (basis points).\n- `IIR`\\u208_it\\u208_: Institutional Investor Sovereign Ratings for country `i` at quarter `t` (ratings scale).\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t` (dimensionless index).\n- Pure Contagion: The portion of co-movement in sovereign risk premia that cannot be explained by changes in economic fundamentals or other common shocks.\n\n---\n\n### Data / Model Specification\n\nThe paper's central argument is that the Risk Appetite Index (RAI) explains both the divergence between market and agency risk measures and the phenomenon of \"pure contagion.\" The core finding is that RAI significantly affects market-based Brady bond spreads (BBY) but has no statistically significant effect on agency-based ratings (IIR).\n\n---\n\nBased on this central finding, which of the following statements accurately describe the mechanisms of divergence and contagion as proposed by the paper?", "model_evaluation_result": {"model_raw_response": "B,D", "model_answer": ["B", "D"], "error": null}}
{"ID": 385, "Question": "### Background\n\n**Research Question.** What is the long-run equilibrium relationship between the U.S. Treasury term premium and foreign holdings, accounting for the dynamic interactions between official and private investors?\n\n**Setting.** A cointegrated Vector Autoregression (VAR) model is used to analyze the long-run equilibrium relationship among the 5-year term premium (`TP_t`), the stock of foreign official holdings (`FOI_t`), and the stock of foreign private holdings (`FPVT_t`). If the variables are cointegrated, the system can be represented as a Vector Error Correction Model (VECM).\n\n### Data / Model Specification\n\nThe VECM contains an error-correction term `\\mathbf{\\Pi} \\cdot \\mathbf{X}_{t-1}`, where `\\mathbf{X}_{t} = (TP_t, FOI_t, FPVT_t)'`. The matrix `\\mathbf{\\Pi}` can be decomposed as `\\mathbf{\\Pi} = \\alpha \\beta'`, where `\\beta'` defines the long-run equilibrium relationship and `\\alpha` is a vector of loading factors representing the speed of adjustment to deviations from this equilibrium.\n\n**Table 1. VAR Long-Run Coefficients and Adjustment Speeds (12 Lags)**\n\n| | Coefficient | t-statistic |\n| :--- | :---: | :---: |\n| **Cointegrating Vector (`\\beta`)** | | |\n| Term premium (`TP_t`) | 1 (normalized) | - |\n| Foreign official (`FOI_t`) | 0.046 | 5.782 |\n| Foreign private (`FPVT_t`) | 0.061 | 2.883 |\n| | | |\n| **Loading Factors (`\\alpha`)** | | |\n| Term premium equation | -0.481 | -5.831 |\n| Foreign official equation | -0.540 | -2.378 |\n| Foreign private equation | 0.096 | 0.274 |\n\n### Question\n\nBased on the VECM results presented in Table 1, which of the following statements accurately describe the long-run dynamics between the term premium and foreign holdings?", "Options": {"A": "The loading factor of -0.481 for the term premium equation indicates a rapid and stable adjustment process, where approximately 48% of any deviation from the long-run equilibrium is corrected in the following month.", "B": "The smaller long-run effect of foreign official inflows compared to short-run estimates is primarily because foreign *official* investors tend to reverse their initial purchases over time.", "C": "The negative loading factor for the term premium equation (-0.481) indicates an unstable, divergent path for the term premium, while the positive factor for the private holdings equation (0.096) implies stability.", "D": "The long-run equilibrium relationship is `TP_t = -0.046 * FOI_t - 0.061 * FPVT_t`, implying that in the long run, both official and private foreign holdings put similar downward pressure on the term premium."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the interpretation of a VECM, converted from a Table QA problem. It uses an **Atomic Decomposition** strategy to test understanding of the cointegrating vector, loading factors, and the economic story reconciling short- and long-run effects. **Correct options (A, B)** test the ability to formulate the long-run equation from the `β` coefficients and to interpret the `α` loading factor for speed and stability. **Distractor C (Reconciliation Error)** presents an incorrect economic mechanism for the dampened long-run effect, misattributing it to official reversals instead of private offsetting. **Distractor D (Adjustment Speed Misinterpretation)** targets a fundamental error in VECM theory, incorrectly associating a negative loading factor with instability.", "qid": "385", "question": "### Background\n\n**Research Question.** What is the long-run equilibrium relationship between the U.S. Treasury term premium and foreign holdings, accounting for the dynamic interactions between official and private investors?\n\n**Setting.** A cointegrated Vector Autoregression (VAR) model is used to analyze the long-run equilibrium relationship among the 5-year term premium (`TP_t`), the stock of foreign official holdings (`FOI_t`), and the stock of foreign private holdings (`FPVT_t`). If the variables are cointegrated, the system can be represented as a Vector Error Correction Model (VECM).\n\n### Data / Model Specification\n\nThe VECM contains an error-correction term `\\mathbf{\\Pi} \\cdot \\mathbf{X}_{t-1}`, where `\\mathbf{X}_{t} = (TP_t, FOI_t, FPVT_t)'`. The matrix `\\mathbf{\\Pi}` can be decomposed as `\\mathbf{\\Pi} = \\alpha \\beta'`, where `\\beta'` defines the long-run equilibrium relationship and `\\alpha` is a vector of loading factors representing the speed of adjustment to deviations from this equilibrium.\n\n**Table 1. VAR Long-Run Coefficients and Adjustment Speeds (12 Lags)**\n\n| | Coefficient | t-statistic |\n| :--- | :---: | :---: |\n| **Cointegrating Vector (`\\beta`)** | | |\n| Term premium (`TP_t`) | 1 (normalized) | - |\n| Foreign official (`FOI_t`) | 0.046 | 5.782 |\n| Foreign private (`FPVT_t`) | 0.061 | 2.883 |\n| | | |\n| **Loading Factors (`\\alpha`)** | | |\n| Term premium equation | -0.481 | -5.831 |\n| Foreign official equation | -0.540 | -2.378 |\n| Foreign private equation | 0.096 | 0.274 |\n\n### Question\n\nBased on the VECM results presented in Table 1, which of the following statements accurately describe the long-run dynamics between the term premium and foreign holdings?", "model_evaluation_result": {"model_raw_response": "A,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 450, "Question": "### Background\n\n**Research Question.** This case evaluates the derivation and practical effectiveness of different hedging strategies by comparing their ability to reduce portfolio risk.\n\n**Setting.** An investor holds a position in a cash commodity and wishes to hedge the price risk using futures. The return on a hedged portfolio is `r_{H,t} = r_{c,t} - \\beta r_{f,t}`. The performance of different strategies is measured by the resulting standard deviation of this portfolio.\n\n### Data / Model Specification\n\nThe time-varying optimal minimum-variance hedge ratio is given by `\\beta_t^* = \\operatorname{Cov}_t(r_{c,t}, r_{f,t}) / \\operatorname{Var}_t(r_{f,t})`. The paper employs a Constant Conditional Correlation (CCC) GARCH model where:\n*   `\\operatorname{Var}_t(r_{f,t}) = h_{r_f,t}^2`\n*   `\\operatorname{Cov}_t(r_{c,t}, r_{f,t}) = \\rho_{r_c r_f} h_{r_c,t} h_{r_f,t}`\n\n**Table 1. Comparison of Hedging Strategy Performance for Crude Oil**\n\n| Strategy | Hedge Ratio ($\\beta$) | Portfolio Std. Dev. |\n|:---|:---:|:---:|\n| CCC-GARCH | 0.74 (average) | 0.016322 |\n| OLS | 0.70 | 0.016416 |\n| Naïve | 1.00 | 0.018017 |\n\n*Source: Adapted from Table 5 in the source paper.*\n\n### Question\n\nBased on the theory and data provided, select all of the following statements that are correct.", "Options": {"A": "The explicit formula for the dynamic hedge ratio (`\\beta_t^*`) derived from the CCC-GARCH specification is `\\rho_{r_c r_f} (h_{r_c,t} / h_{r_f,t})`.", "B": "The primary conceptual advantage of a dynamic hedge over a static OLS hedge is its ability to adapt the hedge ratio to time-varying conditional volatilities and correlations.", "C": "For Crude Oil, the data shows that the dynamic CCC-GARCH hedge reduces portfolio risk (standard deviation) by approximately 9.4% compared to the Naïve hedge.", "D": "The data for Crude Oil demonstrates that the static OLS hedge provides a greater risk reduction than the dynamic CCC-GARCH hedge."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses 'atomic decomposition' to test multiple facets of the hedging analysis: theoretical derivation, conceptual understanding, and empirical calculation. This makes it a 'Computational Judgment' item. Option A tests the derivation of the dynamic hedge ratio formula. Option C tests the ability to calculate percentage risk reduction from the table. Option B is a conceptual distractor that is almost correct but the CCC-GARCH model assumes *constant* correlation, so it cannot adapt to time-varying correlations (Almost Right). Option D is a direct contradiction of the data in the table (Conceptual Opposite). This structure creates a robust multi-select question with two correct, independent statements.", "qid": "450", "question": "### Background\n\n**Research Question.** This case evaluates the derivation and practical effectiveness of different hedging strategies by comparing their ability to reduce portfolio risk.\n\n**Setting.** An investor holds a position in a cash commodity and wishes to hedge the price risk using futures. The return on a hedged portfolio is `r_{H,t} = r_{c,t} - \\beta r_{f,t}`. The performance of different strategies is measured by the resulting standard deviation of this portfolio.\n\n### Data / Model Specification\n\nThe time-varying optimal minimum-variance hedge ratio is given by `\\beta_t^* = \\operatorname{Cov}_t(r_{c,t}, r_{f,t}) / \\operatorname{Var}_t(r_{f,t})`. The paper employs a Constant Conditional Correlation (CCC) GARCH model where:\n*   `\\operatorname{Var}_t(r_{f,t}) = h_{r_f,t}^2`\n*   `\\operatorname{Cov}_t(r_{c,t}, r_{f,t}) = \\rho_{r_c r_f} h_{r_c,t} h_{r_f,t}`\n\n**Table 1. Comparison of Hedging Strategy Performance for Crude Oil**\n\n| Strategy | Hedge Ratio ($\\beta$) | Portfolio Std. Dev. |\n|:---|:---:|:---:|\n| CCC-GARCH | 0.74 (average) | 0.016322 |\n| OLS | 0.70 | 0.016416 |\n| Naïve | 1.00 | 0.018017 |\n\n*Source: Adapted from Table 5 in the source paper.*\n\n### Question\n\nBased on the theory and data provided, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "\n> 🔧 **Using tool:** code_execution\n> 📝 **Parameters:** code: print((0.018017 - 0.016322) / 0.018017 * 100)\n\n\nA,B,C", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 457, "Question": "### Background\nIn the paper's model, a target firm and an “insider” acquirer form a coalition to maximize their joint surplus in a future takeover contest against N “outsiders.” They achieve this by having the target sell an equity stake of size `α*` to the insider. The optimal stake size, `α*`, is determined by the trade-off between extracting surplus from outsiders and ensuring outsiders are willing to pay an investigation cost, *c*, to participate. A larger stake `α` makes the insider a more aggressive bidder, which helps the coalition extract surplus but reduces the outsiders' expected payoff, potentially violating their participation constraint.\n\n### Data / Model Specification\nThe model yields several key predictions about the optimal stake size and its pricing:\n1.  The optimal stake size, `α*`, is increasing in the probability of a takeover, *s*, and decreasing in the outsiders’ cost of investigation, *c*.\n2.  The optimal stake size, `α*`, decreases with any positive stochastic shift in the insider’s synergy value distribution, F_I(·).\n3.  Assuming disagreement between the target and insider would result in a standard second-price auction with no stake, the equity stake `α*` will always sell for a discount to the target’s post-trade market price.\n\n### Question\nAccording to the model's explanation for the stake price discount (Proposition 3), select ALL statements that are consistent with the paper's reasoning.", "Options": {"A": "A third party would be willing to pay less per share for the target's equity after the insider acquires its stake, justifying the initial discount.", "B": "The insider's overbidding provides a 'public good' to the target's original shareholders by raising the expected sale price of the firm.", "C": "The stake is sold at a discount to compensate the insider for the privately costly act of overbidding in states where it wins the auction.", "D": "The discount primarily reflects the information asymmetry between the target, which knows the takeover probability, and the insider."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the economic rationale behind the stake pricing prediction. It uses an Atomic Decomposition strategy, breaking the explanation into the insider's cost (A) and the shareholders' benefit (C). Distractors include a plausible but incorrect alternative theory from corporate finance (B: information asymmetry) and a direct contradiction of a subtle point in the text (D: third-party valuation), testing for deep comprehension beyond surface-level reading.", "qid": "457", "question": "### Background\nIn the paper's model, a target firm and an “insider” acquirer form a coalition to maximize their joint surplus in a future takeover contest against N “outsiders.” They achieve this by having the target sell an equity stake of size `α*` to the insider. The optimal stake size, `α*`, is determined by the trade-off between extracting surplus from outsiders and ensuring outsiders are willing to pay an investigation cost, *c*, to participate. A larger stake `α` makes the insider a more aggressive bidder, which helps the coalition extract surplus but reduces the outsiders' expected payoff, potentially violating their participation constraint.\n\n### Data / Model Specification\nThe model yields several key predictions about the optimal stake size and its pricing:\n1.  The optimal stake size, `α*`, is increasing in the probability of a takeover, *s*, and decreasing in the outsiders’ cost of investigation, *c*.\n2.  The optimal stake size, `α*`, decreases with any positive stochastic shift in the insider’s synergy value distribution, F_I(·).\n3.  Assuming disagreement between the target and insider would result in a standard second-price auction with no stake, the equity stake `α*` will always sell for a discount to the target’s post-trade market price.\n\n### Question\nAccording to the model's explanation for the stake price discount (Proposition 3), select ALL statements that are consistent with the paper's reasoning.", "model_evaluation_result": {"model_raw_response": "\n> 🔍 **Searching for:** \"paper coalition target insider acquirer stake alpha* takeover outsiders investigation cost c\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"Insider Trading in Takeover Targets | Request PDF\" ResearchGate](https://www.researchgate.net/publication/228273519_Insider_Trading_in_Takeover_Targets)\n> [🔗 \"Does target firm insider trading signal the target's synergy potential in mergers and acquisitions? - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304405X21002312)\n> [🔗 \"Insider Trading in Takeover Targets\" Harvard Law School Forum on Corporate Governance](https://corpgov.law.harvard.edu/2012/04/02/insider-trading-in-takeover-targets/)\n> [🔗 \"Hedge funds in M&A deals: Is there exploitation of insider information? - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0929119917301165)\n> [🔗 \"Does Target Firm Insider Trading Signal the Target’s Synergy Potential in Mergers and Acquisitions?\" Harvard Law School Forum on Corporate Governance](https://corpgov.law.harvard.edu/2021/04/02/does-target-firm-insider-trading-signal-the-targets-synergy-potential-in-mergers-and-acquisitions/)\n> [🔗 \"Insider trading in takeover targets - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0929119912000302)\n> [🔗 \"Target-initiated takeover with search frictions - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/pii/S037722172200563X)\n> [🔗 \"Insider Trading & Market Manipulation Literature Watch: Q2 2024 | Insights & Events | Charles River Associates\" Charles River Associates](https://www.crai.com/insights-events/publications/insider-trading-market-manipulation-literature-watch-q2-2024/)\n> [🔗 \"Economicliberties\" American Economic Liberties Project](http://www.economicliberties.us/wp-content/uploads/2022/12/Serial-Acquisitions-Working-Paper-R4-2.pdf)\n> [🔗 \"Insider trading in takeover targets - Anup Agrawal\" Ua](https://aagrawal.people.ua.edu/uploads/9/1/7/7/91770628/itt.pdf)\n\n> 🔍 **Analyzing financial model**\n> ***- The model shows the optimal stake size, `α*`, increases with takeover probability *s* and decreases with outsiders' investigation cost *c*.***\n\n> 🔍 **Searching for:** \"\"optimal stake size\" alpha* takeover \"investigation cost\" c insider coalition\"\n> 📊 **Results requested:** 10\n\n> ***- A larger stake `α` makes the insider bid more aggressively, extracting surplus but risking outsiders' participation.***\n\n> 🔍 **Searching for:** \"paper coalition target insider acquirer stake alpha* takeover outsiders investigation cost c\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Found 10 search results**\n\n🔗 **[Insider Trading in Takeover Targets | Request PDF](https://www.researchgate.net/publication/228273519_Insider_Trading_in_Takeover_Targets)**\n   📄 This paper investigates an unintended consequence of equity-based compensation plans: in highly volatile firms, managers appear to undermine the compe...\n   🌐 *ResearchGate*\n\n🔗 **[Does target firm insider trading signal the target's synergy potential in mergers and acquisitions? - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304405X21002312)**\n   📄 Our study extends the literature on the informational role of insider trading and the regulatory debate surrounding such trading by investigating whet...\n   🌐 *ScienceDirect*\n\n🔗 **[Insider Trading in Takeover Targets](https://corpgov.law.harvard.edu/2012/04/02/insider-trading-in-takeover-targets/)**\n   📄 In our paper, Insider Trading in Takeover Targets, forthcoming in the Journal of Corporate Finance, we provide systematic evidence on the level, patte...\n   🌐 *Harvard Law School Forum on Corporate Governance*\n\n🔗 **[Hedge funds in M&A deals: Is there exploitation of insider information? - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0929119917301165)**\n   📄 In addition, we also find that a target's price run-up, prior to a deal's public announcement date, is significantly greater for deals with greater sh...\n   🌐 *ScienceDirect*\n\n🔗 **[Does Target Firm Insider Trading Signal the Target’s Synergy Potential in Mergers and Acquisitions?](https://corpgov.law.harvard.edu/2021/04/02/does-target-firm-insider-trading-signal-the-targets-synergy-potential-in-mergers-and-acquisitions/)**\n   📄 In our paper forthcoming in the Journal of Financial Economics, we raise a question: can a firm looking for a takeover target use a target firm’s net ...\n   🌐 *Harvard Law School Forum on Corporate Governance*\n\n🔗 **[Insider trading in takeover targets - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0929119912000302)**\n   📄 These effects are even stronger in certain sub-samples with less uncertainty about takeover completion, such as friendly deals, and deals with a singl...\n   🌐 *ScienceDirect*\n\n🔗 **[Target-initiated takeover with search frictions - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S037722172200563X)**\n   📄 This paper attempts to understand such a target’s sellout timing and pricing decisions by developing a target-initiated model, in which the firm rando...\n   🌐 *ScienceDirect*\n\n🔗 **[Insider Trading & Market Manipulation Literature Watch: Q2 2024 | Insights & Events | Charles River Associates](https://www.crai.com/insights-events/publications/insider-trading-market-manipulation-literature-watch-q2-2024/)**\n   📄 Do Proprietary Costs Deter Insider Trading? Insider trading conveys insiders’ private information to outsiders. This private information potentially b...\n   🌐 *Charles River Associates*\n\n🔗 **[Economicliberties](http://www.economicliberties.us/wp-content/uploads/2022/12/Serial-Acquisitions-Working-Paper-R4-2.pdf)**\n   📄 The American Economic Liberties Project is fighting to realize economic liberty for all, in support of a secure, inclusive democratic society.\n   🌐 *American Economic Liberties Project*\n\n🔗 **[Insider trading in takeover targets - Anup Agrawal](https://aagrawal.people.ua.edu/uploads/9/1/7/7/91770628/itt.pdf)**\n   📄 Anup Agrawal is Professor and Powell Chair of Finance at the University of Alabama. His research deals with topics in Corporate Finance and Investment...\n   🌐 *Ua*\n\n\n> 🔍 **Searching for:** \"optimal stake size α* takeover outsiders investigation cost c\"\n> 📊 **Results requested:** 10\n\n> ***- Let's check how the stake price discount ties to the model's reasoning.***\n\n> 🔍 **Searching for:** \"\"optimal stake size\" alpha* takeover \"investigation cost\" c insider coalition\"\n> 📊 **Results requested:** 10\n\n> 🔍 **10 results found**\n\n> [🔗 \"When should we raise the stakes? | Professor Leighton Vaughan Williams\" Leightonvw](https://leightonvw.com/2024/10/01/2453/)\n> [🔗 \"Optimal Toeholds in Takeover Contests | Request PDF\" ResearchGate](https://www.researchgate.net/publication/223262990_Optimal_Toeholds_in_Takeover_Contests)\n> [🔗 \"Understanding Equity Stake: A Key to Business Success\" CGAA](https://www.cgaa.org/article/equity-stake)\n> [🔗 \"Equity Stake: Definition, Calculation & Acquisition\" Growth Equity Interview Guide](https://growthequityinterviewguide.com/venture-capital/startup-equity-ownership/equity-stake)\n> [🔗 \"Common stock returns in corporate takeover bids: Evidence from interstate bank mergers - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/037842669190068W)\n> [🔗 \"Takeover bids and the relative prices of shares that differ in their voting rights - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/0378426695000429)\n> [🔗 \"The Independents Takes Minority Stake in The Outsiders Perspective\" WWD](https://wwd.com/business-news/mergers-acquisitions/the-independents-the-outsiders-perspective-investment-1238114602/)\n> [🔗 \"Nber\" NBER](https://www.nber.org/system/files/working_papers/w4654/w4654.pdf)\n> [🔗 \"Optimal toeholds in takeover contests - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304405X05000358)\n> [🔗 \"Hostile takeovers or friendly mergers? Real options analysis - ScienceDirect\" ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0929119922001353)\n\n> 🔍 **Searching for:** \"\"stake discount\" takeover insider \"participation constraint\"\"\n> 📊 **Results requested:** 5\n\n> ***- The stake `α*` always sells at a discount compared to the target's post-trade market price, assuming disagreement leads to a standard second-price auction.***\n\n> 🔍 **Searching for:** \"optimal stake size α* takeover outsiders investigation cost c\"\n> 📊 **Results requested:** 10\n\n> 🔍 **Found 10 search results**\n\n🔗 **[When should we raise the stakes? | Professor Leighton Vaughan Williams](https://leightonvw.com/2024/10/01/2453/)**\n   📄 The Kelly Criterion A version of this article appears in TWISTED LOGIC: Puzzles, Paradoxes, and Big Questions. By Leighton Vaughan Williams. Chapman &...\n   🌐 *Leightonvw*\n\n🔗 **[Optimal Toeholds in Takeover Contests | Request PDF](https://www.researchgate.net/publication/223262990_Optimal_Toeholds_in_Takeover_Contests)**\n   📄 The Development of the Takeover Auction Process: The Evolution of Property Rights in the Modern Wild West ... William O. Brown ... J. Harold Mulherin ...\n   🌐 *ResearchGate*\n\n🔗 **[Understanding Equity Stake: A Key to Business Success](https://www.cgaa.org/article/equity-stake)**\n   📄 A company's distribution of equity stakes can make it easier or harder for an outsider to acquire a controlling interest in the company. Credit: youtu...\n   🌐 *CGAA*\n\n🔗 **[Equity Stake: Definition, Calculation & Acquisition](https://growthequityinterviewguide.com/venture-capital/startup-equity-ownership/equity-stake)**\n   📄 Discover the ins and outs of equity <strong>stakes</strong>, from types and valuation to acquisition methods and legal considerations. Maximize your i...\n   🌐 *Growth Equity Interview Guide*\n\n🔗 **[Common stock returns in corporate takeover bids: Evidence from interstate bank mergers - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/037842669190068W)**\n   📄 Since 1982, a large number of interstate bank mergers have taken place within the legal framework of state statutes passed in response to the Douglas ...\n   🌐 *ScienceDirect*\n\n🔗 **[Takeover bids and the relative prices of shares that differ in their voting rights - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/0378426695000429)**\n   📄 The relative prices of shares that differ only in their voting rights are modelled. In our model, voting rights become valuable when a control contest...\n   🌐 *ScienceDirect*\n\n🔗 **[The Independents Takes Minority Stake in The Outsiders Perspective](https://wwd.com/business-news/mergers-acquisitions/the-independents-the-outsiders-perspective-investment-1238114602/)**\n   📄 Prior to taking The Outsiders Perspective under its wing by acquiring a minority stake, L’Incubateur invested in Matière Noire, a Paris-based creative...\n   🌐 *WWD*\n\n🔗 **[Nber](https://www.nber.org/system/files/working_papers/w4654/w4654.pdf)**\n   📄 Outsiders know that the fimi wants to raise funds.\n   🌐 *NBER*\n\n🔗 **[Optimal toeholds in takeover contests - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0304405X05000358)**\n   📄 Implicit in this argument is that unlike shares tendered at the takeover stage, the raider owns the toehold whether the takeover is successful or not....\n   🌐 *ScienceDirect*\n\n🔗 **[Hostile takeovers or friendly mergers? Real options analysis - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0929119922001353)**\n   📄 That is, the presence of takeover costs enables us to investigate the type of equilibrium occurring in a merger. Pawlina and Kort (2006) and Kong and ...\n   🌐 *ScienceDirect*\n\n\n\nA,B,C", "model_answer": ["A", "B", "C"], "error": null}}
