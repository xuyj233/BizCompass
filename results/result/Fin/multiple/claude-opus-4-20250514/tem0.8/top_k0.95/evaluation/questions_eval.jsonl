{"ID": 105, "Question": "### Background\n\n**Research Question.** This case investigates the market's immediate reaction to the announcement of a Reverse Takeover (RT) and seeks to identify the firm- and deal-specific characteristics that explain this reaction.\n\n**Setting.** An event study is conducted on a sample of RT announcements to measure the wealth effects for the public firm's shareholders. A multivariate regression is then used to explain the cross-sectional variation in these wealth effects.\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return, the dependent variable in the regression.\n- `Cash_Assets`: The ratio of cash to total assets for the public firm, a measure of liquidity.\n- `Strength_Private`: A dummy variable, =1 if 'strength of the private firm' is cited as a reason for the RT.\n\n---\n\n### Data / Model Specification\n\nThe wealth effect of the RT announcement is measured using Cumulative Abnormal Returns (CARs) around the announcement date. A regression model is then estimated to explain these CARs.\n\n**Table 1: Cumulative Abnormal Returns (CARs) around RT Announcements**\n\n| Sample | No. | CAR (-1,+1) |\n| :--- | :-: | :--- |\n| All firms | 121 | 25.10%*** |\n| Distressed | 44 | 10.17%*** |\n| Functional | 77 | 33.64%*** |\n\n*Note: Table adapted from Table 6. `***` denotes significance at the 1% level.*\n\n**Table 2: Multivariate Regression of 3-day `(-1,+1)` CARs**\n\n| Variable | Model 2 Coefficient (t-stat) |\n| :--- | :--- |\n| Intercept | 0.0032 (2.19)** |\n| Cash to Total Assets | 0.0045 (2.69)*** |\n| Strength of Private Firm | -0.0023 (-2.72)*** |\n\n*Note: Table adapted from Model 2 of Table 7. `**`, `***` denote significance at the 5% and 1% levels, respectively. Other variables are omitted for brevity.*\n\n---\n\n### Question\n\nAccording to the event study and regression results, which of the following statements correctly describe the market's reaction to Reverse Takeover (RT) announcements? Select all that apply.", "Options": {"A": "After controlling for other factors, public firms with greater pre-merger liquidity (higher cash-to-assets ratio) experienced significantly more positive announcement returns.", "B": "The regression results suggest that when a public firm explicitly cited the 'strength of the private firm' as a rationale, the market interpreted this as a positive signal, leading to higher abnormal returns.", "C": "The market reacted more favorably to announcements from financially distressed public firms than to those from financially functional ones.", "D": "On average, the announcement of an RT was associated with a large, statistically significant positive wealth effect for the public firm's shareholders."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an Atomic Decomposition strategy to test the interpretation of both event study results and multivariate regression coefficients. It assesses whether a student can connect the univariate findings (Table 1) with the ceteris paribus relationships identified in the regression (Table 2). The distractors are designed as Conceptual Opposites of the paper's key findings, effectively discriminating between students who have understood the results and those who have not.", "qid": "105", "question": "### Background\n\n**Research Question.** This case investigates the market's immediate reaction to the announcement of a Reverse Takeover (RT) and seeks to identify the firm- and deal-specific characteristics that explain this reaction.\n\n**Setting.** An event study is conducted on a sample of RT announcements to measure the wealth effects for the public firm's shareholders. A multivariate regression is then used to explain the cross-sectional variation in these wealth effects.\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return, the dependent variable in the regression.\n- `Cash_Assets`: The ratio of cash to total assets for the public firm, a measure of liquidity.\n- `Strength_Private`: A dummy variable, =1 if 'strength of the private firm' is cited as a reason for the RT.\n\n---\n\n### Data / Model Specification\n\nThe wealth effect of the RT announcement is measured using Cumulative Abnormal Returns (CARs) around the announcement date. A regression model is then estimated to explain these CARs.\n\n**Table 1: Cumulative Abnormal Returns (CARs) around RT Announcements**\n\n| Sample | No. | CAR (-1,+1) |\n| :--- | :-: | :--- |\n| All firms | 121 | 25.10%*** |\n| Distressed | 44 | 10.17%*** |\n| Functional | 77 | 33.64%*** |\n\n*Note: Table adapted from Table 6. `***` denotes significance at the 1% level.*\n\n**Table 2: Multivariate Regression of 3-day `(-1,+1)` CARs**\n\n| Variable | Model 2 Coefficient (t-stat) |\n| :--- | :--- |\n| Intercept | 0.0032 (2.19)** |\n| Cash to Total Assets | 0.0045 (2.69)*** |\n| Strength of Private Firm | -0.0023 (-2.72)*** |\n\n*Note: Table adapted from Model 2 of Table 7. `**`, `***` denote significance at the 5% and 1% levels, respectively. Other variables are omitted for brevity.*\n\n---\n\n### Question\n\nAccording to the event study and regression results, which of the following statements correctly describe the market's reaction to Reverse Takeover (RT) announcements? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data provided:\n\nFrom Table 1:\n- All firms show CAR of 25.10%*** (significant at 1%)\n- Distressed firms: 10.17%***\n- Functional firms: 33.64%***\n\nFrom Table 2:\n- Cash to Total Assets coefficient: 0.0045*** (positive and significant)\n- Strength of Private Firm coefficient: -0.0023*** (negative and significant)\n\nAnalyzing each option:\n\nA) TRUE - The Cash to Total Assets coefficient is 0.0045 with t-stat 2.69***, showing that higher cash-to-assets ratio is associated with significantly more positive announcement returns.\n\nB) FALSE - The Strength of Private Firm coefficient is -0.0023***, which is negative and significant. This means citing 'strength of private firm' is associated with lower, not higher, abnormal returns.\n\nC) FALSE - Functional firms had CAR of 33.64% while distressed firms had only 10.17%. The market reacted more favorably to functional firms, not distressed ones.\n\nD) TRUE - The overall sample shows CAR of 25.10%***, which is a large, statistically significant positive wealth effect.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 14, "Question": "### Background\n\nThe New Basel Capital Accord proposes a hierarchy of three approaches for calculating credit risk capital: Standardized, Foundation Internal Ratings-Based (F-IRB), and Advanced Internal Ratings-Based (A-IRB). A bank's ability to use the more advanced approaches depends on supervisory approval of its internal systems.\n\n### Data / Model Specification\n\n*   **Standardized:** Banks use external ratings to determine a regulator-prescribed Risk Weight (`RW`).\n*   **F-IRB:** Banks use internal models to estimate Probability of Default (`PD`), but other parameters like Loss Given Default (`LGD`) are supplied by the supervisor.\n*   **A-IRB:** Banks use internal models to estimate all key risk components (`PD`, `LGD`, etc.).\n\nA simplified formula for required capital (`K`) under the IRB approaches is `K = EAD × LGD × f(PD, ρ)`, where `f(·)` is a regulatory function.\n\n### Question\n\nAccording to the principles outlined in the New Basel Capital Accord, which of the following are **NOT** valid characteristics or consequences of this tiered framework? (Select all that apply.)\n", "Options": {"A": "The framework creates a supervisory challenge related to model risk, where banks may be incentivized to use overly optimistic assumptions to lower their capital requirements.", "B": "The Standardized approach is considered the most risk-sensitive because it relies on objective, external credit ratings from agencies.", "C": "A bank moving from the F-IRB to the A-IRB approach gains the ability to use its own internal estimate for Probability of Default (PD).", "D": "The framework provides a direct financial incentive (capital reduction) for banks to invest in more sophisticated, risk-sensitive internal models and data systems."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the key distinctions and rationale behind the tiered Basel framework for credit risk.\nDepth Strategy: Reverse-Reasoning. The candidate must identify statements that contradict the logic of the framework.\nInversion Protocol: The core assessment objective is to identify the single most accurate description of the framework's logic. However, several options (A, C) are also valid consequences, making a 'select all correct' format difficult. The question was inverted to 'Select all that are NOT valid'. The original correct answers (A, C) become distractors, and the original distractors (B, D) become the correct answers, satisfying the ≥2 correct answer rule.\nDistractor Logic (Post-Inversion):\n- A (Now a distractor): This is a correct statement. The primary incentive for moving up the tiers is indeed capital reduction based on more risk-sensitive measures.\n- C (Now a distractor): This is a correct statement. Allowing internal models inherently creates model risk and the potential for regulatory arbitrage, a key supervisory challenge.\n- B (Now a correct answer): This is an incorrect statement. Banks *already* use their own PD under F-IRB; the move to A-IRB allows them to also use internal estimates for LGD and EAD.\n- D (Now a correct answer): This is an incorrect statement. The paper explicitly positions the internal model approaches (IRB) as *more* risk-sensitive than the broad-brush Standardized approach.", "qid": "14", "question": "### Background\n\nThe New Basel Capital Accord proposes a hierarchy of three approaches for calculating credit risk capital: Standardized, Foundation Internal Ratings-Based (F-IRB), and Advanced Internal Ratings-Based (A-IRB). A bank's ability to use the more advanced approaches depends on supervisory approval of its internal systems.\n\n### Data / Model Specification\n\n*   **Standardized:** Banks use external ratings to determine a regulator-prescribed Risk Weight (`RW`).\n*   **F-IRB:** Banks use internal models to estimate Probability of Default (`PD`), but other parameters like Loss Given Default (`LGD`) are supplied by the supervisor.\n*   **A-IRB:** Banks use internal models to estimate all key risk components (`PD`, `LGD`, etc.).\n\nA simplified formula for required capital (`K`) under the IRB approaches is `K = EAD × LGD × f(PD, ρ)`, where `f(·)` is a regulatory function.\n\n### Question\n\nAccording to the principles outlined in the New Basel Capital Accord, which of the following are **NOT** valid characteristics or consequences of this tiered framework? (Select all that apply.)\n", "model_evaluation_result": {"model_raw_response": "Looking at each option to identify which are NOT valid characteristics or consequences of the New Basel Capital Accord framework:\n\nA) This IS a valid characteristic. Model risk and the incentive for banks to use optimistic assumptions to lower capital requirements is a well-recognized supervisory challenge in the Basel framework.\n\nB) This is NOT valid. The Standardized approach is actually the LEAST risk-sensitive approach, not the most. It uses broad risk weights based on external ratings rather than bank-specific risk assessments. The IRB approaches are more risk-sensitive because they use internal models tailored to specific exposures.\n\nC) This is NOT valid. According to the background, F-IRB already allows banks to use internal models to estimate PD. The move from F-IRB to A-IRB gives banks the ability to estimate OTHER parameters like LGD internally, not PD.\n\nD) This IS a valid characteristic. The tiered framework does provide capital reduction incentives for banks to develop more sophisticated internal models, as more advanced approaches typically result in lower capital requirements for well-managed portfolios.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 94, "Question": "### Background\n\nThe author argues that mandating central clearing for Over-the-Counter (OTC) derivatives does not solve the Too-Big-To-Fail (TBTF) problem but rather transfers it to a new, potentially more dangerous entity: the Central Counterparty (CCP) itself. The implicit government guarantee to a TBTF institution is economically equivalent to providing its shareholders with a put option on its assets, where the value of this subsidy, `S`, increases with the institution's asset volatility `σ`.\n\n### Scenario\n\nA regulator is evaluating two distinct policy proposals to mitigate the moral hazard associated with TBTF institutions in the derivatives market:\n\n*   **Policy 1 (Clearing Mandate)**: Mandate that all standardized derivatives be cleared through a single, immense CCP. The goal is to eliminate the TBTF status of individual dealer banks.\n*   **Policy 2 (Direct Capital Regulation)**: Directly address TBTF by imposing significantly higher, risk-adjusted capital requirements on all large dealer banks, forcing them to reduce their leverage (debt relative to assets).\n\n### Question\n\nBased on the author's critique and the economic logic of TBTF subsidies, which of the following statements accurately describe the likely consequences of these policies?\n\nSelect all that apply.", "Options": {"A": "Policy 1 (Clearing Mandate) is likely to create a new TBTF entity in the form of the CCP, as its failure would be catastrophic for the entire financial system.", "B": "Policy 1 (Clearing Mandate) effectively eliminates the TBTF problem by mutualizing default risk across all clearing members, thereby removing the need for any government backstop.", "C": "The TBTF subsidy for the CCP created under Policy 1 is likely to be larger than the sum of the individual subsidies of the dealer banks it replaces, due to the extreme concentration of systemic risk.", "D": "Policy 2 (Direct Capital Regulation) directly mitigates the moral hazard of TBTF by reducing the value of the implicit government subsidy for dealer banks."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses the student's ability to synthesize the paper's core argument against clearing mandates as a solution for the TBTF problem. It uses a Scenario Application strategy, asking the user to evaluate the consequences of two competing policies. Correct options (A, B, D) reflect the author's central thesis: clearing mandates transfer and concentrate TBTF risk (A, D), while direct capital regulation is a more effective tool (B). The distractor (C) presents the flawed rationale for clearing mandates that the author explicitly refutes, making it a strong 'Conceptual Opposite' error.", "qid": "94", "question": "### Background\n\nThe author argues that mandating central clearing for Over-the-Counter (OTC) derivatives does not solve the Too-Big-To-Fail (TBTF) problem but rather transfers it to a new, potentially more dangerous entity: the Central Counterparty (CCP) itself. The implicit government guarantee to a TBTF institution is economically equivalent to providing its shareholders with a put option on its assets, where the value of this subsidy, `S`, increases with the institution's asset volatility `σ`.\n\n### Scenario\n\nA regulator is evaluating two distinct policy proposals to mitigate the moral hazard associated with TBTF institutions in the derivatives market:\n\n*   **Policy 1 (Clearing Mandate)**: Mandate that all standardized derivatives be cleared through a single, immense CCP. The goal is to eliminate the TBTF status of individual dealer banks.\n*   **Policy 2 (Direct Capital Regulation)**: Directly address TBTF by imposing significantly higher, risk-adjusted capital requirements on all large dealer banks, forcing them to reduce their leverage (debt relative to assets).\n\n### Question\n\nBased on the author's critique and the economic logic of TBTF subsidies, which of the following statements accurately describe the likely consequences of these policies?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze each policy option through the lens of the author's critique about TBTF problems and the economic logic of TBTF subsidies.\n\nThe key insight from the background is that the TBTF subsidy S increases with asset volatility σ, and that mandating central clearing transfers rather than solves the TBTF problem.\n\nLet me evaluate each option:\n\n**Option A**: Policy 1 creates a new TBTF entity (the CCP)\n- This aligns directly with the author's main argument that clearing mandates transfer the TBTF problem to the CCP\n- A single CCP concentrating all derivatives clearing would indeed be catastrophic if it failed\n- ✓ This is correct\n\n**Option B**: Policy 1 eliminates TBTF by mutualizing risk\n- This contradicts the author's central thesis\n- While risk is mutualized among clearing members, the CCP itself becomes the new TBTF entity\n- The need for government backstop isn't eliminated, just transferred\n- ✗ This is incorrect\n\n**Option C**: The CCP's TBTF subsidy exceeds the sum of individual bank subsidies\n- Given that S increases with volatility σ, and the CCP concentrates all systemic risk\n- The extreme concentration makes the CCP's failure probability and impact greater\n- This follows logically from the economic framework presented\n- ✓ This is correct\n\n**Option D**: Policy 2 reduces TBTF subsidy by forcing lower leverage\n- Higher capital requirements force banks to reduce leverage (debt/assets ratio)\n- Lower leverage reduces asset volatility σ\n- Since S increases with σ, reducing σ directly reduces the TBTF subsidy value\n- This addresses moral hazard by making the implicit put option less valuable\n- ✓ This is correct\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 129, "Question": "### Background\n\n**Research Question.** How has the market for corporate directors valued the experience of CEOs from regulated versus non-regulated industries over time? The paper hypothesizes that CEOs in regulated industries (e.g., utilities, banks) historically accumulated human capital that was less relevant to competitive markets, leading to a \"directorship penalty.\" However, a wave of deregulation in the U.S. during the 1990s may have altered this perception.\n\n**Setting and Sample.** The study compares the mean number of outside directorships held by retired CEOs from regulated and non-regulated industries across three time periods.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Mean Difference Between Regulated and Non-Regulated Industries**\n\n| Variable | 1989-1993 | 2001-2005 |\n|:---|---:|---:|\n| **Mean Outside Directorships** | |\n| Non-Regulated Firms | 2.06 | 1.68 |\n| Regulated Firms | 1.07 | 1.78 |\n| **Difference (Non-Regulated - Regulated)** | **0.987*** | **-0.103** |\n\n*Note: *** indicates the difference is significant at the 1% level.*\n\n**Table 2: Mean Outside Directorships within Regulated Industries**\n\n| Regulated Industry | 1989-1993 | 1995-1999 |\n|:---|---:|---:|\n| Depository institution | 1.11 | 2.86 |\n| **Total** | **1.07** | **2.21*** |\n\n*Note: *** indicates the total for 1995-1999 is significantly different from 1989-1993 at the 1% level.*\n\n---\n\n### Question\n\nThe paper documents a significant change in the market's valuation of retired CEOs from regulated industries. Based on the data and the paper's hypothesis, select all statements that accurately describe this phenomenon.", "Options": {"A": "The \"directorship penalty\" observed in the 1989-1993 period, where CEOs from regulated firms held significantly fewer outside directorships than their non-regulated peers, had completely disappeared by the 2001-2005 period.", "B": "The Sarbanes-Oxley Act was the primary driver of this change, as it increased the demand for directors with experience in navigating complex regulatory environments.", "C": "The paper's primary explanation for this trend is that deregulation in sectors like banking forced firms to become more competitive, allowing their CEOs to develop and signal generalizable management skills valued by the broader director market.", "D": "The increase in directorships for regulated-industry CEOs was driven primarily by a surge in demand for leaders from the utility sector, which faced the most significant restructuring."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect a long-term empirical trend with its underlying economic cause (deregulation). It uses an **Atomic Decomposition** strategy, with one option stating the main empirical finding and the other stating the proposed mechanism. The depth strategy is **Scenario Application**, linking a broad economic event to specific data patterns. Distractors test for causal misattribution (C, blaming SOX instead of deregulation, which occurred earlier) and factual inaccuracy based on the provided data (D, as the full paper's Table 6, partially represented here, shows the largest increase was in depository institutions, not utilities).", "qid": "129", "question": "### Background\n\n**Research Question.** How has the market for corporate directors valued the experience of CEOs from regulated versus non-regulated industries over time? The paper hypothesizes that CEOs in regulated industries (e.g., utilities, banks) historically accumulated human capital that was less relevant to competitive markets, leading to a \"directorship penalty.\" However, a wave of deregulation in the U.S. during the 1990s may have altered this perception.\n\n**Setting and Sample.** The study compares the mean number of outside directorships held by retired CEOs from regulated and non-regulated industries across three time periods.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Mean Difference Between Regulated and Non-Regulated Industries**\n\n| Variable | 1989-1993 | 2001-2005 |\n|:---|---:|---:|\n| **Mean Outside Directorships** | |\n| Non-Regulated Firms | 2.06 | 1.68 |\n| Regulated Firms | 1.07 | 1.78 |\n| **Difference (Non-Regulated - Regulated)** | **0.987*** | **-0.103** |\n\n*Note: *** indicates the difference is significant at the 1% level.*\n\n**Table 2: Mean Outside Directorships within Regulated Industries**\n\n| Regulated Industry | 1989-1993 | 1995-1999 |\n|:---|---:|---:|\n| Depository institution | 1.11 | 2.86 |\n| **Total** | **1.07** | **2.21*** |\n\n*Note: *** indicates the total for 1995-1999 is significantly different from 1989-1993 at the 1% level.*\n\n---\n\n### Question\n\nThe paper documents a significant change in the market's valuation of retired CEOs from regulated industries. Based on the data and the paper's hypothesis, select all statements that accurately describe this phenomenon.", "model_evaluation_result": {"model_raw_response": "Looking at the data and the paper's hypothesis:\n\nFrom Table 1:\n- 1989-1993: Non-regulated CEOs had 2.06 directorships vs 1.07 for regulated CEOs (difference of 0.987, significant at 1%)\n- 2001-2005: Non-regulated CEOs had 1.68 directorships vs 1.78 for regulated CEOs (difference of -0.103, not significant)\n\nFrom Table 2:\n- Regulated industry CEOs increased from 1.07 directorships (1989-1993) to 2.21 (1995-1999), significant at 1%\n\nEvaluating each option:\n\nA) TRUE - The data shows the penalty (0.987 difference, significant) in 1989-1993 disappeared by 2001-2005 (-0.103 difference, not significant).\n\nB) FALSE - Sarbanes-Oxley was enacted in 2002, but the increase for regulated CEOs occurred between 1989-1993 and 1995-1999 (before SOX).\n\nC) TRUE - This aligns with the paper's stated hypothesis that deregulation in the 1990s allowed regulated industry CEOs to develop competitive market skills.\n\nD) FALSE - Table 2 only shows data for depository institutions, not utilities. We cannot determine if utilities drove the trend.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 82, "Question": "### Background\n\n**Research Question.** How does severe multicollinearity manifest in the estimated coefficients of a housing price model, and can automated variable selection methods like Stepwise Regression (SWR) reliably solve this problem?\n\n**Setting.** Ordinary Least Squares (OLS) and Stepwise Regression (SWR) models are estimated to explain housing sales prices using a large set of property characteristics.\n\n**Variables & Parameters.**\n\n*   `SQFT`: Square feet of living area.\n*   `BDRMS`: Number of bedrooms.\n*   `BATHRMS`: Number of bathrooms.\n*   `hat(b)ⱼ`: The estimated regression coefficient for variable `j`.\n\n---\n\n### Data / Model Specification\n\nThe paper assumes that explanatory variables should be linearly independent for their coefficients to be accurate measures of marginal influence. When this is violated, structural analysis is compromised. Table 1 summarizes the OLS and SWR model results.\n\n**Table 1: Summary of OLS and SWR Models**\n\n| Selected Variables | OLS Regression Coefficient | SWR Regression Coefficient |\n| :--- | :---: | :---: |\n| SQFT | 17.179 | 17.150 |\n| BDRMS | -877.782 | -1019.325 |\n| BATHRMS | -1058.141 | -1096.783 |\n\n*Note: The SWR procedure used a partial F-statistic criterion for variable selection.*\n\n---\n\nBased on the results in Table 1 and the principles discussed in the paper, which of the following statements are valid conclusions regarding the effects of multicollinearity and the utility of Stepwise Regression (SWR)?", "Options": {"A": "The negative coefficients for `BDRMS` and `BATHRMS` in the OLS model are likely statistical artifacts caused by the model's inability to disentangle their effects from the highly correlated `SQFT` variable.", "B": "The persistence of theoretically incorrect negative signs for `BDRMS` and `BATHRMS` in the SWR model demonstrates that variable selection based on individual significance tests (like the partial F-test) does not guarantee an independent set of final predictors.", "C": "The SWR model successfully resolves the multicollinearity issue, as evidenced by its selection of a smaller, more parsimonious set of variables.", "D": "The OLS coefficient for `SQFT` (17.179) is an unbiased and accurate measure of the marginal value of a square foot, as it has the correct sign and is highly significant."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret the classic symptoms of multicollinearity (counter-intuitive coefficient signs) and critique the limitations of automated variable selection methods like SWR. Depth Strategy: Reverse-Reasoning. The user is given the empirical results (negative signs) and must identify the plausible statistical causes and methodological failures. Distractor Logic: Option B is a conceptual opposite, incorrectly claiming SWR solved the problem. Option D is 'Almost Right'—the sign is correct, but the paper's core argument is that multicollinearity makes this coefficient an inaccurate measure of marginal impact, as it also captures the effects of correlated variables.", "qid": "82", "question": "### Background\n\n**Research Question.** How does severe multicollinearity manifest in the estimated coefficients of a housing price model, and can automated variable selection methods like Stepwise Regression (SWR) reliably solve this problem?\n\n**Setting.** Ordinary Least Squares (OLS) and Stepwise Regression (SWR) models are estimated to explain housing sales prices using a large set of property characteristics.\n\n**Variables & Parameters.**\n\n*   `SQFT`: Square feet of living area.\n*   `BDRMS`: Number of bedrooms.\n*   `BATHRMS`: Number of bathrooms.\n*   `hat(b)ⱼ`: The estimated regression coefficient for variable `j`.\n\n---\n\n### Data / Model Specification\n\nThe paper assumes that explanatory variables should be linearly independent for their coefficients to be accurate measures of marginal influence. When this is violated, structural analysis is compromised. Table 1 summarizes the OLS and SWR model results.\n\n**Table 1: Summary of OLS and SWR Models**\n\n| Selected Variables | OLS Regression Coefficient | SWR Regression Coefficient |\n| :--- | :---: | :---: |\n| SQFT | 17.179 | 17.150 |\n| BDRMS | -877.782 | -1019.325 |\n| BATHRMS | -1058.141 | -1096.783 |\n\n*Note: The SWR procedure used a partial F-statistic criterion for variable selection.*\n\n---\n\nBased on the results in Table 1 and the principles discussed in the paper, which of the following statements are valid conclusions regarding the effects of multicollinearity and the utility of Stepwise Regression (SWR)?", "model_evaluation_result": {"model_raw_response": "Looking at the data and the research question about severe multicollinearity in housing price models:\n\nThe key indicators of multicollinearity here are:\n1. Negative coefficients for BDRMS and BATHRMS (which should logically be positive - more bedrooms/bathrooms should increase house price)\n2. These negative coefficients persist in both OLS and SWR models\n3. SQFT likely captures much of the variation that BDRMS and BATHRMS would explain (larger houses have more rooms)\n\nAnalyzing each option:\n\nA) **Correct** - The negative coefficients for bedrooms and bathrooms are counterintuitive and indicate multicollinearity. When SQFT is included, it likely captures most of the size-related variation, making the partial effects of BDRMS and BATHRMS appear negative due to collinearity.\n\nB) **Correct** - The SWR model still shows negative coefficients for BDRMS (-1019.325) and BATHRMS (-1096.783), proving that stepwise selection based on partial F-tests doesn't solve the multicollinearity problem. The selected variables remain highly correlated.\n\nC) **Incorrect** - The table shows SWR selected the same three variables as OLS, not a smaller set. More importantly, the persistence of negative coefficients proves multicollinearity wasn't resolved.\n\nD) **Incorrect** - While the SQFT coefficient is positive and likely significant, in the presence of severe multicollinearity, even this coefficient may be biased. Multicollinearity affects the accuracy of all coefficient estimates in the model.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 109, "Question": "### Background\n\nCorporate finance theory offers competing views on the relationship between debt and other managerial monitoring mechanisms, such as an independent board of directors. These views are termed the \"complementary\" and \"substitutive\" hypotheses.\n\n### Data / Model Specification\n\n*   **Complementary View:** Argues that strong monitoring mechanisms are used together. A powerful board would impose higher debt on management to further constrain them, predicting a positive correlation between outside directors and debt.\n*   **Substitutive View:** Argues that different monitoring mechanisms can replace one another. If a board provides effective oversight, the firm needs less of the disciplinary (but costly) effect of debt, predicting a negative correlation.\n\nThe paper tests these competing views and finds a statistically significant negative correlation between the proportion of outside directors and corporate debt levels.\n\n---\n\nBased on these theories and the paper's findings, which of the following statements are **INCORRECT**?\n", "Options": {"A": "The paper's finding of a negative correlation is consistent with the theory that managers in firms with weak board oversight use high leverage to bond themselves and establish a reputation.", "B": "The economic logic of the substitutive view is that since an independent board already reduces agency costs, the marginal benefit of using costly debt for additional discipline is low.", "C": "The complementary view predicts that firms with more outside directors will have higher leverage ratios.", "D": "The paper's empirical results provide support for the complementary view of corporate governance mechanisms."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the ability to distinguish between two competing theories (complementary vs. substitutive) and evaluate the paper's findings in light of them. The Single-Correct-Answer Inversion Protocol was used. The original objective was to identify the single correct description of the paper's findings, which would have been a statement supporting the substitutive view. By inverting the stem to 'INCORRECT', the distractors become the correct answers. Option A is incorrect because the paper's finding of a *negative* correlation refutes the complementary view. Option D is incorrect because the paper's finding supports the substitutive view, which implies that in the absence of strong board monitoring (i.e., weak oversight), firms would have *higher* leverage, not that the finding itself is consistent with that logic in isolation. Options B and C are correct statements describing the substitutive and complementary theories, respectively, and thus are the new distractors.", "qid": "109", "question": "### Background\n\nCorporate finance theory offers competing views on the relationship between debt and other managerial monitoring mechanisms, such as an independent board of directors. These views are termed the \"complementary\" and \"substitutive\" hypotheses.\n\n### Data / Model Specification\n\n*   **Complementary View:** Argues that strong monitoring mechanisms are used together. A powerful board would impose higher debt on management to further constrain them, predicting a positive correlation between outside directors and debt.\n*   **Substitutive View:** Argues that different monitoring mechanisms can replace one another. If a board provides effective oversight, the firm needs less of the disciplinary (but costly) effect of debt, predicting a negative correlation.\n\nThe paper tests these competing views and finds a statistically significant negative correlation between the proportion of outside directors and corporate debt levels.\n\n---\n\nBased on these theories and the paper's findings, which of the following statements are **INCORRECT**?\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) This statement is CORRECT. The paper finds a negative correlation between outside directors and debt. This means firms with weak boards (few outside directors) have high debt, which is consistent with managers using debt to bond themselves when board oversight is weak.\n\nB) This statement is CORRECT. The substitutive view argues that different monitoring mechanisms can replace each other. If an independent board already provides good monitoring, there's less need for the costly disciplinary effect of debt.\n\nC) This statement is CORRECT. The complementary view explicitly predicts a positive correlation between outside directors and debt (both monitoring mechanisms used together).\n\nD) This statement is INCORRECT. The paper finds a negative correlation between outside directors and debt, which supports the substitutive view, not the complementary view. The complementary view would predict a positive correlation.\n\nD", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 121, "Question": "### Background\n\n**Research Question.** What bank-specific and macroeconomic factors are associated with bank inefficiency?\n\n**Setting / Data-Generating Environment.** A second-stage truncated regression is used to model the determinants of the bootstrap bias-corrected bank inefficiency scores (`$\\hat{\\hat{\\gamma}}_j$`) in the post-crisis Indonesian banking sector (2000-2007).\n\n**Variables & Parameters.**\n\n*   `$\\hat{\\hat{\\gamma}}_j$`: The bootstrap bias-corrected Shephard inefficiency score for bank `j`. Higher values mean higher inefficiency.\n*   `$\\hat{\\hat{\\rho}}_j = 1/\\hat{\\hat{\\gamma}}_j$`: The corresponding cost efficiency score.\n*   `OWN`: Ownership dummy; 1 if >50% foreign ownership, 0 otherwise.\n*   `Size`: Natural logarithm of total assets.\n*   `SOB`: State-Owned Bank dummy; 1 if state-owned, 0 otherwise.\n*   `$\\hat{\\hat{\\beta}}$`: The vector of estimated coefficients.\n\n---\n\n### Data / Model Specification\n\nThe second-stage model regresses inefficiency on a set of environmental variables:\n\n  \n\\hat{\\hat{\\gamma}}_{j} = z_{j}\\hat{\\hat{\\beta}} + \\varepsilon_{j} \\quad \\text{(Eq. 1)}\n \n\n**Table 1. Truncated Regression Results (Model 3, Post-Crisis 2000-2007)**\n\n| Variable | Coefficient (`$\\hat{\\hat{\\beta}}$`) |\n| :--- | :--- |\n| OWN | -1.1239* |\n| Size | 0.2316* |\n| SOB | -1.1659* |\n\n*Note: Dependent variable is the inefficiency score `$\\hat{\\hat{\\gamma}}_j$`. `*` denotes significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the regression results in Table 1, select all of the following statements that are correct.", "Options": {"A": "The results indicate that state-owned banks (`SOB`) were significantly less cost-efficient than foreign-owned banks (`OWN`).", "B": "The coefficient on `OWN` implies that, holding other factors constant, majority foreign-owned banks were significantly more cost-efficient than the baseline group (private domestic banks).", "C": "A bank with a predicted inefficiency score (`$\\hat{\\hat{\\gamma}}_j$`) of 2.5 would have a corresponding cost efficiency score (`$\\hat{\\hat{\\rho}}_j$`) of 0.25.", "D": "The coefficient on `Size` implies that larger banks were, on average, significantly less cost-efficient than smaller banks during this period."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the interpretation of regression coefficients where the dependent variable represents a 'bad' outcome (inefficiency). It uses an atomic decomposition strategy. The correct options test the direct interpretation of signs. Distractor C requires a more nuanced comparison of two coefficients, a common point of error. Distractor D uses a simple calculation to test for a formula misuse error in the fundamental reciprocal relationship between the efficiency and inefficiency scores.", "qid": "121", "question": "### Background\n\n**Research Question.** What bank-specific and macroeconomic factors are associated with bank inefficiency?\n\n**Setting / Data-Generating Environment.** A second-stage truncated regression is used to model the determinants of the bootstrap bias-corrected bank inefficiency scores (`$\\hat{\\hat{\\gamma}}_j$`) in the post-crisis Indonesian banking sector (2000-2007).\n\n**Variables & Parameters.**\n\n*   `$\\hat{\\hat{\\gamma}}_j$`: The bootstrap bias-corrected Shephard inefficiency score for bank `j`. Higher values mean higher inefficiency.\n*   `$\\hat{\\hat{\\rho}}_j = 1/\\hat{\\hat{\\gamma}}_j$`: The corresponding cost efficiency score.\n*   `OWN`: Ownership dummy; 1 if >50% foreign ownership, 0 otherwise.\n*   `Size`: Natural logarithm of total assets.\n*   `SOB`: State-Owned Bank dummy; 1 if state-owned, 0 otherwise.\n*   `$\\hat{\\hat{\\beta}}$`: The vector of estimated coefficients.\n\n---\n\n### Data / Model Specification\n\nThe second-stage model regresses inefficiency on a set of environmental variables:\n\n  \n\\hat{\\hat{\\gamma}}_{j} = z_{j}\\hat{\\hat{\\beta}} + \\varepsilon_{j} \\quad \\text{(Eq. 1)}\n \n\n**Table 1. Truncated Regression Results (Model 3, Post-Crisis 2000-2007)**\n\n| Variable | Coefficient (`$\\hat{\\hat{\\beta}}$`) |\n| :--- | :--- |\n| OWN | -1.1239* |\n| Size | 0.2316* |\n| SOB | -1.1659* |\n\n*Note: Dependent variable is the inefficiency score `$\\hat{\\hat{\\gamma}}_j$`. `*` denotes significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the regression results in Table 1, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results where the dependent variable is the inefficiency score (higher values = higher inefficiency):\n\n**Option A**: The coefficient for SOB is -1.1659* (negative and significant), meaning state-owned banks have LOWER inefficiency scores than the baseline group. Since OWN also has a negative coefficient (-1.1239*), both state-owned and foreign-owned banks are more efficient than the baseline. We cannot determine from these results alone that state-owned banks were less efficient than foreign-owned banks.\n\n**Option B**: The coefficient on OWN is -1.1239* (negative and significant). Since higher inefficiency scores mean lower efficiency, a negative coefficient means foreign-owned banks have lower inefficiency (i.e., higher efficiency) than the baseline group (private domestic banks). This statement is correct.\n\n**Option C**: Given that ρ̂ⱼ = 1/γ̂ⱼ, if γ̂ⱼ = 2.5, then ρ̂ⱼ = 1/2.5 = 0.4, not 0.25. This statement is incorrect.\n\n**Option D**: The coefficient on Size is 0.2316* (positive and significant). Since the dependent variable is inefficiency, a positive coefficient means larger banks have higher inefficiency scores, which means they are less cost-efficient. This statement is correct.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 137, "Question": "### Background\n\n**Research Question.** Leslie Perlow argues that culture is a system of three interconnected layers (artifacts, values, assumptions) and that changing only one layer often fails. This case formalizes this concept using a coordination game to model cultural equilibria and analyze the difficulty and cost of engineering a cultural shift.\n\n**Setting.** Consider a stylized firm where employees' collective behavior determines the dominant work norm. We model this as a symmetric 2-player game where each player must choose a work style. The payoff to each player depends on their own choice and the choice of the other player, reflecting the need for coordination.\n\n**Variables and Parameters.**\n- `a_i`: Action of player `i`, `a_i ∈ {H, F}`.\n- `H`: “High-Effort” work style (e.g., Perlow's “client comes first”).\n- `F`: “Flexible” work style (e.g., work-life balance).\n- `π(a_i, a_j)`: Payoff to player `i` given their action `a_i` and player `j`'s action `a_j`.\n\n---\n\n### Data / Model Specification\n\nThe strategic interaction is represented by the following payoff matrix. The entries are (Player 1's payoff, Player 2's payoff).\n\n**Table 1: Baseline Payoff Matrix**\n| | Player 2 Chooses H | Player 2 Chooses F |\n| :--- | :---: | :---: |\n| **Player 1 Chooses H** | (5, 5) | (1, 2) |\n| **Player 1 Chooses F** | (2, 1) | (4, 4) |\n\nThis game structure allows for multiple, self-sustaining cultural norms.\n\n---\n\n### The Question\n\nBased on the payoff matrix in Table 1, which of the following outcomes represent a pure-strategy Nash Equilibrium? Select all that apply.", "Options": {"A": "Player 1 chooses H, Player 2 chooses F", "B": "Player 1 chooses H, Player 2 chooses H", "C": "Player 1 chooses F, Player 2 chooses H", "D": "Player 1 chooses F, Player 2 chooses F"}, "Answer": ["B", "D"], "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item tests the fundamental skill of identifying Nash equilibria in a matrix game, which is the foundation of the original QA problem. It uses a Computational Judgment strategy. The Multiple Choice format is ideal for a 'select-all-that-apply' task. The correct answers are the two cells where neither player has a unilateral incentive to deviate. The distractors are the off-diagonal cells, which are not equilibria.", "qid": "137", "question": "### Background\n\n**Research Question.** Leslie Perlow argues that culture is a system of three interconnected layers (artifacts, values, assumptions) and that changing only one layer often fails. This case formalizes this concept using a coordination game to model cultural equilibria and analyze the difficulty and cost of engineering a cultural shift.\n\n**Setting.** Consider a stylized firm where employees' collective behavior determines the dominant work norm. We model this as a symmetric 2-player game where each player must choose a work style. The payoff to each player depends on their own choice and the choice of the other player, reflecting the need for coordination.\n\n**Variables and Parameters.**\n- `a_i`: Action of player `i`, `a_i ∈ {H, F}`.\n- `H`: “High-Effort” work style (e.g., Perlow's “client comes first”).\n- `F`: “Flexible” work style (e.g., work-life balance).\n- `π(a_i, a_j)`: Payoff to player `i` given their action `a_i` and player `j`'s action `a_j`.\n\n---\n\n### Data / Model Specification\n\nThe strategic interaction is represented by the following payoff matrix. The entries are (Player 1's payoff, Player 2's payoff).\n\n**Table 1: Baseline Payoff Matrix**\n| | Player 2 Chooses H | Player 2 Chooses F |\n| :--- | :---: | :---: |\n| **Player 1 Chooses H** | (5, 5) | (1, 2) |\n| **Player 1 Chooses F** | (2, 1) | (4, 4) |\n\nThis game structure allows for multiple, self-sustaining cultural norms.\n\n---\n\n### The Question\n\nBased on the payoff matrix in Table 1, which of the following outcomes represent a pure-strategy Nash Equilibrium? Select all that apply.", "model_evaluation_result": {"model_raw_response": "I need to find all pure-strategy Nash equilibria by checking if any player has an incentive to deviate from each possible outcome.\n\nLet me check each option:\n\n**Option A: (H, F)**\n- Player 1 gets 1, but could get 2 by switching to F (given Player 2 plays F)\n- Player 1 would want to deviate, so this is NOT a Nash equilibrium\n\n**Option B: (H, H)**\n- Player 1 gets 5, but would get 2 by switching to F (given Player 2 plays H)\n- Player 2 gets 5, but would get 2 by switching to F (given Player 1 plays H)\n- Neither player wants to deviate, so this IS a Nash equilibrium\n\n**Option C: (F, H)**\n- Player 1 gets 2, but could get 5 by switching to H (given Player 2 plays H)\n- Player 1 would want to deviate, so this is NOT a Nash equilibrium\n\n**Option D: (F, F)**\n- Player 1 gets 4, but would get 1 by switching to H (given Player 2 plays F)\n- Player 2 gets 4, but would get 1 by switching to H (given Player 1 plays F)\n- Neither player wants to deviate, so this IS a Nash equilibrium\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 125, "Question": "### Background\n\n**Research Question.** This case examines whether the market's negative reaction to an initial public debt offering (debt IPO) is driven by the default risk of the issuing firm.\n\n**Setting.** The analysis is based on a sample of 143 debt IPOs from 1971-1994. The study uses an event study to measure the announcement-period stock returns for subsamples based on credit quality.\n\n**Variables & Parameters.**\n*   `SER`: Standardized daily stock excess return on the announcement day.\n*   `Z-statistic`: The test statistic for the null hypothesis that the `SER` is equal to zero.\n*   Investment Grade: Bonds with a Standard & Poor's rating of BBB or higher.\n*   Junk Grade: Bonds with a Standard & Poor's rating of BB or lower, including non-rated issues.\n\n---\n\n### Data / Model Specification\n\nTo test if the negative market reaction is simply a response to high default risk, the announcement day `SER` is calculated separately for Investment Grade and Junk Grade subsamples. The results are in Table 1.\n\n**Table 1: Announcement Day SER by Bond Rating**\n\n| Subsample | Number | SER (%) | Z-statistic |\n| :--- | :--- | :--- | :--- |\n| Investment grade | 39 | -1.07*** | -3.82 |\n| Junk grade | 91 | -0.64** | -2.29 |\n\n*Note: *** and ** denote significance at the 1% and 5% levels, respectively.* \n\n---\n\nBased on the data in Table 1, which of the following statements are **NOT** supported by the evidence? Select all that apply.", "Options": {"A": "A formal statistical test for the difference in means would show that the SER for investment grade issues is significantly more negative than the SER for junk grade issues at the 1% level.", "B": "The negative announcement effect is primarily driven by the high default risk of junk bonds, as these issues show a more negative stock price reaction than investment grade bonds.", "C": "The negative stock price reaction to debt IPOs is statistically significant for both investment grade and junk grade issues.", "D": "The stock price reaction for investment grade issues is economically and statistically more negative than the reaction for junk grade issues."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret and compare statistical results across subsamples and to recognize incorrect conclusions. Depth Strategy: Reverse-Reasoning. The Single-Correct-Answer Inversion Protocol was applied because the core assessment objective—identifying the main takeaway from the table—yields only one correct affirmative statement. The stem was inverted to 'NOT supported'. Distractor Logic: Option A is the single true statement supported by the data (both Z-statistics are significant), making it the distractor in this inverted question. Option B is incorrect; the data show the opposite, with investment grade having a more negative SER. Option C is incorrect; a formal test for the difference in means (Z ≈ -1.09) fails to reject the null of no difference. Option D is incorrect; while the point estimate for investment grade is more negative, the difference is not statistically significant, so one cannot conclude it is 'statistically more negative'.", "qid": "125", "question": "### Background\n\n**Research Question.** This case examines whether the market's negative reaction to an initial public debt offering (debt IPO) is driven by the default risk of the issuing firm.\n\n**Setting.** The analysis is based on a sample of 143 debt IPOs from 1971-1994. The study uses an event study to measure the announcement-period stock returns for subsamples based on credit quality.\n\n**Variables & Parameters.**\n*   `SER`: Standardized daily stock excess return on the announcement day.\n*   `Z-statistic`: The test statistic for the null hypothesis that the `SER` is equal to zero.\n*   Investment Grade: Bonds with a Standard & Poor's rating of BBB or higher.\n*   Junk Grade: Bonds with a Standard & Poor's rating of BB or lower, including non-rated issues.\n\n---\n\n### Data / Model Specification\n\nTo test if the negative market reaction is simply a response to high default risk, the announcement day `SER` is calculated separately for Investment Grade and Junk Grade subsamples. The results are in Table 1.\n\n**Table 1: Announcement Day SER by Bond Rating**\n\n| Subsample | Number | SER (%) | Z-statistic |\n| :--- | :--- | :--- | :--- |\n| Investment grade | 39 | -1.07*** | -3.82 |\n| Junk grade | 91 | -0.64** | -2.29 |\n\n*Note: *** and ** denote significance at the 1% and 5% levels, respectively.* \n\n---\n\nBased on the data in Table 1, which of the following statements are **NOT** supported by the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1:\n- Investment grade: SER = -1.07% (significant at 1%)\n- Junk grade: SER = -0.64% (significant at 5%)\n\nLet me evaluate each statement:\n\nA) \"A formal statistical test for the difference in means would show that the SER for investment grade issues is significantly more negative than the SER for junk grade issues at the 1% level.\"\n- We don't have the standard errors or a formal test statistic for the difference between groups\n- We cannot determine if the difference (-1.07% vs -0.64%) is statistically significant at any level\n- NOT SUPPORTED\n\nB) \"The negative announcement effect is primarily driven by the high default risk of junk bonds, as these issues show a more negative stock price reaction than investment grade bonds.\"\n- The data shows junk bonds have LESS negative reaction (-0.64%) than investment grade (-1.07%)\n- This directly contradicts the statement\n- NOT SUPPORTED\n\nC) \"The negative stock price reaction to debt IPOs is statistically significant for both investment grade and junk grade issues.\"\n- Investment grade: significant at 1% level (Z = -3.82)\n- Junk grade: significant at 5% level (Z = -2.29)\n- Both are statistically significant\n- SUPPORTED\n\nD) \"The stock price reaction for investment grade issues is economically and statistically more negative than the reaction for junk grade issues.\"\n- Economically: -1.07% is more negative than -0.64% ✓\n- Statistically: We don't have a test for the difference between groups\n- Only partially supported (economic part yes, statistical part unknown)\n- NOT FULLY SUPPORTED\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 98, "Question": "### Background\n\n**Research Question.** Can the non-linear relationship between foreign bank entry and investment be characterized as a discrete regime shift at an endogenously determined threshold?\n\n**Setting & Data.** A threshold regression model is estimated on a cross-section of 54 countries. The model allows the marginal effect of foreign bank penetration to differ depending on whether it is above or below an optimally chosen threshold value.\n\n### Data / Model Specification\n\nThe paper estimates a piecewise linear model where the sample is split into two regimes based on the threshold variable `FBSASS` (share of foreign bank assets). The coefficients `α₅` and `α₆` represent the constant marginal effect of `FBSASS` in the low-penetration and high-penetration regimes, respectively.\n\n**Table 1. Threshold Effect Estimation Results**\n\n| Parameter / Variable | Estimate | t-statistic |\n|:---|:---:|:---:|\n| `FBSASS` (if `FBSASS < THR`) (α₅) | -14.10 | -3.07 |\n| `FBSASS` (if `FBSASS > THR`) (α₆) | 4.57 | 1.85 |\n| Estimated `THR` | 0.42 | - |\n| LR test (p-value) | 20.44 (0.01) | - |\n\n*Source: Table 6, column 12 from the paper.*\n\n### Question\n\nBased on the threshold model results presented in **Table 1**, select all of the following statements that are correct.", "Options": {"A": "The Likelihood Ratio (LR) test result (p=0.01) indicates that the threshold model provides a statistically superior fit compared to a simple linear model, corroborating the non-linearity hypothesis.", "B": "The optimal threshold of 0.42 was chosen because it represents the median `FBSASS` value in the sample, ensuring the two regimes have an equal number of countries.", "C": "The coefficient `α̂₆ = 4.57` means that for a country with `FBSASS` = 0.50, the model's predicted investment-to-GDP ratio is 4.57.", "D": "The model estimates a statistically significant negative marginal effect of foreign bank entry on investment for countries with an `FBSASS` below 42%."}, "Answer": ["A", "D"], "pi_justification": "REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the output of a threshold regression model. It uses a Scenario Application strategy, asking for interpretation of the two regimes. The correct options (A, B) are an atomic decomposition of the key findings: the sign/significance of the regime-specific coefficients and the meaning of the test for non-linearity. Distractor C is a Conceptual (Almost Right) error, confusing a marginal effect with a predicted level. Distractor D is a Factual Error regarding the methodology, as the threshold is chosen to minimize SSR, not to balance sample sizes.", "qid": "98", "question": "### Background\n\n**Research Question.** Can the non-linear relationship between foreign bank entry and investment be characterized as a discrete regime shift at an endogenously determined threshold?\n\n**Setting & Data.** A threshold regression model is estimated on a cross-section of 54 countries. The model allows the marginal effect of foreign bank penetration to differ depending on whether it is above or below an optimally chosen threshold value.\n\n### Data / Model Specification\n\nThe paper estimates a piecewise linear model where the sample is split into two regimes based on the threshold variable `FBSASS` (share of foreign bank assets). The coefficients `α₅` and `α₆` represent the constant marginal effect of `FBSASS` in the low-penetration and high-penetration regimes, respectively.\n\n**Table 1. Threshold Effect Estimation Results**\n\n| Parameter / Variable | Estimate | t-statistic |\n|:---|:---:|:---:|\n| `FBSASS` (if `FBSASS < THR`) (α₅) | -14.10 | -3.07 |\n| `FBSASS` (if `FBSASS > THR`) (α₆) | 4.57 | 1.85 |\n| Estimated `THR` | 0.42 | - |\n| LR test (p-value) | 20.44 (0.01) | - |\n\n*Source: Table 6, column 12 from the paper.*\n\n### Question\n\nBased on the threshold model results presented in **Table 1**, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the threshold model results in Table 1, I need to evaluate each statement:\n\n**Statement A**: The LR test statistic is 20.44 with p-value = 0.01. Since p < 0.05, this indicates the threshold model is statistically superior to a linear model at conventional significance levels. This supports the non-linearity hypothesis. **TRUE**\n\n**Statement B**: The threshold of 0.42 is described as \"optimally chosen\" and \"estimated\" in the table. In threshold regression models, the threshold is determined by maximizing the likelihood function (or minimizing the sum of squared residuals), not by ensuring equal sample sizes in each regime. There's no indication it represents the median value. **FALSE**\n\n**Statement C**: The coefficient α₆ = 4.57 represents the marginal effect of FBSASS on investment when FBSASS > 0.42, not the predicted level of investment. For a country with FBSASS = 0.50, this coefficient tells us how investment changes with a unit change in FBSASS, not the absolute level of investment. **FALSE**\n\n**Statement D**: For countries with FBSASS < 0.42 (which equals 42%), the coefficient α₅ = -14.10 with t-statistic = -3.07. The absolute value of the t-statistic (3.07) exceeds conventional critical values (e.g., 1.96 for 5% significance), indicating this negative effect is statistically significant. **TRUE**\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 28, "Question": "### Background\n\n**Research Question.** Can a trading rule based on identifying properties with large negative residuals from a hedonic pricing model generate superior risk-adjusted returns, and what does this imply for real estate market efficiency?\n\n**Setting.** An empirical test using portfolios of apartment buildings sorted by their hedonic pricing error (`e`). Performance is evaluated by comparing Sharpe ratios, and using the Jobson-Korkie (`z_{Sh}`) test for statistical significance against the total sample of properties.\n\n**Variables and Parameters.**\n- `Sharpe Ratio`: A measure of risk-adjusted return, calculated as `Mean Excess Return / Std. Dev.`.\n- `z_{Sh}`: Jobson-Korkie test statistic for the difference in Sharpe ratios.\n\n---\n\n### Data / Model Specification\n\nTo formally test for a difference in risk-adjusted performance, the Jobson-Korkie test was used. The null hypothesis is that the Sharpe ratios of the two portfolios are equal.\n\n**Table 1.** Risk-Adjusted Performance and Statistical Test (1971-1980).\n\n| Portfolio | Sharpe Ratio | `z_{Sh}` vs. Total Sample |\n| :--- | :--- | :--- |\n| Total sample | 0.88 | -- |\n| `e ≤ -0.10` | 0.75 | 0.28 |\n\n---\n\n### Question\n\nGiven the risk-adjusted performance metrics and the formal statistical test results in **Table 1**, which of the following conclusions are supported by the paper's analysis? (Use a 5% significance level, where the two-sided critical z-value is approximately 1.96).", "Options": {"A": "The statistical test fails to reject the null hypothesis, meaning there is no significant evidence that the trading rule generates abnormal risk-adjusted returns.", "B": "The paper's final conclusion is that while the trading rule identifies properties with high raw returns, this outperformance disappears after adjusting for risk, consistent with an efficient market.", "C": "The `z_Sh` statistic of 0.28 indicates that the 'undervalued' portfolio's risk-adjusted performance is statistically superior to the total sample's.", "D": "The findings provide strong evidence against the semi-strong form of market efficiency in the Vancouver apartment market."}, "Answer": ["A", "B"], "pi_justification": "This item tests the ability to interpret a formal statistical test (the Jobson-Korkie test) and connect its result to the broader economic concept of market efficiency. It uses a **Reverse-Reasoning** strategy, asking the user to infer the correct theoretical conclusions from the empirical evidence. Distractor A represents a fundamental misinterpretation of statistical significance. Distractor C is a conceptual opposite of the paper's main finding.", "qid": "28", "question": "### Background\n\n**Research Question.** Can a trading rule based on identifying properties with large negative residuals from a hedonic pricing model generate superior risk-adjusted returns, and what does this imply for real estate market efficiency?\n\n**Setting.** An empirical test using portfolios of apartment buildings sorted by their hedonic pricing error (`e`). Performance is evaluated by comparing Sharpe ratios, and using the Jobson-Korkie (`z_{Sh}`) test for statistical significance against the total sample of properties.\n\n**Variables and Parameters.**\n- `Sharpe Ratio`: A measure of risk-adjusted return, calculated as `Mean Excess Return / Std. Dev.`.\n- `z_{Sh}`: Jobson-Korkie test statistic for the difference in Sharpe ratios.\n\n---\n\n### Data / Model Specification\n\nTo formally test for a difference in risk-adjusted performance, the Jobson-Korkie test was used. The null hypothesis is that the Sharpe ratios of the two portfolios are equal.\n\n**Table 1.** Risk-Adjusted Performance and Statistical Test (1971-1980).\n\n| Portfolio | Sharpe Ratio | `z_{Sh}` vs. Total Sample |\n| :--- | :--- | :--- |\n| Total sample | 0.88 | -- |\n| `e ≤ -0.10` | 0.75 | 0.28 |\n\n---\n\n### Question\n\nGiven the risk-adjusted performance metrics and the formal statistical test results in **Table 1**, which of the following conclusions are supported by the paper's analysis? (Use a 5% significance level, where the two-sided critical z-value is approximately 1.96).", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1:\n\n- Total sample Sharpe ratio: 0.88\n- Portfolio with e ≤ -0.10 (properties with large negative residuals): Sharpe ratio of 0.75\n- z_Sh statistic: 0.28\n\nThe Jobson-Korkie test statistic (z_Sh = 0.28) tests whether the difference in Sharpe ratios is statistically significant. With a 5% significance level and two-sided critical value of 1.96:\n\n- Since |0.28| < 1.96, we fail to reject the null hypothesis that the Sharpe ratios are equal\n- This means there is no statistically significant difference in risk-adjusted performance\n\nEvaluating each option:\n\nA) Correct - The z_Sh of 0.28 is well below the critical value of 1.96, so we fail to reject the null hypothesis. There is no significant evidence of abnormal risk-adjusted returns.\n\nB) Correct - The \"undervalued\" portfolio actually has a lower Sharpe ratio (0.75 vs 0.88), and the difference is not statistically significant. This suggests any raw return advantage disappears after risk adjustment, consistent with market efficiency.\n\nC) Incorrect - The z_Sh of 0.28 indicates no statistical significance (needs to exceed 1.96), and the \"undervalued\" portfolio actually has inferior risk-adjusted performance (0.75 < 0.88).\n\nD) Incorrect - The findings support market efficiency rather than providing evidence against it, since the trading rule fails to generate superior risk-adjusted returns.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 3, "Question": "### Background\n\n**Research Question.** This case examines the proposed link between local weather, investor mood, and asset prices, and critically evaluates whether this relationship is causal or merely a proxy for seasonality.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for anomaly strategies from July 1973 to December 2012. The predictor is the monthly high temperature recorded at the Central Park weather station in New York City.\n\n**Variables & Parameters.**\n- `R_t`: Monthly excess return of an anomaly strategy (dimensionless).\n- `X_{t-1}`: The highest temperature (°F) recorded at Central Park in the preceding month.\n- `b`: Sensitivity of `R_t` to `X_{t-1}`.\n\n---\n\n### Data / Model Specification\n\nThe analysis uses the following OLS regression:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Anomaly Performance by NYC Temperature (Monthly Excess Returns, %)**\n\n| Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A: Better after cold weather** | | | | |\n| Size | 0.28 | -0.06 | -0.60 | 1.05 |\n| | [1.48] | [-4.54] | [-2.18] | [4.10] |\n\n*Note: `t`-statistics are in brackets. `a_H` and `a_L` are average returns after hot (above median) and cold (below median) months. The Size strategy is long small caps, short large caps.*\n\n---\n\n### Question\n\nBased on the provided information regarding the **Size** strategy, select all of the following statements that are correct. Assume the standard deviation of monthly high temperatures is 15°F.", "Options": {"A": "The paper's 'Bozeman critique'—that weather in remote locations has similar predictive power—suggests the NYC temperature finding is likely a proxy for seasonality (e.g., the January effect) rather than a direct effect of local trader mood.", "B": "The difference in average returns between cold and hot months (`a_L - a_H`) is 0.45%.", "C": "The Size premium is positive and statistically significant following both hot and cold months, but larger after cold months.", "D": "Based on the linear model's coefficient `b`, a one-standard-deviation drop in temperature is predicted to increase the monthly Size premium by 0.90%."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test multiple facets of the source QA: calculation, interpretation, and critique. Option A requires a simple calculation based on the provided data (Computational Judgment). Option B assesses comprehension of the paper's core satirical argument against naive behavioral stories. Distractor C is a Factual Error, as the premium is negative and significant after hot months. Distractor D is a Sign Error in the calculation of the difference between `a_L` and `a_H`.", "qid": "3", "question": "### Background\n\n**Research Question.** This case examines the proposed link between local weather, investor mood, and asset prices, and critically evaluates whether this relationship is causal or merely a proxy for seasonality.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for anomaly strategies from July 1973 to December 2012. The predictor is the monthly high temperature recorded at the Central Park weather station in New York City.\n\n**Variables & Parameters.**\n- `R_t`: Monthly excess return of an anomaly strategy (dimensionless).\n- `X_{t-1}`: The highest temperature (°F) recorded at Central Park in the preceding month.\n- `b`: Sensitivity of `R_t` to `X_{t-1}`.\n\n---\n\n### Data / Model Specification\n\nThe analysis uses the following OLS regression:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Anomaly Performance by NYC Temperature (Monthly Excess Returns, %)**\n\n| Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A: Better after cold weather** | | | | |\n| Size | 0.28 | -0.06 | -0.60 | 1.05 |\n| | [1.48] | [-4.54] | [-2.18] | [4.10] |\n\n*Note: `t`-statistics are in brackets. `a_H` and `a_L` are average returns after hot (above median) and cold (below median) months. The Size strategy is long small caps, short large caps.*\n\n---\n\n### Question\n\nBased on the provided information regarding the **Size** strategy, select all of the following statements that are correct. Assume the standard deviation of monthly high temperatures is 15°F.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the data provided for the Size strategy.\n\nLet me analyze each option:\n\n**Option A**: The 'Bozeman critique' suggests that if weather in remote locations (like Bozeman) has similar predictive power as NYC weather, then the effect is likely capturing seasonality rather than local trader mood. This is a logical critique - if local mood were the mechanism, only local weather should matter. This statement appears correct.\n\n**Option B**: Calculating the difference in average returns:\n- a_L (cold months) = 1.05%\n- a_H (hot months) = -0.60%\n- a_L - a_H = 1.05% - (-0.60%) = 1.65%\nThis is not 0.45%, so this statement is incorrect.\n\n**Option C**: Looking at the data:\n- After hot months: a_H = -0.60% (negative, not positive)\n- After cold months: a_L = 1.05% (positive)\nThe Size premium is not positive following both types of months, so this statement is incorrect.\n\n**Option D**: With b = -0.06 and standard deviation = 15°F:\n- A one-standard-deviation drop in temperature means ΔX = -15°F\n- Impact on returns: ΔR = b × ΔX = -0.06 × (-15) = 0.90%\nThis calculation is correct.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 49, "Question": "### Background\n\nA study investigates how the effect of economic policy uncertainty (EPU) on board structure is moderated by firm characteristics. The analysis involves splitting the sample based on factors like CEO power and capital intensity and running separate regressions.\n\n### Data / Model Specification\n\nThe table below shows regression coefficients for `EPU` from models where board characteristics are the dependent variables, run on different firm subsamples.\n\n**Table 1. Board Change Moderators (Selected Panels)**\n\n**Panel C: CEO Power**\n\n| | CEO Power | Mean | EPU Coeff. | Std. Err. |\n| :--- | :--- | :--- | :--- | :--- |\n| Board size | High | 10.75 | -0.032 | (0.037) |\n| | Low | 10.36 | -0.093** | (0.009) |\n| Board independence | High | 78.04% | 0.009 | (0.028) |\n| | Low | 74.30% | 0.096*** | (0.006) |\n\n**Panel D: Capital Intensity**\n\n| | Capital Intensity | Mean | EPU Coeff. | Std. Err. |\n| :--- | :--- | :--- | :--- | :--- |\n| Board size | High | 10.44 | -0.106*** | (0.011) |\n| | Low | 10.32 | -0.080*** | (0.012) |\n| Board independence | High | 73.76% | 0.074*** | (0.008) |\n| | Low | 75.45% | 0.098*** | (0.008) |\n\n*Note: ***, ** denote significance at 1% and 5% levels.*\n\n### Question\n\nBased on a synthesis of the mean values and regression coefficients in the provided tables, which of the following conclusions about the moderating effects of firm characteristics are supported?", "Options": {"A": "Firms with high capital intensity exhibit a weaker response to EPU in terms of board size changes, as their boards are already optimized for their operating environment.", "B": "Firms with high capital intensity, which start with weaker monitoring (lower mean board independence), react more strongly to EPU by reducing board size compared to low capital intensity firms.", "C": "The significant response of board independence to EPU is concentrated in firms with powerful CEOs, suggesting these boards must work harder to enhance monitoring.", "D": "Firms with low CEO power, which have weaker initial monitoring structures (lower mean board independence), significantly increase their board independence in response to rising EPU."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the ability to synthesize information from a complex table, combining mean comparisons (static differences) with regression coefficients (dynamic responses) to evaluate the paper's hypotheses on heterogeneity. It uses a Reverse-Reasoning strategy. \n- **Correct A:** This option correctly links the lower mean independence of high-CI firms (73.76% vs 75.45%) with their stronger reaction in board size reduction (-0.106 vs -0.080), which aligns with the paper's argument.\n- **Correct C:** This option correctly notes that low-CEO-power firms have lower mean independence (74.30% vs 78.04%) and that their response to EPU is large and significant (coeff=0.096***), while the high-power group's response is insignificant.\n- **Distractor B:** This is a Conceptual Opposite. The table shows the significant response is in firms with *low* CEO power, not high.\n- **Distractor D:** This is a Conceptual Opposite. The table shows high-CI firms have a *stronger* response in board size reduction, not weaker.", "qid": "49", "question": "### Background\n\nA study investigates how the effect of economic policy uncertainty (EPU) on board structure is moderated by firm characteristics. The analysis involves splitting the sample based on factors like CEO power and capital intensity and running separate regressions.\n\n### Data / Model Specification\n\nThe table below shows regression coefficients for `EPU` from models where board characteristics are the dependent variables, run on different firm subsamples.\n\n**Table 1. Board Change Moderators (Selected Panels)**\n\n**Panel C: CEO Power**\n\n| | CEO Power | Mean | EPU Coeff. | Std. Err. |\n| :--- | :--- | :--- | :--- | :--- |\n| Board size | High | 10.75 | -0.032 | (0.037) |\n| | Low | 10.36 | -0.093** | (0.009) |\n| Board independence | High | 78.04% | 0.009 | (0.028) |\n| | Low | 74.30% | 0.096*** | (0.006) |\n\n**Panel D: Capital Intensity**\n\n| | Capital Intensity | Mean | EPU Coeff. | Std. Err. |\n| :--- | :--- | :--- | :--- | :--- |\n| Board size | High | 10.44 | -0.106*** | (0.011) |\n| | Low | 10.32 | -0.080*** | (0.012) |\n| Board independence | High | 73.76% | 0.074*** | (0.008) |\n| | Low | 75.45% | 0.098*** | (0.008) |\n\n*Note: ***, ** denote significance at 1% and 5% levels.*\n\n### Question\n\nBased on a synthesis of the mean values and regression coefficients in the provided tables, which of the following conclusions about the moderating effects of firm characteristics are supported?", "model_evaluation_result": {"model_raw_response": "Looking at the data systematically:\n\n**Panel C - CEO Power:**\n- Board size: High CEO power shows insignificant effect (-0.032, not significant), while Low CEO power shows significant negative effect (-0.093**)\n- Board independence: High CEO power shows insignificant effect (0.009, not significant), while Low CEO power shows significant positive effect (0.096***)\n- Mean board independence: High = 78.04%, Low = 74.30%\n\n**Panel D - Capital Intensity:**\n- Board size: Both High (-0.106***) and Low (-0.080***) show significant negative effects, with High having stronger response\n- Board independence: Both High (0.074***) and Low (0.098***) show significant positive effects\n- Mean board independence: High = 73.76%, Low = 75.45%\n\nEvaluating each option:\n\nA) False - High capital intensity firms show a stronger (not weaker) response in board size changes (-0.106 vs -0.080)\n\nB) True - High capital intensity firms have lower mean board independence (73.76% vs 75.45%) and show stronger board size reduction (-0.106 vs -0.080)\n\nC) False - The significant response in board independence is concentrated in firms with LOW CEO power (0.096***), not high CEO power (0.009, not significant)\n\nD) True - Low CEO power firms have lower mean board independence (74.30% vs 78.04%) and significantly increase board independence in response to EPU (0.096***)\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 54, "Question": "### Background\n\nA study tests the hypothesis that the interaction between product life cycle (PLC) and the book-to-market effect is driven by mispricing, which is exacerbated by investor inattention. The market-to-book (M/B) ratio is decomposed into a `Mispricing` component and a `Value-to-Book` component. Fama-MacBeth regressions are run in 'Low' and 'High' investor attention subsamples to test the interaction of PLC with each M/B component.\n\n### Data / Model Specification\n\nThe regression includes interaction terms for PLC with both `Mispricing` and `Value-to-Book`.\n\n**Table 1: Fama-MacBeth Results for Interaction Terms in Attention Subsamples**\n\n| | Low Attention | High Attention |\n| :--- | :---: | :---: |\n| **Variable** | **Coeff. (t-stat)** | **Coeff. (t-stat)** |\n| `PLC × Mispricing` | **-0.04 (-3.64)** | -0.01 (-0.53) |\n| `PLC × Value-to-Book` | 0.01 (0.16) | -0.01 (-0.74) |\n\n*Source: Abridged from Table 7. A high 'Mispricing' value corresponds to overvaluation, so a negative coefficient on its interaction implies that a long PLC strengthens the subsequent return reversal for mispriced stocks.* \n\n---\n\nBased on the full pattern of results in Table 1, which of the following conclusions are directly supported? Select all that apply.", "Options": {"A": "The amplifying effect of PLC on return predictability is specifically linked to the mispricing component of the M/B ratio, not the fundamental value component.", "B": "For firms with high investor attention, a long PLC significantly strengthens the return predictability of both the mispricing and fundamental value components of the M/B ratio.", "C": "Regardless of investor attention, PLC has a significant interaction with the fundamental `Value-to-Book` component in predicting returns.", "D": "The interaction between PLC and mispricing is economically and statistically significant, but only within the low investor attention subsample."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret a complex 'triple-interaction' result from a regression table, which is the paper's sharpest identification test. Depth Strategy: Scenario Application. The user must apply the paper's core theory to the specific pattern of coefficients and significance levels in the table. Distractor Logic: C and D are conceptual opposites that misinterpret the insignificant coefficients. C incorrectly claims significance for the high-attention group, while D incorrectly claims significance for the `Value-to-Book` interaction.", "qid": "54", "question": "### Background\n\nA study tests the hypothesis that the interaction between product life cycle (PLC) and the book-to-market effect is driven by mispricing, which is exacerbated by investor inattention. The market-to-book (M/B) ratio is decomposed into a `Mispricing` component and a `Value-to-Book` component. Fama-MacBeth regressions are run in 'Low' and 'High' investor attention subsamples to test the interaction of PLC with each M/B component.\n\n### Data / Model Specification\n\nThe regression includes interaction terms for PLC with both `Mispricing` and `Value-to-Book`.\n\n**Table 1: Fama-MacBeth Results for Interaction Terms in Attention Subsamples**\n\n| | Low Attention | High Attention |\n| :--- | :---: | :---: |\n| **Variable** | **Coeff. (t-stat)** | **Coeff. (t-stat)** |\n| `PLC × Mispricing` | **-0.04 (-3.64)** | -0.01 (-0.53) |\n| `PLC × Value-to-Book` | 0.01 (0.16) | -0.01 (-0.74) |\n\n*Source: Abridged from Table 7. A high 'Mispricing' value corresponds to overvaluation, so a negative coefficient on its interaction implies that a long PLC strengthens the subsequent return reversal for mispriced stocks.* \n\n---\n\nBased on the full pattern of results in Table 1, which of the following conclusions are directly supported? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to analyze the coefficients and t-statistics for each interaction term across both attention subsamples.\n\nFor the Low Attention subsample:\n- `PLC × Mispricing`: -0.04 (t-stat = -3.64) - This is statistically significant (|t| > 2)\n- `PLC × Value-to-Book`: 0.01 (t-stat = 0.16) - This is not statistically significant\n\nFor the High Attention subsample:\n- `PLC × Mispricing`: -0.01 (t-stat = -0.53) - This is not statistically significant\n- `PLC × Value-to-Book`: -0.01 (t-stat = -0.74) - This is not statistically significant\n\nEvaluating each option:\n\nA) This is supported. The PLC interaction is significant only with the mispricing component (in low attention), not with the value-to-book component (insignificant in both subsamples).\n\nB) This is not supported. In the high attention subsample, neither interaction is statistically significant.\n\nC) This is not supported. The PLC × Value-to-Book interaction is not significant in either subsample.\n\nD) This is supported. The PLC × Mispricing interaction shows both economic significance (coefficient of -0.04) and statistical significance (t = -3.64) only in the low attention subsample, while it's insignificant in the high attention subsample.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 110, "Question": "### Background\n\nA study uses a fixed-effects model (FEM) to estimate the effect of tax aggressiveness (`TAG`) on corporate debt (`DEBT`). The model includes firm-specific fixed effects (`\\alpha_i`) and time-specific fixed effects (`\\lambda_t`).\n\n### Data / Model Specification\n\nThe estimated model is:\n\n  \nDEBT_{it} = \\alpha_i + \\lambda_t + \\beta_1 TAG_{it} + \\delta' X_{it} + \\varepsilon_{it} \n \n\nwhere `X_{it}` is a vector of time-varying control variables.\n\n---\n\nSelect all of the following statements that correctly describe the properties and limitations of this empirical strategy.\n", "Options": {"A": "The coefficient `\\beta_1` is estimated using only the variation *between* different firms at a single point in time.", "B": "To address potential time-varying endogeneity, a valid instrumental variable for the change in tax aggressiveness (`\\Delta TAG_{it}`) in a first-differenced GMM model could be a lagged level of tax aggressiveness (e.g., `TAG_{i, t-2}`).", "C": "The inclusion of firm fixed effects (`\\alpha_i`) controls for bias from unobserved, time-invariant firm characteristics, such as a firm's corporate culture regarding risk.", "D": "This model is robust to time-varying endogeneity, such as the simultaneous determination of debt and tax policy within a given year."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the student's understanding of the econometric methodology used in the paper, specifically the purpose of fixed effects and potential solutions for remaining endogeneity. It uses a Reverse-Reasoning strategy (identifying valid properties of a given method). Option A correctly identifies the primary purpose of firm fixed effects. Option D correctly proposes a standard solution (GMM with lagged instruments) for the endogeneity problem that fixed effects do not solve. Option B is a Conceptual Opposite distractor; fixed effects models use *within-firm* variation over time, not between-firm variation. Option C is also a Conceptual Opposite; fixed effects do *not* solve for time-varying endogeneity like simultaneity, which is a key limitation of the approach.", "qid": "110", "question": "### Background\n\nA study uses a fixed-effects model (FEM) to estimate the effect of tax aggressiveness (`TAG`) on corporate debt (`DEBT`). The model includes firm-specific fixed effects (`\\alpha_i`) and time-specific fixed effects (`\\lambda_t`).\n\n### Data / Model Specification\n\nThe estimated model is:\n\n  \nDEBT_{it} = \\alpha_i + \\lambda_t + \\beta_1 TAG_{it} + \\delta' X_{it} + \\varepsilon_{it} \n \n\nwhere `X_{it}` is a vector of time-varying control variables.\n\n---\n\nSelect all of the following statements that correctly describe the properties and limitations of this empirical strategy.\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement about the fixed-effects model:\n\nA) **Incorrect**. In a fixed-effects model, the coefficient β₁ is estimated using only the *within-firm* variation over time, not the between-firm variation. The firm fixed effects (αᵢ) absorb all between-firm variation.\n\nB) **Correct**. In a first-differenced GMM model addressing endogeneity, lagged levels of the endogenous variable (like TAG_{i,t-2}) can serve as valid instruments for the first-differenced variable (ΔTAG_{it}), provided they are correlated with ΔTAG_{it} but uncorrelated with the differenced error term.\n\nC) **Correct**. This is a fundamental property of fixed-effects models. The firm fixed effects (αᵢ) control for all time-invariant firm characteristics, whether observed or unobserved, including stable factors like corporate culture regarding risk.\n\nD) **Incorrect**. Fixed-effects models control for time-invariant unobserved heterogeneity but do not address time-varying endogeneity. If debt and tax policy are simultaneously determined within each period, this creates time-varying endogeneity that the fixed-effects model cannot handle.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 16, "Question": "### Background\n\n**Research Question.** How do analytical bounds for the fair periodic premium of a unit-linked endowment perform numerically, and what are the implications for pricing practice?\n\n**Setting.** The fair periodic premium for a unit-linked contract with guarantee `G=1000` is calculated using comonotonic lower and upper bounds. The calculations are performed for different policyholder ages (`x=30, 50`), contract maturities (`T=10, 15`), investment shares (`a=0.4, 0.6`), and initial term structures of interest rates (TSIR I: flat, II: normal, III: inverse).\n\n### Data / Model Specification\n\nThe fair premium `P` is bounded by `P^l \\leq P \\leq P^u`. These bounds are calculated by replacing the true (but intractable) embedded call option prices with their analytical lower and upper bounds, respectively. The following table presents a selection of these calculated premium bounds.\n\n**Table 1: Lower and Upper Bounds for the Fair Periodic Premium (G=1000)**\n\n| Age | Share (a) | Maturity T=10, TSIR II | Maturity T=15, TSIR III |\n|:---:|:---------:|:----------------------:|:-----------------------:|\n| 30  | 0.4       | 75.17 / 75.32          | 54.93 / 55.08          |\n| 30  | 0.6       | 79.51 / 85.09          | 61.64 / 70.45          |\n| 50  | 0.4       | 79.12 / 79.31          | 61.29 / 61.46          |\n| 50  | 0.6       | 84.03 / 90.31          | 69.48 / 80.55          |\n\n*Note: Table values are presented as `P^l / P^u`.*\n\n---\n\nBased on the data in **Table 1**, which of the following statements are valid interpretations or conclusions?\n", "Options": {"A": "The gap between the lower and upper premium bounds (`P^u - P^l`) is consistently wider for the higher investment share (`a=0.6`) compared to the lower share (`a=0.4`) in all presented scenarios.", "B": "For a 30-year-old with a 10-year maturity and TSIR II, increasing the investment share `a` from 0.4 to 0.6 increases the average premium by more than 5 currency units.", "C": "Holding the investment share `a` constant at 0.6, the average premium for a 50-year-old is consistently higher than for a 30-year-old across both scenarios shown in the table.", "D": "The tightness of the premium bounds for the `a=0.4` cases suggests that the underlying portfolio's distribution is well-approximated by a comonotonic distribution under the paper's model parameters."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses the ability to perform calculations from a table and draw valid theoretical and comparative inferences, targeting the core numerical results of the paper. It uses a Computational Judgment strategy.\n- **Option A (Correct):** Requires calculating two average premiums and their difference. Age 30, a=0.4 avg = (75.17+75.32)/2 = 75.245. Age 30, a=0.6 avg = (79.51+85.09)/2 = 82.3. The difference is 7.055, which is > 5.\n- **Option B (Correct):** This is a key theoretical conclusion from the paper. The tightness of the bounds indicates the approximation is very good, which happens when the underlying sum of log-normals is 'rather' comonotonic.\n- **Option C (Distractor - Almost Right):** This is true for T=10, TSIR II (87.17 vs 82.3) but false for T=15, TSIR III (75.015 vs 66.045). This tests careful, comprehensive checking of the data.\n- **Option D (Correct):** Requires calculating the gap for all four pairs of scenarios. For a=0.4, gaps are 0.15 and 0.15. For a=0.6, gaps are 5.58 and 8.81. The gaps are indeed consistently wider for a=0.6.", "qid": "16", "question": "### Background\n\n**Research Question.** How do analytical bounds for the fair periodic premium of a unit-linked endowment perform numerically, and what are the implications for pricing practice?\n\n**Setting.** The fair periodic premium for a unit-linked contract with guarantee `G=1000` is calculated using comonotonic lower and upper bounds. The calculations are performed for different policyholder ages (`x=30, 50`), contract maturities (`T=10, 15`), investment shares (`a=0.4, 0.6`), and initial term structures of interest rates (TSIR I: flat, II: normal, III: inverse).\n\n### Data / Model Specification\n\nThe fair premium `P` is bounded by `P^l \\leq P \\leq P^u`. These bounds are calculated by replacing the true (but intractable) embedded call option prices with their analytical lower and upper bounds, respectively. The following table presents a selection of these calculated premium bounds.\n\n**Table 1: Lower and Upper Bounds for the Fair Periodic Premium (G=1000)**\n\n| Age | Share (a) | Maturity T=10, TSIR II | Maturity T=15, TSIR III |\n|:---:|:---------:|:----------------------:|:-----------------------:|\n| 30  | 0.4       | 75.17 / 75.32          | 54.93 / 55.08          |\n| 30  | 0.6       | 79.51 / 85.09          | 61.64 / 70.45          |\n| 50  | 0.4       | 79.12 / 79.31          | 61.29 / 61.46          |\n| 50  | 0.6       | 84.03 / 90.31          | 69.48 / 80.55          |\n\n*Note: Table values are presented as `P^l / P^u`.*\n\n---\n\nBased on the data in **Table 1**, which of the following statements are valid interpretations or conclusions?\n", "model_evaluation_result": {"model_raw_response": "Looking at the table data, I need to evaluate each statement:\n\n**Statement A**: Checking the gap (P^u - P^l) for each scenario:\n- Age 30, T=10, TSIR II: a=0.4 gap = 0.15; a=0.6 gap = 5.58\n- Age 30, T=15, TSIR III: a=0.4 gap = 0.15; a=0.6 gap = 8.81\n- Age 50, T=10, TSIR II: a=0.4 gap = 0.19; a=0.6 gap = 6.28\n- Age 50, T=15, TSIR III: a=0.4 gap = 0.17; a=0.6 gap = 11.07\n\nThe gap is indeed consistently wider for a=0.6 in all scenarios. ✓\n\n**Statement B**: For age 30, T=10, TSIR II:\n- Average premium at a=0.4: (75.17 + 75.32)/2 = 75.245\n- Average premium at a=0.6: (79.51 + 85.09)/2 = 82.30\n- Increase: 82.30 - 75.245 = 7.055 > 5 ✓\n\n**Statement C**: Comparing average premiums for a=0.6:\n- T=10, TSIR II: Age 30 avg = 82.30; Age 50 avg = 87.17 (50 > 30) ✓\n- T=15, TSIR III: Age 30 avg = 66.045; Age 50 avg = 75.015 (50 > 30) ✓\n\n**Statement D**: The bounds for a=0.4 are very tight (gaps of 0.15-0.19), which suggests good approximation by the comonotonic bounds. In the context of unit-linked insurance with guarantees, tight bounds indicate that the comonotonic approximation captures the portfolio distribution well. ✓\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 77, "Question": "### Background\n\n**Research Question.** This case provides a comprehensive examination of agency-induced short-termism in delegated asset management. It investigates the entire causal chain, from fund managers' *perception* that performance monitoring creates pressure, to the *behavioral evidence* of shortened investment horizons, and finally to the direct *correlates* of this behavior.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of 64 UK fund managers. For some analyses, the focus is on the subsample of pension fund managers, who are identified as being subject to the most intense degree of relative performance monitoring. The average holding period of a UK equity is used as a proxy for the manager's investment time horizon.\n\n---\n\n### Data / Model Specification\n\nThe following tables present evidence on managers' perceptions of monitoring, their reported holding periods, and the correlation of those holding periods with incentive structures.\n\n**Table 1. Attitudes towards the influence of external performance measurement**\n\n| To what extent does external performance measurement: | Not at all (%) | Slight (%) | Moderate (%) | Considerable (%) | Extreme (%) |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| ...influence the management of a fund | 25.0 | 9.4 | 17.2 | 23.4 | 25.0 |\n| ...result in a short-term approach | 18.8 | 15.6 | 21.9 | 28.1 | 15.6 |\n\n**Table 2. UK equities: interviewees’ estimates of average holding periods (in months) according to principal type of fund managed**\n\n| | Pension Fund | Life Fund | Private Client | Kruskal-Wallis (chi²) | Significance |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| **Median** | **36** | **48** | **60** | **5.55** | **0.06** |\n\n**Table 3. Correlation of estimates of average holding periods (in months) for pension fund managers**\n\n| Correlation between average holding period (in months) and: | Correlation Coefficient | Test | Significance |\n| :--- | :---: | :---: | :---: |\n| frequency of performance monitoring | -0.1184 | S | 0.217 |\n| proportion of pay composed of bonus | -0.1760 | P | 0.121 |\n\n---\n\n### Question\n\nBased on the survey results presented, which of the following conclusions about the link between performance monitoring and fund manager behavior are supported by the data?", "Options": {"A": "Pension funds, the group subject to the most intense relative monitoring, exhibit a median equity holding period (36 months) that is statistically significantly shorter than that of other fund types.", "B": "The data show that more frequent performance monitoring leads to significantly longer holding periods, as managers take a more considered approach.", "C": "For pension fund managers, a higher proportion of performance-related bonus pay is significantly correlated with shorter holding periods.", "D": "A substantial portion of managers (over 40%) perceive that external performance measurement directly encourages a short-term approach to fund management."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the core empirical findings linking perception, behavior, and incentives. It uses an 'Atomic Decomposition' strategy, breaking the original QA's multi-part question into distinct, verifiable statements. Correct options A and B require correct interpretation of percentages from Table 1 and the median and significance from Table 2. Distractor C is an 'Almost Right' option that tests attention to statistical significance in Table 3. Distractor D is a 'Conceptual Opposite' that tests understanding of the hypothesized relationship's direction.", "qid": "77", "question": "### Background\n\n**Research Question.** This case provides a comprehensive examination of agency-induced short-termism in delegated asset management. It investigates the entire causal chain, from fund managers' *perception* that performance monitoring creates pressure, to the *behavioral evidence* of shortened investment horizons, and finally to the direct *correlates* of this behavior.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of 64 UK fund managers. For some analyses, the focus is on the subsample of pension fund managers, who are identified as being subject to the most intense degree of relative performance monitoring. The average holding period of a UK equity is used as a proxy for the manager's investment time horizon.\n\n---\n\n### Data / Model Specification\n\nThe following tables present evidence on managers' perceptions of monitoring, their reported holding periods, and the correlation of those holding periods with incentive structures.\n\n**Table 1. Attitudes towards the influence of external performance measurement**\n\n| To what extent does external performance measurement: | Not at all (%) | Slight (%) | Moderate (%) | Considerable (%) | Extreme (%) |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| ...influence the management of a fund | 25.0 | 9.4 | 17.2 | 23.4 | 25.0 |\n| ...result in a short-term approach | 18.8 | 15.6 | 21.9 | 28.1 | 15.6 |\n\n**Table 2. UK equities: interviewees’ estimates of average holding periods (in months) according to principal type of fund managed**\n\n| | Pension Fund | Life Fund | Private Client | Kruskal-Wallis (chi²) | Significance |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| **Median** | **36** | **48** | **60** | **5.55** | **0.06** |\n\n**Table 3. Correlation of estimates of average holding periods (in months) for pension fund managers**\n\n| Correlation between average holding period (in months) and: | Correlation Coefficient | Test | Significance |\n| :--- | :---: | :---: | :---: |\n| frequency of performance monitoring | -0.1184 | S | 0.217 |\n| proportion of pay composed of bonus | -0.1760 | P | 0.121 |\n\n---\n\n### Question\n\nBased on the survey results presented, which of the following conclusions about the link between performance monitoring and fund manager behavior are supported by the data?", "model_evaluation_result": {"model_raw_response": "Looking at the data systematically:\n\n**Option A**: Table 2 shows pension funds have a median holding period of 36 months, compared to 48 months for life funds and 60 months for private clients. The Kruskal-Wallis test shows chi² = 5.55 with significance = 0.06. While this is close to the conventional 0.05 threshold, it indicates the difference is marginally significant or significant at the 0.10 level. Given the context emphasizes pension funds are subject to \"the most intense degree of relative performance monitoring\" and they do show the shortest holding periods, this conclusion is supported.\n\n**Option B**: This is contradicted by Table 3, which shows a negative correlation (-0.1184) between frequency of performance monitoring and holding period, though not statistically significant (p = 0.217).\n\n**Option C**: Table 3 shows the correlation between proportion of bonus pay and holding period is -0.1760 with significance = 0.121. This is not statistically significant at conventional levels.\n\n**Option D**: Table 1 shows that 28.1% + 15.6% = 43.7% of managers believe external performance measurement results in a \"considerable\" or \"extreme\" short-term approach. This exceeds 40%.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 150, "Question": "### Background\n\nA risk-averse agent with mean-variance utility must choose their optimal holdings in a futures contract (`ξ`) and the stock market (`S`). The agent may also have a nonmarketable, risky revenue endowment (`r`).\n\n### Data / Model Specification\n\nThe agent's objective is to maximize:\n\n  \nU = E[C] - \\frac{\\alpha}{2} \\mathrm{var}(C) \\quad \\text{(Eq. (1))}\n \n\nFor an agent who trades futures, consumption is given by:\n\n  \nC = W - t + r + \\xi \\tilde{\\Pi} + S R_m \\quad \\text{(Eq. (2))}\n \n\nThis optimization leads to the following optimal futures position (from Proposition 1):\n\n  \n\\xi = \\frac{\\Pi/\\alpha - \\mathrm{cov}(\\tilde{\\Pi}, r + S R_m)}{\\mathrm{var}(\\tilde{\\Pi})} \\quad \\text{(Eq. (3))}\n \n\nWhere `Π` is the risk premium `E[Π̃]`, `α` is risk aversion, and `S` is the simultaneously chosen stock position.\n\n### Question\n\nBased on the structure of the optimal futures position in `Eq. (3)`, which of the following statements are valid interpretations of an agent's motives for holding a futures position?\n\nSelect all that apply.", "Options": {"A": "The term `Π/α` represents the speculative component of demand; its size increases with the expected profit (`Π`) and the agent's risk aversion (`α`).", "B": "The term `-cov(Π̃, S R_m)` represents a cross-hedge against the risk of the agent's stock market portfolio.", "C": "The term `-cov(Π̃, r)` represents a direct hedge against the agent's nonmarketable background risk.", "D": "If the futures payoff `Π̃` is uncorrelated with both the stock market and the agent's nonmarketable revenue `r`, the agent will hold a zero futures position regardless of the premium `Π`."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to decompose and interpret the components of an agent's optimal demand function (Proposition 1), identifying the distinct economic motives for trading. Depth Strategy: Reverse-Reasoning. The candidate is given the optimal demand equation and must identify the correct conceptual interpretation of its constituent parts. Distractor Logic: Option A is a Conceptual Distractor (Sign Error); speculative demand increases with risk *tolerance* (`1/α`), not risk aversion (`α`). Option D is a Conceptual Distractor (Step-Omission Error); if both covariances are zero, the demand simplifies to `ξ = Π / (α * var(Π̃))`, which is the pure speculative demand. This position is only zero if the premium `Π` is zero. Options B and C are correct and accurately describe the two distinct hedging motives embedded in the demand function: hedging background risk (`r`) and cross-hedging the traded asset portfolio (`S R_m`).", "qid": "150", "question": "### Background\n\nA risk-averse agent with mean-variance utility must choose their optimal holdings in a futures contract (`ξ`) and the stock market (`S`). The agent may also have a nonmarketable, risky revenue endowment (`r`).\n\n### Data / Model Specification\n\nThe agent's objective is to maximize:\n\n  \nU = E[C] - \\frac{\\alpha}{2} \\mathrm{var}(C) \\quad \\text{(Eq. (1))}\n \n\nFor an agent who trades futures, consumption is given by:\n\n  \nC = W - t + r + \\xi \\tilde{\\Pi} + S R_m \\quad \\text{(Eq. (2))}\n \n\nThis optimization leads to the following optimal futures position (from Proposition 1):\n\n  \n\\xi = \\frac{\\Pi/\\alpha - \\mathrm{cov}(\\tilde{\\Pi}, r + S R_m)}{\\mathrm{var}(\\tilde{\\Pi})} \\quad \\text{(Eq. (3))}\n \n\nWhere `Π` is the risk premium `E[Π̃]`, `α` is risk aversion, and `S` is the simultaneously chosen stock position.\n\n### Question\n\nBased on the structure of the optimal futures position in `Eq. (3)`, which of the following statements are valid interpretations of an agent's motives for holding a futures position?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the optimal futures position equation:\n\nξ = [Π/α - cov(Π̃, r + S R_m)] / var(Π̃)\n\nLet me analyze each option:\n\nA) The term `Π/α` represents the speculative component of demand; its size increases with the expected profit (`Π`) and the agent's risk aversion (`α`).\n\nThis is incorrect. While Π/α does represent the speculative component, its size decreases with risk aversion α (since α is in the denominator). Higher risk aversion leads to lower speculative demand.\n\nB) The term `-cov(Π̃, S R_m)` represents a cross-hedge against the risk of the agent's stock market portfolio.\n\nThis is correct. The covariance term cov(Π̃, r + S R_m) can be expanded to cov(Π̃, r) + cov(Π̃, S R_m). The negative sign on cov(Π̃, S R_m) means the agent reduces futures holdings when futures payoffs are positively correlated with stock returns, which is a cross-hedging motive.\n\nC) The term `-cov(Π̃, r)` represents a direct hedge against the agent's nonmarketable background risk.\n\nThis is correct. Similar to option B, the negative sign on cov(Π̃, r) means the agent reduces futures holdings when futures payoffs are positively correlated with background risk r, representing a direct hedge.\n\nD) If the futures payoff `Π̃` is uncorrelated with both the stock market and the agent's nonmarketable revenue `r`, the agent will hold a zero futures position regardless of the premium `Π`.\n\nThis is incorrect. If cov(Π̃, r) = 0 and cov(Π̃, S R_m) = 0, then ξ = Π/(α × var(Π̃)). The agent will hold a non-zero position if Π ≠ 0, taking a purely speculative position based on the risk premium.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 58, "Question": "### Background\n\n**Research Question.** What are the true determinants of equilibrium interest rates on auto loans, and how does failing to account for truncation from legally-imposed rate ceilings bias standard econometric estimates?\n\n**Setting / Data-Generating Environment.** The analysis uses a sample of 1,039 auto loans. The key challenge is that observed interest rates are truncated from above by state-level ceilings. The paper contrasts estimates from a standard Ordinary Least Squares (OLS) regression with those from a consistent maximum likelihood procedure (Hausman-Wise, or HW) that explicitly models this truncation.\n\n---\n\n### Data / Model Specification\n\nThe underlying hedonic model for the unobserved equilibrium rate `r_i` is:\n  \n\\ln r_{i} = X_{i}^{\\prime}\\beta + \\epsilon_{i} \\quad \\text{(Eq. (1))}\n \nwhere `ε_i` is an unobserved error term, assumed `N(0, σ^2)`. We only observe `ln(r_i)` if `r_i ≤ c_i`, where `c_i` is the applicable ceiling. This truncation violates a key OLS assumption.\n\n**Table 1: Selected OLS and HW Regression Estimates**\n(Dependent Variable: `ln(r)`)\n\n| Variable | OLS Estimate | HW Estimate |\n|:---|---:|---:|\n| HDCGGD (College Grad) | -0.0493 | -0.1606 |\n| | (0.0293) | (0.0905) |\n\n**Table 2: OLS and HW Estimates for a Low-Ceiling State Indicator**\n(Relative to no-ceiling states)\n\n| Variable | OLS Estimate | HW Estimate |\n|:---|---:|---:|\n| LOCEIL (Low-Ceiling State) | -0.3432 | -0.0302 |\n| | (0.0404) | (0.1338) |\n\n*Asymptotic standard errors in parentheses.*\n\n---\n\n### Question\n\nGiven the model and the regression results comparing OLS and the consistent Hausman-Wise (HW) estimator, which of the following statements correctly describe the econometric problem of truncation bias and its consequences?", "Options": {"A": "The large, negative OLS coefficient for the `LOCEIL` indicator is spurious; it misinterprets a mechanical truncation effect (the removal of high-rate loans from the sample) as a behavioral shift in lender pricing.", "B": "The OLS estimate for the effect of a college degree (`HDCGGD`) is biased toward zero (attenuation bias) compared to the consistent HW estimate, understating the true impact of this characteristic.", "C": "The HW model is preferred because it confirms the OLS finding that equilibrium interest rates are structurally lower in states with low ceilings.", "D": "In the sample of observed loans (where the equilibrium rate is at or below the ceiling), the conditional expectation of the error term `ε_i` is negative, violating a key OLS assumption."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses a deep understanding of the paper's central econometric argument regarding truncation bias. It uses an Atomic Decomposition strategy to test three distinct facets of the issue: the theoretical origin of the bias (A), its consequence for policy variables (B), and its consequence for borrower characteristic variables (C). This requires the user to synthesize theory and empirical results from multiple tables. Distractor D is a Conceptual Opposite that presents the incorrect OLS conclusion as the correct HW conclusion, effectively testing whether the student understands the core methodological critique of the paper.", "qid": "58", "question": "### Background\n\n**Research Question.** What are the true determinants of equilibrium interest rates on auto loans, and how does failing to account for truncation from legally-imposed rate ceilings bias standard econometric estimates?\n\n**Setting / Data-Generating Environment.** The analysis uses a sample of 1,039 auto loans. The key challenge is that observed interest rates are truncated from above by state-level ceilings. The paper contrasts estimates from a standard Ordinary Least Squares (OLS) regression with those from a consistent maximum likelihood procedure (Hausman-Wise, or HW) that explicitly models this truncation.\n\n---\n\n### Data / Model Specification\n\nThe underlying hedonic model for the unobserved equilibrium rate `r_i` is:\n  \n\\ln r_{i} = X_{i}^{\\prime}\\beta + \\epsilon_{i} \\quad \\text{(Eq. (1))}\n \nwhere `ε_i` is an unobserved error term, assumed `N(0, σ^2)`. We only observe `ln(r_i)` if `r_i ≤ c_i`, where `c_i` is the applicable ceiling. This truncation violates a key OLS assumption.\n\n**Table 1: Selected OLS and HW Regression Estimates**\n(Dependent Variable: `ln(r)`)\n\n| Variable | OLS Estimate | HW Estimate |\n|:---|---:|---:|\n| HDCGGD (College Grad) | -0.0493 | -0.1606 |\n| | (0.0293) | (0.0905) |\n\n**Table 2: OLS and HW Estimates for a Low-Ceiling State Indicator**\n(Relative to no-ceiling states)\n\n| Variable | OLS Estimate | HW Estimate |\n|:---|---:|---:|\n| LOCEIL (Low-Ceiling State) | -0.3432 | -0.0302 |\n| | (0.0404) | (0.1338) |\n\n*Asymptotic standard errors in parentheses.*\n\n---\n\n### Question\n\nGiven the model and the regression results comparing OLS and the consistent Hausman-Wise (HW) estimator, which of the following statements correctly describe the econometric problem of truncation bias and its consequences?", "model_evaluation_result": {"model_raw_response": "Looking at this truncation bias problem, I need to analyze each statement carefully.\n\n**Statement A**: The OLS estimate for LOCEIL is -0.3432 (highly negative), while the HW estimate is only -0.0302 (close to zero). This dramatic difference suggests the OLS estimate is picking up the mechanical effect of truncation - in low-ceiling states, we only observe loans with rates below the ceiling, which artificially lowers the average observed rate. The HW estimator corrects for this truncation and shows the true behavioral effect is minimal. This statement is **correct**.\n\n**Statement B**: The OLS estimate for HDCGGD is -0.0493, while the HW estimate is -0.1606. The HW estimate is more negative (larger in absolute value), indicating that OLS underestimates the true effect of having a college degree on reducing interest rates. This is classic attenuation bias due to truncation. This statement is **correct**.\n\n**Statement C**: This is incorrect. The HW model actually shows that the true effect of low ceilings on equilibrium rates is very small (-0.0302), contradicting the large negative OLS estimate. The HW model does not confirm the OLS finding.\n\n**Statement D**: When we have truncation from above (only observing r_i ≤ c_i), the conditional expectation E[ε_i | r_i ≤ c_i] < 0. This is because we're systematically excluding observations with large positive errors. This violates the OLS assumption that E[ε_i | X_i] = 0. This statement is **correct**.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 40, "Question": "### Background\n\nIn the Black-Scholes framework, the valuation of an American put option is a free-boundary problem. The domain is split into a continuation region, where the option price `p(x, τ)` follows a PDE, and an exercise region, where the price is its intrinsic value. The optimal exercise boundary, `c(τ)`, is the critical asset price that separates these regions.\n\n### Data / Model Specification\n\nIn the continuation region (`x > c(τ)`), the option price is governed by the Black-Scholes PDE:\n  \n\\frac{\\partial p}{\\partial\\tau}=\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}p}{\\partial x^{2}}+b x\\frac{\\partial p}{\\partial x}-r p \\quad \\text{(Eq. 1)}\n \nAt the optimal exercise boundary `x = c(τ)`, two conditions must hold:\n1.  **Value Matching Condition**: The option's price must equal its intrinsic value.\n      \n    p(c(\\tau), \\tau) = q - c(\\tau) \\quad \\text{(Eq. 2)}\n     \n2.  **Smooth Pasting Condition**: The option's delta must be continuous and equal to the delta of the payoff function.\n      \n    \\frac{\\partial p(c(\\tau), \\tau)}{\\partial x} = -1 \\quad \\text{(Eq. 3)}\n     \n\n---\n\nBased on the provided model, which of the following statements are valid consequences of applying the boundary conditions to the Black-Scholes PDE at the exercise boundary `x = c(τ)`? Select all that apply.", "Options": {"A": "The time decay of the option price at the boundary, `∂p/∂τ`, is equal to the risk-free rate `r` times the option's intrinsic value `(q - c(τ))`.", "B": "The option's Gamma at the boundary, `∂²p/∂x²`, is given by `(2 / (σ²c(τ)²)) * (rq - (r-b)c(τ))`.", "C": "The option's Gamma at the boundary, `∂²p/∂x²`, must be zero to ensure a smooth transition to the exercise region.", "D": "The time decay of the option price at the boundary, `∂p/∂τ`, is zero."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: Test the user's ability to derive the direct consequences of the value matching and smooth pasting conditions when applied to the Black-Scholes PDE at the free boundary. This is a foundational step for deriving the main boundary evolution equation.\nDepth Strategy: Computational Judgment. The user must perform a multi-step derivation to validate options A and B.\nDistractor Logic:\n- C (Conceptual Opposite): This is a common misconception. Gamma is continuous at the boundary but is generally non-zero. The Gamma in the exercise region (for x < c(τ)) is zero, but the smooth pasting condition applies to the first derivative (Delta), not the second (Gamma).\n- D (Step-Omission Error): This distractor arises if the user incorrectly evaluates the PDE at the boundary, ignoring the Gamma and Delta terms and only keeping the `rp` term, leading to `∂p/∂τ = -rp`.", "qid": "40", "question": "### Background\n\nIn the Black-Scholes framework, the valuation of an American put option is a free-boundary problem. The domain is split into a continuation region, where the option price `p(x, τ)` follows a PDE, and an exercise region, where the price is its intrinsic value. The optimal exercise boundary, `c(τ)`, is the critical asset price that separates these regions.\n\n### Data / Model Specification\n\nIn the continuation region (`x > c(τ)`), the option price is governed by the Black-Scholes PDE:\n  \n\\frac{\\partial p}{\\partial\\tau}=\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}p}{\\partial x^{2}}+b x\\frac{\\partial p}{\\partial x}-r p \\quad \\text{(Eq. 1)}\n \nAt the optimal exercise boundary `x = c(τ)`, two conditions must hold:\n1.  **Value Matching Condition**: The option's price must equal its intrinsic value.\n      \n    p(c(\\tau), \\tau) = q - c(\\tau) \\quad \\text{(Eq. 2)}\n     \n2.  **Smooth Pasting Condition**: The option's delta must be continuous and equal to the delta of the payoff function.\n      \n    \\frac{\\partial p(c(\\tau), \\tau)}{\\partial x} = -1 \\quad \\text{(Eq. 3)}\n     \n\n---\n\nBased on the provided model, which of the following statements are valid consequences of applying the boundary conditions to the Black-Scholes PDE at the exercise boundary `x = c(τ)`? Select all that apply.", "model_evaluation_result": {"model_raw_response": "I need to analyze what happens when we apply the boundary conditions to the Black-Scholes PDE at the exercise boundary.\n\nGiven:\n- PDE in continuation region: $\\frac{\\partial p}{\\partial\\tau}=\\frac{1}{2}\\sigma^{2}x^{2}\\frac{\\partial^{2}p}{\\partial x^{2}}+b x\\frac{\\partial p}{\\partial x}-r p$\n- At boundary $x = c(\\tau)$:\n  - Value matching: $p(c(\\tau), \\tau) = q - c(\\tau)$\n  - Smooth pasting: $\\frac{\\partial p(c(\\tau), \\tau)}{\\partial x} = -1$\n\nLet me evaluate each option:\n\n**Option A**: At the boundary, substituting the value matching condition into the PDE:\n- We have $p = q - c(\\tau)$\n- So $-rp = -r(q - c(\\tau))$\n- From the PDE: $\\frac{\\partial p}{\\partial\\tau} = \\frac{1}{2}\\sigma^{2}c(\\tau)^{2}\\frac{\\partial^{2}p}{\\partial x^{2}} + bc(\\tau)(-1) - r(q - c(\\tau))$\n- This simplifies to: $\\frac{\\partial p}{\\partial\\tau} = \\frac{1}{2}\\sigma^{2}c(\\tau)^{2}\\frac{\\partial^{2}p}{\\partial x^{2}} - bc(\\tau) - rq + rc(\\tau)$\n- This is NOT simply $r(q - c(\\tau))$\n\n**Option B**: From the PDE at the boundary with the conditions:\n- $\\frac{\\partial p}{\\partial\\tau} = \\frac{1}{2}\\sigma^{2}c(\\tau)^{2}\\frac{\\partial^{2}p}{\\partial x^{2}} - bc(\\tau) - r(q - c(\\tau))$\n- Rearranging for $\\frac{\\partial^{2}p}{\\partial x^{2}}$:\n- $\\frac{\\partial^{2}p}{\\partial x^{2}} = \\frac{2}{\\sigma^{2}c(\\tau)^{2}}\\left[\\frac{\\partial p}{\\partial\\tau} + bc(\\tau) + rq - rc(\\tau)\\right]$\n- We need to determine $\\frac{\\partial p}{\\partial\\tau}$ at the boundary\n- Taking the time derivative of $p(c(\\tau), \\tau) = q - c(\\tau)$: $\\frac{\\partial p}{\\partial\\tau} = -c'(\\tau)$\n- Substituting: $\\frac{\\partial^{2}p}{\\partial x^{2}} = \\frac{2}{\\sigma^{2}c(\\tau)^{2}}\\left[-c'(\\tau) + bc(\\tau) + rq - rc(\\tau)\\right]$\n- For the American put, at the optimal boundary: $c'(\\tau) = bc(\\tau) - rq + rc(\\tau) - bc(\\tau) = (r-b)c(\\tau) - rq$\n- Therefore: $\\frac{\\partial^{2}p}{\\partial x^{2}} = \\frac{2}{\\sigma^{2}c(\\tau)^{2}}[rq - (r-b)c(\\tau)]$\n\n**Option C**: The Gamma is NOT zero at the boundary. This would violate the PDE.\n\n**Option D**: From the analysis above, $\\frac{\\partial p}{\\partial\\tau} = -c'(\\tau) \\neq 0$ in general.\n\nB", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 6, "Question": "### Background\n\n**Research Question.** A researcher wishes to test if a single, latent investor sentiment factor drives asset returns, and proposes using the presidential party dummy as an instrumental variable (IV) for this unobserved factor.\n\n**Setting / Data-Generating Environment.** In an IV framework, a valid instrument must satisfy two key conditions:\n1.  **Relevance:** The instrument must be correlated with the endogenous variable (here, latent sentiment).\n2.  **Exclusion Restriction:** The instrument can only affect the outcome variable (asset returns) *through* the endogenous variable. It must be uncorrelated with the regression's error term.\n\n---\n\n### Data / Model Specification\n\n- **Model:** `R_{i,t} = \\alpha_i + \\beta_i (Sentiment_t) + \\varepsilon_{i,t}`\n- **Instrument:** Presidential Party Dummy (`X_{t-1}`)\n\n---\n\n### Question\n\nRegarding the proposal to use the presidential party dummy as an instrument for latent investor sentiment, select all of the following statements that are correct.", "Options": {"A": "The exclusion restriction is likely violated because the president's party can influence asset returns through other channels, such as fiscal policy or regulation, that are not captured by a single sentiment factor.", "B": "If the presidential party dummy is a valid instrument, it guarantees that the estimated model will have a high R-squared.", "C": "The exclusion restriction requires that the presidential party dummy must be completely uncorrelated with asset returns.", "D": "The relevance condition requires that the political party of the president must be correlated with investor sentiment."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the high-level econometric critique from the source QA. It uses the Premise/Assumption Packaging strategy to test the core concepts of instrumental variables. Options A and B correctly state the relevance condition and the likely violation of the exclusion restriction in this specific context. Distractor C is a common Conceptual Error, misstating the exclusion restriction (the instrument must be uncorrelated with the error term, not the dependent variable itself). Distractor D is True but Irrelevant, confusing the purpose of IV (causal identification) with predictive power (R-squared).", "qid": "6", "question": "### Background\n\n**Research Question.** A researcher wishes to test if a single, latent investor sentiment factor drives asset returns, and proposes using the presidential party dummy as an instrumental variable (IV) for this unobserved factor.\n\n**Setting / Data-Generating Environment.** In an IV framework, a valid instrument must satisfy two key conditions:\n1.  **Relevance:** The instrument must be correlated with the endogenous variable (here, latent sentiment).\n2.  **Exclusion Restriction:** The instrument can only affect the outcome variable (asset returns) *through* the endogenous variable. It must be uncorrelated with the regression's error term.\n\n---\n\n### Data / Model Specification\n\n- **Model:** `R_{i,t} = \\alpha_i + \\beta_i (Sentiment_t) + \\varepsilon_{i,t}`\n- **Instrument:** Presidential Party Dummy (`X_{t-1}`)\n\n---\n\n### Question\n\nRegarding the proposal to use the presidential party dummy as an instrument for latent investor sentiment, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "To determine which statements are correct, I need to analyze each option against the IV framework requirements.\n\n**Option A**: \"The exclusion restriction is likely violated because the president's party can influence asset returns through other channels, such as fiscal policy or regulation, that are not captured by a single sentiment factor.\"\n\nThis is CORRECT. The exclusion restriction requires that the instrument (presidential party dummy) affects the outcome (asset returns) ONLY through the endogenous variable (sentiment). However, the president's party can clearly affect asset returns through multiple channels beyond sentiment - fiscal policy, regulatory changes, tax policy, etc. This violates the exclusion restriction.\n\n**Option B**: \"If the presidential party dummy is a valid instrument, it guarantees that the estimated model will have a high R-squared.\"\n\nThis is INCORRECT. A valid instrument ensures consistent estimation of the causal effect, but it does not guarantee a high R-squared. In fact, IV estimation often results in lower R-squared values compared to OLS.\n\n**Option C**: \"The exclusion restriction requires that the presidential party dummy must be completely uncorrelated with asset returns.\"\n\nThis is INCORRECT. The exclusion restriction requires that the instrument be uncorrelated with the error term (ε), not with the outcome variable (asset returns). The instrument can and should be correlated with the outcome variable, but only through its effect on the endogenous variable (sentiment).\n\n**Option D**: \"The relevance condition requires that the political party of the president must be correlated with investor sentiment.\"\n\nThis is CORRECT. The relevance condition explicitly requires that the instrument (presidential party dummy) be correlated with the endogenous variable (investor sentiment). Without this correlation, the instrument would be weak and unable to identify the causal effect.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 140, "Question": "### Background\n\n**Research Question.** What economic and behavioral factors explain the wide cross-sectional variation in the returns from bond refunding operations?\n\n**Setting / Data-Generating Environment.** An empirical study of 40 bond refunding operations by public utilities in 1962-63 found a very wide range of after-tax internal rates of return (IRR), as shown in Table 1. The author investigated potential drivers of this dispersion and found no correlation between the IRRs and quantitative firm characteristics like size, profitability, or debt-to-equity ratios.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Rates of Return Earned on Forty Bond Refundings by Public Utilities in 1962 and 1963 (Percent)**\n\n| | | | |\n| :--- | :--- | :--- | :--- |\n| 3.6* | 9.1 | 11.5* | 14.7* |\n| 5.5 | 9.5 | 11.9* | 14.8 |\n| 5.7 | 9.6* | 12.3 | 15.3* |\n| 6.5 | 9.7 | 12.3 | 16.1 |\n| 6.8 | 9.9 | 12.8* | 16.1 |\n| 7.1* | 10.1 | 12.8 | 17.2 |\n| 7.3 | 10.4* | 13.0 | 19.7 |\n| 8.0 | 10.4 | 13.1 | 23.8 |\n| 8.2 | 11.2 | 13.8 | 26.7* |\n| 8.7 | 11.3 | 14.6* | 43.4 |\n\n*Note: An asterisk (*) denotes that reasons in addition to interest savings were cited for the refunding.*\n\n---\n\n### Question\n\nAccording to the paper's analysis, which of the following factors help explain the wide dispersion of IRRs observed in Table 1? Select all that apply.", "Options": {"A": "The fact that utilities in the natural gas sector were more aggressive and consistently achieved higher IRRs than electric utilities.", "B": "Firms' differing expectations about the future path of interest rates, which affected the timing of their refunding decisions.", "C": "The use of diverse and sometimes theoretically incorrect methods by different firms to measure interest savings, leading to different decision thresholds.", "D": "A systematic relationship where firms with higher debt-to-equity ratios demanded higher IRRs to compensate for financial risk."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses comprehension of the author's qualitative explanations for the empirical results. It uses a Domain Enumeration strategy, asking the user to select valid reasons for the observed IRR dispersion from a list. The correct options (A, B) are two of the five reasons explicitly stated in the paper. The distractors (C, D) are designed to be high-fidelity because they represent plausible hypotheses that the paper explicitly tested and rejected, stating that 'No correlations whatsoever were found' with financial characteristics or the 'particular type of utility'.", "qid": "140", "question": "### Background\n\n**Research Question.** What economic and behavioral factors explain the wide cross-sectional variation in the returns from bond refunding operations?\n\n**Setting / Data-Generating Environment.** An empirical study of 40 bond refunding operations by public utilities in 1962-63 found a very wide range of after-tax internal rates of return (IRR), as shown in Table 1. The author investigated potential drivers of this dispersion and found no correlation between the IRRs and quantitative firm characteristics like size, profitability, or debt-to-equity ratios.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Rates of Return Earned on Forty Bond Refundings by Public Utilities in 1962 and 1963 (Percent)**\n\n| | | | |\n| :--- | :--- | :--- | :--- |\n| 3.6* | 9.1 | 11.5* | 14.7* |\n| 5.5 | 9.5 | 11.9* | 14.8 |\n| 5.7 | 9.6* | 12.3 | 15.3* |\n| 6.5 | 9.7 | 12.3 | 16.1 |\n| 6.8 | 9.9 | 12.8* | 16.1 |\n| 7.1* | 10.1 | 12.8 | 17.2 |\n| 7.3 | 10.4* | 13.0 | 19.7 |\n| 8.0 | 10.4 | 13.1 | 23.8 |\n| 8.2 | 11.2 | 13.8 | 26.7* |\n| 8.7 | 11.3 | 14.6* | 43.4 |\n\n*Note: An asterisk (*) denotes that reasons in addition to interest savings were cited for the refunding.*\n\n---\n\n### Question\n\nAccording to the paper's analysis, which of the following factors help explain the wide dispersion of IRRs observed in Table 1? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data and information provided:\n\n1. The background states that the author \"found no correlation between the IRRs and quantitative firm characteristics like size, profitability, or debt-to-equity ratios.\"\n\nThis directly contradicts option D, which claims there was \"a systematic relationship where firms with higher debt-to-equity ratios demanded higher IRRs.\" Since no correlation was found with debt-to-equity ratios, D is incorrect.\n\nOption A claims utilities in the natural gas sector achieved higher IRRs than electric utilities, but there's no information provided about sector differences or any breakdown by utility type.\n\nOptions B and C are plausible explanations for IRR dispersion:\n- B suggests different expectations about future interest rates affected timing decisions\n- C suggests different measurement methods led to different decision thresholds\n\nHowever, without explicit confirmation from the paper's analysis that these factors were identified as explanations for the dispersion, I cannot definitively select them based solely on the information provided.\n\nGiven that D is explicitly contradicted by the stated findings and there's insufficient information to confirm A, B, or C as factors identified by the paper's analysis:\n\n\"\"", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 102, "Question": "### Background\n\n**Research Question.** What are the main econometric challenges in identifying the true effects of inflation-driven wealth transfers on cross-sectional stock returns?\n\n**Setting.** The study uses a multivariate cross-sectional regression where firm characteristics like leverage, asset tangibility, and systematic risk are often correlated, creating potential for omitted variable bias (OVB). Furthermore, systematic risk (`β`) is not directly observable and must be estimated, introducing a potential errors-in-variables (EIV) problem, which the authors address by forming portfolios.\n\n### Data / Model Specification\n\nThe regression model is:\n  \ny_{i}=\\alpha+a_{0}\\beta_{i}+a_{1}X_{1i}+a_{2}X_{2i}+a_{3}X_{3i}+u_{i} \\quad \\text{(Eq. (1))}\n \nwhere `y` is return, `β` is risk, `X_1` is debt, and `X_2` is fixed assets. The paper's theory and results suggest `a_0 > 0` and `a_2 < 0`.\n\n**Table 1: Correlation Matrix of Explanatory Variables (Period 1964-68)**\n\n| | `β` | `X_1` | `X_2` |\n|:---|:---:|:---:|:---:|\n| **`β`** | 1 | | |\n| **`X_1`** | 0.294 | 1 | |\n| **`X_2`** | -0.370 | 0.391 | 1 |\n\n### Question\n\nConsidering the data in **Table 1** and the econometric methods described, select all correct statements regarding the study's methodological challenges.", "Options": {"A": "The portfolio grouping procedure is used to address the errors-in-variables problem stemming from the estimation of `β`, which would otherwise cause the estimate of its coefficient (`a_0`) to be biased towards zero.", "B": "Based on Table 1, omitting the fixed assets variable (`X_2`) from the regression would likely introduce a negative bias on the debt coefficient (`a_1`), as `X_1` and `X_2` are positively correlated while the true coefficient `a_2` is negative.", "C": "The portfolio grouping method is primarily designed to solve the omitted variable bias problem by ensuring the explanatory variables are orthogonal.", "D": "Based on Table 1, omitting systematic risk (`β`) would introduce a negative bias on the debt coefficient (`a_1`)."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the understanding of key econometric challenges in the paper: omitted variable bias (OVB) and errors-in-variables (EIV). It uses a Premise Packaging strategy, asking to identify correct statements about methodology. Distractors are based on traceable errors: a sign error in calculating OVB direction (distractor C) and a conceptual confusion between the purposes of EIV and OVB mitigation techniques (distractor D).", "qid": "102", "question": "### Background\n\n**Research Question.** What are the main econometric challenges in identifying the true effects of inflation-driven wealth transfers on cross-sectional stock returns?\n\n**Setting.** The study uses a multivariate cross-sectional regression where firm characteristics like leverage, asset tangibility, and systematic risk are often correlated, creating potential for omitted variable bias (OVB). Furthermore, systematic risk (`β`) is not directly observable and must be estimated, introducing a potential errors-in-variables (EIV) problem, which the authors address by forming portfolios.\n\n### Data / Model Specification\n\nThe regression model is:\n  \ny_{i}=\\alpha+a_{0}\\beta_{i}+a_{1}X_{1i}+a_{2}X_{2i}+a_{3}X_{3i}+u_{i} \\quad \\text{(Eq. (1))}\n \nwhere `y` is return, `β` is risk, `X_1` is debt, and `X_2` is fixed assets. The paper's theory and results suggest `a_0 > 0` and `a_2 < 0`.\n\n**Table 1: Correlation Matrix of Explanatory Variables (Period 1964-68)**\n\n| | `β` | `X_1` | `X_2` |\n|:---|:---:|:---:|:---:|\n| **`β`** | 1 | | |\n| **`X_1`** | 0.294 | 1 | |\n| **`X_2`** | -0.370 | 0.391 | 1 |\n\n### Question\n\nConsidering the data in **Table 1** and the econometric methods described, select all correct statements regarding the study's methodological challenges.", "model_evaluation_result": {"model_raw_response": "Looking at this econometric problem, I need to evaluate each statement based on the correlation matrix and the described methodological issues.\n\nLet me analyze each option:\n\n**Option A**: This correctly identifies that portfolio grouping addresses the errors-in-variables (EIV) problem. When β is estimated rather than observed, measurement error causes attenuation bias - the coefficient estimate is biased toward zero. Portfolio grouping reduces this measurement error by averaging, making this statement correct.\n\n**Option B**: To assess omitted variable bias (OVB), I use the formula: OVB = (correlation between included and omitted variable) × (coefficient of omitted variable).\n\nFrom Table 1: corr(X₁, X₂) = 0.391 (positive)\nGiven: a₂ < 0 (negative)\nTherefore: OVB = 0.391 × (negative) = negative\n\nThis means omitting X₂ would negatively bias the coefficient on X₁, making this statement correct.\n\n**Option C**: This mischaracterizes portfolio grouping. The method is designed to address EIV problems (as stated in the background), not OVB. Portfolio grouping doesn't make variables orthogonal - it reduces measurement error in estimated variables like β. This statement is incorrect.\n\n**Option D**: Using the same OVB formula:\nFrom Table 1: corr(X₁, β) = 0.294 (positive)\nGiven: a₀ > 0 (positive)\nTherefore: OVB = 0.294 × (positive) = positive\n\nOmitting β would introduce a positive bias on the debt coefficient, not negative. This statement is incorrect.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 92, "Question": "### Background\n\n**Research Question.** Do conflicting empirical findings on mutual fund 'tournament behavior' stem from a 'sorting bias' in the traditional testing methodology?\n\n**Setting / Data-Generating Environment.** The paper critiques the traditional methodology which sorts funds on first-half returns and measures second-half risk changes.\n\n### Data / Model Specification\n\n**Simulation Evidence**\nA simulation is run where fund returns are generated by a single-factor model and fund betas mean-revert exogenously. **Crucially, no strategic tournament behavior is programmed into the simulation.** The same metrics from the traditional test are then computed on this simulated data.\n-   `Frequency Difference`: Measures apparent tournament behavior.\n-   `Before Ratio`: Measures the degree of risk sorting in the first half.\n\n**Table 1. Simulated Tournament Results (No Actual Tournament Behavior)**\n\n| Year | Frequency Difference | Before Ratio |\n|:----:|:---:|:---:|\n| 1991 | 8.00 | 1.57 |\n| 2001 | -7.64 | 0.60 |\n| **Corr.** | **0.69** | |\n\n### Question\n\nThe paper uses a simulation to critique the causal validity of the traditional methodology. The simulation was designed with **no strategic tournament behavior**. Based on the results in Table 1, select all valid conclusions that can be drawn from this simulation exercise.", "Options": {"A": "Since the simulation (Table 1) and the empirical data show similar patterns, this validates the traditional methodology as a reliable measure of tournament behavior.", "B": "The simulation fails to replicate the empirical findings, as the simulated `Frequency Difference` values are much smaller than the empirical ones, suggesting that real tournament behavior is much stronger than the sorting bias.", "C": "The simulation replicates the key qualitative pattern of the empirical data, generating apparent risk-increasing behavior in a strong market year (1991) and apparent risk-decreasing behavior in a weak market year (2001), purely as a statistical artifact.", "D": "The simulation demonstrates that the strong positive correlation between the `Before Ratio` and the `Frequency Difference` can arise mechanically from risk mean-reversion, without any strategic action from managers."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses a sophisticated concept: the use of simulation for causal inference and methodological critique. It uses a **Reverse-Reasoning** strategy, asking what can be concluded from the simulation's outcome. The correct options require understanding that when a null model (no behavior) replicates empirical patterns, the test methodology is invalidated. Distractors are based on fundamental misinterpretations of this logic, including two **Conceptual Opposites** that draw the exact wrong conclusion from the evidence.", "qid": "92", "question": "### Background\n\n**Research Question.** Do conflicting empirical findings on mutual fund 'tournament behavior' stem from a 'sorting bias' in the traditional testing methodology?\n\n**Setting / Data-Generating Environment.** The paper critiques the traditional methodology which sorts funds on first-half returns and measures second-half risk changes.\n\n### Data / Model Specification\n\n**Simulation Evidence**\nA simulation is run where fund returns are generated by a single-factor model and fund betas mean-revert exogenously. **Crucially, no strategic tournament behavior is programmed into the simulation.** The same metrics from the traditional test are then computed on this simulated data.\n-   `Frequency Difference`: Measures apparent tournament behavior.\n-   `Before Ratio`: Measures the degree of risk sorting in the first half.\n\n**Table 1. Simulated Tournament Results (No Actual Tournament Behavior)**\n\n| Year | Frequency Difference | Before Ratio |\n|:----:|:---:|:---:|\n| 1991 | 8.00 | 1.57 |\n| 2001 | -7.64 | 0.60 |\n| **Corr.** | **0.69** | |\n\n### Question\n\nThe paper uses a simulation to critique the causal validity of the traditional methodology. The simulation was designed with **no strategic tournament behavior**. Based on the results in Table 1, select all valid conclusions that can be drawn from this simulation exercise.", "model_evaluation_result": {"model_raw_response": "Looking at this simulation study that was designed with NO strategic tournament behavior:\n\nThe key findings from Table 1 are:\n- In 1991: Positive Frequency Difference (8.00) with high Before Ratio (1.57)\n- In 2001: Negative Frequency Difference (-7.64) with low Before Ratio (0.60)\n- Strong positive correlation (0.69) between Before Ratio and Frequency Difference\n\nLet me evaluate each option:\n\nA) This is incorrect. Finding similar patterns when there's NO actual tournament behavior programmed into the simulation actually invalidates the traditional methodology, not validates it. The simulation shows these patterns can arise as statistical artifacts.\n\nB) Cannot determine this without seeing the actual empirical values. The question doesn't provide empirical data for comparison.\n\nC) This is correct. The simulation shows that without any strategic behavior, we still get apparent risk-increasing behavior in strong markets (positive Frequency Difference in 1991) and apparent risk-decreasing behavior in weak markets (negative Frequency Difference in 2001). This demonstrates these patterns can be purely statistical artifacts.\n\nD) This is correct. The 0.69 correlation between Before Ratio and Frequency Difference arose mechanically in a simulation with only mean-reversion and no strategic behavior. This shows the correlation can occur without any tournament behavior.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 146, "Question": "### Background\n\n**Research Question.** How does the interaction between tax policy and a firm's growth opportunities affect its systematic risk?\n\n**Setting / Data-Generating Environment.** The study simulates equity betas for a baseline firm (`g=0.25`) and a high-growth firm (`g=1`), where `g` is the rate of expansion upon exercising a growth option. This allows for a direct test of the interaction between tax asymmetry (`δ`) and growth potential.\n\n**Variables & Parameters.**\n- `β_E`: The firm's equity beta.\n- `δ`: The level of tax asymmetry (tax convexity).\n- `g`: The rate of expansion, a proxy for the magnitude of growth opportunities.\n- `X`: The firm's profit flow, an inverse proxy for leverage.\n\n---\n\n### Data / Model Specification\n\nThe following tables show the simulated equity beta for a baseline firm (`g=0.25`) and a high-growth firm (`g=1`).\n\n**Table 1. Equity Beta for Baseline Firm (g=0.25)**\n\n| `δ` | Low Profit (X=2) | Medium Profit (X=4) | High Profit (X=6) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.4718 | 1.0964 | 0.9800 |\n| 0.35 | 1.6578 | 1.1968 | 1.1072 |\n| % Change | **+12.6%** | **+9.1%** | **+13.0%** |\n\n**Table 2. Equity Beta for High-Growth Firm (g=1)**\n\n| `δ` | Low Profit (X=2) | Medium Profit (X=4) | High Profit (X=6) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.0882 | 0.6092 | 0.4360 |\n| 0.35 | 1.3260 | 0.7419 | 0.5554 |\n| % Change | **+21.9%** | **+21.8%** | **+27.4%** |\n\n---\n\n### Question\n\nBased on the provided data and the model's economic principles, which of the following statements about the interaction between tax asymmetry (`δ`) and growth opportunities (`g`) are correct?", "Options": {"A": "For the high-growth firm, increasing tax asymmetry from `δ=0` to `δ=0.35` raises its equity beta by more than 20% across all profit levels shown.", "B": "The percentage increase in equity beta resulting from a higher tax asymmetry (`δ`) is consistently greater for the high-growth firm (`g=1`) than for the baseline firm (`g=0.25`).", "C": "For any given level of tax asymmetry, the absolute equity beta of the high-growth firm is always higher than that of the baseline firm.", "D": "The model implies that the value of a firm's growth options decreases its sensitivity to changes in tax policy."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to identify and quantify an interaction effect from tabular data. It uses an Atomic Decomposition strategy to create two independently verifiable correct statements from the tables. Distractor C is an 'Almost Right' conceptual error that tests for careful reading of absolute vs. relative values. Distractor D is a 'Conceptual Opposite' error that contradicts the core theoretical mechanism where growth options amplify sensitivity.", "qid": "146", "question": "### Background\n\n**Research Question.** How does the interaction between tax policy and a firm's growth opportunities affect its systematic risk?\n\n**Setting / Data-Generating Environment.** The study simulates equity betas for a baseline firm (`g=0.25`) and a high-growth firm (`g=1`), where `g` is the rate of expansion upon exercising a growth option. This allows for a direct test of the interaction between tax asymmetry (`δ`) and growth potential.\n\n**Variables & Parameters.**\n- `β_E`: The firm's equity beta.\n- `δ`: The level of tax asymmetry (tax convexity).\n- `g`: The rate of expansion, a proxy for the magnitude of growth opportunities.\n- `X`: The firm's profit flow, an inverse proxy for leverage.\n\n---\n\n### Data / Model Specification\n\nThe following tables show the simulated equity beta for a baseline firm (`g=0.25`) and a high-growth firm (`g=1`).\n\n**Table 1. Equity Beta for Baseline Firm (g=0.25)**\n\n| `δ` | Low Profit (X=2) | Medium Profit (X=4) | High Profit (X=6) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.4718 | 1.0964 | 0.9800 |\n| 0.35 | 1.6578 | 1.1968 | 1.1072 |\n| % Change | **+12.6%** | **+9.1%** | **+13.0%** |\n\n**Table 2. Equity Beta for High-Growth Firm (g=1)**\n\n| `δ` | Low Profit (X=2) | Medium Profit (X=4) | High Profit (X=6) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.0882 | 0.6092 | 0.4360 |\n| 0.35 | 1.3260 | 0.7419 | 0.5554 |\n| % Change | **+21.9%** | **+21.8%** | **+27.4%** |\n\n---\n\n### Question\n\nBased on the provided data and the model's economic principles, which of the following statements about the interaction between tax asymmetry (`δ`) and growth opportunities (`g`) are correct?", "model_evaluation_result": {"model_raw_response": "Looking at the data systematically:\n\n**For Option A:** Checking the percentage changes for the high-growth firm (g=1):\n- X=2: 21.9% > 20% ✓\n- X=4: 21.8% > 20% ✓\n- X=6: 27.4% > 20% ✓\n\nAll percentage increases exceed 20%, so A is correct.\n\n**For Option B:** Comparing percentage increases between firms:\n- X=2: 21.9% (high-growth) vs 12.6% (baseline) ✓\n- X=4: 21.8% (high-growth) vs 9.1% (baseline) ✓\n- X=6: 27.4% (high-growth) vs 13.0% (baseline) ✓\n\nThe high-growth firm consistently shows greater percentage increases, so B is correct.\n\n**For Option C:** Comparing absolute beta values:\nAt δ=0:\n- X=2: 1.0882 (high-growth) < 1.4718 (baseline) ✗\n- X=4: 0.6092 (high-growth) < 1.0964 (baseline) ✗\n- X=6: 0.4360 (high-growth) < 0.9800 (baseline) ✗\n\nThe high-growth firm has lower absolute betas, so C is incorrect.\n\n**For Option D:** The data shows that firms with greater growth opportunities (g=1) have higher sensitivity to tax policy changes (larger percentage increases in beta). This contradicts the statement, so D is incorrect.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 23, "Question": "### Background\n\n**Research Question.** How can corporations exploit segmentation within developed debt markets arising from regulatory restrictions through financial engineering?\n\n**Setting / Data-Generating Environment.** In 1986, the Kingdom of Denmark issued “Bull and Bear” notes in the French market. The Bear notes' redemption value was inversely related to a stock market index, while the Bull notes' redemption value was directly related to the index. For the issuer, the average redemption value of a pair of notes was fixed, making the total issuance riskless from its perspective.\n\n### Question\n\nSelect all statements that correctly describe the financial engineering behind this issuance.", "Options": {"A": "The structure allowed Denmark to sell substitutes for prohibited derivative instruments at a premium.", "B": "The Bull notes were equivalent to a risk-free bond, as their coupon was guaranteed.", "C": "The issuer (Denmark) was left with a significant, unhedged net long position in the French stock market.", "D": "The Bear notes synthetically replicated a long put option on the stock index, providing portfolio insurance for investors."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: Tests the understanding of how structured products can replicate derivative payoffs to circumvent regulatory barriers. Strategy: Scenario Application. Students must map the described features of the notes to core financial concepts. Distractor Logic: (A) Correct. (B) Correct. (C) Conceptual Opposite: The paper explicitly states the combined issue was riskless for the issuer. (D) Almost Right: The coupon was guaranteed, but the principal redemption was variable, making it a risky, not risk-free, instrument when held in isolation.", "qid": "23", "question": "### Background\n\n**Research Question.** How can corporations exploit segmentation within developed debt markets arising from regulatory restrictions through financial engineering?\n\n**Setting / Data-Generating Environment.** In 1986, the Kingdom of Denmark issued “Bull and Bear” notes in the French market. The Bear notes' redemption value was inversely related to a stock market index, while the Bull notes' redemption value was directly related to the index. For the issuer, the average redemption value of a pair of notes was fixed, making the total issuance riskless from its perspective.\n\n### Question\n\nSelect all statements that correctly describe the financial engineering behind this issuance.", "model_evaluation_result": {"model_raw_response": "Looking at this financial engineering structure, I need to analyze each statement carefully.\n\nThe Bull and Bear notes structure created by Denmark was designed to exploit market segmentation. The key insight is that when combined, a Bull note (positively correlated with the index) and a Bear note (negatively correlated with the index) create a fixed total redemption value for the issuer.\n\nLet me evaluate each option:\n\nA) \"The structure allowed Denmark to sell substitutes for prohibited derivative instruments at a premium.\"\nThis is correct. In 1986, many investors faced regulatory restrictions on directly purchasing derivatives. By packaging index-linked payoffs into bond format, Denmark created synthetic derivatives that circumvented these restrictions. The segmentation allowed them to charge a premium.\n\nB) \"The Bull notes were equivalent to a risk-free bond, as their coupon was guaranteed.\"\nThis is incorrect. Bull notes had redemption values that varied directly with the stock index, making them risky instruments. They were not equivalent to risk-free bonds.\n\nC) \"The issuer (Denmark) was left with a significant, unhedged net long position in the French stock market.\"\nThis is incorrect. The structure was specifically designed so that the Bull and Bear notes offset each other. The average redemption value of a pair was fixed, making the total issuance riskless from Denmark's perspective.\n\nD) \"The Bear notes synthetically replicated a long put option on the stock index, providing portfolio insurance for investors.\"\nThis is correct. Bear notes had redemption values inversely related to the index - they paid more when the index fell. This payoff profile replicates a long put option, which increases in value as the underlying decreases, providing portfolio insurance.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 91, "Question": "### Background\n\n**Research Question.** Do conflicting empirical findings on mutual fund 'tournament behavior'—where managers alter risk based on mid-year performance—stem from a fundamental flaw in the traditional testing methodology? This paper hypothesizes a 'sorting bias' as the explanation.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual funds from 1990-2006. The traditional methodology sorts funds into 'winners' and 'losers' based on first-half returns and then examines their change in risk in the second half.\n\n### Data / Model Specification\n\n**Traditional Methodology**\nFunds are placed in a 2x2 contingency table based on being above/below the median first-half return (`RTN`) and a Risk Adjustment Ratio (`RAR`).\n\n**Table 1. Traditional Contingency Table Results (Selected Years)**\n\n| Year | Low RTN (\"Losers\") \"Low\" RAR | Low RTN (\"Losers\") \"High\" RAR |\n|:----:|:---:|:---:|\n| 1991 | 16.49% | 33.58% |\n| 2001 | 38.51% | 11.49% |\n\n**Testing the Sorting Bias Hypothesis**\nThe paper proposes two metrics to test the sorting bias hypothesis:\n-   `Frequency Difference`: For the 'Low RTN' group, this is `% in High RAR cell - % in Low RAR cell`.\n-   `Before Ratio`: The ratio of the median first-half risk of 'High RTN' funds to 'Low RTN' funds.\n\n**Table 2. Empirical Test of the Sorting Bias**\n\n| Year | Frequency Difference | Before Ratio |\n|:----:|:---:|:---:|\n| 1991 | 17.09 | 1.356 |\n| 2001 | -27.01 | 0.609 |\n| **Corr.** | **0.81** | |\n\n*Note: The correlation of 0.81 is calculated over the full 1990-2006 sample.*\n\n### Question\n\nThe paper argues that traditional tests of tournament behavior are flawed due to a 'sorting bias'. Based on the data in Table 1 and Table 2, select all statements that correctly describe this bias and the empirical evidence supporting it.", "Options": {"A": "The correlation of 0.81 between the `Frequency Difference` and the `Before Ratio` indicates that managers of funds that were riskier in the first half (`Before Ratio`) are behaviorally more likely to increase risk in the second half (`Frequency Difference`).", "B": "The empirical data shows a strong link between the initial risk sort and the final test outcome: in 1991, a high `Before Ratio` (1.356) was followed by a large positive `Frequency Difference` (17.09), while in 2001, a low `Before Ratio` (0.609) was followed by a large negative `Frequency Difference` (-27.01).", "C": "The results in Table 1 for 1991 and 2001 show that manager behavior is inconsistent, with underperforming managers increasing risk in 1991 but also increasing risk in 2001.", "D": "The sorting bias hypothesis posits that in a strong market, sorting on high returns inadvertently selects for high-risk funds, whose risk then mechanically mean-reverts downwards, creating a spurious result."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the student's understanding of the 'sorting bias' mechanism and the empirical evidence supporting it. It uses an **Atomic Decomposition** strategy, combining a conceptual statement about the bias's logic with a data-driven statement from the tables. This tests both theoretical comprehension and data interpretation. Distractors include a **Conceptual Opposite** (misreading the results in Table 1) and a subtle **Conceptual Error** (proposing an incorrect causal interpretation of a correlation).", "qid": "91", "question": "### Background\n\n**Research Question.** Do conflicting empirical findings on mutual fund 'tournament behavior'—where managers alter risk based on mid-year performance—stem from a fundamental flaw in the traditional testing methodology? This paper hypothesizes a 'sorting bias' as the explanation.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual funds from 1990-2006. The traditional methodology sorts funds into 'winners' and 'losers' based on first-half returns and then examines their change in risk in the second half.\n\n### Data / Model Specification\n\n**Traditional Methodology**\nFunds are placed in a 2x2 contingency table based on being above/below the median first-half return (`RTN`) and a Risk Adjustment Ratio (`RAR`).\n\n**Table 1. Traditional Contingency Table Results (Selected Years)**\n\n| Year | Low RTN (\"Losers\") \"Low\" RAR | Low RTN (\"Losers\") \"High\" RAR |\n|:----:|:---:|:---:|\n| 1991 | 16.49% | 33.58% |\n| 2001 | 38.51% | 11.49% |\n\n**Testing the Sorting Bias Hypothesis**\nThe paper proposes two metrics to test the sorting bias hypothesis:\n-   `Frequency Difference`: For the 'Low RTN' group, this is `% in High RAR cell - % in Low RAR cell`.\n-   `Before Ratio`: The ratio of the median first-half risk of 'High RTN' funds to 'Low RTN' funds.\n\n**Table 2. Empirical Test of the Sorting Bias**\n\n| Year | Frequency Difference | Before Ratio |\n|:----:|:---:|:---:|\n| 1991 | 17.09 | 1.356 |\n| 2001 | -27.01 | 0.609 |\n| **Corr.** | **0.81** | |\n\n*Note: The correlation of 0.81 is calculated over the full 1990-2006 sample.*\n\n### Question\n\nThe paper argues that traditional tests of tournament behavior are flawed due to a 'sorting bias'. Based on the data in Table 1 and Table 2, select all statements that correctly describe this bias and the empirical evidence supporting it.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze the sorting bias hypothesis and evaluate which statements correctly describe it based on the provided data.\n\nLet me first understand what's happening:\n- The traditional methodology sorts funds into winners/losers based on first-half returns\n- Then examines their risk changes in the second half using RAR (Risk Adjustment Ratio)\n- The paper suspects a \"sorting bias\" in this methodology\n\nKey data points:\n- Table 1 shows the distribution of \"Low RTN\" (loser) funds across RAR categories\n- Table 2 shows two metrics: Frequency Difference and Before Ratio\n- There's a 0.81 correlation between these metrics\n\nLet me evaluate each option:\n\n**Option A**: This misinterprets the correlation. The Before Ratio compares the median first-half risk between High RTN and Low RTN funds (not about funds being \"riskier\"). The Frequency Difference measures the distribution of Low RTN funds across RAR categories. The correlation doesn't indicate behavioral tendencies to increase risk.\n\n**Option B**: This correctly describes the empirical pattern:\n- 1991: High Before Ratio (1.356) → winners had higher initial risk than losers\n- 1991: Positive Frequency Difference (17.09) → more losers in \"High\" RAR cell\n- 2001: Low Before Ratio (0.609) → winners had lower initial risk than losers  \n- 2001: Negative Frequency Difference (-27.01) → more losers in \"Low\" RAR cell\nThis shows a strong link between initial risk sorting and final outcomes.\n\n**Option C**: This is incorrect. In Table 1:\n- 1991: 33.58% (High RAR) > 16.49% (Low RAR) → losers increased risk\n- 2001: 38.51% (Low RAR) > 11.49% (High RAR) → losers decreased risk\nThe behavior is different, not both increasing risk.\n\n**Option D**: This correctly describes the sorting bias mechanism. When sorting on returns in a strong market, high-return funds tend to be high-risk funds. Their risk naturally mean-reverts downward, creating the appearance that winners decrease risk - but this is mechanical, not behavioral.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 32, "Question": "### Background\n\nThe study examines whether the 2017 Tax Cuts and Jobs Act (TCJA) stimulated investment by alleviating firms' pre-existing financial constraints. The theory is that the tax savings provided a cash windfall that allowed constrained firms to fund positive-NPV projects they previously could not.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated on sub-samples of firms classified as 'constrained' or 'unconstrained' using four different standard measures. The key coefficient on `USFirm*Yr2019` captures the treatment effect on capital expenditures (`CapEx`).\n\n**Table 1: DiD Results by Financial Constraint Status**\n\n| Constraint Measure | Constrained Firms (Coefficient) | Unconstrained Firms (Coefficient) |\n| :----------------- | :------------------------------ | :-------------------------------- |\n| KZ-Index           | 0.0038***                       | -0.0002                           |\n| Free Cash Flow     | 0.0040***                       | 0.0007                            |\n| Modified Z-score   | 0.0030***                       | 0.0003                            |\n| Whited-Wu Index    | 0.0045***                       | 0.0012                            |\n\n*Note: All coefficients for constrained firms are statistically significant; none for unconstrained firms are.*\n\n---\n\nAccording to the results in **Table 1**, which of the following statements are **INCORRECT** interpretations of the findings?\n", "Options": {"A": "The investment-stimulating effect of the TCJA was concentrated in financially constrained firms.", "B": "The results suggest that the TCJA's impact on investment was independent of firms' pre-existing financial health.", "C": "The findings are inconsistent with a perfect Modigliani-Miller world where financing is irrelevant to investment.", "D": "Unconstrained firms showed a statistically significant increase in investment across all four measures."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret a consistent pattern of heterogeneous treatment effects and understand its theoretical implications for corporate finance (i.e., the violation of Modigliani-Miller assumptions).\nDepth Strategy: Reverse-Reasoning and Theoretical Application.\nDistractor Logic: The Single-Correct-Answer Inversion Protocol was applied here. The core finding is that the effect is *only* in constrained firms. This makes statements A and C correct interpretations. Statements B and D are therefore incorrect interpretations. By inverting the stem to ask for INCORRECT statements, we can create a compliant multiple-response item.\n- A (Correct Interpretation, now a distractor): This is the main takeaway from the table.\n- B (Incorrect Interpretation, now a correct answer): The table note explicitly states the coefficients for unconstrained firms are not significant.\n- C (Correct Interpretation, now a distractor): The results show financing matters, which violates the MM theorem, making this a correct interpretation.\n- D (Incorrect Interpretation, now a correct answer): The results show the impact is highly dependent on financial health, making this statement false.", "qid": "32", "question": "### Background\n\nThe study examines whether the 2017 Tax Cuts and Jobs Act (TCJA) stimulated investment by alleviating firms' pre-existing financial constraints. The theory is that the tax savings provided a cash windfall that allowed constrained firms to fund positive-NPV projects they previously could not.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated on sub-samples of firms classified as 'constrained' or 'unconstrained' using four different standard measures. The key coefficient on `USFirm*Yr2019` captures the treatment effect on capital expenditures (`CapEx`).\n\n**Table 1: DiD Results by Financial Constraint Status**\n\n| Constraint Measure | Constrained Firms (Coefficient) | Unconstrained Firms (Coefficient) |\n| :----------------- | :------------------------------ | :-------------------------------- |\n| KZ-Index           | 0.0038***                       | -0.0002                           |\n| Free Cash Flow     | 0.0040***                       | 0.0007                            |\n| Modified Z-score   | 0.0030***                       | 0.0003                            |\n| Whited-Wu Index    | 0.0045***                       | 0.0012                            |\n\n*Note: All coefficients for constrained firms are statistically significant; none for unconstrained firms are.*\n\n---\n\nAccording to the results in **Table 1**, which of the following statements are **INCORRECT** interpretations of the findings?\n", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to identify which statements are INCORRECT interpretations.\n\nLet me analyze each option:\n\nA) \"The investment-stimulating effect of the TCJA was concentrated in financially constrained firms.\"\n- Constrained firms show positive and significant coefficients (0.0030*** to 0.0045***)\n- Unconstrained firms show small, insignificant coefficients (-0.0002 to 0.0012)\n- This statement is CORRECT - the effect was indeed concentrated in constrained firms\n\nB) \"The results suggest that the TCJA's impact on investment was independent of firms' pre-existing financial health.\"\n- The results show a clear difference between constrained and unconstrained firms\n- Constrained firms responded strongly, unconstrained firms did not\n- This statement is INCORRECT - the impact clearly depended on financial health\n\nC) \"The findings are inconsistent with a perfect Modigliani-Miller world where financing is irrelevant to investment.\"\n- In a perfect MM world, financing constraints wouldn't matter for investment\n- The results show financing constraints do matter (constrained firms invested more after getting cash)\n- This statement is CORRECT - the findings do contradict MM assumptions\n\nD) \"Unconstrained firms showed a statistically significant increase in investment across all four measures.\"\n- The note explicitly states \"none for unconstrained firms are\" statistically significant\n- All unconstrained firm coefficients lack significance stars\n- This statement is INCORRECT - unconstrained firms showed no significant increases\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 47, "Question": "### Background\n\n**Research Question.** How robust are hedging strategies for Variable Annuities to the choice of the underlying asset price model, particularly the standard Black-Scholes assumption of a Geometric Brownian Motion (GBM)?\n\n**Setting.** An insurer implements a sophisticated hedging strategy for a long-maturity VA guarantee. The effectiveness of the hedge depends critically on the assumed dynamics of the underlying financial index, `S_t`. The paper's primary analysis relies on the GBM model but includes a robustness check using a model with price jumps.\n\n**Variables and Parameters.**\n- `S_t`: Value of the underlying index at time `t`.\n- `μ`: Constant expected return (drift) of the index.\n- `σ`: Constant volatility of the index.\n- `W_t`: A standard Brownian motion.\n\n---\n\n### Data / Model Specification\n\nThe primary analysis assumes the index `S_t` follows a Geometric Brownian Motion (GBM), which implies that log-returns are normally distributed with constant volatility:\n\n  \n\\frac{d S_{t}}{S_{t}}=\\mu d t+\\sigma d W_{t}\n \n(Eq. 1)\n\n---\n\n### Question\n\nThe Geometric Brownian Motion (GBM) model in Eq. (1) makes key simplifying assumptions about asset returns. Which of the following are well-documented empirical features of financial asset returns that **contradict** the assumptions of the GBM model? (Select all that apply)", "Options": {"A": "The absence of risk-free arbitrage opportunities in the market.", "B": "Leptokurtosis (the return distribution has \"fat tails,\" meaning extreme events are more common than the model predicts).", "C": "Log-returns follow a perfect normal distribution.", "D": "Volatility Clustering (periods of high volatility tend to be followed by periods of high volatility)."}, "Answer": ["B", "D"], "pi_justification": "This question assesses foundational knowledge of empirical asset pricing, which is critical context for the paper's model risk discussion. It uses a select-all-that-apply format to test for specific, named concepts. Distractor C is a 'Conceptual Opposite'—it states the model's assumption, not the empirical contradiction. Distractor D is 'True but Irrelevant'—it's a foundational assumption for the model's framework, not a contradiction of its dynamic properties.", "qid": "47", "question": "### Background\n\n**Research Question.** How robust are hedging strategies for Variable Annuities to the choice of the underlying asset price model, particularly the standard Black-Scholes assumption of a Geometric Brownian Motion (GBM)?\n\n**Setting.** An insurer implements a sophisticated hedging strategy for a long-maturity VA guarantee. The effectiveness of the hedge depends critically on the assumed dynamics of the underlying financial index, `S_t`. The paper's primary analysis relies on the GBM model but includes a robustness check using a model with price jumps.\n\n**Variables and Parameters.**\n- `S_t`: Value of the underlying index at time `t`.\n- `μ`: Constant expected return (drift) of the index.\n- `σ`: Constant volatility of the index.\n- `W_t`: A standard Brownian motion.\n\n---\n\n### Data / Model Specification\n\nThe primary analysis assumes the index `S_t` follows a Geometric Brownian Motion (GBM), which implies that log-returns are normally distributed with constant volatility:\n\n  \n\\frac{d S_{t}}{S_{t}}=\\mu d t+\\sigma d W_{t}\n \n(Eq. 1)\n\n---\n\n### Question\n\nThe Geometric Brownian Motion (GBM) model in Eq. (1) makes key simplifying assumptions about asset returns. Which of the following are well-documented empirical features of financial asset returns that **contradict** the assumptions of the GBM model? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at the GBM model assumptions and the empirical features listed:\n\nThe GBM model assumes:\n- Constant volatility (σ is constant)\n- Log-returns are normally distributed\n- No jumps or discontinuities in prices\n\nAnalyzing each option:\n\nA) The absence of risk-free arbitrage opportunities - This is NOT a contradiction. Both the GBM model and real markets generally exhibit no arbitrage opportunities. This is consistent with the model.\n\nB) Leptokurtosis (fat tails) - This CONTRADICTS GBM. The GBM assumes normal distribution of log-returns, but empirically, financial returns show fat tails with more extreme events than predicted by normal distribution.\n\nC) Log-returns follow a perfect normal distribution - This is what GBM assumes, NOT a contradiction. In reality, log-returns deviate from normality.\n\nD) Volatility Clustering - This CONTRADICTS GBM. The GBM assumes constant volatility σ, but empirically, markets show volatility clustering where high volatility periods cluster together.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 60, "Question": "### Background\n\n**Research Question.** How can a unified affine model framework be used to parsimoniously capture key stylized facts of commodity prices, such as mean reversion, stochastic volatility, seasonality, and jumps, and how do these features interact?\n\n**Setting.** A continuous-time model for the log-spot price of a commodity, designed to nest several prominent models as special cases.\n\n**Variables and Parameters.**\n- `y_t`: Log-spot price of the commodity.\n- `ν_t`: Instantaneous variance of the log-price return.\n- `g_t`: A deterministic function capturing seasonality and time trends.\n- `T_1, T_2`: Binary indicator variables (0 or 1) that activate the seasonality and jump components, respectively.\n\n---\n\n### Data / Model Specification\n\nThe general form for the affine styled-facts dynamics of the log-spot price `y_t` is given by `Eq. (1)`:\n\n  \nd y_{t}=\\left(a-b(y_{t}-T_{1} g_{t})-\\frac{1}{2}\\nu_{t}+T_{1}\\frac{d g_{t}}{d t}-T_{2}\\lambda\\bar{J}\\right)d t+\\sqrt{\\nu_{t}}d W_{S,t}^{P}+{ T}_{2} J d N_{t} \n \n\nThis general framework nests four models based on the settings of `T_1` and `T_2`:\n- MRSV (Mean Reversion, Stochastic Volatility): `{T_1, T_2} = {0, 0}`\n- MRSVJ (MRSV with Jumps): `{T_1, T_2} = {0, 1}`\n- MRSVS (MRSV with Seasonality): `{T_1, T_2} = {1, 0}`\n- MRSVJS (MRSV with Jumps and Seasonality): `{T_1, T_2} = {1, 1}`\n\n---\n\nConsider a scenario where a natural gas market is characterized by strong, predictable winter price spikes and occasional, unpredictable supply disruptions from geopolitical events. According to the model framework in `Eq. (1)`, which of the following statements correctly describe how the different model specifications would capture these market features?\n\nSelect all that apply.", "Options": {"A": "The baseline MRSV model (`T_1=0, T_2=0`) would capture both the winter spikes and supply disruptions through its stochastic volatility component `ν_t`.", "B": "In the MRSVJ model (`T_1=0, T_2=1`), the predictable winter price spikes would likely be misinterpreted as part of the stochastic jump process, potentially leading to a misspecified jump intensity `λ`.", "C": "In the full MRSVJS model (`T_1=1, T_2=1`), the term `T_1 * (d g_t / dt)` in the drift of `y_t` would capture the expected rate of price change due to seasonal factors.", "D": "The MRSVS model (`T_1=1, T_2=0`) would capture the predictable winter price spikes through the deterministic `g_t` function but would be unable to account for the sudden, unpredictable supply disruptions."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to map specific economic phenomena to the correct mathematical components of a complex, nested model. Strategy: Scenario Application. A qualitative market scenario is provided, and the user must identify which model components correctly represent the described dynamics. Distractor Logic: Option A is correct; without a seasonality term, the jump component is forced to explain large, periodic movements. Option B is correct; MRSVS is designed for seasonality but lacks a mechanism for sudden shocks. Option C is correct, identifying the specific term for the seasonal drift. Option D is a conceptual distractor (Formula Misuse); stochastic volatility captures the magnitude of random daily movements, not large, discrete events like supply disruptions or predictable seasonal trends.", "qid": "60", "question": "### Background\n\n**Research Question.** How can a unified affine model framework be used to parsimoniously capture key stylized facts of commodity prices, such as mean reversion, stochastic volatility, seasonality, and jumps, and how do these features interact?\n\n**Setting.** A continuous-time model for the log-spot price of a commodity, designed to nest several prominent models as special cases.\n\n**Variables and Parameters.**\n- `y_t`: Log-spot price of the commodity.\n- `ν_t`: Instantaneous variance of the log-price return.\n- `g_t`: A deterministic function capturing seasonality and time trends.\n- `T_1, T_2`: Binary indicator variables (0 or 1) that activate the seasonality and jump components, respectively.\n\n---\n\n### Data / Model Specification\n\nThe general form for the affine styled-facts dynamics of the log-spot price `y_t` is given by `Eq. (1)`:\n\n  \nd y_{t}=\\left(a-b(y_{t}-T_{1} g_{t})-\\frac{1}{2}\\nu_{t}+T_{1}\\frac{d g_{t}}{d t}-T_{2}\\lambda\\bar{J}\\right)d t+\\sqrt{\\nu_{t}}d W_{S,t}^{P}+{ T}_{2} J d N_{t} \n \n\nThis general framework nests four models based on the settings of `T_1` and `T_2`:\n- MRSV (Mean Reversion, Stochastic Volatility): `{T_1, T_2} = {0, 0}`\n- MRSVJ (MRSV with Jumps): `{T_1, T_2} = {0, 1}`\n- MRSVS (MRSV with Seasonality): `{T_1, T_2} = {1, 0}`\n- MRSVJS (MRSV with Jumps and Seasonality): `{T_1, T_2} = {1, 1}`\n\n---\n\nConsider a scenario where a natural gas market is characterized by strong, predictable winter price spikes and occasional, unpredictable supply disruptions from geopolitical events. According to the model framework in `Eq. (1)`, which of the following statements correctly describe how the different model specifications would capture these market features?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze how each model specification would handle two distinct market features:\n1. Predictable winter price spikes (seasonal pattern)\n2. Unpredictable supply disruptions from geopolitical events (jumps)\n\nLet me evaluate each option:\n\n**Option A**: The baseline MRSV model (T₁=0, T₂=0) only has mean reversion and stochastic volatility. It cannot capture:\n- Predictable seasonal patterns (needs T₁=1 for seasonality function g_t)\n- Discrete jumps from supply disruptions (needs T₂=1 for jump component)\nThe stochastic volatility ν_t alone cannot adequately represent these distinct features. This is incorrect.\n\n**Option B**: The MRSVJ model (T₁=0, T₂=1) has jumps but no seasonality component. Since it lacks the seasonality function g_t, it would indeed try to fit the predictable winter spikes using its jump process. This would lead to misspecification of the jump parameters, as the model would treat predictable seasonal patterns as if they were random jumps. This is correct.\n\n**Option C**: In the MRSVJS model (T₁=1, T₂=1), when T₁=1, the drift includes the term T₁ · (dg_t/dt) = dg_t/dt. This term represents the instantaneous rate of change of the seasonality function, which captures how prices are expected to change due to seasonal factors at any point in time. This is correct.\n\n**Option D**: The MRSVS model (T₁=1, T₂=0) has seasonality but no jump component. It can capture predictable winter spikes through g_t, but without the jump term (T₂=0), it cannot model sudden, discrete price movements from supply disruptions. The continuous diffusion process cannot adequately represent these discontinuous shocks. This is correct.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 143, "Question": "### Background\n\nAn analysis of Italian savings banks (1982-1989) distinguished between standard and augmented economies of scale. The standard measure (`ξ`) captures cost changes from output growth within a fixed branch network. The augmented measure (`Augξ`) also includes the indirect cost effects of expanding the branch network itself to support that output growth. During this period, regulations severely restricted new branch openings.\n\n### Data / Model Specification\n\nThe augmented measure of economies of scale is defined as:\n  \n\\mathrm{Aug}\\xi = \\xi + \\left(\\frac{\\partial{\\mathrm{Ln}}(TC)}{\\partial{\\mathrm{Ln}}(B)}\\right) \\left(\\frac{d\\mathrm{Ln}(B)}{d\\mathrm{Ln}(Y_{1})}\\right) \\quad \\text{(Eq. (1))}\n \nwhere `TC` is total cost, `Y₁` is output (loans), and `B` is the number of branches.\n\nThe paper reports the following estimates for the full sample period:\n- `ξ` (Standard OES) = 0.7845\n- `Augξ` (Augmented OES) = 0.7978\n\n---\n\nBased on the model and empirical results, which of the following statements are valid conclusions?", "Options": {"A": "The empirical finding that `Augξ` > `ξ` implies that, on average, both the cost elasticity of branches (`∂Ln(TC)/∂Ln(B)`) and the branch elasticity of output (`dLn(B)/dLn(Y₁)`) were positive during the sample period.", "B": "The small difference between `Augξ` and `ξ` is consistent with the institutional context of strict branching restrictions, which would suppress the magnitude of `dLn(B)/dLn(Y₁) `.", "C": "The term `∂Ln(TC)/∂Ln(B)` represents the total elasticity of cost with respect to branches, allowing output to adjust simultaneously.", "D": "If branching regulations were completely lifted, one would expect the estimated value of `dLn(B)/dLn(Y₁)` to increase in a subsequent study period."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to deconstruct a composite formula, relate its components to empirical results and institutional context, and predict how a policy change would affect a model parameter. It tests deep reasoning about the interplay between a model, its estimates, and the real-world environment it represents.\nStrategy: Reverse-Reasoning. The user must work backward from the empirical result (`Augξ > ξ`) to infer the properties of the underlying components and connect them to the regulatory setting.\nDistractor Logic:\n- A (Correct): Since `Augξ` > `ξ`, the adjustment term must be positive. Assuming more branches increase costs (`∂Ln(TC)/∂Ln(B)` > 0), it must be that `dLn(B)/dLn(Y₁)` is also positive.\n- B (Correct): Strict regulations would make it difficult for banks to expand their branch network in response to output growth, leading to a small, though positive, elasticity `dLn(B)/dLn(Y₁)`. This would result in `Augξ` being only slightly larger than `ξ`.\n- C (Correct): Deregulation would remove the constraint, allowing banks to more freely adjust their branch network to match output growth, thus increasing the observed elasticity `dLn(B)/dLn(Y₁)`.\n- D (Incorrect - Conceptual Distractor): This misinterprets the partial derivative. `∂Ln(TC)/∂Ln(B)` is the partial elasticity, which measures the effect of branches on cost *holding output and other variables constant*, not allowing them to adjust.", "qid": "143", "question": "### Background\n\nAn analysis of Italian savings banks (1982-1989) distinguished between standard and augmented economies of scale. The standard measure (`ξ`) captures cost changes from output growth within a fixed branch network. The augmented measure (`Augξ`) also includes the indirect cost effects of expanding the branch network itself to support that output growth. During this period, regulations severely restricted new branch openings.\n\n### Data / Model Specification\n\nThe augmented measure of economies of scale is defined as:\n  \n\\mathrm{Aug}\\xi = \\xi + \\left(\\frac{\\partial{\\mathrm{Ln}}(TC)}{\\partial{\\mathrm{Ln}}(B)}\\right) \\left(\\frac{d\\mathrm{Ln}(B)}{d\\mathrm{Ln}(Y_{1})}\\right) \\quad \\text{(Eq. (1))}\n \nwhere `TC` is total cost, `Y₁` is output (loans), and `B` is the number of branches.\n\nThe paper reports the following estimates for the full sample period:\n- `ξ` (Standard OES) = 0.7845\n- `Augξ` (Augmented OES) = 0.7978\n\n---\n\nBased on the model and empirical results, which of the following statements are valid conclusions?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each statement based on the given model and empirical results.\n\nGiven information:\n- ξ = 0.7845 (Standard OES)\n- Augξ = 0.7978 (Augmented OES)\n- Augξ = ξ + (∂Ln(TC)/∂Ln(B))(dLn(B)/dLn(Y₁))\n- Therefore: 0.7978 = 0.7845 + (∂Ln(TC)/∂Ln(B))(dLn(B)/dLn(Y₁))\n- This means: (∂Ln(TC)/∂Ln(B))(dLn(B)/dLn(Y₁)) = 0.0133 > 0\n\nAnalyzing each option:\n\n**Option A**: Since Augξ > ξ, the product (∂Ln(TC)/∂Ln(B))(dLn(B)/dLn(Y₁)) must be positive. For a product to be positive, either both factors are positive or both are negative. Given the economic context (more branches typically increase costs, and more output typically requires more branches), both elasticities being positive is the only sensible interpretation. ✓\n\n**Option B**: The difference between Augξ and ξ is only 0.0133, which is indeed small. Under strict branching restrictions, banks cannot easily open new branches when output increases, so dLn(B)/dLn(Y₁) would be suppressed (close to zero). This would make the product term small, consistent with the observed small difference. ✓\n\n**Option C**: The partial derivative ∂Ln(TC)/∂Ln(B) represents the elasticity of cost with respect to branches while holding output constant (partial derivative). It does not allow output to adjust simultaneously - that would be a total derivative. ✗\n\n**Option D**: If branching regulations were lifted, banks would have more freedom to open branches as output increases. This would logically increase the responsiveness of branches to output growth, making dLn(B)/dLn(Y₁) larger. ✓", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 87, "Question": "### Background\n\nTo isolate the causal impact of the 2000 pro-union legislation, the study compares the abnormal returns of affected New Zealand firms (\"Sample firms\") to a \"Control group\" of NZX-listed firms that operate overseas and are therefore not subject to the law. This difference-in-differences approach aims to control for concurrent market-wide shocks affecting all firms listed on the New Zealand exchange (NZX).\n\n### Data / Model Specification\n\nThe table below shows the combined 3-day Average Cumulative Abnormal Return (ACAR) for the five `Aggressive` legislative events for both the sample and control groups.\n\n**Table 1: Combined 3-Day ACAR for Aggressive Events**\n\n| Group                    | 3-Day ACAR | BMP t-statistic |\n| :----------------------- | :--------: | :-------------: |\n| Panel A: Sample Firms    | -0.044     | [-3.06]***      |\n| Panel B: Control Firms   | -0.012     | [-1.20]         |\n\n*Note: *** denotes significance at the 1% level.*\n\n---\n\nBased on the research design and the data in Table 1, which of the following statements are **NOT** valid conclusions?", "Options": {"A": "The control group experienced a statistically significant negative return, indicating a broad market downturn was the primary driver of the sample firms' negative returns.", "B": "The difference-in-differences estimate of the legislation's impact is -0.032, or -3.2%.", "C": "The use of the control group helps to isolate the legislation's effect from confounding factors specific to the New Zealand capital market.", "D": "The results suggest that the legislation had no impact, as both the sample and control firms experienced negative returns."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item assesses the ability to calculate and interpret a difference-in-differences estimate from a table and understand the causal logic of using a control group. Depth Strategy: Computational Judgment and Reverse-Reasoning. The user must first calculate the DiD estimate and then evaluate the validity of several conclusions based on that calculation and the research design. Inversion Protocol: The core assessment objective is to correctly calculate and interpret the DiD estimate (-3.2%) and understand the research design's logic. This leads to two unique correct conclusions (A and C). To meet the '≥2 correct answers' rule, the Single-Correct-Answer Inversion Protocol was applied. The stem was changed to ask for what is 'NOT' a valid conclusion, making the original distractors (B and D) the new correct answers.", "qid": "87", "question": "### Background\n\nTo isolate the causal impact of the 2000 pro-union legislation, the study compares the abnormal returns of affected New Zealand firms (\"Sample firms\") to a \"Control group\" of NZX-listed firms that operate overseas and are therefore not subject to the law. This difference-in-differences approach aims to control for concurrent market-wide shocks affecting all firms listed on the New Zealand exchange (NZX).\n\n### Data / Model Specification\n\nThe table below shows the combined 3-day Average Cumulative Abnormal Return (ACAR) for the five `Aggressive` legislative events for both the sample and control groups.\n\n**Table 1: Combined 3-Day ACAR for Aggressive Events**\n\n| Group                    | 3-Day ACAR | BMP t-statistic |\n| :----------------------- | :--------: | :-------------: |\n| Panel A: Sample Firms    | -0.044     | [-3.06]***      |\n| Panel B: Control Firms   | -0.012     | [-1.20]         |\n\n*Note: *** denotes significance at the 1% level.*\n\n---\n\nBased on the research design and the data in Table 1, which of the following statements are **NOT** valid conclusions?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which statements are NOT valid conclusions based on the data provided.\n\nLet me analyze each option:\n\n**Option A**: Claims the control group had a statistically significant negative return. However, the table shows the control group's t-statistic is [-1.20], which is not statistically significant (no asterisks). This statement is FALSE.\n\n**Option B**: The difference-in-differences estimate would be: Sample ACAR - Control ACAR = -0.044 - (-0.012) = -0.032 or -3.2%. This calculation is correct, so this statement is VALID.\n\n**Option C**: This correctly describes the purpose of using a control group in a difference-in-differences design - to isolate the treatment effect from market-wide factors. This statement is VALID.\n\n**Option D**: Claims the legislation had no impact because both groups had negative returns. This misunderstands difference-in-differences methodology. The key is the differential impact between groups, not whether both are negative. The sample firms had a much larger and statistically significant negative return compared to the control group, suggesting the legislation did have an impact. This statement is FALSE.\n\nTherefore, the statements that are NOT valid conclusions are A and D.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 15, "Question": "### Background\n\n**Research Question.** What are the empirical determinants of eurobond issuance spreads?\n\n**Setting.** A linear regression model is specified to explain issuance spreads. One of the explanatory variables is `FEES`, the gross fees charged by the underwriting syndicate, intended as a proxy for primary market efficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper notes that there is no clear theoretical prediction for the sign of the coefficient on `FEES`. The empirical analysis finds a statistically significant positive coefficient.\n\n---\n\n### Question\n\nAccording to the paper's discussion, which of the following are valid economic rationales that could explain the relationship between `FEES` and bond spreads? Select all that apply.", "Options": {"A": "Issuers that are less appealing to investors may have to pay both higher spreads to attract capital and higher fees to underwriters, creating a positive correlation.", "B": "Higher fees may reflect a more intensive marketing effort by the syndicate, which generates stronger investor demand and allows the issuer to place the bond at a lower spread.", "C": "Issuers pay higher fees to reputable underwriters to signal high bond quality to the market, which in turn leads to lower spreads.", "D": "Higher fees are a direct component of the bond's yield-to-maturity, so the relationship between fees and spread must be positive by definition."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: Tests comprehension of competing theoretical arguments presented in the text. Strategy: Scenario Application. Distractor Logic: The question requires identifying the two competing hypotheses discussed in the paper. (A) describes the 'Risk Correlation Hypothesis' (predicts positive sign). (B) describes the 'Cost Pass-Through / Effort Hypothesis' (predicts negative sign). Both are discussed as valid a priori arguments. (C) is a conceptual error, confusing fees paid by the issuer with the spread paid to investors. (D) describes a plausible signaling story, but it is not one of the two hypotheses mentioned in the paper, making it a 'True but Irrelevant' type of distractor.", "qid": "15", "question": "### Background\n\n**Research Question.** What are the empirical determinants of eurobond issuance spreads?\n\n**Setting.** A linear regression model is specified to explain issuance spreads. One of the explanatory variables is `FEES`, the gross fees charged by the underwriting syndicate, intended as a proxy for primary market efficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper notes that there is no clear theoretical prediction for the sign of the coefficient on `FEES`. The empirical analysis finds a statistically significant positive coefficient.\n\n---\n\n### Question\n\nAccording to the paper's discussion, which of the following are valid economic rationales that could explain the relationship between `FEES` and bond spreads? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which explanations are consistent with the empirical finding of a positive coefficient between FEES and bond spreads.\n\nLet me analyze each option:\n\nA) This suggests that less appealing issuers face both higher spreads AND higher fees. This would create a positive correlation between FEES and spreads, which matches the empirical finding. This is a valid explanation.\n\nB) This suggests higher fees lead to better marketing, stronger demand, and LOWER spreads. This would create a negative relationship between FEES and spreads, which contradicts the positive coefficient found. This is not valid.\n\nC) This suggests higher fees (for reputable underwriters) signal quality and lead to LOWER spreads. Again, this would create a negative relationship, contradicting the positive coefficient. This is not valid.\n\nD) This claims fees are a direct component of yield-to-maturity. This is incorrect - underwriting fees are paid to the syndicate and are not part of the bond's yield calculation. The yield is based on the coupon payments and principal repayment to bondholders. This is not valid.\n\nOnly option A provides an explanation consistent with the positive relationship found in the empirical analysis.\n\nA", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 83, "Question": "### Background\n\n**Research Question.** Which modeling approach—OLS, SWR, or Principal Component Regression (PCR)—provides the best out-of-sample predictive performance for housing prices, and what does this imply about the trade-off between dimensionality reduction and information loss?\n\n**Setting.** The predictive accuracy of four models (OLS, SWR, PCR16, and PCR44) is evaluated on a holdout sample of housing sales. The PCR16 model uses 16 factors based on an eigenvalue-one criterion, while the PCR44 model uses all 44 possible factors.\n\n---\n\n### Data / Model Specification\n\nIn-sample, the OLS and PCR44 models have identical explanatory power (`SEE` = $3,737). The PCR16 model has a lower in-sample fit (`SEE` = $5,649). Table 1 presents the out-of-sample prediction results.\n\n**Table 1: Prediction Results on Holdout Sample**\n\n| Model | MAE | RMSE | In-Sample SEE |\n| :--- | :---: | :---: | :---: |\n| OLS | 3,369 | 4,500 | 3,737 |\n| SWR | 3,409 | 4,504 | 3,709 |\n| PCR16 | 5,085 | 7,151 | 5,649 |\n| PCR44 | 3,338 | 4,497 | 3,737 |\n\n---\n\nBased on the prediction results in Table 1 and the paper's discussion, which of the following conclusions about the models' predictive performance are supported?\n", "Options": {"A": "The PCR16 model's poor out-of-sample performance suggests that factors with small eigenvalues, which explain little variance in the predictors, can still contain significant information for predicting the outcome variable.", "B": "The eigenvalue-one criterion used for the PCR16 model is shown to be the optimal strategy for maximizing out-of-sample predictive accuracy.", "C": "The PCR44 model, which uses the full set of factors, demonstrates the best predictive accuracy on the holdout sample, outperforming both OLS and SWR.", "D": "Because the OLS and PCR44 models had identical in-sample fit (SEE), their nearly identical out-of-sample performance (MAE, RMSE) was an expected outcome."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item tests the ability to interpret out-of-sample prediction metrics and understand the trade-off between dimensionality reduction and information loss in PCR. Depth Strategy: Scenario Application. The user must apply the concepts of information loss and model robustness to interpret the specific numerical results in the table. Distractor Logic: Option C is 'Almost Right' but misses the paper's subtle point that even with identical in-sample fit, PCR44's slight outperformance suggests greater robustness. Option D is a conceptual opposite; the data clearly shows PCR16 had the worst predictive accuracy, making the eigenvalue-one criterion suboptimal for prediction in this case.", "qid": "83", "question": "### Background\n\n**Research Question.** Which modeling approach—OLS, SWR, or Principal Component Regression (PCR)—provides the best out-of-sample predictive performance for housing prices, and what does this imply about the trade-off between dimensionality reduction and information loss?\n\n**Setting.** The predictive accuracy of four models (OLS, SWR, PCR16, and PCR44) is evaluated on a holdout sample of housing sales. The PCR16 model uses 16 factors based on an eigenvalue-one criterion, while the PCR44 model uses all 44 possible factors.\n\n---\n\n### Data / Model Specification\n\nIn-sample, the OLS and PCR44 models have identical explanatory power (`SEE` = $3,737). The PCR16 model has a lower in-sample fit (`SEE` = $5,649). Table 1 presents the out-of-sample prediction results.\n\n**Table 1: Prediction Results on Holdout Sample**\n\n| Model | MAE | RMSE | In-Sample SEE |\n| :--- | :---: | :---: | :---: |\n| OLS | 3,369 | 4,500 | 3,737 |\n| SWR | 3,409 | 4,504 | 3,709 |\n| PCR16 | 5,085 | 7,151 | 5,649 |\n| PCR44 | 3,338 | 4,497 | 3,737 |\n\n---\n\nBased on the prediction results in Table 1 and the paper's discussion, which of the following conclusions about the models' predictive performance are supported?\n", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each statement against the empirical evidence:\n\n**Option A**: The PCR16 model shows significantly worse out-of-sample performance (MAE=5,085, RMSE=7,151) compared to PCR44 (MAE=3,338, RMSE=4,497). Since PCR16 uses only 16 factors based on the eigenvalue-one criterion while PCR44 uses all 44 factors, this suggests that the excluded factors (those with eigenvalues less than one) contain important information for prediction. This statement is **supported**.\n\n**Option B**: The PCR16 model has the worst out-of-sample performance among all models, with much higher MAE and RMSE values. This clearly shows the eigenvalue-one criterion is NOT optimal for predictive accuracy. This statement is **not supported**.\n\n**Option C**: PCR44 has the lowest MAE (3,338) among all models, outperforming OLS (3,369) and SWR (3,409). While the RMSE is marginally higher than OLS (4,497 vs 4,500), the overall predictive accuracy based on MAE shows PCR44 performs best. This statement is **supported**.\n\n**Option D**: While OLS and PCR44 had identical in-sample fit (SEE=3,737), their out-of-sample performance is very similar but not identical (MAE: 3,369 vs 3,338; RMSE: 4,500 vs 4,497). The similar performance is consistent with their identical in-sample fit, making this a reasonable expectation. This statement is **supported**.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 66, "Question": "### Background\n\n**Research Question.** How can the non-linear valuation equation arising from the optimal reinsurance problem be solved?\n\n**Setting.** After substituting the optimal reinsurance control `q*` back into the Hamilton-Jacobi-Bellman (HJB) equation, the result is a non-linear ordinary differential equation (ODE) for the value function `v(x)`. This ODE can be linearized by applying a Legendre transform, which maps the problem from the primal space (`x`, `v`) to a dual space (`y`, `ŷ`).\n\n---\n\n### Data / Model Specification\n\nThe non-linear ODE for `v(x)` is:\n\n  \n\\beta v(x)=-(\\eta-\\theta)a v_{x}(x)-\\frac{1}{2}\\left(\\frac{\\eta a}{b}\\right)^{2}\\frac{(v_{x}(x))^{2}}{v_{x x}(x)} \\quad \\text{(Eq. (1))}\n \n\nThe Legendre transform provides the following mapping between the primal (`x`, `v`) and dual (`y`, `ŷ`) spaces:\n- `y = v_x(x)`\n- `v(x) = ŷ(y) - y ŷ_y(y)`\n- `v_{xx}(x) = -1 / ŷ_{yy}(y)`\n\n---\n\n### Question\n\nWhich of the following statements correctly describe the application of the Legendre transform to linearize **Eq. (1)**? Select all that apply.", "Options": {"A": "The primal value function `v(x)` is correctly substituted by the expression `ŷ(y) + y ŷ_y(y)`.", "B": "The non-linear term in Eq. (1), `- (1/2)(ηa/b)² (v_x(x))² / v_{xx}(x)`, transforms into a linear term involving the dual function: `(1/m) y² ŷ_{yy}(y)`, where `m = 2(b/(ηa))²`.", "C": "A key step in the transformation is replacing the second derivative of the primal value function, `v_{xx}(x)`, with the expression `-1 / ŷ_{yy}(y)`, where `ŷ_{yy}(y)` is the second derivative of the dual value function.", "D": "The Legendre transform is used because the original HJB equation is non-linear in the control variable `q`."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item assesses understanding of the advanced mathematical technique (Legendre transform) used in the paper. It uses 'Atomic Decomposition' on the derivation process itself. Options A and B correctly identify key mapping relationships and the transformation of the most complex term. Distractor C (Sign Error) presents one of the core mapping relations with an incorrect sign. Distractor D (Conceptual Error) misidentifies the source of the non-linearity that necessitates the transform.", "qid": "66", "question": "### Background\n\n**Research Question.** How can the non-linear valuation equation arising from the optimal reinsurance problem be solved?\n\n**Setting.** After substituting the optimal reinsurance control `q*` back into the Hamilton-Jacobi-Bellman (HJB) equation, the result is a non-linear ordinary differential equation (ODE) for the value function `v(x)`. This ODE can be linearized by applying a Legendre transform, which maps the problem from the primal space (`x`, `v`) to a dual space (`y`, `ŷ`).\n\n---\n\n### Data / Model Specification\n\nThe non-linear ODE for `v(x)` is:\n\n  \n\\beta v(x)=-(\\eta-\\theta)a v_{x}(x)-\\frac{1}{2}\\left(\\frac{\\eta a}{b}\\right)^{2}\\frac{(v_{x}(x))^{2}}{v_{x x}(x)} \\quad \\text{(Eq. (1))}\n \n\nThe Legendre transform provides the following mapping between the primal (`x`, `v`) and dual (`y`, `ŷ`) spaces:\n- `y = v_x(x)`\n- `v(x) = ŷ(y) - y ŷ_y(y)`\n- `v_{xx}(x) = -1 / ŷ_{yy}(y)`\n\n---\n\n### Question\n\nWhich of the following statements correctly describe the application of the Legendre transform to linearize **Eq. (1)**? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to verify each statement about the Legendre transform applied to the given non-linear ODE.\n\nLet me first understand the given information:\n- Original ODE: β v(x) = -(η-θ)a v_x(x) - (1/2)(ηa/b)² (v_x(x))²/v_{xx}(x)\n- Legendre transform relations:\n  - y = v_x(x)\n  - v(x) = ŷ(y) - y ŷ_y(y)\n  - v_{xx}(x) = -1/ŷ_{yy}(y)\n\nNow checking each option:\n\n**Option A**: States v(x) = ŷ(y) + y ŷ_y(y)\nThe given relation is v(x) = ŷ(y) - y ŷ_y(y) (with a minus sign)\nThis is incorrect due to the wrong sign.\n\n**Option B**: Claims the non-linear term transforms to (1/m) y² ŷ_{yy}(y) where m = 2(b/(ηa))²\n\nLet me verify:\n- Original non-linear term: -(1/2)(ηa/b)² (v_x(x))²/v_{xx}(x)\n- Substituting y = v_x(x) and v_{xx}(x) = -1/ŷ_{yy}(y):\n- = -(1/2)(ηa/b)² y²/(-1/ŷ_{yy}(y))\n- = (1/2)(ηa/b)² y² ŷ_{yy}(y)\n\nIf this equals (1/m) y² ŷ_{yy}(y), then:\n1/m = (1/2)(ηa/b)²\nTherefore: m = 2(b/(ηa))²\n\nThis is correct.\n\n**Option C**: States that v_{xx}(x) is replaced with -1/ŷ_{yy}", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 64, "Question": "### Background\n\n**Research Question.** How sensitive are an insurer's optimal dividend and reinsurance policies to the magnitude of the penalty associated with financial ruin?\n\n**Setting.** A numerical example illustrating the theoretical predictions that a higher ruin penalty `P` makes the insurer more conservative. The table shows the optimal policy thresholds (`x_1`, `x_b`) and the initial risk retention `q*(0)` for a range of ruin penalty values `P`.\n\n**Variables and Parameters.**\n- `P`: The penalty for ruin. `P < 0` indicates a salvage value (units: currency).\n- `x_1`: The surplus threshold separating partial and full reinsurance (units: currency).\n- `x_b`: The optimal dividend barrier (units: currency).\n- `q*(0)`: The optimal retention proportion at infinitesimally small surplus (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe following table presents numerical solutions for the optimal policy under the parameterization: `a=1`, `b=0.2`, `θ=0.1`, `β=0.05`, and `η=0.15`.\n\n**Table 1: The effect of `P` on `x_1`, `x_b`, and `q*(0)`**\n\n| P    | x_1   | x_b   | q*(0) |\n|:-----|:------|:------|:------|\n| -1.0 | 0.080 | 0.651 | 0.863 |\n| 10.0 | 0.606 | 1.177 | 0.574 |\n\nTheoretically, in the full retention region `x ∈ [x_1, x_b)`, the value function solves a second-order ODE whose characteristic roots `R_1 > 0` and `R_2 < 0` are given by:\n\n  \nR_{1,2}=\\frac{-\\theta a\\pm\\sqrt{(\\theta a)^{2}+2b^{2}\\beta}}{b^{2}} \\quad \\text{(Eq. (1))}\n \n\nThe size of this full-retention region is constant and given by:\n\n  \nx_{b}-x_{1}=\\frac{1}{R_{1}-R_{2}}\\ln\\left(\\frac{2\\beta-\\eta a R_{2}}{2\\beta-\\eta a R_{1}}\\right) \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nUsing the provided parameters and equations, which of the following statements about the theoretical model are correct? Select all that apply.", "Options": {"A": "The positive characteristic root `R_1` of the value function's ODE in the full retention region is approximately 0.458.", "B": "The theoretically derived size of the full-retention region, `x_b - x_1`, is approximately 0.571, which is consistent with the data in Table 1.", "C": "The size of the full-retention region `x_b - x_1` is approximately 3.38, based on the logarithm term in Eq. (2).", "D": "The negative characteristic root `R_2` is approximately 5.458."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform a multi-step calculation and verify a theoretical result against data, a 'Computational Judgment' strategy. Options A and B are correct intermediate and final results of the calculation. Distractor C (Sign Error) presents `R_2` with the wrong sign. Distractor D (Step-Omission Error) presents an intermediate value from the calculation as the final answer, a common error.", "qid": "64", "question": "### Background\n\n**Research Question.** How sensitive are an insurer's optimal dividend and reinsurance policies to the magnitude of the penalty associated with financial ruin?\n\n**Setting.** A numerical example illustrating the theoretical predictions that a higher ruin penalty `P` makes the insurer more conservative. The table shows the optimal policy thresholds (`x_1`, `x_b`) and the initial risk retention `q*(0)` for a range of ruin penalty values `P`.\n\n**Variables and Parameters.**\n- `P`: The penalty for ruin. `P < 0` indicates a salvage value (units: currency).\n- `x_1`: The surplus threshold separating partial and full reinsurance (units: currency).\n- `x_b`: The optimal dividend barrier (units: currency).\n- `q*(0)`: The optimal retention proportion at infinitesimally small surplus (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe following table presents numerical solutions for the optimal policy under the parameterization: `a=1`, `b=0.2`, `θ=0.1`, `β=0.05`, and `η=0.15`.\n\n**Table 1: The effect of `P` on `x_1`, `x_b`, and `q*(0)`**\n\n| P    | x_1   | x_b   | q*(0) |\n|:-----|:------|:------|:------|\n| -1.0 | 0.080 | 0.651 | 0.863 |\n| 10.0 | 0.606 | 1.177 | 0.574 |\n\nTheoretically, in the full retention region `x ∈ [x_1, x_b)`, the value function solves a second-order ODE whose characteristic roots `R_1 > 0` and `R_2 < 0` are given by:\n\n  \nR_{1,2}=\\frac{-\\theta a\\pm\\sqrt{(\\theta a)^{2}+2b^{2}\\beta}}{b^{2}} \\quad \\text{(Eq. (1))}\n \n\nThe size of this full-retention region is constant and given by:\n\n  \nx_{b}-x_{1}=\\frac{1}{R_{1}-R_{2}}\\ln\\left(\\frac{2\\beta-\\eta a R_{2}}{2\\beta-\\eta a R_{1}}\\right) \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nUsing the provided parameters and equations, which of the following statements about the theoretical model are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "I need to calculate the characteristic roots R₁ and R₂ using Equation (1), then verify the size of the full-retention region using Equation (2).\n\nGiven parameters:\n- a = 1\n- b = 0.2\n- θ = 0.1\n- β = 0.05\n- η = 0.15\n\nFirst, let me calculate R₁ and R₂ using Equation (1):\n\nR₁,₂ = (-θa ± √((θa)² + 2b²β)) / b²\n\nSubstituting values:\n- θa = 0.1 × 1 = 0.1\n- (θa)² = 0.01\n- b² = 0.04\n- 2b²β = 2 × 0.04 × 0.05 = 0.004\n\nSo:\nR₁,₂ = (-0.1 ± √(0.01 + 0.004)) / 0.04\nR₁,₂ = (-0.1 ± √0.014) / 0.04\nR₁,₂ = (-0.1 ± 0.1183) / 0.04\n\nTherefore:\n- R₁ = (-0.1 + 0.1183) / 0.04 = 0.0183 / 0.04 = 0.458\n- R₂ = (-0.1 - 0.1183) / 0.04 = -0.2183 / 0.04 = -5.458\n\nNow, let me calculate the size of the full-retention region using Equation (2):\n\nx_b - x₁ = (1/(R₁ - R₂)) × ln((2β - ηaR₂)/(2β - ηaR₁))\n\nFirst, calculate the terms:\n- R₁ - R₂ = 0.458 - (-5.458) = 5.916\n- ηa = 0.15 × 1 = 0.15\n- 2β - ηaR₂ = 0.1 - 0.15 × (-5.458) = 0.1 + 0.8187 = 0.9187\n- 2β - ηaR₁ = 0.1 - 0.15 × 0.458 = 0.1 - 0.0687 = 0.0313\n\nSo:\nx_b - x₁ = (1/5.916) × ln(0.9187/0.0313) = (1/5.916) × ln(29.35) = (1/5.916) × 3.379 = 0.571\n\nLet me verify this with the data in Table 1:\n- For P = -1.0: x_b - x₁ = 0.651 - 0.080 = 0.571\n- For P = 10.0: x_b - x₁ = 1.177 - 0.606 = 0.571\n\nNow checking each option:\n- A) R₁ ≈ 0.458 ✓ (correct)\n- B) x_b - x₁ ≈ 0.571, consistent with Table 1 ✓ (correct)\n- C) x_b - x₁ ≈ 3.38 ✗ (incorrect, the logarithm term is ~3.38, but the full expression gives 0.571)\n- D) R₂ ≈ 5.458 ✗ (incorrect, R₂ ≈ -5.458, not positive 5.458)\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 117, "Question": "### Background\n\n**Research Question.** In an exploratory analysis with many potential explanatory variables and likely multicollinearity, how can a researcher develop a reliable and parsimonious regression model?\n\n**Setting / Data-Generating Environment.** The paper employs a two-stage methodology to build predictive models for local employment. First, it uses a diagnostic procedure to detect and remove collinear variables. Second, it uses a model selection criterion to choose the \"best\" subset of the remaining predictors. This approach is common in exploratory econometric work where theory provides only loose guidance on the exact model specification.\n\n---\n\n### Data / Model Specification\n\n1.  **Collinearity Diagnostic:** The study uses the Belsley, Kuh, and Welsch (BKW) procedure. A degrading collinearity is flagged if a high **condition index** (e.g., > 30) is associated with high **variance proportions** (e.g., > 0.5) for two or more variables. When collinearity is detected, the author eliminates the variable with the lower Student's t-statistic in a preliminary regression.\n\n2.  **Model Selection:** From the remaining variables, the final model is chosen using Mallows' Cp statistic, which estimates the total prediction error.\n      \n    C_p = \\frac{RSS_p}{s^2} - (n - 2p) \\quad \\text{(Eq. 1)}\n     \n    where `p` is the number of parameters, `n` is the sample size, `RSS_p` is the residual sum of squares, and `s^2` is the estimated error variance from the largest model. For a model with no systematic bias, its expected `C_p` value is approximately `p`.\n\n---\n\n### The Question\n\nGiven the study's small sample size (n=26 years), what are the likely risks of the author's two-stage procedure (using t-tests to screen for collinearity, then minimizing Cp)? Select all that apply.", "Options": {"A": "An increased risk of Type I error, where an irrelevant variable is incorrectly kept in the final model.", "B": "An increased risk of Type II error, where a truly important economic variable is incorrectly dropped from the final model.", "C": "The final selected model may be sensitive to small changes in the data, reflecting sampling noise rather than a stable underlying relationship.", "D": "A guaranteed increase in omitted variable bias compared to simply running the full model."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the ability to synthesize the flaws of the paper's methodology and evaluate its risks in a small-sample context. It uses a Scenario Application strategy. The correct answers (A, B) identify the key risks: the conservative nature of the procedure increases the chance of false negatives (Type II error), and data-driven model selection in small samples leads to instability. Distractors include an overly strong claim (C) and the conceptual opposite (D), as the procedure is more likely to commit Type II than Type I errors.", "qid": "117", "question": "### Background\n\n**Research Question.** In an exploratory analysis with many potential explanatory variables and likely multicollinearity, how can a researcher develop a reliable and parsimonious regression model?\n\n**Setting / Data-Generating Environment.** The paper employs a two-stage methodology to build predictive models for local employment. First, it uses a diagnostic procedure to detect and remove collinear variables. Second, it uses a model selection criterion to choose the \"best\" subset of the remaining predictors. This approach is common in exploratory econometric work where theory provides only loose guidance on the exact model specification.\n\n---\n\n### Data / Model Specification\n\n1.  **Collinearity Diagnostic:** The study uses the Belsley, Kuh, and Welsch (BKW) procedure. A degrading collinearity is flagged if a high **condition index** (e.g., > 30) is associated with high **variance proportions** (e.g., > 0.5) for two or more variables. When collinearity is detected, the author eliminates the variable with the lower Student's t-statistic in a preliminary regression.\n\n2.  **Model Selection:** From the remaining variables, the final model is chosen using Mallows' Cp statistic, which estimates the total prediction error.\n      \n    C_p = \\frac{RSS_p}{s^2} - (n - 2p) \\quad \\text{(Eq. 1)}\n     \n    where `p` is the number of parameters, `n` is the sample size, `RSS_p` is the residual sum of squares, and `s^2` is the estimated error variance from the largest model. For a model with no systematic bias, its expected `C_p` value is approximately `p`.\n\n---\n\n### The Question\n\nGiven the study's small sample size (n=26 years), what are the likely risks of the author's two-stage procedure (using t-tests to screen for collinearity, then minimizing Cp)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this two-stage procedure with a small sample size (n=26), I need to analyze the risks systematically.\n\nThe procedure involves:\n1. Using BKW diagnostics to detect collinearity, then dropping variables based on t-statistics\n2. Using Mallows' Cp to select the final model from remaining variables\n\nWith n=26 and multiple potential predictors, let me evaluate each option:\n\n**Option A (Type I error - keeping irrelevant variables):**\nThe procedure actually tends to be aggressive in removing variables - first through collinearity screening, then through Cp minimization. This makes keeping irrelevant variables less likely, not more likely.\n\n**Option B (Type II error - dropping important variables):**\nThis is a significant risk. With n=26, t-statistics have low power and high variability. When collinearity is present, a truly important variable might have a low t-statistic due to inflated standard errors and get incorrectly dropped in stage 1. Once dropped, it cannot be reconsidered in stage 2.\n\n**Option C (Model instability):**\nThis is also a major concern. With small samples:\n- T-statistics are highly variable, making stage 1 decisions unstable\n- Cp estimates are noisy, making stage 2 selection unreliable\n- Small data perturbations could lead to entirely different models being selected\n\n**Option D (Guaranteed increase in omitted variable bias):**\nThis is too strong a claim. While the procedure risks dropping important variables (creating omitted variable bias), it's not guaranteed to be worse than the full model, which might have severe multicollinearity issues making it practically unusable.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 7, "Question": "### Background\n\n**Research Question.** This case investigates whether non-seasonal, global climatic phenomena can predict anomaly returns, as a test to rule out simple seasonality as an explanation for weather-related predictability.\n\n**Setting / Data-Generating Environment.** The analysis uses a 12-month moving average (MA) of the global temperature anomaly to predict the monthly excess returns of the Earnings-to-price (E/P) strategy.\n\n---\n\n### Data / Model Specification\n\nThe core predictive regression is:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \nwhere `X_{t-1}` is the 12-month MA of the global temperature anomaly.\n\n**Table 1: Climatic Variables and Anomaly Performance (Monthly Excess Returns, %)**\n\n| Predictive Variable / Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Global temp. anomaly (12-mo MA)** | | | | |\n| Earnings-to-price | 1.11 | -2.24 | 0.75 | 1.47 |\n| | [5.10] | [-2.18] | [2.44] | [4.78] |\n\n*Note: `t`-statistics are in brackets. `a_H` and `a_L` are returns conditional on the predictor being above/below its median.*\n\n---\n\n### Question\n\nBased on the provided information, select all of the following statements that are correct.", "Options": {"A": "The E/P strategy performs significantly better when the 12-month MA of global temperature is above its median (`a_H > a_L`).", "B": "The estimated coefficient `b = -2.24` implies that a 0.5°C increase in the smoothed global temperature anomaly is predicted to decrease the monthly E/P premium by 1.12%.", "C": "A primary reason for using a 12-month moving average of temperature is to remove the influence of seasonal weather cycles from the predictor variable.", "D": "By using a global temperature measure, the author confirms that the mood of NYC-based traders is the primary channel for weather effects."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test the purpose of a time-series filter and the interpretation of a regression coefficient. Option A correctly identifies the purpose of a 12-month moving average. Option B is a correct calculation based on the coefficient (Computational Judgment). Distractor C is a Factual Error, directly contradicting the values in Table 1. Distractor D is a Conceptual Opposite to the paper's argument; using a global measure is meant to make the local trader mood story *less* plausible, thereby strengthening the satire.", "qid": "7", "question": "### Background\n\n**Research Question.** This case investigates whether non-seasonal, global climatic phenomena can predict anomaly returns, as a test to rule out simple seasonality as an explanation for weather-related predictability.\n\n**Setting / Data-Generating Environment.** The analysis uses a 12-month moving average (MA) of the global temperature anomaly to predict the monthly excess returns of the Earnings-to-price (E/P) strategy.\n\n---\n\n### Data / Model Specification\n\nThe core predictive regression is:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \nwhere `X_{t-1}` is the 12-month MA of the global temperature anomaly.\n\n**Table 1: Climatic Variables and Anomaly Performance (Monthly Excess Returns, %)**\n\n| Predictive Variable / Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Global temp. anomaly (12-mo MA)** | | | | |\n| Earnings-to-price | 1.11 | -2.24 | 0.75 | 1.47 |\n| | [5.10] | [-2.18] | [2.44] | [4.78] |\n\n*Note: `t`-statistics are in brackets. `a_H` and `a_L` are returns conditional on the predictor being above/below its median.*\n\n---\n\n### Question\n\nBased on the provided information, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) The E/P strategy performs significantly better when the 12-month MA of global temperature is above its median (`a_H > a_L`).\n- From the table: a_H = 0.75% and a_L = 1.47%\n- Since 0.75 < 1.47, we have a_H < a_L, not a_H > a_L\n- The statement claims the opposite of what the data shows\n- **Incorrect**\n\nB) The estimated coefficient `b = -2.24` implies that a 0.5°C increase in the smoothed global temperature anomaly is predicted to decrease the monthly E/P premium by 1.12%.\n- From equation (1): R_t = a + b*X_{t-1} + ε_t\n- With b = -2.24, a 0.5 unit increase in X_{t-1} leads to: -2.24 × 0.5 = -1.12% change in returns\n- **Correct**\n\nC) A primary reason for using a 12-month moving average of temperature is to remove the influence of seasonal weather cycles from the predictor variable.\n- The research question explicitly states this is \"a test to rule out simple seasonality as an explanation\"\n- A 12-month MA averages over all seasons, effectively removing seasonal patterns\n- This aligns with the stated goal of investigating \"non-seasonal, global climatic phenomena\"\n- **Correct**\n\nD) By using a global temperature measure, the author confirms that the mood of NYC-based traders is the primary channel for weather effects.\n- Using global temperature would actually test against location-specific explanations\n- If NYC trader mood were the mechanism, local NYC weather would be more relevant than global temperature\n- The use of global measures suggests testing for broader effects beyond any single location\n- **Incorrect**\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 72, "Question": "### Background\n\n**Research Question.** How does the structure of a Bonus-Malus System's (BMS) transition rules affect its rate of convergence to a steady state?\n\n**Setting / Data-Generating Environment.** The rate of convergence of a Markov chain, which models a BMS, is governed by its second largest eigenvalue modulus (SLEM). A smaller SLEM implies faster convergence. A SLEM of 0 implies convergence in a single step.\n\n---\n\n### Data / Model Specification\n\nConsider a simple \"reset\" BMS with `s` classes. The transition rules are independent of the current class:\n- **0 claims:** The driver moves to the best class (Class 1).\n- **1 or more claims:** The driver moves to the worst class (Class `s`).\n\nThe probability of having 0 claims is `p_0(λ)`, and the probability of having 1 or more claims is `1 - p_0(λ)`.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the mathematical properties of this \"reset\" BMS.", "Options": {"A": "The trace of the transition matrix `M(λ)` is equal to `p_0(λ)`.", "B": "This system design is an example of a slowly converging BMS whose steady-state properties are of little practical relevance.", "C": "The `s x s` transition matrix `M(λ)` for this system has `s-1` eigenvalues equal to 0.", "D": "The system reaches its steady-state distribution in a single transition period."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the understanding of the mathematical properties of a stylized Markov chain, specifically connecting the structure of the transition matrix to its eigenvalues and convergence rate. It uses an Atomic Decomposition strategy, testing key results from the original derivation. Distractor B is a Formula Misuse error, as the trace is 1. Distractor D is a Conceptual Opposite, as the system exhibits the fastest possible convergence.", "qid": "72", "question": "### Background\n\n**Research Question.** How does the structure of a Bonus-Malus System's (BMS) transition rules affect its rate of convergence to a steady state?\n\n**Setting / Data-Generating Environment.** The rate of convergence of a Markov chain, which models a BMS, is governed by its second largest eigenvalue modulus (SLEM). A smaller SLEM implies faster convergence. A SLEM of 0 implies convergence in a single step.\n\n---\n\n### Data / Model Specification\n\nConsider a simple \"reset\" BMS with `s` classes. The transition rules are independent of the current class:\n- **0 claims:** The driver moves to the best class (Class 1).\n- **1 or more claims:** The driver moves to the worst class (Class `s`).\n\nThe probability of having 0 claims is `p_0(λ)`, and the probability of having 1 or more claims is `1 - p_0(λ)`.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the mathematical properties of this \"reset\" BMS.", "model_evaluation_result": {"model_raw_response": "Looking at this \"reset\" BMS, I need to analyze its transition matrix structure and properties.\n\nThe transition matrix M(λ) for this system has a very specific form:\n- Column 1: All entries are p₀(λ) (everyone with 0 claims goes to class 1)\n- Column s: All entries are 1 - p₀(λ) (everyone with 1+ claims goes to class s)\n- All other columns: All zeros (no one stays in intermediate classes)\n\nLet me check each statement:\n\n**Statement A**: The trace is the sum of diagonal elements.\n- M₁₁ = p₀(λ)\n- M₂₂ = 0, M₃₃ = 0, ..., Mₛₛ = 0\n- Trace = p₀(λ) + 0 + ... + 0 = p₀(λ) ✓\n\n**Statement B**: This system converges in one step (as I'll show in D), making it extremely fast-converging, not slowly converging. ✗\n\n**Statement C**: To find eigenvalues, I need to solve det(M - λI) = 0.\nDue to the special structure (only columns 1 and s are non-zero), this determinant can be expanded to show that (s-1) eigenvalues are indeed 0, with the remaining eigenvalue being p₀(λ). ✓\n\n**Statement D**: Starting from any initial distribution, after one transition:\n- All probability mass moves to either class 1 (with probability p₀(λ)) or class s (with probability 1-p₀(λ))\n- This is exactly the steady-state distribution\n- The system reaches steady state in one step ✓\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 86, "Question": "### Background\n\nThe study employs a \"price-reversal\" test to enhance the internal validity of its findings. This test examines the correlation of firm-level abnormal returns across consecutive legislative news events. The core hypothesis is that the correlation should be negative for contradictory news (e.g., an `Aggressive` event followed by a `Moderating` one) and positive for similar news (e.g., two `Moderating` events).\n\n### Data / Model Specification\n\nThe table below presents Pearson Correlation Coefficients (PCC) for two pairs of consecutive events.\n\n**Table 1: Pearson Correlation Coefficients (PCC) for Consecutive Events**\n\n| Correlation Between Events                  | Expected Sign | PCC (p-value)   |\n| :------------------------------------------ | :-----------: | :-------------: |\n| Mar 30 (Aggressive) and May 30 (Moderating) | -             | -0.160 (.041)   |\n| May 30 (Moderating) and Jun 13 (Moderating) | +             | -0.184 (.020)   |\n\n---\n\nBased on the price-reversal test logic and the results in Table 1, which of the following conclusions are supported?\n", "Options": {"A": "The results for the March/May event pair help rule out the alternative explanation that returns are driven by persistent, firm-specific trends unrelated to the legislation.", "B": "The correlation between the two moderating events (May 30 and June 13) is positive and statistically significant, confirming the study's hypothesis.", "C": "The significant negative correlation between the March 30 (Aggressive) and May 30 (Moderating) events is consistent with the price-reversal hypothesis.", "D": "The significant negative correlation between the two moderating events (May 30 and June 13) is an anomaly that contradicts the price-reversal hypothesis."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to understand the causal logic of the price-reversal test, interpret correlation coefficients and p-values, and identify results that are either consistent or anomalous with the stated hypothesis. Depth Strategy: Reverse-Reasoning. The user is given empirical results and must identify the plausible causal inferences and methodological conclusions. Distractor Logic: D is a conceptual opposite that correctly identifies the expected sign but misreads the actual result from the table, representing a common error of conflating expectation with observation.", "qid": "86", "question": "### Background\n\nThe study employs a \"price-reversal\" test to enhance the internal validity of its findings. This test examines the correlation of firm-level abnormal returns across consecutive legislative news events. The core hypothesis is that the correlation should be negative for contradictory news (e.g., an `Aggressive` event followed by a `Moderating` one) and positive for similar news (e.g., two `Moderating` events).\n\n### Data / Model Specification\n\nThe table below presents Pearson Correlation Coefficients (PCC) for two pairs of consecutive events.\n\n**Table 1: Pearson Correlation Coefficients (PCC) for Consecutive Events**\n\n| Correlation Between Events                  | Expected Sign | PCC (p-value)   |\n| :------------------------------------------ | :-----------: | :-------------: |\n| Mar 30 (Aggressive) and May 30 (Moderating) | -             | -0.160 (.041)   |\n| May 30 (Moderating) and Jun 13 (Moderating) | +             | -0.184 (.020)   |\n\n---\n\nBased on the price-reversal test logic and the results in Table 1, which of the following conclusions are supported?\n", "model_evaluation_result": {"model_raw_response": "Looking at the table and the price-reversal test logic:\n\n1. March 30 (Aggressive) → May 30 (Moderating): \n   - Expected sign: Negative (contradictory news)\n   - Actual PCC: -0.160 (p=0.041) - negative and statistically significant\n   - This matches the hypothesis\n\n2. May 30 (Moderating) → June 13 (Moderating):\n   - Expected sign: Positive (similar news)\n   - Actual PCC: -0.184 (p=0.020) - negative and statistically significant\n   - This contradicts the hypothesis\n\nEvaluating each option:\n\nA) True - The negative correlation for contradictory events helps rule out persistent firm-specific trends (which would show positive correlation regardless of event type)\n\nB) False - The correlation is negative (-0.184), not positive\n\nC) True - The negative correlation for contradictory events matches the price-reversal hypothesis\n\nD) True - The negative correlation between similar events contradicts the hypothesis (should be positive)\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 51, "Question": "### Background\n\nAn analyst is investigating whether the interaction between product life cycle (PLC) and the book-to-market (BM) effect is driven by investor inattention. The hypothesis is that the effect should be concentrated in firms that are less scrutinized by the market. To test this, the analyst splits a sample of firms into 'Low Attention' and 'High Attention' groups and runs Fama-MacBeth regressions separately on each subsample.\n\n### Data / Model Specification\n\nThe Fama-MacBeth regression model is estimated for each group:\n\n  \nReturn_{i,t+1} = \\gamma_{0,t} + ... + \\gamma_{BM \\times PLC, t} (BM_{i,t} \\times PLC_{i,t}) + ... + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n \n\nTable 1 presents the key coefficient estimates for the interaction term in each subsample.\n\n**Table 1: Fama-MacBeth Results for `BM × PLC` in Attention Subsamples**\n\n| Subsample | Coefficient (`\\hat{\\gamma}_{BM × PLC}`) | *t-statistic* |\n| :--- | :---: | :---: |\n| Low Investor Attention | 0.02 | (2.75) |\n| High Investor Attention | 0.01 | (1.40) |\n| **Wald Test for Difference** | *p-value* | *(<0.01)* |\n\n*Source: Abridged from Table 5 of the paper. The Wald test confirms the coefficients are statistically different.* \n\n---\n\nBased on the provided results, which of the following conclusions are supported by the evidence? Select all that apply.", "Options": {"A": "The Wald test indicates that the coefficients for the interaction term in the low and high attention groups are statistically indistinguishable.", "B": "The results are consistent with the hypothesis that investor inattention is a key channel through which PLC moderates the book-to-market effect.", "C": "For highly scrutinized firms, a longer product life cycle significantly strengthens the return predictability of the book-to-market ratio.", "D": "The amplifying effect of PLC on the BM-return relationship is statistically significant only for firms with low investor attention."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret split-sample regression results and draw correct inferences about a proposed mechanism (investor inattention). Depth Strategy: Scenario Application. The user must apply the general findings to the specific numerical results in the table. Distractor Logic: C is a conceptual opposite, incorrectly interpreting the insignificant result for the high-attention group. D is a direct contradiction of the Wald test result, testing careful reading of the table's footnote.", "qid": "51", "question": "### Background\n\nAn analyst is investigating whether the interaction between product life cycle (PLC) and the book-to-market (BM) effect is driven by investor inattention. The hypothesis is that the effect should be concentrated in firms that are less scrutinized by the market. To test this, the analyst splits a sample of firms into 'Low Attention' and 'High Attention' groups and runs Fama-MacBeth regressions separately on each subsample.\n\n### Data / Model Specification\n\nThe Fama-MacBeth regression model is estimated for each group:\n\n  \nReturn_{i,t+1} = \\gamma_{0,t} + ... + \\gamma_{BM \\times PLC, t} (BM_{i,t} \\times PLC_{i,t}) + ... + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n \n\nTable 1 presents the key coefficient estimates for the interaction term in each subsample.\n\n**Table 1: Fama-MacBeth Results for `BM × PLC` in Attention Subsamples**\n\n| Subsample | Coefficient (`\\hat{\\gamma}_{BM × PLC}`) | *t-statistic* |\n| :--- | :---: | :---: |\n| Low Investor Attention | 0.02 | (2.75) |\n| High Investor Attention | 0.01 | (1.40) |\n| **Wald Test for Difference** | *p-value* | *(<0.01)* |\n\n*Source: Abridged from Table 5 of the paper. The Wald test confirms the coefficients are statistically different.* \n\n---\n\nBased on the provided results, which of the following conclusions are supported by the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the results in Table 1:\n\n- Low Investor Attention: coefficient = 0.02, t-statistic = 2.75 (statistically significant, as |t| > 1.96)\n- High Investor Attention: coefficient = 0.01, t-statistic = 1.40 (not statistically significant, as |t| < 1.96)\n- Wald Test p-value < 0.01 (indicates the coefficients are statistically different between groups)\n\nLet me evaluate each option:\n\nA) False. The Wald test p-value < 0.01 indicates the coefficients ARE statistically different, not indistinguishable.\n\nB) True. The interaction effect is significant only in the low attention group (0.02 with t=2.75), supporting the hypothesis that investor inattention drives the PLC-BM interaction effect.\n\nC) False. For highly scrutinized (high attention) firms, the coefficient is 0.01 with t=1.40, which is not statistically significant.\n\nD) True. The interaction term is statistically significant only for low attention firms (t=2.75), not for high attention firms (t=1.40).\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 31, "Question": "### Background\n\nThe study investigates whether the investment-stimulating effect of the 2017 Tax Cuts and Jobs Act (TCJA) varied with a firm's capital intensity. A key TCJA provision was 100% \"bonus depreciation,\" allowing immediate expensing of certain capital assets, which would be most valuable to firms that frequently purchase such assets.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated on sub-samples of firms sorted by proxies for capital intensity. The key coefficient on `USFirm*Yr2019` captures the treatment effect.\n\n**Table 1: Heterogeneous Effects by Capital Intensity**\n\n| Sub-sample Split      | High Capital Intensity Group | Coefficient (t-stat) | Low Capital Intensity Group | Coefficient (t-stat) |\n| :-------------------- | :--------------------------- | :------------------- | :-------------------------- | :------------------- |\n| Based on `Tangibility`| High Tangibility             | 0.0038*** (4.60)     | Low Tangibility             | 0.0002 (0.49)        |\n| Based on `R&D`        | Zero R&D                     | 0.0035*** (5.21)     | Non-Zero R&D                | 0.0015* (1.69)       |\n\n*Note: Table is constructed from the paper's Table 3.*\n\n---\n\nBased on the evidence in **Table 1**, which of the following conclusions about the TCJA's effect on investment are supported?\n", "Options": {"A": "Firms with non-zero R&D expenditures increased their capital expenditures more than firms with zero R&D.", "B": "Firms with high asset tangibility showed a statistically significant increase in capital expenditures post-TCJA.", "C": "The results are consistent with the bonus depreciation provision being a key driver of the investment increase.", "D": "The investment-stimulating effect of the TCJA was concentrated in capital-intensive firms."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item tests the ability to interpret heterogeneous treatment effects and link them to specific policy mechanisms discussed in the paper (bonus depreciation).\nDepth Strategy: Reverse-Reasoning. The user must connect the empirical pattern (effects concentrated in capital-intensive firms) to the underlying policy cause (bonus depreciation).\nDistractor Logic:\n- C (Conceptual Opposite): This is a direct contradiction of the results in the table, where the coefficient for Zero R&D firms (0.0035) is larger and more significant than for Non-Zero R&D firms (0.0015).", "qid": "31", "question": "### Background\n\nThe study investigates whether the investment-stimulating effect of the 2017 Tax Cuts and Jobs Act (TCJA) varied with a firm's capital intensity. A key TCJA provision was 100% \"bonus depreciation,\" allowing immediate expensing of certain capital assets, which would be most valuable to firms that frequently purchase such assets.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated on sub-samples of firms sorted by proxies for capital intensity. The key coefficient on `USFirm*Yr2019` captures the treatment effect.\n\n**Table 1: Heterogeneous Effects by Capital Intensity**\n\n| Sub-sample Split      | High Capital Intensity Group | Coefficient (t-stat) | Low Capital Intensity Group | Coefficient (t-stat) |\n| :-------------------- | :--------------------------- | :------------------- | :-------------------------- | :------------------- |\n| Based on `Tangibility`| High Tangibility             | 0.0038*** (4.60)     | Low Tangibility             | 0.0002 (0.49)        |\n| Based on `R&D`        | Zero R&D                     | 0.0035*** (5.21)     | Non-Zero R&D                | 0.0015* (1.69)       |\n\n*Note: Table is constructed from the paper's Table 3.*\n\n---\n\nBased on the evidence in **Table 1**, which of the following conclusions about the TCJA's effect on investment are supported?\n", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each conclusion against the evidence provided.\n\n**Option A**: \"Firms with non-zero R&D expenditures increased their capital expenditures more than firms with zero R&D.\"\n- Zero R&D firms: coefficient = 0.0035*** (highly significant)\n- Non-zero R&D firms: coefficient = 0.0015* (marginally significant)\n- Since 0.0035 > 0.0015, firms with zero R&D actually increased investment MORE than firms with non-zero R&D\n- This statement is FALSE\n\n**Option B**: \"Firms with high asset tangibility showed a statistically significant increase in capital expenditures post-TCJA.\"\n- High Tangibility firms: coefficient = 0.0038*** (t-stat = 4.60)\n- The *** indicates high statistical significance (typically p < 0.01)\n- This statement is TRUE\n\n**Option C**: \"The results are consistent with the bonus depreciation provision being a key driver of the investment increase.\"\n- The background explains that bonus depreciation would be most valuable to firms that frequently purchase capital assets\n- High capital intensity firms (high tangibility, zero R&D) show larger and more significant effects\n- Low capital intensity firms (low tangibility, non-zero R&D) show smaller/insignificant effects\n- This pattern aligns with the bonus depreciation mechanism\n- This statement is TRUE\n\n**Option D**: \"The investment-stimulating effect of the TCJA was concentrated in capital-intensive firms.\"\n- High capital intensity groups show larger coefficients (0.0038*** and 0.0035***)\n- Low capital intensity groups show smaller/weaker effects (0.0002 and 0.0015*)\n- The effects are clearly concentrated in the high capital intensity firms\n- This statement is TRUE\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 132, "Question": "### Background\n\nTo isolate firm-specific trading activity, the study first controls for market-wide influences on volume by estimating a 'turnover market model'. The residual from this model is defined as 'abnormal turnover' and is used as the dependent variable in all subsequent analyses.\n\n### Data / Model Specification\n\nThe turnover market model is estimated for each stock `i` using a 3-year rolling window:\n  \nVT_{it} = a_{i} + b_{i}VTM_{t} + e_{it} \\quad \\text{(Eq. 1)}\n \nwhere `VT_it` is the turnover of stock `i` in month `t`, and `VTM_t` is the average market turnover. Abnormal turnover (`AVT_it`) for a month in the 4th year is then calculated as the residual, using the coefficients estimated from the prior 3 years:\n  \nAVT_{it} = VT_{it} - (\\hat{a}_{i} + \\hat{b}_{i}VTM_{t}) \\quad \\text{(Eq. 2)}\n \n\n---\n\nWhich of the following statements accurately describe this methodology or its implications?\n\nSelect all that apply.", "Options": {"A": "If a firm undergoes a major structural change that permanently increases its trading volume (e.g., S&P 500 inclusion) at the start of the 4th year, the `AVT_it` calculated for that year will be systematically biased upwards.", "B": "This procedure is designed to remove the confounding effect of market-wide shifts in trading sentiment, allowing for a cleaner test of firm-specific drivers of volume.", "C": "A key assumption of this method is that the relationship between a stock's turnover and the market's turnover (i.e., `a_i` and `b_i`) is relatively stable over the 4-year period.", "D": "The coefficient `b_i` represents the sensitivity of stock `i`'s turnover to market-wide trading activity, analogous to beta in the capital asset pricing model."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the paper's core methodological step: the calculation of abnormal volume. It tests the interpretation of the model's parameters, its purpose, its underlying assumptions, and its limitations in the face of structural breaks. Depth Strategy: Reverse-Reasoning. Option C requires the candidate to reason backward from a given outcome (a structural break) to identify its effect on the calculated variable. Distractor Logic: All options are correct statements about the methodology. A tests the economic interpretation of the slope coefficient. B tests the causal inference goal of the procedure. C tests the candidate's ability to identify the consequences of violating the model's stationarity assumption. D explicitly states the stationarity assumption itself. This forces a comprehensive check of understanding rather than identifying a single correct fact.", "qid": "132", "question": "### Background\n\nTo isolate firm-specific trading activity, the study first controls for market-wide influences on volume by estimating a 'turnover market model'. The residual from this model is defined as 'abnormal turnover' and is used as the dependent variable in all subsequent analyses.\n\n### Data / Model Specification\n\nThe turnover market model is estimated for each stock `i` using a 3-year rolling window:\n  \nVT_{it} = a_{i} + b_{i}VTM_{t} + e_{it} \\quad \\text{(Eq. 1)}\n \nwhere `VT_it` is the turnover of stock `i` in month `t`, and `VTM_t` is the average market turnover. Abnormal turnover (`AVT_it`) for a month in the 4th year is then calculated as the residual, using the coefficients estimated from the prior 3 years:\n  \nAVT_{it} = VT_{it} - (\\hat{a}_{i} + \\hat{b}_{i}VTM_{t}) \\quad \\text{(Eq. 2)}\n \n\n---\n\nWhich of the following statements accurately describe this methodology or its implications?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this methodology carefully:\n\nThe study uses a 3-year rolling window to estimate a turnover market model, then applies those coefficients to calculate abnormal turnover in the 4th year.\n\nLet me evaluate each statement:\n\n**A) If a firm undergoes a major structural change... AVT_it will be systematically biased upwards.**\n- TRUE. The model estimates coefficients using the prior 3 years of data. If a structural change occurs at the start of year 4 (like S&P 500 inclusion), the actual turnover will increase but the model's predicted \"normal\" turnover won't capture this change. This results in systematically positive residuals (higher AVT_it).\n\n**B) This procedure is designed to remove the confounding effect of market-wide shifts...**\n- TRUE. By regressing individual stock turnover on market turnover and using the residuals, the method explicitly removes the portion of volume that moves with the overall market, isolating firm-specific effects.\n\n**C) A key assumption... is that the relationship... is relatively stable over the 4-year period.**\n- TRUE. The method uses coefficients estimated from years 1-3 to predict turnover in year 4. This only makes sense if the relationship between the stock's turnover and market turnover remains stable. If this relationship changes, the predictions will be inaccurate.\n\n**D) The coefficient b_i represents the sensitivity... analogous to beta in CAPM.**\n- TRUE. Just as CAPM beta measures a stock's return sensitivity to market returns, b_i measures the stock's turnover sensitivity to market turnover. Both capture how individual securities co-move with their respective market measures.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 1, "Question": "### Background\n\n**Research Question.** This case examines the satirical conclusion of the paper: whether patently absurd variables, such as celestial phenomena, can be shown to have statistically significant power to predict asset returns, thereby critiquing the standard empirical asset pricing methodology.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for anomaly strategies from July 1973 to December 2012. Predictors include the angular separation of planets (aspects) and the number of observed sunspots.\n\n**Variables & Parameters.**\n- `R_t`: Monthly excess return of an anomaly strategy (dimensionless).\n- `X_{t-1}`: The value of the celestial predictor at the start of month `t`.\n- `P_1`, `P_2`: Orbital periods of two planets (years).\n- `S`: Synodic period of the two planets (years).\n- `b`: The sensitivity of `R_t` to `X_{t-1}`.\n\n---\n\n### Data / Model Specification\n\nThe core predictive regression is:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \nThe synodic period `S` (the time between conjunctions) of two planets with orbital periods `P_1` and `P_2` is given by:\n  \nS = \\frac{P_1 \\times P_2}{|P_1 - P_2|} \\quad \\text{(Eq. (2))}\n \nThe paper states the orbital periods of Mars and Saturn are 1.881 and 29.46 years, respectively.\n\n**Table 1: Celestial Phenomena and Anomaly Performance (Monthly Excess Returns, %)**\n\n| Predictive Variable / Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Angle between Mars and Saturn** | | | | |\n| Market | 0.51 | 0.53 | 0.98 | 0.04 |\n| | [2.37] | [2.29] | [3.22] | [0.15] |\n| **Observed number of sunspots** | | | | |\n| UMD (Momentum) | 0.66 | 0.01 | 0.99 | 0.32 |\n| | [3.14] | [2.25] | [3.36] | [1.08] |\n\n*Note: `t`-statistics are in brackets. For the angle predictor, `H` means the planets are opposed and `L` means they are in conjunction. For sunspots, `H` and `L` refer to above/below median activity.*\n\n---\n\n### Question\n\nBased on the provided information, select all of the following statements that are correct.", "Options": {"A": "The Market anomaly performs significantly better when Mars and Saturn are in conjunction (`a_L`), a state where their 'energies are strongly blended'.", "B": "According to the paper's narrative, high sunspot activity is associated with higher momentum strategy returns because it impairs investors' ability to process information.", "C": "The Momentum (UMD) strategy earns a higher average return when sunspot activity is below the median (`a_L`) compared to when it is above the median (`a_H`).", "D": "The synodic period of the Mars-Saturn aspect, calculated using Eq. (2), is approximately 2.01 years."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses core comprehension and calculation from the source QA. It uses the Atomic Decomposition strategy to test the calculation of the synodic period and the interpretation of the paper's satirical narrative for sunspots. Distractor C is a Conceptual Opposite, reversing the finding in Table 1 (market performs better when planets are opposed). Distractor D is a direct factual contradiction of the values for `a_H` and `a_L` for the UMD strategy in Table 1.", "qid": "1", "question": "### Background\n\n**Research Question.** This case examines the satirical conclusion of the paper: whether patently absurd variables, such as celestial phenomena, can be shown to have statistically significant power to predict asset returns, thereby critiquing the standard empirical asset pricing methodology.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for anomaly strategies from July 1973 to December 2012. Predictors include the angular separation of planets (aspects) and the number of observed sunspots.\n\n**Variables & Parameters.**\n- `R_t`: Monthly excess return of an anomaly strategy (dimensionless).\n- `X_{t-1}`: The value of the celestial predictor at the start of month `t`.\n- `P_1`, `P_2`: Orbital periods of two planets (years).\n- `S`: Synodic period of the two planets (years).\n- `b`: The sensitivity of `R_t` to `X_{t-1}`.\n\n---\n\n### Data / Model Specification\n\nThe core predictive regression is:\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \nThe synodic period `S` (the time between conjunctions) of two planets with orbital periods `P_1` and `P_2` is given by:\n  \nS = \\frac{P_1 \\times P_2}{|P_1 - P_2|} \\quad \\text{(Eq. (2))}\n \nThe paper states the orbital periods of Mars and Saturn are 1.881 and 29.46 years, respectively.\n\n**Table 1: Celestial Phenomena and Anomaly Performance (Monthly Excess Returns, %)**\n\n| Predictive Variable / Strategy | `m` | `b` | `a_H` | `a_L` |\n| :--- | :--- | :--- | :--- | :--- |\n| **Angle between Mars and Saturn** | | | | |\n| Market | 0.51 | 0.53 | 0.98 | 0.04 |\n| | [2.37] | [2.29] | [3.22] | [0.15] |\n| **Observed number of sunspots** | | | | |\n| UMD (Momentum) | 0.66 | 0.01 | 0.99 | 0.32 |\n| | [3.14] | [2.25] | [3.36] | [1.08] |\n\n*Note: `t`-statistics are in brackets. For the angle predictor, `H` means the planets are opposed and `L` means they are in conjunction. For sunspots, `H` and `L` refer to above/below median activity.*\n\n---\n\n### Question\n\nBased on the provided information, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the data provided.\n\n**Statement A:** Claims Market anomaly performs significantly better when Mars and Saturn are in conjunction (`a_L`).\n- From Table 1: `a_L` = 0.04% with t-stat [0.15]\n- `a_H` = 0.98% with t-stat [3.22]\n- The conjunction state (`a_L`) has much lower returns (0.04% vs 0.98%) and is not statistically significant (t-stat = 0.15 < 2)\n- This statement is **FALSE**\n\n**Statement B:** Claims high sunspot activity is associated with higher momentum returns due to impaired information processing.\n- From Table 1: For UMD (Momentum), `a_H` = 0.99% and `a_L` = 0.32%\n- Higher sunspot activity (H) does show higher returns (0.99% > 0.32%)\n- The statement correctly describes the paper's satirical narrative\n- This statement is **TRUE**\n\n**Statement C:** Claims Momentum strategy earns higher returns when sunspot activity is below median.\n- From Table 1: `a_L` = 0.32% and `a_H` = 0.99%\n- Below median (L) has lower returns than above median (H): 0.32% < 0.99%\n- This statement is **FALSE**\n\n**Statement D:** Claims synodic period of Mars-Saturn is approximately 2.01 years.\n- Using Eq. (2): S = (P₁ × P₂)/(|P₁ - P₂|)\n- Given: P₁ = 1.881 years (Mars), P₂ = 29.46 years (Saturn)\n- S = (1.881 × 29.46)/(29.46 - 1.881) = 55.424/27.579 ≈ 2.009 years\n- This rounds to approximately 2.01 years\n- This statement is **TRUE**\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 37, "Question": "### Background\n\n**Research Question:** This case dissects the informational value of the ECB's 2014 Comprehensive Assessment (CA) by testing two competing hypotheses: Did the CA provide new, bank-specific information about capital adequacy (the \"sorting hypothesis\"), or did it primarily signal the supervisor's future policy stance and risk preferences (the \"signaling hypothesis\")?\n\n**Setting and Sample:** The analysis uses a cross-sectional regression of post-announcement stock returns for 37 listed European banks. The dependent variable is the cumulative abnormal return (CAR) over the 5 trading days following the CA announcement, `CAR_i[0,+5]`.\n\n### Data / Model Specification\n\nThe two hypotheses are tested with a horse race between two types of models explaining `CAR_i[0,+5]`.\n\n1.  **Sorting Hypothesis Model:** Assumes CARs are driven by the new information in the announced capital curtailments.\n      \n    CAR_{i}[0,+5] = \\beta_{0} + \\beta_{1}AQR_{i} + \\beta_{2}STE_{i} + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n     \n2.  **Signaling Hypothesis Model:** Assumes CARs are driven by a market re-evaluation of banks based on pre-existing characteristics (`Z_i`) that the CA revealed to be disfavored by the supervisor.\n      \n    CAR_{i}[0,+5] = \\gamma_{0} + \\gamma_{1}(SME \\, Exposure)_{i} + \\gamma_{2}\\delta_{Italy} + ... + e_{i} \\quad \\text{(Eq. (2))}\n     \n\n**Table 1: Sorting vs. Signaling Hypothesis Regression Results**\n\n| | **Panel A: Sorting Model** | **Panel B: Signaling Model** |\n| :--- | :--- | :--- |\n| **Variable** | **Coefficient (p-value)** | **Coefficient (p-value)** |\n| AQR (%) | -1.62 (0.1542) | | \n| STE (%) | -0.06 (0.9190) | | \n| SME Exposure | | -0.56*** (0.0028) |\n| Dummy Italy | | -0.05** (0.0416) |\n| **R-squared** | **0.07** | **0.62** |\n| **Adj. R-squared** | **0.02** | **0.51** |\n\n*Source: Adapted from Table 7 in the source paper. `***` and `**` denote significance at the 1% and 5% levels. Panel B shows results from the more parsimonious stepwise regression.*\n\n### Question\n\nBased on the provided models and regression results in **Table 1**, which of the following conclusions are supported by the evidence?", "Options": {"A": "The \"signaling hypothesis\" is supported, as the model in Panel B explains a substantial portion of the variation in returns (Adj. R² = 0.51), and key pre-existing bank characteristics are significant predictors.", "B": "The \"sorting hypothesis\" is rejected, as the model in Panel A has negligible explanatory power (Adj. R² = 0.02) and the coefficients for `AQR` and `STE` are statistically insignificant.", "C": "The significant negative coefficient on `SME Exposure` indicates that the market viewed banks with high SME lending as less risky following the CA announcement.", "D": "The results suggest that the announced capital curtailments (`AQR` and `STE`) were the primary drivers of the market's reaction, but their effect was captured indirectly through the signaling model's variables."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret a 'horse race' between two competing empirical models, a central task in empirical finance. It uses an **Atomic Decomposition** strategy, breaking down the overall conclusion into testable statements about each model's validity (Options A & B). Distractors are based on common errors: a **Conceptual Opposite** interpretation of a coefficient's economic meaning (Option C) and a misattribution of explanatory power between the models (Option D).", "qid": "37", "question": "### Background\n\n**Research Question:** This case dissects the informational value of the ECB's 2014 Comprehensive Assessment (CA) by testing two competing hypotheses: Did the CA provide new, bank-specific information about capital adequacy (the \"sorting hypothesis\"), or did it primarily signal the supervisor's future policy stance and risk preferences (the \"signaling hypothesis\")?\n\n**Setting and Sample:** The analysis uses a cross-sectional regression of post-announcement stock returns for 37 listed European banks. The dependent variable is the cumulative abnormal return (CAR) over the 5 trading days following the CA announcement, `CAR_i[0,+5]`.\n\n### Data / Model Specification\n\nThe two hypotheses are tested with a horse race between two types of models explaining `CAR_i[0,+5]`.\n\n1.  **Sorting Hypothesis Model:** Assumes CARs are driven by the new information in the announced capital curtailments.\n      \n    CAR_{i}[0,+5] = \\beta_{0} + \\beta_{1}AQR_{i} + \\beta_{2}STE_{i} + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n     \n2.  **Signaling Hypothesis Model:** Assumes CARs are driven by a market re-evaluation of banks based on pre-existing characteristics (`Z_i`) that the CA revealed to be disfavored by the supervisor.\n      \n    CAR_{i}[0,+5] = \\gamma_{0} + \\gamma_{1}(SME \\, Exposure)_{i} + \\gamma_{2}\\delta_{Italy} + ... + e_{i} \\quad \\text{(Eq. (2))}\n     \n\n**Table 1: Sorting vs. Signaling Hypothesis Regression Results**\n\n| | **Panel A: Sorting Model** | **Panel B: Signaling Model** |\n| :--- | :--- | :--- |\n| **Variable** | **Coefficient (p-value)** | **Coefficient (p-value)** |\n| AQR (%) | -1.62 (0.1542) | | \n| STE (%) | -0.06 (0.9190) | | \n| SME Exposure | | -0.56*** (0.0028) |\n| Dummy Italy | | -0.05** (0.0416) |\n| **R-squared** | **0.07** | **0.62** |\n| **Adj. R-squared** | **0.02** | **0.51** |\n\n*Source: Adapted from Table 7 in the source paper. `***` and `**` denote significance at the 1% and 5% levels. Panel B shows results from the more parsimonious stepwise regression.*\n\n### Question\n\nBased on the provided models and regression results in **Table 1**, which of the following conclusions are supported by the evidence?", "model_evaluation_result": {"model_raw_response": "Looking at the regression results in Table 1:\n\n**Panel A (Sorting Model):**\n- R² = 0.07, Adjusted R² = 0.02 (very low explanatory power)\n- AQR coefficient: -1.62 (p = 0.1542) - not statistically significant\n- STE coefficient: -0.06 (p = 0.9190) - not statistically significant\n\n**Panel B (Signaling Model):**\n- R² = 0.62, Adjusted R² = 0.51 (substantial explanatory power)\n- SME Exposure coefficient: -0.56*** (p = 0.0028) - highly significant at 1% level\n- Dummy Italy coefficient: -0.05** (p = 0.0416) - significant at 5% level\n\nEvaluating each option:\n\n**A) Supported** - The signaling model has high adjusted R² (0.51) and significant coefficients for pre-existing bank characteristics.\n\n**B) Supported** - The sorting model has negligible explanatory power (Adj. R² = 0.02) and both AQR and STE coefficients are statistically insignificant.\n\n**C) Not supported** - The negative coefficient on SME Exposure means banks with higher SME lending had more negative returns, indicating the market viewed them as MORE risky, not less risky.\n\n**D) Not supported** - If the capital curtailments were driving returns through signaling variables, we would still expect some significance in Panel A. The complete lack of significance suggests they were not primary drivers.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 11, "Question": "### Background\n\nA Canadian life insurance company offers segregated fund products, which are similar to mutual funds but include a capital guarantee upon death or maturity (e.g., a return of at least 75% of premiums paid). The cost of this guarantee is highly sensitive to stock market and interest rate movements. A regulatory task force has recommended a stochastic, simulation-based approach to determine the necessary financial resources.\n\n### Data / Model Specification\n\nThe regulatory framework for segregated fund guarantees is based on the output of an internal stochastic simulation model that generates a probability distribution for the future cost, `C`. The capital requirement, `K`, is determined by the following rule:\n\n  \nK = \\mathrm{CTE}(95) - L \n \n\nwhere `L` is the policy liability and `CTE(95)` is the Conditional Tail Expectation of the cost `C` at the 95th percentile, i.e., the average cost conditional on being in the worst 5% of simulated scenarios. The total financial resource requirement is `L + K = CTE(95)`.\n\nAn insurer's simulation model produces the following outputs for a block of guarantees:\n\n*   Mean of the simulated cost distribution: $50 million\n*   95th percentile of the cost distribution: $120 million\n*   Average of all costs exceeding the 95th percentile: $180 million\n\nThe insurer's accounting policy sets the policy liability `L` equal to the mean of the simulated costs.\n\n### Question\n\nBased on the provided data and model specification, which of the following statements are correct?\n", "Options": {"A": "The policy liability, `L`, for this block of business is $120 million.", "B": "The total financial resource the regulator requires the firm to hold, `L + K`, is $180 million.", "C": "The required capital, `K`, represents the average loss in the worst 5% of scenarios.", "D": "The required capital, `K`, for this block of business is $130 million."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to correctly apply the regulatory capital formula by identifying the correct inputs from a set of distributional statistics and interpreting the components of the formula.\nDepth Strategy: Computational Judgment. The candidate must perform a multi-step calculation (identify L, identify CTE(95), then calculate K) and interpret the meaning of the total financial resource.\nDistractor Logic:\n- C (Step-Omission Error): Incorrectly uses the 95th percentile value for the liability `L` instead of the specified mean.\n- D (Conceptual Distractor - Almost Right): Incorrectly defines capital `K` as being equal to CTE(95), when it is actually the amount *in excess* of the liability `L` needed to reach CTE(95).", "qid": "11", "question": "### Background\n\nA Canadian life insurance company offers segregated fund products, which are similar to mutual funds but include a capital guarantee upon death or maturity (e.g., a return of at least 75% of premiums paid). The cost of this guarantee is highly sensitive to stock market and interest rate movements. A regulatory task force has recommended a stochastic, simulation-based approach to determine the necessary financial resources.\n\n### Data / Model Specification\n\nThe regulatory framework for segregated fund guarantees is based on the output of an internal stochastic simulation model that generates a probability distribution for the future cost, `C`. The capital requirement, `K`, is determined by the following rule:\n\n  \nK = \\mathrm{CTE}(95) - L \n \n\nwhere `L` is the policy liability and `CTE(95)` is the Conditional Tail Expectation of the cost `C` at the 95th percentile, i.e., the average cost conditional on being in the worst 5% of simulated scenarios. The total financial resource requirement is `L + K = CTE(95)`.\n\nAn insurer's simulation model produces the following outputs for a block of guarantees:\n\n*   Mean of the simulated cost distribution: $50 million\n*   95th percentile of the cost distribution: $120 million\n*   Average of all costs exceeding the 95th percentile: $180 million\n\nThe insurer's accounting policy sets the policy liability `L` equal to the mean of the simulated costs.\n\n### Question\n\nBased on the provided data and model specification, which of the following statements are correct?\n", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement based on the given information.\n\nGiven information:\n- Mean of simulated cost distribution: $50 million\n- 95th percentile of cost distribution: $120 million\n- Average of all costs exceeding the 95th percentile: $180 million\n- Policy liability L = mean of simulated costs\n- K = CTE(95) - L\n- CTE(95) = average cost conditional on being in the worst 5% of scenarios\n\nLet me evaluate each option:\n\nA) The policy liability, L, for this block of business is $120 million.\n- The problem states that L equals the mean of simulated costs\n- Mean of simulated costs = $50 million\n- Therefore, L = $50 million, not $120 million\n- This is FALSE\n\nB) The total financial resource the regulator requires the firm to hold, L + K, is $180 million.\n- CTE(95) = average of all costs exceeding the 95th percentile = $180 million\n- Total financial resource = L + K = CTE(95) = $180 million\n- This is TRUE\n\nC) The required capital, K, represents the average loss in the worst 5% of scenarios.\n- K = CTE(95) - L\n- K represents the difference between the average loss in the worst 5% scenarios and the policy liability\n- K does not represent the average loss itself; CTE(95) represents that\n- This is FALSE\n\nD) The required capital, K, for this block of business is $130 million.\n- K = CTE(95) - L\n- K = $180 million - $50 million = $130 million\n- This is TRUE\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 69, "Question": "### Background\n\nIn a competitive insurance market where customer age is unobservable (`θ=0`), conventional firms cannot observe a new customer's risk type. Due to adverse selection, the pool of applicants is riskier than the general population. To compete, firms may engage in \"lowballing\"—setting an initial price `p^c` that is potentially unprofitable in the first period.\n\n### Data / Model Specification\n\nIn equilibrium, the expected lifetime profit `Π^c` for a conventional insurer from a new customer must be zero. This profit consists of two parts: the expected profit/loss in the first period, and the present value of all future expected profits, known as \"information rents.\" These rents are generated by identifying and retaining profitable customers over time.\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the economics of a conventional insurer's pricing strategy in this equilibrium? (Select all that apply)", "Options": {"A": "Firms may offer an initial price `p^c` that is below the expected first-period cost, a strategy known as 'lowballing'.", "B": "An increase in the probability `α` of learning a customer's type unambiguously increases a conventional firm's lifetime profit for any given price `p^c`.", "C": "The viability of lowballing depends on future 'information rents' earned from retaining profitable low-risk customers after their type is learned.", "D": "An increase in the firm's time discount factor `β` (i.e., becoming more patient) decreases the present value of information rents, making lowballing less attractive."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the conceptual understanding of the dynamic pricing strategy at the heart of the model. The correct options (A, B) define 'lowballing' and correctly link it to 'information rents'. Distractor C is a Conceptual Opposite regarding the effect of the discount factor `β`. Distractor D is an 'Almost Right' error; while higher `α` increases future rents, it also worsens adverse selection in the `θ=0` case, so the net effect on profit is ambiguous and is the central topic of the paper.", "qid": "69", "question": "### Background\n\nIn a competitive insurance market where customer age is unobservable (`θ=0`), conventional firms cannot observe a new customer's risk type. Due to adverse selection, the pool of applicants is riskier than the general population. To compete, firms may engage in \"lowballing\"—setting an initial price `p^c` that is potentially unprofitable in the first period.\n\n### Data / Model Specification\n\nIn equilibrium, the expected lifetime profit `Π^c` for a conventional insurer from a new customer must be zero. This profit consists of two parts: the expected profit/loss in the first period, and the present value of all future expected profits, known as \"information rents.\" These rents are generated by identifying and retaining profitable customers over time.\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the economics of a conventional insurer's pricing strategy in this equilibrium? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at each statement about conventional insurer pricing strategies in this competitive market with adverse selection:\n\nA) **Correct**. The background explicitly states that firms may engage in \"lowballing\"—setting an initial price p^c that is potentially unprofitable in the first period. This is a well-established strategy in insurance markets with adverse selection.\n\nB) **Incorrect**. In a competitive equilibrium where expected lifetime profit Π^c must be zero, an increase in α (probability of learning customer type) doesn't increase profit—it changes the pricing structure. Higher α allows more aggressive lowballing (lower p^c) because information rents are earned sooner, but competition ensures zero economic profit remains.\n\nC) **Correct**. The model specification directly states that lifetime profit consists of first-period profit/loss plus \"information rents\" from identifying and retaining profitable customers. The background confirms that lowballing (accepting first-period losses) is viable only because of these future information rents from low-risk customers.\n\nD) **Incorrect**. A higher discount factor β means the firm values future profits more (is more patient). This increases the present value of future information rents, making lowballing more attractive, not less. The statement reverses the correct relationship.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 73, "Question": "### Background\n\n**Research Question.** What are the behavioral and market-level consequences of a \"tough\" Bonus-Malus System (BMS) that imposes severe penalties for claims?\n\n**Setting / Data-Generating Environment.** The analysis focuses on the Belgian BMS, a system known for its tough penalties. The policyholder's decision of whether to report a claim is modeled as an infinite-horizon dynamic programming problem, where the goal is to minimize the present value of future costs. The solution to this problem yields an optimal retention level for each class in the system.\n\n**Variables & Parameters.**\n- `x_i^*`: The optimal retention level for a policyholder in class `i`. Claims below this value are paid out-of-pocket (currency units).\n- `C_{i.m}`: An augmented state, where `i` is the class and `m` is the number of consecutive claim-free years.\n- `λ_true`: The true underlying claim frequency of a driver (claims per year).\n- `β`: The policyholder's annual discount factor (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe calculated optimal retention levels for the Belgian BMS are presented in **Table 1**, expressed as a percentage of the average premium. A key feature is that for many classes, the retention level is extremely high. The phenomenon where policyholders are incentivized to pay for claims out-of-pocket to avoid future premium hikes is known as \"hunger for bonus.\" A special rule states that no policy can be in a class above 14 after four consecutive claim-free years.\n\n**Table 1: Optimal Retentions for Belgian BMS (Selected Classes)**\n| Class | Optimal Retention (% of avg. premium) |\n|:-----:|:-------------------------------------:|\n| 10    | 224.98%                               |\n| 11    | 239.38%                               |\n| 12    | 254.56%                               |\n| 18.0  | 288.16%                               |\n| 18.1  | 326.98%                               |\n| 18.2  | 382.01%                               |\n| 18.3  | 457.52%                               |\n\n---\n\n### Question\n\nBased on the provided information, select all statements that correctly describe the \"hunger for bonus\" phenomenon and its consequences.", "Options": {"A": "According to the model, impatient drivers (low discount factor `β`) are more likely to pay large claims out-of-pocket than patient drivers (high `β`).", "B": "A driver's optimal retention increases as they approach the 4-year claim-free threshold because the opportunity cost of reporting a claim (i.e., forfeiting an imminent large premium reduction) increases.", "C": "A rational policyholder would never pay a claim out-of-pocket if its cost exceeds their annual premium.", "D": "The \"hunger for bonus\" effect can lead an insurer to underestimate the true claim frequency (`λ_true`) because many claims go unreported."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the economic logic and second-order consequences of the 'hunger for bonus' phenomenon. It uses an Atomic Decomposition strategy to test understanding of the underlying NPV logic, the dynamics of incentives, and the resulting data censoring problem. Distractor A is a Conceptual Opposite to the core forward-looking logic of the model. Distractor C is a Reverse-Reasoning Error regarding the effect of the discount factor `β`.", "qid": "73", "question": "### Background\n\n**Research Question.** What are the behavioral and market-level consequences of a \"tough\" Bonus-Malus System (BMS) that imposes severe penalties for claims?\n\n**Setting / Data-Generating Environment.** The analysis focuses on the Belgian BMS, a system known for its tough penalties. The policyholder's decision of whether to report a claim is modeled as an infinite-horizon dynamic programming problem, where the goal is to minimize the present value of future costs. The solution to this problem yields an optimal retention level for each class in the system.\n\n**Variables & Parameters.**\n- `x_i^*`: The optimal retention level for a policyholder in class `i`. Claims below this value are paid out-of-pocket (currency units).\n- `C_{i.m}`: An augmented state, where `i` is the class and `m` is the number of consecutive claim-free years.\n- `λ_true`: The true underlying claim frequency of a driver (claims per year).\n- `β`: The policyholder's annual discount factor (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe calculated optimal retention levels for the Belgian BMS are presented in **Table 1**, expressed as a percentage of the average premium. A key feature is that for many classes, the retention level is extremely high. The phenomenon where policyholders are incentivized to pay for claims out-of-pocket to avoid future premium hikes is known as \"hunger for bonus.\" A special rule states that no policy can be in a class above 14 after four consecutive claim-free years.\n\n**Table 1: Optimal Retentions for Belgian BMS (Selected Classes)**\n| Class | Optimal Retention (% of avg. premium) |\n|:-----:|:-------------------------------------:|\n| 10    | 224.98%                               |\n| 11    | 239.38%                               |\n| 12    | 254.56%                               |\n| 18.0  | 288.16%                               |\n| 18.1  | 326.98%                               |\n| 18.2  | 382.01%                               |\n| 18.3  | 457.52%                               |\n\n---\n\n### Question\n\nBased on the provided information, select all statements that correctly describe the \"hunger for bonus\" phenomenon and its consequences.", "model_evaluation_result": {"model_raw_response": "Looking at this question about the \"hunger for bonus\" phenomenon in the Belgian Bonus-Malus System, I need to evaluate each statement based on the provided information.\n\nLet me analyze each option:\n\n**Option A**: \"Impatient drivers (low β) are more likely to pay large claims out-of-pocket than patient drivers (high β)\"\n- This is incorrect. In dynamic programming, a lower discount factor (impatient drivers) means future costs are discounted more heavily. Impatient drivers care less about future premium increases, so they would be MORE likely to report claims rather than pay out-of-pocket. Patient drivers (high β) value future savings more and would be more willing to pay out-of-pocket to avoid future premium hikes.\n\n**Option B**: \"A driver's optimal retention increases as they approach the 4-year claim-free threshold\"\n- This is correct. The table shows increasing retention levels for classes 18.0 through 18.3 (representing 0-3 consecutive claim-free years). As drivers get closer to the 4-year threshold where they can't exceed class 14, the opportunity cost of reporting a claim increases dramatically. The retention levels rise from 288.16% to 457.52% of average premium.\n\n**Option C**: \"A rational policyholder would never pay a claim out-of-pocket if its cost exceeds their annual premium\"\n- This is incorrect. The table clearly shows optimal retention levels well above 100% of average premium (up to 457.52%). This means rational policyholders would indeed pay claims exceeding their annual premium to avoid future premium increases.\n\n**Option D**: \"The 'hunger for bonus' effect can lead an insurer to underestimate the true claim frequency\"\n- This is correct. When policyholders strategically choose not to report claims below their retention threshold, the insurer only observes reported claims. This systematic underreporting means the observed claim frequency will be lower than the true underlying claim frequency (λ_true).\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 166, "Question": "### Background\n\nA study analyzed the performance and strategic outcomes of *de novo* banks three years after their entry into a market. The analysis used a multivariate regression framework where the key explanatory variables were indicators for different founding motivations reported in a survey.\n\n### Data / Model Specification\n\nThe following table presents selected results from bank-level regressions where the dependent variables are measures of performance or strategy in Year +3 post-entry.\n\n**Table 1: Selected Bank-Level Regression Results (Year +3)**\n\n| Dependent Variable | `RESP1` (Underserved Small Business Market) | `RESP2` (Provide Competition) | `RESP5` (M&A Eliminated Community Banks) |\n| :--- | :--- | :--- | :--- |\n| ROA (%) | -0.0113 | 0.0179 | **-0.2970** (p<0.10) |\n| Profit efficiency | -0.0016 | **0.0158** (p<0.05) | -0.0073 |\n| Amount of Small Bus. loans-to-assets | **0.0645** (p<0.01) | 0.0025 | -0.0114 |\n\n*Note: Table shows estimated coefficients. Significance is based on p-values reported in the original paper.* \n\n---\n\nBased on the statistically significant results in **Table 1**, which of the following conclusions about the strategic paths and performance of *de novo* banks are supported by the evidence? Select all that apply.", "Options": {"A": "Banks founded with the stated goal of serving an underserved small-business market (`RESP1`) successfully execute this strategy, dedicating a significantly larger portion of their assets to small-business loans compared to other *de novo* banks.", "B": "Banks founded with the intent to provide competition (`RESP2`) achieve superior operational excellence, as evidenced by their significantly higher profit efficiency three years after entry.", "C": "Entering a market where M&A activity has eliminated community banks (`RESP5`) is associated with significantly lower bank profitability (ROA) three years post-entry.", "D": "Banks motivated by providing competition (`RESP2`) also demonstrate a significantly higher commitment to small-business lending, as shown by the `Amount of Small Bus. loans-to-assets`."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the candidate's ability to correctly interpret multiple, statistically significant regression coefficients from a table and synthesize them into valid conclusions about distinct business strategies. The depth strategy is 'Computational Judgment,' requiring the user to filter for significance and then translate the numerical results into conceptual statements. Option A is correct based on the significant positive coefficient for RESP1 on small business loans. Option B is correct based on the significant positive coefficient for RESP2 on profit efficiency. Option C is correct based on the significant negative coefficient for RESP5 on ROA. Option D is a 'True but Irrelevant' distractor, as the coefficient for RESP2 on small business loans is positive but statistically insignificant, making the conclusion invalid.", "qid": "166", "question": "### Background\n\nA study analyzed the performance and strategic outcomes of *de novo* banks three years after their entry into a market. The analysis used a multivariate regression framework where the key explanatory variables were indicators for different founding motivations reported in a survey.\n\n### Data / Model Specification\n\nThe following table presents selected results from bank-level regressions where the dependent variables are measures of performance or strategy in Year +3 post-entry.\n\n**Table 1: Selected Bank-Level Regression Results (Year +3)**\n\n| Dependent Variable | `RESP1` (Underserved Small Business Market) | `RESP2` (Provide Competition) | `RESP5` (M&A Eliminated Community Banks) |\n| :--- | :--- | :--- | :--- |\n| ROA (%) | -0.0113 | 0.0179 | **-0.2970** (p<0.10) |\n| Profit efficiency | -0.0016 | **0.0158** (p<0.05) | -0.0073 |\n| Amount of Small Bus. loans-to-assets | **0.0645** (p<0.01) | 0.0025 | -0.0114 |\n\n*Note: Table shows estimated coefficients. Significance is based on p-values reported in the original paper.* \n\n---\n\nBased on the statistically significant results in **Table 1**, which of the following conclusions about the strategic paths and performance of *de novo* banks are supported by the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the statistically significant results in Table 1:\n\nFor RESP1 (Underserved Small Business Market):\n- Amount of Small Bus. loans-to-assets: **0.0645** (p<0.01) - highly significant positive coefficient\n\nFor RESP2 (Provide Competition):\n- Profit efficiency: **0.0158** (p<0.05) - significant positive coefficient\n\nFor RESP5 (M&A Eliminated Community Banks):\n- ROA (%): **-0.2970** (p<0.10) - significant negative coefficient\n\nNow evaluating each option:\n\nA) TRUE - Banks with RESP1 motivation show a significant positive coefficient (0.0645, p<0.01) for small business loans-to-assets, supporting that they dedicate more assets to small business lending.\n\nB) TRUE - Banks with RESP2 motivation show a significant positive coefficient (0.0158, p<0.05) for profit efficiency, indicating superior operational efficiency.\n\nC) TRUE - Banks entering markets where M&A eliminated community banks (RESP5) show a significant negative coefficient (-0.2970, p<0.10) for ROA, indicating lower profitability.\n\nD) FALSE - The coefficient for RESP2 on small business loans-to-assets is 0.0025, which is not marked as statistically significant.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 78, "Question": "### Background\n\n**Research Question.** The simple correlations presented in the study are suggestive but vulnerable to omitted variable bias and endogeneity, making it difficult to establish a causal link between monitoring pressure and shorter holding periods. A more robust empirical test is needed to provide more convincing evidence.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, with a focus on the subsample of pension fund managers for correlational analysis.\n\n---\n\n### Data / Model Specification\n\nThe following table presents simple correlations for the pension fund manager subsample.\n\n**Table 1. Correlation of estimates of average holding periods (in months) for pension fund managers**\n\n| Correlation between average holding period (in months) and: | Correlation Coefficient | Significance |\n| :--- | :---: | :---: |\n| frequency of performance monitoring | -0.1184 | 0.217 |\n| proportion of pay composed of bonus | -0.1760 | 0.121 |\n\n---\n\n### Question\n\nGiven the limitations of the simple correlational evidence in Table 1, which of the following features would be essential components of a more robust empirical design to test for a causal link between monitoring and short-termism?", "Options": {"A": "Using panel data that tracks individual managers over several years to see if changes in monitoring intensity for a given manager lead to changes in their holding period.", "B": "Limiting the sample to only the top-performing decile of managers to reduce noise from underperformers.", "C": "Replacing self-reported holding periods with official press commentary on short-termism to get a more objective measure.", "D": "Incorporating manager fixed effects to control for unobserved, time-invariant manager characteristics like skill or risk preference."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to move from critique to constructive research design, a key higher-order skill. It uses a 'Premise/Assumption Packaging' strategy, asking the user to select the necessary components of a superior econometric model. Correct options A and B identify the two core features of a panel fixed-effects model that would address the endogeneity and omitted variable bias issues inherent in the original study's cross-sectional correlations. Distractor C introduces a methodologically flawed idea (selection bias), while Distractor D suggests a 'True but Irrelevant' data source that does not solve the core causal inference problem.", "qid": "78", "question": "### Background\n\n**Research Question.** The simple correlations presented in the study are suggestive but vulnerable to omitted variable bias and endogeneity, making it difficult to establish a causal link between monitoring pressure and shorter holding periods. A more robust empirical test is needed to provide more convincing evidence.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, with a focus on the subsample of pension fund managers for correlational analysis.\n\n---\n\n### Data / Model Specification\n\nThe following table presents simple correlations for the pension fund manager subsample.\n\n**Table 1. Correlation of estimates of average holding periods (in months) for pension fund managers**\n\n| Correlation between average holding period (in months) and: | Correlation Coefficient | Significance |\n| :--- | :---: | :---: |\n| frequency of performance monitoring | -0.1184 | 0.217 |\n| proportion of pay composed of bonus | -0.1760 | 0.121 |\n\n---\n\n### Question\n\nGiven the limitations of the simple correlational evidence in Table 1, which of the following features would be essential components of a more robust empirical design to test for a causal link between monitoring and short-termism?", "model_evaluation_result": {"model_raw_response": "I need to analyze which features would be essential for a more robust empirical design to establish causality between monitoring and short-termism, given the limitations of simple correlations.\n\nThe current evidence shows weak, non-significant correlations that suffer from:\n- Omitted variable bias\n- Endogeneity concerns\n- Cross-sectional nature (single time period)\n\nLet me evaluate each option:\n\nA) Using panel data that tracks individual managers over several years to see if changes in monitoring intensity for a given manager lead to changes in their holding period.\n- This would allow for within-manager variation over time\n- Helps address endogeneity by looking at changes rather than levels\n- Enables difference-in-differences or fixed effects approaches\n- ESSENTIAL for causal inference\n\nB) Limiting the sample to only the top-performing decile of managers to reduce noise from underperformers.\n- This would create severe selection bias\n- Would reduce external validity\n- Doesn't address the fundamental identification problems\n- NOT essential, actually harmful for causal inference\n\nC) Replacing self-reported holding periods with official press commentary on short-termism to get a more objective measure.\n- Press commentary would be subjective and noisy\n- Would introduce measurement error\n- Less precise than actual holding period data\n- NOT essential, would likely worsen the analysis\n\nD) Incorporating manager fixed effects to control for unobserved, time-invariant manager characteristics like skill or risk preference.\n- Controls for time-invariant unobservables at the manager level\n- Addresses important source of omitted variable bias\n- Standard approach in panel data analysis\n- ESSENTIAL for causal inference\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 90, "Question": "### Background\n\n**Research Question.** A central challenge in corporate finance is to devise empirical tests of strategic risk-shifting that are robust to mechanical artifacts like mean reversion. This paper proposes two novel methodologies based on portfolio holdings to overcome a 'sorting bias' found in traditional return-based tests of mutual fund tournament behavior.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual fund holdings data from 1990-2006. The goal is to test whether underperforming managers ('losers') in the first half of the year actively change their portfolio risk in the second half. The paper also re-examines whether this behavior is linked to overall market conditions.\n\n### Data / Model Specification\n\n**Application: Re-evaluating the Link to Market Conditions**\nPrior research found that tournament behavior depends on first-half market returns. The paper tests whether this finding holds with the new, bias-free measures, and compares it to the behavior of the `Before Ratio`, a proxy for the sorting bias.\n\n**Table 1. Tournament Behavior and Market Conditions**\n\n| Variable | ρ with Return | -Market Mean | +Market Mean | +/- p-value |\n|:---|:---:|:---:|:---:|:---:|\n| Before Ratio | 0.45 | 0.832 | 1.172 | 0.01** |\n| Share Change (Low Dummy) | 0.04 | 1.055 | 0.519 | 0.55 |\n| Portfolio (Low Dummy) | 0.26 | -0.003 | 0.079 | 0.16 |\n\n*Note: `ρ with Return` is the correlation with first-half market return. `-Market Mean` and `+Market Mean` are average values in negative and positive market years. `+/- p-value` tests the difference in means. **Significant at 1% level.*\n\n### Question\n\nThe paper's central argument is that prior findings linking tournament behavior to market conditions are an artifact of a 'sorting bias'. Based on the evidence in Table 1, select all statements that correctly form part of this argument.", "Options": {"A": "The paper's new methodologies confirm the prior literature's finding that tournament behavior is significantly stronger in years with positive market returns.", "B": "The `Before Ratio`, a proxy for the sorting bias, is systematically related to market conditions, as its mean value is significantly different in positive-return versus negative-return years (p-value = 0.01).", "C": "The `Share Change` measure of tournament behavior has a positive correlation (0.04) with market returns, providing weak but confirming evidence that risk-taking by losers increases in stronger markets.", "D": "The new, bias-corrected measure of tournament behavior derived from the portfolio bootstrap method (`Portfolio (Low Dummy)`) shows no statistically significant relationship with market conditions (p-value = 0.16 for the difference in means)."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the student's ability to synthesize evidence to understand the paper's main critique of the prior literature. It uses a **Premise Packaging** strategy, asking the student to select the key premises of the argument against the market-conditions hypothesis. Distractors are designed to test for deep understanding: one is a **Conceptual Opposite** (falsely claiming the new methods confirm the old results), and the other is **True but Irrelevant** (citing a statistically insignificant correlation as meaningful evidence).", "qid": "90", "question": "### Background\n\n**Research Question.** A central challenge in corporate finance is to devise empirical tests of strategic risk-shifting that are robust to mechanical artifacts like mean reversion. This paper proposes two novel methodologies based on portfolio holdings to overcome a 'sorting bias' found in traditional return-based tests of mutual fund tournament behavior.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual fund holdings data from 1990-2006. The goal is to test whether underperforming managers ('losers') in the first half of the year actively change their portfolio risk in the second half. The paper also re-examines whether this behavior is linked to overall market conditions.\n\n### Data / Model Specification\n\n**Application: Re-evaluating the Link to Market Conditions**\nPrior research found that tournament behavior depends on first-half market returns. The paper tests whether this finding holds with the new, bias-free measures, and compares it to the behavior of the `Before Ratio`, a proxy for the sorting bias.\n\n**Table 1. Tournament Behavior and Market Conditions**\n\n| Variable | ρ with Return | -Market Mean | +Market Mean | +/- p-value |\n|:---|:---:|:---:|:---:|:---:|\n| Before Ratio | 0.45 | 0.832 | 1.172 | 0.01** |\n| Share Change (Low Dummy) | 0.04 | 1.055 | 0.519 | 0.55 |\n| Portfolio (Low Dummy) | 0.26 | -0.003 | 0.079 | 0.16 |\n\n*Note: `ρ with Return` is the correlation with first-half market return. `-Market Mean` and `+Market Mean` are average values in negative and positive market years. `+/- p-value` tests the difference in means. **Significant at 1% level.*\n\n### Question\n\nThe paper's central argument is that prior findings linking tournament behavior to market conditions are an artifact of a 'sorting bias'. Based on the evidence in Table 1, select all statements that correctly form part of this argument.", "model_evaluation_result": {"model_raw_response": "Looking at the table and the paper's argument about sorting bias:\n\nThe paper argues that prior findings linking tournament behavior to market conditions are artifacts of a \"sorting bias.\" Let me analyze each option:\n\nA) This is incorrect. The new methodologies actually contradict the prior literature - they show no significant relationship between tournament behavior and market conditions (p-values of 0.55 and 0.16 for the new measures).\n\nB) This is correct. The Before Ratio (proxy for sorting bias) shows a highly significant difference between positive and negative market years (p-value = 0.01), with mean values of 0.832 vs 1.172. This demonstrates the bias is systematically related to market conditions.\n\nC) This is incorrect. While the correlation is positive (0.04), it's extremely weak and the difference in means test shows no significance (p-value = 0.55). This doesn't provide confirming evidence.\n\nD) This is correct. The Portfolio (Low Dummy) measure shows no statistically significant relationship with market conditions, with a p-value of 0.16 for the difference in means test.\n\nThe paper's argument is that the sorting bias (Before Ratio) is driving the apparent relationship with market conditions, while the bias-corrected measures show no such relationship.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 81, "Question": "### Background\n\n**Research Question.** This case investigates the core of the principal-agent problem in delegated asset management: how the formal evaluation and incentive structure shapes a fund manager's objectives and behavior. It examines how the choice of benchmark dictates what managers value (relative vs. absolute return) and what motivates them (career risk vs. direct financial reward).\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, categorized by the primary type of fund they manage and their performance benchmark.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the survey findings on benchmark incidence, the perceived importance of absolute returns, and the key motivating factors for managers.\n\n**Table 1. The incidence of particular benchmarks, according to type of fund**\n\n| Performance Benchmark | Pension Fund (n=41) (%) | Private Client (n=12) (%) |\n| :--- | :---: | :---: |\n| Median fund | 71 | 20 |\n| All-share index | 42 | 70 |\n\n**Table 2. Importance of absolute performance according to type of fund**\n\n| | Pension Fund (Mean value*) | Private Client (Mean value*) | Significance |\n| :--- | :---: | :---: | :---: |\n| **Score** | 2.3 | 3.9 | 0.06 |\n\n*`1 = no importance`, `5 = extremely important`*\n\n**Table 3. Importance of motivating factors according to performance benchmark**\n\n| Motivator | Median Fund (Mean value*) | All-Share (Mean value*) | Significance |\n| :--- | :---: | :---: | :---: |\n| **Trustee approval** | **4.3** | **3.8** | **0.12** |\n| Salary | 3.0 | 3.4 | 0.44 |\n\n*`1 = no importance`, `5 = extremely important`*\n\n---\n\n### Question\n\nThe survey provides a detailed view of the agency problem in fund management. Based on the evidence presented across all three tables, select all statements that accurately describe the incentive structure and resulting behavior of UK pension fund managers.", "Options": {"A": "Private client fund managers face the most intense pressure, as they are judged against both the median fund and the all-share index simultaneously.", "B": "Pension fund managers are primarily judged against a relative benchmark (median fund), a framework that rationally leads them to de-emphasize the importance of absolute investment returns.", "C": "The data show a strong, statistically significant link between higher salary and a manager's focus on trustee approval.", "D": "For managers subject to relative benchmarking, non-financial motivators like 'Trustee approval'—a proxy for job security—are more influential than direct pay, highlighting the dominance of career risk."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an 'Atomic Decomposition' strategy to test the synthesis of the paper's entire argument about the agency problem. It requires connecting evidence across three tables to form a coherent narrative. Correct option A links the benchmark type (Table 1) to the manager's objective function (Table 2). Correct option B identifies the true driver of behavior (Table 3) and interprets it in the context of agency theory. Distractor C mischaracterizes the pressure on a different group, testing for careful reading of Table 1. Distractor D posits a relationship between variables that is not supported by the significance levels in Table 3.", "qid": "81", "question": "### Background\n\n**Research Question.** This case investigates the core of the principal-agent problem in delegated asset management: how the formal evaluation and incentive structure shapes a fund manager's objectives and behavior. It examines how the choice of benchmark dictates what managers value (relative vs. absolute return) and what motivates them (career risk vs. direct financial reward).\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, categorized by the primary type of fund they manage and their performance benchmark.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the survey findings on benchmark incidence, the perceived importance of absolute returns, and the key motivating factors for managers.\n\n**Table 1. The incidence of particular benchmarks, according to type of fund**\n\n| Performance Benchmark | Pension Fund (n=41) (%) | Private Client (n=12) (%) |\n| :--- | :---: | :---: |\n| Median fund | 71 | 20 |\n| All-share index | 42 | 70 |\n\n**Table 2. Importance of absolute performance according to type of fund**\n\n| | Pension Fund (Mean value*) | Private Client (Mean value*) | Significance |\n| :--- | :---: | :---: | :---: |\n| **Score** | 2.3 | 3.9 | 0.06 |\n\n*`1 = no importance`, `5 = extremely important`*\n\n**Table 3. Importance of motivating factors according to performance benchmark**\n\n| Motivator | Median Fund (Mean value*) | All-Share (Mean value*) | Significance |\n| :--- | :---: | :---: | :---: |\n| **Trustee approval** | **4.3** | **3.8** | **0.12** |\n| Salary | 3.0 | 3.4 | 0.44 |\n\n*`1 = no importance`, `5 = extremely important`*\n\n---\n\n### Question\n\nThe survey provides a detailed view of the agency problem in fund management. Based on the evidence presented across all three tables, select all statements that accurately describe the incentive structure and resulting behavior of UK pension fund managers.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze each statement against the data provided in the three tables.\n\n**Statement A**: \"Private client fund managers face the most intense pressure, as they are judged against both the median fund and the all-share index simultaneously.\"\n\nFrom Table 1, private client managers use:\n- Median fund: 20%\n- All-share index: 70%\n\nThese percentages don't sum to 100%, and there's no indication that managers face both benchmarks simultaneously. The data suggests they use one or the other, not both. This statement is **incorrect**.\n\n**Statement B**: \"Pension fund managers are primarily judged against a relative benchmark (median fund), a framework that rationally leads them to de-emphasize the importance of absolute investment returns.\"\n\nFrom Table 1: 71% of pension fund managers use the median fund benchmark (a relative benchmark).\nFrom Table 2: Pension fund managers rate absolute performance importance at 2.3 (on a 1-5 scale), which is relatively low.\n\nThis shows pension fund managers do primarily use relative benchmarks and do de-emphasize absolute returns. This statement is **correct**.\n\n**Statement C**: \"The data show a strong, statistically significant link between higher salary and a manager's focus on trustee approval.\"\n\nTable 3 shows importance ratings for different motivators by benchmark type, not any correlation between salary levels and trustee approval focus. The data doesn't provide information about this relationship. This statement is **incorrect**.\n\n**Statement D**: \"For managers subject to relative benchmarking, non-financial motivators like 'Trustee approval'—a proxy for job security—are more influential than direct pay, highlighting the dominance of career risk.\"\n\nFrom Table 3, for median fund benchmark (relative benchmarking):\n- Trustee approval: 4.3\n- Salary: 3.0\n\nTrustee approval is rated substantially higher (4.3 vs 3.0), indicating non-financial motivators related to job security are indeed more influential than direct pay for those using relative benchmarks. This statement is **correct**.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 26, "Question": "**Research Question.** Does the idiosyncratic component of stock order flow, orthogonal to major macroeconomic shocks, have predictive power for future market returns?\n\n**Setting / Data-Generating Environment.** A structural VAR model isolates an idiosyncratic shock to the buy/sell ratio (`ε_4t`), which is found to negatively predict future real stock returns.\n\n**Data / Model Specification**\nConsider a simple two-period market model to formalize the price pressure hypothesis:\n- The log price at t=0 is `p_0`, which is also the true fundamental value, `v`.\n- At t=1, an uninformed order flow shock `u` occurs, where `u > 0` represents net buying pressure. The paper's idiosyncratic shock `ε_{4t}` is a proxy for `u`.\n- The price at t=1 is determined by `p_1 = p_0 + λu`, where `λ > 0` is the price impact coefficient (inverse of market depth).\n- At t=2, the fundamental value `v` is revealed, and the price becomes `p_2 = v`.\n\nBased on the model above, select all of the following statements that are correct.", "Options": {"A": "If two markets have price impact coefficients `λ_A = 0.1` and `λ_B = 0.2`, Market B will exhibit a more negative expected return for the same order flow shock `u`.", "B": "The total return from t=0 to t=2 is `λu`.", "C": "The expected log return from t=1 to t=2, conditional on a positive shock `u > 0`, is negative.", "D": "The price at t=1, `p_1`, is an unbiased estimate of the fundamental value `v`."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: Tests the ability to formalize and interpret a simple market microstructure model of price pressure. Strategy: Computational Judgment, requiring the student to derive the expected return (`E[Return] = -λu`) and interpret its components. Distractor Logic: (C) tests for a Sign Error and misunderstanding of bias. (D) is a Step-Omission Error, confusing the temporary price impact with the permanent total return.", "qid": "26", "question": "**Research Question.** Does the idiosyncratic component of stock order flow, orthogonal to major macroeconomic shocks, have predictive power for future market returns?\n\n**Setting / Data-Generating Environment.** A structural VAR model isolates an idiosyncratic shock to the buy/sell ratio (`ε_4t`), which is found to negatively predict future real stock returns.\n\n**Data / Model Specification**\nConsider a simple two-period market model to formalize the price pressure hypothesis:\n- The log price at t=0 is `p_0`, which is also the true fundamental value, `v`.\n- At t=1, an uninformed order flow shock `u` occurs, where `u > 0` represents net buying pressure. The paper's idiosyncratic shock `ε_{4t}` is a proxy for `u`.\n- The price at t=1 is determined by `p_1 = p_0 + λu`, where `λ > 0` is the price impact coefficient (inverse of market depth).\n- At t=2, the fundamental value `v` is revealed, and the price becomes `p_2 = v`.\n\nBased on the model above, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at this two-period market model, I need to analyze each statement carefully.\n\nGiven information:\n- t=0: p₀ = v (log price equals fundamental value)\n- t=1: p₁ = p₀ + λu (price affected by order flow shock)\n- t=2: p₂ = v (price reverts to fundamental value)\n- λ > 0 (price impact coefficient)\n- u > 0 represents net buying pressure\n\nLet me evaluate each option:\n\n**Option A**: \"If two markets have price impact coefficients λ_A = 0.1 and λ_B = 0.2, Market B will exhibit a more negative expected return for the same order flow shock u.\"\n\nFor a positive shock u > 0:\n- Market A: Return from t=1 to t=2 = p₂ - p₁ = v - (v + 0.1u) = -0.1u\n- Market B: Return from t=1 to t=2 = v - (v + 0.2u) = -0.2u\n\nSince -0.2u < -0.1u when u > 0, Market B has a more negative return. ✓\n\n**Option B**: \"The total return from t=0 to t=2 is λu.\"\n\nTotal return = p₂ - p₀ = v - v = 0\nThis is not λu. ✗\n\n**Option C**: \"The expected log return from t=1 to t=2, conditional on a positive shock u > 0, is negative.\"\n\nLog return from t=1 to t=2 = p₂ - p₁ = v - (v + λu) = -λu\nSince λ > 0 and u > 0, we have -λu < 0. ✓\n\n**Option D**: \"The price at t=1, p₁, is an unbiased estimate of the fundamental value v.\"\n\nE[p₁] = E[v + λu] = v + λE[u]\nThis equals v only if E[u] = 0. Since the problem doesn't specify that order flow shocks", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 93, "Question": "### Background\n\n**Research Question.** What are the long-run macroeconomic determinants of venture capital (VC) prices?\n\n**Setting.** A Johansen cointegration analysis is used to estimate the long-run equilibrium relationship between the US VC price index and a set of key macroeconomic and financial variables.\n\n---\n\n### Data / Model Specification\n\nThe estimated long-run equilibrium equation is:\n\n  \nP_{VC,t} = -29.67 + 0.70 NASDAQ_t - 2.62 TBILL_t + 3.68 GBY_t + 0.29 CPI_t + 7.22 IP_t \\quad \\text{(Eq. (1))}\n \n\nWhere `P_VC` is the VC price index, `NASDAQ` is the Nasdaq index, `TBILL` is the short-term interest rate, `GBY` is the long-term interest rate, `CPI` is the consumer price index, and `IP` is industrial production.\n\n---\n\nAccording to the estimated long-run relationship in Eq. (1) and the economic arguments presented in the paper, which of the following statements are accurate interpretations of the model's findings? Select all that apply.", "Options": {"A": "The model suggests that the Nasdaq has a smaller positive long-run impact on VC prices than Industrial Production.", "B": "An increase in the short-term T-Bill rate is associated with higher long-run VC prices, as entrepreneurs switch from bank financing to VC.", "C": "A sustained increase in the Nasdaq is associated with higher long-run VC prices, consistent with the hypothesis that a strong exit channel boosts VC valuations.", "D": "An increase in the long-term Government Bond Yield is associated with higher long-run VC prices, suggesting the demand-side effect (entrepreneurs seeking VC over debt) outweighs the supply-side effect (investors switching to bonds)."}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: This question assesses the ability to correctly interpret the signs and economic meaning of coefficients in a multivariate regression model, linking them back to the paper's theoretical framework. Strategy: Scenario Application (applying the model to interpret relationships). Distractor Logic: (A) is a correct interpretation of the positive NASDAQ coefficient. (B) is incorrect as it contradicts the negative sign on TBILL. (C) is a correct interpretation of the positive GBY coefficient, reflecting the paper's reasoning. (D) is a classic 'True but Irrelevant/Misleading' distractor; it correctly compares the numerical values of the unstandardized coefficients (0.70 < 7.22) but represents a flawed interpretation, as one cannot compare the magnitudes of coefficients for variables with different scales and units.", "qid": "93", "question": "### Background\n\n**Research Question.** What are the long-run macroeconomic determinants of venture capital (VC) prices?\n\n**Setting.** A Johansen cointegration analysis is used to estimate the long-run equilibrium relationship between the US VC price index and a set of key macroeconomic and financial variables.\n\n---\n\n### Data / Model Specification\n\nThe estimated long-run equilibrium equation is:\n\n  \nP_{VC,t} = -29.67 + 0.70 NASDAQ_t - 2.62 TBILL_t + 3.68 GBY_t + 0.29 CPI_t + 7.22 IP_t \\quad \\text{(Eq. (1))}\n \n\nWhere `P_VC` is the VC price index, `NASDAQ` is the Nasdaq index, `TBILL` is the short-term interest rate, `GBY` is the long-term interest rate, `CPI` is the consumer price index, and `IP` is industrial production.\n\n---\n\nAccording to the estimated long-run relationship in Eq. (1) and the economic arguments presented in the paper, which of the following statements are accurate interpretations of the model's findings? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the equation coefficients:\n\nP_{VC,t} = -29.67 + 0.70 NASDAQ_t - 2.62 TBILL_t + 3.68 GBY_t + 0.29 CPI_t + 7.22 IP_t\n\nLet me evaluate each option:\n\nA) The model suggests that the Nasdaq has a smaller positive long-run impact on VC prices than Industrial Production.\n- NASDAQ coefficient: 0.70\n- Industrial Production coefficient: 7.22\n- Since 0.70 < 7.22, this statement is TRUE\n\nB) An increase in the short-term T-Bill rate is associated with higher long-run VC prices, as entrepreneurs switch from bank financing to VC.\n- TBILL coefficient: -2.62 (negative)\n- This means higher T-Bill rates are associated with LOWER VC prices, not higher\n- This statement is FALSE\n\nC) A sustained increase in the Nasdaq is associated with higher long-run VC prices, consistent with the hypothesis that a strong exit channel boosts VC valuations.\n- NASDAQ coefficient: 0.70 (positive)\n- This positive relationship supports the statement\n- This statement is TRUE\n\nD) An increase in the long-term Government Bond Yield is associated with higher long-run VC prices, suggesting the demand-side effect (entrepreneurs seeking VC over debt) outweighs the supply-side effect (investors switching to bonds).\n- GBY coefficient: 3.68 (positive)\n- This positive relationship supports the statement\n- This statement is TRUE\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 183, "Question": "### Background\n\n**Research Question.** Do certain equity styles share common long-run stochastic trends (i.e., are they cointegrated)?\n\n**Setting and Data.** The paper finds that the log prices of Large Value (WLV) and Small Growth (WSG) styles, denoted `p_WLV,t` and `p_WSG,t`, are cointegrated.\n\n---\n\n### Data / Model Specification\n\nThe cointegration between WLV and WSG implies their log prices follow a Vector Error Correction Model (VECM). In a VECM, the change in each variable's price is partly a reaction to the previous period's deviation from their long-run equilibrium. This deviation is called the error-correction term, `z_{t-1}`.\n\nLet the long-run equilibrium relationship be `p_{WLV,t} = β p_{WSG,t}`. The error-correction term is then `z_{t-1} = p_{WLV,t-1} - β p_{WSG,t-1}`.\n\nThe VECM for the price changes can be written as:\n  \n\\Delta p_{WLV,t} = \\alpha_1 z_{t-1} + \\text{short-run dynamics} + \\epsilon_{1,t}\n \n  \n\\Delta p_{WSG,t} = \\alpha_2 z_{t-1} + \\text{short-run dynamics} + \\epsilon_{2,t}\n \nThe coefficients `α_1` and `α_2` are the 'speed-of-adjustment' parameters.\n\n---\n\n### Question\n\nSelect all statements that correctly interpret the economic meaning of the speed-of-adjustment coefficients (`α_1` for WLV, `α_2` for WSG) in the VECM.", "Options": {"A": "For the system to revert to its long-run mean, the speed-of-adjustment coefficients `α_1` and `α_2` must both be negative.", "B": "If the WLV price was too high relative to equilibrium (`z_{t-1} > 0`), the adjustment coefficient `α_2` for the WSG price must be positive to induce a positive price change `Δp_{WSG,t}`, helping to close the equilibrium gap from the other side.", "C": "For the system to be stable, if the WLV price was too high relative to its equilibrium level in the previous period (i.e., `z_{t-1} > 0`), its adjustment coefficient `α_1` must be negative to induce a negative price change `Δp_{WLV,t}`.", "D": "A larger absolute value of `α_1` implies that the Large Value style is less responsive to deviations from the long-run equilibrium."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item tests a deep understanding of time-series econometrics (VECM), focusing on the economic interpretation of model parameters. It uses a Scenario Application strategy, asking how the system behaves given a specific deviation from equilibrium. Correct options A and B correctly describe the mechanics of error correction for a two-variable system. Distractor C is a Sign Error, as the coefficients must have opposite signs for this error term specification. Distractor D is a Conceptual Opposite, as a larger coefficient means a *faster* adjustment.", "qid": "183", "question": "### Background\n\n**Research Question.** Do certain equity styles share common long-run stochastic trends (i.e., are they cointegrated)?\n\n**Setting and Data.** The paper finds that the log prices of Large Value (WLV) and Small Growth (WSG) styles, denoted `p_WLV,t` and `p_WSG,t`, are cointegrated.\n\n---\n\n### Data / Model Specification\n\nThe cointegration between WLV and WSG implies their log prices follow a Vector Error Correction Model (VECM). In a VECM, the change in each variable's price is partly a reaction to the previous period's deviation from their long-run equilibrium. This deviation is called the error-correction term, `z_{t-1}`.\n\nLet the long-run equilibrium relationship be `p_{WLV,t} = β p_{WSG,t}`. The error-correction term is then `z_{t-1} = p_{WLV,t-1} - β p_{WSG,t-1}`.\n\nThe VECM for the price changes can be written as:\n  \n\\Delta p_{WLV,t} = \\alpha_1 z_{t-1} + \\text{short-run dynamics} + \\epsilon_{1,t}\n \n  \n\\Delta p_{WSG,t} = \\alpha_2 z_{t-1} + \\text{short-run dynamics} + \\epsilon_{2,t}\n \nThe coefficients `α_1` and `α_2` are the 'speed-of-adjustment' parameters.\n\n---\n\n### Question\n\nSelect all statements that correctly interpret the economic meaning of the speed-of-adjustment coefficients (`α_1` for WLV, `α_2` for WSG) in the VECM.", "model_evaluation_result": {"model_raw_response": "Looking at this VECM question, I need to analyze each statement about the speed-of-adjustment coefficients α₁ and α₂.\n\nGiven:\n- Error correction term: z_{t-1} = p_{WLV,t-1} - β p_{WSG,t-1}\n- When z_{t-1} > 0: WLV is above equilibrium relative to WSG\n- When z_{t-1} < 0: WLV is below equilibrium relative to WSG\n\nLet me evaluate each option:\n\n**Option A**: \"For the system to revert to its long-run mean, the speed-of-adjustment coefficients α₁ and α₂ must both be negative.\"\n\nThis is incorrect. For stability, we need at least one coefficient to be negative and one to be positive (or one could be zero). If both were negative, when z_{t-1} > 0, both prices would decrease, which wouldn't necessarily restore equilibrium.\n\n**Option B**: \"If the WLV price was too high relative to equilibrium (z_{t-1} > 0), the adjustment coefficient α₂ for the WSG price must be positive to induce a positive price change Δp_{WSG,t}, helping to close the equilibrium gap from the other side.\"\n\nThis is correct. When WLV is too high relative to WSG, one way to restore equilibrium is for WSG to increase. With α₂ > 0 and z_{t-1} > 0, we get Δp_{WSG,t} > 0, which helps close the gap.\n\n**Option C**: \"For the system to be stable, if the WLV price was too high relative to its equilibrium level in the previous period (i.e., z_{t-1} > 0), its adjustment coefficient α₁ must be negative to induce a negative price change Δp_{WLV,t}.\"\n\nThis is correct. When WLV is too high, it should decrease to restore equilibrium. With α₁ < 0 and z_{t-1} > 0, we get Δp_{WLV,t} < 0, which brings WLV back down toward equilibrium.\n\n**", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 107, "Question": "### Background\n\nA study investigates the debt-substitution effect, which posits a negative relationship between corporate tax aggressiveness and corporate debt. The analysis uses a fixed-effects panel regression on 6,967 firm-year observations from 2001-2010.\n\n### Data / Model Specification\n\n**Table 1: Descriptive Statistics**\n| Variable | Mean | Std. Dev. |\n| :--- | :---: | :---: |\n| `BDEBT` | 0.326 | 0.895 |\n| `BTG1` | 0.097 | 0.739 |\n\n**Table 2: Regression Results for `BDEBT`**\n| Dependent Variable | Variable | Coefficient | t-statistic |\n| :--- | :--- | :---: | :---: |\n| `BDEBT` (Model FEM1) | `BTG1` | -0.471 | (-2.03) |\n\n---\n\nBased on the provided data, select all of the following statements that are valid.\n", "Options": {"A": "A one-unit increase in `BTG1` is associated with a 0.471% decrease in the `BDEBT` ratio.", "B": "A one standard deviation increase in tax aggressiveness (`BTG1`) is associated with a decrease of approximately 0.348 in the `BDEBT` ratio.", "C": "The coefficient on `BTG1` is statistically significant at the 5% level, supporting the debt-substitution hypothesis.", "D": "The economic magnitude of the debt-substitution effect is substantial, as a one standard deviation increase in `BTG1` is associated with a change in `BDEBT` that is larger than the mean `BDEBT`."}, "Answer": ["B", "C", "D"], "pi_justification": "This item assesses the ability to synthesize information from descriptive and regression tables to evaluate statistical and economic significance. It uses a Computational Judgment strategy. Option A is correct because the t-statistic |-2.03| > 1.96. Option B requires a calculation: -0.471 * 0.739 = -0.348069. Option C requires a second-level judgment based on the result from B, comparing the magnitude of the change (0.348) to the mean `BDEBT` (0.326), confirming the effect is economically large. Option D is a Unit/Scale Error distractor; the coefficient implies a 0.471 *unit* (or 47.1 percentage point) decrease, not a 0.471% decrease.", "qid": "107", "question": "### Background\n\nA study investigates the debt-substitution effect, which posits a negative relationship between corporate tax aggressiveness and corporate debt. The analysis uses a fixed-effects panel regression on 6,967 firm-year observations from 2001-2010.\n\n### Data / Model Specification\n\n**Table 1: Descriptive Statistics**\n| Variable | Mean | Std. Dev. |\n| :--- | :---: | :---: |\n| `BDEBT` | 0.326 | 0.895 |\n| `BTG1` | 0.097 | 0.739 |\n\n**Table 2: Regression Results for `BDEBT`**\n| Dependent Variable | Variable | Coefficient | t-statistic |\n| :--- | :--- | :---: | :---: |\n| `BDEBT` (Model FEM1) | `BTG1` | -0.471 | (-2.03) |\n\n---\n\nBased on the provided data, select all of the following statements that are valid.\n", "model_evaluation_result": {"model_raw_response": "Looking at the regression results and descriptive statistics:\n\n**Analyzing each statement:**\n\nA) The coefficient of -0.471 means a one-unit increase in BTG1 is associated with a 0.471 unit decrease in BDEBT (not 0.471% - the coefficient represents the change in levels, not percentage points). This statement is incorrect.\n\nB) A one standard deviation increase in BTG1 = 0.739\n   Effect on BDEBT = -0.471 × 0.739 = -0.348\n   This statement is correct.\n\nC) The t-statistic is -2.03, which has an absolute value of 2.03. This exceeds the critical value of 1.96 for 5% significance level (two-tailed test). The coefficient is statistically significant at the 5% level. This statement is correct.\n\nD) The change in BDEBT from a one standard deviation increase in BTG1 = -0.348 (in absolute terms: 0.348)\n   Mean BDEBT = 0.326\n   Since 0.348 > 0.326, the economic magnitude is indeed larger than the mean BDEBT. This statement is correct.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 142, "Question": "### Background\n\nAn analysis of the Italian savings bank industry from 1982-1989 used a translog cost function to estimate economies of scale. The study calculated Ordinary Economies of Scale (OES), representing the elasticity of total cost with respect to output, holding the number of branches constant. A value less than 1 indicates economies of scale (i.e., a 1% increase in output leads to a less than 1% increase in cost).\n\n### Data / Model Specification\n\n**Table 1. Ordinary Economies of Scale (OES) Estimates**\n\n| Period      | OES Estimate | Standard Error |\n| :---------- | :----------- | :------------- |\n| 1982-1983   | 0.8203       | 0.0455         |\n| 1984-1985   | 0.7945       | 0.0838         |\n| 1986-1987   | 0.7735       | 0.0689         |\n| 1988-1989   | 0.9502       | 0.2742         |\n| **1982-1989** | **0.7845**   | **0.0254**     |\n\nAn alternative fixed-effects (FE) model, which controls for unobserved, time-invariant bank characteristics, produced a much lower OES estimate of **0.2712**. The paper's authors discarded this result, arguing it could be biased.\n\n---\n\nBased on the provided data and context, which of the following statements are valid interpretations or inferences?", "Options": {"A": "If unobserved managerial quality is positively correlated with output and negatively correlated with costs, then the pooled OES estimate of 0.7845 is biased upwards, making the true economies of scale appear smaller than they are.", "B": "The OES estimate for the 1988-1989 subperiod (0.9502) is not statistically different from 1 (constant returns to scale) at the 5% significance level.", "C": "The trend in OES estimates from 1982 to 1989 suggests that the economies of scale available to Italian savings banks were diminishing over the decade.", "D": "The OES estimate for the full 1982-1989 period (0.7845) implies that a 10% increase in a bank's total loans was associated with a 7.845% increase in its total costs, ceteris paribus."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret elasticity coefficients, conduct statistical inference from point estimates and standard errors, identify trends in data, and understand the direction of omitted variable bias. It combines computational judgment with reverse-reasoning about econometric bias.\nStrategy: Scenario Application. The user must apply statistical and econometric principles to the specific results presented in the table.\nDistractor Logic:\n- A (Correct): Correctly scales the elasticity interpretation from 1% to 10%.\n- B (Correct): Requires calculating a t-statistic: (0.9502 - 1) / 0.2742 ≈ -0.18. The absolute value is far less than the critical value of ~1.96, so the null of OES=1 cannot be rejected.\n- C (Correct): The OES estimate rises from ~0.82 to ~0.95, moving closer to 1. This indicates that the magnitude of scale economies (1 - OES) was shrinking.\n- D (Incorrect - Conceptual Opposite): This describes a downward bias. The bias on the OES coefficient would be `Corr(MgrQuality, Ln(Y)) * Corr(MgrQuality, Ln(TC)) = (+) * (-) = (-)`. A downward bias means the pooled estimate (0.7845) is artificially low, making economies of scale appear *larger* than they truly are. The FE estimate of 0.2712 is consistent with a large downward bias in the pooled model.", "qid": "142", "question": "### Background\n\nAn analysis of the Italian savings bank industry from 1982-1989 used a translog cost function to estimate economies of scale. The study calculated Ordinary Economies of Scale (OES), representing the elasticity of total cost with respect to output, holding the number of branches constant. A value less than 1 indicates economies of scale (i.e., a 1% increase in output leads to a less than 1% increase in cost).\n\n### Data / Model Specification\n\n**Table 1. Ordinary Economies of Scale (OES) Estimates**\n\n| Period      | OES Estimate | Standard Error |\n| :---------- | :----------- | :------------- |\n| 1982-1983   | 0.8203       | 0.0455         |\n| 1984-1985   | 0.7945       | 0.0838         |\n| 1986-1987   | 0.7735       | 0.0689         |\n| 1988-1989   | 0.9502       | 0.2742         |\n| **1982-1989** | **0.7845**   | **0.0254**     |\n\nAn alternative fixed-effects (FE) model, which controls for unobserved, time-invariant bank characteristics, produced a much lower OES estimate of **0.2712**. The paper's authors discarded this result, arguing it could be biased.\n\n---\n\nBased on the provided data and context, which of the following statements are valid interpretations or inferences?", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) If unobserved managerial quality is positively correlated with output and negatively correlated with costs, then the pooled OES estimate would be biased DOWNWARD, not upward. Better managers produce more output with lower costs, making the cost-output relationship appear flatter than it truly is. This would make economies of scale appear LARGER (lower OES), not smaller. This statement is incorrect.\n\nB) For the 1988-1989 period: OES = 0.9502, SE = 0.2742\nTo test if it's different from 1:\n- 95% confidence interval ≈ 0.9502 ± 1.96(0.2742) = [0.413, 1.487]\n- Since this interval contains 1, we cannot reject the null hypothesis that OES = 1 at the 5% level\nThis statement is correct.\n\nC) The OES estimates show a clear trend:\n- 1982-1983: 0.8203\n- 1984-1985: 0.7945 (decreasing, more economies of scale)\n- 1986-1987: 0.7735 (decreasing, more economies of scale)\n- 1988-1989: 0.9502 (increasing, less economies of scale)\nThe values decreased (showing increasing economies of scale) until 1986-1987, then jumped up in 1988-1989 (showing diminishing economies of scale). Overall, the trend suggests diminishing economies of scale over the decade. This statement is correct.\n\nD) OES = 0.7845 means the elasticity of cost with respect to output is 0.7845. Therefore, a 10% increase in output leads to a 10% × 0.7845 = 7.845% increase in costs. This statement is correct.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 21, "Question": "### Background\n\nA study uses a Heckman two-stage model to analyze the effect of ownership structure on the value of family firms. This method is chosen to correct for potential self-selection bias, as firms with certain unobserved characteristics might be more likely to be family-owned, and those same characteristics could also affect firm value.\n\n### Data / Model Specification\n\nThe Heckman model consists of two equations:\n\n1.  **Selection Equation (First Stage):** A probit model estimates the probability that a firm is a family firm (`FF_i=1`) based on a vector of characteristics `Z_i`. The error term is `μ_i`.\n      \nP(FF_i=1) = \\Phi(\\gamma'Z_i)\n \n2.  **Outcome Equation (Second Stage):** A regression estimates firm value (`AVALUE`) for the subsample of family firms. The error term is `ε_it`.\n      \nAVALUE_{it} = \\alpha_{0} + \\beta X_{it-1} + \\delta \\lambda_i + ... + \\varepsilon_{it}\n \n\nThe key correction term is the **Inverse Mills Ratio (`λ_i`)**, which is included as a regressor in the second stage. The errors `(μ_i, ε_it)` are assumed to be bivariate normal with correlation `ρ`.\n\n---\n\nWhich of the following statements about the Heckman model's application in this study are theoretically correct?\n", "Options": {"A": "A statistically significant coefficient `δ` on the Inverse Mills Ratio implies that unobserved factors influencing a firm's choice to be family-owned are correlated with unobserved factors that determine its value.", "B": "If the coefficient `δ` were found to be statistically insignificant, it would suggest that a standard OLS regression on the family firm subsample would have produced unbiased estimates of `β`.", "C": "The model requires at least one variable in the selection equation (`Z_i`) that is excluded from the outcome equation (`X_it`) to ensure proper identification.", "D": "The coefficient `δ` on the Inverse Mills Ratio is a consistent estimator of the product of the correlation `ρ` and the standard deviation of the outcome equation's error term, `σ_ε`."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This question assesses deep knowledge of the econometric methodology (Heckman model) used in the paper. It requires understanding the purpose of the model, the interpretation of its key parameters, and the conditions for its application.\nDepth Strategy: Reverse-Reasoning. The user is given the model and must identify all correct theoretical statements about its properties and interpretation.\nDistractor Logic: All options provided are correct statements about the Heckman two-stage model as described in econometrics literature and applied in the paper. This format rigorously tests comprehensive knowledge, as the user must be confident in affirming all four distinct theoretical points.\n- A (Correct): This is the fundamental interpretation of a significant `δ`; it confirms the presence of selection bias.\n- B (Correct): This is the formal definition of the coefficient `δ = ρσ_ε`.\n- C (Correct): An insignificant `δ` implies `ρ=0`, meaning there is no correlation between the errors and thus no selection bias. OLS would be consistent in that case.\n- D (Correct): This is the exclusion restriction, a critical assumption for the identification of the Heckman model. The paper explicitly mentions using CEO tenure for this purpose.", "qid": "21", "question": "### Background\n\nA study uses a Heckman two-stage model to analyze the effect of ownership structure on the value of family firms. This method is chosen to correct for potential self-selection bias, as firms with certain unobserved characteristics might be more likely to be family-owned, and those same characteristics could also affect firm value.\n\n### Data / Model Specification\n\nThe Heckman model consists of two equations:\n\n1.  **Selection Equation (First Stage):** A probit model estimates the probability that a firm is a family firm (`FF_i=1`) based on a vector of characteristics `Z_i`. The error term is `μ_i`.\n      \nP(FF_i=1) = \\Phi(\\gamma'Z_i)\n \n2.  **Outcome Equation (Second Stage):** A regression estimates firm value (`AVALUE`) for the subsample of family firms. The error term is `ε_it`.\n      \nAVALUE_{it} = \\alpha_{0} + \\beta X_{it-1} + \\delta \\lambda_i + ... + \\varepsilon_{it}\n \n\nThe key correction term is the **Inverse Mills Ratio (`λ_i`)**, which is included as a regressor in the second stage. The errors `(μ_i, ε_it)` are assumed to be bivariate normal with correlation `ρ`.\n\n---\n\nWhich of the following statements about the Heckman model's application in this study are theoretically correct?\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement about the Heckman two-stage model:\n\n**Statement A**: This is correct. The Inverse Mills Ratio (λ) captures the correlation between unobserved factors in the selection equation (becoming a family firm) and the outcome equation (firm value). A statistically significant coefficient δ indicates that ρ ≠ 0, meaning there is correlation between the error terms μ and ε, which represents unobserved factors in both equations.\n\n**Statement B**: This is correct. If δ is statistically insignificant, it suggests ρ = 0, meaning there is no correlation between the unobserved factors in the selection and outcome equations. In this case, there is no selection bias, and standard OLS on the family firm subsample would produce unbiased estimates of β.\n\n**Statement C**: This is correct. The Heckman model requires an exclusion restriction for proper identification - at least one variable must appear in the selection equation (Z) but not in the outcome equation (X). This ensures the model is identified through variation beyond just the nonlinearity of the Inverse Mills Ratio.\n\n**Statement D**: This is correct. In the Heckman model, the coefficient on the Inverse Mills Ratio equals ρσ_ε, where ρ is the correlation between the error terms and σ_ε is the standard deviation of the outcome equation's error term. The coefficient δ provides a consistent estimate of this product.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 22, "Question": "### Background\n\nA study examines how the identity of other large shareholders influences the value of family-controlled firms. It tests two hypotheses regarding blockholder identity:\n\n*   **Hypothesis 3a:** Blockholders that are other families, individuals, or non-financial firms (`IDENTITY1`) are likely to negatively influence firm value due to collusion or rivalry.\n*   **Hypothesis 3b:** Foreign companies as blockholders (`IDENTITY2`) are likely to positively influence firm value due to enhanced monitoring and expertise.\n\n### Data / Model Specification\n\nRegression models were run on a sample of Spanish family firms to estimate the effect of these blockholder identities on industry-adjusted firm value (`AVALUE`). The results are summarized below.\n\n**Table 1: Impact of Blockholder Identity on Family Firm Value**\n\n| Variable    | Coefficient | p-value Category |\n| :---------- | :---------- | :--------------- |\n| `IDENTITY1` | -0.05       | Not Significant  |\n| `IDENTITY2` | 0.27        | p < 0.10         |\n*Source: Table 5, Models 5 and 6 from the paper.*\n\n---\n\nBased on the study's findings, which of the following statements are **NOT** supported conclusions?\n", "Options": {"A": "The presence of foreign investors as significant blockholders is associated with a positive and statistically significant increase in family firm value.", "B": "The paper concludes that blockholder identity is irrelevant for firm valuation in the Spanish context.", "C": "The study's results for blockholder identity are inconclusive, as neither `IDENTITY1` nor `IDENTITY2` showed a statistically significant effect on firm value.", "D": "The data provide strong evidence that having other families or non-financial firms as co-blockholders leads to value destruction through collusion or rivalry."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: The user must interpret a mix of significant and non-significant regression results and correctly identify which conclusions are invalidated by the evidence. This tests for careful reading and the ability to distinguish supported claims from unsupported ones.\nDepth Strategy: Single-Correct-Answer Inversion Protocol. The core assessment objective is to identify the single correct conclusion from the table (Statement A). To create a multiple-response item, the stem is inverted to ask for conclusions that are NOT supported. This transforms the three incorrect statements into the correct answers.\nDistractor Logic:\n- A (New Distractor, was the single correct answer): This statement is a direct and accurate summary of the significant positive result for `IDENTITY2`. It is a supported conclusion, and therefore an incorrect answer to the inverted question.\n- B (Correct Answer): The coefficient for `IDENTITY1` is statistically insignificant. Therefore, the data do NOT provide strong evidence for the hypothesized negative effect. This statement is an unsupported conclusion.\n- C (Correct Answer): This statement is false because `IDENTITY2` DOES show a statistically significant effect. The results are not entirely inconclusive. This statement is an unsupported conclusion.\n- D (Correct Answer): This statement is an overgeneralization and is false. The significant result for `IDENTITY2` explicitly shows that at least one dimension of identity (foreign vs. domestic) is relevant. This statement is an unsupported conclusion.", "qid": "22", "question": "### Background\n\nA study examines how the identity of other large shareholders influences the value of family-controlled firms. It tests two hypotheses regarding blockholder identity:\n\n*   **Hypothesis 3a:** Blockholders that are other families, individuals, or non-financial firms (`IDENTITY1`) are likely to negatively influence firm value due to collusion or rivalry.\n*   **Hypothesis 3b:** Foreign companies as blockholders (`IDENTITY2`) are likely to positively influence firm value due to enhanced monitoring and expertise.\n\n### Data / Model Specification\n\nRegression models were run on a sample of Spanish family firms to estimate the effect of these blockholder identities on industry-adjusted firm value (`AVALUE`). The results are summarized below.\n\n**Table 1: Impact of Blockholder Identity on Family Firm Value**\n\n| Variable    | Coefficient | p-value Category |\n| :---------- | :---------- | :--------------- |\n| `IDENTITY1` | -0.05       | Not Significant  |\n| `IDENTITY2` | 0.27        | p < 0.10         |\n*Source: Table 5, Models 5 and 6 from the paper.*\n\n---\n\nBased on the study's findings, which of the following statements are **NOT** supported conclusions?\n", "model_evaluation_result": {"model_raw_response": "Looking at the results table and the hypotheses, I need to identify which statements are NOT supported by the findings.\n\nFrom the table:\n- IDENTITY1 (other families, individuals, non-financial firms): coefficient = -0.05, not significant\n- IDENTITY2 (foreign companies): coefficient = 0.27, p < 0.10 (significant at 10% level)\n\nLet me evaluate each option:\n\nA) States that foreign investors (IDENTITY2) have a positive and statistically significant effect. This IS supported - coefficient is positive (0.27) and significant (p < 0.10).\n\nB) Claims the paper concludes blockholder identity is irrelevant. This is NOT supported - IDENTITY2 shows a significant effect, so identity does matter.\n\nC) States both IDENTITY1 and IDENTITY2 showed no significant effect. This is NOT supported - IDENTITY2 is significant at p < 0.10.\n\nD) Claims there's strong evidence that other families/non-financial firms (IDENTITY1) lead to value destruction. This is NOT supported - while the coefficient is negative (-0.05), it's not statistically significant, so we cannot conclude there's evidence of value destruction.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 84, "Question": "### Background\n\n**Research Question.** How can Principal Component Regression (PCR) resolve the problem of theoretically incorrect coefficient signs that plagues OLS models of housing prices when multicollinearity is present?\n\n**Setting.** A regression of standardized housing prices is run on 16 orthogonal factors derived from Principal Component Analysis (PCA). These factors represent dimensions like \"quantity\" and \"quality\". In prior OLS models, variables like number of bedrooms (`BDRMS`) had counter-intuitive negative coefficients.\n\n---\n\n### Data / Model Specification\n\nIn contrast to OLS, PCR regresses price on orthogonal factors. The paper identifies Factor 2 as a \"quantity\" dimension, which is a linear combination of variables including `SQFT`, `BDRMS`, and `BATHRMS`. The results for the PCR model with 16 factors (PCR16) show a regression coefficient for Factor 2 of 0.6861, which is positive and highly significant.\n\n---\n\nWhich of the following statements accurately describe how the PCR methodology addresses the multicollinearity problem observed in the OLS model?", "Options": {"A": "The positive coefficient on Factor 2 (the \"quantity\" dimension) correctly captures the aggregate positive impact of house size on price, resolving the nonsensical negative signs seen on individual size-related variables in the OLS model.", "B": "PCR provides direct, unbiased estimates of the marginal dollar impact for each original variable (e.g., `BDRMS`), which is its primary advantage over OLS for structural analysis.", "C": "PCR transforms the original correlated predictor variables into a new set of factors that are, by mathematical construction, orthogonal (uncorrelated) to each other.", "D": "By regressing the outcome on the orthogonal factors, the resulting factor coefficients are not distorted by inter-factor correlations, thus satisfying the linear independence assumption."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's understanding of the fundamental mechanism of Principal Component Regression (PCR) and how it solves the multicollinearity problem. Depth Strategy: Reverse-Reasoning. The user is given the solution (PCR) and must identify all the correct mechanistic steps and interpretive conclusions that make it effective. Distractor Logic: Option D is a conceptual opposite and a key trade-off mentioned in the paper. PCR's main disadvantage for structural analysis is that it *loses* the direct, interpretable coefficients for the original variables, providing coefficients for abstract factors instead.", "qid": "84", "question": "### Background\n\n**Research Question.** How can Principal Component Regression (PCR) resolve the problem of theoretically incorrect coefficient signs that plagues OLS models of housing prices when multicollinearity is present?\n\n**Setting.** A regression of standardized housing prices is run on 16 orthogonal factors derived from Principal Component Analysis (PCA). These factors represent dimensions like \"quantity\" and \"quality\". In prior OLS models, variables like number of bedrooms (`BDRMS`) had counter-intuitive negative coefficients.\n\n---\n\n### Data / Model Specification\n\nIn contrast to OLS, PCR regresses price on orthogonal factors. The paper identifies Factor 2 as a \"quantity\" dimension, which is a linear combination of variables including `SQFT`, `BDRMS`, and `BATHRMS`. The results for the PCR model with 16 factors (PCR16) show a regression coefficient for Factor 2 of 0.6861, which is positive and highly significant.\n\n---\n\nWhich of the following statements accurately describe how the PCR methodology addresses the multicollinearity problem observed in the OLS model?", "model_evaluation_result": {"model_raw_response": "Looking at this question about how PCR addresses multicollinearity in housing price models, I need to evaluate each statement:\n\nA) This is correct. The positive coefficient on Factor 2 (0.6861) represents the \"quantity\" dimension and captures the aggregate positive effect of size-related variables on price. This resolves the counterintuitive negative coefficients (like for BDRMS) seen in the OLS model due to multicollinearity.\n\nB) This is incorrect. PCR does not provide direct estimates of marginal impacts for individual original variables. Instead, it provides coefficients for the principal components/factors, which are linear combinations of the original variables. To get individual variable effects would require back-transformation, and these wouldn't necessarily be \"unbiased\" in the structural sense.\n\nC) This is correct. Principal Component Analysis mathematically transforms correlated predictors into orthogonal (uncorrelated) principal components. This is a fundamental property of PCA and a key reason PCR can address multicollinearity.\n\nD) This is correct. Since the factors are orthogonal by construction, there are no inter-factor correlations to distort the coefficient estimates. This satisfies the linear independence assumption that is violated in the original OLS model due to multicollinearity.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 111, "Question": "### Background\n\n**Research Question.** This case investigates how empirical estimates of the correlation between currency strategies and a benchmark equity portfolio affect the theoretical bounds on return predictability, and how this framework can be used to confront the joint hypothesis problem in asset pricing.\n\n**Setting.** The analysis uses monthly data from 1994–2016 for forward contracts on 23 currencies against the US Dollar. The benchmark portfolio for the marginal investor (`$r_m$`) is the CRSP value-weighted portfolio of US stocks.\n\n**Variables and Parameters.**\n- `$\\phi_1$`: The unadjusted predictability bound (assuming `$\\rho^2=1$`).\n- `$\\phi_2$`: The correlation-adjusted predictability bound.\n- `$RRA_V$`: The upper bound on the marginal investor's relative risk aversion, set to 2.5 or 5.0.\n- `$r_m$`: Excess return on the CRSP value-weighted US stock portfolio.\n- `$\\rho^2(r_i^*, r_m)$`: The squared sample correlation between the optimal predictability-based strategy for currency `i` and the benchmark portfolio `$r_m$`.\n\n---\n\n### Data / Model Specification\n\nThe unadjusted and correlation-adjusted predictability bounds are given by:\n\n  \n\\phi_1 \\approx RRA_V^2 \\sigma^2(r_m) \\quad \\text{(Eq. (1))}\n \n\n  \n\\phi_2 \\equiv \\rho^2(r_i^*, r_m) RRA_V^2 \\sigma^2(r_m) \\quad \\text{(Eq. (2))}\n \n\n**Table 1** below reports the components for calculating these bounds for various currencies. All figures are in percentage points. The estimated predictability (`$\\widehat{R}^2$`) for the Mexican Peso (MEX) forward return is 2.12%.\n\n| Currency | Unadjusted Bound (`$\\phi_1$`) | `$\\rho^2$` (%) | Adjusted Bound (`$\\phi_2$`) |\n| :--- | :--- | :--- | :--- |\n| **RRA = 2.5** | | | |\n| BRA | 1.16 | 0.14 | *[missing]* |\n| CZECH | 1.15 | 0.12 | 0.0014 |\n| HUN | 1.22 | 0.36 | 0.0043 |\n| MEX | 1.25 | 15.92 | 0.1990 |\n| **RRA = 5.0** | | | |\n| BRA | 4.62 | 0.14 | 0.0063 |\n| MEX | 5.00 | 15.92 | 0.7959 |\n\n---\n\n### Question\n\nBased on the provided data and relationships, select all of the following statements that are correct.", "Options": {"A": "The missing correlation-adjusted bound (`$\\phi_2$`) for the Brazilian Real (BRA) at `$RRA_V=2.5$` is 0.1624%.", "B": "To rationalize the observed predictability (`$\\widehat{R}^2 = 2.12\\%` for MEX) without violating the correlation-adjusted bound (`$\\phi_2$`), the marginal investor's relative risk aversion (`$RRA_V$`) must be at least 8.1.", "C": "The market variance `$\\sigma^2(r_m)$` implied by the data for the Mexican Peso (MEX) with an assumed `$RRA_V$` of 2.5 is 0.002.", "D": "The low `$\\rho^2$` values for most currencies indicate that predictability-based currency strategies carry significant systematic equity market risk."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an Atomic Decomposition strategy to test multiple facets of the original problem: a preliminary calculation (market variance), the main 'apex' calculation (implied RRA), and a core conceptual interpretation. This preserves the depth of the original QA while creating a sharp, diagnostic multi-select item. Distractor C is a Conceptual Opposite of the paper's finding on idiosyncratic risk. Distractor D introduces a common decimal/unit error into a simple calculation, testing for careful application of the formula.", "qid": "111", "question": "### Background\n\n**Research Question.** This case investigates how empirical estimates of the correlation between currency strategies and a benchmark equity portfolio affect the theoretical bounds on return predictability, and how this framework can be used to confront the joint hypothesis problem in asset pricing.\n\n**Setting.** The analysis uses monthly data from 1994–2016 for forward contracts on 23 currencies against the US Dollar. The benchmark portfolio for the marginal investor (`$r_m$`) is the CRSP value-weighted portfolio of US stocks.\n\n**Variables and Parameters.**\n- `$\\phi_1$`: The unadjusted predictability bound (assuming `$\\rho^2=1$`).\n- `$\\phi_2$`: The correlation-adjusted predictability bound.\n- `$RRA_V$`: The upper bound on the marginal investor's relative risk aversion, set to 2.5 or 5.0.\n- `$r_m$`: Excess return on the CRSP value-weighted US stock portfolio.\n- `$\\rho^2(r_i^*, r_m)$`: The squared sample correlation between the optimal predictability-based strategy for currency `i` and the benchmark portfolio `$r_m$`.\n\n---\n\n### Data / Model Specification\n\nThe unadjusted and correlation-adjusted predictability bounds are given by:\n\n  \n\\phi_1 \\approx RRA_V^2 \\sigma^2(r_m) \\quad \\text{(Eq. (1))}\n \n\n  \n\\phi_2 \\equiv \\rho^2(r_i^*, r_m) RRA_V^2 \\sigma^2(r_m) \\quad \\text{(Eq. (2))}\n \n\n**Table 1** below reports the components for calculating these bounds for various currencies. All figures are in percentage points. The estimated predictability (`$\\widehat{R}^2$`) for the Mexican Peso (MEX) forward return is 2.12%.\n\n| Currency | Unadjusted Bound (`$\\phi_1$`) | `$\\rho^2$` (%) | Adjusted Bound (`$\\phi_2$`) |\n| :--- | :--- | :--- | :--- |\n| **RRA = 2.5** | | | |\n| BRA | 1.16 | 0.14 | *[missing]* |\n| CZECH | 1.15 | 0.12 | 0.0014 |\n| HUN | 1.22 | 0.36 | 0.0043 |\n| MEX | 1.25 | 15.92 | 0.1990 |\n| **RRA = 5.0** | | | |\n| BRA | 4.62 | 0.14 | 0.0063 |\n| MEX | 5.00 | 15.92 | 0.7959 |\n\n---\n\n### Question\n\nBased on the provided data and relationships, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement based on the given data and formulas.\n\nGiven formulas:\n- φ₁ ≈ RRA_V² × σ²(r_m)  (Eq. 1)\n- φ₂ = ρ²(r_i*, r_m) × RRA_V² × σ²(r_m)  (Eq. 2)\n\n**Statement A**: The missing correlation-adjusted bound (φ₂) for BRA at RRA_V=2.5 is 0.1624%.\n\nFrom the table:\n- For BRA at RRA=2.5: φ₁ = 1.16, ρ² = 0.14%\n- Using Eq. 2: φ₂ = ρ² × φ₁ = 0.0014 × 1.16 = 0.001624 = 0.1624%\n\nThis is correct.\n\n**Statement B**: To rationalize the observed predictability (R̂² = 2.12% for MEX) without violating the correlation-adjusted bound, RRA_V must be at least 8.1.\n\nFor MEX: ρ² = 15.92% = 0.1592\nFrom Eq. 1 with MEX at RRA=2.5: 1.25 = 2.5² × σ²(r_m)\nTherefore: σ²(r_m) = 1.25/6.25 = 0.2\n\nTo find required RRA_V:\nφ₂ ≥ R̂² = 2.12%\n0.1592 × RRA_V² × 0.2 ≥ 0.0212\nRRA_V² ≥ 0.0212/(0.1592 × 0.2) = 0.666\nRRA_V ≥ 0.816\n\nThis gives approximately 0.82, not 8.1. Statement B is incorrect.\n\n**Statement C**: The market variance σ²(r_m) implied by the data for MEX with RRA", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 174, "Question": "### Background\n\nA market maker's optimal strategy depends on the balance between information asymmetry and liquidity demand. In a simplified market with discrete outcomes, a dealer faces a risk-neutral informed trader and a liquidity trader. The dealer must choose an ask price `a` and an ask depth `z`.\n\n**Variables & Parameters.**\n- `a`: Ask price.\n- `z`: Ask depth (quantity limit).\n- `\\bar{v}`: The informed trader's positive valuation, a measure of information asymmetry.\n- `\\bar{\\eta}`: The positive component of the liquidity shock, a measure of potential liquidity demand.\n\n---\n\n### Data / Model Specification\n\nThe dealer's optimal strategy falls into different regimes based on the ratio `\\bar{v}/\\bar{\\eta}`.\n\n**Table 1: Equilibrium Price and Depth under Discrete Distributions**\n\n| Condition | `1/2 \\bar{\\eta} < \\bar{v} \\le 3/5 \\bar{\\eta}` | `3/5 \\bar{\\eta} < \\bar{v} \\le 3 \\bar{\\eta}` | `\\bar{v} > 3 \\bar{\\eta}` |\n| :--- | :--- | :--- | :--- |\n| **Optimal `a`** | `\\bar{v}` | `\\frac{1}{2}\\bar{\\eta}+\\frac{1}{6}\\bar{v}` | n.a. |\n| **Optimal `z`** | n.a. (effectively infinite) | `\\frac{1}{2}\\bar{\\eta}-\\frac{1}{6}\\bar{v}` | 0 |\n\n---\n\nConsider a scenario where `\\bar{\\eta} = 10`. According to the model specifications in Table 1, which of the following statements accurately describe the dealer's optimal strategy under the given market conditions?", "Options": {"A": "If information asymmetry is extreme (`\\bar{v} = 35`), the dealer exits the market by setting the depth `z = 0`.", "B": "If information asymmetry is high (`\\bar{v} = 24`), the dealer sets the ask price `a = 8` and the depth `z = 1`.", "C": "If information asymmetry is low (`\\bar{v} = 5`), the dealer sets the ask price `a = 5` and does not impose a quantity limit.", "D": "If information asymmetry is moderate (`\\bar{v} = 9`), the dealer sets the ask price `a = 6.5` and the depth `z = 3.5`."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to apply the regime-dependent formulas from the provided table to specific numerical scenarios. It requires identifying the correct regime for a given set of parameters and then performing the calculations for optimal price and depth. \nDepth Strategy: Computational Judgment. The user must perform a series of calculations for different scenarios to determine the validity of each option. \nDistractor Logic: Option C is a distractor based on a Formula Misuse error. The user correctly identifies the regime (`3/5 \\bar{\\eta} < \\bar{v} \\le 3 \\bar{\\eta}`), but then misapplies the formulas for `a` and `z`, leading to an incorrect result. Options A, B, and D are all correct applications of the table's logic to their respective regimes.", "qid": "174", "question": "### Background\n\nA market maker's optimal strategy depends on the balance between information asymmetry and liquidity demand. In a simplified market with discrete outcomes, a dealer faces a risk-neutral informed trader and a liquidity trader. The dealer must choose an ask price `a` and an ask depth `z`.\n\n**Variables & Parameters.**\n- `a`: Ask price.\n- `z`: Ask depth (quantity limit).\n- `\\bar{v}`: The informed trader's positive valuation, a measure of information asymmetry.\n- `\\bar{\\eta}`: The positive component of the liquidity shock, a measure of potential liquidity demand.\n\n---\n\n### Data / Model Specification\n\nThe dealer's optimal strategy falls into different regimes based on the ratio `\\bar{v}/\\bar{\\eta}`.\n\n**Table 1: Equilibrium Price and Depth under Discrete Distributions**\n\n| Condition | `1/2 \\bar{\\eta} < \\bar{v} \\le 3/5 \\bar{\\eta}` | `3/5 \\bar{\\eta} < \\bar{v} \\le 3 \\bar{\\eta}` | `\\bar{v} > 3 \\bar{\\eta}` |\n| :--- | :--- | :--- | :--- |\n| **Optimal `a`** | `\\bar{v}` | `\\frac{1}{2}\\bar{\\eta}+\\frac{1}{6}\\bar{v}` | n.a. |\n| **Optimal `z`** | n.a. (effectively infinite) | `\\frac{1}{2}\\bar{\\eta}-\\frac{1}{6}\\bar{v}` | 0 |\n\n---\n\nConsider a scenario where `\\bar{\\eta} = 10`. According to the model specifications in Table 1, which of the following statements accurately describe the dealer's optimal strategy under the given market conditions?", "model_evaluation_result": {"model_raw_response": "I need to analyze each option using the given table and the value of `\\bar{\\eta} = 10`.\n\nFirst, let me identify the regime boundaries when `\\bar{\\eta} = 10`:\n- Regime 1: `1/2 × 10 < \\bar{v} ≤ 3/5 × 10` → `5 < \\bar{v} ≤ 6`\n- Regime 2: `3/5 × 10 < \\bar{v} ≤ 3 × 10` → `6 < \\bar{v} ≤ 30`\n- Regime 3: `\\bar{v} > 3 × 10` → `\\bar{v} > 30`\n\nNow I'll check each option:\n\n**Option A**: `\\bar{v} = 35`\n- This falls in Regime 3 (`\\bar{v} > 30`)\n- According to the table, optimal `z = 0`\n- This is correct ✓\n\n**Option B**: `\\bar{v} = 24`\n- This falls in Regime 2 (`6 < 24 ≤ 30`)\n- Optimal `a = 1/2 × 10 + 1/6 × 24 = 5 + 4 = 9`\n- Optimal `z = 1/2 × 10 - 1/6 × 24 = 5 - 4 = 1`\n- The statement says `a = 8` and `z = 1`\n- The depth is correct but the price is wrong ✗\n\n**Option C**: `\\bar{v} = 5`\n- This doesn't fall in any regime (it's at the boundary `\\bar{v} = 1/2 × 10 = 5`)\n- The table doesn't specify what happens at boundaries\n- Cannot determine if correct ✗\n\n**Option D**: `\\bar{v} = 9`\n- This falls in Regime 2 (`6 < 9 ≤ 30`)\n- Optimal `a = 1/2 ×", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 59, "Question": "### Background\n\n**Research Question.** How do different stylized facts—specifically jump risk and seasonality—contribute to the out-of-sample option pricing performance of an affine asset pricing model for natural gas, and how does this contribution vary across option moneyness and maturity?\n\n**Setting.** The paper evaluates four nested models for Henry Hub natural gas prices: a baseline Mean Reversion Stochastic Volatility (MRSV) model, and extensions that add Jumps (MRSVJ), Seasonality (MRSVS), and both (MRSVJS). The models are tested on their ability to price European call options out-of-sample.\n\n**Variables and Parameters.**\n- `S/K`: Moneyness of an option, the ratio of the spot price to the strike price.\n- OTM (Out-of-the-Money): `S/K < 0.95`\n- ATM (At-the-Money): `0.95 < S/K < 1.05`\n- ITM (In-the-Money): `S/K > 1.05`\n- Pricing Error: The absolute difference between the observed market price of an option and the theoretical price predicted by a model, `|C_market - C_model|`.\n\n---\n\n### Data / Model Specification\n\nThe out-of-sample option pricing performance of the four models is evaluated by calculating the average absolute pricing error for options categorized by moneyness and maturity. The results are summarized in Table 1 below.\n\n**Table 1: Out-of-Sample Mean Absolute Option Pricing Errors ($)**\n\n| | <30 days | 30-60 days | 60-90 days | 90-120 days | 120-180 days | >180 days |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **OTM (S/K < 0.95)** | | | | | | |\n| MRSV | 0.0370 | 0.0532 | 0.0548 | 0.0578 | 0.0745 | 0.1127 |\n| MRSVJ | 0.0256 | 0.0366 | 0.0402 | 0.0477 | 0.0665 | 0.1140 |\n| MRSVS | 0.0237 | 0.0370 | 0.0470 | 0.0563 | 0.0672 | 0.0850 |\n| MRSVJS | 0.0229 | 0.0308 | 0.0371 | 0.0448 | 0.0646 | 0.0793 |\n| **ATM (0.95 < S/K < 1.05)** | | | | | | |\n| MRSV | 0.0614 | 0.0821 | 0.0809 | 0.0903 | 0.1010 | 0.1574 |\n| MRSVJ | 0.0485 | 0.0604 | 0.0667 | 0.0740 | 0.0897 | 0.1720 |\n| MRSVS | 0.0473 | 0.0662 | 0.0808 | 0.0897 | 0.0936 | 0.1353 |\n| MRSVJS | 0.0444 | 0.0547 | 0.0629 | 0.0716 | 0.0838 | 0.1128 |\n| **ITM (1.05 < S/K)** | | | | | | |\n| MRSV | 0.0697 | 0.0949 | 0.1007 | 0.1124 | 0.1315 | 0.1931 |\n| MRSVJ | 0.0648 | 0.0775 | 0.0763 | 0.0847 | 0.1097 | 0.2079 |\n| MRSVS | 0.0688 | 0.0887 | 0.0876 | 0.1038 | 0.0987 | 0.1627 |\n| MRSVJS | 0.0600 | 0.0687 | 0.0736 | 0.0807 | 0.0908 | 0.1415 |\n\n---\n\nBased on the data in Table 1, which of the following statements are valid conclusions about the relative performance of the models?\n\nSelect all that apply.", "Options": {"A": "For short-term ATM options (30-60 days to maturity), adding a jump component (MRSVJ) reduces the pricing error by more than 25% relative to the baseline MRSV model.", "B": "The MRSVS model consistently outperforms the MRSVJ model for all options with less than 90 days to maturity.", "C": "The MRSVJS model demonstrates the lowest pricing error in every single moneyness-maturity category presented in the table.", "D": "For long-term options (>180 days to maturity), adding a jump component without a seasonal component (MRSVJ) consistently increases pricing error compared to the baseline MRSV model."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to perform detailed, multi-cell comparisons and calculations from a data table to evaluate the performance of competing financial models. Strategy: Computational Judgment. The user must verify multiple claims by referencing specific data points and performing percentage change calculations. Distractor Logic: Option A is correct, requiring comparison across all three moneyness categories for >180 days. Option B is correct, requiring a comprehensive scan of the table to confirm MRSVJS's dominance. Option C is correct, requiring a specific calculation: (0.0821 - 0.0604) / 0.0821 ≈ 26.4%. Option D is a conceptual distractor (Almost Right); it is true for OTM <30 days but false for ATM 30-60 days and other categories, requiring careful checking.", "qid": "59", "question": "### Background\n\n**Research Question.** How do different stylized facts—specifically jump risk and seasonality—contribute to the out-of-sample option pricing performance of an affine asset pricing model for natural gas, and how does this contribution vary across option moneyness and maturity?\n\n**Setting.** The paper evaluates four nested models for Henry Hub natural gas prices: a baseline Mean Reversion Stochastic Volatility (MRSV) model, and extensions that add Jumps (MRSVJ), Seasonality (MRSVS), and both (MRSVJS). The models are tested on their ability to price European call options out-of-sample.\n\n**Variables and Parameters.**\n- `S/K`: Moneyness of an option, the ratio of the spot price to the strike price.\n- OTM (Out-of-the-Money): `S/K < 0.95`\n- ATM (At-the-Money): `0.95 < S/K < 1.05`\n- ITM (In-the-Money): `S/K > 1.05`\n- Pricing Error: The absolute difference between the observed market price of an option and the theoretical price predicted by a model, `|C_market - C_model|`.\n\n---\n\n### Data / Model Specification\n\nThe out-of-sample option pricing performance of the four models is evaluated by calculating the average absolute pricing error for options categorized by moneyness and maturity. The results are summarized in Table 1 below.\n\n**Table 1: Out-of-Sample Mean Absolute Option Pricing Errors ($)**\n\n| | <30 days | 30-60 days | 60-90 days | 90-120 days | 120-180 days | >180 days |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **OTM (S/K < 0.95)** | | | | | | |\n| MRSV | 0.0370 | 0.0532 | 0.0548 | 0.0578 | 0.0745 | 0.1127 |\n| MRSVJ | 0.0256 | 0.0366 | 0.0402 | 0.0477 | 0.0665 | 0.1140 |\n| MRSVS | 0.0237 | 0.0370 | 0.0470 | 0.0563 | 0.0672 | 0.0850 |\n| MRSVJS | 0.0229 | 0.0308 | 0.0371 | 0.0448 | 0.0646 | 0.0793 |\n| **ATM (0.95 < S/K < 1.05)** | | | | | | |\n| MRSV | 0.0614 | 0.0821 | 0.0809 | 0.0903 | 0.1010 | 0.1574 |\n| MRSVJ | 0.0485 | 0.0604 | 0.0667 | 0.0740 | 0.0897 | 0.1720 |\n| MRSVS | 0.0473 | 0.0662 | 0.0808 | 0.0897 | 0.0936 | 0.1353 |\n| MRSVJS | 0.0444 | 0.0547 | 0.0629 | 0.0716 | 0.0838 | 0.1128 |\n| **ITM (1.05 < S/K)** | | | | | | |\n| MRSV | 0.0697 | 0.0949 | 0.1007 | 0.1124 | 0.1315 | 0.1931 |\n| MRSVJ | 0.0648 | 0.0775 | 0.0763 | 0.0847 | 0.1097 | 0.2079 |\n| MRSVS | 0.0688 | 0.0887 | 0.0876 | 0.1038 | 0.0987 | 0.1627 |\n| MRSVJS | 0.0600 | 0.0687 | 0.0736 | 0.0807 | 0.0908 | 0.1415 |\n\n---\n\nBased on the data in Table 1, which of the following statements are valid conclusions about the relative performance of the models?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each statement against the data:\n\n**Statement A**: For short-term ATM options (30-60 days to maturity), adding a jump component (MRSVJ) reduces the pricing error by more than 25% relative to the baseline MRSV model.\n\nFor ATM options with 30-60 days:\n- MRSV: 0.0821\n- MRSVJ: 0.0604\n- Reduction: (0.0821 - 0.0604) / 0.0821 = 0.0217 / 0.0821 = 26.4%\n\nThis is greater than 25%, so Statement A is TRUE.\n\n**Statement B**: The MRSVS model consistently outperforms the MRSVJ model for all options with less than 90 days to maturity.\n\nChecking all categories with <90 days:\n- OTM <30 days: MRSVS (0.0237) < MRSVJ (0.0256) ✓\n- OTM 30-60 days: MRSVS (0.0370) > MRSVJ (0.0366) ✗\n- OTM 60-90 days: MRSVS (0.0470) > MRSVJ (0.0402) ✗\n- ATM <30 days: MRSVS (0.0473) < MRSVJ (0.0485) ✓\n- ATM 30-60 days: MRSVS (0.0662) > MRSVJ (0.0604) ✗\n- ATM 60-90 days: MRSVS (0.0808) > MRSVJ (0.0667) ✗\n- ITM <30 days: MRSVS (0.0688) > MRSVJ (0.0648) ✗\n- ITM 30-60 days: MRSVS (0.0887) > MRSVJ (0.0775) ✗\n- ITM 60-90 days: MRSVS (0.0876) > MRSV", "model_answer": ["A", "B"], "error": null}}
{"ID": 20, "Question": "### Background\n\nA study investigates the impact of other large shareholders on family firm value. It tests Hypothesis 1, which posits that both the mere presence of multiple shareholders and their increasing voting power relative to the family should positively influence firm value.\n\n### Data / Model Specification\n\nTwo key variables are used to test this hypothesis:\n\n*   `MLSH`: A dummy variable that is 1 if multiple large shareholders exist, and 0 otherwise.\n*   `VOTING2341`: A continuous variable measuring the contestability of the family's power, calculated as `(SSH + TSH + IVSH) / FSH`, where `FSH` is the family's voting rights and `SSH`, `TSH`, `IVSH` are the rights of the next three largest shareholders.\n\nRegression results for the effect of these variables on industry-adjusted firm value (`AVALUE`) are shown below.\n\n**Table 1: Regression Results for Hypothesis 1**\n\n| Variable     | Model 1 Coeff. (t-stat) | Model 2 Coeff. (t-stat) |\n| :----------- | :---------------------- | :---------------------- |\n| `MLSH`       | 0.52** (3.42)           | —                       |\n| `VOTING2341` | —                       | -0.08 (-0.75)           |\n*Source: Table 5 from the paper. **p<0.01.*\n\n---\n\nBased on the theoretical motivation and the empirical results in Table 1, which of the following conclusions are supported by the study?\n", "Options": {"A": "The market positively values the transition from a single-owner structure to one with at least one other significant blockholder, supporting the idea that baseline monitoring is beneficial.", "B": "The study provides strong evidence that as the voting power of other blockholders increases relative to the family's, firm value consistently increases due to enhanced contestability.", "C": "The results suggest that the *identity* and *intent* of other blockholders might be more important than their mechanical voting power, as the `VOTING2341` ratio fails to capture a significant valuation effect.", "D": "The findings for `MLSH` and `VOTING2341` are contradictory, indicating a fundamental flaw in the study's methodology."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: The user must interpret two separate regression models to evaluate a two-part hypothesis. This requires reconciling a statistically significant result with a null result and drawing nuanced conclusions about the underlying governance mechanisms.\nDepth Strategy: Reverse-Reasoning. Given the empirical results (a significant effect for presence, a null effect for relative power), the user must identify the plausible theoretical explanations.\nDistractor Logic:\n- A (Correct): The significant positive coefficient on `MLSH` directly supports this conclusion.\n- B (Distractor - Conceptual Opposite): This is the opposite of what the results show. The coefficient on `VOTING2341` is statistically insignificant and negative, directly contradicting this statement.\n- C (Correct): The null result for `VOTING2341` suggests it is a noisy proxy for true contestability. A plausible explanation, discussed in the paper, is that the variable fails to account for blockholder identity (e.g., passive ally vs. active monitor), which is a key driver of behavior.\n- D (Distractor - Almost Right): While the results are nuanced, they are not necessarily contradictory. They can be reconciled by concluding that baseline monitoring matters, but the specific measure of contestability used is not a significant driver of value. This points to a limitation of the proxy, not a fundamental methodological flaw.", "qid": "20", "question": "### Background\n\nA study investigates the impact of other large shareholders on family firm value. It tests Hypothesis 1, which posits that both the mere presence of multiple shareholders and their increasing voting power relative to the family should positively influence firm value.\n\n### Data / Model Specification\n\nTwo key variables are used to test this hypothesis:\n\n*   `MLSH`: A dummy variable that is 1 if multiple large shareholders exist, and 0 otherwise.\n*   `VOTING2341`: A continuous variable measuring the contestability of the family's power, calculated as `(SSH + TSH + IVSH) / FSH`, where `FSH` is the family's voting rights and `SSH`, `TSH`, `IVSH` are the rights of the next three largest shareholders.\n\nRegression results for the effect of these variables on industry-adjusted firm value (`AVALUE`) are shown below.\n\n**Table 1: Regression Results for Hypothesis 1**\n\n| Variable     | Model 1 Coeff. (t-stat) | Model 2 Coeff. (t-stat) |\n| :----------- | :---------------------- | :---------------------- |\n| `MLSH`       | 0.52** (3.42)           | —                       |\n| `VOTING2341` | —                       | -0.08 (-0.75)           |\n*Source: Table 5 from the paper. **p<0.01.*\n\n---\n\nBased on the theoretical motivation and the empirical results in Table 1, which of the following conclusions are supported by the study?\n", "model_evaluation_result": {"model_raw_response": "Looking at the regression results and the theoretical framework:\n\n**Model 1** shows that `MLSH` (presence of multiple large shareholders) has a positive and statistically significant coefficient of 0.52 (t-stat = 3.42, p<0.01). This indicates that firms with multiple large shareholders have higher industry-adjusted values compared to single-owner firms.\n\n**Model 2** shows that `VOTING2341` (relative voting power of other blockholders) has a coefficient of -0.08 that is not statistically significant (t-stat = -0.75). This suggests that increasing the voting power of other shareholders relative to the family does not significantly affect firm value.\n\nAnalyzing each option:\n\n**A)** This is supported. The positive and significant coefficient on `MLSH` indicates that transitioning from single ownership to having at least one other significant blockholder is associated with higher firm value, consistent with beneficial monitoring effects.\n\n**B)** This is not supported. The `VOTING2341` coefficient is insignificant, providing no evidence that increasing voting power of other blockholders relative to the family increases firm value.\n\n**C)** This is supported. The insignificant result for `VOTING2341` despite the significant result for `MLSH` suggests that simply having more voting power relative to the family doesn't capture the valuation effect. This implies other factors like the identity and intentions of blockholders may be more important than their mechanical voting rights.\n\n**D)** This is not supported. The results are not contradictory - they test different aspects of the hypothesis. `MLSH` tests for the presence effect while `VOTING2341` tests for the intensity effect. Finding significance for one but not the other is a meaningful empirical result, not a methodological flaw.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 181, "Question": "### Background\n\n**Research Question.** How can the risk-adjusted performance of different equity styles be evaluated to test the behavioral proposition that investor return-chasing leads to predictable cycles of outperformance for 'in-favor' styles and underperformance for their 'twin' styles?\n\n**Setting and Data.** The analysis uses monthly returns for four Fama-French (FF) style benchmarks and the Wilshire 5000 market index from January 1979 to December 2004.\n\n---\n\n### Data / Model Specification\n\nThe Hansen-Jagannathan (HJ) bound is a theoretical condition in asset pricing which states that for any asset `i`, its Sharpe Ratio must be less than or equal to the maximum possible Sharpe Ratio in the economy, `SHP_max`. If the Capital Asset Pricing Model (CAPM) holds, the market portfolio is assumed to be mean-variance efficient, implying its Sharpe Ratio is this maximum (`SHP_M = SHP_max`).\n\n**Table 1: Performance Summary of Fama-French Style Benchmarks (1979-2004)**\n\n| Benchmark | Sharpe Ratio (SHP) | Z-Stat vs. Market |\n| :--- | :--- | :--- |\n| Wilshire 5000 (Market) | 0.461 | - |\n| FF Small Value (FFSV) | 0.831 | 4.201 |\n\n---\n\n### Question\n\nThe finding that the FF Small Value (FFSV) portfolio has a Sharpe Ratio statistically superior to the market proxy has significant implications for asset pricing theory. Based on the data in **Table 1** and the principles of the Hansen-Jagannathan (HJ) bound, select all valid conclusions.", "Options": {"A": "The result directly contradicts the premise that the market portfolio, as proxied by the Wilshire 5000, is mean-variance efficient.", "B": "The FFSV portfolio must be perfectly correlated with the true Stochastic Discount Factor (SDF).", "C": "The existence of the FFSV portfolio implies that the true Stochastic Discount Factor (SDF) cannot be represented solely as a linear function of the market portfolio's return.", "D": "The FFSV portfolio's high Sharpe Ratio confirms that the single-factor CAPM is a sufficient model for explaining returns in this dataset."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item tests deep theoretical knowledge by connecting an empirical result to fundamental asset pricing theory (HJ bound, SDF). It uses a Reverse-Reasoning strategy, asking for the theoretical implications of an observed outcome. Correct options A and B are the two key steps in the proof that the CAPM is violated. Distractor C is a Conceptual Opposite. Distractor D is a sophisticated 'Almost Right' error; the portfolio that *achieves* the maximum Sharpe ratio is perfectly correlated with the SDF, but FFSV merely proves the market is not that portfolio—we don't know if FFSV itself is.", "qid": "181", "question": "### Background\n\n**Research Question.** How can the risk-adjusted performance of different equity styles be evaluated to test the behavioral proposition that investor return-chasing leads to predictable cycles of outperformance for 'in-favor' styles and underperformance for their 'twin' styles?\n\n**Setting and Data.** The analysis uses monthly returns for four Fama-French (FF) style benchmarks and the Wilshire 5000 market index from January 1979 to December 2004.\n\n---\n\n### Data / Model Specification\n\nThe Hansen-Jagannathan (HJ) bound is a theoretical condition in asset pricing which states that for any asset `i`, its Sharpe Ratio must be less than or equal to the maximum possible Sharpe Ratio in the economy, `SHP_max`. If the Capital Asset Pricing Model (CAPM) holds, the market portfolio is assumed to be mean-variance efficient, implying its Sharpe Ratio is this maximum (`SHP_M = SHP_max`).\n\n**Table 1: Performance Summary of Fama-French Style Benchmarks (1979-2004)**\n\n| Benchmark | Sharpe Ratio (SHP) | Z-Stat vs. Market |\n| :--- | :--- | :--- |\n| Wilshire 5000 (Market) | 0.461 | - |\n| FF Small Value (FFSV) | 0.831 | 4.201 |\n\n---\n\n### Question\n\nThe finding that the FF Small Value (FFSV) portfolio has a Sharpe Ratio statistically superior to the market proxy has significant implications for asset pricing theory. Based on the data in **Table 1** and the principles of the Hansen-Jagannathan (HJ) bound, select all valid conclusions.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze what the data tells us about the Hansen-Jagannathan bound and its implications for asset pricing theory.\n\nKey facts from the data:\n- FFSV has a Sharpe Ratio of 0.831\n- Market (Wilshire 5000) has a Sharpe Ratio of 0.461\n- The difference is statistically significant (Z-stat = 4.201)\n\nThe HJ bound states that no asset can have a Sharpe Ratio exceeding the maximum possible Sharpe Ratio in the economy. If CAPM holds, the market portfolio should have this maximum Sharpe Ratio.\n\nLet me evaluate each option:\n\nA) If the market were mean-variance efficient (as CAPM assumes), it would have the maximum possible Sharpe Ratio. Since FFSV has a statistically higher Sharpe Ratio than the market, this directly contradicts mean-variance efficiency of the market proxy. ✓\n\nB) Perfect correlation with the SDF is not required for high Sharpe Ratios. The HJ bound relates to the volatility of the SDF, not perfect correlation with any specific asset. ✗\n\nC) If the SDF were just a linear function of market returns (as in CAPM), then the market would have the maximum Sharpe Ratio. Since FFSV exceeds the market's Sharpe Ratio, the true SDF must involve other factors beyond just the market return. ✓\n\nD) This is the opposite of what the data shows. The fact that FFSV significantly outperforms the market on a risk-adjusted basis demonstrates that the single-factor CAPM is insufficient - it fails to explain why FFSV can achieve such a high Sharpe Ratio. ✗\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 149, "Question": "### Background\n\nA model of futures pricing is developed where speculator participation is limited by a fixed transaction cost, making the number of traders endogenous. Speculators enter the market until the marginal speculator is indifferent between paying the cost to trade versus not trading at all. This equilibrium condition determines the futures risk premium.\n\n### Data / Model Specification\n\nThe equilibrium percentage futures risk premium (`π`) is determined by the indifference condition of the marginal speculator, leading to the following pricing relation from Proposition 3:\n\n  \n\\pi = \\beta_{\\pi m} \\bar{R}_m \\pm \\sigma_{\\pi} \\sqrt{2\\alpha t(1-\\rho^2)} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `π`: Percentage futures risk premium.\n- `t`: Fixed transaction/setup cost for trading futures.\n- `α`: Coefficient of absolute risk aversion.\n- `β_πm`: Market beta of the percentage futures return.\n- `R̄_m`: Expected net return on the stock market.\n- `σ_π`: Standard deviation of the percentage futures return.\n- `ρ`: Correlation between the futures return `π̃` and the market return `R_m`.\n\n### Question\n\nConsider a scenario where the model's assumption of identical speculators is relaxed, and potential speculators instead have a distribution of setup costs (`t`). Which of the following outcomes would be plausible consequences of this more realistic assumption?\n\nSelect all that apply.", "Options": {"A": "The equilibrium risk premium would become sensitive to the intensity of producer hedging pressure (e.g., the covariance of futures with nonmarketable wealth).", "B": "The supply of speculative services would become perfectly elastic at a positive premium determined by the speculator with the lowest setup cost.", "C": "An increase in producer hedging demand would lead to a higher equilibrium risk premium, as higher-cost speculators would need to be induced to enter the market.", "D": "The equilibrium risk premium would be determined solely by the speculator with the median setup cost, regardless of hedging demand."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to critique and extend the model's core assumptions, specifically by reasoning about the effects of heterogeneous agents. Depth Strategy: Scenario Application. The candidate must apply the logic of the model to a new, more complex scenario (heterogeneous costs) not explicitly modeled in the paper's equations but discussed qualitatively. Distractor Logic: Option B is a Conceptual Opposite; with heterogeneous costs, the supply curve for speculation becomes upward-sloping, not perfectly elastic. Option D is a Conceptual Distractor; the premium is set by the *marginal* entrant, whose identity depends on the level of hedging demand, not a fixed statistical measure like the median. Options A and C are correct and represent the key consequences: an upward-sloping supply of speculation means that higher demand (from more intense hedging) requires a higher price (premium) to attract the next, higher-cost speculator, thus re-establishing the link between hedging intensity and the premium.", "qid": "149", "question": "### Background\n\nA model of futures pricing is developed where speculator participation is limited by a fixed transaction cost, making the number of traders endogenous. Speculators enter the market until the marginal speculator is indifferent between paying the cost to trade versus not trading at all. This equilibrium condition determines the futures risk premium.\n\n### Data / Model Specification\n\nThe equilibrium percentage futures risk premium (`π`) is determined by the indifference condition of the marginal speculator, leading to the following pricing relation from Proposition 3:\n\n  \n\\pi = \\beta_{\\pi m} \\bar{R}_m \\pm \\sigma_{\\pi} \\sqrt{2\\alpha t(1-\\rho^2)} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `π`: Percentage futures risk premium.\n- `t`: Fixed transaction/setup cost for trading futures.\n- `α`: Coefficient of absolute risk aversion.\n- `β_πm`: Market beta of the percentage futures return.\n- `R̄_m`: Expected net return on the stock market.\n- `σ_π`: Standard deviation of the percentage futures return.\n- `ρ`: Correlation between the futures return `π̃` and the market return `R_m`.\n\n### Question\n\nConsider a scenario where the model's assumption of identical speculators is relaxed, and potential speculators instead have a distribution of setup costs (`t`). Which of the following outcomes would be plausible consequences of this more realistic assumption?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze what happens when we move from identical speculators (all with the same setup cost t) to heterogeneous speculators with different setup costs.\n\nIn the original model, the equilibrium risk premium is determined by the marginal speculator's indifference condition. With identical speculators, this creates a perfectly elastic supply of speculative services at the premium level given by equation (1).\n\nWith heterogeneous setup costs:\n\n**Option A**: When speculators have different setup costs, the equilibrium will depend on how many speculators need to enter to clear the market. This depends on the total hedging demand from producers. The hedging demand is directly related to producers' risk exposure (covariance of futures with their nonmarketable wealth). So the risk premium would indeed become sensitive to hedging pressure intensity. ✓\n\n**Option B**: This would only be true if all speculators had the same (lowest) setup cost. With a distribution of costs, the supply curve would be upward sloping, not perfectly elastic. Different speculators enter at different premium levels. ✗\n\n**Option C**: With heterogeneous costs, speculators enter in order from lowest to highest cost. Greater hedging demand requires more speculators to participate, including those with higher setup costs who require higher premiums to compensate. This creates an upward-sloping supply curve. ✓\n\n**Option D**: The equilibrium depends on matching supply with demand, not on any single speculator's cost (median or otherwise). The marginal speculator who clears the market could have any cost level depending on the total hedging demand. ✗\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 126, "Question": "### Background\n\n**Research Question.** How can the stochastic dynamics of an epidemic at the population level be modeled and applied to the pricing of insurance contracts?\n\n**Setting.** A population starts with `S_0` susceptible and `I_0` infected individuals. Each individual's health state is assumed to follow an independent SIR Markov process. This framework is used to price an insurance policy that provides benefits upon infection and removal.\n\n**Variables and Parameters.**\n- `S̃(∞)`: Random variable for the final number of individuals who are never infected.\n- `C = S_0 - S̃(∞)`: Random variable for the total number of individuals who get infected (total claims).\n- `D`: Random variable for the total duration of the epidemic.\n- `P`: The continuous premium rate for an individual insurance policy.\n- `S¹`: Lump sum benefit paid on infection.\n- `H`: Continuous annuity benefit rate paid while infected.\n- `S²`: Lump sum benefit paid on removal.\n\n---\n\n### Data / Model Specification\n\nUnder the assumption of individual independence, the following results hold:\n- The probability that an initially susceptible person is never infected is `p = P^{00}(0,∞) = s(∞)/s(0)`.\n- The probability that an initially susceptible person is removed by time `t` is `P^{02}(0,t)`.\n- The probability that an initially infected person is removed by time `t` is `P^{12}(0,t) = 1 - e^{-αt}`.\n\nActuarial functions `ā^{ij}(0,n)` and `Ā^{ij}(0,n)` represent the expected present values of $1 annuities and $1 lump-sum benefits, respectively, for an `n`-year policy.\n\n---\n\nFor a population with `S_0` susceptible and `I_0` infected individuals at time 0, which of the following statements correctly describe the aggregate stochastic outcomes under the model's independence assumption?", "Options": {"A": "The final number of susceptible individuals `S̃(∞)` is independent of the removal rate `α`.", "B": "The duration `D` is defined as the first time `t` at which the number of infected individuals `Ĩ(t)` becomes zero.", "C": "The total number of individuals who eventually get infected, `C`, follows a distribution with mean `S_0 * (1 - s(∞)/s(0))`.", "D": "The probability that the epidemic duration `D` is less than or equal to `t` is given by `(P^{00}(0,∞) + P^{02}(0,t))^{S_0} * (1 - e^{-αt})^{I_0}`."}, "Answer": ["C", "D"], "pi_justification": "This item assesses understanding of the key population-level stochastic results from the paper (total claims and epidemic duration), which were the focus of Q1 and Q2 in the source QA. It uses a Multiple Choice format to efficiently test two major formulas. Distractor C tests a deeper understanding of the model's parameters, as `s(∞)` implicitly depends on `α`. Distractor D targets a specific, high-frequency misconception about the definition of epidemic duration that the paper explicitly addresses.", "qid": "126", "question": "### Background\n\n**Research Question.** How can the stochastic dynamics of an epidemic at the population level be modeled and applied to the pricing of insurance contracts?\n\n**Setting.** A population starts with `S_0` susceptible and `I_0` infected individuals. Each individual's health state is assumed to follow an independent SIR Markov process. This framework is used to price an insurance policy that provides benefits upon infection and removal.\n\n**Variables and Parameters.**\n- `S̃(∞)`: Random variable for the final number of individuals who are never infected.\n- `C = S_0 - S̃(∞)`: Random variable for the total number of individuals who get infected (total claims).\n- `D`: Random variable for the total duration of the epidemic.\n- `P`: The continuous premium rate for an individual insurance policy.\n- `S¹`: Lump sum benefit paid on infection.\n- `H`: Continuous annuity benefit rate paid while infected.\n- `S²`: Lump sum benefit paid on removal.\n\n---\n\n### Data / Model Specification\n\nUnder the assumption of individual independence, the following results hold:\n- The probability that an initially susceptible person is never infected is `p = P^{00}(0,∞) = s(∞)/s(0)`.\n- The probability that an initially susceptible person is removed by time `t` is `P^{02}(0,t)`.\n- The probability that an initially infected person is removed by time `t` is `P^{12}(0,t) = 1 - e^{-αt}`.\n\nActuarial functions `ā^{ij}(0,n)` and `Ā^{ij}(0,n)` represent the expected present values of $1 annuities and $1 lump-sum benefits, respectively, for an `n`-year policy.\n\n---\n\nFor a population with `S_0` susceptible and `I_0` infected individuals at time 0, which of the following statements correctly describe the aggregate stochastic outcomes under the model's independence assumption?", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) The final number of susceptible individuals `S̃(∞)` is independent of the removal rate `α`.\nThis is FALSE. In the SIR model, the removal rate α affects the epidemic dynamics and thus influences how many susceptibles eventually become infected. The final epidemic size depends on the ratio of transmission to removal rates.\n\nB) The duration `D` is defined as the first time `t` at which the number of infected individuals `Ĩ(t)` becomes zero.\nThis is TRUE. This is the standard definition of epidemic duration - when there are no more infected individuals, the epidemic ends.\n\nC) The total number of individuals who eventually get infected, `C`, follows a distribution with mean `S_0 * (1 - s(∞)/s(0))`.\nThis is TRUE. Given that `p = s(∞)/s(0)` is the probability a susceptible person never gets infected, then `1-p` is the probability they do get infected. With `S_0` independent susceptibles, the expected number infected is `S_0 * (1-p) = S_0 * (1 - s(∞)/s(0))`.\n\nD) The probability that the epidemic duration `D` is less than or equal to `t` is given by `(P^{00}(0,∞) + P^{02}(0,t))^{S_0} * (1 - e^{-αt})^{I_0}`.\nThis is TRUE. For the epidemic to end by time t, all individuals must be in states S or R (not I) by time t. For initially susceptible individuals, they must either never get infected (probability `P^{00}(0,∞)`) or be removed by time t (probability `P^{02}(0,t)`). For initially infected individuals, they must be removed by time t (probability `P^{12}(0,t) = 1 - e^{-αt}`). Under independence, we multiply these probabilities.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 185, "Question": "### Background\n\n**Research Question.** What is the causal effect of institutional fund flows on equity style returns?\n\n**Setting and Data.** A researcher estimates the following model using OLS, where `R_pt` is the return of a style portfolio and `DIFF_t` is a measure of net institutional fund flows:\n\n  \nR_{pt} = a_{i0} + b_{i1}R_{mt} + b_{i2}SMB_t + b_{i3}HML_t + b_{i4}UMD_t + b_{i5}\\mathrm{DIFF}_t + \\upsilon_t \\quad \\text{(Eq. (1))}\n \n\nThere is a concern that `DIFF_t` is endogenous because an unobserved economic variable could drive both fund flows and portfolio returns simultaneously. An instrumental variable (IV) approach is proposed to address this.\n\n---\n\n### Data / Model Specification\n\nAn instrumental variable, `Z_t`, can be used to obtain a consistent estimate of `b_i5` if it meets two critical conditions.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the necessary conditions for a variable `Z_t` to be a valid instrument for `DIFF_t` in **Eq. (1)**.", "Options": {"A": "The instrument `Z_t` must be correlated with the endogenous variable `DIFF_t`, a condition known as 'instrument relevance'.", "B": "The one-month lagged portfolio return, `R_{pt-1}`, would be a valid instrument because it is known to be a strong predictor of fund flows.", "C": "The instrument `Z_t` must be uncorrelated with the error term `υ_t` in Eq. (1), meaning it only affects portfolio returns `R_pt` through its effect on fund flows `DIFF_t`. This is known as the 'exclusion restriction'.", "D": "The primary purpose of using an instrumental variable is to correct for heteroskedasticity in the error term `υ_t`."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item tests foundational knowledge of causal inference and econometrics, specifically the theory of instrumental variables. It uses a Premise/Assumption Packaging strategy, asking for the core conditions of a valid IV. Correct options A and B state the two canonical conditions (relevance and exclusion). Distractor C presents a classic example of an *invalid* instrument that violates the exclusion restriction, making it a high-fidelity distractor for students who only focus on the relevance condition. Distractor D is a Conceptual Error, confusing the purpose of IV (endogeneity) with that of robust standard errors (heteroskedasticity).", "qid": "185", "question": "### Background\n\n**Research Question.** What is the causal effect of institutional fund flows on equity style returns?\n\n**Setting and Data.** A researcher estimates the following model using OLS, where `R_pt` is the return of a style portfolio and `DIFF_t` is a measure of net institutional fund flows:\n\n  \nR_{pt} = a_{i0} + b_{i1}R_{mt} + b_{i2}SMB_t + b_{i3}HML_t + b_{i4}UMD_t + b_{i5}\\mathrm{DIFF}_t + \\upsilon_t \\quad \\text{(Eq. (1))}\n \n\nThere is a concern that `DIFF_t` is endogenous because an unobserved economic variable could drive both fund flows and portfolio returns simultaneously. An instrumental variable (IV) approach is proposed to address this.\n\n---\n\n### Data / Model Specification\n\nAn instrumental variable, `Z_t`, can be used to obtain a consistent estimate of `b_i5` if it meets two critical conditions.\n\n---\n\n### Question\n\nSelect all statements that correctly describe the necessary conditions for a variable `Z_t` to be a valid instrument for `DIFF_t` in **Eq. (1)**.", "model_evaluation_result": {"model_raw_response": "To determine which statements correctly describe the necessary conditions for a valid instrument, I need to recall the two key requirements for instrumental variables:\n\n1. **Relevance**: The instrument must be correlated with the endogenous variable\n2. **Exclusion restriction**: The instrument must be uncorrelated with the error term (i.e., it affects the dependent variable only through the endogenous variable)\n\nLet me evaluate each option:\n\n**Option A**: States that Z_t must be correlated with DIFF_t (instrument relevance). This is correct - it's one of the two fundamental requirements for a valid instrument.\n\n**Option B**: Claims R_{pt-1} would be a valid instrument because it predicts fund flows. However, lagged portfolio returns likely affect current portfolio returns directly (not just through fund flows), violating the exclusion restriction. This is incorrect.\n\n**Option C**: States that Z_t must be uncorrelated with the error term υ_t, meaning it only affects R_pt through DIFF_t (exclusion restriction). This is correct - it's the second fundamental requirement for a valid instrument.\n\n**Option D**: Claims the primary purpose of IV is to correct for heteroskedasticity. This is incorrect. The primary purpose of IV is to address endogeneity (correlation between the regressor and error term), not heteroskedasticity.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 127, "Question": "### Background\n\n**Research Question.** This paper investigates how the market for retired CEOs as corporate directors has evolved, with a particular focus on the structural changes surrounding the Sarbanes-Oxley Act (SOX) of 2002. The core question is whether SOX, by increasing director liability and workload, altered both the supply of and demand for retired CEOs, and changed the criteria by which they are selected for board seats.\n\n**Setting and Sample.** The study analyzes three cohorts of retired CEOs from S&P 500 firms: 1989–1993, 1995–1999 (pre-SOX), and 2001–2005 (post-SOX). The analysis focuses on the number of outside directorships held two years after retirement. SOX is treated as a major regulatory shock that increased the costs (legal liability, workload) of serving as a director.\n\n**Variables and Parameters.**\n- `ROA`: Return on assets (%), a measure of accounting performance.\n- `ASR`: Abnormal stock return (%), the compound average annual return minus the CRSP value-weighted index.\n- `LnAsset`: Natural logarithm of total assets, a proxy for firm size.\n\n---\n\n### Data / Model Specification\n\n**Table 1: CEO and Firm Characteristics**\n\n| | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| Number of observations | 197 | 143 | 151 |\n| Firm size ($ billion in 2005) | 18.14 | 20.99 | 36.36 |\n| Abnormal stock return (ASR) (%)| -5.71 | 0.86 | 11.41 |\n\n**Table 2: Mean Number of Outside Directorships**\n\n| Sample period | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| Mean Outside Directorships | 1.73 | 2.38 | 1.70 |\n| *t-test vs prior period* | | *** | *** |\n\n**Table 3: Ordered Logit Regression for Number of Outside Directorships**\n\n| Variable | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| ASR | 1.040 | 1.383 | -1.968*** |\n| LnAsset | 0.672*** | 0.535*** | 0.195 |\n\n*Note: *** represents 1% significance level. A positive coefficient implies the variable increases the likelihood of holding more directorships.*\n\n---\n\n### Question\n\nBased on the paper's analysis of the post-SOX (2001–2005) period, which of the following statements accurately describe the hypothesized supply-side dynamics in the market for retired CEOs as directors? Select all that apply.", "Options": {"A": "The data show that post-SOX, both high ASR and large firm size (LnAsset) became strong negative predictors of directorships, confirming that the market began penalizing CEOs from successful firms.", "B": "The significant negative relationship between a CEO's prior abnormal stock return (ASR) and subsequent directorships is interpreted as evidence that wealthier CEOs (proxied by high ASR) were less willing to accept the increased liability risks associated with board seats.", "C": "The disappearance of the previously positive effect of firm size (LnAsset) on directorships suggests that a reduced supply from high-profile CEOs, who were also averse to the new risks, offset the reputational benefit of hiring them.", "D": "The negative coefficient on ASR indicates that boards post-SOX viewed high stock performance as a sign of excessive risk-taking, leading them to demand fewer of these CEOs."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize regression results with the paper's core supply-side hypothesis for the post-SOX director market. It uses an **Atomic Decomposition** strategy, breaking the complex argument into two distinct, correct propositions (the wealth/risk aversion channel via ASR and the profile/risk aversion channel via LnAsset). The depth strategy is **Reverse-Reasoning**, requiring the user to infer the economic mechanism from statistical results. Distractors are designed to test for a common demand-side misinterpretation (C) and a factual misreading of the regression results in Table 3 (D, as the LnAsset coefficient became insignificant, not negative).", "qid": "127", "question": "### Background\n\n**Research Question.** This paper investigates how the market for retired CEOs as corporate directors has evolved, with a particular focus on the structural changes surrounding the Sarbanes-Oxley Act (SOX) of 2002. The core question is whether SOX, by increasing director liability and workload, altered both the supply of and demand for retired CEOs, and changed the criteria by which they are selected for board seats.\n\n**Setting and Sample.** The study analyzes three cohorts of retired CEOs from S&P 500 firms: 1989–1993, 1995–1999 (pre-SOX), and 2001–2005 (post-SOX). The analysis focuses on the number of outside directorships held two years after retirement. SOX is treated as a major regulatory shock that increased the costs (legal liability, workload) of serving as a director.\n\n**Variables and Parameters.**\n- `ROA`: Return on assets (%), a measure of accounting performance.\n- `ASR`: Abnormal stock return (%), the compound average annual return minus the CRSP value-weighted index.\n- `LnAsset`: Natural logarithm of total assets, a proxy for firm size.\n\n---\n\n### Data / Model Specification\n\n**Table 1: CEO and Firm Characteristics**\n\n| | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| Number of observations | 197 | 143 | 151 |\n| Firm size ($ billion in 2005) | 18.14 | 20.99 | 36.36 |\n| Abnormal stock return (ASR) (%)| -5.71 | 0.86 | 11.41 |\n\n**Table 2: Mean Number of Outside Directorships**\n\n| Sample period | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| Mean Outside Directorships | 1.73 | 2.38 | 1.70 |\n| *t-test vs prior period* | | *** | *** |\n\n**Table 3: Ordered Logit Regression for Number of Outside Directorships**\n\n| Variable | 1989-1993 | 1995-1999 | 2001-2005 |\n|:---|---:|---:|---:|\n| ASR | 1.040 | 1.383 | -1.968*** |\n| LnAsset | 0.672*** | 0.535*** | 0.195 |\n\n*Note: *** represents 1% significance level. A positive coefficient implies the variable increases the likelihood of holding more directorships.*\n\n---\n\n### Question\n\nBased on the paper's analysis of the post-SOX (2001–2005) period, which of the following statements accurately describe the hypothesized supply-side dynamics in the market for retired CEOs as directors? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data and the question's focus on \"supply-side dynamics,\" I need to identify which statements accurately describe how retired CEOs' willingness to serve as directors changed post-SOX.\n\nLet me analyze each option:\n\n**Option A**: Claims both ASR and LnAsset became \"strong negative predictors\"\n- ASR coefficient: -1.968*** (significant negative)\n- LnAsset coefficient: 0.195 (positive, not significant)\n- This is factually incorrect - LnAsset did not become negative\n\n**Option B**: States the negative ASR relationship reflects wealthier CEOs being less willing to accept liability risks\n- ASR coefficient is indeed -1.968*** (significant negative)\n- This is framed as a supply-side explanation (CEOs' willingness)\n- Consistent with the hypothesis that SOX increased costs/risks\n\n**Option C**: Says the disappearance of LnAsset's positive effect suggests reduced supply from high-profile CEOs\n- Pre-SOX: LnAsset had significant positive effects (0.672***, 0.535***)\n- Post-SOX: LnAsset effect is 0.195 (not significant)\n- This is framed as a supply-side explanation\n- Consistent with high-profile CEOs being less willing to serve\n\n**Option D**: Claims boards viewed high stock performance as excessive risk-taking\n- This is a demand-side explanation (what boards want)\n- The question specifically asks for supply-side dynamics\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 97, "Question": "### Background\n\n**Research Question.** Is the non-linear relationship between foreign bank entry and a country's gross domestic investment robust to the exclusion of potentially influential geographic regions?\n\n**Setting & Data.** A cross-sectional regression analysis for 54 countries. The analysis performs a robustness check by re-estimating the main quadratic model on a subsample that excludes all 13 Latin American countries.\n\n### Data / Model Specification\n\nThe non-linear estimating equation is given by:\n  \nI = A + \\dots + \\alpha_{3}FBSASS + \\alpha_{6}FBSASS^{2} + \\varepsilon \\quad \\text{(Eq. (1))}\n \nwhere `I` is the investment-to-GDP ratio and `FBSASS` is the share of foreign bank assets.\n\n**Table 1. Estimation Results for Non-Linear Model**\n\n| | (1) Baseline | (2) Excluding Latin America |\n|:---|:---:|:---:|\n| **Variable** | **Coefficient (t-stat)** | **Coefficient (t-stat)** |\n| `FBSASS` (α₃) | -14.24 (-2.38) | -19.61 (-3.25) |\n| `FBSASS²` (α₆) | 22.84 (3.46) | 27.89 (4.24) |\n| | | |\n| **Sample** | Full (N=54) | N=41 |\n\n*Notes: The baseline results are from Table 4 of the paper. The results in column (2) are from a robustness test in Table 5.*\n\n### Question\n\nBy comparing the estimation results from the baseline model (column 1) with the model excluding Latin American countries (column 2), select all of the following conclusions that are supported by the data.", "Options": {"A": "The analysis suggests the U-shaped relationship is not robust, as the coefficients and the turning point change substantially after excluding the Latin American subsample.", "B": "The results imply that the Latin American countries in the sample were driving the U-shaped finding; without them, the relationship becomes insignificant.", "C": "The exclusion of Latin American countries reveals a steeper estimated U-curve, as the absolute magnitudes of both the linear and quadratic coefficients increase.", "D": "After excluding Latin American countries, the estimated turning point for `FBSASS` increases from approximately 0.31 to 0.35."}, "Answer": ["C", "D"], "pi_justification": "REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to evaluate the robustness of an empirical finding by comparing results across different samples. It uses a Scenario Application strategy. The correct options (A, B) require both calculation and interpretation of the change in the model's shape, representing an atomic decomposition of the robustness check's conclusion. Distractor C is a Conceptual Opposite, as a stable sign and significance pattern despite coefficient changes is a sign of robustness. Distractor D is also a Conceptual Opposite, as the t-statistics actually increase, indicating the relationship becomes even stronger.", "qid": "97", "question": "### Background\n\n**Research Question.** Is the non-linear relationship between foreign bank entry and a country's gross domestic investment robust to the exclusion of potentially influential geographic regions?\n\n**Setting & Data.** A cross-sectional regression analysis for 54 countries. The analysis performs a robustness check by re-estimating the main quadratic model on a subsample that excludes all 13 Latin American countries.\n\n### Data / Model Specification\n\nThe non-linear estimating equation is given by:\n  \nI = A + \\dots + \\alpha_{3}FBSASS + \\alpha_{6}FBSASS^{2} + \\varepsilon \\quad \\text{(Eq. (1))}\n \nwhere `I` is the investment-to-GDP ratio and `FBSASS` is the share of foreign bank assets.\n\n**Table 1. Estimation Results for Non-Linear Model**\n\n| | (1) Baseline | (2) Excluding Latin America |\n|:---|:---:|:---:|\n| **Variable** | **Coefficient (t-stat)** | **Coefficient (t-stat)** |\n| `FBSASS` (α₃) | -14.24 (-2.38) | -19.61 (-3.25) |\n| `FBSASS²` (α₆) | 22.84 (3.46) | 27.89 (4.24) |\n| | | |\n| **Sample** | Full (N=54) | N=41 |\n\n*Notes: The baseline results are from Table 4 of the paper. The results in column (2) are from a robustness test in Table 5.*\n\n### Question\n\nBy comparing the estimation results from the baseline model (column 1) with the model excluding Latin American countries (column 2), select all of the following conclusions that are supported by the data.", "model_evaluation_result": {"model_raw_response": "Looking at the data in the table:\n\n**Column 1 (Baseline):**\n- FBSASS (α₃): -14.24 (t-stat: -2.38)\n- FBSASS² (α₆): 22.84 (t-stat: 3.46)\n\n**Column 2 (Excluding Latin America):**\n- FBSASS (α₃): -19.61 (t-stat: -3.25)\n- FBSASS² (α₆): 27.89 (t-stat: 4.24)\n\nLet me evaluate each option:\n\n**Option A:** Claims the relationship is not robust due to substantial changes. However, both models show significant coefficients (all t-stats > 2), and the signs remain the same (negative linear, positive quadratic = U-shape). The relationship appears robust, not fragile.\n\n**Option B:** Claims Latin American countries were driving the U-shape and without them it becomes insignificant. This is false - the t-statistics actually increase in magnitude after exclusion (from -2.38 to -3.25 and from 3.46 to 4.24), making the relationship more significant.\n\n**Option C:** Claims the exclusion reveals a steeper U-curve with increased absolute magnitudes. This is correct:\n- |α₃| increases from 14.24 to 19.61\n- |α₆| increases from 22.84 to 27.89\nBoth coefficients increase in absolute magnitude, creating a steeper U-shape.\n\n**Option D:** The turning point is calculated as -α₃/(2α₆):\n- Baseline: -(-14.24)/(2×22.84) = 14.24/45.68 ≈ 0.312\n- Excluding LA: -(-19.61)/(2×27.89) = 19.61/55.78 ≈ 0.352\n\nThe turning point does increase from approximately 0.31 to 0.35.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 62, "Question": "### Background\n\n**Research Question.** What are the relative economic magnitudes of default risk, the risk-mitigating effect of catastrophe (CAT) bonds, and the countervailing effect of basis risk in the valuation of a reinsurance contract?\n\n**Setting.** This analysis uses numerical simulations to quantify the fair premium, or Rate on Line (ROL), for a reinsurance contract under several scenarios. The goal is to isolate and compare the impact of the reinsurer's credit risk, its use of CAT bonds as a hedge, and the imperfection of that hedge due to basis risk.\n\n**Variables and Parameters.**\n- `ROL`: Rate on Line, the fair premium for the reinsurance contract.\n- `Default Risk Premium`: The reduction in ROL due to the reinsurer's credit risk, calculated as `ROL_DefaultFree - ROL_DefaultRisky`.\n- `Basis Risk Premium`: The reduction in ROL due to an imperfect hedge, calculated as `ROL_NoBasisRisk - ROL_WithBasisRisk`.\n- `V/L`: The reinsurer's initial asset-to-liability ratio, a measure of its capital position.\n- `(λ, σ_c)`: The frequency and severity parameters of catastrophe risk.\n- `K`: The trigger level of the CAT bond.\n- `ρ_c`: The correlation between the reinsurer's losses and the CAT bond's trigger index. `ρ_c=1` implies no basis risk.\n\n---\n\n### Data / Model Specification\n\nThe following tables, derived from the paper's numerical analysis, provide the ROL and default risk premiums for a reinsurance contract under different conditions. All values are for a one-year contract with an attachment point `A=20` and cap `M=100`.\n\n**Table 1: Reinsurance Contract Values (ROL) without CAT Bonds**\n| (`λ`, `σ_c`) | Default-Free ROL | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- |\n| (2, 2) | 0.30082 | 0.21425 |\n\n**Table 2: Reinsurance Contract Values (ROL) with CAT Bonds (No Basis Risk, `ρ_c=1`)**\n| (`λ`, `σ_c`) | Trigger `K` | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- |\n| (2, 2) | 80 | 0.22703 |\n\n**Table 3: Reinsurance Contract Values (ROL) with CAT Bonds and Basis Risk**\n| (`λ`, `σ_c`) | Trigger `K` | Correlation `ρ_c` | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- | :--- |\n| (2, 2) | 80 | 0.3 | 0.21774 |\n\n---\n\n### Question\n\nBased on the provided data for a reinsurer with V/L=1.1 and facing high catastrophe risk `(λ, σ_c) = (2, 2)`, which of the following statements are correct?", "Options": {"A": "The default risk premium for the contract *with* a CAT bond (K=80, ρ_c=1.0) is 865.7 basis points.", "B": "The basis risk premium associated with the imperfect hedge (ρ_c = 0.3) is larger than the value increase from issuing a perfect-hedge CAT bond.", "C": "The default risk premium for the reinsurance contract, when no CAT bond is issued, is 865.7 basis points.", "D": "Issuing a CAT bond with a trigger of K=80 (assuming no basis risk) increases the contract's value (ROL) by 127.8 basis points."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the user's ability to perform the core calculations from the paper's numerical analysis and make quantitative comparisons. It uses an Atomic Decomposition strategy, breaking the original multi-part question into discrete, verifiable statements. The strategy is Computational Judgment. Distractor C is a Conceptual Opposite, testing the user's ability to compare the magnitudes of two key effects. Distractor D is a Step-Omission Error, using a value from a previous step (default risk premium without a bond) incorrectly in a new context.", "qid": "62", "question": "### Background\n\n**Research Question.** What are the relative economic magnitudes of default risk, the risk-mitigating effect of catastrophe (CAT) bonds, and the countervailing effect of basis risk in the valuation of a reinsurance contract?\n\n**Setting.** This analysis uses numerical simulations to quantify the fair premium, or Rate on Line (ROL), for a reinsurance contract under several scenarios. The goal is to isolate and compare the impact of the reinsurer's credit risk, its use of CAT bonds as a hedge, and the imperfection of that hedge due to basis risk.\n\n**Variables and Parameters.**\n- `ROL`: Rate on Line, the fair premium for the reinsurance contract.\n- `Default Risk Premium`: The reduction in ROL due to the reinsurer's credit risk, calculated as `ROL_DefaultFree - ROL_DefaultRisky`.\n- `Basis Risk Premium`: The reduction in ROL due to an imperfect hedge, calculated as `ROL_NoBasisRisk - ROL_WithBasisRisk`.\n- `V/L`: The reinsurer's initial asset-to-liability ratio, a measure of its capital position.\n- `(λ, σ_c)`: The frequency and severity parameters of catastrophe risk.\n- `K`: The trigger level of the CAT bond.\n- `ρ_c`: The correlation between the reinsurer's losses and the CAT bond's trigger index. `ρ_c=1` implies no basis risk.\n\n---\n\n### Data / Model Specification\n\nThe following tables, derived from the paper's numerical analysis, provide the ROL and default risk premiums for a reinsurance contract under different conditions. All values are for a one-year contract with an attachment point `A=20` and cap `M=100`.\n\n**Table 1: Reinsurance Contract Values (ROL) without CAT Bonds**\n| (`λ`, `σ_c`) | Default-Free ROL | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- |\n| (2, 2) | 0.30082 | 0.21425 |\n\n**Table 2: Reinsurance Contract Values (ROL) with CAT Bonds (No Basis Risk, `ρ_c=1`)**\n| (`λ`, `σ_c`) | Trigger `K` | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- |\n| (2, 2) | 80 | 0.22703 |\n\n**Table 3: Reinsurance Contract Values (ROL) with CAT Bonds and Basis Risk**\n| (`λ`, `σ_c`) | Trigger `K` | Correlation `ρ_c` | Default-Risky ROL (V/L=1.1) |\n| :--- | :--- | :--- | :--- |\n| (2, 2) | 80 | 0.3 | 0.21774 |\n\n---\n\n### Question\n\nBased on the provided data for a reinsurer with V/L=1.1 and facing high catastrophe risk `(λ, σ_c) = (2, 2)`, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "I need to calculate the various premiums based on the provided data.\n\nGiven data:\n- Default-Free ROL = 0.30082\n- Default-Risky ROL without CAT bond (V/L=1.1) = 0.21425\n- Default-Risky ROL with CAT bond (K=80, ρ_c=1.0) = 0.22703\n- Default-Risky ROL with CAT bond (K=80, ρ_c=0.3) = 0.21774\n\nLet me evaluate each option:\n\n**Option A**: Default risk premium for contract WITH CAT bond (K=80, ρ_c=1.0)\n- Default risk premium = Default-Free ROL - Default-Risky ROL with CAT bond\n- = 0.30082 - 0.22703 = 0.07379 = 737.9 basis points\n- This is NOT 865.7 basis points, so A is incorrect.\n\n**Option B**: Comparing basis risk premium vs. value increase from perfect-hedge CAT bond\n- Basis risk premium = ROL with perfect hedge - ROL with imperfect hedge\n- = 0.22703 - 0.21774 = 0.00929 = 92.9 basis points\n- Value increase from issuing perfect-hedge CAT bond = ROL with CAT bond - ROL without CAT bond\n- = 0.22703 - 0.21425 = 0.01278 = 127.8 basis points\n- Since 92.9 < 127.8, the basis risk premium is NOT larger, so B is incorrect.\n\n**Option C**: Default risk premium when no CAT bond is issued\n- Default risk premium = Default-Free ROL - Default-Risky ROL without CAT bond\n- = 0.30082 - 0.21425 = 0.08657 = 865.7 basis points\n- This is correct.\n\n**Option D**: Value increase from issuing CAT bond with K=80 (no basis risk)\n- Value increase = ROL with CAT bond - ROL without CAT bond\n- = ", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 180, "Question": "### Background\n\n**Research Question.** How can the risk-adjusted performance of different equity styles be evaluated to test the behavioral proposition that investor return-chasing leads to predictable cycles of outperformance for 'in-favor' styles and underperformance for their 'twin' styles?\n\n**Setting and Data.** The analysis uses monthly returns for four Fama-French (FF) style benchmarks (Large Growth, Large Value, Small Growth, Small Value) and the Wilshire 5000 market index from January 1979 to December 2004. Performance is assessed using metrics for both total risk and systematic risk.\n\n---\n\n### Data / Model Specification\n\nPerformance is evaluated using two primary methods:\n\n1.  **Total Risk-Adjusted Performance:** The Sharpe Ratio (`SHP`) measures excess return per unit of total risk (standard deviation). The Jobson and Korkie `Z`-statistic tests the null hypothesis that a portfolio's Sharpe Ratio is equal to the market's Sharpe Ratio.\n\n2.  **Systematic Risk-Adjusted Performance:** A four-factor model is used to measure performance after accounting for exposure to known risk factors. The intercept, `a_i0` (alpha), captures the abnormal return.\n\n  \nR_{it} = a_{i0} + b_{i1}R_{mt} + b_{i2}SMB_t + b_{i3}HML_t + b_{i4}UMD_t + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nThe paper's proposition is that 'in-favor' styles (identified as Large Growth and Small Value) will exhibit superior performance, while their 'twin' styles (Large Value and Small Growth) will underperform.\n\n**Table 1: Performance Summary of Fama-French Style Benchmarks (1979-2004)**\n\n| Benchmark | Annualized Return (%) | Annualized S.D. (%) | Sharpe Ratio (SHP) | Z-Stat vs. Market | Four-Factor Alpha (`a_i0`, %/month) | t-stat for Alpha |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Wilshire 5000 (Market) | 13.59 | 15.64 | 0.461 | - | - | - |\n| FF Large Growth (FFLG) | 13.08 | 16.93 | 0.392 | -1.530 | 0.164 | 3.358 |\n| FF Large Value (FFLV) | 15.25 | 14.71 | 0.601 | 2.462 | -0.147 | -2.294 |\n| FF Small Growth (FFSG) | 9.69 | 24.47 | 0.130 | -5.415 | -0.220 | -3.450 |\n| FF Small Value (FFSV) | 20.18 | 16.70 | 0.831 | 4.201 | 0.091 | 2.286 |\n\n---\n\n### Question\n\nBased on the performance metrics in **Table 1**, select all statements that are supported by the data.", "Options": {"A": "Although the FF Small Value (FFSV) portfolio's Sharpe Ratio was higher than the market's, the Z-statistic indicates this difference was not statistically significant at the 1% level.", "B": "The value premium, measured by the difference in Sharpe Ratios (Value SHP - Growth SHP), was substantially more pronounced in the small-cap segment than in the large-cap segment.", "C": "The pattern of systematic risk-adjusted returns (alphas) strongly supports the paper's performance proposition: 'in-favor' styles (FFLG, FFSV) earned significant positive alphas, while their 'twin' styles (FFLV, FFSG) earned significant negative alphas.", "D": "The FF Large Value (FFLV) portfolio underperformed the market on a systematic risk-adjusted basis (alpha) but outperformed the market on a total risk-adjusted basis (Sharpe Ratio)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item tests the ability to synthesize multiple empirical results from a performance table. It uses an Atomic Decomposition strategy, breaking down the original QA into distinct, verifiable claims about the value premium, alpha patterns, and risk-adjusted returns. Distractor C is a Conceptual Opposite, directly contradicting the high Z-statistic. Option D is a nuanced correct answer that requires comparing two different performance metrics against the market benchmark, making it a good discriminator.", "qid": "180", "question": "### Background\n\n**Research Question.** How can the risk-adjusted performance of different equity styles be evaluated to test the behavioral proposition that investor return-chasing leads to predictable cycles of outperformance for 'in-favor' styles and underperformance for their 'twin' styles?\n\n**Setting and Data.** The analysis uses monthly returns for four Fama-French (FF) style benchmarks (Large Growth, Large Value, Small Growth, Small Value) and the Wilshire 5000 market index from January 1979 to December 2004. Performance is assessed using metrics for both total risk and systematic risk.\n\n---\n\n### Data / Model Specification\n\nPerformance is evaluated using two primary methods:\n\n1.  **Total Risk-Adjusted Performance:** The Sharpe Ratio (`SHP`) measures excess return per unit of total risk (standard deviation). The Jobson and Korkie `Z`-statistic tests the null hypothesis that a portfolio's Sharpe Ratio is equal to the market's Sharpe Ratio.\n\n2.  **Systematic Risk-Adjusted Performance:** A four-factor model is used to measure performance after accounting for exposure to known risk factors. The intercept, `a_i0` (alpha), captures the abnormal return.\n\n  \nR_{it} = a_{i0} + b_{i1}R_{mt} + b_{i2}SMB_t + b_{i3}HML_t + b_{i4}UMD_t + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nThe paper's proposition is that 'in-favor' styles (identified as Large Growth and Small Value) will exhibit superior performance, while their 'twin' styles (Large Value and Small Growth) will underperform.\n\n**Table 1: Performance Summary of Fama-French Style Benchmarks (1979-2004)**\n\n| Benchmark | Annualized Return (%) | Annualized S.D. (%) | Sharpe Ratio (SHP) | Z-Stat vs. Market | Four-Factor Alpha (`a_i0`, %/month) | t-stat for Alpha |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Wilshire 5000 (Market) | 13.59 | 15.64 | 0.461 | - | - | - |\n| FF Large Growth (FFLG) | 13.08 | 16.93 | 0.392 | -1.530 | 0.164 | 3.358 |\n| FF Large Value (FFLV) | 15.25 | 14.71 | 0.601 | 2.462 | -0.147 | -2.294 |\n| FF Small Growth (FFSG) | 9.69 | 24.47 | 0.130 | -5.415 | -0.220 | -3.450 |\n| FF Small Value (FFSV) | 20.18 | 16.70 | 0.831 | 4.201 | 0.091 | 2.286 |\n\n---\n\n### Question\n\nBased on the performance metrics in **Table 1**, select all statements that are supported by the data.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each statement:\n\n**Statement A**: FFSV has a Sharpe Ratio of 0.831 vs market's 0.461, with Z-statistic = 4.201. For a two-tailed test at 1% significance level, the critical value is approximately 2.576. Since 4.201 > 2.576, the difference IS statistically significant at the 1% level. This statement is **FALSE**.\n\n**Statement B**: \n- Small-cap value premium: 0.831 - 0.130 = 0.701 (FFSV - FFSG Sharpe Ratios)\n- Large-cap value premium: 0.601 - 0.392 = 0.209 (FFLV - FFLG Sharpe Ratios)\n- 0.701 is substantially larger than 0.209. This statement is **TRUE**.\n\n**Statement C**: According to the paper's proposition:\n- 'In-favor' styles (FFLG, FFSV) should have positive alphas\n- 'Twin' styles (FFLV, FFSG) should have negative alphas\n\nFrom the data:\n- FFLG: alpha = 0.164% (positive, t-stat = 3.358, significant)\n- FFSV: alpha = 0.091% (positive, t-stat = 2.286, significant)\n- FFLV: alpha = -0.147% (negative, t-stat = -2.294, significant)\n- FFSG: alpha = -0.220% (negative, t-stat = -3.450, significant)\n\nThe pattern perfectly matches the proposition. This statement is **TRUE**.\n\n**Statement D**: \n- FFLV Sharpe Ratio = 0.601 > Market Sharpe Ratio = 0.461 (outperformed on total risk-adjusted basis)\n- FFLV alpha = -0.147% < 0 (underperformed on systematic risk-adjusted basis)\n\nThis statement is **TRUE**.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 29, "Question": "### Background\n\n**Research Question.** How can a hedonic regression model be specified to predict real estate values, and how do its properties and limitations affect its use in a trading rule?\n\n**Setting.** A log-linear hedonic regression is used to estimate the value of apartment buildings. The model's coefficients provide insight into valuation drivers, while its residuals (`ε`) are used to identify potentially mispriced properties. The trading rule's validity rests on the assumption that a negative residual primarily reflects \"transactor error\" (seller mispricing).\n\n**Variables and Parameters.**\n- `ln P`: Natural logarithm of the transaction price.\n- `age`: Age of the structure in years.\n- `Lfar`: Natural logarithm of the gross floor area.\n- `Lfas`: Natural logarithm of the average floor area per suite.\n- `B_1, B_2, B_3`: Regression coefficients.\n- `ε`: Regression residual.\n\n---\n\n### Data / Model Specification\n\nThe hedonic regression model is specified as:\n  \n\\ln P = B_{0} + B_{1} \\cdot \\text{age} + B_{2} \\cdot \\text{Lfar} + B_{3} \\cdot \\text{Lfas} + \\epsilon\n\n\\quad \\text{(Eq. 1)}\n \nThe residual `ε` is assumed to comprise three components: analyst error (model misspecification), transactor error (seller mispricing), and idiosyncratic risk.\n\n**Table 1.** Hedonic Coefficient Estimates for 1979.\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `B_1` (Age) | -0.006 | -6.4 |\n| `B_2` (Lfar) | 0.95 | 34.9 |\n| `B_3` (Lfas) | -0.33 | -3.4 |\n\n---\n\n### Question\n\nBased on the hedonic model specification and the empirical results for 1979 shown in **Table 1**, which of the following statements about the model and its application are correct?", "Options": {"A": "A large negative residual could also be caused by 'analyst error,' such as the model failing to account for a property's undesirable location, leading to a flawed investment signal.", "B": "The negative coefficient on `Lfas` (log of floor area per suite) indicates that for a building of a given size, properties with fewer, larger suites are more valuable.", "C": "The coefficient on `Lfar` (log of floor area) is an elasticity, implying that a 1% increase in a building's size is associated with approximately a 0.95% increase in its price.", "D": "The trading rule's logic assumes that a negative residual (`ε`) is primarily due to 'transactor error,' representing a genuine buying opportunity."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses understanding of the paper's core empirical tool: the hedonic regression. It uses an **Atomic Decomposition** strategy, breaking down the model's interpretation into distinct components: coefficient interpretation (A), core assumption of the trading rule (C), and a key limitation/critique (D). The primary distractor (B) is a **Conceptual Opposite**, testing for careful reading of the coefficient's implication.", "qid": "29", "question": "### Background\n\n**Research Question.** How can a hedonic regression model be specified to predict real estate values, and how do its properties and limitations affect its use in a trading rule?\n\n**Setting.** A log-linear hedonic regression is used to estimate the value of apartment buildings. The model's coefficients provide insight into valuation drivers, while its residuals (`ε`) are used to identify potentially mispriced properties. The trading rule's validity rests on the assumption that a negative residual primarily reflects \"transactor error\" (seller mispricing).\n\n**Variables and Parameters.**\n- `ln P`: Natural logarithm of the transaction price.\n- `age`: Age of the structure in years.\n- `Lfar`: Natural logarithm of the gross floor area.\n- `Lfas`: Natural logarithm of the average floor area per suite.\n- `B_1, B_2, B_3`: Regression coefficients.\n- `ε`: Regression residual.\n\n---\n\n### Data / Model Specification\n\nThe hedonic regression model is specified as:\n  \n\\ln P = B_{0} + B_{1} \\cdot \\text{age} + B_{2} \\cdot \\text{Lfar} + B_{3} \\cdot \\text{Lfas} + \\epsilon\n\n\\quad \\text{(Eq. 1)}\n \nThe residual `ε` is assumed to comprise three components: analyst error (model misspecification), transactor error (seller mispricing), and idiosyncratic risk.\n\n**Table 1.** Hedonic Coefficient Estimates for 1979.\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `B_1` (Age) | -0.006 | -6.4 |\n| `B_2` (Lfar) | 0.95 | 34.9 |\n| `B_3` (Lfas) | -0.33 | -3.4 |\n\n---\n\n### Question\n\nBased on the hedonic model specification and the empirical results for 1979 shown in **Table 1**, which of the following statements about the model and its application are correct?", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) This is correct. The residual ε comprises three components: analyst error, transactor error, and idiosyncratic risk. If the model fails to account for important factors like an undesirable location, this would be captured in the analyst error component of the residual. A large negative residual could therefore be due to model misspecification rather than seller mispricing, leading to a flawed investment signal.\n\nB) This is incorrect. The negative coefficient on Lfas (-0.33) means that as the average floor area per suite increases, the price decreases. For a building of given size (constant Lfar), having larger suites means having fewer suites. So the negative coefficient indicates that properties with more, smaller suites are more valuable, not fewer, larger suites.\n\nC) This is correct. In a log-linear model, the coefficient on a logged independent variable represents an elasticity. Since both ln P and Lfar are in logarithms, the coefficient B₂ = 0.95 means that a 1% increase in floor area is associated with a 0.95% increase in price.\n\nD) This is correct. The passage explicitly states that \"The trading rule's validity rests on the assumption that a negative residual primarily reflects 'transactor error' (seller mispricing).\" This is the fundamental logic behind using negative residuals to identify buying opportunities.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 50, "Question": "### Background\n\nA study examines the impact of economic policy uncertainty (EPU) on corporate board structure using a panel regression framework. The central hypothesis is that boards shift their focus from advising to monitoring when uncertainty rises.\n\n### Data / Model Specification\n\nThe estimated model is of the form:\n`Y_{i,t+1} = α_j + β * EPU_t + Γ'X_{it} + ε_{it}`\nwhere `Y` is a board characteristic, `X` is a vector of controls, and `α` represents industry fixed effects.\n\n**Table 1. Determinants of board structure**\n\n| | (1) | (2) | (4) |\n| :--- | :--- | :--- | :--- |\n| **Dependent variable (t+1)** | **Board size** | **Board independence** | **%Insider** |\n| EPU | -0.083** | 0.095*** | -0.039*** |\n| | (0.008) | (0.005) | (0.004) |\n\n*Note: The paper also finds significant negative coefficients for EPU on `%Outside Executive` and `Outside Busyness`, and a significant positive coefficient on `%Female`.*\n\n### Question\n\nAccording to the paper's theoretical framework and the regression results presented, which of the following board structure changes are considered evidence of an increased focus on **monitoring** in response to a rise in EPU?", "Options": {"A": "An increase in the total number of outside board seats held by independent directors (Outside Busyness).", "B": "An increase in the percentage of independent directors.", "C": "An increase in the percentage of outside directors with executive titles.", "D": "A decrease in the percentage of inside directors."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the user's understanding of how different board characteristics are theoretically linked to the concepts of monitoring and advising, and their ability to connect this theory to the paper's empirical findings. It uses a Reverse-Reasoning strategy. \n- **Correct A:** The paper argues that insiders are primarily advisors. The regression shows a significant negative coefficient for `%Insider`, so a decrease is consistent with a shift away from advising toward monitoring.\n- **Correct C:** Independent directors are considered the primary monitors. The regression shows a significant positive coefficient for `Board Independence`, directly supporting an increased focus on monitoring.\n- **Distractor B:** This is a Conceptual Opposite. The paper argues that less busy directors have more time for monitoring. The regression finds a *negative* coefficient for `Outside Busyness`, meaning a *decrease* in busyness is associated with more monitoring, not an increase.\n- **Distractor D:** This is a Conceptual Opposite. The paper frames outside executives as valuable advisors. The regression finds a *negative* coefficient, indicating a shift *away* from this advising role.", "qid": "50", "question": "### Background\n\nA study examines the impact of economic policy uncertainty (EPU) on corporate board structure using a panel regression framework. The central hypothesis is that boards shift their focus from advising to monitoring when uncertainty rises.\n\n### Data / Model Specification\n\nThe estimated model is of the form:\n`Y_{i,t+1} = α_j + β * EPU_t + Γ'X_{it} + ε_{it}`\nwhere `Y` is a board characteristic, `X` is a vector of controls, and `α` represents industry fixed effects.\n\n**Table 1. Determinants of board structure**\n\n| | (1) | (2) | (4) |\n| :--- | :--- | :--- | :--- |\n| **Dependent variable (t+1)** | **Board size** | **Board independence** | **%Insider** |\n| EPU | -0.083** | 0.095*** | -0.039*** |\n| | (0.008) | (0.005) | (0.004) |\n\n*Note: The paper also finds significant negative coefficients for EPU on `%Outside Executive` and `Outside Busyness`, and a significant positive coefficient on `%Female`.*\n\n### Question\n\nAccording to the paper's theoretical framework and the regression results presented, which of the following board structure changes are considered evidence of an increased focus on **monitoring** in response to a rise in EPU?", "model_evaluation_result": {"model_raw_response": "Looking at the theoretical framework and regression results:\n\nThe paper's hypothesis is that boards shift from advising to monitoring when uncertainty (EPU) rises.\n\nFrom the regression results:\n- EPU has a positive coefficient (0.095***) on Board independence\n- EPU has a negative coefficient (-0.039***) on %Insider\n- EPU has negative coefficients on %Outside Executive and Outside Busyness (per the note)\n\nFor monitoring focus, we would expect:\n- More independent directors (they are better monitors than advisors)\n- Fewer insider directors (insiders are more aligned with management)\n- Fewer busy outside directors (busy directors have less time to monitor)\n- Fewer outside executives (they may have conflicts of interest in monitoring)\n\nEvaluating the options:\nA) Increase in Outside Busyness - NO (EPU has negative coefficient; busy directors are less effective monitors)\nB) Increase in % independent directors - YES (EPU has positive coefficient; independent directors enhance monitoring)\nC) Increase in % outside directors with executive titles - NO (EPU has negative coefficient; outside executives may have conflicts)\nD) Decrease in % inside directors - YES (EPU has negative coefficient on %Insider; fewer insiders enhances monitoring)\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 182, "Question": "### Background\n\n**Research Question.** Do certain equity styles share common long-run stochastic trends (i.e., are they cointegrated), making them redundant for long-term diversification? And if so, is a portfolio constructed of only the independent styles better diversified?\n\n**Setting and Data.** The analysis uses Johansen cointegration tests on Wilshire style indices (WLG, WLV, WSG, WSV). Based on these findings, it constructs two portfolios from Fama-French indices: `INDE` (Large Growth and Small Value) and `4STYLE` (all four styles). The diversification of these portfolios is then tested.\n\n---\n\n### Data / Model Specification\n\n1.  **Cointegration Test:** The Johansen trace statistic tests the null hypothesis of `r` cointegrating relationships. A low p-value for `H_0: r = 0` indicates the presence of cointegration.\n\n2.  **Diversification Test:** The residuals (`ε_it`) from a four-factor model regression on a portfolio's returns are regressed on two style factors: `VAL_t` (Value minus Growth) and `CAP_t` (Small minus Large).\n\n  \n\\varepsilon_{it} = \\psi_{i0} + \\psi_{i1} \\mathrm{VAL}_t + \\psi_{i2} \\mathrm{CAP}_t + \\omega_t \\quad \\text{(Eq. (1))}\n \nA well-diversified portfolio should have insignificant coefficients `ψ_i1` and `ψ_i2`.\n\n**Table 1: Bivariate Cointegration Rank Test of Wilshire Indices**\n\n| Style Pair | Null Hypothesis (`H_0`) | `λ_trace` Statistic | p-value |\n| :--- | :--- | :--- | :--- |\n| WLV and WSG | `r = 0` | 16.03 | 0.04 |\n\n**Table 2: Style Portfolio Diversification Comparison**\n\n| Portfolio | `ψ_i1` (VAL Coeff.) | t-statistic | `ψ_i2` (CAP Coeff.) | t-statistic |\n| :--- | :--- | :--- | :--- | :--- |\n| 4STYLE | 0.011 | 1.994 | 0.019 | 3.082 |\n| INDE | -0.001 | -0.029 | -0.002 | -0.032 |\n\n---\n\n### Question\n\nBased on the cointegration and diversification test results provided, select all statements that accurately describe the paper's findings on style diversification.", "Options": {"A": "The `INDE` portfolio, consisting of only Large Growth and Small Value styles, is shown to be well-diversified, as its four-factor model residuals have no significant remaining exposure to broad value or size factors.", "B": "The cointegration test provides statistically significant evidence (at the 5% level) that the Large Value (WLV) and Small Growth (WSG) styles share a common long-run trend, making them redundant diversifiers.", "C": "The diversification test for the `4STYLE` portfolio suggests that its risk-return profile could be improved by increasing its allocation to large-cap and growth stocks.", "D": "The cointegration between WLV and WSG implies that a portfolio combining only these two styles would have lower total risk (standard deviation) than a portfolio of independent styles."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: This item tests the ability to synthesize findings from two different econometric tests (cointegration and OLS on residuals) to form a cohesive conclusion. It uses an Atomic Decomposition strategy. Correct options A and B are direct interpretations of the two key tables. Distractor C is a Conceptual Opposite, as the positive coefficients in Table 2 for 4STYLE imply a need for more *value* and *small-cap* exposure. Distractor D targets a common misconception about cointegration, confusing long-run co-movement with short-term volatility reduction, making it a high-fidelity distractor.", "qid": "182", "question": "### Background\n\n**Research Question.** Do certain equity styles share common long-run stochastic trends (i.e., are they cointegrated), making them redundant for long-term diversification? And if so, is a portfolio constructed of only the independent styles better diversified?\n\n**Setting and Data.** The analysis uses Johansen cointegration tests on Wilshire style indices (WLG, WLV, WSG, WSV). Based on these findings, it constructs two portfolios from Fama-French indices: `INDE` (Large Growth and Small Value) and `4STYLE` (all four styles). The diversification of these portfolios is then tested.\n\n---\n\n### Data / Model Specification\n\n1.  **Cointegration Test:** The Johansen trace statistic tests the null hypothesis of `r` cointegrating relationships. A low p-value for `H_0: r = 0` indicates the presence of cointegration.\n\n2.  **Diversification Test:** The residuals (`ε_it`) from a four-factor model regression on a portfolio's returns are regressed on two style factors: `VAL_t` (Value minus Growth) and `CAP_t` (Small minus Large).\n\n  \n\\varepsilon_{it} = \\psi_{i0} + \\psi_{i1} \\mathrm{VAL}_t + \\psi_{i2} \\mathrm{CAP}_t + \\omega_t \\quad \\text{(Eq. (1))}\n \nA well-diversified portfolio should have insignificant coefficients `ψ_i1` and `ψ_i2`.\n\n**Table 1: Bivariate Cointegration Rank Test of Wilshire Indices**\n\n| Style Pair | Null Hypothesis (`H_0`) | `λ_trace` Statistic | p-value |\n| :--- | :--- | :--- | :--- |\n| WLV and WSG | `r = 0` | 16.03 | 0.04 |\n\n**Table 2: Style Portfolio Diversification Comparison**\n\n| Portfolio | `ψ_i1` (VAL Coeff.) | t-statistic | `ψ_i2` (CAP Coeff.) | t-statistic |\n| :--- | :--- | :--- | :--- | :--- |\n| 4STYLE | 0.011 | 1.994 | 0.019 | 3.082 |\n| INDE | -0.001 | -0.029 | -0.002 | -0.032 |\n\n---\n\n### Question\n\nBased on the cointegration and diversification test results provided, select all statements that accurately describe the paper's findings on style diversification.", "model_evaluation_result": {"model_raw_response": "Looking at the data provided:\n\n**Table 1 Analysis:**\n- The cointegration test for WLV and WSG shows λ_trace = 16.03 with p-value = 0.04\n- Since p-value < 0.05, we reject H₀: r = 0, indicating there IS cointegration between WLV and WSG\n- This means Large Value and Small Growth share a common long-run trend\n\n**Table 2 Analysis:**\n- INDE portfolio: Both coefficients (ψ₁ = -0.001, ψ₂ = -0.002) have very low t-statistics (-0.029 and -0.032), indicating they are not statistically significant\n- 4STYLE portfolio: ψ₂ = 0.019 with t-statistic = 3.082 is clearly significant (t > 1.96)\n\n**Evaluating each option:**\n\nA) TRUE - INDE portfolio shows insignificant coefficients for both VAL and CAP factors (t-stats well below 1.96), indicating no significant remaining exposure to these factors after the four-factor model adjustment.\n\nB) TRUE - The p-value of 0.04 is less than 0.05, providing statistically significant evidence at the 5% level that WLV and WSG are cointegrated.\n\nC) FALSE - The positive significant coefficient on CAP (0.019) indicates the 4STYLE portfolio has excess exposure to small-cap stocks, not that it needs more large-cap. The positive coefficient means when small stocks outperform large stocks, the portfolio does better than expected.\n\nD) FALSE - Cointegration implies the styles move together in the long run, which would actually increase correlation and reduce diversification benefits, leading to higher (not lower) risk compared to independent styles.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 154, "Question": "### Background\n\n**Research Question.** This case investigates whether an unexpected decline in asset market liquidity was a primary, causal factor in the financial distress of firms that had undergone highly leveraged recapitalizations (recaps). The analysis aims to distinguish between two possibilities: 1) distressed firms were forced to conduct 'fire sales' of assets at unexpectedly low prices due to an illiquid market, or 2) these firms simply sold undesirable assets, leading to poor outcomes.\n\n**Setting / Data-Generating Environment.** The study examines the stock market's reaction to asset sale announcements made by recap firms. A key test partitions the sample based on the selling firm's financial condition (Coverage Ratio) and the health of its industry.\n\n### Data / Model Specification\n\nThe market's perception of asset sales is measured by the 3-day cumulative abnormal return (CAR) around the sale announcement.\n\n**Table 1: Average 3-Day CARs for Asset Sale Announcements**\n| Firm Condition | Industry Condition: Healthy | Industry Condition: Troubled |\n|---|:---:|:---:|\n| **Coverage Ratio > 1** (Financially Healthy) | 0.88% | 0.65% |\n| **Coverage Ratio < 1** (Financially Weak) | 0.59% | -1.82% |\n\n*Source: Adapted from Table 3 of the study. 'Troubled' industries are those where peer firms had negative earnings changes.* \n\n### Question\n\nBased on the data in **Table 1** and the study's logic, which of the following statements are correct interpretations of the evidence?\n", "Options": {"A": "The results indicate that any asset sale conducted within a troubled industry is perceived negatively by the market, regardless of the seller's financial health.", "B": "The large negative CAR (-1.82%) is concentrated in the group of financially weak firms selling assets in troubled industries, supporting the 'fire sale' hypothesis that constrained sellers in illiquid markets receive unexpectedly low prices.", "C": "The positive CAR (0.65%) for financially healthy firms in troubled industries suggests that the market views these firms as strategically divesting non-core assets at favorable prices.", "D": "The positive CAR (0.59%) for financially weak firms in healthy industries contradicts the 'poor asset' hypothesis, which would predict a negative reaction regardless of industry condition."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item tests the ability to interpret a 2x2 difference-in-differences research design used to distinguish between competing hypotheses ('fire sale' vs. 'poor asset').\nStrategy: Atomic Decomposition. The core conclusion is broken down into independently verifiable statements about the table's cells.\nDistractor Logic:\n- C is a Conceptual Opposite; it ignores the positive CAR (0.65%) for healthy firms in troubled industries.\n- D is True but Irrelevant/Misleading. While the CAR is positive, the key insight comes from comparing it to the -1.82% for weak firms in the same industry, which isolates the effect of financial constraint. The option offers a plausible but incomplete interpretation.", "qid": "154", "question": "### Background\n\n**Research Question.** This case investigates whether an unexpected decline in asset market liquidity was a primary, causal factor in the financial distress of firms that had undergone highly leveraged recapitalizations (recaps). The analysis aims to distinguish between two possibilities: 1) distressed firms were forced to conduct 'fire sales' of assets at unexpectedly low prices due to an illiquid market, or 2) these firms simply sold undesirable assets, leading to poor outcomes.\n\n**Setting / Data-Generating Environment.** The study examines the stock market's reaction to asset sale announcements made by recap firms. A key test partitions the sample based on the selling firm's financial condition (Coverage Ratio) and the health of its industry.\n\n### Data / Model Specification\n\nThe market's perception of asset sales is measured by the 3-day cumulative abnormal return (CAR) around the sale announcement.\n\n**Table 1: Average 3-Day CARs for Asset Sale Announcements**\n| Firm Condition | Industry Condition: Healthy | Industry Condition: Troubled |\n|---|:---:|:---:|\n| **Coverage Ratio > 1** (Financially Healthy) | 0.88% | 0.65% |\n| **Coverage Ratio < 1** (Financially Weak) | 0.59% | -1.82% |\n\n*Source: Adapted from Table 3 of the study. 'Troubled' industries are those where peer firms had negative earnings changes.* \n\n### Question\n\nBased on the data in **Table 1** and the study's logic, which of the following statements are correct interpretations of the evidence?\n", "model_evaluation_result": {"model_raw_response": "Looking at Table 1 and the research question, I need to evaluate each statement against the data.\n\n**Statement A**: \"Any asset sale conducted within a troubled industry is perceived negatively by the market\"\n- Healthy firms in troubled industries: +0.65% (positive)\n- Weak firms in troubled industries: -1.82% (negative)\n- This is incorrect because healthy firms show positive returns even in troubled industries.\n\n**Statement B**: \"The large negative CAR (-1.82%) is concentrated in the group of financially weak firms selling assets in troubled industries\"\n- This accurately describes the data pattern\n- The -1.82% is indeed the only negative CAR and occurs specifically for weak firms in troubled industries\n- This supports the fire sale hypothesis where constrained sellers in illiquid markets get poor prices\n- This is correct.\n\n**Statement C**: \"The positive CAR (0.65%) for financially healthy firms in troubled industries suggests strategic divestiture at favorable prices\"\n- The data shows +0.65% for this group\n- However, this is an overinterpretation - a small positive CAR doesn't necessarily mean \"favorable prices\" or strategic success\n- The study's focus is on distinguishing fire sales vs. poor assets, not evaluating strategic divestitures\n- This is incorrect.\n\n**Statement D**: \"The positive CAR (0.59%) for financially weak firms in healthy industries contradicts the 'poor asset' hypothesis\"\n- Under the poor asset hypothesis, weak firms would be selling undesirable assets\n- If true, we'd expect negative market reactions regardless of industry conditions\n- The positive CAR (0.59%) for weak firms in healthy industries does contradict this\n- This is correct.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 45, "Question": "### Background\n\n**Research Question.** How do analysts revise their earnings forecasts for firms whose bonds have been downgraded, and how can the economic magnitude of these revisions be assessed, particularly across firms with different valuation levels?\n\n**Setting / Data-Generating Environment.** The study examines monthly abnormal earnings forecast revisions (`AFR`) for downgraded firms. The cumulative effect is measured by `CAFR`, which sums the `AFR`s from month `t=0` to `t=3`. To translate this price-scaled measure into an intuitive percentage, it is multiplied by the firm's price-to-earnings (`PE`) ratio.\n\n**Variables & Parameters.**\n- `CAFR`: Cumulative abnormal forecast revision over months 0 to 3 (dimensionless, as a percentage).\n- `PE Ratio`: Price-to-earnings ratio.\n- **Group 1, 2, 3:** Categories based on the firm's rating change.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents the cumulative abnormal forecast revisions (`CAFR`) and average `PE` ratios for the downgraded firms themselves, categorized by credit quality group.\n\n**Table 1. Downgraded Firms’ Revisions and Valuations**\n\n| Group | CAFR (0-3 months) | t-statistic | Average PE Ratio |\n| :--- | :--- | :--- | :--- |\n| 1 | -0.71 | (-3.63) | 14.9 |\n| 2 | -2.74 | (-4.16) | 26.4 |\n| 3 | -3.69 | (-4.96) | 26.9 |\n\nThe abnormal percentage change in earnings forecasts is estimated as:\n  \n\\text{Abnormal % Change} = CAFR \\times \\text{PE Ratio} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nUsing the data and methodology presented, evaluate the economic significance of earnings forecast revisions for downgraded firms. Select all of the following statements that are correct.", "Options": {"A": "A potential methodological weakness is that the high PE ratio for Group 3 firms, possibly driven by near-zero earnings, could mechanically inflate the calculated percentage change.", "B": "The calculated abnormal percentage change in earnings forecasts for Group 3 firms is approximately -99.3%.", "C": "The raw CAFR value is a more direct measure of economic significance than the PE-adjusted percentage change because it is not affected by market valuation.", "D": "The abnormal percentage change in earnings forecasts for Group 1 firms is -71%."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item combines 'Computational Judgment' with a test of methodological critique. It requires the user to first perform a calculation based on the provided formula (Option A) and then to identify a key limitation of that same calculation (Option B). This assesses both procedural knowledge and critical thinking. Distractor C targets a common calculation slip (unit error), while Distractor D presents a conceptually flawed argument about the purpose of the transformation.", "qid": "45", "question": "### Background\n\n**Research Question.** How do analysts revise their earnings forecasts for firms whose bonds have been downgraded, and how can the economic magnitude of these revisions be assessed, particularly across firms with different valuation levels?\n\n**Setting / Data-Generating Environment.** The study examines monthly abnormal earnings forecast revisions (`AFR`) for downgraded firms. The cumulative effect is measured by `CAFR`, which sums the `AFR`s from month `t=0` to `t=3`. To translate this price-scaled measure into an intuitive percentage, it is multiplied by the firm's price-to-earnings (`PE`) ratio.\n\n**Variables & Parameters.**\n- `CAFR`: Cumulative abnormal forecast revision over months 0 to 3 (dimensionless, as a percentage).\n- `PE Ratio`: Price-to-earnings ratio.\n- **Group 1, 2, 3:** Categories based on the firm's rating change.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents the cumulative abnormal forecast revisions (`CAFR`) and average `PE` ratios for the downgraded firms themselves, categorized by credit quality group.\n\n**Table 1. Downgraded Firms’ Revisions and Valuations**\n\n| Group | CAFR (0-3 months) | t-statistic | Average PE Ratio |\n| :--- | :--- | :--- | :--- |\n| 1 | -0.71 | (-3.63) | 14.9 |\n| 2 | -2.74 | (-4.16) | 26.4 |\n| 3 | -3.69 | (-4.96) | 26.9 |\n\nThe abnormal percentage change in earnings forecasts is estimated as:\n  \n\\text{Abnormal % Change} = CAFR \\times \\text{PE Ratio} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nUsing the data and methodology presented, evaluate the economic significance of earnings forecast revisions for downgraded firms. Select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to evaluate each statement using the data from Table 1 and Equation (1).\n\nLet me calculate the abnormal percentage changes using Equation (1):\n- Group 1: CAFR × PE Ratio = -0.71 × 14.9 = -10.58%\n- Group 2: CAFR × PE Ratio = -2.74 × 26.4 = -72.34%\n- Group 3: CAFR × PE Ratio = -3.69 × 26.9 = -99.26%\n\nNow evaluating each option:\n\n**Option A**: This correctly identifies a methodological issue. When earnings are near zero, PE ratios become very large (price divided by near-zero earnings). This can artificially inflate the calculated percentage change when multiplied by CAFR. Group 3's high PE ratio of 26.9 could indeed be driven by this effect.\n\n**Option B**: My calculation shows -99.26% for Group 3, which rounds to approximately -99.3%. This is correct.\n\n**Option C**: This is incorrect. CAFR is a price-scaled measure (cumulative abnormal forecast revision), not a direct percentage. The PE adjustment is necessary to convert it to an interpretable percentage change in earnings forecasts. Without this adjustment, CAFR lacks clear economic interpretation.\n\n**Option D**: This is incorrect. The calculation for Group 1 yields -10.58%, not -71%. The statement appears to confuse the CAFR value (-0.71) with the calculated percentage change.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 155, "Question": "### Background\n\n**Research Question.** This case investigates whether an unexpected decline in asset market liquidity was a primary, causal factor in the financial distress of firms that had undergone highly leveraged recapitalizations (recaps).\n\n**Setting / Data-Generating Environment.** For the nine firms that ultimately entered financial distress, the study estimates the total cash shortfall from asset sales and compares it to the cash required to avoid default in the year of distress.\n\n### Data / Model Specification\n\n- The **Total Asset Sale Shortfall (`S`)** is the difference between the expected and actual proceeds from asset sales, estimated from market reactions and press reports.\n- The **Additional Cash Required (`C_req,d`)** is the firm's debt obligations minus its net cash flow for the year.\n\n**Table 1: Comparison of Asset Sale Shortfall and Cash Required to Avoid Default ($ millions)**\n| Firm Name | Total Asset Sale Shortfall (`S`) | Additional Cash Required (`C_req,d`) |\n|---|:---:|:---:|\n| Carter Hawley Hale | $612.6 | $38.6 |\n| Goodyear | $489.1 | $139.5 |\n| Harcourt Brace Jovanovich | $924.9 | $243.6 |\n| Holiday | $35.2 | $-33.7 |\n| Interco | $288.5 | $223.1 |\n| Quantum Chemical | $140.7 | $164.4 |\n| Standard Brands Paint | $0.0 | $44.6 |\n| Swank | $0.0 | $8.3 |\n| USG | $54.5 | $351.0 |\n\n*Source: Adapted from Table 4 of the study.* \n\n### Question\n\nAccording to the data in **Table 1**, for which of the following firms was the total asset sale shortfall greater than or equal to the additional cash required to avoid default, providing direct quantitative evidence that asset illiquidity could have been the cause of their distress?\n", "Options": {"A": "USG", "B": "Quantum Chemical", "C": "Carter Hawley Hale", "D": "Interco"}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: This item tests the ability to perform a quantitative comparison to verify a central causal claim of the paper: that the magnitude of the asset sale shortfall was sufficient to cause default.\nStrategy: Domain Enumeration. The user must apply a specific condition (S >= C_req,d) to the dataset and select all firms that satisfy it.\nDistractor Logic:\n- C (Quantum Chemical) is a high-fidelity 'Almost Right' distractor. The paper notes its shortfall was ~85% of the cash needed, making it a very close but incorrect choice.\n- D (USG) is a clear Conceptual Opposite, where the cash required far exceeded the shortfall, making it an easy case to rule out if the core concept is understood.", "qid": "155", "question": "### Background\n\n**Research Question.** This case investigates whether an unexpected decline in asset market liquidity was a primary, causal factor in the financial distress of firms that had undergone highly leveraged recapitalizations (recaps).\n\n**Setting / Data-Generating Environment.** For the nine firms that ultimately entered financial distress, the study estimates the total cash shortfall from asset sales and compares it to the cash required to avoid default in the year of distress.\n\n### Data / Model Specification\n\n- The **Total Asset Sale Shortfall (`S`)** is the difference between the expected and actual proceeds from asset sales, estimated from market reactions and press reports.\n- The **Additional Cash Required (`C_req,d`)** is the firm's debt obligations minus its net cash flow for the year.\n\n**Table 1: Comparison of Asset Sale Shortfall and Cash Required to Avoid Default ($ millions)**\n| Firm Name | Total Asset Sale Shortfall (`S`) | Additional Cash Required (`C_req,d`) |\n|---|:---:|:---:|\n| Carter Hawley Hale | $612.6 | $38.6 |\n| Goodyear | $489.1 | $139.5 |\n| Harcourt Brace Jovanovich | $924.9 | $243.6 |\n| Holiday | $35.2 | $-33.7 |\n| Interco | $288.5 | $223.1 |\n| Quantum Chemical | $140.7 | $164.4 |\n| Standard Brands Paint | $0.0 | $44.6 |\n| Swank | $0.0 | $8.3 |\n| USG | $54.5 | $351.0 |\n\n*Source: Adapted from Table 4 of the study.* \n\n### Question\n\nAccording to the data in **Table 1**, for which of the following firms was the total asset sale shortfall greater than or equal to the additional cash required to avoid default, providing direct quantitative evidence that asset illiquidity could have been the cause of their distress?\n", "model_evaluation_result": {"model_raw_response": "I need to compare the Total Asset Sale Shortfall (S) with the Additional Cash Required (C_req,d) for each firm listed in the options.\n\nFor a firm's asset illiquidity to be a potential cause of distress, the asset sale shortfall should be greater than or equal to the additional cash required (S ≥ C_req,d).\n\nLet me check each option:\n\nA) USG: S = $54.5 million, C_req,d = $351.0 million\n   $54.5 < $351.0, so NO\n\nB) Quantum Chemical: S = $140.7 million, C_req,d = $164.4 million\n   $140.7 < $164.4, so NO\n\nC) Carter Hawley Hale: S = $612.6 million, C_req,d = $38.6 million\n   $612.6 > $38.6, so YES\n\nD) Interco: S = $288.5 million, C_req,d = $223.1 million\n   $288.5 > $223.1, so YES\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 4, "Question": "### Background\n\n**Research Question.** This case critically evaluates whether the reported returns for different anomaly strategies under different weather states are consistent with the absence of arbitrage, a cornerstone of financial theory.\n\n**Setting / Data-Generating Environment.** Consider a simplified two-state economy where the state is either 'Hot' (`H`) or 'Cold' (`L`), occurring with equal probability (`P(H) = P(L) = 0.5`). The no-arbitrage principle implies the existence of a positive Stochastic Discount Factor (SDF), `m = [m_H, m_L]`, that prices all excess returns, meaning `E[m R^e] = 0.5 m_H E[R^e|H] + 0.5 m_L E[R^e|L] = 0` for any strategy.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Conditional Monthly Excess Returns (%)**\n\n| Strategy | `E[R^e|H]` (Return in Hot state) | `E[R^e|L]` (Return in Cold state) |\n| :--- | :--- | :--- |\n| Size | -0.60 | 1.05 |\n| Return on Assets (ROA) | 1.62 | -0.05 |\n\n---\n\n### Question\n\nBased on the no-arbitrage condition and the data in Table 1, select all of the following conclusions that are correct.", "Options": {"A": "It is impossible to find a single, non-trivial SDF that prices both the Size and ROA strategies simultaneously, implying an arbitrage opportunity exists.", "B": "The ratio of SDF states (`m_H / m_L`) required to price the Return on Assets (ROA) strategy is -32.4.", "C": "The ratio of SDF states (`m_H / m_L`) required to price the Size strategy is 1.75.", "D": "A valid SDF exists where `m_H = 1.0` and `m_L = 1.0`, because the unconditional expected returns of both strategies are positive."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Computational Judgment strategy to test the high-difficulty arbitrage question from the source QA. It requires students to set up and solve the SDF pricing equation for each asset and then compare the results. Option A is a correct calculation for one asset. Option B is the correct final conclusion that the system is inconsistent. Distractor C is a Sign Error in the calculation for the ROA strategy. Distractor D represents a fundamental Conceptual Error, ignoring the conditional information and the core pricing equation.", "qid": "4", "question": "### Background\n\n**Research Question.** This case critically evaluates whether the reported returns for different anomaly strategies under different weather states are consistent with the absence of arbitrage, a cornerstone of financial theory.\n\n**Setting / Data-Generating Environment.** Consider a simplified two-state economy where the state is either 'Hot' (`H`) or 'Cold' (`L`), occurring with equal probability (`P(H) = P(L) = 0.5`). The no-arbitrage principle implies the existence of a positive Stochastic Discount Factor (SDF), `m = [m_H, m_L]`, that prices all excess returns, meaning `E[m R^e] = 0.5 m_H E[R^e|H] + 0.5 m_L E[R^e|L] = 0` for any strategy.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Conditional Monthly Excess Returns (%)**\n\n| Strategy | `E[R^e|H]` (Return in Hot state) | `E[R^e|L]` (Return in Cold state) |\n| :--- | :--- | :--- |\n| Size | -0.60 | 1.05 |\n| Return on Assets (ROA) | 1.62 | -0.05 |\n\n---\n\n### Question\n\nBased on the no-arbitrage condition and the data in Table 1, select all of the following conclusions that are correct.", "model_evaluation_result": {"model_raw_response": "I need to analyze the no-arbitrage conditions for the given strategies.\n\nGiven information:\n- Two states: Hot (H) and Cold (L), each with probability 0.5\n- No-arbitrage condition: E[m R^e] = 0.5 m_H E[R^e|H] + 0.5 m_L E[R^e|L] = 0\n\nFor the Size strategy:\n- E[R^e|H] = -0.60%\n- E[R^e|L] = 1.05%\n\nFor the ROA strategy:\n- E[R^e|H] = 1.62%\n- E[R^e|L] = -0.05%\n\nLet me check each option:\n\n**Option A**: To check if a single SDF can price both strategies:\nFor Size: 0.5 m_H (-0.60) + 0.5 m_L (1.05) = 0\nThis gives: -0.60 m_H + 1.05 m_L = 0\nTherefore: m_H/m_L = 1.05/0.60 = 1.75\n\nFor ROA: 0.5 m_H (1.62) + 0.5 m_L (-0.05) = 0\nThis gives: 1.62 m_H - 0.05 m_L = 0\nTherefore: m_H/m_L = 0.05/1.62 ≈ 0.0309\n\nSince the required ratios are different (1.75 ≠ 0.0309), no single SDF can price both strategies. This is correct.\n\n**Option B**: The ratio m_H/m_L for ROA strategy:\nm_H/m_L = 0.05/1.62 ≈ 0.0309, not -32.4. This is incorrect.\n\n**Option C**: The ratio m_H/m_L for Size strategy:\nm_H/m_L = 1.05/0.60 = 1.75. This is correct.\n\n**Option D**: If m_H", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 123, "Question": "### Background\n\n**Research Question.** This case examines the economic channels that explain the cross-sectional variation in stock price reactions to initial public debt offerings (debt IPOs).\n\n**Setting.** The study moves beyond the average negative announcement return to a multivariate OLS regression framework. The dependent variable is the two-day stock excess return (`SER`). The analysis tests several core hypotheses from corporate finance theory.\n\n**Variables & Parameters.**\n*   `SER`: Two-day stock excess return (dependent variable).\n*   `Age`: ln(1 + years since stock IPO), proxy for reputation/low information asymmetry.\n*   `Maturity`: Years to maturity of the bond, proxy for negative signal about growth options.\n*   `Creditchg`: Dummy = 1 if bank loan commitment increases, proxy for continued bank monitoring.\n*   `Mkt/bk`: Market-to-book ratio, proxy for growth opportunities.\n\n---\n\n### Data / Model Specification\n\nThe event study found an average `SER` of -0.86% over two days. To understand what drives this effect, the following cross-sectional regression is estimated:\n\n  \nSER_i = \\beta_0 + \\beta_1 \\text{Age}_i + \\beta_2 \\text{Maturity}_i + \\beta_3 \\text{Creditchg}_i + \\beta_4 \\text{Mkt/bk}_i + \\text{Controls}_i + \\epsilon_i \n \n\nKey hypotheses and their predictions for the coefficients:\n*   **Hypothesis 1B (Monitoring):** Reducing bank debt is bad news. Predicts `β₃ > 0`.\n*   **Hypothesis 2 (Maturity Signal):** Longer maturity is bad news. Predicts `β₂ < 0`.\n*   **Hypothesis 3 (Reputation):** Older firms are penalized less. Predicts `β₁ > 0`.\n*   **Hypothesis 4 (Growth/Flexibility):** High-growth firms benefit more. Predicts `β₄ > 0`.\n\nEstimation results for a representative model are presented in Table 1.\n\n**Table 1: OLS Regression Explaining Announcement Period SER**\n\n| Independent Variable | Coefficient | p-value |\n| :--- | :--- | :--- |\n| Intercept | -0.004 | (0.74) |\n| Age | 0.004 | (0.04) |\n| Maturity | -0.001 | (0.07) |\n| Creditchg | 0.006 | (0.10) |\n| Mkt/bk | 0.004 | (0.05) |\n\n---\n\nBased on the regression results in Table 1, which of the following statements are valid interpretations or conclusions? Select all that apply.", "Options": {"A": "The results are consistent with the theory that longer debt maturity sends a negative signal to the market, leading to a more negative stock price reaction.", "B": "The empirical evidence supports the hypothesis that firms with stronger growth prospects (higher `Mkt/bk`) experience a less adverse stock price reaction to a debt IPO.", "C": "The model predicts that a firm with `Age`=ln(11), `Maturity`=10, `Creditchg`=1, and `Mkt/bk`=2.0 would have a predicted SER (ignoring the intercept) of +1.36%.", "D": "The evidence refutes the hypothesis that maintaining a relationship with a monitoring bank (as proxied by `Creditchg`) mitigates the negative announcement effect."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to interpret multiple regression coefficients in the context of specific economic hypotheses and to perform a multi-step calculation to predict an outcome. Depth Strategy: Computational Judgment. Distractor Logic: Option A is a correct interpretation of the positive and significant coefficient on `Mkt/bk`. Option B is a correct interpretation of the negative and significant coefficient on `Maturity`. Option C requires a correct calculation: (0.004 * ln(11)) + (-0.001 * 10) + (0.006 * 1) + (0.004 * 2.0) = (0.004 * 2.4) - 0.01 + 0.006 + 0.008 = 0.0096 - 0.01 + 0.014 = 0.0136 or +1.36%, so it is correct. Option D is a Conceptual Opposite; the positive coefficient on `Creditchg` supports, not refutes, the hypothesis.", "qid": "123", "question": "### Background\n\n**Research Question.** This case examines the economic channels that explain the cross-sectional variation in stock price reactions to initial public debt offerings (debt IPOs).\n\n**Setting.** The study moves beyond the average negative announcement return to a multivariate OLS regression framework. The dependent variable is the two-day stock excess return (`SER`). The analysis tests several core hypotheses from corporate finance theory.\n\n**Variables & Parameters.**\n*   `SER`: Two-day stock excess return (dependent variable).\n*   `Age`: ln(1 + years since stock IPO), proxy for reputation/low information asymmetry.\n*   `Maturity`: Years to maturity of the bond, proxy for negative signal about growth options.\n*   `Creditchg`: Dummy = 1 if bank loan commitment increases, proxy for continued bank monitoring.\n*   `Mkt/bk`: Market-to-book ratio, proxy for growth opportunities.\n\n---\n\n### Data / Model Specification\n\nThe event study found an average `SER` of -0.86% over two days. To understand what drives this effect, the following cross-sectional regression is estimated:\n\n  \nSER_i = \\beta_0 + \\beta_1 \\text{Age}_i + \\beta_2 \\text{Maturity}_i + \\beta_3 \\text{Creditchg}_i + \\beta_4 \\text{Mkt/bk}_i + \\text{Controls}_i + \\epsilon_i \n \n\nKey hypotheses and their predictions for the coefficients:\n*   **Hypothesis 1B (Monitoring):** Reducing bank debt is bad news. Predicts `β₃ > 0`.\n*   **Hypothesis 2 (Maturity Signal):** Longer maturity is bad news. Predicts `β₂ < 0`.\n*   **Hypothesis 3 (Reputation):** Older firms are penalized less. Predicts `β₁ > 0`.\n*   **Hypothesis 4 (Growth/Flexibility):** High-growth firms benefit more. Predicts `β₄ > 0`.\n\nEstimation results for a representative model are presented in Table 1.\n\n**Table 1: OLS Regression Explaining Announcement Period SER**\n\n| Independent Variable | Coefficient | p-value |\n| :--- | :--- | :--- |\n| Intercept | -0.004 | (0.74) |\n| Age | 0.004 | (0.04) |\n| Maturity | -0.001 | (0.07) |\n| Creditchg | 0.006 | (0.10) |\n| Mkt/bk | 0.004 | (0.05) |\n\n---\n\nBased on the regression results in Table 1, which of the following statements are valid interpretations or conclusions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results and hypotheses:\n\n**Hypothesis 1B (Monitoring):** Predicts β₃ > 0\n- Result: β₃ = 0.006 (p = 0.10)\n- Positive coefficient as predicted, marginally significant\n\n**Hypothesis 2 (Maturity Signal):** Predicts β₂ < 0  \n- Result: β₂ = -0.001 (p = 0.07)\n- Negative coefficient as predicted, marginally significant\n\n**Hypothesis 3 (Reputation):** Predicts β₁ > 0\n- Result: β₁ = 0.004 (p = 0.04)  \n- Positive coefficient as predicted, statistically significant\n\n**Hypothesis 4 (Growth/Flexibility):** Predicts β₄ > 0\n- Result: β₄ = 0.004 (p = 0.05)\n- Positive coefficient as predicted, statistically significant\n\nEvaluating each option:\n\n**A)** The coefficient on Maturity is -0.001 (p = 0.07), which is negative and marginally significant. This supports the theory that longer maturity signals bad news. ✓\n\n**B)** The coefficient on Mkt/bk is 0.004 (p = 0.05), which is positive and significant. This means higher growth firms have less negative reactions. ✓\n\n**C)** Calculating predicted SER (ignoring intercept):\n- Age contribution: 0.004 × ln(11) = 0.004 × 2.398 = 0.0096\n- Maturity contribution: -0.001 × 10 = -0.010\n- Creditchg contribution: 0.006 × 1 = 0.006\n- Mkt/bk contribution: 0.004 × 2.0 = 0.008\n- Total: 0.0096 - 0.010 + 0.006 + 0.008 = 0.0136 = 1.36% ✓\n\n**D)** The coefficient on Creditchg is", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 114, "Question": "### Background\n\nA model of entrepreneurial finance examines how government credit guarantees affect entrepreneurial effort in a market with moral hazard. An entrepreneur with insufficient collateral seeks financing for a project. The interaction between the entrepreneur's incentive compatibility (IC) and the bank's participation constraint (PC) determines the equilibrium contract and effort level.\n\n### Data / Model Specification\n\nThe model identifies a critical threshold for the guarantee strength, `φ`, defined as:\n\n  \n\\hat{\\phi}(Q) \\equiv 1 - \\frac{A}{I-Q} \n \nwhere `A` is the entrepreneur's collateral and `I-Q` is the loan amount.\n\n- If `φ < hat(φ)(Q)`, the entrepreneur pledges full collateral (`β=1`) and effort `e` is *increasing* in `φ`.\n- If `φ ≥ hat(φ)(Q)`, the entrepreneur pledges partial collateral (`β < 1`) and effort `e` is *decreasing* in `φ`.\n\nCredit rationing occurs if `φ` is below a minimum level `underline(φ)`.\n\n### Question\n\nAccording to the model, which of the following statements accurately describe the mechanisms driving credit rationing and the effects of credit guarantees?\n\nSelect all that apply.", "Options": {"A": "For low levels of guarantee (`φ < hat(φ)(Q)`), an increase in `φ` allows the bank to offer better terms (a lower `R`), which strengthens the entrepreneur's incentive to exert effort.", "B": "For high levels of guarantee (`φ ≥ hat(φ)(Q)`), the guarantee is so effective that the entrepreneur substitutes this 'public collateral' for their own private collateral, which weakens their personal incentive to succeed and causes effort to fall.", "C": "Entrepreneurial effort is monotonically increasing in the guarantee level `φ`, as a stronger guarantee always reduces the entrepreneur's cost of capital.", "D": "Credit rationing occurs because a bank's demand for a high success-state repayment `R` (to cover its risk from low collateral) reduces the entrepreneur's incentive to exert effort, making it impossible to satisfy both parties' constraints simultaneously."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the student's understanding of the paper's core theoretical mechanism: the cause of credit rationing and the non-monotonic effect of guarantees on entrepreneurial effort. Depth Strategy: Reverse-Reasoning. The student must identify the multiple valid causal explanations for the model's key outcomes. Distractor Logic: Option D is a conceptual opposite, contradicting the central non-monotonic finding of the paper. It represents a plausible but incorrect simplification of the model's results. The correct options (A, B, C) each describe a distinct and correct part of the model's logic presented in the original QA problem.", "qid": "114", "question": "### Background\n\nA model of entrepreneurial finance examines how government credit guarantees affect entrepreneurial effort in a market with moral hazard. An entrepreneur with insufficient collateral seeks financing for a project. The interaction between the entrepreneur's incentive compatibility (IC) and the bank's participation constraint (PC) determines the equilibrium contract and effort level.\n\n### Data / Model Specification\n\nThe model identifies a critical threshold for the guarantee strength, `φ`, defined as:\n\n  \n\\hat{\\phi}(Q) \\equiv 1 - \\frac{A}{I-Q} \n \nwhere `A` is the entrepreneur's collateral and `I-Q` is the loan amount.\n\n- If `φ < hat(φ)(Q)`, the entrepreneur pledges full collateral (`β=1`) and effort `e` is *increasing* in `φ`.\n- If `φ ≥ hat(φ)(Q)`, the entrepreneur pledges partial collateral (`β < 1`) and effort `e` is *decreasing* in `φ`.\n\nCredit rationing occurs if `φ` is below a minimum level `underline(φ)`.\n\n### Question\n\nAccording to the model, which of the following statements accurately describe the mechanisms driving credit rationing and the effects of credit guarantees?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this model of entrepreneurial finance with credit guarantees and moral hazard, I need to analyze each statement against the model's mechanics.\n\nThe model has a critical threshold φ̂(Q) = 1 - A/(I-Q) that determines two regimes:\n- Below threshold (φ < φ̂(Q)): Full collateral pledged (β=1), effort increasing in φ\n- Above threshold (φ ≥ φ̂(Q)): Partial collateral pledged (β<1), effort decreasing in φ\n\nLet me evaluate each option:\n\n**Option A**: \"For low levels of guarantee (φ < φ̂(Q)), an increase in φ allows the bank to offer better terms (a lower R), which strengthens the entrepreneur's incentive to exert effort.\"\n\nThis correctly describes the mechanism in the low guarantee regime. When guarantees are modest, they reduce the bank's risk exposure, allowing lower interest rates while the entrepreneur maintains full collateral commitment. This improves incentives and increases effort.\n\n**Option B**: \"For high levels of guarantee (φ ≥ φ̂(Q)), the guarantee is so effective that the entrepreneur substitutes this 'public collateral' for their own private collateral, which weakens their personal incentive to succeed and causes effort to fall.\"\n\nThis accurately captures the moral hazard problem in the high guarantee regime. Strong government guarantees crowd out private collateral (β < 1), reducing the entrepreneur's \"skin in the game\" and weakening effort incentives.\n\n**Option C**: \"Entrepreneurial effort is monotonically increasing in the guarantee level φ...\"\n\nThis is false. The model explicitly shows effort is non-monotonic - increasing for φ < φ̂(Q) but decreasing for φ ≥ φ̂(Q).\n\n**Option D**: \"Credit rationing occurs because a bank's demand for a high success-state repayment R (to cover its risk from low collateral) reduces the entrepreneur's incentive to exert effort, making it impossible to satisfy both parties' constraints simultaneously.\"\n\nThis correctly identifies the credit rationing mechanism. With insufficient collateral and no guarantees, the bank needs high R to break even, but high R discourages entrepreneurial effort, creating a conflict between the", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 130, "Question": "### Background\n\nThe study estimates a four-way ANOVA model to assess the impact of a stock's 5, 11, 23, and 35-month winner/loser status on its monthly abnormal volume. The model is estimated separately for different periods of the year to detect seasonal patterns driven by tax motives.\n\n### Data / Model Specification\n\nThe estimated model is:\n  \nAV_{i} = \\alpha + \\beta_{1}D5_{i} + \\beta_{2}D11_{i} + \\beta_{3}D23_{i} + \\beta_{4}D35_{i} + e_{i} \\quad \\text{(Eq. 1)}\n \nwhere `AV_i` is the abnormal volume for stock `i` and `D5_i`, `D11_i`, etc., are binary variables equal to 1 if the stock was a 'winner' over the corresponding prior period (e.g., 5 months), and 0 otherwise. The coefficient `α` represents the abnormal volume for a stock that was a loser across all four periods.\n\nThe following table, derived from Table VI in the paper, presents a selection of estimated coefficients for NYSE stocks for three distinct periods: the baseline (Feb-Oct), tax-loss selling season (Dec), and tax-deferral season (Jan).\n\n**Table 1: Selected ANOVA Coefficients for NYSE Stocks**\n| Period | `α` | `β_1` (5-month) | `β_2` (11-month) |\n| :--- | :--- | :--- | :--- |\n| Feb-Oct | -0.50 | 0.28 | 0.41 |\n| Dec | -0.09 | 0.16 | 0.08 |\n| Jan | -0.55 | 0.48 | 0.46 |\n\n---\n\nBased on the model and data provided, which of the following statements are correct interpretations of the results?\n\nSelect all that apply.", "Options": {"A": "The predicted abnormal volume for a stock that was a 5-month winner but an 11-month loser in December is 0.07%.", "B": "In January, the abnormal volume for a stock that was a loser across all four periods is predicted to be -0.55%, which is lower than the abnormal volume for a similar consistent loser during the Feb-Oct period.", "C": "The combined impact of recent gains (5-month and 11-month) on abnormal volume is substantially dampened in December compared to the Feb-Oct baseline, providing evidence for a tax-induced 'lock-in' effect for winners.", "D": "In the baseline Feb-Oct period, the combined marginal impact on abnormal volume for a stock that is a winner over both the 5-month and 11-month horizons is 0.69%."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret coefficients from a multi-variable regression model, perform calculations based on those coefficients, and connect the numerical results to the paper's core hypotheses about seasonal tax effects ('lock-in' and 'present value' motives). Depth Strategy: Computational Judgment. Candidates must calculate predicted values and differences between periods to evaluate the options. Distractor Logic: All options require careful calculation and interpretation. A is a direct calculation (0.28 + 0.41 = 0.69). B requires comparing the sum for Dec (0.16+0.08=0.24) to the sum for Feb-Oct (0.69) and interpreting the large drop. C requires comparing the 'α' coefficients for Jan (-0.55) and Feb-Oct (-0.50). D is a calculation for a specific scenario in December: α + β1 = -0.09 + 0.16 = 0.07. All are correct, testing comprehensive understanding.", "qid": "130", "question": "### Background\n\nThe study estimates a four-way ANOVA model to assess the impact of a stock's 5, 11, 23, and 35-month winner/loser status on its monthly abnormal volume. The model is estimated separately for different periods of the year to detect seasonal patterns driven by tax motives.\n\n### Data / Model Specification\n\nThe estimated model is:\n  \nAV_{i} = \\alpha + \\beta_{1}D5_{i} + \\beta_{2}D11_{i} + \\beta_{3}D23_{i} + \\beta_{4}D35_{i} + e_{i} \\quad \\text{(Eq. 1)}\n \nwhere `AV_i` is the abnormal volume for stock `i` and `D5_i`, `D11_i`, etc., are binary variables equal to 1 if the stock was a 'winner' over the corresponding prior period (e.g., 5 months), and 0 otherwise. The coefficient `α` represents the abnormal volume for a stock that was a loser across all four periods.\n\nThe following table, derived from Table VI in the paper, presents a selection of estimated coefficients for NYSE stocks for three distinct periods: the baseline (Feb-Oct), tax-loss selling season (Dec), and tax-deferral season (Jan).\n\n**Table 1: Selected ANOVA Coefficients for NYSE Stocks**\n| Period | `α` | `β_1` (5-month) | `β_2` (11-month) |\n| :--- | :--- | :--- | :--- |\n| Feb-Oct | -0.50 | 0.28 | 0.41 |\n| Dec | -0.09 | 0.16 | 0.08 |\n| Jan | -0.55 | 0.48 | 0.46 |\n\n---\n\nBased on the model and data provided, which of the following statements are correct interpretations of the results?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this ANOVA model, I need to evaluate each statement based on the provided coefficients.\n\nLet me work through each option:\n\n**Option A**: For a stock that was a 5-month winner (D5=1) but 11-month loser (D11=0) in December:\nAV = α + β₁(1) + β₂(0) = -0.09 + 0.16(1) + 0.08(0) = -0.09 + 0.16 = 0.07%\n✓ This is correct.\n\n**Option B**: In January, a consistent loser (all D variables = 0) has:\nAV = α = -0.55%\nIn Feb-Oct, a consistent loser has:\nAV = α = -0.50%\nSince -0.55% < -0.50%, the January value is indeed lower.\n✓ This is correct.\n\n**Option C**: Looking at the combined impact of 5-month and 11-month winner status:\n- Feb-Oct: β₁ + β₂ = 0.28 + 0.41 = 0.69\n- December: β₁ + β₂ = 0.16 + 0.08 = 0.24\n\nThe combined impact drops from 0.69 to 0.24 in December, which is a substantial dampening. This is consistent with a 'lock-in' effect where investors avoid selling winners in December to defer capital gains taxes.\n✓ This is correct.\n\n**Option D**: For the Feb-Oct period, the combined marginal impact for both 5-month and 11-month winners:\nβ₁ + β₂ = 0.28 + 0.41 = 0.69%\n✓ This is correct.\n\nAll four statements are correct interpretations of the results.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 169, "Question": "### Background\n\n**Research Question.** What are the dynamic effects of a shock to housing market tightness on market liquidity and prices, and do these empirical effects align with predictions from search and matching theory?\n\n**Setting.** The analysis uses a panel VAR model estimated on quarterly data for 403 Dutch municipalities from 2011-2013. The model examines the interrelationships between market tightness, liquidity, and prices.\n\n**Variables & Parameters.**\n- `Δpr`: Quarterly change in the log house price index.\n- `Δros`: Quarterly change in the rate of sale (liquidity).\n- `Δcph`: Quarterly change in the log clicks per house (market tightness).\n- `t-1`: Subscript denoting a time lag of one quarter.\n\n**Theoretical Prediction.** Search and matching models with information asymmetries predict that a positive shock to market tightness causes an immediate increase in liquidity (matching probability), followed by a more gradual increase in prices as sellers slowly update their reservation prices.\n\n### Data / Model Specification\n\nThe core empirical findings are derived from a panel VAR model. Table 1 below presents a selection of key coefficient estimates, and the text summarizes the impulse-response function (IRF) analysis.\n\n**Table 1. Selected Panel VAR Regression Results**\n\n| Dependent Variable | Regressor | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- |\n| `Δpr_t` | `Δcph_{t-1}` | 0.0036 | (2.6) |\n| `Δros_t` | `Δcph_{t-1}` | 0.0031 | (1.9) |\n\n*Notes: The model includes two lags of all variables and fixed effects. Coefficients are estimated using GMM.*\n\n**Impulse-Response Function Summary:**\nA one-standard-deviation shock to `Δcph` (an increase of 4.6%) leads to:\n- A temporary increase in the rate of sale (`ros`) of approximately 0.1 percentage points after one quarter, which reverts to the pre-shock level by the second quarter.\n- A gradual and permanent increase in the price level (`pr`) of roughly 0.4%.\n\n### Question\n\nBased on the provided regression results and impulse-response function summary, which of the following statements about the impact of a market tightness shock (`Δcph`) are supported by the evidence? (Select all that apply)", "Options": {"A": "The effect of lagged market tightness (`Δcph_{t-1}`) on house price growth (`Δpr_t`) is positive and statistically significant at the 5% level.", "B": "The impulse-response analysis indicates that a one-standard-deviation shock to market tightness results in a permanent increase in the house price level.", "C": "The impulse-response analysis suggests that the impact of a market tightness shock on market liquidity (`ros`) is both gradual and permanent.", "D": "The effect of lagged market tightness (`Δcph_{t-1}`) on the change in the rate of sale (`Δros_t`) is statistically significant at the 1% level."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret regression coefficients and impulse-response function summaries. It uses an **Atomic Decomposition** strategy, breaking down the findings into distinct, verifiable statements. Correct options (A, B) accurately reflect the statistical significance from the table and the permanent nature of the price shock from the IRF summary. Distractors are based on common errors: (C) misstates the significance level for the liquidity effect (t=1.9 is significant at 10%, not 1%), and (D) incorrectly describes the liquidity response, which is immediate and temporary, not gradual and permanent.", "qid": "169", "question": "### Background\n\n**Research Question.** What are the dynamic effects of a shock to housing market tightness on market liquidity and prices, and do these empirical effects align with predictions from search and matching theory?\n\n**Setting.** The analysis uses a panel VAR model estimated on quarterly data for 403 Dutch municipalities from 2011-2013. The model examines the interrelationships between market tightness, liquidity, and prices.\n\n**Variables & Parameters.**\n- `Δpr`: Quarterly change in the log house price index.\n- `Δros`: Quarterly change in the rate of sale (liquidity).\n- `Δcph`: Quarterly change in the log clicks per house (market tightness).\n- `t-1`: Subscript denoting a time lag of one quarter.\n\n**Theoretical Prediction.** Search and matching models with information asymmetries predict that a positive shock to market tightness causes an immediate increase in liquidity (matching probability), followed by a more gradual increase in prices as sellers slowly update their reservation prices.\n\n### Data / Model Specification\n\nThe core empirical findings are derived from a panel VAR model. Table 1 below presents a selection of key coefficient estimates, and the text summarizes the impulse-response function (IRF) analysis.\n\n**Table 1. Selected Panel VAR Regression Results**\n\n| Dependent Variable | Regressor | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- |\n| `Δpr_t` | `Δcph_{t-1}` | 0.0036 | (2.6) |\n| `Δros_t` | `Δcph_{t-1}` | 0.0031 | (1.9) |\n\n*Notes: The model includes two lags of all variables and fixed effects. Coefficients are estimated using GMM.*\n\n**Impulse-Response Function Summary:**\nA one-standard-deviation shock to `Δcph` (an increase of 4.6%) leads to:\n- A temporary increase in the rate of sale (`ros`) of approximately 0.1 percentage points after one quarter, which reverts to the pre-shock level by the second quarter.\n- A gradual and permanent increase in the price level (`pr`) of roughly 0.4%.\n\n### Question\n\nBased on the provided regression results and impulse-response function summary, which of the following statements about the impact of a market tightness shock (`Δcph`) are supported by the evidence? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at the evidence provided:\n\n**For Option A:** The coefficient of `Δcph_{t-1}` on `Δpr_t` is 0.0036 with a t-statistic of 2.6. Since |2.6| > 1.96 (the critical value for 5% significance level), this effect is statistically significant at the 5% level. ✓\n\n**For Option B:** The impulse-response function summary explicitly states that a one-standard-deviation shock to `Δcph` leads to \"a gradual and permanent increase in the price level (`pr`) of roughly 0.4%.\" ✓\n\n**For Option C:** The impulse-response function summary states that the shock leads to \"a temporary increase in the rate of sale (`ros`)... which reverts to the pre-shock level by the second quarter.\" This indicates the effect is temporary, not permanent. ✗\n\n**For Option D:** The coefficient of `Δcph_{t-1}` on `Δros_t` has a t-statistic of 1.9. Since |1.9| < 2.58 (the critical value for 1% significance level), this effect is not statistically significant at the 1% level. ✗\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 76, "Question": "### Background\n\nAn insurer is evaluating three alternative reinsurance strategies for its crop insurance portfolio. The performance of each strategy is assessed using an Asset-Liability Management (ALM) framework, simulated 5,000 times over a 30-year horizon.\n\n**Reinsurance Models:**\n- **Model A (Joint Self-Managed Pool):** Retains all risks internally.\n- **Model B (Group Private Reinsurance):** Cedes all risks to private reinsurers.\n- **Model C (Optimized Portfolio):** Strategically retains a lower-risk group and cedes a higher-risk group.\n\nA regulator imposes a new solvency requirement: the reinsurance pool must hold sufficient initial capital (U₀) to ensure that the 5th percentile of its surplus distribution at year 30 is non-negative (≥ $0).\n\n### Data / Model Specification\n\nThe performance of the three models is evaluated based on simulations assuming zero initial surplus (U₀ = $0). Table 1 presents the 5th percentile surplus from these simulations.\n\n**Table 1: 5th Percentile Surplus at End of Year 30 (U₀ = $0)**\n\n| Model | 5th Percentile Surplus |\n| :--- | :--- |\n| Model A | -$112,903,478 |\n| Model B | -$976,132,222 |\n| Model C | -$552,079,233 |\n\n### Question\n\nBased on the regulatory requirement and the data in Table 1, which of the following conclusions are valid? Select all that apply.", "Options": {"A": "Model C requires more than four times the initial capital of Model A to satisfy the regulatory constraint.", "B": "The required initial capital for Model C is $552,079,233.", "C": "Model B is the least capital-efficient, requiring an initial capital of $976,132,222.", "D": "Model A is the most capital-efficient, requiring the lowest initial capital ($112,903,478) to meet the solvency standard."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "This item assesses the ability to apply a novel constraint (a regulatory rule) to existing data to derive new insights. The depth strategy is 'Scenario Application' combined with 'Computational Judgment.' The user must first calculate the required capital for each model by taking the absolute value of the 5th percentile surplus. Then, they must evaluate four statements based on these calculations, including direct values and relative comparisons. All options are designed to be correct to test for comprehensive calculation and analysis, ensuring the user performs all required steps and comparisons accurately.", "qid": "76", "question": "### Background\n\nAn insurer is evaluating three alternative reinsurance strategies for its crop insurance portfolio. The performance of each strategy is assessed using an Asset-Liability Management (ALM) framework, simulated 5,000 times over a 30-year horizon.\n\n**Reinsurance Models:**\n- **Model A (Joint Self-Managed Pool):** Retains all risks internally.\n- **Model B (Group Private Reinsurance):** Cedes all risks to private reinsurers.\n- **Model C (Optimized Portfolio):** Strategically retains a lower-risk group and cedes a higher-risk group.\n\nA regulator imposes a new solvency requirement: the reinsurance pool must hold sufficient initial capital (U₀) to ensure that the 5th percentile of its surplus distribution at year 30 is non-negative (≥ $0).\n\n### Data / Model Specification\n\nThe performance of the three models is evaluated based on simulations assuming zero initial surplus (U₀ = $0). Table 1 presents the 5th percentile surplus from these simulations.\n\n**Table 1: 5th Percentile Surplus at End of Year 30 (U₀ = $0)**\n\n| Model | 5th Percentile Surplus |\n| :--- | :--- |\n| Model A | -$112,903,478 |\n| Model B | -$976,132,222 |\n| Model C | -$552,079,233 |\n\n### Question\n\nBased on the regulatory requirement and the data in Table 1, which of the following conclusions are valid? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine the initial capital required for each model to meet the regulatory requirement.\n\nThe regulatory requirement states that the 5th percentile of surplus at year 30 must be ≥ $0.\n\nGiven data shows 5th percentile surplus with U₀ = $0:\n- Model A: -$112,903,478\n- Model B: -$976,132,222  \n- Model C: -$552,079,233\n\nSince surplus = initial capital + accumulated results over 30 years, and the accumulated results don't depend on initial capital, adding initial capital U₀ shifts the entire surplus distribution by U₀.\n\nTherefore, to make the 5th percentile = $0:\n- Model A needs U₀ = $112,903,478\n- Model B needs U₀ = $976,132,222\n- Model C needs U₀ = $552,079,233\n\nChecking each option:\n\nA) Model C requires more than 4× the capital of Model A?\n   $552,079,233 ÷ $112,903,478 = 4.89 > 4 ✓\n\nB) Required initial capital for Model C is $552,079,233? ✓\n\nC) Model B is least capital-efficient, requiring $976,132,222? ✓\n\nD) Model A is most capital-efficient, requiring $112,903,478? ✓\n\nAll statements are correct.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 56, "Question": "### Background\n\n**Research Question.** What are the aggregate and distributional consequences of imposing interest rate ceilings on the market for motor vehicle loans?\n\n**Setting / Data-Generating Environment.** This analysis uses the parameters from a consistently estimated hedonic model of the auto loan market to simulate the market-wide effects of different, counterfactual rate ceiling policies. The simulations calculate the average probability that a potential borrower will be rejected (rationed out of the market) and the average interest rate paid by those who successfully obtain a loan under each policy regime.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Simulated Aggregate Impact of Rate Ceilings**\n\n| Ceiling Level | Avg. Rejection Prob. (`ρ̂`) | Avg. Rate for Approved Loans (`θ̂^*`) |\n|:---|---:|---:|\n| No Ceiling | 0.0% | 18.5% |\n| 21% | 30.7% | 13.2% |\n| 16% | 50.6% | 11.1% |\n| 12% | 71.5% | 9.0% |\n\n**Table 2: Simulated Impact on Specific Borrower Groups**\n\n| Borrower Group | Avg. Equilibrium Rate (No Ceiling), `θ̂` | Avg. Rejection Prob. (Ceiling = 12%), `ρ̂` |\n|:---|---:|---:|\n| **Expenditure ($)** | |\n| 0-10,000 | 22.1% | 82.6% |\n| 10,000-18,000 | 17.2% | 68.2% |\n| 18,000 or more | 16.2% | 61.7% |\n| **Household Type** | |\n| Single Male | 24.2% | 86.8% |\n| Single w/children | 26.1% | 90.7% |\n| Couple w/children | 17.3% | 68.4% |\n\n---\n\n### Question\n\nBased on the simulation results in Table 1 and Table 2, which of the following statements are correct interpretations of the impact of interest rate ceilings?", "Options": {"A": "Tightening the ceiling from 21% to 16% increases the average rejection probability by approximately 20 percentage points while reducing the average interest rate for approved loans by approximately 2 percentage points.", "B": "According to Table 2, households with expenditures over $18,000 have a higher unconditional equilibrium rate than couple w/children households, and are therefore less likely to be rejected under a 12% ceiling.", "C": "Under a restrictive 12% ceiling, borrower groups with the highest unconditional equilibrium rates (e.g., single-parent households) experience the highest probabilities of being rationed out of the market.", "D": "The results suggest that as rate ceilings are lowered, the average interest rate for approved loans equalizes across groups primarily because lenders start offering more uniform pricing to all borrowers."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret and synthesize quantitative results from simulation tables. It uses an Atomic Decomposition strategy, breaking down the original QA's analysis of aggregate and distributional effects into distinct, verifiable statements. Option A is a Computational Judgment based on Table 1. Option B requires synthesizing information across columns in Table 2 to understand the mechanism of rationing. Distractor C presents a plausible but incorrect mechanism (behavioral change vs. selection effect), a key conceptual error. Distractor D is an 'Almost Right' distractor that tests for careful reading and avoidance of hasty generalization (the premise is true, but the conclusion is false: 61.7% vs 68.4%).", "qid": "56", "question": "### Background\n\n**Research Question.** What are the aggregate and distributional consequences of imposing interest rate ceilings on the market for motor vehicle loans?\n\n**Setting / Data-Generating Environment.** This analysis uses the parameters from a consistently estimated hedonic model of the auto loan market to simulate the market-wide effects of different, counterfactual rate ceiling policies. The simulations calculate the average probability that a potential borrower will be rejected (rationed out of the market) and the average interest rate paid by those who successfully obtain a loan under each policy regime.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Simulated Aggregate Impact of Rate Ceilings**\n\n| Ceiling Level | Avg. Rejection Prob. (`ρ̂`) | Avg. Rate for Approved Loans (`θ̂^*`) |\n|:---|---:|---:|\n| No Ceiling | 0.0% | 18.5% |\n| 21% | 30.7% | 13.2% |\n| 16% | 50.6% | 11.1% |\n| 12% | 71.5% | 9.0% |\n\n**Table 2: Simulated Impact on Specific Borrower Groups**\n\n| Borrower Group | Avg. Equilibrium Rate (No Ceiling), `θ̂` | Avg. Rejection Prob. (Ceiling = 12%), `ρ̂` |\n|:---|---:|---:|\n| **Expenditure ($)** | |\n| 0-10,000 | 22.1% | 82.6% |\n| 10,000-18,000 | 17.2% | 68.2% |\n| 18,000 or more | 16.2% | 61.7% |\n| **Household Type** | |\n| Single Male | 24.2% | 86.8% |\n| Single w/children | 26.1% | 90.7% |\n| Couple w/children | 17.3% | 68.4% |\n\n---\n\n### Question\n\nBased on the simulation results in Table 1 and Table 2, which of the following statements are correct interpretations of the impact of interest rate ceilings?", "model_evaluation_result": {"model_raw_response": "Looking at the data in the tables, I need to evaluate each statement:\n\n**Statement A**: \"Tightening the ceiling from 21% to 16% increases the average rejection probability by approximately 20 percentage points while reducing the average interest rate for approved loans by approximately 2 percentage points.\"\n\nFrom Table 1:\n- At 21% ceiling: rejection probability = 30.7%, average rate = 13.2%\n- At 16% ceiling: rejection probability = 50.6%, average rate = 11.1%\n- Change in rejection probability: 50.6% - 30.7% = 19.9% ≈ 20 percentage points ✓\n- Change in average rate: 13.2% - 11.1% = 2.1% ≈ 2 percentage points ✓\n\nStatement A is correct.\n\n**Statement B**: \"According to Table 2, households with expenditures over $18,000 have a higher unconditional equilibrium rate than couple w/children households, and are therefore less likely to be rejected under a 12% ceiling.\"\n\nFrom Table 2:\n- Expenditure $18,000+: equilibrium rate = 16.2%, rejection probability at 12% ceiling = 61.7%\n- Couple w/children: equilibrium rate = 17.3%, rejection probability at 12% ceiling = 68.4%\n- 16.2% < 17.3%, so the first part is false (they have a LOWER rate, not higher)\n\nStatement B is incorrect.\n\n**Statement C**: \"Under a restrictive 12% ceiling, borrower groups with the highest unconditional equilibrium rates (e.g., single-parent households) experience the highest probabilities of being rationed out of the market.\"\n\nFrom Table 2:\n- Single w/children: equilibrium rate = 26.1%, rejection probability = 90.7% (highest)\n- Single Male: equilibrium rate = 24.2%, rejection probability = 86.8%\n- Couple w/children: equilibrium rate = 17.3%, rejection probability = 68.4%\n- The pattern shows higher equilibrium rates correspond to higher rejection probabilities ✓\n\nStatement C is correct.\n\n**", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 103, "Question": "### Background\n\n**Research Question.** This case investigates the apparent contradiction between the large positive market reaction to Reverse Takeover (RT) announcements and the poor long-term performance and high failure rate of the surviving firms.\n\n**Setting.** The analysis contrasts the short-term announcement returns (CARs) with the medium-term survival and performance of the newly combined public entities. The sample consists of 121 RTs from 1987-2001. A key puzzle is why the market reacts positively to events that so often lead to failure.\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return at announcement (dimensionless, as %).\n- `Survival_i`: A binary indicator, =1 if firm `i` survives for 2 years, 0 otherwise.\n- `Prior_Distress_i`: Dummy variable; =1 if the public vehicle was in financial distress, 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nThe data present a sharp contrast between the market's initial reaction and subsequent firm survival. A logistic regression is used to model the probability of survival based on pre-merger characteristics.\n\n**Table 1: Post-Reverse Takeover Survival Outcomes (Full Sample, N=121)**\n\n| Outcome | Number of Firms | Percent of Total Sample |\n| :--- | :--- | :--- |\n| **Survival Status** | | |\n| Survived 2 Years | 56 | 46.3% |\n| Did Not Survive 2 Years | 65 | 53.7% |\n| **Details for Non-Surviving Firms** | | |\n| Delisted/moved to Bulletin Board | 33 | 27.3% |\n| Bankruptcy | 24 | 19.8% |\n| Acquired | 25 | 20.7% |\n\n*Note: Table adapted from Panels A and E of Table 9 in the source.*\n\n**Table 2: Announcement CARs by Post-Takeover Survival Outcome (All Firms)**\n\n| Outcome | No. | CAR (-1,+1) | CAR (0) |\n| :--- | :-: | :--- | :--- |\n| Survived | 56 | 18.37%*** | 3.18%*** |\n| Failure | 65 | 29.58%*** | 16.28%*** |\n\n*Note: Table adapted from Panel B of Table 10 in the source. `***` denotes significance at the 1% level.*\n\n**Table 3: Logistic Regression Modeling the Probability of Survival**\n\n| Variable | Model 1 Coefficient (Wald Chi-sq) |\n| :--- | :--- |\n| Intercept | 1.512 (5.38)** |\n| Prior Distress | -1.158 (6.70)*** |\n| Log of Public Firm Size | -0.077 (0.36) |\n| Cash to Total Assets | -1.656 (3.20)* |\n\n*Note: Table adapted from Model 1 of Table 11 in the source. `*`, `**`, `***` denote significance at the 10%, 5%, and 1% levels, respectively.*\n\n---\n\n### Question\n\nBased on the provided data, which of the following conclusions about Reverse Takeovers (RTs) are factually supported? Select all that apply.", "Options": {"A": "The market's initial reaction, measured by 3-day CARs, was significantly more positive for firms that ultimately failed than for those that survived.", "B": "The two-year survival rate for firms created via RT is less than 50%.", "C": "The announcement-day CAR (CAR (0)) was statistically insignificant for firms that ultimately survived.", "D": "Among firms that did not survive for two years, bankruptcy was the most frequent outcome."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test the ability to accurately interpret key quantitative findings from multiple tables regarding RT outcomes and market reactions. It assesses core data comprehension. Distractors are designed based on common misreadings: C (Step-Omission Error) tests for careful comparison within a category, and D (Conceptual Error) tests for correct interpretation of statistical significance symbols.", "qid": "103", "question": "### Background\n\n**Research Question.** This case investigates the apparent contradiction between the large positive market reaction to Reverse Takeover (RT) announcements and the poor long-term performance and high failure rate of the surviving firms.\n\n**Setting.** The analysis contrasts the short-term announcement returns (CARs) with the medium-term survival and performance of the newly combined public entities. The sample consists of 121 RTs from 1987-2001. A key puzzle is why the market reacts positively to events that so often lead to failure.\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return at announcement (dimensionless, as %).\n- `Survival_i`: A binary indicator, =1 if firm `i` survives for 2 years, 0 otherwise.\n- `Prior_Distress_i`: Dummy variable; =1 if the public vehicle was in financial distress, 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nThe data present a sharp contrast between the market's initial reaction and subsequent firm survival. A logistic regression is used to model the probability of survival based on pre-merger characteristics.\n\n**Table 1: Post-Reverse Takeover Survival Outcomes (Full Sample, N=121)**\n\n| Outcome | Number of Firms | Percent of Total Sample |\n| :--- | :--- | :--- |\n| **Survival Status** | | |\n| Survived 2 Years | 56 | 46.3% |\n| Did Not Survive 2 Years | 65 | 53.7% |\n| **Details for Non-Surviving Firms** | | |\n| Delisted/moved to Bulletin Board | 33 | 27.3% |\n| Bankruptcy | 24 | 19.8% |\n| Acquired | 25 | 20.7% |\n\n*Note: Table adapted from Panels A and E of Table 9 in the source.*\n\n**Table 2: Announcement CARs by Post-Takeover Survival Outcome (All Firms)**\n\n| Outcome | No. | CAR (-1,+1) | CAR (0) |\n| :--- | :-: | :--- | :--- |\n| Survived | 56 | 18.37%*** | 3.18%*** |\n| Failure | 65 | 29.58%*** | 16.28%*** |\n\n*Note: Table adapted from Panel B of Table 10 in the source. `***` denotes significance at the 1% level.*\n\n**Table 3: Logistic Regression Modeling the Probability of Survival**\n\n| Variable | Model 1 Coefficient (Wald Chi-sq) |\n| :--- | :--- |\n| Intercept | 1.512 (5.38)** |\n| Prior Distress | -1.158 (6.70)*** |\n| Log of Public Firm Size | -0.077 (0.36) |\n| Cash to Total Assets | -1.656 (3.20)* |\n\n*Note: Table adapted from Model 1 of Table 11 in the source. `*`, `**`, `***` denote significance at the 10%, 5%, and 1% levels, respectively.*\n\n---\n\n### Question\n\nBased on the provided data, which of the following conclusions about Reverse Takeovers (RTs) are factually supported? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data provided:\n\n**Option A:** Table 2 shows that failed firms had CAR (-1,+1) of 29.58% while surviving firms had 18.37%. The failed firms indeed had higher CARs, supporting this statement.\n\n**Option B:** Table 1 shows that 56 firms survived 2 years out of 121 total, which is 46.3%. This is less than 50%, supporting this statement.\n\n**Option C:** Table 2 shows CAR (0) for survived firms was 3.18%***. The three asterisks indicate significance at the 1% level, meaning it was statistically significant, not insignificant. This statement is false.\n\n**Option D:** Table 1 shows among the 65 non-surviving firms:\n- Delisted/moved to Bulletin Board: 33 firms (most frequent)\n- Bankruptcy: 24 firms\n- Acquired: 25 firms\n\nDelisting was the most frequent outcome, not bankruptcy. This statement is false.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 96, "Question": "### Background\n\n**Research Question.** Is the relationship between foreign bank entry and a country's gross domestic investment non-linear, and what is the nature of this relationship?\n\n**Setting & Data.** A cross-sectional regression analysis for 54 countries using data averaged over 1990–1997. The analysis tests a quadratic specification for the effect of foreign bank penetration on the investment-to-GDP ratio.\n\n### Data / Model Specification\n\nThe non-linear estimating equation is given by:\n  \nI = A + \\alpha_{1}LGDP + \\alpha_{2}GRO + \\alpha_{3}FBSASS + \\alpha_{4}LGDP^{2} + \\alpha_{5}GRO^{2} + \\alpha_{6}FBSASS^{2} + \\varepsilon \\quad \\text{(Eq. (1))}\n \nwhere `I` is the investment-to-GDP ratio, `FBSASS` is the share of foreign bank assets in total bank assets, and `LGDP` and `GRO` are controls.\n\n**Table 1. Baseline Estimation Results**\n\n| Variable | Coefficient (t-stat) |\n|:---|:---:|\n| `FBSASS` (α₃) | -14.24 (-2.38) |\n| `FBSASS²` (α₆) | 22.84 (3.46) |\n\n*Notes: The results are from Table 4, column (2a) of the paper.*\n\n### Question\n\nBased on the model specification in **Eq. (1)** and the results in **Table 1**, select all of the following statements that are correct interpretations of the findings.", "Options": {"A": "The calculated turning point of the U-curve is approximately 0.62, derived from the ratio of the coefficients (-α₃ / α₆).", "B": "The calculated turning point of the U-curve, where the marginal effect of `FBSASS` on investment becomes positive, is approximately 0.31.", "C": "The model implies that for a country with a low foreign bank share (e.g., `FBSASS` = 0.10), a small increase in that share is associated with a decrease in the investment ratio.", "D": "The statistically significant positive coefficient on `FBSASS²` implies that the marginal effect of foreign bank entry on investment is always positive."}, "Answer": ["B", "C"], "pi_justification": "REPLACE with Multiple Choice (Table QA rule). This item assesses the core quantitative skill of calculating and interpreting the turning point of a quadratic model. It uses a Computational Judgment strategy. The correct options (A, B) are an atomic decomposition of the main finding, testing both the calculation (`-α₃ / (2α₆)`) and its direct implication for the marginal effect. Distractor C is a high-fidelity Formula Misuse error, omitting the factor of 2 from the denominator. Distractor D represents a common Conceptual error in interpreting quadratic terms in isolation.", "qid": "96", "question": "### Background\n\n**Research Question.** Is the relationship between foreign bank entry and a country's gross domestic investment non-linear, and what is the nature of this relationship?\n\n**Setting & Data.** A cross-sectional regression analysis for 54 countries using data averaged over 1990–1997. The analysis tests a quadratic specification for the effect of foreign bank penetration on the investment-to-GDP ratio.\n\n### Data / Model Specification\n\nThe non-linear estimating equation is given by:\n  \nI = A + \\alpha_{1}LGDP + \\alpha_{2}GRO + \\alpha_{3}FBSASS + \\alpha_{4}LGDP^{2} + \\alpha_{5}GRO^{2} + \\alpha_{6}FBSASS^{2} + \\varepsilon \\quad \\text{(Eq. (1))}\n \nwhere `I` is the investment-to-GDP ratio, `FBSASS` is the share of foreign bank assets in total bank assets, and `LGDP` and `GRO` are controls.\n\n**Table 1. Baseline Estimation Results**\n\n| Variable | Coefficient (t-stat) |\n|:---|:---:|\n| `FBSASS` (α₃) | -14.24 (-2.38) |\n| `FBSASS²` (α₆) | 22.84 (3.46) |\n\n*Notes: The results are from Table 4, column (2a) of the paper.*\n\n### Question\n\nBased on the model specification in **Eq. (1)** and the results in **Table 1**, select all of the following statements that are correct interpretations of the findings.", "model_evaluation_result": {"model_raw_response": "Looking at the quadratic model and the coefficients provided:\n\nThe marginal effect of FBSASS on investment is given by the first derivative:\n∂I/∂FBSASS = α₃ + 2α₆(FBSASS) = -14.24 + 2(22.84)(FBSASS)\n\nTo find the turning point (minimum of the U-curve), set the marginal effect equal to zero:\n-14.24 + 45.68(FBSASS) = 0\nFBSASS = 14.24/45.68 ≈ 0.31\n\nLet me evaluate each option:\n\nA) Incorrect. The turning point calculation -α₃/α₆ = -(-14.24)/22.84 = 14.24/22.84 ≈ 0.62 is wrong. The correct formula for a quadratic function's turning point is -α₃/(2α₆).\n\nB) Correct. Using the proper formula: -α₃/(2α₆) = 14.24/(2×22.84) = 14.24/45.68 ≈ 0.31.\n\nC) Correct. At FBSASS = 0.10, the marginal effect is:\n-14.24 + 45.68(0.10) = -14.24 + 4.568 = -9.672 < 0\nSince the marginal effect is negative, an increase in FBSASS decreases investment.\n\nD) Incorrect. The positive coefficient on FBSASS² only indicates the relationship is U-shaped. The marginal effect depends on the level of FBSASS - it's negative below the turning point (0.31) and positive above it.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 95, "Question": "### Background\n\nThe author critiques a specific provision in the Dodd-Frank Act which states that margin required by a derivatives clearing organization (CCP) \"shall be sufficient to cover potential exposures in normal market conditions.\" The author argues this standard is fundamentally inadequate for preventing systemic crises, which are, by definition, abnormal, tail-risk events.\n\n### Scenario\n\nA CCP implements its margin policy based on the Dodd-Frank provision. It calculates required margin (`M`) using a Value-at-Risk (`VaR`) model based on recent historical data. This model is known to be pro-cyclical: in calm markets, low volatility leads to low `VaR` and low margins; in volatile markets, high `VaR` leads to high margins and large margin calls.\n\n### Question\n\nAccording to the author's analysis, which of the following are valid criticisms or consequences of a margin policy based strictly on covering losses in \"normal market conditions\"?\n\nSelect all that apply.", "Options": {"A": "The policy effectively ensures that systemically threatening tail risks—losses that occur in abnormal conditions—are not collateralized at the transaction level.", "B": "The policy creates a dangerous feedback loop where forced selling to meet margin calls increases market volatility, which in turn triggers even higher margin requirements.", "C": "This margin policy is robust because it prevents the CCP from over-collateralizing positions, thereby improving market liquidity and efficiency during periods of stress.", "D": "During a market crisis, the pro-cyclical nature of the margin calculation can force the CCP to make massive, simultaneous margin calls, amplifying the crisis by causing forced asset sales."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses understanding of the author's critique of the Dodd-Frank margin rule and the concept of pro-cyclicality. It uses a Reverse-Reasoning strategy, asking for the consequences of a flawed policy. Correct options (A, B, D) describe the core problems identified in the paper: the failure to price tail risk (A) and the dangerous, crisis-amplifying feedback loop of pro-cyclical margin calls (B, D). The distractor (C) is a 'Conceptual Opposite,' presenting a positive but false interpretation of the policy's effects during a crisis, which a candidate must correctly identify as incorrect.", "qid": "95", "question": "### Background\n\nThe author critiques a specific provision in the Dodd-Frank Act which states that margin required by a derivatives clearing organization (CCP) \"shall be sufficient to cover potential exposures in normal market conditions.\" The author argues this standard is fundamentally inadequate for preventing systemic crises, which are, by definition, abnormal, tail-risk events.\n\n### Scenario\n\nA CCP implements its margin policy based on the Dodd-Frank provision. It calculates required margin (`M`) using a Value-at-Risk (`VaR`) model based on recent historical data. This model is known to be pro-cyclical: in calm markets, low volatility leads to low `VaR` and low margins; in volatile markets, high `VaR` leads to high margins and large margin calls.\n\n### Question\n\nAccording to the author's analysis, which of the following are valid criticisms or consequences of a margin policy based strictly on covering losses in \"normal market conditions\"?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which options represent valid criticisms or consequences of a margin policy that only covers \"normal market conditions\" according to the author's analysis.\n\nThe author's key argument is that the Dodd-Frank provision requiring margins sufficient for \"normal market conditions\" is fundamentally inadequate because systemic crises are by definition abnormal, tail-risk events.\n\nLet me analyze each option:\n\n**Option A**: States that the policy ensures tail risks (abnormal conditions) are not collateralized at the transaction level. This directly aligns with the author's criticism - if margins only cover normal conditions, then by definition they don't cover abnormal/tail risks. This is a valid criticism.\n\n**Option B**: Describes a feedback loop where margin calls force selling → increase volatility → trigger higher margins. This is a classic procyclical problem that the author identifies with VaR-based margin models. This is a valid criticism.\n\n**Option C**: Claims the policy is \"robust\" and improves efficiency during stress periods. This contradicts the author's entire critique. The author argues this policy is inadequate, not robust. This is NOT a valid criticism according to the author.\n\n**Option D**: Describes how procyclical margins can amplify crises through massive simultaneous margin calls causing forced asset sales. This directly follows from the procyclical nature of the VaR model described in the scenario and represents a key systemic risk. This is a valid criticism.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 113, "Question": "### Background\n\n**Research Question.** This case examines how the paper's long-run risk model is applied to price real bonds.\n\n**Setting.** In a representative agent economy with Epstein-Zin preferences, real bonds are claims to risk-free future consumption. Bond prices are determined by the single, persistent state variable `x_t`, which represents the expected growth rate of durable consumption.\n\n**Variables and Parameters.**\n- `b_{n,t}`: Log price of an `n`-period real zero-coupon bond.\n- `x_t`: Long-run risk state variable, following `x_{t+1} = φx_t + σ_x ω_t ε_{x,t+1}`.\n- `B_{n,1}`: Sensitivity of the `n`-period log bond price to `x_t`.\n- `m_{t+1}`: Log SDF.\n- `ψ`, `α`, `φ`: Model parameters.\n\n---\n\n### Data / Model Specification\n\nThe log bond price is assumed to be linear in the state variable:\n  \nb_{n,t} \\approx B_{n,0} + B_{n,1} x_t \\quad \\text{(Eq. (1))}\n \nAsset prices are determined by the log-linearized Euler equation `0 ≈ E_t[m_{t+1} + r_{asset,t+1}]`. The key term from the expected log SDF is `E_t[m_{t+1}] = const + (1 - 1/ψ)α x_t`.\n\n---\n\n### Question\n\nBased on the model specification, select ALL statements that are correct regarding the pricing of real zero-coupon bonds.", "Options": {"A": "With `ψ=1.5`, real bond yields are counter-cyclical (low when `x_t` is high) because the agent's desire to smooth consumption between goods (goods substitution) dominates the desire to smooth consumption over time (intertemporal substitution).", "B": "The sensitivity of the `n`-period log bond price to the long-run risk state variable, `B_{n,1}`, is correctly given by `B_{n,1} = (1 - 1/ψ)α * (1 - φ^n) / (1 - φ)`.", "C": "The sensitivity of the `n`-period log bond price to the long-run risk state variable, `B_{n,1}`, is correctly given by `B_{n,1} = (1 - 1/ψ)α * φ^n`.", "D": "Real bonds are a hedge against long-run risk and therefore command a negative risk premium, as they pay off well when `x_t` is unexpectedly low."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item uses the Atomic Decomposition strategy to assess understanding of real bond pricing. Option A tests the correct derived formula for bond price sensitivity. Option B tests the core economic intuition behind counter-cyclical yields, which hinges on the relative strength of substitution effects. Distractor C represents a Formula Misuse error, confusing the recursive step with the full solution. Distractor D presents a Conceptual Opposite regarding the bond's risk characteristics and premium.", "qid": "113", "question": "### Background\n\n**Research Question.** This case examines how the paper's long-run risk model is applied to price real bonds.\n\n**Setting.** In a representative agent economy with Epstein-Zin preferences, real bonds are claims to risk-free future consumption. Bond prices are determined by the single, persistent state variable `x_t`, which represents the expected growth rate of durable consumption.\n\n**Variables and Parameters.**\n- `b_{n,t}`: Log price of an `n`-period real zero-coupon bond.\n- `x_t`: Long-run risk state variable, following `x_{t+1} = φx_t + σ_x ω_t ε_{x,t+1}`.\n- `B_{n,1}`: Sensitivity of the `n`-period log bond price to `x_t`.\n- `m_{t+1}`: Log SDF.\n- `ψ`, `α`, `φ`: Model parameters.\n\n---\n\n### Data / Model Specification\n\nThe log bond price is assumed to be linear in the state variable:\n  \nb_{n,t} \\approx B_{n,0} + B_{n,1} x_t \\quad \\text{(Eq. (1))}\n \nAsset prices are determined by the log-linearized Euler equation `0 ≈ E_t[m_{t+1} + r_{asset,t+1}]`. The key term from the expected log SDF is `E_t[m_{t+1}] = const + (1 - 1/ψ)α x_t`.\n\n---\n\n### Question\n\nBased on the model specification, select ALL statements that are correct regarding the pricing of real zero-coupon bonds.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the pricing of real zero-coupon bonds in a long-run risk model with Epstein-Zin preferences.\n\nGiven information:\n- Log bond price: b_{n,t} ≈ B_{n,0} + B_{n,1} x_t\n- State variable: x_{t+1} = φx_t + σ_x ω_t ε_{x,t+1}\n- Key SDF term: E_t[m_{t+1}] = const + (1 - 1/ψ)α x_t\n- Euler equation: 0 ≈ E_t[m_{t+1} + r_{asset,t+1}]\n\nLet me analyze each option:\n\n**Option A**: With ψ=1.5, we have (1 - 1/ψ) = (1 - 2/3) = 1/3 > 0. This means E_t[m_{t+1}] increases with x_t. When expected consumption growth (x_t) is high, the SDF is high, making future payoffs more valuable. This drives bond prices up and yields down, making yields counter-cyclical. The statement about goods vs intertemporal substitution is incorrect terminology for this context.\n\n**Option B**: To find B_{n,1}, I need to solve the bond pricing recursion. Using the Euler equation and the linear form, I can show that:\n- B_{1,1} = (1 - 1/ψ)α\n- B_{n,1} = B_{1,1} + φB_{n-1,1}\n\nThis gives B_{n,1} = (1 - 1/ψ)α × (1 - φ^n)/(1 - φ), which matches option B.\n\n**Option C**: This formula B_{n,1} = (1 - 1/ψ)α × φ^n is incorrect. It doesn't satisfy the recursion relationship.\n\n**Option D**: Real bonds pay off the same regardless of x_t (they're risk-free in real terms). They don't provide a hedge against long-run risk shocks. In fact, with ψ > 1, bond prices move positively with x_t, so they would have a positive risk premium, not negative.\n\nTherefore, only options A and B are correct.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 89, "Question": "### Background\n\n**Research Question.** A central challenge in corporate finance is to devise empirical tests of strategic risk-shifting that are robust to mechanical artifacts like mean reversion. This paper proposes two novel methodologies based on portfolio holdings to overcome a 'sorting bias' found in traditional return-based tests of mutual fund tournament behavior.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual fund holdings data from 1990-2006. The goal is to test whether underperforming managers ('losers') in the first half of the year actively change their portfolio risk in the second half. The paper also re-examines whether this behavior is linked to overall market conditions.\n\n### Data / Model Specification\n\n**Method 1: Active Holdings Change Regression**\nThis method analyzes individual security trades. The dependent variable is the active change in a security's weight, `WgtChg`, defined as:\n  \n\\mathrm{WgtChg}_{j i y}=\\frac{(\\mathrm{DecShares}_{j i y}-\\mathrm{JuneShares}_{j i y}) \\times \\mathrm{DecPrice}_{j y}}{\\mathrm{DecAssets}_{i y}} \\quad \\text{(Eq. 1)}\n \nThis is regressed on security characteristics, including the `Adjusted Standard Deviation` (`Adj. Std.`), which is the security's first-half standard deviation minus the fund's average. The key test is for a differential effect for underperforming funds, identified by an interaction term `LD*AS` (`LowDummy` × `Adj. Std.`).\n\n**Table 1. Active Trading Regression Results (Overall, Ranked Together)**\n\n| Variable | Coefficient | t-value |\n|:---|:---:|:---:|\n| Adj. Std. | 1.217 | 7.74** |\n| LD*AS | 0.677 | 1.64 |\n\n*Note: The dependent variable is `WgtChg` scaled by 100. **Significant at 1% level.*\n\n**Method 2: Bootstrap-Based Aggregate Risk Measure**\nThis method constructs a counterfactual second-half portfolio risk level for each fund. It does so by observing the manager's actual trading volume (e.g., number of stocks sold/bought) but randomizing *which* stocks are traded, assuming non-risk-motivated (e.g., momentum-based) trading. This produces an 'expected' standard deviation (`σ_expected`). The dependent variable is `Excess Risk = σ_actual - σ_expected`. This is regressed on `LowDummy`, an indicator for underperforming funds.\n\n**Table 2. Bootstrap-Based Tournament Results (Average over 1990-2006)**\n\n| Variable | Value | p-value |\n|:---|:---:|:---:|\n| Low Dummy | 0.055 | 0.00** |\n\n*Note: **Significant at 1% level.*\n\n### Question\n\nBased on the results from the paper's two new methodologies (Active Holdings Change and Bootstrap-Based Aggregate Risk), select all of the following statements that are supported by the evidence in Table 1 and Table 2.", "Options": {"A": "The coefficient on the interaction term `LD*AS` in Table 1 (0.677) is statistically significant at the 1% level, confirming that underperforming managers' risk-taking is higher.", "B": "The coefficient on `Adj. Std.` in Table 1 (1.217) indicates that outperforming managers tend to sell securities that are riskier than their portfolio average.", "C": "Based on the bootstrap analysis in Table 2, underperforming funds increase their second-half portfolio risk by an average of 5.5 basis points more than outperforming funds, a result that is statistically significant at the 1% level.", "D": "According to the active holdings regression in Table 1, the total marginal effect of a security's adjusted standard deviation on its scaled weight change for an underperforming manager is 1.894."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the student's ability to interpret the results of the two novel methodologies proposed by the paper. It uses an **Atomic Decomposition** strategy, breaking down the findings from Table 1 and Table 2 into distinct, verifiable statements. The core task is a mix of **Computational Judgment** (calculating the marginal effect for losers) and direct interpretation. Distractors are based on common errors: a **Conceptual Opposite** (misinterpreting the sign of a coefficient) and a **Significance Error** (misstating the statistical significance of a coefficient).", "qid": "89", "question": "### Background\n\n**Research Question.** A central challenge in corporate finance is to devise empirical tests of strategic risk-shifting that are robust to mechanical artifacts like mean reversion. This paper proposes two novel methodologies based on portfolio holdings to overcome a 'sorting bias' found in traditional return-based tests of mutual fund tournament behavior.\n\n**Setting / Data-Generating Environment.** The analysis uses U.S. equity mutual fund holdings data from 1990-2006. The goal is to test whether underperforming managers ('losers') in the first half of the year actively change their portfolio risk in the second half. The paper also re-examines whether this behavior is linked to overall market conditions.\n\n### Data / Model Specification\n\n**Method 1: Active Holdings Change Regression**\nThis method analyzes individual security trades. The dependent variable is the active change in a security's weight, `WgtChg`, defined as:\n  \n\\mathrm{WgtChg}_{j i y}=\\frac{(\\mathrm{DecShares}_{j i y}-\\mathrm{JuneShares}_{j i y}) \\times \\mathrm{DecPrice}_{j y}}{\\mathrm{DecAssets}_{i y}} \\quad \\text{(Eq. 1)}\n \nThis is regressed on security characteristics, including the `Adjusted Standard Deviation` (`Adj. Std.`), which is the security's first-half standard deviation minus the fund's average. The key test is for a differential effect for underperforming funds, identified by an interaction term `LD*AS` (`LowDummy` × `Adj. Std.`).\n\n**Table 1. Active Trading Regression Results (Overall, Ranked Together)**\n\n| Variable | Coefficient | t-value |\n|:---|:---:|:---:|\n| Adj. Std. | 1.217 | 7.74** |\n| LD*AS | 0.677 | 1.64 |\n\n*Note: The dependent variable is `WgtChg` scaled by 100. **Significant at 1% level.*\n\n**Method 2: Bootstrap-Based Aggregate Risk Measure**\nThis method constructs a counterfactual second-half portfolio risk level for each fund. It does so by observing the manager's actual trading volume (e.g., number of stocks sold/bought) but randomizing *which* stocks are traded, assuming non-risk-motivated (e.g., momentum-based) trading. This produces an 'expected' standard deviation (`σ_expected`). The dependent variable is `Excess Risk = σ_actual - σ_expected`. This is regressed on `LowDummy`, an indicator for underperforming funds.\n\n**Table 2. Bootstrap-Based Tournament Results (Average over 1990-2006)**\n\n| Variable | Value | p-value |\n|:---|:---:|:---:|\n| Low Dummy | 0.055 | 0.00** |\n\n*Note: **Significant at 1% level.*\n\n### Question\n\nBased on the results from the paper's two new methodologies (Active Holdings Change and Bootstrap-Based Aggregate Risk), select all of the following statements that are supported by the evidence in Table 1 and Table 2.", "model_evaluation_result": {"model_raw_response": "Looking at the tables and analyzing each statement:\n\nA) The coefficient on `LD*AS` in Table 1 is 0.677 with a t-value of 1.64. This is NOT significant at the 1% level (would need |t| > 2.58). The statement claims it's significant at 1%, which is false.\n\nB) The coefficient on `Adj. Std.` is 1.217 (positive). This means managers tend to INCREASE weights in securities with higher adjusted standard deviation (riskier securities), not sell them. The statement says the opposite, so it's false.\n\nC) Table 2 shows Low Dummy = 0.055 with p-value 0.00 (significant at 1% level). The 0.055 represents 5.5 basis points (0.055 percentage points) higher excess risk for underperforming funds. This statement accurately describes the result.\n\nD) For underperforming managers (when LowDummy = 1), the total marginal effect is:\n   Adj. Std. coefficient + LD*AS coefficient = 1.217 + 0.677 = 1.894\n   This calculation is correct.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 42, "Question": "### Background\n\nWhen extending the American option pricing model to include stochastic volatility, the pricing PDE becomes more complex, incorporating additional terms to account for the new source of risk and its correlation with the asset price.\n\n### Data / Model Specification\n\nThe pricing PDE for an American option `p(x, y, τ)` in a general stochastic volatility framework is:\n  \n\\frac{\\partial p}{\\partial\\tau} = \\frac{1}{2}x^{2}f(y)^{2}\\frac{\\partial^{2}p}{\\partial x^{2}} + \\frac{1}{2}\\lambda(y)^{2}\\frac{\\partial^{2}p}{\\partial y^{2}} + \\rho\\lambda(y)f(y)x\\frac{\\partial^{2}p}{\\partial x\\partial y} + r x\\frac{\\partial p}{\\partial x} + \\eta(y)\\frac{\\partial p}{\\partial y} - r p \\quad \\text{(Eq. 1)}\n \nwhere `y` is the volatility factor, `f(y)` is asset volatility, `λ(y)` is the 'vol of vol', `ρ` is the correlation between asset and volatility shocks, and `η(y)` is the risk-neutral drift of volatility.\n\n---\n\nWhich of the following statements provide an INCORRECT financial interpretation of the terms in the stochastic volatility PDE? Select all that apply.", "Options": {"A": "The term `ρλ(y)f(y)x(∂²p/∂x∂y)` captures the pricing impact of the correlation between asset price and volatility movements.", "B": "The term `η(y)(∂p/∂y)` captures the impact of the expected trend in volatility on the option's price, where `∂p/∂y` is the option's Vega.", "C": "The term `(1/2)λ(y)²(∂²p/∂y²)` represents the option's sensitivity to the risk-neutral drift of the volatility process.", "D": "The term `(1/2)x²f(y)²(∂²p/∂x²)` represents the option's sensitivity to the 'volatility of volatility'."}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: Test the user's ability to correctly identify and interpret the new terms that appear in the pricing PDE under stochastic volatility.\nDepth Strategy: Reverse-Reasoning / Conceptual Judgment. The user must evaluate the correctness of several interpretations.\nInversion Protocol: The initial question, \"Select all correct interpretations,\" would have yielded only two correct answers (B and C). To meet the mission requirement of having at least two correct answers in the key, the Single-Correct-Answer Inversion Protocol was triggered. The stem was changed to ask for INCORRECT interpretations, making the original distractors (A and D) the new correct answers.\nDistractor Logic (Post-Inversion):\n- B (Now a distractor): This is a correct interpretation. `η(y)` is the risk-neutral drift and `∂p/∂y` is Vega.\n- C (Now a distractor): This is a correct interpretation. The term contains the correlation `ρ` and the cross-derivative `∂²p/∂x∂y` (Vanna).\n- A (Now a correct answer): This is an incorrect interpretation. This term represents the convexity adjustment for volatility risk (related to 'Volga' or 'Vomma'), not the sensitivity to the drift.\n- D (Now a correct answer): This is an incorrect interpretation. This is the standard Gamma term, representing convexity with respect to the asset price `x`. The sensitivity to 'vol of vol' is captured by the `λ(y)` term.", "qid": "42", "question": "### Background\n\nWhen extending the American option pricing model to include stochastic volatility, the pricing PDE becomes more complex, incorporating additional terms to account for the new source of risk and its correlation with the asset price.\n\n### Data / Model Specification\n\nThe pricing PDE for an American option `p(x, y, τ)` in a general stochastic volatility framework is:\n  \n\\frac{\\partial p}{\\partial\\tau} = \\frac{1}{2}x^{2}f(y)^{2}\\frac{\\partial^{2}p}{\\partial x^{2}} + \\frac{1}{2}\\lambda(y)^{2}\\frac{\\partial^{2}p}{\\partial y^{2}} + \\rho\\lambda(y)f(y)x\\frac{\\partial^{2}p}{\\partial x\\partial y} + r x\\frac{\\partial p}{\\partial x} + \\eta(y)\\frac{\\partial p}{\\partial y} - r p \\quad \\text{(Eq. 1)}\n \nwhere `y` is the volatility factor, `f(y)` is asset volatility, `λ(y)` is the 'vol of vol', `ρ` is the correlation between asset and volatility shocks, and `η(y)` is the risk-neutral drift of volatility.\n\n---\n\nWhich of the following statements provide an INCORRECT financial interpretation of the terms in the stochastic volatility PDE? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each statement to identify which provide INCORRECT financial interpretations:\n\nA) The term `ρλ(y)f(y)x(∂²p/∂x∂y)` captures the pricing impact of the correlation between asset price and volatility movements.\n- This is CORRECT. This cross-derivative term arises from the correlation ρ between asset price and volatility shocks in stochastic volatility models.\n\nB) The term `η(y)(∂p/∂y)` captures the impact of the expected trend in volatility on the option's price, where `∂p/∂y` is the option's Vega.\n- This is CORRECT. η(y) is the risk-neutral drift of volatility, and ∂p/∂y is indeed the option's sensitivity to volatility (Vega).\n\nC) The term `(1/2)λ(y)²(∂²p/∂y²)` represents the option's sensitivity to the risk-neutral drift of the volatility process.\n- This is INCORRECT. This term represents the diffusion/variance component of the volatility process (the \"vol of vol\" effect), not the drift. The risk-neutral drift is captured by η(y) in the first-order derivative term.\n\nD) The term `(1/2)x²f(y)²(∂²p/∂x²)` represents the option's sensitivity to the 'volatility of volatility'.\n- This is INCORRECT. This term represents the standard diffusion component for the asset price process with stochastic volatility f(y). The \"volatility of volatility\" is λ(y), which appears in the second partial derivative with respect to y.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 116, "Question": "### Background\n\nThe model is extended to include costly bank monitoring. The bank must be incentivized to monitor at a private cost `P`. This introduces a second moral hazard problem, as credit guarantees can weaken the bank's incentive to monitor.\n\n### Data / Model Specification\n\nThe need to incentivize bank monitoring lowers the maximum useful guarantee level to:\n\n  \n\\tilde{\\phi}(Q) = 1 - \\frac{A+P}{I-Q} \n \nwhere `A` is collateral, `P` is monitoring cost, and `I-Q` is the loan amount. This new cap is strictly less than the cap without monitoring costs. As a result, the optimal policy shifts towards co-funding at a lower budget threshold.\n\n### Question\n\nIn the extended model with costly bank monitoring, which of the following statements are **INCORRECT** descriptions of the policy trade-offs and outcomes?\n\nSelect all that apply.", "Options": {"A": "Credit guarantees and co-funding are perfect substitutes, and the introduction of monitoring costs does not change their relative effectiveness.", "B": "The fundamental 'pecking order' of support is completely reversed; co-funding should always be provided before any guarantees are considered.", "C": "The maximum useful guarantee level, `tilde(φ)(Q)`, is strictly increasing in the bank's monitoring cost `P`.", "D": "A higher monitoring cost `P` makes co-funding a relatively more attractive policy tool because guarantees directly undermine the bank's incentive to incur that cost, while co-funding does not."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to analyze how an additional constraint (bank monitoring) alters the model's main conclusions. It tests the understanding of the new trade-off between incentivizing the entrepreneur and the bank. Depth Strategy: Reverse-Reasoning. The student must identify multiple invalid conclusions about the model's extension. Single-Correct-Answer Inversion Protocol: This protocol was applied. The core assessment objective—understanding how monitoring costs shift the optimal policy—has a single, nuanced correct answer (D). To create a valid Multiple Response item, the stem was inverted to ask for INCORRECT statements. The original distractors (A, B, C), which represent plausible misinterpretations, now become the correct answers. Distractor Logic: The new (and only) distractor, D, is the correct summary of the extension's main finding. Options A, B, and C are incorrect because: (A) `tilde(φ)` is decreasing in `P`; (B) the tools are not perfect substitutes and monitoring costs change their relative appeal; (C) the pecking order is modified, not reversed.", "qid": "116", "question": "### Background\n\nThe model is extended to include costly bank monitoring. The bank must be incentivized to monitor at a private cost `P`. This introduces a second moral hazard problem, as credit guarantees can weaken the bank's incentive to monitor.\n\n### Data / Model Specification\n\nThe need to incentivize bank monitoring lowers the maximum useful guarantee level to:\n\n  \n\\tilde{\\phi}(Q) = 1 - \\frac{A+P}{I-Q} \n \nwhere `A` is collateral, `P` is monitoring cost, and `I-Q` is the loan amount. This new cap is strictly less than the cap without monitoring costs. As a result, the optimal policy shifts towards co-funding at a lower budget threshold.\n\n### Question\n\nIn the extended model with costly bank monitoring, which of the following statements are **INCORRECT** descriptions of the policy trade-offs and outcomes?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) This statement claims credit guarantees and co-funding are perfect substitutes and monitoring costs don't change their relative effectiveness. This is INCORRECT because:\n- The introduction of monitoring costs creates a second moral hazard problem\n- Credit guarantees specifically weaken the bank's incentive to monitor\n- This makes guarantees and co-funding imperfect substitutes with different effectiveness\n\nB) This statement claims the pecking order is \"completely reversed\" and co-funding should \"always\" be provided first. This is INCORRECT because:\n- While the model shows a shift toward co-funding at lower budget thresholds, this is not a complete reversal\n- There would still be situations where guarantees are preferred\n- The absolute claim \"always\" is too strong\n\nC) This statement claims the maximum useful guarantee level is strictly increasing in monitoring cost P. This is INCORRECT because:\n- The formula shows: $\\tilde{\\phi}(Q) = 1 - \\frac{A+P}{I-Q}$\n- As P increases, the fraction $\\frac{A+P}{I-Q}$ increases\n- Therefore, $\\tilde{\\phi}(Q)$ decreases as P increases\n- The relationship is strictly decreasing, not increasing\n\nD) This statement is CORRECT. Higher monitoring costs do make co-funding relatively more attractive because:\n- Guarantees directly undermine monitoring incentives (second moral hazard problem)\n- Co-funding doesn't have this negative effect on monitoring incentives\n- This is consistent with the model's finding that co-funding becomes optimal at lower budget thresholds\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 104, "Question": "### Background\n\n**Research Question.** This case investigates the characteristics of firms involved in Reverse Takeovers (RTs) and their stated motivations to understand the underlying economic logic of these transactions.\n\n**Setting.** The analysis uses data on the financial condition of the public 'vehicle' firms, the stated reasons for the merger from proxy statements, and the industrial relatedness of the merging parties for a sample of 121 RTs.\n\n**Variables & Parameters.**\n- `ROA`: Return on Assets, a measure of profitability.\n- `V_ops`: The value of a firm's ongoing operations.\n- `V_listing`: The value of a firm's public stock exchange listing.\n\n---\n\n### Data / Model Specification\n\nThe characteristics of the firms involved in an RT reveal its true purpose. The public firm is often a poorly performing entity, while the private firm has a viable business seeking a public listing.\n\n**Table 1: Summary Characteristics of Public Firms (Year Prior to RT)**\n\n| Variable | Mean | Median |\n| :--- | :--- | :--- |\n| Total Assets (US$ millions) | 4.95 | 0.16 |\n| Return on Assets (ROA, %) | -21.87 | -10.75 |\n| Return on Equity (ROE, %) | -42.44 | -6.70 |\n\n*Note: Table is adapted from Panel A of Table 4 in the source.*\n\n**Table 2: Reasons Cited for Reverse Takeovers**\n\n| Reason(s) Cited in Filing Documents | Cited by Public Firm | Cited by Private Firm | Total Citations |\n| :--- | :--- | :--- | :--- |\n| Solid financial position of private parent | 43 | 16 | 63 |\n| Growth | 15 | 25 | 47 |\n| Going public issues | 7 | 17 | 27 |\n\n*Note: Table is a selection from Panel B of Table 2 in the source.*\n\n**Table 3: Distribution by Degree of Industry Similarity**\n\n| SIC Industry Classification | Before the Takeover | After the Takeover |\n| :--- | :--- | :--- |\n| From the same industry | 27.3% | 52.1% |\n| From complementary industries | 31.4% | 33.1% |\n| From different industries | 41.3% | 14.9% |\n\n*Note: Table is adapted from Panel B of Table 3 in the source. 'After the takeover' same industry is the sum of same as private, public, or both.*\n\n---\n\n### Question\n\nBased on the provided tables characterizing the firms and transactions, which of the following statements are accurate? Select all that apply.", "Options": {"A": "The most frequent motivation cited by the public firm was a desire to access capital markets or achieve other 'going public' objectives.", "B": "The typical public 'vehicle' firm was unprofitable in the year prior to the RT, as measured by median Return on Assets.", "C": "The most common pre-takeover arrangement involved two firms from the same industry seeking to achieve economies of scale.", "D": "Following the RT, the combined entity was most commonly structured to operate within a single industry (that of one or both parents), rather than across different or complementary industries."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to assess the student's ability to synthesize descriptive statistics from multiple tables to build a coherent profile of a typical RT. It tests the understanding of the firm characteristics, motivations, and industrial logic of the transaction. Distractors are based on high-fidelity errors: C (Conceptual Error) confuses the distinct motivations of the public and private firms, and D (Conceptual Opposite) presents the opposite of the empirical finding about pre-merger industry pairings.", "qid": "104", "question": "### Background\n\n**Research Question.** This case investigates the characteristics of firms involved in Reverse Takeovers (RTs) and their stated motivations to understand the underlying economic logic of these transactions.\n\n**Setting.** The analysis uses data on the financial condition of the public 'vehicle' firms, the stated reasons for the merger from proxy statements, and the industrial relatedness of the merging parties for a sample of 121 RTs.\n\n**Variables & Parameters.**\n- `ROA`: Return on Assets, a measure of profitability.\n- `V_ops`: The value of a firm's ongoing operations.\n- `V_listing`: The value of a firm's public stock exchange listing.\n\n---\n\n### Data / Model Specification\n\nThe characteristics of the firms involved in an RT reveal its true purpose. The public firm is often a poorly performing entity, while the private firm has a viable business seeking a public listing.\n\n**Table 1: Summary Characteristics of Public Firms (Year Prior to RT)**\n\n| Variable | Mean | Median |\n| :--- | :--- | :--- |\n| Total Assets (US$ millions) | 4.95 | 0.16 |\n| Return on Assets (ROA, %) | -21.87 | -10.75 |\n| Return on Equity (ROE, %) | -42.44 | -6.70 |\n\n*Note: Table is adapted from Panel A of Table 4 in the source.*\n\n**Table 2: Reasons Cited for Reverse Takeovers**\n\n| Reason(s) Cited in Filing Documents | Cited by Public Firm | Cited by Private Firm | Total Citations |\n| :--- | :--- | :--- | :--- |\n| Solid financial position of private parent | 43 | 16 | 63 |\n| Growth | 15 | 25 | 47 |\n| Going public issues | 7 | 17 | 27 |\n\n*Note: Table is a selection from Panel B of Table 2 in the source.*\n\n**Table 3: Distribution by Degree of Industry Similarity**\n\n| SIC Industry Classification | Before the Takeover | After the Takeover |\n| :--- | :--- | :--- |\n| From the same industry | 27.3% | 52.1% |\n| From complementary industries | 31.4% | 33.1% |\n| From different industries | 41.3% | 14.9% |\n\n*Note: Table is adapted from Panel B of Table 3 in the source. 'After the takeover' same industry is the sum of same as private, public, or both.*\n\n---\n\n### Question\n\nBased on the provided tables characterizing the firms and transactions, which of the following statements are accurate? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the tables systematically:\n\nFrom Table 1: The median ROA is -10.75%, which is negative, indicating the typical public firm was unprofitable.\n\nFrom Table 2: The most frequent citation by public firms was \"Solid financial position of private parent\" (43 citations), not \"Going public issues\" (only 7 citations).\n\nFrom Table 3: Before takeover, only 27.3% were from the same industry (the least common arrangement), while 41.3% were from different industries (most common).\n\nFrom Table 3: After takeover, 52.1% operated in the same industry, which is higher than complementary (33.1%) or different industries (14.9%).\n\nTherefore:\n- A is incorrect (going public was cited only 7 times by public firms)\n- B is correct (median ROA was negative at -10.75%)\n- C is incorrect (same industry was least common at 27.3%)\n- D is correct (52.1% operated in same industry post-takeover)\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 205, "Question": "### Background\n\n**Research Question.** How can statistical tests be combined to build a robust case for the non-stationarity of financial price series, a key property implied by the weak-form Efficient Market Hypothesis?\n\n**Setting / Data-Generating Environment.** The analysis uses daily closing prices for UK financial futures. To test for non-stationarity, the paper employs two complementary methods: the Augmented Dickey-Fuller (ADF) test, which has a null hypothesis of a unit root (non-stationarity), and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, which has a null hypothesis of stationarity.\n\n---\n\n### Data / Model Specification\n\nThe Augmented Dickey-Fuller (ADF) test has a null hypothesis `H₀: The series has a unit root`.\nThe Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test has a null hypothesis `H₀: The series is stationary`.\n\n**Table 1. Stationarity Test Results for FTSE100 Futures**\n\n| Panel | Test | Data Series | Lag | Test Statistic | 5% Critical Value |\n|---|---|---|---|---|---|\n| A | ADF | Price Level | 1 | -0.882 | -3.4139 |\n| A | ADF | First Difference | 1 | -43.03 | -3.4139 |\n| B | KPSS | Price Level | 8 | 3.4577 | 0.146 |\n\n*Source: Adapted from Tables 2 and 3 of the source paper.*\n\n---\n\n### Question\n\nBased on the data presented in **Table 1**, select ALL of the following statements that correctly interpret and synthesize the test results.", "Options": {"A": "The KPSS test statistic (3.4577) is significantly larger than its 5% critical value (0.146), leading to the rejection of the null hypothesis of stationarity.", "B": "The ADF test fails to reject the unit root null hypothesis for the price level series but rejects it for the first-difference series, indicating the series is integrated of order one, I(1).", "C": "The ADF and KPSS tests provide conflicting conclusions, as the ADF test supports non-stationarity while the KPSS test supports stationarity for the price level series.", "D": "The ADF test on the price level series (statistic = -0.882) provides strong evidence to reject the null hypothesis of a unit root at the 5% significance level."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to synthesize evidence from two complementary statistical tests. It uses an 'Atomic Decomposition' strategy. Options A and B test the correct, standalone interpretation of the ADF and KPSS results, respectively. The distractors target common, high-frequency misconceptions. Distractor C presents the classic error of misinterpreting the KPSS test's rejection of stationarity as evidence *for* stationarity. Distractor D is a 'Conceptual Opposite' that tests for a basic misreading of the ADF test outcome. The item effectively captures the core inferential logic of the paper's stationarity analysis.", "qid": "205", "question": "### Background\n\n**Research Question.** How can statistical tests be combined to build a robust case for the non-stationarity of financial price series, a key property implied by the weak-form Efficient Market Hypothesis?\n\n**Setting / Data-Generating Environment.** The analysis uses daily closing prices for UK financial futures. To test for non-stationarity, the paper employs two complementary methods: the Augmented Dickey-Fuller (ADF) test, which has a null hypothesis of a unit root (non-stationarity), and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, which has a null hypothesis of stationarity.\n\n---\n\n### Data / Model Specification\n\nThe Augmented Dickey-Fuller (ADF) test has a null hypothesis `H₀: The series has a unit root`.\nThe Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test has a null hypothesis `H₀: The series is stationary`.\n\n**Table 1. Stationarity Test Results for FTSE100 Futures**\n\n| Panel | Test | Data Series | Lag | Test Statistic | 5% Critical Value |\n|---|---|---|---|---|---|\n| A | ADF | Price Level | 1 | -0.882 | -3.4139 |\n| A | ADF | First Difference | 1 | -43.03 | -3.4139 |\n| B | KPSS | Price Level | 8 | 3.4577 | 0.146 |\n\n*Source: Adapted from Tables 2 and 3 of the source paper.*\n\n---\n\n### Question\n\nBased on the data presented in **Table 1**, select ALL of the following statements that correctly interpret and synthesize the test results.", "model_evaluation_result": {"model_raw_response": "Looking at the test results in Table 1, I need to evaluate each statement:\n\n**Statement A**: The KPSS test statistic (3.4577) is significantly larger than its 5% critical value (0.146), leading to the rejection of the null hypothesis of stationarity.\n- KPSS statistic: 3.4577\n- Critical value: 0.146\n- Since 3.4577 > 0.146, we reject the null hypothesis of stationarity\n- This statement is **CORRECT**\n\n**Statement B**: The ADF test fails to reject the unit root null hypothesis for the price level series but rejects it for the first-difference series, indicating the series is integrated of order one, I(1).\n- Price level ADF: -0.882 > -3.4139 (fails to reject unit root)\n- First difference ADF: -43.03 < -3.4139 (rejects unit root)\n- This pattern indicates I(1) behavior\n- This statement is **CORRECT**\n\n**Statement C**: The ADF and KPSS tests provide conflicting conclusions, as the ADF test supports non-stationarity while the KPSS test supports stationarity for the price level series.\n- ADF fails to reject unit root (supports non-stationarity)\n- KPSS rejects stationarity (supports non-stationarity)\n- Both tests actually support the same conclusion\n- This statement is **INCORRECT**\n\n**Statement D**: The ADF test on the price level series (statistic = -0.882) provides strong evidence to reject the null hypothesis of a unit root at the 5% significance level.\n- ADF statistic: -0.882\n- Critical value: -3.4139\n- Since -0.882 > -3.4139, we fail to reject the unit root\n- This statement is **INCORRECT**\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 17, "Question": "### Background\n\n**Research Question.** How is the fair periodic premium for a unit-linked endowment with guarantees determined by equating the expected present values of premiums and benefits?\n\n**Setting.** An insurance contract for a policyholder aged `x` at inception, with `n` yearly premium payments. The valuation assumes independence between financial markets and mortality risk.\n\n### Data / Model Specification\n\nThe principle of actuarial equivalence states that the expected present value (EPV) of premiums must equal the EPV of benefits. The detailed equation for the fair premium `P` is:\n\n  \nP \\cdot \\underbrace{\\sum_{k=0}^{n-1} P(0,k) \\cdot_k p_x}_{\\text{EPV of Annuity}} = \\underbrace{\\sum_{k=1}^{n} {_{k-1}p_x q_{x+k-1}} \\cdot B_k + _n p_x \\cdot B_n}_{\\text{EPV of Benefits}} \\quad \\text{(Eq. 1)}\n \n\nwhere `B_k` is the time-0 value of the benefit paid at time `k`, given by `B_k = G \\cdot P(0,k) + C_0(X(k),G)`. The term `_k p_x` is the probability of surviving `k` years, and `_{k-1}p_x q_{x+k-1}` is the probability of dying in year `k`.\n\n---\n\nAn insurer is pricing this contract but discovers their mortality tables are outdated and do not account for future improvements in longevity. The true survival probabilities (`_k p_x`) are higher, and thus the true probabilities of dying in any given year (`_{k-1}p_x q_{x+k-1}`) are lower, than the values used in their pricing model. According to the principles in **Eq. (1)**, which of the following are necessary consequences of this mis-estimation?\n", "Options": {"A": "The true Expected Present Value of the death benefits (paid out between year 1 and year `n`) is lower than what the insurer calculated.", "B": "The fair premium `P` calculated by the insurer using the outdated tables is too high, leading to excess profits.", "C": "The true Expected Present Value of the survival benefit (paid out at year `n`) is higher than what the insurer calculated.", "D": "The true Expected Present Value of the premium stream paid by the policyholder is higher than what the insurer calculated."}, "Answer": ["A", "C", "D"], "pi_justification": "This item uses a Scenario Application strategy, asking the user to apply the core premium formula to a novel scenario (mis-estimated mortality). It assesses a deep understanding of how actuarial components drive the valuation.\n- **Option A (Correct):** Higher survival probabilities (`_k p_x`) mean the policyholder is more likely to be alive to pay premiums in later years. This increases the value of the annuity factor on the LHS of Eq. (1), increasing the EPV of premiums.\n- **Option B (Correct):** Higher longevity means lower probabilities of dying in any given year before maturity. This directly reduces the EPV of the death benefit component of the liability.\n- **Option C (Correct):** Higher longevity means a higher probability of surviving to the maturity date `n`. This increases the EPV of the final survival benefit.\n- **Option D (Distractor - Conceptual Opposite):** This is the ultimate conclusion the user must avoid. While the EPV of death benefits decreases, the EPV of the survival benefit (paid at the latest date) increases significantly. The net effect is that the total EPV of benefits increases, meaning the calculated premium `P` was too low, not too high.", "qid": "17", "question": "### Background\n\n**Research Question.** How is the fair periodic premium for a unit-linked endowment with guarantees determined by equating the expected present values of premiums and benefits?\n\n**Setting.** An insurance contract for a policyholder aged `x` at inception, with `n` yearly premium payments. The valuation assumes independence between financial markets and mortality risk.\n\n### Data / Model Specification\n\nThe principle of actuarial equivalence states that the expected present value (EPV) of premiums must equal the EPV of benefits. The detailed equation for the fair premium `P` is:\n\n  \nP \\cdot \\underbrace{\\sum_{k=0}^{n-1} P(0,k) \\cdot_k p_x}_{\\text{EPV of Annuity}} = \\underbrace{\\sum_{k=1}^{n} {_{k-1}p_x q_{x+k-1}} \\cdot B_k + _n p_x \\cdot B_n}_{\\text{EPV of Benefits}} \\quad \\text{(Eq. 1)}\n \n\nwhere `B_k` is the time-0 value of the benefit paid at time `k`, given by `B_k = G \\cdot P(0,k) + C_0(X(k),G)`. The term `_k p_x` is the probability of surviving `k` years, and `_{k-1}p_x q_{x+k-1}` is the probability of dying in year `k`.\n\n---\n\nAn insurer is pricing this contract but discovers their mortality tables are outdated and do not account for future improvements in longevity. The true survival probabilities (`_k p_x`) are higher, and thus the true probabilities of dying in any given year (`_{k-1}p_x q_{x+k-1}`) are lower, than the values used in their pricing model. According to the principles in **Eq. (1)**, which of the following are necessary consequences of this mis-estimation?\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze how using outdated mortality tables (that underestimate survival probabilities) affects each component of the actuarial equivalence equation.\n\nGiven information:\n- True survival probabilities (_k p_x) are HIGHER than the insurer's estimates\n- True death probabilities (_{k-1}p_x q_{x+k-1}) are LOWER than the insurer's estimates\n\nLet me analyze each option:\n\n**Option A**: The EPV of death benefits = Σ_{k=1}^{n} {_{k-1}p_x q_{x+k-1}} · B_k\n\nSince the true death probabilities are LOWER than estimated, the true EPV of death benefits is LOWER than calculated. ✓\n\n**Option B**: This requires careful analysis. The insurer sets premium P by solving:\nP · (EPV of Annuity) = (EPV of Benefits)\n\nWith outdated tables, they overestimate death benefits and underestimate survival benefits. However, they also underestimate the EPV of the premium annuity (since true survival probabilities are higher). The net effect on the calculated premium P is ambiguous - it depends on the relative magnitudes and timing of these effects. ✗\n\n**Option C**: The EPV of survival benefit = _n p_x · B_n\n\nSince the true n-year survival probability is HIGHER than estimated, the true EPV of the survival benefit is HIGHER than calculated. ✓\n\n**Option D**: The EPV of premium stream = P · Σ_{k=0}^{n-1} P(0,k) · _k p_x\n\nSince the true survival probabilities are HIGHER than estimated, the true EPV of the premium stream is HIGHER than calculated. ✓\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 9, "Question": "### Background\n\n**Research Question.** A celebrated finding in empirical finance is that patterns of stock return predictability, such as monotonically increasing coefficients and R-squareds, become stronger at longer horizons. This analysis uses a simulation-based identification strategy to determine if these patterns are evidence of true predictability or statistical artifacts generated under the null hypothesis of no predictability.\n\n**Setting and Environment.** The analysis is based on 100,000 Monte Carlo simulations. Each simulation generates 75 years of annual data where returns are IID noise (no predictability) and the predictor `X_t` follows a persistent AR(1) process calibrated to the historical log dividend yield (`ρ=0.953`). From each simulated dataset, predictive regressions are estimated for horizons of 1 to 5 years.\n\n### Data / Model Specification\n\nThe following tables summarize key results from the simulations conducted under the null hypothesis of no predictability. For context, the actual empirical coefficient ratios from regressing 1- to 5-year stock returns on the log dividend yield over the 1926–2004 sample period are 1.96, 2.98, 3.53, and 3.99.\n\n**Table 1: Simulated Distribution of Coefficient Estimates (`β̂_j`)**\n\n| Horizon (j) | Correlation with `β̂_2` | Correlation with `β̂_3` | Correlation with `β̂_4` | Correlation with `β̂_5` |\n| :--- | :--- | :--- | :--- | :--- |\n| **1** | 0.966 | 0.926 | 0.885 | 0.843 |\n| **2** | 1.000 | 0.980 | 0.946 | 0.909 |\n\n- **Percentage of simulations with monotonic coefficients:** 66.02%\n\n**Table 2: Simulated Distribution of `R²` Statistics**\n\n| Horizon (j) | Correlation with `R₁²` | Correlation with `R₂²` |\n| :--- | :--- | :--- |\n| **1** | 1.000 | 0.949 |\n| **2** | | 1.000 |\n\n- **Percentage of simulations with monotonic `R²`s:** 52.21%\n\n**Table 3: Simulated Distribution of Cross-Horizon Ratios**\n\n| Horizon (j) | Mean Ratio `β̂_j / β̂_1` | Mean Ratio `R_j² / R_1²` |\n| :--- | :--- | :--- |\n| 2 | 1.93 | 1.96 |\n| 3 | 2.80 | 2.88 |\n| 4 | 3.59 | 3.77 |\n| 5 | 4.32 | 4.61 |\n\n### Question\n\nBased on the simulation results conducted under the null hypothesis of no predictability, which of the following conclusions are supported by the provided tables? Select all that apply.", "Options": {"A": "The near-perfect correlation (0.966) between the 1-year and 2-year coefficient estimators implies that they offer little independent information, as both reflect the same underlying sampling error.", "B": "The close match between the simulated mean coefficient ratio for the 2-year horizon (1.93) and the actual empirical ratio (1.96) strongly suggests the observed empirical pattern is a statistical artifact.", "C": "The high frequency of monotonic coefficient patterns (66.02%) is a natural artifact of predictor persistence under the null, not evidence of strengthening predictability.", "D": "The fact that R-squareds are monotonic over half the time (52.21%) provides robust evidence that long-horizon regressions have superior explanatory power."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the core simulation evidence from the paper. It uses the Atomic Decomposition strategy, breaking down the original QA problem's three parts into three independently correct options (A, B, C). The distractor (D) represents a Conceptual Opposite, stating the very misinterpretation the paper seeks to debunk, making it a high-fidelity foil.", "qid": "9", "question": "### Background\n\n**Research Question.** A celebrated finding in empirical finance is that patterns of stock return predictability, such as monotonically increasing coefficients and R-squareds, become stronger at longer horizons. This analysis uses a simulation-based identification strategy to determine if these patterns are evidence of true predictability or statistical artifacts generated under the null hypothesis of no predictability.\n\n**Setting and Environment.** The analysis is based on 100,000 Monte Carlo simulations. Each simulation generates 75 years of annual data where returns are IID noise (no predictability) and the predictor `X_t` follows a persistent AR(1) process calibrated to the historical log dividend yield (`ρ=0.953`). From each simulated dataset, predictive regressions are estimated for horizons of 1 to 5 years.\n\n### Data / Model Specification\n\nThe following tables summarize key results from the simulations conducted under the null hypothesis of no predictability. For context, the actual empirical coefficient ratios from regressing 1- to 5-year stock returns on the log dividend yield over the 1926–2004 sample period are 1.96, 2.98, 3.53, and 3.99.\n\n**Table 1: Simulated Distribution of Coefficient Estimates (`β̂_j`)**\n\n| Horizon (j) | Correlation with `β̂_2` | Correlation with `β̂_3` | Correlation with `β̂_4` | Correlation with `β̂_5` |\n| :--- | :--- | :--- | :--- | :--- |\n| **1** | 0.966 | 0.926 | 0.885 | 0.843 |\n| **2** | 1.000 | 0.980 | 0.946 | 0.909 |\n\n- **Percentage of simulations with monotonic coefficients:** 66.02%\n\n**Table 2: Simulated Distribution of `R²` Statistics**\n\n| Horizon (j) | Correlation with `R₁²` | Correlation with `R₂²` |\n| :--- | :--- | :--- |\n| **1** | 1.000 | 0.949 |\n| **2** | | 1.000 |\n\n- **Percentage of simulations with monotonic `R²`s:** 52.21%\n\n**Table 3: Simulated Distribution of Cross-Horizon Ratios**\n\n| Horizon (j) | Mean Ratio `β̂_j / β̂_1` | Mean Ratio `R_j² / R_1²` |\n| :--- | :--- | :--- |\n| 2 | 1.93 | 1.96 |\n| 3 | 2.80 | 2.88 |\n| 4 | 3.59 | 3.77 |\n| 5 | 4.32 | 4.61 |\n\n### Question\n\nBased on the simulation results conducted under the null hypothesis of no predictability, which of the following conclusions are supported by the provided tables? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the simulation results conducted under the null hypothesis of no predictability:\n\n**Option A**: The correlation of 0.966 between 1-year and 2-year coefficient estimators is indeed near-perfect. Under the null hypothesis, this high correlation indicates that both estimators are capturing the same sampling variation rather than independent predictive information. This is correct.\n\n**Option B**: The simulated mean coefficient ratio for the 2-year horizon (1.93) is remarkably close to the actual empirical ratio (1.96). Since this pattern emerges even under the null hypothesis of no predictability, it strongly suggests the empirical pattern could be a statistical artifact rather than evidence of true predictability. This is correct.\n\n**Option C**: The fact that 66.02% of simulations show monotonic coefficient patterns under the null hypothesis demonstrates that such patterns can arise naturally from predictor persistence alone, without any true predictability. This is correct.\n\n**Option D**: The fact that R-squareds are monotonic 52.21% of the time under the null hypothesis actually undermines the claim that this provides evidence of superior explanatory power. Since this pattern occurs frequently even when there is no true predictability, it cannot be interpreted as robust evidence of genuine predictive power. This is incorrect.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 43, "Question": "### Background\n\n**Research Question.** Is the negative stock price contagion from a bond downgrade transmitted to rivals through changes in their expected future cash flows, and does this transmission mechanism depend on the downgraded firm's credit quality?\n\n**Setting / Data-Generating Environment.** The study investigates the link between stock price reactions (measured by Cumulative Abnormal Returns, `CAR`) and earnings forecast revisions (measured by Cumulative Abnormal Forecast Revisions, `CAFR`) for rival firms following a downgrade announcement. The central hypothesis is that revisions to expected cash flows (proxied by `CAFR`) are the channel driving the valuation changes (measured by `CAR`).\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return for rival firms over the `[-1, 0]` day window (dimensionless, as a percentage).\n- `CAFR`: Cumulative Abnormal Forecast Revision for rival firms over months `t=0` to `t=3` (dimensionless, as a percentage).\n- **Group 1:** Downgrade occurs within the investment-grade category.\n- **Group 2:** Downgrade is from investment to non-investment grade.\n- **Group 3:** Downgrade occurs within the non-investment-grade category.\n\n---\n\n### Data / Model Specification\n\nTable 1 synthesizes the key contagion results for rival firms, showing both the stock market reaction and the earnings forecast revision across the three groups of downgraded firms.\n\n**Table 1. Contagion Effects on Rival Firms**\n\n| Group | Rival CAR (%) <br> (Days -1, 0) | Rival CAR <br> Z-stat | Rival CAFR <br> (Months 0-3) | Rival CAFR <br> t-stat |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | -0.12 | (-1.45) | -0.40 | (-1.42) |\n| 2 | -0.37 | (-0.29) | -1.19 | (-1.69) |\n| 3 | -0.34 | (-2.15)* | -0.69 | (-2.75)** |\n\n*Significant at the 5% level. **Significant at the 1% level.\n\n---\n\n### Question\n\nBased on the evidence in Table 1 regarding contagion effects on rival firms, select all of the following statements that are correct.", "Options": {"A": "The evidence suggests that for rivals of Group 1 firms, the market prices in a negative reaction (significant CAR) even though analysts do not revise earnings forecasts downwards (insignificant CAFR).", "B": "Rivals of firms in Group 3 experience a statistically significant negative revision to their earnings forecasts (CAFR).", "C": "Rivals of firms in Group 2 experience a statistically significant negative stock return (CAR), but not a significant earnings forecast revision (CAFR).", "D": "Rivals of firms in Group 3 experience a statistically significant negative stock return (CAR) following a downgrade announcement."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize results from a table to identify the specific conditions under which contagion occurs. It uses an 'atomic decomposition' strategy, breaking the paper's main finding into two independently verifiable statements (significant CAR and significant CAFR in Group 3). Distractors test for common misinterpretations of statistical significance (Option C) and misreading of table values for other groups (Option D).", "qid": "43", "question": "### Background\n\n**Research Question.** Is the negative stock price contagion from a bond downgrade transmitted to rivals through changes in their expected future cash flows, and does this transmission mechanism depend on the downgraded firm's credit quality?\n\n**Setting / Data-Generating Environment.** The study investigates the link between stock price reactions (measured by Cumulative Abnormal Returns, `CAR`) and earnings forecast revisions (measured by Cumulative Abnormal Forecast Revisions, `CAFR`) for rival firms following a downgrade announcement. The central hypothesis is that revisions to expected cash flows (proxied by `CAFR`) are the channel driving the valuation changes (measured by `CAR`).\n\n**Variables & Parameters.**\n- `CAR`: Cumulative Abnormal Return for rival firms over the `[-1, 0]` day window (dimensionless, as a percentage).\n- `CAFR`: Cumulative Abnormal Forecast Revision for rival firms over months `t=0` to `t=3` (dimensionless, as a percentage).\n- **Group 1:** Downgrade occurs within the investment-grade category.\n- **Group 2:** Downgrade is from investment to non-investment grade.\n- **Group 3:** Downgrade occurs within the non-investment-grade category.\n\n---\n\n### Data / Model Specification\n\nTable 1 synthesizes the key contagion results for rival firms, showing both the stock market reaction and the earnings forecast revision across the three groups of downgraded firms.\n\n**Table 1. Contagion Effects on Rival Firms**\n\n| Group | Rival CAR (%) <br> (Days -1, 0) | Rival CAR <br> Z-stat | Rival CAFR <br> (Months 0-3) | Rival CAFR <br> t-stat |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | -0.12 | (-1.45) | -0.40 | (-1.42) |\n| 2 | -0.37 | (-0.29) | -1.19 | (-1.69) |\n| 3 | -0.34 | (-2.15)* | -0.69 | (-2.75)** |\n\n*Significant at the 5% level. **Significant at the 1% level.\n\n---\n\n### Question\n\nBased on the evidence in Table 1 regarding contagion effects on rival firms, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each statement against the data:\n\n**Statement A**: Claims Group 1 rivals have significant negative CAR but insignificant CAFR.\n- Group 1 CAR: -0.12% with Z-stat (-1.45) - not significant\n- Group 1 CAFR: -0.40% with t-stat (-1.42) - not significant\n- This statement is **incorrect** because the CAR is not significant.\n\n**Statement B**: Claims Group 3 rivals have statistically significant negative CAFR.\n- Group 3 CAFR: -0.69% with t-stat (-2.75)**\n- The ** indicates significance at the 1% level\n- This statement is **correct**.\n\n**Statement C**: Claims Group 2 rivals have significant negative CAR but not significant CAFR.\n- Group 2 CAR: -0.37% with Z-stat (-0.29) - not significant\n- Group 2 CAFR: -1.19% with t-stat (-1.69) - not significant\n- This statement is **incorrect** because the CAR is not significant.\n\n**Statement D**: Claims Group 3 rivals have statistically significant negative CAR.\n- Group 3 CAR: -0.34% with Z-stat (-2.15)*\n- The * indicates significance at the 5% level\n- This statement is **correct**.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 173, "Question": "### Background\n\n**Research Question.** How can the fair value of an insurer's policy reserve be calculated in a closed form when its accumulation depends on a series of embedded options related to market performance?\n\n**Setting and Environment.** We operate in a frictionless Black-Scholes market with a constant risk-free rate `r` and an equity fund `A` following a geometric Brownian motion under the risk-neutral measure `\\hat{\\mathbb{P}}`. The objective is to value the policy reserve, `V_P(t)`, for a participating contract.\n\n**Variables and Parameters.**\n- `V_P(t)`: Market value of the policy reserve at time `t`.\n- `P(t)`: Policyholder's account value at time `t`.\n- `r_P(k)`: Annual crediting rate for the policyholder's account.\n- `r_A(k)`: Annual return on the equity fund `A`.\n- `r_G`: Guaranteed annual crediting rate.\n- `\\beta`: Participation rate in the fund's returns.\n- `r`: Continuously compounded risk-free rate.\n- `\\sigma`: Volatility of the equity fund `A`.\n- `T`: Maturity of the contract in years.\n- `\\hat{\\mathbb{E}}`: Expectation operator under the risk-neutral measure.\n\n---\n\n### Data / Model Specification\n\nThe policyholder's account accumulates annually with a crediting rate given by:\n\n  \nr_{P}(k) = \\max\\{r_{G}, \\beta r_{A}(k)\\} \\quad \\text{where} \\quad r_{A}(k) = \\frac{A(k)-A(k-1)}{A(k-1)} \\quad \\text{(Eq. (1))}\n \n\nThis structure allows for the derivation of a closed-form solution for the policy reserve:\n\n  \nV_P(t) = P(t) \\left[ \\mathrm{e}^{-r}(1+r_{G})+\\beta N(d_{1})-\\mathrm{e}^{-r}(\\beta+r_{G})N(d_{2}) \\right]^{T-t} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### The Question\n\nThe ability to simplify the valuation of `V_P(t)` into the closed-form expression shown in Eq. (2) depends critically on which of the following assumptions? Select all that apply.", "Options": {"A": "The annual crediting rate, `r_P(k)`, depends only on the return in the current year `k`.", "B": "The annual returns on the equity fund, `r_A(k)`, are independent and identically distributed (i.i.d.).", "C": "The risk-free interest rate `r` is stochastic.", "D": "The contract includes a path-dependent feature, such as a high-water mark guarantee."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the understanding of the critical assumptions that enable the closed-form solution. It uses a Reverse-Reasoning strategy. The correct options (A, B) state the necessary conditions: i.i.d. returns (a consequence of the GBM model) and a memoryless contract design. The distractors represent conditions that would explicitly violate the model's requirements: path-dependency (C) and stochastic interest rates (D), both of which would prevent the simple product-of-expectations simplification.", "qid": "173", "question": "### Background\n\n**Research Question.** How can the fair value of an insurer's policy reserve be calculated in a closed form when its accumulation depends on a series of embedded options related to market performance?\n\n**Setting and Environment.** We operate in a frictionless Black-Scholes market with a constant risk-free rate `r` and an equity fund `A` following a geometric Brownian motion under the risk-neutral measure `\\hat{\\mathbb{P}}`. The objective is to value the policy reserve, `V_P(t)`, for a participating contract.\n\n**Variables and Parameters.**\n- `V_P(t)`: Market value of the policy reserve at time `t`.\n- `P(t)`: Policyholder's account value at time `t`.\n- `r_P(k)`: Annual crediting rate for the policyholder's account.\n- `r_A(k)`: Annual return on the equity fund `A`.\n- `r_G`: Guaranteed annual crediting rate.\n- `\\beta`: Participation rate in the fund's returns.\n- `r`: Continuously compounded risk-free rate.\n- `\\sigma`: Volatility of the equity fund `A`.\n- `T`: Maturity of the contract in years.\n- `\\hat{\\mathbb{E}}`: Expectation operator under the risk-neutral measure.\n\n---\n\n### Data / Model Specification\n\nThe policyholder's account accumulates annually with a crediting rate given by:\n\n  \nr_{P}(k) = \\max\\{r_{G}, \\beta r_{A}(k)\\} \\quad \\text{where} \\quad r_{A}(k) = \\frac{A(k)-A(k-1)}{A(k-1)} \\quad \\text{(Eq. (1))}\n \n\nThis structure allows for the derivation of a closed-form solution for the policy reserve:\n\n  \nV_P(t) = P(t) \\left[ \\mathrm{e}^{-r}(1+r_{G})+\\beta N(d_{1})-\\mathrm{e}^{-r}(\\beta+r_{G})N(d_{2}) \\right]^{T-t} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### The Question\n\nThe ability to simplify the valuation of `V_P(t)` into the closed-form expression shown in Eq. (2) depends critically on which of the following assumptions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to identify which assumptions are critical for obtaining the closed-form solution in Eq. (2).\n\nThe closed-form solution has the structure of a power function raised to (T-t), suggesting that the value can be decomposed into identical annual components. This is only possible under specific conditions.\n\nLet me analyze each option:\n\n**Option A**: The annual crediting rate r_P(k) depends only on the return in the current year k.\n- This is TRUE from Eq. (1): r_P(k) = max{r_G, β r_A(k)}\n- The crediting rate depends only on the current year's return r_A(k), not on past returns\n- This Markovian property is essential for the recursive valuation that leads to the closed form\n\n**Option B**: The annual returns r_A(k) are independent and identically distributed (i.i.d.).\n- Under the Black-Scholes model with geometric Brownian motion, log returns are i.i.d. normal\n- This means annual returns r_A(k) are i.i.d. log-normal\n- The i.i.d. property allows the multi-period valuation to be expressed as a power of a single-period value\n- This is critical for obtaining the form [single-period value]^(T-t)\n\n**Option C**: The risk-free interest rate r is stochastic.\n- The problem states we have a \"constant risk-free rate r\"\n- A stochastic interest rate would significantly complicate the valuation\n- This would prevent the clean closed-form solution shown\n- This assumption is NOT required (in fact, the opposite is required)\n\n**Option D**: The contract includes path-dependent features like a high-water mark.\n- Path-dependent features would make the payoff depend on the entire history of returns\n- This would violate the Markovian structure needed for the recursive valuation\n- The closed form in Eq. (2) shows no path-dependent terms\n- This assumption would prevent, not enable, the closed-form solution\n\nTherefore, the critical assumptions are A and B.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 100, "Question": "### Background\n\nTwo competing theories attempt to explain corporate financing decisions. The Pecking Order (PO) theory posits that firms follow a strict hierarchy (Internal Funds -> Debt -> Equity) to minimize information costs, without a specific leverage target. The Target Adjustment (TAR) theory posits that firms have an optimal leverage ratio and adjust towards it over time, balancing costs and benefits like tax shields and financial distress costs.\n\n### Data / Model Specification\n\nTo distinguish between these theories, a \"horse race\" regression is often estimated:\n\n  \n\\Delta D_{i,t} = \\alpha + \\beta_1 DEF_{i,t} + \\beta_2 (Lev^*_{i} - Lev_{i,t-1}) + \\epsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `\\Delta D_{i,t}` is the change in firm `i`'s debt in year `t`.\n- `DEF_{i,t}` is the financing deficit (investment needs minus internal cash flow).\n- `(Lev^*_{i} - Lev_{i,t-1})` is the leverage deficit (the gap between the target leverage and last period's actual leverage).\n\n---\n\nWhich of the following statements accurately describe the predictions, interpretations, or empirical challenges related to these models? Select all that apply.", "Options": {"A": "A key reason the Pecking Order theory prefers debt over equity for external financing is that the value of debt is less sensitive to managers' private information about firm prospects, thus reducing the 'lemons' problem associated with security issuance.", "B": "The Target Adjustment theory predicts that the coefficient `β₂` should be positive and statistically significant, reflecting that firms issue debt to close a leverage deficit (i.e., when actual leverage is below the target).", "C": "A finding that both `β₁` and `β₂` are statistically significant would invalidate both theories, as they are mutually exclusive.", "D": "The Pecking Order theory predicts that the coefficient `β₁` should be positive and close to 1, while `β₂` should be statistically insignificant."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses a comprehensive understanding of the two main competing theories of capital structure: Pecking Order and Target Adjustment. It uses a Reverse-Reasoning strategy by asking the user to identify valid statements about the theories and their empirical tests. Option A correctly states the empirical prediction of the PO theory in the horse race regression. Option B correctly states the prediction of the TAR theory. Option C correctly identifies the core information-asymmetry logic behind the PO theory's preference for debt. Distractor D is a Conceptual Opposite; finding both coefficients significant would suggest that both financing flows and stock adjustments matter, leading to a more integrated theory. Distractor E is also a Conceptual Opposite; the PO theory predicts these firms will have *low* leverage because their high cash flows mean they rarely need external financing.", "qid": "100", "question": "### Background\n\nTwo competing theories attempt to explain corporate financing decisions. The Pecking Order (PO) theory posits that firms follow a strict hierarchy (Internal Funds -> Debt -> Equity) to minimize information costs, without a specific leverage target. The Target Adjustment (TAR) theory posits that firms have an optimal leverage ratio and adjust towards it over time, balancing costs and benefits like tax shields and financial distress costs.\n\n### Data / Model Specification\n\nTo distinguish between these theories, a \"horse race\" regression is often estimated:\n\n  \n\\Delta D_{i,t} = \\alpha + \\beta_1 DEF_{i,t} + \\beta_2 (Lev^*_{i} - Lev_{i,t-1}) + \\epsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `\\Delta D_{i,t}` is the change in firm `i`'s debt in year `t`.\n- `DEF_{i,t}` is the financing deficit (investment needs minus internal cash flow).\n- `(Lev^*_{i} - Lev_{i,t-1})` is the leverage deficit (the gap between the target leverage and last period's actual leverage).\n\n---\n\nWhich of the following statements accurately describe the predictions, interpretations, or empirical challenges related to these models? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\n**A) A key reason the Pecking Order theory prefers debt over equity for external financing is that the value of debt is less sensitive to managers' private information about firm prospects, thus reducing the 'lemons' problem associated with security issuance.**\n\nThis is correct. The Pecking Order theory is fundamentally based on information asymmetry. Debt is preferred over equity because debt values are less sensitive to private information about firm value. This reduces the adverse selection (\"lemons\") problem where investors discount securities due to information disadvantage.\n\n**B) The Target Adjustment theory predicts that the coefficient `β₂` should be positive and statistically significant, reflecting that firms issue debt to close a leverage deficit (i.e., when actual leverage is below the target).**\n\nThis is correct. Under TAR theory, when Lev* > Lev_{t-1} (positive leverage deficit), firms should increase debt to move toward their target, resulting in a positive relationship between the leverage deficit and change in debt.\n\n**C) A finding that both `β₁` and `β₂` are statistically significant would invalidate both theories, as they are mutually exclusive.**\n\nThis is incorrect. The theories are not mutually exclusive in practice. Firms might follow both patterns to some degree - addressing financing deficits while also adjusting toward leverage targets. Finding both coefficients significant suggests elements of both theories are at work.\n\n**D) The Pecking Order theory predicts that the coefficient `β₁` should be positive and close to 1, while `β₂` should be statistically insignificant.**\n\nThis is correct. Pure Pecking Order theory predicts firms use debt dollar-for-dollar to meet financing deficits (β₁ ≈ 1) and don't consider leverage targets (β₂ ≈ 0).\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 202, "Question": "### Background\n\nThis paper proposes that life insurance regulation should be viewed primarily as a mechanism for enforcing long-term, often incomplete, contracts. This approach is designed to solve specific market failures inherent to the life insurance industry, which differ significantly from those in banking.\n\n### Question\n\nAccording to the paper's analysis of the life insurance market, which of the following statements are correct? Select all that apply.", "Options": {"A": "The illiquidity of life insurance policies creates a captive pool of existing policyholders, which can weaken incentives for information disclosure.", "B": "Regulators should aim to minimize the probability of any insurer failing, for example, by setting a universal failure probability target of 0.5%.", "C": "The primary justification for life insurance regulation is to serve as a third-party enforcement mechanism for long-term, incomplete contracts.", "D": "The main regulatory concern for life insurers, similar to banks, is preventing systemic risk from 'insurance runs'."}, "Answer": ["A", "C"], "pi_justification": "This item assesses a synthetic understanding of the paper's core arguments. It requires students to identify the central thesis (A), the key market failure (B), and reject incorrect rationales that the paper explicitly refutes (C and D). Distractor C is a direct Conceptual Opposite. Distractor D (True but Irrelevant/Misleading) represents the type of one-size-fits-all regulation the paper criticizes as potentially inefficient and not aligned with the contract enforcement view.", "qid": "202", "question": "### Background\n\nThis paper proposes that life insurance regulation should be viewed primarily as a mechanism for enforcing long-term, often incomplete, contracts. This approach is designed to solve specific market failures inherent to the life insurance industry, which differ significantly from those in banking.\n\n### Question\n\nAccording to the paper's analysis of the life insurance market, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) The illiquidity of life insurance policies creates a captive pool of existing policyholders, which can weaken incentives for information disclosure.\n- This is correct. The paper identifies that life insurance policies are illiquid (unlike bank deposits), creating a captive customer base. This captivity reduces competitive pressure and weakens incentives for proper information disclosure to policyholders.\n\nB) Regulators should aim to minimize the probability of any insurer failing, for example, by setting a universal failure probability target of 0.5%.\n- This is incorrect. The paper argues against a one-size-fits-all approach and suggests that optimal failure probabilities should vary based on factors like firm characteristics and policyholder preferences.\n\nC) The primary justification for life insurance regulation is to serve as a third-party enforcement mechanism for long-term, incomplete contracts.\n- This is correct. This is the central thesis of the paper - that life insurance regulation should be viewed primarily as a mechanism for enforcing long-term, incomplete contracts.\n\nD) The main regulatory concern for life insurers, similar to banks, is preventing systemic risk from 'insurance runs'.\n- This is incorrect. The paper explicitly distinguishes life insurance from banking, noting that life insurance policies are illiquid and therefore not subject to runs like bank deposits. The regulatory concerns are different.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 119, "Question": "### Background\n\n**Research Question.** Has the efficiency of Indonesian banks converged over time, and how did the Asian financial crisis and subsequent reforms affect the speed of this convergence?\n\n**Setting / Data-Generating Environment.** Panel data analysis of Indonesian bank efficiency is used to test for `β`-convergence (laggards catching up) and `σ`-convergence (reduction in overall dispersion) before and after the 1997-1999 financial crisis.\n\n**Variables & Parameters.**\n\n*   `$\\ln(\\hat{\\hat{\\rho}}_{j,t})$`: Log of the bias-corrected efficiency score for bank `j` at time `t`.\n*   `$E_{j,t}$`: Log deviation of bank `j`'s efficiency from the industry mean at time `t`.\n*   `$\\theta$`: The `β`-convergence parameter. A negative value indicates catch-up.\n*   `$\\eta$`: The `σ`-convergence parameter. A negative value indicates shrinking dispersion.\n\n---\n\n### Data / Model Specification\n\nThe models for `β`- and `σ`-convergence are given by:\n\n  \n\\ln\\hat{\\hat{\\rho}}_{j,t} - \\ln\\hat{\\hat{\\rho}}_{j,t-1} = \\phi + \\theta \\ln\\hat{\\hat{\\rho}}_{j,t} + \\nu_{j,t} \\quad \\text{(Eq. 1: } \\beta \\text{-convergence)}\n \n\n  \nE_{j,t} - E_{j,t-1} = \\alpha + \\eta E_{j,t-1} + \\omega_{j,t} \\quad \\text{(Eq. 2: } \\sigma \\text{-convergence)}\n \n\n**Table 1. Convergence Results (Model 2)**\n\n| Period | Parameter | Coefficient | P-value |\n| :--- | :--- | :--- | :--- |\n| **`β`-Convergence** | | |\n| 1992-1996 | `$\\theta$` | -1.0436 | 0.0000 |\n| 2000-2007 | `$\\theta$` | -0.8249 | 0.0000 |\n| **`σ`-Convergence** | | |\n| 1992-1996 | `$\\eta$` | -1.0509 | 0.0000 |\n| 2000-2007 | `$\\eta$` | -0.8997 | 0.0000 |\n\n---\n\n### Question\n\nBased on the convergence models and the results for Model 2 in Table 1, select all statements that are correct interpretations of the findings.", "Options": {"A": "`β`-convergence is a sufficient condition for `σ`-convergence, meaning a reduction in dispersion is guaranteed if laggard banks are catching up.", "B": "The results indicate that the speed of `β`-convergence was faster in the post-crisis period (2000-2007) than in the pre-crisis period (1992-1996).", "C": "The significant negative coefficient for `$\\eta$` in the post-crisis period provides evidence of `σ`-convergence, meaning the overall dispersion of efficiency scores across the banking sector was decreasing over time.", "D": "The significant negative coefficient for `$\\theta$` in the post-crisis period provides evidence of `β`-convergence, meaning that less efficient banks tended to improve their efficiency at a faster rate than more efficient banks."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the fundamental interpretation of the two types of convergence, which is central to the paper's conclusion. It uses an atomic decomposition strategy. Distractor C is a conceptual opposite based on a direct comparison of coefficients from the table. Distractor D targets a common theoretical misconception about the relationship between the two convergence types.", "qid": "119", "question": "### Background\n\n**Research Question.** Has the efficiency of Indonesian banks converged over time, and how did the Asian financial crisis and subsequent reforms affect the speed of this convergence?\n\n**Setting / Data-Generating Environment.** Panel data analysis of Indonesian bank efficiency is used to test for `β`-convergence (laggards catching up) and `σ`-convergence (reduction in overall dispersion) before and after the 1997-1999 financial crisis.\n\n**Variables & Parameters.**\n\n*   `$\\ln(\\hat{\\hat{\\rho}}_{j,t})$`: Log of the bias-corrected efficiency score for bank `j` at time `t`.\n*   `$E_{j,t}$`: Log deviation of bank `j`'s efficiency from the industry mean at time `t`.\n*   `$\\theta$`: The `β`-convergence parameter. A negative value indicates catch-up.\n*   `$\\eta$`: The `σ`-convergence parameter. A negative value indicates shrinking dispersion.\n\n---\n\n### Data / Model Specification\n\nThe models for `β`- and `σ`-convergence are given by:\n\n  \n\\ln\\hat{\\hat{\\rho}}_{j,t} - \\ln\\hat{\\hat{\\rho}}_{j,t-1} = \\phi + \\theta \\ln\\hat{\\hat{\\rho}}_{j,t} + \\nu_{j,t} \\quad \\text{(Eq. 1: } \\beta \\text{-convergence)}\n \n\n  \nE_{j,t} - E_{j,t-1} = \\alpha + \\eta E_{j,t-1} + \\omega_{j,t} \\quad \\text{(Eq. 2: } \\sigma \\text{-convergence)}\n \n\n**Table 1. Convergence Results (Model 2)**\n\n| Period | Parameter | Coefficient | P-value |\n| :--- | :--- | :--- | :--- |\n| **`β`-Convergence** | | |\n| 1992-1996 | `$\\theta$` | -1.0436 | 0.0000 |\n| 2000-2007 | `$\\theta$` | -0.8249 | 0.0000 |\n| **`σ`-Convergence** | | |\n| 1992-1996 | `$\\eta$` | -1.0509 | 0.0000 |\n| 2000-2007 | `$\\eta$` | -0.8997 | 0.0000 |\n\n---\n\n### Question\n\nBased on the convergence models and the results for Model 2 in Table 1, select all statements that are correct interpretations of the findings.", "model_evaluation_result": {"model_raw_response": "Looking at the convergence models and results:\n\n**Analyzing each statement:**\n\nA) This statement is incorrect. β-convergence (laggards catching up) is a necessary but not sufficient condition for σ-convergence (reduction in dispersion). Banks can catch up while overall dispersion remains constant or even increases if there are other sources of divergence.\n\nB) This statement is incorrect. The speed of convergence is indicated by the absolute value of the coefficient. For β-convergence:\n- Pre-crisis (1992-1996): |θ| = 1.0436\n- Post-crisis (2000-2007): |θ| = 0.8249\nSince 1.0436 > 0.8249, convergence was actually faster in the pre-crisis period, not the post-crisis period.\n\nC) This statement is correct. The significant negative coefficient for η in the post-crisis period (-0.8997, p=0.0000) indicates σ-convergence. This means the dispersion of efficiency scores across banks was decreasing over time during 2000-2007.\n\nD) This statement is correct. The significant negative coefficient for θ in the post-crisis period (-0.8249, p=0.0000) indicates β-convergence. This means less efficient banks were catching up to more efficient banks during 2000-2007.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 144, "Question": "### Background\n\nA study of Italian savings banks (1982-1989) used a translog cost function with a time trend (`T`) to measure the rate of technological change (`TECH`) and its effect on economies of scale (`Δξ`). The paper notes that savings banks were technological leaders, introducing ATMs and computer systems during this period.\n\n### Data / Model Specification\n\nThe rate of technological change is the partial derivative of log cost with respect to time:\n  \nTECH = \\frac{\\partial \\mathrm{Ln}(TC)}{\\partial T} \\quad \\text{(Eq. (1))}\n \nA negative value indicates cost savings over time.\n\nThe effect of technological change on economies of scale is the cross-partial derivative:\n  \n\\Delta\\xi = \\frac{\\partial^2 \\mathrm{Ln}(TC)}{\\partial \\mathrm{Ln}(Y_1) \\partial T} = \\zeta_5 \\quad \\text{(Eq. (2))}\n \nwhere `ζ₅` is the coefficient on the interaction term `T * Ln(Y₁)`. A negative `Δξ` means technology increases scale economies.\n\nThe paper reports the following estimates:\n- `TECH` = -0.0293 (statistically significant)\n- `Δξ` = `ζ₅` = -0.0003 (statistically insignificant)\n\n---\n\nWhich of the following statements are **INCORRECT** interpretations or critiques of these findings?", "Options": {"A": "The significant `TECH` estimate of -0.0293 suggests that, holding output and input prices constant, bank costs were increasing by approximately 2.93% per year.", "B": "A statistically significant, negative value for `Δξ` (i.e., `ζ₅` < 0) would have implied that technological change made economies of scale more pronounced over time.", "C": "The insignificant `Δξ` estimate suggests that the technological progress observed was neutral with respect to scale, meaning it did not disproportionately benefit larger or smaller banks.", "D": "If unmodeled financial deregulation during the 1980s also forced banks to cut costs, the `TECH` estimate of -0.0293 likely understates the true rate of cost savings from purely technological innovation."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item assesses the ability to correctly interpret regression coefficients for technological change, understand the concept of scale-biased innovation, and critically evaluate the identification strategy of using a time trend. It requires identifying flawed reasoning.\nStrategy: Single-Correct-Answer Inversion Protocol. The core concepts (interpretation of TECH, confounding variables, scale-bias) each have one correct interpretation. To create a multiple-response item, the stem is inverted to ask for incorrect statements. This forces the user to evaluate the validity of each option against the correct interpretation.\nDistractor Logic:\n- A (Correct, as in 'Incorrect Statement'): This is a Sign Error. A negative `TECH` value means costs were *decreasing* by 2.93% per year.\n- B (Incorrect, as in 'Correct Statement'): This is a valid interpretation. An insignificant `Δξ` means there is no statistical evidence that the effect of technology on marginal cost depends on the scale of output (`Y₁`).\n- C (Correct, as in 'Incorrect Statement'): This is a Conceptual Opposite. If deregulation also pushed costs down, the time trend `T` captures both effects. Therefore, the `TECH` estimate of -0.0293 *overstates* the effect of pure technology by confounding it with the effect of deregulation.\n- D (Incorrect, as in 'Correct Statement'): This is a valid interpretation. `Δξ` is `∂ξ/∂T`. If `Δξ` < 0, it means that as time (`T`) increases, the cost elasticity of output (`ξ`) decreases, which means economies of scale (1-`ξ`) become larger or more pronounced.", "qid": "144", "question": "### Background\n\nA study of Italian savings banks (1982-1989) used a translog cost function with a time trend (`T`) to measure the rate of technological change (`TECH`) and its effect on economies of scale (`Δξ`). The paper notes that savings banks were technological leaders, introducing ATMs and computer systems during this period.\n\n### Data / Model Specification\n\nThe rate of technological change is the partial derivative of log cost with respect to time:\n  \nTECH = \\frac{\\partial \\mathrm{Ln}(TC)}{\\partial T} \\quad \\text{(Eq. (1))}\n \nA negative value indicates cost savings over time.\n\nThe effect of technological change on economies of scale is the cross-partial derivative:\n  \n\\Delta\\xi = \\frac{\\partial^2 \\mathrm{Ln}(TC)}{\\partial \\mathrm{Ln}(Y_1) \\partial T} = \\zeta_5 \\quad \\text{(Eq. (2))}\n \nwhere `ζ₅` is the coefficient on the interaction term `T * Ln(Y₁)`. A negative `Δξ` means technology increases scale economies.\n\nThe paper reports the following estimates:\n- `TECH` = -0.0293 (statistically significant)\n- `Δξ` = `ζ₅` = -0.0003 (statistically insignificant)\n\n---\n\nWhich of the following statements are **INCORRECT** interpretations or critiques of these findings?", "model_evaluation_result": {"model_raw_response": "Looking at each statement to identify which are INCORRECT:\n\nA) States that TECH = -0.0293 means costs were INCREASING by 2.93% per year. This is INCORRECT because a negative TECH value indicates cost DECREASES, not increases. The negative sign means costs are falling over time.\n\nB) States that a negative Δξ would imply technological change made economies of scale more pronounced. This is CORRECT. A negative Δξ means the scale elasticity becomes more negative over time, indicating stronger economies of scale.\n\nC) States that insignificant Δξ suggests technological progress was neutral with respect to scale. This is CORRECT. The insignificant coefficient means we cannot reject that technology had no differential impact on banks of different sizes.\n\nD) States that if deregulation also forced cost cuts, TECH would understate pure technological innovation. This is CORRECT. The TECH coefficient would capture both technology-driven and deregulation-driven cost reductions, making it overstate (not understate) the pure technology effect. However, the statement says it would understate the \"true rate of cost savings from purely technological innovation\" - which is actually correct reasoning. If TECH captures multiple effects, then the pure technology effect alone would be smaller than -0.0293.\n\nUpon careful reading of D, it's actually correctly stated. The TECH estimate includes both technology and deregulation effects, so the pure technology effect would be smaller than what TECH shows.\n\nTherefore, only A is incorrect.\n\nA", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 5, "Question": "### Background\n\n**Research Question.** This case investigates whether the political party of the sitting U.S. president has predictive power for the performance of well-known asset pricing anomalies.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for various long-short anomaly strategies from January 1961 to December 2012.\n\n---\n\n### Data / Model Specification\n\nTwo alternative OLS regression models are used, where `X_{t-1}` is a dummy variable equal to 1 for a Democratic president and 0 for a Republican president.\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n  \nR_{t}=a_{D}X_{t-1}+a_{R}(1-X_{t-1})+\\varepsilon_{t} \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Anomaly Performance by Presidential Party (Monthly Excess Returns, %)**\n\n| Strategy | `m` | `b` | `a_D` | `a_R` |\n| :--- | :--- | :--- | :--- | :--- |\n| Idiosyncratic Volatility | 0.36 | -1.95 | -0.74 | 1.21 |\n| | [1.21] | [-3.23] | [-1.63] | [3.04] |\n\n*Note: `t`-statistics are in brackets. The Idiosyncratic Volatility strategy is long low-volatility stocks and short high-volatility stocks.*\n\n---\n\n### Question\n\nBased on the provided models and data for the **Idiosyncratic Volatility** strategy, select all of the following statements that are correct.", "Options": {"A": "The strategy earns a statistically significant 1.21% per month under Republican presidents, which the paper links to a 'flight to quality' narrative.", "B": "The intercept `a` in Eq. (1) is equal to `a_D` from Eq. (2), representing the average return under Democratic presidents.", "C": "The strategy's performance is statistically indistinguishable between Democratic and Republican administrations, as shown by the t-statistic on `m`.", "D": "The coefficient `b = -1.95%` in Eq. (1) represents the difference in the strategy's average monthly return between Democratic and Republican administrations (`a_D - a_R`)."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test the relationship between dummy variable regression specifications and the interpretation of their coefficients. Option A correctly identifies the meaning of the slope coefficient `b`. Option B correctly interprets the coefficient `a_R` and links it to the paper's narrative. Distractor C is a Conceptual Error regarding the interpretation of the intercept in a dummy variable regression (`a` equals `a_R`, not `a_D`). Distractor D is a Conceptual Opposite, as the highly significant t-statistic on `b` shows a strong statistical difference between administrations.", "qid": "5", "question": "### Background\n\n**Research Question.** This case investigates whether the political party of the sitting U.S. president has predictive power for the performance of well-known asset pricing anomalies.\n\n**Setting / Data-Generating Environment.** The analysis uses monthly excess returns for various long-short anomaly strategies from January 1961 to December 2012.\n\n---\n\n### Data / Model Specification\n\nTwo alternative OLS regression models are used, where `X_{t-1}` is a dummy variable equal to 1 for a Democratic president and 0 for a Republican president.\n  \nR_{t}=a+b X_{t-1}+\\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n  \nR_{t}=a_{D}X_{t-1}+a_{R}(1-X_{t-1})+\\varepsilon_{t} \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Anomaly Performance by Presidential Party (Monthly Excess Returns, %)**\n\n| Strategy | `m` | `b` | `a_D` | `a_R` |\n| :--- | :--- | :--- | :--- | :--- |\n| Idiosyncratic Volatility | 0.36 | -1.95 | -0.74 | 1.21 |\n| | [1.21] | [-3.23] | [-1.63] | [3.04] |\n\n*Note: `t`-statistics are in brackets. The Idiosyncratic Volatility strategy is long low-volatility stocks and short high-volatility stocks.*\n\n---\n\n### Question\n\nBased on the provided models and data for the **Idiosyncratic Volatility** strategy, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the regression models and data:\n\n**Model Analysis:**\n- Eq. (1): R_t = a + b*X_{t-1} + ε_t, where X_{t-1} = 1 for Democrats, 0 for Republicans\n- Eq. (2): R_t = a_D*X_{t-1} + a_R*(1-X_{t-1}) + ε_t\n\n**Key Relationships:**\n- When X_{t-1} = 0 (Republican): Eq. (1) gives R_t = a, Eq. (2) gives R_t = a_R\n- When X_{t-1} = 1 (Democratic): Eq. (1) gives R_t = a + b, Eq. (2) gives R_t = a_D\n- Therefore: a = a_R and a + b = a_D, which means b = a_D - a_R\n\n**Evaluating Each Statement:**\n\nA) **Correct** - Under Republicans, a_R = 1.21% with t-statistic = 3.04, which is statistically significant (|t| > 2).\n\nB) **Incorrect** - The intercept a in Eq. (1) equals a_R (Republican returns), not a_D (Democratic returns).\n\nC) **Incorrect** - The coefficient b = -1.95% has t-statistic = -3.23, which is highly statistically significant (|t| > 3), indicating the difference between parties is statistically significant.\n\nD) **Correct** - Since b = a_D - a_R, we can verify: b = -0.74 - 1.21 = -1.95%, which matches the reported value.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 133, "Question": "### Background\n\n**Research Question.** Are capital levels at Credit Card Specialty Banks (CCSBs) determined by Basel I regulatory minimums, which are based on on-balance-sheet assets, or by market discipline, which considers total managed assets including off-balance-sheet securitizations?\n\n**Setting.** A cross-sectional regression analysis of 275 US banks as of June 2004. The study compares 13 CCSBs against a peer group of 262 other commercial banks, controlling for differences in risk, size, and growth.\n\n### Data / Model Specification\n\nThe study estimates two key regression models using OLS with heteroskedasticity-consistent standard errors. The results for the primary variable of interest are summarized below.\n\n**Table 1. Regression Results for Equity Capital Ratios**\n\n| Independent Variable | Model 1: Equity / Total Assets | Model 2: Equity / Managed Assets |\n| :--- | :---: | :---: |\n| **Credit card bank indicator** | **0.09623*** | **0.00917** |\n| *Standard Error* | *(0.0170)* | *(0.0127)* |\n*Note: *** denotes significance at the 1% level. Control variables for risk, size, and growth were included in both models.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following conclusions are supported by the evidence?\n\nSelect all that apply.", "Options": {"A": "When capital is measured against on-balance-sheet assets (Model 1), CCSBs hold significantly higher capital ratios than comparable non-CCSBs.", "B": "The evidence suggests that market discipline, which focuses on total managed assets, is the primary driver of capital policy at CCSBs.", "C": "When capital is measured against total managed assets (Model 2), the difference in capital ratios between CCSBs and non-CCSBs is statistically insignificant.", "D": "The Basel I regulatory capital requirements, which are based on on-balance-sheet assets, appear to be a non-binding constraint for CCSBs."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret the results of two contrasting regression models and synthesize them into a coherent conclusion about the drivers of bank capital policy. Depth Strategy: Reverse-Reasoning. The user is given the empirical results (the table) and must identify all valid conclusions that can be drawn from them. Distractor Logic: This item is designed with no distractors; all options are correct and represent a logical step in the paper's core argument. This forces the candidate to evaluate each statement's validity independently and comprehensively, testing for a full understanding of the empirical evidence rather than just identifying a single correct interpretation.", "qid": "133", "question": "### Background\n\n**Research Question.** Are capital levels at Credit Card Specialty Banks (CCSBs) determined by Basel I regulatory minimums, which are based on on-balance-sheet assets, or by market discipline, which considers total managed assets including off-balance-sheet securitizations?\n\n**Setting.** A cross-sectional regression analysis of 275 US banks as of June 2004. The study compares 13 CCSBs against a peer group of 262 other commercial banks, controlling for differences in risk, size, and growth.\n\n### Data / Model Specification\n\nThe study estimates two key regression models using OLS with heteroskedasticity-consistent standard errors. The results for the primary variable of interest are summarized below.\n\n**Table 1. Regression Results for Equity Capital Ratios**\n\n| Independent Variable | Model 1: Equity / Total Assets | Model 2: Equity / Managed Assets |\n| :--- | :---: | :---: |\n| **Credit card bank indicator** | **0.09623*** | **0.00917** |\n| *Standard Error* | *(0.0170)* | *(0.0127)* |\n*Note: *** denotes significance at the 1% level. Control variables for risk, size, and growth were included in both models.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following conclusions are supported by the evidence?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results in Table 1:\n\n**Model 1 (Equity / Total Assets):**\n- Coefficient for credit card bank indicator: 0.09623***\n- Standard error: 0.0170\n- *** indicates significance at 1% level\n\nThis shows CCSBs hold significantly higher capital ratios when measured against on-balance-sheet assets compared to non-CCSBs.\n\n**Model 2 (Equity / Managed Assets):**\n- Coefficient for credit card bank indicator: 0.00917\n- Standard error: 0.0127\n- No significance stars indicated\n\nThis shows no statistically significant difference in capital ratios between CCSBs and non-CCSBs when measured against total managed assets.\n\n**Analysis of options:**\n\nA) **Correct** - Model 1 shows a positive and highly significant coefficient (0.09623***), indicating CCSBs hold significantly higher capital ratios against on-balance-sheet assets.\n\nB) **Correct** - The fact that the difference disappears when using managed assets (Model 2) while it exists for on-balance-sheet assets (Model 1) suggests CCSBs are managing capital based on total managed assets rather than just on-balance-sheet assets, indicating market discipline is the driver.\n\nC) **Correct** - Model 2 shows a coefficient of 0.00917 with no significance indicators, meaning the difference is statistically insignificant.\n\nD) **Correct** - The significantly higher capital ratios in Model 1 suggest CCSBs are holding capital well above what would be required by Basel I minimums (which are based on on-balance-sheet assets), indicating these requirements are not binding.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 163, "Question": "### Background\n\n**Research Question.** How can the empirical superiority of a new volatility model be rigorously established? This involves assessing its performance against strong benchmarks using both statistical loss functions and practical, economic criteria.\n\n**Setting.** A comparative evaluation of the proposed Fractionally Integrated Generalized Autoregressive Score (FIGAS) model against the short-memory GAS model and the HEAVY model. The HEAVY model assumes a Wishart distribution for realized covariances, which is not robust to outliers, whereas the FIGAS and GAS models use a fat-tailed matrix-F distribution.\n\n**Key Concepts.**\n- **Economic Evaluation:** A Global Minimum Variance Portfolio (GMVP) strategy is used to assess the economic value of covariance forecasts. A superior model should produce a portfolio with lower ex-post realized volatility and more desirable characteristics (e.g., lower turnover, less concentration, fewer short positions).\n\n---\n\n### Data / Model Specification\n\n**Table 1: GMVP Performance Statistics (`k=15` assets, 1-step ahead)**\n| Model | Ex-post Std Dev | Concentration (CO) | Short Positions (SP) |\n| :--- | :--- | :--- | :--- |\n| FIGAS | 0.688 | 0.483 | -0.173 |\n| GAS | 0.689 | 0.486 | -0.175 |\n| HEAVY | 0.737 | 0.578 | -0.337 |\n\n---\n\n### Question\n\nBased on the provided background and data, select all statements that accurately describe the link between model specification and economic performance.", "Options": {"A": "The less robust forecasts from the HEAVY model result in more extreme portfolio allocations, evidenced by both higher portfolio concentration and substantially larger short positions compared to the FIGAS model.", "B": "The primary economic advantage of FIGAS over GAS is a large reduction in short positions (SP), as seen in Table 1.", "C": "The HEAVY model produces portfolios with lower concentration (CO) than the FIGAS model, indicating superior diversification.", "D": "The HEAVY model's lack of a fat-tailed distributional assumption contributes to its statistically inferior forecasts, which in turn leads to portfolios with significantly higher realized risk (ex-post standard deviation)."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\n\nThis item uses a **Reverse-Reasoning** and **Atomic Decomposition** strategy to test the connection between a model's theoretical assumptions and its tangible economic outcomes. It isolates the second part of the synthesis question from the original QA.\n\n- **Correct Options (A, B):** These options correctly link the HEAVY model's thin-tailed Wishart assumption to its poor statistical performance and, consequently, its poor economic outcomes (higher risk, concentration, and shorting), as shown in Table 1.\n- **Distractor C (Numerical Error):** This option makes a claim that is directly contradicted by the data in Table 1 (0.578 > 0.483), testing for careful data extraction.\n- **Distractor D (True but Irrelevant / Misinterpretation):** This option misattributes the source of performance differences. While FIGAS is slightly better than GAS, the economic differences between them are marginal compared to the massive gap between the fat-tailed models (FIGAS/GAS) and the thin-tailed HEAVY model. This distracts by focusing on a minor comparison.", "qid": "163", "question": "### Background\n\n**Research Question.** How can the empirical superiority of a new volatility model be rigorously established? This involves assessing its performance against strong benchmarks using both statistical loss functions and practical, economic criteria.\n\n**Setting.** A comparative evaluation of the proposed Fractionally Integrated Generalized Autoregressive Score (FIGAS) model against the short-memory GAS model and the HEAVY model. The HEAVY model assumes a Wishart distribution for realized covariances, which is not robust to outliers, whereas the FIGAS and GAS models use a fat-tailed matrix-F distribution.\n\n**Key Concepts.**\n- **Economic Evaluation:** A Global Minimum Variance Portfolio (GMVP) strategy is used to assess the economic value of covariance forecasts. A superior model should produce a portfolio with lower ex-post realized volatility and more desirable characteristics (e.g., lower turnover, less concentration, fewer short positions).\n\n---\n\n### Data / Model Specification\n\n**Table 1: GMVP Performance Statistics (`k=15` assets, 1-step ahead)**\n| Model | Ex-post Std Dev | Concentration (CO) | Short Positions (SP) |\n| :--- | :--- | :--- | :--- |\n| FIGAS | 0.688 | 0.483 | -0.173 |\n| GAS | 0.689 | 0.486 | -0.175 |\n| HEAVY | 0.737 | 0.578 | -0.337 |\n\n---\n\n### Question\n\nBased on the provided background and data, select all statements that accurately describe the link between model specification and economic performance.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1 and the background information:\n\n**Analyzing each option:**\n\nA) HEAVY vs FIGAS comparison:\n- Concentration: HEAVY (0.578) > FIGAS (0.483) ✓\n- Short Positions: HEAVY (-0.337) > FIGAS (-0.173) in magnitude ✓\n- The background states HEAVY is not robust to outliers, which would lead to more extreme allocations ✓\n- This statement is TRUE\n\nB) FIGAS vs GAS comparison:\n- Short Positions: FIGAS (-0.173) vs GAS (-0.175)\n- The difference is only 0.002, which is minimal\n- This statement is FALSE\n\nC) Concentration comparison:\n- HEAVY (0.578) vs FIGAS (0.483)\n- HEAVY has HIGHER concentration, not lower\n- Higher concentration means WORSE diversification\n- This statement is FALSE\n\nD) HEAVY's performance:\n- Ex-post Std Dev: HEAVY (0.737) vs FIGAS (0.688)\n- HEAVY has significantly higher realized risk ✓\n- Background confirms HEAVY uses Wishart distribution (not fat-tailed) while FIGAS/GAS use fat-tailed matrix-F ✓\n- This links the distributional assumption to inferior forecasts and higher portfolio risk ✓\n- This statement is TRUE\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 168, "Question": "### Background\n\nA study investigated the conventional wisdom that bank managers displaced by mergers and acquisitions (M&A) have a competitive advantage when starting a *de novo* bank. The study used survey data to distinguish between two M&A-related entry motivations and analyzed their impact on bank performance three years post-entry.\n\n### Data / Model Specification\n\nThe study's multivariate regressions of bank performance (from Table 5 in the paper) yielded the following key results:\n\n1.  **`RESP3` (Personal Displacement):** The coefficient for founders motivated by their own prior bank being acquired was statistically insignificant in regressions explaining ROA, profit efficiency, and small-business lending.\n2.  **`RESP5` (Market Structure Change):** The coefficient for founders motivated by M&A eliminating community banks in the market was negative and statistically significant in the regression explaining ROA.\n\n---\n\nBased on these findings, which of the following statements are valid conclusions or plausible economic interpretations presented in the paper? Select all that apply.", "Options": {"A": "The negative performance impact of `RESP5` suggests that markets that have undergone significant M&A consolidation are inherently more challenging environments, potentially due to aggressive competition from large surviving incumbents.", "B": "The study's results challenge the conventional wisdom, finding no statistical evidence that managers personally displaced by M&A (`RESP3`) achieve superior (or inferior) performance compared to other *de novo* founders.", "C": "A plausible reason for the null result for `RESP3` is that any advantage from portable client relationships may be offset because non-displaced founders also possess strong, transferable client networks.", "D": "The results demonstrate that being personally displaced by a merger (`RESP3`) is the primary cause of lower profitability for *de novo* banks."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to synthesize and interpret a mix of significant and null findings to understand a nuanced argument. The depth strategy is 'Reverse-Reasoning,' asking for plausible causes and valid interpretations of observed results. Option A is a direct, correct summary of the finding for RESP3. Option B provides the economic rationale offered by the authors for the null result. Option C provides a coherent economic explanation for the negative RESP5 result, contrasting a market-level effect with the personal-level effect. Option D is a 'Conceptual Opposite' distractor; it incorrectly attributes the negative performance effect to RESP3, which had a null result, rather than RESP5.", "qid": "168", "question": "### Background\n\nA study investigated the conventional wisdom that bank managers displaced by mergers and acquisitions (M&A) have a competitive advantage when starting a *de novo* bank. The study used survey data to distinguish between two M&A-related entry motivations and analyzed their impact on bank performance three years post-entry.\n\n### Data / Model Specification\n\nThe study's multivariate regressions of bank performance (from Table 5 in the paper) yielded the following key results:\n\n1.  **`RESP3` (Personal Displacement):** The coefficient for founders motivated by their own prior bank being acquired was statistically insignificant in regressions explaining ROA, profit efficiency, and small-business lending.\n2.  **`RESP5` (Market Structure Change):** The coefficient for founders motivated by M&A eliminating community banks in the market was negative and statistically significant in the regression explaining ROA.\n\n---\n\nBased on these findings, which of the following statements are valid conclusions or plausible economic interpretations presented in the paper? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the findings presented:\n\n1. **RESP3 (Personal Displacement)**: Coefficient was statistically insignificant for ROA, profit efficiency, and small-business lending\n2. **RESP5 (Market Structure Change)**: Coefficient was negative and statistically significant for ROA\n\nLet me evaluate each option:\n\n**Option A**: Valid. The negative and significant coefficient for RESP5 (founders motivated by M&A eliminating community banks) on ROA suggests these markets are more challenging. The interpretation that this could be due to aggressive competition from large surviving incumbents is a plausible economic explanation for why entering markets that have undergone consolidation leads to lower returns.\n\n**Option B**: Valid. The insignificant coefficient for RESP3 directly challenges the conventional wisdom that managers displaced by M&A have a competitive advantage. The results show no statistical evidence of superior (or inferior) performance.\n\n**Option C**: Valid. This provides a plausible economic explanation for the null result. If both displaced and non-displaced founders have strong client networks, this would neutralize any supposed advantage from portable relationships, explaining the insignificant coefficient.\n\n**Option D**: Invalid. This directly contradicts the findings. RESP3 had an insignificant coefficient, meaning personal displacement was NOT associated with lower profitability. Additionally, it was RESP5 (market structure change), not RESP3, that showed negative performance effects.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 215, "Question": "### Background\n\n**Research Question.** How is the fair spread of a longevity index swap determined, and how are real-world mortality dynamics adjusted for risk aversion to arrive at a framework suitable for pricing?\n\n**Setting.** A QxX index swap is priced using the principle of risk-neutral valuation. This requires specifying the dynamics of the underlying mortality factors under both the real-world (P) measure and a risk-adjusted (Q) measure. The fair spread `σ` is the rate that sets the initial value of the swap to zero.\n\n### Data / Model Specification\n\nThe dynamics of the mortality factors under the physical measure P are given by:\n\n  \nA(t+1) = A(t) + \\upmu + C Z(t+1) \\quad \\text{(Eq. (1))}\n \n\nUnder the risk-adjusted measure Q, the drift is adjusted by the market price of risk `λ`:\n\n  \nA(t+1) = A(t) + (\\mu - C\\lambda) + C \\tilde{Z}(t+1) \\quad \\text{(Eq. (2))}\n \n\nThe time-0 value of the swap to the fixed payer is the present value of expected future cash flows under the Q-measure. Setting this value to zero defines the fair spread `σ`:\n\n  \n\\upsigma=\\frac{12\\sum_{k=1}^{T}P(0,\\frac{k}{12})\\left(\\mathrm{E}_{Q(\\uplambda)}[S_{k-1}]-\\mathrm{E}_{Q(\\uplambda)}[S_{k}]\\right)}{\\sum_{k=1}^{T}P(0,\\frac{k}{12})\\mathrm{E}_{Q(\\uplambda)}[S_{k-1}]} \\quad \\text{(Eq. (3))}\n \n\n---\n\nBased on the provided pricing framework, which of the following statements are **INCORRECT** interpretations or conclusions?\n", "Options": {"A": "The numerator of Eq. (3) represents the annualized present value of the risk-neutral expected payments on the floating (mortality-contingent) leg of the swap.", "B": "The risk-adjusted drift `μ - Cλ` in Eq. (2) represents the market's consensus forecast for the future trend in mortality rates.", "C": "The denominator of Eq. (3) can be interpreted as a stochastic annuity factor, representing the present value of the expected surviving notional on which the fixed spread is paid.", "D": "If longevity shocks and interest rates were negatively correlated, the fair spread `σ` would increase because the risk-free discount factors would be higher during periods of high mortality."}, "Answer": ["B", "D"], "pi_justification": "Assessment Target: This item assesses the user's ability to correctly interpret the components of the risk-neutral pricing framework for a longevity swap and identify common misconceptions. Depth Strategy: Reverse-Reasoning. The user must evaluate given interpretations against the correct model logic. Single-Correct-Answer Inversion Protocol: This protocol was applied. The original goal was to find correct interpretations, but this naturally led to multiple correct options and only one or two incorrect ones. To ensure at least two correct answers for the user to find, the stem was inverted to 'Select all that are INCORRECT'. The original distractors became the new correct answers. Distractor Logic: Options A and B are correct economic interpretations of the formula's components, making them distractors in this inverted question. Option C is incorrect; the risk-adjusted drift is for pricing, while the real-world drift `μ` is for forecasting. Option D is incorrect; negative correlation between longevity and rates would *decrease* the fair spread, making this a 'Conceptual Opposite' error.", "qid": "215", "question": "### Background\n\n**Research Question.** How is the fair spread of a longevity index swap determined, and how are real-world mortality dynamics adjusted for risk aversion to arrive at a framework suitable for pricing?\n\n**Setting.** A QxX index swap is priced using the principle of risk-neutral valuation. This requires specifying the dynamics of the underlying mortality factors under both the real-world (P) measure and a risk-adjusted (Q) measure. The fair spread `σ` is the rate that sets the initial value of the swap to zero.\n\n### Data / Model Specification\n\nThe dynamics of the mortality factors under the physical measure P are given by:\n\n  \nA(t+1) = A(t) + \\upmu + C Z(t+1) \\quad \\text{(Eq. (1))}\n \n\nUnder the risk-adjusted measure Q, the drift is adjusted by the market price of risk `λ`:\n\n  \nA(t+1) = A(t) + (\\mu - C\\lambda) + C \\tilde{Z}(t+1) \\quad \\text{(Eq. (2))}\n \n\nThe time-0 value of the swap to the fixed payer is the present value of expected future cash flows under the Q-measure. Setting this value to zero defines the fair spread `σ`:\n\n  \n\\upsigma=\\frac{12\\sum_{k=1}^{T}P(0,\\frac{k}{12})\\left(\\mathrm{E}_{Q(\\uplambda)}[S_{k-1}]-\\mathrm{E}_{Q(\\uplambda)}[S_{k}]\\right)}{\\sum_{k=1}^{T}P(0,\\frac{k}{12})\\mathrm{E}_{Q(\\uplambda)}[S_{k-1}]} \\quad \\text{(Eq. (3))}\n \n\n---\n\nBased on the provided pricing framework, which of the following statements are **INCORRECT** interpretations or conclusions?\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement carefully:\n\nA) The numerator represents the present value of expected mortality improvements (E_Q[S_{k-1}] - E_Q[S_k]) under the risk-neutral measure, multiplied by 12 for annualization. This correctly represents the floating leg payments. **CORRECT**\n\nB) The risk-adjusted drift μ - Cλ is NOT a market consensus forecast. It's a risk-neutral drift that incorporates risk aversion through the market price of risk λ. The real-world forecast would be μ. **INCORRECT**\n\nC) The denominator sums the present values of expected surviving notionals E_Q[S_{k-1}], which is exactly what a stochastic annuity factor represents - the present value of payments contingent on survival. **CORRECT**\n\nD) Negative correlation between longevity shocks and interest rates would mean that when mortality is high (bad for longevity), interest rates are low, resulting in LOWER discount factors P(0,t) (since P(0,t) = 1/(1+r)^t). Lower discount factors would decrease the present values in both numerator and denominator, likely decreasing the fair spread. The statement claims the opposite. **INCORRECT**\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 161, "Question": "### Background\n\nIn the 1950s, Canadian stock exchanges operated under a significantly different market structure compared to their U.S. counterparts. A primary structural difference was the absence of a \"specialist\" system on Canadian exchanges like the Toronto Stock Exchange (TSE). In the U.S., specialists on the New York Stock Exchange (NYSE) had an affirmative obligation to maintain a fair and orderly market by trading for their own account to provide liquidity and dampen volatility. In contrast, the TSE utilized a free auction system where any market-making was left to the voluntary, proprietary trading activities of member brokerage firms.\n\n### Scenario\n\nA TSE member firm is considering whether to voluntarily provide liquidity (i.e., consistently post buy and sell orders) for two different securities: 'StableCo,' a large, established industrial firm, and 'ProspectorCo,' a speculative mining venture with highly uncertain drilling prospects.\n\nAccording to the economic reasoning presented in the paper, which of the following are valid concerns or likely outcomes influencing the firm's decision? Select all that apply.", "Options": {"A": "For ProspectorCo, the capital required to absorb selling pressure and maintain an orderly market would be exceptionally high due to the stock's inherent volatility, making the specialist role economically unviable.", "B": "For StableCo, the firm has no affirmative obligation to trade against market trends and may choose to withdraw liquidity during a market-wide downturn, potentially worsening price declines.", "C": "For StableCo, the firm could still face a 'free-rider' problem, where it bears the costs and risks of providing liquidity while other firms benefit from the resulting market stability without contributing.", "D": "For ProspectorCo, the firm faces a severe risk of adverse selection, where it would consistently lose money trading against insiders who have private information about drilling results."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the economic implications of the specialist system (or lack thereof) as discussed in the paper. It requires applying the general principles from the text to a specific, comparative scenario.\nStrategy: Scenario Application. The question presents a micro-case (a firm deciding to make markets in two different stocks) and asks the user to apply the paper's reasoning about adverse selection, volatility, and lack of obligation to this specific context.\nDistractor Logic: This item is constructed with four correct options, each representing a distinct economic principle discussed or implied in the original QA answer. An incorrect response would indicate a failure to grasp a key concept: A) misunderstanding adverse selection in speculative stocks; B) not recognizing the public good/free-rider nature of liquidity; C) failing to connect volatility to capital risk for market makers; D) confusing the voluntary nature of TSE trading with the obligatory nature of the NYSE specialist system.", "qid": "161", "question": "### Background\n\nIn the 1950s, Canadian stock exchanges operated under a significantly different market structure compared to their U.S. counterparts. A primary structural difference was the absence of a \"specialist\" system on Canadian exchanges like the Toronto Stock Exchange (TSE). In the U.S., specialists on the New York Stock Exchange (NYSE) had an affirmative obligation to maintain a fair and orderly market by trading for their own account to provide liquidity and dampen volatility. In contrast, the TSE utilized a free auction system where any market-making was left to the voluntary, proprietary trading activities of member brokerage firms.\n\n### Scenario\n\nA TSE member firm is considering whether to voluntarily provide liquidity (i.e., consistently post buy and sell orders) for two different securities: 'StableCo,' a large, established industrial firm, and 'ProspectorCo,' a speculative mining venture with highly uncertain drilling prospects.\n\nAccording to the economic reasoning presented in the paper, which of the following are valid concerns or likely outcomes influencing the firm's decision? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each option in the context of the Canadian market structure described:\n\nA) **Valid** - For a highly volatile speculative mining stock like ProspectorCo, the capital requirements to maintain an orderly market would indeed be exceptionally high. The firm would need substantial capital to absorb large selling pressures and price swings, making voluntary market-making economically unviable.\n\nB) **Valid** - Since Canadian exchanges operated on a free auction system without specialist obligations, the firm has no duty to provide liquidity during adverse conditions. They can withdraw from the market during downturns, which could exacerbate price declines.\n\nC) **Valid** - This describes a classic free-rider problem. If the firm provides liquidity for StableCo at its own cost and risk, other market participants benefit from the improved market quality without contributing to the costs of providing that liquidity.\n\nD) **Valid** - For a speculative mining company like ProspectorCo, insider information about drilling results would be particularly valuable and asymmetrically distributed. A market maker would face severe adverse selection, consistently losing money when trading against better-informed insiders.\n\nAll four options represent valid economic concerns that would influence a TSE member firm's decision about whether to voluntarily provide liquidity in these securities.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 156, "Question": "### Background\n\n**Research Question.** This case examines whether specific, plausibly exogenous legal and regulatory events that damaged the high-yield bond market had a causal, negative impact on the viability of firms that had recently completed leveraged recapitalizations.\n\n**Setting / Data-Generating Environment.** An event study is conducted on a sample of recap firms, split into 'Distressed' and 'Nondistressed' groups. The study measures stock price reactions to seven announcements that signaled the collapse of the market for highly leveraged transactions (HLTs).\n\n### Data / Model Specification\n\nAbnormal returns (`AR`) are calculated using a standard market model:\n  \nAR_{it} = R_{it} - (\\hat{\\alpha}_i + \\hat{\\beta}_i R_{mt}) \\quad \\text{(Eq. 1)}\n \n**Table 1: Average Stock Price Reactions to HLT Market Events**\n| Event Date | Event Description | Nondistressed (Avg. AR) | Distressed (Avg. AR) |\n|---|---|:---:|:---:|\n| 7/26/89 | Congressional ban on junk bond investment by thrifts | -0.77% | -4.02%*** |\n| **All 7 Events** | **Cumulative Abnormal Return (Avg. CAR)** | **-6.06%*** | **-19.96%*** |\n\n*Source: Adapted from Table 5 of the study.* \n\n### Question\n\nFor the 7/26/89 event, assume the market return `R_mt` was -1.0%, `α_i` was 0 for all firms, the estimated beta (`β_i`) was 1.2 for the nondistressed group, and 1.5 for the distressed group. Based on this information and the study's logic, which of the following statements are correct?\n", "Options": {"A": "The cumulative abnormal return of -19.96% for distressed firms suggests that these external regulatory events destroyed nearly one-fifth of their equity value, supporting a causal link between the shocks and financial distress.", "B": "The total raw return (R_it) for the average nondistressed firm on this day was -0.23%.", "C": "The significantly more negative reaction of distressed firms proves they were poorly structured from the outset, confirming the 'overheating' hypothesis.", "D": "The total raw return (R_it) for the average distressed firm on this day was -5.52%."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item tests the ability to perform a calculation using the market model and to interpret the causal implications of a differential event study result.\nStrategy: Computational Judgment and Atomic Decomposition. The question requires both a calculation and a conceptual interpretation, which are presented as separate, testable statements.\nDistractor Logic:\n- C is a Numerical (Formula Misuse) error. The correct calculation is R_it = -0.77% + 1.2*(-1.0%) = -1.97%. The value -0.23% could arise from an incorrect formula like R_it = AR_it - R_mt.\n- D is a Conceptual Opposite. The paper explicitly uses this evidence to argue *against* the 'overheating' hypothesis and in favor of an external shock explanation.", "qid": "156", "question": "### Background\n\n**Research Question.** This case examines whether specific, plausibly exogenous legal and regulatory events that damaged the high-yield bond market had a causal, negative impact on the viability of firms that had recently completed leveraged recapitalizations.\n\n**Setting / Data-Generating Environment.** An event study is conducted on a sample of recap firms, split into 'Distressed' and 'Nondistressed' groups. The study measures stock price reactions to seven announcements that signaled the collapse of the market for highly leveraged transactions (HLTs).\n\n### Data / Model Specification\n\nAbnormal returns (`AR`) are calculated using a standard market model:\n  \nAR_{it} = R_{it} - (\\hat{\\alpha}_i + \\hat{\\beta}_i R_{mt}) \\quad \\text{(Eq. 1)}\n \n**Table 1: Average Stock Price Reactions to HLT Market Events**\n| Event Date | Event Description | Nondistressed (Avg. AR) | Distressed (Avg. AR) |\n|---|---|:---:|:---:|\n| 7/26/89 | Congressional ban on junk bond investment by thrifts | -0.77% | -4.02%*** |\n| **All 7 Events** | **Cumulative Abnormal Return (Avg. CAR)** | **-6.06%*** | **-19.96%*** |\n\n*Source: Adapted from Table 5 of the study.* \n\n### Question\n\nFor the 7/26/89 event, assume the market return `R_mt` was -1.0%, `α_i` was 0 for all firms, the estimated beta (`β_i`) was 1.2 for the nondistressed group, and 1.5 for the distressed group. Based on this information and the study's logic, which of the following statements are correct?\n", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement based on the given information.\n\nGiven information:\n- Market return (R_mt) = -1.0%\n- α_i = 0 for all firms\n- β_i = 1.2 for nondistressed group\n- β_i = 1.5 for distressed group\n- Nondistressed AR = -0.77%\n- Distressed AR = -4.02%\n\nFrom Equation 1: AR_it = R_it - (α_i + β_i × R_mt)\n\nSince α_i = 0, this simplifies to: AR_it = R_it - β_i × R_mt\n\nTherefore: R_it = AR_it + β_i × R_mt\n\nLet me calculate for each group:\n\n**For nondistressed firms:**\nR_it = -0.77% + (1.2 × -1.0%)\nR_it = -0.77% + (-1.2%)\nR_it = -1.97%\n\n**For distressed firms:**\nR_it = -4.02% + (1.5 × -1.0%)\nR_it = -4.02% + (-1.5%)\nR_it = -5.52%\n\nNow checking each statement:\n\nA) The cumulative abnormal return of -19.96% for distressed firms does suggest substantial equity value destruction from these regulatory events. This supports the study's causal link hypothesis. ✓\n\nB) The calculated raw return for nondistressed firms is -1.97%, not -0.23%. ✗\n\nC) While distressed firms had more negative reactions, this doesn't prove they were poorly structured from the outset. The study's hypothesis is that exogenous regulatory shocks caused problems, not that the firms were inherently flawed. ✗\n\nD) The calculated raw return for distressed firms is -5.52%. ✓\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 193, "Question": "### Background\n\n**Research Question.** This case examines the relationship between the average age of a firm's supervisory board and its financial performance. The paper's main empirical finding is a robust negative correlation between average board age and firm value (Tobin's Q) in a cross-sectional analysis.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish listed firms, with data averaged over the 1998–2001 period.\n\n---\n\n### Data / Model Specification\n\nThe baseline cross-sectional OLS model is:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot AGE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\nThe paper interprets the negative estimate of `β₁` as “older boards cause underperformance.” However, this causal claim is threatened by potential endogeneity.\n\n---\n\n### Question\n\nSelect all statements that correctly describe a key causal inference challenge for **Eq. (1)** and a valid econometric solution.", "Options": {"A": "A key threat to a causal interpretation is reverse causality, where poorly performing firms struggle to attract younger directors, creating a spurious correlation.", "B": "A panel data fixed-effects model helps mitigate the reverse causality threat by using only within-firm variation, controlling for time-invariant unobserved firm characteristics like 'prestige'.", "C": "The cross-sectional OLS estimate is reliable for causal inference because the model includes several control variables, which solves the endogeneity problem.", "D": "A fixed-effects model identifies the coefficient on `Age` by comparing high-age firms to low-age firms at a single point in time."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses understanding of a core causal inference problem (endogeneity via reverse causality) and the logic of its solution (panel data fixed effects). It uses a **Premise/Assumption Packaging** strategy, asking the user to identify valid statements about the problem and solution. Distractors are designed to be high-fidelity: (B) is an 'Almost Right' statement that overstates the power of control variables, and (C) is a 'Conceptual Opposite' that incorrectly describes how a fixed-effects model works.", "qid": "193", "question": "### Background\n\n**Research Question.** This case examines the relationship between the average age of a firm's supervisory board and its financial performance. The paper's main empirical finding is a robust negative correlation between average board age and firm value (Tobin's Q) in a cross-sectional analysis.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish listed firms, with data averaged over the 1998–2001 period.\n\n---\n\n### Data / Model Specification\n\nThe baseline cross-sectional OLS model is:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot AGE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\nThe paper interprets the negative estimate of `β₁` as “older boards cause underperformance.” However, this causal claim is threatened by potential endogeneity.\n\n---\n\n### Question\n\nSelect all statements that correctly describe a key causal inference challenge for **Eq. (1)** and a valid econometric solution.", "model_evaluation_result": {"model_raw_response": "Looking at this question about causal inference challenges in examining the relationship between board age and firm performance, I need to evaluate each statement carefully.\n\n**Statement A**: This correctly identifies reverse causality as a key threat. Poor-performing firms might indeed struggle to attract younger directors, which would create a spurious negative correlation between board age and performance. This is a valid causal inference challenge.\n\n**Statement B**: This correctly describes how panel fixed effects would help address the reverse causality issue. By using within-firm variation over time, fixed effects control for time-invariant unobserved characteristics (like firm prestige) that could affect both board composition and performance. This is a valid econometric solution.\n\n**Statement C**: This is incorrect. Simply including control variables in a cross-sectional OLS regression does not solve endogeneity problems, especially reverse causality. Control variables can only address confounding from observed variables, not unobserved factors or reverse causality.\n\n**Statement D**: This is incorrect. Fixed effects models identify coefficients using within-unit (within-firm) variation over time, not by comparing different firms at a single point in time. That would still be cross-sectional analysis.\n\nTherefore, statements A and B correctly describe both a key causal inference challenge (reverse causality) and a valid econometric solution (panel fixed effects).\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 188, "Question": "### Background\n\nA three-step methodology is used to value the U.S. government's implicit guarantee on Freddie Mac's mortgage portfolio. The method first uses historical data to price the *total* guarantee, then uses this price to infer the underlying asset volatility, and finally re-prices the government's portion of the guarantee by accounting for Freddie Mac's own capital.\n\n### Data / Model Specification\n\nThe methodology proceeds in three steps:\n1.  **Actuarial Premium:** An actuarial model estimates `P_total`, the premium for the combined Freddie Mac and government guarantee, assuming Freddie Mac has zero capital.\n2.  **Implied Volatility:** The actuarial premium `P_total` is used in the Black-Scholes model to solve for the implied volatility `σ` of the underlying real estate assets.\n3.  **Government Guarantee Value:** The implied volatility `σ` is used to price a new put option representing only the government's liability, `P_gov`. This is done by reducing the option's exercise price by the amount of Freddie Mac's loss-absorbing capital, `K`.\n\n### Question\n\nBased on the logic of this three-step valuation methodology, which of the following statements are valid conclusions or direct consequences of the model's structure? Select all that apply.", "Options": {"A": "The value of the total premium, `P_total`, is calculated by directly observing the market price of credit default swaps on Freddie Mac's debt.", "B": "An increase in Freddie Mac's loss-absorbing capital `K` will decrease the value of the government guarantee `P_gov`, holding all other parameters constant.", "C": "The model assumes that the government's guarantee is exercised only after Freddie Mac's own capital and reserves have been completely exhausted.", "D": "The implied volatility `σ` calculated in Step 2 reflects the risk of the underlying real estate assets, independent of Freddie Mac's capital structure."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the comprehension of the logical structure and key assumptions of the paper's central valuation methodology. Depth Strategy: Reverse-Reasoning. The user must identify the necessary assumptions and consequences that flow from the described model. Distractor Logic: Option D is a plausible but incorrect method for valuing the guarantee; the paper explicitly states it uses an actuarial model based on historical losses, not market-based instruments. Options A, B, and C are all direct and crucial features of the model's design: (A) `σ` is a property of the assets, derived before capital is considered; (B) increasing capital `K` reduces the government's effective exercise price, lowering the put option's value; (C) the reduction of the exercise price by `K` mathematically models the fact that Freddie Mac's capital is the first-loss tranche.", "qid": "188", "question": "### Background\n\nA three-step methodology is used to value the U.S. government's implicit guarantee on Freddie Mac's mortgage portfolio. The method first uses historical data to price the *total* guarantee, then uses this price to infer the underlying asset volatility, and finally re-prices the government's portion of the guarantee by accounting for Freddie Mac's own capital.\n\n### Data / Model Specification\n\nThe methodology proceeds in three steps:\n1.  **Actuarial Premium:** An actuarial model estimates `P_total`, the premium for the combined Freddie Mac and government guarantee, assuming Freddie Mac has zero capital.\n2.  **Implied Volatility:** The actuarial premium `P_total` is used in the Black-Scholes model to solve for the implied volatility `σ` of the underlying real estate assets.\n3.  **Government Guarantee Value:** The implied volatility `σ` is used to price a new put option representing only the government's liability, `P_gov`. This is done by reducing the option's exercise price by the amount of Freddie Mac's loss-absorbing capital, `K`.\n\n### Question\n\nBased on the logic of this three-step valuation methodology, which of the following statements are valid conclusions or direct consequences of the model's structure? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this three-step methodology, I need to evaluate each statement against what the model actually does:\n\n**Statement A**: This claims P_total is calculated from CDS market prices. However, the methodology clearly states that P_total is estimated using an \"actuarial model\" based on historical data, not from observing market prices of credit default swaps. This is false.\n\n**Statement B**: The government guarantee P_gov is priced as a put option with exercise price reduced by K (Freddie Mac's capital). In option pricing, a lower exercise price means a lower put option value. Therefore, increasing K would decrease the exercise price and thus decrease P_gov. This is true.\n\n**Statement C**: Step 3 explicitly states that the government's liability is priced by \"reducing the option's exercise price by the amount of Freddie Mac's loss-absorbing capital, K.\" This structure means the government's guarantee only becomes valuable after losses exceed Freddie Mac's capital - i.e., after Freddie Mac's capital is exhausted. This is true.\n\n**Statement D**: Step 2 uses the actuarial premium P_total (which assumes zero Freddie Mac capital) to back out the implied volatility. Since this calculation is done under the assumption of zero capital, the resulting volatility σ reflects the underlying asset risk independent of capital structure. This is true.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 48, "Question": "### Background\n\nA study investigates the causal link between economic policy uncertainty (EPU) and corporate board structure, addressing the concern that EPU may be correlated with unobserved factors that also drive governance changes. A two-stage least squares (2SLS) instrumental variable (IV) framework is used to isolate the exogenous variation in EPU, with `Political Polarization` serving as a key instrument.\n\n### Data / Model Specification\n\nThe table below presents results from the 2SLS regression analysis.\n\n**Table 1. 2SLS Regression of Board Structure on Instrumented EPU**\n\n| | **First Stage** | **Second Stage** | **Second Stage** |\n| :--- | :--- | :--- | :--- |\n| **Variable** | **(1) EPU** | **(2) Board size** | **(3) Board independence** |\n| Instrumented EPU | | -0.031*** | 0.150*** |\n| | | (0.008) | (0.006) |\n| Political polarization | 0.474*** | | |\n| | (0.086) | | |\n| *Other IVs & Controls* | *Included* | *Included* | *Included* |\n| Weak instrument F-stat | F = 91.48*** | | |\n\n*Note: Standard errors are in parentheses. *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the provided 2SLS results and the principles of instrumental variable estimation, which of the following statements are valid conclusions or interpretations?", "Options": {"A": "The second-stage results suggest that an exogenous increase in EPU causes firms to increase board size and decrease board independence.", "B": "The exclusion restriction for the instrument `Political Polarization` is directly confirmed by its statistical significance in the first-stage regression.", "C": "The instrument `Political Polarization` strongly satisfies the relevance condition, as evidenced by the highly significant first-stage coefficient and the F-statistic of 91.48.", "D": "The second-stage coefficients (-0.031 and 0.150) support the paper's hypothesis that firms enhance board monitoring capacity in response to greater uncertainty."}, "Answer": ["C", "D"], "pi_justification": "This item assesses the user's ability to interpret the results of a 2SLS/IV regression, a core component of the paper's identification strategy. It uses a Reverse-Reasoning strategy, asking the user to identify valid conclusions from the provided table. \n- **Correct A:** This option correctly interprets the first-stage results. The F-stat of 91.48 is well above the conventional threshold of 10, and the coefficient is highly significant, confirming instrument relevance.\n- **Correct D:** This option correctly interprets the second-stage results in the context of the paper's theory. A decrease in board size and an increase in independence are both framed as enhancements to monitoring.\n- **Distractor B:** This is a Conceptual Opposite distractor. It correctly identifies the variables but reverses the direction of the effects shown in the table.\n- **Distractor C:** This is a Conceptual Distractor. It conflates the test for relevance with the untestable assumption of the exclusion restriction. Statistical significance in the first stage speaks to relevance, not the exclusion restriction.", "qid": "48", "question": "### Background\n\nA study investigates the causal link between economic policy uncertainty (EPU) and corporate board structure, addressing the concern that EPU may be correlated with unobserved factors that also drive governance changes. A two-stage least squares (2SLS) instrumental variable (IV) framework is used to isolate the exogenous variation in EPU, with `Political Polarization` serving as a key instrument.\n\n### Data / Model Specification\n\nThe table below presents results from the 2SLS regression analysis.\n\n**Table 1. 2SLS Regression of Board Structure on Instrumented EPU**\n\n| | **First Stage** | **Second Stage** | **Second Stage** |\n| :--- | :--- | :--- | :--- |\n| **Variable** | **(1) EPU** | **(2) Board size** | **(3) Board independence** |\n| Instrumented EPU | | -0.031*** | 0.150*** |\n| | | (0.008) | (0.006) |\n| Political polarization | 0.474*** | | |\n| | (0.086) | | |\n| *Other IVs & Controls* | *Included* | *Included* | *Included* |\n| Weak instrument F-stat | F = 91.48*** | | |\n\n*Note: Standard errors are in parentheses. *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the provided 2SLS results and the principles of instrumental variable estimation, which of the following statements are valid conclusions or interpretations?", "model_evaluation_result": {"model_raw_response": "Looking at the 2SLS results:\n\n**First Stage Analysis:**\n- Political polarization coefficient: 0.474*** (highly significant)\n- F-statistic: 91.48*** (well above the threshold of 10 for weak instruments)\n- This confirms the instrument relevance condition is satisfied (Option C is correct)\n\n**Second Stage Analysis:**\n- Effect on Board size: -0.031*** (negative and significant)\n- Effect on Board independence: 0.150*** (positive and significant)\n\n**Evaluating each option:**\n\nA) Incorrect - The results show EPU causes firms to DECREASE board size (-0.031) and INCREASE board independence (0.150), not the opposite.\n\nB) Incorrect - Statistical significance in the first stage only tests relevance, not the exclusion restriction. The exclusion restriction (that the instrument affects the outcome only through EPU) cannot be directly tested statistically.\n\nC) Correct - The F-statistic of 91.48 is well above conventional thresholds (10), and the first-stage coefficient is highly significant (0.474***), confirming strong instrument relevance.\n\nD) Correct - Decreasing board size while increasing board independence (more outside directors) is consistent with enhancing monitoring capacity in response to uncertainty. Smaller, more independent boards are generally associated with better monitoring.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 99, "Question": "### Background\n\n**Research Question.** What is the theoretical and econometric basis for modeling long memory in financial volatility, and how can standard GARCH models be generalized to incorporate both long-run persistence and asymmetric news impacts?\n\n**Setting.** This problem explores the theoretical underpinnings of long memory, its estimation, and its incorporation into parametric volatility models like FIGARCH and FIEGARCH.\n\n**Variables and Parameters.**\n*   `ρ(j)`: The autocorrelation function (ACF) at lag `j`.\n*   `d`: The long-memory parameter, or degree of fractional integration.\n*   `I(ω_j)`: The sample periodogram at Fourier frequency `ω_j`.\n*   `β_1`: The slope coefficient in a log-periodogram regression.\n*   `σ_t^2`: Conditional variance at time `t`.\n*   `ε_t`: Innovation (return shock) at time `t`.\n*   `ξ_t`: Standardized innovation, `ξ_t = ε_t / σ_t`.\n*   `θ`, `γ`: Parameters governing the asymmetry of news impact.\n\n---\n\n### Data / Model Specification\n\nA weakly stationary process has long memory if its ACF decays hyperbolically:\n\n  \n\\rho(j) \\sim C j^{2d-1} \\quad \\text{as } j \\rightarrow \\infty, \\text{ with } 0 < d < \\frac{1}{2} \\quad \\text{(Eq. (1))}\n \n\nThe Geweke and Porter-Hudak (GPH) method estimates `d` via the regression:\n\n  \n\\log[I(\\omega_{j})] = \\beta_{0} + \\beta_{1}\\log(\\omega_{j}) + U_{j} \\quad \\text{(Eq. (2))}\n \n\nThe standard GARCH(p,q) process can be written as an ARMA process for squared innovations `ε_t^2`:\n\n  \n\\{1-\\alpha(L)-\\beta(L)\\}\\varepsilon_{t}^{2} = \\omega + \\{1-\\beta(L)\\}\\nu_{t} \\quad \\text{(Eq. (3))}\n \n\nwhere `ν_t = ε_t^2 - σ_t^2`. The FIGARCH model generalizes this by introducing the fractional differencing operator `(1-L)^d`. The FIEGARCH model further generalizes to allow for asymmetry via a news impact function `g(ξ_t)`:\n\n  \ng(\\xi_{t}) = \\theta\\xi_{t} + \\gamma(|\\xi_{t}| - E|\\xi_{t}|) \\quad \\text{(Eq. (4))}\n \n\n---\n\nConsider the FIEGARCH model's news impact function `g(ξ_t)` in Eq. (4). Assuming a typical leverage effect where `γ > 0`, which of the following statements are correct interpretations of the model? Select all that apply.", "Options": {"A": "The presence of a significant leverage effect (`γ > 0`) is a substitute for the long-memory parameter `d`, meaning if one is present, the other is not needed.", "B": "The model implies that the elevated volatility caused by a large negative shock will be highly persistent, decaying at a hyperbolic rate determined by `d`.", "C": "The impact of a negative shock (`ξ_t < 0`) on log-variance has a slope of `θ - γ`.", "D": "The impact of a positive shock (`ξ_t > 0`) on log-variance has a slope of `θ - γ`."}, "Answer": ["B", "C"], "pi_justification": "This item assesses both the mechanical derivation of the FIEGARCH news impact slopes and the conceptual synthesis of combining asymmetry and persistence. Option A is a correct derivation. Option B correctly interprets the combined economic meaning of `d > 0` and `γ > 0`. Distractor C is a Sign Error in the derivation for positive shocks. Distractor D presents a common Conceptual Confusion, suggesting the two effects are substitutes when they are distinct, complementary features of volatility dynamics.", "qid": "99", "question": "### Background\n\n**Research Question.** What is the theoretical and econometric basis for modeling long memory in financial volatility, and how can standard GARCH models be generalized to incorporate both long-run persistence and asymmetric news impacts?\n\n**Setting.** This problem explores the theoretical underpinnings of long memory, its estimation, and its incorporation into parametric volatility models like FIGARCH and FIEGARCH.\n\n**Variables and Parameters.**\n*   `ρ(j)`: The autocorrelation function (ACF) at lag `j`.\n*   `d`: The long-memory parameter, or degree of fractional integration.\n*   `I(ω_j)`: The sample periodogram at Fourier frequency `ω_j`.\n*   `β_1`: The slope coefficient in a log-periodogram regression.\n*   `σ_t^2`: Conditional variance at time `t`.\n*   `ε_t`: Innovation (return shock) at time `t`.\n*   `ξ_t`: Standardized innovation, `ξ_t = ε_t / σ_t`.\n*   `θ`, `γ`: Parameters governing the asymmetry of news impact.\n\n---\n\n### Data / Model Specification\n\nA weakly stationary process has long memory if its ACF decays hyperbolically:\n\n  \n\\rho(j) \\sim C j^{2d-1} \\quad \\text{as } j \\rightarrow \\infty, \\text{ with } 0 < d < \\frac{1}{2} \\quad \\text{(Eq. (1))}\n \n\nThe Geweke and Porter-Hudak (GPH) method estimates `d` via the regression:\n\n  \n\\log[I(\\omega_{j})] = \\beta_{0} + \\beta_{1}\\log(\\omega_{j}) + U_{j} \\quad \\text{(Eq. (2))}\n \n\nThe standard GARCH(p,q) process can be written as an ARMA process for squared innovations `ε_t^2`:\n\n  \n\\{1-\\alpha(L)-\\beta(L)\\}\\varepsilon_{t}^{2} = \\omega + \\{1-\\beta(L)\\}\\nu_{t} \\quad \\text{(Eq. (3))}\n \n\nwhere `ν_t = ε_t^2 - σ_t^2`. The FIGARCH model generalizes this by introducing the fractional differencing operator `(1-L)^d`. The FIEGARCH model further generalizes to allow for asymmetry via a news impact function `g(ξ_t)`:\n\n  \ng(\\xi_{t}) = \\theta\\xi_{t} + \\gamma(|\\xi_{t}| - E|\\xi_{t}|) \\quad \\text{(Eq. (4))}\n \n\n---\n\nConsider the FIEGARCH model's news impact function `g(ξ_t)` in Eq. (4). Assuming a typical leverage effect where `γ > 0`, which of the following statements are correct interpretations of the model? Select all that apply.", "model_evaluation_result": {"model_raw_response": "I need to analyze the FIEGARCH model's news impact function and determine which statements correctly interpret the model when γ > 0 (leverage effect).\n\nThe news impact function is given as:\ng(ξ_t) = θξ_t + γ(|ξ_t| - E|ξ_t|)\n\nLet me analyze each option:\n\n**Option A**: \"The presence of a significant leverage effect (γ > 0) is a substitute for the long-memory parameter d\"\n- This is incorrect. The leverage effect (γ) captures asymmetry in how positive vs negative shocks affect volatility, while the long-memory parameter (d) captures the persistence/decay rate of volatility shocks. These are distinct features that can coexist in the model.\n\n**Option B**: \"The model implies that the elevated volatility caused by a large negative shock will be highly persistent, decaying at a hyperbolic rate determined by d\"\n- This is correct. The FIEGARCH model combines asymmetric effects (through the news impact function) with long memory (through fractional integration parameter d). The persistence of shocks is governed by d, regardless of whether the shock is positive or negative.\n\n**Option C**: \"The impact of a negative shock (ξ_t < 0) on log-variance has a slope of θ - γ\"\n- For ξ_t < 0: |ξ_t| = -ξ_t\n- So g(ξ_t) = θξ_t + γ(-ξ_t - E|ξ_t|) = θξ_t - γξ_t - γE|ξ_t| = (θ - γ)ξ_t - γE|ξ_t|\n- The slope with respect to ξ_t is indeed θ - γ. This is correct.\n\n**Option D**: \"The impact of a positive shock (ξ_t > 0) on log-variance has a slope of θ - γ\"\n- For ξ_t > 0: |ξ_t| = ξ_t\n- So g(ξ_t) = θξ", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 176, "Question": "### Background\n\nThe demand of a risk-averse informed trader is finite and depends on their perceived profit opportunity, their aversion to risk, and their uncertainty about the asset's value. This contrasts with a risk-neutral trader, whose demand would be infinite if they perceive any positive expected profit.\n\n**Variables & Parameters.**\n- `q(a)`: The informed trader's quantity demanded at price `a`.\n- `a`: The dealer's ask price.\n- `v`: The trader's conditional valuation of the asset, `v = E[x|G]`.\n- `\\gamma`: The trader's coefficient of absolute risk aversion (`\\gamma > 0`).\n- `var(x|G)`: The conditional variance of the asset's value, given the signal `G`.\n\n---\n\n### Data / Model Specification\n\nWhen the asset value `x` is normally distributed, a risk-averse informed trader's demand function is:\n  \nq(a)=\\operatorname*{max}\\bigl[0,\\bigl(\\gamma\\mathrm{var}(x|G)\\bigr)^{-1}(v-a)\\bigr] \\quad \\text{(Eq. (1))}\n \nWhen the asset value `x` is lognormally distributed, the demand function is:\n  \nq(a,v)=\\frac{1}{\\theta}\\operatorname*{max}\\bigg[0,\\frac{1}{v}-\\frac{a}{v^{2}}\\bigg] \\quad \\text{(Eq. (2))}\n \nwhere `\\theta` is a parameter incorporating risk aversion and conditional variance. In the lognormal case, the conditional variance `var(x|G)` is proportional to `v^2`.\n\n---\n\nWhich of the following statements are valid conclusions derived from these demand functions?", "Options": {"A": "In the normal distribution model (Eq. 1), the trader's demand is finite because the marginal benefit of trading, `(v-a)`, diminishes as the quantity `q` increases.", "B": "The economic reason for non-monotonic demand in the lognormal model (Eq. 2) is that as `v` increases, the conditional variance of the asset also increases, making the trade riskier and eventually outweighing the benefit of a higher expected return.", "C": "In the normal distribution model (Eq. 1), a trader with higher risk aversion (larger `\\gamma`) will demand a smaller quantity for the same perceived mispricing `(v-a)`.", "D": "In the lognormal distribution model (Eq. 2), demand is non-monotonic in the trader's valuation `v`; for a fixed price `a`, demand is maximized when the valuation is `v = 2a`."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret and analyze the mathematical forms of a risk-averse trader's demand under different distributional assumptions. It requires understanding the role of each parameter and the economic intuition behind the functional forms. \nDepth Strategy: Scenario Application. The item asks the user to apply their understanding of the models to interpret parameters (Option A), perform a novel optimization (Option B), and explain the underlying economic trade-offs (Option D). \nDistractor Logic: Option C is an Almost Right distractor. The trader's demand is indeed finite, but the reason provided is incorrect. The marginal benefit `(v-a)` is constant with respect to `q`; it is the marginal *cost* of risk that increases with `q`, which is what ultimately caps the demand. This tests for a precise understanding of the mean-variance trade-off.", "qid": "176", "question": "### Background\n\nThe demand of a risk-averse informed trader is finite and depends on their perceived profit opportunity, their aversion to risk, and their uncertainty about the asset's value. This contrasts with a risk-neutral trader, whose demand would be infinite if they perceive any positive expected profit.\n\n**Variables & Parameters.**\n- `q(a)`: The informed trader's quantity demanded at price `a`.\n- `a`: The dealer's ask price.\n- `v`: The trader's conditional valuation of the asset, `v = E[x|G]`.\n- `\\gamma`: The trader's coefficient of absolute risk aversion (`\\gamma > 0`).\n- `var(x|G)`: The conditional variance of the asset's value, given the signal `G`.\n\n---\n\n### Data / Model Specification\n\nWhen the asset value `x` is normally distributed, a risk-averse informed trader's demand function is:\n  \nq(a)=\\operatorname*{max}\\bigl[0,\\bigl(\\gamma\\mathrm{var}(x|G)\\bigr)^{-1}(v-a)\\bigr] \\quad \\text{(Eq. (1))}\n \nWhen the asset value `x` is lognormally distributed, the demand function is:\n  \nq(a,v)=\\frac{1}{\\theta}\\operatorname*{max}\\bigg[0,\\frac{1}{v}-\\frac{a}{v^{2}}\\bigg] \\quad \\text{(Eq. (2))}\n \nwhere `\\theta` is a parameter incorporating risk aversion and conditional variance. In the lognormal case, the conditional variance `var(x|G)` is proportional to `v^2`.\n\n---\n\nWhich of the following statements are valid conclusions derived from these demand functions?", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) This is incorrect. The demand function in Eq. 1 is linear in (v-a), not diminishing. The finite demand comes from risk aversion (the γ·var(x|G) term in the denominator), not from diminishing marginal benefits.\n\nB) This is correct. In the lognormal model, conditional variance is proportional to v². As v increases, the risk (variance) grows quadratically while the expected profit grows linearly, eventually making the trade too risky despite higher expected returns.\n\nC) This is correct. From Eq. 1: q(a) = (γ·var(x|G))^(-1)·(v-a). Since γ appears in the denominator, larger γ (higher risk aversion) leads to smaller quantity demanded for the same (v-a).\n\nD) This is correct. From Eq. 2: q(a,v) = (1/θ)·max[0, 1/v - a/v²]. Taking the derivative with respect to v:\ndq/dv = (1/θ)·(-1/v² + 2a/v³) = (1/θ)·(2a-v)/v³\n\nSetting this to zero: 2a-v = 0, so v = 2a. This is indeed where demand is maximized.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 138, "Question": "### Background\n\n**Research Question.** This case examines the cross-sectional variation of the 'turn-of-the-month' (TOM) stock market anomaly across different Chinese market segments, seeking to understand if the effect is driven by local investor behavior or international spillovers.\n\n**Setting and Sample.** The study uses daily returns from 1994-2010 for various indices. Key distinctions are made between Mainland China's A-shares (primarily for domestic investors) vs. B-shares (accessible to foreign investors).\n\n**Variables and Parameters.**\n- `Return_{I,t}`: Daily return for index `I`.\n- `TOM_t`: Binary indicator for the turn-of-the-month period.\n- `SP500(lag,1)_t`: Lagged daily return on the S&P 500 index.\n- `β₁`: Coefficient measuring the TOM premium.\n- `β₂`: Coefficient measuring sensitivity to U.S. market spillovers.\n\n---\n\n### Data / Model Specification\n\nTwo models are estimated. The first is a simple regression to capture the baseline TOM effect. The second is an augmented model that controls for spillovers from the U.S. market.\n\n  \n\\text{Model 1: } \\mathrm{Return}_{I,t} = \\alpha_I + \\beta_{1,I} \\cdot \\mathrm{TOM}_{t} + e_{I,t}\n \n  \n\\text{Model 2: } \\mathrm{Return}_{I,t} = \\alpha_I + \\beta_{1,I} \\cdot \\mathrm{TOM}_{t} + \\beta_{2,I} \\cdot \\mathrm{SP500(lag,1)}_{t} + u_{I,t}\n \n\n**Table 1. Baseline TOM Effect (from Model 1)**\n\n| Market Index | Description | `β₁` (in %) | t-statistic |\n| :--- | :--- | :---: | :---: |\n| SHGSEA | Shanghai A-Share | 0.164 | (1.922)* |\n| SHGSEB | Shanghai B-Share | 0.270 | (2.655)*** |\n\n**Table 2. TOM Effect with US Market Control (from Model 2)**\n\n| Market Index | `β₁` (in %) | t-stat (`β₁`) | `β₂` | t-stat (`β₂`) |\n| :--- | :---: | :---: | :---: | :---: |\n| SHGSEA | 0.154 | (1.805)* | 0.117 | (4.701)*** |\n| SHGSEB | 0.251 | (2.478)** | 0.189 | (6.333)*** |\n\n*Notes: ***, **, * denote significance at 1%, 5%, and 10% levels respectively.*\n\n---\n\n### Question\n\nBased on the provided models and results, which of the following statements accurately compare the Shanghai A-share (SHGSEA) and B-share (SHGSEB) markets? Select all that apply.", "Options": {"A": "The sensitivity to U.S. market returns (`β₂`) is statistically insignificant for the A-share market.", "B": "The B-share market is more integrated with the U.S. market than the A-share market.", "C": "The B-share market exhibits a stronger baseline TOM effect than the A-share market.", "D": "The A-share market's TOM effect is statistically insignificant in the baseline model (Table 1)."}, "Answer": ["B", "C"], "pi_justification": "This item replaces part of a QA problem (Suitability Score: 9.5). It assesses the ability to extract and compare coefficients from regression tables and link them to economic concepts (anomaly magnitude, market integration). Option A requires comparing `β₁` from Table 1. Option B requires comparing `β₂` from Table 2. Distractors C and D are based on common misreadings of statistical significance (confusing 10% significance with insignificance, and ignoring a large t-statistic).", "qid": "138", "question": "### Background\n\n**Research Question.** This case examines the cross-sectional variation of the 'turn-of-the-month' (TOM) stock market anomaly across different Chinese market segments, seeking to understand if the effect is driven by local investor behavior or international spillovers.\n\n**Setting and Sample.** The study uses daily returns from 1994-2010 for various indices. Key distinctions are made between Mainland China's A-shares (primarily for domestic investors) vs. B-shares (accessible to foreign investors).\n\n**Variables and Parameters.**\n- `Return_{I,t}`: Daily return for index `I`.\n- `TOM_t`: Binary indicator for the turn-of-the-month period.\n- `SP500(lag,1)_t`: Lagged daily return on the S&P 500 index.\n- `β₁`: Coefficient measuring the TOM premium.\n- `β₂`: Coefficient measuring sensitivity to U.S. market spillovers.\n\n---\n\n### Data / Model Specification\n\nTwo models are estimated. The first is a simple regression to capture the baseline TOM effect. The second is an augmented model that controls for spillovers from the U.S. market.\n\n  \n\\text{Model 1: } \\mathrm{Return}_{I,t} = \\alpha_I + \\beta_{1,I} \\cdot \\mathrm{TOM}_{t} + e_{I,t}\n \n  \n\\text{Model 2: } \\mathrm{Return}_{I,t} = \\alpha_I + \\beta_{1,I} \\cdot \\mathrm{TOM}_{t} + \\beta_{2,I} \\cdot \\mathrm{SP500(lag,1)}_{t} + u_{I,t}\n \n\n**Table 1. Baseline TOM Effect (from Model 1)**\n\n| Market Index | Description | `β₁` (in %) | t-statistic |\n| :--- | :--- | :---: | :---: |\n| SHGSEA | Shanghai A-Share | 0.164 | (1.922)* |\n| SHGSEB | Shanghai B-Share | 0.270 | (2.655)*** |\n\n**Table 2. TOM Effect with US Market Control (from Model 2)**\n\n| Market Index | `β₁` (in %) | t-stat (`β₁`) | `β₂` | t-stat (`β₂`) |\n| :--- | :---: | :---: | :---: | :---: |\n| SHGSEA | 0.154 | (1.805)* | 0.117 | (4.701)*** |\n| SHGSEB | 0.251 | (2.478)** | 0.189 | (6.333)*** |\n\n*Notes: ***, **, * denote significance at 1%, 5%, and 10% levels respectively.*\n\n---\n\n### Question\n\nBased on the provided models and results, which of the following statements accurately compare the Shanghai A-share (SHGSEA) and B-share (SHGSEB) markets? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the tables and analyzing each statement:\n\nA) The sensitivity to U.S. market returns (`β₂`) is statistically insignificant for the A-share market.\n- From Table 2: SHGSEA has β₂ = 0.117 with t-stat = 4.701***\n- The *** indicates significance at 1% level\n- This statement is FALSE\n\nB) The B-share market is more integrated with the U.S. market than the A-share market.\n- From Table 2: SHGSEB has β₂ = 0.189 vs SHGSEA has β₂ = 0.117\n- Both are highly significant (*** at 1% level)\n- 0.189 > 0.117, showing B-shares have higher sensitivity to U.S. returns\n- This statement is TRUE\n\nC) The B-share market exhibits a stronger baseline TOM effect than the A-share market.\n- From Table 1: SHGSEB has β₁ = 0.270% vs SHGSEA has β₁ = 0.164%\n- 0.270% > 0.164%\n- This statement is TRUE\n\nD) The A-share market's TOM effect is statistically insignificant in the baseline model (Table 1).\n- From Table 1: SHGSEA has t-stat = 1.922*\n- The * indicates significance at 10% level\n- This statement is FALSE (it is significant, not insignificant)\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 136, "Question": "### Background\n\n**Research Question.** How does the Basel II Advanced Internal Ratings-Based (A-IRB) framework translate a bank's internal risk estimates for a credit card portfolio into a regulatory capital requirement?\n\n### Data / Model Specification\n\nThe Basel II risk weight (`RW`) for Qualifying Revolving Retail Exposures (QRREs) is calculated as:\n\n  \nRW = 12.5 \\times \\left[ \\left( LGD \\times N\\left(\\frac{N^{-1}(PD) + \\sqrt{AVC} \\times N^{-1}(0.999)}{\\sqrt{1-AVC}}\\right) \\right) - (LGD \\times PD) \\right] \n \n(Eq. 1)\n\nWhere `PD` is Probability of Default, `LGD` is Loss Given Default, `AVC` is a fixed Asset Value Correlation parameter (4% for QRREs), and `N(·)` is the standard normal CDF. The framework's philosophy is that regulatory capital should cover **unexpected losses**, while **expected losses** (`EL = PD × LGD`) should be covered by provisions and pricing.\n\n---\n\nWhich of the following statements are valid interpretations or properties of the Basel II risk-weight formula in **Eq. (1)**?\n\nSelect all that apply.", "Options": {"A": "The formula isolates unexpected loss by subtracting the average expected loss from the total loss calculated under the stress scenario.", "B": "The term `LGD × N(...)` represents the total loss per dollar of exposure under a severe stress scenario (99.9th percentile), while the term `LGD × PD` represents the average expected loss.", "C": "An increase in the Asset Value Correlation (`AVC`) parameter, holding `PD` and `LGD` constant, would lead to a higher risk weight (`RW`).", "D": "A higher `AVC` increases the required capital because it implies greater systematic risk, meaning more borrowers are likely to default simultaneously in a downturn, thus increasing tail risk."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's conceptual understanding of the Basel II A-IRB risk-weight formula, including the economic intuition behind its components and its sensitivity to key parameters. Depth Strategy: Computational Judgment. While not requiring a full calculation, the user must judge the properties and comparative statics of the formula. Distractor Logic: All options are correct, forming a complete conceptual check on the formula. A tests interpretation of the components. B tests understanding of the 'unexpected loss' concept. C tests the comparative statics with respect to `AVC`. D provides the economic reasoning for the result in C. This structure ensures the candidate understands not just what the formula does, but why it is structured that way.", "qid": "136", "question": "### Background\n\n**Research Question.** How does the Basel II Advanced Internal Ratings-Based (A-IRB) framework translate a bank's internal risk estimates for a credit card portfolio into a regulatory capital requirement?\n\n### Data / Model Specification\n\nThe Basel II risk weight (`RW`) for Qualifying Revolving Retail Exposures (QRREs) is calculated as:\n\n  \nRW = 12.5 \\times \\left[ \\left( LGD \\times N\\left(\\frac{N^{-1}(PD) + \\sqrt{AVC} \\times N^{-1}(0.999)}{\\sqrt{1-AVC}}\\right) \\right) - (LGD \\times PD) \\right] \n \n(Eq. 1)\n\nWhere `PD` is Probability of Default, `LGD` is Loss Given Default, `AVC` is a fixed Asset Value Correlation parameter (4% for QRREs), and `N(·)` is the standard normal CDF. The framework's philosophy is that regulatory capital should cover **unexpected losses**, while **expected losses** (`EL = PD × LGD`) should be covered by provisions and pricing.\n\n---\n\nWhich of the following statements are valid interpretations or properties of the Basel II risk-weight formula in **Eq. (1)**?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the Basel II risk-weight formula for QRREs, I need to analyze each statement:\n\n**Statement A**: The formula isolates unexpected loss by subtracting the average expected loss from the total loss calculated under the stress scenario.\n- This is correct. The formula structure is: RW = 12.5 × [Stress Loss - Expected Loss]\n- The stress loss is LGD × N(...) and expected loss is LGD × PD\n- This subtraction isolates the unexpected loss component\n\n**Statement B**: The term `LGD × N(...)` represents the total loss per dollar of exposure under a severe stress scenario (99.9th percentile), while the term `LGD × PD` represents the average expected loss.\n- This is correct. The N(...) function with the 99.9th percentile (N^(-1)(0.999)) represents the stressed default rate\n- LGD × PD is indeed the expected loss formula\n\n**Statement C**: An increase in the Asset Value Correlation (`AVC`) parameter, holding `PD` and `LGD` constant, would lead to a higher risk weight (`RW`).\n- Looking at the formula: as AVC increases, √AVC increases, making the numerator larger\n- Also, √(1-AVC) decreases, making the denominator smaller\n- Both effects increase the argument inside N(...), which increases the stress loss\n- Since expected loss (LGD × PD) remains constant, the risk weight increases\n- This is correct\n\n**Statement D**: A higher `AVC` increases the required capital because it implies greater systematic risk, meaning more borrowers are likely to default simultaneously in a downturn, thus increasing tail risk.\n- This correctly explains the economic intuition behind Statement C\n- Higher asset correlation means borrowers' creditworthiness is more correlated\n- In downturns, this leads to more simultaneous defaults (systematic risk)\n- This increases tail risk and requires more capital\n- This is correct\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 167, "Question": "### Background\n\nA study estimated a logistic regression model to predict the probability of *de novo* bank entry into a market. The sample consisted of 207 markets with entry and 205 matched markets without entry. The model included various market-relative variables to identify characteristics of attractive entry markets.\n\n### Data / Model Specification\n\nThe probability of entry is modeled as:\n  \nP(\\text{Entry}=1 | X) = \\frac{\\exp(X'\\beta)}{1 + \\exp(X'\\beta)}\n \n\n**Table 1: Logistic Regression Results for De Novo Entry**\n\n| Variable | Estimate (β) | p-value |\n| :--- | :--- | :--- |\n| MRROA (Market-relative ROA) | -0.8970 | 0.0072 |\n| MRSMLAMT (Market-relative amount of small-business loans) | 0.3523 | 0.1123 |\n| MRSMLNUM (Market-relative number of small-business loans) | -0.4076 | 0.0022 |\n| MRLOAN2A (Market-relative loans-to-assets) | 2.7092 | 0.0022 |\n\n---\n\nBased on the regression results in **Table 1**, which of the following market characteristics are statistically significant predictors of a higher likelihood of *de novo* bank entry? Select all that apply.", "Options": {"A": "Markets with a higher relative ratio of total loans to assets (higher MRLOAN2A).", "B": "Markets with a lower relative number of existing small-business loans (lower MRSMLNUM).", "C": "Markets with lower relative profitability (lower MRROA).", "D": "Markets with a higher relative dollar amount of existing small-business loans (higher MRSMLAMT)."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret the direction and statistical significance of coefficients in a logistic regression model. The depth strategy is 'Computational Judgment,' requiring the user to identify significant predictors and their directional impact on the outcome. Options A, B, and C correspond to variables with statistically significant coefficients (p < 0.05) and the correct sign interpretation (negative coefficients for MRROA and MRSMLNUM mean lower values predict entry; positive for MRLOAN2A means higher values predict entry). Option D is a distractor based on a statistically insignificant coefficient (p=0.1123), representing a plausible but unsupported conclusion.", "qid": "167", "question": "### Background\n\nA study estimated a logistic regression model to predict the probability of *de novo* bank entry into a market. The sample consisted of 207 markets with entry and 205 matched markets without entry. The model included various market-relative variables to identify characteristics of attractive entry markets.\n\n### Data / Model Specification\n\nThe probability of entry is modeled as:\n  \nP(\\text{Entry}=1 | X) = \\frac{\\exp(X'\\beta)}{1 + \\exp(X'\\beta)}\n \n\n**Table 1: Logistic Regression Results for De Novo Entry**\n\n| Variable | Estimate (β) | p-value |\n| :--- | :--- | :--- |\n| MRROA (Market-relative ROA) | -0.8970 | 0.0072 |\n| MRSMLAMT (Market-relative amount of small-business loans) | 0.3523 | 0.1123 |\n| MRSMLNUM (Market-relative number of small-business loans) | -0.4076 | 0.0022 |\n| MRLOAN2A (Market-relative loans-to-assets) | 2.7092 | 0.0022 |\n\n---\n\nBased on the regression results in **Table 1**, which of the following market characteristics are statistically significant predictors of a higher likelihood of *de novo* bank entry? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the logistic regression results, I need to identify which variables are statistically significant predictors of higher likelihood of bank entry.\n\nFirst, I'll identify statistically significant variables (typically p-value < 0.05):\n- MRROA: p-value = 0.0072 (significant)\n- MRSMLAMT: p-value = 0.1123 (not significant)\n- MRSMLNUM: p-value = 0.0022 (significant)\n- MRLOAN2A: p-value = 0.0022 (significant)\n\nNext, I'll determine which significant variables predict higher entry likelihood (positive coefficient = higher likelihood):\n- MRROA: β = -0.8970 (negative → lower MRROA increases entry likelihood)\n- MRSMLNUM: β = -0.4076 (negative → lower MRSMLNUM increases entry likelihood)\n- MRLOAN2A: β = 2.7092 (positive → higher MRLOAN2A increases entry likelihood)\n\nEvaluating each option:\n- A) Higher MRLOAN2A: Coefficient is positive (2.7092) and significant (p=0.0022) ✓\n- B) Lower MRSMLNUM: Coefficient is negative (-0.4076) and significant (p=0.0022) ✓\n- C) Lower MRROA: Coefficient is negative (-0.8970) and significant (p=0.0072) ✓\n- D) Higher MRSMLAMT: Not significant (p=0.1123) ✗\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 178, "Question": "### Background\n\n**Research Question.** This case examines the link between executive stock option compensation and incentives to meet short-term earnings targets. The primary hypothesis (H1) posits that a higher proportion of stock option compensation (`OPT`) increases the likelihood that a firm will meet or beat analyst earnings forecasts (`MEET=1`).\n\n**Causal Inference Challenge.** A key challenge is endogeneity. An unobserved factor, such as superior firm quality, could drive both higher option-based pay and stronger performance, creating a spurious correlation. To address this, the paper employs an instrumental variable (IV) approach, using the industry-average `OPT` as an instrument for a firm's own `OPT`.\n\n### Data / Model Specification\n\nThe probability of meeting or beating analyst forecasts (`MEET=1`) is modeled via logistic regression. The percentage change in odds for an indicator variable `j` is `100[exp(β_j) - 1]`. The paper also reports that a one-standard-deviation increase in `OPT` increases the odds of meeting the forecast by 5.3% in the baseline model.\n\n**Table 1: Logistic Regression Results for P(MEET=1)**\n\n| Specification | Variable | Coefficient | z-statistic |\n| :--- | :--- | :---: | :---: |\n| Baseline (OLS) | `OPT` | 0.199 | (2.82)** |\n| | `LIT` | 0.185 | (4.09)** |\n| Instrumental Variable (IV) | `OPT` | 0.042 | (2.65)** |\n\n*N=40,661. **p<0.01. `LIT` is an indicator for high-litigation risk industry.*\n\n### Question\n\nBased on the provided regression results and econometric context, select all statements that represent a correct quantitative interpretation or a valid critique of the identification strategy.", "Options": {"A": "The instrumental variable (IV) estimate for `OPT` (0.042) is smaller than the OLS estimate (0.199), suggesting the OLS estimate was biased downwards due to unobserved managerial talent.", "B": "A key threat to the OLS estimate's causal interpretation is reverse causality: boards may grant more options to managers who consistently meet forecasts, meaning `MEET` causes `OPT`.", "C": "The economic significance of being in a high-litigation industry (`LIT`) is substantially larger than that of a one-standard-deviation increase in `OPT`, with `LIT` increasing the odds of meeting a forecast by approximately 20.3%.", "D": "The IV's exclusion restriction is potentially violated if an unobserved factor, like 'hot industry' status, both increases industry-wide use of options (the instrument) and independently increases pressure on firms to meet earnings targets."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests a comprehensive set of econometrics skills: quantitative interpretation of logistic regression output (Computational Judgment), identification of endogeneity threats (Reverse-Reasoning), and critical evaluation of an instrumental variable strategy. It uses Atomic Decomposition. Option A requires a calculation. Option C identifies reverse causality. Option D critiques the IV's exclusion restriction. Option B is a Conceptual Opposite distractor, incorrectly stating the direction of OLS bias.", "qid": "178", "question": "### Background\n\n**Research Question.** This case examines the link between executive stock option compensation and incentives to meet short-term earnings targets. The primary hypothesis (H1) posits that a higher proportion of stock option compensation (`OPT`) increases the likelihood that a firm will meet or beat analyst earnings forecasts (`MEET=1`).\n\n**Causal Inference Challenge.** A key challenge is endogeneity. An unobserved factor, such as superior firm quality, could drive both higher option-based pay and stronger performance, creating a spurious correlation. To address this, the paper employs an instrumental variable (IV) approach, using the industry-average `OPT` as an instrument for a firm's own `OPT`.\n\n### Data / Model Specification\n\nThe probability of meeting or beating analyst forecasts (`MEET=1`) is modeled via logistic regression. The percentage change in odds for an indicator variable `j` is `100[exp(β_j) - 1]`. The paper also reports that a one-standard-deviation increase in `OPT` increases the odds of meeting the forecast by 5.3% in the baseline model.\n\n**Table 1: Logistic Regression Results for P(MEET=1)**\n\n| Specification | Variable | Coefficient | z-statistic |\n| :--- | :--- | :---: | :---: |\n| Baseline (OLS) | `OPT` | 0.199 | (2.82)** |\n| | `LIT` | 0.185 | (4.09)** |\n| Instrumental Variable (IV) | `OPT` | 0.042 | (2.65)** |\n\n*N=40,661. **p<0.01. `LIT` is an indicator for high-litigation risk industry.*\n\n### Question\n\nBased on the provided regression results and econometric context, select all statements that represent a correct quantitative interpretation or a valid critique of the identification strategy.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the regression results and econometric context.\n\n**Statement A**: Claims OLS estimate (0.199) was biased downwards because IV estimate (0.042) is smaller. This is incorrect. When IV < OLS, it typically suggests OLS was biased *upwards*, not downwards. The unobserved factor (managerial talent) would likely be positively correlated with both OPT and MEET, creating upward bias in OLS.\n\n**Statement B**: Identifies reverse causality as a threat to OLS causal interpretation. This is correct. If boards grant more options to managers who consistently meet forecasts, then MEET would cause OPT rather than the hypothesized OPT causing MEET. This is a valid endogeneity concern that threatens causal inference from OLS.\n\n**Statement C**: Claims LIT increases odds by approximately 20.3%. Let me calculate: For indicator variables, the percentage change in odds is 100[exp(β) - 1]. With β_LIT = 0.185: 100[exp(0.185) - 1] = 100[1.203 - 1] = 20.3%. This is correct. The paper states a one-standard-deviation increase in OPT increases odds by 5.3%, which is indeed substantially smaller than 20.3%.\n\n**Statement D**: Questions the IV's exclusion restriction. This is a valid critique. The exclusion restriction requires that industry-average OPT affects individual firm's MEET only through the firm's own OPT. If 'hot industry' status or other industry-level factors both increase industry-wide option use AND independently create pressure to meet earnings targets, the exclusion restriction would be violated. This is a legitimate concern for the IV strategy.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 124, "Question": "### Background\n\n**Research Question.** This case examines the characteristics that distinguish firms choosing to undertake an initial public debt offering (debt IPO) from those that do not.\n\n**Setting.** The study employs a matched-sample research design. For each of the 143 firms that issued a debt IPO, a control firm is randomly selected from the same industry group with no long-term debt in the year of the IPO. Univariate and multivariate (probit) analyses are used to compare the characteristics of the two groups in the year prior to the offering.\n\n---\n\n### Data / Model Specification\n\nTo identify the determinants of the debt IPO decision, the paper first compares the median characteristics of the sample firms with their matched controls (Table 1), and then estimates a multivariate probit model to control for interdependencies (Table 2).\n\n**Table 1: Comparison of Median Characteristics for Debt IPO Firms and Control Firms**\n\n| Characteristic | Sample Median | Control Median | p-Value for Difference |\n| :--- | :--- | :--- | :--- |\n| Market-to-book ratio | 1.35 | 1.62 | 0.089 |\n\n**Table 2: Probit Model Estimates for the Determinants of a Debt IPO**\n\n| Independent Variable | Coefficient | p-value |\n| :--- | :--- | :--- |\n| Intercept | -2.7363 | (0.00) |\n| Asset size | 0.4876 | (0.00) |\n| Capex | 6.3868 | (0.00) |\n| Market-to-book | 0.0275 | (0.66) |\n\n---\n\nBased on the provided data, which of the following statements represent valid conclusions or critiques of the research design? Select all that apply.", "Options": {"A": "The probit model in Table 2 indicates that, after controlling for other factors, a firm's growth opportunities (Market-to-book ratio) are a primary and significant driver of the decision to issue public debt.", "B": "The probit model results imply that a firm's size is a statistically significant predictor of its likelihood to undertake a debt IPO.", "C": "The univariate results in Table 1 suggest that firms with lower growth opportunities are more likely to undertake a debt IPO, but this conclusion is likely confounded by the omission of firm size.", "D": "A key limitation of the study's research design is that by comparing debt IPO firms to firms with no long-term debt, it cannot distinguish between the decision to use leverage versus the specific choice of public debt over private debt."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to synthesize findings from univariate and multivariate analyses, identify omitted variable bias, and critically evaluate a research design's choice of control group. Depth Strategy: Reverse-Reasoning (identifying plausible causes for conflicting results and flaws in design). Distractor Logic: Option A is correct; it accurately describes the univariate finding and correctly identifies the likely source of bias (omitted variable of firm size). Option C is a correct and sophisticated critique of the control group choice. Option D is a correct interpretation of the highly significant coefficient on `Asset size` in Table 2. Option B is a Conceptual Opposite; the probit model shows that the Market-to-book ratio is statistically insignificant (p=0.66), not a primary driver.", "qid": "124", "question": "### Background\n\n**Research Question.** This case examines the characteristics that distinguish firms choosing to undertake an initial public debt offering (debt IPO) from those that do not.\n\n**Setting.** The study employs a matched-sample research design. For each of the 143 firms that issued a debt IPO, a control firm is randomly selected from the same industry group with no long-term debt in the year of the IPO. Univariate and multivariate (probit) analyses are used to compare the characteristics of the two groups in the year prior to the offering.\n\n---\n\n### Data / Model Specification\n\nTo identify the determinants of the debt IPO decision, the paper first compares the median characteristics of the sample firms with their matched controls (Table 1), and then estimates a multivariate probit model to control for interdependencies (Table 2).\n\n**Table 1: Comparison of Median Characteristics for Debt IPO Firms and Control Firms**\n\n| Characteristic | Sample Median | Control Median | p-Value for Difference |\n| :--- | :--- | :--- | :--- |\n| Market-to-book ratio | 1.35 | 1.62 | 0.089 |\n\n**Table 2: Probit Model Estimates for the Determinants of a Debt IPO**\n\n| Independent Variable | Coefficient | p-value |\n| :--- | :--- | :--- |\n| Intercept | -2.7363 | (0.00) |\n| Asset size | 0.4876 | (0.00) |\n| Capex | 6.3868 | (0.00) |\n| Market-to-book | 0.0275 | (0.66) |\n\n---\n\nBased on the provided data, which of the following statements represent valid conclusions or critiques of the research design? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data provided:\n\n**Table 1 Analysis:**\n- Shows debt IPO firms have median Market-to-book of 1.35 vs 1.62 for controls\n- Lower market-to-book ratio for debt IPO firms (p-value = 0.089, marginally significant at 10% level)\n- Suggests firms with lower growth opportunities are more likely to undertake debt IPOs\n\n**Table 2 Analysis:**\n- Asset size coefficient: 0.4876 (p-value = 0.00) - highly significant\n- Market-to-book coefficient: 0.0275 (p-value = 0.66) - not statistically significant\n- After controlling for other factors, market-to-book is no longer a significant predictor\n\n**Evaluating Each Option:**\n\nA) FALSE - The p-value of 0.66 for Market-to-book in the probit model indicates it is NOT statistically significant, contradicting this statement.\n\nB) TRUE - Asset size has a coefficient of 0.4876 with p-value of 0.00, indicating it is highly statistically significant.\n\nC) TRUE - Table 1 shows lower market-to-book for debt IPO firms (marginally significant), but this effect disappears in the multivariate model when controlling for size, suggesting the univariate result was confounded by omitted variables like firm size.\n\nD) TRUE - The control group consists of firms with no long-term debt at all. This design compares firms issuing debt IPOs against firms with zero leverage, rather than firms that chose private debt. Therefore, it cannot isolate the public vs. private debt decision from the broader leverage decision.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 122, "Question": "### Background\n\n**Research Question.** How do key methodological choices in Data Envelopment Analysis (DEA) affect the measurement of bank efficiency?\n\n**Setting / Data-Generating Environment.** The study estimates bank efficiency using DEA and compares standard DEA scores with bootstrap bias-corrected scores.\n\n**Variables & Parameters.**\n\n*   `$\\hat{\\rho}$`: The standard, non-bootstrapped DEA cost efficiency score.\n*   `$\\hat{\\hat{\\rho}}$`: The bootstrap bias-corrected DEA cost efficiency score.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Bootstrapped Cost Efficiency Results (Model 3, Selected Years)**\n\n| Year | `$\\hat{\\rho}$` (Raw) | `$\\hat{\\hat{\\rho}}$` (Bias-Corrected) |\n| :--- | :--- | :--- |\n| 1994 | 0.3239* | 0.2003 |\n| 2001 | 0.4620* | 0.3282 |\n| 2007 | 0.4200* | 0.2853 |\n\n*Note: `*` denotes that the raw DEA efficiency score is significantly different from the bias-corrected score.*\n\n---\n\n### Question\n\nThe study's DEA methodology involves several key choices that affect the results. Based on the principles described in the paper and the results in Table 1, select all of the following statements that are methodologically correct.", "Options": {"A": "The raw DEA efficiency scores (`$\\hat{\\rho}$`) are systematically biased upwards because the estimated frontier, based on an in-sample envelopment, is almost certain to lie inside the true production frontier.", "B": "The significant difference between `$\\hat{\\rho}$` and `$\\hat{\\hat{\\rho}}$` in Table 1 suggests that the environmental variables used in the second stage are likely endogenous.", "C": "If the true banking technology exhibited increasing returns to scale for small banks, imposing a Constant Returns to Scale (CRS) assumption would tend to overstate the efficiency of these small banks.", "D": "Using a \"common-frontier\" for all years is a primary reason the study's efficiency scores are lower than those in other studies, as banks are compared against the best performers from the entire sample period."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a premise/assumption packaging strategy to test understanding of the core methodological assumptions of DEA and their consequences. The correct options are direct statements of principle from the paper. Distractor C is a conceptual opposite regarding the effect of the CRS assumption. Distractor D is a 'true but irrelevant concept' distractor, confusing the issue of DEA score bias with the separate econometric problem of endogeneity.", "qid": "122", "question": "### Background\n\n**Research Question.** How do key methodological choices in Data Envelopment Analysis (DEA) affect the measurement of bank efficiency?\n\n**Setting / Data-Generating Environment.** The study estimates bank efficiency using DEA and compares standard DEA scores with bootstrap bias-corrected scores.\n\n**Variables & Parameters.**\n\n*   `$\\hat{\\rho}$`: The standard, non-bootstrapped DEA cost efficiency score.\n*   `$\\hat{\\hat{\\rho}}$`: The bootstrap bias-corrected DEA cost efficiency score.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Bootstrapped Cost Efficiency Results (Model 3, Selected Years)**\n\n| Year | `$\\hat{\\rho}$` (Raw) | `$\\hat{\\hat{\\rho}}$` (Bias-Corrected) |\n| :--- | :--- | :--- |\n| 1994 | 0.3239* | 0.2003 |\n| 2001 | 0.4620* | 0.3282 |\n| 2007 | 0.4200* | 0.2853 |\n\n*Note: `*` denotes that the raw DEA efficiency score is significantly different from the bias-corrected score.*\n\n---\n\n### Question\n\nThe study's DEA methodology involves several key choices that affect the results. Based on the principles described in the paper and the results in Table 1, select all of the following statements that are methodologically correct.", "model_evaluation_result": {"model_raw_response": "Looking at each statement carefully:\n\n**Statement A:** This correctly describes a fundamental property of DEA. The DEA frontier is constructed by enveloping the observed data points, which are realizations from the true (but unknown) production process. Since we only observe a finite sample, the estimated frontier will almost certainly lie inside the true frontier, making all units appear more efficient than they truly are. This explains why the raw scores are systematically higher than the bias-corrected scores in Table 1.\n\n**Statement B:** The difference between raw and bias-corrected scores reflects the finite sample bias inherent in DEA estimation, not endogeneity of environmental variables. Bootstrap bias correction addresses the statistical bias from estimating the frontier with limited data, which is a separate issue from second-stage endogeneity concerns.\n\n**Statement C:** This statement is incorrect. Under increasing returns to scale (IRS), small banks can achieve proportionally greater output increases from input increases. If we incorrectly impose CRS when the true technology exhibits IRS, we would understate (not overstate) the efficiency of small banks, as we would be comparing them to an inappropriately scaled benchmark.\n\n**Statement D:** A common-frontier approach (pooling all years) creates a more demanding benchmark since banks are compared against the best performers across all time periods, not just their contemporaries. This would indeed tend to produce lower efficiency scores compared to year-by-year frontier estimation.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 55, "Question": "### Background\n\n**Research Question.** This case seeks to explain the observed asymmetry in monetary policy spillovers, where U.S. quantitative easing (QE) significantly impacts Chinese bond yields, while QE from other major central banks (BoE, ECB, BoJ) does not.\n\n**Setting and Sample.** The analysis is based on an event study of two-day changes in Chinese Treasury yields following QE announcements from the U.S., U.K., Eurozone, and Japan.\n\n---\n\n### Data / Model Specification\n\nThe empirical analysis reveals a stark asymmetry. Following major U.S. QE announcements, Chinese yields respond significantly. In contrast, following major QE announcements from other central banks that produced large moves in their own domestic yields, the impact on Chinese yields was marginal and statistically insignificant, as shown in **Table 1**.\n\n**Table 1: Asymmetric Spillover Effects of QE (Two-day yield changes in basis points)**\n| Announcement Event | Foreign 10Y Yield Change | Chinese 10Y Yield Change |\n| :--- | :--- | :--- |\n| U.S. QE I (Nov 25, 2008) | -31.30 | -28.04 |\n| U.K. QE (Mar 05, 2009) | -63.0 | -2.7 |\n| ECB QE (Jan 22, 2015) | -12.4 | -1.5 |\n| BoJ QE (Jan 29, 2016) | -16.1 | +4.9 |\n\n*Source: Abridged from Tables 1 and 2 in the paper.*\n\nThe paper notes that China's managed floating exchange rate regime gives the U.S. dollar a heavy weight in its currency basket, creating a tight linkage between the two countries' monetary policies.\n\n---\n\n### Question\n\nBased on the principles of international finance and the context provided, which of the following are the most likely economic reasons for the uniquely powerful spillover effect of U.S. monetary policy on China, as observed in **Table 1**? Select all that apply.", "Options": {"A": "The U.S. has a significantly larger GDP than the U.K., Eurozone, or Japan, meaning its domestic economic shocks are proportionally larger for the global economy.", "B": "The U.S. Treasury market serves as the global benchmark for pricing risk-free assets, forcing a worldwide repricing of duration and risk following a U.S. QE shock.", "C": "China's large trade surplus with the U.S. makes its economy exceptionally sensitive to changes in U.S. consumer demand, which is directly manipulated by QE.", "D": "The U.S. dollar's role as the world's primary reserve currency creates a tight policy linkage, as China's central bank must manage its exchange rate against the dollar, making U.S. policy signals highly relevant for the future path of Chinese policy."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the student's ability to synthesize standard international finance theory with the paper's core empirical finding. It replaces part (a) of the source QA. The correct options (A, B) identify the two primary channels discussed in the literature: the portfolio rebalancing channel amplified by the U.S. Treasury's benchmark status, and the signaling channel amplified by the dollar's reserve currency status. Distractor (C) is 'True but Irrelevant' for bond yield spillovers, as it relates to trade channels, not financial transmission. Distractor (D) is a plausible but less precise explanation; while GDP size matters, the unique *financial* roles of the dollar and Treasury market are the direct mechanisms for spillover, making it an 'Almost Right' distractor.", "qid": "55", "question": "### Background\n\n**Research Question.** This case seeks to explain the observed asymmetry in monetary policy spillovers, where U.S. quantitative easing (QE) significantly impacts Chinese bond yields, while QE from other major central banks (BoE, ECB, BoJ) does not.\n\n**Setting and Sample.** The analysis is based on an event study of two-day changes in Chinese Treasury yields following QE announcements from the U.S., U.K., Eurozone, and Japan.\n\n---\n\n### Data / Model Specification\n\nThe empirical analysis reveals a stark asymmetry. Following major U.S. QE announcements, Chinese yields respond significantly. In contrast, following major QE announcements from other central banks that produced large moves in their own domestic yields, the impact on Chinese yields was marginal and statistically insignificant, as shown in **Table 1**.\n\n**Table 1: Asymmetric Spillover Effects of QE (Two-day yield changes in basis points)**\n| Announcement Event | Foreign 10Y Yield Change | Chinese 10Y Yield Change |\n| :--- | :--- | :--- |\n| U.S. QE I (Nov 25, 2008) | -31.30 | -28.04 |\n| U.K. QE (Mar 05, 2009) | -63.0 | -2.7 |\n| ECB QE (Jan 22, 2015) | -12.4 | -1.5 |\n| BoJ QE (Jan 29, 2016) | -16.1 | +4.9 |\n\n*Source: Abridged from Tables 1 and 2 in the paper.*\n\nThe paper notes that China's managed floating exchange rate regime gives the U.S. dollar a heavy weight in its currency basket, creating a tight linkage between the two countries' monetary policies.\n\n---\n\n### Question\n\nBased on the principles of international finance and the context provided, which of the following are the most likely economic reasons for the uniquely powerful spillover effect of U.S. monetary policy on China, as observed in **Table 1**? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify the most likely economic reasons for why U.S. monetary policy has uniquely powerful spillover effects on China compared to other major economies' QE programs.\n\nLet me analyze each option:\n\n**Option A**: While the U.S. does have a large GDP, this alone doesn't explain the asymmetric spillover. The Eurozone's combined GDP is comparable to the U.S., yet its QE had minimal impact on Chinese yields. Size alone isn't the key factor here.\n\n**Option B**: The U.S. Treasury market as a global benchmark is highly relevant. When U.S. yields change significantly due to QE, it forces a global repricing of risk-free rates and duration risk. This explains why U.S. policy changes have such broad international impact - all other assets are priced relative to U.S. Treasuries.\n\n**Option C**: This misunderstands how QE works. QE primarily affects financial conditions through asset prices and yields, not by directly manipulating consumer demand. The trade channel is indirect and wouldn't explain the immediate yield response shown in the data.\n\n**Option D**: This directly addresses the key insight from the paper - China's managed float with heavy dollar weighting creates a tight policy linkage. When the U.S. implements QE, China must consider how this affects their exchange rate management and potentially adjust their own monetary policy accordingly. This explains why Chinese yields move in tandem with U.S. yields but not with other countries' yields.\n\nThe data shows immediate (two-day) yield responses, suggesting financial market channels rather than trade channels. The asymmetry - where only U.S. QE matters - points to the unique role of the dollar in China's monetary framework.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 52, "Question": "### Background\n\nA study investigates whether the interaction between product life cycle (PLC) and the book-to-market (BM) effect is state-dependent, varying with macroeconomic conditions and investor sentiment. The study calculates the Fama-French five-factor alphas for hedge portfolios that are long high-BM stocks and short low-BM stocks (HML portfolios), constructed within different PLC groups. The key metric is the 'Long-Short HML Alpha Spread': the alpha of the long-PLC HML portfolio minus the alpha of the short-PLC HML portfolio.\n\n### Data / Model Specification\n\nTable 1 summarizes this key metric across different market states.\n\n**Table 1: Long-Short HML Alpha Spreads (%) in Subsamples**\n\n| Subsample Condition | Long-Short HML Alpha Spread (%) | *t-statistic* |\n| :--- | :---: | :---: |\n| **Panel A: Business Cycle** | |\n| Expansion | 0.30 | (1.76) |\n| Recession | 2.09 | (2.48) |\n| **Panel B: Investor Sentiment** | |\n| Low Sentiment | 0.21 | (1.20) |\n| High Sentiment | 0.87 | (3.37) |\n\n*Source: Table 3 of the paper.* \n\n---\n\nBased on the results in Table 1, which of the following statements accurately describe the findings and their implications? Select all that apply.", "Options": {"A": "The PLC-BM interaction effect is significantly more pronounced during periods of high investor sentiment, supporting a behavioral mispricing explanation.", "B": "During periods of low investor sentiment, the Long-Short HML Alpha Spread is large and highly statistically significant.", "C": "The results suggest the PLC-BM interaction is a consistent, stable effect that does not vary with market conditions.", "D": "The anomalous return spread associated with the PLC-BM interaction is significantly stronger during economic recessions than during expansions."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item tests the ability to interpret state-dependent portfolio returns and synthesize findings from two different sample splits (business cycle and sentiment) to evaluate competing hypotheses (risk vs. mispricing). Depth Strategy: Reverse-Reasoning. Given the empirical results, the user must identify the plausible theoretical explanations. Distractor Logic: C is a conceptual opposite of the clear state-dependency shown in the table. D is a direct misinterpretation of the insignificant result for the low-sentiment period.", "qid": "52", "question": "### Background\n\nA study investigates whether the interaction between product life cycle (PLC) and the book-to-market (BM) effect is state-dependent, varying with macroeconomic conditions and investor sentiment. The study calculates the Fama-French five-factor alphas for hedge portfolios that are long high-BM stocks and short low-BM stocks (HML portfolios), constructed within different PLC groups. The key metric is the 'Long-Short HML Alpha Spread': the alpha of the long-PLC HML portfolio minus the alpha of the short-PLC HML portfolio.\n\n### Data / Model Specification\n\nTable 1 summarizes this key metric across different market states.\n\n**Table 1: Long-Short HML Alpha Spreads (%) in Subsamples**\n\n| Subsample Condition | Long-Short HML Alpha Spread (%) | *t-statistic* |\n| :--- | :---: | :---: |\n| **Panel A: Business Cycle** | |\n| Expansion | 0.30 | (1.76) |\n| Recession | 2.09 | (2.48) |\n| **Panel B: Investor Sentiment** | |\n| Low Sentiment | 0.21 | (1.20) |\n| High Sentiment | 0.87 | (3.37) |\n\n*Source: Table 3 of the paper.* \n\n---\n\nBased on the results in Table 1, which of the following statements accurately describe the findings and their implications? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each statement against the data:\n\n**Statement A**: \"The PLC-BM interaction effect is significantly more pronounced during periods of high investor sentiment\"\n- High sentiment: 0.87% with t-statistic of 3.37 (highly significant)\n- Low sentiment: 0.21% with t-statistic of 1.20 (not significant)\n- The effect is indeed more pronounced and significant during high sentiment periods\n- **TRUE**\n\n**Statement B**: \"During periods of low investor sentiment, the Long-Short HML Alpha Spread is large and highly statistically significant\"\n- Low sentiment: 0.21% with t-statistic of 1.20\n- This is neither large nor statistically significant (t-stat < 1.96)\n- **FALSE**\n\n**Statement C**: \"The results suggest the PLC-BM interaction is a consistent, stable effect that does not vary with market conditions\"\n- The spreads vary considerably: 0.30% vs 2.09% (business cycle), 0.21% vs 0.87% (sentiment)\n- Statistical significance also varies across conditions\n- This shows the effect is NOT stable and DOES vary with market conditions\n- **FALSE**\n\n**Statement D**: \"The anomalous return spread associated with the PLC-BM interaction is significantly stronger during economic recessions than during expansions\"\n- Recession: 2.09% with t-statistic of 2.48 (significant)\n- Expansion: 0.30% with t-statistic of 1.76 (not significant at 5% level)\n- The spread is indeed significantly stronger during recessions\n- **TRUE**\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 213, "Question": "### Background\n\n**Research Question.** How can the market price of longevity risk be inferred from traded derivatives, and what are the limitations of this approach when multiple risk factors are present?\n\n**Setting.** The market prices of risk, `λ_1` (for level risk) and `λ_2` (for slope risk), are calibrated by forcing a two-factor mortality model to match the observed 500 basis point spread of the 10-year QxX index swap. This process is known as 'backing out' or 'implying' the risk prices from the market.\n\n### Data / Model Specification\n\nThe market prices of risk `(λ_1, λ_2)` are found by solving the pricing equation for the swap, setting the fair spread `σ` to the market quote of 500 basis points. Since there is one equation (one price) and two unknowns (`λ_1`, `λ_2`), the solution is not unique. The tables below show two sets of results: Table 1 shows three possible `(λ_1, λ_2)` pairs implied by the 500 bps spread. Table 2 shows the swap spreads that would be calculated if one were to instead use `λ` values calibrated from an older, non-traded longevity bond (the BNP/EIB bond).\n\n**Table 1.** Implied values of `(λ_1, λ_2)` from the 500 bps QxX swap spread.\n\n| λ_1    | λ_2    |\n|--------|--------|\n| 0      | 3.4345 |\n| 6.4732 | 0      |\n| 2.2357 | 2.2357 |\n\n**Table 2.** Estimates of `σ` (in basis points) using `λ` calibrated from the BNP/EIB bond.\n\n| λ_1   | λ_2   | Fair Spread (σ) |\n|-------|-------|-----------------|\n| 0     | 0.316 | 625             |\n| 0.375 | 0     | 631             |\n| 0.175 | 0.175 | 627             |\n\n---\n\nBased on the data and the paper's discussion, which of the following statements are valid conclusions regarding the calibration of the market prices of risk `(λ_1, λ_2)`?\n", "Options": {"A": "The large, positive `λ` values in Table 1 imply that market participants demand a significant premium for bearing the risk of unexpected longevity improvements.", "B": "The existence of multiple `(λ_1, λ_2)` pairs in Table 1 that produce the same 500 bps spread is a direct consequence of an incomplete market for longevity risk.", "C": "The discrepancy between the ~630 bps fair value in Table 2 and the 500 bps market price (Table 1) likely represents a pure arbitrage opportunity.", "D": "The `λ` values in Table 1 are more credible for pricing the QxX swap than those used in Table 2 because the BNP/EIB bond price was stale and subject to significant basis risk."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret implied risk prices and synthesize information from two tables to understand the concept of market incompleteness and basis risk. Depth Strategy: Reverse-Reasoning. The user is given the results (two different calibrations) and must identify the plausible causes and implications. Distractor Logic: Option A is correct; market incompleteness (two risks, one price) is the reason for non-uniqueness. Option B is correct; positive lambdas signify a premium for bearing longevity risk. Option D is correct, reflecting the paper's explicit limitations on the BNP/EIB calibration (stale price, different underlying population). Option C is a 'Conceptual Opposite' distractor; the paper argues this discrepancy is due to basis risk and other factors, making it an illusory arbitrage, not a pure one.", "qid": "213", "question": "### Background\n\n**Research Question.** How can the market price of longevity risk be inferred from traded derivatives, and what are the limitations of this approach when multiple risk factors are present?\n\n**Setting.** The market prices of risk, `λ_1` (for level risk) and `λ_2` (for slope risk), are calibrated by forcing a two-factor mortality model to match the observed 500 basis point spread of the 10-year QxX index swap. This process is known as 'backing out' or 'implying' the risk prices from the market.\n\n### Data / Model Specification\n\nThe market prices of risk `(λ_1, λ_2)` are found by solving the pricing equation for the swap, setting the fair spread `σ` to the market quote of 500 basis points. Since there is one equation (one price) and two unknowns (`λ_1`, `λ_2`), the solution is not unique. The tables below show two sets of results: Table 1 shows three possible `(λ_1, λ_2)` pairs implied by the 500 bps spread. Table 2 shows the swap spreads that would be calculated if one were to instead use `λ` values calibrated from an older, non-traded longevity bond (the BNP/EIB bond).\n\n**Table 1.** Implied values of `(λ_1, λ_2)` from the 500 bps QxX swap spread.\n\n| λ_1    | λ_2    |\n|--------|--------|\n| 0      | 3.4345 |\n| 6.4732 | 0      |\n| 2.2357 | 2.2357 |\n\n**Table 2.** Estimates of `σ` (in basis points) using `λ` calibrated from the BNP/EIB bond.\n\n| λ_1   | λ_2   | Fair Spread (σ) |\n|-------|-------|-----------------|\n| 0     | 0.316 | 625             |\n| 0.375 | 0     | 631             |\n| 0.175 | 0.175 | 627             |\n\n---\n\nBased on the data and the paper's discussion, which of the following statements are valid conclusions regarding the calibration of the market prices of risk `(λ_1, λ_2)`?\n", "model_evaluation_result": {"model_raw_response": "Looking at this question about calibrating market prices of longevity risk, I need to evaluate each statement based on the data and context provided.\n\n**Statement A**: The large, positive λ values in Table 1 (ranging from 2.2357 to 6.4732) do indicate that market participants require compensation for bearing longevity risk. Positive risk prices mean investors demand a premium for taking on the risk of unexpected longevity improvements. This is a valid conclusion.\n\n**Statement B**: Table 1 shows three different (λ₁, λ₂) pairs that all produce the same 500 bps spread. This non-uniqueness occurs because we have one equation (one market price) but two unknowns (two risk factors). This is indeed a characteristic of incomplete markets where there aren't enough traded instruments to uniquely identify all risk prices. This is valid.\n\n**Statement C**: The ~130 bps difference between Table 2's fair values (~630 bps) and the market price (500 bps) doesn't necessarily represent pure arbitrage. The λ values in Table 2 come from an older, non-traded bond with different characteristics. Different calibration sources, basis risk, and market segmentation can explain price differences without implying arbitrage opportunities. This statement is not valid.\n\n**Statement D**: The λ values from Table 1 are calibrated directly from the current QxX swap market price (500 bps), while Table 2 uses λ values from an older BNP/EIB bond. The text mentions this bond price was \"stale\" and subject to \"significant basis risk.\" Using current market data from the same instrument you're pricing is more appropriate than using outdated data from a different instrument. This is valid.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 200, "Question": "### Background\n\nAn insurance company's management aims to translate a single, clear operational metric—the combined ratio—into superior shareholder value, defined as achieving a return on equity (ROE) that exceeds the cost of equity. Progressive Insurance, a large U.S. auto insurer, has established a core corporate strategy of maximizing growth subject to maintaining specific targets for its underwriting margin, leverage, and investment policies.\n\n**Key Terms:**\n- `CR`: Combined Ratio, the sum of losses and expenses as a percentage of premium revenue.\n- `P`: Premium Revenue, the total revenue from insurance policies sold.\n- `S`: Surplus (or Capital), the capital base of the insurer.\n- `D`: Debt.\n- `E`: Equity, where `S = E + D`.\n- `A_inv`: Investable Assets, the portfolio of assets generating investment returns.\n- `r_inv`: Rate of return on the investment portfolio.\n- `ROE`: Return on Equity.\n\n---\n\n### Data / Model Specification\n\nProgressive's corporate strategy is defined by four explicit financial policies, summarized in Table 1.\n\n**Table 1: Progressive's Stated Financial Policies**\n\n| Policy Metric | Target Value |\n| :--- | :--- |\n| Combined Ratio (`CR`) | `≤ 96%` |\n| Sales-to-Capital (`P/S`) | `3-to-1` |\n| Debt-to-Capital (`D/S`) | `25%` |\n| Investment Allocation | `85% Bonds / 15% Equities` |\n\nThe Return on Equity for an insurer can be decomposed into contributions from its underwriting and investment activities. A simplified (pre-tax) model is:\n\n  \nROE = \\frac{\\text{Underwriting Income} + \\text{Investment Income}}{E} = \\frac{P \\times (1 - CR)}{E} + \\frac{A_{inv} \\times r_{inv}}{E} \\quad \\text{(Eq. (1))}\n \n\nFor the purpose of this analysis, assume that investable assets (`A_inv`) are composed of the firm's capital (surplus) plus the float generated by its premium collections. A reasonable approximation is `A_inv ≈ P + S`.\n\n---\n\n### Question\n\nBased on the provided data, model, and additional return assumptions (4% pre-tax return on bonds, 10% pre-tax return on equities), select all of the following statements that are mathematically correct.", "Options": {"A": "The component of ROE derived from underwriting profit is 16%.", "B": "The total expected pre-tax ROE is approximately 26.1%.", "C": "The expected return on the investment portfolio (`r_inv`) is 7.0%.", "D": "The ratio of Premiums to Equity (`P/E`) for the firm is 4-to-1."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to execute a multi-step financial derivation based on stated corporate policies. It uses a Computational Judgment strategy via atomic decomposition. The correct options (A, B) are key intermediate steps in the derivation of ROE. Distractor C is based on a common error of taking a simple average instead of a weighted average for the portfolio return. Distractor D is a step-omission error, representing only the investment component of ROE, not the total.", "qid": "200", "question": "### Background\n\nAn insurance company's management aims to translate a single, clear operational metric—the combined ratio—into superior shareholder value, defined as achieving a return on equity (ROE) that exceeds the cost of equity. Progressive Insurance, a large U.S. auto insurer, has established a core corporate strategy of maximizing growth subject to maintaining specific targets for its underwriting margin, leverage, and investment policies.\n\n**Key Terms:**\n- `CR`: Combined Ratio, the sum of losses and expenses as a percentage of premium revenue.\n- `P`: Premium Revenue, the total revenue from insurance policies sold.\n- `S`: Surplus (or Capital), the capital base of the insurer.\n- `D`: Debt.\n- `E`: Equity, where `S = E + D`.\n- `A_inv`: Investable Assets, the portfolio of assets generating investment returns.\n- `r_inv`: Rate of return on the investment portfolio.\n- `ROE`: Return on Equity.\n\n---\n\n### Data / Model Specification\n\nProgressive's corporate strategy is defined by four explicit financial policies, summarized in Table 1.\n\n**Table 1: Progressive's Stated Financial Policies**\n\n| Policy Metric | Target Value |\n| :--- | :--- |\n| Combined Ratio (`CR`) | `≤ 96%` |\n| Sales-to-Capital (`P/S`) | `3-to-1` |\n| Debt-to-Capital (`D/S`) | `25%` |\n| Investment Allocation | `85% Bonds / 15% Equities` |\n\nThe Return on Equity for an insurer can be decomposed into contributions from its underwriting and investment activities. A simplified (pre-tax) model is:\n\n  \nROE = \\frac{\\text{Underwriting Income} + \\text{Investment Income}}{E} = \\frac{P \\times (1 - CR)}{E} + \\frac{A_{inv} \\times r_{inv}}{E} \\quad \\text{(Eq. (1))}\n \n\nFor the purpose of this analysis, assume that investable assets (`A_inv`) are composed of the firm's capital (surplus) plus the float generated by its premium collections. A reasonable approximation is `A_inv ≈ P + S`.\n\n---\n\n### Question\n\nBased on the provided data, model, and additional return assumptions (4% pre-tax return on bonds, 10% pre-tax return on equities), select all of the following statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "I need to analyze Progressive's financial policies and calculate various metrics to determine which statements are correct.\n\nGiven information:\n- Combined Ratio (CR) ≤ 96%\n- Sales-to-Capital (P/S) = 3-to-1\n- Debt-to-Capital (D/S) = 25%\n- Investment Allocation: 85% Bonds / 15% Equities\n- Bond return = 4% pre-tax\n- Equity return = 10% pre-tax\n- A_inv ≈ P + S\n\nFirst, let me establish the relationships:\n- P/S = 3\n- D/S = 0.25\n- Since S = E + D, and D/S = 0.25, then D = 0.25S and E = 0.75S\n- Therefore, P/E = P/S × S/E = 3 × (1/0.75) = 4\n\nNow I'll check each statement:\n\n**Statement A: The component of ROE derived from underwriting profit is 16%**\nUnderwriting ROE = P × (1 - CR) / E\nUsing CR = 96% (worst case), P/E = 4:\nUnderwriting ROE = 4 × (1 - 0.96) = 4 × 0.04 = 0.16 = 16% ✓\n\n**Statement B: The total expected pre-tax ROE is approximately 26.1%**\nTotal ROE = Underwriting ROE + Investment ROE\nInvestment ROE = (A_inv × r_inv) / E\nWhere A_inv = P + S = 3S + S = 4S\nAnd r_inv = 0.85 × 4% + 0.15 × 10% = 3.4% + 1.5% = 4.9%\nInvestment ROE = (4S × 0.049) / (0.75S) = 0.196S / 0.75S = 0.261 = 26.1%\nWait, this is just the investment portion. Total ROE = 16% + 26.1% = 42.1%", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 135, "Question": "### Background\n\n**Research Question.** During a period of financial stress, how does the Basel II rule for securitized assets affect a Credit Card Specialty Bank's (CCSB) capital adequacy?\n\n**Setting.** A hypothetical average CCSB that has adopted Basel II experiences a downturn in its portfolio's performance. This triggers a 15% Credit Conversion Factor (CCF) on its off-balance-sheet securitized assets, effectively bringing a portion of their risk back into the regulatory capital calculation.\n\n### Data / Model Specification\n\nDuring a stress period, deteriorating performance of credit card asset-backed securities (CC-ABS) can trigger a positive `CCF`. For a bank with a typical securitization rate of 60.7%, a 15% `CCF` results in a 23.2% increase in the total assets subject to a capital charge. This further increases the bank's Risk-Weighted Assets (RWA), on top of the initial increase from adopting Basel II.\n\n**Table 1. Impact of a 15% CCF on a Basel II CCSB's Total Capital Ratio**\n\n| Scenario | Basel II (Zero CCF) | Basel II (15% CCF Triggered) |\n| :--- | :---: | :---: |\n| Total Capital Ratio | 12.8% | 10.4% |\n*Note: Values reflect the higher-impact scenario for a diversified bank structure. The minimum regulatory Total Capital Ratio is 8%, and the 'well-capitalized' threshold is 10%.*\n\n---\n\nBased on the scenario and data in **Table 1**, which of the following statements are valid consequences or interpretations of the CCF rule being triggered?\n\nSelect all that apply.", "Options": {"A": "The CCF mechanism acts as a pro-cyclical amplifier of financial distress, increasing a bank's capital requirement precisely when its asset quality is deteriorating.", "B": "Triggering the 15% CCF erodes the bank's capital buffer to a razor-thin margin above the 10% 'well-capitalized' threshold, making the Total Capital requirement a binding constraint on management's decisions.", "C": "The CCF rule could create a competitive disadvantage for Basel II banks relative to Basel I banks (which have no CCF), as the former face an explicit, automatic capital penalty during a downturn that the latter do not.", "D": "The 23.2% increase in the asset base for capital calculation is derived from applying the 15% CCF to the 60.7% of the bank's managed portfolio that is securitized."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret the quantitative impact of the CCF rule, understand its derivation, and grasp its broader strategic and economic implications. Depth Strategy: Scenario Application. The user must apply the concept of the CCF rule to the specific scenario of a bank under stress and identify all correct consequences. Distractor Logic: This is another comprehensive assessment item where all options are correct. Option A tests economic intuition (pro-cyclicality). Option B tests quantitative interpretation of the table. Option C tests understanding of the underlying calculation. Option D tests the ability to draw a high-level strategic conclusion. Requiring the selection of all four ensures a holistic understanding of the CCF mechanism's impact.", "qid": "135", "question": "### Background\n\n**Research Question.** During a period of financial stress, how does the Basel II rule for securitized assets affect a Credit Card Specialty Bank's (CCSB) capital adequacy?\n\n**Setting.** A hypothetical average CCSB that has adopted Basel II experiences a downturn in its portfolio's performance. This triggers a 15% Credit Conversion Factor (CCF) on its off-balance-sheet securitized assets, effectively bringing a portion of their risk back into the regulatory capital calculation.\n\n### Data / Model Specification\n\nDuring a stress period, deteriorating performance of credit card asset-backed securities (CC-ABS) can trigger a positive `CCF`. For a bank with a typical securitization rate of 60.7%, a 15% `CCF` results in a 23.2% increase in the total assets subject to a capital charge. This further increases the bank's Risk-Weighted Assets (RWA), on top of the initial increase from adopting Basel II.\n\n**Table 1. Impact of a 15% CCF on a Basel II CCSB's Total Capital Ratio**\n\n| Scenario | Basel II (Zero CCF) | Basel II (15% CCF Triggered) |\n| :--- | :---: | :---: |\n| Total Capital Ratio | 12.8% | 10.4% |\n*Note: Values reflect the higher-impact scenario for a diversified bank structure. The minimum regulatory Total Capital Ratio is 8%, and the 'well-capitalized' threshold is 10%.*\n\n---\n\nBased on the scenario and data in **Table 1**, which of the following statements are valid consequences or interpretations of the CCF rule being triggered?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the provided data and scenario.\n\n**Statement A**: \"The CCF mechanism acts as a pro-cyclical amplifier of financial distress, increasing a bank's capital requirement precisely when its asset quality is deteriorating.\"\n\nThis is clearly true. The scenario explicitly states that \"deteriorating performance\" triggers the CCF, which then increases RWA and thus capital requirements. This happens during stress periods when the bank is already struggling - a textbook example of pro-cyclicality.\n\n**Statement B**: \"Triggering the 15% CCF erodes the bank's capital buffer to a razor-thin margin above the 10% 'well-capitalized' threshold, making the Total Capital requirement a binding constraint on management's decisions.\"\n\nThe data shows the Total Capital Ratio drops from 12.8% to 10.4% when the CCF is triggered. With the well-capitalized threshold at 10%, this leaves only a 0.4% buffer - indeed a \"razor-thin margin.\" This would severely constrain management's ability to take on new risks or expand operations.\n\n**Statement C**: \"The CCF rule could create a competitive disadvantage for Basel II banks relative to Basel I banks (which have no CCF), as the former face an explicit, automatic capital penalty during a downturn that the latter do not.\"\n\nThe statement correctly identifies that Basel I banks don't have CCF rules. During downturns, Basel II banks face automatic capital increases through the CCF mechanism while Basel I banks don't, creating an uneven playing field and competitive disadvantage.\n\n**Statement D**: \"The 23.2% increase in the asset base for capital calculation is derived from applying the 15% CCF to the 60.7% of the bank's managed portfolio that is securitized.\"\n\nLet me verify: If 60.7% of assets are securitized and off-balance-sheet, applying a 15% CCF brings back 15% × 60.7% = 9.105% of total assets. The increase in assets subject to capital charge is 9.105% / (100% - 60.7%) = 9.105% / 39.3% = 23.2%. This matches the stated figure exactly.\n\nAll four statements are valid consequences or interpretations of the CCF", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 225, "Question": "### Background\nIn the later part of its sample, the paper finds that the Bank of Canada's (BoC) response to the standard one-year-ahead inflation gap becomes statistically insignificant. This raises the question of whether the BoC stopped responding to inflation, or if it shifted its focus to a different measure. The paper proposes that the BoC may respond to *persistent* inflation deviations (the average expected deviation over an extended future period) and that this response could be non-linear and asymmetric.\n\n### Data / Model Specification\nTo test this, the baseline monetary policy rule is augmented with a new inflation term, $\\tilde{\\pi}_{t}|\\tilde{\\pi}_{t}|$, which captures a non-linear response to the average expected future inflation deviation, $\\tilde{\\pi}_{t}$. The model allows for an asymmetric response by estimating separate coefficients for positive deviations (overshoots, $\\phi_{id}^{pos}$) and negative deviations (undershoots, $\\phi_{id}^{neg}$). A larger coefficient implies a stronger policy reaction.\n\nAccording to the paper's analysis, which of the following are valid characterizations of the BoC's policy response to persistent inflation deviations?\n", "Options": {"A": "The BoC's response is asymmetric and time-varying, with evidence suggesting a stronger reaction to positive deviations (overshoots) early in the post-disinflation period, and a stronger reaction to negative deviations (undershoots) later in the period, especially after the Global Financial Crisis.", "B": "The BoC's response is non-linear, meaning that as expected persistent deviations from the target grow larger, the policy interest rate response becomes disproportionately stronger.", "C": "The paper finds no evidence that the BoC responds to *past* persistent inflation deviations, suggesting a forward-looking \"bygone\" strategy rather than a backward-looking \"makeup\" strategy.", "D": "The paper finds that the BoC responds more aggressively to positive deviations (overshoots) than negative deviations (undershoots) throughout the entire post-disinflation period (1995-2015)."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the comprehension of the paper's most novel and complex finding: the shift to a non-linear, asymmetric, and time-varying response to persistent inflation deviations. Depth Strategy: Scenario Application. The question asks the user to apply the paper's general findings to characterize the BoC's behavior under different conditions (e.g., small vs. large deviations, early vs. late periods). Distractor Logic: Option D is a conceptual distractor (Almost Right). While the paper does find a strong response to positive deviations, it explicitly states this response was concentrated in the *first half* of the post-disinflation period and that the response to negative deviations became more prominent later. The claim that this holds for the *entire* period is incorrect and tests for a nuanced understanding of the time-varying nature of the asymmetry.", "qid": "225", "question": "### Background\nIn the later part of its sample, the paper finds that the Bank of Canada's (BoC) response to the standard one-year-ahead inflation gap becomes statistically insignificant. This raises the question of whether the BoC stopped responding to inflation, or if it shifted its focus to a different measure. The paper proposes that the BoC may respond to *persistent* inflation deviations (the average expected deviation over an extended future period) and that this response could be non-linear and asymmetric.\n\n### Data / Model Specification\nTo test this, the baseline monetary policy rule is augmented with a new inflation term, $\\tilde{\\pi}_{t}|\\tilde{\\pi}_{t}|$, which captures a non-linear response to the average expected future inflation deviation, $\\tilde{\\pi}_{t}$. The model allows for an asymmetric response by estimating separate coefficients for positive deviations (overshoots, $\\phi_{id}^{pos}$) and negative deviations (undershoots, $\\phi_{id}^{neg}$). A larger coefficient implies a stronger policy reaction.\n\nAccording to the paper's analysis, which of the following are valid characterizations of the BoC's policy response to persistent inflation deviations?\n", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which statements accurately characterize the Bank of Canada's policy response to persistent inflation deviations based on the paper's findings.\n\nLet me analyze each option:\n\n**Option A**: States that the BoC's response is asymmetric and time-varying, with stronger reactions to overshoots early in the post-disinflation period and stronger reactions to undershoots later (especially post-GFC).\n\nThis aligns with the background information which indicates:\n- The response is non-linear and asymmetric\n- Separate coefficients are estimated for positive and negative deviations\n- The response pattern changes over time\n\n**Option B**: States the response is non-linear - as persistent deviations grow larger, the policy response becomes disproportionately stronger.\n\nThis is directly supported by the model specification which includes the term $\\tilde{\\pi}_{t}|\\tilde{\\pi}_{t}|$. This mathematical form creates a non-linear response where the magnitude of the response increases more than proportionally with the size of the deviation.\n\n**Option C**: Claims the paper finds no evidence of response to *past* persistent deviations, suggesting a forward-looking rather than backward-looking strategy.\n\nThe background explicitly states that \"the BoC may respond to *persistent* inflation deviations (the average expected deviation over an extended future period).\" This clearly indicates a forward-looking approach focused on expected future deviations, not past ones.\n\n**Option D**: Claims the BoC responds more aggressively to overshoots throughout the entire 1995-2015 period.\n\nThis contradicts Option A, which indicates the response pattern changes over time. Given that Option A specifies a time-varying response with different patterns in different periods, Option D's claim of a consistent pattern throughout cannot be correct.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 162, "Question": "### Background\n\n**Research Question.** How can the empirical superiority of a new volatility model be rigorously established? This involves assessing its performance against strong benchmarks using both statistical loss functions and practical, economic criteria across different market regimes.\n\n**Setting.** A comparative evaluation of the proposed Fractionally Integrated Generalized Autoregressive Score (FIGAS) model against the short-memory GAS model. The evaluation uses daily realized kernel data for 15 U.S. equities.\n\n**Key Concepts.**\n- **Statistical Evaluation:** The Quasi-Likelihood (QLIK) loss function is used to measure the statistical fit of a forecast. A lower QLIK value indicates a better fit.\n- **Positive Definiteness:** For a covariance matrix `V_t` to be valid, it must be positive definite. The FIGAS(0,d,1) model guarantees this if its scalar parameters `B` and `d` satisfy one of two conditions. Case 1: `0 < B < 1` and `d - B ≥ 0`. Case 2: `ζ-1 < B < 0` and `(d - sqrt(2(2-d)))/2 ≤ B` for some constant `ζ`.\n\n---\n\n### Data / Model Specification\n\n**Table 1: In-Sample FIGAS Parameter Estimates**\n| Parameter | `k=5` assets | `k=15` assets |\n| :--- | :--- | :--- |\n| `d` | 0.663 | 0.653 |\n| `B` | -0.086 | 0.157 |\n\n**Table 2: Average Out-of-Sample QLIK Loss (`k=15` assets)**\n| Period | Model | 1-day ahead | 22-days ahead |\n| :--- | :--- | :--- | :--- |\n| **Crisis** | FIGAS | 27.65 | 32.71 |\n| (2007-2009) | GAS | 27.70 | 32.91 |\n| | *DM t-stat* | *(-1.1)* | *(-0.5)* |\n| **Non-Crisis**| FIGAS | 12.76 | 13.93 |\n| (calm) | GAS | 12.85 | 14.41 |\n| | *DM t-stat* | *(-6.3)* | *(-3.4)* |\n\n---\n\n### Question\n\nBased on the provided data, select all statements that are supported by the empirical evidence.", "Options": {"A": "During non-crisis periods, the FIGAS model's long-memory component provides a statistically significant forecasting advantage over the short-memory GAS model.", "B": "The positive definiteness conditions for the FIGAS model are violated for the `k=5` specification because the parameter `B` is negative, as shown in Table 1.", "C": "The long-memory component of the FIGAS model is most valuable during crisis periods, as shown by the large negative DM t-statistics in the top panel of Table 2.", "D": "During the crisis period, the performance of the FIGAS and GAS models is statistically indistinguishable, indicating that the robust score dynamics common to both are the primary driver of performance in volatile markets."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\n\nThis item assesses the ability to synthesize the paper's central empirical findings from multiple tables. It uses an **Atomic Decomposition** strategy to break down the complex synthesis question from the original QA into distinct, verifiable propositions.\n\n- **Correct Options (A, B):** These two options capture the core nuance of the paper's findings regarding state-dependent performance, directly supported by the QLIK values and DM t-statistics in Table 2.\n- **Distractor C (Conceptual Opposite):** This option presents the exact opposite conclusion from the data, testing for careful reading of the table which shows insignificant DM stats during the crisis.\n- **Distractor D (Formula Misuse):** This option tests the validation conditions. It presents a plausible but incorrect conclusion by ignoring 'Case 2' of the positive definiteness rule, which applies when B is negative. This maps to a specific comprehension slip.", "qid": "162", "question": "### Background\n\n**Research Question.** How can the empirical superiority of a new volatility model be rigorously established? This involves assessing its performance against strong benchmarks using both statistical loss functions and practical, economic criteria across different market regimes.\n\n**Setting.** A comparative evaluation of the proposed Fractionally Integrated Generalized Autoregressive Score (FIGAS) model against the short-memory GAS model. The evaluation uses daily realized kernel data for 15 U.S. equities.\n\n**Key Concepts.**\n- **Statistical Evaluation:** The Quasi-Likelihood (QLIK) loss function is used to measure the statistical fit of a forecast. A lower QLIK value indicates a better fit.\n- **Positive Definiteness:** For a covariance matrix `V_t` to be valid, it must be positive definite. The FIGAS(0,d,1) model guarantees this if its scalar parameters `B` and `d` satisfy one of two conditions. Case 1: `0 < B < 1` and `d - B ≥ 0`. Case 2: `ζ-1 < B < 0` and `(d - sqrt(2(2-d)))/2 ≤ B` for some constant `ζ`.\n\n---\n\n### Data / Model Specification\n\n**Table 1: In-Sample FIGAS Parameter Estimates**\n| Parameter | `k=5` assets | `k=15` assets |\n| :--- | :--- | :--- |\n| `d` | 0.663 | 0.653 |\n| `B` | -0.086 | 0.157 |\n\n**Table 2: Average Out-of-Sample QLIK Loss (`k=15` assets)**\n| Period | Model | 1-day ahead | 22-days ahead |\n| :--- | :--- | :--- | :--- |\n| **Crisis** | FIGAS | 27.65 | 32.71 |\n| (2007-2009) | GAS | 27.70 | 32.91 |\n| | *DM t-stat* | *(-1.1)* | *(-0.5)* |\n| **Non-Crisis**| FIGAS | 12.76 | 13.93 |\n| (calm) | GAS | 12.85 | 14.41 |\n| | *DM t-stat* | *(-6.3)* | *(-3.4)* |\n\n---\n\n### Question\n\nBased on the provided data, select all statements that are supported by the empirical evidence.", "model_evaluation_result": {"model_raw_response": "Looking at the data and analyzing each statement:\n\nA) During non-crisis periods, the FIGAS model shows lower QLIK values than GAS (12.76 vs 12.85 for 1-day, 13.93 vs 14.41 for 22-days), and the DM t-statistics are -6.3 and -3.4, which are large negative values indicating statistical significance. This statement is supported.\n\nB) For k=5, d=0.663 and B=-0.086. Since B is negative, we need to check Case 2 conditions. With d=0.663, we can calculate (d - sqrt(2(2-d)))/2 = (0.663 - sqrt(2(2-0.663)))/2 = (0.663 - 1.637)/2 = -0.487. The condition requires -0.487 ≤ -0.086, which is satisfied. The statement claims the conditions are violated, but they are actually satisfied. This statement is not supported.\n\nC) The DM t-statistics during crisis periods are -1.1 and -0.5, which are small and likely not statistically significant. The statement claims these are \"large negative\" values, which is incorrect. This statement is not supported.\n\nD) During crisis periods, the QLIK values are very close (27.65 vs 27.70 for 1-day, 32.71 vs 32.91 for 22-days), and the DM t-statistics of -1.1 and -0.5 indicate no statistical significance. This suggests the models perform similarly during crisis periods. This statement is supported.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 41, "Question": "### Background\n\nThe evolution of the optimal early exercise boundary `c(τ)` for an American put option in the Black-Scholes framework is governed by a specific differential equation. This equation links the speed of the boundary's movement to the option's sensitivities and the economic trade-offs of early exercise.\n\n### Data / Model Specification\n\nThe differential equation governing the evolution of the early exercise boundary `c(τ)` is:\n  \n\\frac{\\partial c(\\tau)}{\\partial\\tau} = -\\frac{\\partial^{2}p(c(\\tau),\\tau)}{\\partial x\\partial\\tau} \\frac{\\sigma^{2}c^{2}(\\tau)}{2q r - 2(r-b)c(\\tau)} \\quad \\text{(Eq. 1)}\n \nwhere `p` is the option price, `x` is asset price, `τ` is time-to-expiry, `q` is strike, `σ` is volatility, `r` is the risk-free rate, and `b` is the cost of carry.\n\n---\n\nBased on the structure of this boundary evolution equation, which of the following statements are valid interpretations or implications? Select all that apply.", "Options": {"A": "For a stock paying a high dividend such that `b` approaches `r` (cost of carry approaches zero), the denominator approaches zero, causing the boundary speed to become infinite.", "B": "For an American put on a futures contract (where `b=0`), the denominator simplifies to `2r(q-c(τ))`, implying that early exercise can still be optimal and the boundary evolves at a finite speed.", "C": "The speed of the boundary, `∂c/∂τ`, is inversely proportional to the net benefit of waiting to exercise, which is captured by the term `rq - (r-b)c(τ)`.", "D": "If the option's Delta were insensitive to the passage of time at the boundary (i.e., `∂²p/∂x∂τ = 0`), the boundary would not move (`∂c/∂τ = 0`)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: Test the user's ability to interpret the financial meaning of the terms in the boundary evolution equation and analyze its behavior under different parameterizations (e.g., for different underlying assets like futures).\nDepth Strategy: Scenario Application. The user must apply the general formula to specific cases (futures contract, high dividend stock) and interpret the components.\nDistractor Logic:\n- D (Formula Misuse / Conceptual Error): This distractor confuses the case of `b=r` (non-dividend paying stock) with `b=0` (futures). When `b=r`, the denominator becomes `2(rq - (r-r)c) = 2rq`, which is positive and finite. The user might incorrectly think `r-b` approaching zero is the key, but the full term is `rq - (r-b)c`. This tests careful application of the formula.", "qid": "41", "question": "### Background\n\nThe evolution of the optimal early exercise boundary `c(τ)` for an American put option in the Black-Scholes framework is governed by a specific differential equation. This equation links the speed of the boundary's movement to the option's sensitivities and the economic trade-offs of early exercise.\n\n### Data / Model Specification\n\nThe differential equation governing the evolution of the early exercise boundary `c(τ)` is:\n  \n\\frac{\\partial c(\\tau)}{\\partial\\tau} = -\\frac{\\partial^{2}p(c(\\tau),\\tau)}{\\partial x\\partial\\tau} \\frac{\\sigma^{2}c^{2}(\\tau)}{2q r - 2(r-b)c(\\tau)} \\quad \\text{(Eq. 1)}\n \nwhere `p` is the option price, `x` is asset price, `τ` is time-to-expiry, `q` is strike, `σ` is volatility, `r` is the risk-free rate, and `b` is the cost of carry.\n\n---\n\nBased on the structure of this boundary evolution equation, which of the following statements are valid interpretations or implications? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the differential equation for the early exercise boundary evolution:\n\n$$\\frac{\\partial c(\\tau)}{\\partial\\tau} = -\\frac{\\partial^{2}p(c(\\tau),\\tau)}{\\partial x\\partial\\tau} \\frac{\\sigma^{2}c^{2}(\\tau)}{2qr - 2(r-b)c(\\tau)}$$\n\nLet me analyze each option:\n\n**Option A**: When $b \\to r$, the denominator becomes $2qr - 2(r-b)c(\\tau) = 2qr - 2(0)c(\\tau) = 2qr$, which is not zero (assuming $q, r > 0$). The denominator approaches zero only if $b \\to r$ AND $c(\\tau) \\to q$, which is not stated in the option. **FALSE**\n\n**Option B**: For futures where $b = 0$, the denominator becomes:\n$2qr - 2(r-0)c(\\tau) = 2qr - 2rc(\\tau) = 2r(q - c(\\tau))$\n\nThis is indeed the correct simplification. Since $c(\\tau) < q$ for a put option (the boundary is below the strike), the denominator is positive and finite, allowing for finite boundary speed and optimal early exercise. **TRUE**\n\n**Option C**: The denominator can be rewritten as:\n$2qr - 2(r-b)c(\\tau) = 2qr - 2rc(\\tau) + 2bc(\\tau) = 2[rq - (r-b)c(\\tau)]$\n\nThe term $rq - (r-b)c(\\tau)$ represents the net benefit of waiting (interest on strike minus net cost of carry on the asset). The boundary speed is indeed inversely proportional to this term. **TRUE**\n\n**Option D**: If $\\frac{\\partial^2 p}{\\partial x \\partial \\tau} = 0$ at the boundary, then from the equation:\n$\\frac{\\partial c(\\tau)}{\\partial\\tau} = 0$\n\nThis means the boundary would not move, which is the correct interpretation. **TRUE**\n\nB,", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 24, "Question": "### Background\n\n**Research Question.** How can the key parameters of the Generalized Pareto Distribution (GPD), which describes tail behavior, be estimated simply and non-parametrically from financial data?\n\n**Setting / Data-Generating Environment.** The paper applies Pickands' method, a non-parametric procedure, to estimate the GPD shape (`k`) and scale (`σ`) parameters. The method relies on specific order statistics from a sample of excesses. For the same dataset, a Maximum Likelihood Estimation (MLE) can also be performed.\n\n**Variables & Parameters.**\n- `k̂_p`: The Pickands estimate for the GPD shape parameter.\n- `k̂_MLE`: The Maximum Likelihood estimate for the GPD shape parameter.\n\n---\n\n### Data / Model Specification\n\nFor Sample 1 (1988-1993), the paper reports two different estimates for the GPD shape parameter `k`:\n- Pickands estimate: `k̂_p = 0.4309`\n- Maximum Likelihood estimate: `k̂_MLE = 0.1158`\n\n---\n\nBased on the statistical properties of these estimators, which of the following are valid reasons that could explain the substantial discrepancy between `k̂_p` and `k̂_MLE` for the same dataset? (Select all that apply)", "Options": {"A": "The Pickands estimator is known to be biased in small samples, whereas the Maximum Likelihood estimator is always unbiased regardless of sample size.", "B": "The MLE procedure requires assuming the underlying returns follow a normal distribution, an assumption the non-parametric Pickands estimator avoids.", "C": "The Pickands estimator is known to be inefficient because it uses only three specific order statistics from the tail, whereas MLE uses all available tail observations.", "D": "The value of the Pickands estimate is highly sensitive to the choice of `h` (the number of order statistics used), a parameter which may not have been optimally selected."}, "Answer": ["C", "D"], "pi_justification": "This item assesses conceptual understanding of the properties of different statistical estimators. It uses a Reverse-Reasoning strategy, asking for causes of an observed outcome. The correct options (A, B) highlight the key weaknesses of the Pickands estimator (inefficiency, sensitivity). Distractors are based on common misconceptions: C) an overstatement of MLE's properties (it is only asymptotically unbiased), and D) a misstatement of the distributional assumptions of the MLE method used in this context.", "qid": "24", "question": "### Background\n\n**Research Question.** How can the key parameters of the Generalized Pareto Distribution (GPD), which describes tail behavior, be estimated simply and non-parametrically from financial data?\n\n**Setting / Data-Generating Environment.** The paper applies Pickands' method, a non-parametric procedure, to estimate the GPD shape (`k`) and scale (`σ`) parameters. The method relies on specific order statistics from a sample of excesses. For the same dataset, a Maximum Likelihood Estimation (MLE) can also be performed.\n\n**Variables & Parameters.**\n- `k̂_p`: The Pickands estimate for the GPD shape parameter.\n- `k̂_MLE`: The Maximum Likelihood estimate for the GPD shape parameter.\n\n---\n\n### Data / Model Specification\n\nFor Sample 1 (1988-1993), the paper reports two different estimates for the GPD shape parameter `k`:\n- Pickands estimate: `k̂_p = 0.4309`\n- Maximum Likelihood estimate: `k̂_MLE = 0.1158`\n\n---\n\nBased on the statistical properties of these estimators, which of the following are valid reasons that could explain the substantial discrepancy between `k̂_p` and `k̂_MLE` for the same dataset? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at each option carefully:\n\nA) \"The Pickands estimator is known to be biased in small samples, whereas the Maximum Likelihood estimator is always unbiased regardless of sample size.\"\n- This is incorrect. MLE is not always unbiased regardless of sample size. In fact, MLE for the GPD shape parameter is known to have bias in finite samples.\n\nB) \"The MLE procedure requires assuming the underlying returns follow a normal distribution, an assumption the non-parametric Pickands estimator avoids.\"\n- This is incorrect. MLE for GPD parameters does not require assuming the underlying returns follow a normal distribution. The MLE procedure assumes the excesses follow a GPD, not that the original returns are normal.\n\nC) \"The Pickands estimator is known to be inefficient because it uses only three specific order statistics from the tail, whereas MLE uses all available tail observations.\"\n- This is correct. The Pickands estimator uses only a few order statistics (typically three specific quantiles), making it less efficient than MLE which uses all the information in the tail observations. This inefficiency can lead to high variance and different estimates.\n\nD) \"The value of the Pickands estimate is highly sensitive to the choice of `h` (the number of order statistics used), a parameter which may not have been optimally selected.\"\n- This is correct. The Pickands estimator depends on a tuning parameter that determines which order statistics to use. Poor choice of this parameter can lead to substantially different estimates, which could explain the discrepancy with MLE.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 106, "Question": "### Background\n\nA study investigates how board independence (the proportion of outside directors) influences corporate debt and moderates the relationship between tax aggressiveness and debt (the debt-substitution effect). The study uses a fixed-effects panel regression analysis of 6,967 firm-year observations.\n\n### Data / Model Specification\n\nThe study estimates the following fixed-effects interaction model:\n\n  \nBDEBT_{it} = \\alpha_i + \\lambda_t + \\beta_1 BTG1_{it} + \\beta_2 OUTDIR_{it} + \\beta_3 (OUTDIR_{it} \\times BTG1_{it}) + \\text{Controls}_{it} + \\varepsilon_{it} \n \n\nwhere `BDEBT` is the book debt-to-assets ratio, `BTG1` is a measure of tax aggressiveness, and `OUTDIR` is the proportion of non-employee directors on the board.\n\n**Table 1: Regression Results (Model FEM1 for `BDEBT`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :---: | :---: |\n| `BTG1` (for `\\beta_1`) | -0.458 | (-1.94) |\n| `OUTDIR` (for `\\beta_2`) | -0.497 | (-2.08) |\n| `OUTDIR*BTG1` (for `\\beta_3`) | -0.784 | (-3.48) |\n\n---\n\nBased on the model and results, select all of the following statements that are correct interpretations or valid conclusions.", "Options": {"A": "The results imply that as board independence increases, a firm's debt policy becomes *less* sensitive to its tax aggressiveness.", "B": "The negative and statistically significant coefficient on `OUTDIR` supports a \"substitutive\" relationship, where effective board monitoring replaces the need for debt as a disciplinary mechanism.", "C": "The marginal effect of tax aggressiveness (`BTG1`) on debt (`BDEBT`) is given by the expression: `-0.458 - 0.784 * OUTDIR`.", "D": "For a firm where 70% of the board are outside directors (`OUTDIR` = 0.70), a one-unit increase in `BTG1` is associated with a 1.0068 unit decrease in the `BDEBT` ratio."}, "Answer": ["B", "C", "D"], "pi_justification": "This item assesses the ability to interpret direct and interaction effects from a regression table and perform a multi-step calculation. It uses a Computational Judgment strategy. Option A correctly interprets the direct effect of `OUTDIR` as supporting the substitutive view. Option B correctly derives the marginal effect from the interaction model. Option C correctly applies this marginal effect formula to a specific scenario (`OUTDIR`=0.70), requiring calculation: -0.458 - 0.784 * 0.70 = -1.0068. Option D is a conceptual distractor; the negative interaction term means sensitivity *increases* with `OUTDIR`, making the statement incorrect.", "qid": "106", "question": "### Background\n\nA study investigates how board independence (the proportion of outside directors) influences corporate debt and moderates the relationship between tax aggressiveness and debt (the debt-substitution effect). The study uses a fixed-effects panel regression analysis of 6,967 firm-year observations.\n\n### Data / Model Specification\n\nThe study estimates the following fixed-effects interaction model:\n\n  \nBDEBT_{it} = \\alpha_i + \\lambda_t + \\beta_1 BTG1_{it} + \\beta_2 OUTDIR_{it} + \\beta_3 (OUTDIR_{it} \\times BTG1_{it}) + \\text{Controls}_{it} + \\varepsilon_{it} \n \n\nwhere `BDEBT` is the book debt-to-assets ratio, `BTG1` is a measure of tax aggressiveness, and `OUTDIR` is the proportion of non-employee directors on the board.\n\n**Table 1: Regression Results (Model FEM1 for `BDEBT`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :---: | :---: |\n| `BTG1` (for `\\beta_1`) | -0.458 | (-1.94) |\n| `OUTDIR` (for `\\beta_2`) | -0.497 | (-2.08) |\n| `OUTDIR*BTG1` (for `\\beta_3`) | -0.784 | (-3.48) |\n\n---\n\nBased on the model and results, select all of the following statements that are correct interpretations or valid conclusions.", "model_evaluation_result": {"model_raw_response": "Looking at this fixed-effects interaction model, I need to evaluate each statement based on the regression results.\n\nThe model is:\nBDEBT = α_i + λ_t + β₁BTG1 + β₂OUTDIR + β₃(OUTDIR × BTG1) + Controls + ε\n\nWith coefficients:\n- β₁ = -0.458 (t = -1.94)\n- β₂ = -0.497 (t = -2.08)  \n- β₃ = -0.784 (t = -3.48)\n\n**Statement A**: \"The results imply that as board independence increases, a firm's debt policy becomes *less* sensitive to its tax aggressiveness.\"\n\nThe sensitivity of debt to tax aggressiveness is the marginal effect ∂BDEBT/∂BTG1 = β₁ + β₃×OUTDIR = -0.458 - 0.784×OUTDIR\n\nSince β₃ = -0.784 < 0, as OUTDIR increases, this marginal effect becomes more negative (larger in absolute value). This means debt policy becomes MORE sensitive to tax aggressiveness, not less. **Statement A is incorrect.**\n\n**Statement B**: \"The negative and statistically significant coefficient on `OUTDIR` supports a 'substitutive' relationship, where effective board monitoring replaces the need for debt as a disciplinary mechanism.\"\n\nβ₂ = -0.497 with t = -2.08, which is statistically significant at conventional levels. The negative coefficient means higher board independence is associated with lower debt levels, consistent with the substitution hypothesis where board monitoring substitutes for debt discipline. **Statement B is correct.**\n\n**Statement C**: \"The marginal effect of tax aggressiveness (`BTG1`) on debt (`BDEBT`) is given by the expression: `-0.458 - 0.784 * OUTDIR`.\"\n\nTaking the partial derivative: ∂BDEBT/∂BTG1 = β₁ + β₃×OUTDIR = -0.458 - 0.784×OUTDIR\n\nThis matches the given expression exactly. **Statement C is correct.**\n\n**Statement D**:", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 34, "Question": "### Background\n\nA study evaluates several forecasting models for the ASE 20 index. After an initial comparison, the performance is re-evaluated under realistic institutional trading conditions, including transaction costs and the application of leverage to normalize volatility.\n\n### Data / Model Specification\n\nThe study applies a leverage factor to each strategy to normalize its volatility to a common target. The final performance, including transaction costs of 0.14% per trade and leverage costs of 4% p.a., is reported. The table below excerpts the final results for the Genetic Programming (GP) algorithm and the standard Recurrent Neural Network (RNN) from the paper's Table 8.\n\n**Table 1. Final Trading Performance with Costs and Leverage**\n| Model | Ann. Return (incl. costs) (%) | Positions Taken (ann.) | Leverage Factor | Transaction and Leverage Costs (%) |\n| :--- | :--- | :--- | :--- | :--- |\n| GP algorithm | 33.34 | 67 | 1.10 | 9.95 |\n| RNN | 4.64 | 147 | 1.055 | 19.11 |\n\n---\n\nBased on the data in **Table 1** and the provided cost structure, select all of the following statements that are correct.\n", "Options": {"A": "The leverage cost for the GP algorithm is 0.40% per annum.", "B": "The GP algorithm's superior net return is primarily driven by its significantly higher leverage factor compared to the RNN.", "C": "The transaction costs for the RNN model are more than double the transaction costs for the GP algorithm.", "D": "The RNN model's total costs (19.11%) consume over 80% of its gross (pre-cost) annualized return of 23.75%."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to perform multi-step calculations and draw valid conclusions from a data table, focusing on the practical impact of trading frictions. Strategy: Computational Judgment. Distractor Logic: Option A requires calculating transaction costs for both models (RNN: 147 * 0.14% = 20.58%; GP: 67 * 0.14% = 9.38%) and comparing them. Option B requires calculating the leverage cost for the GP model ((1.10 - 1) * 4% = 0.40%). Option C requires a percentage calculation (19.11 / 23.75 ≈ 80.5%). Option D is a 'Conceptual Opposite' distractor; the leverage factors are very similar, and the performance difference is driven by trading frequency and signal quality, not leverage.", "qid": "34", "question": "### Background\n\nA study evaluates several forecasting models for the ASE 20 index. After an initial comparison, the performance is re-evaluated under realistic institutional trading conditions, including transaction costs and the application of leverage to normalize volatility.\n\n### Data / Model Specification\n\nThe study applies a leverage factor to each strategy to normalize its volatility to a common target. The final performance, including transaction costs of 0.14% per trade and leverage costs of 4% p.a., is reported. The table below excerpts the final results for the Genetic Programming (GP) algorithm and the standard Recurrent Neural Network (RNN) from the paper's Table 8.\n\n**Table 1. Final Trading Performance with Costs and Leverage**\n| Model | Ann. Return (incl. costs) (%) | Positions Taken (ann.) | Leverage Factor | Transaction and Leverage Costs (%) |\n| :--- | :--- | :--- | :--- | :--- |\n| GP algorithm | 33.34 | 67 | 1.10 | 9.95 |\n| RNN | 4.64 | 147 | 1.055 | 19.11 |\n\n---\n\nBased on the data in **Table 1** and the provided cost structure, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "Looking at the data and cost structure, I need to analyze each statement:\n\n**A) The leverage cost for the GP algorithm is 0.40% per annum.**\n- Leverage factor for GP = 1.10\n- Leverage cost = 4% p.a. on borrowed amount\n- Borrowed amount = 1.10 - 1.00 = 0.10 (10% leverage)\n- Leverage cost = 0.10 × 4% = 0.40% p.a.\n✓ This is correct.\n\n**B) The GP algorithm's superior net return is primarily driven by its significantly higher leverage factor compared to the RNN.**\n- GP leverage factor: 1.10\n- RNN leverage factor: 1.055\n- The difference is only 0.045 (1.10 vs 1.055)\n- This is not a \"significantly higher\" leverage factor\n- The superior return is more likely due to better trading strategy/signals\n✗ This is incorrect.\n\n**C) The transaction costs for the RNN model are more than double the transaction costs for the GP algorithm.**\n- Transaction cost per trade = 0.14%\n- GP: 67 trades × 0.14% = 9.38%\n- RNN: 147 trades × 0.14% = 20.58%\n- Ratio: 20.58% / 9.38% = 2.19\n- Since 2.19 > 2, RNN's transaction costs are indeed more than double GP's\n✓ This is correct.\n\n**D) The RNN model's total costs (19.11%) consume over 80% of its gross (pre-cost) annualized return of 23.75%.**\n- Net return = 4.64%\n- Total costs = 19.11%\n- Gross return = 4.64% + 19.11% = 23.75%\n- Cost percentage of gross = 19.11% / 23.75% = 80.46%\n- Since 80.46% > 80%, this is true\n✓ This is correct.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 235, "Question": "### Background\n\n**Research Question.** How does the choice of a specific distribution within the Generalised Hyperbolic (GH) family, even when all choices provide a good statistical fit, quantitatively impact the estimation of extreme tail risk?\n\n**Setting.** A risk manager has fitted two different univariate distributions from the skewed GH family—the Student-t and the Variance Gamma (VG)—to the daily loss data of Goldman Sachs stock. Both models pass standard goodness-of-fit tests. The paper's final empirical results show that the Student-t model yields a `TCE_{0.999}` of 7.104, while the VG model yields a `TCE_{0.999}` of 4.841.\n\n---\n\n### Data / Model Specification\n\nThe Tail Conditional Expectation for a `GH_1` random variable `X` is given by:\n  \nTCE_q(X) = \\mu + \\frac{\\gamma}{1-q}k_{\\lambda}\\bar{F}_{GH_{1}}(x_{q};\\lambda+1) + \\frac{\\sigma^{2}}{1-q}k_{\\lambda}f_{GH_{1}}(x_{q};\\lambda+1) \\quad \\text{(Eq. (1))}\n \nThe Student-t distribution is known for its ability to model 'fat tails' (i.e., a higher probability of extreme events) compared to distributions like the Variance Gamma. This is governed by its degrees of freedom parameter `ν`, where a lower `ν` implies fatter tails.\n\n---\n\n### Question\n\nGiven the significant difference between the final `TCE_{0.999}` estimates (7.104 for Student-t vs. 4.841 for VG), select all of the following statements that represent a valid interpretation or conclusion.", "Options": {"A": "A prudent risk manager should favor the Student-t model's estimate because it provides a more conservative quantification of risk, which is appropriate when modeling extreme, low-probability loss events.", "B": "Since both models pass goodness-of-fit tests, the choice between them is arbitrary and should be based on whichever model is computationally simpler.", "C": "The Student-t model's higher TCE is primarily driven by its fatter tails, which increases the magnitude of the skewness (`γ`) and variance (`σ²`) risk contribution terms in Eq. (1).", "D": "The primary reason for the large difference in TCE estimates is the difference in the unconditional mean (`μ`) between the two models."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the quantitative results of risk modeling and connect them to the underlying conceptual differences between models (tail fatness). It uses a Reverse-Reasoning strategy (inferring model properties from outcomes) and premise packaging. Correct options (A, B) synthesize the quantitative results with the paper's core argument for using fat-tailed distributions. Distractor C is a Conceptual Opposite, misattributing the cause of the difference. Distractor D presents a common but flawed argument (True but Irrelevant premise leading to a wrong conclusion), testing the student's understanding of prudential risk management.", "qid": "235", "question": "### Background\n\n**Research Question.** How does the choice of a specific distribution within the Generalised Hyperbolic (GH) family, even when all choices provide a good statistical fit, quantitatively impact the estimation of extreme tail risk?\n\n**Setting.** A risk manager has fitted two different univariate distributions from the skewed GH family—the Student-t and the Variance Gamma (VG)—to the daily loss data of Goldman Sachs stock. Both models pass standard goodness-of-fit tests. The paper's final empirical results show that the Student-t model yields a `TCE_{0.999}` of 7.104, while the VG model yields a `TCE_{0.999}` of 4.841.\n\n---\n\n### Data / Model Specification\n\nThe Tail Conditional Expectation for a `GH_1` random variable `X` is given by:\n  \nTCE_q(X) = \\mu + \\frac{\\gamma}{1-q}k_{\\lambda}\\bar{F}_{GH_{1}}(x_{q};\\lambda+1) + \\frac{\\sigma^{2}}{1-q}k_{\\lambda}f_{GH_{1}}(x_{q};\\lambda+1) \\quad \\text{(Eq. (1))}\n \nThe Student-t distribution is known for its ability to model 'fat tails' (i.e., a higher probability of extreme events) compared to distributions like the Variance Gamma. This is governed by its degrees of freedom parameter `ν`, where a lower `ν` implies fatter tails.\n\n---\n\n### Question\n\nGiven the significant difference between the final `TCE_{0.999}` estimates (7.104 for Student-t vs. 4.841 for VG), select all of the following statements that represent a valid interpretation or conclusion.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to evaluate each statement based on the given information about TCE estimates and the properties of Student-t vs. Variance Gamma distributions.\n\nGiven facts:\n- Both models pass goodness-of-fit tests\n- Student-t TCE₀.₉₉₉ = 7.104\n- VG TCE₀.₉₉₉ = 4.841\n- Student-t has fatter tails (governed by degrees of freedom parameter ν)\n- TCE formula shows contributions from μ, γ (skewness), and σ² (variance) terms\n\nEvaluating each option:\n\n**Option A**: This is correct. When modeling extreme tail risk (99.9th percentile), a conservative approach is prudent. The Student-t's higher TCE reflects its ability to capture fat-tail behavior, which is crucial for extreme risk management. Using the lower VG estimate could lead to underestimating potential extreme losses.\n\n**Option B**: This is incorrect. Despite both models passing goodness-of-fit tests, they produce significantly different tail risk estimates (47% difference). For extreme risk management, this difference is material and cannot be dismissed as arbitrary. Computational simplicity should not override risk accuracy.\n\n**Option C**: This is incorrect. The statement misunderstands the mechanism. Student-t's fatter tails don't increase the skewness (γ) and variance (σ²) parameters themselves. Rather, the fatter tails affect the tail behavior through the distribution functions f_{GH₁} and F̄_{GH₁} in the TCE formula, which capture the tail shape directly.\n\n**Option D**: This is incorrect. The unconditional mean (μ) represents the central tendency of the distribution and would be similar for both models when fitted to the same data. The large difference in TCE estimates at the 99.9th percentile is driven by differences in tail behavior, not the mean.\n\nA", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 159, "Question": "### Background\n\nA study argues that incumbent banks strategically use branch expansion to deter entry by their most threatening competitors. The analysis uses a two-stage model: Stage 1 models firm entry and profitability, while Stage 2 models incumbent firms' branching decisions after correcting for the endogeneity of market structure.\n\n### Data / Model Specification\n\nKey findings from the analysis are summarized below:\n\n**Table 1: Stage 1 Competitive Effects on an Incumbent Multimarket Bank's Payoff**\n\n| Competitor Type Entering Market | Effect on Payoff | t-statistic |\n| :--- | :---: | :---: |\n| First Multimarket (MM) competitor | -1.111 | (-16.65) |\n| First Single-Market (SM) competitor | -0.304 | (-3.64) |\n\n**Table 2: Stage 2 Adjusted Marginal Effect on Number of Branches**\n\n| Variable | Adjusted Effect |\n| :--- | :---: |\n| Effect of one additional MM Competitor on an incumbent's branches | 0.159 |\n| | (5.85) |\n\n---\n\n### Question\n\nTo construct the paper's central argument that incumbents use branching to deter entry specifically by multimarket (MM) banks, which of the following findings are essential premises? Select all that apply.", "Options": {"A": "The competitive harm to an incumbent MM bank's profitability is substantially greater from another MM bank than from a single-market (SM) bank.", "B": "When MM banks compete directly in a market, they respond by expanding their branch networks, indicating that branching is a key dimension of their competitive strategy.", "C": "Single-market banks tend to operate more in markets with significant agricultural activity.", "D": "After correcting for endogeneity, the presence of MM competitors has no statistically significant effect on incumbent branching."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the student's ability to synthesize findings from different parts of the analysis to build the paper's main causal story. It uses a 'Premise/assumption packaging' strategy. Correct option (A) establishes the motive for deterrence (from Table 1). Correct option (C) establishes the strategic weapon used (from Table 2). Distractor (B) is 'True but Irrelevant,' a finding from the paper that is not part of this specific causal chain. Distractor (D) is a 'Conceptual Opposite' that directly contradicts the key finding in Table 2.", "qid": "159", "question": "### Background\n\nA study argues that incumbent banks strategically use branch expansion to deter entry by their most threatening competitors. The analysis uses a two-stage model: Stage 1 models firm entry and profitability, while Stage 2 models incumbent firms' branching decisions after correcting for the endogeneity of market structure.\n\n### Data / Model Specification\n\nKey findings from the analysis are summarized below:\n\n**Table 1: Stage 1 Competitive Effects on an Incumbent Multimarket Bank's Payoff**\n\n| Competitor Type Entering Market | Effect on Payoff | t-statistic |\n| :--- | :---: | :---: |\n| First Multimarket (MM) competitor | -1.111 | (-16.65) |\n| First Single-Market (SM) competitor | -0.304 | (-3.64) |\n\n**Table 2: Stage 2 Adjusted Marginal Effect on Number of Branches**\n\n| Variable | Adjusted Effect |\n| :--- | :---: |\n| Effect of one additional MM Competitor on an incumbent's branches | 0.159 |\n| | (5.85) |\n\n---\n\n### Question\n\nTo construct the paper's central argument that incumbents use branching to deter entry specifically by multimarket (MM) banks, which of the following findings are essential premises? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which findings are essential premises for the paper's central argument that incumbents use branching to deter entry specifically by multimarket (MM) banks.\n\nLet me analyze each option:\n\n**Option A**: \"The competitive harm to an incumbent MM bank's profitability is substantially greater from another MM bank than from a single-market (SM) bank.\"\n- Table 1 shows MM competitor entry has effect of -1.111 (t=-16.65) vs SM competitor entry effect of -0.304 (t=-3.64)\n- This demonstrates MM competitors pose a much greater threat (about 3.7x larger negative effect)\n- This is ESSENTIAL - it establishes why incumbents would want to deter MM entry specifically\n\n**Option B**: \"When MM banks compete directly in a market, they respond by expanding their branch networks, indicating that branching is a key dimension of their competitive strategy.\"\n- Table 2 shows positive effect (0.159, t=5.85) of MM competitors on incumbent branching\n- This is ESSENTIAL - it shows incumbents actually do expand branches in response to MM competition, supporting the deterrence mechanism\n\n**Option C**: \"Single-market banks tend to operate more in markets with significant agricultural activity.\"\n- This is not shown in the data provided\n- Even if true, this would be tangential to the core deterrence argument\n- NOT essential\n\n**Option D**: \"After correcting for endogeneity, the presence of MM competitors has no statistically significant effect on incumbent branching.\"\n- Table 2 shows the opposite - there IS a statistically significant positive effect (t=5.85)\n- This would contradict the paper's argument if true\n- NOT a supporting premise\n\nThe essential premises are A and B.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 44, "Question": "### Background\n\n**Research Question.** The paper's central finding is that significant contagion effects on rivals only occur when a firm with low-rated (non-investment grade) debt is downgraded. The proposed explanation is the \"market visibility\" hypothesis: low-rated firms are monitored less intensely, so their downgrades are more surprising and informationally potent.\n\n**Setting / Data-Generating Environment.** To test this mechanism, the study compares the mean number of analysts following downgraded firms across three credit quality groups.\n\n**Variables & Parameters.**\n- **Analyst Coverage:** The number of earnings analysts per downgraded firm as reported by IBES.\n- **Group 1:** Downgrade within investment grade.\n- **Group 2:** Downgrade from investment to non-investment grade.\n- **Group 3:** Downgrade within non-investment grade.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents summary statistics for analyst coverage across the three groups of downgraded firms.\n\n**Table 1. Distribution of Earnings Analysts per Downgraded Firm**\n\n| Group | Mean | Standard Deviation | Median |\n| :--- | :--- | :--- | :--- |\n| Group 1 | 18.83 | 8.71 | 19 |\n| Group 2 | 14.73 | 6.85 | 14.5 |\n| Group 3 | 9.74 | 6.56 | 9 |\n\n*T-statistics for the difference in means are all highly significant (p < 0.01) for all pairwise comparisons (1 vs 2, 1 vs 3, 2 vs 3).*\n\n---\n\n### Question\n\nThe paper's 'market visibility' hypothesis posits that `Low Credit Rating` → `Low Analyst Coverage` → `High Surprise` → `Strong Contagion`. Based on the data in Table 1, select all statements that are consistent with this hypothesis.", "Options": {"A": "The data imply that high analyst coverage for Group 1 firms is the primary cause of their investment-grade credit rating.", "B": "The data show a monotonic relationship where firms with lower credit quality (from Group 1 to Group 3) are followed by progressively fewer analysts.", "C": "The median number of analysts for Group 1 firms (19) is not statistically different from the median for Group 3 firms (9).", "D": "The significant difference in analyst coverage between groups provides a plausible, empirically-grounded mechanism for why contagion effects are concentrated in Group 3."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the understanding of the paper's central causal mechanism ('market visibility'). It uses a 'premise packaging' strategy, asking the user to select the empirical premises that support the main hypothesis. Option A tests direct data interpretation, while Option B tests the ability to link that data to the broader argument. Distractors target factual misreading (Option C) and a sophisticated logical error of reverse causality (Option D).", "qid": "44", "question": "### Background\n\n**Research Question.** The paper's central finding is that significant contagion effects on rivals only occur when a firm with low-rated (non-investment grade) debt is downgraded. The proposed explanation is the \"market visibility\" hypothesis: low-rated firms are monitored less intensely, so their downgrades are more surprising and informationally potent.\n\n**Setting / Data-Generating Environment.** To test this mechanism, the study compares the mean number of analysts following downgraded firms across three credit quality groups.\n\n**Variables & Parameters.**\n- **Analyst Coverage:** The number of earnings analysts per downgraded firm as reported by IBES.\n- **Group 1:** Downgrade within investment grade.\n- **Group 2:** Downgrade from investment to non-investment grade.\n- **Group 3:** Downgrade within non-investment grade.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents summary statistics for analyst coverage across the three groups of downgraded firms.\n\n**Table 1. Distribution of Earnings Analysts per Downgraded Firm**\n\n| Group | Mean | Standard Deviation | Median |\n| :--- | :--- | :--- | :--- |\n| Group 1 | 18.83 | 8.71 | 19 |\n| Group 2 | 14.73 | 6.85 | 14.5 |\n| Group 3 | 9.74 | 6.56 | 9 |\n\n*T-statistics for the difference in means are all highly significant (p < 0.01) for all pairwise comparisons (1 vs 2, 1 vs 3, 2 vs 3).*\n\n---\n\n### Question\n\nThe paper's 'market visibility' hypothesis posits that `Low Credit Rating` → `Low Analyst Coverage` → `High Surprise` → `Strong Contagion`. Based on the data in Table 1, select all statements that are consistent with this hypothesis.", "model_evaluation_result": {"model_raw_response": "Looking at the market visibility hypothesis and the data in Table 1, I need to evaluate which statements are consistent with the proposed mechanism: Low Credit Rating → Low Analyst Coverage → High Surprise → Strong Contagion.\n\nLet me analyze each option:\n\nA) This statement reverses the causality. The hypothesis suggests low credit ratings lead to low analyst coverage, not that high analyst coverage causes investment-grade ratings. This is inconsistent with the hypothesis.\n\nB) The data clearly shows:\n- Group 1 (investment grade): Mean = 18.83, Median = 19\n- Group 2 (investment to non-investment): Mean = 14.73, Median = 14.5  \n- Group 3 (non-investment grade): Mean = 9.74, Median = 9\n\nThis demonstrates a monotonic decrease in analyst coverage as credit quality decreases, which is exactly what the hypothesis predicts.\n\nC) The text explicitly states that \"T-statistics for the difference in means are all highly significant (p < 0.01) for all pairwise comparisons.\" While this refers to means, the large difference between medians (19 vs 9) combined with the highly significant mean differences strongly suggests the medians are also statistically different. This statement is incorrect.\n\nD) The data shows Group 3 (where contagion effects occur) has significantly lower analyst coverage than Groups 1 and 2. This lower coverage would make downgrades more surprising and informationally potent, providing empirical support for why contagion effects are concentrated in Group 3. This is consistent with the hypothesis.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 128, "Question": "### Background\n\n**Research Question.** Why might a retired CEO remain on their own company's board, and how have the criteria for this decision evolved over time? This question explores the tension between two competing corporate governance theories: (1) the \"reward for excellence\" view, where firms retain high-performing leaders, and (2) the \"managerial entrenchment\" view, where long-tenured CEOs secure a board seat irrespective of performance.\n\n**Setting and Sample.** The analysis compares retired CEOs who serve on their own board two years after retirement (`Inside+2years = 1`) with those who do not (`Inside+2years = 0`) across pre- and post-SOX periods.\n\n**Variables and Parameters.**\n- `ASR`: Abnormal stock return (%), a proxy for recent performance.\n- `Tenure`: The CEO's tenure in years, a proxy for experience and potential entrenchment.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Mean Characteristics by Inside Directorship Status**\n\n| | Do Not Stay (Inside+2yrs=0) | Stay (Inside+2yrs=1) |\n|:---|---:|---:|\n| **1995-1999 (Pre-SOX)** | |\n| ASR (%) | -1.46 | 5.05 |\n| Tenure | 9.41 | 12.71 |\n| **2001-2005 (Post-SOX)** | |\n| ASR (%) | 8.59 | 15.81 |\n| Tenure | 8.28 | 14.12 |\n\n**Table 2: Logit Model for Probability of Serving on Own Board**\n\n| Variable | 1995-1999 | 2001-2005 |\n|:---|---:|---:|\n| ASR | 2.756** | 1.626* |\n| | (0.026) | (0.070) |\n| Tenure | 0.051** | 0.140*** |\n| | (0.039) | (0.000) |\n\n*P-values in parentheses. ***, **, * represent 1%, 5%, and 10% significance levels. A positive coefficient increases the probability of staying on the board.*\n\n---\n\n### Question\n\nThe paper's analysis of CEOs serving on their own boards post-retirement reveals a significant shift after the Sarbanes-Oxley Act (SOX). According to the provided data and the paper's interpretation, which of the following statements correctly characterize this post-SOX change? Select all that apply.", "Options": {"A": "Post-SOX, firms began to penalize high ASR, retaining CEOs with lower stock performance to signal a move away from a \"shareholder value\" focus.", "B": "The increased importance of tenure post-SOX provides conclusive evidence for the managerial entrenchment hypothesis, as longer-tenured CEOs were better able to secure board seats regardless of performance.", "C": "The coefficient on CEO tenure became larger and more statistically significant, suggesting that in the high-scrutiny post-SOX environment, boards placed a higher value on the deep, firm-specific institutional knowledge possessed by long-serving leaders.", "D": "The statistical significance of abnormal stock returns (ASR) as a predictor weakened, consistent with boards becoming more cautious about rewarding recent stock performance, which could have been driven by excessive risk-taking or accounting manipulation."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the understanding of a structural shift in corporate governance priorities post-SOX regarding inside directorships. It uses an **Atomic Decomposition** strategy to present the two key facets of the paper's explanation: the rising importance of firm-specific knowledge (Tenure) and the declining weight on recent stock performance (ASR). The depth strategy is **Reverse-Reasoning**. Distractors are designed to identify over-interpretation of the evidence (C, claiming conclusive proof of entrenchment) and a conceptual reversal of the findings (D, incorrectly suggesting high ASR was penalized, which contradicts both Table 1 and Table 2).", "qid": "128", "question": "### Background\n\n**Research Question.** Why might a retired CEO remain on their own company's board, and how have the criteria for this decision evolved over time? This question explores the tension between two competing corporate governance theories: (1) the \"reward for excellence\" view, where firms retain high-performing leaders, and (2) the \"managerial entrenchment\" view, where long-tenured CEOs secure a board seat irrespective of performance.\n\n**Setting and Sample.** The analysis compares retired CEOs who serve on their own board two years after retirement (`Inside+2years = 1`) with those who do not (`Inside+2years = 0`) across pre- and post-SOX periods.\n\n**Variables and Parameters.**\n- `ASR`: Abnormal stock return (%), a proxy for recent performance.\n- `Tenure`: The CEO's tenure in years, a proxy for experience and potential entrenchment.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Mean Characteristics by Inside Directorship Status**\n\n| | Do Not Stay (Inside+2yrs=0) | Stay (Inside+2yrs=1) |\n|:---|---:|---:|\n| **1995-1999 (Pre-SOX)** | |\n| ASR (%) | -1.46 | 5.05 |\n| Tenure | 9.41 | 12.71 |\n| **2001-2005 (Post-SOX)** | |\n| ASR (%) | 8.59 | 15.81 |\n| Tenure | 8.28 | 14.12 |\n\n**Table 2: Logit Model for Probability of Serving on Own Board**\n\n| Variable | 1995-1999 | 2001-2005 |\n|:---|---:|---:|\n| ASR | 2.756** | 1.626* |\n| | (0.026) | (0.070) |\n| Tenure | 0.051** | 0.140*** |\n| | (0.039) | (0.000) |\n\n*P-values in parentheses. ***, **, * represent 1%, 5%, and 10% significance levels. A positive coefficient increases the probability of staying on the board.*\n\n---\n\n### Question\n\nThe paper's analysis of CEOs serving on their own boards post-retirement reveals a significant shift after the Sarbanes-Oxley Act (SOX). According to the provided data and the paper's interpretation, which of the following statements correctly characterize this post-SOX change? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data and model results:\n\n1. **ASR coefficient changes**: Pre-SOX: 2.756** (p=0.026) → Post-SOX: 1.626* (p=0.070)\n   - Coefficient remains positive but decreases in magnitude\n   - Statistical significance weakens from 5% to 10% level\n\n2. **Tenure coefficient changes**: Pre-SOX: 0.051** (p=0.039) → Post-SOX: 0.140*** (p=0.000)\n   - Coefficient nearly triples in magnitude\n   - Statistical significance strengthens from 5% to 1% level\n\nAnalyzing each option:\n\n**A) Incorrect** - The ASR coefficient remains positive post-SOX (1.626*), meaning higher ASR still increases the probability of staying on the board. There's no evidence of penalizing high ASR.\n\n**B) Incorrect** - While tenure's importance increased, this alone doesn't provide \"conclusive evidence\" for managerial entrenchment. The paper presents this as one of two competing theories, and the increased tenure effect could also reflect valuing experience in a more complex regulatory environment.\n\n**C) Correct** - The tenure coefficient increased from 0.051** to 0.140*** with stronger significance (p-value from 0.039 to 0.000), consistent with boards valuing firm-specific knowledge more highly post-SOX.\n\n**D) Correct** - The ASR coefficient's statistical significance weakened from 5% (p=0.026) to 10% (p=0.070), and its magnitude decreased, suggesting boards became more cautious about using recent stock performance as a criterion.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 201, "Question": "### Background\n\nAn insurance company's management aims to translate a single, clear operational metric—the combined ratio—into superior shareholder value. Progressive Insurance has established a core corporate strategy of maximizing growth subject to maintaining specific targets for its underwriting margin, leverage, and investment policies. The company's central mantra, communicated to all employees, is \"96 and grow.\"\n\n---\n\n### Data / Model Specification\n\nProgressive's corporate strategy is defined by four explicit financial policies, summarized in Table 1.\n\n**Table 1: Progressive's Stated Financial Policies**\n\n| Policy Metric | Target Value |\n| :--- | :--- |\n| Combined Ratio (`CR`) | `≤ 96%` |\n| Sales-to-Capital (`P/S`) | `3-to-1` |\n| Debt-to-Capital (`D/S`) | `25%` |\n\nSuppose a new CFO proposes a more aggressive capital structure, increasing the debt-to-capital ratio (`D/S`) from 25% to 40%, while keeping all other policy targets the same.\n\n---\n\n### Question\n\nBased on the provided model and strategic context, select all of the following statements that represent valid strategic conclusions.", "Options": {"A": "Focusing line managers on the '96 combined ratio' is effective because it is a clear, controllable target that aligns them with the firm's core underwriting competency.", "B": "The CFO's proposal to increase the debt-to-capital ratio to 40% would mechanically increase the firm's target pre-tax ROE, assuming all other policies and returns remain constant.", "C": "The CFO's proposal would decrease the firm's reliance on investment income to achieve its target ROE.", "D": "The primary benefit of the '96 and grow' mantra is that it fully captures all drivers of shareholder value, including investment performance and capital structure."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect quantitative analysis with strategic rationale, using a Scenario Application strategy. Correct options (A, B) test the understanding of the core communication strategy and the mechanical effect of leverage. Distractors C and D are 'Conceptual Opposites' that test for a deeper understanding of the strategic trade-offs and the deliberate simplification embodied in the company's mantra.", "qid": "201", "question": "### Background\n\nAn insurance company's management aims to translate a single, clear operational metric—the combined ratio—into superior shareholder value. Progressive Insurance has established a core corporate strategy of maximizing growth subject to maintaining specific targets for its underwriting margin, leverage, and investment policies. The company's central mantra, communicated to all employees, is \"96 and grow.\"\n\n---\n\n### Data / Model Specification\n\nProgressive's corporate strategy is defined by four explicit financial policies, summarized in Table 1.\n\n**Table 1: Progressive's Stated Financial Policies**\n\n| Policy Metric | Target Value |\n| :--- | :--- |\n| Combined Ratio (`CR`) | `≤ 96%` |\n| Sales-to-Capital (`P/S`) | `3-to-1` |\n| Debt-to-Capital (`D/S`) | `25%` |\n\nSuppose a new CFO proposes a more aggressive capital structure, increasing the debt-to-capital ratio (`D/S`) from 25% to 40%, while keeping all other policy targets the same.\n\n---\n\n### Question\n\nBased on the provided model and strategic context, select all of the following statements that represent valid strategic conclusions.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) This is correct. The combined ratio is a clear operational metric that line managers can directly influence through underwriting decisions and expense management. It aligns with Progressive's core competency in underwriting.\n\nB) This is correct. With higher leverage (D/S increasing from 25% to 40%), the equity base decreases while maintaining the same asset base. Since ROE = Net Income / Equity, a smaller equity base mechanically increases ROE if net income remains constant.\n\nC) This is incorrect. Higher leverage doesn't change the firm's reliance on investment income. The firm still needs the same absolute amount of investment income to achieve its profit targets; it just spreads this over a smaller equity base.\n\nD) This is incorrect. The '96 and grow' mantra focuses only on underwriting performance (combined ratio) and growth. It doesn't capture investment performance or capital structure decisions, which are also important drivers of shareholder value.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 230, "Question": "### Background\n\nThe analysis in the paper compares the cost-effectiveness of different Executive Stock Option (ESO) designs. A key finding is that for executives who are highly risk-averse and whose portfolios are heavily concentrated in company stock, shortening the vesting period is a particularly efficient way to increase their perceived (subjective) value of the option grant.\n\n### Data / Model Specification\n\n**Table 1** shows the 'unit cost of vesting period delta' for a hurdle rate ESO under different scenarios. This metric represents the firm's objective cost for each dollar of subjective value delivered to the executive by shortening the vesting period by one year. A lower unit cost indicates higher cost-effectiveness.\n\n**Table 1: Unit cost of vesting period delta ($)**\n| Risk Aversion (`α`) | Excess Holdings (`θ`) | Unit Cost of Vesting Period Delta ($) |\n| :--- | :--- | :--- |\n| 4 | 75% | 20.55 |\n| 5 | 25% | 96.68 |\n| 5 | 50% | 36.07 |\n| 5 | 75% | 15.23 |\n| 7 | 50% | 23.87 |\n| 7 | 75% | 10.06 |\n\n*Base case parameters: S=K=$100, T=10 years, r=5%, σ=30%, h=10%, q=2%, ν=20%. Deltas are based on decreasing the vesting period from 2 years to 1 year.*\n\n---\n\nBased on the data in **Table 1** and the paper's findings, select all of the following statements that are valid conclusions.\n", "Options": {"A": "For an executive with a fixed level of risk aversion (e.g., α=5), shortening the vesting period becomes a more cost-effective way to deliver value as their portfolio becomes more concentrated in company stock.", "B": "For an executive with a fixed, high level of portfolio concentration (e.g., θ=75%), shortening the vesting period becomes a more cost-effective way to deliver value as they become more risk-averse.", "C": "The data suggest that the subjective value an executive places on the flexibility to exercise early increases at an accelerating rate as their risk aversion and portfolio constraints tighten.", "D": "Shortening the vesting period is most cost-effective for executives with low risk aversion and a diversified portfolio (e.g., α=5, θ=25%)."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret trends from tabular data and connect them to the underlying economic intuition presented in the paper. It uses a Scenario Application strategy. Options A and B require direct trend analysis from the table (across a row and down a column, respectively). Option C requires synthesizing these trends into a higher-level economic conclusion about the non-linear utility cost of constraints. Option D is a conceptual opposite, directly contradicted by the data showing the highest unit cost for this group.", "qid": "230", "question": "### Background\n\nThe analysis in the paper compares the cost-effectiveness of different Executive Stock Option (ESO) designs. A key finding is that for executives who are highly risk-averse and whose portfolios are heavily concentrated in company stock, shortening the vesting period is a particularly efficient way to increase their perceived (subjective) value of the option grant.\n\n### Data / Model Specification\n\n**Table 1** shows the 'unit cost of vesting period delta' for a hurdle rate ESO under different scenarios. This metric represents the firm's objective cost for each dollar of subjective value delivered to the executive by shortening the vesting period by one year. A lower unit cost indicates higher cost-effectiveness.\n\n**Table 1: Unit cost of vesting period delta ($)**\n| Risk Aversion (`α`) | Excess Holdings (`θ`) | Unit Cost of Vesting Period Delta ($) |\n| :--- | :--- | :--- |\n| 4 | 75% | 20.55 |\n| 5 | 25% | 96.68 |\n| 5 | 50% | 36.07 |\n| 5 | 75% | 15.23 |\n| 7 | 50% | 23.87 |\n| 7 | 75% | 10.06 |\n\n*Base case parameters: S=K=$100, T=10 years, r=5%, σ=30%, h=10%, q=2%, ν=20%. Deltas are based on decreasing the vesting period from 2 years to 1 year.*\n\n---\n\nBased on the data in **Table 1** and the paper's findings, select all of the following statements that are valid conclusions.\n", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to analyze the unit cost of vesting period delta across different scenarios.\n\nFor option A: When risk aversion α=5 is fixed, as excess holdings increase from 25% to 50% to 75%, the unit cost decreases from $96.68 to $36.07 to $15.23. Lower unit cost means higher cost-effectiveness, so this statement is correct.\n\nFor option B: When excess holdings θ=75% is fixed, as risk aversion increases from α=4 to α=5 to α=7, the unit cost decreases from $20.55 to $15.23 to $10.06. Again, lower unit cost means higher cost-effectiveness, so this statement is correct.\n\nFor option C: The data shows that as both risk aversion and portfolio concentration increase, the unit cost decreases dramatically (from $96.68 at α=5, θ=25% to $10.06 at α=7, θ=75%). This accelerating decrease in unit cost implies that the subjective value executives place on early exercise flexibility increases at an accelerating rate as their constraints tighten. This statement is correct.\n\nFor option D: The data shows that α=5, θ=25% has the highest unit cost ($96.68), making it the least cost-effective scenario in the table, not the most cost-effective. This statement is incorrect.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 192, "Question": "### Background\n\n**Research Question.** This case examines the relationship between the average age of a firm's supervisory board and its financial performance, testing the hypothesis that older boards are associated with lower firm value.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish listed firms, with data averaged over the 1998–2001 period. The study is motivated by recommendations in Denmark's “Nørby report” on corporate governance concerning board member age.\n\n**Variables & Parameters.**\n\n*   `TOBINQ_i`: Tobin’s Q for firm `i`.\n*   `AGE_i`: The average age of the supervisory board for firm `i`.\n\n---\n\n### Data / Model Specification\n\nThe baseline cross-sectional OLS model is:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot AGE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       | Std. Deviation |\n| :---------- | :--------- | :------------- |\n| Tobin's Q   | 1.3786     | 1.0444         |\n| AGE         | 56.8190    | 4.1902         |\n\n**Table 2: Regression of Tobin's Q on Board Age and Controls**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Constant               | 2.575       | (0.005)** |\n| Age                    | -0.029      | (0.053)*  |\n\n*Note: **Significant at 5%, *Significant at 10%.*\n\n---\n\n### Question\n\nBased on the provided data and regression results, select all statements that correctly interpret the findings.", "Options": {"A": "The coefficient on `Age` is statistically significant at the 10% level.", "B": "The economic magnitude of the effect is negligible, representing less than a 1% change in Tobin's Q relative to its mean for a one-standard-deviation change in age.", "C": "The coefficient on `Age` is statistically significant at the 5% level.", "D": "A one-standard-deviation increase in average board age is associated with a predicted decrease in Tobin's Q of approximately 0.12."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to interpret regression output, distinguishing between statistical and economic significance. It uses an **Atomic Decomposition** strategy, breaking down the interpretation into distinct claims about significance levels and magnitude. The **Computational Judgment** strategy is used for the economic significance calculation. Distractors are designed to trap common errors: (B) misinterpreting the p-value threshold (a classic misconception), and (D) misjudging the economic magnitude (a conceptual opposite).", "qid": "192", "question": "### Background\n\n**Research Question.** This case examines the relationship between the average age of a firm's supervisory board and its financial performance, testing the hypothesis that older boards are associated with lower firm value.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish listed firms, with data averaged over the 1998–2001 period. The study is motivated by recommendations in Denmark's “Nørby report” on corporate governance concerning board member age.\n\n**Variables & Parameters.**\n\n*   `TOBINQ_i`: Tobin’s Q for firm `i`.\n*   `AGE_i`: The average age of the supervisory board for firm `i`.\n\n---\n\n### Data / Model Specification\n\nThe baseline cross-sectional OLS model is:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot AGE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       | Std. Deviation |\n| :---------- | :--------- | :------------- |\n| Tobin's Q   | 1.3786     | 1.0444         |\n| AGE         | 56.8190    | 4.1902         |\n\n**Table 2: Regression of Tobin's Q on Board Age and Controls**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Constant               | 2.575       | (0.005)** |\n| Age                    | -0.029      | (0.053)*  |\n\n*Note: **Significant at 5%, *Significant at 10%.*\n\n---\n\n### Question\n\nBased on the provided data and regression results, select all statements that correctly interpret the findings.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results and descriptive statistics:\n\n1. **Statistical significance of Age coefficient:**\n   - The p-value for Age is 0.053\n   - This is less than 0.10 but greater than 0.05\n   - Therefore, it's significant at the 10% level but NOT at the 5% level\n\n2. **Economic magnitude calculation:**\n   - Coefficient on Age: -0.029\n   - One standard deviation of Age: 4.1902\n   - Effect of one-SD change: -0.029 × 4.1902 = -0.1215\n   - This represents approximately a 0.12 decrease in Tobin's Q\n   - As a percentage of mean Tobin's Q: (0.1215/1.3786) × 100% = 8.8%\n   - This is NOT less than 1%, so the effect is not negligible\n\nBased on this analysis:\n- Option A is correct (significant at 10% level)\n- Option B is incorrect (the effect is 8.8%, not less than 1%)\n- Option C is incorrect (not significant at 5% level)\n- Option D is correct (one-SD increase leads to ~0.12 decrease)\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 253, "Question": "### Background\n\n**Research Question.** How do different tail-focused premium principles comparatively rank two risks with identical mean and variance but different tail thickness?\n\n**Setting.** A numerical example comparing premiums for a heavy-tailed Pareto risk (`X`) and a less heavy-tailed Weibull risk (`Y`). Both risks have been constructed to have the same mean (1.0) and variance (3.0). The goal is to see if the premium principles can correctly identify the Pareto as the more dangerous risk.\n\n**Variables and Parameters.**\n\n*   `X`: A Pareto-distributed loss (heavy-tailed).\n*   `Y`: A Weibull-distributed loss (thinner-tailed).\n*   `p`: The probability level defining the tail.\n*   `T_{1,1,1}`: Premium principle based on cumulative residual entropy.\n*   `T_{1,2,1}`: Premium principle based on Gini Mean Difference.\n*   `T_{1,0.5,1}`: Premium principle based on Wang's right tail deviation.\n\n---\n\n### Data / Model Specification\n\nTwo risks, `X` (Pareto) and `Y` (Weibull), have `E[X] = E[Y] = 1` and `Var(X) = Var(Y) = 3`. The Pareto distribution is known to have a heavier tail than this specific Weibull distribution.\n\n**Table 1: Premiums for Pareto Risk `X` at p=0.99**\n\n| T<sub>1,1,1</sub>(X,0.99) (Entropy) | T<sub>1,2,1</sub>(X,0.99) (Gini) | T<sub>1,0.5,1</sub>(X,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 18.887 | 14.710 | 39.774 |\n\n**Table 2: Premiums for Weibull Risk `Y` at p=0.99**\n\n| T<sub>1,1,1</sub>(Y,0.99) (Entropy) | T<sub>1,2,1</sub>(Y,0.99) (Gini) | T<sub>1,0.5,1</sub>(Y,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 15.476 | 13.510 | 19.911 |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and **Table 2** for the `p=0.99` case, select all statements that are factually correct.", "Options": {"A": "All three premium principles correctly rank the Pareto risk (X) as more costly than the Weibull risk (Y).", "B": "The Wang-based principle (`T_{1,0.5,1}`) exhibits the greatest sensitivity to tail thickness, as measured by the absolute premium difference between risk X and risk Y.", "C": "The Entropy-based principle (`T_{1,1,1}`) is the most robust to model choice, showing the smallest absolute premium difference between the two risks.", "D": "The Gini-based principle (`T_{1,2,1}`) is the only principle that fails to rank the risks correctly according to their tail thickness."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret empirical results from tables. It uses an Atomic Decomposition strategy, breaking down the complex original question into independently verifiable statements about ranking and sensitivity. Distractors are high-fidelity: (C) is a Conceptual Opposite, claiming an incorrect ranking when the ranking is correct, and (D) is an 'Almost Right' error, incorrectly identifying the Entropy-based principle as most robust when it is the Gini-based one.", "qid": "253", "question": "### Background\n\n**Research Question.** How do different tail-focused premium principles comparatively rank two risks with identical mean and variance but different tail thickness?\n\n**Setting.** A numerical example comparing premiums for a heavy-tailed Pareto risk (`X`) and a less heavy-tailed Weibull risk (`Y`). Both risks have been constructed to have the same mean (1.0) and variance (3.0). The goal is to see if the premium principles can correctly identify the Pareto as the more dangerous risk.\n\n**Variables and Parameters.**\n\n*   `X`: A Pareto-distributed loss (heavy-tailed).\n*   `Y`: A Weibull-distributed loss (thinner-tailed).\n*   `p`: The probability level defining the tail.\n*   `T_{1,1,1}`: Premium principle based on cumulative residual entropy.\n*   `T_{1,2,1}`: Premium principle based on Gini Mean Difference.\n*   `T_{1,0.5,1}`: Premium principle based on Wang's right tail deviation.\n\n---\n\n### Data / Model Specification\n\nTwo risks, `X` (Pareto) and `Y` (Weibull), have `E[X] = E[Y] = 1` and `Var(X) = Var(Y) = 3`. The Pareto distribution is known to have a heavier tail than this specific Weibull distribution.\n\n**Table 1: Premiums for Pareto Risk `X` at p=0.99**\n\n| T<sub>1,1,1</sub>(X,0.99) (Entropy) | T<sub>1,2,1</sub>(X,0.99) (Gini) | T<sub>1,0.5,1</sub>(X,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 18.887 | 14.710 | 39.774 |\n\n**Table 2: Premiums for Weibull Risk `Y` at p=0.99**\n\n| T<sub>1,1,1</sub>(Y,0.99) (Entropy) | T<sub>1,2,1</sub>(Y,0.99) (Gini) | T<sub>1,0.5,1</sub>(Y,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 15.476 | 13.510 | 19.911 |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and **Table 2** for the `p=0.99` case, select all statements that are factually correct.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Tables 1 and 2:\n\nFor Pareto risk (X):\n- T_{1,1,1}(X,0.99) = 18.887\n- T_{1,2,1}(X,0.99) = 14.710\n- T_{1,0.5,1}(X,0.99) = 39.774\n\nFor Weibull risk (Y):\n- T_{1,1,1}(Y,0.99) = 15.476\n- T_{1,2,1}(Y,0.99) = 13.510\n- T_{1,0.5,1}(Y,0.99) = 19.911\n\nLet me analyze each option:\n\nA) Checking if all three principles rank X > Y:\n- Entropy: 18.887 > 15.476 ✓\n- Gini: 14.710 > 13.510 ✓\n- Wang: 39.774 > 19.911 ✓\nAll three correctly rank Pareto (X) as more costly than Weibull (Y). **TRUE**\n\nB) Calculating absolute premium differences:\n- Entropy: |18.887 - 15.476| = 3.411\n- Gini: |14.710 - 13.510| = 1.200\n- Wang: |39.774 - 19.911| = 19.863\nWang shows the greatest sensitivity. **TRUE**\n\nC) The Gini-based principle shows the smallest absolute difference (1.200), not the Entropy-based principle (3.411). **FALSE**\n\nD) As shown in option A, all three principles correctly rank X > Y, including the Gini-based principle. **FALSE**\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 224, "Question": "### Background\nA central bank's monetary policy objectives may not be constant over time. A key hypothesis is that the Bank of Canada (BoC) first focused on establishing credibility by bringing inflation down (the \"disinflation period\") and then, once inflation was stable, shifted its focus toward stabilizing the real economy (the \"post-disinflation period\"). This suggests a structural change in its policy reaction function.\n\n### Data / Model Specification\nThe baseline forward-looking policy rule is given by:\n\n  \ni_{t}=c+\\rho i_{t-1}+\\phi_{r}r_{t}+\\phi_{\\pi}E_{t}(\\pi_{t+h_{\\pi}}-\\pi_{t+h_{\\pi}}^{*})+\\phi_{x}E_{t}(x_{t+h_{x}})+\\epsilon_{t}\n \n\nwhere $\\rho$ is the interest rate smoothing parameter, $\\phi_{\\pi}$ is the short-run response to the expected inflation gap, and $\\phi_x$ is the short-run response to the expected output gap. The long-run response to inflation, which determines if the Taylor principle is met, is $\\frac{\\phi_{\\pi}}{1-\\rho}$.\n\nTo test for a structural shift, the model is estimated over two distinct sub-periods. Table 1 presents the results for the disinflation period (1991Q1–1994Q4) and the post-disinflation period (1995Q1–2015Q4).\n\n**Table 1: Disinflation vs. Post-disinflation Period Estimates**\n| Variables | 1991Q1-1994Q4 | 1995Q1-2015Q4 |\n| :--- | :--- | :--- |\n| $\\rho$: Lagged Int. Rate | 0.554** | 0.899*** |\n| | (0.179) | (0.027) |\n| $\\phi_{\\pi}$: Exp. Infl. Gap | 0.927** | 0.329 |\n| | (0.414) | (0.219) |\n| $\\phi_x$: Exp. Output Gap | -0.493 | 0.204*** |\n| | (0.328) | (0.045) |\n*Note: Newey-West HAC standard errors in parentheses. ***, ** denote significance at 1% and 5% levels.*\n\nBased on the evidence in Table 1, which of the following conclusions about the structural shift in the BoC's monetary policy are supported by the data?\n", "Options": {"A": "The response to the expected inflation gap was statistically significant in the disinflation period but became statistically insignificant in the post-disinflation period.", "B": "In the post-disinflation period, the BoC's focus shifted towards stabilizing the real economy, as evidenced by the positive and highly statistically significant coefficient on the expected output gap.", "C": "Policy inertia, as measured by the coefficient on the lagged interest rate ($\\rho$), decreased significantly from the disinflation to the post-disinflation period.", "D": "During the disinflation period, the BoC's policy satisfied the Taylor principle, with a long-run response to inflation of approximately 2.08."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to analyze and synthesize regression results from two different time periods to identify a structural change in policy. It requires calculation, interpretation of statistical significance, and understanding of the economic narrative presented in the paper. Depth Strategy: Reverse-Reasoning. Given the empirical results (the outcome), the user must identify the plausible conclusions about the underlying policy shift. Distractor Logic: Option D is a conceptual distractor (Conceptual Opposite). The coefficient on the lagged interest rate, $\\rho$, *increased* from 0.554 to 0.899, indicating that policy inertia became substantially higher, not lower, in the later period. This is a plausible error for a user who misinterprets the meaning of the coefficient.", "qid": "224", "question": "### Background\nA central bank's monetary policy objectives may not be constant over time. A key hypothesis is that the Bank of Canada (BoC) first focused on establishing credibility by bringing inflation down (the \"disinflation period\") and then, once inflation was stable, shifted its focus toward stabilizing the real economy (the \"post-disinflation period\"). This suggests a structural change in its policy reaction function.\n\n### Data / Model Specification\nThe baseline forward-looking policy rule is given by:\n\n  \ni_{t}=c+\\rho i_{t-1}+\\phi_{r}r_{t}+\\phi_{\\pi}E_{t}(\\pi_{t+h_{\\pi}}-\\pi_{t+h_{\\pi}}^{*})+\\phi_{x}E_{t}(x_{t+h_{x}})+\\epsilon_{t}\n \n\nwhere $\\rho$ is the interest rate smoothing parameter, $\\phi_{\\pi}$ is the short-run response to the expected inflation gap, and $\\phi_x$ is the short-run response to the expected output gap. The long-run response to inflation, which determines if the Taylor principle is met, is $\\frac{\\phi_{\\pi}}{1-\\rho}$.\n\nTo test for a structural shift, the model is estimated over two distinct sub-periods. Table 1 presents the results for the disinflation period (1991Q1–1994Q4) and the post-disinflation period (1995Q1–2015Q4).\n\n**Table 1: Disinflation vs. Post-disinflation Period Estimates**\n| Variables | 1991Q1-1994Q4 | 1995Q1-2015Q4 |\n| :--- | :--- | :--- |\n| $\\rho$: Lagged Int. Rate | 0.554** | 0.899*** |\n| | (0.179) | (0.027) |\n| $\\phi_{\\pi}$: Exp. Infl. Gap | 0.927** | 0.329 |\n| | (0.414) | (0.219) |\n| $\\phi_x$: Exp. Output Gap | -0.493 | 0.204*** |\n| | (0.328) | (0.045) |\n*Note: Newey-West HAC standard errors in parentheses. ***, ** denote significance at 1% and 5% levels.*\n\nBased on the evidence in Table 1, which of the following conclusions about the structural shift in the BoC's monetary policy are supported by the data?\n", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each option:\n\nA) The response to the expected inflation gap ($\\phi_{\\pi}$):\n- Disinflation period: 0.927** (significant at 5% level)\n- Post-disinflation period: 0.329 (no significance stars, indicating not significant)\nThis statement is correct.\n\nB) The expected output gap coefficient ($\\phi_x$):\n- Disinflation period: -0.493 (not significant)\n- Post-disinflation period: 0.204*** (highly significant at 1% level)\nThe shift from insignificant (and negative) to positive and highly significant supports the claim that the BoC shifted focus to stabilizing the real economy. This statement is correct.\n\nC) Policy inertia ($\\rho$):\n- Disinflation period: 0.554**\n- Post-disinflation period: 0.899***\nThe coefficient increased from 0.554 to 0.899, meaning policy inertia increased, not decreased. This statement is incorrect.\n\nD) Taylor principle requires long-run response to inflation > 1:\n- Long-run response = $\\frac{\\phi_{\\pi}}{1-\\rho} = \\frac{0.927}{1-0.554} = \\frac{0.927}{0.446} ≈ 2.08$\nSince 2.08 > 1, the Taylor principle is satisfied. This statement is correct.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 197, "Question": "### Background\n\nA researcher is analyzing the distribution of publication speeds across the top twenty finance journals to inform submission strategies. The study finds that journals cluster into a central 'pack' with some notable outliers.\n\n### Data / Model Specification\n\nThe paper defines the majority 'pack' of top-twenty finance journals as those having a median acceptance time in the range from 10.0 to 14.0 months. Data for a selection of these journals are provided below.\n\n**Table 1. Distribution of Median Acceptance Times for Select Top Twenty Finance Journals (2012-2015)**\n\n| Journal | Median Acceptance Time (Months) |\n| :--- | :--- |\n| Review of Corporate Finance Studies (RCFS) | 7.4 |\n| Review of Asset Pricing Studies (RAPS) | 7.6 |\n| Journal of Corporate Finance (JCF) | 10.3 |\n| Journal of Money, Credit, and Banking (JMCB) | 13.9 |\n| Review of Financial Studies (RFS) | 15.5 |\n| Journal of Financial Econometrics (JFEconometrics) | 21.0 |\n\n---\n\nBased on the provided definition and data, which of the following journals fall OUTSIDE the defined median acceptance time range of the 'pack' (10.0 to 14.0 months)?\n\nSelect all that apply.", "Options": {"A": "Review of Asset Pricing Studies (RAPS)", "B": "Review of Financial Studies (RFS)", "C": "Journal of Financial Econometrics (JFEconometrics)", "D": "Review of Corporate Finance Studies (RCFS)"}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item tests the ability to apply a specific quantitative rule (the definition of the 'pack') to a dataset to correctly classify items. It is a precise test of data interpretation and application of a definition.\n\nChosen Strategy: Computational Judgment. The user must compare each journal's median time to the 10.0-14.0 month range and make a binary judgment (in/out).\n\nDistractor Logic: The distractors in this case would be the journals that are *inside* the pack (JCF and JMCB). By asking which are OUTSIDE, the question is structured to have multiple correct answers. This requires the user to check every single option against the rule, ensuring a comprehensive assessment rather than just finding one correct example.", "qid": "197", "question": "### Background\n\nA researcher is analyzing the distribution of publication speeds across the top twenty finance journals to inform submission strategies. The study finds that journals cluster into a central 'pack' with some notable outliers.\n\n### Data / Model Specification\n\nThe paper defines the majority 'pack' of top-twenty finance journals as those having a median acceptance time in the range from 10.0 to 14.0 months. Data for a selection of these journals are provided below.\n\n**Table 1. Distribution of Median Acceptance Times for Select Top Twenty Finance Journals (2012-2015)**\n\n| Journal | Median Acceptance Time (Months) |\n| :--- | :--- |\n| Review of Corporate Finance Studies (RCFS) | 7.4 |\n| Review of Asset Pricing Studies (RAPS) | 7.6 |\n| Journal of Corporate Finance (JCF) | 10.3 |\n| Journal of Money, Credit, and Banking (JMCB) | 13.9 |\n| Review of Financial Studies (RFS) | 15.5 |\n| Journal of Financial Econometrics (JFEconometrics) | 21.0 |\n\n---\n\nBased on the provided definition and data, which of the following journals fall OUTSIDE the defined median acceptance time range of the 'pack' (10.0 to 14.0 months)?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "I need to identify which journals have median acceptance times outside the range of 10.0 to 14.0 months.\n\nLooking at the data:\n- RCFS: 7.4 months (< 10.0, so OUTSIDE the range)\n- RAPS: 7.6 months (< 10.0, so OUTSIDE the range)\n- JCF: 10.3 months (within 10.0-14.0, so INSIDE the range)\n- JMCB: 13.9 months (within 10.0-14.0, so INSIDE the range)\n- RFS: 15.5 months (> 14.0, so OUTSIDE the range)\n- JFEconometrics: 21.0 months (> 14.0, so OUTSIDE the range)\n\nThe journals OUTSIDE the 10.0-14.0 month range are:\n- A) RAPS (7.6 months)\n- B) RFS (15.5 months)\n- C) JFEconometrics (21.0 months)\n- D) RCFS (7.4 months)\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 179, "Question": "### Background\n\n**Research Context.** A central motivation for this paper is the dramatic shift in executive compensation practices during the 1980s and 1990s, characterized by an \"explosion in stock option grants.\" The paper investigates the potential unintended consequences of this shift on managerial incentives for short-term earnings reporting.\n\n### Data / Model Specification\n\nThe paper documents this trend using data on S&P 1500 firms from 1992-2002.\n\n**Table 1: Median Stock-Based Compensation (`OPT_i`) Over Time**\n\n| Time Period | Median `OPT_i` |\n| :--- | :---: |\n| 1992–1994 | ~20% |\n| 1999–2002 | ~40% |\n\n*`OPT_i` is the Black-Scholes value of options granted as a percentage of total executive compensation.*\n\nThe Black-Scholes formula for a European call option is:\n  \nC(S, K, T, r, \\sigma) = S N(d_1) - K e^{-rT} N(d_2) \\quad \\text{(Eq. 1)}\n \nwhere `d_1 = \\frac{\\ln(S/K) + (r + \\sigma^2/2)T}{\\sigma\\sqrt{T}}` and `d_2 = d_1 - \\sigma\\sqrt{T}`. The option's sensitivity to the stock price is its Delta (`Δ = N(d_1)`).\n\n### Question\n\nRegarding the use of the Black-Scholes model for valuing executive stock options (ESOs) and the incentives they create, select all of the following statements that are correct.", "Options": {"A": "An executive compensated with deep OTM options has a weaker incentive to engage in small-scale earnings management than one with ATM options, because the OTM package has a lower total Delta, reducing the wealth gain from a small stock price increase.", "B": "The standard Black-Scholes model tends to overvalue ESOs because it fails to account for factors like forfeiture risk (if an executive leaves the firm) and the non-tradability that often leads to early exercise.", "C": "To provide an incentive package of equal value, a firm would grant fewer deep out-of-the-money (OTM) options than at-the-money (ATM) options, because OTM options have higher leverage.", "D": "The doubling of median option-based pay from 1992 to 2002 is particularly significant because it coincided with the market placing greater emphasis on meeting analyst forecasts, creating a powerful incentive for managers to hit short-term targets."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). Total Suitability Score = 9.5 (Conceptual Clarity = 9/10, Discriminability = 10/10). The original question's components are highly structured and rely on applying core finance principles (Black-Scholes assumptions, option greeks), making them ideal for conversion. High-fidelity distractors can be built around common misconceptions in option theory. This item uses an Atomic Decomposition strategy to test understanding of model limitations, incentive effects (Reverse-Reasoning), and the paper's motivating context. Distractor B contains a conceptual error about option pricing and the number of options required for equivalent value.", "qid": "179", "question": "### Background\n\n**Research Context.** A central motivation for this paper is the dramatic shift in executive compensation practices during the 1980s and 1990s, characterized by an \"explosion in stock option grants.\" The paper investigates the potential unintended consequences of this shift on managerial incentives for short-term earnings reporting.\n\n### Data / Model Specification\n\nThe paper documents this trend using data on S&P 1500 firms from 1992-2002.\n\n**Table 1: Median Stock-Based Compensation (`OPT_i`) Over Time**\n\n| Time Period | Median `OPT_i` |\n| :--- | :---: |\n| 1992–1994 | ~20% |\n| 1999–2002 | ~40% |\n\n*`OPT_i` is the Black-Scholes value of options granted as a percentage of total executive compensation.*\n\nThe Black-Scholes formula for a European call option is:\n  \nC(S, K, T, r, \\sigma) = S N(d_1) - K e^{-rT} N(d_2) \\quad \\text{(Eq. 1)}\n \nwhere `d_1 = \\frac{\\ln(S/K) + (r + \\sigma^2/2)T}{\\sigma\\sqrt{T}}` and `d_2 = d_1 - \\sigma\\sqrt{T}`. The option's sensitivity to the stock price is its Delta (`Δ = N(d_1)`).\n\n### Question\n\nRegarding the use of the Black-Scholes model for valuing executive stock options (ESOs) and the incentives they create, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) This is correct. Deep OTM options have lower Delta (N(d₁) is smaller when S < K) compared to ATM options. Since the executive's wealth gain from a small stock price increase equals Delta × number of options × price change, a lower Delta means less incentive for small-scale earnings management that produces marginal stock price increases.\n\nB) This is correct. The standard Black-Scholes model assumes options are freely tradable and held to maturity. ESOs have restrictions like forfeiture risk if the executive leaves and non-tradability that often forces early exercise. These factors reduce the true value below the Black-Scholes value.\n\nC) This is incorrect. While OTM options do have higher leverage (percentage returns), they have lower Black-Scholes values than ATM options. To provide an equal value package, a firm would need to grant MORE OTM options, not fewer.\n\nD) This is correct. The paper's research context explicitly states this was a central motivation - the \"explosion in stock option grants\" during the 1980s and 1990s created potential unintended consequences for short-term earnings reporting. The doubling of median option-based pay coinciding with increased market focus on analyst forecasts created strong incentives for earnings management.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 25, "Question": "**Research Question.** Does the idiosyncratic component of stock order flow, orthogonal to major macroeconomic shocks, have predictive power for future market returns?\n\n**Setting / Data-Generating Environment.** A structural VAR model isolates an idiosyncratic shock to the buy/sell ratio (`ε_4t`). This shock is, by construction, orthogonal to oil supply, aggregate demand, and oil-specific demand shocks. The paper finds that this shock negatively predicts future real stock returns.\n\n**Finding:** An unexpected positive shock to the stock order flow imbalance (`ε_{4t} > 0`) causes a sustained decrease in U.S. real stock returns over the subsequent months.\n\nBased on the provided context and standard market microstructure theory, select all statements that correctly characterize the idiosyncratic buy/sell ratio shock (`ε_{4t}`) and the price pressure mechanism explaining its negative predictability for future returns.", "Options": {"A": "The negative predictability arises because `ε_{4t} > 0` reveals negative private information about future firm fundamentals that was not captured by the macroeconomic shocks.", "B": "The shock `ε_{4t}` likely captures non-fundamental trading pressure, such as from large institutional portfolio rebalancing or sentiment-driven trades.", "C": "The mechanism implies that a positive shock (`ε_{4t} > 0`) pushes the market price temporarily *above* its fundamental value, leading to a subsequent correction.", "D": "A market with greater depth (lower price impact `λ`) would exhibit stronger negative predictability following a shock of a given size."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: Tests the conceptual understanding of idiosyncratic order flow shocks and the temporary price pressure mechanism. Strategy: Atomic Decomposition, breaking the concept into two testable parts: the nature of the shock (A) and the direction of the price impact (B). Distractor Logic: (C) presents a plausible but incorrect alternative theory (information-based trading). (D) presents a conceptual opposite regarding the role of market depth, a common point of confusion.", "qid": "25", "question": "**Research Question.** Does the idiosyncratic component of stock order flow, orthogonal to major macroeconomic shocks, have predictive power for future market returns?\n\n**Setting / Data-Generating Environment.** A structural VAR model isolates an idiosyncratic shock to the buy/sell ratio (`ε_4t`). This shock is, by construction, orthogonal to oil supply, aggregate demand, and oil-specific demand shocks. The paper finds that this shock negatively predicts future real stock returns.\n\n**Finding:** An unexpected positive shock to the stock order flow imbalance (`ε_{4t} > 0`) causes a sustained decrease in U.S. real stock returns over the subsequent months.\n\nBased on the provided context and standard market microstructure theory, select all statements that correctly characterize the idiosyncratic buy/sell ratio shock (`ε_{4t}`) and the price pressure mechanism explaining its negative predictability for future returns.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze which statements correctly characterize the idiosyncratic buy/sell ratio shock and its negative predictability for future returns.\n\nLet me examine each option:\n\n**Option A**: This suggests the shock reveals negative private information about fundamentals. However, the finding states that a positive shock to order flow (more buying pressure) predicts negative future returns. If this were revealing negative fundamental information, we would expect immediate negative returns, not positive pressure followed by negative returns. This doesn't align with the described pattern.\n\n**Option B**: This characterizes the shock as non-fundamental trading pressure (portfolio rebalancing, sentiment). This makes sense - the shock is orthogonal to macroeconomic fundamentals by construction, so it likely captures trading not driven by fundamental information. This is consistent with market microstructure theory about non-informational trades.\n\n**Option C**: This describes a price pressure mechanism where positive order flow temporarily pushes prices above fundamental value, followed by a correction (negative returns). This perfectly explains why positive shocks predict negative future returns - it's a temporary overvaluation that subsequently reverses.\n\n**Option D**: This states that greater market depth (lower price impact λ) would lead to stronger negative predictability. This is incorrect. With lower price impact, the same shock would cause smaller price distortions and thus smaller subsequent reversals, leading to weaker (not stronger) predictability.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 211, "Question": "### Background\n\nThe paper employs a competing risk proportional hazard model to forecast borrower outcomes. The model's forecasts are sensitive to the assumptions made about the future paths of time-varying covariates.\n\n### Data / Model Specification\n\nThe proportional hazard rates for prepayment (`p`) and default (`d`) at duration `t` are given by:\n  \nh^{k}(t|X_{t}^{k})=\\exp(g^{k}(t))\\exp(X_{t}^{k}\\beta^{k}) \\quad \\text{(Eq. 1)}\n \nwhere `X_t` is a vector of explanatory variables at time `t`. One key time-varying covariate is the current Loan-to-Value (LTV) ratio, which is updated monthly based on projected house price index (HPI) changes.\n\nThe probability of defaulting over a future horizon from `t+1` to `T` is then calculated as:\n  \n\\operatorname*{Pr}^{D}(t,T)=\\sum_{j=t+1}^{T} S(j-1) h^{d}(j) \\quad \\text{(Eq. 2)}\n \nwhere `S(j-1)` is the probability of the borrower remaining active through month `j-1`.\n\n### Question\n\nBased on the structure of the forecasting model described, which of the following statements are valid implications of using time-varying covariates like current LTV?", "Options": {"A": "The term `S(j-1)h^d(j)` represents the conditional probability of defaulting in month `j` given survival through month `j-1`.", "B": "The model's core assumption is that covariates have an additive, rather than multiplicative, effect on the baseline hazard.", "C": "A more pessimistic forecast for future house prices would lead to higher projected LTVs, which in turn would increase the forecasted default hazard `h^d(j)` for many borrowers.", "D": "The model's loss forecasts are critically dependent on the accuracy of external macroeconomic forecasts, such as future house prices."}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: This item assesses the understanding of the practical implications and underlying assumptions of the econometric model used in the paper, particularly the concept of model risk introduced by time-varying covariates. Depth Strategy: Scenario Application (considering the effect of a change in macro forecasts). Distractor Logic: Option A is a correct high-level implication about model risk. Option B correctly traces the causal chain from a macro forecast (HPI) to a key covariate (LTV) to the model output (hazard rate). Option C is a 'Conceptual Opposite' distractor; the model is explicitly multiplicative (`exp(X*beta)`), not additive. Option D is a 'Conceptual Distractor' that misinterprets a key term; `S(j-1)h^d(j)` is the *unconditional* probability of default in month j, while the *conditional* probability is simply `h^d(j)`.", "qid": "211", "question": "### Background\n\nThe paper employs a competing risk proportional hazard model to forecast borrower outcomes. The model's forecasts are sensitive to the assumptions made about the future paths of time-varying covariates.\n\n### Data / Model Specification\n\nThe proportional hazard rates for prepayment (`p`) and default (`d`) at duration `t` are given by:\n  \nh^{k}(t|X_{t}^{k})=\\exp(g^{k}(t))\\exp(X_{t}^{k}\\beta^{k}) \\quad \\text{(Eq. 1)}\n \nwhere `X_t` is a vector of explanatory variables at time `t`. One key time-varying covariate is the current Loan-to-Value (LTV) ratio, which is updated monthly based on projected house price index (HPI) changes.\n\nThe probability of defaulting over a future horizon from `t+1` to `T` is then calculated as:\n  \n\\operatorname*{Pr}^{D}(t,T)=\\sum_{j=t+1}^{T} S(j-1) h^{d}(j) \\quad \\text{(Eq. 2)}\n \nwhere `S(j-1)` is the probability of the borrower remaining active through month `j-1`.\n\n### Question\n\nBased on the structure of the forecasting model described, which of the following statements are valid implications of using time-varying covariates like current LTV?", "model_evaluation_result": {"model_raw_response": "Looking at this competing risk proportional hazard model, I need to evaluate each statement based on the model structure and equations provided.\n\nLet me analyze each option:\n\n**Option A**: \"The term `S(j-1)h^d(j)` represents the conditional probability of defaulting in month `j` given survival through month `j-1`.\"\n\nThis is correct. In survival analysis, `S(j-1)` is the survival probability through month `j-1`, and `h^d(j)` is the hazard rate for default at time `j`. The product `S(j-1)h^d(j)` gives the unconditional probability of defaulting in month `j`, which equals the conditional probability of defaulting in month `j` given survival through month `j-1`.\n\n**Option B**: \"The model's core assumption is that covariates have an additive, rather than multiplicative, effect on the baseline hazard.\"\n\nThis is incorrect. From Equation 1: `h^k(t|X_t^k) = exp(g^k(t))exp(X_t^k β^k)`, we can see that covariates enter through `exp(X_t^k β^k)`, which means they have a multiplicative effect on the baseline hazard `exp(g^k(t))`, not an additive effect.\n\n**Option C**: \"A more pessimistic forecast for future house prices would lead to higher projected LTVs, which in turn would increase the forecasted default hazard `h^d(j)` for many borrowers.\"\n\nThis is correct. If house prices are forecasted to decline (pessimistic forecast), the value component of LTV decreases, causing LTV ratios to increase. Higher LTV ratios are typically associated with higher default risk, so `h^d(j)` would increase.\n\n**Option D**: \"The model's loss forecasts are critically dependent on the accuracy of external macroeconomic forecasts, such as future house prices.\"\n\nThis is correct. Since the model uses time-varying covariates like current LTV that are updated based on projected house price changes, and these covariates directly affect the hazard rates and thus the probability calculations, the accuracy of the loss forecasts depends critically on the accuracy of these external", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 35, "Question": "### Background\n\nThe study proposes and compares two distinct architectures for combining a linear ARMA model with a non-linear Neural Network (NN) to forecast financial returns. This approach is motivated by the assumption that financial time series contain both linear and non-linear predictable components.\n\n### Data / Model Specification\n\nThe study defines two combination architectures:\n\n1.  **Hybrid Architecture:** A three-step, additive process. First, an ARMA model is fitted. Second, an NN is trained to forecast the ARMA residuals (`ε_t`). Third, the final forecast is the sum of the two component forecasts.\n\n      \n    \\hat{Y}_{t+1|t}^{\\text{Hybrid}} = \\hat{Y}_{t+1|t}^{\\text{ARMA}} + \\hat{\\varepsilon}_{t+1|t}^{\\text{NN}} \n     \n\n2.  **Mixed Architecture:** A two-step process where the ARMA forecast is used as an input feature for the NN. The NN's target is the actual return `Y_{t+1}`, and its input vector is augmented with the ARMA forecast.\n\n      \n    \\hat{Y}_{t+1|t}^{\\text{Mixed}} = \\text{NN}([X_t, \\hat{Y}_{t+1|t}^{\\text{ARMA}}])\n     \n\n---\n\nBased on these definitions, which of the following statements accurately describe one or both of these architectures?\n", "Options": {"A": "The Mixed architecture allows the NN to learn non-linear interactions between the ARMA forecast and other input variables.", "B": "Both architectures use the actual return `Y_{t+1}` as the direct target variable for the NN training process.", "C": "In the Hybrid architecture, the NN is trained to predict the errors made by the ARMA model.", "D": "The Hybrid architecture implicitly assumes a strictly additive relationship between the linear and non-linear components of the return series."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the fundamental structural and conceptual differences between the Hybrid and Mixed modeling approaches. Strategy: Reverse-Reasoning (identifying correct descriptions of the models). Distractor Logic: Options A, B, and D are direct, correct interpretations of the model definitions. Option A correctly identifies the NN's target in the Hybrid model. Option B correctly describes the flexibility of the Mixed model. Option D correctly identifies the implicit assumption of the Hybrid model's additive structure. Option C is a 'Conceptual Distractor' that is only true for the Mixed model; in the Hybrid model, the NN's target is the ARMA residual, not the actual return, making the statement false.", "qid": "35", "question": "### Background\n\nThe study proposes and compares two distinct architectures for combining a linear ARMA model with a non-linear Neural Network (NN) to forecast financial returns. This approach is motivated by the assumption that financial time series contain both linear and non-linear predictable components.\n\n### Data / Model Specification\n\nThe study defines two combination architectures:\n\n1.  **Hybrid Architecture:** A three-step, additive process. First, an ARMA model is fitted. Second, an NN is trained to forecast the ARMA residuals (`ε_t`). Third, the final forecast is the sum of the two component forecasts.\n\n      \n    \\hat{Y}_{t+1|t}^{\\text{Hybrid}} = \\hat{Y}_{t+1|t}^{\\text{ARMA}} + \\hat{\\varepsilon}_{t+1|t}^{\\text{NN}} \n     \n\n2.  **Mixed Architecture:** A two-step process where the ARMA forecast is used as an input feature for the NN. The NN's target is the actual return `Y_{t+1}`, and its input vector is augmented with the ARMA forecast.\n\n      \n    \\hat{Y}_{t+1|t}^{\\text{Mixed}} = \\text{NN}([X_t, \\hat{Y}_{t+1|t}^{\\text{ARMA}}])\n     \n\n---\n\nBased on these definitions, which of the following statements accurately describe one or both of these architectures?\n", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) The Mixed architecture allows the NN to learn non-linear interactions between the ARMA forecast and other input variables.\n- TRUE: In the Mixed architecture, the ARMA forecast is included as an input feature alongside other variables (X_t). The NN can learn any non-linear function of these inputs, including interactions between them.\n\nB) Both architectures use the actual return `Y_{t+1}` as the direct target variable for the NN training process.\n- FALSE: Only the Mixed architecture uses Y_{t+1} as the NN target. In the Hybrid architecture, the NN targets the ARMA residuals (ε_t), not the actual returns.\n\nC) In the Hybrid architecture, the NN is trained to predict the errors made by the ARMA model.\n- TRUE: The Hybrid architecture explicitly states that the NN is trained to forecast the ARMA residuals (ε_t), which are the errors/residuals from the ARMA model.\n\nD) The Hybrid architecture implicitly assumes a strictly additive relationship between the linear and non-linear components of the return series.\n- TRUE: The final forecast in the Hybrid architecture is simply the sum of the ARMA forecast and the NN forecast of residuals. This additive structure assumes no interaction between the linear and non-linear components.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 237, "Question": "### Background\n\nA researcher estimates a Vector Error Correction Model (VECM) for one-minute midquotes for Expedia in 2007, originating from three groups of exchanges: NYSE/ARCA, NASD, and all others (REST). The goal is to assess the relative importance of each exchange group in price discovery using the standard Hasbrouck methodology.\n\n### Data / Model Specification\n\n**Table 1: VECM Estimation Results for Expedia (from paper's Table 7)**\n\n| Dealer | Residual Covariances (Correlations) | Info Shares | |\n|:---|:---|:---|---:|\n| | NYSE | NASD | REST | min | max |\n| NYSE | 1.00 | *0.90* | *0.94* | 0.024 | 0.974 |\n| NASD | 0.96 | 1.15 | *0.87* | 0.001 | 0.824 |\n| REST | 0.96 | 0.95 | 1.05 | 0.024 | 0.964 |\n\n---\n\nBased on the provided results and the context of the Hasbrouck methodology, which of the following statements are valid conclusions or interpretations?", "Options": {"A": "The extremely high residual correlations (e.g., 0.94 between NYSE and REST) are the primary cause of the uninformatively wide information share bounds.", "B": "The results clearly indicate that NASD is the least important market for price discovery, as its minimum information share is the lowest at 0.1%.", "C": "Despite the wide bounds, a definitive conclusion can be made that NYSE is more important than NASD because its minimum bound (2.4%) is higher than NASD's minimum bound (0.1%).", "D": "The wide bounds demonstrate a failure of the Hasbrouck methodology in this context, as the Cholesky decomposition arbitrarily allocates the large shared variance component to whichever market is ordered first."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the user's understanding of the fundamental limitations of the Hasbrouck information share methodology, particularly in highly correlated modern markets. It uses a Reverse-Reasoning strategy (given the outcome of wide bounds, identify the causes). Option A correctly identifies the high residual correlation as the root cause. Option C correctly explains the mechanism of failure (arbitrary allocation of shared variance by Cholesky decomposition). Options B and D are conceptual distractors that represent a naive or incorrect interpretation of the min/max bounds; the wide and overlapping nature of the bounds makes any such ranking impossible.", "qid": "237", "question": "### Background\n\nA researcher estimates a Vector Error Correction Model (VECM) for one-minute midquotes for Expedia in 2007, originating from three groups of exchanges: NYSE/ARCA, NASD, and all others (REST). The goal is to assess the relative importance of each exchange group in price discovery using the standard Hasbrouck methodology.\n\n### Data / Model Specification\n\n**Table 1: VECM Estimation Results for Expedia (from paper's Table 7)**\n\n| Dealer | Residual Covariances (Correlations) | Info Shares | |\n|:---|:---|:---|---:|\n| | NYSE | NASD | REST | min | max |\n| NYSE | 1.00 | *0.90* | *0.94* | 0.024 | 0.974 |\n| NASD | 0.96 | 1.15 | *0.87* | 0.001 | 0.824 |\n| REST | 0.96 | 0.95 | 1.05 | 0.024 | 0.964 |\n\n---\n\nBased on the provided results and the context of the Hasbrouck methodology, which of the following statements are valid conclusions or interpretations?", "model_evaluation_result": {"model_raw_response": "Looking at this VECM estimation for Expedia's price discovery across three exchange groups, I need to evaluate each statement based on the Hasbrouck methodology and the provided results.\n\nThe key features of the data are:\n- Extremely high residual correlations (0.87-0.94)\n- Very wide information share bounds for all exchanges\n- NYSE: 0.024-0.974, NASD: 0.001-0.824, REST: 0.024-0.964\n\nEvaluating each option:\n\nA) This is correct. The Hasbrouck methodology uses Cholesky decomposition, which becomes problematic with high correlations. When residual correlations approach 1, small changes in ordering can dramatically affect how the shared variance is allocated, leading to wide bounds. The correlations here (0.87-0.94) are indeed extremely high and directly cause the uninformatively wide bounds.\n\nB) This is incorrect. With such wide overlapping bounds, we cannot make definitive statements about which market is most or least important. NASD's range (0.001-0.824) substantially overlaps with the others, meaning it could potentially contribute up to 82.4% of price discovery.\n\nC) This is incorrect. While NYSE's minimum (2.4%) exceeds NASD's minimum (0.1%), comparing only minimum bounds ignores the upper bounds. NASD could contribute up to 82.4% while NYSE could be as low as 2.4%. The overlapping ranges prevent definitive conclusions about relative importance.\n\nD) This is correct. The wide bounds don't represent a \"failure\" but rather demonstrate a well-known limitation of the Hasbrouck methodology. With high correlations, the Cholesky decomposition's order-dependence becomes pronounced, arbitrarily allocating the large shared variance component to whichever market is ordered first, resulting in the extreme bounds we observe.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 74, "Question": "### Background\n\n**Research Question.** How does the design complexity of a Bonus-Malus System (BMS)—specifically the number of classes and the severity of penalties—influence its function as a risk classification tool and its financial stability?\n\n**Setting / Data-Generating Environment.** The analysis contrasts two real-world BMS designs: the simple 7-class Brazilian system and the more sophisticated 23-class Belgian system. Their key distinction lies in their transition rules for claim-free years versus years with at-fault claims.\n\n---\n\n### Data / Model Specification\n\n**Brazilian System Rules:**\n- Number of classes: 7\n- Reward (0 claims): `ΔC(0) = -1` (one-class discount)\n- Penalty (1 claim): `ΔC(1) ≈ +1` (one-class penalty, for most classes)\n\n**Table 1: Brazilian System Transition Rules (Selected Classes)**\n| Class | Premium | 0 Claims | 1 Claim | 2 Claims |\n|:-----:|:-------:|:--------:|:-------:|:--------:|\n| 7     | 100     | 6        | 7       | 7        |\n| 6     | 90      | 5        | 7       | 7        |\n| 5     | 85      | 4        | 6       | 7        |\n\n**Belgian System Rules:**\n- Number of classes: 23\n- Reward (0 claims): `ΔC(0) = -1` (one-class discount)\n- Penalty (1st claim): `ΔC(1) = +4` (four-class penalty)\n\n**Aggregate Portfolio Dynamics:**\nFor a BMS to be in financial equilibrium, the total number of penalty classes applied across the portfolio must roughly balance the total number of discount classes awarded. In a typical portfolio with an average claim frequency `λ_avg ≈ 0.1`, about 90% of drivers are claim-free in a given year.\n\n---\n\n### Question\n\nBased on the provided information, select all statements that correctly compare the BMS designs or describe the conditions for financial equilibrium.", "Options": {"A": "For a BMS portfolio with an average claim frequency of `λ=0.1` to remain in financial equilibrium, the average penalty (in classes) for a driver with claims must be significantly larger than the one-class discount for a claim-free driver.", "B": "The Belgian system's design, where a single claim is penalized by four classes, implies it takes four claim-free years to recover from one at-fault claim.", "C": "In the Brazilian system, a driver in Class 6 who has one at-fault claim will move to Class 5.", "D": "The Brazilian system's lenient, symmetric penalty structure is more effective at separating high-risk and low-risk drivers than the Belgian system's asymmetric structure."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to compare different BMS designs and understand the concept of portfolio-level financial equilibrium. It uses an Atomic Decomposition strategy. Option A requires interpreting the penalty-reward structure. Option C tests the conceptual understanding of the equilibrium condition. Distractor B is a Conceptual Opposite regarding the effectiveness of different penalty structures. Distractor D is a simple Table Misreading error.", "qid": "74", "question": "### Background\n\n**Research Question.** How does the design complexity of a Bonus-Malus System (BMS)—specifically the number of classes and the severity of penalties—influence its function as a risk classification tool and its financial stability?\n\n**Setting / Data-Generating Environment.** The analysis contrasts two real-world BMS designs: the simple 7-class Brazilian system and the more sophisticated 23-class Belgian system. Their key distinction lies in their transition rules for claim-free years versus years with at-fault claims.\n\n---\n\n### Data / Model Specification\n\n**Brazilian System Rules:**\n- Number of classes: 7\n- Reward (0 claims): `ΔC(0) = -1` (one-class discount)\n- Penalty (1 claim): `ΔC(1) ≈ +1` (one-class penalty, for most classes)\n\n**Table 1: Brazilian System Transition Rules (Selected Classes)**\n| Class | Premium | 0 Claims | 1 Claim | 2 Claims |\n|:-----:|:-------:|:--------:|:-------:|:--------:|\n| 7     | 100     | 6        | 7       | 7        |\n| 6     | 90      | 5        | 7       | 7        |\n| 5     | 85      | 4        | 6       | 7        |\n\n**Belgian System Rules:**\n- Number of classes: 23\n- Reward (0 claims): `ΔC(0) = -1` (one-class discount)\n- Penalty (1st claim): `ΔC(1) = +4` (four-class penalty)\n\n**Aggregate Portfolio Dynamics:**\nFor a BMS to be in financial equilibrium, the total number of penalty classes applied across the portfolio must roughly balance the total number of discount classes awarded. In a typical portfolio with an average claim frequency `λ_avg ≈ 0.1`, about 90% of drivers are claim-free in a given year.\n\n---\n\n### Question\n\nBased on the provided information, select all statements that correctly compare the BMS designs or describe the conditions for financial equilibrium.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) For a BMS portfolio with λ=0.1 to remain in financial equilibrium:\n- 90% of drivers are claim-free (get 1-class discount)\n- 10% of drivers have claims (get penalty)\n- For balance: 0.9 × 1 = 0.1 × penalty\n- This means penalty must be 9 classes on average\n- Since the average penalty must be significantly larger than 1, this statement is **correct**.\n\nB) Belgian system recovery time:\n- One claim: +4 classes penalty\n- Each claim-free year: -1 class discount\n- To recover: 4 claim-free years needed\n- This statement is **correct**.\n\nC) Brazilian system transition for Class 6 with 1 claim:\n- From Table 1: Class 6 with 1 claim → Class 7\n- The statement says Class 6 → Class 5\n- This statement is **incorrect**.\n\nD) Effectiveness of risk separation:\n- Brazilian system: symmetric (±1 class), lenient penalties\n- Belgian system: asymmetric (+4 penalty vs -1 discount), severe penalties\n- More severe penalties create better separation between risk groups\n- The statement claims Brazilian is more effective, which is **incorrect**.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 229, "Question": "### Background\n\n**Research Question.** How can the theoretical framework for solving partial integro-differential equations (PIDEs) be applied to the practical problem of pricing and hedging a catastrophe (CAT) bond, an instrument exposed to both non-hedgeable insurance risk and hedgeable market risk?\n\n**Setting / Data-Generating Environment.** The price of a CAT bond depends on a three-dimensional state process `X = (Z^1, Z^2, L)`, where `Z^1` is a factor driving the loss intensity, `Z^2` is a factor driving the interest rate, and `L` is the cumulative loss process. The goal is to price the bond and construct a mean-variance optimal hedge for its interest rate exposure using a traded zero-coupon bond.\n\n### Data / Model Specification\n\nThe state process `X_t = (Z^1_t, Z^2_t, L_t)` follows the SDE system under a risk-neutral measure `Q`:\n  \n\\mathrm{d}Z_{t}^{1} = a_{1}(b_{1}-Z_{t}^{1})\\mathrm{d}t+\\sqrt{1-\\rho^{2}}\\sigma_{1}\\mathrm{d}W_{t}^{1}+\\rho\\sigma_{1}\\mathrm{d}W_{t}^{2}+\\gamma^{Z}\\mathrm{d}L_{t} \\quad \\text{(Eq. (1))}\n \n  \n\\mathrm{d}Z_{t}^{2} = a_{2}(b_{2}-Z_{t}^{2})\\mathrm{d}t+\\sigma_{2}\\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (2))}\n \nwhere `W^1, W^2` are independent Brownian motions. The price of the CAT bond is `P_t^CAT = p^CAT(t, X_t)`. The hedging instrument is a zero-coupon bond with price `P_t = p(t, Z^2_t)`. The dynamics of the discounted zero-coupon bond price `P̃_t` are:\n  \n\\mathrm{d}\\widetilde{P}_{t} = \\widetilde{P}_{t} \\frac{\\sigma_{2}p_{z_{2}}(t,Z_{t}^{2})}{p(t,Z_{t}^{2})} \\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nAn investor wants to implement a mean-variance optimal hedge for the CAT bond's interest rate risk by trading the zero-coupon bond. Select ALL of the following statements that are correct regarding the hedging strategy and its theoretical requirements.", "Options": {"A": "The optimal number of zero-coupon bonds to hold, `h_t^*`, is given by `p_{z_{2}}^{\\mathrm{CAT}}/p_{z_{2}} + \\rho(\\sigma_{1}/\\sigma_{2}) (p_{z_{1}}^{\\mathrm{CAT}}/p_{z_{2}})`.", "B": "The optimal hedge ratio `h_t^*` only includes the direct exposure to `Z^2`, given by `p_{z_{2}}^{\\mathrm{CAT}}/p_{z_{2}}`, because the risk from `W^1` is unhedgeable.", "C": "The hedging strategy requires a classical solution because viscosity solutions do not guarantee that the bond price remains positive.", "D": "This hedging strategy is only well-defined if the CAT bond price function, `p^CAT`, is a classical solution to its PIDE, because the formula for `h_t^*` requires the existence of its first-order partial derivatives."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item tests the ability to identify the correct mean-variance hedge ratio in an incomplete market and to understand the theoretical justification for why a classical (smooth) solution of the pricing PIDE is a necessary prerequisite for implementing the hedge. Strategy: Combination of Computational Judgment (identifying the correct formula) and Reverse-Reasoning (identifying the necessary precondition for the formula to be valid). Distractor Logic: (C) is a Step-Omission Error, incorrectly ignoring the cross-hedge term that arises from the correlation `ρ`. (D) is a Conceptual: True but Irrelevant/Incorrect Reason; while positivity is desirable, the direct reason a classical solution is needed is the explicit dependence of the hedge ratio on derivatives, which are not guaranteed to exist for a mere viscosity solution.", "qid": "229", "question": "### Background\n\n**Research Question.** How can the theoretical framework for solving partial integro-differential equations (PIDEs) be applied to the practical problem of pricing and hedging a catastrophe (CAT) bond, an instrument exposed to both non-hedgeable insurance risk and hedgeable market risk?\n\n**Setting / Data-Generating Environment.** The price of a CAT bond depends on a three-dimensional state process `X = (Z^1, Z^2, L)`, where `Z^1` is a factor driving the loss intensity, `Z^2` is a factor driving the interest rate, and `L` is the cumulative loss process. The goal is to price the bond and construct a mean-variance optimal hedge for its interest rate exposure using a traded zero-coupon bond.\n\n### Data / Model Specification\n\nThe state process `X_t = (Z^1_t, Z^2_t, L_t)` follows the SDE system under a risk-neutral measure `Q`:\n  \n\\mathrm{d}Z_{t}^{1} = a_{1}(b_{1}-Z_{t}^{1})\\mathrm{d}t+\\sqrt{1-\\rho^{2}}\\sigma_{1}\\mathrm{d}W_{t}^{1}+\\rho\\sigma_{1}\\mathrm{d}W_{t}^{2}+\\gamma^{Z}\\mathrm{d}L_{t} \\quad \\text{(Eq. (1))}\n \n  \n\\mathrm{d}Z_{t}^{2} = a_{2}(b_{2}-Z_{t}^{2})\\mathrm{d}t+\\sigma_{2}\\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (2))}\n \nwhere `W^1, W^2` are independent Brownian motions. The price of the CAT bond is `P_t^CAT = p^CAT(t, X_t)`. The hedging instrument is a zero-coupon bond with price `P_t = p(t, Z^2_t)`. The dynamics of the discounted zero-coupon bond price `P̃_t` are:\n  \n\\mathrm{d}\\widetilde{P}_{t} = \\widetilde{P}_{t} \\frac{\\sigma_{2}p_{z_{2}}(t,Z_{t}^{2})}{p(t,Z_{t}^{2})} \\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (3))}\n \n\n### Question\n\nAn investor wants to implement a mean-variance optimal hedge for the CAT bond's interest rate risk by trading the zero-coupon bond. Select ALL of the following statements that are correct regarding the hedging strategy and its theoretical requirements.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the mean-variance optimal hedging strategy for a CAT bond using a zero-coupon bond as the hedging instrument.\n\nGiven the dynamics:\n- CAT bond price: $P_t^{CAT} = p^{CAT}(t, X_t)$ where $X_t = (Z^1_t, Z^2_t, L_t)$\n- Zero-coupon bond price: $P_t = p(t, Z^2_t)$\n- The discounted zero-coupon bond follows: $d\\tilde{P}_t = \\tilde{P}_t \\frac{\\sigma_2 p_{z_2}(t,Z^2_t)}{p(t,Z^2_t)} dW^2_t$\n\nFor mean-variance hedging, I need to find the hedge ratio $h_t^*$ that minimizes the variance of the hedged portfolio.\n\nAnalyzing each option:\n\n**Option A**: The optimal hedge ratio formula $h_t^* = \\frac{p_{z_2}^{CAT}}{p_{z_2}} + \\rho\\frac{\\sigma_1}{\\sigma_2}\\frac{p_{z_1}^{CAT}}{p_{z_2}}$\n\nThis is correct. The CAT bond has exposure to both $W^1$ and $W^2$ through:\n- Direct exposure to $W^2$ via $Z^2$\n- Indirect exposure to $W^2$ via $Z^1$ (due to the correlation term $\\rho\\sigma_1 dW^2$ in equation (1))\n\nThe hedge ratio accounts for both exposures to minimize variance.\n\n**Option B**: Claims the hedge ratio only includes direct exposure $\\frac{p_{z_2}^{CAT}}{p_{z_2}}$\n\nThis is incorrect. While risk from $W^1$ is indeed unhedgeable, the CAT bond has indirect exposure to $W^2$ through $Z^1$ (via the correlation $\\rho$), which must be included in the optimal hedge.\n\n**Option C**: Claims classical solutions are required to ensure positive", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 12, "Question": "### Background\n\nAn insurer implements a dynamic hedging program using derivatives to mitigate the risk of its segregated fund guarantees. This program is incorporated into its internal model, which generates a probability distribution for the future cost of the guarantees, `C`. The capital requirement, `K`, is calculated as `K = CTE(95) - L`, where `L` is the policy liability (set to the mean of the cost distribution) and `CTE(95)` is the Conditional Tail Expectation at the 95th percentile.\n\n### Question\n\nAn effective hedging program is designed to pay off when guarantee costs are high (i.e., when markets perform poorly), thereby reducing the severity of large losses. Which of the following are plausible outcomes of introducing such an effective hedging program? (Select all that apply.)\n", "Options": {"A": "The 95th percentile of the cost distribution decreases, as the hedges are designed to be most effective in adverse scenarios.", "B": "The policy liability `L` increases because the insurer must now provision for the cost of the hedging instruments (e.g., option premiums).", "C": "The required capital `K` decreases, as the hedging program reduces the gap between the average tail cost and the mean cost.", "D": "The `CTE(95)` of the cost distribution decreases, reflecting mitigation of the most extreme losses."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to reason about the impact of a risk management action (hedging) on the shape of a probability distribution and the resulting regulatory capital implications.\nDepth Strategy: Reverse-Reasoning. Given the action (hedging), the candidate must identify all plausible effects on the underlying risk model's outputs.\nDistractor Logic:\n- C (Conceptual Distractor - Almost Right): This is a sophisticated distractor. While hedging has a cost, an *effective* program's reduction in expected guarantee payouts should be greater than the cost of the hedge itself. Therefore, the net effect on the mean cost (and thus the liability `L`) should be a decrease, not an increase. This option tempts the user to focus only on the cost of the hedge without considering its benefit.", "qid": "12", "question": "### Background\n\nAn insurer implements a dynamic hedging program using derivatives to mitigate the risk of its segregated fund guarantees. This program is incorporated into its internal model, which generates a probability distribution for the future cost of the guarantees, `C`. The capital requirement, `K`, is calculated as `K = CTE(95) - L`, where `L` is the policy liability (set to the mean of the cost distribution) and `CTE(95)` is the Conditional Tail Expectation at the 95th percentile.\n\n### Question\n\nAn effective hedging program is designed to pay off when guarantee costs are high (i.e., when markets perform poorly), thereby reducing the severity of large losses. Which of the following are plausible outcomes of introducing such an effective hedging program? (Select all that apply.)\n", "model_evaluation_result": {"model_raw_response": "I need to analyze how an effective hedging program affects the distribution of guarantee costs and the resulting capital calculations.\n\nKey points:\n- The hedging program pays off when guarantee costs are high (poor market performance)\n- Capital requirement K = CTE(95) - L\n- L = mean of the cost distribution\n- CTE(95) = Conditional Tail Expectation at 95th percentile (average of losses beyond 95th percentile)\n\nAnalyzing each option:\n\nA) The 95th percentile of the cost distribution decreases, as the hedges are designed to be most effective in adverse scenarios.\n- TRUE: Hedges that pay off in adverse scenarios directly reduce the costs in the tail of the distribution, lowering the 95th percentile.\n\nB) The policy liability L increases because the insurer must now provision for the cost of the hedging instruments (e.g., option premiums).\n- TRUE: The cost of hedging (premiums paid) increases the expected cost, raising the mean of the distribution and thus L.\n\nC) The required capital K decreases, as the hedging program reduces the gap between the average tail cost and the mean cost.\n- TRUE: Effective hedging reduces CTE(95) more than it increases L (the mean), resulting in a net decrease in K = CTE(95) - L.\n\nD) The CTE(95) of the cost distribution decreases, reflecting mitigation of the most extreme losses.\n- TRUE: Since hedges pay off in extreme scenarios, they reduce the severity of tail losses, lowering the average of losses beyond the 95th percentile.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 196, "Question": "### Background\n\nAn assistant professor is developing a submission strategy for a new working paper. The professor is facing a fixed tenure deadline and needs to secure acceptances efficiently. The analysis focuses on articles published between 2012 and 2015 in the top five general-interest finance journals.\n\n### Data / Model Specification\n\nThe study's findings for the median acceptance times of these journals are summarized below.\n\n**Table 1. Median Acceptance Times for Top Five General-Interest Finance Journals (2012-2015)**\n\n| Journal | Median Acceptance Time (Months) |\n| :--- | :--- |\n| Journal of Financial Economics (JFE) | 9.9 |\n| Journal of Financial and Quantitative Analysis (JFQA) | 10.6 |\n| Review of Finance (RF) | 11.7 |\n| Review of Financial Studies (RFS) | 15.5 |\n| Journal of Finance (JF) | 19.8 |\n\n---\n\nBased on the data in **Table 1**, which of the following statements are valid conclusions or strategically sound applications of the findings?\n\nSelect all that apply.", "Options": {"A": "For a high-risk, controversial paper, submitting to JFE or JFQA first is a strategically sound approach to get a quick, high-quality signal on its viability.", "B": "For a methodologically 'safe' paper with a high probability of eventual acceptance, submitting to RFS or JF first is a reasonable strategy to maximize journal prestige, as the time-cost risk is lower.", "C": "The median time to acceptance at the Journal of Finance (JF) is approximately twice as long as the median time at the Journal of Financial Economics (JFE).", "D": "The difference in median acceptance time between the slowest journal (JF) and the fastest journal (JFE) is 9.9 months."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to perform a simple calculation, interpret its magnitude, and apply the findings to solve the 'journal ordering problem' under different scenarios. It combines computational judgment and scenario application.\n\nChosen Strategy: Scenario Application. The question asks the user to apply the data to different strategic situations faced by a researcher.\n\nDistractor Logic: All options are designed to be correct to test comprehensive understanding. This is a less common but valid use of the Multiple Response format, requiring the user to validate every single statement against the data and the strategic context provided in the original QA problem. It forces a thorough evaluation of all aspects of the original problem's solution.", "qid": "196", "question": "### Background\n\nAn assistant professor is developing a submission strategy for a new working paper. The professor is facing a fixed tenure deadline and needs to secure acceptances efficiently. The analysis focuses on articles published between 2012 and 2015 in the top five general-interest finance journals.\n\n### Data / Model Specification\n\nThe study's findings for the median acceptance times of these journals are summarized below.\n\n**Table 1. Median Acceptance Times for Top Five General-Interest Finance Journals (2012-2015)**\n\n| Journal | Median Acceptance Time (Months) |\n| :--- | :--- |\n| Journal of Financial Economics (JFE) | 9.9 |\n| Journal of Financial and Quantitative Analysis (JFQA) | 10.6 |\n| Review of Finance (RF) | 11.7 |\n| Review of Financial Studies (RFS) | 15.5 |\n| Journal of Finance (JF) | 19.8 |\n\n---\n\nBased on the data in **Table 1**, which of the following statements are valid conclusions or strategically sound applications of the findings?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the data provided and strategic considerations for journal submission.\n\nLet me analyze each option:\n\n**Option A**: For a high-risk, controversial paper, submitting to JFE or JFQA first for a quick signal\n- JFE (9.9 months) and JFQA (10.6 months) have the shortest median acceptance times\n- For a high-risk paper with uncertain acceptance probability, getting quick feedback is strategically valuable\n- This allows the author to iterate or move to other journals faster if rejected\n- This is a sound strategy\n\n**Option B**: For a methodologically 'safe' paper with high acceptance probability, submitting to RFS or JF first\n- RFS (15.5 months) and JF (19.8 months) have longer acceptance times but are prestigious journals\n- If the paper has high acceptance probability, the time cost is less risky\n- Authors can afford to wait longer for the prestige benefit\n- This is a reasonable strategy\n\n**Option C**: JF median time is approximately twice as long as JFE median time\n- JF: 19.8 months\n- JFE: 9.9 months\n- Ratio: 19.8 ÷ 9.9 = 2.0\n- This is exactly twice as long, so the statement is valid\n\n**Option D**: Difference between slowest (JF) and fastest (JFE) is 9.9 months\n- JF: 19.8 months\n- JFE: 9.9 months\n- Difference: 19.8 - 9.9 = 9.9 months\n- This is mathematically correct\n\nAll four statements are valid conclusions or strategically sound applications of the data.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 198, "Question": "### Background\n\nThe paper investigates whether differences in publication speeds across finance journals are persistent over time, testing two competing hypotheses.\n\n- **Journal Competition Hypothesis:** Predicts that publication times should converge across peer journals due to competitive pressures.\n- **Editorial Differences Hypothesis:** Predicts that publication times can differ persistently due to stable, internal factors like editorial structure and philosophy.\n\n### Data / Model Specification\n\nTo test for persistence, the paper presents median acceptance times for individual years.\n\n**Table 1. Median Acceptance Time by Year for Select Journals (2012-2015)**\n\n| Journal | 2012 | 2013 | 2014 | 2015 |\n| :--- | :--- | :--- | :--- | :--- |\n| Journal of Financial Economics (JFE) | 9.5 | 10.4 | 9.9 | 9.4 |\n| Review of Financial Studies (RFS) | 13.5 | 16.5 | 16.2 | 15.6 |\n| Journal of Finance (JF) | 17.0 | 19.8 | 19.7 | N/A |\n| **Top Twenty Journals (Combined)** | **11.7** | **11.8** | **11.7** | **11.6** |\n\n---\n\nBased on the data in **Table 1** and the descriptions of the competing hypotheses, which of the following statements are supported by the evidence?\n\nSelect all that apply.", "Options": {"A": "The data contradict the Journal Competition Hypothesis, as there is no evidence of convergence in acceptance times between these major competing journals.", "B": "In every year reported, the median acceptance time for JFE was faster than that of RFS.", "C": "The median acceptance time for the top twenty journals as a group is remarkably consistent, varying by only 0.2 months over the entire period.", "D": "The data support the Editorial Differences Hypothesis, as the speed rankings and large time gaps between JFE, RFS, and JF remain stable over the four-year period."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret time-series data to evaluate competing scientific hypotheses. It requires synthesizing the data patterns with the theoretical predictions of each hypothesis.\n\nChosen Strategy: Reverse-Reasoning. The user is given the empirical results (the table) and must identify all the theoretical conclusions and factual observations that are consistent with that evidence.\n\nDistractor Logic: All options are designed to be correct statements derived from the table and the background. This format forces the candidate to evaluate the evidence from multiple angles: its support for one hypothesis, its contradiction of another, a direct observation of the aggregate trend, and a direct comparison between two entities. This ensures a robust and comprehensive test of the candidate's analytical skills.", "qid": "198", "question": "### Background\n\nThe paper investigates whether differences in publication speeds across finance journals are persistent over time, testing two competing hypotheses.\n\n- **Journal Competition Hypothesis:** Predicts that publication times should converge across peer journals due to competitive pressures.\n- **Editorial Differences Hypothesis:** Predicts that publication times can differ persistently due to stable, internal factors like editorial structure and philosophy.\n\n### Data / Model Specification\n\nTo test for persistence, the paper presents median acceptance times for individual years.\n\n**Table 1. Median Acceptance Time by Year for Select Journals (2012-2015)**\n\n| Journal | 2012 | 2013 | 2014 | 2015 |\n| :--- | :--- | :--- | :--- | :--- |\n| Journal of Financial Economics (JFE) | 9.5 | 10.4 | 9.9 | 9.4 |\n| Review of Financial Studies (RFS) | 13.5 | 16.5 | 16.2 | 15.6 |\n| Journal of Finance (JF) | 17.0 | 19.8 | 19.7 | N/A |\n| **Top Twenty Journals (Combined)** | **11.7** | **11.8** | **11.7** | **11.6** |\n\n---\n\nBased on the data in **Table 1** and the descriptions of the competing hypotheses, which of the following statements are supported by the evidence?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each statement:\n\nA) The Journal Competition Hypothesis predicts convergence in publication times. The data shows:\n- JFE: ~9.4-10.4 months (stable)\n- RFS: ~13.5-16.5 months (stable) \n- JF: ~17.0-19.8 months (stable)\nThe gaps between journals remain large and persistent with no convergence. This contradicts the Journal Competition Hypothesis. ✓\n\nB) Checking each year:\n- 2012: JFE (9.5) < RFS (13.5) ✓\n- 2013: JFE (10.4) < RFS (16.5) ✓\n- 2014: JFE (9.9) < RFS (16.2) ✓\n- 2015: JFE (9.4) < RFS (15.6) ✓\nThis is true for every year. ✓\n\nC) Top Twenty Journals combined:\n- 2012: 11.7\n- 2013: 11.8\n- 2014: 11.7\n- 2015: 11.6\nRange = 11.8 - 11.6 = 0.2 months. This is correct. ✓\n\nD) The Editorial Differences Hypothesis predicts persistent differences. The data shows:\n- Speed ranking remains consistent: JFE fastest, RFS middle, JF slowest\n- Large gaps persist (e.g., ~6 months between JFE and RFS, ~3-4 months between RFS and JF)\n- No convergence over time\nThis supports the Editorial Differences Hypothesis. ✓\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 190, "Question": "### Background\n\nIn a one-period pure exchange economy, aggregate demand for `N` risky assets from two investor types (absolute and benchmark-aware) must equal the fixed supply, `S x_M`. This market clearing condition leads to an equilibrium relationship for expected excess returns (`μ - r**1**`).\n\n### Data / Model Specification\n\nThe optimal portfolios for the two investor types are:\n  \nω_{a}=\\frac{1}{π}Ω^{-1}(μ-R{\\bf1}) \\quad \\text{(Eq. (1))}\n \n  \nω_{b}=Φ+\\frac{1}{π}Ω^{-1}(μ-(A/C){\\bf1}) \\quad \\text{(Eq. (2))}\n \nwhere `ω_a` and `ω_b` are the optimal portfolios, `Φ` is the benchmark portfolio, `R` is the risk-free rate, and `A/C` is the expected return on the global minimum variance portfolio.\n\nThe market clearing condition `W_a ω_a + W_b ω_b = S x_M` can be rearranged into:\n  \nμ-r{\\bf1}= H Ω (S x_M - W_b Φ) \\quad \\text{(Eq. (3))}\n \nwhere `H = π / (W_a + W_b)` is aggregate risk aversion and `r = (W_a R + W_b A/C) / (W_a + W_b)` is a composite hurdle rate.\n\n---\n\nBased on the model's structure, which of the following statements are correct descriptions of the equilibrium?\n", "Options": {"A": "If the benchmark portfolio `Φ` were identical to the market portfolio `x_M`, the equilibrium expected excess returns would still be affected by the presence of benchmark investors through the `W_b` term.", "B": "The aggregate risk aversion `H` is independent of the relative wealth of benchmark vs. absolute investors (`W_b / W_a`), depending only on their total wealth (`W_a + W_b`).", "C": "The model collapses to a standard CAPM framework, with the effective zero-beta rate `r` equaling the risk-free rate `R`, if the wealth of benchmark investors is zero (`W_b = 0`).", "D": "In an economy populated only by benchmark investors (`W_a = 0`), the effective zero-beta rate `r` becomes the expected return on the global minimum variance portfolio, `A/C`."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the theoretical model's equilibrium conditions, its limiting cases, and the distinct roles of its parameters. Depth Strategy: Scenario Application. The candidate must evaluate the model under several specific micro-cases (e.g., `W_a=0`, `W_b=0`, `Φ=x_M`) and interpret the definitions of the composite parameters (`r`, `H`). Distractor Logic: This is a difficult item with four correct answers, requiring comprehensive knowledge. A common error would be to assume that if `Φ=x_M`, the benchmark effect vanishes entirely, but Eq. (3) shows the term becomes `(S - W_b)Ωx_M`, so the magnitude of the market risk premium is still affected by `W_b`. Another plausible error is to misunderstand the definition of `H`, thinking it depends on the wealth *distribution* rather than the total.", "qid": "190", "question": "### Background\n\nIn a one-period pure exchange economy, aggregate demand for `N` risky assets from two investor types (absolute and benchmark-aware) must equal the fixed supply, `S x_M`. This market clearing condition leads to an equilibrium relationship for expected excess returns (`μ - r**1**`).\n\n### Data / Model Specification\n\nThe optimal portfolios for the two investor types are:\n  \nω_{a}=\\frac{1}{π}Ω^{-1}(μ-R{\\bf1}) \\quad \\text{(Eq. (1))}\n \n  \nω_{b}=Φ+\\frac{1}{π}Ω^{-1}(μ-(A/C){\\bf1}) \\quad \\text{(Eq. (2))}\n \nwhere `ω_a` and `ω_b` are the optimal portfolios, `Φ` is the benchmark portfolio, `R` is the risk-free rate, and `A/C` is the expected return on the global minimum variance portfolio.\n\nThe market clearing condition `W_a ω_a + W_b ω_b = S x_M` can be rearranged into:\n  \nμ-r{\\bf1}= H Ω (S x_M - W_b Φ) \\quad \\text{(Eq. (3))}\n \nwhere `H = π / (W_a + W_b)` is aggregate risk aversion and `r = (W_a R + W_b A/C) / (W_a + W_b)` is a composite hurdle rate.\n\n---\n\nBased on the model's structure, which of the following statements are correct descriptions of the equilibrium?\n", "model_evaluation_result": {"model_raw_response": "Looking at this equilibrium model, I need to analyze each statement based on the given equations and market clearing conditions.\n\nLet me work through each option:\n\n**Option A**: If Φ = x_M, then equation (3) becomes:\nμ - r·1 = H Ω (S x_M - W_b x_M) = H Ω x_M (S - W_b)\n\nSince S represents total supply and W_b is only part of total wealth, (S - W_b) ≠ 0. The equilibrium is still affected by W_b even when the benchmark equals the market portfolio. This statement is **correct**.\n\n**Option B**: From the definition H = π / (W_a + W_b), we can see that H depends only on the total wealth (W_a + W_b), not on the relative proportions W_b/W_a. This statement is **correct**.\n\n**Option C**: When W_b = 0:\n- r = (W_a R + W_b A/C) / (W_a + W_b) = W_a R / W_a = R\n- The model reduces to μ - R·1 = H Ω (S x_M), which is the standard CAPM form\n\nThis statement is **correct**.\n\n**Option D**: When W_a = 0:\n- r = (W_a R + W_b A/C) / (W_a + W_b) = W_b (A/C) / W_b = A/C\n- The effective zero-beta rate becomes the expected return on the global minimum variance portfolio\n\nThis statement is **correct**.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 231, "Question": "### Background\n\nA firm is designing a hurdle rate ESO for a senior executive characterized by risk aversion `α=7` and excess stock holdings `θ=75%`. The board wants to grant an option package with a target subjective value of approximately $4.95. They are considering two designs:\n\n*   **Design A ('Standard'):** 2-year vesting period, 10% hurdle rate `h`.\n*   **Design B ('Lenient Vesting'):** 1-year vesting period, but with a hurdle rate `h'` adjusted upwards to keep the subjective value at $4.95.\n\n### Data / Model Specification\n\nFor an executive with `α=7, θ=75%`, the following sensitivities apply:\n\n*   **Subjective vesting period delta:** The subjective value increases by **$1.19** for each 1-year *decrease* in the vesting period.\n*   **Subjective hurdle rate delta:** The subjective value decreases by **$0.50** for each 1 percentage point *increase* in the hurdle rate `h`.\n\nThe paper argues that the superior design is the one that delivers the target subjective value to the executive at the lowest objective cost to the firm.\n\n---\n\nBased on the information provided, which of the following statements are correct?\n", "Options": {"A": "To offset the value gained from shortening the vesting period, the hurdle rate in Design B must be set to 12.38%.", "B": "Design B is superior from a shareholder's perspective because it achieves the same incentive effect for the executive at a lower objective cost to the firm.", "C": "The increase in subjective value from shortening the vesting period from 2 years to 1 year is $1.19.", "D": "To keep the subjective value constant, the hurdle rate in Design B must be decreased to 7.62%."}, "Answer": ["A", "B", "C"], "pi_justification": "This item uses a Computational Judgment strategy, requiring a multi-step calculation to evaluate a contract design trade-off. Option C is a direct data extraction. Option A requires calculating the necessary hurdle rate increase: Required value offset = $1.19; Required % increase = $1.19 / $0.50 = 2.38%; New rate = 10% + 2.38% = 12.38%. Option B requires the strategic conclusion that since both shortening the vesting period and increasing the hurdle rate tend to lower the firm's objective cost, Design B is unambiguously cheaper. Option D represents a Sign Error, incorrectly subtracting the required adjustment.", "qid": "231", "question": "### Background\n\nA firm is designing a hurdle rate ESO for a senior executive characterized by risk aversion `α=7` and excess stock holdings `θ=75%`. The board wants to grant an option package with a target subjective value of approximately $4.95. They are considering two designs:\n\n*   **Design A ('Standard'):** 2-year vesting period, 10% hurdle rate `h`.\n*   **Design B ('Lenient Vesting'):** 1-year vesting period, but with a hurdle rate `h'` adjusted upwards to keep the subjective value at $4.95.\n\n### Data / Model Specification\n\nFor an executive with `α=7, θ=75%`, the following sensitivities apply:\n\n*   **Subjective vesting period delta:** The subjective value increases by **$1.19** for each 1-year *decrease* in the vesting period.\n*   **Subjective hurdle rate delta:** The subjective value decreases by **$0.50** for each 1 percentage point *increase* in the hurdle rate `h`.\n\nThe paper argues that the superior design is the one that delivers the target subjective value to the executive at the lowest objective cost to the firm.\n\n---\n\nBased on the information provided, which of the following statements are correct?\n", "model_evaluation_result": {"model_raw_response": "I need to analyze each option based on the given information.\n\nGiven information:\n- Design A: 2-year vesting, 10% hurdle rate, subjective value = $4.95\n- Design B: 1-year vesting, hurdle rate h' to be determined, subjective value must = $4.95\n- Subjective vesting period delta: +$1.19 for each 1-year decrease\n- Subjective hurdle rate delta: -$0.50 for each 1 percentage point increase\n\nLet me evaluate each option:\n\n**Option A**: To offset the value gained from shortening the vesting period, the hurdle rate in Design B must be set to 12.38%.\n- Shortening vesting from 2 years to 1 year (1-year decrease) increases value by $1.19\n- To offset this and keep subjective value at $4.95, we need to decrease value by $1.19\n- Since each 1% increase in hurdle rate decreases value by $0.50:\n- Required hurdle rate increase = $1.19 / $0.50 = 2.38 percentage points\n- New hurdle rate = 10% + 2.38% = 12.38%\n- This is CORRECT.\n\n**Option B**: Design B is superior from a shareholder's perspective because it achieves the same incentive effect for the executive at a lower objective cost to the firm.\n- The problem states \"the superior design is the one that delivers the target subjective value to the executive at the lowest objective cost to the firm\"\n- However, there's no information provided about the objective costs of Design A vs Design B\n- We cannot determine which has lower objective cost\n- This is INCORRECT (insufficient information).\n\n**Option C**: The increase in subjective value from shortening the vesting period from 2 years to 1 year is $1.19.\n- This directly matches the given sensitivity: \"The subjective value increases by $1.19 for each 1-year decrease in the vesting period\"\n- Going from 2 years to 1 year is a 1-year decrease\n- This is CORRECT.\n\n**Option D**: To keep the subjective value constant, the hurdle rate", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 67, "Question": "### Background\nAn economic model analyzes an innovative sector where managers can exert either high effort ($e_i=1$) or low effort ($e_i=\\underline{e}$) in risk prevention. Managers are heterogeneous in their non-monetary cost of effort, $C_i$. The market's perceived probability of a negative shock is denoted by $\\theta_t$. As long as no shocks occur, confidence increases, and $\\theta_t$ decreases.\n\n### Data / Model Specification\nIn a competitive market with **observable effort**, managers capture the full surplus. The expected surplus from a high-effort manager is:\n\n$$ \\alpha_{t}=(1-\\mu\\theta_{t})Y-(1+r) \\quad \\text{(Eq. 1)} $$\n\nThe expected surplus from a low-effort manager is $\\alpha_t - \\theta_t \\Delta Y$. A manager $i$ will choose the action (high effort, low effort, or stay out) that maximizes their personal rent. This leads to a critical confidence threshold, $\\hat{\\theta}$, that separates two distinct market regimes.\n\n- When confidence is low ($\\theta_t \\ge \\hat{\\theta}$), only high-effort managers enter the market.\n- When confidence is high ($\\theta_t < \\hat{\\theta}$), it becomes profitable for some managers to enter and exert low effort.\n\n### Question\nAssume the innovative sector is in the **high-confidence regime** (i.e., $\\theta_t < \\hat{\\theta}$). According to the model, which of the following statements accurately describe the state of the market and its dynamics as confidence continues to grow (i.e., as $\\theta_t$ continues to decrease)? Select all that apply.", "Options": {"A": "The proportion of managers exerting high effort, relative to the total number of managers in the sector, increases.", "B": "The total potential loss, in the event of a negative shock, increases.", "C": "The total size of the innovative sector increases.", "D": "The cross-sectional variance of default probabilities among managers in the sector is zero."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the dynamics within the high-confidence regime ($\\theta_t < \\hat{\\theta}$) of the observable-effort model. It requires applying knowledge of how market composition, size, and systemic risk evolve as confidence grows.\nDepth Strategy: Scenario Application. The question sets a specific scenario (high-confidence regime with growing confidence) and asks for all correct consequences.\nDistractor Logic:\n- A (Correct): As $\\theta_t$ falls, $\\gamma_t$ (the threshold for low-effort entry) increases, expanding the total number of managers, $F(\\gamma_t)$.\n- B (Conceptual Opposite): As $\\theta_t$ falls, $\\beta_t$ (the threshold for high-effort) decreases while $\\gamma_t$ increases. The fraction of high-effort managers, $F(\\beta_t)/F(\\gamma_t)$, therefore *decreases*, representing a decline in standards.\n- C (Correct): The total potential loss increases due to both a scale effect (more projects in total) and a composition effect (a larger fraction of higher-risk, low-effort projects).\n- D (Almost Right): The cross-sectional variance of default probabilities is non-zero in this regime because there are two distinct groups of managers with different failure rates ($\\\theta_t \\mu$ and $\\\theta_t (\\mu+\\Delta)$). The variance is zero only in the low-confidence regime ($\\theta_t \\ge \\hat{\\theta}$).", "qid": "67", "question": "### Background\nAn economic model analyzes an innovative sector where managers can exert either high effort ($e_i=1$) or low effort ($e_i=\\underline{e}$) in risk prevention. Managers are heterogeneous in their non-monetary cost of effort, $C_i$. The market's perceived probability of a negative shock is denoted by $\\theta_t$. As long as no shocks occur, confidence increases, and $\\theta_t$ decreases.\n\n### Data / Model Specification\nIn a competitive market with **observable effort**, managers capture the full surplus. The expected surplus from a high-effort manager is:\n\n$$ \\alpha_{t}=(1-\\mu\\theta_{t})Y-(1+r) \\quad \\text{(Eq. 1)} $$\n\nThe expected surplus from a low-effort manager is $\\alpha_t - \\theta_t \\Delta Y$. A manager $i$ will choose the action (high effort, low effort, or stay out) that maximizes their personal rent. This leads to a critical confidence threshold, $\\hat{\\theta}$, that separates two distinct market regimes.\n\n- When confidence is low ($\\theta_t \\ge \\hat{\\theta}$), only high-effort managers enter the market.\n- When confidence is high ($\\theta_t < \\hat{\\theta}$), it becomes profitable for some managers to enter and exert low effort.\n\n### Question\nAssume the innovative sector is in the **high-confidence regime** (i.e., $\\theta_t < \\hat{\\theta}$). According to the model, which of the following statements accurately describe the state of the market and its dynamics as confidence continues to grow (i.e., as $\\theta_t$ continues to decrease)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this model, I need to analyze what happens in the high-confidence regime when θ_t < θ̂ and continues to decrease.\n\nIn the high-confidence regime (θ_t < θ̂), the model states that \"it becomes profitable for some managers to enter and exert low effort.\" This means we have both high-effort and low-effort managers in the market.\n\nLet me analyze each option:\n\n**Option A**: The proportion of high-effort managers relative to total managers increases as θ_t decreases.\n- As confidence grows (θ_t decreases), the penalty for low effort (θ_t ΔY) becomes smaller\n- This makes low effort more attractive relative to high effort\n- More managers will find it profitable to enter with low effort\n- Therefore, the proportion of high-effort managers should DECREASE, not increase\n- **FALSE**\n\n**Option B**: Total potential loss in case of a negative shock increases.\n- As θ_t decreases, more low-effort managers enter the market\n- Low-effort managers have higher probability of experiencing negative shocks\n- With more low-effort managers and a growing sector overall, the total potential loss increases\n- **TRUE**\n\n**Option C**: The total size of the innovative sector increases.\n- As θ_t decreases, the expected surplus for both types of managers increases\n- This makes entry more attractive for managers who were previously staying out\n- More managers enter the market (both high and low effort types)\n- **TRUE**\n\n**Option D**: The cross-sectional variance of default probabilities is zero.\n- In the high-confidence regime, we have both high-effort and low-effort managers\n- These two types have different probabilities of negative shocks\n- Therefore, there is variation in default probabilities across managers\n- The variance is NOT zero\n- **FALSE**\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 199, "Question": "### Background\n\nThe **Editorial Differences Hypothesis** posits that publication times differ across peer journals due to internal operational factors. The paper identifies three key mechanisms: editorial structure, editorial philosophy, and individual editor speed.\n\n- **Editorial Structure:** The organization of the editorial team (e.g., single editor vs. many delegated editors).\n- **Editorial Philosophy:** The degree to which an editor actively manages the review process. A 'passive' editor forwards all referee requests to the author, while an 'active' editor filters, prioritizes, and resolves contradictory requests to create a clearer path to convergence.\n\n### Scenario\n\nAn 'active' editor at the *Journal of Financial Economics* receives two referee reports on a paper. Referee 1 loves the paper and recommends acceptance after minor revisions. Referee 2 raises a valid but secondary point and suggests a new, time-consuming robustness test that would take 6 months to complete. The editor, after their own reading, believes the new test is not essential for the paper's main conclusion.\n\n---\n\nAccording to the principles of the Editorial Differences Hypothesis, which of the following actions would be characteristic of this 'active' editor's approach, designed to decrease acceptance time?\n\nSelect all that apply.", "Options": {"A": "The editor sends a decision letter asking the author to complete the minor revisions from Referee 1 and to write a brief discussion in the paper acknowledging Referee 2's point and justifying why the additional test is not essential.", "B": "The editor sends both referee reports to the author with a note saying, \"Please address all referee comments.\"", "C": "The editor sends a decision letter asking the author to complete only the minor revisions from Referee 1 and to ignore the time-consuming test from Referee 2.", "D": "The editor makes a final decision on the paper based on their own judgment and the existing referee reports, without requiring another round of revisions."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to apply a theoretical concept ('active' vs. 'passive' editorial philosophy) to a specific, realistic scenario. It tests for a deep understanding of the mechanisms underlying the paper's main hypothesis.\n\nChosen Strategy: Scenario Application. The question provides a micro-case and asks the user to identify all actions consistent with the 'active editor' model described in the paper.\n\nDistractor Logic:\n- **A, B, C (Correct):** These are all plausible actions of an 'active' editor. They involve making a judgment call to resolve conflict or ambiguity, providing a clear path forward, and reducing the author's workload, all of which would decrease acceptance time. Option A is a direct filter. Option B is the most active choice. Option C is a pragmatic compromise.\n- **D (Incorrect):** This is the definition of a 'passive' editor. It is a conceptually opposite distractor that a candidate who understands the active/passive distinction should easily identify as incorrect.", "qid": "199", "question": "### Background\n\nThe **Editorial Differences Hypothesis** posits that publication times differ across peer journals due to internal operational factors. The paper identifies three key mechanisms: editorial structure, editorial philosophy, and individual editor speed.\n\n- **Editorial Structure:** The organization of the editorial team (e.g., single editor vs. many delegated editors).\n- **Editorial Philosophy:** The degree to which an editor actively manages the review process. A 'passive' editor forwards all referee requests to the author, while an 'active' editor filters, prioritizes, and resolves contradictory requests to create a clearer path to convergence.\n\n### Scenario\n\nAn 'active' editor at the *Journal of Financial Economics* receives two referee reports on a paper. Referee 1 loves the paper and recommends acceptance after minor revisions. Referee 2 raises a valid but secondary point and suggests a new, time-consuming robustness test that would take 6 months to complete. The editor, after their own reading, believes the new test is not essential for the paper's main conclusion.\n\n---\n\nAccording to the principles of the Editorial Differences Hypothesis, which of the following actions would be characteristic of this 'active' editor's approach, designed to decrease acceptance time?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which actions align with an \"active\" editor's approach according to the Editorial Differences Hypothesis.\n\nAn \"active\" editor:\n- Filters and prioritizes referee requests\n- Resolves contradictory requests\n- Creates a clearer path to convergence\n- Aims to decrease acceptance time\n\nLet me analyze each option:\n\n**Option A**: The editor asks for minor revisions from Referee 1 and a brief discussion acknowledging Referee 2's point without the 6-month test. This shows active filtering (excluding the time-consuming test) while maintaining scholarly integrity by acknowledging the point. This creates a clear path to convergence.\n\n**Option B**: The editor simply forwards both reports saying \"address all comments.\" This is the definition of a \"passive\" editor approach - no filtering, no prioritization, no resolution of contradictory requests.\n\n**Option C**: The editor asks to complete only Referee 1's revisions and completely ignore Referee 2. While this shows filtering, it's too extreme - completely dismissing a valid point without acknowledgment doesn't align with responsible editorial practice.\n\n**Option D**: Making a final decision without another revision round. While this could decrease time, the scenario indicates revisions are needed (Referee 1 recommends \"acceptance after minor revisions\"). This would be premature.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 233, "Question": "### Background\n\nThe Ingersoll model provides a utility-theoretic framework for the subjective valuation of ESOs. It modifies the standard Black-Scholes-Merton formula by transforming the risk-free rate (`r`) and dividend yield (`q`) to reflect an executive's risk aversion and portfolio constraints.\n\n### Data / Model Specification\n\nThe transformed rates, `r'` and `q'`, are given by:\n\n  \nq^{\\prime} = q + \\alpha(1-\\theta)\\theta\\nu^{2} \\quad \\text{(Eq. (1))}\n \n  \nr^{\\prime} = r - \\alpha\\theta^{2}\\nu^{2} \\quad \\text{(Eq. (2))}\n \n\nwhere `α` is risk aversion, `θ` is excess stock holdings, and `ν` is residual stock risk. For a constrained executive, `α > 0`, `0 < θ < 1`, and `ν > 0`.\n\n---\n\nBased on these equations, which of the following conclusions about the Ingersoll model are valid?\n", "Options": {"A": "The subjective risk-free rate `r'` is lower than the market rate `r`, reflecting a utility cost or drag on wealth growth due to the executive's forced holding of undiversified risk.", "B": "In the boundary case where an executive's wealth is almost entirely concentrated in the firm's stock (`θ` approaches 1), the adjustment to the dividend yield (`q' - q`) is maximized.", "C": "The subjective dividend yield `q'` is higher than the market yield `q`, reflecting a utility benefit from any event (like a dividend) that reduces the executive's painful risk exposure.", "D": "In the boundary case where an executive's portfolio is perfectly diversified (`θ` approaches 0), the subjective rates `r'` and `q'` converge to the market rates `r` and `q`, and the subjective value equals the risk-neutral value."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses the economic intuition and boundary-case analysis of the Ingersoll model, a core theoretical component of the paper. It uses a Reverse-Reasoning strategy, asking for valid conclusions derived from the model's equations. Options A and B correctly state the economic interpretation of the transformed rates. Option C correctly analyzes the `θ → 0` boundary case, where the model collapses to the standard risk-neutral framework. Option D is a subtle distractor based on a misinterpretation of the `(1-θ)θ` term in Eq. (1); this term is maximized at `θ=0.5` and approaches zero as `θ → 1`, meaning the dividend yield adjustment disappears at full concentration.", "qid": "233", "question": "### Background\n\nThe Ingersoll model provides a utility-theoretic framework for the subjective valuation of ESOs. It modifies the standard Black-Scholes-Merton formula by transforming the risk-free rate (`r`) and dividend yield (`q`) to reflect an executive's risk aversion and portfolio constraints.\n\n### Data / Model Specification\n\nThe transformed rates, `r'` and `q'`, are given by:\n\n  \nq^{\\prime} = q + \\alpha(1-\\theta)\\theta\\nu^{2} \\quad \\text{(Eq. (1))}\n \n  \nr^{\\prime} = r - \\alpha\\theta^{2}\\nu^{2} \\quad \\text{(Eq. (2))}\n \n\nwhere `α` is risk aversion, `θ` is excess stock holdings, and `ν` is residual stock risk. For a constrained executive, `α > 0`, `0 < θ < 1`, and `ν > 0`.\n\n---\n\nBased on these equations, which of the following conclusions about the Ingersoll model are valid?\n", "model_evaluation_result": {"model_raw_response": "Looking at the Ingersoll model equations and the constraints given (α > 0, 0 < θ < 1, ν > 0), I'll analyze each option:\n\n**Option A**: From Eq. (2), r' = r - αθ²ν². Since α > 0, θ² > 0, and ν² > 0, we have r' < r. This correctly reflects the utility cost of holding undiversified risk. ✓\n\n**Option B**: From Eq. (1), the adjustment to dividend yield is q' - q = α(1-θ)θν². Taking the derivative with respect to θ: d/dθ[α(1-θ)θν²] = α(1-2θ)ν². This equals zero when θ = 1/2, is positive for θ < 1/2, and negative for θ > 1/2. Therefore, the adjustment is maximized at θ = 1/2, not as θ approaches 1. As θ → 1, the adjustment actually approaches 0. ✗\n\n**Option C**: From Eq. (1), q' - q = α(1-θ)θν². Since α > 0, (1-θ) > 0 (as θ < 1), θ > 0, and ν² > 0, we have q' > q. This correctly reflects the utility benefit from dividends reducing the executive's risk exposure. ✓\n\n**Option D**: As θ → 0:\n- From Eq. (2): r' = r - αθ²ν² → r (since θ² → 0)\n- From Eq. (1): q' = q + α(1-θ)θν² → q (since θ → 0)\n\nWhen both subjective rates converge to market rates, the subjective value equals the risk-neutral Black-Scholes value. ✓\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 8, "Question": "### Background\n\n**Research Question.** This case examines the theoretical requirements for testing a conditional asset pricing model where a factor's price of risk varies over time.\n\n**Setting / Data-Generating Environment.** A researcher proposes a conditional single-factor model where the expected excess return on asset `i` depends on its constant beta (`\\beta_i`) and a time-varying risk premium (`\\lambda_t`):\n  \nE_t[R_{i,t+1}^e] = \\beta_i \\lambda_t\n \nThey further model the risk premium as a linear function of a predictive instrument `Z_t` (e.g., lagged El Niño), which is known at time `t`:\n  \n\\lambda_t = \\gamma_0 + \\gamma_1 Z_t\n \nThis model is typically tested using the Generalized Method of Moments (GMM).\n\n---\n\n### Question\n\nRegarding the specification and testing of this conditional factor model, select all of the following statements that are correct.", "Options": {"A": "The GMM test of this model relies on moment conditions which state that the pricing error is unpredictable using instruments `W_t` from the time `t` information set.", "B": "If the number of GMM moment conditions is greater than the number of parameters, the model is untestable due to underidentification.", "C": "In this model, the term `\\lambda_t` represents the time-varying expected compensation an investor receives for bearing one unit of the factor risk associated with `\\beta_i`.", "D": "The asset's beta (`\\beta_i`) in this model is time-varying and is a linear function of the instrument `Z_t`."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of advanced econometric theory from the source QA. It uses the Premise/Assumption Packaging strategy to test the key concepts of conditional asset pricing models. Option A correctly defines the time-varying risk premium. Option B correctly states the fundamental principle behind GMM testing in this context. Distractor C is a Conceptual Error, confusing a time-varying risk premium with a time-varying beta. Distractor D is a Conceptual Opposite, as more moments than parameters leads to an *overidentified* system that is testable via the J-statistic.", "qid": "8", "question": "### Background\n\n**Research Question.** This case examines the theoretical requirements for testing a conditional asset pricing model where a factor's price of risk varies over time.\n\n**Setting / Data-Generating Environment.** A researcher proposes a conditional single-factor model where the expected excess return on asset `i` depends on its constant beta (`\\beta_i`) and a time-varying risk premium (`\\lambda_t`):\n  \nE_t[R_{i,t+1}^e] = \\beta_i \\lambda_t\n \nThey further model the risk premium as a linear function of a predictive instrument `Z_t` (e.g., lagged El Niño), which is known at time `t`:\n  \n\\lambda_t = \\gamma_0 + \\gamma_1 Z_t\n \nThis model is typically tested using the Generalized Method of Moments (GMM).\n\n---\n\n### Question\n\nRegarding the specification and testing of this conditional factor model, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at each statement carefully:\n\nA) **Correct**. In GMM testing of conditional asset pricing models, the key moment conditions are based on the orthogonality between pricing errors and instruments from the time t information set. Specifically, E[ε_{i,t+1} ⊗ W_t] = 0, where ε_{i,t+1} = R_{i,t+1}^e - β_i λ_t is the pricing error. This states that pricing errors should be unpredictable using any instruments W_t available at time t.\n\nB) **Incorrect**. When the number of moment conditions exceeds the number of parameters, the model is overidentified, not underidentified. This is actually desirable as it allows for testing the model's validity through overidentification tests (like Hansen's J-test). Underidentification occurs when there are fewer moment conditions than parameters.\n\nC) **Correct**. In the model E_t[R_{i,t+1}^e] = β_i λ_t, the term λ_t represents the market price of risk or risk premium at time t. It is the expected excess return per unit of beta risk. An asset with β_i = 1 would have expected excess return of λ_t, so λ_t is indeed the compensation for bearing one unit of factor risk.\n\nD) **Incorrect**. The model explicitly states that β_i is constant (not time-varying). Only the risk premium λ_t varies over time as a function of Z_t. The model specification is E_t[R_{i,t+1}^e] = β_i λ_t with λ_t = γ_0 + γ_1 Z_t, where β_i has no time subscript, indicating it's constant.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 46, "Question": "### Background\n\n**Research Question.** How can one construct a measure of the unexpected component of analysts' earnings forecast revisions that properly accounts for known empirical regularities in forecast data, such as systematic optimism and infrequent updating (staleness)?\n\n**Setting / Data-Generating Environment.** The analysis uses monthly median earnings per share (EPS) forecasts. It is known that analysts tend to be overly optimistic in initial forecasts and update them infrequently, leading to serial correlation in forecast revisions.\n\n**Variables & Parameters.**\n- `F_{i,t}`: Median EPS forecast for firm `i` in month `t`.\n- `P_{i}^{*}`: Price per share for firm `i` six months prior to a rating revision.\n- `FR_{i,t}`: Raw forecast revision for firm `i` in month `t`, scaled by price.\n- `E(FR_{i,t})`: Expected forecast revision for firm `i` in month `t`.\n- `AFR_{i,t}`: Abnormal forecast revision for firm `i` in month `t`.\n\n---\n\n### Data / Model Specification\n\nThe raw forecast revision is defined as:\n  \nFR_{i,t} = \\frac{(F_{i,t} - F_{i,t-1})}{P_{i}^{*}} \\times 100 \\quad \\text{(Eq. (1))}\n \n\nTo model the expected forecast revision, `E(FR_{i,t})`, and correct for serial correlation, a six-month distributed lag model is estimated on a separate random sample of firms:\n  \nFR_{i,t} = -0.093 + 0.099 FR_{i,t-1} + 0.098 FR_{i,t-2} + 0.091 FR_{i,t-3} + 0.076 FR_{i,t-4} + 0.055 FR_{i,t-5} + 0.026 FR_{i,t-6} + u_{i,t} \\quad \\text{(Eq. (2))}\n \n\nThe abnormal forecast revision (`AFR`) is the difference between the raw revision and the expectation from **Eq. (2)**:\n  \nAFR_{i,t} = FR_{i,t} - E(FR_{i,t}) \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the methodology for calculating Abnormal Forecast Revisions (AFR), select all correct statements regarding the model's structure and potential limitations.", "Options": {"A": "The positive coefficients on the six lagged forecast revisions (`FR_{i,t-j}`) are included to correct for the systematic optimism bias.", "B": "The model's negative intercept (-0.093) is intended to capture and correct for a systematic optimism bias, where analysts' initial forecasts tend to be too high.", "C": "The purpose of subtracting the expected forecast revision `E(FR_{i,t})` is to isolate the component of the revision that was predictable based on past trends.", "D": "If this 1984-1990 model were applied to a 2008 recession where analysts became systematically pessimistic (true intercept < -0.093), the model would systematically overstate the negative impact of a downgrade."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). Total Suitability Score = 9.5 (Conceptual Clarity = 9/10, Discriminability = 10/10). The question's core tasks—interpreting model coefficients and analyzing a misspecification scenario—are highly structured with convergent answers. The potential for high-fidelity distractors based on common econometric reasoning errors (e.g., sign errors, confusing which term corrects which bias) is extremely high, making it ideal for a multiple-choice format. This item uses a 'premise packaging' strategy to test the rationale behind the model's specification (Option A) and a 'Scenario Application' strategy to test for model misspecification bias (Option B). The distractors are designed to catch common conceptual errors: Option C confuses the roles of different model components, and Option D misstates the fundamental goal of calculating an 'abnormal' measure.", "qid": "46", "question": "### Background\n\n**Research Question.** How can one construct a measure of the unexpected component of analysts' earnings forecast revisions that properly accounts for known empirical regularities in forecast data, such as systematic optimism and infrequent updating (staleness)?\n\n**Setting / Data-Generating Environment.** The analysis uses monthly median earnings per share (EPS) forecasts. It is known that analysts tend to be overly optimistic in initial forecasts and update them infrequently, leading to serial correlation in forecast revisions.\n\n**Variables & Parameters.**\n- `F_{i,t}`: Median EPS forecast for firm `i` in month `t`.\n- `P_{i}^{*}`: Price per share for firm `i` six months prior to a rating revision.\n- `FR_{i,t}`: Raw forecast revision for firm `i` in month `t`, scaled by price.\n- `E(FR_{i,t})`: Expected forecast revision for firm `i` in month `t`.\n- `AFR_{i,t}`: Abnormal forecast revision for firm `i` in month `t`.\n\n---\n\n### Data / Model Specification\n\nThe raw forecast revision is defined as:\n  \nFR_{i,t} = \\frac{(F_{i,t} - F_{i,t-1})}{P_{i}^{*}} \\times 100 \\quad \\text{(Eq. (1))}\n \n\nTo model the expected forecast revision, `E(FR_{i,t})`, and correct for serial correlation, a six-month distributed lag model is estimated on a separate random sample of firms:\n  \nFR_{i,t} = -0.093 + 0.099 FR_{i,t-1} + 0.098 FR_{i,t-2} + 0.091 FR_{i,t-3} + 0.076 FR_{i,t-4} + 0.055 FR_{i,t-5} + 0.026 FR_{i,t-6} + u_{i,t} \\quad \\text{(Eq. (2))}\n \n\nThe abnormal forecast revision (`AFR`) is the difference between the raw revision and the expectation from **Eq. (2)**:\n  \nAFR_{i,t} = FR_{i,t} - E(FR_{i,t}) \\quad \\text{(Eq. (3))}\n \n\n---\n\n### Question\n\nBased on the methodology for calculating Abnormal Forecast Revisions (AFR), select all correct statements regarding the model's structure and potential limitations.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze each statement about the Abnormal Forecast Revision (AFR) model.\n\n**Statement A**: \"The positive coefficients on the six lagged forecast revisions are included to correct for the systematic optimism bias.\"\n\nThis is incorrect. The positive coefficients on lagged terms capture serial correlation (persistence) in forecast revisions, not systematic optimism bias. Serial correlation means that past revisions help predict future revisions, which is different from a systematic bias.\n\n**Statement B**: \"The model's negative intercept (-0.093) is intended to capture and correct for a systematic optimism bias, where analysts' initial forecasts tend to be too high.\"\n\nThis is correct. A negative intercept indicates that on average, forecast revisions are negative, which is consistent with analysts starting too optimistic and then revising downward. The model captures this systematic tendency.\n\n**Statement C**: \"The purpose of subtracting the expected forecast revision E(FR_{i,t}) is to isolate the component of the revision that was predictable based on past trends.\"\n\nThis is incorrect. The purpose is actually the opposite - subtracting E(FR_{i,t}) removes the predictable component, leaving only the unexpected/abnormal component. AFR represents what remains after removing predictable patterns.\n\n**Statement D**: \"If this 1984-1990 model were applied to a 2008 recession where analysts became systematically pessimistic (true intercept < -0.093), the model would systematically overstate the negative impact of a downgrade.\"\n\nThis is correct. If the true intercept during 2008 was more negative than -0.093 (indicating greater pessimism), but the model still uses -0.093, then:\n- The model would underestimate how negative the expected revision should be\n- This would make E(FR_{i,t}) less negative than it should be\n- Therefore, AFR = FR_{i,t} - E(FR_{i,t}) would be more negative than the true abnormal component\n- This overstates the negative surprise of downgrades\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 194, "Question": "### Background\n\n**Research Question.** This case investigates whether a higher proportion of managing directors (insiders) on the supervisory board impairs firm performance by weakening monitoring effectiveness.\n\n**Setting / Data-Generating Environment.** The study uses a cross-section of 116 Danish listed firms (1998–2001). The Danish “semi-two-tier” system allows managing directors to serve on the supervisory board, though this practice is empirically rare.\n\n---\n\n### Data / Model Specification\n\nThe empirical test is based on the following cross-sectional OLS regression:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot \\text{MANAGERS}_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       | Std. Deviation |\n| :---------- | :--------- | :------------- |\n| MANAGERS (MAN)| 0.0591     | 0.0975         |\n\n**Table 2: Regression of Tobin's Q on Manager Proportion**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Managers               | -0.466      | (0.531)   |\n\n---\n\n### Question\n\nBased on the provided data and regression results, select all correct interpretations of the findings for the `Managers` variable.", "Options": {"A": "The point estimate of the coefficient is so small (-0.466) that it implies a negligible economic effect on firm performance.", "B": "A likely reason for the statistical insignificance is low statistical power, stemming from the fact that the proportion of managers on boards is very low and does not vary much across firms.", "C": "The regression coefficient on `Managers` is statistically insignificant, meaning the data do not provide strong evidence to reject the null hypothesis of no effect.", "D": "The statistically insignificant result definitively proves that having managers on the supervisory board has no effect on firm value."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the nuanced skill of interpreting a statistically insignificant result, particularly the distinction between statistical and economic significance and the role of statistical power. It uses an **Atomic Decomposition** strategy to break the interpretation into testable propositions. The **Reverse-Reasoning** strategy is employed by asking for the cause of the insignificance. Distractors target common fallacies: (A) misinterpreting a failure to reject the null as proof of the null (a classic conceptual error), and (C) misjudging economic significance by looking only at the coefficient's raw value without scaling (a numerical comprehension error).", "qid": "194", "question": "### Background\n\n**Research Question.** This case investigates whether a higher proportion of managing directors (insiders) on the supervisory board impairs firm performance by weakening monitoring effectiveness.\n\n**Setting / Data-Generating Environment.** The study uses a cross-section of 116 Danish listed firms (1998–2001). The Danish “semi-two-tier” system allows managing directors to serve on the supervisory board, though this practice is empirically rare.\n\n---\n\n### Data / Model Specification\n\nThe empirical test is based on the following cross-sectional OLS regression:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot \\text{MANAGERS}_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       | Std. Deviation |\n| :---------- | :--------- | :------------- |\n| MANAGERS (MAN)| 0.0591     | 0.0975         |\n\n**Table 2: Regression of Tobin's Q on Manager Proportion**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Managers               | -0.466      | (0.531)   |\n\n---\n\n### Question\n\nBased on the provided data and regression results, select all correct interpretations of the findings for the `Managers` variable.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each interpretation of the regression results for the `Managers` variable.\n\nGiven information:\n- Coefficient on Managers: -0.466\n- p-value: 0.531 (not statistically significant)\n- Mean of MANAGERS: 0.0591 (5.91%)\n- Std. Deviation of MANAGERS: 0.0975\n\nLet me evaluate each option:\n\n**Option A**: \"The point estimate of the coefficient is so small (-0.466) that it implies a negligible economic effect\"\n- This is incorrect. A coefficient of -0.466 is not necessarily \"small\" without context. Given that MANAGERS has a mean of 0.0591, a one-unit change would represent going from 0% to 100% managers. The coefficient suggests this would decrease Tobin's Q by 0.466, which could be economically meaningful depending on the scale of Tobin's Q.\n\n**Option B**: \"A likely reason for the statistical insignificance is low statistical power, stemming from the fact that the proportion of managers on boards is very low and does not vary much across firms\"\n- This is correct. The mean is 0.0591 (very low) and the standard deviation is 0.0975, indicating limited variation. Low variation in the independent variable reduces statistical power, making it harder to detect effects even if they exist.\n\n**Option C**: \"The regression coefficient on `Managers` is statistically insignificant, meaning the data do not provide strong evidence to reject the null hypothesis of no effect\"\n- This is correct. With p-value = 0.531, we cannot reject the null hypothesis at conventional significance levels. This is the proper interpretation of statistical insignificance.\n\n**Option D**: \"The statistically insignificant result definitively proves that having managers on the supervisory board has no effect on firm value\"\n- This is incorrect. Failure to reject the null hypothesis does not prove the null hypothesis is true. It only means we lack sufficient evidence to reject it.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 249, "Question": "### Background\n\nA key point of contention in the proposed Volvo-Renault merger was the \"Golden Share,\" a special control right retained by the French government in the new entity, Renault-Volvo Automotive (RVA). This provision was a major concern for institutional investors, who felt it undermined Volvo's position as a partner in the venture.\n\n### Data / Model Specification\n\nThe Golden Share provision granted the French government, already the 65% majority owner of RVA, several powers beyond its ordinary shareholding. These included:\n\n1.  The right to limit any single shareholder's stake to a maximum of 20%.\n2.  The right to force Volvo to reduce its 35% stake down to 20% if the holding company structure were ever dissolved.\n\nIn response to investor concerns, the French Prime Minister offered an assurance that the Golden Share would not be used against Volvo *as long as Volvo did not acquire more than 35% of RVA*.\n\nFrom a financial perspective, such control rights can be valued as a portfolio of contingent claims (options) held by the government at the expense of other shareholders, thereby reducing the *ex-ante* value of those shareholders' stake.\n\n---\n\nBased on the description of the Golden Share, select all of the following statements that accurately characterize its economic implications for Volvo's shareholders.\n", "Options": {"A": "The right to force Volvo to sell down its stake from 35% to 20% upon dissolution of the holding company creates a contingent liability for Volvo, similar to Volvo having written a put option to the French government.", "B": "The existence of the Golden Share meant that Volvo's 35% equity stake did not carry pro-rata control rights, giving the deal what one investor called the \"character of a takeover.\"", "C": "The provision to cap any shareholder's stake at 20% is economically equivalent to the French government holding a call option on Volvo's control premium, which truncates the potential upside value of Volvo's investment.", "D": "The French Prime Minister's assurance meaningfully mitigated the governance risk by guaranteeing that Volvo's 35% stake was secure under all future circumstances."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to translate complex legal/governance terms (a 'Golden Share') into their precise economic and financial equivalents, specifically using the concept of embedded options and contingent claims.\nDepth Strategy: Reverse-Reasoning. Given the terms of the Golden Share, the candidate must identify the plausible economic consequences and valuations.\nDistractor Logic:\n- A: Correct. This accurately frames the control cap as a call option held by the government on the value of Volvo's stake above a certain threshold, which limits Volvo's potential gains.\n- B: This is a Conceptual Distractor (Almost Right). The assurance was economically hollow because it only promised not to challenge the 35% stake as long as it remained at 35%, offering no new protection against the other clauses (e.g., the forced sale upon dissolution).\n- C: Correct. This correctly identifies the forced-sale provision as a put option. The government has the right, but not the obligation, to force Volvo to sell its shares under a specific condition (dissolution), which is a classic put structure written by Volvo.\n- D: Correct. This accurately synthesizes the overall impact of the Golden Share. The non-pro-rata rights and the ultimate, non-negotiable control resting with the French state justify the \"takeover\" characterization, as Volvo was a subordinate party.", "qid": "249", "question": "### Background\n\nA key point of contention in the proposed Volvo-Renault merger was the \"Golden Share,\" a special control right retained by the French government in the new entity, Renault-Volvo Automotive (RVA). This provision was a major concern for institutional investors, who felt it undermined Volvo's position as a partner in the venture.\n\n### Data / Model Specification\n\nThe Golden Share provision granted the French government, already the 65% majority owner of RVA, several powers beyond its ordinary shareholding. These included:\n\n1.  The right to limit any single shareholder's stake to a maximum of 20%.\n2.  The right to force Volvo to reduce its 35% stake down to 20% if the holding company structure were ever dissolved.\n\nIn response to investor concerns, the French Prime Minister offered an assurance that the Golden Share would not be used against Volvo *as long as Volvo did not acquire more than 35% of RVA*.\n\nFrom a financial perspective, such control rights can be valued as a portfolio of contingent claims (options) held by the government at the expense of other shareholders, thereby reducing the *ex-ante* value of those shareholders' stake.\n\n---\n\nBased on the description of the Golden Share, select all of the following statements that accurately characterize its economic implications for Volvo's shareholders.\n", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) This is correct. The right to force Volvo to reduce its stake from 35% to 20% upon dissolution creates a contingent obligation for Volvo to sell shares at the government's discretion. This is economically equivalent to Volvo having written a put option to the French government.\n\nB) This is correct. With the French government already holding 65% plus additional Golden Share powers, Volvo's 35% stake did not carry proportional control rights. The asymmetric control structure indeed gave the arrangement characteristics of a takeover rather than a true partnership.\n\nC) This is correct. The 20% cap on any shareholder's stake limits Volvo's ability to increase its ownership and capture additional control premium. This is economically similar to the government holding a call option on the control value above 20%, truncating Volvo's upside potential.\n\nD) This is incorrect. The Prime Minister's assurance only protected Volvo's 35% stake \"as long as Volvo did not acquire more than 35% of RVA.\" This conditional protection did not guarantee security under all circumstances - for example, it wouldn't protect against forced dilution upon dissolution of the holding company structure.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 255, "Question": "### Background\n\n**Research Question.** Is institutional trading in glamour and value stocks driven by superior information about future stock performance?\n\n**Setting / Data-Generating Environment.** At the end of each quarter, stocks are sorted into portfolios based on institutional trading activity. Glamour stocks (lowest book-to-market decile) with positive Trade Ratios (`TR > 0`) form \"bought\" portfolios, and value stocks (highest book-to-market decile) with negative Trade Ratios (`TR < 0`) form \"sold\" portfolios. These portfolios are further sorted into terciles based on the intensity of trading. The study then tracks the subsequent quarterly characteristic-adjusted abnormal returns (`AR`) of these portfolios for four quarters.\n\n### Data / Model Specification\n\nIf institutions are \"smart money,\" their purchases should predict positive abnormal returns, and their sales should predict negative abnormal returns.\n\n**Table 1. Subsequent Quarterly Abnormal Returns (%) of Portfolios Formed on Institutional Trading**\n| Portfolio | AR1 | AR2 | AR3 | AR4 |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A. Glamour Stocks Bought** | | |\n| Most heavily bought group | -0.185 | -1.144*** | -1.468** | -1.616*** |\n| **Panel B. Value Stocks Sold** | | |\n| Most heavily sold group | 3.001*** | 1.186 | 1.523* | 2.325*** |\n\n*Note: ***, **, * denote significance at the 1%, 5%, and 10% levels, respectively.*\n\n---\n\nBased on the data in **Table 1**, which of the following conclusions are supported by the evidence?\n", "Options": {"A": "The glamour stocks that institutions bought most aggressively showed statistically significant underperformance in multiple quarters following the purchase.", "B": "The evidence strongly refutes the hypothesis that institutions' most intense style-level trades (i.e., buying glamour, selling value) are based on superior information about future performance.", "C": "The glamour stocks most heavily bought by institutions immediately and significantly outperformed their benchmarks in the first quarter after being purchased.", "D": "The value stocks that institutions sold most aggressively went on to significantly outperform their benchmarks in the quarter immediately following the sale."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret the key empirical results regarding the informational content of institutional trades. It requires synthesizing data from both panels of the table to form a conclusion about the 'smart money' hypothesis.\nDepth Strategy: Reverse-Reasoning. Given the empirical results, the user must identify all valid conclusions that can be drawn.\nDistractor Logic:\n- A, B, C (Correct): These are direct, accurate interpretations of the data presented in the table. 'A' comes from Panel B (AR1 = 3.001***). 'B' comes from Panel A (AR2, AR3, AR4 are significant and negative). 'C' is the overall conclusion derived from A and B.\n- D (Incorrect): This is a Conceptual Opposite. The AR1 for heavily bought glamour stocks is -0.185 and statistically insignificant, directly contradicting the claim of significant outperformance.", "qid": "255", "question": "### Background\n\n**Research Question.** Is institutional trading in glamour and value stocks driven by superior information about future stock performance?\n\n**Setting / Data-Generating Environment.** At the end of each quarter, stocks are sorted into portfolios based on institutional trading activity. Glamour stocks (lowest book-to-market decile) with positive Trade Ratios (`TR > 0`) form \"bought\" portfolios, and value stocks (highest book-to-market decile) with negative Trade Ratios (`TR < 0`) form \"sold\" portfolios. These portfolios are further sorted into terciles based on the intensity of trading. The study then tracks the subsequent quarterly characteristic-adjusted abnormal returns (`AR`) of these portfolios for four quarters.\n\n### Data / Model Specification\n\nIf institutions are \"smart money,\" their purchases should predict positive abnormal returns, and their sales should predict negative abnormal returns.\n\n**Table 1. Subsequent Quarterly Abnormal Returns (%) of Portfolios Formed on Institutional Trading**\n| Portfolio | AR1 | AR2 | AR3 | AR4 |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A. Glamour Stocks Bought** | | |\n| Most heavily bought group | -0.185 | -1.144*** | -1.468** | -1.616*** |\n| **Panel B. Value Stocks Sold** | | |\n| Most heavily sold group | 3.001*** | 1.186 | 1.523* | 2.325*** |\n\n*Note: ***, **, * denote significance at the 1%, 5%, and 10% levels, respectively.*\n\n---\n\nBased on the data in **Table 1**, which of the following conclusions are supported by the evidence?\n", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each option against the data:\n\n**Option A**: The glamour stocks bought most aggressively (Panel A, Most heavily bought group) show:\n- AR1: -0.185 (not significant)\n- AR2: -1.144*** (significant at 1%)\n- AR3: -1.468** (significant at 5%)\n- AR4: -1.616*** (significant at 1%)\n\nThis shows statistically significant underperformance in quarters 2, 3, and 4. ✓\n\n**Option B**: The \"smart money\" hypothesis suggests institutions' trades should predict future performance positively. However:\n- Glamour stocks they bought heavily underperformed significantly\n- Value stocks they sold heavily outperformed significantly\n\nThis is the opposite of what superior information would predict, strongly refuting the hypothesis. ✓\n\n**Option C**: In the first quarter (AR1), glamour stocks most heavily bought showed -0.185% return, which is negative and not statistically significant. This contradicts the claim of immediate significant outperformance. ✗\n\n**Option D**: Value stocks sold most aggressively (Panel B, Most heavily sold group) show AR1 = 3.001***, which is positive and significant at the 1% level. This means they outperformed in the quarter immediately following the sale. ✓\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 2, "Question": "### Background\n\n**Research Question.** This case examines the satirical conclusion of the paper: whether patently absurd variables, such as celestial phenomena, can be shown to have statistically significant power to predict asset returns, thereby critiquing the standard empirical asset pricing methodology.\n\n**Setting / Data-Generating Environment.** The fundamental theorem of asset pricing states that the absence of arbitrage is equivalent to the existence of a positive stochastic discount factor (SDF), `m_{t+1}`, such that `E_t[m_{t+1} R_{i,t+1}^e] = 0` for any excess return `R_{i,t+1}^e`. This implies that expected returns are determined by the covariance of returns with the SDF: `E_t[R_{i,t+1}^e] = -\\frac{\\operatorname{Cov}_t(m_{t+1}, R_{i,t+1}^e)}{E_t[m_{t+1}]}`. In rational models, the SDF is high in 'bad times' (when marginal utility of consumption is high) and low in 'good times'.\n\n---\n\n### Data / Model Specification\n\nThe paper finds a statistically significant relationship where lagged sunspot activity `X_t` predicts future excess returns `R_{i,t+1}^e`:\n  \nR_{i,t+1}=a+b X_{t}+\\varepsilon_{t+1}\n \n\n---\n\n### Question\n\nFor the observed statistical relationship between sunspots and asset returns to be consistent with a rational, no-arbitrage asset pricing model, which of the following statements must be true or are valid implications? Select all that apply.", "Options": {"A": "A valid SDF, `m_{t+1}`, must be perfectly correlated with the number of sunspots observed at time `t`.", "B": "The absence of arbitrage requires that the expected excess return on any asset predicted by sunspots must be zero.", "C": "Sunspots must be a priced risk factor, meaning sunspot activity is correlated with a state variable that systematically affects investors' marginal utility.", "D": "Given the economic implausibility of sunspots as a priced risk factor, the significant regression results more likely reflect methodological flaws like data mining rather than a true economic relationship."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the deep theoretical critique from the source QA. It uses the Premise/Assumption Packaging strategy to test understanding of the conditions required for a variable to be a genuine priced risk factor within the SDF framework. Options A and B correctly state the two possible interpretations: either sunspots are a real (but implausible) risk factor, or the methodology is flawed. Distractor C is a Conceptual Error, misstating the no-arbitrage condition. Distractor D is a Formula Misuse/Oversimplification, as the theory requires a non-zero covariance, not perfect correlation.", "qid": "2", "question": "### Background\n\n**Research Question.** This case examines the satirical conclusion of the paper: whether patently absurd variables, such as celestial phenomena, can be shown to have statistically significant power to predict asset returns, thereby critiquing the standard empirical asset pricing methodology.\n\n**Setting / Data-Generating Environment.** The fundamental theorem of asset pricing states that the absence of arbitrage is equivalent to the existence of a positive stochastic discount factor (SDF), `m_{t+1}`, such that `E_t[m_{t+1} R_{i,t+1}^e] = 0` for any excess return `R_{i,t+1}^e`. This implies that expected returns are determined by the covariance of returns with the SDF: `E_t[R_{i,t+1}^e] = -\\frac{\\operatorname{Cov}_t(m_{t+1}, R_{i,t+1}^e)}{E_t[m_{t+1}]}`. In rational models, the SDF is high in 'bad times' (when marginal utility of consumption is high) and low in 'good times'.\n\n---\n\n### Data / Model Specification\n\nThe paper finds a statistically significant relationship where lagged sunspot activity `X_t` predicts future excess returns `R_{i,t+1}^e`:\n  \nR_{i,t+1}=a+b X_{t}+\\varepsilon_{t+1}\n \n\n---\n\n### Question\n\nFor the observed statistical relationship between sunspots and asset returns to be consistent with a rational, no-arbitrage asset pricing model, which of the following statements must be true or are valid implications? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze which statements are true or valid implications for sunspots to be consistent with a rational, no-arbitrage asset pricing model.\n\nLet me evaluate each option:\n\n**Option A**: \"A valid SDF, m_{t+1}, must be perfectly correlated with the number of sunspots observed at time t.\"\n\nThis is too strong. The SDF doesn't need to be perfectly correlated with sunspots. For sunspots to predict returns in a no-arbitrage framework, there just needs to be some correlation between sunspots and the SDF (or between sunspots and something that affects the SDF). Perfect correlation is not required.\n\n**Option B**: \"The absence of arbitrage requires that the expected excess return on any asset predicted by sunspots must be zero.\"\n\nThis is incorrect. The no-arbitrage condition states that E_t[m_{t+1} R_{i,t+1}^e] = 0, which actually implies that expected excess returns can be non-zero if they covary with the SDF. The formula given shows: E_t[R_{i,t+1}^e] = -Cov_t(m_{t+1}, R_{i,t+1}^e)/E_t[m_{t+1}]. So non-zero expected returns are perfectly consistent with no-arbitrage.\n\n**Option C**: \"Sunspots must be a priced risk factor, meaning sunspot activity is correlated with a state variable that systematically affects investors' marginal utility.\"\n\nThis is correct. For sunspots to legitimately predict returns in a rational asset pricing model, they would need to be correlated with something that affects the SDF (which represents marginal utility). This is what it means to be a \"priced risk factor.\"\n\n**Option D**: \"Given the economic implausibility of sunspots as a priced risk factor, the significant regression results more likely reflect methodological flaws like data mining rather than a true economic relationship.\"\n\nThis is a valid implication. Since there's no plausible economic mechanism by which sunspots would affect investors' marginal utility or consumption opportunities, finding a significant relationship is more likely due to statistical artifacts (data mining, p-hacking, spurious correlations) than a true economic relationship. This is", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 175, "Question": "### Background\n\nA monopolistic dealer uses two tools—the ask price `a` and the ask depth `z`—to manage risk from informed traders while profiting from liquidity traders. The dealer's expected profit function when facing a risk-neutral informed trader is given below.\n\n**Variables & Parameters.**\n- `a`: The ask price set by the dealer.\n- `z`: The ask quantity limit (depth).\n- `v`: The informed trader's private valuation of the asset.\n- `d(a)`: The demand from the liquidity trader at price `a`.\n- `I[·]`: The indicator function.\n\n---\n\n### Data / Model Specification\n\nThe dealer's expected profit function is:\n  \nE[\\pi]=E\\big[I[v\\leq a](a-v)\\big]E\\big[I[d(a)\\geq0]\\operatorname*{min}(d(a),z)\\big] + E\\big[I[v>a](a-v)\\big]z \\quad \\text{(Eq. (1))}\n \nThe first-order condition with respect to depth `z` is found by setting its partial derivative to zero:\n  \n\\frac{\\partial E[\\pi]}{\\partial z} = E\\big[I[v\\leq a](a-v)\\big] P(d(a)>z) + E\\big[I[v>a](a-v)\\big] = 0 \\quad \\text{(Eq. (2))}\n \nThis can be rearranged to show the optimal balance between the marginal benefit and marginal cost of providing depth:\n`Marginal Benefit = E\\big[I[v\\leq a](a-v)\\big] P(d(a)>z) = -E\\big[I[v>a](a-v)\\big] = Marginal Cost`\n\n---\n\nBased on this framework, which of the following statements are valid interpretations or consequences of the model?", "Options": {"A": "An increase in information asymmetry (e.g., a higher probability that `v > a`) increases the marginal cost of providing depth, leading the dealer to set a lower optimal depth `z*`.", "B": "The marginal benefit of increasing depth `z` is the expected profit from a marginal sale to a liquidity trader, which occurs only if the informed trader is absent and the liquidity trader's demand is large enough to be constrained by the depth limit.", "C": "The depth `z` is a tool of *selection* that filters out informed traders, while the price `a` is a tool of *mitigation* that caps losses on trades that do occur.", "D": "The model implies that depth is a more targeted instrument than price for managing adverse selection because informed traders are distinguished primarily by their desire for large trade sizes."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the user's understanding of the economic trade-offs involved in setting market depth, as formalized by the model's first-order conditions. It tests the interpretation of marginal costs/benefits, comparative statics, and the core economic intuition of the paper. \nDepth Strategy: Reverse-Reasoning. The user is given the formal model and must identify the correct economic interpretations and conclusions that follow from it. \nDistractor Logic: Option C is a Conceptual Opposite. It correctly identifies the two tools (price and depth) and the two mechanisms (selection and mitigation) but incorrectly swaps their roles. Price is the selection tool (filtering who trades), while depth is the mitigation tool (capping losses).", "qid": "175", "question": "### Background\n\nA monopolistic dealer uses two tools—the ask price `a` and the ask depth `z`—to manage risk from informed traders while profiting from liquidity traders. The dealer's expected profit function when facing a risk-neutral informed trader is given below.\n\n**Variables & Parameters.**\n- `a`: The ask price set by the dealer.\n- `z`: The ask quantity limit (depth).\n- `v`: The informed trader's private valuation of the asset.\n- `d(a)`: The demand from the liquidity trader at price `a`.\n- `I[·]`: The indicator function.\n\n---\n\n### Data / Model Specification\n\nThe dealer's expected profit function is:\n  \nE[\\pi]=E\\big[I[v\\leq a](a-v)\\big]E\\big[I[d(a)\\geq0]\\operatorname*{min}(d(a),z)\\big] + E\\big[I[v>a](a-v)\\big]z \\quad \\text{(Eq. (1))}\n \nThe first-order condition with respect to depth `z` is found by setting its partial derivative to zero:\n  \n\\frac{\\partial E[\\pi]}{\\partial z} = E\\big[I[v\\leq a](a-v)\\big] P(d(a)>z) + E\\big[I[v>a](a-v)\\big] = 0 \\quad \\text{(Eq. (2))}\n \nThis can be rearranged to show the optimal balance between the marginal benefit and marginal cost of providing depth:\n`Marginal Benefit = E\\big[I[v\\leq a](a-v)\\big] P(d(a)>z) = -E\\big[I[v>a](a-v)\\big] = Marginal Cost`\n\n---\n\nBased on this framework, which of the following statements are valid interpretations or consequences of the model?", "model_evaluation_result": {"model_raw_response": "Looking at this model, I need to analyze each statement based on the given profit function and first-order condition.\n\nLet me first understand the model structure:\n- When v ≤ a: Informed traders don't trade (no profit for them), only liquidity traders trade\n- When v > a: Informed traders buy the full depth z (they profit from a-v < 0)\n- The FOC shows: Marginal Benefit = E[I[v≤a](a-v)]P(d(a)>z) = -E[I[v>a](a-v)] = Marginal Cost\n\nAnalyzing each option:\n\n**Option A**: \"An increase in information asymmetry (e.g., a higher probability that v > a) increases the marginal cost of providing depth, leading the dealer to set a lower optimal depth z*.\"\n\nThe marginal cost is -E[I[v>a](a-v)]. When v > a, we have (a-v) < 0, so -(a-v) > 0. A higher probability of v > a increases E[I[v>a](a-v)] in magnitude (more negative), which makes -E[I[v>a](a-v)] larger (more positive). This increases the marginal cost. From the FOC, if marginal cost increases, the dealer must reduce z to increase P(d(a)>z) and maintain equality. ✓\n\n**Option B**: \"The marginal benefit of increasing depth z is the expected profit from a marginal sale to a liquidity trader, which occurs only if the informed trader is absent and the liquidity trader's demand is large enough to be constrained by the depth limit.\"\n\nThe marginal benefit is E[I[v≤a](a-v)]P(d(a)>z). This represents profit when:\n- v ≤ a (informed trader absent/doesn't trade)\n- d(a) > z (liquidity demand exceeds depth, so depth constrains trade)\nThis correctly describes when marginal depth generates additional profit. ✓\n\n**Option C**: \"The depth z is a tool of selection that filters out informed traders, while the price a is a tool of mitigation that caps losses on trades that do occur.\"\n\nThis rever", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 240, "Question": "### Background\n\n**Research Question.** How should regulators use stress test results to set capital requirements?\n\n**Setting and Data-Generating Environment.** A bank regulator is assessing a portfolio whose borrower default probabilities match the distribution from a stress test. The regulator must decide if the change in Expected Loss (EL) is a sufficient basis for setting a capital buffer to cover unexpected losses.\n\n---\n\n### Data / Model Specification\n\nThe distribution of firm-level bankruptcy probabilities before and after the stress test is given below:\n\n**Table 1: Distribution of Bankruptcy Probabilities (`p_it`, as %) Before and After Stress**\n| Scenario                | Mean  | P99   |\n|:------------------------|------:|------:|\n| Before stress           | 1.550 | 11.07 |\n| After stress (GAP only) | 1.732 | 12.18 |\n\n---\n\n### The Question\n\nWhy is the change in a portfolio's Expected Loss (EL) an insufficient metric for setting a capital buffer against unexpected losses (e.g., a 99% Value-at-Risk)? Select all valid reasons.", "Options": {"A": "The stress test shows that the 99th percentile (P99) default rate increases significantly, indicating that tail risk measures like VaR will increase more substantially than the EL.", "B": "Expected Loss already includes a buffer for unexpected losses, making additional capital redundant.", "C": "Capital is designed to cover unexpected losses in the tail of the loss distribution, whereas EL only measures the mean of the distribution.", "D": "Expected Loss cannot be calculated after a stress test because the probabilities are uncertain."}, "Answer": ["A", "C"], "pi_justification": "This question assesses the core conceptual distinction between Expected Loss and Unexpected Loss (or tail risk). It targets a key concept from the original QA's risk management application. Options A and B are both correct statements that highlight why EL is insufficient. Distractor C is factually incorrect (EL is directly calculable from the new mean PD). Distractor D is the Conceptual Opposite of the definition of EL.", "qid": "240", "question": "### Background\n\n**Research Question.** How should regulators use stress test results to set capital requirements?\n\n**Setting and Data-Generating Environment.** A bank regulator is assessing a portfolio whose borrower default probabilities match the distribution from a stress test. The regulator must decide if the change in Expected Loss (EL) is a sufficient basis for setting a capital buffer to cover unexpected losses.\n\n---\n\n### Data / Model Specification\n\nThe distribution of firm-level bankruptcy probabilities before and after the stress test is given below:\n\n**Table 1: Distribution of Bankruptcy Probabilities (`p_it`, as %) Before and After Stress**\n| Scenario                | Mean  | P99   |\n|:------------------------|------:|------:|\n| Before stress           | 1.550 | 11.07 |\n| After stress (GAP only) | 1.732 | 12.18 |\n\n---\n\n### The Question\n\nWhy is the change in a portfolio's Expected Loss (EL) an insufficient metric for setting a capital buffer against unexpected losses (e.g., a 99% Value-at-Risk)? Select all valid reasons.", "model_evaluation_result": {"model_raw_response": "Looking at this question about why changes in Expected Loss (EL) are insufficient for setting capital buffers against unexpected losses:\n\nLet me analyze each option:\n\n**Option A**: \"The stress test shows that the 99th percentile (P99) default rate increases significantly, indicating that tail risk measures like VaR will increase more substantially than the EL.\"\n- The data shows P99 increases from 11.07% to 12.18% (10% increase)\n- The mean only increases from 1.550% to 1.732% (11.7% increase)\n- While both increase, the key insight is that P99 represents tail risk, which is what VaR measures\n- Capital buffers are designed to cover these tail risks, not just average losses\n- **CORRECT**\n\n**Option B**: \"Expected Loss already includes a buffer for unexpected losses, making additional capital redundant.\"\n- This is incorrect. EL is simply the mean of the loss distribution\n- EL does not include any buffer for unexpected losses\n- Capital requirements specifically exist to cover unexpected losses beyond EL\n- **INCORRECT**\n\n**Option C**: \"Capital is designed to cover unexpected losses in the tail of the loss distribution, whereas EL only measures the mean of the distribution.\"\n- This is a fundamental principle of bank capital regulation\n- EL = mean of loss distribution\n- Capital buffers cover losses beyond EL, particularly in the tail (like 99% VaR)\n- **CORRECT**\n\n**Option D**: \"Expected Loss cannot be calculated after a stress test because the probabilities are uncertain.\"\n- The table clearly shows we have probability distributions after the stress test\n- We can calculate EL as the mean of these distributions\n- The statement is factually incorrect\n- **INCORRECT**\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 19, "Question": "### Background\n\nA study analyzes how the distribution of power among large shareholders affects the value of family-controlled firms in Spain. The research uses a Heckman two-stage model to control for the fact that firms are not randomly assigned to be family firms.\n\n### Data / Model Specification\n\nThe study defines three mutually exclusive states for the distribution of power in a family firm:\n\n*   `WHOCONTROLS1`: The family is the sole large shareholder.\n*   `WHOCONTROLS2`: Multiple large shareholders exist, but the family's voting rights exceed the combined rights of the next three largest shareholders (`FSH > SSH + TSH + IVSH`).\n*   `WHOCONTROLS3`: Multiple large shareholders exist, and their combined voting rights exceed the family's (`FSH < SSH + TSH + IVSH`).\n\nThe second stage of the Heckman model estimates the effect of these power structures on industry-adjusted firm value (`AVALUE`). The results are presented in the table below, where `WHOCONTROLS2` is the omitted reference category.\n\n**Table 1: Impact of Power Distribution on Family Firm Value**\n\n| Variable         | Coefficient | t-statistic |\n| :--------------- | :---------- | :---------- |\n| `WHOCONTROLS1`   | -0.47       | -2.74       |\n| `WHOCONTROLS3`   | -0.28       | -1.85       |\n*Source: Table 5, Model 3 from the paper. Both coefficients are statistically significant (p<0.10).*\n\n---\n\nBased on these results, which of the following statements are valid interpretations or conclusions?\n", "Options": {"A": "The relationship between the concentration of non-family blockholder power and firm value is linear and positive.", "B": "A state of contested control, where other blockholders' power exceeds the family's (`WHOCONTROLS3`), is valued more highly by the market than a state of sole family ownership (`WHOCONTROLS1`).", "C": "The presence of a single, unchecked family owner (`WHOCONTROLS1`) is associated with the lowest firm valuation among the three states, suggesting high agency costs.", "D": "The market valuation of family firms is highest when the family retains dominant control but coexists with other large shareholders (`WHOCONTROLS2`)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: The user must synthesize regression coefficients from a table where a reference category is omitted to correctly rank the valuation impact of three distinct corporate governance states. This tests the ability to interpret relative effects and understand the non-linear relationship implied by the results.\nDepth Strategy: Computational Judgment. The user must compare the magnitudes of negative coefficients (-0.47 and -0.28) relative to the baseline (0 for WHOCONTROLS2) to establish a rank ordering of firm value across the three states.\nDistractor Logic:\n- A (Correct): Since WHOCONTROLS2 is the reference and the other two states have negative coefficients, WHOCONTROLS2 must correspond to the highest value.\n- B (Correct): The coefficient for WHOCONTROLS1 (-0.47) is the most negative, indicating the lowest valuation.\n- C (Correct): The coefficient for WHOCONTROLS3 (-0.28) is less negative than that for WHOCONTROLS1 (-0.47), meaning firm value is higher in the WHOCONTROLS3 state than in the WHOCONTROLS1 state.\n- D (Distractor - Conceptual Opposite): The results clearly show a non-linear (concave) relationship. Value is lowest at zero blockholder power (WHOCONTROLS1), highest at intermediate power (WHOCONTROLS2), and lower again at high blockholder power (WHOCONTROLS3).", "qid": "19", "question": "### Background\n\nA study analyzes how the distribution of power among large shareholders affects the value of family-controlled firms in Spain. The research uses a Heckman two-stage model to control for the fact that firms are not randomly assigned to be family firms.\n\n### Data / Model Specification\n\nThe study defines three mutually exclusive states for the distribution of power in a family firm:\n\n*   `WHOCONTROLS1`: The family is the sole large shareholder.\n*   `WHOCONTROLS2`: Multiple large shareholders exist, but the family's voting rights exceed the combined rights of the next three largest shareholders (`FSH > SSH + TSH + IVSH`).\n*   `WHOCONTROLS3`: Multiple large shareholders exist, and their combined voting rights exceed the family's (`FSH < SSH + TSH + IVSH`).\n\nThe second stage of the Heckman model estimates the effect of these power structures on industry-adjusted firm value (`AVALUE`). The results are presented in the table below, where `WHOCONTROLS2` is the omitted reference category.\n\n**Table 1: Impact of Power Distribution on Family Firm Value**\n\n| Variable         | Coefficient | t-statistic |\n| :--------------- | :---------- | :---------- |\n| `WHOCONTROLS1`   | -0.47       | -2.74       |\n| `WHOCONTROLS3`   | -0.28       | -1.85       |\n*Source: Table 5, Model 3 from the paper. Both coefficients are statistically significant (p<0.10).*\n\n---\n\nBased on these results, which of the following statements are valid interpretations or conclusions?\n", "model_evaluation_result": {"model_raw_response": "Looking at the regression results where WHOCONTROLS2 is the reference category:\n\n- WHOCONTROLS1 coefficient: -0.47 (significant)\n- WHOCONTROLS3 coefficient: -0.28 (significant)\n\nThis means:\n- WHOCONTROLS1 firms have values 0.47 lower than WHOCONTROLS2 firms\n- WHOCONTROLS3 firms have values 0.28 lower than WHOCONTROLS2 firms\n\nLet me evaluate each option:\n\nA) The relationship between concentration of non-family blockholder power and firm value is linear and positive.\n- FALSE: The relationship is non-linear. As non-family power increases from state 1→2→3, firm value first increases then decreases.\n\nB) Contested control (WHOCONTROLS3) is valued more highly than sole family ownership (WHOCONTROLS1).\n- TRUE: WHOCONTROLS3 has coefficient -0.28 while WHOCONTROLS1 has coefficient -0.47. Since -0.28 > -0.47, WHOCONTROLS3 firms have higher values than WHOCONTROLS1 firms.\n\nC) Single unchecked family owner (WHOCONTROLS1) is associated with the lowest firm valuation among the three states.\n- TRUE: WHOCONTROLS1 has the most negative coefficient (-0.47), making it the lowest valued state.\n\nD) Market valuation is highest when family retains dominant control but coexists with other large shareholders (WHOCONTROLS2).\n- TRUE: As the reference category with both other states having negative coefficients, WHOCONTROLS2 has the highest valuation.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 53, "Question": "### Background\n\nTo test whether the book-to-market (BM) effect is driven by mispricing or risk, a study examines the Fama-French five-factor alphas of HML (High-BM minus Low-BM) portfolios. These portfolios are constructed separately for firms with short, medium, and long product life cycles (PLC).\n\n### Data / Model Specification\n\nThe excess returns of the HML portfolios within each PLC tercile are regressed on the Fama-French five factors:\n\n  \nR_{HML, PLC, t} = \\alpha_{HML, PLC} + \\beta' f_t + \\epsilon_{HML, PLC, t} \\quad \\text{(Eq. (1))}\n \n\nA non-zero alpha ($\\alpha_{HML, PLC}$) indicates that the five-factor model cannot fully explain the returns of the strategy. Table 1 presents these alphas.\n\n**Table 1: Fama-French 5-Factor Alphas (%) for HML Portfolios**\n\n| HML Portfolio | Short PLC | Medium PLC | Long PLC |\n| :--- | :---: | :---: | :---: |\n| **Alpha (%)** | **-0.11** | **0.02** | **0.46** |\n| *t-statistic* | *(-1.82)* | *(0.23)* | *(3.44)* |\n| **Long-Short (Alpha Spread)** | | | **0.57** |\n| *t-statistic* | | | *(4.03)* |\n\n*Source: Table 2, Panel B of the paper. Alphas are in percent per month.* \n\n---\n\nAccording to the results in Table 1, which of the following statements are INVALID interpretations of the evidence? Select all that apply.", "Options": {"A": "The Fama-French five-factor model successfully explains the returns of the HML strategy for firms with long product life cycles.", "B": "For long-PLC firms, the HML portfolio generates a positive and statistically significant alpha, suggesting the five-factor model fails to price this strategy.", "C": "The moderating effect of PLC on the value premium is fully captured by exposures to the five common risk factors.", "D": "The HML strategy applied to short-PLC firms generates a positive, statistically significant abnormal return after adjusting for common risk factors."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret regression alphas and their statistical significance in the context of testing asset pricing models. Depth Strategy: Reverse-Reasoning. The user must identify which conclusions are contradicted by the data. Inversion Protocol: The core assessment objective—identifying the correct interpretation of the long-PLC alpha—yields only one correct answer (D). To meet the multiple-answer requirement, the Single-Correct-Answer Inversion Protocol was applied. The stem was changed to ask for INVALID interpretations, making the original distractors (A, B, C) the new correct answers.", "qid": "53", "question": "### Background\n\nTo test whether the book-to-market (BM) effect is driven by mispricing or risk, a study examines the Fama-French five-factor alphas of HML (High-BM minus Low-BM) portfolios. These portfolios are constructed separately for firms with short, medium, and long product life cycles (PLC).\n\n### Data / Model Specification\n\nThe excess returns of the HML portfolios within each PLC tercile are regressed on the Fama-French five factors:\n\n  \nR_{HML, PLC, t} = \\alpha_{HML, PLC} + \\beta' f_t + \\epsilon_{HML, PLC, t} \\quad \\text{(Eq. (1))}\n \n\nA non-zero alpha ($\\alpha_{HML, PLC}$) indicates that the five-factor model cannot fully explain the returns of the strategy. Table 1 presents these alphas.\n\n**Table 1: Fama-French 5-Factor Alphas (%) for HML Portfolios**\n\n| HML Portfolio | Short PLC | Medium PLC | Long PLC |\n| :--- | :---: | :---: | :---: |\n| **Alpha (%)** | **-0.11** | **0.02** | **0.46** |\n| *t-statistic* | *(-1.82)* | *(0.23)* | *(3.44)* |\n| **Long-Short (Alpha Spread)** | | | **0.57** |\n| *t-statistic* | | | *(4.03)* |\n\n*Source: Table 2, Panel B of the paper. Alphas are in percent per month.* \n\n---\n\nAccording to the results in Table 1, which of the following statements are INVALID interpretations of the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to identify which statements are INVALID interpretations of the evidence.\n\nLet me analyze each option:\n\n**Option A**: \"The Fama-French five-factor model successfully explains the returns of the HML strategy for firms with long product life cycles.\"\n- The table shows Long PLC alpha = 0.46% with t-statistic = 3.44 (highly significant)\n- A significant non-zero alpha means the model FAILS to explain returns\n- This statement is INVALID\n\n**Option B**: \"For long-PLC firms, the HML portfolio generates a positive and statistically significant alpha, suggesting the five-factor model fails to price this strategy.\"\n- Long PLC alpha = 0.46% with t-statistic = 3.44 (significant at conventional levels)\n- This correctly interprets the significant positive alpha as model failure\n- This statement is VALID\n\n**Option C**: \"The moderating effect of PLC on the value premium is fully captured by exposures to the five common risk factors.\"\n- The Long-Short alpha spread = 0.57% with t-statistic = 4.03 (highly significant)\n- This significant difference in alphas across PLC groups shows the five factors do NOT fully capture the PLC effect\n- This statement is INVALID\n\n**Option D**: \"The HML strategy applied to short-PLC firms generates a positive, statistically significant abnormal return after adjusting for common risk factors.\"\n- Short PLC alpha = -0.11% (NEGATIVE, not positive)\n- t-statistic = -1.82 (not significant at conventional levels)\n- This statement is INVALID (wrong sign and not significant)\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 232, "Question": "### Background\n\nThe paper's methodology integrates the Ingersoll model for subjective valuation with the Longstaff-Schwartz Least Squares Monte Carlo (LSM) algorithm. This allows for the calculation of three distinct option values: Risk-Neutral, Subjective, and Objective.\n\n### Data / Model Specification\n\nThese values are defined by combining a stochastic process (how the stock price evolves) with an exercise strategy (the rule for when to exercise):\n\n*   `P_RN`: Risk-neutral stock price process.\n*   `P_SUB`: Subjective stock price process (perceived by the executive).\n*   `S_RN`: Optimal exercise strategy for an unconstrained, value-maximizing investor.\n*   `S_SUB`: Optimal exercise strategy for the constrained executive.\n\nLet `Value(Process, Strategy)` be the value of cash flows from a `Strategy` evaluated under a given `Process`.\n\n1.  **Risk-Neutral Value (`V_RN`):** `Value(P_RN, S_RN)`\n2.  **Subjective Value (`V_SUB`):** `Value(P_SUB, S_SUB)`\n3.  **Objective Value (`V_OBJ`):** `Value(P_RN, S_SUB)`\n\nTypically, a constrained executive exercises earlier (suboptimally from a value-maximization perspective) than an unconstrained investor, so `S_SUB ≠ S_RN`.\n\n---\n\nBased on these definitions, select all of the following statements that are correct.\n", "Options": {"A": "The Objective Value (`V_OBJ`) represents the firm's economic cost and is calculated by valuing the cash flows from the executive's actual (suboptimal) exercise strategy using the market's risk-neutral pricing measure.", "B": "The inequality `V_OBJ ≤ V_RN` holds because the risk-neutral value is, by definition, maximized by the optimal exercise strategy `S_RN`, and any other strategy (`S_SUB`) must result in an equal or lower value under the same pricing measure `P_RN`.", "C": "To compute the Objective Value via LSM, one must simulate stock paths under the subjective process (`P_SUB`) and then use those same paths to determine cash flows and discount them.", "D": "The difference between `V_OBJ` and `V_SUB` arises because the firm and the executive apply different pricing measures (`P_RN` vs. `P_SUB`) to the exact same stream of expected cash flows generated by the executive's strategy `S_SUB`."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses the deep conceptual understanding of the paper's core valuation methodology. It uses a Reverse-Reasoning strategy, asking for the principles that lead to the observed valuation hierarchy. Options A, B, and C correctly define the Objective Value, explain the `V_OBJ ≤ V_RN` relationship, and explain the source of the `V_OBJ > V_SUB` wedge, respectively. These are the three central pillars of the valuation framework. Option D is a critical distractor describing an incorrect algorithmic implementation; the correct method is to determine the strategy with `P_SUB` but value the resulting cash flows along parallel paths simulated with `P_RN`.", "qid": "232", "question": "### Background\n\nThe paper's methodology integrates the Ingersoll model for subjective valuation with the Longstaff-Schwartz Least Squares Monte Carlo (LSM) algorithm. This allows for the calculation of three distinct option values: Risk-Neutral, Subjective, and Objective.\n\n### Data / Model Specification\n\nThese values are defined by combining a stochastic process (how the stock price evolves) with an exercise strategy (the rule for when to exercise):\n\n*   `P_RN`: Risk-neutral stock price process.\n*   `P_SUB`: Subjective stock price process (perceived by the executive).\n*   `S_RN`: Optimal exercise strategy for an unconstrained, value-maximizing investor.\n*   `S_SUB`: Optimal exercise strategy for the constrained executive.\n\nLet `Value(Process, Strategy)` be the value of cash flows from a `Strategy` evaluated under a given `Process`.\n\n1.  **Risk-Neutral Value (`V_RN`):** `Value(P_RN, S_RN)`\n2.  **Subjective Value (`V_SUB`):** `Value(P_SUB, S_SUB)`\n3.  **Objective Value (`V_OBJ`):** `Value(P_RN, S_SUB)`\n\nTypically, a constrained executive exercises earlier (suboptimally from a value-maximization perspective) than an unconstrained investor, so `S_SUB ≠ S_RN`.\n\n---\n\nBased on these definitions, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The Objective Value (`V_OBJ`) represents the firm's economic cost and is calculated by valuing the cash flows from the executive's actual (suboptimal) exercise strategy using the market's risk-neutral pricing measure.\n\nThis is correct. V_OBJ = Value(P_RN, S_SUB) means we're using the executive's actual exercise strategy (S_SUB) but valuing it under the risk-neutral measure (P_RN), which represents the firm's true economic cost.\n\nB) The inequality `V_OBJ ≤ V_RN` holds because the risk-neutral value is, by definition, maximized by the optimal exercise strategy `S_RN`, and any other strategy (`S_SUB`) must result in an equal or lower value under the same pricing measure `P_RN`.\n\nThis is correct. Since V_RN = Value(P_RN, S_RN) uses the optimal strategy S_RN that maximizes value under P_RN, and V_OBJ = Value(P_RN, S_SUB) uses a different (suboptimal) strategy S_SUB, we must have V_OBJ ≤ V_RN when both are evaluated under the same measure P_RN.\n\nC) To compute the Objective Value via LSM, one must simulate stock paths under the subjective process (`P_SUB`) and then use those same paths to determine cash flows and discount them.\n\nThis is incorrect. To compute V_OBJ = Value(P_RN, S_SUB), we need to simulate paths under the risk-neutral process P_RN, not P_SUB. We would need S_SUB (which might be obtained from a separate calculation), but the simulation for valuation must be under P_RN.\n\nD) The difference between `V_OBJ` and `V_SUB` arises because the firm and the executive apply different pricing measures (`P_RN` vs. `P_SUB`) to the exact same stream of expected cash flows generated by the executive's strategy `S_SUB`.\n\nThis is correct. Both V_OBJ and V_SUB use the same exercise strategy S_SUB, so", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 279, "Question": "### Background\n\n**Research Question.** The study investigates whether the effect of exchange rate changes on investment good prices is linear or if it depends on the economic context, specifically the degree of currency misalignment.\n\n**Variables & Parameters.**\n- `\\Delta\\mathrm{PI}_{i m t}`: Percentage change in the investment price index.\n- `(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}})`: Percentage change in the domestic currency cost of imported investment goods.\n- `V_{mt}`: The currency misalignment for country `m` at time `t`, defined as the ratio of the actual exchange rate to its Purchasing Power Parity (PPP) level. A value `V > 1` indicates the domestic currency is undervalued.\n\n---\n\n### Data / Model Specification\n\nTo test for non-linearities, the study specifies a model where the pass-through effect is interacted with the currency misalignment `V_{mt}`. The estimated equation for 'Total Industries' is given below.\n\n  \n\\Delta\\mathrm{PI}_{i m t}=\\delta_{i0}+ 0.1132 \\times \\left[V_{mt} \\times \\left(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}}\\right)\\right]+\\text{controls} + \\text{Fixed Effects} + \\nu_{i m t} \\quad \\text{(Eq. (1))}\n \n\nThe estimated coefficient on the interaction term, 0.1132, has a t-statistic of 4.97.\n\n---\n\n### Question\n\nBased on the non-linear model specification and results in **Eq. (1)**, which of the following conclusions are supported by the evidence?", "Options": {"A": "The pass-through elasticity is greater when the domestic currency is undervalued (e.g., V=1.2) than when it is at its PPP value (V=1.0).", "B": "The pass-through elasticity for 'Total Industries' is constant and equal to 0.1132, regardless of the currency's valuation.", "C": "If the currency is 20% undervalued (`V=1.2`), a 10% currency depreciation is predicted to increase investment good prices by approximately 1.36%.", "D": "A currency depreciation has a smaller impact on investment good prices when the currency is already undervalued, as firms are less sensitive to further price changes."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret and apply a non-linear interaction model, a core skill tested in the original QA problem. It uses a 'Computational Judgment' and 'Atomic Decomposition' strategy. Option A tests the conceptual interpretation of the positive interaction coefficient. Option B requires a precise calculation based on the model. Distractor C is a 'Conceptual Opposite' error. Distractor D is a 'Step-Omission Error', as it incorrectly interprets the coefficient from an interaction model as a constant marginal effect.", "qid": "279", "question": "### Background\n\n**Research Question.** The study investigates whether the effect of exchange rate changes on investment good prices is linear or if it depends on the economic context, specifically the degree of currency misalignment.\n\n**Variables & Parameters.**\n- `\\Delta\\mathrm{PI}_{i m t}`: Percentage change in the investment price index.\n- `(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}})`: Percentage change in the domestic currency cost of imported investment goods.\n- `V_{mt}`: The currency misalignment for country `m` at time `t`, defined as the ratio of the actual exchange rate to its Purchasing Power Parity (PPP) level. A value `V > 1` indicates the domestic currency is undervalued.\n\n---\n\n### Data / Model Specification\n\nTo test for non-linearities, the study specifies a model where the pass-through effect is interacted with the currency misalignment `V_{mt}`. The estimated equation for 'Total Industries' is given below.\n\n  \n\\Delta\\mathrm{PI}_{i m t}=\\delta_{i0}+ 0.1132 \\times \\left[V_{mt} \\times \\left(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}}\\right)\\right]+\\text{controls} + \\text{Fixed Effects} + \\nu_{i m t} \\quad \\text{(Eq. (1))}\n \n\nThe estimated coefficient on the interaction term, 0.1132, has a t-statistic of 4.97.\n\n---\n\n### Question\n\nBased on the non-linear model specification and results in **Eq. (1)**, which of the following conclusions are supported by the evidence?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze the non-linear model specification and determine which conclusions are supported.\n\nThe model shows:\nΔPI_{imt} = δ_{i0} + 0.1132 × [V_{mt} × (Δe_{mt} + Δc_t^f)] + controls + FE + ν_{imt}\n\nThe key insight is that the pass-through elasticity equals 0.1132 × V_{mt}, which varies with the currency misalignment V.\n\nLet me evaluate each option:\n\n**Option A**: When V = 1.2 (undervalued), elasticity = 0.1132 × 1.2 = 0.1358\nWhen V = 1.0 (PPP), elasticity = 0.1132 × 1.0 = 0.1132\nSince 0.1358 > 0.1132, this statement is TRUE.\n\n**Option B**: This claims elasticity is constant at 0.1132. However, the elasticity = 0.1132 × V_{mt}, which varies with V. This is FALSE.\n\n**Option C**: With V = 1.2 and 10% depreciation:\nΔPI = 0.1132 × 1.2 × 10% = 0.1358 × 10% = 1.358% ≈ 1.36%\nThis calculation is correct. TRUE.\n\n**Option D**: This claims smaller impact when undervalued. But since elasticity = 0.1132 × V increases with V, the impact is actually larger when the currency is undervalued (higher V). This is FALSE.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 241, "Question": "### Background\n\n**Research Question.** How do the market prices of local vs. global macroeconomic risks vary over the business cycle, and what are the implications for investors in small, open economies like Finland and Sweden?\n\n**Setting.** This study uses a GMM framework to test if the sensitivity of asset returns to industrial production growth and volatility is state-dependent. The analysis is conducted using both local (country-specific) and global (OECD-wide) production data as risk factors. The state of the business cycle is proxied by the lagged 12-month growth in the relevant industrial production series (`IPL`).\n\n### Data / Model Specification\n\nThe core model for conditional expected stock returns (`r_m`) is:\n\n  \nE[r_{mt} | F_{t-1}] \\propto (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) E[YP_t | F_{t-1}] + (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) E[e_{1t}^2 | F_{t-1}] \\quad \\text{(Eq. 1)}\n \n\nwhere `YP_t` is production growth and `e_{1t}^2` is its volatility. The key hypothesis is that the interaction terms `\\lambda_{1s}` and `\\kappa_{1s}` are non-zero, indicating that the prices of growth and volatility risk are state-dependent. The table below summarizes the key hypothesis test results from the paper for stock returns.\n\n**Table 1: Summary of Hypothesis Tests for State-Dependency of Stock Returns (p-values)**\n\n| Factor Origin | Country | H₀: Price of Growth Risk is Constant (`λ₁s = 0`) | H₀: Price of Volatility Risk is Constant (`κ₁s = 0`) |\n| :--- | :--- | :--- | :--- |\n| **Local** | Finland | 0.009 | 0.000 |\n| **Local** | Sweden | 0.287 | 0.000 |\n| **Global (OECD)** | Finland & Sweden | 0.000 | 0.412 |\n\n*Source: Synthesized from Tables 2, 3, and 4 of the paper. For the Global factor, the test for `κ₁s=0` is for stocks only, while the test for `λ₁s=0` is a joint test across stocks and bonds.*\n\n### Question\n\nBased on the provided model specification and hypothesis test results in Table 1, which of the following conclusions about the state-dependency of risk pricing for *stocks* are statistically supported at the 5% significance level?", "Options": {"A": "The price of local *growth* risk is state-dependent in Finland, but not in Sweden.", "B": "The price of local *growth* risk is state-dependent in both Finland and Sweden.", "C": "The price of local *volatility* risk is state-dependent in both Finland and Sweden.", "D": "The price of global *growth* risk is state-dependent, but the price of global *volatility* risk is not."}, "Answer": ["A", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to accurately interpret statistical evidence from a summary table. It uses the 'atomic decomposition' strategy to break down the complex findings of the original QA into distinct, verifiable propositions. The distractor (D) is designed to catch a common error of overgeneralization, where a finding for one country (Finland) is incorrectly applied to another (Sweden).", "qid": "241", "question": "### Background\n\n**Research Question.** How do the market prices of local vs. global macroeconomic risks vary over the business cycle, and what are the implications for investors in small, open economies like Finland and Sweden?\n\n**Setting.** This study uses a GMM framework to test if the sensitivity of asset returns to industrial production growth and volatility is state-dependent. The analysis is conducted using both local (country-specific) and global (OECD-wide) production data as risk factors. The state of the business cycle is proxied by the lagged 12-month growth in the relevant industrial production series (`IPL`).\n\n### Data / Model Specification\n\nThe core model for conditional expected stock returns (`r_m`) is:\n\n  \nE[r_{mt} | F_{t-1}] \\propto (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) E[YP_t | F_{t-1}] + (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) E[e_{1t}^2 | F_{t-1}] \\quad \\text{(Eq. 1)}\n \n\nwhere `YP_t` is production growth and `e_{1t}^2` is its volatility. The key hypothesis is that the interaction terms `\\lambda_{1s}` and `\\kappa_{1s}` are non-zero, indicating that the prices of growth and volatility risk are state-dependent. The table below summarizes the key hypothesis test results from the paper for stock returns.\n\n**Table 1: Summary of Hypothesis Tests for State-Dependency of Stock Returns (p-values)**\n\n| Factor Origin | Country | H₀: Price of Growth Risk is Constant (`λ₁s = 0`) | H₀: Price of Volatility Risk is Constant (`κ₁s = 0`) |\n| :--- | :--- | :--- | :--- |\n| **Local** | Finland | 0.009 | 0.000 |\n| **Local** | Sweden | 0.287 | 0.000 |\n| **Global (OECD)** | Finland & Sweden | 0.000 | 0.412 |\n\n*Source: Synthesized from Tables 2, 3, and 4 of the paper. For the Global factor, the test for `κ₁s=0` is for stocks only, while the test for `λ₁s=0` is a joint test across stocks and bonds.*\n\n### Question\n\nBased on the provided model specification and hypothesis test results in Table 1, which of the following conclusions about the state-dependency of risk pricing for *stocks* are statistically supported at the 5% significance level?", "model_evaluation_result": {"model_raw_response": "Looking at Table 1 and comparing the p-values to the 5% significance level (0.05):\n\nFor local growth risk (H₀: λ₁s = 0):\n- Finland: p-value = 0.009 < 0.05 → Reject H₀ → State-dependent\n- Sweden: p-value = 0.287 > 0.05 → Fail to reject H₀ → Not state-dependent\n\nFor local volatility risk (H₀: κ₁s = 0):\n- Finland: p-value = 0.000 < 0.05 → Reject H₀ → State-dependent\n- Sweden: p-value = 0.000 < 0.05 → Reject H₀ → State-dependent\n\nFor global growth risk (H₀: λ₁s = 0):\n- Finland & Sweden: p-value = 0.000 < 0.05 → Reject H₀ → State-dependent\n\nFor global volatility risk (H₀: κ₁s = 0):\n- Finland & Sweden: p-value = 0.412 > 0.05 → Fail to reject H₀ → Not state-dependent\n\nTherefore:\n- Option A is correct (local growth risk is state-dependent in Finland but not Sweden)\n- Option C is correct (local volatility risk is state-dependent in both countries)\n- Option D is correct (global growth risk is state-dependent but global volatility risk is not)\n\nA,C,D", "model_answer": ["A", "C", "D"], "error": null}}
{"ID": 209, "Question": "### Background\n\nA proportional hazard model was used to estimate the determinants of default and prepayment for FHA borrowers. The results are presented as exponentiated coefficients (hazard ratios). A hazard ratio above 1 indicates an increased risk relative to a baseline borrower, while a ratio below 1 indicates a decreased risk.\n\n**Baseline Borrower Profile:** A borrower with a current Loan-to-Value (LTV) below 80%, a FICO score above 720, and a Debt-to-Income (DTI) ratio below 28.\n\n### Data / Model Specification\n\n**Table 1: Selected Borrower-Based Hazard Ratio Estimates**\n\n| Variable | Default Hazard Ratio | Prepayment Hazard Ratio |\n| :--- | :--- | :--- |\n| **Loan-to-Value (LTV):** | | |\n| 120 or higher | 3.72 | 0.50 |\n| **Credit Score (FICO):** | | |\n| Less than 580 | 10.82 | 0.43 |\n\n*Note: Coefficients are relative to the baseline borrower profile.*\n\n### Question\n\nA high-risk borrower is defined by having *both* a current LTV of 120% or higher *and* a FICO score below 580. Based on the data in Table 1, which of the following statements accurately describe this borrower's risk profile relative to the baseline borrower?", "Options": {"A": "The borrower's monthly probability of default is more than 40 times higher.", "B": "The borrower's monthly probability of prepaying (a sustainable exit) is less than one-quarter (21.5%) of the baseline.", "C": "The borrower's monthly probability of default is approximately 14.5 times higher.", "D": "The combination of high default risk and low prepayment risk implies a prolonged period of high-risk exposure for the FHA."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to synthesize multiple risk factors from a hazard model to create a composite risk profile. It requires both calculation and conceptual interpretation. Depth Strategy: Computational Judgment and Scenario Application. Distractor Logic: Option A requires multiplying the two default hazard ratios (`3.72 * 10.82 = 40.25`). Option B requires multiplying the two prepayment hazard ratios (`0.50 * 0.43 = 0.215`). Option C requires a correct conceptual interpretation of the combined effect. Option D is a 'Formula Misuse' distractor, created by incorrectly adding the hazard ratios (`3.72 + 10.82 = 14.54`) instead of multiplying them, a common error.", "qid": "209", "question": "### Background\n\nA proportional hazard model was used to estimate the determinants of default and prepayment for FHA borrowers. The results are presented as exponentiated coefficients (hazard ratios). A hazard ratio above 1 indicates an increased risk relative to a baseline borrower, while a ratio below 1 indicates a decreased risk.\n\n**Baseline Borrower Profile:** A borrower with a current Loan-to-Value (LTV) below 80%, a FICO score above 720, and a Debt-to-Income (DTI) ratio below 28.\n\n### Data / Model Specification\n\n**Table 1: Selected Borrower-Based Hazard Ratio Estimates**\n\n| Variable | Default Hazard Ratio | Prepayment Hazard Ratio |\n| :--- | :--- | :--- |\n| **Loan-to-Value (LTV):** | | |\n| 120 or higher | 3.72 | 0.50 |\n| **Credit Score (FICO):** | | |\n| Less than 580 | 10.82 | 0.43 |\n\n*Note: Coefficients are relative to the baseline borrower profile.*\n\n### Question\n\nA high-risk borrower is defined by having *both* a current LTV of 120% or higher *and* a FICO score below 580. Based on the data in Table 1, which of the following statements accurately describe this borrower's risk profile relative to the baseline borrower?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to calculate the hazard ratios for a borrower with both LTV ≥ 120 and FICO < 580.\n\nFor hazard ratios in a proportional hazards model with multiple risk factors:\n- Default hazard ratio = 3.72 × 10.82 = 40.25\n- Prepayment hazard ratio = 0.50 × 0.43 = 0.215\n\nNow checking each option:\n\nA) Default probability more than 40 times higher: 40.25 > 40 ✓\n\nB) Prepayment probability less than one-quarter (21.5%) of baseline: 0.215 = 21.5% ✓\n\nC) Default probability approximately 14.5 times higher: 40.25 ≠ 14.5 ✗\n\nD) High default risk and low prepayment risk implies prolonged high-risk exposure: With 40.25× higher default risk and only 0.215× the prepayment probability, this borrower is much more likely to default and much less likely to exit through prepayment, creating prolonged exposure ✓\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 219, "Question": "### Background\n\nThe paper contrasts two models of international monetary policy cooperation: a theoretical \"Global Planner\" model and a more realistic \"Bargaining Game\" model. The New Keynesian literature, which the paper endorses as a useful framework, assumes the ultimate objective of policy is to maximize the welfare of households.\n\n### Data / Model Specification\n\nThe two views of cooperation can be formalized with distinct objective functions.\n\n**Model 1 (Academic View - Global Planner):** A single planner chooses policies `{ \\pi_i }` for all countries `i=1,...,N` to solve:\n\n  \n\\max_{\\{\\pi_i\\}} \\sum_{i=1}^{N} \\omega_i W_i(\\pi_1, ..., \\pi_N)\n\\quad \\text{(Eq. 1)}\n \nwhere `W_i` is the welfare of country `i` and `\\omega_i` is its weight in the global objective.\n\n**Model 2 (Practice - Nash Bargaining Game):** The negotiated policy outcome `{ \\pi_i }` is the solution to:\n\n  \n\\max_{\\{\\pi_i\\}} \\prod_{i=1}^{N} (W_i(\\pi_1, ..., \\pi_N) - W_i^{Nash})^{\\beta_i}\n\\quad \\text{(Eq. 2)}\n \nwhere `W_i^{Nash}` is the welfare country `i` achieves in the non-cooperative (Nash) equilibrium (the \"disagreement point\"), and `\\beta_i` is its bargaining power.\n\n### Question\n\nBased on the comparison between the Global Planner (Eq. 1) and Nash Bargaining (Eq. 2) models of cooperation, which of the following statements are valid conclusions?\n", "Options": {"A": "In the Nash Bargaining model (Eq. 2), a country that is relatively self-sufficient and suffers little in the non-cooperative equilibrium (i.e., has a high `W_i^{Nash}`) possesses greater bargaining leverage.", "B": "A country's ability to inflict large negative spillovers on other countries in a non-cooperative scenario would decrease its leverage in the bargaining process, as it would be seen as a destabilizing actor.", "C": "The Nash Bargaining model (Eq. 2) inherently guarantees that the cooperative outcome is a Pareto improvement over the non-cooperative (Nash) equilibrium for all participating countries.", "D": "The Global Planner model (Eq. 1) may produce a globally optimal policy set that leaves at least one country worse off than it would be in the non-cooperative (Nash) equilibrium."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses the candidate's ability to interpret and contrast two formal models of policy cooperation. It uses a Reverse-Reasoning strategy, asking for valid conclusions derived from the models' structures. \n- **Correct Option A:** This is correct because the objective in Eq. 2 maximizes the product of gains from cooperation `(W_i - W_i^{Nash})`. Any solution must ensure this term is non-negative for all countries, which is the definition of a Pareto improvement.\n- **Correct Option C:** This is correct because the global planner maximizes a sum, which could involve making one country worse off if the gains to others are sufficiently large to increase the total.\n- **Correct Option D:** This is correct. A high disagreement point `W_i^{Nash}` means the country has a strong outside option and is less desperate for a deal, thus increasing its leverage.\n- **Distractor B:** This is a Conceptual Opposite. The ability to inflict negative spillovers *lowers the disagreement point of other countries*, making them more desperate for a deal and thus *increasing* the threatening country's leverage.", "qid": "219", "question": "### Background\n\nThe paper contrasts two models of international monetary policy cooperation: a theoretical \"Global Planner\" model and a more realistic \"Bargaining Game\" model. The New Keynesian literature, which the paper endorses as a useful framework, assumes the ultimate objective of policy is to maximize the welfare of households.\n\n### Data / Model Specification\n\nThe two views of cooperation can be formalized with distinct objective functions.\n\n**Model 1 (Academic View - Global Planner):** A single planner chooses policies `{ \\pi_i }` for all countries `i=1,...,N` to solve:\n\n  \n\\max_{\\{\\pi_i\\}} \\sum_{i=1}^{N} \\omega_i W_i(\\pi_1, ..., \\pi_N)\n\\quad \\text{(Eq. 1)}\n \nwhere `W_i` is the welfare of country `i` and `\\omega_i` is its weight in the global objective.\n\n**Model 2 (Practice - Nash Bargaining Game):** The negotiated policy outcome `{ \\pi_i }` is the solution to:\n\n  \n\\max_{\\{\\pi_i\\}} \\prod_{i=1}^{N} (W_i(\\pi_1, ..., \\pi_N) - W_i^{Nash})^{\\beta_i}\n\\quad \\text{(Eq. 2)}\n \nwhere `W_i^{Nash}` is the welfare country `i` achieves in the non-cooperative (Nash) equilibrium (the \"disagreement point\"), and `\\beta_i` is its bargaining power.\n\n### Question\n\nBased on the comparison between the Global Planner (Eq. 1) and Nash Bargaining (Eq. 2) models of cooperation, which of the following statements are valid conclusions?\n", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\n**Option A**: In the Nash Bargaining model, the objective is to maximize ∏(W_i - W_i^Nash)^βi. A country with high W_i^Nash (relatively self-sufficient, suffers little in non-cooperation) has a high disagreement point. This means other countries need to offer more to induce its cooperation, giving it greater bargaining leverage. **TRUE**\n\n**Option B**: A country that can inflict large negative spillovers actually has more leverage, not less. Its threat to not cooperate is more credible and damaging to others, forcing them to make greater concessions. This is the opposite of what the statement claims. **FALSE**\n\n**Option C**: The Nash Bargaining solution requires (W_i - W_i^Nash) > 0 for all i, otherwise the product would be zero or undefined. This means every country must achieve welfare above their non-cooperative level, guaranteeing Pareto improvement over the Nash equilibrium. **TRUE**\n\n**Option D**: The Global Planner maximizes weighted global welfare ∑ω_i W_i without any constraint ensuring each country is better off than in non-cooperation. Depending on the weights ω_i, the optimal solution could leave some countries worse off than W_i^Nash. **TRUE**\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 242, "Question": "### Background\n\n**Research Question.** How do the market prices of local vs. global macroeconomic risks vary over the business cycle, and what are the implications for investors in small, open economies like Finland and Sweden?\n\n**Setting.** This study uses a GMM framework to test if the sensitivity of asset returns to industrial production growth and volatility is state-dependent. The paper finds that for Finland, the positive link between expected stock returns and expected local production growth is strongest when the economy is weak. It also finds that higher global production risk lowers bond yields, an effect that is most pronounced during weak global business conditions.\n\n### Data / Model Specification\n\nThe core model for conditional expected stock returns (`r_m`) is:\n\n  \nE[r_{mt} | F_{t-1}] \\propto (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) E[YP_t | F_{t-1}] + (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) E[e_{1t}^2 | F_{t-1}] \\quad \\text{(Eq. 1)}\n \n\nThe table below summarizes key hypothesis test results for stock returns.\n\n**Table 1: Summary of Hypothesis Tests for State-Dependency of Stock Returns (p-values)**\n\n| Factor Origin | Country | H₀: Price of Growth Risk is Constant (`λ₁s = 0`) | H₀: Price of Volatility Risk is Constant (`κ₁s = 0`) |\n| :--- | :--- | :--- | :--- |\n| **Local** | Finland | 0.009 | 0.000 |\n| **Local** | Sweden | 0.287 | 0.000 |\n| **Global (OECD)** | Finland & Sweden | 0.000 | 0.412 |\n\n### Question\n\nBased on the paper's empirical findings and theoretical framework, which of the following interpretations or actions are justified?", "Options": {"A": "A portfolio manager should consider increasing their allocation to Finnish stocks when the local Finnish economy is weak, as this is when the expected return premium for bearing local growth risk is highest.", "B": "The finding that higher global risk lowers bond yields, especially in a downturn, is a classic example of a \"flight to safety\" driven by a precautionary savings motive.", "C": "The results in Table 1 suggest that the price of global volatility risk for stocks is highly sensitive to the state of the business cycle.", "D": "The state-dependency of risk prices is likely driven by investors becoming *less* risk-averse during economic downturns as they seek higher returns."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a 'Scenario Application' strategy to assess whether a user can connect the paper's empirical results to both its theoretical underpinnings and practical investment actions. Correct options (A, B) require synthesizing a finding with its theoretical name or its strategic implication. Distractor (C) is a 'Conceptual Opposite' of the paper's core theory (counter-cyclical risk aversion). Distractor (D) tests for careful reading of the statistical results in the provided table.", "qid": "242", "question": "### Background\n\n**Research Question.** How do the market prices of local vs. global macroeconomic risks vary over the business cycle, and what are the implications for investors in small, open economies like Finland and Sweden?\n\n**Setting.** This study uses a GMM framework to test if the sensitivity of asset returns to industrial production growth and volatility is state-dependent. The paper finds that for Finland, the positive link between expected stock returns and expected local production growth is strongest when the economy is weak. It also finds that higher global production risk lowers bond yields, an effect that is most pronounced during weak global business conditions.\n\n### Data / Model Specification\n\nThe core model for conditional expected stock returns (`r_m`) is:\n\n  \nE[r_{mt} | F_{t-1}] \\propto (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) E[YP_t | F_{t-1}] + (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) E[e_{1t}^2 | F_{t-1}] \\quad \\text{(Eq. 1)}\n \n\nThe table below summarizes key hypothesis test results for stock returns.\n\n**Table 1: Summary of Hypothesis Tests for State-Dependency of Stock Returns (p-values)**\n\n| Factor Origin | Country | H₀: Price of Growth Risk is Constant (`λ₁s = 0`) | H₀: Price of Volatility Risk is Constant (`κ₁s = 0`) |\n| :--- | :--- | :--- | :--- |\n| **Local** | Finland | 0.009 | 0.000 |\n| **Local** | Sweden | 0.287 | 0.000 |\n| **Global (OECD)** | Finland & Sweden | 0.000 | 0.412 |\n\n### Question\n\nBased on the paper's empirical findings and theoretical framework, which of the following interpretations or actions are justified?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each option against the empirical findings and theoretical framework presented.\n\n**Option A:** The paper states that \"for Finland, the positive link between expected stock returns and expected local production growth is strongest when the economy is weak.\" Table 1 shows p-value of 0.009 for Finland's local growth risk state-dependency (rejecting constant price hypothesis). This supports that Finnish stocks offer higher expected returns when bearing local growth risk during weak economic conditions. ✓\n\n**Option B:** The paper explicitly states \"higher global production risk lowers bond yields, an effect that is most pronounced during weak global business conditions.\" This is indeed a classic \"flight to safety\" phenomenon where investors move to safer assets (bonds) during uncertain times, driven by precautionary savings motives. ✓\n\n**Option C:** Table 1 shows p-value of 0.412 for global volatility risk state-dependency, which fails to reject the null hypothesis that the price is constant. This directly contradicts the claim that global volatility risk price is \"highly sensitive\" to business cycle states. ✗\n\n**Option D:** The finding that risk premiums are highest during weak economic conditions suggests investors require MORE compensation for bearing risk during downturns, indicating they become MORE risk-averse, not less. This contradicts standard financial theory and the paper's findings. ✗\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 187, "Question": "### Background\n\nAn actuarial model is developed for the pure risk premium (`x`) per dollar of guaranteed mortgage. The model explicitly accounts for the statistical dependence between the frequency of mortgage defaults (`z`) and the severity of those defaults (`y`).\n\n### Data / Model Specification\n\nThe mean pure risk premium is given by:\n\n  \nE(x) = E(z)E(y) + Cov(z,y) \\quad \\text{(Eq. (1))}\n \n\nThe paper argues that for mortgage portfolios, `Cov(z,y)` is positive, especially during economic downturns, because a single macroeconomic shock (e.g., a recession) drives both default frequency and loss severity upwards simultaneously.\n\n### Question\n\nAccording to the model in **Eq. (1)** and the paper's reasoning, which of the following scenarios would likely exhibit a strongly positive covariance between claim frequency (`z`) and claim severity (`y`), leading to an underestimation of expected losses if the covariance term were ignored? Select all that apply.", "Options": {"A": "A portfolio of mortgage guarantees concentrated in a region heavily dependent on a single industry, such as oil and gas.", "B": "A portfolio of automobile insurance policies spread across a large, stable country with uncorrelated, idiosyncratic accident causes.", "C": "A large, geographically diversified portfolio of individual life insurance policies with fixed death benefits.", "D": "A portfolio of property insurance policies concentrated in a single coastal region prone to hurricanes."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item assesses the ability to generalize the core concept of correlated frequency and severity to novel scenarios. Depth Strategy: Scenario Application. The user must apply the principle from the paper's mortgage model to different types of insurance portfolios. Distractor Logic: Options B and D represent classic examples of insurance portfolios where risks are idiosyncratic and largely uncorrelated, meaning `Cov(z,y)` would be near zero. These are conceptual opposites to the paper's central argument. Options A and C are analogous to the paper's Southwest 'stress case,' where a single systemic risk factor (a hurricane, a regional recession) drives both the number and size of losses, making them correct applications of the concept.", "qid": "187", "question": "### Background\n\nAn actuarial model is developed for the pure risk premium (`x`) per dollar of guaranteed mortgage. The model explicitly accounts for the statistical dependence between the frequency of mortgage defaults (`z`) and the severity of those defaults (`y`).\n\n### Data / Model Specification\n\nThe mean pure risk premium is given by:\n\n  \nE(x) = E(z)E(y) + Cov(z,y) \\quad \\text{(Eq. (1))}\n \n\nThe paper argues that for mortgage portfolios, `Cov(z,y)` is positive, especially during economic downturns, because a single macroeconomic shock (e.g., a recession) drives both default frequency and loss severity upwards simultaneously.\n\n### Question\n\nAccording to the model in **Eq. (1)** and the paper's reasoning, which of the following scenarios would likely exhibit a strongly positive covariance between claim frequency (`z`) and claim severity (`y`), leading to an underestimation of expected losses if the covariance term were ignored? Select all that apply.", "model_evaluation_result": {"model_raw_response": "I need to analyze which scenarios would exhibit strongly positive covariance between claim frequency (z) and claim severity (y).\n\nThe key insight from the background is that positive covariance occurs when a single macroeconomic shock or common factor drives both frequency and severity upward simultaneously.\n\nLet me evaluate each option:\n\nA) A portfolio of mortgage guarantees concentrated in a region heavily dependent on a single industry (oil and gas):\n- If the oil/gas industry suffers, many people lose jobs simultaneously (increasing default frequency)\n- Property values in the region would also decline (increasing loss severity)\n- Both are driven by the same economic shock\n- This exhibits strong positive covariance\n\nB) A portfolio of automobile insurance policies spread across a large, stable country with uncorrelated, idiosyncratic accident causes:\n- Accidents are independent and uncorrelated\n- No common factor driving both frequency and severity\n- This would have near-zero covariance\n\nC) A large, geographically diversified portfolio of individual life insurance policies with fixed death benefits:\n- Deaths are largely independent across individuals\n- Death benefits are fixed (severity doesn't vary)\n- No common factor affecting both frequency and severity\n- This would have near-zero covariance\n\nD) A portfolio of property insurance policies concentrated in a single coastal region prone to hurricanes:\n- A hurricane affects many properties simultaneously (increasing claim frequency)\n- The same hurricane causes severe damage to affected properties (increasing severity)\n- Both frequency and severity are driven by the same event\n- This exhibits strong positive covariance\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 223, "Question": "### Background\nThe behavior of central banks is often modeled using a monetary policy reaction function, or a \"Taylor rule,\" which describes how the policy interest rate is set in response to macroeconomic conditions. A key question is whether a central bank's focus shifts over time. Another critical aspect is the type of data used for estimation; using real-time data available to policymakers at the time of their decisions is considered crucial for accurate inference, as opposed to ex-post revised data which contains information unavailable at the time.\n\n### Data / Model Specification\nThe baseline forward-looking policy rule estimated in this paper is:\n\n  \ni_{t}=c+\\rho i_{t-1}+\\phi_{r}r_{t}+\\phi_{\\pi}E_{t}(\\pi_{t+h_{\\pi}}-\\pi_{t+h_{\\pi}}^{*})+\\phi_{x}E_{t}(x_{t+h_{x}})+\\epsilon_{t} \n \n\nwhere $i_t$ is the nominal policy rate, $i_{t-1}$ is its lag, $r_t$ is a measure of the long-run real interest rate, $E_t(\\pi_{t+h_{\\pi}} - \\pi_{t+h_{\\pi}}^*)$ is the expected future inflation gap, and $E_t(x_{t+h_x})$ is the expected future output gap. The parameter $\\rho$ captures interest rate smoothing, while $\\phi_{\\pi}$ and $\\phi_x$ are the short-run responses to inflation and output gaps, respectively. The long-run response to inflation is $\\frac{\\phi_{\\pi}}{1-\\rho}$. The **Taylor principle** is satisfied if this long-run response is greater than 1.\n\nThe paper estimates this model over the full sample period (1991Q1–2015Q4) using two different datasets. Table 1 below presents key estimation results.\n\n**Table 1: Monetary Policy Reaction Function Estimates (1991Q1 – 2015Q4)**\n| Variables | (1) Baseline (Real-Time Data) | (14) Revised Data |\n| :--- | :--- | :--- |\n| $\\rho$: Lagged Int. Rate | 0.889*** | 0.867*** |\n| | (0.028) | (0.025) |\n| $\\phi_{\\pi}$: Exp. Infl. Gap | 0.304*** | 0.004 |\n| | (0.106) | (0.096) |\n| $\\phi_x$: Exp. Output Gap | 0.098*** | 0.167*** |\n| | (0.036) | (0.049) |\n*Note: Newey-West HAC standard errors in parentheses. *** denotes significance at the 1% level.*\n\nBased on the data provided, which of the following statements are correct interpretations of the estimation results?\n", "Options": {"A": "The analysis using revised data suggests that the Bank of Canada's policy did not satisfy the Taylor principle, as the long-run response to inflation is approximately 0.03.", "B": "The estimation with real-time data finds a statistically insignificant response to the expected output gap, indicating the BoC did not consider the state of the real economy.", "C": "The estimation with revised data finds a statistically insignificant response to the expected inflation gap, suggesting an analysis with this data would incorrectly conclude the BoC was not actively targeting inflation.", "D": "Using real-time data, the calculated long-run response to the expected inflation gap is approximately 2.74, satisfying the Taylor principle."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to perform calculations based on a provided formula, interpret the results in the context of a key economic principle (the Taylor principle), and compare and contrast findings based on different data types (real-time vs. revised). Depth Strategy: Computational Judgment. The user must calculate the long-run response for two scenarios and then evaluate the statistical and economic significance of the findings. Distractor Logic: Option D is a conceptual distractor (Almost Right). While the response to the output gap is smaller in the real-time data model than in the revised data model, it is still highly statistically significant (0.098***), making the statement that the response was insignificant incorrect.", "qid": "223", "question": "### Background\nThe behavior of central banks is often modeled using a monetary policy reaction function, or a \"Taylor rule,\" which describes how the policy interest rate is set in response to macroeconomic conditions. A key question is whether a central bank's focus shifts over time. Another critical aspect is the type of data used for estimation; using real-time data available to policymakers at the time of their decisions is considered crucial for accurate inference, as opposed to ex-post revised data which contains information unavailable at the time.\n\n### Data / Model Specification\nThe baseline forward-looking policy rule estimated in this paper is:\n\n  \ni_{t}=c+\\rho i_{t-1}+\\phi_{r}r_{t}+\\phi_{\\pi}E_{t}(\\pi_{t+h_{\\pi}}-\\pi_{t+h_{\\pi}}^{*})+\\phi_{x}E_{t}(x_{t+h_{x}})+\\epsilon_{t} \n \n\nwhere $i_t$ is the nominal policy rate, $i_{t-1}$ is its lag, $r_t$ is a measure of the long-run real interest rate, $E_t(\\pi_{t+h_{\\pi}} - \\pi_{t+h_{\\pi}}^*)$ is the expected future inflation gap, and $E_t(x_{t+h_x})$ is the expected future output gap. The parameter $\\rho$ captures interest rate smoothing, while $\\phi_{\\pi}$ and $\\phi_x$ are the short-run responses to inflation and output gaps, respectively. The long-run response to inflation is $\\frac{\\phi_{\\pi}}{1-\\rho}$. The **Taylor principle** is satisfied if this long-run response is greater than 1.\n\nThe paper estimates this model over the full sample period (1991Q1–2015Q4) using two different datasets. Table 1 below presents key estimation results.\n\n**Table 1: Monetary Policy Reaction Function Estimates (1991Q1 – 2015Q4)**\n| Variables | (1) Baseline (Real-Time Data) | (14) Revised Data |\n| :--- | :--- | :--- |\n| $\\rho$: Lagged Int. Rate | 0.889*** | 0.867*** |\n| | (0.028) | (0.025) |\n| $\\phi_{\\pi}$: Exp. Infl. Gap | 0.304*** | 0.004 |\n| | (0.106) | (0.096) |\n| $\\phi_x$: Exp. Output Gap | 0.098*** | 0.167*** |\n| | (0.036) | (0.049) |\n*Note: Newey-West HAC standard errors in parentheses. *** denotes significance at the 1% level.*\n\nBased on the data provided, which of the following statements are correct interpretations of the estimation results?\n", "model_evaluation_result": {"model_raw_response": "Looking at the estimation results, I need to calculate the long-run responses and evaluate each statement.\n\nGiven that the long-run response to inflation is $\\frac{\\phi_{\\pi}}{1-\\rho}$:\n\nFor real-time data (column 1):\n- Long-run response = 0.304/(1-0.889) = 0.304/0.111 ≈ 2.74\n\nFor revised data (column 14):\n- Long-run response = 0.004/(1-0.867) = 0.004/0.133 ≈ 0.03\n\nNow evaluating each statement:\n\nA) TRUE - The revised data shows $\\phi_{\\pi} = 0.004$ (not statistically significant), giving a long-run response of approximately 0.03, which is far below 1 and thus violates the Taylor principle.\n\nB) FALSE - The real-time data shows $\\phi_x = 0.098***$, which is statistically significant at the 1% level (indicated by ***).\n\nC) TRUE - The revised data shows $\\phi_{\\pi} = 0.004$ without significance stars, meaning it's statistically insignificant. This would incorrectly suggest the BoC wasn't actively targeting inflation.\n\nD) TRUE - Using real-time data, the long-run response is 0.304/(1-0.889) ≈ 2.74, which is greater than 1 and satisfies the Taylor principle.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 151, "Question": "### Background\n\nThe study uses OLS and Instrumental Variables (IV) regressions to estimate the causal effect of Consumer-Directed Health Plan (CDHP) features on total health care spending. The IV strategy is intended to correct for endogeneity bias stemming from employees' decisions about how much to contribute to their spending accounts.\n\n### Data / Model Specification\n\nThe core regression model is:\n\n  \nTotal\\_Spending_{igt} = \\beta_0 + \\beta_{Account} \\text{Spending Account}_{igt} + \\beta_{Deductible} \\text{Deductible}_{gt} + \\dots + \\eta_g + \\varepsilon_{igt} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Estimated Effects on Total Health Care Spending**\n\n| Regressor        | OLS Estimate   | IV Estimate    |\n| :--------------- | :------------: | :------------: |\n| Spending Account | 1.23** (0.17)  | 0.92 (0.84)    |\n| Deductible       | -0.55** (0.13) | -0.56** (0.13) |\n\n*Notes: Robust standard errors in parentheses. ** significant at 1%. The first-stage partial F-statistic for the IV model is 18.8. The J-test of overidentifying restrictions has a p-value of 0.42.*\n\n---\n\nBased on the results in **Table 1** and the paper's discussion, select all of the following statements that are valid interpretations or conclusions.\n", "Options": {"A": "The IV estimate for the Spending Account coefficient (0.92) is not statistically significant at the 5% level.", "B": "The reported J-test p-value of 0.42 provides strong evidence that the instruments are invalid because they are correlated with the error term.", "C": "A policy change that increases the spending account by $500 and the deductible by $1,000 is predicted by the IV model to decrease total annual spending by $100.", "D": "The fact that the OLS estimate for the Spending Account (1.23) is larger than the IV estimate (0.92) suggests that, on net, adverse selection is a more dominant force than advantageous selection in this sample."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret regression coefficients, standard errors, and the implications of comparing OLS and IV estimates. It also tests the ability to perform a multi-step calculation based on the model. Depth Strategy: Computational Judgment and Reverse-Reasoning. Distractor Logic: Option A is a correct interpretation of statistical significance (t-stat = 0.92/0.84 ≈ 1.095 < 1.96). Option B correctly interprets the direction of the OLS-IV difference as evidence for the direction of net selection bias. Option C requires a correct calculation: (0.92 * 500) + (-0.56 * 1000) = 460 - 560 = -100. Option D is a Conceptual Opposite; a high p-value for the J-test indicates a failure to reject the null of valid instruments, supporting the IV strategy.", "qid": "151", "question": "### Background\n\nThe study uses OLS and Instrumental Variables (IV) regressions to estimate the causal effect of Consumer-Directed Health Plan (CDHP) features on total health care spending. The IV strategy is intended to correct for endogeneity bias stemming from employees' decisions about how much to contribute to their spending accounts.\n\n### Data / Model Specification\n\nThe core regression model is:\n\n  \nTotal\\_Spending_{igt} = \\beta_0 + \\beta_{Account} \\text{Spending Account}_{igt} + \\beta_{Deductible} \\text{Deductible}_{gt} + \\dots + \\eta_g + \\varepsilon_{igt} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Estimated Effects on Total Health Care Spending**\n\n| Regressor        | OLS Estimate   | IV Estimate    |\n| :--------------- | :------------: | :------------: |\n| Spending Account | 1.23** (0.17)  | 0.92 (0.84)    |\n| Deductible       | -0.55** (0.13) | -0.56** (0.13) |\n\n*Notes: Robust standard errors in parentheses. ** significant at 1%. The first-stage partial F-statistic for the IV model is 18.8. The J-test of overidentifying restrictions has a p-value of 0.42.*\n\n---\n\nBased on the results in **Table 1** and the paper's discussion, select all of the following statements that are valid interpretations or conclusions.\n", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) The IV estimate for Spending Account is 0.92 with standard error 0.84. The t-statistic is 0.92/0.84 ≈ 1.10, which is less than 1.96 (the critical value for 5% significance). Therefore, this coefficient is not statistically significant at the 5% level. **TRUE**\n\nB) The J-test p-value of 0.42 is high (well above 0.05 or 0.10). A high p-value means we fail to reject the null hypothesis that the instruments are valid (uncorrelated with the error term). This provides evidence supporting, not against, instrument validity. **FALSE**\n\nC) Using the IV model:\n- Spending account increase of $500: 0.92 × 500 = $460\n- Deductible increase of $1,000: -0.56 × 1,000 = -$560\n- Net effect: $460 - $560 = -$100\nThe prediction is a decrease of $100 in total spending. **TRUE**\n\nD) The OLS estimate (1.23) being larger than the IV estimate (0.92) suggests positive selection bias in OLS. This occurs when people who choose higher spending accounts also have unobserved characteristics that lead to higher healthcare spending (adverse selection). The IV estimate corrects for this bias. **TRUE**\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 30, "Question": "### Background\n\nThe study investigates which specific provisions of the 2017 Tax Cuts and Jobs Act (TCJA) drove an observed increase in corporate investment. The TCJA contained distinct provisions: some primarily benefited multinational corporations (MNCs) by shifting to a territorial tax system, while others, like accelerated depreciation, were available to all U.S. firms.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated separately for purely domestic U.S. firms and for multinational U.S. firms. The dependent variable is either the GAAP effective tax rate (`GAAPETR`) or capital expenditures (`CapEx`). The key coefficient captures the post-TCJA change for U.S. firms relative to Canadian firms.\n\n**Table 1: DiD Results for Domestic vs. Multinational Firms**\n\n| Dependent Variable | Sub-sample    | Coefficient on `USFirm*Yr2019` | t-statistic |\n| :----------------- | :------------ | :------------------------------ | :---------- |\n| `GAAPETR`          | Domestic      | -0.0518***                      | [-3.49]     |\n| `GAAPETR`          | Multinational | -0.0943***                      | [-3.83]     |\n| `CapEx`            | Domestic      | 0.0031***                       | [4.24]      |\n| `CapEx`            | Multinational | -0.0001                         | [-0.07]     |\n\n*Note: Table is constructed from the paper's Table 4.*\n\n---\n\nBased on the results in **Table 1**, select all of the following statements that are valid conclusions.\n", "Options": {"A": "Multinational firms experienced a larger reduction in their GAAP effective tax rate than domestic firms.", "B": "The investment response (change in `CapEx`) was statistically significant for domestic firms.", "C": "The investment response (change in `CapEx`) was statistically significant for multinational firms.", "D": "The results suggest that the shift to a territorial tax system was the primary driver of the aggregate increase in investment."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: The item assesses the ability to synthesize results from multiple regressions to draw a higher-order conclusion about policy mechanisms. It tests the core finding that domestic-facing provisions, not international ones, drove the investment increase.\nDepth Strategy: Reverse-Reasoning. Given the empirical results, the user must identify the most plausible causal mechanism.\nDistractor Logic:\n- C (Conceptual Opposite): This is directly contradicted by the t-statistic of -0.07 for multinational firms' `CapEx`.\n- D (Conceptual Opposite): This is the key incorrect inference. The group that benefited most from the territorial system (MNCs) did not increase investment, which refutes this conclusion. This is a common misinterpretation error.", "qid": "30", "question": "### Background\n\nThe study investigates which specific provisions of the 2017 Tax Cuts and Jobs Act (TCJA) drove an observed increase in corporate investment. The TCJA contained distinct provisions: some primarily benefited multinational corporations (MNCs) by shifting to a territorial tax system, while others, like accelerated depreciation, were available to all U.S. firms.\n\n### Data / Model Specification\n\nA difference-in-differences (DiD) model is estimated separately for purely domestic U.S. firms and for multinational U.S. firms. The dependent variable is either the GAAP effective tax rate (`GAAPETR`) or capital expenditures (`CapEx`). The key coefficient captures the post-TCJA change for U.S. firms relative to Canadian firms.\n\n**Table 1: DiD Results for Domestic vs. Multinational Firms**\n\n| Dependent Variable | Sub-sample    | Coefficient on `USFirm*Yr2019` | t-statistic |\n| :----------------- | :------------ | :------------------------------ | :---------- |\n| `GAAPETR`          | Domestic      | -0.0518***                      | [-3.49]     |\n| `GAAPETR`          | Multinational | -0.0943***                      | [-3.83]     |\n| `CapEx`            | Domestic      | 0.0031***                       | [4.24]      |\n| `CapEx`            | Multinational | -0.0001                         | [-0.07]     |\n\n*Note: Table is constructed from the paper's Table 4.*\n\n---\n\nBased on the results in **Table 1**, select all of the following statements that are valid conclusions.\n", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each statement against the empirical results:\n\n**Statement A**: \"Multinational firms experienced a larger reduction in their GAAP effective tax rate than domestic firms.\"\n- Domestic firms: -0.0518 reduction in GAAPETR\n- Multinational firms: -0.0943 reduction in GAAPETR\n- Since -0.0943 is a larger reduction than -0.0518, this statement is TRUE.\n\n**Statement B**: \"The investment response (change in CapEx) was statistically significant for domestic firms.\"\n- Domestic firms CapEx coefficient: 0.0031***\n- The three asterisks indicate statistical significance at the 1% level\n- This statement is TRUE.\n\n**Statement C**: \"The investment response (change in CapEx) was statistically significant for multinational firms.\"\n- Multinational firms CapEx coefficient: -0.0001\n- No asterisks and t-statistic of [-0.07] indicates no statistical significance\n- This statement is FALSE.\n\n**Statement D**: \"The results suggest that the shift to a territorial tax system was the primary driver of the aggregate increase in investment.\"\n- The territorial tax system primarily benefited multinational firms\n- However, only domestic firms showed a significant increase in investment (CapEx)\n- Multinational firms showed no significant investment response despite larger tax rate reductions\n- This suggests provisions available to all firms (like accelerated depreciation) drove investment increases, not the territorial system\n- This statement is FALSE.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 246, "Question": "### Background\n\n**Research Question.** Under what technical conditions does an optimal reinsurance premium exist as the solution to a joint loss-minimization problem, and what first-order condition must this premium satisfy?\n\n**Setting.** An optimal reinsurance premium, `c*`, is sought to minimize a joint expected loss function for an insurer and a reinsurer.\n\n### Data / Model Specification\n\nThe joint expected loss function is:\n  \nf_{1}(c)=\\mathbb{E}[g(X)\\varphi_{1}((I(X)-c)_{+})]+\\mathbb{E}[h(X)\\varphi_{2}(X-I(X)+c)] \n \nwhere `φ_1` and `φ_2` are quantifying functions for the reinsurer's and insurer's risks, respectively. The optimal premium `c*` minimizes `f_1(c)`. If `φ_1` and `φ_2` are differentiable and `φ_1'(0) = 0`, the optimal premium `c*` is the solution to the first-order condition (FOC):\n  \n\\mathbb{E}[h(X)\\varphi_{2}^{\\prime}(X-I(X)+c)]=\\mathbb{E}[g(X)\\varphi_{1}^{\\prime}((I(X)-c)_{+})] \\quad \\text{(Eq. 1)}\n \n\n### Question\n\nAccording to the paper's framework, which of the following are valid interpretations or consequences of the first-order condition in Eq. (1) and its underlying assumptions? Select all that apply.", "Options": {"A": "If the insurer's quantifying function `φ_2` were non-convex (e.g., S-shaped as in Prospect Theory), the objective function `f_1(c)` would remain convex, ensuring a unique optimal premium.", "B": "The condition `φ_1'(0) = 0` implies that if the premium `c` is already sufficient to cover the ceded loss `I(X)`, the reinsurer gains no marginal risk-reduction benefit from a small additional increase in `c`.", "C": "The right-hand side of Eq. (1) represents the reinsurer's expected marginal benefit (i.e., reduction in disutility) from an incremental increase in the premium `c`.", "D": "The left-hand side of Eq. (1) represents the insurer's expected marginal disutility from an incremental increase in the premium `c`."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: Assess understanding of the economic interpretation of the central first-order condition (FOC) derived in the paper. Strategy: Reverse-Reasoning (interpreting the components of a given equation). Distractor Logic: Option D is a Conceptual Opposite. The paper's proof of a unique optimum relies on the convexity of `φ_1` and `φ_2`. Violating this assumption for `φ_2` would break the convexity of the overall objective function `f_1(c)`, potentially leading to multiple local minima, not a unique optimum. Options A, B, and C are correct interpretations of the FOC as an economic equilibrium, as explained in the original QA's detailed answer.", "qid": "246", "question": "### Background\n\n**Research Question.** Under what technical conditions does an optimal reinsurance premium exist as the solution to a joint loss-minimization problem, and what first-order condition must this premium satisfy?\n\n**Setting.** An optimal reinsurance premium, `c*`, is sought to minimize a joint expected loss function for an insurer and a reinsurer.\n\n### Data / Model Specification\n\nThe joint expected loss function is:\n  \nf_{1}(c)=\\mathbb{E}[g(X)\\varphi_{1}((I(X)-c)_{+})]+\\mathbb{E}[h(X)\\varphi_{2}(X-I(X)+c)] \n \nwhere `φ_1` and `φ_2` are quantifying functions for the reinsurer's and insurer's risks, respectively. The optimal premium `c*` minimizes `f_1(c)`. If `φ_1` and `φ_2` are differentiable and `φ_1'(0) = 0`, the optimal premium `c*` is the solution to the first-order condition (FOC):\n  \n\\mathbb{E}[h(X)\\varphi_{2}^{\\prime}(X-I(X)+c)]=\\mathbb{E}[g(X)\\varphi_{1}^{\\prime}((I(X)-c)_{+})] \\quad \\text{(Eq. 1)}\n \n\n### Question\n\nAccording to the paper's framework, which of the following are valid interpretations or consequences of the first-order condition in Eq. (1) and its underlying assumptions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each option against the first-order condition (FOC) and the model framework.\n\nThe FOC is:\n$$\\mathbb{E}[h(X)\\varphi_{2}^{\\prime}(X-I(X)+c)]=\\mathbb{E}[g(X)\\varphi_{1}^{\\prime}((I(X)-c)_{+})]$$\n\nLet me examine each option:\n\n**Option A**: Claims that if φ₂ is non-convex (S-shaped), f₁(c) would remain convex ensuring a unique optimal premium.\n\nThis is false. The convexity of f₁(c) depends on the convexity of both φ₁ and φ₂. If φ₂ is non-convex (like an S-shaped function from Prospect Theory), the second term of f₁(c) would not necessarily be convex, which could make f₁(c) non-convex overall. A non-convex objective function can have multiple local minima, so uniqueness would not be guaranteed.\n\n**Option B**: States that φ₁'(0) = 0 implies that if c ≥ I(X), the reinsurer gains no marginal risk-reduction benefit from increasing c.\n\nThis is true. When c ≥ I(X), we have (I(X) - c)₊ = 0. Given φ₁'(0) = 0, this means φ₁'((I(X) - c)₊) = 0, so the reinsurer's marginal benefit (right-hand side of FOC) is zero for those states where the premium already covers the ceded loss.\n\n**Option C**: Claims the right-hand side represents the reinsurer's expected marginal benefit from increasing c.\n\nThis is true. Taking the derivative of the reinsurer's loss function with respect to c:\n- When I(X) > c: ∂/∂c[(I(X) - c)] = -1\n- When I(X) ≤ c: ∂/∂c[0] = 0\n\nThe negative of this gives the marginal", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 160, "Question": "### Background\n\nThis paper examines the structure and performance of Canadian stock exchanges in the late 1950s, contrasting them with their U.S. counterparts. A key feature of the Canadian market is the prevalence of speculative, low-priced mining and oil stocks, which are traded alongside more conventional industrial stocks. This analysis investigates how structural differences manifest in observable market outcomes like trading volume, share turnover, and price volatility.\n\n### Data / Model Specification\n\nThe analysis uses data from 1957. The following tables provide key statistics on share turnover and price volatility for samples of stocks listed on the Toronto Stock Exchange (TSE).\n\n**Table 1: Percentage Distributions of Combined Turnovers for 114 Toronto Listings During 1957**\n\n| SHARE TURNOVER (PER CENT) | Industrial | Oil   | Mining |\n|:--------------------------|:-----------|:------|:-------|\n| 0-5                       | 54.0       | 12.5  | 3.6    |\n| 5-10                      | 30.0       | 25.0  | 12.5   |\n| 10-20                     | 10.0       | 12.5  | 26.7   |\n| 20-30                     | 4.0        |       | 12.5   |\n| 30-50                     | 2.0        | 25.0  | 17.9   |\n| 50-70                     |            | 25.0  | 10.7   |\n| 70-100                    |            |       | 3.6    |\n| 100 and over              |            |       | 12.5   |\n| **Total**                 | **100.0**  | **100.0** | **100.0**|\n| **No. of Issues**         | **50**     | **8**     | **56**     |\n\n**Table 2: Ratios of Yearly High to Low Prices in 1957 for 114 Toronto Listings**\n\n| SHARE PRICE   | 1.0-1.5 | 1.5-2.0 | 2.0-3.0 | 3.0 and Over | TOTAL ISSUES |\n|:--------------|:--------|:--------|:--------|:-------------|:-------------|\n| **All industrials** | 32      | 11      | 3       | 4            | 50           |\n| **All mining and oil** | 3       | 8       | 21      | 32           | 64           |\n| **Totals**    | **35**  | **19**  | **24**  | **36**       | **114**      |\n\nBased on a quantitative analysis of the provided data, which of the following statements accurately characterize the Toronto Stock Exchange in 1957? Select all that apply.", "Options": {"A": "More than half of all mining and oil stocks had a share turnover of 30% or greater.", "B": "Fewer than one in five industrial stocks experienced high price volatility, defined as a yearly high-to-low price ratio of 2.0 or greater.", "C": "The vast majority (over 80%) of industrial stocks were relatively illiquid, exhibiting a share turnover of 10% or less.", "D": "The vast majority (over 80%) of mining and oil stocks were highly volatile, with their yearly high price being at least double their yearly low price."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to perform multiple calculations from two different tables and synthesize the results to form a coherent, multi-faceted picture of the Canadian market structure as presented in the paper. It directly tests the core empirical findings that support the paper's thesis about market segmentation.\nStrategy: Computational Judgment. The user must calculate four separate quantitative claims and judge their validity against the data.\nDistractor Logic: This item is designed with four correct options to rigorously test comprehension. An incorrect answer would stem from calculation errors (e.g., misinterpreting ranges, incorrect weighting), misreading the tables, or confusing the industrial and mining/oil categories. For example, a user might only sum one column for a category or miscalculate the weighted average for the combined mining and oil group.", "qid": "160", "question": "### Background\n\nThis paper examines the structure and performance of Canadian stock exchanges in the late 1950s, contrasting them with their U.S. counterparts. A key feature of the Canadian market is the prevalence of speculative, low-priced mining and oil stocks, which are traded alongside more conventional industrial stocks. This analysis investigates how structural differences manifest in observable market outcomes like trading volume, share turnover, and price volatility.\n\n### Data / Model Specification\n\nThe analysis uses data from 1957. The following tables provide key statistics on share turnover and price volatility for samples of stocks listed on the Toronto Stock Exchange (TSE).\n\n**Table 1: Percentage Distributions of Combined Turnovers for 114 Toronto Listings During 1957**\n\n| SHARE TURNOVER (PER CENT) | Industrial | Oil   | Mining |\n|:--------------------------|:-----------|:------|:-------|\n| 0-5                       | 54.0       | 12.5  | 3.6    |\n| 5-10                      | 30.0       | 25.0  | 12.5   |\n| 10-20                     | 10.0       | 12.5  | 26.7   |\n| 20-30                     | 4.0        |       | 12.5   |\n| 30-50                     | 2.0        | 25.0  | 17.9   |\n| 50-70                     |            | 25.0  | 10.7   |\n| 70-100                    |            |       | 3.6    |\n| 100 and over              |            |       | 12.5   |\n| **Total**                 | **100.0**  | **100.0** | **100.0**|\n| **No. of Issues**         | **50**     | **8**     | **56**     |\n\n**Table 2: Ratios of Yearly High to Low Prices in 1957 for 114 Toronto Listings**\n\n| SHARE PRICE   | 1.0-1.5 | 1.5-2.0 | 2.0-3.0 | 3.0 and Over | TOTAL ISSUES |\n|:--------------|:--------|:--------|:--------|:-------------|:-------------|\n| **All industrials** | 32      | 11      | 3       | 4            | 50           |\n| **All mining and oil** | 3       | 8       | 21      | 32           | 64           |\n| **Totals**    | **35**  | **19**  | **24**  | **36**       | **114**      |\n\nBased on a quantitative analysis of the provided data, which of the following statements accurately characterize the Toronto Stock Exchange in 1957? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data systematically:\n\n**For Option A:** Mining and oil stocks with turnover ≥30%\n- Mining: 17.9% + 10.7% + 3.6% + 12.5% = 44.7%\n- Oil: 25.0% + 25.0% = 50.0%\n- Combined mining and oil: Need to calculate weighted average\n  - Total issues: 56 mining + 8 oil = 64\n  - Stocks with ≥30% turnover: (56 × 0.447) + (8 × 0.50) = 25.03 + 4 = 29.03\n  - Percentage: 29.03/64 = 45.4%\n- This is less than 50%, so A is FALSE\n\n**For Option B:** Industrial stocks with high-to-low ratio ≥2.0\n- From Table 2: 3 + 4 = 7 industrial stocks\n- Percentage: 7/50 = 14%\n- This is less than 20% (one in five), so B is TRUE\n\n**For Option C:** Industrial stocks with turnover ≤10%\n- From Table 1: 54.0% + 30.0% = 84.0%\n- This is over 80%, so C is TRUE\n\n**For Option D:** Mining and oil stocks with high-to-low ratio ≥2.0\n- From Table 2: 21 + 32 = 53 stocks\n- Percentage: 53/64 = 82.8%\n- This is over 80%, so D is TRUE\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 79, "Question": "### Background\n\n**Research Question.** This case challenges the canonical definitions of risk from Modern Portfolio Theory (MPT) and the Capital Asset Pricing Model (CAPM). It investigates what professional fund managers—operating as agents within an organizational context—actually perceive as risk, and how this perception is shaped by their evaluation benchmark.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers. Respondents were asked to rate the importance of various risk factors.\n\n---\n\n### Data / Model Specification\n\nThe following tables report the percentage of fund managers who rated different risk factors as 'important', first broken down by fund type and then by performance benchmark.\n\n**Table 1. Attitudes to risk according to type of fund (% rating factor as 'important')**\n\n| Risk Factor | Total Sample (%) | Pension Fund (%) | Private Client (%) |\n| :--- | :---: | :---: | :---: |\n| Commercial risk | 40 | 51 | 25 |\n| Asset-liability matching | 79 | 61 | 75 |\n| Total variability of return | 29 | 27 | 17 |\n| Beta of portfolio | 14 | 19 | 8 |\n\n**Table 2. Attitudes to risk measures according to performance benchmark (% rating factor as 'important')**\n\n| Risk Factor | Median Fund (n=23) (%) | All-Share Index (n=15) (%) | None (n=8) (%) | Kruskal-Wallis Significance |\n| :--- | :---: | :---: | :---: | :---: |\n| **Commercial risk** | **54** | **21** | **25** | **0.07** |\n| Asset-liability matching | 79 | 64 | 75 | 0.46 |\n\n---\n\n### Question\n\nThe survey data reveals a significant divergence between practitioner and academic definitions of risk. Based on the tables, which of the following statements accurately characterize fund managers' attitudes toward risk?", "Options": {"A": "Asset-liability matching is considered the most important risk by a vast majority of managers, regardless of their fund type or performance benchmark.", "B": "The canonical CAPM risk measure, beta, is the second most important risk factor for pension fund managers after asset-liability matching.", "C": "The perceived importance of 'commercial risk' is significantly higher for managers evaluated against a relative benchmark (median fund) than for those with an absolute or no benchmark.", "D": "Private client fund managers, who are not subject to intense relative monitoring, view 'total variability of return' as their primary risk."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an 'Atomic Decomposition' strategy to test comprehension of the paper's key findings on risk perception. It requires synthesizing data from two tables. Correct option A identifies the dominant risk factor across all groups. Correct option B identifies the key statistically significant difference related to benchmarking. Distractor C is an 'Almost Right' option that incorrectly ranks the importance of risk factors from Table 1. Distractor D misattributes the primary risk for a specific subgroup, testing for careful reading of Table 1.", "qid": "79", "question": "### Background\n\n**Research Question.** This case challenges the canonical definitions of risk from Modern Portfolio Theory (MPT) and the Capital Asset Pricing Model (CAPM). It investigates what professional fund managers—operating as agents within an organizational context—actually perceive as risk, and how this perception is shaped by their evaluation benchmark.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers. Respondents were asked to rate the importance of various risk factors.\n\n---\n\n### Data / Model Specification\n\nThe following tables report the percentage of fund managers who rated different risk factors as 'important', first broken down by fund type and then by performance benchmark.\n\n**Table 1. Attitudes to risk according to type of fund (% rating factor as 'important')**\n\n| Risk Factor | Total Sample (%) | Pension Fund (%) | Private Client (%) |\n| :--- | :---: | :---: | :---: |\n| Commercial risk | 40 | 51 | 25 |\n| Asset-liability matching | 79 | 61 | 75 |\n| Total variability of return | 29 | 27 | 17 |\n| Beta of portfolio | 14 | 19 | 8 |\n\n**Table 2. Attitudes to risk measures according to performance benchmark (% rating factor as 'important')**\n\n| Risk Factor | Median Fund (n=23) (%) | All-Share Index (n=15) (%) | None (n=8) (%) | Kruskal-Wallis Significance |\n| :--- | :---: | :---: | :---: | :---: |\n| **Commercial risk** | **54** | **21** | **25** | **0.07** |\n| Asset-liability matching | 79 | 64 | 75 | 0.46 |\n\n---\n\n### Question\n\nThe survey data reveals a significant divergence between practitioner and academic definitions of risk. Based on the tables, which of the following statements accurately characterize fund managers' attitudes toward risk?", "model_evaluation_result": {"model_raw_response": "Looking at the data in both tables:\n\nFrom Table 1:\n- Asset-liability matching: 79% (Total), 61% (Pension), 75% (Private) - highest rated across all groups\n- Commercial risk: 40% (Total), 51% (Pension), 25% (Private)\n- Total variability of return: 29% (Total), 27% (Pension), 17% (Private)\n- Beta of portfolio: 14% (Total), 19% (Pension), 8% (Private)\n\nFrom Table 2:\n- Commercial risk shows significant difference (p=0.07) across benchmarks: 54% (Median Fund) vs 21% (All-Share Index) vs 25% (None)\n- Asset-liability matching shows no significant difference (p=0.46): 79% (Median Fund) vs 64% (All-Share Index) vs 75% (None)\n\nEvaluating each option:\n\nA) TRUE - Asset-liability matching is rated important by 79% overall, and is the highest rated factor across all fund types and benchmarks (61-79% range).\n\nB) FALSE - For pension fund managers, commercial risk (51%) is the second most important after asset-liability matching (61%), not beta (19%).\n\nC) TRUE - Commercial risk importance is 54% for median fund benchmark vs 21% for All-Share Index and 25% for None, with statistical significance (p=0.07).\n\nD) FALSE - For private client managers, asset-liability matching (75%) is the primary risk, not total variability of return (17%).\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 148, "Question": "### Background\n\nA model of futures pricing is developed where speculator participation is limited by a fixed transaction cost, making the number of traders endogenous. Speculators enter the market until the marginal speculator is indifferent between paying the cost to trade versus not trading at all. This equilibrium condition determines the futures risk premium.\n\n### Data / Model Specification\n\nThe equilibrium percentage futures risk premium (`π`) is determined by the indifference condition of the marginal speculator, leading to the following pricing relation from Proposition 3:\n\n  \n\\pi = \\beta_{\\pi m} \\bar{R}_m \\pm \\sigma_{\\pi} \\sqrt{2\\alpha t(1-\\rho^2)} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `π`: Percentage futures risk premium.\n- `t`: Fixed transaction/setup cost for trading futures.\n- `α`: Coefficient of absolute risk aversion.\n- `β_πm`: Market beta of the percentage futures return.\n- `R̄_m`: Expected net return on the stock market.\n- `σ_π`: Standard deviation of the percentage futures return.\n- `ρ`: Correlation between the futures return `π̃` and the market return `R_m`.\n\n### Question\n\nBased on the equilibrium pricing model in `Eq. (1)`, which of the following statements are valid interpretations or consequences of the model's structure?\n\nSelect all that apply.", "Options": {"A": "The model predicts that the risk premium is determined by the marginal speculator's indifference condition, which makes the premium sensitive to the parameters of that speculator (`α`, `t`) and the statistical properties of the futures contract itself (`β_πm`, `σ_π`, `ρ`).", "B": "The component of the risk premium related to hedging pressure is proportional to the residual variance of the futures return, `σ_π^2(1-ρ^2)`.", "C": "The premium for bearing residual risk is independent of the covariance between the futures return and producers' nonmarketable revenue risks.", "D": "If the setup cost `t` were to double, the absolute magnitude of the hedging-related component of the premium would also double."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to correctly interpret the components of the central pricing equation (Proposition 3) and understand its key theoretical implications. Depth Strategy: Reverse-Reasoning. The candidate is given the final equation and must identify the valid conceptual underpinnings and consequences that led to it. Distractor Logic: Option A is a Formula Misuse error, incorrectly stating proportionality to residual variance instead of residual standard deviation, a key distinction made in the paper. Option C is a Numerical Distractor (Formula Misuse), as the premium is proportional to the square root of `t`, not `t` itself, so doubling `t` increases the premium by a factor of √2, not 2. Options B and D are correct and capture two of the most surprising and important results of the model: the irrelevance of nonmarketable risk covariance for the premium (a departure from Mayers CAPM) and the central role of the marginal speculator's parameters.", "qid": "148", "question": "### Background\n\nA model of futures pricing is developed where speculator participation is limited by a fixed transaction cost, making the number of traders endogenous. Speculators enter the market until the marginal speculator is indifferent between paying the cost to trade versus not trading at all. This equilibrium condition determines the futures risk premium.\n\n### Data / Model Specification\n\nThe equilibrium percentage futures risk premium (`π`) is determined by the indifference condition of the marginal speculator, leading to the following pricing relation from Proposition 3:\n\n  \n\\pi = \\beta_{\\pi m} \\bar{R}_m \\pm \\sigma_{\\pi} \\sqrt{2\\alpha t(1-\\rho^2)} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `π`: Percentage futures risk premium.\n- `t`: Fixed transaction/setup cost for trading futures.\n- `α`: Coefficient of absolute risk aversion.\n- `β_πm`: Market beta of the percentage futures return.\n- `R̄_m`: Expected net return on the stock market.\n- `σ_π`: Standard deviation of the percentage futures return.\n- `ρ`: Correlation between the futures return `π̃` and the market return `R_m`.\n\n### Question\n\nBased on the equilibrium pricing model in `Eq. (1)`, which of the following statements are valid interpretations or consequences of the model's structure?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the equilibrium pricing model and analyzing each statement:\n\n**Statement A**: The model shows that π depends on marginal speculator parameters (α, t) and futures contract properties (β_πm, σ_π, ρ). This directly follows from equation (1), where all these parameters appear in the pricing formula. ✓\n\n**Statement B**: The hedging pressure component contains the term √(2αt(1-ρ²)). Since (1-ρ²) represents the proportion of variance that is residual (not explained by market correlation), and this appears under a square root (not squared), the hedging component is proportional to the residual standard deviation σ_π√(1-ρ²), not the residual variance σ_π²(1-ρ²). ✗\n\n**Statement C**: The model as presented doesn't explicitly include producers' nonmarketable revenue risks. The hedging pressure component σ_π√(2αt(1-ρ²)) depends on futures return characteristics, not on any covariance with producers' risks. Without additional model details showing how producers' risks enter, this statement appears consistent with the given equation. ✓\n\n**Statement D**: The hedging component is ±σ_π√(2αt(1-ρ²)). If t doubles to 2t, this becomes ±σ_π√(4αt(1-ρ²)) = ±σ_π√(2αt(1-ρ²))×√2. The magnitude increases by a factor of √2, not 2. ✗\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 214, "Question": "### Background\n\n**Research Question.** How can a parsimonious stochastic model capture the primary dynamics of a population's mortality curve over time, and how sensitive is this model to its underlying assumptions?\n\n**Setting.** The Cairns two-factor model is used to describe the evolution of single-year death probabilities. The model's factors, `A(t)`, are assumed to follow a bivariate random walk with drift.\n\n### Data / Model Specification\n\nThe Cairns model specifies the logit of the death probability as a linear function of age:\n\n  \n\\ln\\left(\\frac{q_{x,t}}{1-q_{x,t}}\\right) = A_{1}(t)+A_{2}(t)x \\quad \\text{(Eq. (1))}\n \n\nThe dynamics of the mortality factors are modeled as a bivariate random walk with drift:\n\n  \nA(t+1) = A(t) + \\upmu + C Z(t+1) \\quad \\text{(Eq. (2))}\n \n\nThis implies that the annual changes, `D(t+1) = A(t+1) - A(t)`, are i.i.d. multivariate normal with mean `μ`. The Maximum Likelihood Estimator (MLE) for the drift vector `μ` simplifies to `μ_hat = (A(n) - A(0)) / n`, where `n` is the number of years in the sample.\n\n---\n\nBased on the model specification and its properties, which of the following statements are valid?\n", "Options": {"A": "The random-walk-with-drift specification makes long-term forecasts highly sensitive to anomalies or structural breaks near the end of the estimation sample.", "B": "The estimator for the drift `μ` depends only on the first and last observations of the factor `A(t)` in the sample period.", "C": "A decrease in the `A_1(t)` factor corresponds to a parallel downward shift in the mortality curve on a logit scale, reducing mortality at all ages.", "D": "An increase in the `A_2(t)` factor makes the mortality curve less steep, indicating that mortality improvements are faster at higher ages than at lower ages."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the Cairns two-factor mortality model, including the interpretation of its factors and the critical flaw in its standard estimation method. Depth Strategy: Scenario Application. The user must mentally apply changes to the model's factors and parameters to determine the consequences. Distractor Logic: Option A is a correct interpretation of the level factor. Option B is a correct statement about the MLE for the drift, as shown in the paper. Option C is the correct implication of the flaw identified in B, which is a central point of the paper's model risk section. Option D is a 'Conceptual Opposite' distractor; an increase in `A_2(t)` (the slope) makes the curve steeper, not less steep.", "qid": "214", "question": "### Background\n\n**Research Question.** How can a parsimonious stochastic model capture the primary dynamics of a population's mortality curve over time, and how sensitive is this model to its underlying assumptions?\n\n**Setting.** The Cairns two-factor model is used to describe the evolution of single-year death probabilities. The model's factors, `A(t)`, are assumed to follow a bivariate random walk with drift.\n\n### Data / Model Specification\n\nThe Cairns model specifies the logit of the death probability as a linear function of age:\n\n  \n\\ln\\left(\\frac{q_{x,t}}{1-q_{x,t}}\\right) = A_{1}(t)+A_{2}(t)x \\quad \\text{(Eq. (1))}\n \n\nThe dynamics of the mortality factors are modeled as a bivariate random walk with drift:\n\n  \nA(t+1) = A(t) + \\upmu + C Z(t+1) \\quad \\text{(Eq. (2))}\n \n\nThis implies that the annual changes, `D(t+1) = A(t+1) - A(t)`, are i.i.d. multivariate normal with mean `μ`. The Maximum Likelihood Estimator (MLE) for the drift vector `μ` simplifies to `μ_hat = (A(n) - A(0)) / n`, where `n` is the number of years in the sample.\n\n---\n\nBased on the model specification and its properties, which of the following statements are valid?\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\n**A) The random-walk-with-drift specification makes long-term forecasts highly sensitive to anomalies or structural breaks near the end of the estimation sample.**\n\nThis is TRUE. Since μ̂ = (A(n) - A(0))/n, the drift estimate depends only on the endpoints. Any anomaly or structural break near the end of the sample will significantly affect A(n) and thus the drift estimate, which is then extrapolated indefinitely in long-term forecasts.\n\n**B) The estimator for the drift μ depends only on the first and last observations of the factor A(t) in the sample period.**\n\nThis is TRUE. The MLE for the drift is explicitly given as μ̂ = (A(n) - A(0))/n, which uses only A(0) and A(n).\n\n**C) A decrease in the A₁(t) factor corresponds to a parallel downward shift in the mortality curve on a logit scale, reducing mortality at all ages.**\n\nThis is TRUE. From equation (1), ln(q/(1-q)) = A₁(t) + A₂(t)x. A decrease in A₁(t) reduces the logit for all ages x equally (parallel shift). Since the logit function is increasing, a lower logit means lower mortality q.\n\n**D) An increase in the A₂(t) factor makes the mortality curve less steep, indicating that mortality improvements are faster at higher ages than at lower ages.**\n\nThis is FALSE. A₂(t) is the slope coefficient. An increase in A₂(t) makes the curve MORE steep (higher slope), not less steep. Additionally, a steeper positive slope means mortality increases faster with age, which represents slower improvements (or faster deterioration) at higher ages, not faster improvements.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 191, "Question": "### Background\n\nThe model's central prediction is that the active management risk premium is, on average, positive. This premium arises because benchmark-aware investors create demand for portfolios that deviate from the market, and other investors must be compensated for absorbing this risk.\n\n### Data / Model Specification\n\nThe equilibrium condition for the active management risk premium, constructed to be orthogonal to the market factor, is:\n  \nβ̂μ_{M} - μ_{Φ} = (π w) × [Φ'ΩΦ(1-ρ_{MΦ}^{2})] \\quad \\text{(Eq. (1))}\n \nwhere:\n- The left-hand side is the expected active management risk premium.\n- `π` is the CARA coefficient of risk aversion (`π > 0`).\n- `w` is the wealth share of benchmark investors, `W_b / (W_a + W_b)`.\n- `Φ'ΩΦ` is the variance of the benchmark portfolio's return.\n- `ρ_{MΦ}` is the correlation between the market and benchmark portfolios.\n\n---\n\nAccording to the relationship in **Eq. (1)**, which of the following conditions would lead to a predicted active management risk premium of exactly zero?\n", "Options": {"A": "The benchmark portfolio is the global minimum variance portfolio (`Φ'ΩΦ` is minimized).", "B": "The benchmark portfolio is perfectly correlated with the market portfolio (`ρ_{MΦ}^2 = 1`).", "C": "The economy contains no benchmark-aware investors (`w = 0`).", "D": "Investors are risk-neutral (`π = 0`)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to interpret the economic determinants of the active management risk premium as specified by the model's core theoretical equation. Depth Strategy: Reverse-Reasoning. The question asks for the conditions under which the predicted effect (a positive risk premium) disappears, forcing the user to identify all necessary components of the causal mechanism. Distractor Logic: Option D is a conceptual distractor. While minimizing the benchmark's variance would reduce the premium's magnitude, it would not drive it to exactly zero unless the variance itself was zero (i.e., a risk-free benchmark), which is not what is stated. This tests whether the user understands the difference between reducing and eliminating the premium.", "qid": "191", "question": "### Background\n\nThe model's central prediction is that the active management risk premium is, on average, positive. This premium arises because benchmark-aware investors create demand for portfolios that deviate from the market, and other investors must be compensated for absorbing this risk.\n\n### Data / Model Specification\n\nThe equilibrium condition for the active management risk premium, constructed to be orthogonal to the market factor, is:\n  \nβ̂μ_{M} - μ_{Φ} = (π w) × [Φ'ΩΦ(1-ρ_{MΦ}^{2})] \\quad \\text{(Eq. (1))}\n \nwhere:\n- The left-hand side is the expected active management risk premium.\n- `π` is the CARA coefficient of risk aversion (`π > 0`).\n- `w` is the wealth share of benchmark investors, `W_b / (W_a + W_b)`.\n- `Φ'ΩΦ` is the variance of the benchmark portfolio's return.\n- `ρ_{MΦ}` is the correlation between the market and benchmark portfolios.\n\n---\n\nAccording to the relationship in **Eq. (1)**, which of the following conditions would lead to a predicted active management risk premium of exactly zero?\n", "model_evaluation_result": {"model_raw_response": "Looking at equation (1): β̂μ_{M} - μ_{Φ} = (π w) × [Φ'ΩΦ(1-ρ_{MΦ}^{2})]\n\nFor the active management risk premium (left side) to equal exactly zero, the right side must equal zero.\n\nThe right side equals zero when any of these factors equals zero:\n- π = 0 (risk neutrality)\n- w = 0 (no benchmark investors)\n- (1-ρ_{MΦ}^{2}) = 0, which occurs when ρ_{MΦ}^{2} = 1 (perfect correlation)\n\nNote that Φ'ΩΦ (variance) cannot be zero unless the benchmark has zero variance, which is not realistic.\n\nChecking each option:\n- A) Minimizing Φ'ΩΦ doesn't make it zero, so this doesn't work\n- B) ρ_{MΦ}^2 = 1 makes (1-ρ_{MΦ}^{2}) = 0 ✓\n- C) w = 0 makes the product zero ✓\n- D) π = 0 makes the product zero ✓\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 217, "Question": "### Background\n\n**Research Question.** Can circuit breakers effectively stabilize financial markets by taming endogenous volatility, and how does this conclusion hold up against the efficient-market critique that such interventions merely delay necessary price discovery, especially when the fundamental value is non-stationary?\n\n**Setting.** The paper's agent-based model is used as a laboratory to study the effects of circuit breakers. The analysis is made more realistic by allowing the log fundamental value `F_t` to evolve as a random walk, which provides a framework for evaluating Fama's critique that circuit breakers hinder the market's ability to track new information.\n\n**Variables and Parameters.**\n- `P_t`: Log price of the stock at time `t`.\n- `F_t`: Log fundamental value at time `t`.\n- `n_t`: A normally distributed fundamental shock, `n_t \\sim N(0, (\\sigma^F)^2)`.\n- `s`: The circuit breaker threshold, i.e., the maximum allowed absolute log price change.\n- `V_t`: Market volatility, an EWMA of squared price changes.\n- `\\sigma_t^2`: Trading intensity (variance of speculator demand).\n- `\\rho_t`: Correlation of speculator demands (herding).\n- `C_t`: A condition measuring sharp price reversals.\n- Distortion: A measure of the deviation between `P_t` and `F_t`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental value evolves as a random walk:\n  \nF_t = F_{t-1} + n_t \\quad \\text{(Eq. 1)}\n \nThe potential price `P_{t+1}^*` is calculated based on aggregate demand, and the circuit breaker rule determines the actual price `P_{t+1}`:\n  \nP_{t+1} = \\begin{cases} P_t + s & \\text{if } (P_{t+1}^* - P_t) > s \\\\ P_t - s & \\text{if } (P_{t+1}^* - P_t) < -s \\\\ P_{t+1}^* & \\text{otherwise} \\end{cases} \\quad \\text{(Eq. 2)}\n \nKey endogenous feedback variables are defined as:\n  \nV_{t} = d V_{t-1} + (1-d)(P_{t}-P_{t-1})^{2} \\quad \\text{(Eq. 3)}\n \n  \n\\sigma_{t}^{2} = e^{l} + \\frac{e^{h}-e^{l}}{1+\\exp[-e^{s}(V_{t}-\\bar{V})]} \\quad \\text{(Eq. 4)}\n \n  \nC_{t} = ((P_{t}-P_{t-1})-(P_{t-1}-P_{t-2}))^{2} \\quad \\text{(Eq. 5)}\n \n  \n\\rho_{t} = f^{l} + \\frac{f^{h}-f^{l}}{1+\\exp[-f^{s}(C_{t}-\\bar{C})]} \\quad \\text{(Eq. 6)}\n \n\n---\n\n### Question\n\nThe paper uses the model to evaluate the effectiveness of circuit breakers, particularly in a market with a non-stationary fundamental value (`F_t`). Select all conclusions that are supported by the model's analysis.", "Options": {"A": "A trade-off exists: while generally stabilizing, an overly restrictive circuit breaker can increase price distortion by preventing the market price from tracking large, legitimate shocks to the fundamental value.", "B": "The model suggests that for any level of fundamental volatility (`\\sigma^F > 0`), circuit breakers will always reduce both price volatility and price distortion.", "C": "The model's results contradict the 'volatility spillover' hypothesis; by capping a large price change, circuit breakers reduce the input to the volatility feedback loop, leading to lower, not higher, subsequent volatility.", "D": "The primary reason for modeling the fundamental value `F_t` as a random walk is to show that circuit breakers are only effective in markets without new information."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to synthesize the model's key policy conclusions and trade-offs. It uses a **Premise Packaging** strategy, asking for all valid conclusions from the policy experiment. **Correct options (B, D)** capture the two central, nuanced findings: the refutation of the volatility spillover hypothesis and the existence of a trade-off where overly tight breakers can harm price discovery. **Distractors** target common misinterpretations: (A) misstates the reason for the random-walk assumption (Conceptual Opposite), while (C) presents an oversimplified 'always better' conclusion, ignoring the critical trade-off detailed in the paper (Almost Right).", "qid": "217", "question": "### Background\n\n**Research Question.** Can circuit breakers effectively stabilize financial markets by taming endogenous volatility, and how does this conclusion hold up against the efficient-market critique that such interventions merely delay necessary price discovery, especially when the fundamental value is non-stationary?\n\n**Setting.** The paper's agent-based model is used as a laboratory to study the effects of circuit breakers. The analysis is made more realistic by allowing the log fundamental value `F_t` to evolve as a random walk, which provides a framework for evaluating Fama's critique that circuit breakers hinder the market's ability to track new information.\n\n**Variables and Parameters.**\n- `P_t`: Log price of the stock at time `t`.\n- `F_t`: Log fundamental value at time `t`.\n- `n_t`: A normally distributed fundamental shock, `n_t \\sim N(0, (\\sigma^F)^2)`.\n- `s`: The circuit breaker threshold, i.e., the maximum allowed absolute log price change.\n- `V_t`: Market volatility, an EWMA of squared price changes.\n- `\\sigma_t^2`: Trading intensity (variance of speculator demand).\n- `\\rho_t`: Correlation of speculator demands (herding).\n- `C_t`: A condition measuring sharp price reversals.\n- Distortion: A measure of the deviation between `P_t` and `F_t`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental value evolves as a random walk:\n  \nF_t = F_{t-1} + n_t \\quad \\text{(Eq. 1)}\n \nThe potential price `P_{t+1}^*` is calculated based on aggregate demand, and the circuit breaker rule determines the actual price `P_{t+1}`:\n  \nP_{t+1} = \\begin{cases} P_t + s & \\text{if } (P_{t+1}^* - P_t) > s \\\\ P_t - s & \\text{if } (P_{t+1}^* - P_t) < -s \\\\ P_{t+1}^* & \\text{otherwise} \\end{cases} \\quad \\text{(Eq. 2)}\n \nKey endogenous feedback variables are defined as:\n  \nV_{t} = d V_{t-1} + (1-d)(P_{t}-P_{t-1})^{2} \\quad \\text{(Eq. 3)}\n \n  \n\\sigma_{t}^{2} = e^{l} + \\frac{e^{h}-e^{l}}{1+\\exp[-e^{s}(V_{t}-\\bar{V})]} \\quad \\text{(Eq. 4)}\n \n  \nC_{t} = ((P_{t}-P_{t-1})-(P_{t-1}-P_{t-2}))^{2} \\quad \\text{(Eq. 5)}\n \n  \n\\rho_{t} = f^{l} + \\frac{f^{h}-f^{l}}{1+\\exp[-f^{s}(C_{t}-\\bar{C})]} \\quad \\text{(Eq. 6)}\n \n\n---\n\n### Question\n\nThe paper uses the model to evaluate the effectiveness of circuit breakers, particularly in a market with a non-stationary fundamental value (`F_t`). Select all conclusions that are supported by the model's analysis.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each option based on the model's structure and what it would likely show about circuit breakers.\n\nOption A: This describes a classic trade-off. Circuit breakers limit price movements (by the rule in Eq. 2), which can stabilize markets by preventing excessive volatility feedback loops. However, when the fundamental value F_t experiences large legitimate shocks (through n_t), the circuit breaker prevents the price from fully adjusting, increasing |P_t - F_t| (distortion). This trade-off is a natural consequence of the model's structure.\n\nOption B: This claims circuit breakers ALWAYS reduce both volatility and distortion for any σ^F > 0. This is too strong a claim. While circuit breakers may reduce volatility through dampening feedback effects, they can increase distortion when preventing necessary price adjustments to track F_t, especially for large fundamental shocks.\n\nOption C: This aligns with the model's feedback mechanism. When circuit breakers cap price changes, they reduce the input (P_t - P_{t-1})² to the volatility measure V_t (Eq. 3). Lower V_t then reduces trading intensity σ_t² through Eq. 4, creating less volatile subsequent trading. This contradicts the volatility spillover hypothesis that suggests capped volatility would resurface later.\n\nOption D: This misinterprets the purpose of the random walk specification. The random walk for F_t is meant to create a realistic scenario where fundamental values change over time, allowing the model to test how circuit breakers perform when prices need to track moving fundamentals - not to show they only work without new information.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 212, "Question": "### Background\n\n**Research Question.** How do different sources of uncertainty—specifically parameter risk and model risk—contribute to the total uncertainty in pricing a longevity-linked derivative?\n\n**Setting.** The fair spread `σ` for a 10-year QxX index swap is estimated using a two-factor stochastic mortality model. The analysis first quantifies **parameter risk**—the uncertainty in model parameters `μ` and `V` estimated from a fixed data sample (1971-2005)—by generating a 95% confidence interval (CI) for the spread. It then quantifies **model risk** by re-estimating the central value of the spread using different data samples, reflecting uncertainty about a potential structural break in mortality trends around the year 2000.\n\n### Data / Model Specification\n\n**Table 1.** 95% Confidence Intervals for `σ` from Parameter Risk Analysis (in basis points).\nThis table reflects uncertainty assuming the model is correctly specified and fitted to the 1971-2005 data.\n\n| λ_1   | λ_2   | 95% CI for σ |\n|-------|-------|--------------|\n| 0     | 0.316 | (581, 668)   |\n| 0.375 | 0     | (587, 673)   |\n| 0.175 | 0.175 | (583, 671)   |\n\n**Table 2.** Central Estimates of `σ` from Model Risk Analysis (in basis points).\nThis table shows how the central estimate of `σ` changes based on the estimation period.\n- **Scenario 1:** Full sample (1971-2005).\n- **Scenario 2:** Pre-break sample (1971-2000), assuming old trends revert.\n- **Scenario 3:** Post-break sample (2000-2005), assuming the new, slower-improvement trend is permanent.\n\n| (λ_1, λ_2)    | Scenario 1 | Scenario 2 | Scenario 3 |\n|---------------|------------|------------|------------|\n| (0, 0.316)    | 625        | 634        | 589        |\n| (0.375, 0)    | 631        | 638        | 594        |\n| (0.175, 0.175)| 627        | 635        | 591        |\n\n---\n\nBased on the provided data for the case where `(λ_1, λ_2) = (0.375, 0)`, select all of the following statements that are correct.\n", "Options": {"A": "The lowest central estimate from model risk (Scenario 3) falls within the 95% confidence interval for parameter risk, indicating that parameter risk fully accounts for model risk.", "B": "A heuristic 95% uncertainty range that combines parameter risk with the model risk from Scenario 3 (permanent trend change) is (551, 637) basis points.", "C": "The uncertainty in the swap spread `σ` attributable to model risk, as measured by the range of central estimates across scenarios, is 44 basis points.", "D": "The central estimate of the swap spread `σ` under the baseline model (Scenario 1) is 630 basis points."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to quantify and synthesize two distinct types of risk (parameter and model risk) from tabular data. It requires calculation, comparison, and the construction of a combined risk measure. Depth Strategy: Computational Judgment. The user must perform several calculations: finding the midpoint of a CI, calculating the range of central estimates, and constructing a new CI by shifting the parameter risk interval. Distractor Logic: Option A is correct (midpoint of (587, 673) is 630). Option B is correct (range of 638 to 594 is 44). Option C is correct (the parameter risk uncertainty is ±43 bps; applying this to the Scenario 3 central estimate of 594 gives 594 ± 43 = (551, 637)). Option D is a plausible but incorrect conceptual conclusion (a 'True but Irrelevant' distractor), as the two risks are distinct and additive, not nested.", "qid": "212", "question": "### Background\n\n**Research Question.** How do different sources of uncertainty—specifically parameter risk and model risk—contribute to the total uncertainty in pricing a longevity-linked derivative?\n\n**Setting.** The fair spread `σ` for a 10-year QxX index swap is estimated using a two-factor stochastic mortality model. The analysis first quantifies **parameter risk**—the uncertainty in model parameters `μ` and `V` estimated from a fixed data sample (1971-2005)—by generating a 95% confidence interval (CI) for the spread. It then quantifies **model risk** by re-estimating the central value of the spread using different data samples, reflecting uncertainty about a potential structural break in mortality trends around the year 2000.\n\n### Data / Model Specification\n\n**Table 1.** 95% Confidence Intervals for `σ` from Parameter Risk Analysis (in basis points).\nThis table reflects uncertainty assuming the model is correctly specified and fitted to the 1971-2005 data.\n\n| λ_1   | λ_2   | 95% CI for σ |\n|-------|-------|--------------|\n| 0     | 0.316 | (581, 668)   |\n| 0.375 | 0     | (587, 673)   |\n| 0.175 | 0.175 | (583, 671)   |\n\n**Table 2.** Central Estimates of `σ` from Model Risk Analysis (in basis points).\nThis table shows how the central estimate of `σ` changes based on the estimation period.\n- **Scenario 1:** Full sample (1971-2005).\n- **Scenario 2:** Pre-break sample (1971-2000), assuming old trends revert.\n- **Scenario 3:** Post-break sample (2000-2005), assuming the new, slower-improvement trend is permanent.\n\n| (λ_1, λ_2)    | Scenario 1 | Scenario 2 | Scenario 3 |\n|---------------|------------|------------|------------|\n| (0, 0.316)    | 625        | 634        | 589        |\n| (0.375, 0)    | 631        | 638        | 594        |\n| (0.175, 0.175)| 627        | 635        | 591        |\n\n---\n\nBased on the provided data for the case where `(λ_1, λ_2) = (0.375, 0)`, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "Looking at the data for (λ_1, λ_2) = (0.375, 0):\n\nFrom Table 1: 95% CI for parameter risk = (587, 673)\nFrom Table 2: Scenario 1 = 631, Scenario 2 = 638, Scenario 3 = 594\n\nLet me check each statement:\n\nA) The lowest central estimate from model risk is Scenario 3 = 594. The 95% CI for parameter risk is (587, 673). Since 594 is within (587, 673), the first part is true. However, this doesn't mean parameter risk \"fully accounts for\" model risk - they measure different types of uncertainty. This statement is incorrect.\n\nB) To combine parameter risk with Scenario 3:\n- Parameter risk CI width = 673 - 587 = 86, so ±43 from center\n- Scenario 3 central estimate = 594\n- Combined range = 594 ± 43 = (551, 637)\nThis is correct.\n\nC) Model risk range = highest - lowest central estimate = 638 - 594 = 44 basis points\nThis is correct.\n\nD) The central estimate under Scenario 1 = 631 basis points (not 630)\nThis is incorrect.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 203, "Question": "### Background\n\n**Research Question.** How can the Random Walk Hypothesis be tested in a manner that is robust to heteroscedasticity, and how can this test be adapted to measure changes in market efficiency over time?\n\n**Setting / Data-Generating Environment.** The analysis uses the natural logarithm of daily closing prices for UK financial futures. The Lo-MacKinlay Variance-Ratio (VR) test is employed to assess whether the variance of multi-period returns scales linearly with the time horizon, a key property of a random walk. The sample is split to compare market characteristics before and after the introduction of an electronic trading (ET) system in 1999.\n\n**Variables & Parameters.**\n\n*   `VR(q)`: The variance ratio, comparing the variance of `q`-period returns to `q` times the variance of one-period returns.\n*   `Z(q)`: The standardized test statistic for `VR(q)` under the assumption of homoscedasticity.\n*   `Z*(q)`: The standardized test statistic for `VR(q)` that is robust to heteroscedasticity.\n*   `|VR(q) - 1|`: The absolute deviation of the variance ratio from one, used as a proxy for the degree of market inefficiency.\n\n---\n\n### Data / Model Specification\n\nThe null hypothesis of the Variance-Ratio test is `H₀: VR(q) = 1`, which corresponds to a random walk. The test statistics are asymptotically standard normal, with a 5% critical value of approximately 1.96 for a two-sided test.\n\n**Table 1. Variance-Ratio Test Results for UK Futures**\n\n| Panel | Contract | Lag (q) | Statistic | Value | 5% Critical Value |\n|---|---|---|---|---|---|\n| A | Short Sterling (Before ET) | 2 | `Z(q)` | -4.32 | 1.96 |\n| A | Short Sterling (Before ET) | 2 | `Z*(q)` | -0.64 | 1.96 |\n| B | FTSE100 Futures | 2 | `|VR(q) - 1|` (Before ET) | 0.0067 | N/A |\n| B | FTSE100 Futures | 2 | `|VR(q) - 1|` (Incl. ET) | 0.0013 | N/A |\n\n*Source: Adapted from Table 4 of the source paper. Panel A highlights the difference between standard and robust statistics. Panel B shows the change in the inefficiency metric before and after the introduction of electronic trading (ET).* \n\n---\n\n### Question\n\nBased on the data presented in **Table 1**, select ALL of the following statements that are correct interpretations of the results.", "Options": {"A": "The heteroscedasticity-robust Z*-statistic for Short Sterling (-0.64) provides strong evidence to reject the random walk hypothesis, confirming the market's inefficiency.", "B": "For the Short Sterling contract, the standard Z-statistic (-4.32) would lead to a rejection of the random walk hypothesis at the 5% significance level, while the heteroscedasticity-robust Z*-statistic (-0.64) would not.", "C": "For FTSE100 futures, the increase in the inefficiency metric `|VR(2) - 1|` after the introduction of electronic trading suggests a decline in market efficiency.", "D": "The measure of market inefficiency for FTSE100 futures, `|VR(2) - 1|`, decreased from 0.0067 to 0.0013 in the period including electronic trading, indicating the market became more efficient."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to interpret statistical test results in the context of financial data. It uses an 'Atomic Decomposition' strategy, breaking down the complex source QA into two independently verifiable conclusions from the table. Option A tests the core concept of robust inference (Z vs Z*), while Option B tests the application of the paper's novel efficiency metric. Distractors C and D are designed as 'Conceptual Opposites' that represent common misinterpretations of the test statistics and the efficiency metric, respectively.", "qid": "203", "question": "### Background\n\n**Research Question.** How can the Random Walk Hypothesis be tested in a manner that is robust to heteroscedasticity, and how can this test be adapted to measure changes in market efficiency over time?\n\n**Setting / Data-Generating Environment.** The analysis uses the natural logarithm of daily closing prices for UK financial futures. The Lo-MacKinlay Variance-Ratio (VR) test is employed to assess whether the variance of multi-period returns scales linearly with the time horizon, a key property of a random walk. The sample is split to compare market characteristics before and after the introduction of an electronic trading (ET) system in 1999.\n\n**Variables & Parameters.**\n\n*   `VR(q)`: The variance ratio, comparing the variance of `q`-period returns to `q` times the variance of one-period returns.\n*   `Z(q)`: The standardized test statistic for `VR(q)` under the assumption of homoscedasticity.\n*   `Z*(q)`: The standardized test statistic for `VR(q)` that is robust to heteroscedasticity.\n*   `|VR(q) - 1|`: The absolute deviation of the variance ratio from one, used as a proxy for the degree of market inefficiency.\n\n---\n\n### Data / Model Specification\n\nThe null hypothesis of the Variance-Ratio test is `H₀: VR(q) = 1`, which corresponds to a random walk. The test statistics are asymptotically standard normal, with a 5% critical value of approximately 1.96 for a two-sided test.\n\n**Table 1. Variance-Ratio Test Results for UK Futures**\n\n| Panel | Contract | Lag (q) | Statistic | Value | 5% Critical Value |\n|---|---|---|---|---|---|\n| A | Short Sterling (Before ET) | 2 | `Z(q)` | -4.32 | 1.96 |\n| A | Short Sterling (Before ET) | 2 | `Z*(q)` | -0.64 | 1.96 |\n| B | FTSE100 Futures | 2 | `|VR(q) - 1|` (Before ET) | 0.0067 | N/A |\n| B | FTSE100 Futures | 2 | `|VR(q) - 1|` (Incl. ET) | 0.0013 | N/A |\n\n*Source: Adapted from Table 4 of the source paper. Panel A highlights the difference between standard and robust statistics. Panel B shows the change in the inefficiency metric before and after the introduction of electronic trading (ET).* \n\n---\n\n### Question\n\nBased on the data presented in **Table 1**, select ALL of the following statements that are correct interpretations of the results.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each statement:\n\n**Statement A**: The Z*-statistic for Short Sterling is -0.64, which has an absolute value of 0.64. This is less than the 5% critical value of 1.96, so we would NOT reject the null hypothesis. The statement claims this provides \"strong evidence to reject the random walk hypothesis,\" which is incorrect.\n\n**Statement B**: The standard Z-statistic is -4.32 (absolute value 4.32 > 1.96), so we would reject H₀. The Z*-statistic is -0.64 (absolute value 0.64 < 1.96), so we would NOT reject H₀. This statement correctly describes both outcomes.\n\n**Statement C**: The |VR(2) - 1| values show 0.0067 (Before ET) and 0.0013 (Incl. ET). Since 0.0013 < 0.0067, the inefficiency metric DECREASED, not increased. This statement is incorrect.\n\n**Statement D**: The inefficiency metric decreased from 0.0067 to 0.0013, and since a smaller deviation from 1 indicates the variance ratio is closer to 1 (the random walk value), this indicates greater market efficiency. This statement is correct.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 207, "Question": "### Background\n\n**Research Question:** This case investigates the primary drivers of Seasoned Equity Offering (SEO) timing in China's emerging market. It seeks to distinguish between two competing explanations for why firms issue equity: opportunistic market timing versus regulatory pressure.\n\n**Setting / Data-Generating Environment:** The analysis uses a Log-Logistic duration model to analyze the time from a firm's Initial Public Offering (IPO) to its first SEO. The key empirical challenge is that firm profitability, stock returns, and market-to-book ratios are often correlated, making it difficult to isolate the true driver of SEO timing.\n\n### Data / Model Specification\n\nThe duration model estimates the effect of covariates on the time until an SEO. The model's interpretation rule is: *\"a significantly positive (negative) sign suggests that the variable is positively (negatively) related to the duration, but negatively (positively) related to the hazard probability and the probability of equity financing.\"* A negative coefficient thus implies a shorter time to an SEO and a higher probability of issuance.\n\nTwo primary hypotheses are considered:\n1.  **Market Timing Hypothesis:** Managers opportunistically issue equity when they perceive their stock is overvalued, as proxied by a high Market-to-Book (`M/B`) ratio or high recent `Stock returns`.\n2.  **Regulatory Pressure Hypothesis:** The Chinese Securities Regulatory Commission (CSRC) requires firms to meet a minimum Return on Equity (`ROE`) to qualify for an SEO. This creates a powerful incentive for firms to report high profitability (`ROA` is used as a proxy) immediately before an issuance.\n\nTable 1 presents results from models testing the market timing hypothesis in isolation. Table 2 presents results from a multivariate \"horse race\" regression that includes both a profitability measure (`ROA`) and a market timing measure (`M/B` ratio) to see which has more explanatory power.\n\n**Table 1: Duration Analysis of Market Timing**\n\n| Variable | Model (1) Coeff. (t-stat) | Model (2) Coeff. (t-stat) |\n| :--- | :--- | :--- |\n| Market-to-book ratio (`M/B`) | -0.017** (-2.145) | | \n| Stock returns | | -1.515*** (-4.239) |\n\n*Source: Adapted from Table 6 of the paper. ** and *** denote significance at the 5% and 1% levels, respectively.*\n\n**Table 2: Duration Analysis of Profitability vs. Market Timing**\n\n| Variable | Coefficient (t-statistic) |\n| :--- | :--- |\n| Return on asset (`ROA`) | -4.477*** (-4.939) |\n| Market-to-book ratio (`M/B`) | -0.009 (-1.135) |\n\n*Source: Adapted from Table 8, Panel B of the paper. *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the provided data and interpretation rule, which of the following statements are correct interpretations of the regression results? (Select all that apply)", "Options": {"A": "In the joint model (Table 2), both the `M/B` ratio and `ROA` are statistically significant predictors of SEO timing.", "B": "When both `ROA` and `M/B` are included in the model (Table 2), `ROA` is the only statistically significant predictor of SEO timing.", "C": "The results in Table 1 suggest that a higher Market-to-Book ratio is associated with a longer time to an SEO.", "D": "In the standalone model (Table 1), higher recent stock returns are associated with a significantly shorter time to an SEO."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the fundamental skill of reading and interpreting regression tables, specifically focusing on coefficient signs and statistical significance. It is a select-all-valid task. Options A and B are correct readings of the tables. Distractor C represents a classic 'Sign Error,' misinterpreting the negative coefficient on M/B. Distractor D represents a 'Significance Error,' failing to notice that the M/B ratio's t-statistic in Table 2 is well below the threshold for significance.", "qid": "207", "question": "### Background\n\n**Research Question:** This case investigates the primary drivers of Seasoned Equity Offering (SEO) timing in China's emerging market. It seeks to distinguish between two competing explanations for why firms issue equity: opportunistic market timing versus regulatory pressure.\n\n**Setting / Data-Generating Environment:** The analysis uses a Log-Logistic duration model to analyze the time from a firm's Initial Public Offering (IPO) to its first SEO. The key empirical challenge is that firm profitability, stock returns, and market-to-book ratios are often correlated, making it difficult to isolate the true driver of SEO timing.\n\n### Data / Model Specification\n\nThe duration model estimates the effect of covariates on the time until an SEO. The model's interpretation rule is: *\"a significantly positive (negative) sign suggests that the variable is positively (negatively) related to the duration, but negatively (positively) related to the hazard probability and the probability of equity financing.\"* A negative coefficient thus implies a shorter time to an SEO and a higher probability of issuance.\n\nTwo primary hypotheses are considered:\n1.  **Market Timing Hypothesis:** Managers opportunistically issue equity when they perceive their stock is overvalued, as proxied by a high Market-to-Book (`M/B`) ratio or high recent `Stock returns`.\n2.  **Regulatory Pressure Hypothesis:** The Chinese Securities Regulatory Commission (CSRC) requires firms to meet a minimum Return on Equity (`ROE`) to qualify for an SEO. This creates a powerful incentive for firms to report high profitability (`ROA` is used as a proxy) immediately before an issuance.\n\nTable 1 presents results from models testing the market timing hypothesis in isolation. Table 2 presents results from a multivariate \"horse race\" regression that includes both a profitability measure (`ROA`) and a market timing measure (`M/B` ratio) to see which has more explanatory power.\n\n**Table 1: Duration Analysis of Market Timing**\n\n| Variable | Model (1) Coeff. (t-stat) | Model (2) Coeff. (t-stat) |\n| :--- | :--- | :--- |\n| Market-to-book ratio (`M/B`) | -0.017** (-2.145) | | \n| Stock returns | | -1.515*** (-4.239) |\n\n*Source: Adapted from Table 6 of the paper. ** and *** denote significance at the 5% and 1% levels, respectively.*\n\n**Table 2: Duration Analysis of Profitability vs. Market Timing**\n\n| Variable | Coefficient (t-statistic) |\n| :--- | :--- |\n| Return on asset (`ROA`) | -4.477*** (-4.939) |\n| Market-to-book ratio (`M/B`) | -0.009 (-1.135) |\n\n*Source: Adapted from Table 8, Panel B of the paper. *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the provided data and interpretation rule, which of the following statements are correct interpretations of the regression results? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at the tables and the interpretation rule provided:\n\nThe interpretation rule states: \"a significantly positive (negative) sign suggests that the variable is positively (negatively) related to the duration, but negatively (positively) related to the hazard probability and the probability of equity financing.\" So a negative coefficient means shorter time to SEO and higher probability of issuance.\n\nAnalyzing each option:\n\nA) In Table 2, ROA has coefficient -4.477*** (significant at 1% level), but M/B has coefficient -0.009 with t-stat -1.135 (not significant). So both are NOT statistically significant. FALSE.\n\nB) In Table 2, ROA is significant (***) but M/B is not significant (no stars, low t-stat). TRUE.\n\nC) In Table 1, M/B has coefficient -0.017** (negative and significant). According to the interpretation rule, a negative coefficient means shorter time to SEO, not longer. FALSE.\n\nD) In Table 1 Model (2), Stock returns has coefficient -1.515*** (negative and significant). According to the interpretation rule, a negative coefficient means shorter time to SEO. TRUE.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 226, "Question": "### Background\n\n**Research Question.** This case examines the core hypotheses of public finance regarding the capitalization of local fiscal policies. Specifically, it investigates whether residential and business property owners have different preferences for public services versus tax burdens, leading to differential impacts on their respective property values. The analysis tests two primary hypotheses:\n\n- **Hypothesis 1 (H1):** Residential property values are more responsive to changes in school spending than to changes in property taxes.\n- **Hypothesis 2 (H2):** Business property values are more responsive to changes in property taxes than to changes in school spending.\n\n**Setting / Data-Generating Environment.** The study uses a panel of 152 communities in southeast Michigan from 1983-2002. In this setting, public schools are primarily financed by property taxes. The Tiebout model suggests that mobile agents (both residents and firms) sort into communities that offer their preferred tax-service package. Residents, who directly benefit from schools, may value them highly. Firms, modeled as profit-maximizers, may view property taxes as a direct cost of business while receiving little direct benefit from K-12 education.\n\n### Data / Model Specification\n\nThe study estimates a dynamic first-difference 2SLS model to determine the long-run elasticities of property values with respect to fiscal policy. The model includes contemporaneous and three-year lagged values of policy variables to capture the full capitalization effect over time. Table 1 below presents the estimated coefficients for both residential and business property value growth.\n\n**Table 1: Selected 2SLS Regression Coefficients for Property Value Growth**\n\n| Dependent Variable | Regressor | Coefficient | Std. Error |\n| :--- | :--- | :--- | :--- |\n| **ΔLog Residential Property Value** | | | |\n| | Log Property Tax | -0.256*** | (0.040) |\n| | Log Property Tax t-1 | -0.123*** | (0.020) |\n| | Log Property Tax t-2 | -0.029*** | (0.010) |\n| | Log Property Tax t-3 | 0.004 | (0.012) |\n| | Log School Spending | 0.623*** | (0.236) |\n| | Log School Spending t-1 | -0.029 | (0.186) |\n| | Log School Spending t-2 | -0.156 | (0.162) |\n| | Log School Spending t-3 | 0.157 | (0.111) |\n| **ΔLog Business Property Value** | | | |\n| | Log Property Tax | -0.194 | (0.136) |\n| | Log Property Tax t-1 | -0.403** | (0.185) |\n| | Log Property Tax t-2 | -0.601** | (0.275) |\n| | Log Property Tax t-3 | 0.216 | (0.172) |\n| | Log School Spending | -1.458* | (0.810) |\n| | Log School Spending t-1 | 0.057 | (0.428) |\n| | Log School Spending t-2 | -0.231 | (0.194) |\n| | Log School Spending t-3 | 0.494 | (0.471) |\n\n*Note: *, **, *** indicate significance at the 10%, 5%, and 1% levels, respectively. The long-run elasticity is the sum of the contemporaneous and lagged coefficients.*\n\n### Question\n\nBased on the long-run elasticities derived from the coefficients in Table 1, select all of the following statements that are correct.", "Options": {"A": "The long-run elasticity of residential property values with respect to school spending is approximately +0.60.", "B": "The results support Hypothesis 2 (business values are more responsive to taxes than spending), as the long-run tax effect is driven by statistically significant coefficients while the school spending effect is not statistically significant in most periods.", "C": "The long-run elasticity of residential property values with respect to property taxes is -0.256.", "D": "The results reject Hypothesis 1 (residential values are more responsive to spending than taxes) because the long-run tax elasticity is negative while the spending elasticity is positive."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to calculate long-run elasticities from a dynamic model and interpret them in the context of the paper's hypotheses. It uses an Atomic Decomposition strategy to create multiple correct statements from the original multi-part QA. The depth strategy is Computational Judgment. Distractors are based on common errors: (C) a Step-Omission Error, using only the contemporaneous coefficient, and (D) a Conceptual Opposite error, misinterpreting responsiveness as being about the sign of the effect rather than its absolute magnitude.", "qid": "226", "question": "### Background\n\n**Research Question.** This case examines the core hypotheses of public finance regarding the capitalization of local fiscal policies. Specifically, it investigates whether residential and business property owners have different preferences for public services versus tax burdens, leading to differential impacts on their respective property values. The analysis tests two primary hypotheses:\n\n- **Hypothesis 1 (H1):** Residential property values are more responsive to changes in school spending than to changes in property taxes.\n- **Hypothesis 2 (H2):** Business property values are more responsive to changes in property taxes than to changes in school spending.\n\n**Setting / Data-Generating Environment.** The study uses a panel of 152 communities in southeast Michigan from 1983-2002. In this setting, public schools are primarily financed by property taxes. The Tiebout model suggests that mobile agents (both residents and firms) sort into communities that offer their preferred tax-service package. Residents, who directly benefit from schools, may value them highly. Firms, modeled as profit-maximizers, may view property taxes as a direct cost of business while receiving little direct benefit from K-12 education.\n\n### Data / Model Specification\n\nThe study estimates a dynamic first-difference 2SLS model to determine the long-run elasticities of property values with respect to fiscal policy. The model includes contemporaneous and three-year lagged values of policy variables to capture the full capitalization effect over time. Table 1 below presents the estimated coefficients for both residential and business property value growth.\n\n**Table 1: Selected 2SLS Regression Coefficients for Property Value Growth**\n\n| Dependent Variable | Regressor | Coefficient | Std. Error |\n| :--- | :--- | :--- | :--- |\n| **ΔLog Residential Property Value** | | | |\n| | Log Property Tax | -0.256*** | (0.040) |\n| | Log Property Tax t-1 | -0.123*** | (0.020) |\n| | Log Property Tax t-2 | -0.029*** | (0.010) |\n| | Log Property Tax t-3 | 0.004 | (0.012) |\n| | Log School Spending | 0.623*** | (0.236) |\n| | Log School Spending t-1 | -0.029 | (0.186) |\n| | Log School Spending t-2 | -0.156 | (0.162) |\n| | Log School Spending t-3 | 0.157 | (0.111) |\n| **ΔLog Business Property Value** | | | |\n| | Log Property Tax | -0.194 | (0.136) |\n| | Log Property Tax t-1 | -0.403** | (0.185) |\n| | Log Property Tax t-2 | -0.601** | (0.275) |\n| | Log Property Tax t-3 | 0.216 | (0.172) |\n| | Log School Spending | -1.458* | (0.810) |\n| | Log School Spending t-1 | 0.057 | (0.428) |\n| | Log School Spending t-2 | -0.231 | (0.194) |\n| | Log School Spending t-3 | 0.494 | (0.471) |\n\n*Note: *, **, *** indicate significance at the 10%, 5%, and 1% levels, respectively. The long-run elasticity is the sum of the contemporaneous and lagged coefficients.*\n\n### Question\n\nBased on the long-run elasticities derived from the coefficients in Table 1, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "I need to calculate the long-run elasticities from the coefficients in Table 1 and evaluate each statement.\n\n**Calculating Long-Run Elasticities:**\n\nFor Residential Property Values:\n- Property Tax: -0.256 + (-0.123) + (-0.029) + 0.004 = -0.404\n- School Spending: 0.623 + (-0.029) + (-0.156) + 0.157 = 0.595\n\nFor Business Property Values:\n- Property Tax: -0.194 + (-0.403) + (-0.601) + 0.216 = -0.982\n- School Spending: -1.458 + 0.057 + (-0.231) + 0.494 = -1.138\n\n**Evaluating Each Option:**\n\nA) The long-run elasticity of residential property values with respect to school spending is approximately +0.60.\n- Calculated: 0.595 ≈ 0.60\n- This is CORRECT.\n\nB) The results support Hypothesis 2 (business values are more responsive to taxes than spending), as the long-run tax effect is driven by statistically significant coefficients while the school spending effect is not statistically significant in most periods.\n- For business property values:\n  - Tax elasticity: -0.982 (absolute value = 0.982)\n  - School spending elasticity: -1.138 (absolute value = 1.138)\n- The absolute value of the school spending elasticity (1.138) is actually larger than the tax elasticity (0.982), which contradicts H2.\n- While it's true that more tax coefficients are significant, the statement about H2 being supported is INCORRECT.\n\nC) The long-run elasticity of residential property values with respect to property taxes is -0.256.\n- This is just the contemporaneous coefficient, not the long-run elasticity.\n- The long-run elasticity is -0.404.\n- This is INCORRECT.\n\nD) The results reject Hypothesis 1 (residential values are more responsive to spending than taxes) because the long-run tax elasticity is negative while the spending elasticity is positive.\n- This reasoning is", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 252, "Question": "### Background\n\n**Research Question.** What are the fundamental conditions that ensure valuation equivalence between accounting-based (e.g., Residual Income, RI) and cash-flow-based (e.g., Free Cash Flow, FCF) models?\n\n**Setting.** The paper argues that both RI and FCF models can be viewed as discounted dividend models. Therefore, for their valuations to be equivalent, both the numerators (expected dividends) and the denominators (the product of discount factors) in their respective valuation formulas must be identical.\n\n---\n\n### Question\n\nAccording to the paper, which of the following statements correctly describe the jointly sufficient conditions for ensuring valuation equivalence between Residual Income (RI) and Free Cash Flow (FCF) models? (Select all that apply)", "Options": {"A": "Imposing the Modigliani-Miller (MM) condition on discount rates is necessary to align the denominators (discount factors) of the valuation formulas.", "B": "Imposing the Modigliani-Miller (MM) condition on discount rates is necessary to align the numerators (expected dividends) of the valuation formulas.", "C": "Enforcing the income statement and balance sheet identities on forecasts is necessary to align the numerators (expected dividends) of the valuation formulas.", "D": "Using the exact same pro-forma spreadsheet for both models is the only way to ensure the accounting identities hold."}, "Answer": ["A", "C"], "pi_justification": "Assessment Target: This question tests the recall and understanding of the paper's central thesis—the two core conditions for valuation equivalence. It specifically assesses whether the student can correctly map each condition to its role in the valuation formula (numerator vs. denominator).\nStrategy: This is a select-all-valid task.\nDistractor Logic:\n- A & B are the two correct conditions and their correct mappings.\n- C (Conceptual Mismatch): This distractor incorrectly maps the MM condition to the numerator, testing for a precise understanding of its role.\n- D (Overly Restrictive): This distractor presents a sufficient but not necessary condition. The paper explicitly notes that other forecasting methods (like a constrained VAR) can also enforce the identities, so stating pro-formas are the *only* way is incorrect.", "qid": "252", "question": "### Background\n\n**Research Question.** What are the fundamental conditions that ensure valuation equivalence between accounting-based (e.g., Residual Income, RI) and cash-flow-based (e.g., Free Cash Flow, FCF) models?\n\n**Setting.** The paper argues that both RI and FCF models can be viewed as discounted dividend models. Therefore, for their valuations to be equivalent, both the numerators (expected dividends) and the denominators (the product of discount factors) in their respective valuation formulas must be identical.\n\n---\n\n### Question\n\nAccording to the paper, which of the following statements correctly describe the jointly sufficient conditions for ensuring valuation equivalence between Residual Income (RI) and Free Cash Flow (FCF) models? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at this question about valuation equivalence between RI and FCF models, I need to identify which statements correctly describe the jointly sufficient conditions.\n\nThe background tells us that:\n1. Both RI and FCF models can be viewed as discounted dividend models\n2. For valuation equivalence, both numerators (expected dividends) and denominators (discount factors) must be identical\n\nLet me analyze each option:\n\n**Option A**: \"Imposing the MM condition on discount rates is necessary to align the denominators (discount factors)\"\n- The MM condition relates to how discount rates adjust with leverage\n- This would indeed affect the denominators (discount factors) in the valuation formulas\n- This appears correct\n\n**Option B**: \"Imposing the MM condition on discount rates is necessary to align the numerators (expected dividends)\"\n- The MM condition is about discount rates, not about the cash flows or dividends themselves\n- This would not directly align the numerators\n- This appears incorrect\n\n**Option C**: \"Enforcing income statement and balance sheet identities on forecasts is necessary to align the numerators (expected dividends)\"\n- Accounting identities ensure consistency between income, cash flows, and balance sheet items\n- This consistency is crucial for ensuring that both models arrive at the same expected dividends\n- This appears correct\n\n**Option D**: \"Using the exact same pro-forma spreadsheet for both models is the only way to ensure accounting identities hold\"\n- While using the same spreadsheet might help, it's not the \"only way\" to ensure accounting identities hold\n- The statement is too restrictive with \"only way\"\n- This appears incorrect\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 157, "Question": "### Background\n\n**Research Question.** This case investigates the core innovation of the paper: the use of a deep neural network (NN) as a computationally efficient surrogate for the bond pricing function in complex, non-linear term structure models.\n\n**Setting / Data-Generating Environment.** The paper proposes a two-stage solution: first, use a slow but accurate Monte Carlo (MC) simulation to generate a large training dataset of model parameter inputs and corresponding bond yield outputs. Second, train a deep NN to learn this mapping. The trained NN is a function `ŷ = ŷ(θ)` composed of alternating linear transformations (via weight matrices `β` and bias vectors `α`) and non-linear activation functions `κ(.)`. The trained NN can then be used as a rapid pricing engine inside a Bayesian MCMC estimation loop.\n\n### Question\n\nSelect all statements that accurately describe the paper's proposed methodology.", "Options": {"A": "The computationally intensive MC simulation is performed only once (offline) to create the training data, making the millions of subsequent (online) price calculations required by the MCMC algorithm fast and feasible.", "B": "The non-linear activation function `κ(.)` is critical because, without it, the entire multi-layer network would collapse into a single, less expressive linear function.", "C": "The primary role of the bias vectors `α` is to learn the complex interactions between inputs, while the weight matrices `β` scale the outputs of the activation functions.", "D": "The MC estimator is not used directly within the MCMC loop because it is less accurate than the trained neural network."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: Tests understanding of the NN architecture and the logic of the two-stage training/estimation process. Strategy: Select-all-valid task. Distractor Logic: Options A and B are correct statements summarizing key aspects of the methodology. Option C is incorrect; it reverses the roles of weights (interactions) and biases (shifts/translations). Option D is incorrect; it misidentifies the reason for not using MC directly. The paper shows MC can be highly accurate (by increasing simulations), but it is prohibitively slow, which is the actual reason it's replaced by the NN for inference.", "qid": "157", "question": "### Background\n\n**Research Question.** This case investigates the core innovation of the paper: the use of a deep neural network (NN) as a computationally efficient surrogate for the bond pricing function in complex, non-linear term structure models.\n\n**Setting / Data-Generating Environment.** The paper proposes a two-stage solution: first, use a slow but accurate Monte Carlo (MC) simulation to generate a large training dataset of model parameter inputs and corresponding bond yield outputs. Second, train a deep NN to learn this mapping. The trained NN is a function `ŷ = ŷ(θ)` composed of alternating linear transformations (via weight matrices `β` and bias vectors `α`) and non-linear activation functions `κ(.)`. The trained NN can then be used as a rapid pricing engine inside a Bayesian MCMC estimation loop.\n\n### Question\n\nSelect all statements that accurately describe the paper's proposed methodology.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) This accurately describes the two-stage approach: MC simulation is run once offline to generate training data, then the trained NN enables fast online calculations during MCMC. This is the core computational efficiency gain.\n\nB) This is correct. Without non-linear activation functions, multiple linear layers would mathematically collapse to a single linear transformation (since linear functions composed together remain linear), severely limiting the network's ability to approximate complex non-linear functions.\n\nC) This is incorrect. In neural networks, weight matrices β learn the interactions and relationships between inputs/features, while bias vectors α provide offset/shift parameters. The statement reverses their roles.\n\nD) This is incorrect. The MC estimator isn't used in the MCMC loop because it's too computationally expensive (slow), not because it's less accurate. In fact, MC simulation provides the ground truth for training the NN.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 277, "Question": "### Background\n\n**Research Question.** What is the joint and incremental value relevance of book value (a stock measure from the balance sheet) and earnings (a flow measure from the income statement) for valuing international banks?\n\n**Setting / Data-Generating Environment.** The analysis uses a pooled sample of 28,786 quarterly observations for 813 international banking institutions during the 1993–2004 period.\n\n### Data / Model Specification\n\nThree pooled OLS regressions are estimated to assess the value relevance of book value per share (`BV`) and earnings per share (`EPS`) for explaining market value per share (`MV`):\n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + e_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} EPS_i + e_{i} \\quad \\text{(Eq. 2)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + \\beta_{2} EPS_i + e_{i} \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Pooled OLS Regressions of Value Relevance**\n\n| Variable          | Model 1 (Eq. 1) | Model 2 (Eq. 2) | Model 3 (Eq. 3) |\n| :---------------- | :-------------- | :-------------- | :-------------- |\n| Intercept         | 18.452***       | 17.160***       | 14.340***       |\n| Book Value (`β₁`) | 0.559***        |                 | 0.462***        |\n| Earnings (`β₂`)   |                 | 0.376***        | 0.241***        |\n| R-squared         | 0.291           | 0.250           | 0.357           |\n\n*Note: *** denotes significance at the 1% level.*\n\n### Question\n\nTo test the hypothesis that the value relevance of earnings is weaker for loss-making firms, a researcher wants to modify Model 3. Select all of the following that correctly describe a valid approach.", "Options": {"A": "The researcher should split the sample into profitable and loss-making firms and test if the R-squared of the regression for the loss-making sample is lower.", "B": "To test if the market's response to losses is weaker, the researcher should test the one-sided alternative hypothesis `Hₐ: δ < 0` against the null `H₀: δ = 0`.", "C": "The researcher should add an interaction term, `δ * (D_loss * EPS)`, where `D_loss` is a dummy variable equal to 1 for firms with negative EPS. The coefficient for loss-making firms would then be `β₂ + δ`.", "D": "If the hypothesis is true, the main effect of earnings, `β₂`, should become statistically insignificant in the modified model."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses advanced modeling skills, specifically how to specify and test for asymmetric effects (a non-linearity). It uses a Reverse-Reasoning strategy, asking for the correct setup to test a given hypothesis. Correct options require specifying the correct interaction term model (A) and the corresponding one-sided hypothesis test (B). Distractor C misidentifies which coefficient is the target of the test, while D proposes an alternative but less direct and formally inferior testing procedure (comparing R-squared).", "qid": "277", "question": "### Background\n\n**Research Question.** What is the joint and incremental value relevance of book value (a stock measure from the balance sheet) and earnings (a flow measure from the income statement) for valuing international banks?\n\n**Setting / Data-Generating Environment.** The analysis uses a pooled sample of 28,786 quarterly observations for 813 international banking institutions during the 1993–2004 period.\n\n### Data / Model Specification\n\nThree pooled OLS regressions are estimated to assess the value relevance of book value per share (`BV`) and earnings per share (`EPS`) for explaining market value per share (`MV`):\n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + e_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} EPS_i + e_{i} \\quad \\text{(Eq. 2)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + \\beta_{2} EPS_i + e_{i} \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Pooled OLS Regressions of Value Relevance**\n\n| Variable          | Model 1 (Eq. 1) | Model 2 (Eq. 2) | Model 3 (Eq. 3) |\n| :---------------- | :-------------- | :-------------- | :-------------- |\n| Intercept         | 18.452***       | 17.160***       | 14.340***       |\n| Book Value (`β₁`) | 0.559***        |                 | 0.462***        |\n| Earnings (`β₂`)   |                 | 0.376***        | 0.241***        |\n| R-squared         | 0.291           | 0.250           | 0.357           |\n\n*Note: *** denotes significance at the 1% level.*\n\n### Question\n\nTo test the hypothesis that the value relevance of earnings is weaker for loss-making firms, a researcher wants to modify Model 3. Select all of the following that correctly describe a valid approach.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate which approaches correctly test whether the value relevance of earnings is weaker for loss-making firms.\n\nLet me analyze each option:\n\n**Option A**: Splitting the sample and comparing R-squared values\n- This is a valid approach to test overall model fit differences between groups\n- If earnings have weaker value relevance for loss-making firms, we'd expect lower R-squared for that subsample\n- This is a common method in accounting research for testing differential value relevance\n- ✓ Valid approach\n\n**Option B**: Testing one-sided hypothesis Ha: δ < 0\n- If δ represents the interaction coefficient (as suggested in option C), then δ < 0 would mean the earnings coefficient for loss-making firms (β₂ + δ) is less than for profitable firms (β₂)\n- This correctly tests whether earnings have weaker value relevance for loss-making firms\n- ✓ Valid approach\n\n**Option C**: Adding interaction term δ * (D_loss * EPS)\n- This is the standard approach for testing differential coefficients across groups\n- D_loss = 1 for negative EPS firms, 0 otherwise\n- For profitable firms: coefficient = β₂\n- For loss-making firms: coefficient = β₂ + δ\n- If δ is negative and significant, it indicates weaker value relevance for losses\n- ✓ Valid approach\n\n**Option D**: Main effect β₂ becoming insignificant\n- This is incorrect reasoning\n- β₂ represents the earnings coefficient for the reference group (profitable firms when D_loss = 0)\n- Even if earnings have weaker relevance for losses, they can still be highly relevant for profits\n- The hypothesis concerns the differential effect (δ), not the main effect (β₂)\n- ✗ Invalid approach\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 239, "Question": "### Background\n\nThe multivariate random walk plus noise model provides a structural framework for analyzing price discovery. A key identification challenge is that the vector of reaction parameters, `α`, is not uniquely identified from the data.\n\n### Data / Model Specification\n\nThe asymmetry in the first-lag autocovariance matrix of price changes, `Γ_1`, is linked to `α` by the moment condition:\n\n  \n\\Gamma_{1}-\\Gamma_{1}^{\\prime}=\\sigma^{2}(\\iota\\alpha^{\\prime}-\\alpha\\iota^{\\prime}) \\quad \\text{(Eq. (1))}\n \n\nWhere `ι` is a vector of ones. The `(i,j)` element of the matrix on the right is `σ^2(α_j - α_i)`. This implies that if `α` is a valid solution, any vector `α̃ = α - wι` (where `w` is an arbitrary scalar) is also a solution, as it produces the same observable differences `α̃_j - α̃_i = α_j - α_i`.\n\n---\n\nWhich of the following are valid methods for achieving full identification of the `α` vector, as discussed in the paper?", "Options": {"A": "The 'Beveridge-Nelson (BN) normalization', which is derived from the reduced-form VMA representation and provides one admissible solution for `α`.", "B": "The 'Watson normalization', which designates one market `i` as a central market and sets its reaction parameter `α_i` to zero.", "C": "Assuming that the reduced-form VMA error covariance matrix, `Σ`, is diagonal.", "D": "Imposing the restriction that the idiosyncratic noise covariance matrix, `Ω`, is diagonal, which provides testable over-identifying restrictions for systems with more than two markets (`N > 2`)."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses the user's comprehensive knowledge of the different identification strategies discussed in the paper to resolve the under-identification of the `α` vector. It is a direct test of factual recall and conceptual understanding. Options A, B, and C describe three distinct and valid identification schemes discussed in the text (Watson, diagonal `Ω`, and BN). Option D is a conceptual distractor; the paper explicitly states that diagonality of the reduced-form covariance matrix `Σ` is 'violated in any empirical application', distinguishing it from the plausible assumption of a diagonal structural noise matrix `Ω`.", "qid": "239", "question": "### Background\n\nThe multivariate random walk plus noise model provides a structural framework for analyzing price discovery. A key identification challenge is that the vector of reaction parameters, `α`, is not uniquely identified from the data.\n\n### Data / Model Specification\n\nThe asymmetry in the first-lag autocovariance matrix of price changes, `Γ_1`, is linked to `α` by the moment condition:\n\n  \n\\Gamma_{1}-\\Gamma_{1}^{\\prime}=\\sigma^{2}(\\iota\\alpha^{\\prime}-\\alpha\\iota^{\\prime}) \\quad \\text{(Eq. (1))}\n \n\nWhere `ι` is a vector of ones. The `(i,j)` element of the matrix on the right is `σ^2(α_j - α_i)`. This implies that if `α` is a valid solution, any vector `α̃ = α - wι` (where `w` is an arbitrary scalar) is also a solution, as it produces the same observable differences `α̃_j - α̃_i = α_j - α_i`.\n\n---\n\nWhich of the following are valid methods for achieving full identification of the `α` vector, as discussed in the paper?", "model_evaluation_result": {"model_raw_response": "Looking at this price discovery model, I need to identify which methods achieve full identification of the α vector.\n\nFrom equation (1), we can see that α is only identified up to differences (αⱼ - αᵢ), meaning any transformation α̃ = α - wι produces the same observable moments. This creates an identification problem that requires additional restrictions.\n\nLet me evaluate each option:\n\nA) The Beveridge-Nelson (BN) normalization: This is a standard approach in the price discovery literature that comes from the reduced-form VMA representation. It provides a specific normalization that pins down one particular solution for α among the infinite set of observationally equivalent solutions. This is a valid identification method.\n\nB) The Watson normalization: This sets one market's reaction parameter (say α₁) to zero, which effectively normalizes all other αᵢ values relative to this benchmark market. Since α₁ = 0 fixes the arbitrary constant w, this resolves the identification problem. This is also a valid identification method.\n\nC) Diagonal Σ (reduced-form error covariance): This assumption alone doesn't resolve the fundamental identification issue. The problem stems from the structural relationship in equation (1), not from the reduced-form error structure. A diagonal Σ doesn't provide the necessary restriction to pin down the level of α.\n\nD) Diagonal Ω (idiosyncratic noise covariance): While this provides testable restrictions when N > 2, it doesn't directly address the identification problem of α. The identification issue exists because we can only identify differences αⱼ - αᵢ, and assuming diagonal Ω doesn't change this fundamental limitation.\n\nTherefore, only the normalization approaches (A and B) actually achieve full identification of α.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 254, "Question": "### Background\n\n**Research Question.** How do different tail-focused premium principles comparatively rank two risks with identical mean and variance but different tail thickness?\n\n**Setting.** A numerical example comparing premiums for a heavy-tailed Pareto risk (`X`) and a less heavy-tailed Weibull risk (`Y`). Both risks have been constructed to have the same mean (1.0) and variance (3.0). The goal is to see if the premium principles can correctly identify the Pareto as the more dangerous risk.\n\n**Variables and Parameters.**\n\n*   `X`: A Pareto-distributed loss (heavy-tailed).\n*   `Y`: A Weibull-distributed loss (thinner-tailed).\n*   `p`: The probability level defining the tail.\n*   `T_{1,1,1}`: Premium principle based on cumulative residual entropy.\n*   `T_{1,2,1}`: Premium principle based on Gini Mean Difference.\n*   `T_{1,0.5,1}`: Premium principle based on Wang's right tail deviation.\n\n---\n\n### Data / Model Specification\n\nTwo risks, `X` (Pareto) and `Y` (Weibull), have `E[X] = E[Y] = 1` and `Var(X) = Var(Y) = 3`. The Pareto distribution is known to have a heavier tail than this specific Weibull distribution.\n\n**Table 1: Premiums for Pareto Risk `X` at p=0.99**\n\n| T<sub>1,1,1</sub>(X,0.99) (Entropy) | T<sub>1,2,1</sub>(X,0.99) (Gini) | T<sub>1,0.5,1</sub>(X,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 18.887 | 14.710 | 39.774 |\n\n**Table 2: Premiums for Weibull Risk `Y` at p=0.99**\n\n| T<sub>1,1,1</sub>(Y,0.99) (Entropy) | T<sub>1,2,1</sub>(Y,0.99) (Gini) | T<sub>1,0.5,1</sub>(Y,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 15.476 | 13.510 | 19.911 |\n\n---\n\n### Question\n\nAn actuary and a regulator are evaluating the data in **Table 1** and **Table 2**. Based on this evidence, select all valid conclusions.", "Options": {"A": "An actuary who is uncertain about the true tail shape and wants to set a price that is least dependent on this specific assumption would favor the Gini-based principle (`T_{1,2,1}`).", "B": "A regulator whose primary goal is to ensure solvency by heavily penalizing potential model misspecification toward heavy tails would prefer the Wang-based principle (`T_{1,0.5,1}`).", "C": "All three principles are equally suitable for setting regulatory capital, as they all correctly identify the Pareto risk as the more dangerous one.", "D": "For regulatory purposes, the Gini-based principle (`T_{1,2,1}`) is superior because it results in the highest premium for the heavy-tailed Pareto risk, ensuring maximum safety."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to apply empirical findings to a practical scenario involving risk management and regulation. It uses a Scenario Application strategy. The correct options require connecting the observed sensitivity of the Wang principle to regulatory goals and the observed robustness of the Gini principle to pricing stability goals. Distractors are designed to be plausible but incorrect: (C) contains a direct Factual Error from the tables (Wang's premium is highest), and (D) is a 'True but Irrelevant' statement that ignores the critical differences in magnitude and sensitivity, which are paramount in this context.", "qid": "254", "question": "### Background\n\n**Research Question.** How do different tail-focused premium principles comparatively rank two risks with identical mean and variance but different tail thickness?\n\n**Setting.** A numerical example comparing premiums for a heavy-tailed Pareto risk (`X`) and a less heavy-tailed Weibull risk (`Y`). Both risks have been constructed to have the same mean (1.0) and variance (3.0). The goal is to see if the premium principles can correctly identify the Pareto as the more dangerous risk.\n\n**Variables and Parameters.**\n\n*   `X`: A Pareto-distributed loss (heavy-tailed).\n*   `Y`: A Weibull-distributed loss (thinner-tailed).\n*   `p`: The probability level defining the tail.\n*   `T_{1,1,1}`: Premium principle based on cumulative residual entropy.\n*   `T_{1,2,1}`: Premium principle based on Gini Mean Difference.\n*   `T_{1,0.5,1}`: Premium principle based on Wang's right tail deviation.\n\n---\n\n### Data / Model Specification\n\nTwo risks, `X` (Pareto) and `Y` (Weibull), have `E[X] = E[Y] = 1` and `Var(X) = Var(Y) = 3`. The Pareto distribution is known to have a heavier tail than this specific Weibull distribution.\n\n**Table 1: Premiums for Pareto Risk `X` at p=0.99**\n\n| T<sub>1,1,1</sub>(X,0.99) (Entropy) | T<sub>1,2,1</sub>(X,0.99) (Gini) | T<sub>1,0.5,1</sub>(X,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 18.887 | 14.710 | 39.774 |\n\n**Table 2: Premiums for Weibull Risk `Y` at p=0.99**\n\n| T<sub>1,1,1</sub>(Y,0.99) (Entropy) | T<sub>1,2,1</sub>(Y,0.99) (Gini) | T<sub>1,0.5,1</sub>(Y,0.99) (Wang) |\n|:---:|:---:|:---:|\n| 15.476 | 13.510 | 19.911 |\n\n---\n\n### Question\n\nAn actuary and a regulator are evaluating the data in **Table 1** and **Table 2**. Based on this evidence, select all valid conclusions.", "model_evaluation_result": {"model_raw_response": "Looking at the data:\n\nFor Pareto (X): T₁,₁,₁ = 18.887, T₁,₂,₁ = 14.710, T₁,₀.₅,₁ = 39.774\nFor Weibull (Y): T₁,₁,₁ = 15.476, T₁,₂,₁ = 13.510, T₁,₀.₅,₁ = 19.911\n\nAll three principles correctly identify Pareto as riskier (higher premiums).\n\nAnalyzing each option:\n\nA) The Gini-based principle shows the smallest difference between Pareto and Weibull premiums (14.710 vs 13.510 = 8.9% difference), compared to Entropy (22.1% difference) and Wang (99.7% difference). This makes it least sensitive to tail shape differences, supporting the claim that an actuary uncertain about tail shape would favor it.\n\nB) The Wang-based principle shows the largest difference between heavy and light tails (39.774 vs 19.911), indicating it most heavily penalizes heavy tails. This aligns with a regulator wanting to ensure solvency by penalizing model misspecification toward heavy tails.\n\nC) While all three correctly rank Pareto as riskier, they differ dramatically in their sensitivity to tail risk (Wang shows 99.7% difference vs Gini's 8.9%). This makes them not \"equally suitable\" for regulatory capital.\n\nD) The Wang-based principle (39.774) produces the highest premium for Pareto, not Gini (14.710). This statement is factually incorrect.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 262, "Question": "### Background\n\nThis case investigates internal tensions within family-controlled firms in Taiwan's \"insider-oriented\" corporate governance system. In such systems, the primary agency conflict (Type II) is between controlling shareholders (the family) and minority shareholders. This contrasts with \"outsider systems\" where the main conflict (Type I) is between professional managers and dispersed shareholders. As Taiwanese firms hire professional managers, a hybrid conflict emerges.\n\n### Data / Model Specification\n\n**Hypothesis Two:** Company directors view the level of family control in Taiwan as a hindrance to good corporate governance and as an area where improvements and controls are necessary.\n\nRelevant empirical findings from the survey include:\n- 65% of responding directors believe that directors are selected by family members.\n- Directors strongly disagree with the statement \"Outside directors should have family ties to other directors, chairman, and CEO\" (mean score of 2.11 on a 7-point scale where 1=strongly disagree).\n\n### Question\n\nBased on the provided context and empirical findings, which of the following are valid interpretations supporting Hypothesis Two? Select all that apply.", "Options": {"A": "The data reveals a paradox where directors, though often appointed by families, recognize the need for professional checks and balances on family power.", "B": "The findings indicate that directors believe the primary agency problem in Taiwan is the Type I conflict between managers and shareholders, not the Type II conflict involving family control.", "C": "The strong opposition to family ties for outside directors suggests a desire for objective, independent oversight to counterbalance the influence of the controlling family.", "D": "The fact that 65% of directors are selected by families implies that directors are fully aligned with family interests and see no need for governance reform."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to synthesize qualitative and quantitative information to interpret an apparent paradox, which was the core of the original QA problem. It uses a Reverse-Reasoning strategy. Option A is a direct, valid interpretation of the low mean score on the 'family ties' question. Option B correctly identifies and explains the paradox that directors appointed by families still want independent oversight, which strongly supports the hypothesis. Distractor C is a conceptual error, as the text explicitly states the Type II conflict is primary in insider systems. Distractor D is a plausible but incorrect inference; it ignores the countervailing evidence from the 'family ties' question and misinterprets the central tension.", "qid": "262", "question": "### Background\n\nThis case investigates internal tensions within family-controlled firms in Taiwan's \"insider-oriented\" corporate governance system. In such systems, the primary agency conflict (Type II) is between controlling shareholders (the family) and minority shareholders. This contrasts with \"outsider systems\" where the main conflict (Type I) is between professional managers and dispersed shareholders. As Taiwanese firms hire professional managers, a hybrid conflict emerges.\n\n### Data / Model Specification\n\n**Hypothesis Two:** Company directors view the level of family control in Taiwan as a hindrance to good corporate governance and as an area where improvements and controls are necessary.\n\nRelevant empirical findings from the survey include:\n- 65% of responding directors believe that directors are selected by family members.\n- Directors strongly disagree with the statement \"Outside directors should have family ties to other directors, chairman, and CEO\" (mean score of 2.11 on a 7-point scale where 1=strongly disagree).\n\n### Question\n\nBased on the provided context and empirical findings, which of the following are valid interpretations supporting Hypothesis Two? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which interpretations support Hypothesis Two, which states that company directors view family control as a hindrance to good corporate governance.\n\nLet me analyze each option:\n\n**Option A**: \"The data reveals a paradox where directors, though often appointed by families, recognize the need for professional checks and balances on family power.\"\n- This is supported by the data showing 65% of directors are selected by families, yet directors strongly disagree with outside directors having family ties (mean score 2.11/7)\n- This paradox directly supports the hypothesis that directors see family control as problematic\n\n**Option B**: \"The findings indicate that directors believe the primary agency problem in Taiwan is the Type I conflict between managers and shareholders, not the Type II conflict involving family control.\"\n- This contradicts the hypothesis, which is specifically about Type II conflicts (family control issues)\n- The background clearly states Type II conflict is the primary issue in Taiwan's insider-oriented system\n\n**Option C**: \"The strong opposition to family ties for outside directors suggests a desire for objective, independent oversight to counterbalance the influence of the controlling family.\"\n- The mean score of 2.11/7 (strong disagreement) with family ties for outside directors clearly indicates directors want independent oversight\n- This directly supports the hypothesis that directors see family control as needing improvement\n\n**Option D**: \"The fact that 65% of directors are selected by families implies that directors are fully aligned with family interests and see no need for governance reform.\"\n- This contradicts the empirical evidence showing directors strongly oppose family ties for outside directors\n- If directors were fully aligned with family interests, they wouldn't oppose such ties\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 236, "Question": "### Background\n\nA researcher estimates an Unobserved Components (UC) model for the two-minute midquotes of the five most active dealers in Intel stock in 1999. The goal is to obtain precise estimates of each dealer's contribution to price discovery.\n\n### Data / Model Specification\n\nThe UC model is estimated under two different identifying assumptions, yielding the results below.\n\n**Table 1: UC Model Estimation Results for Intel (from paper's Table 5)**\n\n| Panel | Identification Assumption | Dealer | `α` | `IS` | J-stat (df) |\n|:---|:---|:---|---:|---:|:---|\n| B | Approx. Diagonal `Ω` | ISLD | 0.003 | 0.251 | 103.21 (20) |\n| | | INCA | -0.044 | 0.461 | |\n| | | SLKC | -0.079 | 0.030 | |\n| C | Diagonal `Ω` (Imposed) | ISLD | 0.000 | 0.187 | 162.03 (29) |\n| | | INCA | -0.008 | 0.446 | |\n| | | SLKC | 0.087 | 0.155 | |\n\n*Note: Panel B is observationally equivalent to a VMA model. Panel C imposes 9 additional restrictions (the off-diagonal elements of the noise covariance matrix `Ω` are zero). The 5% critical value for a `χ^2` distribution with 9 degrees of freedom is approximately 16.92.*\n\n---\n\nBased on the provided results, select all of the following statements that are valid conclusions.", "Options": {"A": "The large increase in SLKC's information share from Panel B to Panel C is primarily driven by its `α` parameter changing from negative to positive, a shift forced by the GMM estimator to better fit contemporaneous covariances after the diagonality constraint was imposed.", "B": "The test statistic for the validity of the diagonal `Ω` restriction is calculated as `ΔJ = 162.03 - 103.21 = 58.82`.", "C": "The information share (IS) estimates under the diagonal `Ω` assumption (Panel C) provide a clear ranking, with INCA being the most informative dealer, followed by ISLD, and then SLKC.", "D": "A formal specification test fails to reject the null hypothesis that the `Ω` matrix is diagonal at the 5% significance level."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret empirical results, conduct a formal hypothesis test, and understand the underlying mechanics of GMM estimation. It uses a Scenario Application strategy. Option A tests direct interpretation of the table. Option D tests the calculation of the test statistic. Option C assesses a deep understanding of the trade-offs in constrained estimation, as explained in the paper. Option B is a conceptual distractor; the test statistic (58.82) is far greater than the critical value (16.92), leading to a strong rejection of the null hypothesis.", "qid": "236", "question": "### Background\n\nA researcher estimates an Unobserved Components (UC) model for the two-minute midquotes of the five most active dealers in Intel stock in 1999. The goal is to obtain precise estimates of each dealer's contribution to price discovery.\n\n### Data / Model Specification\n\nThe UC model is estimated under two different identifying assumptions, yielding the results below.\n\n**Table 1: UC Model Estimation Results for Intel (from paper's Table 5)**\n\n| Panel | Identification Assumption | Dealer | `α` | `IS` | J-stat (df) |\n|:---|:---|:---|---:|---:|:---|\n| B | Approx. Diagonal `Ω` | ISLD | 0.003 | 0.251 | 103.21 (20) |\n| | | INCA | -0.044 | 0.461 | |\n| | | SLKC | -0.079 | 0.030 | |\n| C | Diagonal `Ω` (Imposed) | ISLD | 0.000 | 0.187 | 162.03 (29) |\n| | | INCA | -0.008 | 0.446 | |\n| | | SLKC | 0.087 | 0.155 | |\n\n*Note: Panel B is observationally equivalent to a VMA model. Panel C imposes 9 additional restrictions (the off-diagonal elements of the noise covariance matrix `Ω` are zero). The 5% critical value for a `χ^2` distribution with 9 degrees of freedom is approximately 16.92.*\n\n---\n\nBased on the provided results, select all of the following statements that are valid conclusions.", "model_evaluation_result": {"model_raw_response": "Looking at this UC model estimation problem, I need to evaluate each statement based on the provided data.\n\nLet me analyze each option:\n\n**Option A**: This statement claims that SLKC's information share increase (from 0.030 to 0.155) is \"primarily driven\" by its α parameter changing from -0.079 to 0.087. While both changes occurred, the statement makes a causal claim about what \"drove\" the change that cannot be verified from the data alone. The relationship between α and IS in UC models is complex and depends on the full model structure.\n\n**Option B**: The test statistic for comparing nested models is indeed calculated as the difference in J-statistics: 162.03 - 103.21 = 58.82. This is correct.\n\n**Option C**: In Panel C, the information shares are:\n- INCA: 0.446\n- ISLD: 0.187  \n- SLKC: 0.155\n\nThis gives a clear ranking of INCA > ISLD > SLKC, which matches the statement.\n\n**Option D**: To test the diagonal Ω restriction:\n- Test statistic: 58.82 (from option B)\n- Degrees of freedom: 9 (the additional restrictions imposed)\n- Critical value at 5%: approximately 16.92\n\nSince 58.82 > 16.92, we reject the null hypothesis that Ω is diagonal. Therefore, this statement is false.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 210, "Question": "### Background\n\nThe paper critiques the FHA's use of a **mortgage-based** data framework for risk assessment, arguing it systematically understates risk compared to a **borrower-based** framework. The key difference is the treatment of internal FHA-to-FHA refinances.\n\n*   In the **mortgage-based** framework, an internal refinance is recorded as a *prepayment* of the old loan. When forecasting, the model treats all predicted prepayments as terminal events, removing the loan's risk from the portfolio.\n*   In the **borrower-based** framework, an internal refinance is an intermediate event. The borrower's risk exposure only terminates upon a true exit from the FHA system (e.g., sale) or a default.\n\nThe FHA defends its mortgage-based approach by arguing that *in-sample*, the new refinanced loan replaces the old one, so the risk is not lost from the historical data used for model estimation.\n\n### Question\n\nAccording to the paper's argument, which of the following are valid consequences or critiques of the FHA's mortgage-based risk assessment methodology?", "Options": {"A": "The FHA's in-sample defense is valid, proving that the mortgage-based framework is robust for both estimation and forecasting.", "B": "It causes a temporal misattribution of risk, where a default by a borrower from an early vintage (e.g., 2007) who refinanced is incorrectly assigned to a later vintage (e.g., 2009).", "C": "The methodology suffers from an out-of-sample forecasting fallacy, as the simulation of future prepayments fails to carry forward the risk of borrowers who internally refinance.", "D": "It systematically overestimates the rate at which risk leaves the FHA system, leading to an underestimation of future cumulative defaults."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the comprehension of the paper's central methodological critique regarding the mortgage-based vs. borrower-based frameworks. Depth Strategy: Reverse-Reasoning (identifying all valid consequences of a flawed methodology). Distractor Logic: Options A, B, and C are all distinct, correct restatements of the paper's core arguments against the FHA's methodology. Option D is a 'Conceptual Opposite' distractor; it presents the FHA's flawed defense as a valid conclusion, directly contradicting the paper's main thesis.", "qid": "210", "question": "### Background\n\nThe paper critiques the FHA's use of a **mortgage-based** data framework for risk assessment, arguing it systematically understates risk compared to a **borrower-based** framework. The key difference is the treatment of internal FHA-to-FHA refinances.\n\n*   In the **mortgage-based** framework, an internal refinance is recorded as a *prepayment* of the old loan. When forecasting, the model treats all predicted prepayments as terminal events, removing the loan's risk from the portfolio.\n*   In the **borrower-based** framework, an internal refinance is an intermediate event. The borrower's risk exposure only terminates upon a true exit from the FHA system (e.g., sale) or a default.\n\nThe FHA defends its mortgage-based approach by arguing that *in-sample*, the new refinanced loan replaces the old one, so the risk is not lost from the historical data used for model estimation.\n\n### Question\n\nAccording to the paper's argument, which of the following are valid consequences or critiques of the FHA's mortgage-based risk assessment methodology?", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) This contradicts the paper's main critique. The paper argues that while the FHA's in-sample defense may have some merit for estimation, it fails for forecasting. The mortgage-based framework is not robust for forecasting.\n\nB) This is a valid consequence. When a borrower refinances internally, their original vintage loan is marked as prepaid, and any subsequent default is attributed to the new refinance vintage. This creates temporal misattribution of risk.\n\nC) This is a valid critique and represents the core of the paper's argument. When the model simulates future prepayments, it treats them as terminal events, but internal refinances should carry forward risk since the borrower remains in the FHA system.\n\nD) This is a valid consequence. By treating internal refinances as prepayments (terminal events), the mortgage-based framework assumes risk leaves the system when it actually doesn't, leading to underestimation of future defaults.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 250, "Question": "### Background\n\nLate in the campaign against the Volvo-Renault merger, the Swedish Small Shareholders' Association (Aktiespararna) introduced a novel legal threat. This tactic was designed to shift the risk-reward calculation for Volvo's Board of Directors, moving the potential consequences from the realm of business judgment to legal and personal liability.\n\n### Data / Model Specification\n\n1.  **The Legal Argument:** The Aktiespararna claimed the merger violated Volvo's by-laws, which allegedly required the company to maintain at least a 50% majority ownership of its core business. The proposed deal would give Volvo only a 35% stake.\n2.  **The Threat:** The association publicly announced it would sue to overturn the merger if the shareholder vote passed.\n3.  **The Consequence:** A successful lawsuit would be \"disastrous for the board,\" potentially exposing them to personal financial liability for knowingly violating the company's by-laws, in addition to the immense corporate cost of unwinding a consummated merger.\n\n---\n\nBased on this information, select all of the following statements that are valid assessments of the Aktiespararna's legal strategy.\n", "Options": {"A": "The strategy's primary goal was to increase the potential upside for shareholders by securing a better merger price from Renault.", "B": "The threat fundamentally altered the board's decision calculus by introducing a low-probability but catastrophic-cost event (losing the lawsuit), which could make the expected value of proceeding with the vote negative.", "C": "The legal argument shifted the debate from a subjective question of financial merit (which is debatable) to an objective question of legal compliance with the corporate charter.", "D": "The legal threat was powerful because it sought to pierce the protections of the business judgment rule, which typically shields directors from liability for decisions made in good faith."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of corporate governance and shareholder activism, specifically the strategic use of litigation threats to influence board decisions.\nDepth Strategy: Scenario Application. The candidate must apply principles of corporate law (business judgment rule, fiduciary duty) and decision theory (expected value under uncertainty) to the specific scenario described.\nDistractor Logic:\n- A: Correct. A core concept in corporate law is that the business judgment rule does not protect directors who act illegally or violate the corporate charter. The lawsuit was designed to frame the board's action this way, exposing them to personal liability.\n- B: This is a Conceptual Distractor (True but Irrelevant/Incorrect Goal). The stated goal of the Aktiespararna was to kill the deal entirely, not to negotiate a better price. Their actions were aimed at making the deal too risky to proceed with at all.\n- C: Correct. This accurately describes the risk from a decision theory perspective. Even if the board felt they would win the shareholder vote and likely win the lawsuit, the sheer magnitude of the cost of losing (`C_L`) could make the risk unacceptable, driving the expected value of proceeding below the expected value of withdrawing.\n- D: Correct. This captures the strategic brilliance of the move. Financial arguments are matters of opinion. A legal argument about by-laws is a matter of fact and law, making it a much more potent and less ambiguous threat for a risk-averse board.", "qid": "250", "question": "### Background\n\nLate in the campaign against the Volvo-Renault merger, the Swedish Small Shareholders' Association (Aktiespararna) introduced a novel legal threat. This tactic was designed to shift the risk-reward calculation for Volvo's Board of Directors, moving the potential consequences from the realm of business judgment to legal and personal liability.\n\n### Data / Model Specification\n\n1.  **The Legal Argument:** The Aktiespararna claimed the merger violated Volvo's by-laws, which allegedly required the company to maintain at least a 50% majority ownership of its core business. The proposed deal would give Volvo only a 35% stake.\n2.  **The Threat:** The association publicly announced it would sue to overturn the merger if the shareholder vote passed.\n3.  **The Consequence:** A successful lawsuit would be \"disastrous for the board,\" potentially exposing them to personal financial liability for knowingly violating the company's by-laws, in addition to the immense corporate cost of unwinding a consummated merger.\n\n---\n\nBased on this information, select all of the following statements that are valid assessments of the Aktiespararna's legal strategy.\n", "model_evaluation_result": {"model_raw_response": "Looking at each option carefully:\n\nA) The strategy's primary goal was to increase the potential upside for shareholders by securing a better merger price from Renault.\n- This is incorrect. The Aktiespararna was opposing the merger entirely, not trying to negotiate better terms. They wanted to stop the merger, not improve it.\n\nB) The threat fundamentally altered the board's decision calculus by introducing a low-probability but catastrophic-cost event (losing the lawsuit), which could make the expected value of proceeding with the vote negative.\n- This is correct. The threat of personal liability and the \"disastrous\" consequences of unwinding a consummated merger created a classic low-probability/high-impact risk that could make proceeding unattractive even if the lawsuit had a small chance of success.\n\nC) The legal argument shifted the debate from a subjective question of financial merit (which is debatable) to an objective question of legal compliance with the corporate charter.\n- This is correct. By focusing on whether the merger violated the by-laws (an objective legal question), the Aktiespararna moved away from subjective business judgment about whether the merger was financially beneficial.\n\nD) The legal threat was powerful because it sought to pierce the protections of the business judgment rule, which typically shields directors from liability for decisions made in good faith.\n- This is correct. The business judgment rule normally protects directors from liability for business decisions, but knowingly violating corporate by-laws would likely fall outside this protection, exposing directors to personal liability.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 248, "Question": "### Background\n\nThe public debate surrounding the 1993 Volvo-Renault merger pitted two competing frameworks against each other. The pro-merger rationale, often summarized as the \"Big is Beautiful\" argument, focused on the strategic imperative of achieving economies of scale in the global automotive industry. This view was contrasted with a critical shareholder value analysis, championed by journalist Peter Malmqvist, which argued the deal was financially detrimental to Volvo's owners.\n\n### Data / Model Specification\n\nLet `V_V` be the standalone market value of Volvo, `V_R` be the standalone market value of Renault, and `S` be the present value of all future synergies from the merger, estimated at over $5 billion. In the proposed deal, Volvo shareholders contribute their entire firm (value `V_V`) and in return receive a 35% stake in the new, combined firm (RVA).\n\nThe Net Present Value (NPV) of the merger to Volvo's shareholders can be expressed as the value of what they receive minus the value of what they give up:\n\n  \nNPV_V = (0.35 \\times (V_V + V_R + S)) - V_V\n \n\nThis can be rearranged to:\n\n  \nNPV_V = 0.35 \\times (V_R + S) - 0.65 \\times V_V\n \n\nMalmqvist's critique asserted that Volvo's truck division was \"world-class\" while Renault's was \"weak,\" implying that Volvo's standalone value (`V_V`) was disproportionately high relative to Renault's (`V_R`).\n\n---\n\nBased on the provided model and context, select all of the following statements that are valid conclusions.\n", "Options": {"A": "Even if the projected synergies (`S`) are positive and substantial, the deal can still destroy value for Volvo shareholders (`NPV_V < 0`).", "B": "According to the NPV formula, the deal is value-destructive for Volvo shareholders if the value of the assets they contribute (`0.65 \\times V_V`) exceeds the value of the assets they effectively acquire (`0.35 \\times (V_R + S)`).", "C": "The merger would create positive value for Volvo shareholders (`NPV_V > 0`) if the value of Renault plus synergies is greater than the value of Volvo (`V_R + S > V_V`).", "D": "Malmqvist's argument implies that the ratio of `V_V` to `V_R` is high, which increases the likelihood that `0.65 \\times V_V` will be larger than `0.35 \\times (V_R + S)`."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: The item assesses the candidate's ability to interpret a formal M&A valuation model (NPV to one party's shareholders) and connect it to the qualitative critiques presented in the case. It requires understanding how value is created versus how it is distributed.\nDepth Strategy: Computational Judgment. The user must analyze the provided NPV formula and evaluate the conditions under which the NPV is positive or negative, linking this formal analysis to the narrative arguments.\nDistractor Logic:\n- A: This is a common conceptual error (Formula Misuse). It simplifies the condition incorrectly. The correct condition is `0.35 * (V_R + S) > 0.65 * V_V`, not `V_R + S > V_V`.\n- B: Correct. This is the central thesis of the critique. If `V_V` is sufficiently large relative to `V_R + S`, the NPV will be negative.\n- C: Correct. This is a direct and correct interpretation of the rearranged NPV formula, framing it as a trade-off between the value contributed and the value acquired.\n- D: Correct. This correctly translates Malmqvist's qualitative critique ('world-class' vs. 'weak') into its quantitative implication for the variables in the NPV model, demonstrating a synthesis of narrative and formula.", "qid": "248", "question": "### Background\n\nThe public debate surrounding the 1993 Volvo-Renault merger pitted two competing frameworks against each other. The pro-merger rationale, often summarized as the \"Big is Beautiful\" argument, focused on the strategic imperative of achieving economies of scale in the global automotive industry. This view was contrasted with a critical shareholder value analysis, championed by journalist Peter Malmqvist, which argued the deal was financially detrimental to Volvo's owners.\n\n### Data / Model Specification\n\nLet `V_V` be the standalone market value of Volvo, `V_R` be the standalone market value of Renault, and `S` be the present value of all future synergies from the merger, estimated at over $5 billion. In the proposed deal, Volvo shareholders contribute their entire firm (value `V_V`) and in return receive a 35% stake in the new, combined firm (RVA).\n\nThe Net Present Value (NPV) of the merger to Volvo's shareholders can be expressed as the value of what they receive minus the value of what they give up:\n\n  \nNPV_V = (0.35 \\times (V_V + V_R + S)) - V_V\n \n\nThis can be rearranged to:\n\n  \nNPV_V = 0.35 \\times (V_R + S) - 0.65 \\times V_V\n \n\nMalmqvist's critique asserted that Volvo's truck division was \"world-class\" while Renault's was \"weak,\" implying that Volvo's standalone value (`V_V`) was disproportionately high relative to Renault's (`V_R`).\n\n---\n\nBased on the provided model and context, select all of the following statements that are valid conclusions.\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) Even if the projected synergies (`S`) are positive and substantial, the deal can still destroy value for Volvo shareholders (`NPV_V < 0`).\n\nFrom the formula: NPV_V = 0.35 × (V_R + S) - 0.65 × V_V\n\nFor NPV_V < 0: 0.35 × (V_R + S) < 0.65 × V_V\n\nThis can occur even with large positive S if V_V is sufficiently large relative to V_R. ✓\n\nB) According to the NPV formula, the deal is value-destructive for Volvo shareholders if the value of the assets they contribute (`0.65 × V_V`) exceeds the value of the assets they effectively acquire (`0.35 × (V_R + S)`).\n\nThis directly states the condition for NPV_V < 0:\nNPV_V = 0.35 × (V_R + S) - 0.65 × V_V < 0\n⟺ 0.35 × (V_R + S) < 0.65 × V_V ✓\n\nC) The merger would create positive value for Volvo shareholders (`NPV_V > 0`) if the value of Renault plus synergies is greater than the value of Volvo (`V_R + S > V_V`).\n\nTesting this condition:\nIf V_R + S > V_V, then:\n0.35 × (V_R + S) > 0.35 × V_V\n\nBut we need: 0.35 × (V_R + S) > 0.65 × V_V\n\nThe condition V_R + S > V_V is not sufficient because 0.35 × V_V < 0.65 × V_V. ✗\n\nD) Malmqvist's argument implies that the ratio of `V_V` to `V_R` is high, which increases the likelihood that `0.65 × V_V` will be larger than `0.35 × (V_R + S)`.\n\nMalmqvist described Volvo", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 164, "Question": "### Background\n\nA simulated health insurance market evolves over multiple years. An insurer offers a single, community-rated premium rebate for a €500 deductible. Individuals are heterogeneous in health risk and are characterized by their risk aversion (`r`) and their potential for moral hazard reduction (`m`). The market can experience an \"adverse selection spiral,\" where the market for the deductible option collapses over time.\n\n### Data / Model Specification\n\nThe market evolves according to two rules:\n1.  **Consumer Choice:** Individuals `i` take the deductible in year `t` if the Offered Premium Rebate (`OPR_t`) exceeds their Demanded Premium Rebate (`DPR_i`).\n2.  **Insurer Pricing:** The insurer sets `OPR_{t+1}` equal to the average realized cost savings from the pool of individuals who took the deductible in year `t`.\n\nAdverse selection occurs because individuals with lower health risks have lower `DPR_i` and are thus more likely to opt-in.\n\n**Table 1: Percentage of Insured Taking a Deductible (Scenario I)**\n\n| Year | Level of Risk Aversion | m=0 | m = 0.05 | m = 0.1 |\n| :--- | :--- | :-- | :--- | :--- |\n| 1 | r=0.000 | 56 | 93 | 100 |\n| | r=0.003 | 28 | 84 | 99 |\n| | r=0.005 | 18 | 76 | 99 |\n| 2 | r=0.000 | 0 | 79 | 100 |\n| | r=0.003 | 0 | 22 | 98 |\n| | r=0.005 | 0 | 0 | 96 |\n| 3 | r=0.000 | 0 | 23 | 100 |\n| | r=0.003 | 0 | 0 | 97 |\n| | r=0.005 | 0 | 0 | 88 |\n| 4 | r=0.000 | 0 | 0 | 100 |\n| | r=0.003 | 0 | 0 | 95 |\n| | r=0.005 | 0 | 0 | 55 |\n\n---\n\nBased on the data in Table 1 and the model description, which of the following statements are valid conclusions about the dynamics of the adverse selection spiral?\n", "Options": {"A": "The market is stable (i.e., avoids a spiral) only in the scenario where `m=0.1` and `r=0.000`, because this is the only case where the initial offered rebate was high enough to attract even the unhealthiest individuals.", "B": "If the moral hazard reduction (`m`) is zero, the market for deductibles is guaranteed to collapse to zero participation by Year 2, regardless of the level of risk aversion.", "C": "A higher level of consumer risk aversion (`r`) consistently leads to a lower initial participation rate (Year 1) for any given level of moral hazard reduction (`m`).", "D": "In the scenario with `m=0.05` and `r=0.003`, the insurer's offered premium rebate in Year 2 (`OPR_2`) must have been lower than the rebate offered in Year 1 (`OPR_1`)."}, "Answer": ["B", "C", "D"], "pi_justification": "This item assesses the ability to interpret the provided data table in the context of the described dynamic model of adverse selection. It uses a 'Reverse-Reasoning' strategy, asking the user to identify valid conclusions that must be true given the observed outcomes. \n- **A (Correct):** The drop in participation from 84% to 22% implies the pool of participants in Year 1 was healthier than average, leading to lower-than-expected savings and thus a lower `OPR_2`. \n- **B (Correct):** A direct observation from the table. For `m=0`, `m=0.05`, and `m=0.1`, as `r` increases, the Year 1 participation rate decreases. This is because higher `r` increases the demanded premium rebate. \n- **C (Distractor - Almost Right):** The market is stable, but the reason is that the initial rebate attracts *all* individuals (100% participation), which prevents adverse selection. The term 'unhealthiest' is correct, but the key is *full* participation. The phrasing is subtly less precise than the underlying mechanism. More importantly, the paper notes that even with `r>0` and `m=0.1`, the spiral is very slow, so stability isn't *exclusive* to the `r=0` case, although it is the most stable. \n- **D (Correct):** A direct observation from the table. For `m=0`, participation is 56%, 28%, and 18% in Year 1, but drops to 0% for all `r` values in Year 2. This demonstrates the spiral's power when moral hazard savings are absent.", "qid": "164", "question": "### Background\n\nA simulated health insurance market evolves over multiple years. An insurer offers a single, community-rated premium rebate for a €500 deductible. Individuals are heterogeneous in health risk and are characterized by their risk aversion (`r`) and their potential for moral hazard reduction (`m`). The market can experience an \"adverse selection spiral,\" where the market for the deductible option collapses over time.\n\n### Data / Model Specification\n\nThe market evolves according to two rules:\n1.  **Consumer Choice:** Individuals `i` take the deductible in year `t` if the Offered Premium Rebate (`OPR_t`) exceeds their Demanded Premium Rebate (`DPR_i`).\n2.  **Insurer Pricing:** The insurer sets `OPR_{t+1}` equal to the average realized cost savings from the pool of individuals who took the deductible in year `t`.\n\nAdverse selection occurs because individuals with lower health risks have lower `DPR_i` and are thus more likely to opt-in.\n\n**Table 1: Percentage of Insured Taking a Deductible (Scenario I)**\n\n| Year | Level of Risk Aversion | m=0 | m = 0.05 | m = 0.1 |\n| :--- | :--- | :-- | :--- | :--- |\n| 1 | r=0.000 | 56 | 93 | 100 |\n| | r=0.003 | 28 | 84 | 99 |\n| | r=0.005 | 18 | 76 | 99 |\n| 2 | r=0.000 | 0 | 79 | 100 |\n| | r=0.003 | 0 | 22 | 98 |\n| | r=0.005 | 0 | 0 | 96 |\n| 3 | r=0.000 | 0 | 23 | 100 |\n| | r=0.003 | 0 | 0 | 97 |\n| | r=0.005 | 0 | 0 | 88 |\n| 4 | r=0.000 | 0 | 0 | 100 |\n| | r=0.003 | 0 | 0 | 95 |\n| | r=0.005 | 0 | 0 | 55 |\n\n---\n\nBased on the data in Table 1 and the model description, which of the following statements are valid conclusions about the dynamics of the adverse selection spiral?\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each statement against the data in Table 1 and the model mechanics.\n\nLet me examine each option:\n\n**Option A**: Claims the market is stable only when m=0.1 and r=0.000.\n- Looking at the data, when m=0.1 and r=0.000, participation is 100% in all years (1-4).\n- This is indeed the only case showing 100% participation throughout.\n- The statement's reasoning about \"initial offered rebate\" being high enough is plausible - with 100% participation, there's no adverse selection occurring.\n- This appears correct.\n\n**Option B**: Claims that when m=0, the market collapses to zero by Year 2 regardless of risk aversion.\n- For m=0, r=0.000: Year 1 = 56%, Year 2 = 0%\n- For m=0, r=0.003: Year 1 = 28%, Year 2 = 0%  \n- For m=0, r=0.005: Year 1 = 18%, Year 2 = 0%\n- Indeed, all cases with m=0 show 0% participation by Year 2.\n- This is correct.\n\n**Option C**: Claims higher risk aversion leads to lower initial participation.\n- Year 1 participation rates:\n  - m=0: r=0.000 (56%) > r=0.003 (28%) > r=0.005 (18%) ✓\n  - m=0.05: r=0.000 (93%) > r=0.003 (84%) > r=0.005 (76%) ✓\n  - m=0.1: r=0.000 (100%) > r=0.003 (99%) > r=0.005 (99%) ✓\n- The pattern holds across all moral hazard levels.\n- This is correct.\n\n**Option D**: Claims OPR_2 < OPR_1 for m=0.05, r=0.003.\n- Year 1: 84% participation\n- Year 2: 22% participation\n- The sharp decline from", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 177, "Question": "### Background\n\n**Research Question.** This case examines whether executive stock options incentivize managers to strategically report *small* positive earnings surprises, a practice often interpreted as \"storing\" earnings for future periods. The analysis is conditional on firms having already met or beaten analyst forecasts, aiming to distinguish incentives for earnings management from incentives for strong performance.\n\n**Hypothesis.** The core hypothesis (H2) is that the likelihood a firm reports a small earnings surprise (0 to 1 cent per share), conditional on reporting a non-negative surprise, is positively related to the extent to which its senior managers are compensated with stock options.\n\n### Data / Model Specification\n\nThe study estimates two logistic regression models. The first models the probability of meeting or beating the forecast (`MEET=1`). The second models the probability of reporting a small surprise (`SMALL=1`), conditional on having met or beaten the forecast.\n\n1.  `P(MEET_{i,q}=1)` is modeled on the full sample (N=40,661).\n2.  `P(SMALL_{i,q}=1 | MEET_{i,q}=1)` is modeled on the subsample with non-negative surprises (N=29,672).\n\n`SMALL` is an indicator equal to 1 if the earnings surprise is in [0, 1] cents per share, and 0 if the surprise is > 1 cent per share. `OPT` is the proportion of executive pay from stock options. `ROA` is the quarterly return on assets.\n\n**Table 1: Logistic Regression Results**\n\n| Variable | Panel A: P(MEET=1) | Panel B: P(SMALL=1 | MEET=1) |\n| :--- | :---: | :---: |\n| | Coefficient (β) | Coefficient (γ) |\n| `OPT` | 0.199** | 0.323** |\n| `ROA` | 6.740** | -1.581* |\n\n*N=40,661 (Panel A), N=29,672 (Panel B). **p<0.01, *p<0.05.*\n\n### Question\n\nBased on the provided regression results and context, select all statements that accurately interpret the findings regarding the `ROA` coefficient and the causal argument for earnings management.", "Options": {"A": "The positive `ROA` coefficient in Panel A reflects that fundamentally more profitable firms are more likely to meet earnings targets.", "B": "The positive `OPT` coefficient in Panel A (0.199) is stronger evidence of earnings management than the `OPT` coefficient in Panel B (0.323) because it applies to a larger sample.", "C": "The negative `ROA` coefficient in Panel B is consistent with the idea that, conditional on meeting a forecast, the most profitable firms are likely to report large surprises, making a small surprise less probable.", "D": "The Panel B result is less likely to be driven by an unobserved 'firm quality' confounder, as high-quality firms would arguably signal their strength with large earnings beats, not small ones."}, "Answer": ["A", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret contrasting regression coefficients and evaluate causal arguments, core skills for consuming empirical research. It uses an Atomic Decomposition strategy, breaking down the original multi-part question into distinct, verifiable statements. Options A, B, and D are correct interpretations drawn directly from the source answer. Option C is a high-fidelity distractor (True but Irrelevant premise, False conclusion) that misrepresents the paper's central argument about the strength of causal evidence.", "qid": "177", "question": "### Background\n\n**Research Question.** This case examines whether executive stock options incentivize managers to strategically report *small* positive earnings surprises, a practice often interpreted as \"storing\" earnings for future periods. The analysis is conditional on firms having already met or beaten analyst forecasts, aiming to distinguish incentives for earnings management from incentives for strong performance.\n\n**Hypothesis.** The core hypothesis (H2) is that the likelihood a firm reports a small earnings surprise (0 to 1 cent per share), conditional on reporting a non-negative surprise, is positively related to the extent to which its senior managers are compensated with stock options.\n\n### Data / Model Specification\n\nThe study estimates two logistic regression models. The first models the probability of meeting or beating the forecast (`MEET=1`). The second models the probability of reporting a small surprise (`SMALL=1`), conditional on having met or beaten the forecast.\n\n1.  `P(MEET_{i,q}=1)` is modeled on the full sample (N=40,661).\n2.  `P(SMALL_{i,q}=1 | MEET_{i,q}=1)` is modeled on the subsample with non-negative surprises (N=29,672).\n\n`SMALL` is an indicator equal to 1 if the earnings surprise is in [0, 1] cents per share, and 0 if the surprise is > 1 cent per share. `OPT` is the proportion of executive pay from stock options. `ROA` is the quarterly return on assets.\n\n**Table 1: Logistic Regression Results**\n\n| Variable | Panel A: P(MEET=1) | Panel B: P(SMALL=1 | MEET=1) |\n| :--- | :---: | :---: |\n| | Coefficient (β) | Coefficient (γ) |\n| `OPT` | 0.199** | 0.323** |\n| `ROA` | 6.740** | -1.581* |\n\n*N=40,661 (Panel A), N=29,672 (Panel B). **p<0.01, *p<0.05.*\n\n### Question\n\nBased on the provided regression results and context, select all statements that accurately interpret the findings regarding the `ROA` coefficient and the causal argument for earnings management.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze each statement about the ROA coefficient and the causal argument for earnings management.\n\nLet me examine each option:\n\n**Option A**: \"The positive ROA coefficient in Panel A reflects that fundamentally more profitable firms are more likely to meet earnings targets.\"\n- Panel A shows ROA coefficient = 6.740** (positive and significant)\n- This means higher ROA → higher probability of MEET=1\n- This is a straightforward interpretation: more profitable firms are indeed more likely to meet/beat forecasts\n- **This is correct**\n\n**Option B**: \"The positive OPT coefficient in Panel A (0.199) is stronger evidence of earnings management than the OPT coefficient in Panel B (0.323) because it applies to a larger sample.\"\n- This misunderstands the research design\n- Panel B is specifically designed to isolate earnings management by conditioning on MEET=1\n- The larger coefficient in Panel B (0.323 vs 0.199) actually suggests stronger evidence\n- Sample size alone doesn't determine strength of evidence for earnings management\n- **This is incorrect**\n\n**Option C**: \"The negative ROA coefficient in Panel B is consistent with the idea that, conditional on meeting a forecast, the most profitable firms are likely to report large surprises, making a small surprise less probable.\"\n- Panel B shows ROA coefficient = -1.581* (negative and significant)\n- Among firms that beat forecasts, higher ROA → lower probability of SMALL=1\n- This means profitable firms tend to report larger surprises (>1 cent), not small ones\n- **This is correct**\n\n**Option D**: \"The Panel B result is less likely to be driven by an unobserved 'firm quality' confounder, as high-quality firms would arguably signal their strength with large earnings beats, not small ones.\"\n- This addresses the key identification strategy\n- By conditioning on MEET=1, Panel B separates earnings management incentives from performance incentives\n- High-quality firms would indeed want to signal strength with large beats\n- Finding that OPT increases small surprises (while controlling for ROA) suggests manipulation rather than quality\n- **This is correct**\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 70, "Question": "### Background\n\n**Research Question**: This case considers the reliability of regression estimates when the dependent variable is measured with error.\n\n**Setting / Data-Generating Environment**: In the study, the dependent variable is the house value-to-income ratio. The house value is not an audited appraisal but is instead \"as estimated by the owner\" within one year of purchase. This self-reported value may contain measurement error.\n\n---\n\n### Question\n\nConsidering the potential for measurement error in the self-reported house value, which of the following statements are true? Select all that apply.", "Options": {"A": "If the estimation error is classical, the OLS coefficients on income will be biased towards zero (attenuation bias).", "B": "If the estimation error is classical (i.e., random and uncorrelated with income or other regressors), the OLS coefficients on income remain unbiased, but their standard errors will increase.", "C": "If lower-income households tend to systematically overestimate their home's value and higher-income households tend to underestimate it, the OLS coefficients on income will be biased downwards (i.e., become more negative).", "D": "Any measurement error in the dependent variable, regardless of its nature, will render the OLS estimates biased and inconsistent."}, "Answer": ["B", "C"], "pi_justification": "This item assesses understanding of both classical and non-classical measurement error in the dependent variable. It uses a Scenario Application strategy. Option A is a correct statement of the consequences of classical ME in the DV. Option C correctly diagnoses the direction of bias from a specific non-classical ME scenario where the error is negatively correlated with a regressor. Distractors are based on high-frequency misconceptions: B incorrectly applies the rule for ME in an *independent* variable (attenuation bias) to the dependent variable. D is an overgeneralization; classical ME in the DV does not cause bias.", "qid": "70", "question": "### Background\n\n**Research Question**: This case considers the reliability of regression estimates when the dependent variable is measured with error.\n\n**Setting / Data-Generating Environment**: In the study, the dependent variable is the house value-to-income ratio. The house value is not an audited appraisal but is instead \"as estimated by the owner\" within one year of purchase. This self-reported value may contain measurement error.\n\n---\n\n### Question\n\nConsidering the potential for measurement error in the self-reported house value, which of the following statements are true? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question about measurement error in the dependent variable (house value-to-income ratio), I need to analyze each statement carefully.\n\nLet me consider the regression model: Y* = β₀ + β₁X + ε, where Y* is the true house value-to-income ratio, X is income, and we observe Y = Y* + u (where u is measurement error).\n\n**Option A**: This describes attenuation bias, which occurs when there's measurement error in the *independent* variable, not the dependent variable. When the dependent variable has measurement error, it doesn't cause attenuation bias in the coefficients. FALSE.\n\n**Option B**: With classical measurement error in the dependent variable (random, mean zero, uncorrelated with regressors), the OLS estimator remains unbiased because E[u|X] = 0. However, the error variance increases from σ² to σ² + σᵤ², leading to larger standard errors. TRUE.\n\n**Option C**: If lower-income households overestimate and higher-income households underestimate their home values, this creates a negative correlation between the measurement error and income. This violates the classical measurement error assumption and will bias the coefficient on income downward (more negative). TRUE.\n\n**Option D**: This is too broad. Classical measurement error in the dependent variable (uncorrelated with regressors) does not bias OLS estimates - it only affects efficiency. Only non-classical measurement error causes bias. FALSE.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 195, "Question": "### Background\n\n**Research Question.** This case examines the economic trade-offs of supervisory board size and its empirical impact on firm performance within the Danish “semi-two-tier” corporate governance system.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish firms (1998–2001). The institutional context is characterized by supervisory boards that are, on average, much smaller than their U.S. counterparts.\n\n---\n\n### Data / Model Specification\n\nThe primary empirical model is a cross-sectional OLS regression:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot BOARDSIZE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       |\n| :---------- | :--------- |\n| BOARDSIZE (N) | 5.2241     |\n\n**Table 2: Regression of Tobin's Q on Board Size**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Board size             | -0.051      | (0.482)   |\n\n---\n\n### Question\n\nSelect all statements that provide a valid interpretation of or explanation for the regression results concerning board size.", "Options": {"A": "A plausible explanation for the null finding is that the true relationship is concave (initially beneficial, then detrimental), and the sample of small Danish boards is concentrated in the initial, flatter portion of this curve.", "B": "The paper reports a strong positive correlation between `Board size` and firm `SIZE`, which invalidates the regression results entirely due to multicollinearity.", "C": "The negative sign of the coefficient (-0.051), despite being insignificant, confirms the agency cost theory that larger boards are always harmful.", "D": "The statistically insignificant coefficient on `Board size` fails to support the hypothesis that larger boards are detrimental to firm performance in this sample."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to interpret a null result and critique a model's functional form based on theory and sample characteristics. It uses a **Premise/Assumption Packaging** strategy, asking for valid interpretations and explanations. Distractors are designed to be subtle: (B) incorrectly treats an insignificant coefficient as confirmatory evidence (a conceptual error), and (D) overstates the consequence of multicollinearity, a common 'Almost Right' misconception.", "qid": "195", "question": "### Background\n\n**Research Question.** This case examines the economic trade-offs of supervisory board size and its empirical impact on firm performance within the Danish “semi-two-tier” corporate governance system.\n\n**Setting / Data-Generating Environment.** The analysis uses a cross-sectional sample of 116 Danish firms (1998–2001). The institutional context is characterized by supervisory boards that are, on average, much smaller than their U.S. counterparts.\n\n---\n\n### Data / Model Specification\n\nThe primary empirical model is a cross-sectional OLS regression:\n\n  \nTOBINQ_i = \\beta_0 + \\beta_1 \\cdot BOARDSIZE_i + \\text{Controls}_i'\\gamma + \\epsilon_i \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Descriptive Statistics (Selected Variables)**\n\n| Series      | Mean       |\n| :---------- | :--------- |\n| BOARDSIZE (N) | 5.2241     |\n\n**Table 2: Regression of Tobin's Q on Board Size**\n\n| Independent variables: | Coefficient | (p-value) |\n| :--------------------- | :---------- | :-------- |\n| Board size             | -0.051      | (0.482)   |\n\n---\n\n### Question\n\nSelect all statements that provide a valid interpretation of or explanation for the regression results concerning board size.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results and the context provided:\n\nThe coefficient for Board size is -0.051 with a p-value of 0.482, which is not statistically significant at conventional levels (0.05 or 0.10).\n\nLet me evaluate each option:\n\nA) This is a valid interpretation. The null finding could indeed be explained by a concave relationship where benefits initially increase then decrease with board size. Given that Danish boards are small (mean = 5.22), the sample could be concentrated in the flatter, initial portion of such a curve where effects are minimal.\n\nB) The passage mentions nothing about multicollinearity or a correlation between Board size and firm SIZE being reported. This statement makes a claim not supported by the provided information.\n\nC) This is invalid. A statistically insignificant coefficient cannot \"confirm\" any theory. The insignificance means we cannot reject the null hypothesis of no effect.\n\nD) This is a valid interpretation. With p = 0.482, the coefficient is not statistically significant, meaning the data fails to support the hypothesis that larger boards harm performance in this Danish sample.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 282, "Question": "### Background\n\nThe paper investigates why U.S. industries experienced starkly different competitive outcomes between 1960 and 1986, despite facing similar macroeconomic conditions. It challenges the theory that a high cost of capital was the primary cause of competitive decline, proposing instead that firm-specific organizational capabilities are the key differentiating factor.\n\n### Data / Model Specification\n\nConsider two competing stylized models for firm value (`V`):\n\n  \nV_1 = f(I, r) \\quad \\text{(Eq. 1)}\n \n\n  \nV_2 = g(I, r, k) \\quad \\text{with} \\quad \\frac{\\partial g}{\\partial k} > 0 \\quad \\text{and} \\quad \\frac{\\partial^2 g}{\\partial I \\partial k} > 0 \\quad \\text{(Eq. 2)}\n \n\nEq. (1) represents the view that value is driven by investment levels (`I`) and a common cost of capital (`r`). Eq. (2) introduces a firm-specific organizational capability (`k`) that is complementary with capital investment.\n\n**Table 1: Changes in “World Market Share” of U.S. Companies by Major Industry, 1960-1986**\n\n| Industry                             | Percentage Change |\n| :----------------------------------- | :---------------- |\n| Iron and Steel                       | -58%              |\n| Autos and Trucks                     | -33%              |\n| Paper and Paper Products             | -4%               |\n| Aerospace                            | +6%               |\n\n### Question\n\nBased on the paper's argument, the provided models, and the data in Table 1, select all of the following statements that represent valid conclusions or interpretations.", "Options": {"A": "The data for the Aerospace and Iron and Steel industries, which both have long-lived capital assets, provides strong evidence against the sufficiency of the model in Eq. (1) to explain competitive outcomes.", "B": "The significant market share loss in the Autos and Trucks industry is consistent with a failure to develop capabilities like 'internal integration' (speed and efficiency in product development), as described in the paper.", "C": "The model in Eq. (2) implies that firms with higher levels of organizational capability (`k`) will have a lower optimal level of investment (`I*`), as capabilities substitute for capital.", "D": "According to the model in Eq. (2), the marginal productivity of capital investment (`∂g/∂I`) is independent of the level of organizational capability (`k`)."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the student's ability to synthesize the paper's core empirical puzzle (Table 1) with its central theoretical argument (the importance of capabilities, formalized in Eq. 2). It uses a 'Scenario Application' strategy. Option A is correct because the divergent outcomes of two industries with similar capital horizons refutes the simple cost-of-capital theory (Eq. 1). Option C is correct as it applies the paper's specific example of 'internal integration' to a relevant industry from the table. Distractor B is a 'Conceptual Opposite' error; the complementarity in Eq. 2 (`∂²g/∂I∂k > 0`) means capability *increases* the optimal investment level. Distractor D is a 'Formula Misuse' error, directly contradicting the complementarity assumption in Eq. 2.", "qid": "282", "question": "### Background\n\nThe paper investigates why U.S. industries experienced starkly different competitive outcomes between 1960 and 1986, despite facing similar macroeconomic conditions. It challenges the theory that a high cost of capital was the primary cause of competitive decline, proposing instead that firm-specific organizational capabilities are the key differentiating factor.\n\n### Data / Model Specification\n\nConsider two competing stylized models for firm value (`V`):\n\n  \nV_1 = f(I, r) \\quad \\text{(Eq. 1)}\n \n\n  \nV_2 = g(I, r, k) \\quad \\text{with} \\quad \\frac{\\partial g}{\\partial k} > 0 \\quad \\text{and} \\quad \\frac{\\partial^2 g}{\\partial I \\partial k} > 0 \\quad \\text{(Eq. 2)}\n \n\nEq. (1) represents the view that value is driven by investment levels (`I`) and a common cost of capital (`r`). Eq. (2) introduces a firm-specific organizational capability (`k`) that is complementary with capital investment.\n\n**Table 1: Changes in “World Market Share” of U.S. Companies by Major Industry, 1960-1986**\n\n| Industry                             | Percentage Change |\n| :----------------------------------- | :---------------- |\n| Iron and Steel                       | -58%              |\n| Autos and Trucks                     | -33%              |\n| Paper and Paper Products             | -4%               |\n| Aerospace                            | +6%               |\n\n### Question\n\nBased on the paper's argument, the provided models, and the data in Table 1, select all of the following statements that represent valid conclusions or interpretations.", "model_evaluation_result": {"model_raw_response": "Looking at each option carefully:\n\nA) The Aerospace industry gained 6% market share while Iron and Steel lost 58%, despite both having long-lived capital assets. If Eq. (1) were sufficient (where only investment and cost of capital matter), we'd expect similar outcomes for industries with similar capital characteristics facing the same macroeconomic conditions. This stark difference provides strong evidence against Eq. (1)'s sufficiency. ✓\n\nB) The paper argues that organizational capabilities (like internal integration for speed and efficiency in product development) are key differentiators. The 33% market share loss in Autos and Trucks is indeed consistent with a failure to develop such capabilities, as the paper's thesis suggests. ✓\n\nC) This is incorrect. The condition ∂²g/∂I∂k > 0 means that k and I are complements, not substitutes. Higher k increases the marginal productivity of investment, leading to higher (not lower) optimal investment levels.\n\nD) This is incorrect. The condition ∂²g/∂I∂k > 0 explicitly states that the marginal productivity of capital (∂g/∂I) increases with k, meaning they are not independent.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 139, "Question": "### Background\n\n**Research Question.** What were the realized returns from bond refunding operations by public utilities in 1962-63, and were these operations profitable according to a theoretically sound decision rule?\n\n**Setting / Data-Generating Environment.** The analysis is based on an empirical study of 40 bond refunding operations undertaken by public utilities during 1962-63. The primary metric for evaluation is the after-tax internal rate of return (IRR) on the net cash investment required for the refunding.\n\n**Variables & Parameters.**\n- `IRR`: The after-tax internal rate of return on the refunding investment.\n- `I_0`: Net cash investment required at time 0 to execute the refunding.\n- `S_t`: Net after-tax cash savings at time `t` resulting from the refunding.\n- `y_{new}`: The pre-tax yield on the new refunding bonds. For the sample period, this was between 4% and 5%.\n- `k_d`: The after-tax cost of the new debt, which serves as the hurdle rate for the investment.\n- `τ`: The corporate income tax rate, assumed to be 50%.\n\n---\n\n### Data / Model Specification\n\nThe Internal Rate of Return (`IRR`) for each refunding operation is calculated by solving the following equation for `IRR`:\n  \nI_0 = \\sum_{t=1}^{T} \\frac{S_t}{(1+IRR)^t} \\quad \\text{(Eq. (1))}\n \nThe author's proposed decision rule is that a refunding operation is profitable if its `IRR` exceeds the relevant cost of funds:\n  \n\\text{Refund if } IRR > k_d = y_{new}(1-\\tau) \\quad \\text{(Eq. (2))}\n \nEmpirical results for the 40 refunding operations are presented in Table 1.\n\n**Table 1: Rates of Return Earned on Forty Bond Refundings by Public Utilities in 1962 and 1963 (Percent)**\n\n| | | | |\n| :--- | :--- | :--- | :--- |\n| 3.6* | 9.1 | 11.5* | 14.7* |\n| 5.5 | 9.5 | 11.9* | 14.8 |\n| 5.7 | 9.6* | 12.3 | 15.3* |\n| 6.5 | 9.7 | 12.3 | 16.1 |\n| 6.8 | 9.9 | 12.8* | 16.1 |\n| 7.1* | 10.1 | 12.8 | 17.2 |\n| 7.3 | 10.4* | 13.0 | 19.7 |\n| 8.0 | 10.4 | 13.1 | 23.8 |\n| 8.2 | 11.2 | 13.8 | 26.7* |\n| 8.7 | 11.3 | 14.6* | 43.4 |\n\n*Note: An asterisk (*) denotes that reasons in addition to interest savings were cited for the refunding.*\n\n---\n\n### Question\n\nBased on the provided data and decision rule, select all of the following statements that are correct assessments of the 1962-63 public utility bond refundings.", "Options": {"A": "According to the author's proposed framework, all 40 refunding operations were profitable for the firms.", "B": "The refunding with the lowest IRR of 3.6% was unprofitable because this return is below the firm's overall cost of capital, which is typically higher than the cost of debt.", "C": "The appropriate hurdle rate is the pre-tax yield on the new bonds (4-5%), which implies the refunding with a 3.6% IRR was unprofitable.", "D": "The appropriate after-tax hurdle rate (`k_d`) for these decisions was in the range of 2.0% to 2.5%."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses the ability to perform a quantitative evaluation using the paper's central decision rule. It uses a Computational Judgment strategy, decomposed into two atomic propositions (calculating the hurdle rate and applying it to the data) to create a genuine multi-select question. Distractor C represents a common error of using a pre-tax instead of after-tax cost of debt. Distractor D introduces a conceptual error discussed in the paper: incorrectly using the overall cost of capital as the hurdle rate for a low-risk refunding decision.", "qid": "139", "question": "### Background\n\n**Research Question.** What were the realized returns from bond refunding operations by public utilities in 1962-63, and were these operations profitable according to a theoretically sound decision rule?\n\n**Setting / Data-Generating Environment.** The analysis is based on an empirical study of 40 bond refunding operations undertaken by public utilities during 1962-63. The primary metric for evaluation is the after-tax internal rate of return (IRR) on the net cash investment required for the refunding.\n\n**Variables & Parameters.**\n- `IRR`: The after-tax internal rate of return on the refunding investment.\n- `I_0`: Net cash investment required at time 0 to execute the refunding.\n- `S_t`: Net after-tax cash savings at time `t` resulting from the refunding.\n- `y_{new}`: The pre-tax yield on the new refunding bonds. For the sample period, this was between 4% and 5%.\n- `k_d`: The after-tax cost of the new debt, which serves as the hurdle rate for the investment.\n- `τ`: The corporate income tax rate, assumed to be 50%.\n\n---\n\n### Data / Model Specification\n\nThe Internal Rate of Return (`IRR`) for each refunding operation is calculated by solving the following equation for `IRR`:\n  \nI_0 = \\sum_{t=1}^{T} \\frac{S_t}{(1+IRR)^t} \\quad \\text{(Eq. (1))}\n \nThe author's proposed decision rule is that a refunding operation is profitable if its `IRR` exceeds the relevant cost of funds:\n  \n\\text{Refund if } IRR > k_d = y_{new}(1-\\tau) \\quad \\text{(Eq. (2))}\n \nEmpirical results for the 40 refunding operations are presented in Table 1.\n\n**Table 1: Rates of Return Earned on Forty Bond Refundings by Public Utilities in 1962 and 1963 (Percent)**\n\n| | | | |\n| :--- | :--- | :--- | :--- |\n| 3.6* | 9.1 | 11.5* | 14.7* |\n| 5.5 | 9.5 | 11.9* | 14.8 |\n| 5.7 | 9.6* | 12.3 | 15.3* |\n| 6.5 | 9.7 | 12.3 | 16.1 |\n| 6.8 | 9.9 | 12.8* | 16.1 |\n| 7.1* | 10.1 | 12.8 | 17.2 |\n| 7.3 | 10.4* | 13.0 | 19.7 |\n| 8.0 | 10.4 | 13.1 | 23.8 |\n| 8.2 | 11.2 | 13.8 | 26.7* |\n| 8.7 | 11.3 | 14.6* | 43.4 |\n\n*Note: An asterisk (*) denotes that reasons in addition to interest savings were cited for the refunding.*\n\n---\n\n### Question\n\nBased on the provided data and decision rule, select all of the following statements that are correct assessments of the 1962-63 public utility bond refundings.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to evaluate each statement based on the given decision rule and data.\n\nThe key decision rule is: Refund if IRR > k_d = y_new(1-τ)\n\nGiven:\n- y_new = 4% to 5% (pre-tax yield on new bonds)\n- τ = 50% (tax rate)\n- Therefore: k_d = y_new(1-0.5) = 0.5 × y_new = 2% to 2.5%\n\nNow evaluating each option:\n\nA) \"All 40 refunding operations were profitable\"\n- The lowest IRR in the data is 3.6%\n- Since 3.6% > 2.5% (the maximum hurdle rate), all IRRs exceed the hurdle rate\n- According to the author's framework, all operations were profitable\n- This is CORRECT\n\nB) \"The 3.6% IRR refunding was unprofitable because it's below the firm's overall cost of capital\"\n- The decision rule specifically uses k_d (after-tax cost of debt), not the overall cost of capital\n- This contradicts the author's framework\n- This is INCORRECT\n\nC) \"The appropriate hurdle rate is the pre-tax yield (4-5%)\"\n- The decision rule explicitly uses the after-tax cost: k_d = y_new(1-τ)\n- Using pre-tax yield as the hurdle rate contradicts the given framework\n- This is INCORRECT\n\nD) \"The appropriate after-tax hurdle rate was in the range of 2.0% to 2.5%\"\n- k_d = y_new(1-τ) = (4% to 5%) × 0.5 = 2.0% to 2.5%\n- This correctly applies the given formula\n- This is CORRECT\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 61, "Question": "### Background\n\n**Research Question.** How is the latent volatility process modeled and estimated within a particle filtering framework, particularly when its innovations are correlated with innovations in the price process?\n\n**Setting.** The discrete-time state equation for the evolution of the latent variance process (`ν_t`) in the MRSVJ model, which is a core component of the particle filtering algorithm.\n\n**Variables and Parameters.**\n- `ν_t`: Variance at time `t`.\n- `ρ`: Correlation between price and variance shocks.\n- `y_t`: Log-spot price at time `t`.\n- `ε_2`: A standard normal random variable independent of the price shock.\n\n---\n\n### Data / Model Specification\n\nFor the particle filter, the state update equation for the latent variance `ν_t` incorporates information from the observed log-price `y_t`:\n\n  \n\\nu_{t}=\\kappa\\theta+(1-\\kappa)\\nu_{t-1}+\\sigma_{\\nu}\\rho\\left(y_{t}-a-(1-b)y_{t-1}+\\frac{\\nu_{t-1}}{2}+\\lambda\\bar{J}-\\sum_{i=1}^{N_{t}}J_{i}\\right) +\\sigma_{\\nu}\\sqrt{\\nu_{t-1}(1-\\rho^{2})}\\varepsilon_{2} \n \n\nThe paper's empirical results find a significant negative correlation, `ρ < 0`.\n\n---\n\nGiven the state update equation for `ν_t` and the empirical finding that `ρ` is negative, which of the following statements are valid inferences about the behavior of the particle filter?\n\nSelect all that apply.", "Options": {"A": "The term containing `(1-ρ^2)` ensures that the model's update to volatility remains well-defined and real-valued, as correlation must be between -1 and 1.", "B": "If the correlation `ρ` were exactly zero, the observed price `y_t` would have no direct influence on the updated estimate of `ν_t` beyond its role in determining jumps.", "C": "A large positive price surprise (`y_t` is much higher than expected) will cause the filter to decrease its estimate of `ν_t`.", "D": "If an observed price `y_t` is unexpectedly low (a negative surprise), the filter will revise its estimate of the current volatility `ν_t` upwards."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret a complex state-space update equation and reason about its behavior under specific conditions, linking it to the economic concept of the leverage effect. Strategy: Reverse-Reasoning. Given the model equation and an empirical result (`ρ < 0`), the user must deduce the consequences for the filtering process. Distractor Logic: Option A is correct; a negative surprise `(y_t - E[y_t])` multiplied by a negative `ρ` results in a positive update to `ν_t`. Option B is correct, as `1-ρ^2` must be non-negative. Option D is correct; a positive surprise multiplied by a negative `ρ` results in a negative update. Option C is a conceptual distractor (Almost Right); if `ρ=0`, the main term linking `y_t` to `ν_t` disappears, but `y_t` is still used to calculate the weights for resampling particles, so it has an indirect influence. However, the question asks about the *direct influence in the update equation*, making the statement largely correct in that context, but the other three are more directly and unequivocally true. To ensure a clear set of correct answers, A, B, and D are chosen as the most robustly correct interpretations of the equation's structure. *Self-correction: Re-evaluating C. The prompt asks about the state update equation itself. In that equation, if ρ=0, the entire term containing the price surprise `y_t` vanishes. Therefore, C is also a correct statement about the direct update mechanism. The final answer should be A, B, C, D. However, to maintain a high bar and avoid ambiguity about 'direct influence', I will select the three most certain options.* Final Decision: A, B, and D are the most direct and unambiguous consequences of the equation's structure and the negative `ρ`.", "qid": "61", "question": "### Background\n\n**Research Question.** How is the latent volatility process modeled and estimated within a particle filtering framework, particularly when its innovations are correlated with innovations in the price process?\n\n**Setting.** The discrete-time state equation for the evolution of the latent variance process (`ν_t`) in the MRSVJ model, which is a core component of the particle filtering algorithm.\n\n**Variables and Parameters.**\n- `ν_t`: Variance at time `t`.\n- `ρ`: Correlation between price and variance shocks.\n- `y_t`: Log-spot price at time `t`.\n- `ε_2`: A standard normal random variable independent of the price shock.\n\n---\n\n### Data / Model Specification\n\nFor the particle filter, the state update equation for the latent variance `ν_t` incorporates information from the observed log-price `y_t`:\n\n  \n\\nu_{t}=\\kappa\\theta+(1-\\kappa)\\nu_{t-1}+\\sigma_{\\nu}\\rho\\left(y_{t}-a-(1-b)y_{t-1}+\\frac{\\nu_{t-1}}{2}+\\lambda\\bar{J}-\\sum_{i=1}^{N_{t}}J_{i}\\right) +\\sigma_{\\nu}\\sqrt{\\nu_{t-1}(1-\\rho^{2})}\\varepsilon_{2} \n \n\nThe paper's empirical results find a significant negative correlation, `ρ < 0`.\n\n---\n\nGiven the state update equation for `ν_t` and the empirical finding that `ρ` is negative, which of the following statements are valid inferences about the behavior of the particle filter?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this state update equation for the latent variance in a particle filtering framework, I need to analyze each statement carefully.\n\nThe key insight is that the term containing ρ multiplies the price surprise/innovation:\n`ρ(y_t - a - (1-b)y_{t-1} + ν_{t-1}/2 + λJ̄ - Σ J_i)`\n\nGiven that ρ < 0 (negative correlation), let me evaluate each option:\n\n**Option A**: The term `(1-ρ²)` appears under a square root in the last term. Since ρ must be between -1 and 1 for it to be a valid correlation coefficient, we have 0 ≤ ρ² ≤ 1, which means 0 ≤ (1-ρ²) ≤ 1. This ensures the square root is well-defined and real-valued. ✓\n\n**Option B**: If ρ = 0, the entire middle term `σ_ν ρ(...)` becomes zero, eliminating the direct influence of the price innovation on ν_t. The observed price y_t would only affect ν_t indirectly through the jump component if jumps are determined from price observations. ✓\n\n**Option C**: With ρ < 0, a large positive price surprise (where the expression in parentheses is positive) gets multiplied by negative ρ, resulting in a negative contribution to ν_t. This decreases the volatility estimate. ✓\n\n**Option D**: With ρ < 0, a negative price surprise (where the expression in parentheses is negative) gets multiplied by negative ρ, resulting in a positive contribution to ν_t. This increases the volatility estimate, consistent with the leverage effect in financial markets. ✓\n\nAll four statements are valid inferences.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 165, "Question": "### Background\nThe paper analyzes a scenario where an insurer offers premium rebates for a deductible that are partially risk-rated based on age and gender. This creates 18 distinct premium risk groups. The simulation parameters are fixed at `m=0.05` (moral hazard reduction) and `r=0.003` (risk aversion).\n\n### Data / Model Specification\nThe adverse selection spiral can now occur *within* each premium risk group. A spiral is avoided within a group only if the group-specific offered premium rebate is high enough to attract the highest-risk members of that specific group.\n\n**Table 1: Percentage of Insured Women Taking a Deductible, by Age Group**\n(`m=0.05`, `r=0.003`)\n\n| Age Group | Year 1 | Year 10 |\n| :--- | :--- | :--- |\n| 0-9 | 97% | 0% |\n| 10-19 | 97% | 0% |\n| 20-29 | 97% | 0% |\n| 30-39 | 96% | 0% |\n| 40-49 | 94% | 0% |\n| 50-59 | 94% | 0% |\n| 60-69 | 98% | 57% |\n| 70-79 | 100% | 100% |\n| 80+ | 100% | 100% |\n\n---\n\nBased on the data in Table 1 and the model of partial risk-rating, which of the following statements are valid inferences?\n", "Options": {"A": "The initial offered premium rebate for the \"80+\" age group must have been greater than the highest demanded premium rebate from any individual within that same group.", "B": "Partial risk-rating successfully eliminates the adverse selection spiral for the insurance pool as a whole.", "C": "The initial offered premium rebate for the \"20-29\" age group was lower than the initial offered premium rebate for the \"70-79\" age group.", "D": "For the \"60-69\" age group, the offered premium rebate in Year 2 was lower than in Year 1, but the rebate stabilized at a level sufficient to retain a majority of the group by Year 10."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to apply the general theory of the adverse selection spiral to the specific case of partial risk-rating, using the provided table as evidence. It uses a 'Reverse-Reasoning' strategy, requiring the user to infer the underlying conditions that must have produced the observed outcomes.\n- **A (Correct):** The only way for participation to be 100% in Year 1 and remain stable is if the offered rebate exceeds the demanded rebate of every single member, including the one with the highest demand (`DPR_max`).\n- **B (Distractor - Conceptual Opposite):** The table clearly shows the spiral is *not* eliminated; it is merely localized. Six of the nine groups shown collapse to 0% participation. This is a key finding of the paper.\n- **C (Correct):** The offered rebate is based on the average expected costs of the group. Since older age groups have much higher average health costs, the rebate offered to the \"70-79\" group must have been significantly higher than the one offered to the much healthier \"20-29\" group.\n- **D (Distractor - Almost Right):** The first part is correct; the drop from 98% to 57% implies the rebate must have decreased. However, the paper states that the spiral continues until participation is 0% or 100% within a group. The 57% is a snapshot in Year 10; the spiral is still ongoing and will eventually lead to 0% participation for this group as well, so it has not 'stabilized'.", "qid": "165", "question": "### Background\nThe paper analyzes a scenario where an insurer offers premium rebates for a deductible that are partially risk-rated based on age and gender. This creates 18 distinct premium risk groups. The simulation parameters are fixed at `m=0.05` (moral hazard reduction) and `r=0.003` (risk aversion).\n\n### Data / Model Specification\nThe adverse selection spiral can now occur *within* each premium risk group. A spiral is avoided within a group only if the group-specific offered premium rebate is high enough to attract the highest-risk members of that specific group.\n\n**Table 1: Percentage of Insured Women Taking a Deductible, by Age Group**\n(`m=0.05`, `r=0.003`)\n\n| Age Group | Year 1 | Year 10 |\n| :--- | :--- | :--- |\n| 0-9 | 97% | 0% |\n| 10-19 | 97% | 0% |\n| 20-29 | 97% | 0% |\n| 30-39 | 96% | 0% |\n| 40-49 | 94% | 0% |\n| 50-59 | 94% | 0% |\n| 60-69 | 98% | 57% |\n| 70-79 | 100% | 100% |\n| 80+ | 100% | 100% |\n\n---\n\nBased on the data in Table 1 and the model of partial risk-rating, which of the following statements are valid inferences?\n", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze each statement based on the data showing deductible uptake rates across age groups over time.\n\nLet me examine each option:\n\n**Option A**: The \"80+\" age group shows 100% uptake in both Year 1 and Year 10. For 100% uptake to occur, the offered premium rebate must be at least as high as the highest demanded rebate within that group. If even one person's demanded rebate exceeded the offered rebate, uptake would be less than 100%. This statement is valid.\n\n**Option B**: The data clearly shows adverse selection spirals occurring in most age groups (0-9 through 50-59 groups all drop from ~95%+ to 0%). While some groups avoid the spiral, the statement claims partial risk-rating \"eliminates\" the spiral for the pool \"as a whole,\" which is false given that 6 out of 9 groups experience complete spiral. This statement is invalid.\n\n**Option C**: This compares initial rebates between groups, but the model doesn't provide information about how rebates are set across different age groups initially - only how they evolve within groups based on adverse selection. We cannot infer cross-group rebate comparisons from the uptake data alone. This statement cannot be validated.\n\n**Option D**: The \"60-69\" group shows 98% uptake in Year 1 and 57% in Year 10. The model indicates that when higher-risk individuals leave (reducing uptake), the offered rebate must decrease in subsequent years. The 57% retention by Year 10 suggests the rebate stabilized at a level where the marginal individuals are indifferent between taking and not taking the deductible. This matches the described pattern. This statement is valid.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 260, "Question": "### Background\n\nThis case examines the attitudes of corporate directors in an emerging market towards global corporate governance standards. The analysis is based on a survey of directors of companies listed on the Taiwan Security Exchange (TSE). Taiwan's corporate governance system is characterized as \"insider-oriented,\" with ownership and control concentrated in founding families, contrasting with \"outsider systems\" where ownership is dispersed. Director attitudes are measured on a 7-point Likert scale, where 1 = strongly disagree and 7 = strongly agree.\n\n### Data / Model Specification\n\nThe central proposition to be evaluated is:\n\n**Hypothesis One:** Company directors want corporate governance reform in Taiwan and want to adopt internationally acceptable corporate governance standards.\n\nSurvey evidence regarding this hypothesis is presented in Table 1.\n\n**Table 1: General Attitudes towards Corporate Governance**\n\n| Statement | N | Mean | Median | Mode | SD |\n| :--- | :-: | :-: | :-: | :-: | :-: |\n| (a) I believe that there should be increased international harmonisation of corporate governance standards | 55 | 5.05 | 5 | 4 | 1.35 |\n| (c) I believe that Taiwan companies should adopt a more Anglo-Saxon model of corporate governance | 53 | 4.04 | 4 | 3 | 1.22 |\n| (d) I believe that international harmonisation of corporate governance standards will not encourage foreign investors to invest in Taiwan companies | 55 | 2.76 | 3 | 3 | 1.05 |\n\n*Note: Scores are on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).* \n\n### Question\n\nBased on the survey data in Table 1 and the background information, which of the following conclusions can be validly drawn about the Taiwanese directors' stance on corporate governance reform? Select all that apply.", "Options": {"A": "Directors show a clear preference for a selective adaptation of global principles rather than a wholesale adoption of the Anglo-Saxon model.", "B": "Directors are enthusiastic about fully replacing their current insider-oriented system with a US/UK-style outsider model.", "C": "Directors strongly believe that adopting international governance standards is a key mechanism for attracting foreign capital.", "D": "Directors are largely indifferent to the idea of international corporate governance reform, viewing it as irrelevant to their firms."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to synthesize multiple data points to form a nuanced conclusion, a key objective of the original QA problem. It uses a Reverse-Reasoning strategy, asking for valid conclusions from the provided data. Option A is correct because the strong disagreement with statement (d) (mean 2.76) implies strong agreement with its opposite. Option B is correct because the high mean for harmonization (5.05) contrasts with the neutral mean for the specific Anglo-Saxon model (4.04). Distractor C is a conceptual opposite, directly contradicted by the high mean for statement (a). Distractor D is an overstatement and a conceptual opposite of the nuanced conclusion in B, representing a plausible misinterpretation of the data.", "qid": "260", "question": "### Background\n\nThis case examines the attitudes of corporate directors in an emerging market towards global corporate governance standards. The analysis is based on a survey of directors of companies listed on the Taiwan Security Exchange (TSE). Taiwan's corporate governance system is characterized as \"insider-oriented,\" with ownership and control concentrated in founding families, contrasting with \"outsider systems\" where ownership is dispersed. Director attitudes are measured on a 7-point Likert scale, where 1 = strongly disagree and 7 = strongly agree.\n\n### Data / Model Specification\n\nThe central proposition to be evaluated is:\n\n**Hypothesis One:** Company directors want corporate governance reform in Taiwan and want to adopt internationally acceptable corporate governance standards.\n\nSurvey evidence regarding this hypothesis is presented in Table 1.\n\n**Table 1: General Attitudes towards Corporate Governance**\n\n| Statement | N | Mean | Median | Mode | SD |\n| :--- | :-: | :-: | :-: | :-: | :-: |\n| (a) I believe that there should be increased international harmonisation of corporate governance standards | 55 | 5.05 | 5 | 4 | 1.35 |\n| (c) I believe that Taiwan companies should adopt a more Anglo-Saxon model of corporate governance | 53 | 4.04 | 4 | 3 | 1.22 |\n| (d) I believe that international harmonisation of corporate governance standards will not encourage foreign investors to invest in Taiwan companies | 55 | 2.76 | 3 | 3 | 1.05 |\n\n*Note: Scores are on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).* \n\n### Question\n\nBased on the survey data in Table 1 and the background information, which of the following conclusions can be validly drawn about the Taiwanese directors' stance on corporate governance reform? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the survey data in Table 1:\n\n**Statement (a)**: Mean = 5.05 (above the neutral point of 4)\n- Directors agree with increased international harmonization of corporate governance standards\n\n**Statement (c)**: Mean = 4.04 (barely above neutral)\n- Directors are neutral to slightly positive about adopting the Anglo-Saxon model specifically\n\n**Statement (d)**: Mean = 2.76 (well below neutral)\n- Directors disagree that international harmonization won't encourage foreign investment (meaning they believe it WILL encourage foreign investment)\n\nAnalyzing each option:\n\n**Option A**: CORRECT\n- Directors support international harmonization (5.05) but are only neutral about the Anglo-Saxon model specifically (4.04)\n- This indicates selective adaptation rather than wholesale adoption\n\n**Option B**: INCORRECT\n- Mean of 4.04 for Anglo-Saxon model adoption is barely above neutral\n- This does not indicate enthusiasm for fully replacing their system\n\n**Option C**: CORRECT\n- Mean of 2.76 for statement (d) shows directors disagree that harmonization won't attract foreign investors\n- This means they believe it WILL attract foreign capital\n\n**Option D**: INCORRECT\n- Mean of 5.05 for international harmonization shows clear support, not indifference\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 270, "Question": "### Background\n\nStandard methods for insurance claims reserving, such as the Payments Per Claim Finalized (PPCF) method, often assume that the expected inflation-adjusted payment per claim is a stable function of the development year. However, this assumption can be violated if the speed at which claims are finalized changes over time. To address this, the paper proposes a 'See-Saw' method that models expected PPCF as a function of both the portfolio's maturity (measured in 'operational time') and the speed of finalization within a given period.\n\n### Data / Model Specification\n\nThe proposed 'See-Saw' model specifies the expected PPCF, denoted `E[P_ij]`, for claims from origin year `i` in development year `j` as a continuous, piecewise linear function of average operational time (`T̄_ij`) and speed of finalization (`F_ij`). The model is estimated via weighted regression.\n\nThe core model is given by:\n\n  \nE[P_{ij}] = \\alpha + \\sum_{k} \\beta_{k} \\bar{T}_{ij}^{(k)} + \\sum_{k} \\gamma_{k} F_{ij}^{(k)} \\quad \\text{(Eq. (1))}\n \n\nwhere `k` indexes distinct intervals of operational time. The coefficients `β_k` represent changes in the slope of the relationship with operational time, and `γ_k` represents the sensitivity of PPCF to finalization speed `F_ij` within the `k`-th operational time interval.\n\n- **Average Operational Time (`T̄_ij`)**: A measure of maturity, ranging from 0 (no claims finalized) to 1 (all claims finalized).\n- **Speed of Finalization (`F_ij`)**: The proportion of total claims finalized in the period `(i, j)`.\n\nEmpirical data and regression results from the paper are provided in Table 1 and Table 2 below.\n\n**Table 1: Operational Times and Speeds of Finalization (Excerpt for 1972 & 1973)**\n\n| Accident Year | | Dev Year 0 | Dev Year 1 | Dev Year 2 | Dev Year 3 | Dev Year 4 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **1972** | (1) `T̄_ij` | 0.035 | 0.190 | 0.370 | 0.565 | 0.805 |\n| | (2) `F_ij` | 0.07 | 0.24 | 0.12 | 0.27 | 0.21 |\n| **1973** | (1) `T̄_ij` | 0.030 | 0.135 | 0.365 | 0.670 | 0.870 |\n| | (2) `F_ij` | 0.06 | 0.15 | 0.31 | 0.30 | 0.10 |\n\n**Table 2: Regression Coefficients for the See-Saw Model**\n\n| Range k | From Op. Time | To Op. Time | βk | γk | α |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1 | 0 | 0.15 | -2191 | -33950 | |\n| 2 | 0.15 | 0.35 | +10410 | -22090 | |\n| 3 | 0.35 | 0.55 | -1194 | -30380 | -188100 |\n| 4 | 0.55 | 0.75 | -16720 | -30870 | |\n| 5 | 0.75 | 0.85 | -45.09 | -35260 | |\n| 6 | 0.85 | 0.95 | +14660 | -38010 | |\n| 7 | 0.95 | 1.00 | +208100 | -145200 | |\n\nBased on the See-Saw model and the provided data, which of the following statements are correct? (Select all that apply)", "Options": {"A": "The consistently negative `γ_k` coefficients in Table 2 imply that, at any given stage of maturity, an increase in the speed of finalization is associated with a decrease in the average payment per finalized claim.", "B": "For the 1973 accident year in development year 3, the average operational time of 0.670 falls into the `k=4` range, which has an associated `γ_4` coefficient of -30870.", "C": "For the 1973 accident year in development year 3, the average operational time of 0.670 falls into the `k=5` operational time range.", "D": "The `β_k` coefficients represent the direct impact of finalization speed on PPCF, while the `γ_k` coefficients capture the effect of operational time."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses interpretation and application of the paper's core model. It uses the 'Atomic Decomposition' strategy to create two independently verifiable correct statements from the original QA's parts 1 and 2. Option A tests the economic interpretation of the key 'see-saw' coefficient (`γ_k`). Option B tests the ability to correctly apply the model's piecewise structure by mapping a data point to the correct parameter set. Distractor C is a 'Conceptual Opposite' error, swapping the meaning of `β_k` and `γ_k`. Distractor D is a 'Lookup Error', testing careful reading of the coefficient table's ranges.", "qid": "270", "question": "### Background\n\nStandard methods for insurance claims reserving, such as the Payments Per Claim Finalized (PPCF) method, often assume that the expected inflation-adjusted payment per claim is a stable function of the development year. However, this assumption can be violated if the speed at which claims are finalized changes over time. To address this, the paper proposes a 'See-Saw' method that models expected PPCF as a function of both the portfolio's maturity (measured in 'operational time') and the speed of finalization within a given period.\n\n### Data / Model Specification\n\nThe proposed 'See-Saw' model specifies the expected PPCF, denoted `E[P_ij]`, for claims from origin year `i` in development year `j` as a continuous, piecewise linear function of average operational time (`T̄_ij`) and speed of finalization (`F_ij`). The model is estimated via weighted regression.\n\nThe core model is given by:\n\n  \nE[P_{ij}] = \\alpha + \\sum_{k} \\beta_{k} \\bar{T}_{ij}^{(k)} + \\sum_{k} \\gamma_{k} F_{ij}^{(k)} \\quad \\text{(Eq. (1))}\n \n\nwhere `k` indexes distinct intervals of operational time. The coefficients `β_k` represent changes in the slope of the relationship with operational time, and `γ_k` represents the sensitivity of PPCF to finalization speed `F_ij` within the `k`-th operational time interval.\n\n- **Average Operational Time (`T̄_ij`)**: A measure of maturity, ranging from 0 (no claims finalized) to 1 (all claims finalized).\n- **Speed of Finalization (`F_ij`)**: The proportion of total claims finalized in the period `(i, j)`.\n\nEmpirical data and regression results from the paper are provided in Table 1 and Table 2 below.\n\n**Table 1: Operational Times and Speeds of Finalization (Excerpt for 1972 & 1973)**\n\n| Accident Year | | Dev Year 0 | Dev Year 1 | Dev Year 2 | Dev Year 3 | Dev Year 4 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **1972** | (1) `T̄_ij` | 0.035 | 0.190 | 0.370 | 0.565 | 0.805 |\n| | (2) `F_ij` | 0.07 | 0.24 | 0.12 | 0.27 | 0.21 |\n| **1973** | (1) `T̄_ij` | 0.030 | 0.135 | 0.365 | 0.670 | 0.870 |\n| | (2) `F_ij` | 0.06 | 0.15 | 0.31 | 0.30 | 0.10 |\n\n**Table 2: Regression Coefficients for the See-Saw Model**\n\n| Range k | From Op. Time | To Op. Time | βk | γk | α |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1 | 0 | 0.15 | -2191 | -33950 | |\n| 2 | 0.15 | 0.35 | +10410 | -22090 | |\n| 3 | 0.35 | 0.55 | -1194 | -30380 | -188100 |\n| 4 | 0.55 | 0.75 | -16720 | -30870 | |\n| 5 | 0.75 | 0.85 | -45.09 | -35260 | |\n| 6 | 0.85 | 0.95 | +14660 | -38010 | |\n| 7 | 0.95 | 1.00 | +208100 | -145200 | |\n\nBased on the See-Saw model and the provided data, which of the following statements are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to evaluate each statement based on the See-Saw model specification and the provided data.\n\nLet me analyze each option:\n\n**Option A**: \"The consistently negative `γ_k` coefficients in Table 2 imply that, at any given stage of maturity, an increase in the speed of finalization is associated with a decrease in the average payment per finalized claim.\"\n\nFrom Table 2, all γ_k coefficients are indeed negative (ranging from -22,090 to -145,200). According to the model equation, γ_k represents the sensitivity of PPCF to finalization speed F_ij. Since these coefficients are negative, higher finalization speeds lead to lower expected PPCF values. This statement is **correct**.\n\n**Option B**: \"For the 1973 accident year in development year 3, the average operational time of 0.670 falls into the `k=4` range, which has an associated `γ_4` coefficient of -30870.\"\n\nFrom Table 1, T̄_ij for 1973 year 3 is 0.670. Looking at Table 2, range k=4 spans from 0.55 to 0.75. Since 0.55 < 0.670 < 0.75, this operational time does fall into range k=4, which has γ_4 = -30,870. This statement is **correct**.\n\n**Option C**: \"For the 1973 accident year in development year 3, the average operational time of 0.670 falls into the `k=5` operational time range.\"\n\nRange k=5 spans from 0.75 to 0.85. Since 0.670 < 0.75, the operational time 0.670 does not fall into range k=5. This statement is **incorrect**.\n\n**Option D**: \"The `β_k` coefficients represent the direct impact of finalization speed on PPCF, while the `γ_k` coefficients capture the effect of operational time.\"\n\nAccording to the model specification, β_k represents changes in the slope of the relationship with operational time (T̄_ij), while γ_k represents the sensitivity to finalization speed (F_", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 218, "Question": "### Background\n\n**Research Question.** This case explores the empirical strategy used to isolate the causal effect of a bank's monitoring effort on a borrower's loan announcement return, focusing on the role of robustness checks in addressing threats to identification.\n\n**Setting.** The analysis employs a cross-sectional regression where the dependent variable is the borrower's standardized cumulative abnormal return (`SCAR`). The main independent variable is `MONITORING EFFORT`. The main result is subjected to sensitivity analysis by adding more control variables.\n\n**Variables & Parameters.**\n- `SCAR_i`: Standardized cumulative abnormal return for borrower `i`.\n- `MONITORING_EFFORT_i`: Proxy for the monitoring ability of borrower `i`'s lead bank.\n- `LOAN LOSS RATIO`: A proxy for the bank's portfolio risk.\n- `BETA`: A measure of the borrower's systematic market risk.\n\n---\n\n### Data / Model Specification\n\nThe conceptual regression model is:\n  \nSCAR_i = \\beta_0 + \\beta_1 MONITORING\\_EFFORT_i + Controls_i + \\epsilon_i \\quad \\text{(Eq. (1))}\n \n\n---\n\n### The Question\n\nTo strengthen the causal interpretation of their findings, the authors perform robustness checks by adding more variables to the `Controls` vector in Eq. (1). Which of the following statements correctly identify the purpose of these specific checks? (Select all that apply.)", "Options": {"A": "Including the bank's `LOAN LOSS RATIO` helps to rule out the alternative explanation that `MONITORING EFFORT` is simply capturing the bank's general risk-taking behavior.", "B": "Including the bank's `LOAN LOSS RATIO` directly tests whether the `MONITORING EFFORT` proxy is biased due to the bank's involvement in investment banking.", "C": "Including the borrower's `BETA` is primarily intended to increase the regression's R-squared value.", "D": "Including the borrower's `BETA` helps to address the concern that high-`MONITORING EFFORT` banks appear to add more value merely because they select borrowers with different levels of systematic risk."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the user's understanding of the role of robustness checks in causal inference. It uses a Reverse-Reasoning strategy. Options A and B are correct as they accurately describe how controlling for a specific risk measure (bank-level or borrower-level) helps to isolate the effect of the variable of interest from a potential confounder. Distractor C (Conceptual Confusion) misattributes the purpose of the control; `LOAN LOSS RATIO` measures portfolio risk, not business mix bias. Distractor D (True but Irrelevant) states a potential side effect (higher R-squared) but misses the primary purpose of a robustness check, which is to test the stability of a key coefficient against alternative explanations.", "qid": "218", "question": "### Background\n\n**Research Question.** This case explores the empirical strategy used to isolate the causal effect of a bank's monitoring effort on a borrower's loan announcement return, focusing on the role of robustness checks in addressing threats to identification.\n\n**Setting.** The analysis employs a cross-sectional regression where the dependent variable is the borrower's standardized cumulative abnormal return (`SCAR`). The main independent variable is `MONITORING EFFORT`. The main result is subjected to sensitivity analysis by adding more control variables.\n\n**Variables & Parameters.**\n- `SCAR_i`: Standardized cumulative abnormal return for borrower `i`.\n- `MONITORING_EFFORT_i`: Proxy for the monitoring ability of borrower `i`'s lead bank.\n- `LOAN LOSS RATIO`: A proxy for the bank's portfolio risk.\n- `BETA`: A measure of the borrower's systematic market risk.\n\n---\n\n### Data / Model Specification\n\nThe conceptual regression model is:\n  \nSCAR_i = \\beta_0 + \\beta_1 MONITORING\\_EFFORT_i + Controls_i + \\epsilon_i \\quad \\text{(Eq. (1))}\n \n\n---\n\n### The Question\n\nTo strengthen the causal interpretation of their findings, the authors perform robustness checks by adding more variables to the `Controls` vector in Eq. (1). Which of the following statements correctly identify the purpose of these specific checks? (Select all that apply.)", "model_evaluation_result": {"model_raw_response": "Looking at this question about robustness checks in a regression analysis examining the causal effect of bank monitoring effort on borrower loan announcement returns, I need to evaluate each option based on whether it correctly identifies the purpose of adding specific control variables.\n\nLet me analyze each option:\n\n**Option A**: Including the bank's `LOAN LOSS RATIO` helps to rule out the alternative explanation that `MONITORING EFFORT` is simply capturing the bank's general risk-taking behavior.\n- This is correct. The loan loss ratio is a proxy for the bank's portfolio risk and general risk-taking behavior. By controlling for this, researchers can better isolate the effect of monitoring effort from the confounding effect of the bank's risk profile.\n\n**Option B**: Including the bank's `LOAN LOSS RATIO` directly tests whether the `MONITORING EFFORT` proxy is biased due to the bank's involvement in investment banking.\n- This is incorrect. The loan loss ratio measures portfolio risk, not investment banking activities. This control addresses risk-taking behavior, not investment banking involvement.\n\n**Option C**: Including the borrower's `BETA` is primarily intended to increase the regression's R-squared value.\n- This is incorrect. In causal analysis, controls are added to address confounding factors and strengthen causal interpretation, not primarily to improve model fit statistics.\n\n**Option D**: Including the borrower's `BETA` helps to address the concern that high-`MONITORING EFFORT` banks appear to add more value merely because they select borrowers with different levels of systematic risk.\n- This is correct. BETA measures systematic market risk. If high-monitoring banks systematically select borrowers with different risk profiles, this could create a spurious relationship. Controlling for BETA helps isolate the true monitoring effect from selection effects based on borrower risk characteristics.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 147, "Question": "### Background\n\n**Research Question.** How does the degree of convexity in the corporate tax code affect the systematic risk (equity beta) of a firm?\n\n**Setting / Data-Generating Environment.** The study uses simulations from a structural model to quantify the impact of tax asymmetry on equity beta. The model is parameterized with baseline values, and the effect of tax asymmetry is examined for firms with different levels of operating leverage.\n\n**Variables & Parameters.**\n- `β_E`: The firm's equity beta.\n- `δ`: The level of tax asymmetry, representing tax convexity. `δ=0` is a linear tax system.\n- `X`: The firm's profit flow, an inverse proxy for leverage. Lower `X` implies higher leverage.\n\n---\n\n### Data / Model Specification\n\nThe model's simulation results for the base case parameters are presented for varying levels of tax asymmetry (`δ`) and profit (`X`). Lower profit levels, for a fixed debt payment, correspond to higher leverage.\n\n**Table 1. Equity Beta for Different Tax Asymmetry and Leverage Levels**\n\n| `δ` | Low Profit (X=2) (High Leverage) | Medium Profit (X=4) (Medium Leverage) | High Profit (X=6) (Low Leverage) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.4718 | 1.0964 | 0.9800 |\n| 0.05 | 1.5006 | 1.1152 | 1.0039 |\n| 0.10 | 1.5326 | 1.1321 | 1.0248 |\n| 0.15 | 1.5623 | 1.1474 | 1.0439 |\n| 0.20 | 1.5895 | 1.1614 | 1.0615 |\n| 0.25 | 1.6143 | 1.1742 | 1.0779 |\n| 0.30 | 1.6370 | 1.1859 | 1.0930 |\n| 0.35 | 1.6578 | 1.1968 | 1.1072 |\n\n---\n\n### Question\n\nBased on the simulation results in Table 1 and the underlying economic theory, which of the following statements are correct?", "Options": {"A": "In a linear tax system (`δ=0`), a firm's financial leverage (as proxied by `X`) has no impact on its equity beta.", "B": "The model demonstrates a positive monotonic relationship between tax asymmetry (`δ`) and equity beta (`β_E`) for all leverage levels shown.", "C": "The economic mechanism for the relationship is that a higher `δ` reduces the government's role as a risk-sharing partner in downturns, thereby increasing the risk borne by equity holders.", "D": "The absolute increase in equity beta when `δ` changes from 0.00 to 0.35 is largest for the low-leverage (High Profit, X=6) firm."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the main finding from a simulation table and connect it to the core economic intuition. It uses an Atomic Decomposition strategy, combining a direct data interpretation (A) with a theoretical explanation (B). Distractor C is a 'Numerical: Step-Omission Error' that requires a simple calculation to disprove. Distractor D is a 'Conceptual Opposite' that contradicts a basic principle of leverage shown clearly in the `δ=0` row of the table.", "qid": "147", "question": "### Background\n\n**Research Question.** How does the degree of convexity in the corporate tax code affect the systematic risk (equity beta) of a firm?\n\n**Setting / Data-Generating Environment.** The study uses simulations from a structural model to quantify the impact of tax asymmetry on equity beta. The model is parameterized with baseline values, and the effect of tax asymmetry is examined for firms with different levels of operating leverage.\n\n**Variables & Parameters.**\n- `β_E`: The firm's equity beta.\n- `δ`: The level of tax asymmetry, representing tax convexity. `δ=0` is a linear tax system.\n- `X`: The firm's profit flow, an inverse proxy for leverage. Lower `X` implies higher leverage.\n\n---\n\n### Data / Model Specification\n\nThe model's simulation results for the base case parameters are presented for varying levels of tax asymmetry (`δ`) and profit (`X`). Lower profit levels, for a fixed debt payment, correspond to higher leverage.\n\n**Table 1. Equity Beta for Different Tax Asymmetry and Leverage Levels**\n\n| `δ` | Low Profit (X=2) (High Leverage) | Medium Profit (X=4) (Medium Leverage) | High Profit (X=6) (Low Leverage) |\n|:---|:---:|:---:|:---:|\n| 0.00 | 1.4718 | 1.0964 | 0.9800 |\n| 0.05 | 1.5006 | 1.1152 | 1.0039 |\n| 0.10 | 1.5326 | 1.1321 | 1.0248 |\n| 0.15 | 1.5623 | 1.1474 | 1.0439 |\n| 0.20 | 1.5895 | 1.1614 | 1.0615 |\n| 0.25 | 1.6143 | 1.1742 | 1.0779 |\n| 0.30 | 1.6370 | 1.1859 | 1.0930 |\n| 0.35 | 1.6578 | 1.1968 | 1.1072 |\n\n---\n\n### Question\n\nBased on the simulation results in Table 1 and the underlying economic theory, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "Looking at Table 1 and analyzing each statement:\n\n**Statement A**: \"In a linear tax system (δ=0), a firm's financial leverage (as proxied by X) has no impact on its equity beta.\"\n\nWhen δ=0:\n- Low Profit (X=2, High Leverage): β_E = 1.4718\n- Medium Profit (X=4, Medium Leverage): β_E = 1.0964  \n- High Profit (X=6, Low Leverage): β_E = 0.9800\n\nThe equity beta clearly varies with leverage levels even when δ=0. Higher leverage (lower X) corresponds to higher equity beta. This statement is **incorrect**.\n\n**Statement B**: \"The model demonstrates a positive monotonic relationship between tax asymmetry (δ) and equity beta (β_E) for all leverage levels shown.\"\n\nExamining each column:\n- For X=2: β_E increases from 1.4718 to 1.6578 as δ increases from 0 to 0.35\n- For X=4: β_E increases from 1.0964 to 1.1968 as δ increases from 0 to 0.35\n- For X=6: β_E increases from 0.9800 to 1.1072 as δ increases from 0 to 0.35\n\nIn all cases, β_E increases monotonically with δ. This statement is **correct**.\n\n**Statement C**: \"The economic mechanism for the relationship is that a higher δ reduces the government's role as a risk-sharing partner in downturns, thereby increasing the risk borne by equity holders.\"\n\nThis is a standard economic explanation for tax convexity effects. In a linear tax system (δ=0), the government shares both upside and downside risk proportionally. With tax convexity (δ>0), the government takes less in losses during downturns while still taking its share in good times, effectively reducing its risk-sharing role and increasing equity risk. This statement is **correct**.\n\n**Statement D**: \"The absolute increase in equity beta when δ changes from 0.00 to 0.35 is largest for the low-leverage (High Profit, X=6)", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 216, "Question": "### Background\n\n**Research Question.** Can circuit breakers effectively stabilize financial markets by taming endogenous volatility, and how does this conclusion hold up against the efficient-market critique that such interventions merely delay necessary price discovery, especially when the fundamental value is non-stationary?\n\n**Setting.** The paper's agent-based model is used as a laboratory to study the effects of circuit breakers. The analysis is made more realistic by allowing the log fundamental value `F_t` to evolve as a random walk, which provides a framework for evaluating Fama's critique that circuit breakers hinder the market's ability to track new information.\n\n**Variables and Parameters.**\n- `P_t`: Log price of the stock at time `t`.\n- `F_t`: Log fundamental value at time `t`.\n- `n_t`: A normally distributed fundamental shock, `n_t \\sim N(0, (\\sigma^F)^2)`.\n- `s`: The circuit breaker threshold, i.e., the maximum allowed absolute log price change.\n- `V_t`: Market volatility, an EWMA of squared price changes.\n- `\\sigma_t^2`: Trading intensity (variance of speculator demand).\n- `\\rho_t`: Correlation of speculator demands (herding).\n- `C_t`: A condition measuring sharp price reversals.\n- Distortion: A measure of the deviation between `P_t` and `F_t`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental value evolves as a random walk:\n  \nF_t = F_{t-1} + n_t \\quad \\text{(Eq. 1)}\n \nThe potential price `P_{t+1}^*` is calculated based on aggregate demand, and the circuit breaker rule determines the actual price `P_{t+1}`:\n  \nP_{t+1} = \\begin{cases} P_t + s & \\text{if } (P_{t+1}^* - P_t) > s \\\\ P_t - s & \\text{if } (P_{t+1}^* - P_t) < -s \\\\ P_{t+1}^* & \\text{otherwise} \\end{cases} \\quad \\text{(Eq. 2)}\n \nKey endogenous feedback variables are defined as:\n  \nV_{t} = d V_{t-1} + (1-d)(P_{t}-P_{t-1})^{2} \\quad \\text{(Eq. 3)}\n \n  \n\\sigma_{t}^{2} = e^{l} + \\frac{e^{h}-e^{l}}{1+\\exp[-e^{s}(V_{t}-\\bar{V})]} \\quad \\text{(Eq. 4)}\n \n  \nC_{t} = ((P_{t}-P_{t-1})-(P_{t-1}-P_{t-2}))^{2} \\quad \\text{(Eq. 5)}\n \n  \n\\rho_{t} = f^{l} + \\frac{f^{h}-f^{l}}{1+\\exp[-f^{s}(C_{t}-\\bar{C})]} \\quad \\text{(Eq. 6)}\n \n\n---\n\n### Question\n\nAccording to the model presented, circuit breakers stabilize the market through both direct and indirect effects. Select all statements that accurately describe an **indirect** mechanism by which circuit breakers reduce market volatility.", "Options": {"A": "They cause unfulfilled orders to be executed in the next period with higher priority, leading to a faster resolution of price discovery.", "B": "By capping the magnitude of price changes, they prevent the volatility measure `V_t` from rising excessively, which in turn suppresses speculators' trading intensity `\\sigma_t^2`.", "C": "They directly reduce the fundamental trading parameter `c`, making speculators less responsive to mispricing.", "D": "By limiting the size of consecutive price swings, they cap the value of the reversal condition `C_t`, which prevents the correlation `\\rho_t` from jumping to high levels associated with herding."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the understanding of the specific indirect feedback mechanisms through which circuit breakers operate in the model. It uses an **Atomic Decomposition** strategy, breaking down the complex answer from the original QA into distinct, verifiable propositions. **Correct options (A, C)** describe the two primary indirect channels: dampening the volatility-intensity loop and preventing the reversal-herding loop. **Distractors** are designed to be high-fidelity: (B) presents a plausible but incorrect mechanism by confusing variables with parameters (Conceptual Error), while (D) introduces a false premise about order execution that aligns with the 'volatility spillover' critique the paper argues against (Conceptual Opposite).", "qid": "216", "question": "### Background\n\n**Research Question.** Can circuit breakers effectively stabilize financial markets by taming endogenous volatility, and how does this conclusion hold up against the efficient-market critique that such interventions merely delay necessary price discovery, especially when the fundamental value is non-stationary?\n\n**Setting.** The paper's agent-based model is used as a laboratory to study the effects of circuit breakers. The analysis is made more realistic by allowing the log fundamental value `F_t` to evolve as a random walk, which provides a framework for evaluating Fama's critique that circuit breakers hinder the market's ability to track new information.\n\n**Variables and Parameters.**\n- `P_t`: Log price of the stock at time `t`.\n- `F_t`: Log fundamental value at time `t`.\n- `n_t`: A normally distributed fundamental shock, `n_t \\sim N(0, (\\sigma^F)^2)`.\n- `s`: The circuit breaker threshold, i.e., the maximum allowed absolute log price change.\n- `V_t`: Market volatility, an EWMA of squared price changes.\n- `\\sigma_t^2`: Trading intensity (variance of speculator demand).\n- `\\rho_t`: Correlation of speculator demands (herding).\n- `C_t`: A condition measuring sharp price reversals.\n- Distortion: A measure of the deviation between `P_t` and `F_t`.\n\n---\n\n### Data / Model Specification\n\nThe fundamental value evolves as a random walk:\n  \nF_t = F_{t-1} + n_t \\quad \\text{(Eq. 1)}\n \nThe potential price `P_{t+1}^*` is calculated based on aggregate demand, and the circuit breaker rule determines the actual price `P_{t+1}`:\n  \nP_{t+1} = \\begin{cases} P_t + s & \\text{if } (P_{t+1}^* - P_t) > s \\\\ P_t - s & \\text{if } (P_{t+1}^* - P_t) < -s \\\\ P_{t+1}^* & \\text{otherwise} \\end{cases} \\quad \\text{(Eq. 2)}\n \nKey endogenous feedback variables are defined as:\n  \nV_{t} = d V_{t-1} + (1-d)(P_{t}-P_{t-1})^{2} \\quad \\text{(Eq. 3)}\n \n  \n\\sigma_{t}^{2} = e^{l} + \\frac{e^{h}-e^{l}}{1+\\exp[-e^{s}(V_{t}-\\bar{V})]} \\quad \\text{(Eq. 4)}\n \n  \nC_{t} = ((P_{t}-P_{t-1})-(P_{t-1}-P_{t-2}))^{2} \\quad \\text{(Eq. 5)}\n \n  \n\\rho_{t} = f^{l} + \\frac{f^{h}-f^{l}}{1+\\exp[-f^{s}(C_{t}-\\bar{C})]} \\quad \\text{(Eq. 6)}\n \n\n---\n\n### Question\n\nAccording to the model presented, circuit breakers stabilize the market through both direct and indirect effects. Select all statements that accurately describe an **indirect** mechanism by which circuit breakers reduce market volatility.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which statements describe **indirect** mechanisms by which circuit breakers reduce market volatility according to the model.\n\nLet me analyze each option:\n\n**Option A**: \"They cause unfulfilled orders to be executed in the next period with higher priority, leading to a faster resolution of price discovery.\"\n- The model equations don't show any mechanism for unfulfilled orders or priority execution systems. This appears to be describing a different type of market mechanism not present in this model.\n\n**Option B**: \"By capping the magnitude of price changes, they prevent the volatility measure `V_t` from rising excessively, which in turn suppresses speculators' trading intensity `\\sigma_t^2`.\"\n- This describes an indirect mechanism: Circuit breakers → cap price changes → reduce V_t (via Eq. 3) → reduce σ_t² (via Eq. 4)\n- This is a clear feedback loop where the circuit breaker indirectly affects trading intensity through the volatility measure.\n\n**Option C**: \"They directly reduce the fundamental trading parameter `c`, making speculators less responsive to mispricing.\"\n- The word \"directly\" explicitly indicates this is not an indirect mechanism.\n- Also, parameter `c` is not defined in the given model specification.\n\n**Option D**: \"By limiting the size of consecutive price swings, they cap the value of the reversal condition `C_t`, which prevents the correlation `\\rho_t` from jumping to high levels associated with herding.\"\n- This describes an indirect mechanism: Circuit breakers → limit price swings → reduce C_t (via Eq. 5) → prevent high ρ_t (via Eq. 6)\n- This is another feedback loop where the circuit breaker indirectly affects herding behavior through the reversal condition.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 80, "Question": "### Background\n\n**Research Question.** This case challenges the canonical definitions of risk from Modern Portfolio Theory (MPT) and the Capital Asset Pricing Model (CAPM). The survey findings reveal a profound disconnect between the practitioner's context-driven view of risk and the abstract, universal definitions used in finance theory.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, who operate as agents within an organizational context. The survey shows that for these managers, practical risks like 'asset-liability matching' and 'commercial risk' (career/business risk) dominate theoretical risks like 'beta' and 'total variability of return'.\n\n---\n\n### Question\n\nThe empirical findings challenge the universality of finance theory's standard risk models. Select all of the following theoretical critiques that are directly supported by the survey evidence.", "Options": {"A": "The survey confirms that while managers' stated risk preferences differ from theory, their actual portfolio construction closely follows the principles of mean-variance optimization.", "B": "The evidence suggests that a fund manager's perception of risk is endogenous to their institutional and contractual environment, not an objective property of an asset's return distribution.", "C": "The results show that asset-liability matching is simply a practical application of minimizing the beta of a portfolio relative to a liability index.", "D": "The agent-principal relationship in delegated management, with its focus on career risk and mandate fulfillment, makes the 'rational individual investor' assumed in MPT/CAPM a poor behavioral model for professional fund managers."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to translate empirical findings into a coherent theoretical critique. It uses an 'Atomic Decomposition' strategy, breaking down the original QA's high-level critique into testable propositions. Correct options A and B capture the core arguments that risk is endogenous and that the standard behavioral model of the 'investor' is inappropriate for agents. Distractor C is a 'Conceptual Opposite,' creating a false conflict between perception and action. Distractor D creates a false equivalence between a practical heuristic (A-L matching) and a theoretical concept (beta) that the paper explicitly shows managers disregard.", "qid": "80", "question": "### Background\n\n**Research Question.** This case challenges the canonical definitions of risk from Modern Portfolio Theory (MPT) and the Capital Asset Pricing Model (CAPM). The survey findings reveal a profound disconnect between the practitioner's context-driven view of risk and the abstract, universal definitions used in finance theory.\n\n**Setting and Sample.** The analysis is based on a 1990 survey of UK fund managers, who operate as agents within an organizational context. The survey shows that for these managers, practical risks like 'asset-liability matching' and 'commercial risk' (career/business risk) dominate theoretical risks like 'beta' and 'total variability of return'.\n\n---\n\n### Question\n\nThe empirical findings challenge the universality of finance theory's standard risk models. Select all of the following theoretical critiques that are directly supported by the survey evidence.", "model_evaluation_result": {"model_raw_response": "Looking at each option against the survey evidence described:\n\nA) This contradicts the findings. The survey shows a \"profound disconnect\" between practitioners' views and theory, not that their actions follow mean-variance optimization despite different stated preferences.\n\nB) This is directly supported. The survey shows managers prioritize context-specific risks like \"asset-liability matching\" and \"commercial risk\" over theoretical measures, confirming that risk perception depends on their institutional environment rather than being an objective property of returns.\n\nC) This mischaracterizes the findings. Asset-liability matching is presented as a practical risk that \"dominates\" theoretical risks like beta, not as an application of beta minimization.\n\nD) This is directly supported. The survey explicitly shows that managers focus on \"career/business risk\" and operate \"as agents within an organizational context,\" demonstrating that the agent-principal relationship makes the rational individual investor assumption inappropriate for professional fund managers.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 247, "Question": "### Background\n\n**Research Question.** What are the key economic properties of the modified expectile reinsurance premium, and what do they imply about its suitability for practical applications in insurance risk management?\n\n**Setting.** The modified expectile premium `c = ~e_{X,I(X),α}` is the unique solution to:\n  \n(1-\\alpha)c=\\alpha\\mathbb{E}[(I(X)-c)_{+}]-(1-\\alpha)\\mathbb{E}[X-I(X)] \n \n\n### Data / Model Specification\n\nThis premium has several important properties, including:\n\n**Subadditivity:** For two risks `X` and `Y`, if `I(X+Y) ≤ I_1(X) + I_2(Y)`, then `~e_{X+Y,I(X+Y),α} ≤ ~e_{X,I_1(X),α} + ~e_{Y,I_2(Y),α}`.\n\n**Risk Loading:** The premium includes a non-negative risk loading (`~e_{X,I(X),α} ≥ E[I(X)]`) if the reinsurer's weight `α` exceeds a certain threshold:\n  \n\\alpha \\ge \\alpha_{X,I(X)}^{*} = 1-\\frac{\\mathbb{E}[(I(X)-\\mathbb{E}[I(X)])_{+}]}{\\mathbb{E}[(I(X)-E[I(X)])_{+}]+\\mathbb{E}[X]} \n \n\n### Question\n\nBased on the specified properties of the modified expectile premium, select all of the following statements that are **INCORRECT**.", "Options": {"A": "If the reinsurer's weight `α` is less than the threshold `α*`, the resulting premium `~e_{X,I(X),α}` will be less than the expected ceded loss `E[I(X)]`.", "B": "The risk loading threshold `α*` will be higher for a ceded loss `I_B(X)` with high tail risk compared to a ceded loss `I_A(X)` with lower tail risk, assuming both have the same expected value `E[I(X)]`.", "C": "The risk loading property ensures the premium is always greater than the expected ceded loss, regardless of the value of `α`.", "D": "The subadditivity property implies that an insurer can potentially lower its total reinsurance cost by bundling different risk portfolios (e.g., life and property) into a single contract rather than reinsuring them separately."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: Evaluate understanding of the economic properties (subadditivity, risk loading) of the modified expectile premium. Strategy: Scenario Application and Computational Judgment. The Single-Correct-Answer Inversion Protocol was used. The original assessment objective was to identify correct properties. However, only one option (A) was unambiguously correct based on direct interpretation. To create a valid Multiple Response item, the stem was inverted to 'select all that are INCORRECT'. Distractor Logic: Option A is a correct interpretation of subadditivity and is now a distractor. Option C is a correct deduction from the risk loading property and is also a distractor. The new correct answers are B and D. Option B is incorrect because, as shown in the original QA, a contract with higher tail risk has a *lower* `α*` threshold. Option D is incorrect because the risk loading is conditional on `α ≥ α*`; it is not guaranteed for all `α`.", "qid": "247", "question": "### Background\n\n**Research Question.** What are the key economic properties of the modified expectile reinsurance premium, and what do they imply about its suitability for practical applications in insurance risk management?\n\n**Setting.** The modified expectile premium `c = ~e_{X,I(X),α}` is the unique solution to:\n  \n(1-\\alpha)c=\\alpha\\mathbb{E}[(I(X)-c)_{+}]-(1-\\alpha)\\mathbb{E}[X-I(X)] \n \n\n### Data / Model Specification\n\nThis premium has several important properties, including:\n\n**Subadditivity:** For two risks `X` and `Y`, if `I(X+Y) ≤ I_1(X) + I_2(Y)`, then `~e_{X+Y,I(X+Y),α} ≤ ~e_{X,I_1(X),α} + ~e_{Y,I_2(Y),α}`.\n\n**Risk Loading:** The premium includes a non-negative risk loading (`~e_{X,I(X),α} ≥ E[I(X)]`) if the reinsurer's weight `α` exceeds a certain threshold:\n  \n\\alpha \\ge \\alpha_{X,I(X)}^{*} = 1-\\frac{\\mathbb{E}[(I(X)-\\mathbb{E}[I(X)])_{+}]}{\\mathbb{E}[(I(X)-E[I(X)])_{+}]+\\mathbb{E}[X]} \n \n\n### Question\n\nBased on the specified properties of the modified expectile premium, select all of the following statements that are **INCORRECT**.", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement to determine which are INCORRECT based on the properties of the modified expectile premium.\n\nLet me examine each statement:\n\n**Statement A**: \"If the reinsurer's weight α is less than the threshold α*, the resulting premium ~e_{X,I(X),α} will be less than the expected ceded loss E[I(X)].\"\n\nFrom the risk loading property, we know that ~e_{X,I(X),α} ≥ E[I(X)] if and only if α ≥ α*. This means if α < α*, then ~e_{X,I(X),α} < E[I(X)]. Therefore, Statement A is CORRECT.\n\n**Statement B**: \"The risk loading threshold α* will be higher for a ceded loss I_B(X) with high tail risk compared to a ceded loss I_A(X) with lower tail risk, assuming both have the same expected value E[I(X)].\"\n\nLooking at the formula for α*:\nα* = 1 - E[(I(X)-E[I(X)])_+] / (E[(I(X)-E[I(X)])_+] + E[X])\n\nIf I_B(X) has higher tail risk than I_A(X) with the same expected value, then E[(I_B(X)-E[I(X)])_+] > E[(I_A(X)-E[I(X)])_+]. This means the numerator in the fraction is larger for I_B(X), making the fraction larger, which makes α*_B smaller (not higher) than α*_A. Therefore, Statement B is INCORRECT.\n\n**Statement C**: \"The risk loading property ensures the premium is always greater than the expected ceded loss, regardless of the value of α.\"\n\nThe risk loading property explicitly states that ~e_{X,I(X),α} ≥ E[I(X)] only if α ≥ α*. This means the premium is NOT always greater than the expected ceded loss - it depends on whether α meets the threshold. Therefore, Statement C is INCORRECT.\n\n**Statement D**: \"The subadditivity property implies that an insurer can potentially lower its total reinsurance cost by bundling", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 85, "Question": "### Background\n\nThe study examines the stock market impact of pro-union legislation in New Zealand, segmenting firms into High (HU), Medium (MU), and Low (LU) unionization groups based on prior unionization density. It compares their Average Cumulative Abnormal Returns (ACARs) in response to `Aggressive` legislative events that signal stronger union power. The central hypothesis is that these events are more costly for LU firms than for HU firms, for whom the new regulations may be *inframarginal*—that is, less binding than their existing labor-management practices.\n\n### Data / Model Specification\n\nThe table below presents the 5-day ACARs for the HU and LU groups in response to the combined set of `Aggressive` events. A two-sample t-test is used to determine if the difference in returns between the two groups is statistically significant.\n\n**Table 1: 5-Day ACAR in Response to Aggressive Events**\n\n| Group    | 5-Day ACAR | BMP t-statistic |\n| :------- | :--------: | :-------------: |\n| HU group | -0.030     | [-0.13]         |\n| LU group | -0.093     | [-2.50]***      |\n\n**Two-sample test of difference:**\n- `t-statistic (HU vs. LU)`: 1.67**\n\n*Note: ***, ** denote significance at the 1% and 5% levels, respectively.*\n\n---\n\nBased on the data in Table 1 and the study's framework, which of the following statements are valid interpretations or conclusions?", "Options": {"A": "The LU group experienced a smaller negative valuation impact than the HU group, as shown by the smaller absolute value of its t-statistic.", "B": "The legislation's impact was likely inframarginal for the HU group, as evidenced by their statistically insignificant stock price reaction.", "C": "The market reaction for the LU group was statistically significant, suggesting investors priced in a substantial negative impact from the legislation for these firms.", "D": "The difference in the market's reaction between the HU and LU groups is statistically significant at the 5% level."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret statistical results from an event study table (significance, magnitude) and connect them to the core economic concept of 'inframarginality' as described in the paper. Depth Strategy: Computational Judgment. The user must evaluate multiple pieces of evidence from the table (t-statistics for individual groups, t-statistic for the difference) to form a comprehensive conclusion. Distractor Logic: D is a numerical distractor that misinterprets the meaning of the t-statistic relative to the ACAR value and incorrectly compares the groups' impacts.", "qid": "85", "question": "### Background\n\nThe study examines the stock market impact of pro-union legislation in New Zealand, segmenting firms into High (HU), Medium (MU), and Low (LU) unionization groups based on prior unionization density. It compares their Average Cumulative Abnormal Returns (ACARs) in response to `Aggressive` legislative events that signal stronger union power. The central hypothesis is that these events are more costly for LU firms than for HU firms, for whom the new regulations may be *inframarginal*—that is, less binding than their existing labor-management practices.\n\n### Data / Model Specification\n\nThe table below presents the 5-day ACARs for the HU and LU groups in response to the combined set of `Aggressive` events. A two-sample t-test is used to determine if the difference in returns between the two groups is statistically significant.\n\n**Table 1: 5-Day ACAR in Response to Aggressive Events**\n\n| Group    | 5-Day ACAR | BMP t-statistic |\n| :------- | :--------: | :-------------: |\n| HU group | -0.030     | [-0.13]         |\n| LU group | -0.093     | [-2.50]***      |\n\n**Two-sample test of difference:**\n- `t-statistic (HU vs. LU)`: 1.67**\n\n*Note: ***, ** denote significance at the 1% and 5% levels, respectively.*\n\n---\n\nBased on the data in Table 1 and the study's framework, which of the following statements are valid interpretations or conclusions?", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1 and analyzing each statement:\n\nA) This statement is incorrect. The LU group experienced a LARGER negative valuation impact (-0.093) compared to the HU group (-0.030). The t-statistic magnitude refers to statistical significance, not the size of the valuation impact.\n\nB) This statement is correct. The HU group's 5-day ACAR of -0.030 has a t-statistic of -0.13, which is not statistically significant. This supports the hypothesis that the legislation was inframarginal (less binding) for highly unionized firms.\n\nC) This statement is correct. The LU group's t-statistic of -2.50*** indicates significance at the 1% level, showing that investors viewed the pro-union legislation as having a substantial negative impact on low-unionization firms.\n\nD) This statement is correct. The two-sample t-statistic of 1.67** indicates the difference between HU and LU groups is statistically significant at the 5% level (as denoted by the ** notation).\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 153, "Question": "### Background\n\nThe study uses an instrumental variables (IV) approach to estimate the causal effect of an employee's total spending account balance (`Total_Account`) on their health care spending. The primary instrument is the minimum account balance within the employee's firm and coverage tier (`Min_Account`). The validity of this strategy rests on two key assumptions: relevance and the exclusion restriction.\n\n### Data / Model Specification\n\n**Second Stage (Structural Equation):**\n  \nSpending_{igt} = \\beta_0 + \\beta_1 Total\\_Account_{igt} + \\dots + \\varepsilon_{igt} \\quad \\text{(Eq. (1))}\n \n**First Stage Equation:**\n  \nTotal\\_Account_{igt} = \\alpha_0 + \\alpha_1 Min\\_Account_{gt} + \\dots + u_{igt} \\quad \\text{(Eq. (2))}\n \n\n---\n\nAccording to the paper's description of its IV strategy, which of the following statements are valid claims or assumptions?\n", "Options": {"A": "The exclusion restriction assumes that the firm's contribution policy (proxied by `Min_Account`) does not directly respond to a single employee's idiosyncratic health shocks.", "B": "The relevance of the `Min_Account` instrument stems from the fact that the firm's contribution is a common component in all employees' total account balances.", "C": "The exclusion restriction can be formally tested using the first-stage F-statistic.", "D": "The exclusion restriction is more likely to be violated in larger firms, where individual health shocks are more likely to influence aggregate benefit design."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the core assumptions underlying the paper's instrumental variables identification strategy. Depth Strategy: Reverse-Reasoning (identifying necessary pre-conditions for the model's validity). Distractor Logic: Option A correctly states the core logic of the exclusion restriction in this context. Option B correctly explains the economic mechanism that makes the instrument relevant. Option C is a Conceptual Opposite; the paper explicitly argues the opposite, stating that the assumption is *more* plausible in larger firms and for this reason they exclude very small firms. Option D is a Conceptual Distractor (Formula Misuse); the F-statistic tests for instrument relevance (strength), not the exclusion restriction, which is untestable.", "qid": "153", "question": "### Background\n\nThe study uses an instrumental variables (IV) approach to estimate the causal effect of an employee's total spending account balance (`Total_Account`) on their health care spending. The primary instrument is the minimum account balance within the employee's firm and coverage tier (`Min_Account`). The validity of this strategy rests on two key assumptions: relevance and the exclusion restriction.\n\n### Data / Model Specification\n\n**Second Stage (Structural Equation):**\n  \nSpending_{igt} = \\beta_0 + \\beta_1 Total\\_Account_{igt} + \\dots + \\varepsilon_{igt} \\quad \\text{(Eq. (1))}\n \n**First Stage Equation:**\n  \nTotal\\_Account_{igt} = \\alpha_0 + \\alpha_1 Min\\_Account_{gt} + \\dots + u_{igt} \\quad \\text{(Eq. (2))}\n \n\n---\n\nAccording to the paper's description of its IV strategy, which of the following statements are valid claims or assumptions?\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement about the instrumental variables (IV) approach:\n\nA) **Valid claim**: The exclusion restriction requires that the instrument (Min_Account) affects the outcome (Spending) only through the endogenous variable (Total_Account). This statement correctly identifies that the firm's contribution policy should not respond to individual employee health shocks for the exclusion restriction to hold.\n\nB) **Valid claim**: The relevance condition requires that the instrument (Min_Account) is correlated with the endogenous variable (Total_Account). Since the firm's minimum contribution is indeed a component of employees' total account balances, this creates the necessary correlation for instrument relevance.\n\nC) **Invalid claim**: The first-stage F-statistic tests for instrument relevance (whether the instrument is sufficiently correlated with the endogenous variable), not the exclusion restriction. The exclusion restriction cannot be formally tested and must be argued based on economic reasoning.\n\nD) **Invalid claim**: This statement is backwards. In larger firms, individual employee health shocks are less likely to influence firm-wide benefit design decisions because any single employee represents a smaller fraction of the total workforce. The exclusion restriction would be more likely to be violated in smaller firms where individual shocks could have more influence on aggregate decisions.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 313, "Question": "### Background\n\n**Research Question.** This study evaluates the evidence for money endogeneity versus exogeneity in the United States, paying close attention to the effects of a major monetary policy regime shift in the mid-1980s. The analysis seeks to synthesize results from multiple econometric tests to form a robust conclusion.\n\n**Setting / Data-Generating Environment.** The analysis uses quarterly time-series data for the US, split into two sub-periods to account for a structural break: US 1 (1975:3–1986:4), a period characterized by monetary aggregate targeting, and US 2 (1987:1–2007:1), a period characterized by interest rate targeting. The study employs Vector Error-Correction Models (VECM) for cointegrated series and trivariate Vector Autoregressions (VAR) for robustness checks.\n\n**Variables & Parameters.**\n- `BL`: Bank Loans\n- `MS`: Money Supply\n- `DEP`: Bank Deposits\n- `ECT`: Error-Correction Term. Its coefficient indicates the speed of adjustment to long-run equilibrium.\n- `→`: Unidirectional Granger causality.\n- `↔`: Bidirectional Granger causality.\n- `EXO`: Conclusion that money supply is Exogenous.\n- `ENDO`: Conclusion that money supply is Endogenous.\n\n---\n\n### Data / Model Specification\n\nEvidence is drawn from three sources: a summary of primary causality tests, detailed VECM estimates, and a trivariate VAR robustness check.\n\n**Table 1: Summary of Causality Test Results for BL and MS**\n\n| Period | Long Run | Conclusion | Monetary Policy |\n| :--- | :--- | :--- | :--- |\n| US 1 | MS → BL | EXO | Monetary Targeting |\n| US 2 | BL ↔ MS | ENDO | Interest Rate Targeting |\n\n**Table 2: VECM Results for US 2 (1987:1–2007:1)**\n\n| DV | INDV | ECT Coefficient | t-Statistic | Long Run Conclusion |\n| :--- | :--- | :--- | :--- | :--- |\n| BL | MS | -0.170 | [-5.825]*** | MS → BL |\n| MS | BL | -0.123 | [-6.919]*** | BL → MS |\n\n*Note: DV = Dependent Variable, INDV = Independent Variable. *** denotes significance at 1%.*\n\n**Table 3: Trivariate VAR Causality Results for US 2 (1987:1–2007:1)**\n\n| DV | INDV | Causal Link |\n| :--- | :--- | :--- |\n| DEP | BL | BL → DEP |\n| DEP | MS | MS ↔ DEP |\n| MS | BL & DEP | BL&DEP → MS |\n\n---\n\n### Question\n\nBased on the provided evidence for the United States, select **all** statements that are correctly supported by the data and analysis.", "Options": {"A": "The VECM results for US 2 show a statistically significant error-correction mechanism where the money supply adjusts to deviations from its long-run equilibrium with bank loans at a speed of 12.3% per quarter.", "B": "In the US 1 period, the long-run causality ran from Bank Loans to Money Supply (BL → MS), consistent with the Post-Keynesian view.", "C": "The trivariate VAR analysis for US 2 weakens the case for endogeneity by showing that bank deposits are an omitted variable that explains the relationship between bank loans and money supply.", "D": "The findings suggest a structural break in the nature of money supply determination, with the US 1 period (monetary targeting) showing evidence for exogenous money and the US 2 period (interest rate targeting) showing evidence for endogenous money."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize findings across different time periods and econometric models for the US case. It uses an Atomic Decomposition strategy, breaking the original QA's conclusions into distinct, verifiable statements. Correct options (A, B) test the core contrast between policy regimes and the interpretation of a key VECM parameter. Distractors are based on common conceptual opposites (C: misinterpreting the robustness check's implication; D: reversing the causality for the US 1 period).", "qid": "313", "question": "### Background\n\n**Research Question.** This study evaluates the evidence for money endogeneity versus exogeneity in the United States, paying close attention to the effects of a major monetary policy regime shift in the mid-1980s. The analysis seeks to synthesize results from multiple econometric tests to form a robust conclusion.\n\n**Setting / Data-Generating Environment.** The analysis uses quarterly time-series data for the US, split into two sub-periods to account for a structural break: US 1 (1975:3–1986:4), a period characterized by monetary aggregate targeting, and US 2 (1987:1–2007:1), a period characterized by interest rate targeting. The study employs Vector Error-Correction Models (VECM) for cointegrated series and trivariate Vector Autoregressions (VAR) for robustness checks.\n\n**Variables & Parameters.**\n- `BL`: Bank Loans\n- `MS`: Money Supply\n- `DEP`: Bank Deposits\n- `ECT`: Error-Correction Term. Its coefficient indicates the speed of adjustment to long-run equilibrium.\n- `→`: Unidirectional Granger causality.\n- `↔`: Bidirectional Granger causality.\n- `EXO`: Conclusion that money supply is Exogenous.\n- `ENDO`: Conclusion that money supply is Endogenous.\n\n---\n\n### Data / Model Specification\n\nEvidence is drawn from three sources: a summary of primary causality tests, detailed VECM estimates, and a trivariate VAR robustness check.\n\n**Table 1: Summary of Causality Test Results for BL and MS**\n\n| Period | Long Run | Conclusion | Monetary Policy |\n| :--- | :--- | :--- | :--- |\n| US 1 | MS → BL | EXO | Monetary Targeting |\n| US 2 | BL ↔ MS | ENDO | Interest Rate Targeting |\n\n**Table 2: VECM Results for US 2 (1987:1–2007:1)**\n\n| DV | INDV | ECT Coefficient | t-Statistic | Long Run Conclusion |\n| :--- | :--- | :--- | :--- | :--- |\n| BL | MS | -0.170 | [-5.825]*** | MS → BL |\n| MS | BL | -0.123 | [-6.919]*** | BL → MS |\n\n*Note: DV = Dependent Variable, INDV = Independent Variable. *** denotes significance at 1%.*\n\n**Table 3: Trivariate VAR Causality Results for US 2 (1987:1–2007:1)**\n\n| DV | INDV | Causal Link |\n| :--- | :--- | :--- |\n| DEP | BL | BL → DEP |\n| DEP | MS | MS ↔ DEP |\n| MS | BL & DEP | BL&DEP → MS |\n\n---\n\n### Question\n\nBased on the provided evidence for the United States, select **all** statements that are correctly supported by the data and analysis.", "model_evaluation_result": {"model_raw_response": "Looking at each statement carefully:\n\nA) The VECM results for US 2 show that when MS is the dependent variable and BL is the independent variable, the ECT coefficient is -0.123 with a t-statistic of [-6.919]***. This means money supply adjusts to deviations from long-run equilibrium with bank loans at 12.3% per quarter, and it's statistically significant at 1%. This statement is correct.\n\nB) Table 1 clearly shows that for US 1, the long-run causality is MS → BL (money supply causes bank loans), not BL → MS. This statement is incorrect.\n\nC) The trivariate VAR results show that BL&DEP → MS, meaning bank loans and deposits jointly cause money supply. This actually strengthens the endogeneity case by showing that bank lending activities (loans and deposits) drive money supply creation. The statement incorrectly claims this weakens the endogeneity case. This statement is incorrect.\n\nD) Table 1 shows US 1 (monetary targeting period) had MS → BL causality with conclusion EXO (exogenous money), while US 2 (interest rate targeting period) had BL ↔ MS causality with conclusion ENDO (endogenous money). This represents a structural break in money supply determination between the two periods. This statement is correct.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 258, "Question": "### Background\n\n**Research Question.** What is the empirical journey from a raw correlation to a final, causal estimate of the net economic impact of M&A rumors on deal outcomes?\n\n**Setting / Data-Generating Environment.** The analysis progresses through several stages: (1) examining raw descriptive data, (2) estimating a naive model that ignores endogeneity and selection bias, and (3) estimating a full structural model using Indirect Inference (II) to correct for these biases.\n\n**Variables & Parameters.**\n- `Closing`: Binary variable; 1 if a deal is completed, 0 otherwise.\n- `Ln(Price)`: The natural logarithm of the deal's transaction multiple (e.g., EV/Sales).\n- `Leak`: Binary variable; 1 if a deal was subject to a pre-announcement rumor, 0 otherwise.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Tabulation of Deal Closings and Rumors**\n\n| | Not Closed | Closed | Totals |\n| :--- | :--- | :--- | :--- |\n| **Not Leaked** | 11,977 | 38,627 | 50,604 |\n| **Leaked** | 11,377 | 6,063 | 17,440 |\n| **Totals** | 23,354 | 44,690 | 68,044 |\n\n**Table 2: Naive Probit Model for Deal Completion, Pr(Closing = 1)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.904*** |\n| | (0.019) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 7, Specification (3). `***` denotes p < 0.01.*\n\n**Table 3: Naive OLS Model for Deal Value, Ln(Price)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.066*** |\n| | (0.005) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 8, estimated on observed prices only. `***` denotes p < 0.01.*\n\n---\n\n### Question\n\nBased on the provided raw data and naive model estimates, select all of the following statements that are correct.", "Options": {"A": "The naive OLS model in Table 3 suggests that, for deals with observed prices, a rumor is associated with a 6.6% lower transaction value.", "B": "The naive probit model in Table 2 implies that a rumor causes a 90.4 percentage point decrease in the probability of deal completion.", "C": "The naive models (Table 2 and Table 3) collectively suggest that rumors increase deal value but decrease the probability of closing.", "D": "According to the raw data in Table 1, rumored deals are approximately 41.5 percentage points less likely to close than non-rumored deals."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret both raw descriptive data and naive regression outputs, which is the first step in the paper's empirical narrative. It uses an 'Atomic Decomposition' strategy, breaking down the initial findings into distinct, verifiable statements. Distractor C tests for the common error of misinterpreting a probit coefficient as a marginal effect (Formula Misuse). Distractor D presents a 'Conceptual Opposite' of the naive price finding to test careful reading of the tables.", "qid": "258", "question": "### Background\n\n**Research Question.** What is the empirical journey from a raw correlation to a final, causal estimate of the net economic impact of M&A rumors on deal outcomes?\n\n**Setting / Data-Generating Environment.** The analysis progresses through several stages: (1) examining raw descriptive data, (2) estimating a naive model that ignores endogeneity and selection bias, and (3) estimating a full structural model using Indirect Inference (II) to correct for these biases.\n\n**Variables & Parameters.**\n- `Closing`: Binary variable; 1 if a deal is completed, 0 otherwise.\n- `Ln(Price)`: The natural logarithm of the deal's transaction multiple (e.g., EV/Sales).\n- `Leak`: Binary variable; 1 if a deal was subject to a pre-announcement rumor, 0 otherwise.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Tabulation of Deal Closings and Rumors**\n\n| | Not Closed | Closed | Totals |\n| :--- | :--- | :--- | :--- |\n| **Not Leaked** | 11,977 | 38,627 | 50,604 |\n| **Leaked** | 11,377 | 6,063 | 17,440 |\n| **Totals** | 23,354 | 44,690 | 68,044 |\n\n**Table 2: Naive Probit Model for Deal Completion, Pr(Closing = 1)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.904*** |\n| | (0.019) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 7, Specification (3). `***` denotes p < 0.01.*\n\n**Table 3: Naive OLS Model for Deal Value, Ln(Price)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.066*** |\n| | (0.005) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 8, estimated on observed prices only. `***` denotes p < 0.01.*\n\n---\n\n### Question\n\nBased on the provided raw data and naive model estimates, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The naive OLS model in Table 3 shows a coefficient of -0.066 for Leak. Since the dependent variable is Ln(Price), this coefficient represents the log difference. A coefficient of -0.066 means approximately a 6.6% lower transaction value for leaked deals. This statement is correct.\n\nB) The naive probit model in Table 2 shows a coefficient of -0.904 for Leak. In a probit model, coefficients are not directly interpretable as percentage point changes in probability. The -0.904 is the effect on the z-score (latent variable), not a 90.4 percentage point decrease in probability. This statement is incorrect.\n\nC) Looking at both models:\n- Table 2 (probit): negative coefficient (-0.904) suggests rumors decrease probability of closing\n- Table 3 (OLS): negative coefficient (-0.066) suggests rumors decrease deal value\nBoth effects are negative, so rumors do not increase deal value. This statement is incorrect.\n\nD) From Table 1:\n- Not Leaked closing rate: 38,627/50,604 = 76.3%\n- Leaked closing rate: 6,063/17,440 = 34.8%\n- Difference: 34.8% - 76.3% = -41.5 percentage points\nThis statement is correct.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 112, "Question": "### Background\n\n**Research Question.** This case examines how the paper's long-run risk model is applied to price the aggregate stock market.\n\n**Setting.** In a representative agent economy with Epstein-Zin preferences, the aggregate stock market is a claim to a dividend stream. The stock price is determined by the single, persistent state variable `x_t`, which represents the expected growth rate of durable consumption.\n\n**Variables and Parameters.**\n- `z_t = log(P_t/Y_t)`: Log price-dividend ratio.\n- `x_t`: Long-run risk state variable, following `x_{t+1} = φx_t + σ_x ω_t ε_{x,t+1}`.\n- `A_1`: Sensitivity of the log stock price-dividend ratio to `x_t`.\n- `r_{t+1}`: Log stock return.\n- `m_{t+1}`: Log SDF.\n- `Δy_{t+1}`: Log dividend growth.\n- `λ`: Exposure of dividend growth to `x_t`.\n- `ψ`, `α`, `φ`, `ζ`, `κ_1`: Model parameters.\n\n---\n\n### Data / Model Specification\n\nThe log price-dividend ratio is assumed to be linear in the state variable:\n  \nz_t \\approx A_0 + A_1 x_t \\quad \\text{(Eq. (1))}\n \nAsset prices are determined by the log-linearized Euler equation `0 ≈ E_t[m_{t+1} + r_{asset,t+1}]`. The key term from the expected log SDF is `E_t[m_{t+1}] = const + (1 - 1/ψ)α x_t`. The expected log dividend growth is `E_t[Δy_{t+1}] = μ_y + λx_t`.\n\nThe equity risk premium is given by:\n  \nE_t[r_{t+1} - r_{f,t}] + \\text{Jensen's term} \\approx \\beta_x \\pi_x \\sigma_x^2 \\omega_t^2 = \\beta_x \\pi_x \\sigma_x^2 (1 - \\zeta x_t) \\quad \\text{(Eq. (2))}\n \nwhere `β_x = κ_1 A_1` is the stock's exposure to long-run risk and `π_x` is the market price of that risk.\n\n---\n\n### Question\n\nBased on the model specification, select ALL statements that are correct regarding the pricing of the aggregate stock market.", "Options": {"A": "The sensitivity of the log price-dividend ratio to the long-run risk state variable, `A_1`, is correctly given by `A_1 = (λ + (1 - 1/ψ)α) / (1 - κ_1 φ)`.", "B": "The equity risk premium is pro-cyclical because in good times (high `x_t`), the conditional volatility `ω_t` is high, increasing the compensation required for bearing risk.", "C": "The sensitivity of the log price-dividend ratio to the long-run risk state variable, `A_1`, is correctly given by `A_1 = λ / (1 - κ_1 φ)`.", "D": "The stock's exposure to long-run risk, `β_x = κ_1 A_1`, is large primarily because the high persistence of `x_t` (`φ` ≈ 1) makes the denominator `(1 - κ_1 φ)` very small, amplifying the sensitivity of the stock price to growth news."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item uses the Atomic Decomposition strategy to test both the derivation of the stock valuation sensitivity (`A_1`) and the economic interpretation of the resulting risk premium. Option A tests the precise formula. Option B tests the key insight about persistence amplification. Distractor C represents a common Step-Omission Error of ignoring the wealth channel's effect on the SDF. Distractor D presents a Conceptual Opposite, incorrectly linking high `x_t` to high volatility and a pro-cyclical premium.", "qid": "112", "question": "### Background\n\n**Research Question.** This case examines how the paper's long-run risk model is applied to price the aggregate stock market.\n\n**Setting.** In a representative agent economy with Epstein-Zin preferences, the aggregate stock market is a claim to a dividend stream. The stock price is determined by the single, persistent state variable `x_t`, which represents the expected growth rate of durable consumption.\n\n**Variables and Parameters.**\n- `z_t = log(P_t/Y_t)`: Log price-dividend ratio.\n- `x_t`: Long-run risk state variable, following `x_{t+1} = φx_t + σ_x ω_t ε_{x,t+1}`.\n- `A_1`: Sensitivity of the log stock price-dividend ratio to `x_t`.\n- `r_{t+1}`: Log stock return.\n- `m_{t+1}`: Log SDF.\n- `Δy_{t+1}`: Log dividend growth.\n- `λ`: Exposure of dividend growth to `x_t`.\n- `ψ`, `α`, `φ`, `ζ`, `κ_1`: Model parameters.\n\n---\n\n### Data / Model Specification\n\nThe log price-dividend ratio is assumed to be linear in the state variable:\n  \nz_t \\approx A_0 + A_1 x_t \\quad \\text{(Eq. (1))}\n \nAsset prices are determined by the log-linearized Euler equation `0 ≈ E_t[m_{t+1} + r_{asset,t+1}]`. The key term from the expected log SDF is `E_t[m_{t+1}] = const + (1 - 1/ψ)α x_t`. The expected log dividend growth is `E_t[Δy_{t+1}] = μ_y + λx_t`.\n\nThe equity risk premium is given by:\n  \nE_t[r_{t+1} - r_{f,t}] + \\text{Jensen's term} \\approx \\beta_x \\pi_x \\sigma_x^2 \\omega_t^2 = \\beta_x \\pi_x \\sigma_x^2 (1 - \\zeta x_t) \\quad \\text{(Eq. (2))}\n \nwhere `β_x = κ_1 A_1` is the stock's exposure to long-run risk and `π_x` is the market price of that risk.\n\n---\n\n### Question\n\nBased on the model specification, select ALL statements that are correct regarding the pricing of the aggregate stock market.", "model_evaluation_result": {"model_raw_response": "Looking at this long-run risk model for pricing the aggregate stock market, I need to analyze each statement carefully.\n\n**For Statement A**: The sensitivity coefficient A₁ comes from solving the Euler equation. Given that:\n- Expected log dividend growth: E_t[Δy_{t+1}] = μ_y + λx_t\n- Expected log SDF contains the term: (1 - 1/ψ)α x_t\n- Using the log-linearized return equation and Euler condition\n\nThe correct formula for A₁ should be: A₁ = λ / (1 - κ₁φ)\n\nThis represents how dividend growth exposure (λ) is amplified by the persistence of the state variable. Statement A includes an additional term (1 - 1/ψ)α in the numerator, which is incorrect for the price-dividend ratio sensitivity.\n\n**For Statement B**: The equity risk premium equation shows:\nE_t[r_{t+1} - r_{f,t}] ≈ β_x π_x σ_x² ω_t² = β_x π_x σ_x² (1 - ζx_t)\n\nSince ω_t² = (1 - ζx_t), when x_t is high (good times), ω_t² is actually LOW (assuming ζ > 0). This means the equity risk premium is counter-cyclical, not pro-cyclical. Statement B incorrectly states the relationship.\n\n**For Statement C**: This correctly identifies A₁ = λ / (1 - κ₁φ), which is the proper solution from the Euler equation for the log price-dividend ratio's sensitivity to the long-run risk state variable.\n\n**For Statement D**: With φ ≈ 1 (high persistence) and κ₁ ≈ 1, the denominator (1 - κ₁φ) becomes very small, creating a large amplification effect. This makes β_x = κ₁A₁ large, explaining why stocks have high exposure to long-run risk. This statement correctly identifies the economic mechanism.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 335, "Question": "### Background\n\n**Research Question.** A study establishes a causal link suggesting that higher local air pollution leads to less corporate tax avoidance (i.e., a higher effective tax rate, ETR). This question explores the economic significance of this finding, the mechanisms driving it, and how the effect varies across different types of firms.\n\n**Setting / Data-Generating Environment.** The analysis uses a panel of Chinese A-share firms. The main effect is estimated via a fixed-effects model. The mechanisms and heterogeneous effects are explored using interaction models.\n\n**Variables & Parameters.**\n- `ETR_it`: Cash effective tax rate for firm `i` in year `t`. A higher `ETR` indicates less tax avoidance.\n- `AQI_it`: City-level Air Quality Index, scaled by 1000. Higher `AQI` means worse pollution.\n- `PC_it`: An indicator variable = 1 if the firm has politically connected executives, 0 otherwise.\n- `DUAL_it`: An indicator variable = 1 if the CEO is also the board chair (proxy for managerial entrenchment).\n- `SOE_it`: An indicator variable = 1 if the firm is a state-owned enterprise.\n\n### Data / Model Specification\n\nThe baseline model is:\n  \nETR_{it} = \\beta_{0} + \\beta_{1} \\mathrm{AQI}_{it} + \\text{Controls}_{it} + \\mu_{i} + \\eta_{t} + \\varepsilon_{it}\n \n\nThe interaction model is:\n  \nETR_{it} = \\alpha_{0} + \\alpha_{1} (AQI_{it} \\times Moderator_{it}) + \\alpha_{2} AQI_{it} + \\alpha_{3} Moderator_{it} + ... + \\nu_{it}\n \n\n**Table 1: Summary Statistics**\n\n| Variable | Mean  | Std. Dev. |\n| :---     | :---- | :---      |\n| ETR      | 0.207 | 0.126     |\n| AQI      | 0.082 | 0.023     |\n\n**Table 2: Baseline Regression Result**\n\n| Variable | Coefficient (β₁) |\n| :---     | :---        |\n| AQI      | 0.084**     |\n|          | (0.038)     |\n\n**Table 3: Channel Test (Governmental Pressure)**\n\n| Variables | Coefficient (ETR) |\n| :---      | :---: |\n| AQI       | 0.024** |\n|           | (0.008) |\n| AQI*PC    | -0.036*** |\n|           | (0.011) |\n\n**Table 4: Heterogeneous Effects Tests**\n\n| Variables | (Col 2) CEO Duality | (Col 3) State-Owned |\n| :---      | :---: | :---: |\n| AQI       | 0.133*** | 0.117** |\n|           | (0.044) | (0.057) |\n| AQI*DUAL  | -0.108*** | | \n|           | (0.009) | | \n| AQI*SOE   | | 0.357*** |\n|           | | (0.106) |\n\n*Note: Standard errors in parentheses. **, *** denote significance at 5% and 1% levels.*\n\n### The Question\n\nBased on the provided data and regression results, which of the following statements are quantitatively supported? (Select all that apply)", "Options": {"A": "A one-standard-deviation increase in AQI is associated with an increase in ETR of approximately 0.19 percentage points.", "B": "For firms with political connections (PC=1), the marginal effect of AQI on ETR is negative (-0.012).", "C": "For firms with CEO duality (DUAL=1), the marginal effect of AQI on ETR is 0.133.", "D": "For state-owned enterprises (SOE=1), the positive effect of AQI on ETR is weaker than for non-SOEs."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses an 'atomic decomposition' strategy to test multiple quantitative conclusions from the paper. The assessment target is the ability to perform multi-step calculations (economic magnitude, marginal effects with interactions) and interpret the results correctly. This is a Computational Judgment task. Distractor C is a conceptual error (misinterpreting the sign of an interaction term, which indicates amplification, not weakening). Distractor D is a step-omission error (failing to include the interaction term in the marginal effect calculation, reporting only the baseline coefficient).", "qid": "335", "question": "### Background\n\n**Research Question.** A study establishes a causal link suggesting that higher local air pollution leads to less corporate tax avoidance (i.e., a higher effective tax rate, ETR). This question explores the economic significance of this finding, the mechanisms driving it, and how the effect varies across different types of firms.\n\n**Setting / Data-Generating Environment.** The analysis uses a panel of Chinese A-share firms. The main effect is estimated via a fixed-effects model. The mechanisms and heterogeneous effects are explored using interaction models.\n\n**Variables & Parameters.**\n- `ETR_it`: Cash effective tax rate for firm `i` in year `t`. A higher `ETR` indicates less tax avoidance.\n- `AQI_it`: City-level Air Quality Index, scaled by 1000. Higher `AQI` means worse pollution.\n- `PC_it`: An indicator variable = 1 if the firm has politically connected executives, 0 otherwise.\n- `DUAL_it`: An indicator variable = 1 if the CEO is also the board chair (proxy for managerial entrenchment).\n- `SOE_it`: An indicator variable = 1 if the firm is a state-owned enterprise.\n\n### Data / Model Specification\n\nThe baseline model is:\n  \nETR_{it} = \\beta_{0} + \\beta_{1} \\mathrm{AQI}_{it} + \\text{Controls}_{it} + \\mu_{i} + \\eta_{t} + \\varepsilon_{it}\n \n\nThe interaction model is:\n  \nETR_{it} = \\alpha_{0} + \\alpha_{1} (AQI_{it} \\times Moderator_{it}) + \\alpha_{2} AQI_{it} + \\alpha_{3} Moderator_{it} + ... + \\nu_{it}\n \n\n**Table 1: Summary Statistics**\n\n| Variable | Mean  | Std. Dev. |\n| :---     | :---- | :---      |\n| ETR      | 0.207 | 0.126     |\n| AQI      | 0.082 | 0.023     |\n\n**Table 2: Baseline Regression Result**\n\n| Variable | Coefficient (β₁) |\n| :---     | :---        |\n| AQI      | 0.084**     |\n|          | (0.038)     |\n\n**Table 3: Channel Test (Governmental Pressure)**\n\n| Variables | Coefficient (ETR) |\n| :---      | :---: |\n| AQI       | 0.024** |\n|           | (0.008) |\n| AQI*PC    | -0.036*** |\n|           | (0.011) |\n\n**Table 4: Heterogeneous Effects Tests**\n\n| Variables | (Col 2) CEO Duality | (Col 3) State-Owned |\n| :---      | :---: | :---: |\n| AQI       | 0.133*** | 0.117** |\n|           | (0.044) | (0.057) |\n| AQI*DUAL  | -0.108*** | | \n|           | (0.009) | | \n| AQI*SOE   | | 0.357*** |\n|           | | (0.106) |\n\n*Note: Standard errors in parentheses. **, *** denote significance at 5% and 1% levels.*\n\n### The Question\n\nBased on the provided data and regression results, which of the following statements are quantitatively supported? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at each option systematically:\n\n**Option A:** A one-standard-deviation increase in AQI is associated with an increase in ETR of approximately 0.19 percentage points.\n- From Table 1: AQI std. dev. = 0.023\n- From Table 2: β₁ = 0.084\n- Effect = 0.084 × 0.023 = 0.001932 = 0.19 percentage points ✓\n\n**Option B:** For firms with political connections (PC=1), the marginal effect of AQI on ETR is negative (-0.012).\n- From Table 3: Base effect of AQI = 0.024\n- Interaction effect AQI*PC = -0.036\n- Marginal effect when PC=1: 0.024 + (-0.036) = -0.012 ✓\n\n**Option C:** For firms with CEO duality (DUAL=1), the marginal effect of AQI on ETR is 0.133.\n- From Table 4 Col 2: Base effect of AQI = 0.133\n- Interaction effect AQI*DUAL = -0.108\n- Marginal effect when DUAL=1: 0.133 + (-0.108) = 0.025 ≠ 0.133 ✗\n\n**Option D:** For state-owned enterprises (SOE=1), the positive effect of AQI on ETR is weaker than for non-SOEs.\n- From Table 4 Col 3: Base effect of AQI (for non-SOEs) = 0.117\n- Interaction effect AQI*SOE = 0.357 (positive)\n- Marginal effect for SOEs: 0.117 + 0.357 = 0.474\n- Since 0.474 > 0.117, the effect is stronger for SOEs, not weaker ✗\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 267, "Question": "### Background\n\nThe analysis interprets coefficients from a regression of mutual fund alpha on flow risk, a dummy for derivative use, and the interaction between the two to test the \"flow management hypothesis.\"\n\n### Data / Model Specification\n\nThe estimated interaction model is:\n  \n\\alpha_{i}=\\beta_{0}+\\dots+\\beta_{3}flow\\_risk_{i}+\\beta_{4}flow\\_mgmt_{i}+\\beta_{5}derivatives_{i} +\\dots+\\varepsilon_{i}\n \nwhere `flow_mgmt_i` is the interaction term `derivatives_i × flow_risk_i`. This model is estimated using a two-stage least squares (2SLS) approach.\n\n**Table 1: Key Coefficient Estimates (2SLS, from Table 7, Col. 6)**\n\n| Variable | Coefficient (`\\hat{\\beta}`) |\n| :--- | :--- |\n| `flow_risk` | -0.0871 |\n| `flow_mgmt` | 0.0529 |\n| `derivatives` | -0.0011 |\n\n---\n\nBased on the model and data in **Table 1**, select all of the following statements that are correct.\n", "Options": {"A": "The total marginal effect of a one-unit increase in `flow_risk` on monthly alpha for a fund that uses derivatives is -0.0342.", "B": "The results suggest that using derivatives mitigates approximately 61% of the negative performance impact from flow risk.", "C": "For a fund that does not use derivatives, the marginal effect of a one-unit increase in `flow_risk` on monthly alpha is -0.0342.", "D": "The negative coefficient on the `derivatives` variable implies that, after accounting for flow management benefits, the net effect of using derivatives is a performance drag, possibly due to implementation costs."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item tests the ability to correctly interpret an interaction model, calculate marginal effects for different groups, and synthesize these calculations into a quantitative and conceptual understanding of the paper's central finding (the flow management hypothesis). It uses a Computational Judgment strategy.\n\nDistractor Logic:\n- A (Correct): Requires calculating the total marginal effect for a user fund: `β_flow_risk + β_flow_mgmt = -0.0871 + 0.0529 = -0.0342`.\n- B (Correct): Requires calculating the percentage mitigation: `|β_flow_mgmt / β_flow_risk| = |0.0529 / -0.0871| ≈ 0.607` or 60.7%.\n- C (Correct): Requires correct interpretation of the main effect (`derivatives`) in an interaction model, which represents the effect when the interacting variable (`flow_risk`) is zero.\n- D (Incorrect): This is a Step-Omission Error. It incorrectly applies the user fund's marginal effect to the non-user fund. The correct marginal effect for a non-user is simply the coefficient on `flow_risk`, which is -0.0871.", "qid": "267", "question": "### Background\n\nThe analysis interprets coefficients from a regression of mutual fund alpha on flow risk, a dummy for derivative use, and the interaction between the two to test the \"flow management hypothesis.\"\n\n### Data / Model Specification\n\nThe estimated interaction model is:\n  \n\\alpha_{i}=\\beta_{0}+\\dots+\\beta_{3}flow\\_risk_{i}+\\beta_{4}flow\\_mgmt_{i}+\\beta_{5}derivatives_{i} +\\dots+\\varepsilon_{i}\n \nwhere `flow_mgmt_i` is the interaction term `derivatives_i × flow_risk_i`. This model is estimated using a two-stage least squares (2SLS) approach.\n\n**Table 1: Key Coefficient Estimates (2SLS, from Table 7, Col. 6)**\n\n| Variable | Coefficient (`\\hat{\\beta}`) |\n| :--- | :--- |\n| `flow_risk` | -0.0871 |\n| `flow_mgmt` | 0.0529 |\n| `derivatives` | -0.0011 |\n\n---\n\nBased on the model and data in **Table 1**, select all of the following statements that are correct.\n", "model_evaluation_result": {"model_raw_response": "I need to analyze the regression model and coefficients to determine which statements are correct.\n\nGiven model:\nα_i = β_0 + ... + β_3(flow_risk_i) + β_4(flow_mgmt_i) + β_5(derivatives_i) + ... + ε_i\n\nWhere flow_mgmt_i = derivatives_i × flow_risk_i\n\nCoefficients:\n- β_3 (flow_risk) = -0.0871\n- β_4 (flow_mgmt) = 0.0529\n- β_5 (derivatives) = -0.0011\n\nLet me evaluate each statement:\n\n**Statement A**: The total marginal effect of flow_risk for a fund that uses derivatives (derivatives = 1):\n∂α/∂flow_risk = β_3 + β_4 × derivatives = -0.0871 + 0.0529 × 1 = -0.0342\nThis is correct.\n\n**Statement B**: Mitigation calculation:\n- Effect without derivatives: -0.0871\n- Effect with derivatives: -0.0342\n- Mitigation = (0.0871 - 0.0342) / 0.0871 = 0.0529 / 0.0871 ≈ 0.607 or about 61%\nThis is correct.\n\n**Statement C**: For a fund that does NOT use derivatives (derivatives = 0):\n∂α/∂flow_risk = β_3 + β_4 × derivatives = -0.0871 + 0.0529 × 0 = -0.0871\nThe statement says -0.0342, which is incorrect.\n\n**Statement D**: The coefficient on derivatives is -0.0011, which is negative. This represents the direct effect of using derivatives on alpha, holding flow_risk constant. The negative sign suggests that, apart from the flow management benefit (captured by the interaction term), derivatives have a negative impact on performance, which could be due to implementation costs.\nThis is correct.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 323, "Question": "### Background\n\n**Research Question.** This case explores the formal modeling of dynamic relationships between cointegrated time series, building to the Vector Error Correction (VEC) model.\n\n**Setting.** Consider a bivariate system of two log stock price indices, `P_t = [X_t, Y_t]'`, which are I(1) and share a single cointegrating relationship.\n\n---\n\n### Data / Model Specification\n\nA Vector Autoregression (VAR) of I(1) variables can be written in the Vector Error Correction (VEC) form:\n\n  \n\\Delta P_t = \\Pi P_{t-1} + \\sum_{i=1}^{k-1} \\Gamma_i \\Delta P_{t-i} + \\epsilon_t\n\\quad \\text{(Eq. (1))}\n \n\nThe Granger Representation Theorem states that the long-run impact matrix `Π` can be factored as `Π = αβ'`, where `α` contains adjustment coefficients and `β` is the cointegrating vector.\n\n---\n\n### Question\n\nSelect all statements that are TRUE regarding the VEC model in **Eq. (1)** and its relationship to standard Granger causality tests for cointegrated I(1) series.", "Options": {"A": "The `α` vector defines the long-run equilibrium relationship, while the `β` vector measures the speed of adjustment to equilibrium.", "B": "A non-zero element in the `α` vector for a given variable is evidence of long-run causality, as it shows the variable adjusts to past equilibrium errors.", "C": "The `β` vector defines the long-run equilibrium relationship between the levels of the series in `P_t`.", "D": "A standard Granger causality test on the levels of the series in `P_t` is misspecified because it omits the error correction term, `αβ'P_{t-1}`."}, "Answer": ["B", "C", "D"], "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.5). This item assesses the conceptual understanding of the VEC model's components and its superiority over simpler models for cointegrated series. Options A and B are correct definitions of `β` and `α`. Option D correctly identifies the misspecification of a standard Granger causality test in levels, a key point in the original QA. Distractor C represents the most common conceptual error: swapping the roles of `α` and `β`.", "qid": "323", "question": "### Background\n\n**Research Question.** This case explores the formal modeling of dynamic relationships between cointegrated time series, building to the Vector Error Correction (VEC) model.\n\n**Setting.** Consider a bivariate system of two log stock price indices, `P_t = [X_t, Y_t]'`, which are I(1) and share a single cointegrating relationship.\n\n---\n\n### Data / Model Specification\n\nA Vector Autoregression (VAR) of I(1) variables can be written in the Vector Error Correction (VEC) form:\n\n  \n\\Delta P_t = \\Pi P_{t-1} + \\sum_{i=1}^{k-1} \\Gamma_i \\Delta P_{t-i} + \\epsilon_t\n\\quad \\text{(Eq. (1))}\n \n\nThe Granger Representation Theorem states that the long-run impact matrix `Π` can be factored as `Π = αβ'`, where `α` contains adjustment coefficients and `β` is the cointegrating vector.\n\n---\n\n### Question\n\nSelect all statements that are TRUE regarding the VEC model in **Eq. (1)** and its relationship to standard Granger causality tests for cointegrated I(1) series.", "model_evaluation_result": {"model_raw_response": "Looking at each statement about the VEC model:\n\nA) States that α defines the long-run equilibrium relationship and β measures speed of adjustment. This is **incorrect** - it has the roles reversed. The β vector defines the cointegrating (long-run equilibrium) relationship, while α contains the adjustment coefficients.\n\nB) States that a non-zero element in α indicates long-run causality because it shows adjustment to equilibrium errors. This is **correct**. The α vector contains the error correction coefficients that show how each variable responds to deviations from the long-run equilibrium.\n\nC) States that β defines the long-run equilibrium relationship between the levels of the series. This is **correct**. The cointegrating vector β defines the linear combination of the I(1) variables that is stationary, representing the long-run equilibrium relationship.\n\nD) States that standard Granger causality tests on levels are misspecified because they omit the error correction term αβ'P_{t-1}. This is **correct**. When series are cointegrated, the error correction term must be included in the model. Omitting it leads to misspecification and invalid inference.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 334, "Question": "### Background\n\n**Research Question.** A central puzzle in international finance is that different stationary regressions testing the forward exchange rate unbiasedness hypothesis yield conflicting results. The standard forward premium regression typically rejects unbiasedness, while a seemingly minor modification appears to support it. Can this conflict be explained by differences in the magnitude of simultaneity bias across specifications?\n\n**Setting.** An econometric analysis of two stationary models. The first is the standard forward premium regression of the spot rate change `S_t - S_{t-1}` on the forward premium `F_{t-1} - S_{t-1}`. The second is a modified version, regressing `S_t - S_{t-2}` on `F_{t-1} - S_{t-2}`. Under the null hypothesis of unbiasedness, the true slope coefficient `\\beta` is 1 in both models.\n\n**Variables & Parameters.**\n- `S_t`, `S_{t-1}`, `S_{t-2}`: Log spot exchange rates.\n- `F_{t-1}`: Log forward exchange rate.\n- `\\eta_t`: Innovation to the spot rate process, `\\eta_t = S_t - S_{t-1}`.\n- `e_{t-1}`: Innovation to the forward rate process, `e_{t-1} = F_{t-1} - S_{t-1}`.\n- `\\beta`: The slope coefficient in the regression.\n\n---\n\n### Data / Model Specification\n\nThe analysis hinges on the statistical properties of the spot and forward rate innovations, particularly the \"relative error variance characteristic\"—the empirical fact that the variance of the spot rate innovation is vastly larger than that of the forward rate innovation.\n\n**Table 1. Summary Statistics of DGP Residuals (UK)**\n| Statistic | Spot Rate Innovation (`\\eta_t`) | Forward Rate Innovation (`e_{t-1}`) |\n| :--- | :--- | :--- |\n| Variance | 0.0010692 | 0.0000078 |\n| Covariance `cov(e_{t-1}, \\eta_t)` | | -0.000013 |\n\n**Asymptotic Bias of OLS Estimator `\\hat{\\beta}`:**\n- **Standard Regression (`S_t - S_{t-1}` on `F_{t-1} - S_{t-1}`):**\n      \n    \\text{Bias} = \\frac{\\mathrm{cov}(e_{t-1}, \\eta_t)}{\\mathrm{var}(e_{t-1})} - 1 \\quad \\text{(Eq. (1))}\n     \n- **Modified Regression (`S_t - S_{t-2}` on `F_{t-1} - S_{t-2}`):**\n      \n    \\text{Bias} = \\frac{\\mathrm{cov}(e_{t-1}, \\eta_t) - \\mathrm{var}(e_{t-1})}{\\mathrm{var}(\\eta_{t-1}) + \\mathrm{var}(e_{t-1})} \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nUsing the provided data for the UK and the asymptotic bias formulas (where `plim(\\hat{\\beta}) = 1 + \\text{Bias}`), which of the following statements are correct? Select all that apply.", "Options": {"A": "Both regression specifications suffer from equally severe simultaneity bias, as the numerators of their respective bias formulas are nearly identical.", "B": "The asymptotic bias in the standard forward premium regression (Eq. (1)) is approximately -1.67.", "C": "The modified regression (Eq. (2)) exhibits negligible bias because the large spot rate innovation variance, `var(\\eta_{t-1})`, inflates the denominator of the bias formula, mitigating the endogeneity problem.", "D": "The probability limit of the slope coefficient in the standard forward premium regression (Eq. (1)) is approximately -1.67, indicating a bias strong enough to flip the coefficient's sign."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to calculate and compare asymptotic biases for two different stationary regression models. It employs a Computational Judgment strategy with atomic decomposition. Correct options A and B require accurate calculation for the standard model and a correct conceptual explanation for the modified model's result. Distractor C targets a common step-omission error (confusing the probability limit with the bias term, which is -2.67). Distractor D is a 'True but Irrelevant' statement that leads to a false conclusion by ignoring the crucial role of the denominator.", "qid": "334", "question": "### Background\n\n**Research Question.** A central puzzle in international finance is that different stationary regressions testing the forward exchange rate unbiasedness hypothesis yield conflicting results. The standard forward premium regression typically rejects unbiasedness, while a seemingly minor modification appears to support it. Can this conflict be explained by differences in the magnitude of simultaneity bias across specifications?\n\n**Setting.** An econometric analysis of two stationary models. The first is the standard forward premium regression of the spot rate change `S_t - S_{t-1}` on the forward premium `F_{t-1} - S_{t-1}`. The second is a modified version, regressing `S_t - S_{t-2}` on `F_{t-1} - S_{t-2}`. Under the null hypothesis of unbiasedness, the true slope coefficient `\\beta` is 1 in both models.\n\n**Variables & Parameters.**\n- `S_t`, `S_{t-1}`, `S_{t-2}`: Log spot exchange rates.\n- `F_{t-1}`: Log forward exchange rate.\n- `\\eta_t`: Innovation to the spot rate process, `\\eta_t = S_t - S_{t-1}`.\n- `e_{t-1}`: Innovation to the forward rate process, `e_{t-1} = F_{t-1} - S_{t-1}`.\n- `\\beta`: The slope coefficient in the regression.\n\n---\n\n### Data / Model Specification\n\nThe analysis hinges on the statistical properties of the spot and forward rate innovations, particularly the \"relative error variance characteristic\"—the empirical fact that the variance of the spot rate innovation is vastly larger than that of the forward rate innovation.\n\n**Table 1. Summary Statistics of DGP Residuals (UK)**\n| Statistic | Spot Rate Innovation (`\\eta_t`) | Forward Rate Innovation (`e_{t-1}`) |\n| :--- | :--- | :--- |\n| Variance | 0.0010692 | 0.0000078 |\n| Covariance `cov(e_{t-1}, \\eta_t)` | | -0.000013 |\n\n**Asymptotic Bias of OLS Estimator `\\hat{\\beta}`:**\n- **Standard Regression (`S_t - S_{t-1}` on `F_{t-1} - S_{t-1}`):**\n      \n    \\text{Bias} = \\frac{\\mathrm{cov}(e_{t-1}, \\eta_t)}{\\mathrm{var}(e_{t-1})} - 1 \\quad \\text{(Eq. (1))}\n     \n- **Modified Regression (`S_t - S_{t-2}` on `F_{t-1} - S_{t-2}`):**\n      \n    \\text{Bias} = \\frac{\\mathrm{cov}(e_{t-1}, \\eta_t) - \\mathrm{var}(e_{t-1})}{\\mathrm{var}(\\eta_{t-1}) + \\mathrm{var}(e_{t-1})} \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nUsing the provided data for the UK and the asymptotic bias formulas (where `plim(\\hat{\\beta}) = 1 + \\text{Bias}`), which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "I need to calculate the asymptotic bias for both regression specifications using the provided data.\n\nGiven data:\n- var(η_t) = 0.0010692\n- var(e_{t-1}) = 0.0000078\n- cov(e_{t-1}, η_t) = -0.000013\n\nFor the standard regression (Eq. 1):\nBias = cov(e_{t-1}, η_t) / var(e_{t-1}) - 1\nBias = (-0.000013) / (0.0000078) - 1\nBias = -1.667 - 1 = -2.667\n\nSo plim(β̂) = 1 + Bias = 1 + (-2.667) = -1.667\n\nFor the modified regression (Eq. 2):\nBias = [cov(e_{t-1}, η_t) - var(e_{t-1})] / [var(η_{t-1}) + var(e_{t-1})]\n\nSince η_{t-1} = S_{t-1} - S_{t-2}, we have var(η_{t-1}) = var(η_t) = 0.0010692\n\nBias = [(-0.000013) - 0.0000078] / [0.0010692 + 0.0000078]\nBias = [-0.0000208] / [0.001077]\nBias = -0.0193\n\nSo plim(β̂) = 1 + Bias = 1 + (-0.0193) = 0.9807\n\nNow evaluating each statement:\n\nA) False. The biases are very different: -2.667 vs -0.0193. While the numerators are similar in absolute terms, the denominators are vastly different.\n\nB) False. The asymptotic bias is -2.667, not -1.67.\n\nC) True. The modified regression has negligible bias (-0.0193) because var(η_{t-1}) = 0.0010692 is much larger than var(e_{t-1}) = 0.0000078", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 265, "Question": "### Background\n\n**Research Question.** This study seeks to determine if international investment restrictions are binding by examining their effect on closed-end country fund premiums. The simple hypothesis is that countries with more severe restrictions should exhibit higher premiums.\n\n**Setting / Data-Generating Environment.** The analysis uses weekly data for a sample of closed-end country funds from 1981 to 1989. The paper notes that a simple cross-sectional comparison of premium *levels* is problematic due to confounding factors, motivating a shift to an event study of premium *changes*.\n\n---\n\n### Data / Model Specification\n\n**Table 1** below presents descriptive statistics for the average percentage premiums of selected country funds.\n\n**Table 1: Average Percentage Premiums for Selected Closed-End Country Funds**\n\n| Fund (Country) | Mean Premium (%) |\n| :--- | :---: |\n| **High Restriction Markets** | |\n| Korea | 44.35 |\n| Taiwan | 40.96 |\n| Mexico | -7.46 |\n| Brazil | -28.82 |\n| **Low Restriction Markets** | |\n| U.K. | -21.37 |\n| Germany | -4.32 |\n\n*Source: Adapted from Table II, Panel A of the source paper.*\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the paper's methodological reasoning, select all of the following statements that are accurate.", "Options": {"A": "The large premium for the Korea Fund (+44.35%) is definitive proof that its investment restrictions were the most binding among all countries in the sample.", "B": "The data reveal a non-monotonic relationship between presumed restriction severity and premium levels, as high-restriction countries like Brazil and Mexico show large average discounts.", "C": "The average discount on the U.K. Fund (-21.37%) is primarily explained by the country's low level of investment restrictions.", "D": "The puzzle presented by the premium levels motivates the paper's use of an event study on premium *changes*, which can isolate the effect of a policy change from time-invariant, country-specific confounding factors."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the understanding of the core methodological motivation for the entire paper. It uses a 'Premise/Assumption Packaging' strategy. Option A requires correct interpretation of the descriptive data in the table. Option B requires understanding why the pattern in Option A leads to the choice of an event-study methodology. Distractor C is a 'Conceptual' error (overstatement), as the paper argues one cannot make this conclusion from levels alone. Distractor D is also a 'Conceptual' error, as it makes a causal claim the paper explicitly avoids, noting that the cause of domestic fund discounts is an unsolved puzzle.", "qid": "265", "question": "### Background\n\n**Research Question.** This study seeks to determine if international investment restrictions are binding by examining their effect on closed-end country fund premiums. The simple hypothesis is that countries with more severe restrictions should exhibit higher premiums.\n\n**Setting / Data-Generating Environment.** The analysis uses weekly data for a sample of closed-end country funds from 1981 to 1989. The paper notes that a simple cross-sectional comparison of premium *levels* is problematic due to confounding factors, motivating a shift to an event study of premium *changes*.\n\n---\n\n### Data / Model Specification\n\n**Table 1** below presents descriptive statistics for the average percentage premiums of selected country funds.\n\n**Table 1: Average Percentage Premiums for Selected Closed-End Country Funds**\n\n| Fund (Country) | Mean Premium (%) |\n| :--- | :---: |\n| **High Restriction Markets** | |\n| Korea | 44.35 |\n| Taiwan | 40.96 |\n| Mexico | -7.46 |\n| Brazil | -28.82 |\n| **Low Restriction Markets** | |\n| U.K. | -21.37 |\n| Germany | -4.32 |\n\n*Source: Adapted from Table II, Panel A of the source paper.*\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the paper's methodological reasoning, select all of the following statements that are accurate.", "model_evaluation_result": {"model_raw_response": "Looking at Table 1 and the paper's methodological reasoning:\n\n**Option A** is incorrect. While Korea shows a large premium (+44.35%), this alone cannot prove that its restrictions were the most binding. The paper explicitly notes that \"a simple cross-sectional comparison of premium *levels* is problematic due to confounding factors.\" Premium levels can be influenced by many factors beyond investment restrictions.\n\n**Option B** is correct. The data clearly shows that while Korea and Taiwan (high-restriction markets) have large premiums (+44.35% and +40.96%), other high-restriction markets like Mexico (-7.46%) and Brazil (-28.82%) show discounts. This demonstrates a non-monotonic relationship - the relationship between restriction severity and premium levels is not consistently positive or negative.\n\n**Option C** is incorrect. The paper warns against attributing premium levels to restriction levels alone due to confounding factors. The U.K.'s discount of -21.37% cannot be primarily explained by low investment restrictions without controlling for other country-specific factors.\n\n**Option D** is correct. The paper explicitly states that cross-sectional comparison of premium levels is problematic due to confounding factors, which motivates their shift to an event study of premium *changes*. By examining changes rather than levels, the study can better isolate the effect of policy changes from time-invariant, country-specific factors that might affect premium levels.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 170, "Question": "### Background\n\n**Research Question.** What are the dynamic effects of a shock to housing market tightness on market liquidity and prices, and do these empirical effects align with predictions from search and matching theory?\n\n**Setting.** The analysis uses a panel VAR model estimated on quarterly data for 403 Dutch municipalities from 2011-2013. The model examines the interrelationships between market tightness, liquidity, and prices.\n\n**Theoretical Prediction.** Search and matching models with information asymmetries predict that a positive shock to market tightness causes an immediate increase in liquidity (matching probability), followed by a more gradual increase in prices as sellers slowly update their reservation prices. This is because sellers' reservation prices are based on *perceived* market tightness (`λ*`), which adjusts slowly, while the probability of meeting a buyer depends on *true* market tightness (`λ`), which changes instantly.\n\n**Empirical Finding.** A shock to market tightness leads to a temporary increase in the rate of sale (`ros`) but a gradual and permanent increase in the price level (`pr`).\n\n### Question\n\nThe paper's empirical findings show a key difference in the dynamic response of liquidity and prices to a market tightness shock. According to the theoretical framework presented, which of the following mechanisms correctly explain this observed pattern? (Select all that apply)", "Options": {"A": "The immediate increase in liquidity is driven by the seller's meeting probability, which is a function of the true, contemporaneous market tightness (`λ`).", "B": "Prices adjust slowly because buyers, not sellers, are slow to update their expectations about market conditions.", "C": "The permanent effect on prices and temporary effect on liquidity are inconsistent with the predictions of search and matching theory.", "D": "The gradual price increase is caused by sellers' reservation prices being based on perceived market tightness (`λ*`), which adapts slowly to changes in true market tightness."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a **Reverse-Reasoning** strategy, asking for the theoretical causes of an observed empirical pattern. It tests the core theoretical contribution of the paper. Correct options (A, B) use **Atomic Decomposition** to separate the explanations for the fast liquidity response (tied to true tightness `λ`) and the slow price response (tied to perceived tightness `λ*`). Distractors are based on conceptual errors: (C) incorrectly assigns the information friction to buyers instead of sellers, and (D) claims the findings contradict the theory when they actually provide strong support for it.", "qid": "170", "question": "### Background\n\n**Research Question.** What are the dynamic effects of a shock to housing market tightness on market liquidity and prices, and do these empirical effects align with predictions from search and matching theory?\n\n**Setting.** The analysis uses a panel VAR model estimated on quarterly data for 403 Dutch municipalities from 2011-2013. The model examines the interrelationships between market tightness, liquidity, and prices.\n\n**Theoretical Prediction.** Search and matching models with information asymmetries predict that a positive shock to market tightness causes an immediate increase in liquidity (matching probability), followed by a more gradual increase in prices as sellers slowly update their reservation prices. This is because sellers' reservation prices are based on *perceived* market tightness (`λ*`), which adjusts slowly, while the probability of meeting a buyer depends on *true* market tightness (`λ`), which changes instantly.\n\n**Empirical Finding.** A shock to market tightness leads to a temporary increase in the rate of sale (`ros`) but a gradual and permanent increase in the price level (`pr`).\n\n### Question\n\nThe paper's empirical findings show a key difference in the dynamic response of liquidity and prices to a market tightness shock. According to the theoretical framework presented, which of the following mechanisms correctly explain this observed pattern? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which mechanisms correctly explain the observed pattern where liquidity increases immediately but temporarily, while prices increase gradually but permanently, following a market tightness shock.\n\nLet me analyze each option against the theoretical framework provided:\n\nOption A: \"The immediate increase in liquidity is driven by the seller's meeting probability, which is a function of the true, contemporaneous market tightness (λ).\"\n- This aligns with the theory that states \"the probability of meeting a buyer depends on true market tightness (λ), which changes instantly\"\n- This correctly explains why liquidity responds immediately\n- ✓ Correct\n\nOption B: \"Prices adjust slowly because buyers, not sellers, are slow to update their expectations about market conditions.\"\n- The theoretical framework explicitly states that \"sellers' reservation prices are based on perceived market tightness (λ*), which adjusts slowly\"\n- This option incorrectly attributes the slow adjustment to buyers rather than sellers\n- ✗ Incorrect\n\nOption C: \"The permanent effect on prices and temporary effect on liquidity are inconsistent with the predictions of search and matching theory.\"\n- The background states these empirical findings align with the theoretical predictions\n- The theory predicts exactly this pattern: immediate liquidity increase followed by gradual price increase\n- ✗ Incorrect\n\nOption D: \"The gradual price increase is caused by sellers' reservation prices being based on perceived market tightness (λ*), which adapts slowly to changes in true market tightness.\"\n- This directly matches the theoretical prediction that \"sellers' reservation prices are based on perceived market tightness (λ*), which adjusts slowly\"\n- This correctly explains why prices increase gradually\n- ✓ Correct\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 296, "Question": "### Background\n\n**Research Question.** How do real-world frictions like transaction costs and differences in asset characteristics affect the viability of sophisticated trading strategies?\n\n**Setting.** The paper argues that value-weighted industry portfolios are a better testing ground for its strategy than characteristic-sorted portfolios (e.g., 25 portfolios sorted on size and book-to-market). The rationale involves two key dimensions: the inherent factor structure of the assets and their trading costs. The performance of a strategy on size/BM portfolios is used to test this argument.\n\n**Variables and Parameters.**\n- `\\alpha_{gross}`: Four-factor alpha before transaction costs.\n- `\\alpha_{net}`: Four-factor alpha after transaction costs.\n\n---\n\n### Data / Model Specification\n\nThe paper argues that characteristic-sorted portfolios have a tight factor structure, meaning most of their return variation is explained by common factors. Furthermore, it notes that portfolios with extreme characteristics (e.g., small-cap stocks) have much higher trading costs. **Table 1** below shows the performance of the long-short combo strategies applied to the 25 size/BM portfolios.\n\n**Table 1. Performance of Size/BM-Rotation Strategies (Annualized, %)**\n\n| Strategy | `\\alpha_{gross}` | `\\alpha_{net}` |\n| :--- | :--- | :--- |\n| L-S ABMA | 7.16*** | 2.62** |\n| L-S DMSFE| 7.01*** | 2.67** |\n| L-S MEAN | 5.18** | 0.04 |\n\n*Source: Table VI in the original paper. `**` and `***` denote statistical significance.*\n\n---\n\n### Question\n\nBased on the paper's arguments and the data in Table 1, which of the following statements accurately describe the challenges and performance of applying the combo strategy to characteristic-sorted (size/BM) portfolios?", "Options": {"A": "The paper argues that characteristic-sorted portfolios are less ideal for demonstrating alpha because their returns have a tight factor structure, leaving little idiosyncratic variance to exploit.", "B": "The combo strategies fail to generate statistically significant *gross* alpha when applied to size/BM portfolios, indicating the forecasts are ineffective for these assets.", "C": "The erosion of alpha by transaction costs is so severe for the MEAN strategy that a statistically significant gross alpha is reduced to a statistically insignificant net alpha.", "D": "The strategy's reliance on trading extreme portfolios (e.g., small-cap stocks) leads to high transaction costs, which substantially erode the gross alpha."}, "Answer": ["A", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the paper's rationale for choosing test assets and the impact of real-world frictions. It uses an **Atomic Decomposition** strategy to test the distinct concepts of factor structure, transaction costs, and the resulting gap between gross and net alpha. Option C is a 'Conceptual Opposite' distractor, incorrectly claiming the forecasts fail at the gross level, which tests careful reading of the table.", "qid": "296", "question": "### Background\n\n**Research Question.** How do real-world frictions like transaction costs and differences in asset characteristics affect the viability of sophisticated trading strategies?\n\n**Setting.** The paper argues that value-weighted industry portfolios are a better testing ground for its strategy than characteristic-sorted portfolios (e.g., 25 portfolios sorted on size and book-to-market). The rationale involves two key dimensions: the inherent factor structure of the assets and their trading costs. The performance of a strategy on size/BM portfolios is used to test this argument.\n\n**Variables and Parameters.**\n- `\\alpha_{gross}`: Four-factor alpha before transaction costs.\n- `\\alpha_{net}`: Four-factor alpha after transaction costs.\n\n---\n\n### Data / Model Specification\n\nThe paper argues that characteristic-sorted portfolios have a tight factor structure, meaning most of their return variation is explained by common factors. Furthermore, it notes that portfolios with extreme characteristics (e.g., small-cap stocks) have much higher trading costs. **Table 1** below shows the performance of the long-short combo strategies applied to the 25 size/BM portfolios.\n\n**Table 1. Performance of Size/BM-Rotation Strategies (Annualized, %)**\n\n| Strategy | `\\alpha_{gross}` | `\\alpha_{net}` |\n| :--- | :--- | :--- |\n| L-S ABMA | 7.16*** | 2.62** |\n| L-S DMSFE| 7.01*** | 2.67** |\n| L-S MEAN | 5.18** | 0.04 |\n\n*Source: Table VI in the original paper. `**` and `***` denote statistical significance.*\n\n---\n\n### Question\n\nBased on the paper's arguments and the data in Table 1, which of the following statements accurately describe the challenges and performance of applying the combo strategy to characteristic-sorted (size/BM) portfolios?", "model_evaluation_result": {"model_raw_response": "Looking at the data and arguments presented:\n\nFrom Table 1:\n- All strategies show statistically significant gross alpha (7.16***, 7.01***, 5.18**)\n- After transaction costs, L-S ABMA and L-S DMSFE maintain statistical significance (2.62**, 2.67**)\n- L-S MEAN drops from 5.18** (significant) to 0.04 (no significance indicators)\n\nAnalyzing each option:\n\nA) **Correct** - The paper explicitly states that characteristic-sorted portfolios have a \"tight factor structure, meaning most of their return variation is explained by common factors,\" making them \"less ideal\" for the strategy.\n\nB) **Incorrect** - All strategies generate statistically significant gross alpha (marked with ** or ***).\n\nC) **Correct** - The MEAN strategy shows gross alpha of 5.18** (statistically significant) but net alpha of 0.04 with no significance indicators, indicating the significance was lost due to transaction costs.\n\nD) **Correct** - The paper notes that \"portfolios with extreme characteristics (e.g., small-cap stocks) have much higher trading costs,\" and the data shows substantial erosion from gross to net alpha across all strategies.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 287, "Question": "### Background\n\n**Research Question.** Do corporate insiders systematically engage in abnormal trading ahead of value-relevant corporate events, such as an exchange listing announcement, and how can this be tested empirically?\n\n**Setting.** The study examines insider trading around the month of a firm's application for exchange listing (event month 0). It calculates the mean abnormal number of net purchasers (`MANP_t`) for a sample of firms in each month relative to the announcement.\n\n**Variables & Parameters.**\n- `MANP_t`: Mean abnormal number of net purchasers in event month `t`.\n- `CUMNP_t`: Cumulated mean abnormal number of net purchasers over a specified window.\n- Event Month 0: The month of the first public announcement of a formal listing application.\n\n---\n\n### Data / Model Specification\n\nAbnormal insider trading is measured using `MANP_t`, the cross-sectional average of firm-level abnormal net purchasers. A positive `MANP_t` indicates net buying activity above historical firm-specific norms. The statistical significance of this activity is assessed with a t-statistic.\n\n**Table 1. Mean Abnormal Net Insider Purchasing Activity (NYSE & AMEX Listers)**\n\n| Event Month(s) | `MANP_t` | t-statistic |\n|:---------------|:-----------|:------------|\n| -12            | 0.405      | 2.480**     |\n| -11            | 0.426      | 2.624***    |\n| -6             | 0.436      | 2.803***    |\n| -1             | 0.071      | 0.476       |\n| 0              | 0.435      | 2.941***    |\n| +1             | 0.616      | 4.231***    |\n| **[-12, -1]**  | **3.082**  | **5.692***  |\n\n*Note: Table is an abridged version of the original. `CUMNP` is reported for summary rows. ***p<0.01, **p<0.05.*\n\n---\n\nBased on the data in Table 1 and the study's context, which of the following statements are valid interpretations or conclusions?", "Options": {"A": "The cumulative measure `CUMNP` for the period [-12, -1] provides stronger statistical evidence of systematic pre-announcement buying than any single month's `MANP` in that period.", "B": "The pattern of significant `MANP` values dispersed over the pre-announcement year suggests that insiders may possess and act on information about a potential listing long before it is publicly announced.", "C": "Significant abnormal insider purchasing activity is observed both in the month of the listing announcement (month 0) and the month immediately following it (month +1).", "D": "The data show that abnormal insider buying is concentrated in the month immediately prior to the listing announcement (month -1)."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to interpret statistical results from a table (`MANP` and `CUMNP`), synthesize patterns over time, and draw valid conclusions about insider behavior. Depth Strategy: Reverse-Reasoning. Given the empirical results, the user must identify the plausible conclusions. Distractor Logic: Option B is a 'Conceptual Opposite' distractor. The table explicitly shows that `MANP` in month -1 is small and statistically insignificant, contradicting the idea of concentrated buying. The correct options require recognizing the power of cumulative statistics (A), identifying significant point estimates (C), and interpreting the overall temporal pattern (D).", "qid": "287", "question": "### Background\n\n**Research Question.** Do corporate insiders systematically engage in abnormal trading ahead of value-relevant corporate events, such as an exchange listing announcement, and how can this be tested empirically?\n\n**Setting.** The study examines insider trading around the month of a firm's application for exchange listing (event month 0). It calculates the mean abnormal number of net purchasers (`MANP_t`) for a sample of firms in each month relative to the announcement.\n\n**Variables & Parameters.**\n- `MANP_t`: Mean abnormal number of net purchasers in event month `t`.\n- `CUMNP_t`: Cumulated mean abnormal number of net purchasers over a specified window.\n- Event Month 0: The month of the first public announcement of a formal listing application.\n\n---\n\n### Data / Model Specification\n\nAbnormal insider trading is measured using `MANP_t`, the cross-sectional average of firm-level abnormal net purchasers. A positive `MANP_t` indicates net buying activity above historical firm-specific norms. The statistical significance of this activity is assessed with a t-statistic.\n\n**Table 1. Mean Abnormal Net Insider Purchasing Activity (NYSE & AMEX Listers)**\n\n| Event Month(s) | `MANP_t` | t-statistic |\n|:---------------|:-----------|:------------|\n| -12            | 0.405      | 2.480**     |\n| -11            | 0.426      | 2.624***    |\n| -6             | 0.436      | 2.803***    |\n| -1             | 0.071      | 0.476       |\n| 0              | 0.435      | 2.941***    |\n| +1             | 0.616      | 4.231***    |\n| **[-12, -1]**  | **3.082**  | **5.692***  |\n\n*Note: Table is an abridged version of the original. `CUMNP` is reported for summary rows. ***p<0.01, **p<0.05.*\n\n---\n\nBased on the data in Table 1 and the study's context, which of the following statements are valid interpretations or conclusions?", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each statement:\n\nA) The cumulative measure `CUMNP` for [-12, -1] shows a t-statistic of 5.692***, which is indeed higher than any single month's t-statistic in that period (the highest single month is -6 with t=2.803***). This provides stronger statistical evidence of systematic pre-announcement buying. **TRUE**\n\nB) The data shows significant `MANP` values (with *** significance) at months -12, -11, and -6, indicating significant abnormal insider buying dispersed throughout the pre-announcement year. This pattern suggests insiders may have information about potential listing well before public announcement. **TRUE**\n\nC) Month 0 (announcement month) shows `MANP` = 0.435 with t = 2.941***, and month +1 shows `MANP` = 0.616 with t = 4.231***. Both are highly significant, confirming abnormal insider purchasing in both months. **TRUE**\n\nD) Month -1 shows `MANP` = 0.071 with t = 0.476 (not significant). This is actually the lowest and only non-significant value in the table, so abnormal insider buying is NOT concentrated in month -1. **FALSE**\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 337, "Question": "### Background\n\n**Research Question.** What is the relationship between factor risk premia estimated using multivariate betas versus univariate betas, and are the two specifications economically equivalent?\n\n**Setting.** An econometrician is considering two alternative specifications for a `K`-factor model. One uses multivariate betas (`B`) from a multiple regression of returns on all factors simultaneously. The other uses univariate betas (`B*`) from separate simple regressions of returns on each factor.\n\n**Variables and Parameters.**\n- `b`: `K`x1 vector of multivariate risk premia.\n- `b*`: `K`x1 vector of univariate risk premia.\n- `Ω`: `K`x`K` covariance matrix of factors, `Var(y)`.\n- `D`: `K`x`K` diagonal matrix where `Dₖₖ = Ωₖₖ = Var(yₖ)`.\n\n---\n\n### Data / Model Specification\n\nThe risk premia from the two models are related by the following transformation:\n  \nb = \\Omega D^{-1} b^* \\quad \\text{(Eq. (1))}\n \nConsider a two-factor model where the factor covariance matrix `Ω` and the univariate risk premia `b*` are given by:\n  \n\\Omega = \\begin{pmatrix} 1 & -0.5 \\\\ -0.5 & 1 \\end{pmatrix}, \\quad b^* = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n \n\n---\n\n### Question\n\nGiven the model specifications and the specific numerical example, select all of the following statements that are correct.", "Options": {"A": "In the multivariate model, the risk premium for the first factor (`b₁`) is 0.", "B": "This example demonstrates that a factor's multivariate risk premium (`bₖ`) can be zero even if its univariate premium (`b*ₖ`) is non-zero, because the multivariate specification accounts for the factor's correlation with other priced factors.", "C": "The finding that `b₁ = 0` must be a numerical error, because a factor with a positive univariate risk premium (`b*₁ = 1`) cannot have a zero multivariate risk premium.", "D": "In the multivariate model, the risk premium for the second factor (`b₂`) is 2.0."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: Tests the ability to calculate the transformation between univariate and multivariate risk premia and understand the economic non-equivalence. Strategy: Scenario Application & Computational Judgment. A specific numerical scenario is provided, requiring both calculation and conceptual interpretation. Distractor Logic: Option B is a Numerical Error (specifically, a Formula Misuse error where `b₂` is incorrectly assumed to equal `b*₂`). Option D is a classic Conceptual error, stating the core misconception that the two premia specifications are economically equivalent.", "qid": "337", "question": "### Background\n\n**Research Question.** What is the relationship between factor risk premia estimated using multivariate betas versus univariate betas, and are the two specifications economically equivalent?\n\n**Setting.** An econometrician is considering two alternative specifications for a `K`-factor model. One uses multivariate betas (`B`) from a multiple regression of returns on all factors simultaneously. The other uses univariate betas (`B*`) from separate simple regressions of returns on each factor.\n\n**Variables and Parameters.**\n- `b`: `K`x1 vector of multivariate risk premia.\n- `b*`: `K`x1 vector of univariate risk premia.\n- `Ω`: `K`x`K` covariance matrix of factors, `Var(y)`.\n- `D`: `K`x`K` diagonal matrix where `Dₖₖ = Ωₖₖ = Var(yₖ)`.\n\n---\n\n### Data / Model Specification\n\nThe risk premia from the two models are related by the following transformation:\n  \nb = \\Omega D^{-1} b^* \\quad \\text{(Eq. (1))}\n \nConsider a two-factor model where the factor covariance matrix `Ω` and the univariate risk premia `b*` are given by:\n  \n\\Omega = \\begin{pmatrix} 1 & -0.5 \\\\ -0.5 & 1 \\end{pmatrix}, \\quad b^* = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n \n\n---\n\n### Question\n\nGiven the model specifications and the specific numerical example, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "I need to calculate the multivariate risk premia using the given formula and then evaluate each statement.\n\nGiven:\n- Ω = [[1, -0.5], [-0.5, 1]]\n- b* = [1, 2]\n- Formula: b = Ω D^(-1) b*\n\nFirst, I'll find D (diagonal matrix with variances):\n- D₁₁ = Ω₁₁ = 1\n- D₂₂ = Ω₂₂ = 1\n- So D = [[1, 0], [0, 1]]\n\nTherefore D^(-1) = [[1, 0], [0, 1]]\n\nNow I'll calculate b = Ω D^(-1) b*:\n- D^(-1) b* = [[1, 0], [0, 1]] × [1, 2] = [1, 2]\n- b = Ω × [1, 2] = [[1, -0.5], [-0.5, 1]] × [1, 2]\n- b₁ = 1×1 + (-0.5)×2 = 1 - 1 = 0\n- b₂ = (-0.5)×1 + 1×2 = -0.5 + 2 = 1.5\n\nNow evaluating each statement:\n\nA) \"In the multivariate model, the risk premium for the first factor (b₁) is 0.\"\n- From my calculation: b₁ = 0 ✓ TRUE\n\nB) \"This example demonstrates that a factor's multivariate risk premium (bₖ) can be zero even if its univariate premium (b*ₖ) is non-zero, because the multivariate specification accounts for the factor's correlation with other priced factors.\"\n- We have b*₁ = 1 (non-zero) but b₁ = 0\n- The factors are correlated (ρ = -0.5)\n- This is exactly what's happening ✓ TRUE\n\nC) \"The finding that b₁ = 0 must", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 342, "Question": "### Background\n\n**Research Question.** This case investigates the source of stock return predictability from moving averages by comparing predictors constructed from low-frequency (monthly) versus high-frequency (daily) price data.\n\n**Setting and Sample.** The analysis uses monthly excess returns on the S&P 500 index from January 1963 to December 2015. Predictor variables are constructed at the end of each month `t` to forecast the excess return in month `t+1`.\n\n### Data / Model Specification\n\nThe predictive relationship is modeled using a bivariate linear regression:\n  \nr_{t+1} = \\alpha + \\beta Z_t + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \nThe novel daily predictor, `MADP_t(K)`, is constructed as the K-day simple moving average of daily prices divided by the current day's closing price, `P_{t,d}`:\n  \nM A D P_{t}(K) = \\frac{M A D_{t}(K)}{P_{t,d}} \\quad \\text{where} \\quad M A D_{t}(K)=\\frac{1}{K}\\sum_{j=0}^{K-1}P_{t,d-j} \\quad \\text{(Eq. 2)}\n \n\n**Table 1: Bivariate Predictive Regression Results (Selected)**\n\n| Predictor `Z_t` | Slope (`β`) | t-statistic | In-Sample `R^2` (%) | Out-of-Sample `R_{os}^2` (%) |\n|:---|---:|:---:|---:|---:|\n| MAMP(12) | -0.65 | (-0.34) | 0.02 | -1.87 |\n| MADP(3) | 1.28 | (4.24)*** | 2.75 | 1.79** |\n| MADP(20) | 0.40 | (1.27) | 0.25 | -1.14 |\n\n*Note: `MAMP(12)` is a predictor based on a 12-month moving average. ***, ** denote significance at the 1% and 5% levels, respectively.*\n\n### Question\n\nBased on the provided model specifications and empirical results in Table 1, select all statements that are correctly supported by the evidence.", "Options": {"A": "The `MAMP(12)` predictor shows stronger out-of-sample performance than the `MADP(20)` predictor, as indicated by its less negative `R_{os}^2`.", "B": "A value of `MADP_t(3) > 1` indicates that the current price `P_{t,d}` is lower than the average price over the preceding 3 days.", "C": "The combination of a positive slope coefficient (`β > 0`) for `MADP(3)` and its definition implies that the predictor captures a short-term mean-reversion effect.", "D": "The significant t-statistic for `MADP(3)` suggests it captures a short-term momentum effect."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the **Atomic Decomposition** strategy to test the core interpretation of the predictor and the main empirical finding. Option A tests the basic definition of the predictor. Option B tests the economic interpretation of the regression results. The distractors test for common misinterpretations: C requires a careful comparison of negative numbers in the table (a common slip), and D presents a conceptual opposite (momentum vs. mean-reversion).", "qid": "342", "question": "### Background\n\n**Research Question.** This case investigates the source of stock return predictability from moving averages by comparing predictors constructed from low-frequency (monthly) versus high-frequency (daily) price data.\n\n**Setting and Sample.** The analysis uses monthly excess returns on the S&P 500 index from January 1963 to December 2015. Predictor variables are constructed at the end of each month `t` to forecast the excess return in month `t+1`.\n\n### Data / Model Specification\n\nThe predictive relationship is modeled using a bivariate linear regression:\n  \nr_{t+1} = \\alpha + \\beta Z_t + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \nThe novel daily predictor, `MADP_t(K)`, is constructed as the K-day simple moving average of daily prices divided by the current day's closing price, `P_{t,d}`:\n  \nM A D P_{t}(K) = \\frac{M A D_{t}(K)}{P_{t,d}} \\quad \\text{where} \\quad M A D_{t}(K)=\\frac{1}{K}\\sum_{j=0}^{K-1}P_{t,d-j} \\quad \\text{(Eq. 2)}\n \n\n**Table 1: Bivariate Predictive Regression Results (Selected)**\n\n| Predictor `Z_t` | Slope (`β`) | t-statistic | In-Sample `R^2` (%) | Out-of-Sample `R_{os}^2` (%) |\n|:---|---:|:---:|---:|---:|\n| MAMP(12) | -0.65 | (-0.34) | 0.02 | -1.87 |\n| MADP(3) | 1.28 | (4.24)*** | 2.75 | 1.79** |\n| MADP(20) | 0.40 | (1.27) | 0.25 | -1.14 |\n\n*Note: `MAMP(12)` is a predictor based on a 12-month moving average. ***, ** denote significance at the 1% and 5% levels, respectively.*\n\n### Question\n\nBased on the provided model specifications and empirical results in Table 1, select all statements that are correctly supported by the evidence.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The `MAMP(12)` predictor shows stronger out-of-sample performance than the `MADP(20)` predictor, as indicated by its less negative `R_{os}^2`.\n- MAMP(12): R²_os = -1.87%\n- MADP(20): R²_os = -1.14%\n- Since -1.14% > -1.87%, MADP(20) actually performs better (less negative is better)\n- This statement is FALSE\n\nB) A value of `MADP_t(3) > 1` indicates that the current price `P_{t,d}` is lower than the average price over the preceding 3 days.\n- MADP_t(3) = MAD_t(3) / P_{t,d}\n- If MADP_t(3) > 1, then MAD_t(3) > P_{t,d}\n- This means the 3-day average price is higher than the current price\n- This statement is TRUE\n\nC) The combination of a positive slope coefficient (`β > 0`) for `MADP(3)` and its definition implies that the predictor captures a short-term mean-reversion effect.\n- β = 1.28 > 0 for MADP(3)\n- When MADP(3) > 1 (current price below average), the model predicts positive returns\n- When MADP(3) < 1 (current price above average), the model predicts negative returns\n- This is consistent with mean reversion (prices reverting to their average)\n- This statement is TRUE\n\nD) The significant t-statistic for `MADP(3)` suggests it captures a short-term momentum effect.\n- The positive β means high MADP(3) predicts high returns\n- High MADP(3) occurs when current price is low relative to recent average\n- This predicts reversal (low prices → high returns), not momentum\n- This statement is FALSE\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 39, "Question": "### Background\n\n**Research Question:** This case examines the market's valuation of the ECB's 2014 Comprehensive Assessment (CA) through an event study. The central question is whether the market reacted to new, bank-specific (\"idiosyncratic\") information or to news about a systematic or systemic factor affecting all banks.\n\n**Setting and Sample:** The analysis focuses on the stock returns of 38 listed European banks around the CA results announcement on October 26, 2014. The study forgoes a control group of non-CA banks, arguing it would introduce confounding biases.\n\n### Data / Model Specification\n\nThe study computes Cumulative Average Abnormal Returns (CAARs) for portfolios of banks. Banks are grouped into quartiles based on the total capital curtailment they received (AQR + STE), with the 1st Quartile containing the banks that performed the worst.\n\n**Table 1: Event Study Results (CAARs)**\n\n| | **Pre-Event** | **Post-Event** |\n| :--- | :--- | :--- |\n| **Group of Banks** | **CAAR [-5, 0]** | **CAAR [0, +5]** |\n| Full Sample (38 banks) | 5.26%*** | -7.66%** |\n| 1st Quartile (Worst performing) | 7.47%* | -14.68%* |\n| 4th Quartile (Best performing) | 3.22%** | -4.70%*** |\n\n*Source: Adapted from Table 6, Panel A in the source paper. `***`, `**`, `*` denote significance at 1%, 5%, and 10% levels.*\n\nThe paper also reports that the difference in post-event CAARs between the 1st and 4th quartiles is not statistically significant.\n\n### Question\n\nBased on the event study results presented in **Table 1** and the accompanying information, which of the following interpretations are valid?", "Options": {"A": "The fact that the post-event CAAR difference between the worst-performing (1st Quartile) and best-performing (4th Quartile) banks was not statistically significant indicates the CA was largely ineffective at helping investors \"sort\" banks based on their individual results.", "B": "The reversal from a significant positive CAAR (+5.26%) before the event to a significant negative CAAR (-7.66%) after the event for the full sample suggests the market was caught by a widespread negative surprise.", "C": "The results show a clear sorting effect, as the best-performing banks (4th Quartile) experienced positive abnormal returns after the announcement, while the worst-performing banks experienced negative returns.", "D": "The positive CAAR of +7.47% for the worst-performing banks before the announcement is strong evidence of information leakage, as the market correctly anticipated these banks would perform poorly."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize time-series and cross-sectional evidence from an event study to form a coherent economic conclusion. It uses an **Atomic Decomposition** strategy, with correct options representing the key time-series finding (widespread surprise, Option A) and the key cross-sectional finding (failure to sort, Option B). Distractors target frequent misinterpretations of event study results, such as confusing the meaning of pre-event returns (**Conceptual Opposite**, Option C) and making a claim factually contradicted by the data table (**Factual Error**, Option D).", "qid": "39", "question": "### Background\n\n**Research Question:** This case examines the market's valuation of the ECB's 2014 Comprehensive Assessment (CA) through an event study. The central question is whether the market reacted to new, bank-specific (\"idiosyncratic\") information or to news about a systematic or systemic factor affecting all banks.\n\n**Setting and Sample:** The analysis focuses on the stock returns of 38 listed European banks around the CA results announcement on October 26, 2014. The study forgoes a control group of non-CA banks, arguing it would introduce confounding biases.\n\n### Data / Model Specification\n\nThe study computes Cumulative Average Abnormal Returns (CAARs) for portfolios of banks. Banks are grouped into quartiles based on the total capital curtailment they received (AQR + STE), with the 1st Quartile containing the banks that performed the worst.\n\n**Table 1: Event Study Results (CAARs)**\n\n| | **Pre-Event** | **Post-Event** |\n| :--- | :--- | :--- |\n| **Group of Banks** | **CAAR [-5, 0]** | **CAAR [0, +5]** |\n| Full Sample (38 banks) | 5.26%*** | -7.66%** |\n| 1st Quartile (Worst performing) | 7.47%* | -14.68%* |\n| 4th Quartile (Best performing) | 3.22%** | -4.70%*** |\n\n*Source: Adapted from Table 6, Panel A in the source paper. `***`, `**`, `*` denote significance at 1%, 5%, and 10% levels.*\n\nThe paper also reports that the difference in post-event CAARs between the 1st and 4th quartiles is not statistically significant.\n\n### Question\n\nBased on the event study results presented in **Table 1** and the accompanying information, which of the following interpretations are valid?", "model_evaluation_result": {"model_raw_response": "Looking at the data and interpretations:\n\n**Option A**: The paper states that the post-event CAAR difference between 1st quartile (-14.68%) and 4th quartile (-4.70%) is not statistically significant. While there's a numerical difference of about 10%, the lack of statistical significance means we cannot confidently say the market differentiated between banks based on their CA results. This interpretation is valid.\n\n**Option B**: The full sample shows a reversal from +5.26% (significant) pre-event to -7.66% (significant) post-event. This pattern suggests the market was expecting positive news but received negative news instead - a widespread negative surprise. This interpretation is valid.\n\n**Option C**: This is incorrect. The 4th quartile (best-performing banks) had a CAAR of -4.70%, not positive returns. Both quartiles experienced negative returns after the announcement.\n\n**Option D**: While the worst-performing banks did have positive pre-event returns (+7.47%), this doesn't constitute \"strong evidence\" of information leakage. If the market had correctly anticipated poor performance, we would expect negative pre-event returns for these banks, not positive ones. The positive returns suggest the market was not anticipating the poor results.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 308, "Question": "### Background\n\n**Research Question.** To analyze the economic consequences of diversification, one must first have a reliable way to quantify it. The Herfindahl Index (HI) is a standard measure of concentration used in economics and finance.\n\n**Setting.** The analysis considers diversification for a sample of equity REITs along two dimensions: property type and geographic location. The study transforms the standard HI to create a more intuitive measure for its regression analysis.\n\n---\n\n### Data / Model Specification\n\nThe Herfindahl Index is defined as the sum of the squared proportions of assets in each category:\n  \n\\mathrm{Herfindahl Index} \\, (HI) = \\sum_{i=1}^{I} P_{i}^{2} \n \n**Eq. (1)**\n\nwhere `P_i` is the proportion of assets in category `i`.\n\nIn the subsequent regression analysis, the measure for diversification is constructed as the negative of the Herfindahl Index:\n  \n\\text{Geographic Diversification} = -HI_{\\text{geographic}}\n \n**Eq. (2)**\n\n---\n\n### Question\n\nA hypothetical REIT has its assets distributed across three geographic regions: 50% in the Northeast, 30% in the Pacific, and 20% in the Southeast. Based on the provided definitions, which of the following statements are correct? Select all that apply.", "Options": {"A": "A positive and significant coefficient on the `Geographic Diversification` variable would indicate that more diversified firms have lower value.", "B": "To find evidence of a 'diversification discount,' the regression analysis would need to find a statistically significant negative coefficient on the `Geographic Diversification` variable.", "C": "The value of the `Geographic Diversification` variable used in the regression for this REIT is -0.38.", "D": "The geographic Herfindahl Index (HI) for this REIT is 1.00."}, "Answer": ["B", "C"], "pi_justification": "Score_A: 9. Score_B: 10. Total_Score: 9.5. Judgment: REPLACE with Multiple Choice (Score: 9.5). This item uses a Computational Judgment strategy to test the understanding of how a key variable is constructed and interpreted. It requires a multi-step process: calculate the Herfindahl Index, transform it into the regression variable, and then correctly reason about the expected sign of its coefficient under a specific hypothesis. Distractor (A) is a 'Step-Omission Error' (summing proportions without squaring). Distractor (D) is a 'Conceptual Opposite' / 'Sign Error' that reflects a common misunderstanding of the transformed variable's meaning.", "qid": "308", "question": "### Background\n\n**Research Question.** To analyze the economic consequences of diversification, one must first have a reliable way to quantify it. The Herfindahl Index (HI) is a standard measure of concentration used in economics and finance.\n\n**Setting.** The analysis considers diversification for a sample of equity REITs along two dimensions: property type and geographic location. The study transforms the standard HI to create a more intuitive measure for its regression analysis.\n\n---\n\n### Data / Model Specification\n\nThe Herfindahl Index is defined as the sum of the squared proportions of assets in each category:\n  \n\\mathrm{Herfindahl Index} \\, (HI) = \\sum_{i=1}^{I} P_{i}^{2} \n \n**Eq. (1)**\n\nwhere `P_i` is the proportion of assets in category `i`.\n\nIn the subsequent regression analysis, the measure for diversification is constructed as the negative of the Herfindahl Index:\n  \n\\text{Geographic Diversification} = -HI_{\\text{geographic}}\n \n**Eq. (2)**\n\n---\n\n### Question\n\nA hypothetical REIT has its assets distributed across three geographic regions: 50% in the Northeast, 30% in the Pacific, and 20% in the Southeast. Based on the provided definitions, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to:\n1. Calculate the Herfindahl Index for the given REIT\n2. Determine the Geographic Diversification value\n3. Analyze what the regression coefficients would mean\n\n**Step 1: Calculate the Herfindahl Index**\n\nGiven proportions:\n- Northeast: 50% = 0.50\n- Pacific: 30% = 0.30  \n- Southeast: 20% = 0.20\n\nUsing Equation (1):\nHI = (0.50)² + (0.30)² + (0.20)²\nHI = 0.25 + 0.09 + 0.04\nHI = 0.38\n\n**Step 2: Calculate Geographic Diversification**\n\nUsing Equation (2):\nGeographic Diversification = -HI_geographic = -0.38\n\n**Step 3: Analyze each option**\n\n**Option A**: \"A positive and significant coefficient on the Geographic Diversification variable would indicate that more diversified firms have lower value.\"\n\nSince Geographic Diversification = -HI, a higher (less negative) value indicates MORE diversification. If the coefficient is positive, then as diversification increases, firm value increases. This statement is incorrect.\n\n**Option B**: \"To find evidence of a 'diversification discount,' the regression analysis would need to find a statistically significant negative coefficient on the Geographic Diversification variable.\"\n\nA 'diversification discount' means diversified firms have lower value. Since higher Geographic Diversification values indicate more diversification, a negative coefficient would mean more diversification leads to lower value. This statement is correct.\n\n**Option C**: \"The value of the Geographic Diversification variable used in the regression for this REIT is -0.38.\"\n\nFrom our calculation: Geographic Diversification = -0.38. This statement is correct.\n\n**Option D**: \"The geographic Herfindahl Index (HI) for this REIT is 1.00.\"\n\nFrom our calculation: HI = 0.38, not 1.00. This statement is incorrect.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 275, "Question": "### Background\n\n**Research Question.** What institutional factors explain the cross-country variation in the value relevance of bank financial statements, and can these empirical relationships be interpreted causally?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage regression approach using a sample of 813 banks from 38 countries over the period 1993-2004. In the first stage, bank-specific value relevance is estimated. In the second stage, a single cross-sectional regression is run to explain the variation in these estimates.\n\n### Data / Model Specification\n\nIn the second stage, the Extent of Value Relevance (`EVR`), proxied by the coefficient on book value from the first-stage regressions, is modeled as a function of country-level and bank-specific factors:\n\n  \n\\mathrm{EVR}_{i} = a_{0} + b_{1} \\text{Transparency}_i + b_{2} \\text{Legal Environment}_i + b_{3} \\text{Accounting Cluster}_i + ... + \\varepsilon_{i}\n \n\nKey independent variables are defined as:\n*   `EVR`: The bank-specific coefficient on book value from a first-stage regression of market value on book value.\n*   `Legal Environment`: An indicator variable equal to 1 for common law systems and 0 for code law systems.\n*   `Transparency`: A country-level index (CIFAR) measuring the extent of mandatory financial disclosure.\n*   `Accounting Cluster`: An indicator variable equal to 1 for the British-American accounting cluster and 0 otherwise.\n\n**Table 1: Second-Stage Regression Results (Dependent Variable = EVR)**\n\n| Correlated Factor     | Coefficient | P-value |\n| :-------------------- | :---------- | :------ |\n| Transparency          | 0.045       | 0.001   |\n| Corporate environment | 0.042       | 0.011   |\n| Legal environment     | 0.041       | 0.037   |\n| Accounting cluster    | 0.114       | 0.001   |\n\n*Source: Adapted from Table 5, Panel A of the paper.*\n\n**Table 2: Correlation Matrix Snippet**\n\n|                     | Legal Env. | Transparency | Acct. Cluster |\n| :------------------ | :--------- | :----------- | :------------ |\n| **Legal Env.**      | 1.00       |              |               |\n| **Transparency**    | 0.40*      | 1.00         |               |\n| **Acct. Cluster**   | 0.48*      | 0.57*        | 1.00          |\n\n*Source: Adapted from Table 4 of the paper. * denotes significant correlation.*\n\n### Question\n\nA researcher argues that the significant coefficient on `Legal Environment` in Table 1 cannot be interpreted as a clean causal effect. Based on the provided tables, select all statements that correctly identify an econometric problem supporting this argument.", "Options": {"A": "If `Transparency` were omitted from the regression in Table 1, the estimated coefficient on `Legal Environment` would likely be biased upwards, as `Legal Environment` would incorrectly capture some of the positive effect of the correlated `Transparency` variable.", "B": "The strong positive correlations between `Legal Environment`, `Transparency`, and `Accounting Cluster` (as shown in Table 2) make it difficult to statistically isolate the unique effect of the legal system from the effects of correlated institutional features.", "C": "The p-value for `Legal Environment` (0.037) is not low enough to rule out that the finding is due to random chance, which prevents a causal interpretation.", "D": "If `Transparency` were omitted from the regression, the coefficient on `Legal Environment` would be biased towards zero due to the confounding effect."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to critique causal claims using evidence of multicollinearity and the theory of omitted variable bias. It uses a Premise/Assumption Packaging strategy, asking students to select valid econometric arguments. Correct options require identifying multicollinearity from a correlation matrix (A) and correctly reasoning about the direction of omitted variable bias (B). Distractor C presents the opposite direction for the bias (a Sign Error), and D confuses statistical significance with the separate issue of causal identification (a Conceptual Error).", "qid": "275", "question": "### Background\n\n**Research Question.** What institutional factors explain the cross-country variation in the value relevance of bank financial statements, and can these empirical relationships be interpreted causally?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage regression approach using a sample of 813 banks from 38 countries over the period 1993-2004. In the first stage, bank-specific value relevance is estimated. In the second stage, a single cross-sectional regression is run to explain the variation in these estimates.\n\n### Data / Model Specification\n\nIn the second stage, the Extent of Value Relevance (`EVR`), proxied by the coefficient on book value from the first-stage regressions, is modeled as a function of country-level and bank-specific factors:\n\n  \n\\mathrm{EVR}_{i} = a_{0} + b_{1} \\text{Transparency}_i + b_{2} \\text{Legal Environment}_i + b_{3} \\text{Accounting Cluster}_i + ... + \\varepsilon_{i}\n \n\nKey independent variables are defined as:\n*   `EVR`: The bank-specific coefficient on book value from a first-stage regression of market value on book value.\n*   `Legal Environment`: An indicator variable equal to 1 for common law systems and 0 for code law systems.\n*   `Transparency`: A country-level index (CIFAR) measuring the extent of mandatory financial disclosure.\n*   `Accounting Cluster`: An indicator variable equal to 1 for the British-American accounting cluster and 0 otherwise.\n\n**Table 1: Second-Stage Regression Results (Dependent Variable = EVR)**\n\n| Correlated Factor     | Coefficient | P-value |\n| :-------------------- | :---------- | :------ |\n| Transparency          | 0.045       | 0.001   |\n| Corporate environment | 0.042       | 0.011   |\n| Legal environment     | 0.041       | 0.037   |\n| Accounting cluster    | 0.114       | 0.001   |\n\n*Source: Adapted from Table 5, Panel A of the paper.*\n\n**Table 2: Correlation Matrix Snippet**\n\n|                     | Legal Env. | Transparency | Acct. Cluster |\n| :------------------ | :--------- | :----------- | :------------ |\n| **Legal Env.**      | 1.00       |              |               |\n| **Transparency**    | 0.40*      | 1.00         |               |\n| **Acct. Cluster**   | 0.48*      | 0.57*        | 1.00          |\n\n*Source: Adapted from Table 4 of the paper. * denotes significant correlation.*\n\n### Question\n\nA researcher argues that the significant coefficient on `Legal Environment` in Table 1 cannot be interpreted as a clean causal effect. Based on the provided tables, select all statements that correctly identify an econometric problem supporting this argument.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to identify which statements correctly explain why the coefficient on `Legal Environment` cannot be interpreted as a clean causal effect.\n\nLet me analyze each option:\n\n**Option A**: This correctly identifies omitted variable bias. From Table 2, `Legal Environment` and `Transparency` have a correlation of 0.40. From Table 1, `Transparency` has a positive coefficient (0.045). If `Transparency` were omitted, the `Legal Environment` variable would pick up some of this positive effect due to their positive correlation, causing upward bias. This is a classic econometric problem that prevents causal interpretation.\n\n**Option B**: This correctly identifies multicollinearity. Table 2 shows significant positive correlations between all three variables (0.40, 0.48, and 0.57). When independent variables are highly correlated, it becomes difficult to separate their individual effects. The coefficient on `Legal Environment` may be capturing effects that actually come from correlated institutional features. This is another valid econometric concern for causal interpretation.\n\n**Option C**: This is incorrect. A p-value of 0.037 indicates statistical significance at the 5% level (p < 0.05). While not significant at the 1% level, this is conventionally considered sufficient to reject the null hypothesis. More importantly, statistical significance alone doesn't determine causality - the issue here is about identification, not significance levels.\n\n**Option D**: This is incorrect. Given the positive correlation between `Legal Environment` and `Transparency` (0.40) and the positive coefficient on `Transparency` (0.045), omitting `Transparency` would cause the coefficient on `Legal Environment` to be biased upward, not toward zero. This contradicts the standard omitted variable bias formula.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 355, "Question": "### Background\n\nThe aggregate claims process in insurance, `X(t)`, starts at zero and increases with discrete jumps as claims occur. In contrast, stock prices are often modeled with geometric Brownian motion (GBM).\n\n### Question\n\nWhich of the following are valid reasons why geometric Brownian motion (GBM) is a fundamentally unsuitable model for the aggregate claims process `X(t)`? Select all that apply.", "Options": {"A": "The claims process starts at `X(0) = 0`, while a GBM models an asset with a positive starting price `S(0) > 0`.", "B": "A GBM process can decrease in value, whereas the aggregate claims process is non-decreasing.", "C": "The paths of a GBM are continuous, whereas the claims process consists of discrete jumps.", "D": "GBM assumes the volatility of the process is constant, which is unrealistic for claims."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: Understanding the fundamental properties of stochastic processes used in finance and insurance. Strategy: Conceptual Differentiation. Distractor Logic: (A), (B), and (D) are all correct and fundamental mismatches between the process properties. (C) is a distractor of the 'True but Irrelevant' type. While constant volatility might be an unrealistic assumption for some applications, it is not a *fundamental* reason GBM is unsuitable for claims; more advanced stochastic volatility models are still diffusion processes and share the other, more critical flaws (continuity, ability to decrease).", "qid": "355", "question": "### Background\n\nThe aggregate claims process in insurance, `X(t)`, starts at zero and increases with discrete jumps as claims occur. In contrast, stock prices are often modeled with geometric Brownian motion (GBM).\n\n### Question\n\nWhich of the following are valid reasons why geometric Brownian motion (GBM) is a fundamentally unsuitable model for the aggregate claims process `X(t)`? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each option to determine which represent valid reasons why GBM is fundamentally unsuitable for modeling aggregate claims:\n\nA) **Valid** - The aggregate claims process starts at X(0) = 0 (no claims initially), while GBM requires S(0) > 0 since it models percentage changes. GBM cannot handle a zero starting value as it would involve division by zero.\n\nB) **Valid** - GBM can both increase and decrease in value (stock prices go up and down), while aggregate claims only increase or stay constant as new claims are added. The non-decreasing nature of claims is incompatible with GBM.\n\nC) **Valid** - GBM has continuous paths (no jumps), while the claims process has discrete jumps when claims occur. This is a fundamental structural difference between the processes.\n\nD) **Not valid** - While constant volatility might be unrealistic, this is not a fundamental incompatibility. GBM can be extended to have time-varying or stochastic volatility while maintaining its essential structure.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 134, "Question": "### Background\n\n**Research Question.** How does the shift from Basel I to the Basel II A-IRB framework differentially impact the required Tier 1 and Total Capital of monoline credit card banks versus diversified banking organizations?\n\n**Setting.** The analysis compares the change in minimum regulatory capital requirements for credit card portfolios under two polar cases: a specialized monoline credit card bank and a credit card subsidiary within a large, diversified bank. The differences arise from portfolio-wide calculations for loan loss reserves.\n\n### Data / Model Specification\n\nThe transition from Basel I to Basel II involves two critical rule changes regarding the Allowance for Loan and Lease Losses (`ALLL`):\n\n1.  **ALLL Cap (affects Total Capital):** The maximum amount of `ALLL` that can be included in Tier 2 capital changes. The Basel II cap is more generous for high-loss portfolios like credit cards.\n2.  **Reserve Shortfall (affects Tier 1 Capital):** Basel II introduces a deduction from Tier 1 capital if `ALLL` is less than Expected Losses (`EL`).\n\nBoth calculations are performed on a whole-bank basis. The estimated impact of these changes is summarized in the table below.\n\n**Table 1. Estimated Percentage Change in Required Capital from Basel I to Basel II**\n\n| Bank Type | Change in Required Total Capital | Change in Required Tier 1 Capital |\n| :--- | :---: | :---: |\n| Diversified Holding Co. | +44.3% | +2.2% |\n| Monoline Credit Card Bank | +23.6% | +13.2% |\n\n---\n\nBased on the rules and the data in **Table 1**, which of the following statements accurately explain the divergent outcomes for the two bank types?\n\nSelect all that apply.", "Options": {"A": "A diversified bank avoids a large increase in required Tier 1 Capital because reserve surpluses from its other loan portfolios (e.g., mortgages) can offset the reserve shortfall from its credit card portfolio in the whole-bank calculation.", "B": "A monoline bank's required Tier 1 Capital increases more than a diversified bank's because its portfolio-wide reserves are likely less than its expected losses, triggering a significant Tier 1 capital deduction for the reserve shortfall.", "C": "A diversified bank experiences a larger increase in required Total Capital because its portfolio-wide reserves were likely already below the Basel I ALLL cap, so it receives no benefit from the new, more generous Basel II cap.", "D": "A monoline bank's required Total Capital increases less than a diversified bank's because the more generous Basel II ALLL cap allows it to count more of its large, pre-existing reserves as Tier 2 capital, providing a partial capital relief."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's understanding of the specific, nuanced mechanisms within the Basel II framework (ALLL cap and reserve shortfall) and their differential impact based on a bank's business model. Depth Strategy: Reverse-Reasoning. The user is given the final outcomes (the table) and must identify all the correct causal explanations based on the provided rules. Distractor Logic: Similar to the previous item, this question is structured with four correct options, each explaining a different facet of the observed phenomenon. This comprehensive format ensures the candidate understands the full picture—why each bank type is affected differently for both Tier 1 and Total capital—rather than just one piece of the puzzle.", "qid": "134", "question": "### Background\n\n**Research Question.** How does the shift from Basel I to the Basel II A-IRB framework differentially impact the required Tier 1 and Total Capital of monoline credit card banks versus diversified banking organizations?\n\n**Setting.** The analysis compares the change in minimum regulatory capital requirements for credit card portfolios under two polar cases: a specialized monoline credit card bank and a credit card subsidiary within a large, diversified bank. The differences arise from portfolio-wide calculations for loan loss reserves.\n\n### Data / Model Specification\n\nThe transition from Basel I to Basel II involves two critical rule changes regarding the Allowance for Loan and Lease Losses (`ALLL`):\n\n1.  **ALLL Cap (affects Total Capital):** The maximum amount of `ALLL` that can be included in Tier 2 capital changes. The Basel II cap is more generous for high-loss portfolios like credit cards.\n2.  **Reserve Shortfall (affects Tier 1 Capital):** Basel II introduces a deduction from Tier 1 capital if `ALLL` is less than Expected Losses (`EL`).\n\nBoth calculations are performed on a whole-bank basis. The estimated impact of these changes is summarized in the table below.\n\n**Table 1. Estimated Percentage Change in Required Capital from Basel I to Basel II**\n\n| Bank Type | Change in Required Total Capital | Change in Required Tier 1 Capital |\n| :--- | :---: | :---: |\n| Diversified Holding Co. | +44.3% | +2.2% |\n| Monoline Credit Card Bank | +23.6% | +13.2% |\n\n---\n\nBased on the rules and the data in **Table 1**, which of the following statements accurately explain the divergent outcomes for the two bank types?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze how Basel I to Basel II transition affects capital requirements differently for monoline credit card banks versus diversified banks.\n\nKey facts from the data:\n- Diversified banks: +44.3% Total Capital increase, +2.2% Tier 1 increase\n- Monoline banks: +23.6% Total Capital increase, +13.2% Tier 1 increase\n\nThe two critical rule changes:\n1. ALLL Cap (affects Total Capital) - Basel II cap is more generous for high-loss portfolios\n2. Reserve Shortfall (affects Tier 1) - Basel II deducts from Tier 1 if ALLL < EL\n\nBoth calculations are done on a whole-bank basis.\n\nAnalyzing each option:\n\nA) This explains why diversified banks have a small Tier 1 increase (+2.2%). Other loan portfolios (like mortgages) typically have reserve surpluses that can offset credit card shortfalls in the whole-bank calculation. ✓\n\nB) This explains why monoline banks have a large Tier 1 increase (+13.2%). With only credit cards (high-loss portfolio), their ALLL is likely below EL, triggering the Tier 1 deduction. ✓\n\nC) This explains why diversified banks have a large Total Capital increase (+44.3%). If they were already below the Basel I cap, they don't benefit from the more generous Basel II cap. ✓\n\nD) This explains why monoline banks have a smaller Total Capital increase (+23.6%). Their large credit card reserves can now count more toward Tier 2 capital under the generous Basel II cap. ✓\n\nAll four statements correctly explain the divergent outcomes based on the portfolio composition effects and the whole-bank calculation methodology.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 33, "Question": "### Background\n\nThe study uses a difference-in-differences (DiD) research design to estimate the causal impact of the 2017 Tax Cuts and Jobs Act (TCJA) on corporate capital expenditures (`CapEx`). The design compares U.S. firms (treatment group) to Canadian firms (control group) before and after the TCJA's implementation in 2018. The post-treatment period is defined as calendar year 2019.\n\n### Data / Model Specification\n\nThe core DiD model is specified as:\n\n  \nCapEx_{it} = \\beta_1 (USFirm_i \\times Yr19_t) + \\gamma'X_{it} + \\Lambda_i + \\Omega_t + \\Pi_t + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nwhere `USFirm` is an indicator for U.S. firms, `Yr19` is an indicator for the year 2019, `Λ_i` represents firm fixed effects, and `Π_t` represents year fixed effects.\n\n---\n\nBased on the principles of this research design, select all of the following statements that are true.\n", "Options": {"A": "The firm fixed effects (`Λ_i`) control for time-invariant differences between U.S. and Canadian firms, such as industry composition or average size.", "B": "The year fixed effects (`Π_t`) control for macroeconomic trends, such as global business cycles, that affect both U.S. and Canadian firms.", "C": "A key identifying assumption is that, absent the TCJA, the trend in capital expenditures for U.S. firms would have been the same as the trend for Canadian firms.", "D": "The coefficient `β_1` measures the average level of capital expenditures for U.S. firms in 2019."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the mechanics and assumptions of the difference-in-differences (DiD) methodology, which is the core econometric strategy of the paper.\nDepth Strategy: Conceptual Judgment. The user must evaluate the role of each component of the DiD regression and its key underlying assumption.\nDistractor Logic:\n- A, B, C (Correct): These are all fundamental, correct statements about the components and assumptions of a fixed-effects DiD model. A tests the role of firm fixed effects, B tests the role of time fixed effects, and C tests the crucial parallel trends assumption.\n- D (Step-Omission Error): This is a common misinterpretation of a DiD coefficient. `β_1` does not measure a level; it measures the *difference* in the *change* of `CapEx` for the treatment group relative to the control group. It isolates the treatment effect, not the post-treatment outcome level.", "qid": "33", "question": "### Background\n\nThe study uses a difference-in-differences (DiD) research design to estimate the causal impact of the 2017 Tax Cuts and Jobs Act (TCJA) on corporate capital expenditures (`CapEx`). The design compares U.S. firms (treatment group) to Canadian firms (control group) before and after the TCJA's implementation in 2018. The post-treatment period is defined as calendar year 2019.\n\n### Data / Model Specification\n\nThe core DiD model is specified as:\n\n  \nCapEx_{it} = \\beta_1 (USFirm_i \\times Yr19_t) + \\gamma'X_{it} + \\Lambda_i + \\Omega_t + \\Pi_t + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nwhere `USFirm` is an indicator for U.S. firms, `Yr19` is an indicator for the year 2019, `Λ_i` represents firm fixed effects, and `Π_t` represents year fixed effects.\n\n---\n\nBased on the principles of this research design, select all of the following statements that are true.\n", "model_evaluation_result": {"model_raw_response": "Looking at this difference-in-differences (DiD) research design, I need to evaluate each statement based on the principles of DiD methodology.\n\nA) The firm fixed effects (`Λ_i`) control for time-invariant differences between U.S. and Canadian firms, such as industry composition or average size.\n**TRUE** - Firm fixed effects absorb all time-invariant characteristics at the firm level, including systematic differences between U.S. and Canadian firms.\n\nB) The year fixed effects (`Π_t`) control for macroeconomic trends, such as global business cycles, that affect both U.S. and Canadian firms.\n**TRUE** - Year fixed effects capture common time trends affecting all firms in the sample, controlling for macroeconomic factors that impact both groups.\n\nC) A key identifying assumption is that, absent the TCJA, the trend in capital expenditures for U.S. firms would have been the same as the trend for Canadian firms.\n**TRUE** - This is the parallel trends assumption, which is fundamental to DiD identification. It assumes that without treatment, the treatment and control groups would have followed parallel paths.\n\nD) The coefficient `β_1` measures the average level of capital expenditures for U.S. firms in 2019.\n**FALSE** - The coefficient `β_1` on the interaction term (USFirm × Yr19) measures the differential change in capital expenditures for U.S. firms relative to Canadian firms after the TCJA implementation. It's the DiD estimator, not the level of CapEx.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 338, "Question": "### Background\n\n**Research Question.** How are the equilibrium trading strategies and market liquidity determined when one versus two informed central banks intervene in a foreign exchange market?\n\n**Setting.** A Kyle-style microstructure model where informed central banks strategically trade. We compare the linear Nash equilibrium under unilateral intervention (one bank) with the one under coordinated intervention (two identical banks).\n\n**Variables and Parameters.**\n- `x_k`, `x_i`: Central bank order (coordinated, unilateral).\n- `f`: Fundamental value.\n- `t`: Central bank target.\n- `q`: Weight on stabilization objective.\n- `λ`, `λ_1`: Price impact parameter (coordinated, unilateral). A higher `λ` means lower liquidity.\n- `β`, `β_1`: Coefficients measuring the intensity of trading on fundamentals in the linear strategy `x = α + βf + γt`.\n\n---\n\n### Data / Model Specification\n\nIn equilibrium, the central bank's trading intensity on fundamentals is given by:\n-   Coordinated: `β = 1 / (λ(4qλ+3))`\n-   Unilateral: `β_1 = 1 / (2λ_1(qλ_1+1))`\n\nThe paper proves that market liquidity is higher in the coordinated case, which means the price impact parameter is lower (`λ < λ_1`).\n\n---\n\n### Question\n\nBased on the model specifications provided, which of the following statements about a central bank's trading intensity on fundamentals (`β`) are correct?", "Options": {"A": "In the coordinated case, each central bank trades *less* aggressively on its fundamental information than a bank acting alone (`β < β_1`).", "B": "The higher trading aggressiveness in the coordinated case (`β`) is a strategic response to the *greater* market liquidity (lower price impact, `λ < λ_1`) that prevails under coordination.", "C": "The higher trading aggressiveness in the coordinated case (`β`) is necessary to overcome the *lower* market liquidity (higher price impact, `λ > λ_1`) that prevails under coordination.", "D": "In the coordinated case, each central bank trades *more* aggressively on its fundamental information than a bank acting alone (`β > β_1`)."}, "Answer": ["B", "D"], "pi_justification": "Replaced QA (Suitability Score: 9.5). The original QA's core analytical insight—a counter-intuitive result about trading intensity—is an atomic, indisputable conclusion ideal for a choice format. This item uses Atomic Decomposition to test both the directional result and its underlying economic cause, ensuring a genuine multi-select question. Distractors are high-fidelity: (C) is a strong conceptual opposite based on flawed intuition, and (D) presents a causal error, testing for a complete understanding of the mechanism.", "qid": "338", "question": "### Background\n\n**Research Question.** How are the equilibrium trading strategies and market liquidity determined when one versus two informed central banks intervene in a foreign exchange market?\n\n**Setting.** A Kyle-style microstructure model where informed central banks strategically trade. We compare the linear Nash equilibrium under unilateral intervention (one bank) with the one under coordinated intervention (two identical banks).\n\n**Variables and Parameters.**\n- `x_k`, `x_i`: Central bank order (coordinated, unilateral).\n- `f`: Fundamental value.\n- `t`: Central bank target.\n- `q`: Weight on stabilization objective.\n- `λ`, `λ_1`: Price impact parameter (coordinated, unilateral). A higher `λ` means lower liquidity.\n- `β`, `β_1`: Coefficients measuring the intensity of trading on fundamentals in the linear strategy `x = α + βf + γt`.\n\n---\n\n### Data / Model Specification\n\nIn equilibrium, the central bank's trading intensity on fundamentals is given by:\n-   Coordinated: `β = 1 / (λ(4qλ+3))`\n-   Unilateral: `β_1 = 1 / (2λ_1(qλ_1+1))`\n\nThe paper proves that market liquidity is higher in the coordinated case, which means the price impact parameter is lower (`λ < λ_1`).\n\n---\n\n### Question\n\nBased on the model specifications provided, which of the following statements about a central bank's trading intensity on fundamentals (`β`) are correct?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to compare the trading intensities β and β₁ under the given conditions.\n\nGiven:\n- Coordinated: β = 1/(λ(4qλ+3))\n- Unilateral: β₁ = 1/(2λ₁(qλ₁+1))\n- Market liquidity is higher in coordinated case: λ < λ₁\n\nTo compare β and β₁, I'll analyze their ratio:\nβ/β₁ = [1/(λ(4qλ+3))] / [1/(2λ₁(qλ₁+1))] = 2λ₁(qλ₁+1) / (λ(4qλ+3))\n\nSince λ < λ₁, let me substitute λ = rλ₁ where 0 < r < 1:\nβ/β₁ = 2(qλ₁+1) / (r(4qrλ₁+3))\n\nFor β > β₁, I need this ratio > 1:\n2(qλ₁+1) > r(4qrλ₁+3)\n\nLet me verify with a specific example: q = 1, λ₁ = 1, r = 0.5 (so λ = 0.5)\n- Left side: 2(1+1) = 4\n- Right side: 0.5(4×0.5×1+3) = 0.5(5) = 2.5\n- Since 4 > 2.5, we have β > β₁\n\nThis can be verified more generally - the inequality holds for reasonable parameter values.\n\nNow evaluating the options:\n\nA) States β < β₁ - This is FALSE based on our analysis\n\nB) States higher trading aggressiveness in coordinated case (β > β₁) is a response to greater market liquidity (λ < λ₁) - This is TRUE and consistent with our findings\n\nC) States higher trading aggressiveness is needed to overcome lower market liquidity (λ > λ₁) - This is FALSE because we're", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 204, "Question": "### Background\n\n**Research Question.** The paper suggests a causal link between the introduction of an electronic trading (ET) system in 1999 and an observed improvement in market efficiency for FTSE100 futures. However, this simple pre-post empirical design is vulnerable to confounding variables.\n\n**Setting / Data-Generating Environment.** The analysis compares a measure of market inefficiency, `|VR(2) - 1|`, for FTSE100 futures in a period before the 1999 introduction of ET versus a period that includes the post-ET era. A decrease in this metric is interpreted as an efficiency gain caused by the new trading system.\n\n---\n\n### Question\n\nSelect ALL of the following statements that represent plausible confounding factors, challenging the paper's causal claim that the ET system *caused* the observed efficiency improvement.", "Options": {"A": "The implementation of the Sarbanes-Oxley Act in the US in 2002 is a likely cause for the observed efficiency gains in the UK futures market.", "B": "The results are confounded by the 2008 global financial crisis, which occurred during the post-automation sample period and dramatically increased market volatility.", "C": "The analysis period overlaps with the peak and collapse of the Dot-com bubble, a significant market regime shift that could be a confounding variable explaining the change in efficiency.", "D": "The study's pre-post comparison fails to account for the introduction of the Euro in 1999, a major structural change in European financial markets that could independently alter price dynamics."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses higher-order reasoning about causal inference and research design. It uses a 'Premise/Assumption Packaging' strategy, asking the user to identify valid critiques of the paper's methodology. This is a 'Reverse-Reasoning' task, requiring the user to think about alternative explanations for an observed outcome. The correct options (A, B) are major, contemporaneous historical events that serve as powerful confounders. Distractor C is a 'True but Irrelevant' fact due to incorrect timing (the sample ends in 2002), while D is thematically and geographically irrelevant, testing for careful consideration of context.", "qid": "204", "question": "### Background\n\n**Research Question.** The paper suggests a causal link between the introduction of an electronic trading (ET) system in 1999 and an observed improvement in market efficiency for FTSE100 futures. However, this simple pre-post empirical design is vulnerable to confounding variables.\n\n**Setting / Data-Generating Environment.** The analysis compares a measure of market inefficiency, `|VR(2) - 1|`, for FTSE100 futures in a period before the 1999 introduction of ET versus a period that includes the post-ET era. A decrease in this metric is interpreted as an efficiency gain caused by the new trading system.\n\n---\n\n### Question\n\nSelect ALL of the following statements that represent plausible confounding factors, challenging the paper's causal claim that the ET system *caused* the observed efficiency improvement.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which factors could plausibly confound the causal claim that the electronic trading (ET) system introduced in 1999 caused the observed efficiency improvement in FTSE100 futures.\n\nLet me evaluate each option:\n\n**Option A: Sarbanes-Oxley Act (2002)**\n- This was US legislation focused on corporate governance and accounting standards\n- It primarily affected US-listed companies\n- The FTSE100 is a UK index, so the direct impact would be minimal\n- This is unlikely to be a plausible confounding factor\n\n**Option B: 2008 Global Financial Crisis**\n- This was a major global event that dramatically affected all financial markets\n- It occurred during the post-ET period\n- Financial crises typically increase volatility and can affect market efficiency measures\n- This is a plausible confounding factor\n\n**Option C: Dot-com bubble (peak and collapse around 1999-2001)**\n- This was a major market regime shift that overlapped with the ET introduction\n- The bubble burst could have independently improved market efficiency as speculative excess was removed\n- The timing coincides closely with the 1999 ET introduction\n- This is a plausible confounding factor\n\n**Option D: Introduction of the Euro (1999)**\n- This was a major structural change in European financial markets\n- It occurred in the same year as the ET introduction\n- It could have affected cross-border trading, liquidity, and price dynamics in European markets including the UK\n- This is a plausible confounding factor\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 261, "Question": "### Background\n\nThis case explores the attitudes of corporate directors towards performance-based remuneration in an insider-dominated governance system. The analysis is based on a survey of 55 directors of listed companies in Taiwan, a market where the \"political risk level... is extremely high.\" Responses are measured on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).\n\n### Data / Model Specification\n\n**Table 1: Factors Affecting Corporate Performance**\n\n| Statement | N | Mean | Median | Mode | SD |\n| :--- | :-: | :-: | :-: | :-: | :-: |\n| (c) The success of companies does not depend on directors' performance. | 55 | 3.20 | 3 | 2 | 1.39 |\n| (d) Directors' remuneration should be linked to share price performance. | 55 | 2.38 | 2 | 2 | 1.19 |\n\n*Note: Scores are on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).* \n\n### Question\n\nAccording to the data in Table 1 and the background context, which of the following statements accurately reflect the directors' views on performance and compensation? Select all that apply.", "Options": {"A": "Directors collectively believe that their own performance is a critical determinant of their company's success.", "B": "Directors strongly oppose linking their personal remuneration to the company's share price performance.", "C": "Directors strongly advocate for stock-based compensation to align their interests with those of shareholders.", "D": "Directors believe that their performance has no significant impact on the success of their companies."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the ability to interpret survey data, including a negatively worded item, to identify a key contradiction in director attitudes. It uses a Reverse-Reasoning strategy. Option A is correct because disagreement with the negative statement (c) (mean 3.20) implies agreement with its positive counterpart. Option B is correct based on the very low mean score for statement (d) (2.38). Distractor C is a conceptual opposite of the correct interpretation of statement (c). Distractor D is a conceptual opposite of the correct interpretation of statement (d). The item forces the candidate to resolve the apparent contradiction that directors accept responsibility for performance but reject financial accountability for it.", "qid": "261", "question": "### Background\n\nThis case explores the attitudes of corporate directors towards performance-based remuneration in an insider-dominated governance system. The analysis is based on a survey of 55 directors of listed companies in Taiwan, a market where the \"political risk level... is extremely high.\" Responses are measured on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).\n\n### Data / Model Specification\n\n**Table 1: Factors Affecting Corporate Performance**\n\n| Statement | N | Mean | Median | Mode | SD |\n| :--- | :-: | :-: | :-: | :-: | :-: |\n| (c) The success of companies does not depend on directors' performance. | 55 | 3.20 | 3 | 2 | 1.39 |\n| (d) Directors' remuneration should be linked to share price performance. | 55 | 2.38 | 2 | 2 | 1.19 |\n\n*Note: Scores are on a 7-point Likert scale (1=strongly disagree, 7=strongly agree).* \n\n### Question\n\nAccording to the data in Table 1 and the background context, which of the following statements accurately reflect the directors' views on performance and compensation? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1:\n\nFor statement (c) \"The success of companies does not depend on directors' performance\":\n- Mean: 3.20 (below the neutral midpoint of 4)\n- Median: 3\n- Mode: 2\nThis indicates directors tend to disagree with this statement, meaning they believe their performance DOES affect company success.\n\nFor statement (d) \"Directors' remuneration should be linked to share price performance\":\n- Mean: 2.38 (well below neutral)\n- Median: 2\n- Mode: 2\nThis shows strong disagreement with linking remuneration to share price.\n\nEvaluating each option:\n\nA) \"Directors collectively believe that their own performance is a critical determinant of their company's success.\"\n- This is correct. Since directors disagree with statement (c), they believe their performance does matter for company success.\n\nB) \"Directors strongly oppose linking their personal remuneration to the company's share price performance.\"\n- This is correct. The low scores (mean 2.38) for statement (d) indicate strong opposition to share price-linked compensation.\n\nC) \"Directors strongly advocate for stock-based compensation...\"\n- This is incorrect. The data shows the opposite - directors disagree with linking pay to share price.\n\nD) \"Directors believe that their performance has no significant impact...\"\n- This is incorrect. The data shows directors disagree with this notion.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 266, "Question": "### Background\n\n**Research Question.** This study investigates the effects of international investment restrictions on asset prices. The premium on a closed-end country fund can be affected by both investment barriers and the ratio of the domestic (U.S.) to foreign market price of risk.\n\n**Setting / Data-Generating Environment.** The paper discusses the October 1987 stock market crash as a natural experiment. This event is viewed as a large, sudden shock that increased the U.S. market price of risk, making U.S. investors more risk-averse and demanding of higher returns on domestic assets.\n\n---\n\n### Question\n\nBased on the paper's logic regarding the effect of relative market prices of risk on fund premiums, select all predicted outcomes of a large, sudden increase in the U.S. market price of risk (such as the October 1987 crash).", "Options": {"A": "The price-to-NAV ratios of funds that initially traded at a large premium (e.g., Korea Fund) and funds that traded at a discount (e.g., U.K. Fund) would both be expected to fall.", "B": "The net asset values (NAVs) of the funds would fall while their U.S. share prices would remain relatively stable, causing the premium to decrease.", "C": "Only funds from countries with high investment restrictions would be affected, as market segmentation is required for relative risk prices to matter.", "D": "An increase in U.S. risk aversion would generally drive down country fund share prices relative to their net asset values, as U.S. investors become less willing to pay a premium to access foreign assets."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a 'Scenario Application' strategy, asking students to apply the paper's theoretical logic about a confounding factor (relative risk prices) to a specific event (the 1987 crash). Option A correctly identifies the mechanism. Option B correctly applies this mechanism to different types of funds. Distractor C is a 'Conceptual Opposite' regarding the mechanics of the price change (the shock is to U.S. investors, so the U.S. price is the primary mover). Distractor D is 'Almost Right' but incorrect; while segmentation is the backdrop, the shock to relative risk prices would affect all funds, not just those from restricted markets.", "qid": "266", "question": "### Background\n\n**Research Question.** This study investigates the effects of international investment restrictions on asset prices. The premium on a closed-end country fund can be affected by both investment barriers and the ratio of the domestic (U.S.) to foreign market price of risk.\n\n**Setting / Data-Generating Environment.** The paper discusses the October 1987 stock market crash as a natural experiment. This event is viewed as a large, sudden shock that increased the U.S. market price of risk, making U.S. investors more risk-averse and demanding of higher returns on domestic assets.\n\n---\n\n### Question\n\nBased on the paper's logic regarding the effect of relative market prices of risk on fund premiums, select all predicted outcomes of a large, sudden increase in the U.S. market price of risk (such as the October 1987 crash).", "model_evaluation_result": {"model_raw_response": "Looking at this question about the effects of international investment restrictions on asset prices, I need to analyze how a sudden increase in the U.S. market price of risk (like the October 1987 crash) would affect closed-end country fund premiums.\n\nLet me evaluate each option:\n\n**Option A**: This predicts that both funds trading at premiums (Korea Fund) and funds trading at discounts (U.K. Fund) would see their price-to-NAV ratios fall. This makes sense because when U.S. investors become more risk-averse, they would demand higher returns on all risky assets, driving down the prices of country fund shares relative to their NAVs, regardless of whether they initially traded at premiums or discounts.\n\n**Option B**: This suggests NAVs would fall while U.S. share prices remain stable. This is incorrect. During a market crash, we'd expect U.S. share prices to fall significantly as risk-averse investors sell off risky assets. The NAVs might also fall if foreign markets are affected, but the key mechanism is that U.S. share prices would likely fall more than NAVs.\n\n**Option C**: This claims only funds from countries with high investment restrictions would be affected. This is too restrictive. While the effect might be stronger for funds from countries with investment restrictions, the increase in U.S. risk aversion would affect all country funds to some degree, as U.S. investors would demand higher returns on all foreign investments.\n\n**Option D**: This correctly states that increased U.S. risk aversion would drive down country fund share prices relative to their NAVs, as U.S. investors become less willing to pay premiums for foreign asset access. This is the fundamental mechanism at work.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 304, "Question": "### Background\n\n**Research Question.** What determines the long-run behavior of credibility forecasts, and under what conditions might a forecast fail to become fully reliant on observed data, even with a large sample size?\n\n**Setting.** In the multidimensional credibility framework, the forecast `\\mathbf{f}(\\mathcal{D})` depends on the credibility matrix `\\mathbf{Z}`. The asymptotic behavior of `\\mathbf{Z}` as the sample size `n \\to \\infty` determines whether the forecast becomes \"fully credible\" (i.e., depends only on the sample statistics `\\mathbf{t}(\\mathcal{D})`).\n\n**Variables and Parameters.**\n- `\\mathbf{Z}`: The 3x3 credibility matrix.\n- `\\mathbf{D}`: The 3x3 between-risk covariance matrix, `\\mathbf{D} = \\mathcal{C}\\langle\\mathbf{m}(\\tilde{\\theta}); \\mathbf{m}(\\tilde{\\theta})\\rangle`.\n- `\\mathbf{E}(n)`: The 3x3 within-risk covariance matrix.\n- `\\mathbf{m}(\\theta) = [m_1(\\theta), m_2(\\theta), m_1(\\theta)^2]'`: The vector of conditional moments.\n\n---\n\n### Data / Model Specification\n\nAssuming the between-risk matrix `\\mathbf{D}` is invertible, the credibility matrix `\\mathbf{Z}` can be written as:\n\n  \n\\mathbf{Z} = n(n\\mathbf{I} + \\mathbf{N}(n))^{-1}, \\quad \\text{where } \\mathbf{N}(n) = \\mathbf{E}(n)\\mathbf{D}^{-1} \\quad \\text{(Eq. (1))}\n \n\nHowever, if `\\mathbf{D}` is singular (not invertible), this expression is invalid. This occurs if there is a linear dependency among the components of the conditional mean vector `\\mathbf{m}(\\theta)`. For example, for a Poisson(`\\pi`) likelihood, `m_2(\\pi) = m_1(\\pi) + m_1(\\pi)^2`.\n\n---\n\n### Question\n\nBased on the information provided about the asymptotic properties of credibility forecasts, select all of the following statements that are true.", "Options": {"A": "If the between-risk matrix `\\mathbf{D}` is invertible, the credibility matrix `\\mathbf{Z}` converges to the identity matrix `\\mathbf{I}` as `n \\to \\infty`, meaning the forecast becomes \"fully credible\" and depends only on the sample statistics `\\mathbf{t}(\\mathcal{D})`.", "B": "When `\\mathbf{D}` is singular, the forecast `\\mathbf{f}(\\mathcal{D})` fails to become fully credible because it remains dependent on the prior mean vector `\\mathbf{m}` even as `n \\to \\infty`.", "C": "The `\\mathbf{D}` matrix becomes singular if there is a perfect linear relationship among the conditional moments (e.g., `m_2(\\theta) = m_1(\\theta) + m_{11}(\\theta)`), because this implies a linear combination of the components of the random vector `\\mathbf{m}(\\tilde{\\theta})` has zero variance.", "D": "A \"fully credible\" forecast is one where the mean-squared forecast error matrix `\\mathbf{\\Psi}` becomes zero, indicating a perfect prediction of the future observation with no uncertainty."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item tests understanding of the conditions for and implications of the asymptotic behavior of credibility forecasts. The strategy is premise/assumption packaging, presenting complex statements about the model's long-run properties. Option C is a sophisticated 'Almost Right' distractor that misidentifies why the forecast fails full credibility in the singular case. Option D is a 'Conceptual' distractor that confuses the elimination of estimation risk with the elimination of total forecast error (process risk).", "qid": "304", "question": "### Background\n\n**Research Question.** What determines the long-run behavior of credibility forecasts, and under what conditions might a forecast fail to become fully reliant on observed data, even with a large sample size?\n\n**Setting.** In the multidimensional credibility framework, the forecast `\\mathbf{f}(\\mathcal{D})` depends on the credibility matrix `\\mathbf{Z}`. The asymptotic behavior of `\\mathbf{Z}` as the sample size `n \\to \\infty` determines whether the forecast becomes \"fully credible\" (i.e., depends only on the sample statistics `\\mathbf{t}(\\mathcal{D})`).\n\n**Variables and Parameters.**\n- `\\mathbf{Z}`: The 3x3 credibility matrix.\n- `\\mathbf{D}`: The 3x3 between-risk covariance matrix, `\\mathbf{D} = \\mathcal{C}\\langle\\mathbf{m}(\\tilde{\\theta}); \\mathbf{m}(\\tilde{\\theta})\\rangle`.\n- `\\mathbf{E}(n)`: The 3x3 within-risk covariance matrix.\n- `\\mathbf{m}(\\theta) = [m_1(\\theta), m_2(\\theta), m_1(\\theta)^2]'`: The vector of conditional moments.\n\n---\n\n### Data / Model Specification\n\nAssuming the between-risk matrix `\\mathbf{D}` is invertible, the credibility matrix `\\mathbf{Z}` can be written as:\n\n  \n\\mathbf{Z} = n(n\\mathbf{I} + \\mathbf{N}(n))^{-1}, \\quad \\text{where } \\mathbf{N}(n) = \\mathbf{E}(n)\\mathbf{D}^{-1} \\quad \\text{(Eq. (1))}\n \n\nHowever, if `\\mathbf{D}` is singular (not invertible), this expression is invalid. This occurs if there is a linear dependency among the components of the conditional mean vector `\\mathbf{m}(\\theta)`. For example, for a Poisson(`\\pi`) likelihood, `m_2(\\pi) = m_1(\\pi) + m_1(\\pi)^2`.\n\n---\n\n### Question\n\nBased on the information provided about the asymptotic properties of credibility forecasts, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\n**Statement A**: When D is invertible, from Eq. (1), Z = n(nI + N(n))^(-1). As n → ∞, we can factor out n: Z = n(nI + N(n))^(-1) = (I + N(n)/n)^(-1). Since N(n) = E(n)D^(-1) and E(n) is finite (within-risk covariance), N(n)/n → 0 as n → ∞. Therefore, Z → I, making the forecast fully credible. **TRUE**\n\n**Statement B**: When D is singular, D^(-1) doesn't exist, so Eq. (1) is invalid. The credibility matrix Z cannot converge to I, and the forecast cannot become fully credible. It will remain dependent on prior information even with large n. **TRUE**\n\n**Statement C**: If there's a perfect linear relationship among conditional moments (like m₂(θ) = m₁(θ) + m₁₁(θ)), then there exists a non-zero vector c such that c'·m(θ) is constant for all θ. This means Var(c'·m(θ̃)) = 0, which implies c'Dc = 0 for non-zero c, making D singular. **TRUE**\n\n**Statement D**: A \"fully credible\" forecast means Z → I, so the forecast relies entirely on sample statistics. However, this doesn't mean the mean-squared forecast error Ψ becomes zero. Even with perfect credibility, there's still inherent uncertainty in predicting future observations due to the random nature of the data. **FALSE**\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 10, "Question": "### Background\n\n**Research Question.** How can joint statistical tests distinguish between predictors whose apparent long-horizon success is a statistical artifact of their own persistence, and those that exhibit genuine predictive power for stock returns?\n\n**Setting and Environment.** This case examines the empirical results from regressing U.S. stock returns on two different predictors over multiple horizons (1-5 years) for the period 1926–2004. The analysis contrasts a traditional, highly persistent predictor (log dividend yield) with a more modern, less persistent one (log net payout yield).\n\n### Data / Model Specification\n\nThe following table synthesizes empirical findings for the two predictors. The joint test evaluates the null hypothesis `H₀: β₁ = β₂ = β₃ = β₄ = β₅ = 0`.\n\n**Table 1: Empirical Results for Two Predictors (1926-2004)**\n\n| Predictor / Statistic | Horizon 1 | Horizon 2 | Horizon 3 | Horizon 4 | Horizon 5 | Joint Test |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **Log Dividend Yield** | | | | | | |\n| `β̂_j` | 0.131 | 0.257 | 0.390 | 0.461 | 0.521 | |\n| `R_j²` (%) | 5.16 | 9.55 | 15.84 | 18.14 | 20.76 | |\n| `β̂_j / β̂₁` | 1.00 | 1.96 | 2.98 | 3.53 | 3.99 | |\n| Persistence `ρ̂₁c` | 0.953 | | | | | |\n| **Wald (Sim. p-value)** | | | | | | **0.293** |\n| **Log Net Payout Yield** | | | | | | |\n| `β̂_j` | 0.718 | 1.321 | 1.536 | 1.537 | 1.512 | |\n| `R_j²` (%) | 23.40 | 37.99 | 36.89 | 30.25 | 26.25 | |\n| Persistence `ρ̂₁c` | 0.713 | | | | | |\n| **Wald (Sim. p-value)** | | | | | | **0.001** |\n\n### Question\n\nThe table presents empirical results for two predictors: the highly persistent Log Dividend Yield and the less persistent Log Net Payout Yield. According to the paper's framework, which of the following interpretations are valid? Select all that apply.", "Options": {"A": "Any predictor with high persistence like the Log Dividend Yield (ρ=0.953) will fail to reject the joint Wald test, regardless of its underlying predictive power.", "B": "The results provide stronger evidence that the Log Net Payout Yield, rather than the Log Dividend Yield, represents a genuine priced risk factor in the context of asset pricing models.", "C": "The joint Wald test for the Log Dividend Yield is not statistically significant (p=0.293) because its monotonically increasing coefficient ratios are almost identical to the spurious pattern predicted by the null hypothesis for a predictor with high persistence (ρ=0.953).", "D": "The Log Net Payout Yield is identified as a genuine predictor because its non-monotonic R² pattern is highly inconsistent with the pattern expected under the null, leading to a significant joint test (p=0.001)."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the application of the paper's joint testing framework to contrast a spurious predictor with a genuine one. It uses the Atomic Decomposition strategy, converting the three parts of the original QA into three correct, independent statements (A, B, D). The distractor (C) is a high-fidelity 'Almost Right' option that presents a common oversimplification of the paper's nuanced argument about patterns, not just persistence.", "qid": "10", "question": "### Background\n\n**Research Question.** How can joint statistical tests distinguish between predictors whose apparent long-horizon success is a statistical artifact of their own persistence, and those that exhibit genuine predictive power for stock returns?\n\n**Setting and Environment.** This case examines the empirical results from regressing U.S. stock returns on two different predictors over multiple horizons (1-5 years) for the period 1926–2004. The analysis contrasts a traditional, highly persistent predictor (log dividend yield) with a more modern, less persistent one (log net payout yield).\n\n### Data / Model Specification\n\nThe following table synthesizes empirical findings for the two predictors. The joint test evaluates the null hypothesis `H₀: β₁ = β₂ = β₃ = β₄ = β₅ = 0`.\n\n**Table 1: Empirical Results for Two Predictors (1926-2004)**\n\n| Predictor / Statistic | Horizon 1 | Horizon 2 | Horizon 3 | Horizon 4 | Horizon 5 | Joint Test |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **Log Dividend Yield** | | | | | | |\n| `β̂_j` | 0.131 | 0.257 | 0.390 | 0.461 | 0.521 | |\n| `R_j²` (%) | 5.16 | 9.55 | 15.84 | 18.14 | 20.76 | |\n| `β̂_j / β̂₁` | 1.00 | 1.96 | 2.98 | 3.53 | 3.99 | |\n| Persistence `ρ̂₁c` | 0.953 | | | | | |\n| **Wald (Sim. p-value)** | | | | | | **0.293** |\n| **Log Net Payout Yield** | | | | | | |\n| `β̂_j` | 0.718 | 1.321 | 1.536 | 1.537 | 1.512 | |\n| `R_j²` (%) | 23.40 | 37.99 | 36.89 | 30.25 | 26.25 | |\n| Persistence `ρ̂₁c` | 0.713 | | | | | |\n| **Wald (Sim. p-value)** | | | | | | **0.001** |\n\n### Question\n\nThe table presents empirical results for two predictors: the highly persistent Log Dividend Yield and the less persistent Log Net Payout Yield. According to the paper's framework, which of the following interpretations are valid? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze the empirical results and their interpretation in the context of distinguishing between spurious and genuine predictors.\n\nLet me examine each option:\n\n**Option A**: \"Any predictor with high persistence like the Log Dividend Yield (ρ=0.953) will fail to reject the joint Wald test, regardless of its underlying predictive power.\"\n\nThis is too absolute. While high persistence makes it harder to reject the null (because the spurious pattern under the null mimics what might appear to be predictive power), it doesn't guarantee failure to reject if there's strong genuine predictive power. The statement \"regardless of its underlying predictive power\" makes this incorrect.\n\n**Option B**: \"The results provide stronger evidence that the Log Net Payout Yield, rather than the Log Dividend Yield, represents a genuine priced risk factor in the context of asset pricing models.\"\n\nThe joint test results support this interpretation:\n- Log Net Payout Yield: p=0.001 (highly significant, rejecting the null)\n- Log Dividend Yield: p=0.293 (not significant, failing to reject the null)\nThis provides stronger statistical evidence for the Log Net Payout Yield being a genuine predictor.\n\n**Option C**: \"The joint Wald test for the Log Dividend Yield is not statistically significant (p=0.293) because its monotonically increasing coefficient ratios are almost identical to the spurious pattern predicted by the null hypothesis for a predictor with high persistence (ρ=0.953).\"\n\nThis accurately describes the results. The coefficient ratios (1.00, 1.96, 2.98, 3.53, 3.99) show a monotonic increase that closely matches what would be expected under the null for a highly persistent predictor. This is why the test fails to reject (p=0.293).\n\n**Option D**: \"The Log Net Payout Yield is identified as a genuine predictor because its non-monotonic R² pattern is highly inconsistent with the pattern expected under the null, leading to a significant joint test (p=0.001).\"\n\nThe R² values (23.40%, 37.99%, 36.89%, 30.25%, 26.25%) indeed show a non-monotonic pattern", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 71, "Question": "### Background\n\n**Research Question.** How can the performance of Bonus-Malus Systems (BMSs) be evaluated, and what are the practical limitations of common evaluation metrics, particularly when comparing systems of varying complexity?\n\n**Setting / Data-Generating Environment.** The analysis considers two key performance dimensions of a BMS. The first is its ability to differentiate between high- and low-risk drivers in its steady state, measured by elasticity. The second is the time it takes for the system to reach this steady state, measured by the rate of convergence.\n\n**Variables & Parameters.**\n- `λ`: A policyholder's individual claim frequency (claims per year).\n- `P(λ)`: The stationary average premium for a driver with frequency `λ` (currency units).\n- `η(λ)`: The elasticity of the mean stationary premium with respect to the claim frequency (dimensionless).\n- `n`: Number of years (transitions) since the policyholder entered the system.\n- `p_{ij}^n(λ)`: The probability of moving from a starting class `i` to class `j` in exactly `n` years (dimensionless).\n- `(TV)_n`: The total variation distance after `n` years, measuring the distance from the steady-state distribution (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Elasticity**\nThe elasticity `η(λ)` measures the percentage change in the long-run average premium `P(λ)` for a one percent change in the underlying risk `λ`. It is a steady-state measure of risk differentiation.\n\n  \nη(λ) = \\frac{d P(λ)/P(λ)}{dλ/λ} = \\frac{d \\ln P(λ)}{d \\ln λ} \\quad \\text{(Eq. (1))}\n \nA system is considered effective at differentiating risk if its elasticity is close to 1. Most real-world systems have `η(λ) < 1`, implying good drivers subsidize bad drivers.\n\n**Rate of Convergence**\nThe speed at which a BMS approaches its steady state can be measured by tracking the total variation distance `(TV)_n` over time. A value near 2 indicates the system is far from its steady state, while a value of 0 indicates it has fully converged.\n\n**Table 1: Total Variation for Four Systems (Starting from Entry Class, λ=0.10)**\n| Years (n) | Belgium | Japan | Taiwan | Switzerland |\n|:---------:|:-------:|:------:|:------:|:-----------:|\n| 0         | 1.9913  | 1.9950 | 2.000  | 1.9742      |\n| 10        | 1.7769  | 1.1551 | 0      | 1.0124      |\n| 20        | 0.9120  | 0.3217 | 0      | 0.3541      |\n| 30        | 0.4209  | 0.0529 | 0      | 0.1348      |\n| 60        | 0.0382  | 0.0007 | 0      | 0.0061      |\n\n---\n\n### Question\n\nBased on the provided information, select all statements that are correct interpretations of BMS performance metrics.", "Options": {"A": "Based on Table 1, the Belgian system converges to its steady state much more slowly than the Japanese system.", "B": "According to Table 1, the Taiwanese system takes more than 20 years to fully stabilize.", "C": "A low elasticity value (e.g., `η(λ) < 1`) indicates that cross-subsidization from bad drivers to good drivers is minimal.", "D": "An elasticity of `η(λ) = 0.4` implies that a driver who is 20% riskier (higher `λ`) will pay approximately 8% more in long-run average premiums."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret two key BMS performance metrics: elasticity (a measure of risk differentiation) and rate of convergence (a measure of speed to stationarity). It uses an Atomic Decomposition strategy, breaking down the original QA's complex synthesis into distinct, verifiable statements. Distractor B is a direct misreading of the provided table (Numerical Misreading). Distractor D represents a fundamental misunderstanding of elasticity's implication for cross-subsidization (Conceptual Opposite).", "qid": "71", "question": "### Background\n\n**Research Question.** How can the performance of Bonus-Malus Systems (BMSs) be evaluated, and what are the practical limitations of common evaluation metrics, particularly when comparing systems of varying complexity?\n\n**Setting / Data-Generating Environment.** The analysis considers two key performance dimensions of a BMS. The first is its ability to differentiate between high- and low-risk drivers in its steady state, measured by elasticity. The second is the time it takes for the system to reach this steady state, measured by the rate of convergence.\n\n**Variables & Parameters.**\n- `λ`: A policyholder's individual claim frequency (claims per year).\n- `P(λ)`: The stationary average premium for a driver with frequency `λ` (currency units).\n- `η(λ)`: The elasticity of the mean stationary premium with respect to the claim frequency (dimensionless).\n- `n`: Number of years (transitions) since the policyholder entered the system.\n- `p_{ij}^n(λ)`: The probability of moving from a starting class `i` to class `j` in exactly `n` years (dimensionless).\n- `(TV)_n`: The total variation distance after `n` years, measuring the distance from the steady-state distribution (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Elasticity**\nThe elasticity `η(λ)` measures the percentage change in the long-run average premium `P(λ)` for a one percent change in the underlying risk `λ`. It is a steady-state measure of risk differentiation.\n\n  \nη(λ) = \\frac{d P(λ)/P(λ)}{dλ/λ} = \\frac{d \\ln P(λ)}{d \\ln λ} \\quad \\text{(Eq. (1))}\n \nA system is considered effective at differentiating risk if its elasticity is close to 1. Most real-world systems have `η(λ) < 1`, implying good drivers subsidize bad drivers.\n\n**Rate of Convergence**\nThe speed at which a BMS approaches its steady state can be measured by tracking the total variation distance `(TV)_n` over time. A value near 2 indicates the system is far from its steady state, while a value of 0 indicates it has fully converged.\n\n**Table 1: Total Variation for Four Systems (Starting from Entry Class, λ=0.10)**\n| Years (n) | Belgium | Japan | Taiwan | Switzerland |\n|:---------:|:-------:|:------:|:------:|:-----------:|\n| 0         | 1.9913  | 1.9950 | 2.000  | 1.9742      |\n| 10        | 1.7769  | 1.1551 | 0      | 1.0124      |\n| 20        | 0.9120  | 0.3217 | 0      | 0.3541      |\n| 30        | 0.4209  | 0.0529 | 0      | 0.1348      |\n| 60        | 0.0382  | 0.0007 | 0      | 0.0061      |\n\n---\n\n### Question\n\nBased on the provided information, select all statements that are correct interpretations of BMS performance metrics.", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) Based on Table 1, the Belgian system converges to its steady state much more slowly than the Japanese system.\n- At year 10: Belgium TV = 1.7769, Japan TV = 1.1551\n- At year 20: Belgium TV = 0.9120, Japan TV = 0.3217\n- At year 30: Belgium TV = 0.4209, Japan TV = 0.0529\n- Belgium consistently has higher TV values, meaning it's further from steady state\n- This statement is CORRECT\n\nB) According to Table 1, the Taiwanese system takes more than 20 years to fully stabilize.\n- Taiwan shows TV = 0 at years 10, 20, 30, and 60\n- This means it reached steady state by year 10 or earlier\n- This statement is INCORRECT\n\nC) A low elasticity value (e.g., η(λ) < 1) indicates that cross-subsidization from bad drivers to good drivers is minimal.\n- The text states: \"Most real-world systems have η(λ) < 1, implying good drivers subsidize bad drivers\"\n- Low elasticity means MORE cross-subsidization, not minimal\n- This statement is INCORRECT\n\nD) An elasticity of η(λ) = 0.4 implies that a driver who is 20% riskier (higher λ) will pay approximately 8% more in long-run average premiums.\n- Elasticity η = (% change in premium)/(% change in risk)\n- 0.4 = (% change in premium)/20%\n- % change in premium = 0.4 × 20% = 8%\n- This statement is CORRECT\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 131, "Question": "### Background\n\nThe study uses a one-way analysis of variance to measure the difference in abnormal trading volume between 'winner' and 'loser' stocks. This helps disentangle the baseline effects of non-tax motives from the seasonal effects of tax-motivated trading.\n\n### Data / Model Specification\n\nThe following cross-sectional regression is run for each month:\n  \nAV_{i} = \\alpha + \\beta DN_{i} + e_{i} \\quad \\text{(Eq. 1)}\n \nwhere `AV_i` is the abnormal volume (in percent per month), and `DN_i` is a binary variable equal to 1 for 'winners' and 0 for 'losers'. Thus, `α` represents the average abnormal volume for losers, and `α+β` represents the average for winners.\n\nThe table below summarizes the average abnormal volumes for NYSE winners and losers, using an 11-month lookback period.\n\n**Table 1: Average Abnormal Volume (%) for NYSE Winners and Losers (N=11)**\n| Group | Jan | Feb-Oct | Dec |\n| :--- | :--- | :--- | :--- |\n| Winners (`α+β`) | 0.48 | 0.32 | 0.12 |\n| Losers (`α`) | -0.37 | -0.37 | -0.16 |\n\n---\n\nBased on the model and data, which of the following conclusions are supported by the evidence?\n\nSelect all that apply.", "Options": {"A": "The difference in abnormal volume between winners and losers (the 'winner-loser gap') shrinks from 0.69% in the baseline Feb-Oct period to 0.28% in December.", "B": "The shrinkage of the winner-loser gap in December is driven by both an increase in the abnormal volume of losers and a decrease in the abnormal volume of winners, relative to the baseline period.", "C": "The winner-loser gap in January (0.85%) is wider than the baseline gap, consistent with the 'present value motive' where investors postpone realizing gains until the new tax year.", "D": "The estimated value of the `β` coefficient from Eq. (1) for the December period is 0.28%."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item tests the ability to calculate and interpret the key metric—the winner-loser volume gap—across different seasonal periods. It requires understanding how changes in this gap provide evidence for the paper's main hypotheses. Depth Strategy: Computational Judgment. Candidates must calculate the gap for each period and compare them to assess the statements. Distractor Logic: A requires two calculations: Feb-Oct (0.32 - (-0.37) = 0.69) and Dec (0.12 - (-0.16) = 0.28). B requires comparing the components: Loser volume rises (-0.37 to -0.16) and Winner volume falls (0.32 to 0.12). C requires calculating the Jan gap (0.48 - (-0.37) = 0.85) and comparing it to the baseline. D tests the interpretation of β as the difference, which is the same as the gap calculated in A for December. All options are correct and test different facets of the same core result.", "qid": "131", "question": "### Background\n\nThe study uses a one-way analysis of variance to measure the difference in abnormal trading volume between 'winner' and 'loser' stocks. This helps disentangle the baseline effects of non-tax motives from the seasonal effects of tax-motivated trading.\n\n### Data / Model Specification\n\nThe following cross-sectional regression is run for each month:\n  \nAV_{i} = \\alpha + \\beta DN_{i} + e_{i} \\quad \\text{(Eq. 1)}\n \nwhere `AV_i` is the abnormal volume (in percent per month), and `DN_i` is a binary variable equal to 1 for 'winners' and 0 for 'losers'. Thus, `α` represents the average abnormal volume for losers, and `α+β` represents the average for winners.\n\nThe table below summarizes the average abnormal volumes for NYSE winners and losers, using an 11-month lookback period.\n\n**Table 1: Average Abnormal Volume (%) for NYSE Winners and Losers (N=11)**\n| Group | Jan | Feb-Oct | Dec |\n| :--- | :--- | :--- | :--- |\n| Winners (`α+β`) | 0.48 | 0.32 | 0.12 |\n| Losers (`α`) | -0.37 | -0.37 | -0.16 |\n\n---\n\nBased on the model and data, which of the following conclusions are supported by the evidence?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Let me analyze each option based on the data provided.\n\nFirst, I'll calculate the winner-loser gaps (β values) for each period:\n- Jan: 0.48 - (-0.37) = 0.85%\n- Feb-Oct: 0.32 - (-0.37) = 0.69%\n- Dec: 0.12 - (-0.16) = 0.28%\n\nNow examining each option:\n\n**Option A**: The winner-loser gap shrinks from 0.69% (Feb-Oct) to 0.28% (Dec).\n- Feb-Oct gap: 0.32 - (-0.37) = 0.69% ✓\n- Dec gap: 0.12 - (-0.16) = 0.28% ✓\n- This is correct.\n\n**Option B**: The December shrinkage is driven by both an increase in losers' abnormal volume and a decrease in winners' abnormal volume.\n- Losers: -0.37% (Feb-Oct) → -0.16% (Dec) = increase of 0.21% ✓\n- Winners: 0.32% (Feb-Oct) → 0.12% (Dec) = decrease of 0.20% ✓\n- This is correct.\n\n**Option C**: The January gap (0.85%) is wider than the baseline gap (0.69%).\n- Jan gap: 0.48 - (-0.37) = 0.85% ✓\n- Baseline gap: 0.69% ✓\n- 0.85% > 0.69% ✓\n- This is correct.\n\n**Option D**: The β coefficient for December is 0.28%.\n- β = (α + β) - α = winners - losers\n- December β = 0.12 - (-0.16) = 0.28% ✓\n- This is correct.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 281, "Question": "### Background\n\n**Research Question.** The study seeks to explain why the aggregate exchange rate pass-through to investment good prices is modest, despite being very high in certain key industrial sectors.\n\n**Setting / Data-Generating Environment.** The analysis uses panel data for 22 industrial sectors. The paper's key insight is that the aggregate effect is a composite of highly heterogeneous sector-level effects.\n\n---\n\n### Data / Model Specification\n\nThe paper reports the following statistically significant pass-through elasticities from its baseline model:\n\n**Table 1: Selected Pass-Through Elasticities (`\\hat{\\alpha}_{i1}`)**\n\n| Industry                               | Pass-Through Elasticity |\n| :------------------------------------- | :---------------------- |\n| Total Industries                       | 0.1225                  |\n| Transport, storage and communication   | 0.3151                  |\n| Finance, insurance, real estate, etc.  | 0.0242 (insignificant)  |\n\nAdditionally, the paper notes that the 'Finance' sector has a very large share of total investment in the economy (34.1%).\n\n---\n\n### Question\n\nGiven these findings, which of the following conclusions or explanations are valid?", "Options": {"A": "The aggregate pass-through for 'Total Industries' can be formally expressed as an investment-weighted average of the individual sector elasticities.", "B": "The 'Transport' sector's high pass-through is the primary driver of the aggregate 'Total Industries' result.", "C": "A policy-induced currency depreciation is unlikely to act as an industrial strategy because the aggregate effect on investment costs is small (1.23% for a 10% depreciation).", "D": "The low aggregate pass-through is heavily influenced by the 'Finance' sector, which combines a very low pass-through elasticity with a very large weight in total economy-wide investment."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize micro-level results to explain a macro-level finding, a key reasoning step in the original QA problem. It uses a 'Reverse-Reasoning' strategy. Option A correctly states the aggregation principle. Option B correctly applies this principle to explain the specific empirical puzzle presented in the stem. Distractor C is a 'Conceptual Opposite', as the Transport sector's high pass-through is offset by other sectors, it does not drive the low aggregate result. Distractor D contradicts the paper's main policy conclusion; the small aggregate effect masks large, heterogeneous sectoral effects, which is precisely why it *does* act as an industrial strategy.", "qid": "281", "question": "### Background\n\n**Research Question.** The study seeks to explain why the aggregate exchange rate pass-through to investment good prices is modest, despite being very high in certain key industrial sectors.\n\n**Setting / Data-Generating Environment.** The analysis uses panel data for 22 industrial sectors. The paper's key insight is that the aggregate effect is a composite of highly heterogeneous sector-level effects.\n\n---\n\n### Data / Model Specification\n\nThe paper reports the following statistically significant pass-through elasticities from its baseline model:\n\n**Table 1: Selected Pass-Through Elasticities (`\\hat{\\alpha}_{i1}`)**\n\n| Industry                               | Pass-Through Elasticity |\n| :------------------------------------- | :---------------------- |\n| Total Industries                       | 0.1225                  |\n| Transport, storage and communication   | 0.3151                  |\n| Finance, insurance, real estate, etc.  | 0.0242 (insignificant)  |\n\nAdditionally, the paper notes that the 'Finance' sector has a very large share of total investment in the economy (34.1%).\n\n---\n\n### Question\n\nGiven these findings, which of the following conclusions or explanations are valid?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each option based on the data provided.\n\nOption A: The aggregate pass-through being an investment-weighted average of sector elasticities is a standard economic principle. The paper explicitly mentions that \"the aggregate effect is a composite of highly heterogeneous sector-level effects,\" which supports this interpretation. This is valid.\n\nOption B: While Transport has a high pass-through (0.3151), there's no information about its investment share. Without knowing its weight in total investment, we cannot conclude it's the primary driver of the aggregate result. This is not valid.\n\nOption C: A 10% depreciation would lead to approximately 1.23% increase in aggregate investment good prices (0.1225 × 10%). This is indeed a small effect, supporting the conclusion that currency depreciation would have limited impact as an industrial strategy tool. This is valid.\n\nOption D: The Finance sector has both a very low pass-through (0.0242, insignificant) and a very large investment share (34.1%). When calculating a weighted average, this combination would significantly pull down the aggregate pass-through, explaining why it's much lower than sectors like Transport. This is valid.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 318, "Question": "### Background\n\nThe study examines two FTSE 350 UK utilities operating in a sector where a five-yearly regulatory review constrains profit potential in the core regulated business. This context forces firms to make distinct strategic choices about whether to focus on the regulated core or diversify into higher-risk, unregulated businesses.\n\n### Data / Model Specification\n\nAccording to contingency theory, remuneration policy should be tailored to corporate strategy. The following evidence was gathered:\n\n> **Fact 1:** The two utilities studied, despite being in the same industry, adopted very different remuneration policies that aligned with their distinct strategic aims.\n\n> **Evidence (HR Director 1):** \"He pointed out that the core business, being a utility, was relatively riskfree... and so the remuneration packages had to reflect that. However, he noted that the risk was much higher in some of the group’s unregulated activities, and this part of the business merited a different reward structure.\"\n\n### Question\n\nBased on contingency theory and the provided evidence, which of the following remuneration design choices would be considered rational and strategically aligned for a utility?\n\nSelect all that apply.", "Options": {"A": "For a utility focused on diversification into high-risk unregulated markets, a high-leverage pay structure with a large stock option component is appropriate to incentivize entrepreneurial risk-taking.", "B": "For a utility focused exclusively on its stable, price-capped regulated business, a high-leverage pay structure with a large stock option component is appropriate to maximize shareholder returns.", "C": "For a utility focused on diversification into high-risk unregulated markets, a low-leverage pay structure with a high proportion of fixed salary is appropriate to ensure executive retention.", "D": "For a utility focused exclusively on its stable, price-capped regulated business, a low-leverage pay structure with a high proportion of fixed salary and bonuses tied to operational efficiency is appropriate."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the ability to apply contingency theory to the specific scenarios described in the paper. It uses a 'Scenario Application' strategy. The correct options (A, C) represent the two distinct, logically consistent pairings of strategy and remuneration structure discussed in the source material (high-risk/high-leverage and low-risk/low-leverage). The distractors (B, D) present mismatched pairings that violate the logic of contingency theory and the HR Director's statement, representing plausible 'Formula Misuse' or conceptual errors.", "qid": "318", "question": "### Background\n\nThe study examines two FTSE 350 UK utilities operating in a sector where a five-yearly regulatory review constrains profit potential in the core regulated business. This context forces firms to make distinct strategic choices about whether to focus on the regulated core or diversify into higher-risk, unregulated businesses.\n\n### Data / Model Specification\n\nAccording to contingency theory, remuneration policy should be tailored to corporate strategy. The following evidence was gathered:\n\n> **Fact 1:** The two utilities studied, despite being in the same industry, adopted very different remuneration policies that aligned with their distinct strategic aims.\n\n> **Evidence (HR Director 1):** \"He pointed out that the core business, being a utility, was relatively riskfree... and so the remuneration packages had to reflect that. However, he noted that the risk was much higher in some of the group’s unregulated activities, and this part of the business merited a different reward structure.\"\n\n### Question\n\nBased on contingency theory and the provided evidence, which of the following remuneration design choices would be considered rational and strategically aligned for a utility?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question through the lens of contingency theory, I need to identify which remuneration designs align with each strategic choice.\n\nThe key insight from the HR Director is that regulated utility businesses are \"relatively risk-free\" while unregulated activities have \"much higher\" risk and merit \"a different reward structure.\"\n\nAccording to contingency theory, remuneration should match the risk profile and strategic objectives:\n\n**For diversified utilities (high-risk unregulated markets):**\n- Need to incentivize entrepreneurial risk-taking\n- Executives should share in both upside potential and downside risk\n- High-leverage pay (large variable/equity component) aligns interests with shareholders in risky ventures\n\n**For focused utilities (stable regulated business only):**\n- Operating in low-risk, stable environment with capped profits\n- Less need for risk-taking incentives\n- Low-leverage pay (high fixed salary proportion) matches the stable, predictable nature\n- Bonuses tied to operational efficiency make sense given profit constraints\n\nAnalyzing each option:\n- A: Correct - High-leverage pay for high-risk strategy\n- B: Incorrect - Mismatches high-leverage pay with low-risk regulated business\n- C: Incorrect - Mismatches low-leverage pay with high-risk strategy\n- D: Correct - Low-leverage pay for low-risk regulated business\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 347, "Question": "### Background\n\n**Research Question.** This case explores potential econometric biases in predictive regressions and appropriate methods to address them.\n\n**Setting and Sample.** An econometrician is concerned about potential biases when estimating the predictive regression `r_{t+1} = α + β_1 F_{1,t}^{\\text{short}} + ε_{t+1}` using Ordinary Least Squares (OLS), where `F_{1,t}^{\\text{short}}` is a persistent financial time series predictor.\n\n### Data / Model Specification\n\nThe predictive regression model is:\n  \nr_{t+1} = \\alpha + \\beta_1 F_{1,t}^{\\text{short}} + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \nThe predictor `F_{1,t}^{\\text{short}}` is known to be persistent (highly autocorrelated).\n\n### Question\n\nSelect all statements that accurately describe a potential econometric problem in this setting and a valid methodological response.", "Options": {"A": "A valid approach to mitigate this bias is to use an instrumental variables (IV) framework, for instance, by using the lagged predictor `F_{1,t-1}^{\\text{short}}` as an instrument for `F_{1,t}^{\\text{short}}`.", "B": "A key concern is the Stambaugh (1999) bias, which can affect the OLS estimate of `β_1` if the predictor is persistent and its innovations are contemporaneously correlated with the return innovations.", "C": "The Stambaugh bias can be completely eliminated by ensuring the sample size is sufficiently large (e.g., > 1000 observations), as the bias is only a small-sample problem.", "D": "The only significant issue is potential autocorrelation in the errors `ε_{t+1}`, which can be fixed by using Newey-West standard errors, leaving the OLS coefficient `β_1` unbiased."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a **Reverse-Reasoning** and **Premise/Assumption Packaging** strategy to assess deep econometric knowledge. It requires identifying the specific bias relevant to predictive regressions (Option A) and its corresponding solution (Option B). Distractors offer common but incorrect or incomplete solutions: C confuses a fix for standard errors with a fix for a biased coefficient (True but Irrelevant/Incomplete), and D mischaracterizes the nature of the bias, which persists in any finite sample (Conceptual Error).", "qid": "347", "question": "### Background\n\n**Research Question.** This case explores potential econometric biases in predictive regressions and appropriate methods to address them.\n\n**Setting and Sample.** An econometrician is concerned about potential biases when estimating the predictive regression `r_{t+1} = α + β_1 F_{1,t}^{\\text{short}} + ε_{t+1}` using Ordinary Least Squares (OLS), where `F_{1,t}^{\\text{short}}` is a persistent financial time series predictor.\n\n### Data / Model Specification\n\nThe predictive regression model is:\n  \nr_{t+1} = \\alpha + \\beta_1 F_{1,t}^{\\text{short}} + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \nThe predictor `F_{1,t}^{\\text{short}}` is known to be persistent (highly autocorrelated).\n\n### Question\n\nSelect all statements that accurately describe a potential econometric problem in this setting and a valid methodological response.", "model_evaluation_result": {"model_raw_response": "Looking at this econometric problem, I need to evaluate each statement about the predictive regression with a persistent predictor.\n\nLet me analyze each option:\n\n**Option A**: Using lagged predictor F_{1,t-1}^{short} as an instrument for F_{1,t}^{short}\n- This is NOT a valid approach. The lagged predictor would likely be a weak instrument since it's highly correlated with F_{1,t}^{short} (due to persistence) but not necessarily strongly correlated with r_{t+1} after controlling for F_{1,t}^{short}\n- More importantly, this doesn't address the actual bias problem (Stambaugh bias) which arises from the correlation between predictor innovations and return innovations\n- IV is used for endogeneity issues, but the Stambaugh bias is a different type of problem\n\n**Option B**: Stambaugh (1999) bias concern\n- This is CORRECT. The Stambaugh bias occurs when:\n  1. The predictor is persistent (highly autocorrelated) - which is stated in the problem\n  2. The innovations in the predictor are contemporaneously correlated with return innovations\n- This bias affects the OLS estimate of β₁ in predictive regressions and is a well-documented issue in financial econometrics\n\n**Option C**: Stambaugh bias eliminated with large samples\n- This is INCORRECT. While the Stambaugh bias does decrease with sample size, it doesn't completely disappear even in large samples\n- The bias has an asymptotic component that persists regardless of sample size\n- It's not purely a small-sample problem\n\n**Option D**: Only issue is autocorrelation, fixed by Newey-West\n- This is INCORRECT. The main concern with persistent predictors is the Stambaugh bias, not just autocorrelation in errors\n- Newey-West standard errors correct for heteroskedasticity and autocorrelation in the error terms but do NOT fix coefficient bias\n- The OLS coefficient β₁ would still be biased due to the Stambaugh effect\n\nB", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 57, "Question": "### Background\n\n**Research Question.** What are the aggregate and distributional consequences of imposing interest rate ceilings on the market for motor vehicle loans?\n\n**Setting / Data-Generating Environment.** The paper uses a consistently estimated hedonic model to simulate market outcomes. A key methodological challenge is that the estimation sample contains only *approved* loans. To produce market-wide estimates of rejection rates, the model uses a specific weighting scheme. The paper also considers a hypothetical \"risk-adjusted\" ceiling policy to evaluate its theoretical properties.\n\n---\n\n### Data / Model Specification\n\nThe probability that a prospective loan `i` will be rejected under a ceiling `c` is given by:\n  \n\\rho_{i} = 1 - \\Phi\\left(\\frac{\\ln c - X_{i}^{\\prime}\\beta}{\\sigma}\\right) \\quad \\text{(Eq. (1))}\n \nThe unconditional equilibrium rate for loan `i` is:\n  \n\\theta_{i} = \\exp(X_{i}^{\\prime}\\beta + \\sigma^{2}/2) \\quad \\text{(Eq. (2))}\n \nwhere `Φ(·)` is the standard normal CDF, `X_i` is a vector of borrower characteristics, `β` is a vector of coefficients, and `σ` is the standard deviation of the error term.\n\n---\n\n### Question\n\nBased on the paper's methodology for analyzing the effects of rate ceilings, which of the following statements are correct?", "Options": {"A": "To estimate the market-wide rejection rate from a sample of only approved loans, individual observations must be weighted to correct for sample selection bias; a simple average would systematically understate the true rationing effect by under-representing high-risk borrowers.", "B": "A hypothetical policy that sets a personalized ceiling for each borrower equal to their predicted unconditional equilibrium rate (`c_i = θ_i`) would fail to eliminate rationing, instead imposing a uniform rejection probability on all borrowers that depends only on the error variance (`σ^2`).", "C": "The weighting scheme is necessary because the sample of approved loans contains a disproportionately high number of high-risk borrowers, whose influence must be down-weighted to reflect the general population.", "D": "The proposed risk-adjusted ceiling policy (`c_i = θ_i`) is shown to be optimal because it allows lenders to perfectly price risk, thereby eliminating credit rationing for all borrower types."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the paper's core methodological contributions and theoretical implications, using a Reverse-Reasoning strategy (evaluating conclusions from complex procedures). It uses Atomic Decomposition to test two key, non-obvious insights from the original QA: the logic of weighting for sample selection and the critique of a hypothetical policy. Distractors C and D are Conceptual Opposites, directly contradicting the correct logic and findings, which helps discriminate between deep and superficial understanding.", "qid": "57", "question": "### Background\n\n**Research Question.** What are the aggregate and distributional consequences of imposing interest rate ceilings on the market for motor vehicle loans?\n\n**Setting / Data-Generating Environment.** The paper uses a consistently estimated hedonic model to simulate market outcomes. A key methodological challenge is that the estimation sample contains only *approved* loans. To produce market-wide estimates of rejection rates, the model uses a specific weighting scheme. The paper also considers a hypothetical \"risk-adjusted\" ceiling policy to evaluate its theoretical properties.\n\n---\n\n### Data / Model Specification\n\nThe probability that a prospective loan `i` will be rejected under a ceiling `c` is given by:\n  \n\\rho_{i} = 1 - \\Phi\\left(\\frac{\\ln c - X_{i}^{\\prime}\\beta}{\\sigma}\\right) \\quad \\text{(Eq. (1))}\n \nThe unconditional equilibrium rate for loan `i` is:\n  \n\\theta_{i} = \\exp(X_{i}^{\\prime}\\beta + \\sigma^{2}/2) \\quad \\text{(Eq. (2))}\n \nwhere `Φ(·)` is the standard normal CDF, `X_i` is a vector of borrower characteristics, `β` is a vector of coefficients, and `σ` is the standard deviation of the error term.\n\n---\n\n### Question\n\nBased on the paper's methodology for analyzing the effects of rate ceilings, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze each statement based on the methodology described.\n\n**Statement A**: This correctly identifies that when you only observe approved loans, you're missing all the rejected applications. High-risk borrowers are more likely to be rejected, so they're underrepresented in the approved loan sample. To estimate market-wide rejection rates, you need to weight observations to correct for this selection bias. Without weighting, you would indeed understate the true rationing effect.\n\n**Statement B**: Let's check this mathematically. If we set c_i = θ_i = exp(X_i'β + σ²/2), then:\nρ_i = 1 - Φ((ln(θ_i) - X_i'β)/σ)\n     = 1 - Φ((X_i'β + σ²/2 - X_i'β)/σ)\n     = 1 - Φ(σ/2)\n\nThis is constant for all borrowers! So even with personalized ceilings equal to predicted rates, there's still a uniform rejection probability that depends only on σ. This statement is correct.\n\n**Statement C**: This is backwards. The approved loan sample would have disproportionately *low* high-risk borrowers (because they're more likely to be rejected), not high. The weighting would need to up-weight high-risk borrowers to reflect their true population proportion, not down-weight them.\n\n**Statement D**: This contradicts what we found in Statement B. The policy c_i = θ_i doesn't eliminate rationing - it creates a uniform rejection probability of 1 - Φ(σ/2) for all borrowers. This is far from optimal or eliminating credit rationing.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 186, "Question": "### Background\n\nAn analysis uses an option-pricing framework to value the U.S. government's implicit guarantee on Freddie Mac's mortgage portfolio. The guarantee is modeled as a put option written by the government, with an exercise price that is reduced by Freddie Mac's own loss-absorbing capital. The model is calibrated under two scenarios: a 'Normal' scenario (20% of mortgages are 'at-risk') and a 'Stress' scenario (50% of mortgages are 'at-risk', with higher loss rates). The analysis then simulates the effect of increasing Freddie Mac's capital to 3.0% of its total mortgage book.\n\n### Data / Model Specification\n\nThe value of the government guarantee, expressed in basis points (b.p.) of the total mortgage portfolio, is calculated for different levels of underlying credit risk (Total Mean Pure Premium, or MPP) and under different capital and economic scenarios.\n\n**Table 1: Value of the Implicit Government Guarantee (in basis points)**\n| Total MPP (b.p.) | Baseline Capital, Normal Scenario | Baseline Capital, Stress Scenario | 3.0% Capital, Normal Scenario | 3.0% Capital, Stress Scenario |\n| :---: | :---: | :---: | :---: | :---: |\n| 35.0 | 16.2 | 22.6 | 4.1 | 11.3 |\n| 27.1 | 11.3 | 16.7 | 2.3 | 7.8 |\n| 15.0 | 4.8 | 8.3 | 0.6 | 3.2 |\n\n*Source: Synthesized from Tables 6, 6A, 7, and 7A in the paper.*\n\n### Question\n\nBased on the data in **Table 1** for the 27.1 b.p. MPP case, which of the following statements are correct interpretations of the effect of increasing Freddie Mac's capital from the baseline to 3.0%?", "Options": {"A": "The additional capital is relatively less effective during the Stress Scenario, reducing the guarantee by only 53.3% compared to a 79.6% reduction in the Normal Scenario.", "B": "The absolute reduction in the government guarantee is greater under the Normal Scenario (9.0 b.p.) than under the Stress Scenario (8.9 b.p.).", "C": "After the capital increase, the remaining government guarantee under the Stress Scenario (7.8 b.p.) is more than triple the remaining guarantee under the Normal Scenario (2.3 b.p.).", "D": "The percentage reduction in the government guarantee is greater under the Stress Scenario than under the Normal Scenario."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to perform multi-step calculations (absolute and percentage changes) from tabular data and draw nuanced conclusions about policy effectiveness under different conditions. Depth Strategy: Computational Judgment. The user must calculate and compare values to evaluate each statement. Distractor Logic: Option B is a conceptual opposite; the percentage reduction is actually smaller in the stress scenario, a key finding of the paper. Options A, C, and D are all arithmetically verifiable and correct interpretations of the data, requiring careful calculation and comparison.", "qid": "186", "question": "### Background\n\nAn analysis uses an option-pricing framework to value the U.S. government's implicit guarantee on Freddie Mac's mortgage portfolio. The guarantee is modeled as a put option written by the government, with an exercise price that is reduced by Freddie Mac's own loss-absorbing capital. The model is calibrated under two scenarios: a 'Normal' scenario (20% of mortgages are 'at-risk') and a 'Stress' scenario (50% of mortgages are 'at-risk', with higher loss rates). The analysis then simulates the effect of increasing Freddie Mac's capital to 3.0% of its total mortgage book.\n\n### Data / Model Specification\n\nThe value of the government guarantee, expressed in basis points (b.p.) of the total mortgage portfolio, is calculated for different levels of underlying credit risk (Total Mean Pure Premium, or MPP) and under different capital and economic scenarios.\n\n**Table 1: Value of the Implicit Government Guarantee (in basis points)**\n| Total MPP (b.p.) | Baseline Capital, Normal Scenario | Baseline Capital, Stress Scenario | 3.0% Capital, Normal Scenario | 3.0% Capital, Stress Scenario |\n| :---: | :---: | :---: | :---: | :---: |\n| 35.0 | 16.2 | 22.6 | 4.1 | 11.3 |\n| 27.1 | 11.3 | 16.7 | 2.3 | 7.8 |\n| 15.0 | 4.8 | 8.3 | 0.6 | 3.2 |\n\n*Source: Synthesized from Tables 6, 6A, 7, and 7A in the paper.*\n\n### Question\n\nBased on the data in **Table 1** for the 27.1 b.p. MPP case, which of the following statements are correct interpretations of the effect of increasing Freddie Mac's capital from the baseline to 3.0%?", "model_evaluation_result": {"model_raw_response": "Looking at the data for the 27.1 b.p. MPP case:\n- Baseline Capital, Normal Scenario: 11.3 b.p.\n- Baseline Capital, Stress Scenario: 16.7 b.p.\n- 3.0% Capital, Normal Scenario: 2.3 b.p.\n- 3.0% Capital, Stress Scenario: 7.8 b.p.\n\nLet me evaluate each option:\n\nA) Percentage reductions:\n- Normal Scenario: (11.3 - 2.3) / 11.3 = 9.0 / 11.3 = 79.6%\n- Stress Scenario: (16.7 - 7.8) / 16.7 = 8.9 / 16.7 = 53.3%\nThis statement is correct - the capital is relatively less effective in the Stress Scenario.\n\nB) Absolute reductions:\n- Normal Scenario: 11.3 - 2.3 = 9.0 b.p.\n- Stress Scenario: 16.7 - 7.8 = 8.9 b.p.\nThis statement is correct - the absolute reduction is greater in the Normal Scenario.\n\nC) Remaining guarantees after capital increase:\n- Normal Scenario: 2.3 b.p.\n- Stress Scenario: 7.8 b.p.\n- Ratio: 7.8 / 2.3 = 3.39\nThis statement is correct - the Stress Scenario guarantee is more than triple the Normal Scenario guarantee.\n\nD) This contradicts option A. The percentage reduction is 79.6% in Normal vs 53.3% in Stress, so the percentage reduction is greater in the Normal Scenario, not the Stress Scenario. This statement is incorrect.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 356, "Question": "### Background\n\n**Research Question.** Which firm characteristics explain the cross-sectional variation in commercial bank stock returns following deregulation announcements?\n\n**Setting and Sample.** The study analyzes the stock market reaction of U.S. commercial banks to a series of deregulation events in the 1990s. It investigates why some banks benefited more than others by regressing their event-day abnormal returns on firm characteristics like size and prior experience with investment banking activities.\n\n### Data / Model Specification\n\nThe paper estimates the following cross-sectional model via OLS for each of four events:\n\n  \n\\mathrm{CAR}_{i} = \\beta_{0} + \\beta_{1}\\mathrm{Ln}(\\mathrm{SIZE}_{i}) + \\beta_{2}\\mathrm{SEC}20_{i} + \\text{controls}_i + e_{i} \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Cross-Sectional Regression Results for Commercial Banks: Event 1 (Easing of Firewall Restrictions)**\n\n| Variable    | Coefficient | t-statistic |\n| :---------- | :---------- | :---------- |\n| Intercept   | -6.44       | (-2.82)**   |\n| Ln(SIZE)    | 0.36        | (2.27)*     |\n| SEC 20      | 0.98        | (2.47)*     |\n| Adj. R2     | 0.28        |             |\n\n*Note: Adapted from Table 5, Panel A of the source paper. ** indicates significance at the 0.01 level, * at the 0.05 level. The dependent variable is the two-day CAR in percent.*\n\n---\n\n### Question\n\nBased on the model and results in **Table 1**, select all statements that are correct.", "Options": {"A": "The coefficient on `Ln(SIZE)` implies that smaller banks were expected to benefit more from the easing of firewalls.", "B": "The `SEC20` coefficient is not statistically significant at the 5% level, indicating that prior experience in investment banking was irrelevant.", "C": "If unobserved managerial quality is positively correlated with both bank size and abnormal returns, the estimated coefficient on `Ln(SIZE)` is likely biased upwards, potentially overstating the true effect of size.", "D": "The results suggest that, holding size constant, banks with a Section 20 subsidiary experienced a two-day abnormal return that was, on average, 0.98 percentage points higher than banks without one."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret regression coefficients and understand the direction of omitted variable bias. It uses the 'Atomic Decomposition' strategy, breaking down the original QA's interpretation and bias analysis into distinct, verifiable statements. Option A tests direct interpretation of a coefficient's magnitude and meaning. Option B tests the application of OVB theory. Distractor C is a 'Conceptual Opposite' (sign error). Distractor D is a 'Comprehension Slip' (misreading statistical significance).", "qid": "356", "question": "### Background\n\n**Research Question.** Which firm characteristics explain the cross-sectional variation in commercial bank stock returns following deregulation announcements?\n\n**Setting and Sample.** The study analyzes the stock market reaction of U.S. commercial banks to a series of deregulation events in the 1990s. It investigates why some banks benefited more than others by regressing their event-day abnormal returns on firm characteristics like size and prior experience with investment banking activities.\n\n### Data / Model Specification\n\nThe paper estimates the following cross-sectional model via OLS for each of four events:\n\n  \n\\mathrm{CAR}_{i} = \\beta_{0} + \\beta_{1}\\mathrm{Ln}(\\mathrm{SIZE}_{i}) + \\beta_{2}\\mathrm{SEC}20_{i} + \\text{controls}_i + e_{i} \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Cross-Sectional Regression Results for Commercial Banks: Event 1 (Easing of Firewall Restrictions)**\n\n| Variable    | Coefficient | t-statistic |\n| :---------- | :---------- | :---------- |\n| Intercept   | -6.44       | (-2.82)**   |\n| Ln(SIZE)    | 0.36        | (2.27)*     |\n| SEC 20      | 0.98        | (2.47)*     |\n| Adj. R2     | 0.28        |             |\n\n*Note: Adapted from Table 5, Panel A of the source paper. ** indicates significance at the 0.01 level, * at the 0.05 level. The dependent variable is the two-day CAR in percent.*\n\n---\n\n### Question\n\nBased on the model and results in **Table 1**, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the table and analyzing each statement:\n\nA) The coefficient on `Ln(SIZE)` is 0.36 (positive), which means larger banks (higher Ln(SIZE)) had higher abnormal returns. This contradicts the statement that smaller banks benefited more. **Incorrect**\n\nB) The `SEC20` coefficient has a t-statistic of 2.47 with * notation, indicating significance at the 0.05 level. The statement claims it's not significant at the 5% level. **Incorrect**\n\nC) This describes a classic omitted variable bias scenario. If managerial quality is positively correlated with both size and abnormal returns but is omitted from the model, the coefficient on `Ln(SIZE)` would capture both the true size effect plus the effect of managerial quality working through size. This would bias the coefficient upward. **Correct**\n\nD) The SEC20 coefficient is 0.98, which directly means that banks with a Section 20 subsidiary had abnormal returns 0.98 percentage points higher than those without, holding size constant. This is the standard interpretation of a regression coefficient. **Correct**\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 353, "Question": "### Background\nA study models the joint effect of private anti-self-dealing regulations and ownership structure on corporate valuation. A key challenge in such models is endogeneity, where the ownership structure may be correlated with unobserved factors that also affect firm value.\n\n### Data / Model Specification\nThe core empirical model is:\n\n  \n\\text{Tobin's Q}_{it} = \\alpha + \\beta_{1}\\mathrm{WEDGE}_{it} + \\beta_{2}\\mathrm{PRIVATE}_{ct} + \\beta_{3}(\\mathrm{WEDGE}_{it} * \\mathrm{PRIVATE}_{ct}) + \\dots + \\varepsilon_{it} \\quad \\text{(Eq. 1)}\n \n\nThe paper argues that the ownership wedge (`WEDGE`) can be treated as exogenous. However, a critic suggests `WEDGE` is endogenous because it is correlated with an unobserved firm-level governance quality (`U_i`) that also affects `Tobin's Q`. To address this, an instrumental variable (`Z_i`) is proposed for `WEDGE`.\n\n---\n\nWhich of the following conditions are necessary for `Z_i` to be a valid instrument in a Generalized Method of Moments (GMM) framework designed to obtain consistent estimates of the coefficients in Eq. (1)? Select all that apply.", "Options": {"A": "The instrument `Z_i` must be uncorrelated with the other exogenous variables in the model, such as `PRIVATE_ct`.", "B": "The instrument `Z_i` must be uncorrelated with the dependent variable, `Tobin's Q_it`.", "C": "The instrument `Z_i` must be strongly correlated with the endogenous variable, `WEDGE_it`.", "D": "The instrument `Z_i` must be uncorrelated with the model's error term, `ε_it`."}, "Answer": ["C", "D"], "pi_justification": "This item assesses foundational knowledge of causal inference and instrumental variables (IV/GMM), a key methodological challenge discussed in the paper. It uses a Depth Strategy of applying general econometric principles to the specific model in the paper. Options A (the exclusion restriction) and B (instrument relevance) are the two core, defining conditions for a valid instrument. Option C is a distractor; instruments do not need to be uncorrelated with other exogenous regressors. Option D is a Conceptual Opposite; a valid instrument *must* be correlated with the dependent variable, but only *through* its effect on the endogenous variable.", "qid": "353", "question": "### Background\nA study models the joint effect of private anti-self-dealing regulations and ownership structure on corporate valuation. A key challenge in such models is endogeneity, where the ownership structure may be correlated with unobserved factors that also affect firm value.\n\n### Data / Model Specification\nThe core empirical model is:\n\n  \n\\text{Tobin's Q}_{it} = \\alpha + \\beta_{1}\\mathrm{WEDGE}_{it} + \\beta_{2}\\mathrm{PRIVATE}_{ct} + \\beta_{3}(\\mathrm{WEDGE}_{it} * \\mathrm{PRIVATE}_{ct}) + \\dots + \\varepsilon_{it} \\quad \\text{(Eq. 1)}\n \n\nThe paper argues that the ownership wedge (`WEDGE`) can be treated as exogenous. However, a critic suggests `WEDGE` is endogenous because it is correlated with an unobserved firm-level governance quality (`U_i`) that also affects `Tobin's Q`. To address this, an instrumental variable (`Z_i`) is proposed for `WEDGE`.\n\n---\n\nWhich of the following conditions are necessary for `Z_i` to be a valid instrument in a Generalized Method of Moments (GMM) framework designed to obtain consistent estimates of the coefficients in Eq. (1)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "To determine which conditions are necessary for `Z_i` to be a valid instrument in a GMM framework, I need to recall the fundamental requirements for instrumental variables.\n\nFor an instrument to be valid, it must satisfy two key conditions:\n\n1. **Relevance**: The instrument must be correlated with the endogenous variable it's instrumenting for.\n\n2. **Exogeneity**: The instrument must be uncorrelated with the error term in the structural equation.\n\nLet me evaluate each option:\n\n**Option A**: The instrument `Z_i` must be uncorrelated with other exogenous variables.\n- This is NOT a necessary condition. An instrument can be correlated with other exogenous variables without affecting its validity.\n\n**Option B**: The instrument `Z_i` must be uncorrelated with the dependent variable.\n- This is NOT a necessary condition. The instrument can be correlated with the dependent variable through its correlation with the endogenous variable. What matters is that it's uncorrelated with the error term.\n\n**Option C**: The instrument `Z_i` must be strongly correlated with `WEDGE_it`.\n- This IS necessary. This is the relevance condition - the instrument must be correlated with the endogenous variable it's instrumenting for.\n\n**Option D**: The instrument `Z_i` must be uncorrelated with `ε_it`.\n- This IS necessary. This is the exogeneity condition - the instrument must be uncorrelated with the error term for the estimates to be consistent.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 316, "Question": "### Background\n\n**Research Question.** How can a CCP's risk management framework be designed to account for the fact that some members persistently contribute more to systemic \"crowded risk\" than others?\n\n**Setting.** The paper proposes allocating total CCP risk (`ExpCCP`) back to individual members (`ExpCCP_j`). The difference between a member's allocated risk and the margin they posted is their \"funding gap.\" A key policy challenge is whether to use this allocation for high-frequency (daily) margin calls, which could create trading uncertainty, or for lower-frequency capital charges.\n\n**Variables and Parameters.**\n- `ExpCCP_j`: The portion of total CCP exposure allocated to member `j`.\n- **Member-specific funding gap:** The difference between a member's allocated risk (`ExpCCP_j`) and the actual margin they posted.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Correlation of Member Funding Gaps Across High-Stress Days**\n\nThis table shows the pairwise correlation of member-specific funding gaps across ten different high-stress days. A positive correlation indicates that members who under-contribute to collateral on one stress day tend to do so on other stress days as well. Asterisks (*) denote statistical significance at the 5% level.\n\n| | May 10 | Apr 23 | Apr 26 | May 24 | May 11 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| May 10, 2010 | 1.00 | 0.36* | 0.35* | 0.63* | 0.73* |\n| Apr 23, 2010 | | 1.00 | 0.71* | 0.41* | 0.35* |\n| Apr 26, 2010 | | | 1.00 | 0.24 | 0.62* |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the policy context described, select all statements that represent valid interpretations or policy recommendations supported by the paper.", "Options": {"A": "The significant positive correlations in the table support the hypothesis of \"type persistence,\" where the same members tend to be the primary contributors to crowded risk across different stress events.", "B": "Given the evidence of persistence, using a member's average `ExpCCP_j` to set their quarterly default fund contribution is a policy supported by the paper to make polluters pay without creating daily trading uncertainty.", "C": "The fact that no correlation is 1.00 (except with the same day) indicates that polluter behavior is essentially random, making long-term surcharges based on past behavior ineffective.", "D": "The data justifies implementing immediate, ad-hoc daily margin calls based on `ExpCCP_j` for all members during stress events to ensure polluters pay."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect an empirical finding ('polluter persistence') to a specific policy recommendation. It uses the 'Premise/Assumption Packaging' strategy. Options A and B are correct interpretations and policy conclusions drawn directly from the paper's text and the original QA answer. Distractor C proposes a policy ('ad-hoc daily margin calls') that the paper explicitly argues against due to the uncertainty it creates for traders. Distractor D represents a 'Conceptual Opposite' misinterpretation of statistical evidence, confusing 'not perfectly correlated' with 'random'.", "qid": "316", "question": "### Background\n\n**Research Question.** How can a CCP's risk management framework be designed to account for the fact that some members persistently contribute more to systemic \"crowded risk\" than others?\n\n**Setting.** The paper proposes allocating total CCP risk (`ExpCCP`) back to individual members (`ExpCCP_j`). The difference between a member's allocated risk and the margin they posted is their \"funding gap.\" A key policy challenge is whether to use this allocation for high-frequency (daily) margin calls, which could create trading uncertainty, or for lower-frequency capital charges.\n\n**Variables and Parameters.**\n- `ExpCCP_j`: The portion of total CCP exposure allocated to member `j`.\n- **Member-specific funding gap:** The difference between a member's allocated risk (`ExpCCP_j`) and the actual margin they posted.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Correlation of Member Funding Gaps Across High-Stress Days**\n\nThis table shows the pairwise correlation of member-specific funding gaps across ten different high-stress days. A positive correlation indicates that members who under-contribute to collateral on one stress day tend to do so on other stress days as well. Asterisks (*) denote statistical significance at the 5% level.\n\n| | May 10 | Apr 23 | Apr 26 | May 24 | May 11 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| May 10, 2010 | 1.00 | 0.36* | 0.35* | 0.63* | 0.73* |\n| Apr 23, 2010 | | 1.00 | 0.71* | 0.41* | 0.35* |\n| Apr 26, 2010 | | | 1.00 | 0.24 | 0.62* |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the policy context described, select all statements that represent valid interpretations or policy recommendations supported by the paper.", "model_evaluation_result": {"model_raw_response": "Looking at the table and context:\n\nThe table shows statistically significant positive correlations between member funding gaps across different high-stress days (most correlations marked with *). This indicates that members who under-contribute on one stress day tend to under-contribute on other stress days as well.\n\nAnalyzing each option:\n\nA) **Correct** - The significant positive correlations (0.35* to 0.73*) clearly support \"type persistence\" - the same members consistently contribute more to crowded risk across different stress events.\n\nB) **Correct** - Given the evidence of persistence, using average ExpCCP_j for quarterly default fund contributions is a sensible policy that makes persistent \"polluters\" pay while avoiding the daily trading uncertainty mentioned in the background.\n\nC) **Incorrect** - The correlations ranging from 0.24 to 0.73 with most being statistically significant actually demonstrate persistence, not randomness. The fact that correlations aren't 1.00 doesn't mean behavior is random.\n\nD) **Incorrect** - The background specifically mentions that daily margin calls based on ExpCCP_j could create \"trading uncertainty,\" which is presented as a drawback. The paper appears to favor lower-frequency solutions like capital charges.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 312, "Question": "### Background\n\n**Research Question.** How did the composition of foreign direct investment (FDI) into Argentina after its 1991 reforms drive a fundamental shift in corporate ownership, and what were the implications for corporate governance?\n\n**Setting / Data-Generating Environment.** The analysis focuses on Argentina during its decade of intense economic liberalization (1990-1999). Sweeping reforms attracted a massive inflow of foreign capital, leading to a dramatic change in the ownership structure of the country's top corporations, which had previously been dominated by domestic families and the state. This shift from concentrated family/state ownership to control by foreign corporations and investment funds had profound effects on prevailing corporate governance practices.\n\n### Data / Model Specification\n\nThe composition of foreign investment in Argentina from 1990 to 1999 is detailed in Table 1.\n\n**Table 1: Foreign Investment in Argentina from 1990–1999 (in millions of dollars)**\n\n| Category      | 1990-1996 | 1997   | 1998   | 1999    | Total      |\n|:--------------|:----------|:-------|:-------|:--------|:-----------|\n| M&A           | 8,131.3   | 11,792.5 | 9,120.7  | 19,507.5  | 48,552.0   |\n| Privatizations| 8,763.0   | 1,111.2  | 442.5    | 5,926.7   | 16,243.3   |\n| Greenfield    | 5,552.4   | 4,143.3  | 6,263.9  | 5,194.7   | 21,154.3   |\n| Expansion     | 18,258.0  | 7,453.5  | 7,670.6  | 7,272.6   | 40,654.7   |\n| **Total**     | 40,704.7  | 24,500.5 | 23,497.6 | 37,901.5  | 126,604.3  |\n\nThis influx of capital resulted in a significant ownership transition, as exemplified by the cases in Table 2.\n\n**Table 2: Examples of Ownership Transition in Top Argentine Corporations**\n\n| Corporation     | Sector       | Pre-1991 Controlling Shareholders | Post-1991 Controlling Shareholders |\n|:----------------|:-------------|:----------------------------------|:-----------------------------------|\n| YPF             | Oil & Gas    | State                             | Repsol (Spain)                     |\n| Banco Francés   | Banking      | Domestic Families                 | Banco Bilbao Vizcaya (Spain)       |\n| Norte           | Supermarkets | Domestic                          | Promodes (Carrefour France)        |\n\n### Question\n\nThe shift in ownership from domestic family-owned firms to foreign corporations, driven by the investment patterns shown, had profound implications for corporate governance. Select ALL statements that correctly describe these theoretical implications.", "Options": {"A": "The corporate restructuring in Argentina in the 1990s was primarily financed by leveraged buy-outs and junk bonds, mirroring the US experience in the 1980s.", "B": "The ownership transition would be expected to replace the dominant Type 2 agency problem (majority vs. minority shareholders) with the classic Type 1 agency problem (managers vs. dispersed owners).", "C": "From a real options perspective, an M&A strategy is valuable because it allows an acquirer to resolve a target firm's governance problems, thereby increasing the value of embedded options like the option to expand or restructure.", "D": "The primary agency conflict likely shifted from manager-shareholder conflicts (Type 1) to conflicts between controlling and minority shareholders (Type 2)."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect empirical patterns to core corporate finance theories (agency theory, real options). It uses a Scenario Application strategy. The correct options (B, C) require applying these theories to the specific context of Argentina's reforms. Distractor A is a direct Conceptual Opposite, testing for understanding of agency problem types. Distractor D introduces a plausible but factually incorrect comparison (the paper explicitly states LBOs/junk bonds were not used), testing for close reading of the paper's specific details.", "qid": "312", "question": "### Background\n\n**Research Question.** How did the composition of foreign direct investment (FDI) into Argentina after its 1991 reforms drive a fundamental shift in corporate ownership, and what were the implications for corporate governance?\n\n**Setting / Data-Generating Environment.** The analysis focuses on Argentina during its decade of intense economic liberalization (1990-1999). Sweeping reforms attracted a massive inflow of foreign capital, leading to a dramatic change in the ownership structure of the country's top corporations, which had previously been dominated by domestic families and the state. This shift from concentrated family/state ownership to control by foreign corporations and investment funds had profound effects on prevailing corporate governance practices.\n\n### Data / Model Specification\n\nThe composition of foreign investment in Argentina from 1990 to 1999 is detailed in Table 1.\n\n**Table 1: Foreign Investment in Argentina from 1990–1999 (in millions of dollars)**\n\n| Category      | 1990-1996 | 1997   | 1998   | 1999    | Total      |\n|:--------------|:----------|:-------|:-------|:--------|:-----------|\n| M&A           | 8,131.3   | 11,792.5 | 9,120.7  | 19,507.5  | 48,552.0   |\n| Privatizations| 8,763.0   | 1,111.2  | 442.5    | 5,926.7   | 16,243.3   |\n| Greenfield    | 5,552.4   | 4,143.3  | 6,263.9  | 5,194.7   | 21,154.3   |\n| Expansion     | 18,258.0  | 7,453.5  | 7,670.6  | 7,272.6   | 40,654.7   |\n| **Total**     | 40,704.7  | 24,500.5 | 23,497.6 | 37,901.5  | 126,604.3  |\n\nThis influx of capital resulted in a significant ownership transition, as exemplified by the cases in Table 2.\n\n**Table 2: Examples of Ownership Transition in Top Argentine Corporations**\n\n| Corporation     | Sector       | Pre-1991 Controlling Shareholders | Post-1991 Controlling Shareholders |\n|:----------------|:-------------|:----------------------------------|:-----------------------------------|\n| YPF             | Oil & Gas    | State                             | Repsol (Spain)                     |\n| Banco Francés   | Banking      | Domestic Families                 | Banco Bilbao Vizcaya (Spain)       |\n| Norte           | Supermarkets | Domestic                          | Promodes (Carrefour France)        |\n\n### Question\n\nThe shift in ownership from domestic family-owned firms to foreign corporations, driven by the investment patterns shown, had profound implications for corporate governance. Select ALL statements that correctly describe these theoretical implications.", "model_evaluation_result": {"model_raw_response": "Looking at the data and theoretical implications:\n\nThe data shows foreign investment came primarily through M&A and privatizations, with foreign corporations acquiring controlling stakes in Argentine companies previously owned by domestic families or the state.\n\nAnalyzing each option:\n\nA) False - The data shows foreign direct investment through M&A and privatizations, not leveraged buyouts and junk bonds. The financing came from foreign corporations acquiring Argentine companies.\n\nB) False - This reverses the actual transition. Argentina moved FROM concentrated family/state ownership (Type 1 problems) TO foreign corporate control (Type 2 problems with controlling vs minority shareholders).\n\nC) True - Real options theory suggests M&A creates value by allowing acquirers to resolve governance issues in target firms, thereby increasing the value of growth options, restructuring options, and other embedded real options.\n\nD) True - The ownership transition was FROM dispersed ownership with manager-shareholder conflicts TO concentrated foreign corporate ownership, creating conflicts between controlling foreign shareholders and minority shareholders.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 298, "Question": "### Background\n\n**Research Question.** This case examines a modern econometric technique for isolating business cycle components of time series, which is a prerequisite for analyzing the cyclical comovement between macroeconomic variables like bank capital and real GDP.\n\n**Setting / Data-Generating Environment.** The setting involves a stationary or non-stationary time series, `y_t`, observed over a sample period. The goal is to separate its long-run trend from its cyclical component. The specific application mentioned is for quarterly macroeconomic data, where business cycle frequencies are of primary interest.\n\n**Variables & Parameters.**\n- `y_t`: The value of a time series at time `t` (e.g., log Real GDP).\n- `h`: The forecast horizon, set to 8 for quarterly data (2 years).\n- `\\beta_0, \\ldots, \\beta_4`: OLS regression coefficients.\n- `v_{t+h}`: The population error term of the predictive regression.\n- `\\hat{v}_{t+h}`: The estimated residual from the OLS regression, defined as the cyclical component of the series at time `t+h`.\n- `t`: Time index for observations (e.g., quarters).\n\n---\n\n### Data / Model Specification\n\nThe Hamilton (2018) method for detrending a time series `y_t` is based on the following linear projection:\n  \ny_{t+h} = \\beta_0 + \\beta_1 y_t + \\beta_2 y_{t-1} + \\beta_3 y_{t-2} + \\beta_4 y_{t-3} + v_{t+h} \\quad \\text{(Eq. 1)}\n \nThe trend component is the predicted value from this regression, `\\hat{y}_{t+h}`. The cyclical component is defined as the residual from this regression:\n  \n\\hat{v}_{t+h} = y_{t+h} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 y_t + \\hat{\\beta}_2 y_{t-1} + \\hat{\\beta}_3 y_{t-2} + \\hat{\\beta}_4 y_{t-3}) \\quad \\text{(Eq. 2)}\n \nFor business cycle analysis of quarterly data, the horizon `h` is set to 8 quarters.\n\n---\n\n### The Question\n\nCompared to the widely used Hodrick-Prescott (H-P) filter, what are the key methodological advantages of the Hamilton (2018) method for business cycle analysis? (Select all that apply)", "Options": {"A": "It is a two-sided filter that produces a visually smoother trend component, making it easier to interpret.", "B": "It avoids imposing a fixed, ad-hoc smoothness assumption on the trend, allowing the data to determine the trend's properties.", "C": "It is a one-sided filter that uses only past and current information, which prevents spurious cyclical patterns caused by future data.", "D": "It is guaranteed to produce a stationary cyclical component, regardless of whether the original series has a unit root."}, "Answer": ["B", "C"], "pi_justification": "This Multiple Choice item tests the student's ability to identify the key methodological advantages of the Hamilton filter over the commonly used H-P filter. It uses a 'select-all-valid' strategy. The correct options (A, B) highlight its one-sided nature and data-driven flexibility. Distractors are designed to be plausible but incorrect: (C) incorrectly attributes a property of the H-P filter to the Hamilton filter, and (D) makes an overly strong claim about guaranteed stationarity, a common point of confusion.", "qid": "298", "question": "### Background\n\n**Research Question.** This case examines a modern econometric technique for isolating business cycle components of time series, which is a prerequisite for analyzing the cyclical comovement between macroeconomic variables like bank capital and real GDP.\n\n**Setting / Data-Generating Environment.** The setting involves a stationary or non-stationary time series, `y_t`, observed over a sample period. The goal is to separate its long-run trend from its cyclical component. The specific application mentioned is for quarterly macroeconomic data, where business cycle frequencies are of primary interest.\n\n**Variables & Parameters.**\n- `y_t`: The value of a time series at time `t` (e.g., log Real GDP).\n- `h`: The forecast horizon, set to 8 for quarterly data (2 years).\n- `\\beta_0, \\ldots, \\beta_4`: OLS regression coefficients.\n- `v_{t+h}`: The population error term of the predictive regression.\n- `\\hat{v}_{t+h}`: The estimated residual from the OLS regression, defined as the cyclical component of the series at time `t+h`.\n- `t`: Time index for observations (e.g., quarters).\n\n---\n\n### Data / Model Specification\n\nThe Hamilton (2018) method for detrending a time series `y_t` is based on the following linear projection:\n  \ny_{t+h} = \\beta_0 + \\beta_1 y_t + \\beta_2 y_{t-1} + \\beta_3 y_{t-2} + \\beta_4 y_{t-3} + v_{t+h} \\quad \\text{(Eq. 1)}\n \nThe trend component is the predicted value from this regression, `\\hat{y}_{t+h}`. The cyclical component is defined as the residual from this regression:\n  \n\\hat{v}_{t+h} = y_{t+h} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 y_t + \\hat{\\beta}_2 y_{t-1} + \\hat{\\beta}_3 y_{t-2} + \\hat{\\beta}_4 y_{t-3}) \\quad \\text{(Eq. 2)}\n \nFor business cycle analysis of quarterly data, the horizon `h` is set to 8 quarters.\n\n---\n\n### The Question\n\nCompared to the widely used Hodrick-Prescott (H-P) filter, what are the key methodological advantages of the Hamilton (2018) method for business cycle analysis? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at the Hamilton (2018) method described and comparing it to the Hodrick-Prescott filter:\n\nThe Hamilton method uses a linear projection based on past values (y_t, y_{t-1}, y_{t-2}, y_{t-3}) to predict y_{t+h}. This makes it a one-sided filter that only uses past and current information.\n\nKey advantages over the H-P filter:\n\n**Option A is incorrect**: The H-P filter is the two-sided filter that produces smooth trends. The Hamilton method is one-sided, not two-sided.\n\n**Option B is correct**: The H-P filter imposes a fixed smoothness parameter (lambda) that determines how smooth the trend should be. The Hamilton method lets the data determine the trend properties through OLS regression without imposing such assumptions.\n\n**Option C is correct**: The Hamilton method only uses past and current data (one-sided), while the H-P filter uses both past and future data (two-sided). Using future data can create spurious cyclical patterns, especially near the sample endpoints.\n\n**Option D is incorrect**: The Hamilton method does not guarantee stationarity of the cyclical component. If the original series has a unit root, the residuals may still be non-stationary.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 367, "Question": "### Background\n\n**Research Question.** Are news narratives a better source of information for measuring ICAPM state variables than traditional macroeconomic data?\n\n**Setting / Data-Generating Environment.** The paper's model is estimated using two different sets of inputs: (1) 180 narrative attention series derived from daily *Wall Street Journal* articles, and (2) 129 standard numerical macroeconomic data series from the FRED-MD database (e.g., quarterly GDP, monthly inflation). The model using narrative inputs achieves an out-of-sample MVE Sharpe ratio of 1.3, while the model using FRED-MD inputs achieves a Sharpe ratio of only 0.7.\n\n---\n\n### The Question\n\nBased on the paper's premise and results, select all of the following that represent plausible advantages of using high-frequency news narratives over traditional, low-frequency macroeconomic data for measuring ICAPM state variables.", "Options": {"A": "Timeliness: News is released daily, allowing for a more synchronous measurement of asset covariance with fundamental shocks compared to lagged monthly or quarterly macro data.", "B": "Forward-Looking Content: News narratives synthesize complex information and report on expectations and sentiment, which are better proxies for 'revisions to future investment opportunities' than backward-looking macro data.", "C": "Objectivity: News articles from reputable sources are less prone to measurement error and revisions than official government statistics.", "D": "Simplicity: The 'bag-of-words' representation of text is a lower-dimensional and less noisy data format than numerical time-series."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the core argument for why news text is a valuable data source in asset pricing. It uses a Scenario Application strategy, asking the user to apply the paper's logic to compare two data types. The correct options (A, C) identify the key advantages of timeliness and forward-looking content. Distractor B is an 'Almost Right' but incorrect claim, as news has its own significant biases. Distractor D is a 'Conceptual Opposite', as raw text data is extremely high-dimensional and noisy.", "qid": "367", "question": "### Background\n\n**Research Question.** Are news narratives a better source of information for measuring ICAPM state variables than traditional macroeconomic data?\n\n**Setting / Data-Generating Environment.** The paper's model is estimated using two different sets of inputs: (1) 180 narrative attention series derived from daily *Wall Street Journal* articles, and (2) 129 standard numerical macroeconomic data series from the FRED-MD database (e.g., quarterly GDP, monthly inflation). The model using narrative inputs achieves an out-of-sample MVE Sharpe ratio of 1.3, while the model using FRED-MD inputs achieves a Sharpe ratio of only 0.7.\n\n---\n\n### The Question\n\nBased on the paper's premise and results, select all of the following that represent plausible advantages of using high-frequency news narratives over traditional, low-frequency macroeconomic data for measuring ICAPM state variables.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which options represent plausible advantages of using high-frequency news narratives over traditional macroeconomic data for measuring ICAPM state variables.\n\nLet me analyze each option:\n\nA) Timeliness: This is clearly an advantage. The paper explicitly states that news is released daily while traditional macro data is monthly or quarterly. This allows for more synchronous measurement with asset prices, which is crucial for ICAPM models that need to capture contemporaneous relationships.\n\nB) Forward-Looking Content: This is also a plausible advantage. ICAPM state variables are meant to capture \"revisions to future investment opportunities.\" News narratives naturally incorporate expectations, sentiment, and forward-looking information, while traditional macro data tends to be backward-looking (reporting what already happened).\n\nC) Objectivity: This is not a plausible advantage. News articles, even from reputable sources, are subject to editorial decisions, bias, and interpretation. Official government statistics, while subject to revisions, follow standardized methodologies. The claim that news is more objective than official statistics is not credible.\n\nD) Simplicity: This is not a plausible advantage. The paper mentions using 180 narrative attention series (compared to 129 macro series), and text analysis through \"bag-of-words\" representation is computationally complex and high-dimensional, not simpler or lower-dimensional than numerical time series.\n\nThe paper's superior results (Sharpe ratio of 1.3 vs 0.7) likely stem from the timeliness and forward-looking nature of news, not from objectivity or simplicity.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 300, "Question": "### Background\n\n**Research Question.** Under what specific conditions involving binding order limits can a firm increase its total trading profit by allowing a less-informed (rookie) trader to trade competitively alongside an expert, effectively creating a negative “training cost”?\n\n**Setting.** A firm employs an expert trader (H) with perfect information and a rookie trader (L) whose signal about an asset's value `\\tilde{\\nu}` is corrupted by noise. The firm imposes a binding aggregate order limit `X` and allocates capital between the traders via individual limits `X_H` and `X_L`. A rational market maker sets prices based on the aggregate order flow.\n\n**Key Concepts.**\n- **Competition Effect**: In a duopoly, traders act more aggressively than a monopolist, which reveals information faster and tends to lower total profits.\n- **Information-Hiding Effect**: The rookie's noisy trades obscure the expert's trades, increasing information asymmetry, which can lead to a higher price impact and potentially higher profits for the informed traders as a group.\n\n---\n\n### Data / Model Specification\n\n**Lemma 2** from the paper states that in a constrained duopoly, the aggregate expected profit can be higher than in a constrained monopoly if the parameters fall within a specific range. There is no closed-form solution, but numerical analysis provides the following key results.\n\n**Table 1: Numerical Results for Noisy Duopoly vs. Constrained Monopoly**\n\n| Case | Aggregate Limit (X) | Expert's Limit (X_H) | Change in E[π] vs. Monopoly | Effect of increasing Rookie Noise (η_L) on E[π] |\n| :--- | :--- | :--- | :--- | :--- |\n| 1    | 2.0                 | 1.5 or 1.0           | Higher                       | Positive                            |\n| 2    | 1.0 or 2.0          | 0.5                  | Lower                        | Negative                            |\n\n---\n\n### Question\n\nBased on the provided theory and numerical results in Table 1, select all statements that correctly describe the conditions under which allowing a rookie trader to trade alongside an expert can increase the firm's aggregate expected profit.", "Options": {"A": "In all cases where the aggregate order limit X is constrained, adding a rookie trader is guaranteed to increase firm profits because of the information-hiding effect.", "B": "The profit gains in the successful noisy duopoly case are primarily driven by the rookie's trades being more profitable on a standalone basis than the expert's.", "C": "A high aggregate order limit (e.g., X=2) combined with a high allocation to the expert (e.g., X_H=1.5) can result in higher expected profit compared to a constrained monopoly.", "D": "Under the profitable scenario identified in the table (X=2, X_H=1.5), the 'information-hiding effect' is strong enough that increasing the rookie's signal noise (η_L) is beneficial to the firm."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the paper's core theoretical concepts (competition vs. information-hiding) with specific numerical evidence from a table. It uses an **Atomic Decomposition** strategy, breaking down the conditions for negative training costs into distinct, verifiable statements. Option A tests direct interpretation of Case 1. Option B tests the interpretation of the marginal effect of noise in Case 1. Distractor C presents an overgeneralization (a 'Conceptual: Almost Right' error), while Distractor D posits a 'Conceptual Opposite' explanation for the source of profit gains.", "qid": "300", "question": "### Background\n\n**Research Question.** Under what specific conditions involving binding order limits can a firm increase its total trading profit by allowing a less-informed (rookie) trader to trade competitively alongside an expert, effectively creating a negative “training cost”?\n\n**Setting.** A firm employs an expert trader (H) with perfect information and a rookie trader (L) whose signal about an asset's value `\\tilde{\\nu}` is corrupted by noise. The firm imposes a binding aggregate order limit `X` and allocates capital between the traders via individual limits `X_H` and `X_L`. A rational market maker sets prices based on the aggregate order flow.\n\n**Key Concepts.**\n- **Competition Effect**: In a duopoly, traders act more aggressively than a monopolist, which reveals information faster and tends to lower total profits.\n- **Information-Hiding Effect**: The rookie's noisy trades obscure the expert's trades, increasing information asymmetry, which can lead to a higher price impact and potentially higher profits for the informed traders as a group.\n\n---\n\n### Data / Model Specification\n\n**Lemma 2** from the paper states that in a constrained duopoly, the aggregate expected profit can be higher than in a constrained monopoly if the parameters fall within a specific range. There is no closed-form solution, but numerical analysis provides the following key results.\n\n**Table 1: Numerical Results for Noisy Duopoly vs. Constrained Monopoly**\n\n| Case | Aggregate Limit (X) | Expert's Limit (X_H) | Change in E[π] vs. Monopoly | Effect of increasing Rookie Noise (η_L) on E[π] |\n| :--- | :--- | :--- | :--- | :--- |\n| 1    | 2.0                 | 1.5 or 1.0           | Higher                       | Positive                            |\n| 2    | 1.0 or 2.0          | 0.5                  | Lower                        | Negative                            |\n\n---\n\n### Question\n\nBased on the provided theory and numerical results in Table 1, select all statements that correctly describe the conditions under which allowing a rookie trader to trade alongside an expert can increase the firm's aggregate expected profit.", "model_evaluation_result": {"model_raw_response": "Looking at the data and theory provided:\n\nFrom Table 1:\n- Case 1: X=2.0, X_H=1.5 or 1.0 → Higher profit vs monopoly, increasing rookie noise has positive effect\n- Case 2: X=1.0 or 2.0, X_H=0.5 → Lower profit vs monopoly, increasing rookie noise has negative effect\n\nAnalyzing each option:\n\nA) False - Case 2 shows situations where adding a rookie trader decreases profits, so it's not guaranteed in all constrained cases.\n\nB) False - The rookie has noisy information while the expert has perfect information. The profit gains come from the information-hiding effect, not from the rookie's trades being more profitable on their own.\n\nC) True - Case 1 explicitly shows that X=2 with X_H=1.5 results in higher expected profit compared to constrained monopoly.\n\nD) True - Case 1 shows that under X=2, X_H=1.5, increasing rookie noise (η_L) has a positive effect on expected profit, confirming the information-hiding effect is beneficial.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 376, "Question": "### Background\n\n**Research Question.** How do modern financial innovations in the syndicated loan market, such as the \"alphabet loan\" structure, allow LBO sponsors to increase leverage while managing different lender appetites, and what new risks do these structures create?\n\n**Setting.** This case examines the financial engineering behind tranched senior secured loans used in LBOs. This structure partitions a single senior loan into multiple classes (A, B, C) with different terms, attracting both traditional banks and non-bank lenders like prime rate funds.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Typical Characteristics of Alphabet Loan Tranches**\n\n| Characteristic | Tranche A (Term Loan A) | Tranche B (Term Loan B) |\n| :--- | :--- | :--- |\n| Lenders | Commercial Banks | Prime Rate Funds, Institutional Investors |\n| Maturity (`M`) | 6-7 years | up to 9 years |\n| Spread (`S`) | LIBOR + 200-250 bps | LIBOR + 275-375 bps |\n| Amortization (`Amort`) | Significant, scheduled | Nominal or none (bullet) |\n\n**Proposition 1.** Tranches B, C, and D have the same legal priority and security as Tranche A.\n\n---\n\n### Question\n\nBased on the provided information about alphabet loan structures, select all of the following statements that are correct.", "Options": {"A": "Tranche B lenders receive a higher spread because their claims have a lower legal priority in a bankruptcy liquidation compared to Tranche A lenders.", "B": "The primary benefit of the alphabet structure is that it allows the borrower to increase total leverage by replacing senior bank debt with subordinated debt.", "C": "Tranche B is considered riskier than Tranche A primarily due to its effective subordination in the cash flow waterfall, where principal is repaid only after Tranche A is fully amortized.", "D": "The structure segments the lender market by offering a shorter-maturity, amortizing Tranche A for risk-averse commercial banks and a higher-yield, longer-maturity Tranche B for yield-driven institutional investors."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of a specific financial innovation. It uses a 'Scenario Application' strategy with 'Atomic Decomposition' to test comprehension of market segmentation, risk pricing, and the crucial distinction between legal and effective priority. Distractor C is a 'Conceptual Opposite' that directly contradicts Proposition 1. Distractor D is a subtle 'Conceptual Error' because Tranche B is legally senior, not subordinated, debt.", "qid": "376", "question": "### Background\n\n**Research Question.** How do modern financial innovations in the syndicated loan market, such as the \"alphabet loan\" structure, allow LBO sponsors to increase leverage while managing different lender appetites, and what new risks do these structures create?\n\n**Setting.** This case examines the financial engineering behind tranched senior secured loans used in LBOs. This structure partitions a single senior loan into multiple classes (A, B, C) with different terms, attracting both traditional banks and non-bank lenders like prime rate funds.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Typical Characteristics of Alphabet Loan Tranches**\n\n| Characteristic | Tranche A (Term Loan A) | Tranche B (Term Loan B) |\n| :--- | :--- | :--- |\n| Lenders | Commercial Banks | Prime Rate Funds, Institutional Investors |\n| Maturity (`M`) | 6-7 years | up to 9 years |\n| Spread (`S`) | LIBOR + 200-250 bps | LIBOR + 275-375 bps |\n| Amortization (`Amort`) | Significant, scheduled | Nominal or none (bullet) |\n\n**Proposition 1.** Tranches B, C, and D have the same legal priority and security as Tranche A.\n\n---\n\n### Question\n\nBased on the provided information about alphabet loan structures, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at this question about alphabet loan structures, I need to carefully analyze each statement against the provided information.\n\nFrom the data given:\n- All tranches (A, B, C, D) have the same legal priority and security (Proposition 1)\n- Tranche A: 6-7 year maturity, LIBOR + 200-250 bps, significant scheduled amortization, held by commercial banks\n- Tranche B: up to 9 year maturity, LIBOR + 275-375 bps, nominal/no amortization (bullet), held by prime rate funds/institutional investors\n\nAnalyzing each option:\n\nA) This is incorrect. Proposition 1 explicitly states that all tranches have the same legal priority. The higher spread for Tranche B is not due to lower legal priority in bankruptcy.\n\nB) This is incorrect. The alphabet structure doesn't replace senior debt with subordinated debt - all tranches maintain the same senior secured status (per Proposition 1). The benefit is market segmentation, not changing the debt priority structure.\n\nC) This is correct. While all tranches have equal legal priority, Tranche B faces effective subordination through the cash flow waterfall. Since Tranche A has significant scheduled amortization while Tranche B has nominal/no amortization, Tranche A gets paid down first. This creates time subordination risk for Tranche B holders.\n\nD) This is correct. The structure clearly segments lenders: commercial banks get shorter-maturity (6-7 years), lower-spread, amortizing Tranche A that suits their risk preferences, while institutional investors get longer-maturity (up to 9 years), higher-spread, bullet Tranche B that provides the higher yields they seek.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 321, "Question": "### Background\n\n**Research Question.** This study examines why prior research has found mixed evidence on the link between board gender diversity and firm performance, proposing that the impact of female directors is task-dependent.\n\n**Setting / Data-Generating Environment.** The analysis uses survey data from CEOs of 201 Norwegian firms to measure board effectiveness in different domains.\n\n**Variables & Parameters.**\n- `Ratio of Women Directors`: The number of female directors as a percentage of total board size.\n- `Board Strategic Control`: A survey-based measure of the board's effectiveness in monitoring long-term, qualitative, and ex-ante strategic issues. The paper's theory links this to `Communal Characteristics` (e.g., interpersonal sensitivity, concern for stakeholders), which are more strongly ascribed to women.\n- `Board Operational Control`: A survey-based measure of the board's effectiveness in monitoring short-term, quantitative, and ex-post financial issues (e.g., budgets, investments).\n\n---\n\n### Data / Model Specification\n\nThe study's main empirical finding is that the effect of gender diversity is differential, as summarized in the stylized regression results below.\n\n**Table 1: Stylized Regression Results for Board Control Tasks**\n| | (1) Strategic Control | (2) Operational Control |\n|:---|:---:|:---:|\n| **Ratio of Women Directors** | **0.45*** | 0.08 |\n| | (0.18) | (0.21) |\n| Control Variables | Included | Included |\n| N | 201 | 201 |\n\n*Note: * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the provided background and **Table 1**, select all statements that accurately interpret the study's findings regarding the differential impact of women directors and their implications.", "Options": {"A": "The lack of a significant effect on operational control implies that female directors are less skilled in the quantitative tasks associated with financial monitoring.", "B": "The results indicate that the contributions of female directors are specialized, providing a comparative advantage for long-term strategic oversight but not for routine, short-term operational monitoring.", "C": "The study demonstrates that boards with more women are generally more effective across all major board functions, including both strategic and operational control.", "D": "This differential impact helps explain prior 'mixed evidence' on firm performance, as the net effect of gender diversity becomes context-dependent on which board task is more critical for a given firm's success."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret the paper's core finding of a 'differential impact' and understand its implications for the broader literature. It uses an 'atomic decomposition' strategy. Option A tests the direct interpretation of the regression results in the context of the paper's theory. Option B tests the ability to connect this finding to resolving a puzzle in prior research. Distractor C is a conceptual opposite, representing a complete misreading of the table. Distractor D is a subtle misinterpretation, confusing a null differential effect with a negative performance judgment.", "qid": "321", "question": "### Background\n\n**Research Question.** This study examines why prior research has found mixed evidence on the link between board gender diversity and firm performance, proposing that the impact of female directors is task-dependent.\n\n**Setting / Data-Generating Environment.** The analysis uses survey data from CEOs of 201 Norwegian firms to measure board effectiveness in different domains.\n\n**Variables & Parameters.**\n- `Ratio of Women Directors`: The number of female directors as a percentage of total board size.\n- `Board Strategic Control`: A survey-based measure of the board's effectiveness in monitoring long-term, qualitative, and ex-ante strategic issues. The paper's theory links this to `Communal Characteristics` (e.g., interpersonal sensitivity, concern for stakeholders), which are more strongly ascribed to women.\n- `Board Operational Control`: A survey-based measure of the board's effectiveness in monitoring short-term, quantitative, and ex-post financial issues (e.g., budgets, investments).\n\n---\n\n### Data / Model Specification\n\nThe study's main empirical finding is that the effect of gender diversity is differential, as summarized in the stylized regression results below.\n\n**Table 1: Stylized Regression Results for Board Control Tasks**\n| | (1) Strategic Control | (2) Operational Control |\n|:---|:---:|:---:|\n| **Ratio of Women Directors** | **0.45*** | 0.08 |\n| | (0.18) | (0.21) |\n| Control Variables | Included | Included |\n| N | 201 | 201 |\n\n*Note: * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the provided background and **Table 1**, select all statements that accurately interpret the study's findings regarding the differential impact of women directors and their implications.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results in Table 1 and the study's background:\n\n**Table 1 Analysis:**\n- Strategic Control: Coefficient = 0.45 (significant at 5% level)\n- Operational Control: Coefficient = 0.08 (not significant)\n\nThis shows female directors have a positive significant effect on strategic control but no significant effect on operational control.\n\n**Evaluating each option:**\n\nA) **Incorrect** - The lack of significance doesn't imply female directors are \"less skilled.\" It simply means there's no statistically significant relationship found. The study doesn't make claims about skill levels.\n\nB) **Correct** - The results show female directors have a significant positive effect on strategic control (long-term oversight) but not on operational control (short-term monitoring), indicating specialized contributions.\n\nC) **Incorrect** - The results explicitly show differential effects, not general effectiveness across all functions. There's no significant effect on operational control.\n\nD) **Correct** - The differential impact (significant for strategic, not for operational) provides an explanation for mixed prior evidence. If some firms benefit more from strategic oversight while others need operational monitoring, the overall effect of gender diversity would vary by firm context.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 264, "Question": "### Background\n\n**Research Question.** This study tests whether government-imposed barriers to international investment are effective in segmenting capital markets by analyzing the price behavior of closed-end country funds around announcements of changes in these restrictions.\n\n**Setting / Data-Generating Environment.** The study uses an event-study methodology with weekly data for five country funds. The central hypothesis is that an announcement of a liberalization of investment restrictions will cause a fund's premium to decrease as the barrier to investment is removed.\n\n---\n\n### Data / Model Specification\n\nThe core empirical test is the following regression model, estimated separately for each fund `j`:\n\n  \n\\pi_{j t}=\\delta_{0j}+\\delta_{1j}D_{1j t}+\\delta_{2j}D_{2j t}+\\delta_{3j}D_{3j t}+\\epsilon_{j t} \\quad \\text{(Eq. 1)}\n \n\nwhere `π_jt` is the weekly premium change and `D_kjt` are dummies for windows around a liberalization announcement. The key coefficient is `δ_2j`, measuring the effect during the announcement window.\n\n**Table 1: Regression Results for Changes in Fund Premiums**\n\n| Fund | `δ_1j` | `δ_2j` | `δ_3j` |\n| :--- | :--- | :--- | :--- |\n| France | -1.32 (0.09) | -3.85 (0.03) | 0.41 (0.67) |\n| Japan | -0.85 (0.02) | -0.46 (0.45) | 0.19 (0.66) |\n| Korea | -2.74 (0.01) | -3.11 (0.02) | 0.06 (0.62) |\n| Mexico | -1.13 (0.21) | -5.41 (0.00) | 0.71 (0.94) |\n| Taiwan | -0.42 (0.56) | 0.82 (0.56) | -0.10 (0.39) |\n\n*Source: Adapted from Table IV of the source paper. One-sided p-values are in parentheses.*\n\n---\n\n### Question\n\nBased on the findings in **Table 1** and the paper's overall argument, select all conclusions that are strongly supported by the evidence.", "Options": {"A": "The results imply that for countries with previously binding restrictions, the cost of capital for local projects would likely change as markets become more integrated post-liberalization.", "B": "The positive `δ_3j` coefficient for France (0.41) indicates a statistically significant price reversal in the post-announcement period, questioning the long-term impact of the liberalization.", "C": "The significant premium declines for funds like Mexico and Korea following liberalization announcements suggest that government-imposed barriers were effective in segmenting international capital markets.", "D": "The lack of a significant effect for the Japan Fund during the event window (`δ_2j` p-value = 0.45) proves that its capital markets were fully integrated with global markets."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect the specific empirical results to the paper's main theoretical conclusions. It uses a 'Premise/Assumption Packaging' strategy. Options A and B are the two central, correct conclusions drawn from the significant negative coefficients. Distractor C is a 'Conceptual' error, asking the student to correctly interpret statistical insignificance (p=0.67). Distractor D is an 'Almost Right' conceptual error; it makes too strong a claim ('proves') and ignores the significant pre-announcement effect for Japan, which suggests anticipation rather than perfect integration.", "qid": "264", "question": "### Background\n\n**Research Question.** This study tests whether government-imposed barriers to international investment are effective in segmenting capital markets by analyzing the price behavior of closed-end country funds around announcements of changes in these restrictions.\n\n**Setting / Data-Generating Environment.** The study uses an event-study methodology with weekly data for five country funds. The central hypothesis is that an announcement of a liberalization of investment restrictions will cause a fund's premium to decrease as the barrier to investment is removed.\n\n---\n\n### Data / Model Specification\n\nThe core empirical test is the following regression model, estimated separately for each fund `j`:\n\n  \n\\pi_{j t}=\\delta_{0j}+\\delta_{1j}D_{1j t}+\\delta_{2j}D_{2j t}+\\delta_{3j}D_{3j t}+\\epsilon_{j t} \\quad \\text{(Eq. 1)}\n \n\nwhere `π_jt` is the weekly premium change and `D_kjt` are dummies for windows around a liberalization announcement. The key coefficient is `δ_2j`, measuring the effect during the announcement window.\n\n**Table 1: Regression Results for Changes in Fund Premiums**\n\n| Fund | `δ_1j` | `δ_2j` | `δ_3j` |\n| :--- | :--- | :--- | :--- |\n| France | -1.32 (0.09) | -3.85 (0.03) | 0.41 (0.67) |\n| Japan | -0.85 (0.02) | -0.46 (0.45) | 0.19 (0.66) |\n| Korea | -2.74 (0.01) | -3.11 (0.02) | 0.06 (0.62) |\n| Mexico | -1.13 (0.21) | -5.41 (0.00) | 0.71 (0.94) |\n| Taiwan | -0.42 (0.56) | 0.82 (0.56) | -0.10 (0.39) |\n\n*Source: Adapted from Table IV of the source paper. One-sided p-values are in parentheses.*\n\n---\n\n### Question\n\nBased on the findings in **Table 1** and the paper's overall argument, select all conclusions that are strongly supported by the evidence.", "model_evaluation_result": {"model_raw_response": "Looking at the table and the research question, I need to evaluate each conclusion based on the statistical evidence presented.\n\n**Option A**: The results show significant premium declines for several funds (France, Korea, Mexico) following liberalization announcements. This suggests that removing barriers changes the relative attractiveness of domestic vs. foreign investment, which would indeed affect the cost of capital for local projects as markets integrate. This is strongly supported.\n\n**Option B**: While France's δ₃ⱼ coefficient is positive (0.41), the p-value is 0.67, indicating it is NOT statistically significant. Therefore, this conclusion about a \"statistically significant price reversal\" is incorrect.\n\n**Option C**: The significant negative δ₂ⱼ coefficients for Mexico (-5.41, p=0.00), Korea (-3.11, p=0.02), and France (-3.85, p=0.03) show that premiums declined when liberalization was announced. This is exactly what would happen if barriers were previously effective in segmenting markets - removing the barriers causes the premium to fall as direct investment becomes possible. This is strongly supported.\n\n**Option D**: The lack of significance for Japan (p=0.45) suggests the barriers may not have been binding, but this doesn't \"prove\" full integration. There could be other explanations, and proving full integration would require additional evidence beyond just this non-significant result. This conclusion is too strong.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 289, "Question": "### Background\n\n**Research Question.** Do insiders trade on advance knowledge of negative corporate events, such as an involuntary exchange delisting?\n\n**Setting.** The study analyzes insider trading for firms that are delisted from an exchange, an event associated with significant negative stock returns. Delisting is typically involuntary. A key challenge is distinguishing trading based on specific delisting information from trading based on the general poor performance that leads to delisting.\n\n**Variables & Parameters.**\n- `MANP_t`: Mean abnormal number of net purchasers in event month `t`.\n\n---\n\n### Data / Model Specification\n\nThe table below shows the `MANP_t` for firms approaching delisting. A negative value indicates net selling activity above historical norms.\n\n**Table 1. Mean Abnormal Net Insider Purchasing Activity (Delisting Firms)**\n\n| Event Month(s) | `MANP_t` | t-statistic |\n|:---------------|:-----------|:------------|\n| -9             | -0.460     | -2.616***   |\n| -7             | -0.358     | -1.892*     |\n| -5             | -0.403     | -2.116**    |\n| -3             | -0.510     | -2.439**    |\n| **[-12, -1]**  | **-3.446** | **-5.214*** |\n\n*Note: Table is an abridged version of the original. ***p<0.01, **p<0.05, *p<0.10.*\n\n---\n\nBased on the data and the context of involuntary delistings, which of the following statements are valid conclusions or arguments? Select all that apply.", "Options": {"A": "The data show a persistent pattern of statistically significant abnormal net selling by insiders in the months leading up to the delisting event.", "B": "The involuntary nature of delisting makes it less likely that insider selling is merely a reaction to general poor performance, as the specific timing of the delisting event is the key piece of private information.", "C": "The `MANP_t` values are consistently positive, indicating that insiders attempt to support their firm's stock price by buying shares before a delisting.", "D": "The evidence of anticipatory selling before a negative delisting event strengthens the paper's overall conclusion by demonstrating symmetric insider behavior around both good and bad news."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret statistical results for the delisting sample and synthesize them with the institutional context (involuntary nature of delisting) to evaluate the strength of the paper's argument. Depth Strategy: Reverse-Reasoning. Given the results and context, the user must identify all plausible conclusions. Distractor Logic: Option D is a 'Conceptual Opposite' distractor, directly contradicting the negative `MANP_t` values in the table. The correct options require recognizing the symmetry argument (A), understanding the nuance of involuntary events for identification (B), and accurately reading the statistical pattern in the table (C).", "qid": "289", "question": "### Background\n\n**Research Question.** Do insiders trade on advance knowledge of negative corporate events, such as an involuntary exchange delisting?\n\n**Setting.** The study analyzes insider trading for firms that are delisted from an exchange, an event associated with significant negative stock returns. Delisting is typically involuntary. A key challenge is distinguishing trading based on specific delisting information from trading based on the general poor performance that leads to delisting.\n\n**Variables & Parameters.**\n- `MANP_t`: Mean abnormal number of net purchasers in event month `t`.\n\n---\n\n### Data / Model Specification\n\nThe table below shows the `MANP_t` for firms approaching delisting. A negative value indicates net selling activity above historical norms.\n\n**Table 1. Mean Abnormal Net Insider Purchasing Activity (Delisting Firms)**\n\n| Event Month(s) | `MANP_t` | t-statistic |\n|:---------------|:-----------|:------------|\n| -9             | -0.460     | -2.616***   |\n| -7             | -0.358     | -1.892*     |\n| -5             | -0.403     | -2.116**    |\n| -3             | -0.510     | -2.439**    |\n| **[-12, -1]**  | **-3.446** | **-5.214*** |\n\n*Note: Table is an abridged version of the original. ***p<0.01, **p<0.05, *p<0.10.*\n\n---\n\nBased on the data and the context of involuntary delistings, which of the following statements are valid conclusions or arguments? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the data and context provided.\n\nLet me analyze each option:\n\n**Option A**: \"The data show a persistent pattern of statistically significant abnormal net selling by insiders in the months leading up to the delisting event.\"\n- The MANP_t values are all negative (-0.460, -0.358, -0.403, -0.510, -3.446)\n- Negative MANP_t indicates net selling above historical norms\n- All values shown are statistically significant (*, **, or ***)\n- This shows a persistent pattern of abnormal selling\n- **TRUE**\n\n**Option B**: \"The involuntary nature of delisting makes it less likely that insider selling is merely a reaction to general poor performance, as the specific timing of the delisting event is the key piece of private information.\"\n- The background states delisting is \"typically involuntary\"\n- It mentions the challenge of \"distinguishing trading based on specific delisting information from trading based on the general poor performance\"\n- For involuntary delistings, the specific timing would be private information that insiders might have\n- This makes the selling pattern more likely to be based on specific delisting knowledge rather than just general performance\n- **TRUE**\n\n**Option C**: \"The MANP_t values are consistently positive, indicating that insiders attempt to support their firm's stock price by buying shares before a delisting.\"\n- All MANP_t values shown are negative, not positive\n- Negative values indicate net selling, not buying\n- **FALSE**\n\n**Option D**: \"The evidence of anticipatory selling before a negative delisting event strengthens the paper's overall conclusion by demonstrating symmetric insider behavior around both good and bad news.\"\n- The data only shows behavior around delisting (bad news)\n- There's no information about insider behavior around good news\n- We cannot conclude anything about \"symmetric\" behavior from this data alone\n- **FALSE**\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 245, "Question": "### Background\n\n**Research Question.** How do different reinsurance pricing principles behave under increasing risk aversion, and what are the practical implications for contract negotiation when the underlying loss distribution is heavy-tailed, characteristic of catastrophic events?\n\n**Setting.** We analyze a limited stop-loss reinsurance contract, `I(X) = (X-a)_+ ∧ b`, where the underlying loss `X` follows a heavy-tailed Pareto distribution. This scenario is designed to have the same expected underlying loss (`E[X]=100`) and expected ceded loss (`E[I(X)]=60.65`) as a corresponding case with an exponential distribution, allowing for a direct comparison of the impact of tail risk.\n\n### Data / Model Specification\n\nThe following table presents calculated reinsurance premiums for the specified contract under a Pareto loss distribution. The weight factor `α` represents the reinsurer's risk aversion or share of the loss function weight.\n\n**Table 1. Reinsurance premiums for `I(X)=(X-30)_+ ∧ 287.07` and Pareto Risk `X`**\n\n| α | `~e_{X,I(X),α}` | `e_α(I(X))` | Modified Quantile | Classical Quantile | Risk-Adjusted |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n| 0.82 | 80.95 | 113.22 | 101.55 | 124.22 | 108.14 |\n| 0.85 | 93.06 | 124.45 | 126.55 | 146.41 | 122.13 |\n| 0.88 | 107.88 | 138.00 | 158.53 | 175.48 | 140.44 |\n| 0.90 | 119.95 | 148.90 | 186.03 | 200.89 | 143.53 |\n| 0.92 | 134.61 | 161.97 | 221.36 | 234.16 | 143.53 |\n| 0.95 | 164.57 | 188.16 | 287.07 | 287.07 | 143.53 |\n| 0.98 | 215.43 | 231.00 | 287.07 | 287.07 | 143.53 |\n\n*Note: The policy limit `b` is 287.07. The expected ceded loss `E[I(X)]` is 60.65.*\n\n### Question\n\nBased on the data in Table 1 and the principles discussed in the paper, select all of the following statements that are valid conclusions or interpretations.", "Options": {"A": "The modified expectile premium `~e_{X,I(X),α}` is consistently higher than the classical expectile premium `e_α(I(X))` for all values of `α` shown, reflecting its more accurate modeling of the insurer's total risk exposure.", "B": "For a reinsurer with a high risk aversion (`α ≥ 0.95`), the choice between the Modified Quantile and Classical Quantile principles is immaterial as both result in a premium equal to the policy limit of 287.07.", "C": "The Risk-Adjusted premium principle is less sensitive to increases in the reinsurer's risk aversion `α` in the high-aversion range (e.g., `α > 0.90`) compared to the modified expectile premium `~e_{X,I(X),α}`.", "D": "At `α = 0.90`, the premium loading for the modified expectile premium (`~e_{X,I(X),α} - E[I(X)]`) is less than the premium loading for the classical expectile premium (`e_α(I(X)) - E[I(X)]`)."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: Synthesize and interpret data from a table to compare the behavior of different reinsurance premium principles under a heavy-tailed risk distribution. Strategy: Computational Judgment. Distractor Logic: Option C is a Conceptual Opposite; the paper (and table) shows the modified premium is *lower* than the classical one, as stated in Proposition 3.3 and Remark 3.2. Options A, B, and D require direct data extraction and simple calculations (e.g., calculating premium loading) from the table, confirming their validity.", "qid": "245", "question": "### Background\n\n**Research Question.** How do different reinsurance pricing principles behave under increasing risk aversion, and what are the practical implications for contract negotiation when the underlying loss distribution is heavy-tailed, characteristic of catastrophic events?\n\n**Setting.** We analyze a limited stop-loss reinsurance contract, `I(X) = (X-a)_+ ∧ b`, where the underlying loss `X` follows a heavy-tailed Pareto distribution. This scenario is designed to have the same expected underlying loss (`E[X]=100`) and expected ceded loss (`E[I(X)]=60.65`) as a corresponding case with an exponential distribution, allowing for a direct comparison of the impact of tail risk.\n\n### Data / Model Specification\n\nThe following table presents calculated reinsurance premiums for the specified contract under a Pareto loss distribution. The weight factor `α` represents the reinsurer's risk aversion or share of the loss function weight.\n\n**Table 1. Reinsurance premiums for `I(X)=(X-30)_+ ∧ 287.07` and Pareto Risk `X`**\n\n| α | `~e_{X,I(X),α}` | `e_α(I(X))` | Modified Quantile | Classical Quantile | Risk-Adjusted |\n|:---:|:---:|:---:|:---:|:---:|:---:|\n| 0.82 | 80.95 | 113.22 | 101.55 | 124.22 | 108.14 |\n| 0.85 | 93.06 | 124.45 | 126.55 | 146.41 | 122.13 |\n| 0.88 | 107.88 | 138.00 | 158.53 | 175.48 | 140.44 |\n| 0.90 | 119.95 | 148.90 | 186.03 | 200.89 | 143.53 |\n| 0.92 | 134.61 | 161.97 | 221.36 | 234.16 | 143.53 |\n| 0.95 | 164.57 | 188.16 | 287.07 | 287.07 | 143.53 |\n| 0.98 | 215.43 | 231.00 | 287.07 | 287.07 | 143.53 |\n\n*Note: The policy limit `b` is 287.07. The expected ceded loss `E[I(X)]` is 60.65.*\n\n### Question\n\nBased on the data in Table 1 and the principles discussed in the paper, select all of the following statements that are valid conclusions or interpretations.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each statement:\n\n**Statement A**: \"The modified expectile premium `~e_{X,I(X),α}` is consistently higher than the classical expectile premium `e_α(I(X))` for all values of `α` shown\"\n\nChecking the data:\n- α = 0.82: 80.95 < 113.22 ❌\n- α = 0.85: 93.06 < 124.45 ❌\n- α = 0.88: 107.88 < 138.00 ❌\n- α = 0.90: 119.95 < 148.90 ❌\n- α = 0.92: 134.61 < 161.97 ❌\n- α = 0.95: 164.57 < 188.16 ❌\n- α = 0.98: 215.43 < 231.00 ❌\n\nThe modified expectile premium is consistently LOWER than the classical expectile premium, not higher. **Statement A is FALSE**.\n\n**Statement B**: \"For a reinsurer with high risk aversion (`α ≥ 0.95`), the choice between the Modified Quantile and Classical Quantile principles is immaterial as both result in a premium equal to the policy limit of 287.07\"\n\nChecking the data for α ≥ 0.95:\n- α = 0.95: Modified Quantile = 287.07, Classical Quantile = 287.07 ✓\n- α = 0.98: Modified Quantile = 287.07, Classical Quantile = 287.07 ✓\n\nBoth quantile methods reach the policy limit for high risk aversion. **Statement B is TRUE**.\n\n**Statement C**: \"The Risk-Adjusted premium principle is less sensitive to increases in the reinsurer's risk aversion `α` in the high-aversion range (e.g., `α > 0.90`) compared to the modified expectile premium\"\n\nFor α > 0.90:\n- Risk-Adjusted premium: stays constant at 143.53 for α = 0", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 327, "Question": "### Background\n\n**Research Question.** This case provides a comprehensive empirical evaluation of the consumption-based asset pricing model (CCAPM), testing whether incorporating taxes and tax-timing options can resolve its well-known empirical failures.\n\n**Setting.** The analysis compares the benchmark tax-free CCAPM (Model 1) against the paper's preferred specification, an asymmetric tax model where losses are realized immediately and gains are realized after one year at a preferential rate (Model 4). Both models are estimated using the Generalized Method of Moments (GMM) on monthly U.S. data from 1959-1990 for an equity portfolio and a one-month T-bill.\n\n**Variables and Parameters.**\n\n*   `β`: Subjective time discount factor.\n*   `γ`: Utility parameter, where `1-γ` is the coefficient of relative risk aversion (RRA).\n*   `τ_c`: Short-term capital gains tax rate.\n*   `τ_d`: Tax rate on dividend/interest income.\n*   `J-statistic (χ²)`: GMM test for a model's overidentifying restrictions. A high value (low p-value) indicates model rejection.\n\n---\n\n### Data / Model Specification\n\nThe GMM estimation results for the tax-free and asymmetric tax models are summarized in Table 1.\n\n**Table 1: GMM Estimation Results for Competing CCAPM Specifications**\n\n| Parameter / Statistic | Model 1 (Tax-Free) | Model 4 (Asymmetric Tax) |\n| :--- | :--- | :--- |\n| `β` (Time Preference) | 0.998 (0.001) | 1.000 (0.001) |\n| `γ` (Utility Parameter) | 1.176 (0.371) | 0.982 (0.114) |\n| `τ_c` (Capital Gains Tax) | - | 0.099 (0.022) |\n| `τ_d` (Dividend Tax) | - | 0.320 (0.113) |\n| **Goodness-of-fit** | | |\n| `J-statistic (χ²)` | 33.424 | 11.154 |\n| Degrees of Freedom | 6 | 4 |\n| p-value | (< 0.001) | (0.025) |\n\n*Standard errors are in parentheses.* \n\n---\n\n### Question\n\nBased on the GMM estimation results in Table 1, select ALL statements that correctly interpret the parameter estimates and their implications for the models.", "Options": {"A": "Model 1 implies a negative coefficient of relative risk aversion (1 - 1.176 = -0.176), which contradicts the standard economic assumption of risk-averse agents.", "B": "The utility parameter γ is statistically indistinguishable between Model 1 (1.176) and Model 4 (0.982), indicating that taxes do not resolve the risk aversion puzzle.", "C": "In Model 4, the dividend tax rate (τ_d = 32.0%) is not statistically significant at the 5% level, weakening the case for including taxes.", "D": "Model 4 produces a statistically significant and positive estimate for the short-term capital gains tax rate (τ_c = 9.9%), a major improvement in economic plausibility over the tax-free model."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret GMM parameter estimates and their statistical and economic significance. It uses an **Atomic Decomposition** strategy, breaking down the multifaceted comparison from the original QA into discrete, testable propositions. The depth strategy is **Computational Judgment**, as it requires calculating the RRA and t-statistics to evaluate the options. Distractors are high-fidelity: Option C is a **Numerical (Comprehension Slip)** error, as the t-statistic (0.320/0.113 ≈ 2.83) is significant. Option D is a **Conceptual (Misinterpretation)** error, as the point estimates imply qualitatively different economic behavior (risk-loving vs. risk-averse) and are statistically different.", "qid": "327", "question": "### Background\n\n**Research Question.** This case provides a comprehensive empirical evaluation of the consumption-based asset pricing model (CCAPM), testing whether incorporating taxes and tax-timing options can resolve its well-known empirical failures.\n\n**Setting.** The analysis compares the benchmark tax-free CCAPM (Model 1) against the paper's preferred specification, an asymmetric tax model where losses are realized immediately and gains are realized after one year at a preferential rate (Model 4). Both models are estimated using the Generalized Method of Moments (GMM) on monthly U.S. data from 1959-1990 for an equity portfolio and a one-month T-bill.\n\n**Variables and Parameters.**\n\n*   `β`: Subjective time discount factor.\n*   `γ`: Utility parameter, where `1-γ` is the coefficient of relative risk aversion (RRA).\n*   `τ_c`: Short-term capital gains tax rate.\n*   `τ_d`: Tax rate on dividend/interest income.\n*   `J-statistic (χ²)`: GMM test for a model's overidentifying restrictions. A high value (low p-value) indicates model rejection.\n\n---\n\n### Data / Model Specification\n\nThe GMM estimation results for the tax-free and asymmetric tax models are summarized in Table 1.\n\n**Table 1: GMM Estimation Results for Competing CCAPM Specifications**\n\n| Parameter / Statistic | Model 1 (Tax-Free) | Model 4 (Asymmetric Tax) |\n| :--- | :--- | :--- |\n| `β` (Time Preference) | 0.998 (0.001) | 1.000 (0.001) |\n| `γ` (Utility Parameter) | 1.176 (0.371) | 0.982 (0.114) |\n| `τ_c` (Capital Gains Tax) | - | 0.099 (0.022) |\n| `τ_d` (Dividend Tax) | - | 0.320 (0.113) |\n| **Goodness-of-fit** | | |\n| `J-statistic (χ²)` | 33.424 | 11.154 |\n| Degrees of Freedom | 6 | 4 |\n| p-value | (< 0.001) | (0.025) |\n\n*Standard errors are in parentheses.* \n\n---\n\n### Question\n\nBased on the GMM estimation results in Table 1, select ALL statements that correctly interpret the parameter estimates and their implications for the models.", "model_evaluation_result": {"model_raw_response": "Looking at the estimation results in Table 1, I need to evaluate each statement:\n\n**Statement A**: Model 1 has γ = 1.176, which means the coefficient of relative risk aversion (RRA) = 1 - γ = 1 - 1.176 = -0.176. A negative RRA indeed contradicts the standard economic assumption that agents are risk-averse (which requires RRA > 0). This statement is **correct**.\n\n**Statement B**: To determine if the γ parameters are statistically distinguishable, I need to check if their confidence intervals overlap:\n- Model 1: γ = 1.176 (0.371), so 95% CI ≈ [0.434, 1.918]\n- Model 4: γ = 0.982 (0.114), so 95% CI ≈ [0.758, 1.206]\n\nThe confidence intervals overlap substantially (from 0.758 to 1.206), indicating the parameters are not statistically distinguishable at conventional significance levels. This statement is **correct**.\n\n**Statement C**: The dividend tax rate in Model 4 is τ_d = 0.320 with SE = 0.113. The t-statistic = 0.320/0.113 ≈ 2.83, which exceeds the critical value of 1.96 for 5% significance. Therefore, τ_d is statistically significant at the 5% level. This statement is **incorrect**.\n\n**Statement D**: The capital gains tax rate in Model 4 is τ_c = 0.099 with SE = 0.022. The t-statistic = 0.099/0.022 ≈ 4.5, which is highly significant. The positive estimate is economically plausible (tax rates should be positive), which is an improvement over Model 1 that has no tax parameters and produces an implausible negative RRA. This statement is **correct**.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 389, "Question": "### Background\n\n**Research Question.** Does the degree of information asymmetry affect the magnitude of Seasoned Equity Offering (SEO) underpricing? The paper's central hypothesis is that SEOs by REITs that went public in or after 1990 (\"post-1990\") are more underpriced than those by REITs that went public before 1990 (\"pre-1990\").\n\n**Setting.** The study posits that post-1990 REITs are characterized by greater valuation uncertainty because they are actively managed \"management plays\" with valuable growth options, often use complex UPREIT organizational structures with potential conflicts of interest, and have significantly higher ownership by better-informed institutional investors.\n\n**Variables & Parameters.**\n- `Underpricing (UR)`: The offer-to-close return `(P_1 / P_0 - 1) * 100` (percentage).\n- `LNOFFSIZE`: Natural log of the offer size.\n- `LNINST`: Natural log of (1 + institutional ownership fraction).\n- `UNDRANK`: Underwriter rank (higher rank = better reputation).\n- `UPREIT`: Dummy variable (1 if UPREIT structure).\n\n---\n\n### Data / Model Specification\n\nThe study estimates a multivariate OLS regression for each subsample to test the asymmetric information hypothesis. A representative model is:\n\n  \nUR_{j}=\\beta_{1}+\\beta_{2} UNDRANK_{j} + \\beta_{3} UPREIT_{j} +\\beta_{4}LNINST_{j}+\\beta_{5}LNOFFSIZE_{j}+\\varepsilon_{j} \\quad \\text{(Eq. (1))}\n \n\nKey empirical results are summarized below.\n\n**Table 1. Multivariate OLS Regression Estimates of Underpricing (UR)**\n\n| Variable | Pre-1990 IPO (n=57) | Post-1990 IPO (n=114) |\n| :--- | :--- | :--- |\n| LNOFFSIZE | 1.154 (1.87)* | -0.671 (-2.49)* |\n| LNINST | -0.573 (-1.46) | 0.690 (1.75)* |\n| UNDRANK | -0.207 (-0.88) | -0.221 (-1.84)* |\n| UPREIT | -0.123 (-0.14) | 0.715 (1.98)* |\n\n*Source: Synthesized from Model 2 results in Table 5 of the source document. t-statistics are in parentheses. An asterisk (*) denotes significance at the 10% level or lower.*\n\n---\n\n### Question\n\nBased on the multivariate regression results for the **post-1990 sample** in **Table 1**, select all findings that support the asymmetric information hypothesis.", "Options": {"A": "REITs with the more complex UPREIT structure were associated with significantly more underpricing.", "B": "The size of the offer (`LNOFFSIZE`) was not a statistically significant predictor of underpricing for this group.", "C": "Higher institutional ownership (`LNINST`) was associated with significantly *less* underpricing.", "D": "SEOs with higher-ranked underwriters (`UNDRANK`) were associated with significantly less underpricing."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the core multivariate findings supporting the paper's central hypothesis. It uses an **Atomic Decomposition** strategy, breaking the overall conclusion into testable propositions about individual regression coefficients. Correct options (A, B) accurately state significant findings consistent with the theory (reputable underwriters and simpler structures reduce asymmetry and thus underpricing). Distractor C is a **Conceptual Opposite** (reversing the sign of the LNINST coefficient), while Distractor D is a **Factual Error** (LNOFFSIZE was significant).", "qid": "389", "question": "### Background\n\n**Research Question.** Does the degree of information asymmetry affect the magnitude of Seasoned Equity Offering (SEO) underpricing? The paper's central hypothesis is that SEOs by REITs that went public in or after 1990 (\"post-1990\") are more underpriced than those by REITs that went public before 1990 (\"pre-1990\").\n\n**Setting.** The study posits that post-1990 REITs are characterized by greater valuation uncertainty because they are actively managed \"management plays\" with valuable growth options, often use complex UPREIT organizational structures with potential conflicts of interest, and have significantly higher ownership by better-informed institutional investors.\n\n**Variables & Parameters.**\n- `Underpricing (UR)`: The offer-to-close return `(P_1 / P_0 - 1) * 100` (percentage).\n- `LNOFFSIZE`: Natural log of the offer size.\n- `LNINST`: Natural log of (1 + institutional ownership fraction).\n- `UNDRANK`: Underwriter rank (higher rank = better reputation).\n- `UPREIT`: Dummy variable (1 if UPREIT structure).\n\n---\n\n### Data / Model Specification\n\nThe study estimates a multivariate OLS regression for each subsample to test the asymmetric information hypothesis. A representative model is:\n\n  \nUR_{j}=\\beta_{1}+\\beta_{2} UNDRANK_{j} + \\beta_{3} UPREIT_{j} +\\beta_{4}LNINST_{j}+\\beta_{5}LNOFFSIZE_{j}+\\varepsilon_{j} \\quad \\text{(Eq. (1))}\n \n\nKey empirical results are summarized below.\n\n**Table 1. Multivariate OLS Regression Estimates of Underpricing (UR)**\n\n| Variable | Pre-1990 IPO (n=57) | Post-1990 IPO (n=114) |\n| :--- | :--- | :--- |\n| LNOFFSIZE | 1.154 (1.87)* | -0.671 (-2.49)* |\n| LNINST | -0.573 (-1.46) | 0.690 (1.75)* |\n| UNDRANK | -0.207 (-0.88) | -0.221 (-1.84)* |\n| UPREIT | -0.123 (-0.14) | 0.715 (1.98)* |\n\n*Source: Synthesized from Model 2 results in Table 5 of the source document. t-statistics are in parentheses. An asterisk (*) denotes significance at the 10% level or lower.*\n\n---\n\n### Question\n\nBased on the multivariate regression results for the **post-1990 sample** in **Table 1**, select all findings that support the asymmetric information hypothesis.", "model_evaluation_result": {"model_raw_response": "Looking at the post-1990 IPO results in Table 1, I need to identify which findings support the asymmetric information hypothesis.\n\nThe asymmetric information hypothesis suggests that greater information asymmetry leads to more underpricing. Let me analyze each option:\n\nA) UPREIT coefficient = 0.715 (1.98)* - This is positive and significant, meaning UPREIT structures are associated with MORE underpricing. Since UPREITs are more complex structures that increase information asymmetry, this supports the hypothesis. ✓\n\nB) LNOFFSIZE coefficient = -0.671 (-2.49)* - This is negative and significant (marked with *), so offer size IS a statistically significant predictor. This statement is false. ✗\n\nC) LNINST coefficient = 0.690 (1.75)* - This is positive and significant, meaning higher institutional ownership is associated with MORE underpricing, not less. This statement is false. ✗\n\nD) UNDRANK coefficient = -0.221 (-1.84)* - This is negative and significant, meaning higher-ranked underwriters are associated with LESS underpricing. Since better underwriters reduce information asymmetry and this reduces underpricing, this supports the hypothesis. ✓\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 307, "Question": "### Background\n\n**Research Question.** Does diversification affect the market valuation of Real Estate Investment Trusts (REITs)? Answering this requires first understanding the typical diversification strategy of REITs and then estimating the relationship between diversification and value.\n\n**Setting.** The analysis uses summary statistics to profile the sample and a baseline regression model to estimate the diversification discount. All variables in the regression are standardized to have a mean of zero and a standard deviation of one.\n\n**Variables & Parameters.**\n*   `PropertyTypeDiversification`: Measure of diversification across property types (Negative Herfindahl Index).\n*   `GeographicDiversification(Region)`: Measure of diversification across geographic regions (Negative Herfindahl Index).\n*   `Tobin's q`: The dependent variable in the regression, measuring firm value.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Summary Statistics (Selected Variables)**\n| Variable | Mean | Median |\n| :--- | :--- | :--- |\n| PropertyTypeDiversification | -0.837 | -0.965 |\n| GeographicDiversification(Region) | -0.451 | -0.357 |\n\n*Note: A Herfindahl Index of 1.0 (perfect concentration) corresponds to a diversification score of -1.0.*\n\n**Table 2. The Relation Between Diversification and Firm Value**\n| Dependent variable: Tobin's q | (1) Regions |\n| :--- | :---: |\n| **Geographic Diversification** | **-0.159** |\n| | (2.35)** |\n| **PropType Diversification** | **-0.032** |\n| | (0.72) |\n\n*Note: t-statistics in parentheses. ** denotes significance at the 5% level.* \n\n---\n\n### Question\n\nBased on the descriptive statistics in Table 1 and regression results in Table 2, which of the following statements about REIT diversification strategies and their valuation impact are correct? Select all that apply.", "Options": {"A": "The coefficient of -0.159 on Geographic Diversification indicates a diversification premium, as firms become more focused (less diversified), their value increases.", "B": "The typical REIT in the sample is highly diversified across both property types and geographic regions, as shown in Table 1.", "C": "The data shows a statistically significant geographic diversification discount, where a one standard deviation increase in diversification is associated with a 0.159 standard deviation decrease in Tobin's q.", "D": "The lack of a significant valuation effect for property type diversification is likely attributable to the high concentration (low variation) in this dimension, as shown by the median PropertyTypeDiversification score of -0.965 in Table 1."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize descriptive statistics with baseline regression results to understand both a key finding (the geographic discount) and a key null result. It uses an Atomic Decomposition strategy. Distractor (B) is a 'Conceptual Opposite' regarding property type diversification. Distractor (C) is an 'Almost Right' conceptual error that mislabels the nature of the discount, a common point of confusion.", "qid": "307", "question": "### Background\n\n**Research Question.** Does diversification affect the market valuation of Real Estate Investment Trusts (REITs)? Answering this requires first understanding the typical diversification strategy of REITs and then estimating the relationship between diversification and value.\n\n**Setting.** The analysis uses summary statistics to profile the sample and a baseline regression model to estimate the diversification discount. All variables in the regression are standardized to have a mean of zero and a standard deviation of one.\n\n**Variables & Parameters.**\n*   `PropertyTypeDiversification`: Measure of diversification across property types (Negative Herfindahl Index).\n*   `GeographicDiversification(Region)`: Measure of diversification across geographic regions (Negative Herfindahl Index).\n*   `Tobin's q`: The dependent variable in the regression, measuring firm value.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Summary Statistics (Selected Variables)**\n| Variable | Mean | Median |\n| :--- | :--- | :--- |\n| PropertyTypeDiversification | -0.837 | -0.965 |\n| GeographicDiversification(Region) | -0.451 | -0.357 |\n\n*Note: A Herfindahl Index of 1.0 (perfect concentration) corresponds to a diversification score of -1.0.*\n\n**Table 2. The Relation Between Diversification and Firm Value**\n| Dependent variable: Tobin's q | (1) Regions |\n| :--- | :---: |\n| **Geographic Diversification** | **-0.159** |\n| | (2.35)** |\n| **PropType Diversification** | **-0.032** |\n| | (0.72) |\n\n*Note: t-statistics in parentheses. ** denotes significance at the 5% level.* \n\n---\n\n### Question\n\nBased on the descriptive statistics in Table 1 and regression results in Table 2, which of the following statements about REIT diversification strategies and their valuation impact are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the data provided in Tables 1 and 2.\n\nLet me analyze each option:\n\n**Option A**: \"The coefficient of -0.159 on Geographic Diversification indicates a diversification premium, as firms become more focused (less diversified), their value increases.\"\n\nThis is incorrect. A negative coefficient on diversification indicates a diversification **discount**, not a premium. The negative relationship means that as diversification increases, firm value decreases.\n\n**Option B**: \"The typical REIT in the sample is highly diversified across both property types and geographic regions, as shown in Table 1.\"\n\nLooking at Table 1:\n- PropertyTypeDiversification: Mean = -0.837, Median = -0.965\n- GeographicDiversification(Region): Mean = -0.451, Median = -0.357\n\nSince these are negative Herfindahl indices where -1.0 represents perfect concentration, values close to -1.0 indicate high concentration (low diversification). The property type scores are very close to -1.0, indicating high concentration. The geographic scores show more diversification but still lean toward concentration. This statement is incorrect.\n\n**Option C**: \"The data shows a statistically significant geographic diversification discount, where a one standard deviation increase in diversification is associated with a 0.159 standard deviation decrease in Tobin's q.\"\n\nFrom Table 2:\n- Geographic Diversification coefficient = -0.159\n- t-statistic = 2.35 (significant at 5% level, denoted by **)\n- Since variables are standardized, the coefficient directly represents the effect of a one standard deviation change\n\nThis statement is correct.\n\n**Option D**: \"The lack of a significant valuation effect for property type diversification is likely attributable to the high concentration (low variation) in this dimension, as shown by the median PropertyTypeDiversification score of -0.965 in Table 1.\"\n\nFrom the data:\n- Property Type Diversification coefficient = -0.032\n- t-statistic = 0.72 (not significant)\n- Median = -0.965 (very close to -1.0, indicating high concentration)\n\nWhen most firms are highly concentrated (low variation in the independent variable), it", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 256, "Question": "### Background\n\n**Research Question.** Does a fundamental institutional preference for glamour stocks (and aversion to value stocks) persist after accounting for well-known drivers of institutional demand such as firm size and momentum?\n\n**Setting / Data-Generating Environment.** The study employs Fama-MacBeth style regressions. For each quarter `t`, a cross-sectional regression of the `Adjusted TR(i,t)` is run on lagged firm characteristics. The final reported coefficients are the time-series averages of the quarterly regression coefficients.\n\n### Data / Model Specification\n\nThe multivariate regression model for each quarter `t` is:\n  \n\\text{Adjusted } TR_{i,t} = \\alpha_t + \\beta_{1,t}(\\text{Lag size}_{i,t-1}) + \\beta_{2,t}(\\text{Past 6-month return}_{i,t-1}) + \\epsilon_{i,t} \n \n**Table 1. Time-Series Averages of Coefficients from Quarterly Regressions**\n| | **Panel A. Glamour Stocks** | **Panel B. Value Stocks** |\n| :--- | :--- | :--- |\n| Intercept | 0.3571*** | -0.1141*** |\n| Lag size | -0.0404*** | 0.0254** |\n| Past 6-month return | -0.0260 | 0.2374*** |\n\n*Note: *** denotes significance at 1%, ** at 5%.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following statements accurately describe institutional trading preferences?\n", "Options": {"A": "Within the universe of value stocks, institutions show a statistically significant preference for those with stronger past 6-month returns.", "B": "After controlling for size and momentum, a significant baseline institutional preference for buying glamour stocks and selling value stocks remains.", "C": "Across both glamour and value stocks, institutions consistently prefer larger firms, as indicated by positive and significant 'Lag size' coefficients in both panels.", "D": "Within the universe of glamour stocks, institutions show a statistically significant preference for smaller firms over larger firms."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item tests the ability to interpret multiple coefficients from a multivariate regression table and compare results across different sample groups (Glamour vs. Value).\nDepth Strategy: Scenario Application. The general regression model is provided, and the user must apply it to interpret the specific coefficient estimates for different stock styles.\nDistractor Logic:\n- A (Correct): This is a correct interpretation of the significant positive intercept in Panel A and the significant negative intercept in Panel B.\n- B (Correct): This is a correct interpretation of the significant positive coefficient on 'Past 6-month return' in Panel B.\n- C (Correct): This is a correct interpretation of the significant negative coefficient on 'Lag size' in Panel A.\n- D (Incorrect): This is a Conceptual Opposite / Factual Error. The 'Lag size' coefficient is significantly negative for glamour stocks, indicating a preference for smaller firms in that category, which contradicts the statement.", "qid": "256", "question": "### Background\n\n**Research Question.** Does a fundamental institutional preference for glamour stocks (and aversion to value stocks) persist after accounting for well-known drivers of institutional demand such as firm size and momentum?\n\n**Setting / Data-Generating Environment.** The study employs Fama-MacBeth style regressions. For each quarter `t`, a cross-sectional regression of the `Adjusted TR(i,t)` is run on lagged firm characteristics. The final reported coefficients are the time-series averages of the quarterly regression coefficients.\n\n### Data / Model Specification\n\nThe multivariate regression model for each quarter `t` is:\n  \n\\text{Adjusted } TR_{i,t} = \\alpha_t + \\beta_{1,t}(\\text{Lag size}_{i,t-1}) + \\beta_{2,t}(\\text{Past 6-month return}_{i,t-1}) + \\epsilon_{i,t} \n \n**Table 1. Time-Series Averages of Coefficients from Quarterly Regressions**\n| | **Panel A. Glamour Stocks** | **Panel B. Value Stocks** |\n| :--- | :--- | :--- |\n| Intercept | 0.3571*** | -0.1141*** |\n| Lag size | -0.0404*** | 0.0254** |\n| Past 6-month return | -0.0260 | 0.2374*** |\n\n*Note: *** denotes significance at 1%, ** at 5%.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following statements accurately describe institutional trading preferences?\n", "model_evaluation_result": {"model_raw_response": "Looking at the regression results in Table 1:\n\n**Panel A (Glamour Stocks):**\n- Intercept: 0.3571*** (significant at 1%)\n- Lag size: -0.0404*** (significant at 1%)\n- Past 6-month return: -0.0260 (not significant)\n\n**Panel B (Value Stocks):**\n- Intercept: -0.1141*** (significant at 1%)\n- Lag size: 0.0254** (significant at 5%)\n- Past 6-month return: 0.2374*** (significant at 1%)\n\nLet me evaluate each option:\n\n**Option A:** \"Within the universe of value stocks, institutions show a statistically significant preference for those with stronger past 6-month returns.\"\n- In Panel B (Value Stocks), the Past 6-month return coefficient is 0.2374***, which is positive and significant at 1%.\n- This indicates institutions prefer value stocks with better recent performance. ✓\n\n**Option B:** \"After controlling for size and momentum, a significant baseline institutional preference for buying glamour stocks and selling value stocks remains.\"\n- The intercepts represent baseline preferences after controlling for other variables.\n- Glamour stocks intercept: 0.3571*** (positive, significant)\n- Value stocks intercept: -0.1141*** (negative, significant)\n- This shows institutions have a baseline preference for glamour (positive) and aversion to value (negative). ✓\n\n**Option C:** \"Across both glamour and value stocks, institutions consistently prefer larger firms...\"\n- Glamour stocks Lag size: -0.0404*** (negative)\n- Value stocks Lag size: 0.0254** (positive)\n- The signs are opposite, so institutions do NOT consistently prefer larger firms across both categories. ✗\n\n**Option D:** \"Within the universe of glamour stocks, institutions show a statistically significant preference for smaller firms over larger firms.\"\n- Glamour stocks Lag size: -0.0404*** (negative and significant)\n- A negative coefficient means institutions prefer smaller firms within glamour stocks. ✓\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 392, "Question": "### Background\n\n**Research Question.** How can one estimate the *causal* effect of SEO underpricing on a firm's subsequent stock performance, avoiding biases from unobserved confounding factors (e.g., 'market hype') that might drive both?\n\n**Setting.** A standard OLS regression of future returns on underpricing may be biased. An instrumental variable (IV) approach can potentially solve this problem. A proposed instrument for `Underpricing` is the rank of the lead underwriter (`UNDRANK`).\n\n---\n\n### Question\n\nTo estimate the causal effect of `Underpricing` on future returns, one could use underwriter rank (`UNDRANK`) as an instrumental variable (IV). For an IV strategy to be valid, the instrument must satisfy two key conditions. Select all statements that correctly describe these conditions as they apply to `UNDRANK`.", "Options": {"A": "Orthogonality: `Underpricing` must be statistically independent of `UNDRANK`.", "B": "Exclusion Restriction: `UNDRANK` must be uncorrelated with any unobserved factors that determine future returns, affecting returns *only* through its influence on `Underpricing`.", "C": "Exogeneity: `UNDRANK` must be highly correlated with the future stock returns it is trying to explain.", "D": "Relevance: `UNDRANK` must have a statistically significant causal effect on the level of `Underpricing`."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item converts a difficult, open-ended question about research design into a focused test of the underlying theoretical principles. It uses a **Premise/Assumption Packaging** strategy, asking for the necessary conditions for a valid IV. Correct options (A, B) state the relevance and exclusion restriction conditions accurately. Distractor C is a **Conceptual Error** that describes a violation of the exclusion restriction. Distractor D is a **Conceptual Opposite** of the relevance condition.", "qid": "392", "question": "### Background\n\n**Research Question.** How can one estimate the *causal* effect of SEO underpricing on a firm's subsequent stock performance, avoiding biases from unobserved confounding factors (e.g., 'market hype') that might drive both?\n\n**Setting.** A standard OLS regression of future returns on underpricing may be biased. An instrumental variable (IV) approach can potentially solve this problem. A proposed instrument for `Underpricing` is the rank of the lead underwriter (`UNDRANK`).\n\n---\n\n### Question\n\nTo estimate the causal effect of `Underpricing` on future returns, one could use underwriter rank (`UNDRANK`) as an instrumental variable (IV). For an IV strategy to be valid, the instrument must satisfy two key conditions. Select all statements that correctly describe these conditions as they apply to `UNDRANK`.", "model_evaluation_result": {"model_raw_response": "To determine which statements correctly describe the two key conditions for a valid instrumental variable, I need to recall the fundamental requirements for IV estimation:\n\n1. **Relevance**: The instrument must be correlated with the endogenous variable (Underpricing)\n2. **Exclusion Restriction/Exogeneity**: The instrument must affect the outcome (future returns) only through the endogenous variable, not directly\n\nLet me evaluate each option:\n\nA) \"Orthogonality: `Underpricing` must be statistically independent of `UNDRANK`\"\n- This is incorrect. The instrument must be correlated with (not independent of) the endogenous variable for relevance.\n\nB) \"Exclusion Restriction: `UNDRANK` must be uncorrelated with any unobserved factors that determine future returns, affecting returns *only* through its influence on `Underpricing`\"\n- This is correct. This describes the exclusion restriction perfectly.\n\nC) \"Exogeneity: `UNDRANK` must be highly correlated with the future stock returns it is trying to explain\"\n- This is incorrect. The instrument should NOT be directly correlated with the outcome variable (future returns). It should only affect the outcome through the endogenous variable.\n\nD) \"Relevance: `UNDRANK` must have a statistically significant causal effect on the level of `Underpricing`\"\n- This is correct. This describes the relevance condition accurately.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 172, "Question": "### Background\n\n**Research Question.** What is the appropriate econometric framework for modeling the dynamic relationships between house prices, liquidity, and market tightness in a short panel dataset where variables may have different time-series properties?\n\n**Setting.** The study uses a quarterly panel of 403 Dutch municipalities from 2011 to 2013 (`T=12`, `N=403`). A panel Vector Autoregression (VAR) is proposed, but its specification and estimation must account for non-stationarity and the biases inherent in short dynamic panels.\n\n**Variables & Parameters.**\n- `pr`: Log house price index.\n- `ros`: Rate of sale (liquidity measure).\n- `cph`: Log clicks per house (market tightness measure).\n- `Δ`: The first-difference operator.\n- `I(1)` / `I(0)`: Integrated of order one (non-stationary) / zero (stationary).\n\n### Data / Model Specification\n\nTo determine the correct specification, panel unit root tests were conducted. The results are in Table 1. The proposed empirical model is a panel VAR, estimated using the Generalized Method of Moments (GMM).\n\n**Table 1. Panel Unit Root Test Results**\n\n| Variable | Lags | p-value | Conclusion |\n| :--- | :--- | :--- | :--- |\n| `pr` | 1 | 1.0000 | Non-stationary |\n| `ros` | 1 | 0.0020 | Stationary |\n| `cph` | 1 | 0.0000 | Stationary |\n| `Δpr` | 1 | 0.0000 | Stationary |\n\n*Notes: H₀: All panels contain unit roots. A low p-value rejects H₀.*\n\n### Question\n\nBased on the provided methodological details, which of the following statements accurately describe the econometric specification and estimation strategy used in the paper? (Select all that apply)", "Options": {"A": "The log house price series (`pr`) is included in the VAR in first-differences because unit root tests show it is non-stationary (`I(1)`), and this avoids the problem of spurious regression.", "B": "According to the unit root tests, the rate of sale (`ros`) and clicks per house (`cph`) are non-stationary and must also be first-differenced.", "C": "Nickell's bias is a concern in panels with a large time dimension (`large T`) and is addressed by using standard fixed effects estimation.", "D": "The choice of GMM for estimation is necessary to correct for Nickell's bias, which occurs when including lagged dependent variables in a panel with a small time dimension (`T`)."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the paper's core econometric methodology. It uses a **Premise/Assumption Packaging** strategy, combining key justifications for the model specification and estimation choices. Correct options (A, B) accurately state the reason for differencing the price series (non-stationarity) and for using GMM (Nickell's bias in short panels). Distractors are based on classic econometric misconceptions: (C) misreads the unit root test results for the other variables, and (D) incorrectly defines the conditions for Nickell's bias (`small T`, not `large T`) and its solution (GMM, not standard fixed effects).", "qid": "172", "question": "### Background\n\n**Research Question.** What is the appropriate econometric framework for modeling the dynamic relationships between house prices, liquidity, and market tightness in a short panel dataset where variables may have different time-series properties?\n\n**Setting.** The study uses a quarterly panel of 403 Dutch municipalities from 2011 to 2013 (`T=12`, `N=403`). A panel Vector Autoregression (VAR) is proposed, but its specification and estimation must account for non-stationarity and the biases inherent in short dynamic panels.\n\n**Variables & Parameters.**\n- `pr`: Log house price index.\n- `ros`: Rate of sale (liquidity measure).\n- `cph`: Log clicks per house (market tightness measure).\n- `Δ`: The first-difference operator.\n- `I(1)` / `I(0)`: Integrated of order one (non-stationary) / zero (stationary).\n\n### Data / Model Specification\n\nTo determine the correct specification, panel unit root tests were conducted. The results are in Table 1. The proposed empirical model is a panel VAR, estimated using the Generalized Method of Moments (GMM).\n\n**Table 1. Panel Unit Root Test Results**\n\n| Variable | Lags | p-value | Conclusion |\n| :--- | :--- | :--- | :--- |\n| `pr` | 1 | 1.0000 | Non-stationary |\n| `ros` | 1 | 0.0020 | Stationary |\n| `cph` | 1 | 0.0000 | Stationary |\n| `Δpr` | 1 | 0.0000 | Stationary |\n\n*Notes: H₀: All panels contain unit roots. A low p-value rejects H₀.*\n\n### Question\n\nBased on the provided methodological details, which of the following statements accurately describe the econometric specification and estimation strategy used in the paper? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at the unit root test results and the methodological context:\n\nFrom Table 1:\n- `pr` has p-value = 1.0000 → Cannot reject H₀ → Non-stationary (I(1))\n- `ros` has p-value = 0.0020 → Reject H₀ → Stationary (I(0))\n- `cph` has p-value = 0.0000 → Reject H₀ → Stationary (I(0))\n- `Δpr` has p-value = 0.0000 → Reject H₀ → Stationary (I(0))\n\nEvaluating each option:\n\nA) TRUE - The unit root tests show `pr` is non-stationary (I(1)), and first-differencing it (Δpr) makes it stationary. This is the correct approach to avoid spurious regression.\n\nB) FALSE - The unit root tests show `ros` and `cph` are stationary (I(0)), not non-stationary. They do not need to be first-differenced.\n\nC) FALSE - Nickell's bias is a concern in panels with small T (not large T). The setting has T=12, which is small. Also, standard fixed effects estimation does not address this bias.\n\nD) TRUE - With T=12 and a dynamic panel VAR (which includes lagged dependent variables), Nickell's bias is indeed a concern. GMM is an appropriate method to address this bias in short panels.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 269, "Question": "### Background\n\nThe paper tests the \"flow management hypothesis\" using an interaction model to distinguish whether derivatives improve performance by mitigating flow risk or through other channels like market timing.\n\n### Data / Model Specification\n\nThe model is:\n  \n\\alpha_{i}=\\dots+\\beta_{3}flow\\_risk_{i}+\\beta_{4}flow\\_mgmt_{i}+\\beta_{5}derivatives_{i} +\\dots+\\varepsilon_{i}\n \nwhere `flow_mgmt_i = derivatives_i × flow_risk_i`.\n\n---\n\nAccording to the paper's theoretical framework and empirical strategy for testing the flow management hypothesis, which of the following statements are correct interpretations or predictions?\n", "Options": {"A": "The marginal effect of `flow_risk` on alpha for a derivative-using fund is given by the sum of two coefficients: `β_3 + β_4`.", "B": "The flow management hypothesis predicts that the coefficient on the interaction term, `β_4`, should be positive, indicating that derivatives reduce the negative impact of flow risk.", "C": "The flow management hypothesis predicts that the coefficient on `flow_risk`, `β_3`, should be positive, as higher risk requires more skillful management.", "D": "The coefficient `β_5` on the main `derivatives` term isolates the performance impact of using derivatives that is unrelated to flow management, such as fixed implementation costs or fees."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses a deep understanding of the econometric specification of an interaction model and how it's used to test a specific economic hypothesis. It requires interpreting the roles of the main and interaction effects in the context of the paper's core argument. It uses a Reverse-Reasoning strategy (identifying correct theoretical predictions).\n\nDistractor Logic:\n- A (Correct): Correctly derives the marginal effect for the treatment group (`derivatives_i = 1`) in an interaction model.\n- B (Correct): Correctly states the central prediction of the flow management hypothesis for the interaction term's coefficient.\n- C (Correct): Correctly interprets the main effect's coefficient (`β_5`) in an interaction model as the effect when the interacting variable (`flow_risk`) is zero.\n- D (Incorrect): This is a Conceptual Opposite. The hypothesis predicts `β_3` will be negative, as it represents the baseline performance drag from flow risk for funds that *do not* use derivatives.", "qid": "269", "question": "### Background\n\nThe paper tests the \"flow management hypothesis\" using an interaction model to distinguish whether derivatives improve performance by mitigating flow risk or through other channels like market timing.\n\n### Data / Model Specification\n\nThe model is:\n  \n\\alpha_{i}=\\dots+\\beta_{3}flow\\_risk_{i}+\\beta_{4}flow\\_mgmt_{i}+\\beta_{5}derivatives_{i} +\\dots+\\varepsilon_{i}\n \nwhere `flow_mgmt_i = derivatives_i × flow_risk_i`.\n\n---\n\nAccording to the paper's theoretical framework and empirical strategy for testing the flow management hypothesis, which of the following statements are correct interpretations or predictions?\n", "model_evaluation_result": {"model_raw_response": "Looking at this model specification and the flow management hypothesis:\n\nThe model is:\nα_i = ... + β_3(flow_risk_i) + β_4(flow_mgmt_i) + β_5(derivatives_i) + ... + ε_i\n\nwhere flow_mgmt_i = derivatives_i × flow_risk_i\n\nLet me analyze each option:\n\n**Option A**: The marginal effect of flow_risk on alpha for a derivative-using fund (derivatives_i = 1) is:\n∂α/∂flow_risk = β_3 + β_4(derivatives_i) = β_3 + β_4(1) = β_3 + β_4\nThis is correct.\n\n**Option B**: The flow management hypothesis suggests that derivatives help mitigate the negative impact of flow risk. If flow risk normally hurts performance (β_3 < 0), then derivatives should reduce this negative impact. This would mean β_4 > 0, making the total effect (β_3 + β_4) less negative. This interpretation is correct.\n\n**Option C**: The flow management hypothesis would predict that flow risk is generally harmful to performance, so β_3 should be negative, not positive. This statement is incorrect.\n\n**Option D**: The coefficient β_5 captures the direct effect of derivatives on performance that is independent of flow risk (when flow_risk = 0). This could include fixed costs, fees, or other non-flow-related impacts of derivative usage. This interpretation is correct.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 364, "Question": "### Background\n\n**Research Question.** Is there a post-earnings announcement drift (PEAD) following management earnings forecasts (MEFs), and does its magnitude differ between growth and value firms? How does the pattern of drift relate to the immediate market reaction to the news?\n\n**Setting and Sample.** The study analyzes abnormal returns for Australian firms following an MEF. It distinguishes between the immediate 3-day reaction and the subsequent 60-day drift.\n\n**Variables and Parameters.**\n- `BH_3`: The 3-day market-adjusted abnormal return centered on the MEF announcement.\n- `BH_60`: The 60-day size-adjusted buy-and-hold abnormal return, calculated for the window beginning 2 days after the MEF announcement and ending 61 days after (`[+2, +61]`).\n- `Growth`: A dummy variable equal to unity if a firm falls within the lowest two quintiles of the book-to-market ratio.\n- `Value`: A dummy variable equal to unity if a firm falls within the highest two quintiles of the book-to-market ratio.\n- `D_BadA`: A dummy variable equal to unity for MEFs classified as bad news relative to the mean analyst earnings benchmark.\n\n---\n\n### Data / Model Specification\n\nThe study estimates an interaction model to analyze both the immediate return and the post-announcement drift. The key results for bad news announcements are summarized from the 'Overall' sample analysis in the paper's Tables 3 and 4.\n\n  \n\\text{Abnormal Return} = \\alpha + \\beta_{1}(\\text{Growth} \\cdot D_{\\mathrm{BadA}}) + \\beta_{3}(\\text{Value} \\cdot D_{\\mathrm{BadA}}) + \\dots + \\text{Controls} + \\varepsilon\n \n\n**Table 1: Summary of Market Reactions to Bad News MEFs**\n\n| Variable | Immediate Return (`BH_3`) | Post-Announcement Drift (`BH_60`) |\n| :--- | :--- | :--- |\n| `Growth * D_BadA` (β1) | -0.053*** | -0.0422*** |\n| `Value * D_BadA` (β3) | -0.027*** | -0.0504*** |\n| **Wald Test: |β1|=|β3|** | p = 0.029 | p = 0.696 |\n\n*Source: 'Overall' columns of Tables 3 and 4 in the source paper. *** indicates p<0.01.* \n\n---\n\n### Question\n\nBased on the model specification and empirical results in **Table 1**, which of the following statements accurately describe the market reaction to bad news MEFs?", "Options": {"A": "The post-announcement drift (`BH_60`) is significantly more negative for value firms than for growth firms at the 5% level.", "B": "The total expected abnormal return over the 63-day period (combining immediate reaction and post-announcement drift) for a growth firm is approximately -9.52%.", "C": "The immediate return (`BH_3`) accounts for the majority of the total 63-day negative return for value firms.", "D": "The immediate negative return (`BH_3`) is significantly more severe for growth firms than for value firms at the 5% level."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret statistical significance from p-values and perform multi-step calculations using regression coefficients from a table. It uses an Atomic Decomposition strategy to create a genuine multi-select question. Distractor C tests the common confusion between magnitude and statistical significance. Distractor D targets a core conclusion of the paper regarding the *timing* of the reaction for value firms, making it a high-fidelity misconception.", "qid": "364", "question": "### Background\n\n**Research Question.** Is there a post-earnings announcement drift (PEAD) following management earnings forecasts (MEFs), and does its magnitude differ between growth and value firms? How does the pattern of drift relate to the immediate market reaction to the news?\n\n**Setting and Sample.** The study analyzes abnormal returns for Australian firms following an MEF. It distinguishes between the immediate 3-day reaction and the subsequent 60-day drift.\n\n**Variables and Parameters.**\n- `BH_3`: The 3-day market-adjusted abnormal return centered on the MEF announcement.\n- `BH_60`: The 60-day size-adjusted buy-and-hold abnormal return, calculated for the window beginning 2 days after the MEF announcement and ending 61 days after (`[+2, +61]`).\n- `Growth`: A dummy variable equal to unity if a firm falls within the lowest two quintiles of the book-to-market ratio.\n- `Value`: A dummy variable equal to unity if a firm falls within the highest two quintiles of the book-to-market ratio.\n- `D_BadA`: A dummy variable equal to unity for MEFs classified as bad news relative to the mean analyst earnings benchmark.\n\n---\n\n### Data / Model Specification\n\nThe study estimates an interaction model to analyze both the immediate return and the post-announcement drift. The key results for bad news announcements are summarized from the 'Overall' sample analysis in the paper's Tables 3 and 4.\n\n  \n\\text{Abnormal Return} = \\alpha + \\beta_{1}(\\text{Growth} \\cdot D_{\\mathrm{BadA}}) + \\beta_{3}(\\text{Value} \\cdot D_{\\mathrm{BadA}}) + \\dots + \\text{Controls} + \\varepsilon\n \n\n**Table 1: Summary of Market Reactions to Bad News MEFs**\n\n| Variable | Immediate Return (`BH_3`) | Post-Announcement Drift (`BH_60`) |\n| :--- | :--- | :--- |\n| `Growth * D_BadA` (β1) | -0.053*** | -0.0422*** |\n| `Value * D_BadA` (β3) | -0.027*** | -0.0504*** |\n| **Wald Test: |β1|=|β3|** | p = 0.029 | p = 0.696 |\n\n*Source: 'Overall' columns of Tables 3 and 4 in the source paper. *** indicates p<0.01.* \n\n---\n\n### Question\n\nBased on the model specification and empirical results in **Table 1**, which of the following statements accurately describe the market reaction to bad news MEFs?", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each statement:\n\nA) The post-announcement drift (`BH_60`) is significantly more negative for value firms than for growth firms at the 5% level.\n- Growth firms: β1 = -0.0422\n- Value firms: β3 = -0.0504\n- Wald Test for |β1|=|β3|: p = 0.696\n- Since p = 0.696 > 0.05, the difference is NOT significant at the 5% level.\n- FALSE\n\nB) The total expected abnormal return over the 63-day period (combining immediate reaction and post-announcement drift) for a growth firm is approximately -9.52%.\n- Growth firm immediate return (BH_3): -0.053 = -5.3%\n- Growth firm drift (BH_60): -0.0422 = -4.22%\n- Total: -5.3% + (-4.22%) = -9.52%\n- TRUE\n\nC) The immediate return (`BH_3`) accounts for the majority of the total 63-day negative return for value firms.\n- Value firm immediate return (BH_3): -0.027 = -2.7%\n- Value firm drift (BH_60): -0.0504 = -5.04%\n- Total: -2.7% + (-5.04%) = -7.74%\n- Proportion from immediate return: 2.7/7.74 = 34.9%\n- This is less than 50%, so NOT the majority\n- FALSE\n\nD) The immediate negative return (`BH_3`) is significantly more severe for growth firms than for value firms at the 5% level.\n- Growth firms: β1 = -0.053\n- Value firms: β3 = -0.027\n- Wald Test for |β1|=|β3|: p = 0.029\n- Since p = 0.029 < 0.05, the difference IS significant at the 5% level.\n- TRUE\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 388, "Question": "### Background\n\n**Research Question.** How do flawed internal performance metrics and incentive schemes lead to systematic capital misallocation, creating vulnerabilities that activists can exploit through strategic proposals like spin-offs or redesigned compensation contracts?\n\n**Setting.** A multi-division corporation where capital allocation decisions are decentralized. Divisional managers are compensated based on their division's performance, but the chosen metric may not align with overall shareholder value creation. The firm operates an internal capital market to allocate a fixed capital budget between its divisions.\n\n**Variables & Parameters.**\n*   `ROIC`: Return on Invested Capital (Net Operating Profit After Tax / Invested Capital).\n*   `WACC`: Weighted Average Cost of Capital, the firm's hurdle rate.\n*   `V_A(K_A)`, `V_B(K_B)`: The standalone value of Division A and B, respectively, as a function of the capital `K` allocated to them. Assume these functions exhibit positive but diminishing returns to capital (i.e., `V' > 0`, `V'' < 0`).\n\n---\n\n### Data / Model Specification\n\nThe paper argues that activists often push for the breakup of conglomerates due to \"poor capital allocation decisions as one business subsidizes the other.\" This can happen when a conglomerate systematically overallocates capital to its less productive divisions, resulting in an allocation where the marginal return to capital is unequal across divisions (`V_A'(K_A) > V_B'(K_B)`). Activists may propose structural changes like spin-offs to resolve these inefficiencies.\n\n---\n\n### Question\n\nAccording to the paper's logic on conglomerate structure and capital allocation, select all statements that are TRUE.", "Options": {"A": "A spin-off can create value by dismantling an inefficient internal capital market, forcing the newly independent businesses to raise capital externally based on their standalone prospects.", "B": "In a conglomerate with inefficient capital allocation, value is destroyed because one business unit effectively subsidizes another, preventing capital from flowing to its highest-return use.", "C": "The primary value created by a spin-off is the reduction of corporate-level taxes, which is more significant than any gains from improved capital allocation.", "D": "A conglomerate creates maximum value when it allocates capital such that the average ROIC is equal across all divisions."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the understanding of a key strategic vulnerability in conglomerates—inefficient internal capital markets—and the mechanism by which a spin-off acts as a structural remedy. It uses a premise-packaging strategy. The correct options (B, C) identify the core problem (cross-subsidization) and the solution (dismantling the internal market). Distractor A targets the common economic error of confusing average vs. marginal returns in optimization problems. Distractor D presents an alternative, but secondary, rationale for spin-offs to test for focus on the paper's central argument.", "qid": "388", "question": "### Background\n\n**Research Question.** How do flawed internal performance metrics and incentive schemes lead to systematic capital misallocation, creating vulnerabilities that activists can exploit through strategic proposals like spin-offs or redesigned compensation contracts?\n\n**Setting.** A multi-division corporation where capital allocation decisions are decentralized. Divisional managers are compensated based on their division's performance, but the chosen metric may not align with overall shareholder value creation. The firm operates an internal capital market to allocate a fixed capital budget between its divisions.\n\n**Variables & Parameters.**\n*   `ROIC`: Return on Invested Capital (Net Operating Profit After Tax / Invested Capital).\n*   `WACC`: Weighted Average Cost of Capital, the firm's hurdle rate.\n*   `V_A(K_A)`, `V_B(K_B)`: The standalone value of Division A and B, respectively, as a function of the capital `K` allocated to them. Assume these functions exhibit positive but diminishing returns to capital (i.e., `V' > 0`, `V'' < 0`).\n\n---\n\n### Data / Model Specification\n\nThe paper argues that activists often push for the breakup of conglomerates due to \"poor capital allocation decisions as one business subsidizes the other.\" This can happen when a conglomerate systematically overallocates capital to its less productive divisions, resulting in an allocation where the marginal return to capital is unequal across divisions (`V_A'(K_A) > V_B'(K_B)`). Activists may propose structural changes like spin-offs to resolve these inefficiencies.\n\n---\n\n### Question\n\nAccording to the paper's logic on conglomerate structure and capital allocation, select all statements that are TRUE.", "model_evaluation_result": {"model_raw_response": "Looking at this question about capital allocation in conglomerates and activist interventions, I need to evaluate each statement based on the paper's logic.\n\nThe key insight from the background is that value is maximized when marginal returns to capital are equalized across divisions (V_A'(K_A) = V_B'(K_B)), not average returns. The paper emphasizes how poor internal capital allocation leads to subsidization and inefficiency.\n\nLet me analyze each option:\n\n**Option A**: \"A spin-off can create value by dismantling an inefficient internal capital market, forcing the newly independent businesses to raise capital externally based on their standalone prospects.\"\n- This aligns perfectly with the paper's logic. When internal capital markets misallocate resources, external markets can better evaluate and fund each business based on its true prospects.\n- TRUE ✓\n\n**Option B**: \"In a conglomerate with inefficient capital allocation, value is destroyed because one business unit effectively subsidizes another, preventing capital from flowing to its highest-return use.\"\n- This directly reflects the paper's argument about \"poor capital allocation decisions as one business subsidizes the other.\"\n- The subsidization prevents optimal allocation where marginal returns would be equalized.\n- TRUE ✓\n\n**Option C**: \"The primary value created by a spin-off is the reduction of corporate-level taxes...\"\n- The paper focuses on capital allocation inefficiencies, not tax benefits.\n- This contradicts the paper's emphasis on operational improvements through better capital allocation.\n- FALSE ✗\n\n**Option D**: \"A conglomerate creates maximum value when it allocates capital such that the average ROIC is equal across all divisions.\"\n- This is incorrect. Optimal allocation requires equal marginal returns (V_A'(K_A) = V_B'(K_B)), not equal average returns.\n- With diminishing returns, equalizing average ROIC would lead to inefficient allocation.\n- FALSE ✗\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 184, "Question": "### Background\n\n**Research Question.** Do institutional investors exhibit return-chasing behavior in specific equity styles, and do their subsequent fund flows influence the performance of those styles?\n\n**Setting and Data.** The study uses a two-equation system to analyze the relationship between investor fund flows and style portfolio returns, using monthly data from 1979-2004. The `INDE` portfolio is an equal-weighted portfolio of Large Growth and Small Value styles.\n\n---\n\n### Data / Model Specification\n\n1.  A regression to test for return-chasing, where current fund flows (`DIFF_t`, proxying net institutional flows) are regressed on past style returns:\n      \n    \\mathrm{DIFF}_{t} = b_0 + b_{1}R_{\\mathrm{FFLG},t-1} + b_{2}R_{\\mathrm{FFLV},t-1} + ... + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n     \n\n2.  An augmented five-factor model to test the performance impact of flows, where portfolio returns (`R_pt`) are regressed on standard factors plus the contemporaneous flow variable:\n      \n    R_{pt} = a_{i0} + ... + b_{i5}\\mathrm{DIFF}_t + \\upsilon_t \\quad \\text{(Eq. (2))}\n     \n\n**Table 1: Investor Class Test Results**\n\n| Regression | Coefficient | Estimate | t-statistic |\n| :--- | :--- | :--- | :--- |\n| **Panel A: Fund Flow Allocation (Eq. 1)** | | |\n| | `b_1` on `R_{FFLG,t-1}` | 0.216 | 2.453 |\n| | `b_2` on `R_{FFLV,t-1}` | -0.292 | -3.106 |\n| **Panel B: Performance Impact (Eq. 2)** | | |\n| For INDE Portfolio | `b_{i5}` on `DIFF_t` | 0.041 | 3.616 |\n\n---\n\n### Question\n\nThe paper uses this two-equation system to test a behavioral narrative about institutional investors. Based on the results in **Table 1**, select all conclusions that are supported by the evidence.", "Options": {"A": "The evidence from Eq. (1) suggests that retail investors, not institutional investors, tend to chase positive returns in the Large Value style.", "B": "The results from Eq. (1) indicate that institutional investors exhibit return-chasing behavior in the Large Growth style, as shown by the positive and significant coefficient on lagged FFLG returns.", "C": "The results from Eq. (2) show that net institutional fund flows (`DIFF_t`) have a positive and statistically significant contemporaneous association with the returns of the `INDE` portfolio, even after controlling for standard risk factors.", "D": "The significant coefficient on `DIFF_t` in Eq. (2) definitively proves that institutional fund flows are the primary cause of momentum in the `INDE` portfolio."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item tests the ability to interpret and synthesize the results of a two-stage regression analysis to build a coherent behavioral story. It uses an Atomic Decomposition strategy. Options A and B are direct interpretations of the two key regression results. Option C requires a slightly deeper inference: since `DIFF` is Institutional-Retail flows, a negative coefficient on past returns means positive returns are associated with lower net institutional flows, implying retail flows are dominant. Distractor D is an 'Almost Right' error that overstates causality, a critical distinction in econometrics.", "qid": "184", "question": "### Background\n\n**Research Question.** Do institutional investors exhibit return-chasing behavior in specific equity styles, and do their subsequent fund flows influence the performance of those styles?\n\n**Setting and Data.** The study uses a two-equation system to analyze the relationship between investor fund flows and style portfolio returns, using monthly data from 1979-2004. The `INDE` portfolio is an equal-weighted portfolio of Large Growth and Small Value styles.\n\n---\n\n### Data / Model Specification\n\n1.  A regression to test for return-chasing, where current fund flows (`DIFF_t`, proxying net institutional flows) are regressed on past style returns:\n      \n    \\mathrm{DIFF}_{t} = b_0 + b_{1}R_{\\mathrm{FFLG},t-1} + b_{2}R_{\\mathrm{FFLV},t-1} + ... + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n     \n\n2.  An augmented five-factor model to test the performance impact of flows, where portfolio returns (`R_pt`) are regressed on standard factors plus the contemporaneous flow variable:\n      \n    R_{pt} = a_{i0} + ... + b_{i5}\\mathrm{DIFF}_t + \\upsilon_t \\quad \\text{(Eq. (2))}\n     \n\n**Table 1: Investor Class Test Results**\n\n| Regression | Coefficient | Estimate | t-statistic |\n| :--- | :--- | :--- | :--- |\n| **Panel A: Fund Flow Allocation (Eq. 1)** | | |\n| | `b_1` on `R_{FFLG,t-1}` | 0.216 | 2.453 |\n| | `b_2` on `R_{FFLV,t-1}` | -0.292 | -3.106 |\n| **Panel B: Performance Impact (Eq. 2)** | | |\n| For INDE Portfolio | `b_{i5}` on `DIFF_t` | 0.041 | 3.616 |\n\n---\n\n### Question\n\nThe paper uses this two-equation system to test a behavioral narrative about institutional investors. Based on the results in **Table 1**, select all conclusions that are supported by the evidence.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each conclusion against the evidence provided in Table 1.\n\nLet me analyze each option:\n\n**Option A**: \"The evidence from Eq. (1) suggests that retail investors, not institutional investors, tend to chase positive returns in the Large Value style.\"\n- The study explicitly states that DIFF_t proxies for \"net institutional flows\"\n- Equation (1) tests institutional investor behavior, not retail investor behavior\n- The coefficient for Large Value (b_2 = -0.292) is negative, not positive\n- This conclusion is not supported\n\n**Option B**: \"The results from Eq. (1) indicate that institutional investors exhibit return-chasing behavior in the Large Growth style, as shown by the positive and significant coefficient on lagged FFLG returns.\"\n- The coefficient b_1 on R_FFLG,t-1 is 0.216 with t-statistic 2.453\n- This is positive and statistically significant (t > 2)\n- This shows institutional investors increase flows following positive Large Growth returns\n- This conclusion is supported\n\n**Option C**: \"The results from Eq. (2) show that net institutional fund flows (DIFF_t) have a positive and statistically significant contemporaneous association with the returns of the INDE portfolio, even after controlling for standard risk factors.\"\n- The coefficient b_i5 on DIFF_t is 0.041 with t-statistic 3.616\n- This is positive and highly statistically significant\n- Equation (2) is described as an \"augmented five-factor model,\" indicating it controls for standard risk factors\n- This conclusion is supported\n\n**Option D**: \"The significant coefficient on DIFF_t in Eq. (2) definitively proves that institutional fund flows are the primary cause of momentum in the INDE portfolio.\"\n- While the coefficient is significant, this shows association, not causation\n- The word \"definitively proves\" overstates what regression analysis can establish\n- \"Primary cause\" is too strong a claim based solely on this evidence\n- This conclusion is not supported\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 101, "Question": "### Background\n\n**Research Question.** How can the theoretical predictions about inflation-driven wealth transfers be empirically tested, and what are the main findings from a cross-sectional analysis of stock returns?\n\n**Setting.** The study uses a multivariate cross-sectional regression to test its hypotheses over three distinct time periods with varying levels of inflation. The key hypotheses are that a firm's stock price is higher the higher its net debtor position (`a_1 > 0`), the lower its proportion of fixed assets (`a_2 < 0`), and the lower its inventory undercosting (`a_3 < 0`).\n\n### Data / Model Specification\n\nThe theoretical model is tested using the following cross-sectional regression of average monthly returns (`y_i`) on firm characteristics:\n  \ny_{i}=\\alpha+a_{0}\\beta_{i}+a_{1}X_{1i}+a_{2}X_{2i}+a_{3}X_{3i}+u_{i}\n \nwhere `y` is the average monthly return, `β` is systematic risk, `X_1` is the net debtor position, `X_2` is fixed assets (plant), and `X_3` is the inventory cost understatement, all scaled by book value.\n\n**Table 1: Cross-Sectional Regression Results (Full Specification)**\n\n| Period | Total Inflation | Const. `α` | Sys. Risk `a_0` | Debt `a_1` | Plant `a_2` | Inv. `a_3` | R² |\n|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| 1964-68 | 17% | .0083 (2.75) | .0068 (3.18) | .0006 (0.18) | -.0143 (-3.86) | -.0099 (-1.57) | 0.73 |\n| 1959-63 | 5% | .0087 (2.00) | -.0043 (-1.18) | -.0027 (-0.66) | -.0047 (-2.32) | -.0061 (-1.71) | 0.28 |\n| 1954-58 | 8% | .0180 (2.54) | .0047 (2.54) | .0014 (0.34) | -.0122 (-3.14) | -.0042 (-0.63) | 0.44 |\n*Note: t-values are in parentheses. A one-tailed t-test at 5% significance requires a t-value of approximately ±1.7.* \n\n### Question\n\nBased on the regression results in **Table 1**, select all statements that represent a correct interpretation of the findings.", "Options": {"A": "The systematic risk coefficient (`a_0`) is positive and statistically significant in all three periods, consistent with standard asset pricing theory.", "B": "The pattern of the plant coefficient (`a_2`) across the three periods is consistent with the theory that the tax-shield loss from fixed assets is more severe during periods of higher inflation.", "C": "The Debtor-Creditor hypothesis (`a_1 > 0`) is not statistically supported in any of the three periods, as the t-statistics for the debt coefficient are consistently small.", "D": "The inventory tax effect (`a_3 < 0`) is statistically significant at the 5% level in the high-inflation 1964-68 period."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret multivariate regression results in the context of economic hypotheses. It uses an Atomic Decomposition strategy, breaking down the paper's main findings into distinct, verifiable claims about statistical significance and comparative statics. Distractors are designed based on common misinterpretations: confusing significance levels (the paper notes 10% significance for `a_3`, not 5%) and misreading results for a specific sub-period (the risk coefficient `a_0` is insignificant in 1959-63).", "qid": "101", "question": "### Background\n\n**Research Question.** How can the theoretical predictions about inflation-driven wealth transfers be empirically tested, and what are the main findings from a cross-sectional analysis of stock returns?\n\n**Setting.** The study uses a multivariate cross-sectional regression to test its hypotheses over three distinct time periods with varying levels of inflation. The key hypotheses are that a firm's stock price is higher the higher its net debtor position (`a_1 > 0`), the lower its proportion of fixed assets (`a_2 < 0`), and the lower its inventory undercosting (`a_3 < 0`).\n\n### Data / Model Specification\n\nThe theoretical model is tested using the following cross-sectional regression of average monthly returns (`y_i`) on firm characteristics:\n  \ny_{i}=\\alpha+a_{0}\\beta_{i}+a_{1}X_{1i}+a_{2}X_{2i}+a_{3}X_{3i}+u_{i}\n \nwhere `y` is the average monthly return, `β` is systematic risk, `X_1` is the net debtor position, `X_2` is fixed assets (plant), and `X_3` is the inventory cost understatement, all scaled by book value.\n\n**Table 1: Cross-Sectional Regression Results (Full Specification)**\n\n| Period | Total Inflation | Const. `α` | Sys. Risk `a_0` | Debt `a_1` | Plant `a_2` | Inv. `a_3` | R² |\n|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| 1964-68 | 17% | .0083 (2.75) | .0068 (3.18) | .0006 (0.18) | -.0143 (-3.86) | -.0099 (-1.57) | 0.73 |\n| 1959-63 | 5% | .0087 (2.00) | -.0043 (-1.18) | -.0027 (-0.66) | -.0047 (-2.32) | -.0061 (-1.71) | 0.28 |\n| 1954-58 | 8% | .0180 (2.54) | .0047 (2.54) | .0014 (0.34) | -.0122 (-3.14) | -.0042 (-0.63) | 0.44 |\n*Note: t-values are in parentheses. A one-tailed t-test at 5% significance requires a t-value of approximately ±1.7.* \n\n### Question\n\nBased on the regression results in **Table 1**, select all statements that represent a correct interpretation of the findings.", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each statement against the regression results:\n\n**Statement A**: \"The systematic risk coefficient (`a_0`) is positive and statistically significant in all three periods\"\n- 1964-68: a_0 = 0.0068, t = 3.18 (significant, as |t| > 1.7)\n- 1959-63: a_0 = -0.0043, t = -1.18 (negative and not significant)\n- 1954-58: a_0 = 0.0047, t = 2.54 (significant)\nThis is FALSE because a_0 is negative in 1959-63 and not significant in that period.\n\n**Statement B**: \"The pattern of the plant coefficient (`a_2`) across the three periods is consistent with the theory that the tax-shield loss from fixed assets is more severe during periods of higher inflation\"\n- The theory predicts a_2 < 0, with more negative values during higher inflation\n- 1964-68 (17% inflation): a_2 = -0.0143\n- 1959-63 (5% inflation): a_2 = -0.0047\n- 1954-58 (8% inflation): a_2 = -0.0122\nThe coefficient is most negative (-0.0143) in the highest inflation period and least negative (-0.0047) in the lowest inflation period. This is TRUE.\n\n**Statement C**: \"The Debtor-Creditor hypothesis (`a_1 > 0`) is not statistically supported in any of the three periods\"\n- 1964-68: a_1 = 0.0006, t = 0.18 (not significant)\n- 1959-63: a_1 = -0.0027, t = -0.66 (not significant)\n- 1954-58: a_1 = 0.0014, t = 0.34 (not significant)\nAll t-statistics are well below 1.7 in absolute value. This is TRUE.\n\n**Statement D**: \"The inventory tax effect (`a_3 < 0`) is statistically significant at the 5", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 272, "Question": "### Background\n\n**Research Question.** This case evaluates the empirical performance of the Empirical Average Cost of Capital (EACC) model against conventional WACC measures by testing their ability to forecast future corporate profitability.\n\n**Setting.** The analysis focuses on McDonald's and its industry (SIC 58) from 2000-Q4 to 2016-Q1. The EACC is estimated using a rolling 40-quarter regression, and these estimates are used to produce one-quarter-ahead forecasts of Net Operating Profit After Taxes (NOPAT).\n\n**Variables & Parameters.**\n- `EACC`: Empirical Average Cost of Capital, estimated using three different measures of `TOTAL CAPITAL`: Book Value, Market Value, and the average of the two (dimensionless).\n- `WACC`: Conventional Weighted Average Cost of Capital, including a textbook CAPM-based estimate and various Ibbotson estimates (dimensionless).\n- `MAE`: Mean Absolute Error of the quarterly NOPAT forecasts, expressed as a percentage of average NOPAT (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe underlying model is the EACC regression:\n\n  \n\\text{NOPAT}_{i,t} = K_i (\\text{TOTAL CAPITAL}_{i,t-1}) + e_{i,t}\n \n\nForecasts are generated as `\\widehat{NOPAT}_{i,t} = \\hat{K}_{i,t-1} \\times \\text{TOTAL CAPITAL}_{i,t-1}`, where `\\hat{K}_{i,t-1}` is estimated using a rolling window of data up to `t-1`. The performance of these forecasts is evaluated using the Mean Absolute Error (MAE).\n\n**Table 1: EACC and WACC Estimates for McDonald's and its Industry (SIC 58)**\n\n| Variable                                      | Mean (%) | MAE (%) |\n| :-------------------------------------------- | :------- | :------ |\n| **McDonald's Estimates (2000 Q4-2016 Q1):**   |          |         |\n| Required EACC - Book Value                    | 15.62    | 15.2    |\n| Required EACC - Market Value                  | 5.81     | 12.8    |\n| Required EACC - Avg. of Book and Market Values| 8.59     | 11.2    |\n| Conventional WACC based on CAPM               | 5.27     | 38.4    |\n| **SIC 58's Estimates (2000 Q4-2012 Q4):**     |          |         |\n| Required EACC - Book Value                    | 17.76    | 17.4    |\n| Required EACC - Market Value                  | 7.52     | 16.0    |\n| Required EACC - Avg. of Book and Market Values| 10.66    | 15.3    |\n| Conventional WACC based on CAPM               | 7.28     | 29.5    |\n| Ibbotson Average CAPM                         | 9.42     | 31.9    |\n\n---\n\nBased on the data in Table 1 and the paper's discussion of measurement biases, select all of the following statements that are valid conclusions or interpretations.", "Options": {"A": "For McDonald's, the EACC estimate derived from the book value of capital (15.62%) is higher than the estimate from the market value of capital (5.81%), which is consistent with the paper's hypothesis that book values may understate the true capital base and market values may overstate it by including future growth opportunities.", "B": "The EACC model using the average of book and market values for McDonald's has a Mean Absolute Error (MAE) of 11.2%, which is more than three times lower than the MAE of the conventional WACC (38.4%), indicating superior forecasting accuracy for NOPAT.", "C": "For the industry SIC 58, the conventional WACC based on CAPM (MAE of 29.5%) provides more accurate NOPAT forecasts than the Ibbotson Average CAPM (MAE of 31.9%).", "D": "The paper suggests that for a privately held company, the EACC model using market values would be the most appropriate choice due to its lower MAE for McDonald's."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret empirical results from a table and connect them to the theoretical arguments presented in the paper. It uses a 'Computational Judgment' strategy, requiring the user to compare values and calculate relative differences (e.g., MAE comparison). Option A tests the understanding of the hypothesized biases. Option B tests the ability to compare forecasting performance quantitatively. Option C requires a careful reading of the industry-level data. The distractor (D) is a 'Conceptual Distractor' (Almost Right) because while the market-value-based EACC has a low MAE, the paper explicitly states that for privately held firms, the book value approach is appropriate since market values are unavailable.", "qid": "272", "question": "### Background\n\n**Research Question.** This case evaluates the empirical performance of the Empirical Average Cost of Capital (EACC) model against conventional WACC measures by testing their ability to forecast future corporate profitability.\n\n**Setting.** The analysis focuses on McDonald's and its industry (SIC 58) from 2000-Q4 to 2016-Q1. The EACC is estimated using a rolling 40-quarter regression, and these estimates are used to produce one-quarter-ahead forecasts of Net Operating Profit After Taxes (NOPAT).\n\n**Variables & Parameters.**\n- `EACC`: Empirical Average Cost of Capital, estimated using three different measures of `TOTAL CAPITAL`: Book Value, Market Value, and the average of the two (dimensionless).\n- `WACC`: Conventional Weighted Average Cost of Capital, including a textbook CAPM-based estimate and various Ibbotson estimates (dimensionless).\n- `MAE`: Mean Absolute Error of the quarterly NOPAT forecasts, expressed as a percentage of average NOPAT (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe underlying model is the EACC regression:\n\n  \n\\text{NOPAT}_{i,t} = K_i (\\text{TOTAL CAPITAL}_{i,t-1}) + e_{i,t}\n \n\nForecasts are generated as `\\widehat{NOPAT}_{i,t} = \\hat{K}_{i,t-1} \\times \\text{TOTAL CAPITAL}_{i,t-1}`, where `\\hat{K}_{i,t-1}` is estimated using a rolling window of data up to `t-1`. The performance of these forecasts is evaluated using the Mean Absolute Error (MAE).\n\n**Table 1: EACC and WACC Estimates for McDonald's and its Industry (SIC 58)**\n\n| Variable                                      | Mean (%) | MAE (%) |\n| :-------------------------------------------- | :------- | :------ |\n| **McDonald's Estimates (2000 Q4-2016 Q1):**   |          |         |\n| Required EACC - Book Value                    | 15.62    | 15.2    |\n| Required EACC - Market Value                  | 5.81     | 12.8    |\n| Required EACC - Avg. of Book and Market Values| 8.59     | 11.2    |\n| Conventional WACC based on CAPM               | 5.27     | 38.4    |\n| **SIC 58's Estimates (2000 Q4-2012 Q4):**     |          |         |\n| Required EACC - Book Value                    | 17.76    | 17.4    |\n| Required EACC - Market Value                  | 7.52     | 16.0    |\n| Required EACC - Avg. of Book and Market Values| 10.66    | 15.3    |\n| Conventional WACC based on CAPM               | 7.28     | 29.5    |\n| Ibbotson Average CAPM                         | 9.42     | 31.9    |\n\n---\n\nBased on the data in Table 1 and the paper's discussion of measurement biases, select all of the following statements that are valid conclusions or interpretations.", "model_evaluation_result": {"model_raw_response": "Looking at the data and analyzing each statement:\n\nA) **Valid**: The data shows McDonald's EACC from book value (15.62%) is indeed higher than from market value (5.81%). This aligns with the stated hypothesis that book values may understate capital (requiring higher returns to achieve the same NOPAT) while market values may overstate it by including growth opportunities (resulting in lower required returns).\n\nB) **Valid**: McDonald's EACC using average values has MAE of 11.2%, while conventional WACC has MAE of 38.4%. Since 38.4/11.2 = 3.43, the conventional WACC's MAE is indeed more than three times higher, confirming the EACC's superior forecasting accuracy.\n\nC) **Valid**: For SIC 58, conventional WACC based on CAPM has MAE of 29.5% compared to Ibbotson Average CAPM's MAE of 31.9%. Lower MAE indicates better forecasting accuracy, so the conventional WACC performs better.\n\nD) **Invalid**: For privately held companies, market values wouldn't be available (no public market trading). The statement contradicts this practical limitation by suggesting the use of market values for a private company.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 152, "Question": "### Background\n\nThe study's theoretical framework models consumer choice along a kinked budget constraint. The theory predicts that changes to the deductible primarily affect the choices of high-spending individuals who expect to be in \"Region 3\" (post-deductible), while having little effect on low-spenders in \"Region 1\" (spending from the account). To test this, the sample is stratified by age as a proxy for expected health expenditure.\n\n### Data / Model Specification\n\n**Table 1: OLS Estimates of the Effect of Deductible on Total Spending, by Age Group**\n\n| Regressor  | Age < 50       | Age ≥ 50       |\n| :--------- | :------------: | :------------: |\n| Deductible | -0.42** (0.17) | -0.88** (0.21) |\n\n*Notes: OLS estimates are displayed. Robust standard errors in parentheses. ** significant at 1%.*\n\n---\n\nBased on the theoretical framework and the empirical results in **Table 1**, select all of the following statements that are supported by the evidence.\n", "Options": {"A": "According to the theoretical model, a change in the deductible primarily impacts high-spenders through a substitution effect, as they switch to lower-cost providers.", "B": "The empirical results are consistent with the theory that the deductible has a stronger marginal effect on spending for individuals with higher expected health care needs.", "C": "For the \"Age ≥ 50\" group, a $1 increase in the deductible is associated with an $0.88 decrease in total spending.", "D": "The difference in the deductible's effect between the two age groups is statistically significant at the 1% level."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to connect the paper's theoretical predictions to its empirical tests and correctly interpret the stratified regression results. Depth Strategy: Scenario Application (applying the general theory to the specific empirical test). Distractor Logic: Option A is a correct high-level conclusion from the table; the coefficient for the older group (-0.88) is more than double that of the younger group (-0.42). Option C is a direct, correct interpretation of the coefficient for the older group. Option B is a Numerical Distractor (Step-Omission Error); a formal hypothesis test shows the difference is not statistically significant at the 5% level, let alone 1% (t-stat ≈ -1.70). Option D is a Conceptual Distractor; the paper describes the deductible's effect on high-spenders as a negative *income effect* (shifting the budget line down), not a substitution effect.", "qid": "152", "question": "### Background\n\nThe study's theoretical framework models consumer choice along a kinked budget constraint. The theory predicts that changes to the deductible primarily affect the choices of high-spending individuals who expect to be in \"Region 3\" (post-deductible), while having little effect on low-spenders in \"Region 1\" (spending from the account). To test this, the sample is stratified by age as a proxy for expected health expenditure.\n\n### Data / Model Specification\n\n**Table 1: OLS Estimates of the Effect of Deductible on Total Spending, by Age Group**\n\n| Regressor  | Age < 50       | Age ≥ 50       |\n| :--------- | :------------: | :------------: |\n| Deductible | -0.42** (0.17) | -0.88** (0.21) |\n\n*Notes: OLS estimates are displayed. Robust standard errors in parentheses. ** significant at 1%.*\n\n---\n\nBased on the theoretical framework and the empirical results in **Table 1**, select all of the following statements that are supported by the evidence.\n", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement against the theoretical framework and empirical evidence provided.\n\nStatement A: The theoretical framework describes consumer choice along a kinked budget constraint with regions related to deductibles. It states that deductible changes primarily affect high-spending individuals in \"Region 3\" (post-deductible). However, it doesn't mention anything about substitution effects or switching to lower-cost providers. This statement adds mechanisms not described in the given framework.\n\nStatement B: The theory predicts deductible changes primarily affect high-spenders (those expecting to be in Region 3), while having little effect on low-spenders. The empirical results show a coefficient of -0.42 for Age < 50 and -0.88 for Age ≥ 50. Since age is used as a proxy for expected health expenditure (older individuals typically have higher health needs), the stronger effect for the Age ≥ 50 group (-0.88 vs -0.42) is consistent with the theory.\n\nStatement C: The table shows the coefficient for Age ≥ 50 is -0.88. In an OLS regression, this coefficient represents the marginal effect - a $1 increase in the deductible is associated with a $0.88 decrease in total spending for this group.\n\nStatement D: While both coefficients are significant at the 1% level (indicated by **), the table doesn't provide any test statistics for the difference between the two coefficients. We cannot determine if the difference (-0.88 vs -0.42) is statistically significant without additional information such as a t-test for the difference or confidence intervals.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 263, "Question": "### Background\n\n**Research Question.** This study tests whether government-imposed barriers to international investment are effective in segmenting capital markets by analyzing the price behavior of closed-end country funds around announcements of changes in these restrictions.\n\n**Setting / Data-Generating Environment.** The study uses an event-study methodology with weekly data for five country funds (France, Japan, Korea, Mexico, Taiwan) that experienced announced changes in their home country's investment restrictions. The central hypothesis is that an announcement of a liberalization (tightening) of investment restrictions will cause a fund's premium to decrease (increase).\n\n**Variables & Parameters.**\n- `π_jt`: The weekly percentage change in the premium of fund `j` in week `t`.\n- `D_1jt`: Dummy variable for the pre-announcement window. `D_1jt = 1` for a loosening (`-1` for a tightening) if `t` is between 2 and 7 weeks *before* the announcement; 0 otherwise.\n- `D_2jt`: Dummy variable for the event window. `D_2jt = 1` for a loosening (`-1` for a tightening) if `t` is between 1 week *before* and 1 week *after* the announcement; 0 otherwise.\n- `D_3jt`: Dummy variable for the post-announcement window. `D_3jt = 1` for a loosening (`-1` for a tightening) if `t` is between 2 and 7 weeks *after* the announcement; 0 otherwise.\n- `δ_1j, δ_2j, δ_3j`: Regression coefficients measuring the average weekly premium change during the respective windows.\n\n---\n\n### Data / Model Specification\n\nThe core empirical test is the following regression model, estimated separately for each fund `j`:\n\n  \n\\pi_{j t}=\\delta_{0j}+\\delta_{1j}D_{1j t}+\\delta_{2j}D_{2j t}+\\delta_{3j}D_{3j t}+\\epsilon_{j t} \\quad \\text{(Eq. 1)}\n \n\nThe alternative hypothesis is that liberalizations reduce premiums, implying `δ_kj < 0`.\n\n**Table 1: Regression Results for Changes in Fund Premiums**\n\n| Fund | `δ_1j` | `δ_2j` | `δ_3j` | N | Events |\n| :--- | :--- | :--- | :--- | :-: | :---: |\n| France | -1.32 (0.09) | -3.85 (0.03) | 0.41 (0.67) | 132 | 2 |\n| Japan | -0.85 (0.02) | -0.46 (0.45) | 0.19 (0.66) | 292 | 8 |\n| Korea | -2.74 (0.01) | -3.11 (0.02) | 0.06 (0.62) | 208 | 5 |\n| Mexico | -5.41 (0.00) | 0.71 (0.94) | 314 | 6 |\n| Taiwan | -0.42 (0.56) | 0.82 (0.56) | -0.10 (0.39) | 68 | 2 |\n\n*Source: Adapted from Table IV of the source paper. One-sided p-values are in parentheses.*\n\n---\n\n### Question\n\nBased on the regression results in **Table 1**, select all of the following statements that represent a correct interpretation or calculation.", "Options": {"A": "The statistically significant coefficient on `δ_1j` for the Korea Fund suggests that information about the liberalization was likely anticipated by the market before the official announcement.", "B": "The total cumulative decline in the Mexico Fund's premium associated with a liberalization announcement during the three-week event window is 16.23%.", "C": "For the France Fund, the total impact of a liberalization announcement is a premium decline of 3.85% during the event window.", "D": "The results for the Taiwan Fund (`δ_2j` = 0.82) indicate that its investment restrictions were not binding, as the premium change during the event window is not statistically significant."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform calculations and interpret coefficients directly from the regression table. It uses an 'Atomic Decomposition' strategy, breaking down the findings into distinct, verifiable statements. Option A is a 'Computational Judgment' task. Option B tests interpretation of the pre-announcement effect. Distractor C makes an inferential leap (insignificance doesn't prove non-binding, could be anticipated/unimportant event). Distractor D is a 'Step-Omission Error', confusing the average weekly effect with the total cumulative effect.", "qid": "263", "question": "### Background\n\n**Research Question.** This study tests whether government-imposed barriers to international investment are effective in segmenting capital markets by analyzing the price behavior of closed-end country funds around announcements of changes in these restrictions.\n\n**Setting / Data-Generating Environment.** The study uses an event-study methodology with weekly data for five country funds (France, Japan, Korea, Mexico, Taiwan) that experienced announced changes in their home country's investment restrictions. The central hypothesis is that an announcement of a liberalization (tightening) of investment restrictions will cause a fund's premium to decrease (increase).\n\n**Variables & Parameters.**\n- `π_jt`: The weekly percentage change in the premium of fund `j` in week `t`.\n- `D_1jt`: Dummy variable for the pre-announcement window. `D_1jt = 1` for a loosening (`-1` for a tightening) if `t` is between 2 and 7 weeks *before* the announcement; 0 otherwise.\n- `D_2jt`: Dummy variable for the event window. `D_2jt = 1` for a loosening (`-1` for a tightening) if `t` is between 1 week *before* and 1 week *after* the announcement; 0 otherwise.\n- `D_3jt`: Dummy variable for the post-announcement window. `D_3jt = 1` for a loosening (`-1` for a tightening) if `t` is between 2 and 7 weeks *after* the announcement; 0 otherwise.\n- `δ_1j, δ_2j, δ_3j`: Regression coefficients measuring the average weekly premium change during the respective windows.\n\n---\n\n### Data / Model Specification\n\nThe core empirical test is the following regression model, estimated separately for each fund `j`:\n\n  \n\\pi_{j t}=\\delta_{0j}+\\delta_{1j}D_{1j t}+\\delta_{2j}D_{2j t}+\\delta_{3j}D_{3j t}+\\epsilon_{j t} \\quad \\text{(Eq. 1)}\n \n\nThe alternative hypothesis is that liberalizations reduce premiums, implying `δ_kj < 0`.\n\n**Table 1: Regression Results for Changes in Fund Premiums**\n\n| Fund | `δ_1j` | `δ_2j` | `δ_3j` | N | Events |\n| :--- | :--- | :--- | :--- | :-: | :---: |\n| France | -1.32 (0.09) | -3.85 (0.03) | 0.41 (0.67) | 132 | 2 |\n| Japan | -0.85 (0.02) | -0.46 (0.45) | 0.19 (0.66) | 292 | 8 |\n| Korea | -2.74 (0.01) | -3.11 (0.02) | 0.06 (0.62) | 208 | 5 |\n| Mexico | -5.41 (0.00) | 0.71 (0.94) | 314 | 6 |\n| Taiwan | -0.42 (0.56) | 0.82 (0.56) | -0.10 (0.39) | 68 | 2 |\n\n*Source: Adapted from Table IV of the source paper. One-sided p-values are in parentheses.*\n\n---\n\n### Question\n\nBased on the regression results in **Table 1**, select all of the following statements that represent a correct interpretation or calculation.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results in Table 1, I need to evaluate each statement:\n\n**Statement A**: \"The statistically significant coefficient on δ₁ⱼ for the Korea Fund suggests that information about the liberalization was likely anticipated by the market before the official announcement.\"\n\n- Korea Fund: δ₁ⱼ = -2.74 (p = 0.01)\n- This coefficient is statistically significant at conventional levels\n- δ₁ⱼ represents the pre-announcement window (2-7 weeks before)\n- A significant negative coefficient before the announcement suggests the market was already adjusting premiums downward, indicating anticipation of the liberalization\n- **This statement is CORRECT**\n\n**Statement B**: \"The total cumulative decline in the Mexico Fund's premium associated with a liberalization announcement during the three-week event window is 16.23%.\"\n\n- Mexico Fund: δ₂ⱼ = -5.41 (p = 0.00)\n- The event window is 3 weeks (1 week before to 1 week after)\n- The coefficient represents the average weekly change during this window\n- Total cumulative change = -5.41% × 3 weeks = -16.23%\n- **This statement is CORRECT**\n\n**Statement C**: \"For the France Fund, the total impact of a liberalization announcement is a premium decline of 3.85% during the event window.\"\n\n- France Fund: δ₂ⱼ = -3.85 (p = 0.03)\n- This coefficient represents the average weekly change during the event window\n- The event window is 3 weeks, not 1 week\n- Total impact during event window = -3.85% × 3 weeks = -11.55%, not -3.85%\n- **This statement is INCORRECT**\n\n**Statement D**: \"The results for the Taiwan Fund (δ₂ⱼ = 0.82) indicate that its investment restrictions were not binding, as the premium change during the event window is not statistically significant.\"\n\n- Taiwan Fund: δ₂ⱼ = 0.82 (p = 0.56)\n- The p-value of 0.56", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 234, "Question": "### Background\n\n**Research Question.** How does the choice of a specific distribution within the Generalised Hyperbolic (GH) family, even when all choices provide a good statistical fit, quantitatively impact the estimation of extreme tail risk?\n\n**Setting.** A risk manager has fitted two different univariate distributions from the skewed GH family—the Student-t and the Variance Gamma (VG)—to the daily loss data of Goldman Sachs stock. Both models pass standard goodness-of-fit tests, but the manager needs to select one to calculate the Tail Conditional Expectation (TCE) for setting regulatory capital against extreme losses.\n\n**Variables and Parameters.**\n- `X`: A univariate random variable representing the daily loss of Goldman Sachs stock.\n- `TCE_q(X)`: The Tail Conditional Expectation of `X` at quantile level `q`.\n- `μ, γ, σ²`: Location, skewness, and scale parameters of `X`.\n- `λ, χ, ψ`: Parameters of the GIG mixing distribution.\n- `q`: The quantile level, set to 0.999 for an extreme risk scenario.\n- `k_λ`: A constant derived from the GIG mixing distribution.\n\n---\n\n### Data / Model Specification\n\nThe Tail Conditional Expectation for a `GH_1` random variable `X` is given by:\n  \nTCE_q(X) = \\mu + \\frac{\\gamma}{1-q}k_{\\lambda}\\bar{F}_{GH_{1}}(x_{q};\\lambda+1) + \\frac{\\sigma^{2}}{1-q}k_{\\lambda}f_{GH_{1}}(x_{q};\\lambda+1) \\quad \\text{(Eq. (1))}\n \nwhere `f` and `\\bar{F}` are the pdf and survival function, respectively, of a related `GH_1` distribution with parameter `λ+1`.\n\nFor the limiting cases, the coefficient `k_λ` simplifies:\n- For the Student-t distribution: `k_λ = 1`.\n- For the Variance Gamma (VG) distribution: `k_λ = 2λ / ψ`.\n\nTable 1 below provides the estimated parameters for Goldman Sachs losses under the two models, extracted from the paper's empirical analysis.\n\n**Table 1: Univariate Parameter Estimates for Goldman Sachs Losses**\n| Parameter | Student-t | Variance Gamma (VG) |\n|:----------|:----------|:--------------------|\n| `λ`       | -2.88775  | 1.87933             |\n| `χ`       | 3.77551   | 0.00000             |\n| `ψ`       | 0.00000   | 3.75866             |\n| `μ`       | -0.28039  | -0.25329            |\n| `σ`       | 1.55206   | 1.54198             |\n| `γ`       | 0.22783   | 0.19946             |\n\nTo facilitate the calculation, assume that for the `q=0.999` quantile, the following intermediate values for the `GH_1(λ+1, ...)` distribution have been computed:\n- For the Student-t model: `\\bar{F}_{GH_{1}}(x_{0.999}; λ+1) = 0.0021` and `f_{GH_{1}}(x_{0.999}; λ+1) = 0.0015`.\n- For the VG model: `\\bar{F}_{GH_{1}}(x_{0.999}; λ+1) = 0.0018` and `f_{GH_{1}}(x_{0.999}; λ+1) = 0.0011`.\n\n---\n\n### Question\n\nBased on the provided data and formulas, select all of the following statements that are correct.", "Options": {"A": "If the term `1/(1-q)` were omitted from Eq. (1), the calculated `TCE_{0.999}` for the Student-t model would be approximately 0.198.", "B": "For the VG model, the coefficient `k_λ` is approximately 0.5, which explains its lower TCE value compared to the Student-t model.", "C": "The calculated `TCE_{0.999}` for the Student-t model is approximately 3.81.", "D": "The calculated `TCE_{0.999}` for the Variance Gamma (VG) model is approximately 2.72."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to apply the complex TCE formula from the paper to empirical data for two different models. It uses a Computational Judgment strategy with atomic decomposition. The correct options (A, B) are derived from direct calculation. Distractor C tests a key intermediate step (`k_λ` calculation) using a Formula Misuse error (`k_λ` is 1.0, not 0.5). Distractor D is based on a Step-Omission Error (forgetting the `1/(1-q)` term), a common slip in applying such formulas.", "qid": "234", "question": "### Background\n\n**Research Question.** How does the choice of a specific distribution within the Generalised Hyperbolic (GH) family, even when all choices provide a good statistical fit, quantitatively impact the estimation of extreme tail risk?\n\n**Setting.** A risk manager has fitted two different univariate distributions from the skewed GH family—the Student-t and the Variance Gamma (VG)—to the daily loss data of Goldman Sachs stock. Both models pass standard goodness-of-fit tests, but the manager needs to select one to calculate the Tail Conditional Expectation (TCE) for setting regulatory capital against extreme losses.\n\n**Variables and Parameters.**\n- `X`: A univariate random variable representing the daily loss of Goldman Sachs stock.\n- `TCE_q(X)`: The Tail Conditional Expectation of `X` at quantile level `q`.\n- `μ, γ, σ²`: Location, skewness, and scale parameters of `X`.\n- `λ, χ, ψ`: Parameters of the GIG mixing distribution.\n- `q`: The quantile level, set to 0.999 for an extreme risk scenario.\n- `k_λ`: A constant derived from the GIG mixing distribution.\n\n---\n\n### Data / Model Specification\n\nThe Tail Conditional Expectation for a `GH_1` random variable `X` is given by:\n  \nTCE_q(X) = \\mu + \\frac{\\gamma}{1-q}k_{\\lambda}\\bar{F}_{GH_{1}}(x_{q};\\lambda+1) + \\frac{\\sigma^{2}}{1-q}k_{\\lambda}f_{GH_{1}}(x_{q};\\lambda+1) \\quad \\text{(Eq. (1))}\n \nwhere `f` and `\\bar{F}` are the pdf and survival function, respectively, of a related `GH_1` distribution with parameter `λ+1`.\n\nFor the limiting cases, the coefficient `k_λ` simplifies:\n- For the Student-t distribution: `k_λ = 1`.\n- For the Variance Gamma (VG) distribution: `k_λ = 2λ / ψ`.\n\nTable 1 below provides the estimated parameters for Goldman Sachs losses under the two models, extracted from the paper's empirical analysis.\n\n**Table 1: Univariate Parameter Estimates for Goldman Sachs Losses**\n| Parameter | Student-t | Variance Gamma (VG) |\n|:----------|:----------|:--------------------|\n| `λ`       | -2.88775  | 1.87933             |\n| `χ`       | 3.77551   | 0.00000             |\n| `ψ`       | 0.00000   | 3.75866             |\n| `μ`       | -0.28039  | -0.25329            |\n| `σ`       | 1.55206   | 1.54198             |\n| `γ`       | 0.22783   | 0.19946             |\n\nTo facilitate the calculation, assume that for the `q=0.999` quantile, the following intermediate values for the `GH_1(λ+1, ...)` distribution have been computed:\n- For the Student-t model: `\\bar{F}_{GH_{1}}(x_{0.999}; λ+1) = 0.0021` and `f_{GH_{1}}(x_{0.999}; λ+1) = 0.0015`.\n- For the VG model: `\\bar{F}_{GH_{1}}(x_{0.999}; λ+1) = 0.0018` and `f_{GH_{1}}(x_{0.999}; λ+1) = 0.0011`.\n\n---\n\n### Question\n\nBased on the provided data and formulas, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "I need to calculate the TCE values for both models using the given formula and parameters.\n\nGiven formula:\nTCE_q(X) = μ + (γ/(1-q))k_λF̄_GH₁(x_q;λ+1) + (σ²/(1-q))k_λf_GH₁(x_q;λ+1)\n\nGiven parameters:\n- q = 0.999, so 1-q = 0.001\n- For Student-t: k_λ = 1\n- For VG: k_λ = 2λ/ψ\n\nLet me check each option:\n\n**Option A:** If 1/(1-q) were omitted from Eq. (1)\nFor Student-t without 1/(1-q):\nTCE = μ + γk_λF̄ + σ²k_λf\nTCE = -0.28039 + 0.22783(1)(0.0021) + (1.55206)²(1)(0.0015)\nTCE = -0.28039 + 0.000478 + 2.4089(0.0015)\nTCE = -0.28039 + 0.000478 + 0.003613\nTCE = -0.276299 ≈ -0.276\n\nThis is not approximately 0.198, so Option A is incorrect.\n\n**Option B:** Calculate k_λ for VG model\nk_λ = 2λ/ψ = 2(1.87933)/3.75866 = 3.75866/3.75866 = 1.0\n\nThis is not approximately 0.5, so Option B is incorrect.\n\n**Option C:** Calculate TCE_0.999 for Student-t\nTCE = -0.28039 + (0.22783/0.001)(1)(0.0021) + (1.55206²/0.001)(1)(0.0015)\nTCE = -0.28039 + 227.83(0.0021) + 2408.9(0.0015)", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 299, "Question": "### Background\n\n**Research Question.** How can one construct a simple, empirically tractable measure of stock illiquidity that captures the price impact of trades, and how should this measure be adjusted to be comparable over long time periods characterized by market-wide price inflation?\n\n**Setting / Data-Generating Environment.** The measures are constructed using daily firm-level stock data and aggregate market capitalization data.\n\n---\n\n### Data / Model Specification\n\nThe raw Amihud illiquidity measure is:\n  \n\\mathsf{Amihud}_{i,t}^{RAW} = 10^{6} \\times \\frac{1}{D_{i,t}} \\sum_{d=1}^{D_{i,t}} \\frac{|R_{i,t,d}|}{DVOL_{i,t,d}} \\quad \\text{(Eq. (1))}\n \nwhere `R` is stock return and `DVOL` is dollar volume.\n\nThe price-inflation adjusted measure is:\n  \nAmihud_{i,t}^{AP} = \\min\\Big(0.25 + 0.30 \\times \\mathsf{Amihud}_{i,t}^{RAW} \\times P_{t-1}^{M}, 71.9\\Big) \\quad \\text{(Eq. (2))}\n \nwhere `P_{t-1}^{M}` is a market-wide price index.\n\n---\n\nBased on the definitions in Eq. (1) and Eq. (2), which of the following statements about the Amihud illiquidity measures are correct? (Select all that apply)", "Options": {"A": "The term `P_{t-1}^{M}` in Eq. (2) is designed to adjust for changes in the number of shares traded, not the price level.", "B": "A higher value of `Amihud_{i,t}^{RAW}` indicates lower stock liquidity, as it implies a larger price movement per dollar traded.", "C": "The ratio `|R_{i,t,d}| / DVOL_{i,t,d}` measures the dollar volume required to move the price by 1%.", "D": "A key reason for the adjustment in Eq. (2) is that sustained market-wide price appreciation would mechanically cause `Amihud_{i,t}^{RAW}` to trend downwards, creating a spurious appearance of increasing liquidity over time."}, "Answer": ["B", "D"], "pi_justification": "This item assesses a comprehensive understanding of the Amihud illiquidity measure, covering its basic intuition, a key time-series bias, and the rationale for its correction. Using a Multiple Choice format allows for efficient testing of multiple related concepts. Distractor C offers an incorrect rationale for the price-inflation adjustment, while Distractor D presents a conceptually inverted definition of the measure's core component.", "qid": "299", "question": "### Background\n\n**Research Question.** How can one construct a simple, empirically tractable measure of stock illiquidity that captures the price impact of trades, and how should this measure be adjusted to be comparable over long time periods characterized by market-wide price inflation?\n\n**Setting / Data-Generating Environment.** The measures are constructed using daily firm-level stock data and aggregate market capitalization data.\n\n---\n\n### Data / Model Specification\n\nThe raw Amihud illiquidity measure is:\n  \n\\mathsf{Amihud}_{i,t}^{RAW} = 10^{6} \\times \\frac{1}{D_{i,t}} \\sum_{d=1}^{D_{i,t}} \\frac{|R_{i,t,d}|}{DVOL_{i,t,d}} \\quad \\text{(Eq. (1))}\n \nwhere `R` is stock return and `DVOL` is dollar volume.\n\nThe price-inflation adjusted measure is:\n  \nAmihud_{i,t}^{AP} = \\min\\Big(0.25 + 0.30 \\times \\mathsf{Amihud}_{i,t}^{RAW} \\times P_{t-1}^{M}, 71.9\\Big) \\quad \\text{(Eq. (2))}\n \nwhere `P_{t-1}^{M}` is a market-wide price index.\n\n---\n\nBased on the definitions in Eq. (1) and Eq. (2), which of the following statements about the Amihud illiquidity measures are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at the Amihud illiquidity measures defined in the equations:\n\n**Equation 1** defines the raw Amihud measure as the average of daily price impact (absolute return divided by dollar volume).\n\n**Equation 2** adjusts this raw measure by multiplying it by a market-wide price index.\n\nLet me evaluate each statement:\n\n**A) \"The term P_{t-1}^{M} in Eq. (2) is designed to adjust for changes in the number of shares traded, not the price level.\"**\n- This is incorrect. P_{t-1}^{M} is explicitly described as a \"market-wide price index,\" so it adjusts for price level changes, not share volume changes.\n\n**B) \"A higher value of Amihud_{i,t}^{RAW} indicates lower stock liquidity, as it implies a larger price movement per dollar traded.\"**\n- This is correct. The Amihud measure captures price impact - higher values mean larger price movements for a given dollar volume, indicating lower liquidity (higher illiquidity).\n\n**C) \"The ratio |R_{i,t,d}| / DVOL_{i,t,d} measures the dollar volume required to move the price by 1%.\"**\n- This is incorrect. The ratio measures the price movement (return) per dollar traded, not the dollar volume required for a 1% price movement. The units would be %/$ not $/%.\n\n**D) \"A key reason for the adjustment in Eq. (2) is that sustained market-wide price appreciation would mechanically cause Amihud_{i,t}^{RAW} to trend downwards, creating a spurious appearance of increasing liquidity over time.\"**\n- This is correct. When prices increase market-wide, dollar volumes (DVOL) increase mechanically even if the number of shares traded remains constant. This would cause the raw Amihud measure to decrease over time, falsely suggesting improving liquidity. The adjustment by P_{t-1}^{M} corrects for this inflation effect.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 171, "Question": "### Background\n\n**Research Question.** Is the transmission of a housing demand shock from market tightness to prices and liquidity state-dependent, varying with local market conditions such as urbanicity and recent performance?\n\n**Setting.** The study investigates geographical and state-dependent heterogeneity by splitting its panel of Dutch municipalities into subsamples and re-estimating the panel VAR model for each.\n\n**Variables & Parameters.**\n- `Δpr_t`: Quarterly change in the log house price index.\n- `Δros_t`: Quarterly change in the rate of sale (liquidity).\n- `Δcph_{t-1}`, `Δcph_{t-2}`: Changes in log clicks per house (market tightness), lagged one and two quarters.\n\n### Data / Model Specification\n\nThe panel VAR model is estimated separately for different subsamples. Table 1 summarizes the key coefficients for the effect of lagged market tightness on price and liquidity changes.\n\n**Table 1. Panel VAR Results for Urban vs. Rural and High- vs. Low-Return Areas**\n\n| Panel | Sample | Dependent Var. | Regressor | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| A | **Urban** | `Δros_t` | `Δcph_{t-1}` | 0.0093 | (2.3) |\n| | **Rural** | `Δros_t` | `Δcph_{t-1}` | 0.0012 | (0.4) |\n| B | **50% Highest Returns** | `Δpr_t` | `Δcph_{t-1}` | 0.0053 | (2.7) |\n| | | `Δpr_t` | `Δcph_{t-2}` | -0.0011 | (-0.5) |\n| | **50% Lowest Returns** | `Δpr_t` | `Δcph_{t-1}` | 0.0042 | (1.7) |\n| | | `Δpr_t` | `Δcph_{t-2}` | 0.0039 | (1.9) |\n\n*Notes: Results are from separate VAR estimations on each subsample.*\n\n### Question\n\nBased on the subsample analysis in Table 1, which of the following conclusions about the heterogeneity of housing market dynamics are supported by the results? (Select all that apply)", "Options": {"A": "The effect of a market tightness shock on liquidity is stronger and more statistically significant in rural areas than in urban areas.", "B": "Price discovery is slower in low-return regions, as the impact of a market tightness shock on prices (`Δpr_t`) is spread across both the first and second lags.", "C": "In high-return markets, the price response to a market tightness shock is statistically insignificant.", "D": "The transmission of a market tightness shock to liquidity (`Δros_t`) is statistically significant in urban areas but not in rural areas."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to compare and synthesize results across different subsamples. It uses an **Atomic Decomposition** strategy, creating separate, testable claims about the urban/rural and high/low return findings. Correct options (A, B) accurately describe the key differences shown in the table: the presence of a liquidity channel only in urban areas, and the delayed price response in low-return areas. Distractors are designed based on direct misreadings of the table: (C) is a conceptual opposite (the effect is highly significant), and (D) reverses the urban/rural finding.", "qid": "171", "question": "### Background\n\n**Research Question.** Is the transmission of a housing demand shock from market tightness to prices and liquidity state-dependent, varying with local market conditions such as urbanicity and recent performance?\n\n**Setting.** The study investigates geographical and state-dependent heterogeneity by splitting its panel of Dutch municipalities into subsamples and re-estimating the panel VAR model for each.\n\n**Variables & Parameters.**\n- `Δpr_t`: Quarterly change in the log house price index.\n- `Δros_t`: Quarterly change in the rate of sale (liquidity).\n- `Δcph_{t-1}`, `Δcph_{t-2}`: Changes in log clicks per house (market tightness), lagged one and two quarters.\n\n### Data / Model Specification\n\nThe panel VAR model is estimated separately for different subsamples. Table 1 summarizes the key coefficients for the effect of lagged market tightness on price and liquidity changes.\n\n**Table 1. Panel VAR Results for Urban vs. Rural and High- vs. Low-Return Areas**\n\n| Panel | Sample | Dependent Var. | Regressor | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| A | **Urban** | `Δros_t` | `Δcph_{t-1}` | 0.0093 | (2.3) |\n| | **Rural** | `Δros_t` | `Δcph_{t-1}` | 0.0012 | (0.4) |\n| B | **50% Highest Returns** | `Δpr_t` | `Δcph_{t-1}` | 0.0053 | (2.7) |\n| | | `Δpr_t` | `Δcph_{t-2}` | -0.0011 | (-0.5) |\n| | **50% Lowest Returns** | `Δpr_t` | `Δcph_{t-1}` | 0.0042 | (1.7) |\n| | | `Δpr_t` | `Δcph_{t-2}` | 0.0039 | (1.9) |\n\n*Notes: Results are from separate VAR estimations on each subsample.*\n\n### Question\n\nBased on the subsample analysis in Table 1, which of the following conclusions about the heterogeneity of housing market dynamics are supported by the results? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each statement against the empirical results.\n\n**Option A**: \"The effect of a market tightness shock on liquidity is stronger and more statistically significant in rural areas than in urban areas.\"\n- Urban: coefficient = 0.0093, t-statistic = 2.3 (statistically significant)\n- Rural: coefficient = 0.0012, t-statistic = 0.4 (not statistically significant)\n- This is FALSE - the effect is stronger and significant in urban areas, not rural areas.\n\n**Option B**: \"Price discovery is slower in low-return regions, as the impact of a market tightness shock on prices (`Δpr_t`) is spread across both the first and second lags.\"\n- High-return regions: Δcph_{t-1} coefficient = 0.0053 (t=2.7, significant), Δcph_{t-2} coefficient = -0.0011 (t=-0.5, not significant)\n- Low-return regions: Δcph_{t-1} coefficient = 0.0042 (t=1.7, marginally significant), Δcph_{t-2} coefficient = 0.0039 (t=1.9, marginally significant)\n- In low-return regions, both lags show positive, similar magnitude coefficients with comparable t-statistics, suggesting the effect is spread across multiple periods. This indicates slower price discovery.\n- This is TRUE.\n\n**Option C**: \"In high-return markets, the price response to a market tightness shock is statistically insignificant.\"\n- High-return regions: Δcph_{t-1} has coefficient = 0.0053 with t-statistic = 2.7\n- A t-statistic of 2.7 indicates statistical significance at conventional levels\n- This is FALSE.\n\n**Option D**: \"The transmission of a market tightness shock to liquidity (`Δros_t`) is statistically significant in urban areas but not in rural areas.\"\n- Urban: t-statistic = 2.3 (statistically significant)\n- Rural: t-statistic = 0.4 (not statistically significant)\n- This is TRUE.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 372, "Question": "### Background\n\n**Research Question.** To establish the baseline empirical relationship between government industrial policy and corporate financialization.\n\n**Theoretical Framework.** The paper's central hypothesis (H1) posits that industrial policy reduces corporate financialization. Financialization is driven by two primary motives: a “prevention motivation” (risk management) and a “crowding-out effect” (agency conflicts).\n\n### Data / Model Specification\n\n-   `FINRATIO`: The ratio of a firm's financial assets to its total assets. The sample average is 0.0435.\n-   `IP1`: A dummy variable equal to 1 if a firm's industry is supported by industrial policy, and 0 otherwise.\n-   `AGENCY`: A proxy for agency costs.\n\nThe baseline multivariate model is:\n  \nFINRATIO_{i,t+1} = \\alpha_{0} + \\alpha_{1} IP1_{it} + \\alpha' ControlVariables_{i,t} + \\text{FixedEffects} + e_{i,t+1} \\quad \\text{(Eq. 1)}\n \n\n**Table 1. Univariate Mean Difference Test**\n\n| Group | `FINRATIO` Mean |\n| :--- | :--- |\n| `IP1` = 1 (Supported) | 0.0356 |\n| `IP1` = 0 (Unsupported) | 0.0547 |\n\n**Table 2. Selected Multivariate Regression Results (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | -0.0126 | (-11.05) |\n| `AGENCY`| 0.0594 | (6.18) |\n\n*Note: All presented coefficients are significant at p < .01.*\n\n### Question\n\nBased on the univariate and multivariate results presented, select all of the following statements that are correct.", "Options": {"A": "After controlling for firm characteristics, industrial policy is associated with a 1.26 percentage point reduction in a firm's financialization ratio.", "B": "The multivariate estimate of the policy effect (-0.0126) is larger in magnitude than the unconditional difference from Table 1, suggesting the presence of positive confounding bias.", "C": "The economic significance of the policy effect is modest, reducing the average firm's `FINRATIO` by approximately 1.26%.", "D": "The positive and significant coefficient on `AGENCY` is consistent with the theory that agency problems can lead to higher financialization (the \"crowding-out effect\")."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses Atomic Decomposition to test interpretation of regression coefficients, both for the main variable and a key control, and the concept of economic significance. Correct options A and B test direct interpretation of the main effect and a theory-linked control. Distractors are based on misinterpreting the comparison between univariate and multivariate results (C, which has both a magnitude and sign error in its logic) and confusing coefficient magnitude with percentage effect (D, a Unit Error).", "qid": "372", "question": "### Background\n\n**Research Question.** To establish the baseline empirical relationship between government industrial policy and corporate financialization.\n\n**Theoretical Framework.** The paper's central hypothesis (H1) posits that industrial policy reduces corporate financialization. Financialization is driven by two primary motives: a “prevention motivation” (risk management) and a “crowding-out effect” (agency conflicts).\n\n### Data / Model Specification\n\n-   `FINRATIO`: The ratio of a firm's financial assets to its total assets. The sample average is 0.0435.\n-   `IP1`: A dummy variable equal to 1 if a firm's industry is supported by industrial policy, and 0 otherwise.\n-   `AGENCY`: A proxy for agency costs.\n\nThe baseline multivariate model is:\n  \nFINRATIO_{i,t+1} = \\alpha_{0} + \\alpha_{1} IP1_{it} + \\alpha' ControlVariables_{i,t} + \\text{FixedEffects} + e_{i,t+1} \\quad \\text{(Eq. 1)}\n \n\n**Table 1. Univariate Mean Difference Test**\n\n| Group | `FINRATIO` Mean |\n| :--- | :--- |\n| `IP1` = 1 (Supported) | 0.0356 |\n| `IP1` = 0 (Unsupported) | 0.0547 |\n\n**Table 2. Selected Multivariate Regression Results (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | -0.0126 | (-11.05) |\n| `AGENCY`| 0.0594 | (6.18) |\n\n*Note: All presented coefficients are significant at p < .01.*\n\n### Question\n\nBased on the univariate and multivariate results presented, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the data provided:\n\nFrom Table 1:\n- IP1 = 1 (Supported): FINRATIO = 0.0356\n- IP1 = 0 (Unsupported): FINRATIO = 0.0547\n- Unconditional difference = 0.0356 - 0.0547 = -0.0191\n\nFrom Table 2:\n- IP1 coefficient = -0.0126 (significant at p < .01)\n- AGENCY coefficient = 0.0594 (significant at p < .01)\n\nEvaluating each statement:\n\nA) After controlling for firm characteristics, industrial policy is associated with a 1.26 percentage point reduction in a firm's financialization ratio.\n- The coefficient is -0.0126, which represents a 0.0126 unit change in FINRATIO\n- Since FINRATIO is already expressed as a ratio (not a percentage), this is a 1.26 percentage point reduction\n- CORRECT\n\nB) The multivariate estimate of the policy effect (-0.0126) is larger in magnitude than the unconditional difference from Table 1, suggesting the presence of positive confounding bias.\n- Unconditional difference: -0.0191\n- Multivariate estimate: -0.0126\n- |-0.0126| = 0.0126 < |-0.0191| = 0.0191\n- The multivariate estimate is SMALLER in magnitude, not larger\n- INCORRECT\n\nC) The economic significance of the policy effect is modest, reducing the average firm's FINRATIO by approximately 1.26%.\n- Average FINRATIO = 0.0435\n- Reduction = 0.0126\n- Percentage reduction = 0.0126/0.0435 ≈ 0.290 = 29.0%\n- This is not 1.26% but rather about 29%\n- INCORRECT\n\nD) The positive and significant coefficient on AGENCY is consistent with the theory that agency problems can lead to higher financialization (the \"crowding-out effect\").\n- AGENCY coefficient = 0.0594 (positive and significant)\n- The theory states agency", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 243, "Question": "### Background\n\n**Research Question.** How can one construct a robust econometric framework to test the hypothesis that the market prices of macroeconomic risks are not constant, but vary predictably over the business cycle?\n\n**Setting.** The study's empirical core is a Generalized Method of Moments (GMM) framework designed to jointly model and estimate the relationship between asset returns and the conditional moments of industrial production.\n\n### Data / Model Specification\n\n1.  **Variable Construction:** The model uses two key industrial production variables:\n    *   The realized production growth to be priced, `YP_t`, is the 12-month log growth of *seasonally adjusted* industrial production: `YP_t = ln(IP(sa)_t) - ln(IP(sa)_{t-12})`.\n    *   The business cycle proxy, `IPL_t`, is the 12-month log growth of *seasonally unadjusted* industrial production: `IPL_t = ln(IP_t) - ln(IP_{t-12})`. To ensure it is in the investors' information set, it is lagged two months (`IPL_{t-2}`) in the pricing equations.\n\n2.  **GMM System:** The model is estimated as a system of equations:\n\n      \n    e_{1t} = YP_t - \\phi_0 - \\sum_{j} \\phi_j IPL_{t-j} \\quad \\text{(Eq. 1)}\n     \n\n      \n    e_{2t} = r_{mt} - \\eta_{0s} - (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) YP_t - (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) e_{1t}^2 \\quad \\text{(Eq. 2)}\n     \n\n    where `r_{mt}` is the stock return, `e_{1t}` is the forecast error for production growth, and `e_{1t}^2` proxies for production volatility.\n\n### Question\n\nBased on the provided description of the study's methodology, which of the following statements about the data construction and model interpretation are correct?", "Options": {"A": "The two-month lag on the business cycle proxy (`IPL_{t-2}`) is primarily to account for the statistical problem of time-aggregation in flow variables.", "B": "A statistical rejection of the null hypothesis `H₀: λ₁s = 0` would imply that the price of growth risk is state-dependent, supporting the paper's central thesis over standard unconditional models.", "C": "The business cycle proxy `IPL_t` is based on seasonally *unadjusted* data to avoid look-ahead bias, as seasonal adjustment filters often use future information.", "D": "The target variable `YP_t` is based on seasonally *adjusted* data because it provides a noisier, more realistic measure of production growth available to investors."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 10.0). This item assesses understanding of the critical data construction choices and hypothesis-testing logic in the paper. It uses an 'atomic decomposition' strategy. The correct options (A, C) test the core rationale for variable choice and the interpretation of the central hypothesis. Distractor (B) points to a real but secondary issue (time-aggregation) to create a plausible 'True but Irrelevant' distractor for the specific question of the two-month lag. Distractor (D) is a 'Conceptual Opposite', inverting the reason for seasonal adjustment.", "qid": "243", "question": "### Background\n\n**Research Question.** How can one construct a robust econometric framework to test the hypothesis that the market prices of macroeconomic risks are not constant, but vary predictably over the business cycle?\n\n**Setting.** The study's empirical core is a Generalized Method of Moments (GMM) framework designed to jointly model and estimate the relationship between asset returns and the conditional moments of industrial production.\n\n### Data / Model Specification\n\n1.  **Variable Construction:** The model uses two key industrial production variables:\n    *   The realized production growth to be priced, `YP_t`, is the 12-month log growth of *seasonally adjusted* industrial production: `YP_t = ln(IP(sa)_t) - ln(IP(sa)_{t-12})`.\n    *   The business cycle proxy, `IPL_t`, is the 12-month log growth of *seasonally unadjusted* industrial production: `IPL_t = ln(IP_t) - ln(IP_{t-12})`. To ensure it is in the investors' information set, it is lagged two months (`IPL_{t-2}`) in the pricing equations.\n\n2.  **GMM System:** The model is estimated as a system of equations:\n\n      \n    e_{1t} = YP_t - \\phi_0 - \\sum_{j} \\phi_j IPL_{t-j} \\quad \\text{(Eq. 1)}\n     \n\n      \n    e_{2t} = r_{mt} - \\eta_{0s} - (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) YP_t - (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) e_{1t}^2 \\quad \\text{(Eq. 2)}\n     \n\n    where `r_{mt}` is the stock return, `e_{1t}` is the forecast error for production growth, and `e_{1t}^2` proxies for production volatility.\n\n### Question\n\nBased on the provided description of the study's methodology, which of the following statements about the data construction and model interpretation are correct?", "model_evaluation_result": {"model_raw_response": "Looking at this econometric framework, I need to evaluate each statement about the data construction and model interpretation.\n\nA) The two-month lag on IPL_{t-2} is described as ensuring the variable \"is in the investors' information set.\" This is about information availability/timing, not about time-aggregation of flow variables. **Incorrect.**\n\nB) In equation 2, the coefficient λ₁s multiplies the interaction term IPL_{t-2} × YP_t. If λ₁s = 0, then the price of growth risk would be constant (just λ₀s). If λ₁s ≠ 0, then the price of growth risk varies with the business cycle (λ₀s + λ₁s × IPL_{t-2}). Rejecting H₀: λ₁s = 0 would indeed support state-dependent pricing of growth risk, which is the paper's central thesis. **Correct.**\n\nC) Seasonal adjustment typically uses centered moving averages or other filters that incorporate both past and future data points. Using seasonally unadjusted data for the business cycle proxy avoids this look-ahead bias, ensuring the variable only contains information available at time t. **Correct.**\n\nD) Seasonally adjusted data removes seasonal noise and provides a cleaner measure of the underlying economic trend. The statement claiming it's \"noisier\" contradicts the purpose of seasonal adjustment. **Incorrect.**\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 302, "Question": "### Background\n\n**Research Question.** How can the theoretical results of joint credibility forecasting be applied and verified using a specific numerical example where the forecasts are known to be exact?\n\n**Setting.** The paper analyzes a Normal likelihood `p(x|\\mu, \\omega) = \\text{Normal}(\\mu, \\omega^{-1})` with a conjugate Normal-Gamma prior. The chosen hyperparameters are `x_{01}=10`, `n_{01}=10`, `x_{02}=21`, and `\\alpha=6.5`. This specific choice satisfies the condition `2\\alpha = n_{01}+3`, which ensures that the credibility forecasts for the first and second moments, `f_1(\\mathcal{D})` and `f_2(\\mathcal{D})`, are exactly equal to the true Bayesian predictive means, `m_1(\\mathcal{D})` and `m_2(\\mathcal{D})`.\n\n**Variables and Parameters.**\n- `n`: The number of observations.\n- `\\mathbf{Z}`: The 3x3 joint credibility matrix.\n- `\\mathbf{f}(\\mathcal{D})`: The vector of joint forecasts for `[m_1, m_2, m_{11}]'`.\n- `\\mathbf{f}^*(\\mathcal{D})`: The vector of independent forecasts.\n- `\\mathbf{mse}(f_i)`: The mean-squared approximation error for the forecast of moment `i` using the joint predictor.\n- `\\mathbf{mse}(f_i^*)`: The mean-squared approximation error for the forecast of moment `i` using the independent predictor.\n- `n_{01}, n_{02}`: The time constants for the independent forecasts of the first and second moments, respectively. For this example, `n_{01}=10` and `n_{02}=10`.\n- `z_1, z_2`: The scalar credibility factors for the independent forecasts.\n\n---\n\n### Data / Model Specification\n\nThe prior moments for this example are `\\mathbf{m} = [m_1, m_2, m_{11}]' = [1, 2.1, 1.1]'`.\nThe paper provides the following numerical results for the joint and independent predictors for a sample size of `n=10`.\n\n**Table 1: Prediction Results for Example A (Normal-Gamma) at n=10**\n| Predictor Type | Credibility Matrix (Z) | mse f(D) `[f_1, f_2, f_{11}]` |\n|:---:|:---:|:---:|\n| **Joint** | `[[0.50000, 0.00000, 0.00000],` | `[0.05000,` |\n| | `[0.00000, 0.50000, 0.00000],` | `0.34667,` |\n| | `[0.47619, 0.04762, 0.21429]]` | `0.21862]` |\n| **Independent** | `[[0.50000, 0.00000, 0.00000],` | `[0.05000,` |\n| | `[0.00000, 0.50000, 0.00000],` | `0.34667,` |\n| | `[0.00000, 0.00000, 0.47265]]` | `0.22500]` |\n\n---\n\n### Question\n\nBased on the information provided, select all of the following statements that are true.", "Options": {"A": "Given observed sample statistics `\\mathbf{t}(\\mathcal{D}) = [1.5, 3.0, 1.2]'` for `n=10`, the joint credibility forecast for the second moment, `f_2(\\mathcal{D})`, is 2.05.", "B": "Given observed sample statistics `\\mathbf{t}(\\mathcal{D}) = [1.5, 3.0, 1.2]'` for `n=10`, the joint credibility forecast for the mean, `f_1(\\mathcal{D})`, is 1.25.", "C": "For `n=10`, the mean-squared error of the independent forecast `f_{11}^*` is approximately 2.9% higher than that of the joint forecast `f_{11}`.", "D": "For a sample size of `n=10`, the independent credibility factors for the first and second moments (`z_1` and `z_2`) are both equal to 0.50."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to verify credibility factor calculations, compare mean-squared errors, and apply the joint forecast formula. The strategy is atomic decomposition, combining verification, comparison, and calculation into a single multi-select item (Computational Judgment). Option D is a numerical distractor based on a plausible calculation error (using `m_{11}` instead of `m_2` in the forecast formula), testing for careful application of the model.", "qid": "302", "question": "### Background\n\n**Research Question.** How can the theoretical results of joint credibility forecasting be applied and verified using a specific numerical example where the forecasts are known to be exact?\n\n**Setting.** The paper analyzes a Normal likelihood `p(x|\\mu, \\omega) = \\text{Normal}(\\mu, \\omega^{-1})` with a conjugate Normal-Gamma prior. The chosen hyperparameters are `x_{01}=10`, `n_{01}=10`, `x_{02}=21`, and `\\alpha=6.5`. This specific choice satisfies the condition `2\\alpha = n_{01}+3`, which ensures that the credibility forecasts for the first and second moments, `f_1(\\mathcal{D})` and `f_2(\\mathcal{D})`, are exactly equal to the true Bayesian predictive means, `m_1(\\mathcal{D})` and `m_2(\\mathcal{D})`.\n\n**Variables and Parameters.**\n- `n`: The number of observations.\n- `\\mathbf{Z}`: The 3x3 joint credibility matrix.\n- `\\mathbf{f}(\\mathcal{D})`: The vector of joint forecasts for `[m_1, m_2, m_{11}]'`.\n- `\\mathbf{f}^*(\\mathcal{D})`: The vector of independent forecasts.\n- `\\mathbf{mse}(f_i)`: The mean-squared approximation error for the forecast of moment `i` using the joint predictor.\n- `\\mathbf{mse}(f_i^*)`: The mean-squared approximation error for the forecast of moment `i` using the independent predictor.\n- `n_{01}, n_{02}`: The time constants for the independent forecasts of the first and second moments, respectively. For this example, `n_{01}=10` and `n_{02}=10`.\n- `z_1, z_2`: The scalar credibility factors for the independent forecasts.\n\n---\n\n### Data / Model Specification\n\nThe prior moments for this example are `\\mathbf{m} = [m_1, m_2, m_{11}]' = [1, 2.1, 1.1]'`.\nThe paper provides the following numerical results for the joint and independent predictors for a sample size of `n=10`.\n\n**Table 1: Prediction Results for Example A (Normal-Gamma) at n=10**\n| Predictor Type | Credibility Matrix (Z) | mse f(D) `[f_1, f_2, f_{11}]` |\n|:---:|:---:|:---:|\n| **Joint** | `[[0.50000, 0.00000, 0.00000],` | `[0.05000,` |\n| | `[0.00000, 0.50000, 0.00000],` | `0.34667,` |\n| | `[0.47619, 0.04762, 0.21429]]` | `0.21862]` |\n| **Independent** | `[[0.50000, 0.00000, 0.00000],` | `[0.05000,` |\n| | `[0.00000, 0.50000, 0.00000],` | `0.34667,` |\n| | `[0.00000, 0.00000, 0.47265]]` | `0.22500]` |\n\n---\n\n### Question\n\nBased on the information provided, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement based on the provided information.\n\nGiven information:\n- Prior moments: **m** = [m₁, m₂, m₁₁]' = [1, 2.1, 1.1]'\n- Sample size n = 10\n- Joint credibility matrix **Z** and independent credibility matrix are provided\n- Sample statistics **t**(D) = [1.5, 3.0, 1.2]'\n\nThe credibility forecast formula is: **f**(D) = **Z****t**(D) + (I - **Z**)**m**\n\nLet me check each statement:\n\n**Statement A**: Joint credibility forecast for the second moment f₂(D)\nUsing the second row of the joint credibility matrix:\nf₂(D) = [0, 0.5, 0] × [1.5, 3.0, 1.2]' + (1 - 0.5) × 2.1\nf₂(D) = 0 + 1.5 + 0 + 0.5 × 2.1\nf₂(D) = 1.5 + 1.05 = 2.55 ≠ 2.05\nStatement A is FALSE.\n\n**Statement B**: Joint credibility forecast for the mean f₁(D)\nUsing the first row of the joint credibility matrix:\nf₁(D) = [0.5, 0, 0] × [1.5, 3.0, 1.2]' + (1 - 0.5) × 1\nf₁(D) = 0.75 + 0 + 0 + 0.5 × 1\nf₁(D) = 0.75 + 0.5 = 1.25\nStatement B is TRUE.\n\n**Statement C**: Comparison of MSE for f₁₁\nFrom the table:\n- MSE for joint f₁₁: 0.21862\n- MSE for independent f₁₁*: 0.22500\nPercentage difference = (0.22500 - 0.21862", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 280, "Question": "### Background\n\n**Research Question.** This study empirically estimates the magnitude of exchange rate pass-through to investment good prices and investigates how this effect varies across different industries in a panel of OECD countries.\n\n**Setting / Data-Generating Environment.** The analysis uses an unbalanced panel of annual data for various industrial sectors across 12 OECD countries from 1971 to 1997.\n\n---\n\n### Data / Model Specification\n\nThe pass-through elasticity `\\alpha_{i1}` is estimated for each industry `i` using the following panel regression model, which includes country (`D_m`) and year (`D_t`) fixed effects.\n\n  \n\\Delta\\mathrm{PI}_{i m t}=\\alpha_{i0}+\\alpha_{i1}\\left(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}}\\right)+\\alpha_{i2}\\Delta\\mathrm{PI}_{m t}^{\\mathrm{N}}+\\alpha_{i3}\\Delta c_{m t}+\\sum D_{t}+\\sum D_{m}+\\nu_{i m t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Selected Regression Results for Pass-Through Elasticity (`\\hat{\\alpha}_{i1}`)**\n\n| Industry                                                  | Coefficient (`\\hat{\\alpha}_{i1}`) | t-statistic |\n| :-------------------------------------------------------- | :-------------------------------- | :---------- |\n| Transport, storage and communication                      | 0.3151                            | (2.37)      |\n| Finance, insurance, real estate and business services     | 0.0242                            | (0.49)      |\n\n*Source: Adapted from Table 1 in the source paper. t-statistics are heteroscedasticity-robust.*\n\n---\n\n### Question\n\nBased on the model specification in **Eq. (1)** and the results in **Table 1**, which of the following statements are correct interpretations of the empirical strategy and findings?", "Options": {"A": "The results indicate that a 1% depreciation of the domestic currency leads to a statistically significant increase of approximately 0.315% in the investment good prices for the 'Transport' sector.", "B": "The inclusion of year fixed effects (`D_t`) helps to control for unobserved global shocks, such as a worldwide change in commodity prices, that could affect investment prices in all sample countries simultaneously.", "C": "The coefficient for the 'Finance' sector (0.0242) implies that exchange rate changes have a small but statistically significant positive effect on its investment good prices.", "D": "The country fixed effects (`D_m`) are included to account for country-specific shocks that vary over time, such as a national recession in a single country."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the baseline model's identification strategy and the interpretation of its results, which were key parts of the original QA problem. It uses an 'Atomic Decomposition' strategy, breaking the interpretation into distinct claims. Option A correctly describes the role of year fixed effects. Option B correctly interprets a coefficient and its statistical significance. Distractor C misinterprets country fixed effects, which capture time-invariant factors ('Conceptual Opposite'). Distractor D incorrectly assesses statistical significance based on the t-statistic ('Misinterpretation of Significance').", "qid": "280", "question": "### Background\n\n**Research Question.** This study empirically estimates the magnitude of exchange rate pass-through to investment good prices and investigates how this effect varies across different industries in a panel of OECD countries.\n\n**Setting / Data-Generating Environment.** The analysis uses an unbalanced panel of annual data for various industrial sectors across 12 OECD countries from 1971 to 1997.\n\n---\n\n### Data / Model Specification\n\nThe pass-through elasticity `\\alpha_{i1}` is estimated for each industry `i` using the following panel regression model, which includes country (`D_m`) and year (`D_t`) fixed effects.\n\n  \n\\Delta\\mathrm{PI}_{i m t}=\\alpha_{i0}+\\alpha_{i1}\\left(\\Delta e_{m t}+\\Delta c_{t}^{\\mathrm{f}}\\right)+\\alpha_{i2}\\Delta\\mathrm{PI}_{m t}^{\\mathrm{N}}+\\alpha_{i3}\\Delta c_{m t}+\\sum D_{t}+\\sum D_{m}+\\nu_{i m t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Selected Regression Results for Pass-Through Elasticity (`\\hat{\\alpha}_{i1}`)**\n\n| Industry                                                  | Coefficient (`\\hat{\\alpha}_{i1}`) | t-statistic |\n| :-------------------------------------------------------- | :-------------------------------- | :---------- |\n| Transport, storage and communication                      | 0.3151                            | (2.37)      |\n| Finance, insurance, real estate and business services     | 0.0242                            | (0.49)      |\n\n*Source: Adapted from Table 1 in the source paper. t-statistics are heteroscedasticity-robust.*\n\n---\n\n### Question\n\nBased on the model specification in **Eq. (1)** and the results in **Table 1**, which of the following statements are correct interpretations of the empirical strategy and findings?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the model specification and regression results.\n\nLet me analyze each option:\n\n**Option A**: States that a 1% depreciation leads to a 0.315% increase in investment good prices for Transport sector.\n- The coefficient is 0.3151 with t-statistic of 2.37\n- With heteroscedasticity-robust standard errors, a t-statistic of 2.37 is statistically significant (typically above 1.96 for 5% significance level)\n- The positive coefficient means depreciation (increase in exchange rate) leads to higher investment good prices\n- This interpretation is correct\n\n**Option B**: States that year fixed effects control for unobserved global shocks affecting all countries simultaneously.\n- Year fixed effects (D_t) capture time-specific effects that are common across all countries\n- Examples include global commodity price changes, worldwide financial crises, or global technology shocks\n- This is a correct interpretation of what year fixed effects do\n\n**Option C**: States the Finance sector coefficient implies a small but statistically significant positive effect.\n- The coefficient is 0.0242 with t-statistic of 0.49\n- A t-statistic of 0.49 is far below conventional significance thresholds (1.96 for 5% level)\n- This effect is NOT statistically significant\n- This statement is incorrect\n\n**Option D**: States country fixed effects account for country-specific shocks that vary over time.\n- Country fixed effects (D_m) capture time-invariant country-specific characteristics\n- They do NOT capture time-varying country-specific shocks like a national recession\n- Time-varying country-specific shocks would require country-time interaction terms\n- This statement is incorrect\n\nTherefore, the correct options are A and B.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 382, "Question": "### Background\n\nA risk-averse insured, with utility function `U(·)`, chooses an indemnity function `I(·)` to maximize their expected utility. The insured's belief is `\\mathbb{P}` (with density `f_\\mathbb{P}(x)`) and the insurer's is `\\mathbb{Q}` (with density `f_\\mathbb{Q}(x)`).\n\n---\n\n### Data / Model Specification\n\nThe insured's optimization problem is:\n  \n\\operatorname*{max}_{I(\\cdot)\\in\\mathfrak{C}}\\mathbb{E}^{\\mathbb{P}}\\left[U(w_{I}(X))\\right] \\quad \\text{(Eq. (1))}\n \nFor an interior solution where there is partial insurance coverage (`0 < I^{*'}(x) < 1`), the optimal contract `I^*(x)` must satisfy the following pointwise first-order condition:\n  \nU'(w_{I^*}(x)) f_\\mathbb{P}(x) = \\lambda f_\\mathbb{Q}(x) \\quad \\text{(Eq. (2))}\n \nHere, `w_{I^*}(x)` is the insured's final wealth when the loss is `x`, and `\\lambda` is a positive constant representing the shadow price of insurance.\n\n---\n\n### Question\n\nBased on the optimality condition in **Eq. (2)**, which of the following interpretations are correct? Select all that apply.", "Options": {"A": "The constant `\\lambda` is equal to the insurer's safety loading factor, `1+\\rho`.", "B": "The left-hand side, `U'(w_{I^*}(x)) f_\\mathbb{P}(x)`, represents the insured's state-price density: the marginal utility of wealth in state `x` multiplied by the insured's subjective probability density of that state.", "C": "The condition implies that the insured's marginal utility of wealth, `U'(w_{I^*}(x))`, must be highest in states `x` that the insurer considers most probable (i.e., where `f_\\mathbb{Q}(x)` is highest).", "D": "The right-hand side, `\\lambda f_\\mathbb{Q}(x)`, represents the effective marginal cost of providing indemnity in state `x`, as determined by the insurer's beliefs and the overall cost of the insurance contract."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item assesses the interpretation of the paper's general first-order condition for an optimal contract, using an 'atomic decomposition' strategy. The correct options (A, B) break the condition into its left-hand side (insured's valuation) and right-hand side (insurer's effective price). Distractors are based on common errors: C incorrectly relates marginal utility directly to the insurer's density without accounting for the insured's own beliefs, and D provides a simplistic and incorrect definition for the complex shadow price `lambda`.", "qid": "382", "question": "### Background\n\nA risk-averse insured, with utility function `U(·)`, chooses an indemnity function `I(·)` to maximize their expected utility. The insured's belief is `\\mathbb{P}` (with density `f_\\mathbb{P}(x)`) and the insurer's is `\\mathbb{Q}` (with density `f_\\mathbb{Q}(x)`).\n\n---\n\n### Data / Model Specification\n\nThe insured's optimization problem is:\n  \n\\operatorname*{max}_{I(\\cdot)\\in\\mathfrak{C}}\\mathbb{E}^{\\mathbb{P}}\\left[U(w_{I}(X))\\right] \\quad \\text{(Eq. (1))}\n \nFor an interior solution where there is partial insurance coverage (`0 < I^{*'}(x) < 1`), the optimal contract `I^*(x)` must satisfy the following pointwise first-order condition:\n  \nU'(w_{I^*}(x)) f_\\mathbb{P}(x) = \\lambda f_\\mathbb{Q}(x) \\quad \\text{(Eq. (2))}\n \nHere, `w_{I^*}(x)` is the insured's final wealth when the loss is `x`, and `\\lambda` is a positive constant representing the shadow price of insurance.\n\n---\n\n### Question\n\nBased on the optimality condition in **Eq. (2)**, which of the following interpretations are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the first-order condition from equation (2):\n\nU'(w_{I^*}(x)) f_ℙ(x) = λ f_ℚ(x)\n\nLet me analyze each option:\n\n**Option A**: The constant λ is equal to the insurer's safety loading factor, 1+ρ.\n- This is incorrect. λ is a Lagrange multiplier representing the shadow price of insurance from the optimization problem. While it relates to the overall pricing of insurance, it's not simply equal to the safety loading factor.\n\n**Option B**: The left-hand side represents the insured's state-price density.\n- This is correct. U'(w_{I^*}(x)) f_ℙ(x) is indeed the insured's state-price density - it combines the marginal utility in state x with the subjective probability of that state occurring.\n\n**Option C**: The condition implies U'(w_{I^*}(x)) must be highest where f_ℚ(x) is highest.\n- This is incorrect. From the FOC: U'(w_{I^*}(x)) = λ f_ℚ(x)/f_ℙ(x). The marginal utility is highest where the ratio f_ℚ(x)/f_ℙ(x) is highest, not simply where f_ℚ(x) is highest.\n\n**Option D**: The right-hand side represents the effective marginal cost of providing indemnity.\n- This is correct. λ f_ℚ(x) represents the marginal cost to the insurer of providing coverage in state x, where λ is the overall price level and f_ℚ(x) reflects how costly the insurer views that particular state.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 309, "Question": "### Background\nAn insurer is analyzing the risk profile of a specific fleet of vehicles. The fleet constitutes 50% of the traffic ($\\rho^{\\Phi}=0.5$) and operates under a moderately aggressive driving configuration, denoted $\\xi^{2a}$. The insurer uses a sophisticated microscopic traffic simulation model to project the aggregate annual loss, $L$, for this fleet. The model accounts for non-uniform accident occurrences, where the likelihood of an accident depends on local traffic conditions.\n\nThe analysis compares different assumptions for the distribution of individual accident losses (Gamma vs. Log-normal) and for the dispersion of those losses, measured by the coefficient of variation ($c_v$). The goal is to understand how these modeling choices impact key statistical functionals and risk measures used for pricing and capital allocation.\n\n### Data / Model Specification\nThe following risk measures are used:\n- **Value-at-Risk (VaR)**: $\\operatorname{VaR}_{p}(L)=\\operatorname*{inf}\\{x\\in\\mathbb{R}\\colon P(L\\leq x)\\geq p\\}$\n- **Expected Shortfall (ES)**: $\\mathrm{ES}_{p}(L)=\\frac{1}{1-p}\\int_{p}^{1}\\mathrm{VaR}_{q}(L)\\mathrm{d}q$\n\nTable 1 presents the simulation results for the aggregate annual loss $L$ under various model specifications. All values are based on 10,000 independent samples of $L$.\n\n**Table 1: Statistical Functionals of L for $\\rho^{\\Phi}=0.5$ and $\\xi^{2a}$**\n| | **Gamma** | | | **Log-normal** | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** |\n| **E(L)** | 1577.8 | 1571.5 | 1578.4 | 1581.7 | 1576.2 | 1582.7 |\n| **Var(L)** | 160,179.5 | 247,943.9 | 626,190.6 | 162,067.5 | 247,069.8 | 614,713.4 |\n| **Skewness** | 0.333 | 0.561 | 0.993 | 0.377 | 0.660 | 2.152 |\n| **VaR$_{0.90}$(L)** | 2111.9 | 2219.1 | 2634.2 | 2112.9 | 2242.4 | 2526.6 |\n| **ES$_{0.90}$(L)** | 2331.7 | 2538.8 | 3233.3 | 2342.4 | 2557.5 | 3280.3 |\n| **VaR$_{0.95}$(L)** | 2275.9 | 2471.3 | 3070.5 | 2288.4 | 2468.1 | 2996.2 |\n| **ES$_{0.95}$(L)** | 2468.9 | 2748.0 | 3628.9 | 2491.4 | 2772.5 | 3838.4 |\n| **VaR$_{0.99}$(L)** | 2587.3 | 2891.3 | 3999.6 | 2608.8 | 2943.8 | 4228.6 |\n| **ES$_{0.99}$(L)** | 2755.9 | 3188.6 | 4490.2 | 2809.0 | 3234.4 | 5424.2 |\n\n### Question\nBased on the data in Table 1, which of the following statements are correct?", "Options": {"A": "Across all scenarios presented, the expected annual loss, E(L), is highly sensitive to the choice of the coefficient of variation ($c_v$).", "B": "For the Gamma loss model, increasing the coefficient of variation ($c_v$) from 0.5 to 2.0 results in a more than 250% increase in the variance of the aggregate loss, Var(L).", "C": "When assuming the highest dispersion ($c_v=2.0$), switching the loss model from Gamma to Log-normal increases the required capital to cover the worst 1% of outcomes (as measured by ES$_{0.99}$) by more than 900.", "D": "For the Log-normal distribution with $c_v=1.0$, the 95% Value-at-Risk (VaR$_{0.95}$) is greater than the 90% Expected Shortfall (ES$_{0.90}$)."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret tabular data, perform comparative calculations, and draw valid conclusions about risk drivers. It uses an 'Atomic Decomposition' strategy, breaking down the insights from the original multi-part question into distinct, verifiable statements. Distractors are designed based on conceptual misinterpretation (confusing E(L) sensitivity) and misreading of table values (comparing VaR and ES incorrectly).", "qid": "309", "question": "### Background\nAn insurer is analyzing the risk profile of a specific fleet of vehicles. The fleet constitutes 50% of the traffic ($\\rho^{\\Phi}=0.5$) and operates under a moderately aggressive driving configuration, denoted $\\xi^{2a}$. The insurer uses a sophisticated microscopic traffic simulation model to project the aggregate annual loss, $L$, for this fleet. The model accounts for non-uniform accident occurrences, where the likelihood of an accident depends on local traffic conditions.\n\nThe analysis compares different assumptions for the distribution of individual accident losses (Gamma vs. Log-normal) and for the dispersion of those losses, measured by the coefficient of variation ($c_v$). The goal is to understand how these modeling choices impact key statistical functionals and risk measures used for pricing and capital allocation.\n\n### Data / Model Specification\nThe following risk measures are used:\n- **Value-at-Risk (VaR)**: $\\operatorname{VaR}_{p}(L)=\\operatorname*{inf}\\{x\\in\\mathbb{R}\\colon P(L\\leq x)\\geq p\\}$\n- **Expected Shortfall (ES)**: $\\mathrm{ES}_{p}(L)=\\frac{1}{1-p}\\int_{p}^{1}\\mathrm{VaR}_{q}(L)\\mathrm{d}q$\n\nTable 1 presents the simulation results for the aggregate annual loss $L$ under various model specifications. All values are based on 10,000 independent samples of $L$.\n\n**Table 1: Statistical Functionals of L for $\\rho^{\\Phi}=0.5$ and $\\xi^{2a}$**\n| | **Gamma** | | | **Log-normal** | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** |\n| **E(L)** | 1577.8 | 1571.5 | 1578.4 | 1581.7 | 1576.2 | 1582.7 |\n| **Var(L)** | 160,179.5 | 247,943.9 | 626,190.6 | 162,067.5 | 247,069.8 | 614,713.4 |\n| **Skewness** | 0.333 | 0.561 | 0.993 | 0.377 | 0.660 | 2.152 |\n| **VaR$_{0.90}$(L)** | 2111.9 | 2219.1 | 2634.2 | 2112.9 | 2242.4 | 2526.6 |\n| **ES$_{0.90}$(L)** | 2331.7 | 2538.8 | 3233.3 | 2342.4 | 2557.5 | 3280.3 |\n| **VaR$_{0.95}$(L)** | 2275.9 | 2471.3 | 3070.5 | 2288.4 | 2468.1 | 2996.2 |\n| **ES$_{0.95}$(L)** | 2468.9 | 2748.0 | 3628.9 | 2491.4 | 2772.5 | 3838.4 |\n| **VaR$_{0.99}$(L)** | 2587.3 | 2891.3 | 3999.6 | 2608.8 | 2943.8 | 4228.6 |\n| **ES$_{0.99}$(L)** | 2755.9 | 3188.6 | 4490.2 | 2809.0 | 3234.4 | 5424.2 |\n\n### Question\nBased on the data in Table 1, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement based on the data in Table 1.\n\n**Statement A**: \"Across all scenarios presented, the expected annual loss, E(L), is highly sensitive to the choice of the coefficient of variation ($c_v$).\"\n\nLooking at E(L) values:\n- Gamma: 1577.8 ($c_v=0.5$), 1571.5 ($c_v=1.0$), 1578.4 ($c_v=2.0$)\n- Log-normal: 1581.7 ($c_v=0.5$), 1576.2 ($c_v=1.0$), 1582.7 ($c_v=2.0$)\n\nThe values range from 1571.5 to 1582.7, a variation of about 11.2 on a base of ~1577, which is less than 1%. This is not \"highly sensitive.\"\n\nStatement A is **FALSE**.\n\n**Statement B**: \"For the Gamma loss model, increasing the coefficient of variation ($c_v$) from 0.5 to 2.0 results in a more than 250% increase in the variance of the aggregate loss, Var(L).\"\n\nFor Gamma:\n- Var(L) at $c_v=0.5$: 160,179.5\n- Var(L) at $c_v=2.0$: 626,190.6\n\nPercentage increase = (626,190.6 - 160,179.5) / 160,179.5 × 100% = 466,011.1 / 160,179.5 × 100% ≈ 291%\n\nThis is indeed more than 250%.\n\nStatement B is **TRUE**.\n\n**Statement C**: \"When assuming the highest dispersion ($c_v=2.0$), switching the loss model from Gamma to Log-normal increases the required capital to cover the worst 1% of outcomes (as measured by ES$_{0.99}$) by more than 900.\"\n\nAt $c_v=2.0$:\n- ES$_{0.99}$ for Gamma: 4490.2\n- ES", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 360, "Question": "### Background\n\n**Research Question.** Does a bad manager's incentive to gamble persist in a more realistic setting with decreasing returns to scale (DRS) and an objective of maximizing assets under management (AUM)?\n\n**Setting and Environment.** The baseline model is extended to incorporate DRS, where a manager's expected return `μ` is a decreasing linear function of the fund's AUM, `s`. The manager's objective is no longer to simply attract capital, but to maximize the expected AUM, `E[s]`. The good manager is assumed to choose the minimum feasible risk level, `σ_g^* = σ_`.\n\n**Variables and Parameters.**\n\n*   `s`: Amount of money (AUM) invested in the active fund (units of capital).\n*   `μ_g(s) = μ_g_bar - (μ_g_bar - μ_o)s`: Expected return for a good manager with AUM `s`.\n*   `μ_b(s) = μ_o - (μ_o - μ_b_bar)s`: Expected return for a bad manager with AUM `s`.\n*   `ψ`: Prior probability of a good manager (dimensionless).\n*   `σ_`: Minimum feasible risk (dimensionless).\n*   `σ_b^*`: Equilibrium risk choice for the bad manager (dimensionless).\n\n---\n\n### Data / Model Specification\n\nNumerical simulations produce the following equilibrium choices for the bad manager's risk, `σ_b^*`:\n\n**Table 1: Equilibrium `σ_b^*` under Decreasing Returns to Scale**\n\n| `ψ` (Prior Good) | `σ_`=0.03 | `σ_`=0.05 | `σ_`=0.07 | `σ_`=0.09 |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A: High Skill Gap (`μ_g_bar`=1.2, `μ_b_bar`=1.05, `μ_o`=1.1)** | | | | |\n| 0.3 | 6.15 | 6.92 | 7.83 | 9.64 |\n| 0.5 | 5.41 | 6.27 | 7.70 | 9.38 |\n| 0.7 | 5.37 | 6.40 | 7.81 | 9.52 |\n| **Panel B: Low Skill Gap (`μ_g_bar`=1.1, `μ_b_bar`=1.0, `μ_o`=1.05)** | | | | |\n| 0.3 | 4.92 | 5.88 | 7.42 | 9.26 |\n| 0.5 | 4.68 | 5.54 | 7.22 | 9.15 |\n| 0.7 | 4.91 | 5.88 | 7.41 | 9.27 |\n\n*Note: In all simulations, the good manager's equilibrium choice is `σ_g^* = σ_`.* \n\n---\n\n### Question\n\nBased on the numerical results in **Table 1**, select **all** statements that are supported by the data.", "Options": {"A": "For any given prior belief `ψ`, the bad manager's optimal risk `σ_b^*` increases as the good manager's minimum risk `σ_` increases.", "B": "For any given minimum risk `σ_`, the bad manager's optimal risk `σ_b^*` is monotonically decreasing as the prior probability of a good manager `ψ` increases.", "C": "In all scenarios presented, the bad manager's equilibrium risk choice `σ_b^*` is substantially larger than the minimum feasible risk `σ_`, supporting the robustness of the 'gambling' incentive under decreasing returns to scale.", "D": "A smaller skill gap (Panel B vs. Panel A) incentivizes the bad manager to take on more risk to compensate for their smaller intrinsic advantage."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the **Atomic Decomposition** strategy to test multiple core findings from the simulation table. It assesses **Computational Judgment** by requiring direct interpretation of the table to verify the robustness of the gambling equilibrium (A) and a key comparative static (C). Distractor B targets a common error of misinterpreting a non-monotonic relationship as monotonic. Distractor D presents a plausible but incorrect economic intuition, serving as a **Conceptual Opposite** error.", "qid": "360", "question": "### Background\n\n**Research Question.** Does a bad manager's incentive to gamble persist in a more realistic setting with decreasing returns to scale (DRS) and an objective of maximizing assets under management (AUM)?\n\n**Setting and Environment.** The baseline model is extended to incorporate DRS, where a manager's expected return `μ` is a decreasing linear function of the fund's AUM, `s`. The manager's objective is no longer to simply attract capital, but to maximize the expected AUM, `E[s]`. The good manager is assumed to choose the minimum feasible risk level, `σ_g^* = σ_`.\n\n**Variables and Parameters.**\n\n*   `s`: Amount of money (AUM) invested in the active fund (units of capital).\n*   `μ_g(s) = μ_g_bar - (μ_g_bar - μ_o)s`: Expected return for a good manager with AUM `s`.\n*   `μ_b(s) = μ_o - (μ_o - μ_b_bar)s`: Expected return for a bad manager with AUM `s`.\n*   `ψ`: Prior probability of a good manager (dimensionless).\n*   `σ_`: Minimum feasible risk (dimensionless).\n*   `σ_b^*`: Equilibrium risk choice for the bad manager (dimensionless).\n\n---\n\n### Data / Model Specification\n\nNumerical simulations produce the following equilibrium choices for the bad manager's risk, `σ_b^*`:\n\n**Table 1: Equilibrium `σ_b^*` under Decreasing Returns to Scale**\n\n| `ψ` (Prior Good) | `σ_`=0.03 | `σ_`=0.05 | `σ_`=0.07 | `σ_`=0.09 |\n| :--- | :--- | :--- | :--- | :--- |\n| **Panel A: High Skill Gap (`μ_g_bar`=1.2, `μ_b_bar`=1.05, `μ_o`=1.1)** | | | | |\n| 0.3 | 6.15 | 6.92 | 7.83 | 9.64 |\n| 0.5 | 5.41 | 6.27 | 7.70 | 9.38 |\n| 0.7 | 5.37 | 6.40 | 7.81 | 9.52 |\n| **Panel B: Low Skill Gap (`μ_g_bar`=1.1, `μ_b_bar`=1.0, `μ_o`=1.05)** | | | | |\n| 0.3 | 4.92 | 5.88 | 7.42 | 9.26 |\n| 0.5 | 4.68 | 5.54 | 7.22 | 9.15 |\n| 0.7 | 4.91 | 5.88 | 7.41 | 9.27 |\n\n*Note: In all simulations, the good manager's equilibrium choice is `σ_g^* = σ_`.* \n\n---\n\n### Question\n\nBased on the numerical results in **Table 1**, select **all** statements that are supported by the data.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each statement:\n\n**Statement A**: \"For any given prior belief `ψ`, the bad manager's optimal risk `σ_b^*` increases as the good manager's minimum risk `σ_` increases.\"\n\nChecking across each row:\n- Panel A, ψ=0.3: 6.15 → 6.92 → 7.83 → 9.64 ✓\n- Panel A, ψ=0.5: 5.41 → 6.27 → 7.70 → 9.38 ✓\n- Panel A, ψ=0.7: 5.37 → 6.40 → 7.81 → 9.52 ✓\n- Panel B, ψ=0.3: 4.92 → 5.88 → 7.42 → 9.26 ✓\n- Panel B, ψ=0.5: 4.68 → 5.54 → 7.22 → 9.15 ✓\n- Panel B, ψ=0.7: 4.91 → 5.88 → 7.41 → 9.27 ✓\n\nAll rows show increasing values. **Statement A is TRUE**.\n\n**Statement B**: \"For any given minimum risk `σ_`, the bad manager's optimal risk `σ_b^*` is monotonically decreasing as the prior probability of a good manager `ψ` increases.\"\n\nChecking each column:\n- Panel A, σ_=0.03: 6.15 → 5.41 → 5.37 ✓\n- Panel A, σ_=0.05: 6.92 → 6.27 → 6.40 ✗ (not monotonic)\n- Panel A, σ_=0.07: 7.83 → 7.70 → 7.81 ✗ (not monotonic)\n- Panel A, σ_=0.09: 9.64 → 9.38 → 9.52", "model_answer": ["A", "B"], "error": null}}
{"ID": 291, "Question": "### Background\n\n**Research Question.** How can the classical exponential bound on aggregate claims be refined for different classes of claim severity distributions, particularly those exhibiting opposing 'aging' characteristics?\n\n**Setting / Data-Generating Environment.** We are in the classical collective risk model. The goal is to refine the standard Lundberg exponential bound by incorporating information about the shape of the claim size distribution `F(x)`, specifically whether it belongs to the New Worse than Used in Convex ordering (NWUC) class or the Decreasing Mean Residual Lifetime (DMRL) class.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `\\overline{F}(x)`: The survival function of individual claim sizes.\n- `p_0, \\phi`: Parameters of the claim frequency distribution.\n- `\\kappa`: The adjustment coefficient, `\\kappa > 0`.\n\n---\n\n### Data / Model Specification\n\nThe classical Lundberg inequality provides a general upper bound:\n  \n\\overline{G}(x) \\leq \\frac{1-p_{0}}{\\phi} \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (1))}\n \nIf `F(x)` is **NWUC** (a class of distributions with no 'wear-out' effects), a tighter bound (Theorem 5) is available:\n  \n\\overline{G}(x) \\leq (1-p_{0}) \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (2))}\n \nIf `F(x)` is **DMRL** (characteristic of risks with 'wear-out' effects), a different refined bound (Theorem 6) is available.\n\n---\n\n### Question\n\nBased on the paper's analysis of refined exponential bounds, select all of the following statements that are correct.", "Options": {"A": "DMRL distributions characterize risks with 'positive aging' or wear-out, where the expected remaining lifetime decreases as the current age or loss amount increases.", "B": "The refined bound for NWUC distributions in Eq. (2) is tighter than the classical Lundberg bound in Eq. (1) by a factor of `1-p_0`.", "C": "The refined bound for NWUC distributions in Eq. (2) is tighter than the classical Lundberg bound in Eq. (1) by a constant factor of `\\phi`.", "D": "NWUC distributions characterize risks with 'positive aging' or wear-out, a property exemplified by equipment breakdown risk where failure becomes more likely over time."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the conceptual understanding of the paper's key refinements to the exponential bound. It uses an Atomic Decomposition strategy, breaking down the original QA's comparison into distinct, testable propositions. Correct options (A, B) accurately state the improvement factor and the definition of a DMRL risk profile. Distractor (C) presents an incorrect improvement factor (Formula Misuse). Distractor (D) swaps the properties of NWUC and DMRL risks (Conceptual Opposite), targeting a common point of confusion.", "qid": "291", "question": "### Background\n\n**Research Question.** How can the classical exponential bound on aggregate claims be refined for different classes of claim severity distributions, particularly those exhibiting opposing 'aging' characteristics?\n\n**Setting / Data-Generating Environment.** We are in the classical collective risk model. The goal is to refine the standard Lundberg exponential bound by incorporating information about the shape of the claim size distribution `F(x)`, specifically whether it belongs to the New Worse than Used in Convex ordering (NWUC) class or the Decreasing Mean Residual Lifetime (DMRL) class.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `\\overline{F}(x)`: The survival function of individual claim sizes.\n- `p_0, \\phi`: Parameters of the claim frequency distribution.\n- `\\kappa`: The adjustment coefficient, `\\kappa > 0`.\n\n---\n\n### Data / Model Specification\n\nThe classical Lundberg inequality provides a general upper bound:\n  \n\\overline{G}(x) \\leq \\frac{1-p_{0}}{\\phi} \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (1))}\n \nIf `F(x)` is **NWUC** (a class of distributions with no 'wear-out' effects), a tighter bound (Theorem 5) is available:\n  \n\\overline{G}(x) \\leq (1-p_{0}) \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (2))}\n \nIf `F(x)` is **DMRL** (characteristic of risks with 'wear-out' effects), a different refined bound (Theorem 6) is available.\n\n---\n\n### Question\n\nBased on the paper's analysis of refined exponential bounds, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at this question about refined exponential bounds for different classes of claim severity distributions:\n\nLet me analyze each option:\n\n**Option A**: DMRL (Decreasing Mean Residual Lifetime) distributions characterize risks with 'positive aging' or wear-out, where the expected remaining lifetime decreases as the current age or loss amount increases.\n- This is correct. DMRL is defined by the property that the mean residual lifetime decreases, which is exactly what characterizes wear-out or positive aging.\n\n**Option B**: The refined bound for NWUC distributions in Eq. (2) is tighter than the classical Lundberg bound in Eq. (1) by a factor of `1-p_0`.\n- Comparing the bounds:\n  - Classical: $\\overline{G}(x) \\leq \\frac{1-p_{0}}{\\phi} e^{-\\kappa x}$\n  - NWUC refined: $\\overline{G}(x) \\leq (1-p_{0}) e^{-\\kappa x}$\n- The ratio is: $\\frac{(1-p_{0})}{\\frac{1-p_{0}}{\\phi}} = \\phi$\n- So the refined bound is tighter by a factor of $\\phi$, not $1-p_0$. This is incorrect.\n\n**Option C**: The refined bound for NWUC distributions in Eq. (2) is tighter than the classical Lundberg bound in Eq. (1) by a constant factor of `\\phi`.\n- As shown above, this is correct. The NWUC bound is indeed tighter by exactly a factor of $\\phi$.\n\n**Option D**: NWUC distributions characterize risks with 'positive aging' or wear-out, a property exemplified by equipment breakdown risk where failure becomes more likely over time.\n- This is incorrect. The background states that NWUC is \"a class of distributions with no 'wear-out' effects\", which is the opposite of positive aging. DMRL distributions have wear-out effects, not NWUC.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 326, "Question": "## Background\n\n**Research Question.** This case explores the theoretical mechanism through which a firm can lower its cost of capital by cross-listing its shares on a foreign exchange, thereby moving from a segmented home market to an integrated global capital market, and the resulting change in its risk profile.\n\n**Setting.** Consider a firm from a segmented home market (`H`) that cross-lists its shares in the U.S., which is part of the integrated global market (`G`). Before listing, the firm's cost of capital is determined by a local CAPM. After listing, its cost of capital is determined by a global CAPM.\n\n**Variables & Parameters.**\n- `E[R_i]`: Expected return on asset `i`.\n- `R_f`: Risk-free rate.\n- `β_{i,H}`: Beta of asset `i` with respect to the home market portfolio, `H`.\n- `β_{i,G}`: Beta of asset `i` with respect to the global market portfolio, `G`.\n- `λ_H`: Market price of risk in the segmented home market.\n- `λ_G`: Market price of risk in the integrated global market.\n\n---\n\n## Data / Model Specification\n\nIn a **segmented** home market, the firm's cost of capital is determined by a local CAPM:\n  \nE[R_i] = R_f + \\beta_{i,H} \\lambda_H \\quad \\text{(Eq. (1))}\n \nAfter a successful cross-listing **integrates** the firm into the global market, its cost of capital is determined by a global CAPM:\n  \nE[R_i] = R_f + \\beta_{i,G} \\lambda_G \\quad \\text{(Eq. (2))}\n \nIt is generally assumed that `λ_H > λ_G`. Empirical evidence (**Finding #2** in the paper) suggests that after listing, a firm's measured home market beta (`β_{i,H}`) declines.\n\n---\n\n## Question\n\nAccording to the asset pricing framework and empirical findings described, select all correct statements regarding how a cross-listing in the U.S. can lower a non-U.S. firm's cost of capital.", "Options": {"A": "The model assumes that the market price of risk is higher in the integrated global market than in the segmented home market (λ_G > λ_H).", "B": "The cost of capital can fall because the 'price of risk' is lower in the integrated global market (λ_G) than in the segmented home market (λ_H), as risk is shared among a broader base of investors.", "C": "The cost of capital can also fall due to a change in the relevant 'quantity of risk', as the firm's risk is measured by its global beta (β_i,G) rather than its home market beta (β_i,H).", "D": "The primary benefit of cross-listing comes from an increase in the firm's home market beta (β_i,H), which signals higher quality to global investors."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). Conceptual Clarity = 9/10, Discriminability = 9/10. This item assesses the core theoretical mechanism for cost of capital reduction via market integration. It uses an Atomic Decomposition strategy, breaking the concept into the 'price of risk' and 'quantity of risk' channels. Distractors C and D are designed as conceptual opposites, directly testing for common misunderstandings of the theory by reversing the direction of the effects or the core assumption.", "qid": "326", "question": "## Background\n\n**Research Question.** This case explores the theoretical mechanism through which a firm can lower its cost of capital by cross-listing its shares on a foreign exchange, thereby moving from a segmented home market to an integrated global capital market, and the resulting change in its risk profile.\n\n**Setting.** Consider a firm from a segmented home market (`H`) that cross-lists its shares in the U.S., which is part of the integrated global market (`G`). Before listing, the firm's cost of capital is determined by a local CAPM. After listing, its cost of capital is determined by a global CAPM.\n\n**Variables & Parameters.**\n- `E[R_i]`: Expected return on asset `i`.\n- `R_f`: Risk-free rate.\n- `β_{i,H}`: Beta of asset `i` with respect to the home market portfolio, `H`.\n- `β_{i,G}`: Beta of asset `i` with respect to the global market portfolio, `G`.\n- `λ_H`: Market price of risk in the segmented home market.\n- `λ_G`: Market price of risk in the integrated global market.\n\n---\n\n## Data / Model Specification\n\nIn a **segmented** home market, the firm's cost of capital is determined by a local CAPM:\n  \nE[R_i] = R_f + \\beta_{i,H} \\lambda_H \\quad \\text{(Eq. (1))}\n \nAfter a successful cross-listing **integrates** the firm into the global market, its cost of capital is determined by a global CAPM:\n  \nE[R_i] = R_f + \\beta_{i,G} \\lambda_G \\quad \\text{(Eq. (2))}\n \nIt is generally assumed that `λ_H > λ_G`. Empirical evidence (**Finding #2** in the paper) suggests that after listing, a firm's measured home market beta (`β_{i,H}`) declines.\n\n---\n\n## Question\n\nAccording to the asset pricing framework and empirical findings described, select all correct statements regarding how a cross-listing in the U.S. can lower a non-U.S. firm's cost of capital.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze each statement against the given framework and empirical findings.\n\nThe key information provided:\n- Before listing: E[R_i] = R_f + β_{i,H} λ_H (segmented market)\n- After listing: E[R_i] = R_f + β_{i,G} λ_G (integrated market)\n- It is generally assumed that λ_H > λ_G\n- Empirical evidence shows β_{i,H} declines after listing\n\nLet me evaluate each option:\n\n**Option A**: States λ_G > λ_H\nThis directly contradicts the given assumption that \"λ_H > λ_G\". The model explicitly assumes the opposite.\n**Incorrect**\n\n**Option B**: States cost of capital falls because λ_G < λ_H (risk shared among broader base)\nThis aligns perfectly with the given assumption λ_H > λ_G. When the price of risk is lower in the integrated market, it reduces the cost of capital. The explanation about risk sharing among a broader base of investors is the economic rationale for why λ_G < λ_H.\n**Correct**\n\n**Option C**: States cost of capital falls due to change in relevant risk measure from β_{i,H} to β_{i,G}\nThis is accurate. After cross-listing, the firm's risk is measured by its global beta rather than home market beta. This represents a change in the \"quantity of risk\" that affects the cost of capital calculation.\n**Correct**\n\n**Option D**: States primary benefit comes from increase in β_{i,H}\nThis contradicts the empirical evidence (Finding #2) which states that β_{i,H} declines after listing. Additionally, an increase in beta would increase, not decrease, the cost of capital.\n**Incorrect**\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 361, "Question": "### Background\n\n**Research Question.** How do rational investors infer a fund manager's unobserved quality from a single return realization when the manager can strategically choose the portfolio's risk level?\n\n**Setting and Environment.** We consider a model with an actively managed fund and a passive index fund. The active manager has a private quality type—'good' (`μ_g`) or 'bad' (`μ_b`)—which determines the mean of the fund's return. Risk-neutral investors observe the fund's realized return `R` and must decide whether to invest. Critically, the manager's choice of portfolio risk `σ` is unobservable to investors.\n\n**Variables and Parameters.**\n\n*   `R`: Realized net-of-fee return of the active fund.\n*   `μ`: The manager's unobserved quality (`μ_g` or `μ_b`).\n*   `σ`: The manager's strategic choice of portfolio risk.\n*   `ψ`: The prior probability that the manager is of good quality.\n*   `μ_o`: The known expected return of the passive index fund.\n*   The model assumes `μ_b < μ_o < μ_g`.\n\n---\n\n### Data / Model Specification\n\nThe return-generating process for the active fund is given by:\n  \nR = \\mu + \\sigma\\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0,1) \\quad \\text{(Eq. 1)}\n \nInvestors are rational and risk-neutral, investing in the active fund if its posterior expected return exceeds that of the index fund:\n  \nP(\\text{good}|R)\\mu_{g} + P(\\text{bad}|R)\\mu_{b} > \\mu_{o} \\quad \\text{(Eq. 2)}\n \nInvestors use Bayes' rule to update their beliefs. The posterior probability can be expressed using the prior `ψ` and the likelihood ratio `L(R) = P(R|bad) / P(R|good)` as:\n  \nP(\\text{good}|R) = \\frac{\\psi}{\\psi + (1-\\psi)L(R)} \\quad \\text{(Eq. 3)}\n \nIn a 'gambling' equilibrium, the good manager chooses minimum risk `σ_g^*` and the bad manager chooses higher risk `σ_b^* > σ_g^*`.\n\n---\n\n### Question\n\nBased on the model of investor behavior, select **all** of the following statements that are mathematically correct.", "Options": {"A": "In a 'gambling' equilibrium where `σ_b^* > σ_g^*`, the log-likelihood ratio `log(L(R))` is a U-shaped quadratic function of the return `R`, which is why the investment region for `R` can be a finite interval.", "B": "A rational, risk-neutral investor will invest in the active fund only if their posterior belief `P(good|R)` exceeds the threshold `τ = (μ_o - μ_b) / (μ_g - μ_b)`.", "C": "In a 'gambling' equilibrium, the investment region is a finite interval because the log-likelihood ratio `log(L(R))` is a linear function of `R`.", "D": "The investment threshold `τ` is derived as `(μ_g - μ_b) / (μ_o - μ_b)`."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 10.0). The original QA problem's core components (deriving the investment threshold and identifying the mathematical driver of the interval outcome) are highly suitable for conversion. This item uses the **Atomic Decomposition** strategy to test these two sharp, factual insights. It assesses **Computational Judgment** (deriving τ) and **Reverse-Reasoning** (understanding the mathematical cause of the interval outcome). Distractor B is a **Formula Misuse** error for the threshold τ. Distractor C is a **Conceptual Opposite** error regarding the functional form of the log-likelihood ratio.", "qid": "361", "question": "### Background\n\n**Research Question.** How do rational investors infer a fund manager's unobserved quality from a single return realization when the manager can strategically choose the portfolio's risk level?\n\n**Setting and Environment.** We consider a model with an actively managed fund and a passive index fund. The active manager has a private quality type—'good' (`μ_g`) or 'bad' (`μ_b`)—which determines the mean of the fund's return. Risk-neutral investors observe the fund's realized return `R` and must decide whether to invest. Critically, the manager's choice of portfolio risk `σ` is unobservable to investors.\n\n**Variables and Parameters.**\n\n*   `R`: Realized net-of-fee return of the active fund.\n*   `μ`: The manager's unobserved quality (`μ_g` or `μ_b`).\n*   `σ`: The manager's strategic choice of portfolio risk.\n*   `ψ`: The prior probability that the manager is of good quality.\n*   `μ_o`: The known expected return of the passive index fund.\n*   The model assumes `μ_b < μ_o < μ_g`.\n\n---\n\n### Data / Model Specification\n\nThe return-generating process for the active fund is given by:\n  \nR = \\mu + \\sigma\\epsilon, \\quad \\text{where } \\epsilon \\sim \\mathcal{N}(0,1) \\quad \\text{(Eq. 1)}\n \nInvestors are rational and risk-neutral, investing in the active fund if its posterior expected return exceeds that of the index fund:\n  \nP(\\text{good}|R)\\mu_{g} + P(\\text{bad}|R)\\mu_{b} > \\mu_{o} \\quad \\text{(Eq. 2)}\n \nInvestors use Bayes' rule to update their beliefs. The posterior probability can be expressed using the prior `ψ` and the likelihood ratio `L(R) = P(R|bad) / P(R|good)` as:\n  \nP(\\text{good}|R) = \\frac{\\psi}{\\psi + (1-\\psi)L(R)} \\quad \\text{(Eq. 3)}\n \nIn a 'gambling' equilibrium, the good manager chooses minimum risk `σ_g^*` and the bad manager chooses higher risk `σ_b^* > σ_g^*`.\n\n---\n\n### Question\n\nBased on the model of investor behavior, select **all** of the following statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each statement about the investor behavior model.\n\nLet me start by understanding the key relationships:\n- Investors invest if: P(good|R)μ_g + P(bad|R)μ_b > μ_o\n- Since P(bad|R) = 1 - P(good|R), this becomes: P(good|R)(μ_g - μ_b) > μ_o - μ_b\n- Therefore: P(good|R) > (μ_o - μ_b)/(μ_g - μ_b)\n\nThis confirms that the threshold τ = (μ_o - μ_b)/(μ_g - μ_b).\n\nNow let me examine each option:\n\n**Option A**: In a gambling equilibrium with σ_b > σ_g, the log-likelihood ratio is:\nlog(L(R)) = log(P(R|bad)/P(R|good)) = (1/2)[(R-μ_g)²/σ_g² - (R-μ_b)²/σ_b²]\n\nSince σ_b > σ_g, the coefficient of R² is (1/2)[1/σ_g² - 1/σ_b²] > 0, making this a U-shaped quadratic. This U-shape means P(good|R) is highest for intermediate R values, creating a finite investment interval. ✓\n\n**Option B**: From the derivation above, investors invest when P(good|R) > τ where τ = (μ_o - μ_b)/(μ_g - μ_b). This is exactly what option B states. ✓\n\n**Option C**: As shown for option A, log(L(R)) is quadratic, not linear, when σ_b ≠ σ_g. The finite interval results from the quadratic (U-shaped) nature, not from linearity. ✗\n\n**Option D**: The correct threshold is τ = (μ_o - μ_b)/(μ_g - μ_b), not (μ_g - μ", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 375, "Question": "### Background\n\n**Research Question.** What combination of misaligned incentives and fragile financial engineering led to the high rate of LBO defaults in the late 1980s, and how did the market correct these flaws in the early 1990s?\n\n**Setting.** The case examines the structural evolution of LBOs, contrasting the aggressive, high-failure deals of 1985-1989 with the more conservative, restructured deals of the early 1990s. The analysis focuses on the role of purchase prices, capital structure, and promoter incentives as documented by Kaplan and Stein.\n\n**Propositions from Kaplan and Stein on Late-80s LBOs:**\n1.  **High Prices:** Buyout prices as multiples of cash flow rose sharply.\n2.  **Fragile Structures:** Cash flow coverage of debt obligations fell dramatically as senior banks shortened maturities while total leverage increased.\n3.  **Inflexible Debt:** Public junk bonds (including PIK/zero-coupon) replaced private subordinated debt, increasing reorganization costs.\n4.  **Misaligned Incentives:** Promoters' net investment fell due to smaller equity stakes and substantial upfront fees.\n\n**Model of Promoter's Payoff.** A deal promoter's payoff `Π` can be modeled as the sum of certain upfront fees and a share of the uncertain future equity value:\n  \n\\Pi = F(P) + \\alpha \\cdot E[\\max(0, V - (P-E))] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the provided context regarding the flaws in late-1980s LBOs, select all statements that accurately describe the structural and incentive problems that led to high failure rates.", "Options": {"A": "The primary cause of defaults was the use of public junk bonds, which were inherently riskier than private debt regardless of the deal structure.", "B": "Senior banks in the late-80s protected the firm's stability by extending maturities on their loans to match the long-dated subordinated debt.", "C": "The combination of short-maturity, amortizing senior bank debt with long-dated, non-cash-paying subordinated debt created a severe liquidity squeeze in the early years of the LBO.", "D": "A key incentive flaw was that promoters' large, certain upfront fees could outweigh their share of potential losses from overpaying for a deal, especially when their equity stake was small."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses comprehension of the core conceptual arguments about incentive misalignment and capital structure fragility. It uses a 'Premise/assumption packaging' strategy to test the main conclusions from the original QA. Distractor C is a 'Conceptual Opposite' of the facts presented in the paper. Distractor D is an 'Almost Right' oversimplification that misses the crucial point about the interaction between different debt tranches.", "qid": "375", "question": "### Background\n\n**Research Question.** What combination of misaligned incentives and fragile financial engineering led to the high rate of LBO defaults in the late 1980s, and how did the market correct these flaws in the early 1990s?\n\n**Setting.** The case examines the structural evolution of LBOs, contrasting the aggressive, high-failure deals of 1985-1989 with the more conservative, restructured deals of the early 1990s. The analysis focuses on the role of purchase prices, capital structure, and promoter incentives as documented by Kaplan and Stein.\n\n**Propositions from Kaplan and Stein on Late-80s LBOs:**\n1.  **High Prices:** Buyout prices as multiples of cash flow rose sharply.\n2.  **Fragile Structures:** Cash flow coverage of debt obligations fell dramatically as senior banks shortened maturities while total leverage increased.\n3.  **Inflexible Debt:** Public junk bonds (including PIK/zero-coupon) replaced private subordinated debt, increasing reorganization costs.\n4.  **Misaligned Incentives:** Promoters' net investment fell due to smaller equity stakes and substantial upfront fees.\n\n**Model of Promoter's Payoff.** A deal promoter's payoff `Π` can be modeled as the sum of certain upfront fees and a share of the uncertain future equity value:\n  \n\\Pi = F(P) + \\alpha \\cdot E[\\max(0, V - (P-E))] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the provided context regarding the flaws in late-1980s LBOs, select all statements that accurately describe the structural and incentive problems that led to high failure rates.", "model_evaluation_result": {"model_raw_response": "Looking at this question about the structural and incentive problems in late-1980s LBOs, I need to evaluate each statement against the provided context from Kaplan and Stein.\n\nStatement A claims that public junk bonds were inherently riskier than private debt regardless of deal structure. However, the context indicates that public junk bonds replaced private subordinated debt, which \"increased reorganization costs\" - not that they were inherently riskier, but rather that they made reorganizations more difficult and costly. This statement is incorrect.\n\nStatement B suggests senior banks extended maturities to match long-dated subordinated debt. This directly contradicts the context, which states that \"senior banks shortened maturities while total leverage increased.\" This statement is false.\n\nStatement C describes a combination of short-maturity senior debt with long-dated, non-cash-paying subordinated debt creating liquidity problems. This aligns perfectly with the context: senior banks shortened maturities (creating near-term cash obligations) while deals included PIK/zero-coupon bonds (which don't require cash payments initially). This mismatch would indeed create severe liquidity pressure in early years. This statement is correct.\n\nStatement D addresses the incentive misalignment where promoters' upfront fees could outweigh their potential losses, especially with small equity stakes. The context supports this: \"Promoters' net investment fell due to smaller equity stakes and substantial upfront fees.\" The model equation shows promoters received certain fees F(P) plus only a share α of uncertain equity value. With small α (equity stake) and large F(P) (fees), promoters could profit even from bad deals. This statement is correct.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 68, "Question": "### Background\nThis problem analyzes the impact of moral hazard (unobservable effort) on an innovative sector. The model assumes conditions under which moral hazard is a binding constraint, specifically when the perceived probability of a negative shock, $\\theta_t$, is moderately high:\n\n$$ \\theta_{t} > \\frac{Y-(1+r)}{Y} \\quad \\text{(Eq. 1)} $$\n\nUnder these conditions, the paper proves that neither a pure high-effort equilibrium nor a separating equilibrium (with different contracts for high- and low-effort managers) can exist. The market settles on a pooling equilibrium where a single contract is offered to all managers.\n\n### Data / Model Specification\nIn the pooling equilibrium, some managers who accept the contract will exert high effort, while others exert low effort. This arrangement is sustained by a mechanism of cross-subsidization.\n\n- **High-effort managers** generate a large positive social surplus but receive an expected compensation that is less than this surplus.\n- **Low-effort managers** may generate a negative social surplus but receive a compensation that is greater than their contribution, effectively earning informational rents.\n\nThis dynamic leads to several key differences compared to the first-best (observable effort) scenario.\n\n### Question\nAccording to the paper's analysis of the moral hazard case, which of the following are valid consequences of the pooling equilibrium that emerges when moral hazard is a binding constraint? Select all that apply.", "Options": {"A": "Aggregate losses, in the event of a negative shock, are higher than they would be in the first-best case for the same level of confidence.", "B": "The expected compensation for managers exerting low effort is less than the social surplus they generate.", "C": "The inflated growth of the sector is funded by investors who accept lower-than-market returns on average.", "D": "For certain levels of confidence, the total size of the innovative sector can be larger than it would be in the first-best (observable effort) case."}, "Answer": ["A", "D"], "pi_justification": "Assessment Target: This item tests the candidate's understanding of the key implications of the pooling equilibrium under moral hazard, as contrasted with the first-best scenario.\nDepth Strategy: Reverse-Reasoning. The question presents several outcomes and asks the candidate to identify which ones are plausible consequences of the specified moral hazard equilibrium.\nDistractor Logic:\n- A (Correct): Implication 8 explicitly states that moral hazard spurs the entry of inefficient managers, which can inflate the size of the innovative sector beyond its first-best level for a range of $\\theta_t$.\n- B (Conceptual Opposite): This is the opposite of the cross-subsidization mechanism. Low-effort managers earn informational rents, meaning their expected compensation *exceeds* the (often negative) surplus they generate.\n- C (Formula Misuse/Conceptual Distractor): The model assumes competitive investors who must break even. The inflated growth is funded by the *high-effort managers* (who are paid less than their surplus) subsidizing the low-effort managers, not by investors taking losses.\n- D (Correct): Implication 10 states that the value-destroying entry of low-effort managers makes the sector more fragile, leading to higher aggregate losses in case of a shock compared to the first-best outcome.", "qid": "68", "question": "### Background\nThis problem analyzes the impact of moral hazard (unobservable effort) on an innovative sector. The model assumes conditions under which moral hazard is a binding constraint, specifically when the perceived probability of a negative shock, $\\theta_t$, is moderately high:\n\n$$ \\theta_{t} > \\frac{Y-(1+r)}{Y} \\quad \\text{(Eq. 1)} $$\n\nUnder these conditions, the paper proves that neither a pure high-effort equilibrium nor a separating equilibrium (with different contracts for high- and low-effort managers) can exist. The market settles on a pooling equilibrium where a single contract is offered to all managers.\n\n### Data / Model Specification\nIn the pooling equilibrium, some managers who accept the contract will exert high effort, while others exert low effort. This arrangement is sustained by a mechanism of cross-subsidization.\n\n- **High-effort managers** generate a large positive social surplus but receive an expected compensation that is less than this surplus.\n- **Low-effort managers** may generate a negative social surplus but receive a compensation that is greater than their contribution, effectively earning informational rents.\n\nThis dynamic leads to several key differences compared to the first-best (observable effort) scenario.\n\n### Question\nAccording to the paper's analysis of the moral hazard case, which of the following are valid consequences of the pooling equilibrium that emerges when moral hazard is a binding constraint? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this moral hazard problem, I need to analyze each option based on the pooling equilibrium described.\n\nFrom the background, we know that in the pooling equilibrium:\n- High-effort managers generate large positive surplus but receive less compensation than their surplus\n- Low-effort managers may generate negative surplus but receive more compensation than their contribution (informational rents)\n- This creates a cross-subsidization mechanism\n\nLet me evaluate each option:\n\n**Option A**: \"Aggregate losses, in the event of a negative shock, are higher than they would be in the first-best case for the same level of confidence.\"\n- In the pooling equilibrium, some managers exert low effort (unlike first-best where all would exert high effort)\n- Low-effort managers are more likely to fail when negative shocks occur\n- Therefore, aggregate losses would indeed be higher\n- **This is correct**\n\n**Option B**: \"The expected compensation for managers exerting low effort is less than the social surplus they generate.\"\n- The text explicitly states that low-effort managers \"may generate a negative social surplus but receive a compensation that is greater than their contribution\"\n- This directly contradicts option B\n- **This is incorrect**\n\n**Option C**: \"For certain levels of confidence, the total size of the innovative sector can be larger than it would be in the first-best (observable effort) case.\"\n- The pooling equilibrium attracts both high-effort and low-effort managers with a single contract\n- Low-effort managers who wouldn't participate in first-best (due to negative surplus) now participate due to informational rents\n- This could increase total sector size despite inefficiency\n- **This is correct**\n\n**Option D**: \"The inflated growth of the sector is funded by investors who accept lower-than-market returns on average.\"\n- High-effort managers generate surplus but are under-compensated\n- Low-effort managers are over-compensated relative to their (possibly negative) contribution\n- For this cross-subsidization to work, investors must be accepting below-market returns on average\n- **This is correct**\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 390, "Question": "### Background\n\n**Research Question.** Does the degree of information asymmetry affect the magnitude of Seasoned Equity Offering (SEO) underpricing? The paper's central hypothesis is that SEOs by REITs that went public in or after 1990 (\"post-1990\") are more underpriced than those by REITs that went public before 1990 (\"pre-1990\").\n\n**Setting.** The study posits that post-1990 REITs operate in a high-information-asymmetry environment, while pre-1990 REITs operate in a lower-asymmetry environment. The effect of variables like offer size may differ depending on which economic force—price pressure vs. information production—dominates in a given environment.\n\n**Variables & Parameters.**\n- `Underpricing (UR)`: The offer-to-close return `(P_1 / P_0 - 1) * 100` (percentage).\n- `LNOFFSIZE`: Natural log of the offer size.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a multivariate OLS regression for each subsample. Key results for the offer size variable are summarized below.\n\n**Table 1. Multivariate OLS Regression Estimates of Underpricing (UR)**\n\n| Variable | Pre-1990 IPO (n=57) | Post-1990 IPO (n=114) |\n| :--- | :--- | :--- |\n| LNOFFSIZE | 1.154 (1.87)* | -0.671 (-2.49)* |\n\n*Source: Synthesized from Model 2 results in Table 5 of the source document. t-statistics are in parentheses. An asterisk (*) denotes significance at the 10% level or lower.*\n\n---\n\n### Question\n\nThe study finds a significant relationship between offer size (`LNOFFSIZE`) and underpricing that differs between the two subsamples. Select all statements that accurately describe these findings and their proposed economic interpretations.", "Options": {"A": "For the pre-1990 (low-asymmetry) sample, larger offers were linked to *more* underpricing, consistent with a 'price pressure' effect dominating.", "B": "For the post-1990 sample, the positive coefficient on `LNOFFSIZE` suggests that price pressure from large new supply was the dominant factor.", "C": "For the post-1990 (high-asymmetry) sample, larger offers were linked to *less* underpricing, consistent with an 'information production' or 'certification' effect dominating.", "D": "The study found that offer size was an insignificant predictor of underpricing in both the pre-1990 and post-1990 environments."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses a nuanced finding: the reversal of a key coefficient across subsamples. It uses a **Scenario Application** strategy, where the 'scenarios' are the low- vs. high-asymmetry environments. The correct options (A, B) correctly link the empirical finding (coefficient sign) to the corresponding economic theory (price pressure vs. information production) for each environment. Distractor C is a **Factual Error** (the coefficient was negative for post-1990), and Distractor D is also a **Factual Error** (the coefficient was significant in both samples).", "qid": "390", "question": "### Background\n\n**Research Question.** Does the degree of information asymmetry affect the magnitude of Seasoned Equity Offering (SEO) underpricing? The paper's central hypothesis is that SEOs by REITs that went public in or after 1990 (\"post-1990\") are more underpriced than those by REITs that went public before 1990 (\"pre-1990\").\n\n**Setting.** The study posits that post-1990 REITs operate in a high-information-asymmetry environment, while pre-1990 REITs operate in a lower-asymmetry environment. The effect of variables like offer size may differ depending on which economic force—price pressure vs. information production—dominates in a given environment.\n\n**Variables & Parameters.**\n- `Underpricing (UR)`: The offer-to-close return `(P_1 / P_0 - 1) * 100` (percentage).\n- `LNOFFSIZE`: Natural log of the offer size.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a multivariate OLS regression for each subsample. Key results for the offer size variable are summarized below.\n\n**Table 1. Multivariate OLS Regression Estimates of Underpricing (UR)**\n\n| Variable | Pre-1990 IPO (n=57) | Post-1990 IPO (n=114) |\n| :--- | :--- | :--- |\n| LNOFFSIZE | 1.154 (1.87)* | -0.671 (-2.49)* |\n\n*Source: Synthesized from Model 2 results in Table 5 of the source document. t-statistics are in parentheses. An asterisk (*) denotes significance at the 10% level or lower.*\n\n---\n\n### Question\n\nThe study finds a significant relationship between offer size (`LNOFFSIZE`) and underpricing that differs between the two subsamples. Select all statements that accurately describe these findings and their proposed economic interpretations.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1:\n\n- Pre-1990 IPO: LNOFFSIZE coefficient = 1.154 (positive, significant at 10% level)\n- Post-1990 IPO: LNOFFSIZE coefficient = -0.671 (negative, significant at 10% level)\n\nLet me evaluate each option:\n\nA) For the pre-1990 (low-asymmetry) sample, larger offers were linked to *more* underpricing, consistent with a 'price pressure' effect dominating.\n- The coefficient is positive (1.154), meaning larger offers → more underpricing\n- This is consistent with price pressure (larger supply → lower prices → more underpricing)\n- **CORRECT**\n\nB) For the post-1990 sample, the positive coefficient on `LNOFFSIZE` suggests that price pressure from large new supply was the dominant factor.\n- The coefficient for post-1990 is NEGATIVE (-0.671), not positive\n- **INCORRECT**\n\nC) For the post-1990 (high-asymmetry) sample, larger offers were linked to *less* underpricing, consistent with an 'information production' or 'certification' effect dominating.\n- The coefficient is negative (-0.671), meaning larger offers → less underpricing\n- This is consistent with information production/certification (larger offers attract more scrutiny/analysis, reducing information asymmetry and thus underpricing)\n- **CORRECT**\n\nD) The study found that offer size was an insignificant predictor of underpricing in both the pre-1990 and post-1990 environments.\n- Both coefficients are marked with asterisks indicating significance at 10% level or lower\n- **INCORRECT**\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 351, "Question": "### Background\nAn international study investigates how private anti-self-dealing regulations affect firm value, and whether this effect is conditional on a firm's ownership structure. The analysis is based on OLS regression results from a sample of 4,634 international firms.\n\n### Data / Model Specification\nThe estimated OLS model is:\n\n  \n\\widehat{\\text{Tobin's Q}} = \\hat{\\alpha} + \\hat{\\beta}_{1}\\mathrm{WEDGE} + \\hat{\\beta}_{2}\\mathrm{PRIVATE} + \\hat{\\beta}_{3}(\\mathrm{WEDGE} * \\mathrm{PRIVATE}) + \\dots \\quad \\text{(Eq. 1)}\n \n\nWhere `Tobin's Q` is a proxy for corporate valuation, `PRIVATE` is a country-level index for the strictness of private control of self-dealing, and `WEDGE` is the firm-level ownership wedge (the difference between control rights and cash-flow ownership of the largest shareholder, in percent).\n\n**Table 1: Key Regression Results**\n\n| Variable                | Coefficient | t-statistic |\n| :---------------------- | :---------- | :---------- |\n| `PRIVATE` (`β₂`)        | 0.64        | 3.80        |\n| `PRIVATE*WEDGE` (`β₃`)  | -0.02       | -2.07       |\n\n*Source: Table 5, Model 3 of the source paper.*\n\n---\n\nBased on the provided model and results, which of the following statements are valid interpretations or calculations? Select all that apply.", "Options": {"A": "The results suggest that as a controlling shareholder's incentive to expropriate wealth increases, the effectiveness of shareholder-led protective regulations diminishes.", "B": "The positive valuation effect of private control regulations is completely neutralized when the ownership wedge reaches 32%.", "C": "The marginal effect of stricter private regulation on firm value is positive for a firm with an ownership wedge of 10%.", "D": "For a firm with a zero ownership wedge, a one-unit increase in the `PRIVATE` index is associated with a 0.62 increase in Tobin's Q."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret an interaction model and perform multi-step calculations based on it. It uses a Computational Judgment strategy. Option A requires calculating the marginal effect at a specific wedge value. Option B requires solving for the wedge that makes the marginal effect zero. Option D tests the conceptual interpretation of the negative interaction term. Option C is a numerical distractor (Step-Omission Error), as it incorrectly subtracts the interaction coefficient from the main effect (0.64 - 0.02 = 0.62) instead of interpreting the main effect directly for WEDGE=0.", "qid": "351", "question": "### Background\nAn international study investigates how private anti-self-dealing regulations affect firm value, and whether this effect is conditional on a firm's ownership structure. The analysis is based on OLS regression results from a sample of 4,634 international firms.\n\n### Data / Model Specification\nThe estimated OLS model is:\n\n  \n\\widehat{\\text{Tobin's Q}} = \\hat{\\alpha} + \\hat{\\beta}_{1}\\mathrm{WEDGE} + \\hat{\\beta}_{2}\\mathrm{PRIVATE} + \\hat{\\beta}_{3}(\\mathrm{WEDGE} * \\mathrm{PRIVATE}) + \\dots \\quad \\text{(Eq. 1)}\n \n\nWhere `Tobin's Q` is a proxy for corporate valuation, `PRIVATE` is a country-level index for the strictness of private control of self-dealing, and `WEDGE` is the firm-level ownership wedge (the difference between control rights and cash-flow ownership of the largest shareholder, in percent).\n\n**Table 1: Key Regression Results**\n\n| Variable                | Coefficient | t-statistic |\n| :---------------------- | :---------- | :---------- |\n| `PRIVATE` (`β₂`)        | 0.64        | 3.80        |\n| `PRIVATE*WEDGE` (`β₃`)  | -0.02       | -2.07       |\n\n*Source: Table 5, Model 3 of the source paper.*\n\n---\n\nBased on the provided model and results, which of the following statements are valid interpretations or calculations? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this regression model and the results, I need to analyze each statement carefully.\n\nThe model shows:\n- β₂ (PRIVATE) = 0.64 (positive effect of private regulations on firm value)\n- β₃ (PRIVATE*WEDGE) = -0.02 (negative interaction effect)\n\nThe marginal effect of PRIVATE on Tobin's Q is: ∂Q/∂PRIVATE = β₂ + β₃ × WEDGE = 0.64 - 0.02 × WEDGE\n\nLet me evaluate each option:\n\n**Option A**: The negative interaction coefficient (β₃ = -0.02) indicates that as the ownership wedge increases (higher incentive to expropriate), the positive effect of private regulations decreases. This is a valid interpretation.\n\n**Option B**: To find when the effect is neutralized:\n0.64 - 0.02 × WEDGE = 0\nWEDGE = 0.64/0.02 = 32%\nThis is correct.\n\n**Option C**: For WEDGE = 10%:\nMarginal effect = 0.64 - 0.02 × 10 = 0.64 - 0.20 = 0.44 > 0\nThe marginal effect is positive, so this is correct.\n\n**Option D**: For WEDGE = 0:\nMarginal effect = 0.64 - 0.02 × 0 = 0.64\nThe statement says 0.62, which is incorrect.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 370, "Question": "### Background\n\n**Research Question.** To test the causal mechanisms through which government industrial policy influences corporate financialization, distinguishing between two primary theoretical motivations.\n\n**Theoretical Framework.** Corporate financialization—the strategy of non-financial corporations (NFCs) allocating more resources to financial rather than real investments—is driven by two main motives:\n1.  **Prevention Motivation:** Firms hold liquid financial assets as a precautionary measure to manage future operating risks and ease financial constraints. This is a risk-management strategy.\n2.  **Crowding-Out Effect:** Agency conflicts lead managers to favor short-term financial investments over long-term real investments. This is a corporate governance problem.\n\nThe study uses a three-step mediation framework to test four potential channels: two related to the prevention motive (operational uncertainty, financial constraints) and two related to the crowding-out effect (information transparency, investment opportunities).\n\n### Data / Model Specification\n\nThe analysis finds that the two channels related to the prevention motivation are statistically significant, while the two channels related to the crowding-out effect are not. The key results for the two significant channels are presented below.\n\n**Table 1. Mediation Effect Test of Operational Uncertainty**\n\n| | (1) Path a | (2) Path b | (3) Path c |\n| :--- | :--- | :--- | :--- |\n| **Dependent Var.** | `FINRATIO` | `UNCERTAINTY` | `FINRATIO` |\n| `IP1` | -0.0126*** | -0.0977*** | -0.0113*** |\n| `UNCERTAINTY` | | | 0.0112*** |\n\n*Note: `FINRATIO` is the ratio of financial assets to total assets. `IP1` is a dummy for industrial policy support. `UNCERTAINTY` is a proxy for firm-level operational risk. *** denotes p < .01.*\n\n**Table 2. Mediation Effect Test of Easing Financial Constraints**\n\n| | (1) Path a | (2) Path b | (3) Path c |\n| :--- | :--- | :--- | :--- |\n| **Dependent Var.** | `FINRATIO` | `KZ` | `FINRATIO` |\n| `IP1` | -0.0126*** | -0.0794*** | -0.0122*** |\n| `KZ` | | | 0.0022* |\n\n*Note: `KZ` is the Kaplan-Zingales index of financial constraints (higher value = more constrained). * denotes p < .10, *** denotes p < .01.*\n\n### Question\n\nBased on the mediation analysis results in Table 1 and Table 2, select all of the following statements that are correct.", "Options": {"A": "Industrial policy is shown to significantly ease firms' financial constraints, as measured by the KZ index.", "B": "The indirect effect of industrial policy on financialization transmitted through the operational uncertainty channel is approximately -0.0011.", "C": "The results indicate that higher operational uncertainty leads to lower levels of corporate financialization.", "D": "The total effect of industrial policy on financialization (-0.0126) is fully explained by the two significant mediation channels combined."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses Computational Judgment and Atomic Decomposition to test the student's ability to interpret and calculate mediation effects from regression tables. Option A tests the interpretation of Path b for the financial constraints channel. Option B requires calculation of the indirect effect for the uncertainty channel. Distractors are based on a sign error in interpreting Path c (C) and a conceptual error misinterpreting partial vs. total mediation effects (D).", "qid": "370", "question": "### Background\n\n**Research Question.** To test the causal mechanisms through which government industrial policy influences corporate financialization, distinguishing between two primary theoretical motivations.\n\n**Theoretical Framework.** Corporate financialization—the strategy of non-financial corporations (NFCs) allocating more resources to financial rather than real investments—is driven by two main motives:\n1.  **Prevention Motivation:** Firms hold liquid financial assets as a precautionary measure to manage future operating risks and ease financial constraints. This is a risk-management strategy.\n2.  **Crowding-Out Effect:** Agency conflicts lead managers to favor short-term financial investments over long-term real investments. This is a corporate governance problem.\n\nThe study uses a three-step mediation framework to test four potential channels: two related to the prevention motive (operational uncertainty, financial constraints) and two related to the crowding-out effect (information transparency, investment opportunities).\n\n### Data / Model Specification\n\nThe analysis finds that the two channels related to the prevention motivation are statistically significant, while the two channels related to the crowding-out effect are not. The key results for the two significant channels are presented below.\n\n**Table 1. Mediation Effect Test of Operational Uncertainty**\n\n| | (1) Path a | (2) Path b | (3) Path c |\n| :--- | :--- | :--- | :--- |\n| **Dependent Var.** | `FINRATIO` | `UNCERTAINTY` | `FINRATIO` |\n| `IP1` | -0.0126*** | -0.0977*** | -0.0113*** |\n| `UNCERTAINTY` | | | 0.0112*** |\n\n*Note: `FINRATIO` is the ratio of financial assets to total assets. `IP1` is a dummy for industrial policy support. `UNCERTAINTY` is a proxy for firm-level operational risk. *** denotes p < .01.*\n\n**Table 2. Mediation Effect Test of Easing Financial Constraints**\n\n| | (1) Path a | (2) Path b | (3) Path c |\n| :--- | :--- | :--- | :--- |\n| **Dependent Var.** | `FINRATIO` | `KZ` | `FINRATIO` |\n| `IP1` | -0.0126*** | -0.0794*** | -0.0122*** |\n| `KZ` | | | 0.0022* |\n\n*Note: `KZ` is the Kaplan-Zingales index of financial constraints (higher value = more constrained). * denotes p < .10, *** denotes p < .01.*\n\n### Question\n\nBased on the mediation analysis results in Table 1 and Table 2, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the mediation analysis results:\n\n**For Table 1 (Operational Uncertainty):**\n- Path a: IP1 → FINRATIO = -0.0126***\n- Path b: IP1 → UNCERTAINTY = -0.0977***\n- Path c: UNCERTAINTY → FINRATIO = 0.0112***\n- Indirect effect = Path b × Path c = -0.0977 × 0.0112 = -0.0011\n\n**For Table 2 (Financial Constraints):**\n- Path a: IP1 → FINRATIO = -0.0126***\n- Path b: IP1 → KZ = -0.0794***\n- Path c: KZ → FINRATIO = 0.0022*\n- Indirect effect = Path b × Path c = -0.0794 × 0.0022 = -0.0002\n\n**Evaluating each option:**\n\nA) **Correct** - The coefficient for IP1 → KZ is -0.0794***, which is negative and highly significant. Since higher KZ means more constrained, a negative coefficient means industrial policy reduces financial constraints.\n\nB) **Correct** - The indirect effect through operational uncertainty is -0.0977 × 0.0112 = -0.0011 (approximately).\n\nC) **Incorrect** - The coefficient for UNCERTAINTY → FINRATIO is 0.0112***, which is positive. This means higher operational uncertainty leads to higher (not lower) corporate financialization.\n\nD) **Incorrect** - The total indirect effects are -0.0011 + (-0.0002) = -0.0013, which is only about 10% of the total effect (-0.0126). The mediation is partial, not full.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 328, "Question": "### Background\n\n**Research Question.** This case provides a comprehensive empirical evaluation of the consumption-based asset pricing model (CCAPM), testing whether incorporating taxes and tax-timing options can resolve its well-known empirical failures.\n\n**Setting.** The analysis compares the benchmark tax-free CCAPM (Model 1) against the paper's preferred specification, an asymmetric tax model (Model 4). Both models are estimated using the Generalized Method of Moments (GMM).\n\n**Variables and Parameters.**\n\n*   `J-statistic (χ²)`: GMM test for a model's overidentifying restrictions. A high value (low p-value) indicates model rejection.\n*   `u_{j,t+1}`: The model's pricing error for asset `j` (risk-adjusted after-tax return).\n\n---\n\n### Data / Model Specification\n\nThe GMM estimation results for the tax-free and asymmetric tax models are summarized in Table 1.\n\n**Table 1: GMM Estimation Results for Competing CCAPM Specifications**\n\n| Parameter / Statistic | Model 1 (Tax-Free) | Model 4 (Asymmetric Tax) |\n| :--- | :--- | :--- |\n| `J-statistic (χ²)` | 33.424 | 11.154 |\n| p-value | (< 0.001) | (0.025) |\n\nTo diagnose the rejection of Model 4, diagnostic OLS regressions were run to test if the model's pricing errors (`u_{j,t+1}`) are predictable by the GMM instruments. Key results for the one-month T-bill are in Table 2.\n\n**Table 2: Diagnostic OLS Regression for T-Bill Returns**\n\n| Dependent Variable | `R²` |\n| :--- | :--- |\n| Before-Tax Real Return | 0.221 |\n| Model 4 Pricing Error (`u_{f,t+1}`) | 0.104 |\n\nFinally, a formal Newey-West test of the null hypothesis that the tax-free model (Model 1) is correct against the alternative that the asymmetric tax model (Model 4) is correct yields a `χ²` statistic of 34.611 (p < 0.001).\n\n---\n\n### Question\n\nBased on the full set of statistical evidence provided, select ALL statements that correctly synthesize the findings regarding the models' performance and comparison.", "Options": {"A": "The diagnostic regression in Table 2 shows that Model 4 explains only 10.4% of the variation in T-bill returns, which is why it is rejected by the J-test.", "B": "The fact that both Model 1 and Model 4 are rejected by the J-test implies that incorporating taxes offers no meaningful improvement to the consumption-based asset pricing model.", "C": "The rejection of Model 4 by the J-test is partly explained by its failure to fully account for the predictability in T-bill returns, as the model's pricing errors still have a significant R² of 10.4%.", "D": "The Newey-West test result strongly indicates that the asymmetric tax model (Model 4) provides a statistically significant improvement in fit over the tax-free model (Model 1)."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize results from multiple, distinct statistical tests (J-test, OLS diagnostic, Newey-West comparison). It uses an **Atomic Decomposition** strategy to isolate key conclusions from the original QA's apex question. The depth strategy is **Reverse-Reasoning**, requiring the user to infer the reasons for model failure and the logic behind model comparison. Distractors are high-fidelity: Option C is a **Conceptual Opposite** that represents a common misinterpretation of nested vs. non-nested tests. Option D is a **Numerical (Misinterpretation)** error, confusing the R² of the pricing error with the R² of the return itself.", "qid": "328", "question": "### Background\n\n**Research Question.** This case provides a comprehensive empirical evaluation of the consumption-based asset pricing model (CCAPM), testing whether incorporating taxes and tax-timing options can resolve its well-known empirical failures.\n\n**Setting.** The analysis compares the benchmark tax-free CCAPM (Model 1) against the paper's preferred specification, an asymmetric tax model (Model 4). Both models are estimated using the Generalized Method of Moments (GMM).\n\n**Variables and Parameters.**\n\n*   `J-statistic (χ²)`: GMM test for a model's overidentifying restrictions. A high value (low p-value) indicates model rejection.\n*   `u_{j,t+1}`: The model's pricing error for asset `j` (risk-adjusted after-tax return).\n\n---\n\n### Data / Model Specification\n\nThe GMM estimation results for the tax-free and asymmetric tax models are summarized in Table 1.\n\n**Table 1: GMM Estimation Results for Competing CCAPM Specifications**\n\n| Parameter / Statistic | Model 1 (Tax-Free) | Model 4 (Asymmetric Tax) |\n| :--- | :--- | :--- |\n| `J-statistic (χ²)` | 33.424 | 11.154 |\n| p-value | (< 0.001) | (0.025) |\n\nTo diagnose the rejection of Model 4, diagnostic OLS regressions were run to test if the model's pricing errors (`u_{j,t+1}`) are predictable by the GMM instruments. Key results for the one-month T-bill are in Table 2.\n\n**Table 2: Diagnostic OLS Regression for T-Bill Returns**\n\n| Dependent Variable | `R²` |\n| :--- | :--- |\n| Before-Tax Real Return | 0.221 |\n| Model 4 Pricing Error (`u_{f,t+1}`) | 0.104 |\n\nFinally, a formal Newey-West test of the null hypothesis that the tax-free model (Model 1) is correct against the alternative that the asymmetric tax model (Model 4) is correct yields a `χ²` statistic of 34.611 (p < 0.001).\n\n---\n\n### Question\n\nBased on the full set of statistical evidence provided, select ALL statements that correctly synthesize the findings regarding the models' performance and comparison.", "model_evaluation_result": {"model_raw_response": "Looking at the statistical evidence:\n\n1. **J-test results**: Model 1 has J-statistic = 33.424 (p < 0.001), Model 4 has J-statistic = 11.154 (p = 0.025). Both models are rejected at conventional significance levels, but Model 4 has a much lower J-statistic.\n\n2. **Diagnostic regression**: The R² for Model 4's pricing error is 0.104, meaning 10.4% of the variation in the pricing error is predictable by the instruments. This indicates some remaining predictability that the model doesn't capture.\n\n3. **Newey-West test**: χ² = 34.611 (p < 0.001), testing H₀: Model 1 is correct vs H₁: Model 4 is correct. The strong rejection supports Model 4 over Model 1.\n\nEvaluating each option:\n\nA) Incorrect. The 10.4% R² is for the pricing error, not for T-bill returns themselves. The model explains much more of the returns (the before-tax return has R² = 0.221).\n\nB) Incorrect. While both models are rejected, Model 4 shows substantial improvement (lower J-statistic and Newey-West test favors it).\n\nC) Correct. The 10.4% R² for the pricing error indicates remaining predictability that contributes to the model's rejection.\n\nD) Correct. The Newey-West test with p < 0.001 strongly supports Model 4 over Model 1.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 398, "Question": "### Background\n\n**Research Question.** The efficiency of the S&P 500 futures market can be tested under two competing hypotheses: the simple unbiasedness hypothesis, which posits a direct cointegrating relationship between spot and futures prices, and the cost-of-carry model, which argues for a trivariate cointegrating relationship that includes a measure of financing costs. This investigation seeks to determine which model is empirically supported by analyzing the stability of the estimated relationships and the validity of theoretical parameter restrictions.\n\n**Setting / Data-Generating Environment.** A Vector Error Correction Model (VECM) framework is applied to daily S&P 500 spot prices (`s_t`), futures prices (`f_t`), and the 3-month Treasury bill rate (`r_t`) from April 1982 to June 2003. The analysis proceeds in three stages: (1) estimating a bivariate model for `(s_t, f_t)`, (2) estimating a trivariate model for `(s_t, f_t, r_t)`, and (3) testing for weak exogeneity within the preferred model to determine the locus of price discovery.\n\n### Data / Model Specification\n\nThe core of the analysis rests on the VECM representation of a system of `p` variables `X_t`:\n  \n\\Delta X_{t}=\\alpha \\beta' X_{t-1} + \\sum_{j=1}^{k-1} \\Gamma_j \\Delta X_{t-j} + \\mu + \\delta t + \\varepsilon_t \\quad \\text{(Eq. (1))}\n \nwhere `\\beta` contains the `r` long-run cointegrating vectors and `\\alpha` contains the adjustment coefficients. A variable `i` is weakly exogenous if its corresponding adjustment coefficient, `\\alpha_i`, is zero.\n\n**Table 1: Summary of Empirical Findings for Bivariate and Trivariate Models**\n\n| Feature | Bivariate Model (`s_t`, `f_t`) | Trivariate Model (`s_t`, `f_t`, `r_t`) |\n| :--- | :--- | :--- |\n| **Cointegrating Rank** | `r=1` is found (Trace stat = 44.59 > 25.47 crit.) | `r=1` is found (Trace stat = 25.08 ≈ 25.47 crit.) |\n| **Parameter Stability** | **Rejected**. The relationship is unstable over the sample. | **Not Rejected**. The relationship is stable over the sample. |\n| **Symmetry Test (`H_0: \\beta` contains `[1, -1]`)** | **Rejected**. (LR stat = 5.44, `\\chi^2(1)`). | **Not Rejected**. (LR stat = 0.27, `\\chi^2(1)`). |\n| **Weak Exogeneity of Spot Price (`H_0: \\alpha_s=0`)** | Not tested (model misspecified). | **Not Rejected**. (LR stat = 1.36, `\\chi^2(1)`). |\n| **Weak Exogeneity of Futures Price (`H_0: \\alpha_f=0`)** | Not tested (model misspecified). | **Rejected**. (LR stat = 31.47, `\\chi^2(1)`). |\n\n### Question\n\nBased on the findings for the Bivariate Model in **Table 1**, select all statements that correctly diagnose why this model is likely misspecified.", "Options": {"A": "The trace statistic of 44.59 is excessively high, which points to data contamination rather than a genuine economic relationship.", "B": "The successful identification of one cointegrating vector (r=1) is sufficient evidence to confirm the validity of the simple unbiasedness hypothesis, despite other contradictory findings.", "C": "The rejection of parameter stability indicates that the estimated long-run relationship is not consistent over time, a primary symptom of an omitted variable.", "D": "The rejection of the `[1, -1]` symmetry restriction contradicts a core theoretical expectation of asset pricing, suggesting the model is capturing a spurious relationship."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to diagnose model misspecification by synthesizing multiple, seemingly contradictory statistical results. It uses the **Atomic Decomposition** strategy, breaking the complex conclusion of the original QA problem into independently verifiable statements about the evidence. Options A and B are the two key pillars of the argument against the bivariate model. Distractor C represents a common error: focusing on a single confirmatory result while ignoring contradictory evidence ('confirmation bias'). Distractor D is a plausible but incorrect interpretation of a large test statistic, misattributing it to data error instead of model error.", "qid": "398", "question": "### Background\n\n**Research Question.** The efficiency of the S&P 500 futures market can be tested under two competing hypotheses: the simple unbiasedness hypothesis, which posits a direct cointegrating relationship between spot and futures prices, and the cost-of-carry model, which argues for a trivariate cointegrating relationship that includes a measure of financing costs. This investigation seeks to determine which model is empirically supported by analyzing the stability of the estimated relationships and the validity of theoretical parameter restrictions.\n\n**Setting / Data-Generating Environment.** A Vector Error Correction Model (VECM) framework is applied to daily S&P 500 spot prices (`s_t`), futures prices (`f_t`), and the 3-month Treasury bill rate (`r_t`) from April 1982 to June 2003. The analysis proceeds in three stages: (1) estimating a bivariate model for `(s_t, f_t)`, (2) estimating a trivariate model for `(s_t, f_t, r_t)`, and (3) testing for weak exogeneity within the preferred model to determine the locus of price discovery.\n\n### Data / Model Specification\n\nThe core of the analysis rests on the VECM representation of a system of `p` variables `X_t`:\n  \n\\Delta X_{t}=\\alpha \\beta' X_{t-1} + \\sum_{j=1}^{k-1} \\Gamma_j \\Delta X_{t-j} + \\mu + \\delta t + \\varepsilon_t \\quad \\text{(Eq. (1))}\n \nwhere `\\beta` contains the `r` long-run cointegrating vectors and `\\alpha` contains the adjustment coefficients. A variable `i` is weakly exogenous if its corresponding adjustment coefficient, `\\alpha_i`, is zero.\n\n**Table 1: Summary of Empirical Findings for Bivariate and Trivariate Models**\n\n| Feature | Bivariate Model (`s_t`, `f_t`) | Trivariate Model (`s_t`, `f_t`, `r_t`) |\n| :--- | :--- | :--- |\n| **Cointegrating Rank** | `r=1` is found (Trace stat = 44.59 > 25.47 crit.) | `r=1` is found (Trace stat = 25.08 ≈ 25.47 crit.) |\n| **Parameter Stability** | **Rejected**. The relationship is unstable over the sample. | **Not Rejected**. The relationship is stable over the sample. |\n| **Symmetry Test (`H_0: \\beta` contains `[1, -1]`)** | **Rejected**. (LR stat = 5.44, `\\chi^2(1)`). | **Not Rejected**. (LR stat = 0.27, `\\chi^2(1)`). |\n| **Weak Exogeneity of Spot Price (`H_0: \\alpha_s=0`)** | Not tested (model misspecified). | **Not Rejected**. (LR stat = 1.36, `\\chi^2(1)`). |\n| **Weak Exogeneity of Futures Price (`H_0: \\alpha_f=0`)** | Not tested (model misspecified). | **Rejected**. (LR stat = 31.47, `\\chi^2(1)`). |\n\n### Question\n\nBased on the findings for the Bivariate Model in **Table 1**, select all statements that correctly diagnose why this model is likely misspecified.", "model_evaluation_result": {"model_raw_response": "Looking at the findings for the Bivariate Model in Table 1, I need to identify which statements correctly diagnose why this model is likely misspecified.\n\nLet me analyze each option:\n\n**Option A**: \"The trace statistic of 44.59 is excessively high, which points to data contamination rather than a genuine economic relationship.\"\n- The trace statistic (44.59) exceeds the critical value (25.47), which simply confirms the presence of one cointegrating relationship\n- A high trace statistic above the critical value is actually what we expect when cointegration exists\n- This is not indicative of data contamination or model misspecification\n- **Incorrect**\n\n**Option B**: \"The successful identification of one cointegrating vector (r=1) is sufficient evidence to confirm the validity of the simple unbiasedness hypothesis, despite other contradictory findings.\"\n- While r=1 is found, the model shows parameter instability and rejects the symmetry restriction\n- These failures indicate the model is misspecified despite finding cointegration\n- Finding r=1 alone is not sufficient to validate the hypothesis\n- **Incorrect**\n\n**Option C**: \"The rejection of parameter stability indicates that the estimated long-run relationship is not consistent over time, a primary symptom of an omitted variable.\"\n- Parameter instability means the cointegrating relationship changes over time\n- This is indeed a classic symptom of omitted variable bias\n- The background suggests the trivariate model (which includes the interest rate) shows stable parameters, supporting this diagnosis\n- **Correct**\n\n**Option D**: \"The rejection of the [1, -1] symmetry restriction contradicts a core theoretical expectation of asset pricing, suggesting the model is capturing a spurious relationship.\"\n- The [1, -1] restriction represents the theoretical relationship between spot and futures prices\n- Its rejection indicates the empirical relationship doesn't match theory\n- This suggests the model is not capturing the true economic relationship, possibly due to misspecification\n- **Correct**\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 189, "Question": "### Background\n\nA Fama-MacBeth two-pass regression is used to test a two-factor model on monthly returns of US stock portfolios from 1983 to 1997. The market factor is the US-MSCI index return, and the active management factor is constructed relative to the S&P 500 benchmark. The model's key prediction is that the average active management risk premium, `γ̄_S&P`, is positive.\n\n### Data / Model Specification\n\nThe second-pass cross-sectional regression for each month `t` is:\n  \nR_{i,t}=γ_{0,t} + γ_{M,t}β̂_{M,t-1}^i + γ_{S&P,t}β̂_{S&P,t-1}^i + η_{i,t} \n \nwhere `γ_{M,t}` and `γ_{S&P,t}` are the estimated prices of market risk and active management risk, respectively.\n\n**Table 1. Fama-MacBeth Estimates of Risk Premia (×100)**\n*Panel A: US-MSCI (Market) vs. S&P 500 (Benchmark)*\n\n| Period      | `γ̄_{M,t}` | `γ̄_{S&P,t}` | `t(γ̄_{M,t})` | `t(γ̄_{S&P,t})` |\n| :---------- | :---------- | :------------ | :------------- | :--------------- |\n| 1983-1997   | 2.41        | 1.78          | 1.65           | 1.80             |\n| 1983-1991   | -0.28       | 0.36          | -0.17          | 0.41             |\n| 1992-1997   | 6.44        | 3.91          | 2.34           | 1.87             |\n\n---\n\nBased on the results in **Table 1**, which of the following conclusions are statistically supported?\n(Assume a one-tailed test for the active management premium and a two-tailed test for the market premium.)", "Options": {"A": "For the 1983-1991 sub-period, the market risk premium (`γ̄_M`) is statistically indistinguishable from zero at the 10% significance level.", "B": "For the full 1983-1997 period, the active management risk premium (`γ̄_S&P`) is positive and statistically significant at the 5% level.", "C": "For the 1992-1997 sub-period, the market risk premium (`γ̄_M`) is positive and statistically significant at the 1% level.", "D": "The positive and significant active management risk premium observed in the full sample is primarily driven by the results from the 1992-1997 sub-period."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to correctly interpret the results of a Fama-MacBeth regression, including hypothesis testing with appropriate significance levels and identifying the source of statistical significance across different time periods. Depth Strategy: Computational Judgment. The user must perform multiple hypothesis tests based on the provided t-statistics and critical values (e.g., ~1.65 for 5% one-tailed, ~1.96 for 5% two-tailed) and synthesize the findings. Distractor Logic: Option D is a numerical distractor. The t-statistic of 2.34 is significant at the 5% level (critical value ~1.96) but not at the 1% level (critical value ~2.58), testing for precision in statistical inference.", "qid": "189", "question": "### Background\n\nA Fama-MacBeth two-pass regression is used to test a two-factor model on monthly returns of US stock portfolios from 1983 to 1997. The market factor is the US-MSCI index return, and the active management factor is constructed relative to the S&P 500 benchmark. The model's key prediction is that the average active management risk premium, `γ̄_S&P`, is positive.\n\n### Data / Model Specification\n\nThe second-pass cross-sectional regression for each month `t` is:\n  \nR_{i,t}=γ_{0,t} + γ_{M,t}β̂_{M,t-1}^i + γ_{S&P,t}β̂_{S&P,t-1}^i + η_{i,t} \n \nwhere `γ_{M,t}` and `γ_{S&P,t}` are the estimated prices of market risk and active management risk, respectively.\n\n**Table 1. Fama-MacBeth Estimates of Risk Premia (×100)**\n*Panel A: US-MSCI (Market) vs. S&P 500 (Benchmark)*\n\n| Period      | `γ̄_{M,t}` | `γ̄_{S&P,t}` | `t(γ̄_{M,t})` | `t(γ̄_{S&P,t})` |\n| :---------- | :---------- | :------------ | :------------- | :--------------- |\n| 1983-1997   | 2.41        | 1.78          | 1.65           | 1.80             |\n| 1983-1991   | -0.28       | 0.36          | -0.17          | 0.41             |\n| 1992-1997   | 6.44        | 3.91          | 2.34           | 1.87             |\n\n---\n\nBased on the results in **Table 1**, which of the following conclusions are statistically supported?\n(Assume a one-tailed test for the active management premium and a two-tailed test for the market premium.)", "model_evaluation_result": {"model_raw_response": "Looking at the table and the statistical significance tests:\n\nFor option A (1983-1991 market premium):\n- t-statistic = -0.17\n- Two-tailed test at 10% level requires |t| > 1.645\n- Since |-0.17| = 0.17 < 1.645, we cannot reject the null hypothesis\n- The market premium is statistically indistinguishable from zero ✓\n\nFor option B (1983-1997 active management premium):\n- t-statistic = 1.80\n- One-tailed test at 5% level requires t > 1.645\n- Since 1.80 > 1.645, the premium is statistically significant at 5% level ✓\n\nFor option C (1992-1997 market premium):\n- t-statistic = 2.34\n- Two-tailed test at 1% level requires |t| > 2.576\n- Since 2.34 < 2.576, it's not significant at the 1% level ✗\n\nFor option D (driver of full sample results):\n- Full sample (1983-1997): γ̄_S&P = 1.78, t = 1.80 (significant at 5%)\n- 1983-1991: γ̄_S&P = 0.36, t = 0.41 (not significant)\n- 1992-1997: γ̄_S&P = 3.91, t = 1.87 (significant at 5%)\n- The 1992-1997 period shows much larger premium and is the only sub-period with statistical significance ✓\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 303, "Question": "### Background\n\n**Research Question.** In a Bayesian setting, how is the total uncertainty about a future observation decomposed into process risk versus parameter uncertainty, and how does this decomposition inform the optimal linear forecast of the mean?\n\n**Setting.** Consider a random observable, `\\tilde{x}` (e.g., an insurance claim), whose likelihood `p(x|\\theta)` depends on an unknown risk parameter `\\tilde{\\theta}`. The parameter `\\tilde{\\theta}` is itself a random variable with a prior density `p(\\theta)`. The goal is to forecast the next observation, `\\tilde{x}_{n+1}`, using a linear function of the mean of `n` previous observations, `\\bar{x}`.\n\n**Variables and Parameters.**\n- `\\tilde{x}`: A random observable.\n- `\\tilde{\\theta}`: An unobservable random parameter.\n- `m_1 = \\mathcal{E}[\\tilde{x}]`: The prior mean of `\\tilde{x}`.\n- `e`: Expected process variance, or \"within-risk\" variance.\n- `d`: Variance of hypothetical means, or \"between-risk\" variance.\n- `c`: Total variance of the observable `\\tilde{x}`.\n- `f_1^*(\\mathcal{D})`: The linear credibility forecast for the mean.\n- `z_1`: The credibility factor.\n\n---\n\n### Data / Model Specification\n\nThe total variance of `\\tilde{x}` can be decomposed according to the law of total variance:\n\n  \nc = \\mathcal{V}\\{\\tilde{x}\\} = e + d \\quad \\text{(Eq. (1))}\n \n\nwhere `e` and `d` are defined as:\n\n  \ne = \\mathcal{E} [\\mathcal{V}\\{\\tilde{x}|\\tilde{\\theta}\\}] \\quad \\text{(Eq. (2))}\n \n\n  \nd = \\mathcal{V} [\\mathcal{E}\\{\\tilde{x}|\\tilde{\\theta}\\}] \\quad \\text{(Eq. (3))}\n \n\nThe optimal linear forecast (credibility formula) for the mean is a weighted average of the prior mean `m_1` and the sample mean `\\bar{x}`:\n\n  \nf_{1}^{*}(\\mathcal{D})=(1-z_{1})m_{1}+z_{1}\\bar{x} \\quad \\text{(Eq. (4))}\n \n\n---\n\n### Question\n\nBased on the credibility framework provided, select all of the following statements that are true.", "Options": {"A": "The component `e`, the \"within-risk\" variance, represents the variation in average risk levels across the entire population of policyholders.", "B": "The \"credibility time constant\" `n_{01}`, which determines how much weight is given to new data, is defined by the ratio of the expected process variance to the variance of hypothetical means (`e/d`).", "C": "In an actuarial context, the component `d`, the \"between-risk\" variance, represents parameter uncertainty about a policyholder's true underlying risk type.", "D": "The credibility factor `z_1` is given by `d / (e/n + d)`. In the limit as `n \\to \\infty`, `z_1` approaches `d/e`."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item tests understanding of the fundamental decomposition of variance in credibility theory (`e` vs. `d`) and the resulting formula for the optimal credibility weight. The strategy is atomic decomposition, breaking down core concepts into distinct, verifiable statements. Option C is a 'Conceptual Opposite' distractor, swapping the interpretations of `e` and `d`, a common error. Option D presents a correct intermediate formula for `z_1` but draws an incorrect conclusion about its asymptotic limit, testing for a deeper conceptual understanding.", "qid": "303", "question": "### Background\n\n**Research Question.** In a Bayesian setting, how is the total uncertainty about a future observation decomposed into process risk versus parameter uncertainty, and how does this decomposition inform the optimal linear forecast of the mean?\n\n**Setting.** Consider a random observable, `\\tilde{x}` (e.g., an insurance claim), whose likelihood `p(x|\\theta)` depends on an unknown risk parameter `\\tilde{\\theta}`. The parameter `\\tilde{\\theta}` is itself a random variable with a prior density `p(\\theta)`. The goal is to forecast the next observation, `\\tilde{x}_{n+1}`, using a linear function of the mean of `n` previous observations, `\\bar{x}`.\n\n**Variables and Parameters.**\n- `\\tilde{x}`: A random observable.\n- `\\tilde{\\theta}`: An unobservable random parameter.\n- `m_1 = \\mathcal{E}[\\tilde{x}]`: The prior mean of `\\tilde{x}`.\n- `e`: Expected process variance, or \"within-risk\" variance.\n- `d`: Variance of hypothetical means, or \"between-risk\" variance.\n- `c`: Total variance of the observable `\\tilde{x}`.\n- `f_1^*(\\mathcal{D})`: The linear credibility forecast for the mean.\n- `z_1`: The credibility factor.\n\n---\n\n### Data / Model Specification\n\nThe total variance of `\\tilde{x}` can be decomposed according to the law of total variance:\n\n  \nc = \\mathcal{V}\\{\\tilde{x}\\} = e + d \\quad \\text{(Eq. (1))}\n \n\nwhere `e` and `d` are defined as:\n\n  \ne = \\mathcal{E} [\\mathcal{V}\\{\\tilde{x}|\\tilde{\\theta}\\}] \\quad \\text{(Eq. (2))}\n \n\n  \nd = \\mathcal{V} [\\mathcal{E}\\{\\tilde{x}|\\tilde{\\theta}\\}] \\quad \\text{(Eq. (3))}\n \n\nThe optimal linear forecast (credibility formula) for the mean is a weighted average of the prior mean `m_1` and the sample mean `\\bar{x}`:\n\n  \nf_{1}^{*}(\\mathcal{D})=(1-z_{1})m_{1}+z_{1}\\bar{x} \\quad \\text{(Eq. (4))}\n \n\n---\n\n### Question\n\nBased on the credibility framework provided, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\n**A) The component `e`, the \"within-risk\" variance, represents the variation in average risk levels across the entire population of policyholders.**\n\nThis is **FALSE**. From Eq. (2), `e = E[V{x|θ}]` represents the expected variance of observations given the parameter θ. This is the \"within-risk\" variance - the variability of individual observations around their conditional mean for a given risk type. It does NOT represent variation in average risk levels across the population.\n\n**B) The \"credibility time constant\" `n_{01}`, which determines how much weight is given to new data, is defined by the ratio of the expected process variance to the variance of hypothetical means (`e/d`).**\n\nThis is **TRUE**. In credibility theory, the credibility time constant is indeed defined as n₀₁ = e/d. This ratio determines how quickly the credibility factor increases with sample size.\n\n**C) In an actuarial context, the component `d`, the \"between-risk\" variance, represents parameter uncertainty about a policyholder's true underlying risk type.**\n\nThis is **TRUE**. From Eq. (3), `d = V[E{x|θ}]` represents the variance of the conditional means across different values of θ. This captures the uncertainty about which risk type (parameter value) a policyholder belongs to.\n\n**D) The credibility factor `z_1` is given by `d / (e/n + d)`. In the limit as `n → ∞`, `z_1` approaches `d/e`.**\n\nThis is **FALSE**. The correct formula for the credibility factor is:\nz₁ = n/(n + e/d)\n\nThis can be rewritten as z₁ = nd/(nd + e) = d/(d + e/n)\n\nAs n → ∞, z₁ approaches 1, not d/e.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 341, "Question": "### Background\n\n**Research Question.** How does the market's valuation of a firm's current operations (assets in place) and future opportunities (real options) depend on the interaction between the firm's profitability and its growth stage?\n\n**Setting.** A valuation model is estimated on distinct subsamples of firms, partitioned by profitability (profit vs. loss) and sales growth (low vs. high). The analysis focuses on the valuation coefficients of Net Income (`NI`), a proxy for assets in place, and R&D expenditures (`RD`), a proxy for real options. The persistence of abnormal earnings is also estimated as a foundational input for the valuation model and as a justification for the valuation patterns.\n\n**Variables and Parameters.**\n\n*   `P_{i,t}`: Price per share of firm `i` at the end of year `t`.\n*   `NI_{i,t}`: Net income per share, a proxy for the value of assets in place.\n*   `RD_{i,t}`: R&D expenditures per share, a proxy for investment in real options.\n*   `NI_{i,t}^a`: Abnormal earnings for firm `i` in period `t`.\n*   `\\omega_1`: The persistence parameter of abnormal earnings.\n*   `\\beta_3`, `\\beta_4`: Regression coefficients on `NI` and `RD`, respectively.\n\n---\n\n### Data / Model Specification\n\nThe following valuation model is estimated:\n\n  \nP_{i,t} = \\beta_0 + \\beta_1 BV_{i,t} + \\beta_2 DIV_{i,t} + \\beta_3 NI_{i,t} + \\beta_4 RD_{i,t} + \\beta_5 OTHER_{i,t} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nThe key hypotheses regarding profitability and growth are:\n*   **H2a:** For firms reporting a loss, the market places a lower weight on net income and positively values R&D expenditures.\n*   **H2b:** For firms reporting a loss, the market places a higher value on R&D expenditures when the firm is experiencing high sales growth.\n*   **H2c:** For firms reporting a profit, the market positively values R&D expenditures when the firm is experiencing high sales growth, but not when the firm is experiencing low sales growth.\n\nTable 1 below presents the estimated persistence parameters (`\\hat{\\omega}_1`) for various subsamples. Table 2 presents the key coefficient estimates (`\\hat{\\beta}_3` and `\\hat{\\beta}_4`) from estimating Eq. (1) on four subsamples.\n\n**Table 1: Abnormal Earnings Persistence Parameters (`\\hat{\\omega}_1`)**\n\n| | Entire sample | Low sales growth firms | High sales growth firms |\n| :--- | :---: | :---: | :---: |\n| **Entire sample** | 0.271 | 0.350 | 0.229 |\n| **Loss firms** | 0.133 | 0.207 | 0.081 |\n| **Profit firms** | 0.339 | 0.391 | 0.288 |\n\n*Note: Differences between high and low sales growth firms are statistically significant at p < 0.05.*\n\n**Table 2: Valuation Regression Results for `NI` and `RD`**\n\n| Variable | Subsample | Low Growth | High Growth |\n| :--- | :--- | :---: | :---: |\n| **NI** (`\\hat{\\beta}_3`) | Loss Firms | 0.440 | 0.371 |\n| | | (1.38) | (0.60) |\n| | Profit Firms | **3.608** | **1.252** |\n| | | (19.26) | (4.31) |\n| **RD** (`\\hat{\\beta}_4`) | Loss Firms | **2.100** | **4.081** |\n| | | (5.09) | (5.21) |\n| | Profit Firms | **-3.285** | **1.880** |\n| | | (10.48) | (4.23) |\n\n*Note: t-statistics are in parentheses. Bolded coefficients are significant at p < 0.01. Differences between low and high growth are statistically significant.*\n\n---\n\n### Question\n\nBy synthesizing the findings on abnormal earnings persistence (Table 1) and net income valuation (Table 2), which of the following conclusions are supported by the data?", "Options": {"A": "The abnormal earnings of loss-making firms are more persistent than those of profitable firms, justifying the higher valuation of Book Value (`BV`) for loss firms.", "B": "High-growth firms have higher `NI` valuation coefficients because their higher R&D spending signals greater future profitability.", "C": "The market places a lower valuation multiple on the Net Income (`NI`) of high-growth firms in part because their abnormal earnings are empirically less persistent than those of low-growth firms.", "D": "The statistically insignificant valuation of Net Income (`NI`) for loss-making firms is consistent with their low abnormal earnings persistence."}, "Answer": ["C", "D"], "pi_justification": "This item captures the synthesis component of the original Table QA problem, requiring students to connect results from two different tables (earnings persistence and valuation). The strategy is **Reverse-Reasoning**, where the observed valuation pattern for Net Income is explained by the underlying persistence data. Distractors test for incorrect causal links (D) or factual errors in reading the persistence table (B).", "qid": "341", "question": "### Background\n\n**Research Question.** How does the market's valuation of a firm's current operations (assets in place) and future opportunities (real options) depend on the interaction between the firm's profitability and its growth stage?\n\n**Setting.** A valuation model is estimated on distinct subsamples of firms, partitioned by profitability (profit vs. loss) and sales growth (low vs. high). The analysis focuses on the valuation coefficients of Net Income (`NI`), a proxy for assets in place, and R&D expenditures (`RD`), a proxy for real options. The persistence of abnormal earnings is also estimated as a foundational input for the valuation model and as a justification for the valuation patterns.\n\n**Variables and Parameters.**\n\n*   `P_{i,t}`: Price per share of firm `i` at the end of year `t`.\n*   `NI_{i,t}`: Net income per share, a proxy for the value of assets in place.\n*   `RD_{i,t}`: R&D expenditures per share, a proxy for investment in real options.\n*   `NI_{i,t}^a`: Abnormal earnings for firm `i` in period `t`.\n*   `\\omega_1`: The persistence parameter of abnormal earnings.\n*   `\\beta_3`, `\\beta_4`: Regression coefficients on `NI` and `RD`, respectively.\n\n---\n\n### Data / Model Specification\n\nThe following valuation model is estimated:\n\n  \nP_{i,t} = \\beta_0 + \\beta_1 BV_{i,t} + \\beta_2 DIV_{i,t} + \\beta_3 NI_{i,t} + \\beta_4 RD_{i,t} + \\beta_5 OTHER_{i,t} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nThe key hypotheses regarding profitability and growth are:\n*   **H2a:** For firms reporting a loss, the market places a lower weight on net income and positively values R&D expenditures.\n*   **H2b:** For firms reporting a loss, the market places a higher value on R&D expenditures when the firm is experiencing high sales growth.\n*   **H2c:** For firms reporting a profit, the market positively values R&D expenditures when the firm is experiencing high sales growth, but not when the firm is experiencing low sales growth.\n\nTable 1 below presents the estimated persistence parameters (`\\hat{\\omega}_1`) for various subsamples. Table 2 presents the key coefficient estimates (`\\hat{\\beta}_3` and `\\hat{\\beta}_4`) from estimating Eq. (1) on four subsamples.\n\n**Table 1: Abnormal Earnings Persistence Parameters (`\\hat{\\omega}_1`)**\n\n| | Entire sample | Low sales growth firms | High sales growth firms |\n| :--- | :---: | :---: | :---: |\n| **Entire sample** | 0.271 | 0.350 | 0.229 |\n| **Loss firms** | 0.133 | 0.207 | 0.081 |\n| **Profit firms** | 0.339 | 0.391 | 0.288 |\n\n*Note: Differences between high and low sales growth firms are statistically significant at p < 0.05.*\n\n**Table 2: Valuation Regression Results for `NI` and `RD`**\n\n| Variable | Subsample | Low Growth | High Growth |\n| :--- | :--- | :---: | :---: |\n| **NI** (`\\hat{\\beta}_3`) | Loss Firms | 0.440 | 0.371 |\n| | | (1.38) | (0.60) |\n| | Profit Firms | **3.608** | **1.252** |\n| | | (19.26) | (4.31) |\n| **RD** (`\\hat{\\beta}_4`) | Loss Firms | **2.100** | **4.081** |\n| | | (5.09) | (5.21) |\n| | Profit Firms | **-3.285** | **1.880** |\n| | | (10.48) | (4.23) |\n\n*Note: t-statistics are in parentheses. Bolded coefficients are significant at p < 0.01. Differences between low and high growth are statistically significant.*\n\n---\n\n### Question\n\nBy synthesizing the findings on abnormal earnings persistence (Table 1) and net income valuation (Table 2), which of the following conclusions are supported by the data?", "model_evaluation_result": {"model_raw_response": "Looking at the data systematically:\n\nFrom Table 1 (Abnormal Earnings Persistence):\n- Loss firms have lower persistence (0.133) than profit firms (0.339)\n- High growth firms have lower persistence than low growth firms across all categories\n- For profit firms specifically: low growth (0.391) > high growth (0.288)\n\nFrom Table 2 (Valuation Coefficients):\n- NI coefficients for loss firms are not statistically significant (t-stats of 1.38 and 0.60)\n- NI coefficients for profit firms are significant and higher for low growth (3.608) than high growth (1.252)\n- RD coefficients show different patterns for loss vs profit firms\n\nEvaluating each option:\n\nA) FALSE - Loss firms have LOWER persistence (0.133) than profit firms (0.339), not higher.\n\nB) FALSE - High-growth firms have LOWER NI valuation coefficients than low-growth firms (1.252 vs 3.608 for profit firms).\n\nC) TRUE - The data shows:\n   - High-growth firms have lower NI valuation multiples (1.252 vs 3.608 for profit firms)\n   - High-growth firms have lower abnormal earnings persistence (0.288 vs 0.391 for profit firms)\n   - This relationship is consistent with valuation theory where less persistent earnings receive lower multiples\n\nD) TRUE - Loss firms show:\n   - Statistically insignificant NI coefficients (t-stats below 2)\n   - Very low abnormal earnings persistence (0.133)\n   - This alignment makes economic sense as unreliable earnings would not be valued by the market\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 387, "Question": "### Background\n\n**Research Question.** How do flawed internal performance metrics and incentive schemes lead to systematic capital misallocation, creating vulnerabilities that activists can exploit through strategic proposals like spin-offs or redesigned compensation contracts?\n\n**Setting.** A multi-division corporation where capital allocation decisions are decentralized. Divisional managers are compensated based on their division's performance, but the chosen metric may not align with overall shareholder value creation. The firm operates an internal capital market to allocate a fixed capital budget between its divisions.\n\n**Variables & Parameters.**\n*   `ROIC`: Return on Invested Capital (Net Operating Profit After Tax / Invested Capital).\n*   `WACC`: Weighted Average Cost of Capital, the firm's hurdle rate.\n*   `EVA`: Economic Value Added, defined as `(ROIC - WACC) × Invested Capital`.\n\n---\n\n### Data / Model Specification\n\nThe paper argues that operational and strategic vulnerabilities often stem from flawed internal processes. For example, a focus on maximizing average Return on Invested Capital (ROIC) can lead to suboptimal investment decisions. The text provides a specific case:\n\n*   A company has a corporate `WACC` of 10%.\n*   It has two divisions, A and B, with current performance:\n    *   Division A: `ROIC_A` = 30%\n    *   Division B: `ROIC_B` = 6%\n*   Two new, independent projects are available:\n    *   Project A (for Division A): offers `ROIC_new_A` = 20%\n    *   Project B (for Division B): offers `ROIC_new_B` = 8%\n\n---\n\n### Question\n\nBased on the scenario provided, select all statements that are TRUE regarding the investment decisions and outcomes under different incentive schemes.", "Options": {"A": "If incentives are switched to maximizing Economic Value Added (EVA), the manager of Division B will accept the new project offering an 8% ROIC.", "B": "Under an ROIC-maximization scheme, the manager of the low-return Division B (6% ROIC) will reject the new project offering an 8% ROIC.", "C": "If incentives are switched to maximizing Economic Value Added (EVA), both divisions will make investment decisions that align with shareholder value creation.", "D": "Under an ROIC-maximization scheme, the manager of the high-return Division A (30% ROIC) will reject the new project offering a 20% ROIC."}, "Answer": ["C", "D"], "pi_justification": "This item assesses the ability to diagnose capital allocation distortions caused by flawed performance metrics (ROIC) and to evaluate the corrective properties of a superior metric (EVA). It uses a Computational Judgment strategy, requiring the user to apply two different decision rules to a concrete scenario from the paper. The correct options (A, D) are derived by correctly applying both rules. Distractor B incorrectly applies the ROIC rule, while distractor C incorrectly applies the EVA rule, targeting common conceptual errors.", "qid": "387", "question": "### Background\n\n**Research Question.** How do flawed internal performance metrics and incentive schemes lead to systematic capital misallocation, creating vulnerabilities that activists can exploit through strategic proposals like spin-offs or redesigned compensation contracts?\n\n**Setting.** A multi-division corporation where capital allocation decisions are decentralized. Divisional managers are compensated based on their division's performance, but the chosen metric may not align with overall shareholder value creation. The firm operates an internal capital market to allocate a fixed capital budget between its divisions.\n\n**Variables & Parameters.**\n*   `ROIC`: Return on Invested Capital (Net Operating Profit After Tax / Invested Capital).\n*   `WACC`: Weighted Average Cost of Capital, the firm's hurdle rate.\n*   `EVA`: Economic Value Added, defined as `(ROIC - WACC) × Invested Capital`.\n\n---\n\n### Data / Model Specification\n\nThe paper argues that operational and strategic vulnerabilities often stem from flawed internal processes. For example, a focus on maximizing average Return on Invested Capital (ROIC) can lead to suboptimal investment decisions. The text provides a specific case:\n\n*   A company has a corporate `WACC` of 10%.\n*   It has two divisions, A and B, with current performance:\n    *   Division A: `ROIC_A` = 30%\n    *   Division B: `ROIC_B` = 6%\n*   Two new, independent projects are available:\n    *   Project A (for Division A): offers `ROIC_new_A` = 20%\n    *   Project B (for Division B): offers `ROIC_new_B` = 8%\n\n---\n\n### Question\n\nBased on the scenario provided, select all statements that are TRUE regarding the investment decisions and outcomes under different incentive schemes.", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement based on the given scenario where divisions make investment decisions under different incentive schemes.\n\nGiven information:\n- Corporate WACC = 10%\n- Division A: Current ROIC = 30%, New project ROIC = 20%\n- Division B: Current ROIC = 6%, New project ROIC = 8%\n\nLet me evaluate each option:\n\n**Option A**: Under EVA maximization, Division B will accept the 8% ROIC project.\n- EVA = (ROIC - WACC) × Invested Capital\n- For the new project: EVA = (8% - 10%) × Capital = -2% × Capital < 0\n- Since EVA is negative, Division B would REJECT the project under EVA maximization.\n- This statement is FALSE.\n\n**Option B**: Under ROIC maximization, Division B will reject the 8% ROIC project.\n- Current ROIC for Division B = 6%\n- New project ROIC = 8%\n- Since 8% > 6%, accepting the project would INCREASE the division's average ROIC.\n- Under ROIC maximization, Division B would ACCEPT the project.\n- This statement is FALSE.\n\n**Option C**: Under EVA maximization, both divisions align with shareholder value.\n- Division A's new project: EVA = (20% - 10%) × Capital = 10% × Capital > 0 → ACCEPT\n- Division B's new project: EVA = (8% - 10%) × Capital = -2% × Capital < 0 → REJECT\n- EVA > 0 means value creation; EVA < 0 means value destruction.\n- Under EVA, divisions accept only value-creating projects (ROIC > WACC).\n- This statement is TRUE.\n\n**Option D**: Under ROIC maximization, Division A will reject the 20% ROIC project.\n- Current ROIC for Division A = 30%\n- New project ROIC = 20%\n- Since 20% < 30%, accepting the project would DECREASE the division's average ROIC.\n- Under ROIC maximization, Division A would REJECT the project.\n- This statement", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 350, "Question": "### Background\n\n**Research Question.** This case examines the empirical evidence for traditional fiscal theories of inflation, which posit that inflation is driven by government financing needs. The \"efficient government\" or \"tax-smoothing\" model predicts that all tax instruments, including the inflation tax, should be used in tandem to finance government spending, implying specific correlations between inflation and fiscal variables.\n\n**Variables & Parameters.**\n- `π`: Annual inflation rate.\n- `Budget surplus/GDP`: Ratio of government budget surplus to GDP.\n- `Expenditure/GDP`: Ratio of government expenditure to GDP.\n- `Revenue/GDP`: Ratio of government revenue to GDP.\n\n---\n\n### Data / Model Specification\n\nThe efficient government model predicts:\n1.  A positive correlation between `π` and `Expenditure/GDP`.\n2.  A positive correlation between `π` and `Revenue/GDP`.\n3.  A negative correlation between `π` and `Budget surplus/GDP`.\n\n**Table 1. Time-Series Correlations: Annual `π` with Fiscal Variables (Excerpt)**\n\n| Country | (a) Budget surplus/GDP | (b) Expenditure/GDP | (c) Revenue/GDP |\n| :------ | :--------------------- | :------------------ | :-------------- |\n| Mexico  | -0.879                 | 0.880               | 0.736           |\n| Ghana   | -0.060                 | -0.621              | -0.763          |\n\n**Table 2. Cross-Country Correlation Matrix (using sample averages)**\n\n|                     | `π`    | Expenditure/GDP | Revenue/GDP |\n| :------------------ | :----- | :-------------- | :---------- |\n| `π`                 | 1.0    | -0.019          | -0.210      |\n| Expenditure/GDP     | -0.019 | 1.0             | 0.929       |\n| Revenue/GDP         | -0.210 | 0.929           | 1.0         |\n\n---\n\n### Question\n\nAccording to the provided tables and the paper's analysis, which of the following statements accurately describe the empirical evidence regarding traditional fiscal theories of inflation?", "Options": {"A": "The time-series data present a mixed picture: Mexico's fiscal correlations are consistent with the efficient government model's predictions, whereas Ghana's are not.", "B": "The paper concludes that the time-series evidence, exemplified by countries like Mexico, provides universal support for the tax-smoothing model of public finance.", "C": "The cross-country data in Table 2 show a strong positive correlation between inflation and government expenditure, supporting the idea that high-spending countries inflate more.", "D": "The cross-country data in Table 2 contradict the efficient government model's key predictions, showing a near-zero correlation between inflation and expenditure and a negative correlation between inflation and revenue."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize evidence from both time-series and cross-sectional data to evaluate a theory. It uses an Atomic Decomposition strategy, with correct options summarizing the findings from each data source. Distractors are designed to trap common errors: a direct misreading of the data (C) and an overgeneralization from a single case (D).", "qid": "350", "question": "### Background\n\n**Research Question.** This case examines the empirical evidence for traditional fiscal theories of inflation, which posit that inflation is driven by government financing needs. The \"efficient government\" or \"tax-smoothing\" model predicts that all tax instruments, including the inflation tax, should be used in tandem to finance government spending, implying specific correlations between inflation and fiscal variables.\n\n**Variables & Parameters.**\n- `π`: Annual inflation rate.\n- `Budget surplus/GDP`: Ratio of government budget surplus to GDP.\n- `Expenditure/GDP`: Ratio of government expenditure to GDP.\n- `Revenue/GDP`: Ratio of government revenue to GDP.\n\n---\n\n### Data / Model Specification\n\nThe efficient government model predicts:\n1.  A positive correlation between `π` and `Expenditure/GDP`.\n2.  A positive correlation between `π` and `Revenue/GDP`.\n3.  A negative correlation between `π` and `Budget surplus/GDP`.\n\n**Table 1. Time-Series Correlations: Annual `π` with Fiscal Variables (Excerpt)**\n\n| Country | (a) Budget surplus/GDP | (b) Expenditure/GDP | (c) Revenue/GDP |\n| :------ | :--------------------- | :------------------ | :-------------- |\n| Mexico  | -0.879                 | 0.880               | 0.736           |\n| Ghana   | -0.060                 | -0.621              | -0.763          |\n\n**Table 2. Cross-Country Correlation Matrix (using sample averages)**\n\n|                     | `π`    | Expenditure/GDP | Revenue/GDP |\n| :------------------ | :----- | :-------------- | :---------- |\n| `π`                 | 1.0    | -0.019          | -0.210      |\n| Expenditure/GDP     | -0.019 | 1.0             | 0.929       |\n| Revenue/GDP         | -0.210 | 0.929           | 1.0         |\n\n---\n\n### Question\n\nAccording to the provided tables and the paper's analysis, which of the following statements accurately describe the empirical evidence regarding traditional fiscal theories of inflation?", "model_evaluation_result": {"model_raw_response": "Looking at the empirical evidence presented:\n\n**Time-series data (Table 1):**\n- Mexico: Shows correlations consistent with the efficient government model (negative correlation with budget surplus: -0.879, positive with expenditure: 0.880, positive with revenue: 0.736)\n- Ghana: Shows correlations inconsistent with the model (negative correlations with both expenditure: -0.621 and revenue: -0.763)\n\n**Cross-country data (Table 2):**\n- Correlation between π and Expenditure/GDP: -0.019 (near zero, not positive as predicted)\n- Correlation between π and Revenue/GDP: -0.210 (negative, not positive as predicted)\n\nEvaluating each option:\n\nA) TRUE - Mexico's correlations align with the model's predictions while Ghana's do not, presenting a mixed picture.\n\nB) FALSE - The evidence is mixed (Ghana contradicts the model), so it cannot provide \"universal support.\"\n\nC) FALSE - The cross-country correlation between inflation and expenditure is -0.019 (near zero), not a \"strong positive correlation.\"\n\nD) TRUE - The cross-country data show near-zero correlation with expenditure (-0.019) and negative correlation with revenue (-0.210), contradicting the model's predictions of positive correlations.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 268, "Question": "### Background\n\nThe analysis uses a cross-sectional regression to estimate the impact of flow risk on mutual fund performance while controlling for the endogeneity of investor flows.\n\n### Data / Model Specification\n\nThe baseline model is:\n  \n\\alpha_{i}=\\beta_{0}+\\beta_{1} \\cdot inflow_{i}+\\beta_{2} \\cdot outflow_{i}+\\beta_{3} \\cdot flow\\_risk_{i}+\\dots+\\varepsilon_{i}\n \nwhere `flow_risk` is the mean of monthly absolute net flows. The model is estimated using two-stage least squares (2SLS) with lagged instruments.\n\n**Table 1: Selected Summary Statistics and Regression Results**\n\n| Variable | Mean | Std. Dev. | Coefficient (2SLS) |\n| :--- | :--- | :--- | :--- |\n| `flow_risk` | 0.0294 | 0.0187 | -0.0795 |\n| `inflow` | 0.0376 | 0.0410 | 0.0796 |\n| `outflow` | 0.0310 | 0.0377 | -0.0576 |\n\n---\n\nBased on the model and data in **Table 1**, which of the following conclusions are supported?\n", "Options": {"A": "The negative and significant coefficient on `flow_risk` supports the paper's flow risk hypothesis, indicating that the magnitude of investor flows, regardless of direction, is detrimental to performance.", "B": "The positive and significant coefficient on `inflow` is consistent with a \"smart money\" effect, where investors direct capital to better-performing funds.", "C": "The estimated annualized performance drag for a fund with `flow_risk` one standard deviation above the mean is approximately -1.78%.", "D": "The estimated monthly performance drag for a fund with `flow_risk` one standard deviation above the mean is approximately -0.0795%."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the ability to interpret regression coefficients, calculate their economic significance, and connect them to the underlying economic hypotheses (flow risk and smart money effects). It uses a Computational Judgment strategy.\n\nDistractor Logic:\n- A (Correct): Requires a multi-step calculation: `StdDev(flow_risk) × Coeff(flow_risk) × 12 = 0.0187 × (-0.0795) × 12 ≈ -0.0178` or -1.78%.\n- B (Correct): Requires the correct economic interpretation of the `inflow` coefficient in the context of performance-chasing literature.\n- C (Correct): Requires the correct interpretation of the main coefficient of interest (`flow_risk`) and linking it to the paper's primary hypothesis.\n- D (Incorrect): This is a Unit/Scale Error. It confuses the coefficient's value (the effect of a one-unit change) with the effect of a one-standard-deviation change and also misinterprets the scale (it's -0.149%, not -0.0795%).", "qid": "268", "question": "### Background\n\nThe analysis uses a cross-sectional regression to estimate the impact of flow risk on mutual fund performance while controlling for the endogeneity of investor flows.\n\n### Data / Model Specification\n\nThe baseline model is:\n  \n\\alpha_{i}=\\beta_{0}+\\beta_{1} \\cdot inflow_{i}+\\beta_{2} \\cdot outflow_{i}+\\beta_{3} \\cdot flow\\_risk_{i}+\\dots+\\varepsilon_{i}\n \nwhere `flow_risk` is the mean of monthly absolute net flows. The model is estimated using two-stage least squares (2SLS) with lagged instruments.\n\n**Table 1: Selected Summary Statistics and Regression Results**\n\n| Variable | Mean | Std. Dev. | Coefficient (2SLS) |\n| :--- | :--- | :--- | :--- |\n| `flow_risk` | 0.0294 | 0.0187 | -0.0795 |\n| `inflow` | 0.0376 | 0.0410 | 0.0796 |\n| `outflow` | 0.0310 | 0.0377 | -0.0576 |\n\n---\n\nBased on the model and data in **Table 1**, which of the following conclusions are supported?\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to evaluate each conclusion based on the regression results and calculations.\n\n**Option A**: The coefficient on `flow_risk` is -0.0795, which is negative. The text states this coefficient is from a 2SLS regression, suggesting it's likely significant (though significance levels aren't explicitly shown). This negative coefficient indicates that higher flow risk (magnitude of flows regardless of direction) is associated with lower alpha, supporting the flow risk hypothesis. ✓\n\n**Option B**: The coefficient on `inflow` is 0.0796, which is positive. Again, from the 2SLS context, this appears to be significant. A positive coefficient means higher inflows are associated with better performance, which is consistent with the \"smart money\" effect where investors successfully identify and invest in better-performing funds. ✓\n\n**Option C**: To calculate the annualized performance drag:\n- One standard deviation of `flow_risk` = 0.0187\n- Coefficient on `flow_risk` = -0.0795\n- Monthly performance impact = -0.0795 × 0.0187 = -0.00148665\n- Annualized impact = -0.00148665 × 12 = -0.0178398 ≈ -1.78%\nThis matches the statement. ✓\n\n**Option D**: The monthly performance drag calculation:\n- One standard deviation of `flow_risk` = 0.0187\n- Coefficient on `flow_risk` = -0.0795\n- Monthly performance impact = -0.0795 × 0.0187 = -0.00148665 ≈ -0.149%\n\nThe statement claims this equals -0.0795%, which is incorrect. The coefficient itself (-0.0795) represents the change in alpha per unit change in flow_risk, not the effect of a one standard deviation change. ✗\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 363, "Question": "### Background\n\nThis case examines the effectiveness of traditional corporate governance mechanisms and proposes an evolution in the relationship between corporate management and large institutional investors. The author argues that conventional mechanisms are often reactive and ill-suited to the modern reality of concentrated ownership, necessitating a shift from a 'control' orientation to a more collaborative 'value-added' orientation.\n\n### Data / Model Specification\n\nThe analysis contrasts two models for the investor-management relationship:\n\n| Feature             | Control Orientation                                | Value-Added Orientation                                      |\n| :------------------ | :------------------------------------------------- | :----------------------------------------------------------- |\n| **Core Assumption** | Adversarial; investors must control managers.      | Collaborative; based on mutual dependence.                   |\n| **Primary Goal**    | Discipline managers; react to poor performance.    | Create shared understanding; prevent crises.                 |\n| **Key Tools**       | Incentive contracts, board pressure, hostile threats. | Continuous dialogue, information sharing, strategic counsel. |\n| **Information Flow**| Limited, backward-looking, constrained by legal rules. | Rich, forward-looking, focused on strategic assumptions.     |\n| **Focus**           | Crisis management.                                 | Crisis prevention.                                           |\n\nThis shift is motivated by two empirical realities: (1) External interventions like hostile takeovers are 'lagging indicators' of managerial failure, occurring only after significant value is destroyed. (2) The rise of large, concentrated institutional investors, who cannot easily sell their illiquid stakes, has created a 'mutual dependence' between managers and major shareholders, making the old adversarial model untenable.\n\nTo formalize this strategic shift, consider a one-shot game between a manager (M) and a large institutional investor (I). The manager first decides whether to be 'Transparent' (T) or 'Opaque' (O). The investor then decides whether to be 'Supportive' (S) or 'Adversarial' (A). The payoffs (Manager, Investor) are given in Table 1.\n\n**Table 1: Payoff Matrix**\n\n|         | Investor: Supportive (S) | Investor: Adversarial (A) |\n| :------ | :---------------------- | :------------------------ |\n| **M: T**| (3, 3)                  | (0, 2)                    |\n| **M: O**| (2, 0)                  | (1, 1)                    |\n\n### Question\n\nBased on the one-shot game described in Table 1, select ALL of the following statements that are correct.", "Options": {"A": "The outcome (Opaque, Adversarial) is a pure-strategy Nash Equilibrium.", "B": "The 'Adversarial' strategy is a dominant strategy for the Investor.", "C": "The outcome (Transparent, Supportive) is a pure-strategy Nash Equilibrium.", "D": "The outcome (Opaque, Supportive) is a pure-strategy Nash Equilibrium."}, "Answer": ["A", "C"], "pi_justification": "This item replaces a QA problem (Suitability Score: 9.0) whose core analytical task was a game-theoretic calculation, making it highly suitable for conversion. It uses a 'Computational Judgment' strategy, requiring the user to derive the Nash Equilibria. The 'Atomic Decomposition' method is used to create two independently correct options (the two pure-strategy NE), ensuring a genuine multi-select format. Distractors are based on common, high-fidelity errors in game theory: C incorrectly identifies a dominant strategy, and D identifies a non-equilibrium outcome where one player has a clear incentive to deviate.", "qid": "363", "question": "### Background\n\nThis case examines the effectiveness of traditional corporate governance mechanisms and proposes an evolution in the relationship between corporate management and large institutional investors. The author argues that conventional mechanisms are often reactive and ill-suited to the modern reality of concentrated ownership, necessitating a shift from a 'control' orientation to a more collaborative 'value-added' orientation.\n\n### Data / Model Specification\n\nThe analysis contrasts two models for the investor-management relationship:\n\n| Feature             | Control Orientation                                | Value-Added Orientation                                      |\n| :------------------ | :------------------------------------------------- | :----------------------------------------------------------- |\n| **Core Assumption** | Adversarial; investors must control managers.      | Collaborative; based on mutual dependence.                   |\n| **Primary Goal**    | Discipline managers; react to poor performance.    | Create shared understanding; prevent crises.                 |\n| **Key Tools**       | Incentive contracts, board pressure, hostile threats. | Continuous dialogue, information sharing, strategic counsel. |\n| **Information Flow**| Limited, backward-looking, constrained by legal rules. | Rich, forward-looking, focused on strategic assumptions.     |\n| **Focus**           | Crisis management.                                 | Crisis prevention.                                           |\n\nThis shift is motivated by two empirical realities: (1) External interventions like hostile takeovers are 'lagging indicators' of managerial failure, occurring only after significant value is destroyed. (2) The rise of large, concentrated institutional investors, who cannot easily sell their illiquid stakes, has created a 'mutual dependence' between managers and major shareholders, making the old adversarial model untenable.\n\nTo formalize this strategic shift, consider a one-shot game between a manager (M) and a large institutional investor (I). The manager first decides whether to be 'Transparent' (T) or 'Opaque' (O). The investor then decides whether to be 'Supportive' (S) or 'Adversarial' (A). The payoffs (Manager, Investor) are given in Table 1.\n\n**Table 1: Payoff Matrix**\n\n|         | Investor: Supportive (S) | Investor: Adversarial (A) |\n| :------ | :---------------------- | :------------------------ |\n| **M: T**| (3, 3)                  | (0, 2)                    |\n| **M: O**| (2, 0)                  | (1, 1)                    |\n\n### Question\n\nBased on the one-shot game described in Table 1, select ALL of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "I need to analyze this game theory problem step by step.\n\nFirst, let me understand the game:\n- Manager moves first: chooses Transparent (T) or Opaque (O)\n- Investor moves second: chooses Supportive (S) or Adversarial (A)\n- This is a sequential game, but I need to find Nash equilibria\n\nThe payoff matrix shows (Manager payoff, Investor payoff):\n- (T, S): (3, 3)\n- (T, A): (0, 2)\n- (O, S): (2, 0)\n- (O, A): (1, 1)\n\nTo find Nash equilibria, I need to check each outcome to see if either player would want to deviate unilaterally.\n\nLet me check each option:\n\n**Option A: (O, A) is a Nash equilibrium**\n- At (O, A): payoffs are (1, 1)\n- If Manager deviates to T (keeping Investor at A): payoffs become (0, 2). Manager gets 0 < 1, so won't deviate.\n- If Investor deviates to S (keeping Manager at O): payoffs become (2, 0). Investor gets 0 < 1, so won't deviate.\n- Neither player wants to deviate, so (O, A) IS a Nash equilibrium. ✓\n\n**Option B: 'Adversarial' is a dominant strategy for Investor**\n- For a dominant strategy, A must be better than S regardless of Manager's choice.\n- If Manager plays T: Investor gets 2 with A vs 3 with S. So S is better.\n- If Manager plays O: Investor gets 1 with A vs 0 with S. So A is better.\n- Since A is not always better, it's NOT a dominant strategy. ✗\n\n**Option C: (T, S) is a Nash equilibrium**\n- At (T, S): payoffs are (3, 3)\n- If Manager deviates to O (keeping Investor at S): payoffs become (2, 0). Manager gets 2 < 3, so won't deviate.\n- If Investor de", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 368, "Question": "### Background\n\nThis case examines the conceptual foundations of the Z-score, a widely used measure of bank solvency. The Z-score is defined as a measure of the number of standard deviations that a bank's asset returns must fall to cause insolvency. It is calculated as:\n\n  \nZ \\cong \\frac{ROAA + K}{\\sigma}\n \n\nwhere `ROAA` is the return on average assets, `K` is the equity-to-assets ratio, and `σ` is the standard deviation of `ROAA`. The Z-score can be additively decomposed into two components, `Z = Z_1 + Z_2`.\n\n### Question\n\nBased on the definition and standard interpretation of the Z-score, which of the following statements are correct? Select all that apply.", "Options": {"A": "Using a short rolling window to calculate `σ` can create an artificially high Z-score if a bank has a period of unusually stable returns, as `σ` would be close to zero.", "B": "The component `Z_1 = ROAA / σ` can be interpreted as a 'performance buffer' that measures how many standard deviations of losses can be absorbed by current earnings.", "C": "The component `Z_2 = K / σ` measures the bank's risk-adjusted profitability.", "D": "The two main components of the Z-score are `ROAA` and `K`, representing the two primary sources of solvency."}, "Answer": ["A", "B"], "pi_justification": "This item tests foundational understanding of the Z-score, the paper's key dependent variable. It assesses knowledge of the components' interpretation, a key methodological nuance in its calculation, and the precise formula. Distractors are based on common conceptual errors: C swaps the interpretations of the two components, and D confuses the raw inputs (`ROAA`, `K`) with the risk-adjusted component ratios (`ROAA/σ`, `K/σ`).", "qid": "368", "question": "### Background\n\nThis case examines the conceptual foundations of the Z-score, a widely used measure of bank solvency. The Z-score is defined as a measure of the number of standard deviations that a bank's asset returns must fall to cause insolvency. It is calculated as:\n\n  \nZ \\cong \\frac{ROAA + K}{\\sigma}\n \n\nwhere `ROAA` is the return on average assets, `K` is the equity-to-assets ratio, and `σ` is the standard deviation of `ROAA`. The Z-score can be additively decomposed into two components, `Z = Z_1 + Z_2`.\n\n### Question\n\nBased on the definition and standard interpretation of the Z-score, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) This is correct. The Z-score formula has σ in the denominator. If σ approaches zero (during unusually stable periods with a short window), the Z-score would approach infinity, creating an artificially high value that doesn't reflect true solvency.\n\nB) This is correct. Z₁ = ROAA/σ represents how many standard deviations below zero the current return (ROAA) is. This acts as a performance buffer showing how much volatility in returns can be absorbed by current profitability before reaching zero returns.\n\nC) This is incorrect. Z₂ = K/σ represents the capital buffer - how many standard deviations of losses the equity capital can absorb. It's not a measure of risk-adjusted profitability.\n\nD) This is incorrect. The two main components are Z₁ and Z₂ (the additive decomposition mentioned), not ROAA and K themselves. ROAA and K are inputs to the formula, not the components of the decomposition.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 405, "Question": "### Background\n\n**Research Question.** Having established that liquidity enhances market efficiency, this question explores two deeper issues: (1) Did this effect vary by firm size? and (2) What is the underlying market-maker behavior that generates this predictability in the first place? The paper contrasts two hypotheses for predictability: it arises either from rational but capacity-constrained market makers, or from cognitively limited (behaviorally biased) market makers.\n\n**Setting and Sample.** The analysis uses 5-minute data for NYSE stocks, stratified by firm size. Order imbalance (`OIB$`) is decomposed into an expected component (`OIBP$`) and an unexpected innovation (`OIBI$`) using an AR(12) model.\n\n### Data / Model Specification\n\n**Model 1: Size-Stratified Regressions.** The main interaction model is run separately for Large, Mid-Cap, and Small firm portfolios.\n  \nReturn_t = \\alpha + \\beta_1 OIB\\$_{t-1} + \\beta_2 (OIB\\$_{t-1} \\cdot ILD_t) + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n**Model 2: OIB Decomposition.** The predictive regression is re-run using the decomposed components of order flow.\n  \nReturn_t = \\alpha + \\beta OIBI\\$_{t-1} + \\gamma OIBP\\$_{t-1} + ... + \\epsilon_t \\quad \\text{(Eq. 2)}\n \n\n**Table 1. Regression Results for Heterogeneity and Mechanism**\n\n| Panel A: Baseline Predictability (`β₁` from Eq. 1) by Firm Size and Tick Regime | Large Firms | Small Firms |\n| :--- | :--- | :--- |\n| Eighths Regime | 0.0293 | 0.0115 |\n| Decimal Regime | 0.0029 | 0.0245 |\n\n| Panel B: OIB Decomposition Results (Eq. 2, Sixteenths Regime) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| Unexpected OIB (`OIBI$t-1`) | 0.0131 | 7.51 |\n| Expected OIB (`OIBP$t-1`) | 0.0012 | 0.16 |\n\n### Question\n\nBased on the provided results regarding heterogeneity and market-maker behavior, which of the following conclusions are supported by the paper?", "Options": {"A": "The evidence from Panel A suggests that the reduction in minimum tick size had a much stronger positive impact on the market efficiency of Large Firms compared to Small Firms.", "B": "For Small Firms, the baseline predictability coefficient (`β₁`) decreased from the Eighths to the Decimal regime, indicating an improvement in market efficiency.", "C": "The results in Panel B, where only the unexpected component of order flow (`OIBI$`) predicts returns, are consistent with a market where predictability arises from the limited risk-bearing capacity of rational market makers, not their cognitive biases.", "D": "The results in Panel B support the hypothesis of cognitively limited market makers, as both expected and unexpected order flow components significantly predict returns."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret findings on both subgroup heterogeneity and the underlying economic mechanism. It uses an Atomic Decomposition strategy, testing two distinct conclusions from the paper. Distractor C is a Conceptual Opposite, reversing the observed trend for small firms shown in the table. Distractor D misinterprets the results of the mechanism test, supporting the rejected hypothesis.", "qid": "405", "question": "### Background\n\n**Research Question.** Having established that liquidity enhances market efficiency, this question explores two deeper issues: (1) Did this effect vary by firm size? and (2) What is the underlying market-maker behavior that generates this predictability in the first place? The paper contrasts two hypotheses for predictability: it arises either from rational but capacity-constrained market makers, or from cognitively limited (behaviorally biased) market makers.\n\n**Setting and Sample.** The analysis uses 5-minute data for NYSE stocks, stratified by firm size. Order imbalance (`OIB$`) is decomposed into an expected component (`OIBP$`) and an unexpected innovation (`OIBI$`) using an AR(12) model.\n\n### Data / Model Specification\n\n**Model 1: Size-Stratified Regressions.** The main interaction model is run separately for Large, Mid-Cap, and Small firm portfolios.\n  \nReturn_t = \\alpha + \\beta_1 OIB\\$_{t-1} + \\beta_2 (OIB\\$_{t-1} \\cdot ILD_t) + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n**Model 2: OIB Decomposition.** The predictive regression is re-run using the decomposed components of order flow.\n  \nReturn_t = \\alpha + \\beta OIBI\\$_{t-1} + \\gamma OIBP\\$_{t-1} + ... + \\epsilon_t \\quad \\text{(Eq. 2)}\n \n\n**Table 1. Regression Results for Heterogeneity and Mechanism**\n\n| Panel A: Baseline Predictability (`β₁` from Eq. 1) by Firm Size and Tick Regime | Large Firms | Small Firms |\n| :--- | :--- | :--- |\n| Eighths Regime | 0.0293 | 0.0115 |\n| Decimal Regime | 0.0029 | 0.0245 |\n\n| Panel B: OIB Decomposition Results (Eq. 2, Sixteenths Regime) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| Unexpected OIB (`OIBI$t-1`) | 0.0131 | 7.51 |\n| Expected OIB (`OIBP$t-1`) | 0.0012 | 0.16 |\n\n### Question\n\nBased on the provided results regarding heterogeneity and market-maker behavior, which of the following conclusions are supported by the paper?", "model_evaluation_result": {"model_raw_response": "Looking at the data systematically:\n\n**Panel A Analysis:**\n- Large Firms: β₁ decreased from 0.0293 (Eighths) to 0.0029 (Decimal) - a reduction of ~90%\n- Small Firms: β₁ increased from 0.0115 (Eighths) to 0.0245 (Decimal) - an increase of ~113%\n\nOption A is correct: The reduction in tick size improved market efficiency for Large Firms (lower predictability) much more than for Small Firms (predictability actually increased).\n\nOption B is incorrect: Small Firms' predictability coefficient increased, not decreased, indicating worse market efficiency.\n\n**Panel B Analysis:**\n- Unexpected OIB (OIBI$_{t-1}$): coefficient = 0.0131, t-stat = 7.51 (highly significant)\n- Expected OIB (OIBP$_{t-1}$): coefficient = 0.0012, t-stat = 0.16 (not significant)\n\nOption C is correct: Only unexpected order flow predicts returns. This pattern is consistent with rational but capacity-constrained market makers who cannot fully accommodate unexpected order flow shocks due to inventory/capital constraints.\n\nOption D is incorrect: Expected order flow does not significantly predict returns (t-stat = 0.16), contradicting the cognitive bias hypothesis which would predict both components to be significant.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 374, "Question": "### Background\n\n**Research Question.** What combination of misaligned incentives and fragile financial engineering led to the high rate of LBO defaults in the late 1980s, and how did the market correct these flaws in the early 1990s?\n\n**Setting.** The case examines the structural evolution of LBOs, contrasting the aggressive, high-failure deals of 1985-1989 with the more conservative, restructured deals of the early 1990s. The analysis focuses on the role of purchase prices, capital structure, and promoter incentives as documented by Kaplan and Stein.\n\n**Variables & Parameters.**\n- `P`: Purchase Price (monetary units).\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization (monetary units).\n- `D`: Total Debt (monetary units).\n- `E`: Total Equity (monetary units).\n- `I`: Interest Expense (monetary units).\n- `C_p`: Purchase Multiple, `P / EBITDA` (dimensionless).\n- `E_pct`: Equity Percentage, `E / (D+E)` (dimensionless).\n- `ICR`: Interest Coverage Ratio, `EBITDA / I` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Evolution of LBO Financial Structures**\n\n| Metric | Late 1980s (1986-88) | Early 1990s (1992-95) |\n| :--- | :--- | :--- |\n| Purchase Multiple (`C_p`) | 7.0x - 10.0x | 5.0x - 6.0x |\n| Equity Percentage (`E_pct`) | 5% - 10% | 20% - 30% |\n| Interest Coverage (`ICR`) | Often < 1.0x | Minimum of 2.0x |\n\n---\n\n### Question\n\nConsider a company with a stable `EBITDA` of $200 million and an average interest rate on debt of 10%. Based on the data in **Table 1**, select all of the following statements that are correct when comparing a representative \"late-1980s\" deal (8.0x purchase multiple, 10% equity) with a representative \"early-1990s\" deal (5.5x purchase multiple, 25% equity).", "Options": {"A": "The total debt in the late-1980s deal is $1,440 million.", "B": "The Interest Coverage Ratio (ICR) for the early-1990s deal is approximately 2.42x.", "C": "The early-1990s deal structure has a higher debt-to-equity ratio than the late-1980s deal.", "D": "The annual interest expense for the late-1980s deal is $160 million."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform multi-step calculations based on table data and draw accurate comparisons, a core quantitative skill. It uses a 'Computational Judgment' strategy with 'Atomic Decomposition' to create multiple correct, independently verifiable statements. Distractor C is a 'Step-Omission Error' (confusing equity with interest expense), and Distractor D is a 'Conceptual Opposite' regarding leverage trends.", "qid": "374", "question": "### Background\n\n**Research Question.** What combination of misaligned incentives and fragile financial engineering led to the high rate of LBO defaults in the late 1980s, and how did the market correct these flaws in the early 1990s?\n\n**Setting.** The case examines the structural evolution of LBOs, contrasting the aggressive, high-failure deals of 1985-1989 with the more conservative, restructured deals of the early 1990s. The analysis focuses on the role of purchase prices, capital structure, and promoter incentives as documented by Kaplan and Stein.\n\n**Variables & Parameters.**\n- `P`: Purchase Price (monetary units).\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization (monetary units).\n- `D`: Total Debt (monetary units).\n- `E`: Total Equity (monetary units).\n- `I`: Interest Expense (monetary units).\n- `C_p`: Purchase Multiple, `P / EBITDA` (dimensionless).\n- `E_pct`: Equity Percentage, `E / (D+E)` (dimensionless).\n- `ICR`: Interest Coverage Ratio, `EBITDA / I` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Evolution of LBO Financial Structures**\n\n| Metric | Late 1980s (1986-88) | Early 1990s (1992-95) |\n| :--- | :--- | :--- |\n| Purchase Multiple (`C_p`) | 7.0x - 10.0x | 5.0x - 6.0x |\n| Equity Percentage (`E_pct`) | 5% - 10% | 20% - 30% |\n| Interest Coverage (`ICR`) | Often < 1.0x | Minimum of 2.0x |\n\n---\n\n### Question\n\nConsider a company with a stable `EBITDA` of $200 million and an average interest rate on debt of 10%. Based on the data in **Table 1**, select all of the following statements that are correct when comparing a representative \"late-1980s\" deal (8.0x purchase multiple, 10% equity) with a representative \"early-1990s\" deal (5.5x purchase multiple, 25% equity).", "model_evaluation_result": {"model_raw_response": "I need to calculate the financial metrics for both deals and compare them.\n\nGiven information:\n- EBITDA = $200 million\n- Interest rate on debt = 10%\n\n**Late 1980s deal:**\n- Purchase Multiple (C_p) = 8.0x\n- Equity Percentage (E_pct) = 10%\n\nPurchase Price (P) = C_p × EBITDA = 8.0 × $200M = $1,600M\n\nSince E_pct = 10%, then E = 0.10 × $1,600M = $160M\nAnd D = P - E = $1,600M - $160M = $1,440M\n\nInterest Expense (I) = 0.10 × D = 0.10 × $1,440M = $144M\n\nInterest Coverage Ratio (ICR) = EBITDA / I = $200M / $144M ≈ 1.39x\n\n**Early 1990s deal:**\n- Purchase Multiple (C_p) = 5.5x\n- Equity Percentage (E_pct) = 25%\n\nPurchase Price (P) = C_p × EBITDA = 5.5 × $200M = $1,100M\n\nSince E_pct = 25%, then E = 0.25 × $1,100M = $275M\nAnd D = P - E = $1,100M - $275M = $825M\n\nInterest Expense (I) = 0.10 × D = 0.10 × $825M = $82.5M\n\nInterest Coverage Ratio (ICR) = EBITDA / I = $200M / $82.5M ≈ 2.42x\n\nNow let me check each option:\n\nA) The total debt in the late-1980s deal is $1,440 million.\n✓ Correct - I calculated D = $1,440M for the late-1980s deal.\n\nB) The Interest Coverage Ratio (ICR) for the early-1990s deal is approximately 2.42x.\n✓ Correct - I calculated ICR ≈ 2.", "model_answer": ["A", "B", "D"], "error": null}}
{"ID": 383, "Question": "### Background\n\n**Research Question.** How can the causal effect of foreign official inflows on U.S. Treasury yields be estimated when these inflows are endogenous to market conditions, and are the findings robust across different measures of risk premium?\n\n**Setting.** A two-stage least squares (2SLS) regression framework is used to estimate the short-run causal effect of foreign official inflows on two different measures of the bond risk premium: the model-based 5-year term premium (`\\Delta TP_t^5`) and the model-free realized one-year excess return (`XR_6t`). A key challenge is that unobserved “flight-to-safety” episodes can simultaneously lower Treasury yields and reduce foreign official inflows, biasing ordinary least squares (OLS) estimates.\n\n### Data / Model Specification\n\nThe general regression model is:\n\n  \n\\text{Risk Premium}_t = \\alpha + \\beta_1 (\\text{Foreign Inflows}_t) + \\mathbf{X}_t'\\gamma + u_t\n \n\nwhere `\\mathbf{X}_t` is a vector of exogenous controls. To address the endogeneity of `\\text{Foreign Inflows}_t`, instruments such as Japanese foreign exchange interventions (`JPYFXINT_t`) are used.\n\n**Table 1. The Impact of Foreign Official Inflows on U.S. Bond Risk Premia**\n\n| | (1) OLS | (2) IV / 2SLS | (3) IV / 2SLS | (4) IV / 2SLS |\n| :--- | :---: | :---: | :---: | :---: |\n| **Dependent Variable:** | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `XR_6t` |\n| | | | | |\n| `\\Delta FOIL/DEBT_{t-1}` | 0.052 | -0.135** | | |\n| (Monthly Inflows) | (0.030) | (0.061) | | |\n| | | | | |\n| `\\Delta FOIL_JAPAN/DEBT_{t-1}` | | | -0.147** | |\n| (Japanese Monthly Inflows) | | | (0.059) | | |\n| | | | | |\n| `\\sum_{12} FOI_t / DEBT_{t-12}` | | | | 0.595*** |\n| (Annual Inflows) | | | | (0.184) |\n| | | | | |\n| **Specification Tests** | | | | |\n| Cragg-Donald F-Stat | | 15.72 | 97.59 | 92.79 |\n| *Weak Inst. Critical Value* | | *11.59* | *8.96* | *11.59* |\n| Hansen J Test (p-value) | | 0.3498 | n.a. | 0.3563 |\n\n*Notes: Standard errors in parentheses. *p<0.1, **p<0.05, ***p<0.01. `\\Delta TP_t^5` is the monthly change in the 5-year term premium. `XR_6t` is the one-year realized excess return. `FOIL/DEBT` denotes foreign official inflows scaled by outstanding marketable debt.*\n\n### Question\n\nBased on the provided theory and regression results in Table 1, select all of the following statements that are correct interpretations of the findings regarding endogeneity and the main instrumental variable (IV) estimate.", "Options": {"A": "The 2SLS estimate in Column (2) indicates that a $100 billion foreign official inflow would lower the 5-year term premium by approximately 46 basis points, and this effect is statistically significant at the 5% level.", "B": "The diagnostic tests for the specification in Column (2) suggest the instruments are weak, as the Cragg-Donald F-statistic (15.72) is low, and invalid, as the Hansen J-test p-value (0.3498) is high.", "C": "The positive OLS coefficient (0.052) in Column (1) is consistent with an upward bias caused by \"flight-to-safety\" episodes, where unobserved risk aversion simultaneously lowers yields and reduces foreign official inflows.", "D": "The 2SLS coefficient of -0.135 implies that after correcting for endogeneity, increased foreign official inflows are associated with a statistically significant *increase* in the 5-year term premium."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the core concepts of endogeneity bias and IV estimate interpretation, converted from a Table QA problem. It uses an **Atomic Decomposition** strategy, breaking the original QA's parts 1 and 2 into distinct, verifiable statements. **Correct options (A, B)** test the user's ability to explain the OLS bias mechanism and to correctly interpret the magnitude and significance of the 2SLS result. **Distractor C (Sign Error)** targets a fundamental misreading of the coefficient's sign. **Distractor D (Instrument Misinterpretation)** targets common errors in interpreting diagnostic statistics: confusing a high F-statistic with weakness and a high p-value on the Hansen J-test with invalidity.", "qid": "383", "question": "### Background\n\n**Research Question.** How can the causal effect of foreign official inflows on U.S. Treasury yields be estimated when these inflows are endogenous to market conditions, and are the findings robust across different measures of risk premium?\n\n**Setting.** A two-stage least squares (2SLS) regression framework is used to estimate the short-run causal effect of foreign official inflows on two different measures of the bond risk premium: the model-based 5-year term premium (`\\Delta TP_t^5`) and the model-free realized one-year excess return (`XR_6t`). A key challenge is that unobserved “flight-to-safety” episodes can simultaneously lower Treasury yields and reduce foreign official inflows, biasing ordinary least squares (OLS) estimates.\n\n### Data / Model Specification\n\nThe general regression model is:\n\n  \n\\text{Risk Premium}_t = \\alpha + \\beta_1 (\\text{Foreign Inflows}_t) + \\mathbf{X}_t'\\gamma + u_t\n \n\nwhere `\\mathbf{X}_t` is a vector of exogenous controls. To address the endogeneity of `\\text{Foreign Inflows}_t`, instruments such as Japanese foreign exchange interventions (`JPYFXINT_t`) are used.\n\n**Table 1. The Impact of Foreign Official Inflows on U.S. Bond Risk Premia**\n\n| | (1) OLS | (2) IV / 2SLS | (3) IV / 2SLS | (4) IV / 2SLS |\n| :--- | :---: | :---: | :---: | :---: |\n| **Dependent Variable:** | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `XR_6t` |\n| | | | | |\n| `\\Delta FOIL/DEBT_{t-1}` | 0.052 | -0.135** | | |\n| (Monthly Inflows) | (0.030) | (0.061) | | |\n| | | | | |\n| `\\Delta FOIL_JAPAN/DEBT_{t-1}` | | | -0.147** | |\n| (Japanese Monthly Inflows) | | | (0.059) | | |\n| | | | | |\n| `\\sum_{12} FOI_t / DEBT_{t-12}` | | | | 0.595*** |\n| (Annual Inflows) | | | | (0.184) |\n| | | | | |\n| **Specification Tests** | | | | |\n| Cragg-Donald F-Stat | | 15.72 | 97.59 | 92.79 |\n| *Weak Inst. Critical Value* | | *11.59* | *8.96* | *11.59* |\n| Hansen J Test (p-value) | | 0.3498 | n.a. | 0.3563 |\n\n*Notes: Standard errors in parentheses. *p<0.1, **p<0.05, ***p<0.01. `\\Delta TP_t^5` is the monthly change in the 5-year term premium. `XR_6t` is the one-year realized excess return. `FOIL/DEBT` denotes foreign official inflows scaled by outstanding marketable debt.*\n\n### Question\n\nBased on the provided theory and regression results in Table 1, select all of the following statements that are correct interpretations of the findings regarding endogeneity and the main instrumental variable (IV) estimate.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement against the theory and regression results in Table 1.\n\n**Statement A**: \"The 2SLS estimate in Column (2) indicates that a $100 billion foreign official inflow would lower the 5-year term premium by approximately 46 basis points...\"\n\nThe coefficient in Column (2) is -0.135**. To interpret this, I need to understand the scaling. The variable is `ΔFOIL/DEBT_{t-1}`, which represents foreign official inflows scaled by outstanding marketable debt. Without knowing the exact scaling factor, I cannot verify the 46 basis points calculation. However, the negative coefficient (-0.135) with ** significance (p<0.05) does indicate that foreign official inflows lower the term premium at the 5% significance level. This statement could be correct if the scaling interpretation is accurate.\n\n**Statement B**: \"The diagnostic tests for the specification in Column (2) suggest the instruments are weak... and invalid...\"\n\n- Cragg-Donald F-stat = 15.72 > Critical value = 11.59 → Instruments are NOT weak\n- Hansen J-test p-value = 0.3498 > 0.10 → Fail to reject null of valid instruments → Instruments are valid\n\nThis statement is incorrect on both counts.\n\n**Statement C**: \"The positive OLS coefficient (0.052) in Column (1) is consistent with an upward bias caused by 'flight-to-safety' episodes...\"\n\nThe background explains that flight-to-safety episodes simultaneously lower Treasury yields (and thus term premiums) and reduce foreign official inflows. This creates a negative correlation between the error term and foreign inflows, leading to upward bias in OLS. The positive OLS coefficient (0.052) compared to the negative IV coefficient (-0.135) is indeed consistent with this upward bias story. This statement is correct.\n\n**Statement D**: \"The 2SLS coefficient of -0.135 implies... increased foreign official inflows are associated with a statistically significant increase in the 5-year term premium.\"\n\nThe coefficient is -0.135, which means increased foreign official inflows are associated with a DECREASE (not increase) in the term premium. This statement is incorrect.\n\nBased on this analysis, statements A and C appear to", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 354, "Question": "### Background\nA study models the effect of country-level public anti-self-dealing regulations (`PUBLIC_c`) on firm-level corporate valuation (`Tobin's Q_it`). The model includes country fixed effects (`δ_c`) to control for unobserved, time-invariant country characteristics.\n\n### Data / Model Specification\nThe model is specified as:\n\n  \n\\text{Tobin's Q}_{it} = \\alpha + \\beta_{1}\\mathrm{WEDGE}_{it} + \\beta_{2}\\mathrm{PUBLIC}_{c} + \\beta_{3}(\\mathrm{WEDGE}_{it} * \\mathrm{PUBLIC}_{c}) + \\dots + \\delta_c + \\varepsilon_{it} \\quad \\text{(Eq. 1)}\n \n\nWhere `i` is firm, `c` is country, and `t` is year. The `PUBLIC_c` index is time-invariant within the sample period. In a standard 'within-estimator' fixed-effects model, the coefficient `β₂` on the time-invariant `PUBLIC_c` variable would be unidentified due to perfect collinearity with the country fixed effects `δ_c`.\n\n---\n\nGiven this specification, which of the following statements about the identification of the model's coefficients are correct? Select all that apply.", "Options": {"A": "The coefficient `β₂` is unidentified in this model due to perfect collinearity between `PUBLIC_c` and the set of country fixed effects `δ_c`.", "B": "The coefficient on the firm-level variable `WEDGE_it`, `β₁`, is identified primarily from the variation of `WEDGE` over time for each individual firm.", "C": "The coefficient on the interaction term, `β₃`, is identified primarily from the variation of `WEDGE_it` across firms within the same country.", "D": "The main effect of public regulation, `β₂`, is identified by assuming a linear effect of `PUBLIC_c` and using the cross-country variation that remains after omitting one country's fixed effect as a baseline."}, "Answer": ["C", "D"], "pi_justification": "This item assesses a sophisticated understanding of econometric identification in panel data models, a key methodological issue for the paper's empirical strategy. It uses a Depth Strategy focusing on the technical underpinnings of the model. Option A is correct because the interaction term varies within countries (due to firm-level `WEDGE`), allowing its coefficient to be estimated separately from the country fixed effects. Option B correctly describes how the coefficient on a time-invariant regressor is identified in a Least Squares Dummy Variable (LSDV) model. Option C is a distractor (Conceptual Opposite) that incorrectly applies the logic of a 'within-estimator' to this specific model. Option D is a distractor (Almost Right); while time-series variation could contribute, the primary source of identification for `β₁` in this pooled OLS setup is the cross-sectional variation between firms.", "qid": "354", "question": "### Background\nA study models the effect of country-level public anti-self-dealing regulations (`PUBLIC_c`) on firm-level corporate valuation (`Tobin's Q_it`). The model includes country fixed effects (`δ_c`) to control for unobserved, time-invariant country characteristics.\n\n### Data / Model Specification\nThe model is specified as:\n\n  \n\\text{Tobin's Q}_{it} = \\alpha + \\beta_{1}\\mathrm{WEDGE}_{it} + \\beta_{2}\\mathrm{PUBLIC}_{c} + \\beta_{3}(\\mathrm{WEDGE}_{it} * \\mathrm{PUBLIC}_{c}) + \\dots + \\delta_c + \\varepsilon_{it} \\quad \\text{(Eq. 1)}\n \n\nWhere `i` is firm, `c` is country, and `t` is year. The `PUBLIC_c` index is time-invariant within the sample period. In a standard 'within-estimator' fixed-effects model, the coefficient `β₂` on the time-invariant `PUBLIC_c` variable would be unidentified due to perfect collinearity with the country fixed effects `δ_c`.\n\n---\n\nGiven this specification, which of the following statements about the identification of the model's coefficients are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this fixed effects model specification, I need to evaluate each statement about coefficient identification.\n\n**Statement A**: The coefficient β₂ is unidentified due to perfect collinearity between PUBLIC_c and country fixed effects δ_c.\n\nThis is **correct**. Since PUBLIC_c is time-invariant within countries and the model includes country fixed effects, there is perfect collinearity. The country fixed effects absorb all time-invariant country-level variation, making it impossible to separately identify β₂.\n\n**Statement B**: The coefficient β₁ is identified primarily from the variation of WEDGE over time for each individual firm.\n\nThis is **correct**. In a fixed effects model with country fixed effects, the within-estimator uses deviations from country means. Since WEDGE_it varies over time within firms (and thus within countries), β₁ is identified from this within-country temporal variation.\n\n**Statement C**: The coefficient β₃ is identified primarily from the variation of WEDGE_it across firms within the same country.\n\nThis is **incorrect**. The interaction term WEDGE_it × PUBLIC_c varies both across firms within countries and over time. Since PUBLIC_c differs across countries, the interaction term's identification comes from how the relationship between WEDGE and Tobin's Q differs across countries with different PUBLIC values. It's not primarily about within-country cross-firm variation.\n\n**Statement D**: The main effect β₂ is identified by assuming a linear effect and using cross-country variation after omitting one country's fixed effect.\n\nThis is **incorrect**. Even if one country fixed effect is omitted as a baseline (which is standard practice to avoid the dummy variable trap), β₂ remains unidentified because PUBLIC_c is still perfectly collinear with the remaining country fixed effects. The time-invariant nature of PUBLIC_c means it cannot be separated from country fixed effects regardless of normalization choices.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 220, "Question": "### Background\n\nThe paper argues that the goal of optimal international policy cooperation is often misunderstood. It is not necessarily to eliminate or minimize spillovers. In fact, under certain conditions, a cooperative arrangement would intentionally propagate shocks across borders to achieve a better global outcome.\n\n### Data / Model Specification\n\nThe analysis distinguishes between different types of shocks and the objectives of a global planner.\n\n*   **Risk-Sharing Motive:** In response to a negative **distortionary shock** (e.g., a domestic financial crisis) in one country, a key goal of cooperative policy is to act as a substitute for missing insurance markets. This involves spreading the negative welfare consequences across all countries.\n*   **Efficient Allocation Motive:** In response to a negative **productivity shock** in one country, global efficiency requires that production should fall primarily in that country, as it has become the less efficient place to produce.\n\n### Question\n\nConsider a scenario where the Home country is hit by a severe, negative idiosyncratic shock. According to the paper's principles, which of the following statements accurately describe the optimal cooperative policy response and its effect on spillovers to the Foreign country?\n", "Options": {"A": "If the shock is a negative productivity shock, the risk-sharing motive is irrelevant, and the cooperative policy should aim to fully insulate the Foreign country's consumption from the shock.", "B": "If the shock is a negative productivity shock, the efficient allocation motive implies that the cooperative policy should aim to contain the decline in production primarily within the Home country.", "C": "If the shock is a financial crisis (a distortionary shock), the risk-sharing motive dictates that the cooperative policy should aim to spread the consumption loss to the Foreign country.", "D": "If the shock is a financial crisis (a distortionary shock), the optimal cooperative policy will likely result in a smaller negative spillover to the Foreign country compared to a non-cooperative scenario where the Foreign country actively insulates itself."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the candidate's understanding of the paper's nuanced argument about the role of spillovers in optimal policy cooperation. It uses a Scenario Application strategy, asking the user to apply the paper's principles to specific shock types.\n- **Correct Option A:** This is a direct application of the risk-sharing motive for distortionary shocks. The goal is to share the welfare pain, which means spreading the consumption loss.\n- **Correct Option B:** This is a direct application of the efficient allocation motive for productivity shocks. It is globally efficient for the less productive country to produce less.\n- **Distractor C:** This is a Conceptual Opposite and a key point of the paper. To achieve risk-sharing, the cooperative policy must *transmit* the shock, likely leading to a *larger* negative spillover than a non-cooperative, self-insulating policy.\n- **Distractor D:** This is an Almost Right distractor. While the efficient allocation motive is primary for production, the risk-sharing motive still applies to consumption. Optimal policy would still aim to share some of the consumption risk, not fully insulate the Foreign country.", "qid": "220", "question": "### Background\n\nThe paper argues that the goal of optimal international policy cooperation is often misunderstood. It is not necessarily to eliminate or minimize spillovers. In fact, under certain conditions, a cooperative arrangement would intentionally propagate shocks across borders to achieve a better global outcome.\n\n### Data / Model Specification\n\nThe analysis distinguishes between different types of shocks and the objectives of a global planner.\n\n*   **Risk-Sharing Motive:** In response to a negative **distortionary shock** (e.g., a domestic financial crisis) in one country, a key goal of cooperative policy is to act as a substitute for missing insurance markets. This involves spreading the negative welfare consequences across all countries.\n*   **Efficient Allocation Motive:** In response to a negative **productivity shock** in one country, global efficiency requires that production should fall primarily in that country, as it has become the less efficient place to produce.\n\n### Question\n\nConsider a scenario where the Home country is hit by a severe, negative idiosyncratic shock. According to the paper's principles, which of the following statements accurately describe the optimal cooperative policy response and its effect on spillovers to the Foreign country?\n", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the paper's principles about optimal international policy cooperation.\n\nLet me analyze each option:\n\n**Option A**: \"If the shock is a negative productivity shock, the risk-sharing motive is irrelevant, and the cooperative policy should aim to fully insulate the Foreign country's consumption from the shock.\"\n\nThis is incorrect. The paper explicitly states that for distortionary shocks, the risk-sharing motive involves spreading negative welfare consequences across all countries. While the efficient allocation motive is primary for productivity shocks, this doesn't mean risk-sharing is completely irrelevant or that Foreign consumption should be fully insulated.\n\n**Option B**: \"If the shock is a negative productivity shock, the efficient allocation motive implies that the cooperative policy should aim to contain the decline in production primarily within the Home country.\"\n\nThis is correct. The paper clearly states: \"In response to a negative productivity shock in one country, global efficiency requires that production should fall primarily in that country, as it has become the less efficient place to produce.\"\n\n**Option C**: \"If the shock is a financial crisis (a distortionary shock), the risk-sharing motive dictates that the cooperative policy should aim to spread the consumption loss to the Foreign country.\"\n\nThis is correct. The paper states that for distortionary shocks like a financial crisis, \"a key goal of cooperative policy is to act as a substitute for missing insurance markets. This involves spreading the negative welfare consequences across all countries.\"\n\n**Option D**: \"If the shock is a financial crisis (a distortionary shock), the optimal cooperative policy will likely result in a smaller negative spillover to the Foreign country compared to a non-cooperative scenario where the Foreign country actively insulates itself.\"\n\nThis is incorrect. Under cooperation with risk-sharing motives, spillovers would be intentionally propagated to spread losses. In a non-cooperative scenario where Foreign actively insulates itself, spillovers would be minimized. Therefore, cooperative policy would result in larger, not smaller, negative spillovers.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 406, "Question": "### Background\n\n**Research Question.** Beyond empirical costs, the optimal design of sovereign debt issuance requires navigating theoretical risks and institutional constraints. The choice between a sealed-bid auction and a fixed-price offering is state-dependent, with each mechanism having advantages in different market environments.\n\n**Setting / Data-Generating Environment.** The Treasury has raised several institutional objections to extending auctions to long-term securities, including: (i) vulnerability in weak markets due to investor uncertainty, (ii) the potential to encourage undesirable speculation, and (iii) tax complications that could harm market liquidity.\n\n### Data / Model Specification\n\nUnder the OID (Original Issue Discount) rule, if a bond is issued at a price `P_i` below a certain threshold, a secondary buyer's future tax liability depends on `P_i`. The taxable ordinary income is calculated pro-rata:\n\n  \n\\text{Taxable Ordinary Income} = \\frac{m}{M}(100-P_i) \n\\quad \\text{(Eq. (1))}\n \n\nwhere `M` is the total months to maturity and `m` is the holding period of the secondary buyer.\n\n### Question\n\nAccording to the paper's analysis of institutional arguments, which of the following are valid concerns or counter-arguments regarding the use of sealed-bid auctions for long-term securities? Select all that apply.", "Options": {"A": "The author argues that auctions can curtail speculation by eliminating the time lag, forcing competitors to bid away potential profits and transfer them to the Treasury.", "B": "The Original Issue Discount (OID) tax rule, when applied to a multi-price auction, would create non-fungible securities with different tax implications for secondary buyers, harming market liquidity.", "C": "Auctions may increase speculation because the time lag between the price announcement and the sale creates arbitrage opportunities for non-professional investors.", "D": "In weak markets with high uncertainty, the winner's curse is amplified, potentially leading rational bidders to bid timidly or abstain, thus raising Treasury's costs."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the understanding of three key institutional arguments discussed in the paper. It uses a 'select-all-that-apply' format to test comprehension of multiple distinct concepts from the text. Distractor B is a high-fidelity 'Conceptual Opposite,' as it correctly describes a mechanism for speculation but attributes it to the wrong issuance type (auctions instead of fixed-price offerings).", "qid": "406", "question": "### Background\n\n**Research Question.** Beyond empirical costs, the optimal design of sovereign debt issuance requires navigating theoretical risks and institutional constraints. The choice between a sealed-bid auction and a fixed-price offering is state-dependent, with each mechanism having advantages in different market environments.\n\n**Setting / Data-Generating Environment.** The Treasury has raised several institutional objections to extending auctions to long-term securities, including: (i) vulnerability in weak markets due to investor uncertainty, (ii) the potential to encourage undesirable speculation, and (iii) tax complications that could harm market liquidity.\n\n### Data / Model Specification\n\nUnder the OID (Original Issue Discount) rule, if a bond is issued at a price `P_i` below a certain threshold, a secondary buyer's future tax liability depends on `P_i`. The taxable ordinary income is calculated pro-rata:\n\n  \n\\text{Taxable Ordinary Income} = \\frac{m}{M}(100-P_i) \n\\quad \\text{(Eq. (1))}\n \n\nwhere `M` is the total months to maturity and `m` is the holding period of the secondary buyer.\n\n### Question\n\nAccording to the paper's analysis of institutional arguments, which of the following are valid concerns or counter-arguments regarding the use of sealed-bid auctions for long-term securities? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question about sovereign debt issuance mechanisms and their institutional considerations, I need to evaluate each option against the background information provided.\n\nThe background indicates that the Treasury has raised concerns about extending auctions to long-term securities, including: (1) vulnerability in weak markets due to investor uncertainty, (2) potential to encourage undesirable speculation, and (3) tax complications that could harm market liquidity.\n\nLet me analyze each option:\n\n**Option A**: This presents auctions as a solution to speculation by eliminating time lag and forcing competitive bidding that transfers profits to the Treasury. This appears to be a counter-argument defending auctions against the speculation concern.\n\n**Option B**: This directly relates to concern (3) about tax complications. The OID rule formula shows that different issue prices (P_i) in a multi-price auction would create different tax liabilities for secondary buyers, making securities non-fungible and harming liquidity.\n\n**Option C**: This contradicts the logic in Option A. If auctions eliminate time lag (as stated in A), they cannot simultaneously create arbitrage opportunities due to time lag. This appears incorrect.\n\n**Option D**: This directly addresses concern (1) about vulnerability in weak markets. The winner's curse phenomenon would indeed be amplified when uncertainty is high, causing rational bidders to bid conservatively or not participate.\n\nBased on this analysis, options A, B, and D all present valid concerns or counter-arguments from the paper's institutional analysis.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 395, "Question": "### Background\n\nThe analysis of Thiele's PDE system for a surplus-linked insurance contract is facilitated by converting it from a terminal-value problem into an initial-value problem. This requires a change of variables for both time (`t`) and the surplus value (`x`).\n\n### Data / Model Specification\n\nThe original Thiele's PDE for a single state `j` includes the following terms related to the evolution of the surplus:\n  \n\\partial_{t}V^{j} + \\frac{1}{2}\\pi^{2}\\sigma^{2}x^{2}\\partial_{x}^{2}V^j + \\left(r x+c^{j}-\\delta^{j}\\right)\\partial_{x}V^j\n \nThis is transformed using the change of variables `\\tau = T-t` and `y = \\log x`. The transformed system is an initial-value problem:\n  \n\\partial_{\\tau}{\\bf V} = \\left({\\mathcal{A}}(\\tau)+{\\bf T}-r\\right){\\bf V}+e^{r\\tau}\\beta, \\quad {\\bf V}(0)=0\n \nThe `j`-th diagonal element of the operator `\\mathcal{A}` is given by:\n  \n\\mathcal{A}^{j}=\\frac{1}{2}\\pi^{2}\\sigma^{2}\\partial_{y}^{2}+\\left(r+\\left(c^{j}-\\delta^{j}\\right)e^{-y}-\\frac{1}{2}\\pi^{2}\\sigma^{2}\\right)\\partial_{y} \\quad \\text{(Eq. 1)}\n \n\n---\n\nBased on the provided transformation, which of the following statements about the operators and their financial interpretation are correct?\n", "Options": {"A": "The off-diagonal operator `\\mathbf{T}` governs the evolution of the reserve *between* different insurance states, capturing insurance risks like mortality and morbidity.", "B": "The diagonal operator `\\mathcal{A}` governs the evolution of the reserve *within* a given insurance state, capturing the financial risk from the surplus's diffusion and drift.", "C": "The transformation `y = \\log x` converts the `x^2 \\partial_x^2` term into a term proportional to `\\partial_y^2 - \\partial_y`, which is a key step in deriving Eq. (1).", "D": "If contributions and dividends upon transition are zero (`c^{jk} - \\delta^{jk} \\equiv 0`), the operators `\\mathcal{A}` and `\\mathbf{T}` commute because `\\mathbf{T}` becomes independent of the spatial variable `y`."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the student's understanding of the core mathematical transformation in the paper and their ability to connect the structure of the resulting operators (`\\mathcal{A}`, `\\mathbf{T}`) to their distinct financial roles (financial vs. insurance risk). It also tests a key special case (commutativity). Strategy: This is a direct application of the paper's central model setup. All four options are correct statements derived from the original QA problem, requiring a comprehensive understanding of the derivation and interpretation. The item is designed to have multiple correct answers to ensure a thorough check of knowledge.", "qid": "395", "question": "### Background\n\nThe analysis of Thiele's PDE system for a surplus-linked insurance contract is facilitated by converting it from a terminal-value problem into an initial-value problem. This requires a change of variables for both time (`t`) and the surplus value (`x`).\n\n### Data / Model Specification\n\nThe original Thiele's PDE for a single state `j` includes the following terms related to the evolution of the surplus:\n  \n\\partial_{t}V^{j} + \\frac{1}{2}\\pi^{2}\\sigma^{2}x^{2}\\partial_{x}^{2}V^j + \\left(r x+c^{j}-\\delta^{j}\\right)\\partial_{x}V^j\n \nThis is transformed using the change of variables `\\tau = T-t` and `y = \\log x`. The transformed system is an initial-value problem:\n  \n\\partial_{\\tau}{\\bf V} = \\left({\\mathcal{A}}(\\tau)+{\\bf T}-r\\right){\\bf V}+e^{r\\tau}\\beta, \\quad {\\bf V}(0)=0\n \nThe `j`-th diagonal element of the operator `\\mathcal{A}` is given by:\n  \n\\mathcal{A}^{j}=\\frac{1}{2}\\pi^{2}\\sigma^{2}\\partial_{y}^{2}+\\left(r+\\left(c^{j}-\\delta^{j}\\right)e^{-y}-\\frac{1}{2}\\pi^{2}\\sigma^{2}\\right)\\partial_{y} \\quad \\text{(Eq. 1)}\n \n\n---\n\nBased on the provided transformation, which of the following statements about the operators and their financial interpretation are correct?\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each statement about the transformed Thiele's PDE system.\n\n**Statement A**: The off-diagonal operator **T** governs evolution between different insurance states.\n- In the transformed equation, we have operators **A** (diagonal) and **T**\n- The original Thiele's PDE for insurance contracts includes transition rates between states (e.g., healthy to disabled, alive to dead)\n- These transitions are captured by off-diagonal elements in the system\n- This is correct ✓\n\n**Statement B**: The diagonal operator A governs evolution within a given insurance state.\n- From Eq. (1), A^j contains terms with ∂_y^2 and ∂_y\n- These terms come from the financial diffusion (π²σ²x²∂_x²) and drift (rx + c^j - δ^j)∂_x\n- This represents the financial risk from surplus evolution within state j\n- This is correct ✓\n\n**Statement C**: The transformation y = log x converts x²∂_x² into a term proportional to ∂_y² - ∂_y.\nLet me verify:\n- If y = log x, then x = e^y\n- ∂_x = ∂_y/∂_x × ∂_y = e^(-y)∂_y\n- ∂_x² = ∂_x(e^(-y)∂_y) = e^(-y)∂_y(e^(-y)∂_y) = e^(-2y)(∂_y² - ∂_y)\n- Therefore: x²∂_x² = e^(2y) × e^(-2y)(∂_y² - ∂_y) = ∂_y² - ∂_y\n- This is correct ✓\n\n**Statement D**: If c^(jk) - δ^(jk) ≡ 0, then A and T commute.\n- The statement mentions \"contributions and dividends upon transition\"\n- If these are zero, T would only contain transition rates (no y-dependence)\n- A contains y", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 292, "Question": "### Background\n\n**Research Question.** How can the classical exponential bound on aggregate claims be refined for different classes of claim severity distributions, particularly those exhibiting opposing 'aging' characteristics?\n\n**Setting / Data-Generating Environment.** We are in the classical collective risk model. The goal is to refine the standard Lundberg exponential bound by incorporating information about the shape of the claim size distribution `F(x)`.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `\\overline{F}(x)`: The survival function of individual claim sizes.\n- `p_0, \\phi`: Parameters of the claim frequency distribution.\n- `\\kappa`: The adjustment coefficient, `\\kappa > 0`.\n- `r_F(x)`: The mean residual lifetime of `F`, `E[Y-x | Y>x]`.\n\n---\n\n### Data / Model Specification\n\nIf a claim size distribution `F(x)` is **DMRL** (Decreasing Mean Residual Lifetime), a refined bound (Theorem 6) is available:\n  \n\\overline{G}(x) \\leq \\phi^{-1}(1-p_{0}) \\{1+\\kappa\\alpha(x)\\}^{-1} \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (1))}\n \nwhere `\\alpha(x) = (\\int_{x}^{\\infty} e^{\\kappa y} \\overline{F}(y) dy) / (e^{\\kappa x} \\overline{F}(x))`.\n\n---\n\n### Question\n\nConsider a uniform claim severity distribution on `[0, x_0]`, where `\\overline{F}(x) = 1 - x/x_0` for `x \\in [0, x_0]`. Based on an analysis of this distribution, select all of the following statements that are correct.", "Options": {"A": "The improvement factor over the classical Lundberg bound, `\\{1+\\kappa\\alpha(x)\\}^{-1}`, simplifies to `\\frac{e^{\\kappa(x_0-x)} - 1}{\\kappa(x_0-x)}`.", "B": "The mean residual lifetime function is `r_F(x) = (x_0 - x) / 2`, which confirms that the uniform distribution is DMRL.", "C": "The mean residual lifetime function is `r_F(x) = (x_0 - x)^2 / (2x_0)`, which confirms that the uniform distribution is DMRL.", "D": "The improvement factor over the classical Lundberg bound, `\\{1+\\kappa\\alpha(x)\\}^{-1}`, simplifies to `\\frac{\\kappa(x_0-x)}{e^{\\kappa(x_0-x)} - 1}`."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the ability to apply the paper's theoretical results to a concrete example, using a Computational Judgment strategy. It requires performing the multi-step calculation from the original QA problem. Correct options (A, B) represent the correct intermediate result for the mean residual lifetime and the correct final result for the improvement factor. Distractor (C) presents an incorrect formula for `r_F(x)` based on a common calculation error (forgetting to divide by `\\overline{F}(x)`), a Step-Omission Error. Distractor (D) presents the reciprocal of the correct improvement factor, a common algebraic slip (Formula Misuse).", "qid": "292", "question": "### Background\n\n**Research Question.** How can the classical exponential bound on aggregate claims be refined for different classes of claim severity distributions, particularly those exhibiting opposing 'aging' characteristics?\n\n**Setting / Data-Generating Environment.** We are in the classical collective risk model. The goal is to refine the standard Lundberg exponential bound by incorporating information about the shape of the claim size distribution `F(x)`.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `\\overline{F}(x)`: The survival function of individual claim sizes.\n- `p_0, \\phi`: Parameters of the claim frequency distribution.\n- `\\kappa`: The adjustment coefficient, `\\kappa > 0`.\n- `r_F(x)`: The mean residual lifetime of `F`, `E[Y-x | Y>x]`.\n\n---\n\n### Data / Model Specification\n\nIf a claim size distribution `F(x)` is **DMRL** (Decreasing Mean Residual Lifetime), a refined bound (Theorem 6) is available:\n  \n\\overline{G}(x) \\leq \\phi^{-1}(1-p_{0}) \\{1+\\kappa\\alpha(x)\\}^{-1} \\mathrm{e}^{-\\kappa x}, \\quad x \\geq 0 \\quad \\text{(Eq. (1))}\n \nwhere `\\alpha(x) = (\\int_{x}^{\\infty} e^{\\kappa y} \\overline{F}(y) dy) / (e^{\\kappa x} \\overline{F}(x))`.\n\n---\n\n### Question\n\nConsider a uniform claim severity distribution on `[0, x_0]`, where `\\overline{F}(x) = 1 - x/x_0` for `x \\in [0, x_0]`. Based on an analysis of this distribution, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "I need to analyze the uniform distribution on [0, x₀] and verify the statements about its mean residual lifetime function and the improvement factor.\n\nGiven information:\n- Uniform distribution on [0, x₀]\n- Survival function: $\\overline{F}(x) = 1 - x/x_0$ for $x \\in [0, x_0]$\n- Density function: $f(x) = 1/x_0$ for $x \\in [0, x_0]$\n\nFirst, let me calculate the mean residual lifetime function:\n$$r_F(x) = E[Y-x | Y>x] = \\frac{\\int_x^{x_0} (y-x)f(y)dy}{\\overline{F}(x)}$$\n\nFor the uniform distribution:\n$$r_F(x) = \\frac{\\int_x^{x_0} (y-x) \\cdot \\frac{1}{x_0} dy}{1 - x/x_0}$$\n\n$$= \\frac{\\frac{1}{x_0} \\int_x^{x_0} (y-x) dy}{(x_0-x)/x_0}$$\n\n$$= \\frac{1}{x_0-x} \\int_x^{x_0} (y-x) dy$$\n\n$$= \\frac{1}{x_0-x} \\left[\\frac{(y-x)^2}{2}\\right]_x^{x_0}$$\n\n$$= \\frac{1}{x_0-x} \\cdot \\frac{(x_0-x)^2}{2}$$\n\n$$= \\frac{x_0-x}{2}$$\n\nThis confirms that option B is correct and option C is incorrect.\n\nSince $r_F(x) = (x_0-x)/2$ is decreasing in $x$, the uniform distribution is indeed DMRL.\n\nNow, let me calculate $\\alpha(x)$ for the improvement factor:\n$$\\alpha(x) = \\frac{\\int_{x}^{\\infty} e^{\\kappa y} \\", "model_answer": ["B", "C"], "error": null}}
{"ID": 322, "Question": "### Background\n\nA study on board governance finds that a higher ratio of women directors significantly improves a board's effectiveness in `Strategic Control` (monitoring long-term, qualitative issues like brand reputation, CSR, and innovation) but has no statistically significant effect on `Board Operational Control` (monitoring short-term, quantitative issues like budgets and cost management).\n\n---\n\n### Question\n\nConsider two firms: Firm A, a high-end consumer technology company whose value depends heavily on long-term innovation and brand image, and Firm B, a regulated utility whose value depends primarily on efficient capital allocation and cost control. Based on the study's findings, select all likely consequences of increasing the ratio of women directors on each firm's board.", "Options": {"A": "In Firm B, where operational control is the primary driver of value, the effect of a higher ratio of women directors on overall firm performance is likely to be statistically insignificant.", "B": "In Firm B, a higher ratio of women directors would likely harm firm performance by shifting the board's focus away from critical operational details.", "C": "In Firm A, where strategic control is a key driver of value, a higher ratio of women directors is likely to translate into a measurable improvement in overall firm performance.", "D": "The impact of increasing the ratio of women directors on firm performance would be equally positive for both Firm A and Firm B, as good governance practices are universally beneficial."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to apply a core theoretical finding to novel business scenarios, a key 'Scenario Application' skill. It tests whether the student can connect the specific type of board effectiveness improved by gender diversity (strategic control) to the business model where that effectiveness is most critical. Options A and B represent the two correct applications of this logic. Distractor C introduces a common error of interpreting a null effect as a negative one (Sign Error). Distractor D directly contradicts the paper's central premise of task-dependency (Conceptual Opposite).", "qid": "322", "question": "### Background\n\nA study on board governance finds that a higher ratio of women directors significantly improves a board's effectiveness in `Strategic Control` (monitoring long-term, qualitative issues like brand reputation, CSR, and innovation) but has no statistically significant effect on `Board Operational Control` (monitoring short-term, quantitative issues like budgets and cost management).\n\n---\n\n### Question\n\nConsider two firms: Firm A, a high-end consumer technology company whose value depends heavily on long-term innovation and brand image, and Firm B, a regulated utility whose value depends primarily on efficient capital allocation and cost control. Based on the study's findings, select all likely consequences of increasing the ratio of women directors on each firm's board.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze which consequences are likely based on the study's findings.\n\nThe study shows:\n- Higher ratio of women directors → Improved Strategic Control (long-term, qualitative issues)\n- Higher ratio of women directors → No significant effect on Operational Control (short-term, quantitative issues)\n\nFor Firm A (high-end tech company):\n- Value depends on innovation and brand image (strategic control matters)\n- Higher ratio of women directors would improve strategic control\n- This should translate to improved firm performance\n\nFor Firm B (regulated utility):\n- Value depends on capital allocation and cost control (operational control matters)\n- Higher ratio of women directors has no significant effect on operational control\n- Therefore, no significant effect on firm performance expected\n\nAnalyzing each option:\n\nA) Correct - Since Firm B's value depends on operational control, and the study shows no significant effect on operational control from more women directors, the overall effect on firm performance would likely be insignificant.\n\nB) Incorrect - The study doesn't indicate that more women directors would harm operational control or shift focus away from it; it simply shows no significant effect.\n\nC) Correct - Since Firm A's value depends on strategic control, and the study shows improved strategic control from more women directors, this should translate to improved firm performance.\n\nD) Incorrect - This contradicts the study's findings which show differential effects on strategic vs operational control, suggesting the impact would vary by firm type.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 315, "Question": "### Background\n\n**Research Question.** How can a Central Clearing Party (CCP) identify the specific sources of \"crowded risk\" in real time, and what do these sources reveal about the nature of systemic risk?\n\n**Setting.** An empirical analysis of a European equity CCP focuses on member behavior during high-stress periods. The study uses a theoretical tool, the elasticity of CCP exposure (`e_{σ^f}^{ExpCCP}`), to measure how sensitive the CCP's total risk is to changes in the volatility of specific risk factors.\n\n**Variables and Parameters.**\n- `ExpCCP`: The CCP's total exposure to aggregate member loss.\n- `e_{σ^f}^{ExpCCP}`: The elasticity of `ExpCCP` with respect to the volatility of a candidate risk factor `f`.\n- **Candidate risk factors:** Market (STOXXNordic30), Nokia stock.\n- **Key Dates:** \"Nokia day\" (Apr 26, 2010), driven by a firm-specific earnings shock; \"Bailout day\" (May 10, 2010), driven by a macroeconomic announcement.\n\n---\n\n### Data / Model Specification\n\n**Table 1: CCP Exposure Elasticity to Various Risk Factors**\n\nThis table reports the elasticity of total CCP exposure (`ExpCCP`) with respect to a 1% change in the daily volatility of three candidate risk factors on three different days.\n\n| | Date | Risk factor | Elasticity |\n| :--- | :--- | :--- | :--- |\n| Median-CrowdIx day | Jul 29, 2010 | Market | 0.91 |\n| | | Nokia | 0.15 |\n| Bailout program | May 10, 2010 | Market | 0.98 |\n| | | Nokia | 0.14 |\n| Nokia reports Q1 | Apr 26, 2010 | Market | 0.19 |\n| | | Nokia | 1.05 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, select all statements that are valid conclusions supported by the paper's analysis.", "Options": {"A": "On a typical day (Median-CrowdIx day), the market is the dominant risk factor, which proves that idiosyncratic risks are generally not systemic.", "B": "On the \"Bailout day\" (May 10), the Nokia stock was a more significant source of systemic risk than the overall market.", "C": "The findings demonstrate that a purely firm-specific (idiosyncratic) event can become a source of systemic risk for the CCP if members' positions are crowded on that single factor.", "D": "On the \"Nokia day\" (Apr 26), the Nokia stock was the primary crowded risk factor, as indicated by its elasticity of 1.05."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret elasticity data to identify crowded risk factors and draw correct high-level conclusions. It uses the 'Atomic Decomposition' strategy, breaking down the original QA question's first part into independently verifiable statements. Options A and B are correct statements directly derivable from the table and the paper's core argument. Distractor C is a 'Conceptual Opposite' error, reversing the finding for the Bailout day. Distractor D is a 'Misleading Generalization,' using a correct premise (market dominance on a typical day) to draw a conclusion that is explicitly contradicted by the Nokia day evidence.", "qid": "315", "question": "### Background\n\n**Research Question.** How can a Central Clearing Party (CCP) identify the specific sources of \"crowded risk\" in real time, and what do these sources reveal about the nature of systemic risk?\n\n**Setting.** An empirical analysis of a European equity CCP focuses on member behavior during high-stress periods. The study uses a theoretical tool, the elasticity of CCP exposure (`e_{σ^f}^{ExpCCP}`), to measure how sensitive the CCP's total risk is to changes in the volatility of specific risk factors.\n\n**Variables and Parameters.**\n- `ExpCCP`: The CCP's total exposure to aggregate member loss.\n- `e_{σ^f}^{ExpCCP}`: The elasticity of `ExpCCP` with respect to the volatility of a candidate risk factor `f`.\n- **Candidate risk factors:** Market (STOXXNordic30), Nokia stock.\n- **Key Dates:** \"Nokia day\" (Apr 26, 2010), driven by a firm-specific earnings shock; \"Bailout day\" (May 10, 2010), driven by a macroeconomic announcement.\n\n---\n\n### Data / Model Specification\n\n**Table 1: CCP Exposure Elasticity to Various Risk Factors**\n\nThis table reports the elasticity of total CCP exposure (`ExpCCP`) with respect to a 1% change in the daily volatility of three candidate risk factors on three different days.\n\n| | Date | Risk factor | Elasticity |\n| :--- | :--- | :--- | :--- |\n| Median-CrowdIx day | Jul 29, 2010 | Market | 0.91 |\n| | | Nokia | 0.15 |\n| Bailout program | May 10, 2010 | Market | 0.98 |\n| | | Nokia | 0.14 |\n| Nokia reports Q1 | Apr 26, 2010 | Market | 0.19 |\n| | | Nokia | 1.05 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, select all statements that are valid conclusions supported by the paper's analysis.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to evaluate each statement against the elasticity values provided.\n\nThe elasticity values show how sensitive the CCP's total exposure is to changes in volatility of different risk factors. A higher elasticity means that risk factor is a more significant source of systemic risk.\n\nLet me analyze each option:\n\nA) On the Median-CrowdIx day, market elasticity is 0.91 vs Nokia's 0.15, showing market dominance. However, this doesn't \"prove that idiosyncratic risks are generally not systemic\" - the Nokia day data contradicts this general claim.\n\nB) On Bailout day, market elasticity is 0.98 vs Nokia's 0.14. The market was clearly more significant, not Nokia. This statement is false.\n\nC) This is supported by the data. On Nokia day, an earnings announcement (firm-specific event) resulted in Nokia having the highest elasticity (1.05), demonstrating that idiosyncratic events can become systemic risks when positions are crowded.\n\nD) On Nokia day, Nokia's elasticity of 1.05 is much higher than the market's 0.19, making Nokia the primary crowded risk factor. This is supported by the data.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 379, "Question": "### Background\n\n**Research Question.** How does the scale and structure of foreign bank presence in a domestic financial system create risks of macroprudential policy “leakage,” thereby motivating the need for international policy reciprocity?\n\n**Setting.** The analysis focuses on European Union countries, where the distinction between foreign bank subsidiaries and branches is critical for understanding regulatory boundaries. Subsidiaries are subject to host country regulation, while branches, in the absence of reciprocity, are typically subject to home country regulation.\n\n**Variables & Parameters.**\n- **Foreign Subsidiaries' Assets/GDP**: The ratio of assets held by foreign-owned, locally incorporated banking subsidiaries to the host country's Gross Domestic Product.\n- **Foreign Branches' Assets/GDP**: The ratio of assets held by branches of foreign banks to the host country's GDP.\n- **Leakage**: The phenomenon where the intended effect of a domestic credit-tightening policy is offset by an increase in lending from less-regulated entities, particularly foreign bank branches.\n\n---\n\n### Data / Model Specification\n\nThe following table provides data on the presence of foreign banks in selected EU countries.\n\n**Table 1: Foreign Bank Presence in Selected EU Countries**\n\n| Country | Share of Foreign Loans (2014) | Foreign Subsidiaries Assets/GDP (2013) | Foreign Branches Assets/GDP (2013) |\n| :--- | :--- | :--- | :--- |\n| IE | 52% | 50% and above | 30% and above |\n| ES | 10% | 0-10% | 10-20% |\n| GB | 44% | 10-25% | 30% and above |\n| FR | 24% | 10-25% | 0-10% |\n| BE | 48% | 50% and above | 30% and above |\n| LU | 80% | 50% and above | 30% and above |\n\n---\n\n### Question\n\nBased on the provided data and regulatory context, which of the following conclusions about macroprudential policy leakage are supported?", "Options": {"A": "According to the table, Spain (ES) and France (FR) face the most severe risk of leakage because their foreign loan shares are the highest among the countries listed.", "B": "Policy leakage is primarily a concern in countries with a large presence of foreign subsidiaries, as these entities are not bound by host-country regulations.", "C": "In countries like the UK (GB) and Ireland (IE), with a large presence of foreign bank branches (assets > 30% of GDP), a unilateral tightening of host-country regulation on LTVs is highly susceptible to leakage.", "D": "The primary mechanism for leakage is that foreign bank branches are typically subject to home-country, not host-country, regulation, allowing them to expand lending when host-country rules tighten for other banks."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to connect empirical data on banking structure with the core theoretical concept of policy leakage. It uses a 'premise packaging' strategy, combining a data-driven conclusion (Option A) with a statement of the underlying mechanism (Option B). Distractors are a 'Conceptual Opposite' (confusing branches with subsidiaries) and a 'Factual Error' (misreading the table) to ensure sharp discrimination.", "qid": "379", "question": "### Background\n\n**Research Question.** How does the scale and structure of foreign bank presence in a domestic financial system create risks of macroprudential policy “leakage,” thereby motivating the need for international policy reciprocity?\n\n**Setting.** The analysis focuses on European Union countries, where the distinction between foreign bank subsidiaries and branches is critical for understanding regulatory boundaries. Subsidiaries are subject to host country regulation, while branches, in the absence of reciprocity, are typically subject to home country regulation.\n\n**Variables & Parameters.**\n- **Foreign Subsidiaries' Assets/GDP**: The ratio of assets held by foreign-owned, locally incorporated banking subsidiaries to the host country's Gross Domestic Product.\n- **Foreign Branches' Assets/GDP**: The ratio of assets held by branches of foreign banks to the host country's GDP.\n- **Leakage**: The phenomenon where the intended effect of a domestic credit-tightening policy is offset by an increase in lending from less-regulated entities, particularly foreign bank branches.\n\n---\n\n### Data / Model Specification\n\nThe following table provides data on the presence of foreign banks in selected EU countries.\n\n**Table 1: Foreign Bank Presence in Selected EU Countries**\n\n| Country | Share of Foreign Loans (2014) | Foreign Subsidiaries Assets/GDP (2013) | Foreign Branches Assets/GDP (2013) |\n| :--- | :--- | :--- | :--- |\n| IE | 52% | 50% and above | 30% and above |\n| ES | 10% | 0-10% | 10-20% |\n| GB | 44% | 10-25% | 30% and above |\n| FR | 24% | 10-25% | 0-10% |\n| BE | 48% | 50% and above | 30% and above |\n| LU | 80% | 50% and above | 30% and above |\n\n---\n\n### Question\n\nBased on the provided data and regulatory context, which of the following conclusions about macroprudential policy leakage are supported?", "model_evaluation_result": {"model_raw_response": "Looking at the data and regulatory context:\n\n1. **Option A is incorrect**: Spain (10%) and France (24%) actually have among the lowest foreign loan shares in the table. Luxembourg (80%), Ireland (52%), and Belgium (48%) have much higher shares.\n\n2. **Option B is incorrect**: The background explicitly states that subsidiaries ARE subject to host country regulation (\"Subsidiaries are subject to host country regulation\"), so they cannot be the primary source of leakage.\n\n3. **Option C is correct**: Both UK and Ireland have foreign branches' assets > 30% of GDP. Since branches are subject to home country regulation (not host country), when the host country tightens LTV regulations, these branches can continue lending under their home country's rules, creating leakage.\n\n4. **Option D is correct**: This accurately describes the mechanism stated in the background - branches are \"typically subject to home country regulation,\" which allows them to bypass host country tightening and expand lending, creating the leakage effect.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 340, "Question": "### Background\n\n**Research Question.** How does the market's valuation of a firm's current operations (assets in place) and future opportunities (real options) depend on the interaction between the firm's profitability and its growth stage?\n\n**Setting.** A valuation model is estimated on distinct subsamples of firms, partitioned by profitability (profit vs. loss) and sales growth (low vs. high). The analysis focuses on the valuation coefficients of Net Income (`NI`), a proxy for assets in place, and R&D expenditures (`RD`), a proxy for real options. The persistence of abnormal earnings is also estimated as a foundational input for the valuation model and as a justification for the valuation patterns.\n\n**Variables and Parameters.**\n\n*   `P_{i,t}`: Price per share of firm `i` at the end of year `t`.\n*   `NI_{i,t}`: Net income per share, a proxy for the value of assets in place.\n*   `RD_{i,t}`: R&D expenditures per share, a proxy for investment in real options.\n*   `NI_{i,t}^a`: Abnormal earnings for firm `i` in period `t`.\n*   `\\omega_1`: The persistence parameter of abnormal earnings.\n*   `\\beta_3`, `\\beta_4`: Regression coefficients on `NI` and `RD`, respectively.\n\n---\n\n### Data / Model Specification\n\nThe following valuation model is estimated:\n\n  \nP_{i,t} = \\beta_0 + \\beta_1 BV_{i,t} + \\beta_2 DIV_{i,t} + \\beta_3 NI_{i,t} + \\beta_4 RD_{i,t} + \\beta_5 OTHER_{i,t} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nThe key hypotheses regarding profitability and growth are:\n*   **H2a:** For firms reporting a loss, the market places a lower weight on net income and positively values R&D expenditures.\n*   **H2b:** For firms reporting a loss, the market places a higher value on R&D expenditures when the firm is experiencing high sales growth.\n*   **H2c:** For firms reporting a profit, the market positively values R&D expenditures when the firm is experiencing high sales growth, but not when the firm is experiencing low sales growth.\n\nTable 1 below presents the estimated persistence parameters (`\\hat{\\omega}_1`) for various subsamples. Table 2 presents the key coefficient estimates (`\\hat{\\beta}_3` and `\\hat{\\beta}_4`) from estimating Eq. (1) on four subsamples.\n\n**Table 1: Abnormal Earnings Persistence Parameters (`\\hat{\\omega}_1`)**\n\n| | Entire sample | Low sales growth firms | High sales growth firms |\n| :--- | :---: | :---: | :---: |\n| **Entire sample** | 0.271 | 0.350 | 0.229 |\n| **Loss firms** | 0.133 | 0.207 | 0.081 |\n| **Profit firms** | 0.339 | 0.391 | 0.288 |\n\n*Note: Differences between high and low sales growth firms are statistically significant at p < 0.05.*\n\n**Table 2: Valuation Regression Results for `NI` and `RD`**\n\n| Variable | Subsample | Low Growth | High Growth |\n| :--- | :--- | :---: | :---: |\n| **NI** (`\\hat{\\beta}_3`) | Loss Firms | 0.440 | 0.371 |\n| | | (1.38) | (0.60) |\n| | Profit Firms | **3.608** | **1.252** |\n| | | (19.26) | (4.31) |\n| **RD** (`\\hat{\\beta}_4`) | Loss Firms | **2.100** | **4.081** |\n| | | (5.09) | (5.21) |\n| | Profit Firms | **-3.285** | **1.880** |\n| | | (10.48) | (4.23) |\n\n*Note: t-statistics are in parentheses. Bolded coefficients are significant at p < 0.01. Differences between low and high growth are statistically significant.*\n\n---\n\n### Question\n\nBased on the regression results in Table 2, which of the following statements accurately interpret the valuation of R&D expenditures (`RD`)?", "Options": {"A": "The positive valuation of `RD` for high-growth profitable firms is inconsistent with Hypothesis 2c.", "B": "For profitable firms, `RD` is valued positively regardless of the growth stage, but the valuation is higher in high-growth environments.", "C": "For loss-making firms, the market values `RD` more highly when they are in a high-growth stage compared to a low-growth stage.", "D": "The negative valuation of `RD` for low-growth profitable firms is consistent with the view that, in a stable environment, R&D may be seen as a risky diversion of cash from a successful core business."}, "Answer": ["C", "D"], "pi_justification": "This item replaces a portion of a multi-part Table QA problem, assessing the ability to interpret conditional regression coefficients in line with the paper's core hypotheses. It uses an **Atomic Decomposition** strategy, breaking down the complex findings on R&D valuation into distinct, verifiable statements. Distractors are designed to trap common misinterpretations, such as ignoring the sign change (B) or misreading the hypothesis (D).", "qid": "340", "question": "### Background\n\n**Research Question.** How does the market's valuation of a firm's current operations (assets in place) and future opportunities (real options) depend on the interaction between the firm's profitability and its growth stage?\n\n**Setting.** A valuation model is estimated on distinct subsamples of firms, partitioned by profitability (profit vs. loss) and sales growth (low vs. high). The analysis focuses on the valuation coefficients of Net Income (`NI`), a proxy for assets in place, and R&D expenditures (`RD`), a proxy for real options. The persistence of abnormal earnings is also estimated as a foundational input for the valuation model and as a justification for the valuation patterns.\n\n**Variables and Parameters.**\n\n*   `P_{i,t}`: Price per share of firm `i` at the end of year `t`.\n*   `NI_{i,t}`: Net income per share, a proxy for the value of assets in place.\n*   `RD_{i,t}`: R&D expenditures per share, a proxy for investment in real options.\n*   `NI_{i,t}^a`: Abnormal earnings for firm `i` in period `t`.\n*   `\\omega_1`: The persistence parameter of abnormal earnings.\n*   `\\beta_3`, `\\beta_4`: Regression coefficients on `NI` and `RD`, respectively.\n\n---\n\n### Data / Model Specification\n\nThe following valuation model is estimated:\n\n  \nP_{i,t} = \\beta_0 + \\beta_1 BV_{i,t} + \\beta_2 DIV_{i,t} + \\beta_3 NI_{i,t} + \\beta_4 RD_{i,t} + \\beta_5 OTHER_{i,t} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nThe key hypotheses regarding profitability and growth are:\n*   **H2a:** For firms reporting a loss, the market places a lower weight on net income and positively values R&D expenditures.\n*   **H2b:** For firms reporting a loss, the market places a higher value on R&D expenditures when the firm is experiencing high sales growth.\n*   **H2c:** For firms reporting a profit, the market positively values R&D expenditures when the firm is experiencing high sales growth, but not when the firm is experiencing low sales growth.\n\nTable 1 below presents the estimated persistence parameters (`\\hat{\\omega}_1`) for various subsamples. Table 2 presents the key coefficient estimates (`\\hat{\\beta}_3` and `\\hat{\\beta}_4`) from estimating Eq. (1) on four subsamples.\n\n**Table 1: Abnormal Earnings Persistence Parameters (`\\hat{\\omega}_1`)**\n\n| | Entire sample | Low sales growth firms | High sales growth firms |\n| :--- | :---: | :---: | :---: |\n| **Entire sample** | 0.271 | 0.350 | 0.229 |\n| **Loss firms** | 0.133 | 0.207 | 0.081 |\n| **Profit firms** | 0.339 | 0.391 | 0.288 |\n\n*Note: Differences between high and low sales growth firms are statistically significant at p < 0.05.*\n\n**Table 2: Valuation Regression Results for `NI` and `RD`**\n\n| Variable | Subsample | Low Growth | High Growth |\n| :--- | :--- | :---: | :---: |\n| **NI** (`\\hat{\\beta}_3`) | Loss Firms | 0.440 | 0.371 |\n| | | (1.38) | (0.60) |\n| | Profit Firms | **3.608** | **1.252** |\n| | | (19.26) | (4.31) |\n| **RD** (`\\hat{\\beta}_4`) | Loss Firms | **2.100** | **4.081** |\n| | | (5.09) | (5.21) |\n| | Profit Firms | **-3.285** | **1.880** |\n| | | (10.48) | (4.23) |\n\n*Note: t-statistics are in parentheses. Bolded coefficients are significant at p < 0.01. Differences between low and high growth are statistically significant.*\n\n---\n\n### Question\n\nBased on the regression results in Table 2, which of the following statements accurately interpret the valuation of R&D expenditures (`RD`)?", "model_evaluation_result": {"model_raw_response": "Looking at Table 2, I need to analyze the R&D coefficients (β̂₄) for each subsample:\n\n**Loss Firms:**\n- Low Growth: β̂₄ = 2.100 (significant)\n- High Growth: β̂₄ = 4.081 (significant)\n\n**Profit Firms:**\n- Low Growth: β̂₄ = -3.285 (significant)\n- High Growth: β̂₄ = 1.880 (significant)\n\nNow evaluating each statement:\n\n**A) The positive valuation of RD for high-growth profitable firms is inconsistent with Hypothesis 2c.**\n- H2c states: \"For firms reporting a profit, the market positively values R&D expenditures when the firm is experiencing high sales growth, but not when the firm is experiencing low sales growth.\"\n- The data shows β̂₄ = 1.880 (positive) for high-growth profitable firms\n- This is CONSISTENT with H2c, not inconsistent\n- Statement A is FALSE\n\n**B) For profitable firms, RD is valued positively regardless of the growth stage, but the valuation is higher in high-growth environments.**\n- Low-growth profitable firms: β̂₄ = -3.285 (NEGATIVE)\n- High-growth profitable firms: β̂₄ = 1.880 (positive)\n- RD is NOT valued positively regardless of growth stage\n- Statement B is FALSE\n\n**C) For loss-making firms, the market values RD more highly when they are in a high-growth stage compared to a low-growth stage.**\n- Low-growth loss firms: β̂₄ = 2.100\n- High-growth loss firms: β̂₄ = 4.081\n- 4.081 > 2.100, and the note states differences between low and high growth are statistically significant\n- Statement C is TRUE\n\n**D) The negative valuation of RD for low-growth profitable firms is consistent with the view that, in a stable environment, R&D may be seen as a risky diversion of cash from a successful core business.**\n- Low-growth profitable firms: β̂₄", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 359, "Question": "### Background\n\n**Research Question.** How robust are event study findings to the specification of the model for expected returns?\n\n**Setting and Sample.** An event study using daily stock returns for commercial banks. The study uses a single-factor market model to estimate expected returns but acknowledges that a two-factor model including an interest rate factor is a common alternative for bank stocks, which are known to be sensitive to interest rate movements.\n\n### Data / Model Specification\n\nThe study employs a single-factor market model to define abnormal returns (`AR`):\n\n  \nR_{i,t} - R_{f,t} = \\alpha_i + \\beta_i (R_{m,t} - R_{f,t}) + \\epsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nAn alternative, potentially more accurate, two-factor model for bank stocks would include an interest rate factor, `Δr_t`:\n\n  \nR_{i,t} - R_{f,t} = \\alpha_i^* + \\beta_i^* (R_{m,t} - R_{f,t}) + \\delta_i \\Delta r_t + \\nu_{i,t} \\quad \\text{(Eq. (2))}\n \n\nIf the single-factor model (**Eq. (1)**) is used when the true process is the two-factor model (**Eq. (2)**), the calculated abnormal return may be biased.\n\n---\n\n### Question\n\nRegarding the potential misspecification of the single-factor model used in the study, select all correct statements.", "Options": {"A": "A rejection of the null hypothesis in a GMM J-test of the single-factor model's overidentifying restrictions would indicate that the model is misspecified, thus challenging the validity of the abnormal returns calculated from it.", "B": "A GMM J-test is used to confirm that the model's parameters (`α_i`, `β_i`) are statistically significant.", "C": "The omission of an interest rate factor would cause the `CAR` to be understated, regardless of the direction of interest rate movements on the event day.", "D": "If bank stocks have a positive sensitivity to interest rates (`δ_i > 0`), and an unexpected drop in interest rates occurred during an event window, the reported positive `CAR` would likely be an overestimation of the true effect of the deregulation announcement."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of omitted variable bias in the context of event study methodology and knowledge of advanced specification tests. It uses the 'Premise/Assumption Packaging' strategy. Option A requires applying the logic of OVB to a specific scenario to determine the direction of bias. Option B tests the correct interpretation of a GMM J-test as a tool for assessing model validity. Distractor C is a 'Conceptual Opposite' that incorrectly specifies the bias direction. Distractor D is 'Almost Right' but confuses a specification test (J-test) with a test of parameter significance (t-test).", "qid": "359", "question": "### Background\n\n**Research Question.** How robust are event study findings to the specification of the model for expected returns?\n\n**Setting and Sample.** An event study using daily stock returns for commercial banks. The study uses a single-factor market model to estimate expected returns but acknowledges that a two-factor model including an interest rate factor is a common alternative for bank stocks, which are known to be sensitive to interest rate movements.\n\n### Data / Model Specification\n\nThe study employs a single-factor market model to define abnormal returns (`AR`):\n\n  \nR_{i,t} - R_{f,t} = \\alpha_i + \\beta_i (R_{m,t} - R_{f,t}) + \\epsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\nAn alternative, potentially more accurate, two-factor model for bank stocks would include an interest rate factor, `Δr_t`:\n\n  \nR_{i,t} - R_{f,t} = \\alpha_i^* + \\beta_i^* (R_{m,t} - R_{f,t}) + \\delta_i \\Delta r_t + \\nu_{i,t} \\quad \\text{(Eq. (2))}\n \n\nIf the single-factor model (**Eq. (1)**) is used when the true process is the two-factor model (**Eq. (2)**), the calculated abnormal return may be biased.\n\n---\n\n### Question\n\nRegarding the potential misspecification of the single-factor model used in the study, select all correct statements.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\n**Statement A**: A GMM J-test examines overidentifying restrictions to test model specification. If the null hypothesis (that the model is correctly specified) is rejected, it indicates misspecification. This would indeed challenge the validity of abnormal returns calculated from a misspecified model. **This is correct.**\n\n**Statement B**: The GMM J-test is not used to test parameter significance - it tests model specification through overidentifying restrictions. This confuses the purpose of the J-test with standard t-tests or F-tests for parameter significance. **This is incorrect.**\n\n**Statement C**: The direction of bias in CAR depends on both the sign of the omitted variable's coefficient (δ_i) and the direction of the variable's movement. The statement incorrectly claims CAR would always be understated regardless of interest rate movement direction. **This is incorrect.**\n\n**Statement D**: With δ_i > 0, bank returns increase when interest rates rise. If rates drop unexpectedly, this creates a negative contribution to returns (δ_i × Δr_t < 0). The single-factor model would attribute this negative effect to the event itself, making any positive CAR appear larger than it truly is - an overestimation. **This is correct.**\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 75, "Question": "### Background\n\nAn insurer is evaluating three alternative reinsurance strategies for its crop insurance portfolio, which is composed of risks from 10 different regions. The goal is to find a strategy that optimizes the trade-off between retaining risk for cost savings and spreading risk for stability. The performance of each strategy is assessed using an Asset-Liability Management (ALM) framework, simulated 5,000 times over a 30-year horizon.\n\n**Reinsurance Models:**\n- **Model A (Joint Self-Managed Pool):** All risks from participating regions are pooled and retained internally. This model avoids paying loading fees to private reinsurers but is exposed to undiversified, widespread catastrophic losses.\n- **Model B (Group Private Reinsurance):** The pool acts as a single entity to purchase private reinsurance for all its risks, transferring them to the international market. This model is protected from catastrophic events but incurs significant costs from reinsurance premium loading fees.\n- **Model C (Optimized Portfolio):** A genetic algorithm allocates crop risks into two groups: a lower-risk group is retained in the self-managed pool (like Model A), and a higher-risk group is ceded to private reinsurers (like Model B).\n\n### Data / Model Specification\n\nThe performance of the three models is evaluated based on the simulated distributions of the actuarial risk measures. Table 1 presents key statistics from these simulations at the end of the 30-year horizon, under the assumption of **zero initial surplus (U₀ = $0)**.\n\n**Table 1: Simulated Actuarial Risk Measures at End of Year 30 (U₀ = $0)**\n\n| Risk Measure | Model A (Self-Managed) | Model B (Private Reinsurance) | Model C (Optimized Portfolio) |\n| :--- | :--- | :--- | :--- |\n| Survival Probability | 35.8% | 16.7% | 33.6% |\n| Average Surplus | $1,432,003,119 | $360,857,396 | $842,471,082 |\n| 5th Percentile Surplus | -$112,903,478 | -$976,132,222 | -$552,079,233 |\n| Average Deficit at Ruin | -$157,799,946 | -$65,466,143 | -$73,023,649 |\n\n### Question\n\nBased on the model descriptions and the data in Table 1, which of the following statements accurately characterize the performance trade-offs of the pure strategies (Model A and Model B)? Select all that apply.", "Options": {"A": "Model B's primary weakness is its severe capital erosion due to high costs, demonstrated by its 5th Percentile Surplus of -$976,132,222, the most negative of all models.", "B": "Model A's primary strength is its cost efficiency, demonstrated by its Average Surplus of $1,432,003,119, which is the highest among all models.", "C": "Model B's primary strength is its robust protection against catastrophic losses, reflected in its Average Deficit at Ruin of -$65,466,143, the smallest (least severe) among all models.", "D": "Model A's primary weakness is its high cost structure, evidenced by its low Survival Probability of 35.8%."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the ability to interpret quantitative results from a table in the context of qualitative model descriptions. It targets the core trade-offs discussed in the paper. The depth strategy is 'Computational Judgment,' requiring the user to compare values across the table to identify strengths (best performance on a metric) and weaknesses (worst performance). Distractor C is a 'Conceptual Opposite' error; Model A's weakness is catastrophic risk, not high cost, and its low survival probability is a symptom of that risk, not its cost structure.", "qid": "75", "question": "### Background\n\nAn insurer is evaluating three alternative reinsurance strategies for its crop insurance portfolio, which is composed of risks from 10 different regions. The goal is to find a strategy that optimizes the trade-off between retaining risk for cost savings and spreading risk for stability. The performance of each strategy is assessed using an Asset-Liability Management (ALM) framework, simulated 5,000 times over a 30-year horizon.\n\n**Reinsurance Models:**\n- **Model A (Joint Self-Managed Pool):** All risks from participating regions are pooled and retained internally. This model avoids paying loading fees to private reinsurers but is exposed to undiversified, widespread catastrophic losses.\n- **Model B (Group Private Reinsurance):** The pool acts as a single entity to purchase private reinsurance for all its risks, transferring them to the international market. This model is protected from catastrophic events but incurs significant costs from reinsurance premium loading fees.\n- **Model C (Optimized Portfolio):** A genetic algorithm allocates crop risks into two groups: a lower-risk group is retained in the self-managed pool (like Model A), and a higher-risk group is ceded to private reinsurers (like Model B).\n\n### Data / Model Specification\n\nThe performance of the three models is evaluated based on the simulated distributions of the actuarial risk measures. Table 1 presents key statistics from these simulations at the end of the 30-year horizon, under the assumption of **zero initial surplus (U₀ = $0)**.\n\n**Table 1: Simulated Actuarial Risk Measures at End of Year 30 (U₀ = $0)**\n\n| Risk Measure | Model A (Self-Managed) | Model B (Private Reinsurance) | Model C (Optimized Portfolio) |\n| :--- | :--- | :--- | :--- |\n| Survival Probability | 35.8% | 16.7% | 33.6% |\n| Average Surplus | $1,432,003,119 | $360,857,396 | $842,471,082 |\n| 5th Percentile Surplus | -$112,903,478 | -$976,132,222 | -$552,079,233 |\n| Average Deficit at Ruin | -$157,799,946 | -$65,466,143 | -$73,023,649 |\n\n### Question\n\nBased on the model descriptions and the data in Table 1, which of the following statements accurately characterize the performance trade-offs of the pure strategies (Model A and Model B)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data and model descriptions, I need to evaluate each statement about the trade-offs between Model A (Self-Managed) and Model B (Private Reinsurance).\n\nLet me analyze each option:\n\n**Option A**: \"Model B's primary weakness is its severe capital erosion due to high costs, demonstrated by its 5th Percentile Surplus of -$976,132,222, the most negative of all models.\"\n- Model B has the most negative 5th percentile surplus (-$976,132,222)\n- The model description states Model B \"incurs significant costs from reinsurance premium loading fees\"\n- This severe capital erosion aligns with Model B's high cost structure\n- ✓ This is correct\n\n**Option B**: \"Model A's primary strength is its cost efficiency, demonstrated by its Average Surplus of $1,432,003,119, which is the highest among all models.\"\n- Model A has the highest average surplus ($1,432,003,119)\n- The model description states Model A \"avoids paying loading fees to private reinsurers\"\n- High average surplus reflects cost savings from avoiding reinsurance fees\n- ✓ This is correct\n\n**Option C**: \"Model B's primary strength is its robust protection against catastrophic losses, reflected in its Average Deficit at Ruin of -$65,466,143, the smallest (least severe) among all models.\"\n- Model B has the smallest average deficit at ruin (-$65,466,143)\n- The model description states Model B \"is protected from catastrophic events\"\n- When ruin occurs, the severity is limited due to reinsurance protection\n- ✓ This is correct\n\n**Option D**: \"Model A's primary weakness is its high cost structure, evidenced by its low Survival Probability of 35.8%.\"\n- This is contradictory - Model A actually has LOW costs (avoids reinsurance fees)\n- Model A's survival probability (35.8%) is higher than Model B's (16.7%)\n- Model A's weakness is catastrophic risk exposure, not high costs\n- ✗ This is incorrect\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 227, "Question": "### Background\n\n**Research Question.** This case examines the core hypotheses of public finance regarding the capitalization of local fiscal policies. Specifically, it investigates whether residential and business property owners have different preferences for public services versus tax burdens, leading to differential impacts on their respective property values. The analysis tests two primary hypotheses:\n\n- **Hypothesis 1 (H1):** Residential property values are more responsive to changes in school spending than to changes in property taxes.\n- **Hypothesis 2 (H2):** Business property values are more responsive to changes in property taxes than to changes in school spending.\n\n**Setting / Data-Generating Environment.** The study uses a panel of 152 communities in southeast Michigan from 1983-2002. In this setting, public schools are primarily financed by property taxes. The Tiebout model suggests that mobile agents (both residents and firms) sort into communities that offer their preferred tax-service package. Residents, who directly benefit from schools, may value them highly. Firms, modeled as profit-maximizers, may view property taxes as a direct cost of business while receiving little direct benefit from K-12 education.\n\n### Data / Model Specification\n\nThe study estimates a dynamic first-difference 2SLS model to determine the long-run elasticities of property values with respect to fiscal policy. The model includes contemporaneous and three-year lagged values of policy variables to capture the full capitalization effect over time. Table 1 below presents the estimated coefficients for both residential and business property value growth.\n\n**Table 1: Selected 2SLS Regression Coefficients for Property Value Growth**\n\n| Dependent Variable | Regressor | Coefficient | Std. Error |\n| :--- | :--- | :--- | :--- |\n| **ΔLog Residential Property Value** | | | |\n| | Log Property Tax | -0.256*** | (0.040) |\n| | Log Property Tax t-1 | -0.123*** | (0.020) |\n| | Log Property Tax t-2 | -0.029*** | (0.010) |\n| | Log Property Tax t-3 | 0.004 | (0.012) |\n| | Log School Spending | 0.623*** | (0.236) |\n| | Log School Spending t-1 | -0.029 | (0.186) |\n| | Log School Spending t-2 | -0.156 | (0.162) |\n| | Log School Spending t-3 | 0.157 | (0.111) |\n| **ΔLog Business Property Value** | | | |\n| | Log Property Tax | -0.194 | (0.136) |\n| | Log Property Tax t-1 | -0.403** | (0.185) |\n| | Log Property Tax t-2 | -0.601** | (0.275) |\n| | Log Property Tax t-3 | 0.216 | (0.172) |\n| | Log School Spending | -1.458* | (0.810) |\n| | Log School Spending t-1 | 0.057 | (0.428) |\n| | Log School Spending t-2 | -0.231 | (0.194) |\n| | Log School Spending t-3 | 0.494 | (0.471) |\n\n*Note: *, **, *** indicate significance at the 10%, 5%, and 1% levels, respectively. The long-run elasticity is the sum of the contemporaneous and lagged coefficients.*\n\n### Question\n\nA city council proposes a budget-neutral policy: raise property taxes by 10% to fund a 10% increase in school spending. Based on the long-run elasticities derived from Table 1, select all of the following statements that accurately describe the predicted outcomes of this policy.", "Options": {"A": "The policy is predicted to be substantially more harmful to business properties than to residential properties, reflecting the higher tax sensitivity of business capital.", "B": "The policy is predicted to increase the value of business properties by approximately 1.6%, as businesses respond positively to higher school spending.", "C": "The policy is predicted to increase the value of residential properties, as the positive impact from increased school spending outweighs the negative impact from higher taxes.", "D": "The policy is Pareto-improving because it benefits residents without making business owners significantly worse off."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize multiple empirical results to evaluate a policy trade-off. It uses a Scenario Application strategy. Distractors are designed to trap common errors: (C) a Sign Error, where the negative long-run spending elasticity for businesses is incorrectly treated as positive, and (D) a Conceptual error, misinterpreting the definition of a Pareto-improving policy.", "qid": "227", "question": "### Background\n\n**Research Question.** This case examines the core hypotheses of public finance regarding the capitalization of local fiscal policies. Specifically, it investigates whether residential and business property owners have different preferences for public services versus tax burdens, leading to differential impacts on their respective property values. The analysis tests two primary hypotheses:\n\n- **Hypothesis 1 (H1):** Residential property values are more responsive to changes in school spending than to changes in property taxes.\n- **Hypothesis 2 (H2):** Business property values are more responsive to changes in property taxes than to changes in school spending.\n\n**Setting / Data-Generating Environment.** The study uses a panel of 152 communities in southeast Michigan from 1983-2002. In this setting, public schools are primarily financed by property taxes. The Tiebout model suggests that mobile agents (both residents and firms) sort into communities that offer their preferred tax-service package. Residents, who directly benefit from schools, may value them highly. Firms, modeled as profit-maximizers, may view property taxes as a direct cost of business while receiving little direct benefit from K-12 education.\n\n### Data / Model Specification\n\nThe study estimates a dynamic first-difference 2SLS model to determine the long-run elasticities of property values with respect to fiscal policy. The model includes contemporaneous and three-year lagged values of policy variables to capture the full capitalization effect over time. Table 1 below presents the estimated coefficients for both residential and business property value growth.\n\n**Table 1: Selected 2SLS Regression Coefficients for Property Value Growth**\n\n| Dependent Variable | Regressor | Coefficient | Std. Error |\n| :--- | :--- | :--- | :--- |\n| **ΔLog Residential Property Value** | | | |\n| | Log Property Tax | -0.256*** | (0.040) |\n| | Log Property Tax t-1 | -0.123*** | (0.020) |\n| | Log Property Tax t-2 | -0.029*** | (0.010) |\n| | Log Property Tax t-3 | 0.004 | (0.012) |\n| | Log School Spending | 0.623*** | (0.236) |\n| | Log School Spending t-1 | -0.029 | (0.186) |\n| | Log School Spending t-2 | -0.156 | (0.162) |\n| | Log School Spending t-3 | 0.157 | (0.111) |\n| **ΔLog Business Property Value** | | | |\n| | Log Property Tax | -0.194 | (0.136) |\n| | Log Property Tax t-1 | -0.403** | (0.185) |\n| | Log Property Tax t-2 | -0.601** | (0.275) |\n| | Log Property Tax t-3 | 0.216 | (0.172) |\n| | Log School Spending | -1.458* | (0.810) |\n| | Log School Spending t-1 | 0.057 | (0.428) |\n| | Log School Spending t-2 | -0.231 | (0.194) |\n| | Log School Spending t-3 | 0.494 | (0.471) |\n\n*Note: *, **, *** indicate significance at the 10%, 5%, and 1% levels, respectively. The long-run elasticity is the sum of the contemporaneous and lagged coefficients.*\n\n### Question\n\nA city council proposes a budget-neutral policy: raise property taxes by 10% to fund a 10% increase in school spending. Based on the long-run elasticities derived from Table 1, select all of the following statements that accurately describe the predicted outcomes of this policy.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to calculate the long-run elasticities from Table 1 and then apply them to analyze the proposed policy.\n\nFirst, let me calculate the long-run elasticities (sum of contemporaneous and lagged coefficients):\n\n**Residential Properties:**\n- Property Tax elasticity: -0.256 + (-0.123) + (-0.029) + 0.004 = -0.404\n- School Spending elasticity: 0.623 + (-0.029) + (-0.156) + 0.157 = 0.595\n\n**Business Properties:**\n- Property Tax elasticity: -0.194 + (-0.403) + (-0.601) + 0.216 = -0.982\n- School Spending elasticity: -1.458 + 0.057 + (-0.231) + 0.494 = -1.138\n\nNow, applying the policy (10% increase in both property taxes and school spending):\n\n**Residential property value change:**\n(-0.404 × 10%) + (0.595 × 10%) = -4.04% + 5.95% = +1.91%\n\n**Business property value change:**\n(-0.982 × 10%) + (-1.138 × 10%) = -9.82% + (-11.38%) = -21.2%\n\nLet me evaluate each option:\n\n**Option A:** \"The policy is predicted to be substantially more harmful to business properties than to residential properties, reflecting the higher tax sensitivity of business capital.\"\n- This is TRUE. Business properties lose 21.2% while residential properties gain 1.91%. This is a massive difference, and businesses do show higher tax sensitivity (-0.982 vs -0.404).\n\n**Option B:** \"The policy is predicted to increase the value of business properties by approximately 1.6%...\"\n- This is FALSE. Business properties are predicted to DECREASE by 21.2%, not increase.\n\n**Option C:** \"The policy is predicted to increase the value of residential properties, as the positive impact from increased school spending outweighs the negative impact from higher taxes.\"\n- This is TRUE. Residential properties increase by 1.91%, and this is indeed because the positive school spending", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 423, "Question": "### Background\n\nThe empirical finding that an external factor (stock market volatility) predicts bond market volatility, even after controlling for the yield curve itself, challenges standard Affine Term Structure Models (ATSMs). This suggests the need for a modified theoretical framework.\n\n### Question\n\nThe empirical findings in the paper challenge standard ATSMs. Which of the following describe valid theoretical modifications or properties of an extended model that could reconcile theory with this evidence? Select all that apply.", "Options": {"A": "The puzzle can be resolved within a standard ATSM by simply adding more 'spanned' factors, such as a fourth or fifth principal component of the yield curve.", "B": "The evidence can be accommodated by assuming that the innovations to the yield curve factors are driven by a GARCH process that is independent of the equity market.", "C": "Introduce an 'unspanned' stochastic volatility factor that governs the variance of the yield curve's state variables but does not directly enter the bond pricing equation itself.", "D": "In the extended model, the innovations to the new unspanned volatility factor must be allowed to be correlated with innovations to equity market volatility."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the 'Apex' knowledge from the original QA, focusing on how to theoretically resolve the empirical puzzle. It uses a Reverse-Reasoning strategy, asking for model properties that would generate the observed results. Correct options A and B together describe the standard solution: an unspanned stochastic volatility factor correlated with the external variable. Distractor C misses the core issue, which is about an *external* (unspanned) factor, not a lack of internal (spanned) factors. Distractor D describes a model that would generate volatility clustering but would fail to explain why an *external* variable like stock volatility has predictive power.", "qid": "423", "question": "### Background\n\nThe empirical finding that an external factor (stock market volatility) predicts bond market volatility, even after controlling for the yield curve itself, challenges standard Affine Term Structure Models (ATSMs). This suggests the need for a modified theoretical framework.\n\n### Question\n\nThe empirical findings in the paper challenge standard ATSMs. Which of the following describe valid theoretical modifications or properties of an extended model that could reconcile theory with this evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) Adding more 'spanned' factors (like 4th or 5th principal components) wouldn't resolve the puzzle. Spanned factors by definition affect bond prices directly and would be captured by the yield curve itself. The empirical finding is that stock market volatility predicts bond volatility even after controlling for the yield curve, so adding more yield curve factors won't help.\n\nB) A GARCH process that is independent of the equity market would not explain why stock market volatility predicts bond volatility. The key empirical finding is the predictive relationship between equity and bond volatility, which requires some connection between the markets.\n\nC) This is a valid modification. An 'unspanned' stochastic volatility factor would affect the variance of yield curve factors without directly entering bond prices. This allows volatility to vary without being fully captured by the yield curve itself, which could explain why external variables like stock volatility have predictive power.\n\nD) This is also valid. For stock market volatility to predict bond volatility in the extended model, the unspanned volatility factor must be correlated with equity market volatility. Otherwise, there would be no channel through which equity volatility could provide information about future bond volatility.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 421, "Question": "### Background\n\nA portfolio manager, observing the paper's findings, considers a dynamic hedging strategy: they will significantly increase their portfolio's allocation to T-bonds whenever lagged stock volatility crosses into an 'Above Median' state, anticipating that T-bonds will provide a strong hedge during the more frequent stock market crashes that occur in such regimes.\n\n**Table 1. Average Daily T-bond Returns During Extreme Stock Declines (<5th percentile)**\n\n| Lagged Volatility was: | # of Obs. | Avg Ret 30-yr T-bond (%) |\n| :--- | :--- | :--- |\n| Below Median | 31 | 0.59 |\n| Above Median | 167 | 0.61 |\n\n### Question\n\nWhich of the following represent significant, real-world risks of implementing this dynamic hedging strategy that are not directly apparent from the data in Table 1? Select all that apply.", "Options": {"A": "Execution Risk: Attempting to buy T-bonds when the FTQ mechanism is most active means competing with massive, one-sided demand, leading to high market impact costs and adverse price movements.", "B": "Basis Risk: During a crisis, the stable price relationship between cash Treasury bonds and T-bond futures can break down, making a futures-based hedge unreliable and imperfect.", "C": "Signal Risk: The signal to hedge (lagged stock volatility) is only available with a significant delay, making it impossible to implement the strategy in a timely manner.", "D": "Hedging Ineffectiveness: The strategy is flawed because Table 1 shows that T-bonds provide a weaker hedge (0.61% vs 0.59%) during high-volatility regimes."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to critique a practical application of the paper's findings, converting the 'Apex' part of the original QA. It uses a Premise/assumption packaging strategy, asking the user to select valid risks of a proposed strategy. The correct options capture the core concepts of execution risk and basis risk. Distractor C misinterprets the data in the table (the hedge is effective, and the difference is minimal). Distractor D introduces an irrelevant constraint, as the paper's premise is that lagged volatility is a timely and available signal.", "qid": "421", "question": "### Background\n\nA portfolio manager, observing the paper's findings, considers a dynamic hedging strategy: they will significantly increase their portfolio's allocation to T-bonds whenever lagged stock volatility crosses into an 'Above Median' state, anticipating that T-bonds will provide a strong hedge during the more frequent stock market crashes that occur in such regimes.\n\n**Table 1. Average Daily T-bond Returns During Extreme Stock Declines (<5th percentile)**\n\n| Lagged Volatility was: | # of Obs. | Avg Ret 30-yr T-bond (%) |\n| :--- | :--- | :--- |\n| Below Median | 31 | 0.59 |\n| Above Median | 167 | 0.61 |\n\n### Question\n\nWhich of the following represent significant, real-world risks of implementing this dynamic hedging strategy that are not directly apparent from the data in Table 1? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each option carefully:\n\nA) Execution Risk: This is a significant real-world risk. When stock volatility is high and crashes are occurring, the Flight-to-Quality (FTQ) mechanism creates massive one-sided demand for T-bonds. The portfolio manager would be trying to buy T-bonds precisely when everyone else is also rushing to buy them, leading to high market impact costs and adverse price movements. This execution challenge is not captured in Table 1's data.\n\nB) Basis Risk: This is also a significant real-world risk. During market crises, the normally stable relationship between cash Treasury bonds and T-bond futures can break down due to liquidity issues, margin calls, and other market stresses. If implementing the hedge through futures (which is common for large positions), this basis risk could make the hedge unreliable. This risk is not apparent from Table 1.\n\nC) Signal Risk: This is NOT a significant risk as described. The option claims lagged stock volatility is \"only available with a significant delay,\" but this is incorrect. Stock volatility can be calculated almost instantaneously from recent price movements. The term \"lagged\" in the context simply means using past volatility to predict future crashes, not that there's a delay in obtaining the signal.\n\nD) Hedging Ineffectiveness: This is NOT a risk. The option misinterprets the data - T-bonds actually provide a slightly better hedge during high-volatility regimes (0.61% vs 0.59%), not worse. More importantly, the difference is economically insignificant, and T-bonds still provide positive returns during crashes in both regimes.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 13, "Question": "### Background\n\nThe paper contrasts the traditional factor-based Risk-Based Capital (RBC) system with a proposed Unified Valuation System (UVS). The UVS framework requires an insurer to use an internal model to generate a full probability distribution of the present value of future net costs, `PV(NC)`, for a block of policies. Liabilities and capital are then defined from this single distribution.\n\n### Data / Model Specification\n\nA potential UVS implementation defines the policy liability and total required assets as:\n\n  \n\\text{Policy Liability: } L_{UVS} = \\mathrm{CTE}(70) \\quad \\text{(Eq. 1)}\n \n\n  \n\\text{Total Required Assets: } L_{UVS} + K_{UVS} = \\mathrm{CTE}(95) \\quad \\text{(Eq. 2)}\n \n\nwhere `CTE(α)` is the Conditional Tail Expectation at the `α`-th percentile of the `PV(NC)` distribution and `K_UVS` is the required capital.\n\n### Question\n\nBased on the principles of the UVS framework as described, which of the following statements are valid conclusions or characteristics? (Select all that apply.)\n", "Options": {"A": "A primary weakness of the traditional factor-based RBC system that UVS addresses is its static, retrospective nature, as it relies on a company's latest financial statements rather than forward-looking projections.", "B": "The UVS framework is inherently more risk-sensitive than a traditional factor-based RBC system because it directly reflects a company's specific risk profile and risk management effectiveness in its capital calculation.", "C": "The required capital, `K_UVS`, represents the financial resources needed to cover the average loss in the region between the 70th and 95th percentiles of the cost distribution.", "D": "The required capital can be expressed as `K_UVS = CTE(95) - CTE(70)`."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the candidate's ability to synthesize the conceptual advantages of the UVS framework and derive the capital formula from its definitional components.\nDepth Strategy: Scenario Application / Computational Judgment. The candidate must apply the UVS principles to contrast it with the older system and perform an algebraic derivation.\nDistractor Logic:\n- A (Conceptual Distractor - Almost Right): This is a subtle misinterpretation of the derived formula `K_UVS = CTE(95) - CTE(70)`. This formula represents the difference between two conditional averages, not the average of the values *between* two percentiles. The latter would be a different statistical quantity. This tests for a precise understanding of the CTE measure.", "qid": "13", "question": "### Background\n\nThe paper contrasts the traditional factor-based Risk-Based Capital (RBC) system with a proposed Unified Valuation System (UVS). The UVS framework requires an insurer to use an internal model to generate a full probability distribution of the present value of future net costs, `PV(NC)`, for a block of policies. Liabilities and capital are then defined from this single distribution.\n\n### Data / Model Specification\n\nA potential UVS implementation defines the policy liability and total required assets as:\n\n  \n\\text{Policy Liability: } L_{UVS} = \\mathrm{CTE}(70) \\quad \\text{(Eq. 1)}\n \n\n  \n\\text{Total Required Assets: } L_{UVS} + K_{UVS} = \\mathrm{CTE}(95) \\quad \\text{(Eq. 2)}\n \n\nwhere `CTE(α)` is the Conditional Tail Expectation at the `α`-th percentile of the `PV(NC)` distribution and `K_UVS` is the required capital.\n\n### Question\n\nBased on the principles of the UVS framework as described, which of the following statements are valid conclusions or characteristics? (Select all that apply.)\n", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) This correctly identifies a key weakness of traditional RBC systems that UVS addresses. Traditional RBC relies on historical data from financial statements, while UVS uses forward-looking projections through the PV(NC) distribution.\n\nB) This is accurate. UVS uses company-specific internal models to generate the full probability distribution, making it inherently more risk-sensitive than standardized factor-based approaches.\n\nC) This is incorrect. CTE(95) - CTE(70) represents the difference between two conditional tail expectations, not the \"average loss in the region between the 70th and 95th percentiles.\" The CTE values are already conditional averages of the tail regions.\n\nD) This is mathematically correct based on the given equations:\n   - From Eq. 1: L_UVS = CTE(70)\n   - From Eq. 2: L_UVS + K_UVS = CTE(95)\n   - Therefore: K_UVS = CTE(95) - L_UVS = CTE(95) - CTE(70)\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 324, "Question": "## Background\n\n**Research Question.** This case examines the strategic decision-making of a large Chinese firm, Huaneng Power International (HPI), in choosing a venue for its international equity offering, highlighting the trade-offs between market depth, valuation, and strategic goals.\n\n**Setting.** In 1994, HPI, a recently privatized power company, needed to raise $750 million to fund a massive expansion plan. Management was considering a Depositary Receipt offering on the New York Stock Exchange (NYSE) versus the more common route for Chinese firms at the time, the Hong Kong Stock Exchange (HKEX).\n\n**Variables & Parameters.**\n- **P/E Multiple:** Price-to-Earnings ratio, a valuation metric.\n- **Offering Size:** The total amount of capital raised in an equity issue (in USD).\n- **EPS (Earnings Per Share):** HPI's earnings per share, given as $1.45.\n\n---\n\n## Data / Model Specification\n\nData on comparable Chinese company IPOs from 1993-1994 are presented in Table 1. HPI's sister company, Shandong Huaneng, had previously listed on the NYSE and was trading at a P/E of 14x. HPI's planned offering of $750 million was substantially larger than any of the recent HKEX listings.\n\n**Table 1: Chinese Company IPOs in 1993 and 1994**\n\n| Company | Exchange | Offering Size | P/E Multiple |\n| :--- | :--- | :--- | :--- |\n| Dongfang Electrical | Hong Kong | $62m | 12.0x |\n| Tianjin Bohai Chem. | Hong Kong | $53m | 11.2x |\n| Yizheng Chemical | Hong Kong | $308m | 13.5x |\n| Kunming Machine | Hong Kong | $17m | 5.4x |\n| Beiren Printing | Hong Kong | $27m | 15.2x |\n| Shanghai Petrochem. | Hong Kong | $343m | 11.3x |\n| Tsingtao Brewery | Hong Kong | n/a | 14.0x |\n| Shandong Huaneng | NYSE | $333m | 14.0x (trading) |\n\n---\n\n## Question\n\nBased on the provided background and data in Table 1, select all statements that accurately describe the strategic rationale and quantitative trade-offs for Huaneng Power International (HPI) choosing an NYSE listing over a Hong Kong listing.", "Options": {"A": "The total capital raised by HPI on the NYSE ($625 million) exceeded its initial target of $750 million, indicating strong U.S. investor demand.", "B": "A valuation analysis suggested a potential premium on the NYSE, where HPI's sister company traded at a 14.0x P/E multiple, compared to a simple average of approximately 11.8x for recent Hong Kong IPOs.", "C": "Listing in Hong Kong at a price implied by the average local P/E multiple would have allowed HPI to meet its $750 million funding goal.", "D": "The NYSE was favored for its market capacity, as HPI's $750 million target was more than double the size of the largest recent Chinese IPOs in Hong Kong."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize qualitative strategic factors (market capacity) and quantitative valuation metrics (P/E multiples) from case data. It uses an Atomic Decomposition strategy to break down the core decision into two distinct, verifiable propositions. Distractor C is a factual error from the text (under-raising capital), while D is a computational error that reverses the conclusion of the market capacity analysis.", "qid": "324", "question": "## Background\n\n**Research Question.** This case examines the strategic decision-making of a large Chinese firm, Huaneng Power International (HPI), in choosing a venue for its international equity offering, highlighting the trade-offs between market depth, valuation, and strategic goals.\n\n**Setting.** In 1994, HPI, a recently privatized power company, needed to raise $750 million to fund a massive expansion plan. Management was considering a Depositary Receipt offering on the New York Stock Exchange (NYSE) versus the more common route for Chinese firms at the time, the Hong Kong Stock Exchange (HKEX).\n\n**Variables & Parameters.**\n- **P/E Multiple:** Price-to-Earnings ratio, a valuation metric.\n- **Offering Size:** The total amount of capital raised in an equity issue (in USD).\n- **EPS (Earnings Per Share):** HPI's earnings per share, given as $1.45.\n\n---\n\n## Data / Model Specification\n\nData on comparable Chinese company IPOs from 1993-1994 are presented in Table 1. HPI's sister company, Shandong Huaneng, had previously listed on the NYSE and was trading at a P/E of 14x. HPI's planned offering of $750 million was substantially larger than any of the recent HKEX listings.\n\n**Table 1: Chinese Company IPOs in 1993 and 1994**\n\n| Company | Exchange | Offering Size | P/E Multiple |\n| :--- | :--- | :--- | :--- |\n| Dongfang Electrical | Hong Kong | $62m | 12.0x |\n| Tianjin Bohai Chem. | Hong Kong | $53m | 11.2x |\n| Yizheng Chemical | Hong Kong | $308m | 13.5x |\n| Kunming Machine | Hong Kong | $17m | 5.4x |\n| Beiren Printing | Hong Kong | $27m | 15.2x |\n| Shanghai Petrochem. | Hong Kong | $343m | 11.3x |\n| Tsingtao Brewery | Hong Kong | n/a | 14.0x |\n| Shandong Huaneng | NYSE | $333m | 14.0x (trading) |\n\n---\n\n## Question\n\nBased on the provided background and data in Table 1, select all statements that accurately describe the strategic rationale and quantitative trade-offs for Huaneng Power International (HPI) choosing an NYSE listing over a Hong Kong listing.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the provided data.\n\nLet me analyze each option:\n\n**Option A**: States that HPI raised $625 million on NYSE, exceeding its $750 million target. This is contradictory - $625 million is less than $750 million, not more. This statement is FALSE.\n\n**Option B**: Comparing P/E multiples:\n- Shandong Huaneng (NYSE): 14.0x P/E\n- Hong Kong IPOs average: (12.0 + 11.2 + 13.5 + 5.4 + 15.2 + 11.3 + 14.0) ÷ 7 = 82.6 ÷ 7 = 11.8x\nThe NYSE shows a higher valuation (14.0x vs 11.8x), suggesting a potential premium. This statement is TRUE.\n\n**Option C**: Testing if Hong Kong listing would meet the $750M goal:\n- Average Hong Kong P/E: 11.8x\n- HPI's EPS: $1.45\n- Stock price at 11.8x P/E: $1.45 × 11.8 = $17.11\n- Shares needed for $750M: $750M ÷ $17.11 = 43.84 million shares\nThis calculation shows it's mathematically possible. This statement is TRUE.\n\n**Option D**: Examining market capacity:\n- HPI's target: $750 million\n- Largest Hong Kong IPO in the data: Shanghai Petrochem at $343 million\n- $750M is indeed more than double $343M (2.19x)\nThis highlights NYSE's greater market capacity. This statement is TRUE.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 336, "Question": "### Background\n\n**Research Question.** What is the correct asymptotic distribution for estimators from a two-stage cross-sectional regression, and how does it differ under assumptions of conditional heteroskedasticity versus homoskedasticity?\n\n**Setting.** We analyze the asymptotic properties of the GLS estimator `ĉ` from a two-stage cross-sectional regression as the number of time-series observations `T` goes to infinity. The analysis contrasts the simple Fama-MacBeth (FM) variance estimator with more general forms.\n\n**Variables and Parameters.**\n- `ĉ`: The `(1+L+K)`x1 GLS estimate of the cross-sectional parameters.\n- `c`: The true parameter vector.\n- `uₜ`: `N`x1 vector of residuals from the time-series regressions `Rₜ = A + B yₜ + uₜ`.\n- `Δ`: An `N`x`N` constant covariance matrix of `uₜ` under homoskedasticity.\n\n---\n\n### Data / Model Specification\n\nThe full asymptotic variance of `√T(ĉ - c)` under general (conditionally heteroskedastic) conditions is given by:\n  \nS = V + W - G \\quad \\text{(Eq. (1))}\n \nwhere `V` is the variance from using sample average returns, `W` is the variance from using estimated betas, and `G` is twice the covariance between these two error sources. The standard Fama-MacBeth (FM) procedure produces an estimate of `V` but ignores `W` and `G`.\n\nUnder the special assumption of **conditional homoskedasticity** (i.e., `E[uₜuₜ'|Y] = Δ`), the `G` term becomes zero and the variance simplifies to:\n  \nS_{homo} = V + \\tilde{W} \\quad \\text{(Eq. (2))}\n \nwhere `W̃` is a specific positive semi-definite form of `W`.\n\n---\n\n### Question\n\nBased on the analysis of the asymptotic variance of the cross-sectional regression estimator, select all of the following statements that are correct.", "Options": {"A": "The `W` term in the general variance formula `S = V + W - G` captures the additional variance arising from the errors-in-variables problem of using estimated betas (`B̂`) instead of true betas (`B`).", "B": "When asset returns are conditionally homoskedastic, the Fama-MacBeth procedure systematically understates the true sampling variance because it correctly estimates `V` but omits the positive semi-definite term `W̃`.", "C": "Under the assumption of conditional homoskedasticity, the Fama-MacBeth variance estimator (`V`) is a consistent estimator of the true asymptotic variance (`S_homo`).", "D": "In the general case (conditional heteroskedasticity), the Fama-MacBeth procedure is guaranteed to overstate the precision of the estimates because the omitted term `W - G` is always positive semi-definite."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: Tests understanding of the components of the asymptotic variance formula and the impact of conditional homoskedasticity on the bias of the Fama-MacBeth procedure. Strategy: Atomic Decomposition. The question asks students to evaluate four distinct propositions about the variance components. Distractor Logic: Option B is a Step-Omission Error, as it correctly identifies that FM estimates V but incorrectly concludes this makes it consistent for S_homo (it omits W̃). Option C is a Conceptual Opposite, incorrectly stating the direction of bias is known in the general case, which is a key finding of the paper.", "qid": "336", "question": "### Background\n\n**Research Question.** What is the correct asymptotic distribution for estimators from a two-stage cross-sectional regression, and how does it differ under assumptions of conditional heteroskedasticity versus homoskedasticity?\n\n**Setting.** We analyze the asymptotic properties of the GLS estimator `ĉ` from a two-stage cross-sectional regression as the number of time-series observations `T` goes to infinity. The analysis contrasts the simple Fama-MacBeth (FM) variance estimator with more general forms.\n\n**Variables and Parameters.**\n- `ĉ`: The `(1+L+K)`x1 GLS estimate of the cross-sectional parameters.\n- `c`: The true parameter vector.\n- `uₜ`: `N`x1 vector of residuals from the time-series regressions `Rₜ = A + B yₜ + uₜ`.\n- `Δ`: An `N`x`N` constant covariance matrix of `uₜ` under homoskedasticity.\n\n---\n\n### Data / Model Specification\n\nThe full asymptotic variance of `√T(ĉ - c)` under general (conditionally heteroskedastic) conditions is given by:\n  \nS = V + W - G \\quad \\text{(Eq. (1))}\n \nwhere `V` is the variance from using sample average returns, `W` is the variance from using estimated betas, and `G` is twice the covariance between these two error sources. The standard Fama-MacBeth (FM) procedure produces an estimate of `V` but ignores `W` and `G`.\n\nUnder the special assumption of **conditional homoskedasticity** (i.e., `E[uₜuₜ'|Y] = Δ`), the `G` term becomes zero and the variance simplifies to:\n  \nS_{homo} = V + \\tilde{W} \\quad \\text{(Eq. (2))}\n \nwhere `W̃` is a specific positive semi-definite form of `W`.\n\n---\n\n### Question\n\nBased on the analysis of the asymptotic variance of the cross-sectional regression estimator, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at each statement carefully:\n\n**Statement A**: \"The `W` term in the general variance formula `S = V + W - G` captures the additional variance arising from the errors-in-variables problem of using estimated betas (`B̂`) instead of true betas (`B`).\"\n\nThis is correct. The background explicitly states that `W` is \"the variance from using estimated betas,\" which is precisely the errors-in-variables problem that arises when we use estimated betas instead of true betas in the second-stage regression.\n\n**Statement B**: \"When asset returns are conditionally homoskedastic, the Fama-MacBeth procedure systematically understates the true sampling variance because it correctly estimates `V` but omits the positive semi-definite term `W̃`.\"\n\nThis is correct. Under homoskedasticity, the variance is `S_homo = V + W̃` where `W̃` is explicitly stated to be \"positive semi-definite.\" Since FM only estimates `V` and ignores `W̃`, it systematically understates the true variance.\n\n**Statement C**: \"Under the assumption of conditional homoskedasticity, the Fama-MacBeth variance estimator (`V`) is a consistent estimator of the true asymptotic variance (`S_homo`).\"\n\nThis is incorrect. Under homoskedasticity, the true asymptotic variance is `S_homo = V + W̃`, not just `V`. The FM estimator estimates only `V`, so it cannot be a consistent estimator of `S_homo` which includes the additional term `W̃`.\n\n**Statement D**: \"In the general case (conditional heteroskedasticity), the Fama-MacBeth procedure is guaranteed to overstate the precision of the estimates because the omitted term `W - G` is always positive semi-definite.\"\n\nThis is incorrect. The omitted term is `W - G`, and there's no guarantee this is always positive semi-definite. In fact, `G` could be large enough that `W - G` becomes negative definite, in which case FM would actually overstate the variance (understate precision), not overstate precision.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 417, "Question": "### Background\n\n**Research Question.** This case examines the empirical validity of the Basel II regulatory framework for Small and Medium Enterprise (SME) credit risk, focusing on how misspecified asset correlation assumptions lead to a misallocation of bank capital.\n\n**Setting.** The analysis contrasts the asset correlation (AC) patterns prescribed by the Basel II rules with those empirically estimated from a large sample of Italian SMEs.\n\n**Variables & Parameters.**\n- `PD`: Probability of Default.\n- `S`: Annual sales of the SME, in millions of euros.\n\n---\n\n### Data / Model Specification\n\nThe Basel II framework for SMEs assumes that asset correlation (AC) is a decreasing function of `PD` and an increasing function of firm size `S`.\n\n**Table 1. Empirical Asset Correlation by Size and Risk Classes (1994–2008)**\n\n| Risk Classes | Total Sample | <1 | 1-5 | 5-7.5 | 7.5-10 | 10-25 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **Total Sample** | | **0.0647** | **0.0539** | **0.0686** | **0.1054** | **0.1172** |\n| A | 0.0410 | 0.0588 | 0.0521 | 0.0860 | 0.0910 | 0.0501 |\n| B | 0.0470 | 0.0805 | 0.0457 | 0.0591 | 0.0713 | 0.0555 |\n| C | 0.0518 | 0.0931 | 0.0493 | 0.0678 | 0.0690 | 0.0471 |\n| D | 0.0592 | 0.0835 | 0.0606 | 0.0638 | 0.0719 | 0.0797 |\n| E | 0.0549 | 0.0698 | 0.0773 | 0.1339 | 0.2019 | 0.2440 |\n| F | 0.0664 | 0.0766 | 0.2995 | 0.5167 | 0.5737 | 0.7148 |\n*Source: Adapted from Table 6 of the paper. Size classes are in millions of euros.* \n\n---\n\n### The Question\n\nAccording to the empirical results presented in **Table 1**, which of the following findings about asset correlation (AC) for Italian SMEs contradict the standard Basel II assumptions? (Select all that apply)", "Options": {"A": "The AC for the smallest firms (<1 million euros) is the highest among all size classes in the 'Total Sample'.", "B": "The AC for the safest firms (Risk Class A) is consistently negative across all size classes.", "C": "The relationship between AC and firm size is J-shaped, not monotonically positive.", "D": "The relationship between AC and credit risk (i.e., moving from risk class A to F) is positive, not negative."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to interpret tabular data and contrast it with a stated theoretical framework (Basel II assumptions). It is a Multiple Choice question testing the two core empirical findings of the paper. Correct options A and B capture the central contradictions identified in the original QA problem. Distractors are based on plausible misinterpretations of the table (C, a 'Total Sample' row misreading) or simple factual errors (D).", "qid": "417", "question": "### Background\n\n**Research Question.** This case examines the empirical validity of the Basel II regulatory framework for Small and Medium Enterprise (SME) credit risk, focusing on how misspecified asset correlation assumptions lead to a misallocation of bank capital.\n\n**Setting.** The analysis contrasts the asset correlation (AC) patterns prescribed by the Basel II rules with those empirically estimated from a large sample of Italian SMEs.\n\n**Variables & Parameters.**\n- `PD`: Probability of Default.\n- `S`: Annual sales of the SME, in millions of euros.\n\n---\n\n### Data / Model Specification\n\nThe Basel II framework for SMEs assumes that asset correlation (AC) is a decreasing function of `PD` and an increasing function of firm size `S`.\n\n**Table 1. Empirical Asset Correlation by Size and Risk Classes (1994–2008)**\n\n| Risk Classes | Total Sample | <1 | 1-5 | 5-7.5 | 7.5-10 | 10-25 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| **Total Sample** | | **0.0647** | **0.0539** | **0.0686** | **0.1054** | **0.1172** |\n| A | 0.0410 | 0.0588 | 0.0521 | 0.0860 | 0.0910 | 0.0501 |\n| B | 0.0470 | 0.0805 | 0.0457 | 0.0591 | 0.0713 | 0.0555 |\n| C | 0.0518 | 0.0931 | 0.0493 | 0.0678 | 0.0690 | 0.0471 |\n| D | 0.0592 | 0.0835 | 0.0606 | 0.0638 | 0.0719 | 0.0797 |\n| E | 0.0549 | 0.0698 | 0.0773 | 0.1339 | 0.2019 | 0.2440 |\n| F | 0.0664 | 0.0766 | 0.2995 | 0.5167 | 0.5737 | 0.7148 |\n*Source: Adapted from Table 6 of the paper. Size classes are in millions of euros.* \n\n---\n\n### The Question\n\nAccording to the empirical results presented in **Table 1**, which of the following findings about asset correlation (AC) for Italian SMEs contradict the standard Basel II assumptions? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each option against the Basel II assumptions that asset correlation (AC) decreases with probability of default (PD) and increases with firm size.\n\nLet me analyze each option:\n\n**Option A**: \"The AC for the smallest firms (<1 million euros) is the highest among all size classes in the 'Total Sample'.\"\n- Looking at the Total Sample row: <1 (0.0647), 1-5 (0.0539), 5-7.5 (0.0686), 7.5-10 (0.1054), 10-25 (0.1172)\n- The smallest firms (<1) have AC of 0.0647, which is NOT the highest (10-25 has 0.1172)\n- This statement is FALSE\n\n**Option B**: \"The AC for the safest firms (Risk Class A) is consistently negative across all size classes.\"\n- Risk Class A values: 0.0588, 0.0521, 0.0860, 0.0910, 0.0501\n- All values are positive, not negative\n- This statement is FALSE\n\n**Option C**: \"The relationship between AC and firm size is J-shaped, not monotonically positive.\"\n- Total Sample by size: 0.0647 → 0.0539 → 0.0686 → 0.1054 → 0.1172\n- This shows a decrease from <1 to 1-5, then increases thereafter\n- This is indeed J-shaped, contradicting Basel II's assumption of monotonically increasing AC with size\n- This statement is TRUE\n\n**Option D**: \"The relationship between AC and credit risk (i.e., moving from risk class A to F) is positive, not negative.\"\n- Looking at the Total Sample column (first data column): A (0.0410) → B (0.0470) → C (0.0518) → D (0.0592) → E (0.0549) → F (0.0664)\n- The trend is generally increasing from A to F (with slight dip at E)\n- Higher risk classes (closer to F) have higher PD, so this shows AC increasing with PD\n- This contra", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 290, "Question": "### Background\n\n**Research Question.** How can 'normal' insider trading be modeled to establish a benchmark against which to measure abnormal, information-driven trading activity?\n\n**Setting.** The study models the monthly number of net insider purchasers (`NP_{jt}`) for each firm `j` using a time-series process. The parameters are estimated during a non-event period to capture typical behavior.\n\n---\n\n### Data / Model Specification\n\nThe empirical model for 'normal' trading is an AR(1) process:\n\n  \nNP_{jt} = \\alpha_j + \\beta_j NP_{jt-1} + \\epsilon_{jt} \\quad \\text{(Eq. 1)}\n \n\nFrom the estimated coefficients `α̂_j` and `β̂_j`, the long-run mean number of net purchasers, `γ̂_j`, is calculated as:\n\n  \n\\hat{\\gamma}_j = \\frac{\\hat{\\alpha}_j}{1 - \\hat{\\beta}_j} \\quad \\text{(Eq. 2)}\n \n\nFor firms that are listing, the parameters of Eq. (1) are estimated over a post-listing window (months +7 to +60). A successful listing may represent a permanent structural break, improving the firm's prospects.\n\n---\n\nConsidering the model and the choice of a post-listing estimation window for listing firms, which of the following statements are valid? Select all that apply.", "Options": {"A": "The parameter `β_j` represents the persistence of insider trading; a value close to 1 implies that periods of net buying tend to be followed by more net buying.", "B": "Using a post-listing estimation window could create a conservative test; if listing improves firm prospects and raises the true `γ_j`, the benchmark for 'normal' buying will be set high, potentially underestimating the degree of abnormal buying in the pre-listing period.", "C": "If a firm's estimation yields `α̂ = 0.2` and `β̂ = 0.6`, its long-run mean number of net purchasers `γ̂` is 0.12.", "D": "If a firm's estimation yields `α̂ = 0.2` and `β̂ = 0.6`, its long-run mean number of net purchasers `γ̂` is 0.5, indicating a baseline tendency for net buying."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses understanding of the paper's core econometric model, including parameter interpretation, calculation, and the ability to critique the research design for potential bias (structural breaks). Depth Strategy: Computational Judgment and Scenario Application. The user must interpret parameters, perform a calculation, and evaluate the methodological choice of the estimation window. Distractor Logic: Option D is a 'Formula Misuse' distractor, resulting from incorrectly multiplying `α̂` and `β̂` instead of applying the correct formula from Eq. (2). The correct options require correct interpretation of `β` (A), correct calculation of `γ` (B), and a sophisticated understanding of the potential bias from a structural break (C).", "qid": "290", "question": "### Background\n\n**Research Question.** How can 'normal' insider trading be modeled to establish a benchmark against which to measure abnormal, information-driven trading activity?\n\n**Setting.** The study models the monthly number of net insider purchasers (`NP_{jt}`) for each firm `j` using a time-series process. The parameters are estimated during a non-event period to capture typical behavior.\n\n---\n\n### Data / Model Specification\n\nThe empirical model for 'normal' trading is an AR(1) process:\n\n  \nNP_{jt} = \\alpha_j + \\beta_j NP_{jt-1} + \\epsilon_{jt} \\quad \\text{(Eq. 1)}\n \n\nFrom the estimated coefficients `α̂_j` and `β̂_j`, the long-run mean number of net purchasers, `γ̂_j`, is calculated as:\n\n  \n\\hat{\\gamma}_j = \\frac{\\hat{\\alpha}_j}{1 - \\hat{\\beta}_j} \\quad \\text{(Eq. 2)}\n \n\nFor firms that are listing, the parameters of Eq. (1) are estimated over a post-listing window (months +7 to +60). A successful listing may represent a permanent structural break, improving the firm's prospects.\n\n---\n\nConsidering the model and the choice of a post-listing estimation window for listing firms, which of the following statements are valid? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The parameter β_j represents the persistence of insider trading; a value close to 1 implies that periods of net buying tend to be followed by more net buying.\n\nThis is correct. In an AR(1) model, the coefficient β_j on the lagged dependent variable measures persistence. When β_j is close to 1, the current value is highly dependent on the previous value, meaning net buying in one period tends to persist into the next period.\n\nB) Using a post-listing estimation window could create a conservative test; if listing improves firm prospects and raises the true γ_j, the benchmark for 'normal' buying will be set high, potentially underestimating the degree of abnormal buying in the pre-listing period.\n\nThis is correct. If listing improves firm prospects, insiders may engage in more net buying post-listing, raising the estimated \"normal\" level (γ̂_j). When this elevated benchmark is used to assess pre-listing trading, it would make pre-listing buying appear less abnormal than it actually was, creating a conservative (harder to reject) test.\n\nC) If a firm's estimation yields α̂ = 0.2 and β̂ = 0.6, its long-run mean number of net purchasers γ̂ is 0.12.\n\nLet me calculate: γ̂ = α̂/(1-β̂) = 0.2/(1-0.6) = 0.2/0.4 = 0.5\nThis gives 0.5, not 0.12, so this statement is incorrect.\n\nD) If a firm's estimation yields α̂ = 0.2 and β̂ = 0.6, its long-run mean number of net purchasers γ̂ is 0.5, indicating a baseline tendency for net buying.\n\nAs calculated above, γ̂ = 0.5, which is correct. This positive value indicates a baseline tendency for net buying (more purchasers than sellers on average).\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 371, "Question": "### Background\n\n**Research Question.** To test the causal mechanisms through which government industrial policy influences corporate financialization, distinguishing between two primary theoretical motivations.\n\n**Theoretical Framework.** Corporate financialization is driven by two main motives:\n1.  **Prevention Motivation:** Firms hold liquid financial assets as a precautionary measure to manage future operating risks and ease financial constraints.\n2.  **Crowding-Out Effect:** Agency conflicts lead managers to favor short-term financial investments over long-term real investments.\n\nThe study tests four potential channels. It finds that the two channels related to the **prevention motivation** (operational uncertainty, financial constraints) are statistically significant mediators of the policy's effect. In contrast, it finds that the two channels related to the **crowding-out effect** (information transparency, investment opportunities) are not statistically significant mediators.\n\n### Question\n\nConsidering the full set of mechanism findings—that the prevention-related channels are significant mediators while the agency-related channels are not—select all conclusions that are directly supported by this pattern of evidence.", "Options": {"A": "The insignificance of the investment opportunities channel as a mediator means that industrial policy does not create new real investment opportunities for supported firms.", "B": "Industrial policy appears to curb financialization mainly by providing an external substitute for firms' internal need to self-insure against risk.", "C": "The evidence suggests that financialization in this context is primarily a defensive, risk-management strategy for firms.", "D": "The findings imply that industrial policy is an effective tool for improving corporate governance and reducing agency conflicts between managers and shareholders."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Reverse-Reasoning strategy, asking students to infer the underlying driver of corporate behavior from a pattern of empirical results. It tests the synthesis of multiple findings. Correct options (A, B) represent the core theoretical takeaway from the mechanism analysis. Distractors are based on misinterpreting the implications of null results for the agency channel (C) and misunderstanding the logic of mediation, where a variable can be affected by the treatment without being a mediator (D).", "qid": "371", "question": "### Background\n\n**Research Question.** To test the causal mechanisms through which government industrial policy influences corporate financialization, distinguishing between two primary theoretical motivations.\n\n**Theoretical Framework.** Corporate financialization is driven by two main motives:\n1.  **Prevention Motivation:** Firms hold liquid financial assets as a precautionary measure to manage future operating risks and ease financial constraints.\n2.  **Crowding-Out Effect:** Agency conflicts lead managers to favor short-term financial investments over long-term real investments.\n\nThe study tests four potential channels. It finds that the two channels related to the **prevention motivation** (operational uncertainty, financial constraints) are statistically significant mediators of the policy's effect. In contrast, it finds that the two channels related to the **crowding-out effect** (information transparency, investment opportunities) are not statistically significant mediators.\n\n### Question\n\nConsidering the full set of mechanism findings—that the prevention-related channels are significant mediators while the agency-related channels are not—select all conclusions that are directly supported by this pattern of evidence.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze what conclusions are directly supported by the pattern of evidence where prevention-related channels (operational uncertainty, financial constraints) are significant mediators, while agency-related channels (information transparency, investment opportunities) are not.\n\nLet me evaluate each option:\n\n**Option A**: \"The insignificance of the investment opportunities channel as a mediator means that industrial policy does not create new real investment opportunities for supported firms.\"\n\nThis is a logical error. The insignificance of investment opportunities as a mediator only means this channel doesn't explain how policy affects financialization. It doesn't tell us whether the policy creates investment opportunities or not - it just means any such opportunities don't mediate the financialization effect.\n\n**Option B**: \"Industrial policy appears to curb financialization mainly by providing an external substitute for firms' internal need to self-insure against risk.\"\n\nThis directly follows from the findings. The significant prevention-related channels show that policy reduces financialization by addressing firms' precautionary motives (reducing operational uncertainty and easing financial constraints). This effectively substitutes for their need to hold liquid assets as self-insurance.\n\n**Option C**: \"The evidence suggests that financialization in this context is primarily a defensive, risk-management strategy for firms.\"\n\nThis is strongly supported. Since only the prevention-motivation channels are significant mediators (not the agency/crowding-out channels), it indicates that financialization is driven by defensive, precautionary motives rather than agency problems.\n\n**Option D**: \"The findings imply that industrial policy is an effective tool for improving corporate governance and reducing agency conflicts between managers and shareholders.\"\n\nThis is not supported. The agency-related channels (information transparency, investment opportunities) were not significant mediators, suggesting the policy doesn't work through reducing agency conflicts.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 381, "Question": "### Background\n\nA risk-averse insured chooses an indemnity function `I(·)` to maximize their expected utility of final wealth. The problem is characterized by belief heterogeneity, where the insured's belief `\\mathbb{P}` differs from the insurer's `\\mathbb{Q}`.\n\n---\n\n### Data / Model Specification\n\nThe set of admissible insurance contracts, `\\mathfrak{C}`, is defined to be incentive-compatible and to follow the principle of indemnity:\n  \n\\mathfrak{C} = \\{ I(x) : 0 \\le I(x) \\le x, \\text{ and both } I(x) \\text{ and } x-I(x) \\text{ are increasing functions} \\} \\quad \\text{(Eq. (1))}\n \nFor a differentiable `I(x)`, the condition that `I(x)` and `x-I(x)` are increasing is equivalent to `0 \\le I'(x) \\le 1`.\n\n---\n\n### Question\n\nAccording to the economic principles discussed in the paper, which of the following statements correctly identify the rationale for the constraints imposed on admissible insurance contracts `I(x)` in the set `\\mathfrak{C}`? Select all that apply.", "Options": {"A": "The principle of indemnity (`0 \\le I(x) \\le x`) is primarily to ensure the insurer's premium calculation is actuarially fair.", "B": "The constraint `I'(x) \\ge 0` is imposed to prevent the insured from profiting on a loss by receiving an indemnity greater than the loss itself.", "C": "The condition that `I(x)` is increasing (i.e., `I'(x) \\ge 0`) is designed to prevent the insured from having an incentive to under-report a loss.", "D": "The condition that `x - I(x)` is increasing (i.e., `I'(x) \\le 1`) is designed to prevent the insured from having an incentive to exaggerate a loss."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item assesses understanding of the foundational model constraints. It uses a 'premise packaging' strategy to test the distinct economic rationales for different parts of the incentive-compatibility conditions. Distractors are designed by swapping the rationales for the constraints (D is a swap of the rationale for `I(x) <= x`) and by introducing a true but irrelevant concept (C confuses moral hazard with premium principles).", "qid": "381", "question": "### Background\n\nA risk-averse insured chooses an indemnity function `I(·)` to maximize their expected utility of final wealth. The problem is characterized by belief heterogeneity, where the insured's belief `\\mathbb{P}` differs from the insurer's `\\mathbb{Q}`.\n\n---\n\n### Data / Model Specification\n\nThe set of admissible insurance contracts, `\\mathfrak{C}`, is defined to be incentive-compatible and to follow the principle of indemnity:\n  \n\\mathfrak{C} = \\{ I(x) : 0 \\le I(x) \\le x, \\text{ and both } I(x) \\text{ and } x-I(x) \\text{ are increasing functions} \\} \\quad \\text{(Eq. (1))}\n \nFor a differentiable `I(x)`, the condition that `I(x)` and `x-I(x)` are increasing is equivalent to `0 \\le I'(x) \\le 1`.\n\n---\n\n### Question\n\nAccording to the economic principles discussed in the paper, which of the following statements correctly identify the rationale for the constraints imposed on admissible insurance contracts `I(x)` in the set `\\mathfrak{C}`? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the constraints in the set $\\mathfrak{C}$ and their economic rationale:\n\n**Option A**: The principle of indemnity ($0 \\le I(x) \\le x$) ensures that indemnity doesn't exceed the actual loss. This is about preventing overcompensation and moral hazard, not about actuarial fairness of premiums. This is incorrect.\n\n**Option B**: The constraint $I'(x) \\ge 0$ means indemnity increases with loss size. This prevents situations where larger losses receive smaller indemnities, which could incentivize loss manipulation. However, the principle of indemnity ($I(x) \\le x$) already prevents receiving indemnity greater than the loss. This statement confuses different constraints. This is incorrect.\n\n**Option C**: If $I'(x) \\ge 0$ (indemnity is increasing), then reporting a smaller loss results in smaller indemnity. This removes the incentive to under-report losses, as doing so would reduce the payout. This is correct.\n\n**Option D**: If $I'(x) \\le 1$, then the retained loss $x - I(x)$ is increasing. This means the insured bears more of the loss as the loss size increases. If an insured exaggerates their loss from $x$ to $x'$, their retained portion increases from $x - I(x)$ to $x' - I(x')$, creating a disincentive to exaggerate. This is correct.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 399, "Question": "### Background\n\n**Research Question.** The efficiency of the S&P 500 futures market can be tested under two competing hypotheses: the simple unbiasedness hypothesis, which posits a direct cointegrating relationship between spot and futures prices, and the cost-of-carry model, which argues for a trivariate cointegrating relationship that includes a measure of financing costs. This investigation seeks to determine which model is empirically supported by analyzing the stability of the estimated relationships and the validity of theoretical parameter restrictions.\n\n**Setting / Data-Generating Environment.** A Vector Error Correction Model (VECM) framework is applied to daily S&P 500 spot prices (`s_t`), futures prices (`f_t`), and the 3-month Treasury bill rate (`r_t`) from April 1982 to June 2003. The analysis proceeds in three stages: (1) estimating a bivariate model for `(s_t, f_t)`, (2) estimating a trivariate model for `(s_t, f_t, r_t)`, and (3) testing for weak exogeneity within the preferred model to determine the locus of price discovery.\n\n### Data / Model Specification\n\nThe core of the analysis rests on the VECM representation of a system of `p` variables `X_t`:\n  \n\\Delta X_{t}=\\alpha \\beta' X_{t-1} + \\sum_{j=1}^{k-1} \\Gamma_j \\Delta X_{t-j} + \\mu + \\delta t + \\varepsilon_t \\quad \\text{(Eq. (1))}\n \nwhere `\\beta` contains the `r` long-run cointegrating vectors and `\\alpha` contains the adjustment coefficients. A variable `i` is weakly exogenous if its corresponding adjustment coefficient, `\\alpha_i`, is zero.\n\n**Table 1: Summary of Empirical Findings for Bivariate and Trivariate Models**\n\n| Feature | Bivariate Model (`s_t`, `f_t`) | Trivariate Model (`s_t`, `f_t`, `r_t`) |\n| :--- | :--- | :--- |\n| **Cointegrating Rank** | `r=1` is found (Trace stat = 44.59 > 25.47 crit.) | `r=1` is found (Trace stat = 25.08 ≈ 25.47 crit.) |\n| **Parameter Stability** | **Rejected**. The relationship is unstable over the sample. | **Not Rejected**. The relationship is stable over the sample. |\n| **Symmetry Test (`H_0: \\beta` contains `[1, -1]`)** | **Rejected**. (LR stat = 5.44, `\\chi^2(1)`). | **Not Rejected**. (LR stat = 0.27, `\\chi^2(1)`). |\n| **Weak Exogeneity of Spot Price (`H_0: \\alpha_s=0`)** | Not tested (model misspecified). | **Not Rejected**. (LR stat = 1.36, `\\chi^2(1)`). |\n| **Weak Exogeneity of Futures Price (`H_0: \\alpha_f=0`)** | Not tested (model misspecified). | **Rejected**. (LR stat = 31.47, `\\chi^2(1)`). |\n\n### Question\n\nAccording to the evidence for the Trivariate Model in **Table 1**, select all conclusions that are supported by its results.", "Options": {"A": "The model is superior because it yields a stable cointegrating relationship that is also consistent with the theoretically expected `[1, -1]` symmetry between spot and futures prices.", "B": "The weak exogeneity test results imply a 'leader-follower' dynamic where the spot market incorporates new information first, and the futures market adjusts to restore equilibrium.", "C": "The failure to reject the weak exogeneity of both the spot price and the futures price implies that price discovery occurs simultaneously in both markets.", "D": "The futures price is found to be weakly exogenous, indicating it is the primary source of new information and drives the price discovery process."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the evidence supporting the preferred model and interpret its causal implications. It uses the **Premise/Assumption Packaging** strategy, asking for all valid conclusions from the trivariate analysis. Options A and B correctly summarize the model's success and the price discovery finding. Distractor C is a **Conceptual Opposite**, reversing the roles of the spot and futures markets. Distractor D is a factual error, as the weak exogeneity of the futures price is strongly rejected, making the premise of the statement false.", "qid": "399", "question": "### Background\n\n**Research Question.** The efficiency of the S&P 500 futures market can be tested under two competing hypotheses: the simple unbiasedness hypothesis, which posits a direct cointegrating relationship between spot and futures prices, and the cost-of-carry model, which argues for a trivariate cointegrating relationship that includes a measure of financing costs. This investigation seeks to determine which model is empirically supported by analyzing the stability of the estimated relationships and the validity of theoretical parameter restrictions.\n\n**Setting / Data-Generating Environment.** A Vector Error Correction Model (VECM) framework is applied to daily S&P 500 spot prices (`s_t`), futures prices (`f_t`), and the 3-month Treasury bill rate (`r_t`) from April 1982 to June 2003. The analysis proceeds in three stages: (1) estimating a bivariate model for `(s_t, f_t)`, (2) estimating a trivariate model for `(s_t, f_t, r_t)`, and (3) testing for weak exogeneity within the preferred model to determine the locus of price discovery.\n\n### Data / Model Specification\n\nThe core of the analysis rests on the VECM representation of a system of `p` variables `X_t`:\n  \n\\Delta X_{t}=\\alpha \\beta' X_{t-1} + \\sum_{j=1}^{k-1} \\Gamma_j \\Delta X_{t-j} + \\mu + \\delta t + \\varepsilon_t \\quad \\text{(Eq. (1))}\n \nwhere `\\beta` contains the `r` long-run cointegrating vectors and `\\alpha` contains the adjustment coefficients. A variable `i` is weakly exogenous if its corresponding adjustment coefficient, `\\alpha_i`, is zero.\n\n**Table 1: Summary of Empirical Findings for Bivariate and Trivariate Models**\n\n| Feature | Bivariate Model (`s_t`, `f_t`) | Trivariate Model (`s_t`, `f_t`, `r_t`) |\n| :--- | :--- | :--- |\n| **Cointegrating Rank** | `r=1` is found (Trace stat = 44.59 > 25.47 crit.) | `r=1` is found (Trace stat = 25.08 ≈ 25.47 crit.) |\n| **Parameter Stability** | **Rejected**. The relationship is unstable over the sample. | **Not Rejected**. The relationship is stable over the sample. |\n| **Symmetry Test (`H_0: \\beta` contains `[1, -1]`)** | **Rejected**. (LR stat = 5.44, `\\chi^2(1)`). | **Not Rejected**. (LR stat = 0.27, `\\chi^2(1)`). |\n| **Weak Exogeneity of Spot Price (`H_0: \\alpha_s=0`)** | Not tested (model misspecified). | **Not Rejected**. (LR stat = 1.36, `\\chi^2(1)`). |\n| **Weak Exogeneity of Futures Price (`H_0: \\alpha_f=0`)** | Not tested (model misspecified). | **Rejected**. (LR stat = 31.47, `\\chi^2(1)`). |\n\n### Question\n\nAccording to the evidence for the Trivariate Model in **Table 1**, select all conclusions that are supported by its results.", "model_evaluation_result": {"model_raw_response": "Looking at the evidence for the Trivariate Model in Table 1:\n\n1. **Model Stability and Symmetry**: The trivariate model shows parameter stability (not rejected) and the symmetry test for β containing [1, -1] is not rejected (LR stat = 0.27). This supports option A.\n\n2. **Weak Exogeneity Tests**:\n   - Spot price (s_t): H₀: αₛ=0 is **not rejected** (LR stat = 1.36)\n   - Futures price (f_t): H₀: αf=0 is **rejected** (LR stat = 31.47)\n\nThis means:\n- The spot price IS weakly exogenous (αₛ=0)\n- The futures price is NOT weakly exogenous (αf≠0)\n\n3. **Interpretation**: When a variable is weakly exogenous (αᵢ=0), it doesn't adjust to restore equilibrium - it leads. When a variable is not weakly exogenous (αᵢ≠0), it adjusts to deviations from equilibrium - it follows.\n\nTherefore:\n- Option A is correct (stable relationship with correct symmetry)\n- Option B is correct (spot leads, futures follows)\n- Option C is incorrect (only spot is weakly exogenous, not both)\n- Option D is incorrect (futures is NOT weakly exogenous)\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 349, "Question": "### Background\n\n**Research Question.** This case investigates the paper's central empirical claim: that a country's underlying \"financial structure\" determines its average inflation rate. The analysis compares country-specific estimates of inflation-tax capacity and welfare costs. A key caveat discussed in the paper is that governments might engage in \"financial repression\" (e.g., capital controls) to deliberately make money demand less elastic, thereby enhancing their inflation-tax capacity.\n\n**Variables & Parameters.**\n- `π*`: The estimated revenue-maximizing inflation rate.\n- `S*`: The estimated maximum seigniorage revenue as a share of GDP.\n- `Ω(μ)`: The marginal welfare cost of seigniorage (`dW/dS`).\n- `α`: The semi-elasticity of currency demand with respect to inflation.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Basic Laffer Surface Calculations (Excerpt for select countries)**\n\n| Country   | `π*`   | `S*`   | `Ω(μ)` |\n| :-------- | :----- | :----- | :--------- |\n| Argentina | 2.3166 | 0.1723 | 0.4959     |\n| Japan     | 0.3562 | 0.0137 | 0.1580     |\n\n---\n\n### Question\n\nConsidering the country-specific data in **Table 1** and the paper's discussion of financial repression, select all correct interpretations.", "Options": {"A": "The paper notes that if governments actively engage in financial repression to make money demand less elastic (lower `α`), the calculated shoe leather costs would represent an underestimate of the true total social cost of raising revenue.", "B": "The marginal welfare cost (`Ω(μ)`) in Argentina (0.4959) is lower than in Japan (0.1580), indicating that Argentina can raise inflation tax revenue more efficiently.", "C": "Argentina's estimated revenue-maximizing inflation rate (`π*` = 2.3166) is more than six times that of Japan (`π*` = 0.3562), which is consistent with the financial structure hypothesis explaining their divergent inflation histories.", "D": "The data show that Japan's maximum potential seigniorage as a share of GDP (`S*`) is significantly higher than Argentina's, reflecting a more developed financial system."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the application of the paper's theory to specific country data and assesses understanding of a key theoretical caveat (financial repression). It uses an Atomic Decomposition strategy to create two correct statements, one empirical and one conceptual. Distractors are based on a Numerical Error/Conceptual Opposite (C) and a direct data misreading (D).", "qid": "349", "question": "### Background\n\n**Research Question.** This case investigates the paper's central empirical claim: that a country's underlying \"financial structure\" determines its average inflation rate. The analysis compares country-specific estimates of inflation-tax capacity and welfare costs. A key caveat discussed in the paper is that governments might engage in \"financial repression\" (e.g., capital controls) to deliberately make money demand less elastic, thereby enhancing their inflation-tax capacity.\n\n**Variables & Parameters.**\n- `π*`: The estimated revenue-maximizing inflation rate.\n- `S*`: The estimated maximum seigniorage revenue as a share of GDP.\n- `Ω(μ)`: The marginal welfare cost of seigniorage (`dW/dS`).\n- `α`: The semi-elasticity of currency demand with respect to inflation.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Basic Laffer Surface Calculations (Excerpt for select countries)**\n\n| Country   | `π*`   | `S*`   | `Ω(μ)` |\n| :-------- | :----- | :----- | :--------- |\n| Argentina | 2.3166 | 0.1723 | 0.4959     |\n| Japan     | 0.3562 | 0.0137 | 0.1580     |\n\n---\n\n### Question\n\nConsidering the country-specific data in **Table 1** and the paper's discussion of financial repression, select all correct interpretations.", "model_evaluation_result": {"model_raw_response": "Looking at the data and analyzing each option:\n\nA) This is correct. Financial repression makes money demand less elastic (lower α), which increases the government's ability to extract inflation tax revenue. However, the calculated shoe leather costs only capture the welfare loss from reduced money holdings. Financial repression creates additional distortions and welfare losses (like capital misallocation) that aren't captured in the standard shoe leather cost calculation, making it an underestimate of total social costs.\n\nB) This is incorrect. The marginal welfare cost Ω(μ) for Argentina (0.4959) is HIGHER than Japan's (0.1580), not lower. A higher marginal welfare cost means Argentina faces greater welfare losses per unit of seigniorage revenue, indicating LESS efficient revenue raising, not more.\n\nC) This is correct. Argentina's π* (2.3166) is indeed more than 6 times Japan's π* (0.3562). This large difference in revenue-maximizing inflation rates is consistent with the financial structure hypothesis - countries with less developed financial systems (like Argentina) have higher optimal inflation rates for revenue extraction, which helps explain their historically higher inflation.\n\nD) This is incorrect. The data shows Japan's S* (0.0137) is LOWER than Argentina's S* (0.1723), not higher. This actually supports the financial structure hypothesis - Japan's more developed financial system provides better money substitutes, reducing the government's ability to extract seigniorage revenue.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 222, "Question": "### Background\n\n**Research Question.** How can dynamic panel GMM estimators be used to obtain consistent estimates in corporate finance settings when regressors are endogenous and persistent, and how are the validity of these estimators tested?\n\n**Setting.** A dynamic panel data model is used to analyze bank behavior. The model includes potentially endogenous regressors like a corporate governance index (`CGI`) and accounts for unobserved bank-specific effects. The paper uses a two-step system GMM estimator as a robustness check for its main findings.\n\n### Data / Model Specification\n\nConsider the canonical dynamic panel model:\n\n  \ny_{it} = \\beta' x_{it} + \\mu_i + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nTo address endogeneity, the system GMM estimator uses lagged levels of variables as instruments for the first-differenced equation and lagged differences as instruments for the levels equation.\n\n**Table 1: GMM Diagnostic Statistics (from paper's Table 4, Column 6)**\n| Test | Null Hypothesis | Reported p-value |\n| :--- | :--- | :---: |\n| AR(2) Test | No second-order serial correlation in differenced residuals | Insignificant* |\n| Hansen J-Test | Instruments are jointly valid (exogenous) | 0.224 |\n*Notes: The paper states the AR(2) test is insignificant without reporting the exact p-value.*\n\n### Question\n\nThe authors use a two-step system GMM estimator as a robustness check. Based on the diagnostic statistics provided in **Table 1**, select all of the following statements that represent a correct interpretation of the tests and their implications.", "Options": {"A": "The insignificant AR(2) test is a necessary condition for the validity of instruments lagged two or more periods, as it supports the core assumption that the original idiosyncratic errors are not serially correlated.", "B": "The Hansen J-test p-value of 0.224 provides strong evidence that the corporate governance index (`CGI`) is exogenous.", "C": "The Hansen J-test p-value of 0.224 indicates that the null hypothesis of joint instrument validity cannot be rejected, supporting the overall model specification.", "D": "A significant AR(2) test (p < 0.05) would be required to confirm the validity of the GMM model's instruments."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the user's understanding of the specification tests for dynamic panel GMM estimators, a critical part of evaluating empirical research. The strategy is Atomic Decomposition of diagnostic interpretations. Correct options (A, B) require a precise understanding of the null hypotheses and purpose of the AR(2) and Hansen tests. Distractor C is a Conceptual Opposite regarding the AR(2) test's interpretation. Distractor D reflects a subtle but common confusion between instrument validity and regressor exogeneity (Conceptual Confusion).", "qid": "222", "question": "### Background\n\n**Research Question.** How can dynamic panel GMM estimators be used to obtain consistent estimates in corporate finance settings when regressors are endogenous and persistent, and how are the validity of these estimators tested?\n\n**Setting.** A dynamic panel data model is used to analyze bank behavior. The model includes potentially endogenous regressors like a corporate governance index (`CGI`) and accounts for unobserved bank-specific effects. The paper uses a two-step system GMM estimator as a robustness check for its main findings.\n\n### Data / Model Specification\n\nConsider the canonical dynamic panel model:\n\n  \ny_{it} = \\beta' x_{it} + \\mu_i + \\varepsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nTo address endogeneity, the system GMM estimator uses lagged levels of variables as instruments for the first-differenced equation and lagged differences as instruments for the levels equation.\n\n**Table 1: GMM Diagnostic Statistics (from paper's Table 4, Column 6)**\n| Test | Null Hypothesis | Reported p-value |\n| :--- | :--- | :---: |\n| AR(2) Test | No second-order serial correlation in differenced residuals | Insignificant* |\n| Hansen J-Test | Instruments are jointly valid (exogenous) | 0.224 |\n*Notes: The paper states the AR(2) test is insignificant without reporting the exact p-value.*\n\n### Question\n\nThe authors use a two-step system GMM estimator as a robustness check. Based on the diagnostic statistics provided in **Table 1**, select all of the following statements that represent a correct interpretation of the tests and their implications.", "model_evaluation_result": {"model_raw_response": "Looking at the diagnostic statistics provided, I need to evaluate each statement about the GMM estimator tests.\n\n**Statement A**: The insignificant AR(2) test is indeed a necessary condition for the validity of instruments lagged two or more periods. In GMM estimation, if the original errors are not serially correlated, then the first-differenced errors should exhibit first-order serial correlation (AR(1)) but not second-order serial correlation (AR(2)). An insignificant AR(2) test supports this assumption, validating the use of instruments lagged two or more periods. This statement is **correct**.\n\n**Statement B**: The Hansen J-test with p-value 0.224 tests the joint validity of all instruments, not the exogeneity of any specific variable like CGI. The test cannot isolate whether individual regressors are exogenous. This statement is **incorrect**.\n\n**Statement C**: The Hansen J-test p-value of 0.224 (> 0.05) means we fail to reject the null hypothesis of joint instrument validity at conventional significance levels. This supports the overall model specification and the validity of the instrument set. This statement is **correct**.\n\n**Statement D**: This statement has the logic backwards. We want an *insignificant* AR(2) test (p > 0.05), not a significant one. A significant AR(2) test would indicate second-order serial correlation in the differenced residuals, which would invalidate instruments lagged two or more periods. This statement is **incorrect**.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 384, "Question": "### Background\n\n**Research Question.** How can the causal effect of foreign official inflows on U.S. Treasury yields be estimated when these inflows are endogenous to market conditions, and are the findings robust across different measures of risk premium?\n\n**Setting.** A two-stage least squares (2SLS) regression framework is used to estimate the short-run causal effect of foreign official inflows on two different measures of the bond risk premium: the model-based 5-year term premium (`\\Delta TP_t^5`) and the model-free realized one-year excess return (`XR_6t`). A key challenge is that unobserved “flight-to-safety” episodes can simultaneously lower Treasury yields and reduce foreign official inflows, biasing ordinary least squares (OLS) estimates.\n\n### Data / Model Specification\n\nThe general regression model is:\n\n  \n\\text{Risk Premium}_t = \\alpha + \\beta_1 (\\text{Foreign Inflows}_t) + \\mathbf{X}_t'\\gamma + u_t\n \n\nwhere `\\mathbf{X}_t` is a vector of exogenous controls. To address the endogeneity of `\\text{Foreign Inflows}_t`, instruments such as Japanese foreign exchange interventions (`JPYFXINT_t`) are used.\n\n**Table 1. The Impact of Foreign Official Inflows on U.S. Bond Risk Premia**\n\n| | (1) OLS | (2) IV / 2SLS | (3) IV / 2SLS | (4) IV / 2SLS |\n| :--- | :---: | :---: | :---: | :---: |\n| **Dependent Variable:** | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `XR_6t` |\n| | | | | |\n| `\\Delta FOIL/DEBT_{t-1}` | 0.052 | -0.135** | | |\n| (Monthly Inflows) | (0.030) | (0.061) | | |\n| | | | | |\n| `\\Delta FOIL_JAPAN/DEBT_{t-1}` | | | -0.147** | |\n| (Japanese Monthly Inflows) | | | (0.059) | | |\n| | | | | |\n| `\\sum_{12} FOI_t / DEBT_{t-12}` | | | | 0.595*** |\n| (Annual Inflows) | | | | (0.184) |\n| | | | | |\n| **Specification Tests** | | | | |\n| Cragg-Donald F-Stat | | 15.72 | 97.59 | 92.79 |\n| *Weak Inst. Critical Value* | | *11.59* | *8.96* | *11.59* |\n| Hansen J Test (p-value) | | 0.3498 | n.a. | 0.3563 |\n\n*Notes: Standard errors in parentheses. *p<0.1, **p<0.05, ***p<0.01. `\\Delta TP_t^5` is the monthly change in the 5-year term premium. `XR_6t` is the one-year realized excess return. `FOIL/DEBT` denotes foreign official inflows scaled by outstanding marketable debt.*\n\n### Question\n\nThe study uses several methods to validate its main finding. Based on the results in Table 1, select all statements that accurately describe these validation and robustness checks.", "Options": {"A": "The result in Column (4) validates the main finding using a model-free measure of risk premium (realized excess returns), showing that foreign inflows have a statistically significant effect that is directionally consistent with the term premium analysis.", "B": "The positive coefficient (0.595) in the excess returns regression (Column 4) contradicts the negative coefficient (-0.135) in the term premium regression (Column 2), suggesting the paper's main conclusion is not robust.", "C": "The analysis of Japanese inflows in Column (3) serves as a strong robustness check because its result (-0.147) is quantitatively similar to the main estimate (-0.135) and is identified by an exceptionally strong instrument (F-stat ≈ 98).", "D": "The coefficient of 0.595 in Column (4) is much larger in absolute value than the coefficient in Column (2), indicating that the economic impact of foreign inflows on realized excess returns is far greater than the impact on the term premium."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the ability to synthesize results from multiple robustness checks, converted from a Table QA problem. It uses an **Atomic Decomposition** strategy, transforming parts 3 and 4 of the original QA into testable propositions. **Correct options (A, B)** require understanding why the Japan-specific result is a powerful check and why the excess return regression provides model-free validation. **Distractor C (Magnitude Misinterpretation)** tempts a superficial comparison of coefficients that have different units and meanings. **Distractor D (Conceptual Error)** targets a critical misunderstanding: a positive effect on excess returns is mechanically linked to a negative effect on yields, so the results are consistent, not contradictory.", "qid": "384", "question": "### Background\n\n**Research Question.** How can the causal effect of foreign official inflows on U.S. Treasury yields be estimated when these inflows are endogenous to market conditions, and are the findings robust across different measures of risk premium?\n\n**Setting.** A two-stage least squares (2SLS) regression framework is used to estimate the short-run causal effect of foreign official inflows on two different measures of the bond risk premium: the model-based 5-year term premium (`\\Delta TP_t^5`) and the model-free realized one-year excess return (`XR_6t`). A key challenge is that unobserved “flight-to-safety” episodes can simultaneously lower Treasury yields and reduce foreign official inflows, biasing ordinary least squares (OLS) estimates.\n\n### Data / Model Specification\n\nThe general regression model is:\n\n  \n\\text{Risk Premium}_t = \\alpha + \\beta_1 (\\text{Foreign Inflows}_t) + \\mathbf{X}_t'\\gamma + u_t\n \n\nwhere `\\mathbf{X}_t` is a vector of exogenous controls. To address the endogeneity of `\\text{Foreign Inflows}_t`, instruments such as Japanese foreign exchange interventions (`JPYFXINT_t`) are used.\n\n**Table 1. The Impact of Foreign Official Inflows on U.S. Bond Risk Premia**\n\n| | (1) OLS | (2) IV / 2SLS | (3) IV / 2SLS | (4) IV / 2SLS |\n| :--- | :---: | :---: | :---: | :---: |\n| **Dependent Variable:** | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `\\Delta TP_t^5` | `XR_6t` |\n| | | | | |\n| `\\Delta FOIL/DEBT_{t-1}` | 0.052 | -0.135** | | |\n| (Monthly Inflows) | (0.030) | (0.061) | | |\n| | | | | |\n| `\\Delta FOIL_JAPAN/DEBT_{t-1}` | | | -0.147** | |\n| (Japanese Monthly Inflows) | | | (0.059) | | |\n| | | | | |\n| `\\sum_{12} FOI_t / DEBT_{t-12}` | | | | 0.595*** |\n| (Annual Inflows) | | | | (0.184) |\n| | | | | |\n| **Specification Tests** | | | | |\n| Cragg-Donald F-Stat | | 15.72 | 97.59 | 92.79 |\n| *Weak Inst. Critical Value* | | *11.59* | *8.96* | *11.59* |\n| Hansen J Test (p-value) | | 0.3498 | n.a. | 0.3563 |\n\n*Notes: Standard errors in parentheses. *p<0.1, **p<0.05, ***p<0.01. `\\Delta TP_t^5` is the monthly change in the 5-year term premium. `XR_6t` is the one-year realized excess return. `FOIL/DEBT` denotes foreign official inflows scaled by outstanding marketable debt.*\n\n### Question\n\nThe study uses several methods to validate its main finding. Based on the results in Table 1, select all statements that accurately describe these validation and robustness checks.", "model_evaluation_result": {"model_raw_response": "Looking at this table and the validation methods used:\n\n**Column (4) Analysis:**\n- Uses realized excess returns (XR_6t) as an alternative, model-free measure of risk premium\n- Shows coefficient of 0.595*** (highly significant)\n- This is directionally consistent with Column (2)'s -0.135** because:\n  - Higher foreign inflows → lower term premium (Column 2)\n  - Higher foreign inflows → higher excess returns (Column 4)\n  - Both indicate that foreign inflows reduce risk premia/increase bond prices\n- Therefore, option A is correct\n\n**Option B Assessment:**\n- The positive coefficient in Column (4) does NOT contradict the negative coefficient in Column (2)\n- They measure different but related concepts that should move in opposite directions\n- Option B is incorrect\n\n**Column (3) Analysis:**\n- Uses Japanese inflows specifically\n- Coefficient is -0.147** vs -0.135** in Column (2) - very similar magnitudes\n- F-statistic of 97.59 indicates an exceptionally strong instrument\n- This provides a strong robustness check\n- Option C is correct\n\n**Comparing Magnitudes:**\n- Column (2): Monthly change in term premium\n- Column (4): Annual (one-year) excess returns\n- These are measured over different time horizons and in different units\n- Cannot directly compare the absolute values of 0.595 vs 0.135\n- Option D incorrectly compares coefficients with different units/scales\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 358, "Question": "### Background\n\n**Research Question.** How do investment banks' valuations respond to deregulation events that simultaneously increase competition and the likelihood of being an acquisition target?\n\n**Setting and Sample.** An event study of U.S. investment banks. The paper posits two competing forces: a negative 'competition effect' as commercial banks enter their market, and a positive 'acquisition target effect' as those same commercial banks become potential buyers. The paper also notes that rumors preceded some announcements, suggesting potential information leakage.\n\n### Data / Model Specification\n\n**Table 1. Two-Day Cumulative Abnormal Returns `CAR(-1,0)` for Investment Banks**\n\n| Event | Description                       | `CAR(-1,0)` (%) | t-statistic |\n| :---- | :-------------------------------- | :-------------- | :---------- |\n| 2     | Increasing Revenue Limits         | 0.53            | (1.07)      |\n| 3     | Bankers Trust Acquires Alex Brown | 3.89            | (7.93)**    |\n| 4     | FSMA Passage                      | 6.18            | (4.19)**    |\n\n*Note: Adapted from Table 3 of the source paper. ** indicates significance at the 0.01 level.*\n\n---\n\n### Question\n\nBased on the paper's hypotheses and the results in **Table 1**, select all statements that represent a valid conclusion.", "Options": {"A": "The highly significant positive returns for Events 3 and 4 suggest that for major, transformative announcements, the market perceived the 'acquisition target effect' to be much stronger than the negative 'competition effect' for investment banks.", "B": "The paper's observation of pre-announcement rumors for Event 3 implies that the reported `CAR(-1,0)` of 3.89% may understate the total valuation impact, as some price adjustment could have occurred before day -1.", "C": "The results indicate that the 'competition effect' from commercial banks entering their market consistently dominated any potential benefits for investment banks across all events.", "D": "The insignificant result for Event 2 proves that increasing revenue limits for commercial banks had no economic impact whatsoever on the investment banking sector."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize empirical results with competing theoretical hypotheses and understand the methodological implications of information leakage. It uses the 'Atomic Decomposition' strategy. Option A tests the core synthesis of the competing effects theory with the data. Option B tests the understanding of how information leakage affects event study interpretation. Distractor C is a 'Conceptual Opposite' of the paper's main finding. Distractor D represents a common but incorrect interpretation of statistical insignificance ('absence of evidence is not evidence of absence').", "qid": "358", "question": "### Background\n\n**Research Question.** How do investment banks' valuations respond to deregulation events that simultaneously increase competition and the likelihood of being an acquisition target?\n\n**Setting and Sample.** An event study of U.S. investment banks. The paper posits two competing forces: a negative 'competition effect' as commercial banks enter their market, and a positive 'acquisition target effect' as those same commercial banks become potential buyers. The paper also notes that rumors preceded some announcements, suggesting potential information leakage.\n\n### Data / Model Specification\n\n**Table 1. Two-Day Cumulative Abnormal Returns `CAR(-1,0)` for Investment Banks**\n\n| Event | Description                       | `CAR(-1,0)` (%) | t-statistic |\n| :---- | :-------------------------------- | :-------------- | :---------- |\n| 2     | Increasing Revenue Limits         | 0.53            | (1.07)      |\n| 3     | Bankers Trust Acquires Alex Brown | 3.89            | (7.93)**    |\n| 4     | FSMA Passage                      | 6.18            | (4.19)**    |\n\n*Note: Adapted from Table 3 of the source paper. ** indicates significance at the 0.01 level.*\n\n---\n\n### Question\n\nBased on the paper's hypotheses and the results in **Table 1**, select all statements that represent a valid conclusion.", "model_evaluation_result": {"model_raw_response": "Looking at the data and the paper's hypotheses about competing effects (negative competition effect vs. positive acquisition target effect), let me evaluate each statement:\n\nA) **Valid**: Events 3 and 4 show highly significant positive returns (3.89% and 6.18% respectively, both significant at 0.01 level). This suggests the acquisition target effect (positive) outweighed the competition effect (negative) for these major events.\n\nB) **Valid**: The paper notes rumors preceded some announcements. If information leaked before day -1 for Event 3, some price adjustment would have already occurred, meaning the true total impact would be larger than the measured CAR(-1,0) of 3.89%.\n\nC) **Invalid**: This contradicts the data. Events 3 and 4 show significant positive returns, indicating the acquisition target effect dominated, not the competition effect.\n\nD) **Invalid**: Statistical insignificance doesn't prove \"no economic impact whatsoever.\" It only means we cannot reject the null hypothesis at conventional significance levels. The effect could still exist but be too small to detect statistically, or there could be offsetting effects.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 294, "Question": "### Background\n\n**Research Question.** How can a tail risk bound be constructed for aggregate claims when the severity distribution is heavy-tailed, possessing only a finite number of moments and no moment generating function?\n\n**Setting / Data-Generating Environment.** We consider a collective risk model where the claim size distribution `F(y)` is heavy-tailed. The paper develops a Pareto-type bound.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `E[X]`: The mean of aggregate claims.\n- `E[Y]`: The mean of individual claim sizes.\n- `E[N]`: The mean number of claims.\n- `\\phi`: A parameter of the claim frequency distribution.\n\n---\n\n### Data / Model Specification\n\nThe general Pareto bound with `r=1` is:\n  \n\\overline{G}(x) \\leq \\frac{(1-p_{0})E[Y]}{\\phi E[Y] + (1-\\phi)x} \\quad \\text{(Eq. (1))}\n \nFor the special case of a compound geometric distribution (`p_n=(1-\\phi)\\phi^n`), this simplifies to:\n  \n\\overline{G}(x) \\leq \\frac{E[X]}{E[X]+x}, \\quad x \\geq 0 \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nIn the derivation that simplifies the general Pareto bound in **Eq. (1)** to the specific bound for a compound geometric distribution in **Eq. (2)**, which of the following statements represent **INCORRECT** steps, assumptions, or intermediate formulas?", "Options": {"A": "After substituting `p_0 = 1-\\phi`, the bound in Eq. (1) simplifies to `\\frac{\\phi E[Y]}{\\phi E[Y] + (1-\\phi)E[X]}`.", "B": "A key property of the compound geometric distribution is that `p_0 = 1-\\phi`.", "C": "For a compound geometric distribution, the mean number of claims is `E[N] = \\phi`.", "D": "A key relationship used in the derivation is Wald's identity, `E[X] = E[N]E[Y]`."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the user's ability to follow a multi-step algebraic derivation by identifying flawed steps. It uses a Complement-set Selection strategy. The correct answers are the incorrect statements (A, B). Statement (A) provides an incorrect formula for the mean of a geometric distribution (Formula Misuse). Statement (B) contains an algebraic error in the denominator during an intermediate step (Step-Omission/Algebraic Error). The distractors (C, D) are correct and necessary premises for the derivation, forcing the user to check the full logical chain.", "qid": "294", "question": "### Background\n\n**Research Question.** How can a tail risk bound be constructed for aggregate claims when the severity distribution is heavy-tailed, possessing only a finite number of moments and no moment generating function?\n\n**Setting / Data-Generating Environment.** We consider a collective risk model where the claim size distribution `F(y)` is heavy-tailed. The paper develops a Pareto-type bound.\n\n**Variables & Parameters.**\n- `\\overline{G}(x)`: The tail probability of aggregate claims.\n- `E[X]`: The mean of aggregate claims.\n- `E[Y]`: The mean of individual claim sizes.\n- `E[N]`: The mean number of claims.\n- `\\phi`: A parameter of the claim frequency distribution.\n\n---\n\n### Data / Model Specification\n\nThe general Pareto bound with `r=1` is:\n  \n\\overline{G}(x) \\leq \\frac{(1-p_{0})E[Y]}{\\phi E[Y] + (1-\\phi)x} \\quad \\text{(Eq. (1))}\n \nFor the special case of a compound geometric distribution (`p_n=(1-\\phi)\\phi^n`), this simplifies to:\n  \n\\overline{G}(x) \\leq \\frac{E[X]}{E[X]+x}, \\quad x \\geq 0 \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nIn the derivation that simplifies the general Pareto bound in **Eq. (1)** to the specific bound for a compound geometric distribution in **Eq. (2)**, which of the following statements represent **INCORRECT** steps, assumptions, or intermediate formulas?", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement to determine which ones are INCORRECT in the derivation from Eq. (1) to Eq. (2).\n\nLet me start by understanding the compound geometric distribution and the derivation:\n\nFor a compound geometric distribution with `p_n = (1-φ)φ^n`:\n- `p_0 = (1-φ)φ^0 = 1-φ` ✓\n- The mean number of claims: `E[N] = Σ(n=1 to ∞) n·p_n = Σ(n=1 to ∞) n·(1-φ)φ^n = (1-φ)φ/(1-φ)² = φ/(1-φ)`\n\nNow let me check each option:\n\n**Option A**: After substituting `p_0 = 1-φ`, the bound in Eq. (1) becomes:\n`\\overline{G}(x) ≤ (1-(1-φ))E[Y]/(φE[Y] + (1-φ)x) = φE[Y]/(φE[Y] + (1-φ)x)`\n\nThe statement claims it simplifies to `φE[Y]/(φE[Y] + (1-φ)E[X])`. This replaces `x` with `E[X]`, which is incorrect. The variable `x` should remain as is.\n\n**Option B**: For compound geometric, `p_0 = 1-φ` is correct ✓\n\n**Option C**: States `E[N] = φ`. But we calculated `E[N] = φ/(1-φ)`, not `φ`. This is incorrect.\n\n**Option D**: Wald's identity states `E[X] = E[N]E[Y]`. This is a correct relationship used in such derivations ✓\n\nTo verify the final derivation to Eq. (2):\n- Using `E[N] = φ/(1-φ)` and Wald's identity: `E[X] = E[N]E[Y] = φE[Y]/(1-φ", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 344, "Question": "### Background\n\n**Research Question.** This case assesses the economic significance of return predictability by evaluating the performance of trading strategies based on moving average signals.\n\n**Setting and Sample.** The out-of-sample performance of dynamic trading strategies is evaluated for a mean-variance investor. Performance is measured by the Certainty Equivalent Return (CER) gain relative to a benchmark.\n\n### Data / Model Specification\n\nThe Certainty Equivalent Return (CER) for a portfolio `p` is the risk-free return that provides an investor with the same utility as the risky portfolio:\n  \nCER_p = \\hat{\\mu}_p - \\frac{\\gamma}{2} \\hat{\\sigma}_p^2 \\quad \\text{(Eq. 1)}\n \nwhere `\\hat{\\mu}_p` is the average return, `\\hat{\\sigma}_p^2` is the return variance, and `γ` is the risk aversion coefficient. The economic value of a model is measured by the CER gain, `Δ(cer) = CER_{model} - CER_{benchmark}`.\n\n**Table 1: Economic Evaluation of Predictability (Selected Results)**\n\n| Strategy | `Δ(cer)` (%) (No cost) | `Δ(cer)` (%) (50bps cost) |\n|:---|---:|---:|\n| MADP(3) | 1.64 | 1.39 |\n| PC-long | 0.80 | 0.45 |\n\n*Note: `PC-long` is a strategy based on the first principal component of long-term (20-200 day) MADP predictors.*\n\n### Question\n\nBased on the definition of Certainty Equivalent Return (CER) and the results in Table 1, select all of the following statements that are correct.", "Options": {"A": "After accounting for transaction costs, the strategy based on the short-term `MADP(3)` signal delivers more than three times the economic value (`Δ(cer)`) of the strategy based on the long-term `PC-long` signal.", "B": "The CER gain, `Δ(cer)`, provides a more complete measure of economic value than average return alone because it incorporates a penalty for portfolio variance.", "C": "A positive `Δ(cer)` indicates that the strategy's average return is higher than the benchmark, regardless of its risk.", "D": "The `PC-long` strategy loses a smaller absolute amount of its CER gain (0.35%) to transaction costs than the `MADP(3)` strategy (0.25%), implying it involves less frequent trading."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses **Atomic Decomposition** to test the conceptual understanding of CER and the ability to interpret the table's results comparatively. Option A tests the definition. Option B requires a simple calculation and comparison. Distractor C presents a plausible but incorrect inference (a larger cost drag suggests *more* trading, not less). Distractor D represents a key conceptual error, ignoring the risk-adjustment inherent in CER.", "qid": "344", "question": "### Background\n\n**Research Question.** This case assesses the economic significance of return predictability by evaluating the performance of trading strategies based on moving average signals.\n\n**Setting and Sample.** The out-of-sample performance of dynamic trading strategies is evaluated for a mean-variance investor. Performance is measured by the Certainty Equivalent Return (CER) gain relative to a benchmark.\n\n### Data / Model Specification\n\nThe Certainty Equivalent Return (CER) for a portfolio `p` is the risk-free return that provides an investor with the same utility as the risky portfolio:\n  \nCER_p = \\hat{\\mu}_p - \\frac{\\gamma}{2} \\hat{\\sigma}_p^2 \\quad \\text{(Eq. 1)}\n \nwhere `\\hat{\\mu}_p` is the average return, `\\hat{\\sigma}_p^2` is the return variance, and `γ` is the risk aversion coefficient. The economic value of a model is measured by the CER gain, `Δ(cer) = CER_{model} - CER_{benchmark}`.\n\n**Table 1: Economic Evaluation of Predictability (Selected Results)**\n\n| Strategy | `Δ(cer)` (%) (No cost) | `Δ(cer)` (%) (50bps cost) |\n|:---|---:|---:|\n| MADP(3) | 1.64 | 1.39 |\n| PC-long | 0.80 | 0.45 |\n\n*Note: `PC-long` is a strategy based on the first principal component of long-term (20-200 day) MADP predictors.*\n\n### Question\n\nBased on the definition of Certainty Equivalent Return (CER) and the results in Table 1, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the CER formula and the data provided.\n\nLet me analyze each option:\n\n**Option A**: After accounting for transaction costs, MADP(3) has Δ(cer) = 1.39% and PC-long has Δ(cer) = 0.45%. The ratio is 1.39/0.45 = 3.09, which is indeed more than three times. ✓\n\n**Option B**: The CER formula explicitly shows CER = μ̂ - (γ/2)σ̂². This incorporates both return (μ̂) and a penalty for variance (σ̂²), making it a more complete measure than average return alone. ✓\n\n**Option C**: This is incorrect. A positive Δ(cer) means CER_model > CER_benchmark, but since CER includes both return and risk components, we cannot conclude that the average return alone is higher. The model could have lower return but sufficiently lower risk to produce a higher CER.\n\n**Option D**: This is incorrect. MADP(3) loses 1.64 - 1.39 = 0.25% to transaction costs, while PC-long loses 0.80 - 0.45 = 0.35%. Since PC-long loses MORE in absolute terms (0.35% > 0.25%), this actually suggests PC-long involves MORE frequent trading, not less.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 434, "Question": "### Background\n\n**Research Question.** How do different methods for ranking underwriter reputation—by deal count versus deal value—reveal fundamental differences in the structure of US and European IPO markets?\n\n**Setting / Data-Generating Environment.** The study constructs underwriter rankings for IPOs in major European markets and the US. It compares two primary ranking methods and analyzes the correlation of these rankings across different market segments.\n\n**Variables & Parameters.**\n\n*   `Equally-weighted ranking`: A ranking based on the number of IPOs an underwriter has completed. This metric emphasizes deal frequency.\n*   `Proceeds-weighted ranking`: A ranking based on the total capital raised across all IPOs an underwriter has completed. This metric emphasizes deal size.\n*   `Rank correlation coefficient`: A statistical measure of the association between two sets of rankings. A value near 1 indicates strong agreement, while a value near 0 indicates no association.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Top Underwriters on London AIM (Panel A: Equally-weighted)**\n\n| Rank | Name | IPOs (No.) |\n| :--- | :--- | :--- |\n| 1 | Evolution Sec | 123 |\n| 2 | Collins Stewart | 90 |\n| 3 | WH Ireland | 62 |\n| ... | ... | ... |\n\n*Note: The paper states that proceeds-weighted rankings for AIM are dominated by different firms.* \n\n**Table 2: Selected Rank Correlation Coefficients**\n\n| Markets Compared | Correlation Coefficient |\n| :--- | :--- |\n| London: AIM vs. Official List | 0.170* |\n| Paris: Marché Libre vs. Premier Marché | 0.023 |\n| US: NYSE vs. NASDAQ | 0.929*** |\n\n*Significance levels: * p<0.10, ** p<0.05, *** p<0.01*\n\nThe paper notes that in the US, the top underwriters are largely the same regardless of the ranking metric used (equally- vs. proceeds-weighted) and the exchange (NYSE vs. NASDAQ).\n\n---\n\n### The Question\n\nBased on the provided data and the paper's arguments about market structure, select all statements that are correct.", "Options": {"A": "The low correlation (0.170) between AIM and Official List rankings suggests that underwriter reputation is not an important factor in the UK market, unlike in the highly correlated US market.", "B": "An underwriter's high rank on a 'proceeds-weighted' list, such as for Evolution Securities on the AIM, signifies a business model focused on a high volume of small deals.", "C": "A hypothetical regulation forcing main-market banks to co-lead large AIM IPOs would likely increase the AIM-Official List rank correlation while simultaneously strengthening the paper's core argument that a single 'integrated' reputation measure is misleading for Europe.", "D": "The stark contrast between the high NYSE-NASDAQ rank correlation (0.929) and the low AIM-Official List correlation (0.170) indicates that the US IPO underwriting market is highly integrated, while the UK market is fragmented with specialized underwriters for different segments."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). This item assesses understanding of the paper's central thesis on market fragmentation by testing interpretation of statistical evidence (rank correlations) and the logic of a hypothetical scenario. It uses an Atomic Decomposition strategy. Distractors are designed to target a direct conceptual error in ranking definitions (C) and a faulty inference from the statistical evidence (D).", "qid": "434", "question": "### Background\n\n**Research Question.** How do different methods for ranking underwriter reputation—by deal count versus deal value—reveal fundamental differences in the structure of US and European IPO markets?\n\n**Setting / Data-Generating Environment.** The study constructs underwriter rankings for IPOs in major European markets and the US. It compares two primary ranking methods and analyzes the correlation of these rankings across different market segments.\n\n**Variables & Parameters.**\n\n*   `Equally-weighted ranking`: A ranking based on the number of IPOs an underwriter has completed. This metric emphasizes deal frequency.\n*   `Proceeds-weighted ranking`: A ranking based on the total capital raised across all IPOs an underwriter has completed. This metric emphasizes deal size.\n*   `Rank correlation coefficient`: A statistical measure of the association between two sets of rankings. A value near 1 indicates strong agreement, while a value near 0 indicates no association.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Top Underwriters on London AIM (Panel A: Equally-weighted)**\n\n| Rank | Name | IPOs (No.) |\n| :--- | :--- | :--- |\n| 1 | Evolution Sec | 123 |\n| 2 | Collins Stewart | 90 |\n| 3 | WH Ireland | 62 |\n| ... | ... | ... |\n\n*Note: The paper states that proceeds-weighted rankings for AIM are dominated by different firms.* \n\n**Table 2: Selected Rank Correlation Coefficients**\n\n| Markets Compared | Correlation Coefficient |\n| :--- | :--- |\n| London: AIM vs. Official List | 0.170* |\n| Paris: Marché Libre vs. Premier Marché | 0.023 |\n| US: NYSE vs. NASDAQ | 0.929*** |\n\n*Significance levels: * p<0.10, ** p<0.05, *** p<0.01*\n\nThe paper notes that in the US, the top underwriters are largely the same regardless of the ranking metric used (equally- vs. proceeds-weighted) and the exchange (NYSE vs. NASDAQ).\n\n---\n\n### The Question\n\nBased on the provided data and the paper's arguments about market structure, select all statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the data and analyzing each statement:\n\nA) This is incorrect. The low correlation doesn't mean reputation is unimportant - it means underwriter reputation is segment-specific in the UK. Different underwriters specialize in different market segments (AIM vs Official List), unlike the US where the same underwriters dominate across exchanges.\n\nB) This is incorrect. Evolution Securities ranks #1 on the equally-weighted (deal count) list for AIM, not the proceeds-weighted list. The note specifically states that proceeds-weighted rankings are \"dominated by different firms,\" meaning Evolution likely ranks lower on that metric. Their high equally-weighted rank indicates many small deals, not large ones.\n\nC) This is correct. If main-market banks were forced to co-lead large AIM IPOs, they would appear on both AIM and Official List rankings, increasing the correlation between the two. However, this would be an artificial integration that actually reinforces the paper's argument - it would mask the underlying market fragmentation and make a single reputation measure even more misleading for Europe.\n\nD) This is correct. The correlation data clearly shows:\n- US: 0.929 correlation between NYSE and NASDAQ = integrated market with same underwriters across exchanges\n- UK: 0.170 correlation between AIM and Official List = fragmented market with specialized underwriters for each segment\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 145, "Question": "### Background\n\n**Research Question.** What is the valuation effect of security issuance announcements by Bank Holding Companies (BHCs), and what do these effects imply about the competing hypotheses regarding information asymmetry and bank capital adequacy?\n\n**Setting.** The study analyzes the stock market's reaction to announcements of common stock and straight debt offerings by BHCs. The findings are evaluated in light of two competing theories.\n\n**Hypotheses.**\n1.  **Regulation/Asymmetry Reduction Hypothesis:** Predicts a negative announcement effect for equity, but one that is less severe than for industrial firms due to regulatory oversight reducing information asymmetry. The effect for straight debt is expected to be insignificant, similar to industrial firms.\n2.  **Bank Capital Hypothesis:** Predicts a significant negative announcement effect for both equity and subordinated debt (which can qualify as regulatory capital), possibly even larger than for industrial firms, as the issuance signals capital inadequacy and heightened bankruptcy risk.\n\n---\n\n### Data / Model Specification\n\nAn event study was conducted to measure the abnormal stock returns around the announcement day (`t=0`). The key results for common stock and straight debt offerings are summarized in Table 1. For context, prior studies find a typical two-day announcement effect for industrial firms' common stock offerings of -3.17%.\n\n**Table 1: Two-Day (t=0 to t+1) Cumulative Average Excess Returns (CAER) for BHC Security Offerings**\n\n| Security Type | N | CAER (%) | z-Statistic |\n| :--- | :-: | :--- | :--- |\n| Common Stock | 19 | -1.507 | -3.065 |\n| Straight Debt (Nonshelf) | 50 | 0.095 | 0.294 |\n\n*Source: Synthesized from Table 3 and Table 4 in the paper.*\n\n---\n\n### The Question\n\nBased on the provided data and context, which of the following conclusions are supported by the event study results?", "Options": {"A": "The announcement of a straight debt offering leads to a statistically significant positive stock price reaction, supporting the idea of positive leverage effects.", "B": "The results for both common stock and straight debt offerings are consistent with the bank capital hypothesis, which predicts negative reactions for any form of capital raising.", "C": "The announcement of a common stock offering is associated with a statistically significant negative stock price reaction at the 1% level.", "D": "The magnitude of the negative reaction to common stock offerings (-1.507%) is more consistent with the regulation/asymmetry reduction hypothesis than the bank capital hypothesis, given the industrial firm benchmark of -3.17%."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the 'Atomic Decomposition' strategy to test the core empirical findings and their theoretical interpretations. The assessment combines 'Computational Judgment' (interpreting the z-statistic for option A) and 'Scenario Application' (evaluating hypotheses against data for option B). Distractor C presents a plausible but incorrect interpretation (sign and significance error). Distractor D tests a misunderstanding of how the evidence adjudicates between the two main hypotheses (conceptual opposite).", "qid": "145", "question": "### Background\n\n**Research Question.** What is the valuation effect of security issuance announcements by Bank Holding Companies (BHCs), and what do these effects imply about the competing hypotheses regarding information asymmetry and bank capital adequacy?\n\n**Setting.** The study analyzes the stock market's reaction to announcements of common stock and straight debt offerings by BHCs. The findings are evaluated in light of two competing theories.\n\n**Hypotheses.**\n1.  **Regulation/Asymmetry Reduction Hypothesis:** Predicts a negative announcement effect for equity, but one that is less severe than for industrial firms due to regulatory oversight reducing information asymmetry. The effect for straight debt is expected to be insignificant, similar to industrial firms.\n2.  **Bank Capital Hypothesis:** Predicts a significant negative announcement effect for both equity and subordinated debt (which can qualify as regulatory capital), possibly even larger than for industrial firms, as the issuance signals capital inadequacy and heightened bankruptcy risk.\n\n---\n\n### Data / Model Specification\n\nAn event study was conducted to measure the abnormal stock returns around the announcement day (`t=0`). The key results for common stock and straight debt offerings are summarized in Table 1. For context, prior studies find a typical two-day announcement effect for industrial firms' common stock offerings of -3.17%.\n\n**Table 1: Two-Day (t=0 to t+1) Cumulative Average Excess Returns (CAER) for BHC Security Offerings**\n\n| Security Type | N | CAER (%) | z-Statistic |\n| :--- | :-: | :--- | :--- |\n| Common Stock | 19 | -1.507 | -3.065 |\n| Straight Debt (Nonshelf) | 50 | 0.095 | 0.294 |\n\n*Source: Synthesized from Table 3 and Table 4 in the paper.*\n\n---\n\n### The Question\n\nBased on the provided data and context, which of the following conclusions are supported by the event study results?", "model_evaluation_result": {"model_raw_response": "Looking at the data and hypotheses:\n\n**For Common Stock:**\n- CAER = -1.507% with z-statistic = -3.065\n- A z-statistic of -3.065 indicates statistical significance at the 1% level (|z| > 2.576)\n- The negative reaction of -1.507% is less severe than the industrial firm benchmark of -3.17%\n\n**For Straight Debt:**\n- CAER = 0.095% with z-statistic = 0.294\n- This z-statistic is not statistically significant (|z| < 1.96 for 5% level)\n\n**Evaluating each option:**\n\nA) Incorrect - The straight debt CAER is positive (0.095%) but not statistically significant (z = 0.294)\n\nB) Incorrect - The bank capital hypothesis predicts negative reactions for both equity and debt that qualify as regulatory capital. While common stock shows a negative reaction, straight debt shows no significant reaction\n\nC) Correct - The z-statistic of -3.065 for common stock indicates statistical significance at the 1% level\n\nD) Correct - The regulation/asymmetry reduction hypothesis predicts a negative but less severe reaction for equity compared to industrial firms. The -1.507% reaction is indeed less severe than the -3.17% benchmark, supporting this hypothesis over the bank capital hypothesis which would predict an even larger negative reaction\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 317, "Question": "### Background\n\n**Research Question.** Do recent German legal reforms genuinely empower shareholders, or do they create an illusion of enhanced rights while imposing restrictions that maintain the status quo? \n\n**Setting.** The analysis focuses on two distinct proposals in a draft German bill on \"Corporate Integrity and the Right of Avoidance.\" One proposal addresses shareholder participation in annual meetings, while the other reforms the mechanism for enforcing director liability. The paper suggests that despite their stated aims, these reforms exhibit a \"restrictive tendency.\"\n\n**Variables & Parameters.**\n- **Institutional Investors:** Large investment managers (e.g., mutual funds, pension funds) whose participation is crucial for effective shareholder oversight.\n- **Liquidity:** The ability to trade an asset quickly without affecting its price. A perceived lack of liquidity is a major deterrent for institutional investors.\n- **Derivative Suit:** A lawsuit brought by a shareholder on behalf of the corporation against directors for damages caused to the company.\n- **Duty of Care:** A director's obligation to act on an informed basis. A breach can be caused by ordinary negligence, recklessness, or fraud.\n\n---\n\n### Data / Model Specification\n\nThe draft bill contains two key reforms concerning shareholder rights:\n\n1.  **Voting Participation Reform:** To encourage participation by foreign institutional investors, the ambiguous \"deposit of the shares\" requirement is to be replaced. The new system allows proof of ownership via a certificate from a depositary bank that refers to a **record date** 14 days before the shareholders’ meeting. This clarifies that shares are not subject to a trading \"lock-up.\"\n\n2.  **Director Liability Reform:** The law governing derivative suits is amended:\n    *   **Threshold Lowered:** The ownership threshold for shareholders to initiate a suit is lowered from 5% of capital (or €500,000) to 1% (or €100,000).\n    *   **Procedural Hurdle Added:** Under the proposed §148 AktG, a suit is admissible only with the leave of a district court. The court will only grant leave if the plaintiff presents facts justifying the suspicion that the damage was caused by **\"untruth or recklessness\"** (i.e., fraud or severe negligence).\n\n---\n\n### The Question\n\nBased on the analysis of the two proposed reforms, select all statements that accurately characterize their effects on shareholder rights and corporate governance in Germany.", "Options": {"A": "The 'record date' reform encourages institutional investors to vote by locking up their shares for 14 days, ensuring they have a vested interest in the meeting's outcome.", "B": "The derivative suit reform represents a clear net empowerment of shareholders, as the reduction in the capital threshold to 1% makes it significantly easier to hold directors accountable for any breach of their duty of care.", "C": "The proposed derivative suit reform, despite lowering ownership thresholds, effectively restricts shareholder power by requiring court leave based on a standard of 'untruth or recklessness', thereby shielding directors from suits based on ordinary negligence.", "D": "The 'record date' reform is designed to increase shareholder participation primarily by eliminating a perceived liquidity constraint, which lowers the economic cost of voting for institutional investors."}, "Answer": ["C", "D"], "pi_justification": "This item replaces a QA problem deemed highly suitable for conversion (Score: 9.5). It uses an **Atomic Decomposition** strategy to test the core analytical tasks of the original question: interpreting the net effect of two distinct legal reforms. The item functions as a **Computational Judgment** task, requiring the user to weigh the empowering and restrictive elements of each reform. **Correct options (A, B)** accurately describe the mechanisms of the voting reform and the restrictive nature of the derivative suit reform. **Distractor (C)** represents a common comprehension error, focusing only on the lowered threshold while ignoring the restrictive procedural hurdle. **Distractor (D)** presents a conceptual opposite, claiming the reform creates a lock-up when its entire purpose is to clarify that no lock-up exists.", "qid": "317", "question": "### Background\n\n**Research Question.** Do recent German legal reforms genuinely empower shareholders, or do they create an illusion of enhanced rights while imposing restrictions that maintain the status quo? \n\n**Setting.** The analysis focuses on two distinct proposals in a draft German bill on \"Corporate Integrity and the Right of Avoidance.\" One proposal addresses shareholder participation in annual meetings, while the other reforms the mechanism for enforcing director liability. The paper suggests that despite their stated aims, these reforms exhibit a \"restrictive tendency.\"\n\n**Variables & Parameters.**\n- **Institutional Investors:** Large investment managers (e.g., mutual funds, pension funds) whose participation is crucial for effective shareholder oversight.\n- **Liquidity:** The ability to trade an asset quickly without affecting its price. A perceived lack of liquidity is a major deterrent for institutional investors.\n- **Derivative Suit:** A lawsuit brought by a shareholder on behalf of the corporation against directors for damages caused to the company.\n- **Duty of Care:** A director's obligation to act on an informed basis. A breach can be caused by ordinary negligence, recklessness, or fraud.\n\n---\n\n### Data / Model Specification\n\nThe draft bill contains two key reforms concerning shareholder rights:\n\n1.  **Voting Participation Reform:** To encourage participation by foreign institutional investors, the ambiguous \"deposit of the shares\" requirement is to be replaced. The new system allows proof of ownership via a certificate from a depositary bank that refers to a **record date** 14 days before the shareholders’ meeting. This clarifies that shares are not subject to a trading \"lock-up.\"\n\n2.  **Director Liability Reform:** The law governing derivative suits is amended:\n    *   **Threshold Lowered:** The ownership threshold for shareholders to initiate a suit is lowered from 5% of capital (or €500,000) to 1% (or €100,000).\n    *   **Procedural Hurdle Added:** Under the proposed §148 AktG, a suit is admissible only with the leave of a district court. The court will only grant leave if the plaintiff presents facts justifying the suspicion that the damage was caused by **\"untruth or recklessness\"** (i.e., fraud or severe negligence).\n\n---\n\n### The Question\n\nBased on the analysis of the two proposed reforms, select all statements that accurately characterize their effects on shareholder rights and corporate governance in Germany.", "model_evaluation_result": {"model_raw_response": "Looking at each option carefully:\n\n**Option A**: This is incorrect. The text explicitly states that the record date reform \"clarifies that shares are not subject to a trading 'lock-up.'\" The reform is designed to eliminate the perception of a lock-up, not to create one.\n\n**Option B**: This is incorrect. While the ownership threshold is lowered, the reform adds a significant procedural hurdle - court approval based on \"untruth or recklessness.\" This means directors are now protected from suits based on ordinary negligence, making it harder, not easier, to hold them accountable for breaches of duty of care.\n\n**Option C**: This is correct. The text clearly indicates that despite lowering the threshold, the new requirement for court leave based on \"untruth or recklessness\" (fraud or severe negligence) shields directors from suits based on ordinary negligence. This aligns with the paper's thesis about reforms having a \"restrictive tendency.\"\n\n**Option D**: This is correct. The text states that \"a perceived lack of liquidity is a major deterrent for institutional investors\" and that the record date reform \"clarifies that shares are not subject to a trading 'lock-up,'\" thereby addressing the liquidity concern that was preventing institutional investor participation.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 419, "Question": "### Background\n\n**Research Question.** How do solvency capital requirements that explicitly model both process and systematic risk compare to a simpler standard formula that only captures systematic risk, and how does this comparison depend on portfolio size?\n\n**Setting and Environment.** The analysis considers a portfolio of immediate life annuities issued to a cohort of retirees aged `s=65`. Capital is calculated using two methods: (1) a full stochastic approach that models the entire liability distribution, and (2) the Solvency II QIS5 standard approach, which uses a deterministic shock. The comparison is made for portfolios of varying cohort sizes at different valuation times `t` (years since retirement).\n\n**Variables and Parameters.**\n\n*   `SR`: Solvency Ratio, defined as `SC_t / V_t`.\n*   `l(s+t)`: Cohort size (number of policies).\n*   `t`: Time of valuation (years since retirement age `s=65`).\n*   `SC_t`: Solvency capital at time `t`.\n*   `V_t`: Actuarial reserve (best estimate liability) at time `t`.\n*   `SCR_t`: Solvency Capital Requirement from the QIS5 standard formula.\n*   `k_t^Δ`: Coefficient of variation of the portfolio's random liability under a systematic shock.\n\n---\n\n### Data / Model Specification\n\nThe numerical experiment compares the VaR Solvency Ratio (`VaR SR`) from a stochastic internal model against the limiting ratio from the Solvency II standard approach (`Limiting SR = SCR_t / V_t`). The stochastic model accounts for both process risk (random fluctuations) and systematic risk (a permanent 20% decrease in mortality rates), while the standard approach only accounts for the systematic risk shock.\n\nFor large portfolios, the liability distribution is approximated by a normal distribution. The VaR Solvency Ratio at a confidence level of `1-ε` is calculated as:\n\n  \nSR_t^{VaR} = \\frac{SCR_t + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot V_t^\\Delta}{V_t} = \\left( \\frac{SCR_t}{V_t} \\right) + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot \\left( \\frac{V_t^\\Delta}{V_t} \\right) \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ⁻¹` is the inverse of the standard normal CDF. The confidence level is 99.5%, for which `Φ⁻¹(0.995) = 2.576`.\n\n**Table 1: Simulation Results for Longevity Risk**\n\n| Cohort Size `l(s+t)` | Valuation Time `t` | Coeff. of Variation `k_t^Δ` | `V_t^Δ / V_t` Ratio | Limiting SR (QIS5) | Calculated VaR SR |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 100 | 0 | 1.977% | 1.0551 | 5.51% | 10.88% |\n| 1,000 | 0 | 0.625% | 1.0551 | 5.51% | 7.20% |\n| 100,000 | 0 | 0.063% | 1.0551 | 5.51% | 5.68% |\n| 100 | 10 | 2.760% | 1.0834 | 8.34% | 16.04% |\n| 1,000 | 10 | 0.873% | 1.0834 | 8.34% | 10.78% |\n| 100,000 | 10 | 0.087% | 1.0834 | 8.34% | 8.58% |\n\n---\n\n### Question\n\nUsing the provided data, formulas, and the value `Φ⁻¹(0.995) = 2.576`, select all of the following statements that are mathematically correct.", "Options": {"A": "The VaR Solvency Ratio for the cohort of 1,000 at `t=0` is 7.12%, calculated as `5.51% + 2.576 * 0.00625`.", "B": "The component of the VaR Solvency Ratio attributable to process risk for the cohort of 100 at `t=10` is approximately 7.70%.", "C": "As the portfolio size grows infinitely large, the total VaR Solvency Ratio approaches zero.", "D": "For a hypothetical portfolio of 2,500 annuitants at `t=10` with `k_t^Δ = 0.552%`, the total VaR Solvency Ratio is 9.88%."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Computational Judgment strategy to test calculations from the original QA problem's second and third questions. Options A and B are both correct, independently verifiable calculations based on the provided data and formula. Distractor C is designed based on a Formula Misuse error (omitting the `V_t^Δ / V_t` term). Distractor D is a Conceptual error, incorrectly stating that total risk vanishes rather than converging to the systematic risk component.", "qid": "419", "question": "### Background\n\n**Research Question.** How do solvency capital requirements that explicitly model both process and systematic risk compare to a simpler standard formula that only captures systematic risk, and how does this comparison depend on portfolio size?\n\n**Setting and Environment.** The analysis considers a portfolio of immediate life annuities issued to a cohort of retirees aged `s=65`. Capital is calculated using two methods: (1) a full stochastic approach that models the entire liability distribution, and (2) the Solvency II QIS5 standard approach, which uses a deterministic shock. The comparison is made for portfolios of varying cohort sizes at different valuation times `t` (years since retirement).\n\n**Variables and Parameters.**\n\n*   `SR`: Solvency Ratio, defined as `SC_t / V_t`.\n*   `l(s+t)`: Cohort size (number of policies).\n*   `t`: Time of valuation (years since retirement age `s=65`).\n*   `SC_t`: Solvency capital at time `t`.\n*   `V_t`: Actuarial reserve (best estimate liability) at time `t`.\n*   `SCR_t`: Solvency Capital Requirement from the QIS5 standard formula.\n*   `k_t^Δ`: Coefficient of variation of the portfolio's random liability under a systematic shock.\n\n---\n\n### Data / Model Specification\n\nThe numerical experiment compares the VaR Solvency Ratio (`VaR SR`) from a stochastic internal model against the limiting ratio from the Solvency II standard approach (`Limiting SR = SCR_t / V_t`). The stochastic model accounts for both process risk (random fluctuations) and systematic risk (a permanent 20% decrease in mortality rates), while the standard approach only accounts for the systematic risk shock.\n\nFor large portfolios, the liability distribution is approximated by a normal distribution. The VaR Solvency Ratio at a confidence level of `1-ε` is calculated as:\n\n  \nSR_t^{VaR} = \\frac{SCR_t + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot V_t^\\Delta}{V_t} = \\left( \\frac{SCR_t}{V_t} \\right) + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot \\left( \\frac{V_t^\\Delta}{V_t} \\right) \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ⁻¹` is the inverse of the standard normal CDF. The confidence level is 99.5%, for which `Φ⁻¹(0.995) = 2.576`.\n\n**Table 1: Simulation Results for Longevity Risk**\n\n| Cohort Size `l(s+t)` | Valuation Time `t` | Coeff. of Variation `k_t^Δ` | `V_t^Δ / V_t` Ratio | Limiting SR (QIS5) | Calculated VaR SR |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 100 | 0 | 1.977% | 1.0551 | 5.51% | 10.88% |\n| 1,000 | 0 | 0.625% | 1.0551 | 5.51% | 7.20% |\n| 100,000 | 0 | 0.063% | 1.0551 | 5.51% | 5.68% |\n| 100 | 10 | 2.760% | 1.0834 | 8.34% | 16.04% |\n| 1,000 | 10 | 0.873% | 1.0834 | 8.34% | 10.78% |\n| 100,000 | 10 | 0.087% | 1.0834 | 8.34% | 8.58% |\n\n---\n\n### Question\n\nUsing the provided data, formulas, and the value `Φ⁻¹(0.995) = 2.576`, select all of the following statements that are mathematically correct.", "model_evaluation_result": {"model_raw_response": "I need to verify each statement using the provided formula and data.\n\nThe formula for VaR Solvency Ratio is:\nSR_t^{VaR} = (SCR_t/V_t) + Φ^{-1}(1-ε) × k_t^Δ × (V_t^Δ/V_t)\n\nWhere:\n- SCR_t/V_t = Limiting SR (QIS5)\n- Φ^{-1}(0.995) = 2.576\n- k_t^Δ = Coefficient of Variation\n- V_t^Δ/V_t = Ratio from the table\n\n**Statement A**: Cohort of 1,000 at t=0\n- Limiting SR = 5.51%\n- k_t^Δ = 0.625% = 0.00625\n- V_t^Δ/V_t = 1.0551\n- Calculation: 5.51% + 2.576 × 0.00625 × 1.0551\n- = 5.51% + 2.576 × 0.00625 × 1.0551\n- = 5.51% + 0.01698 = 5.51% + 1.698% = 7.208% ≈ 7.20%\n\nThe statement calculates it as 5.51% + 2.576 × 0.00625 = 5.51% + 0.0161 = 7.12%, which is incorrect because it omits the V_t^Δ/V_t ratio.\n\n**Statement B**: Cohort of 100 at t=10\n- Total VaR SR = 16.04% (from table)\n- Limiting SR = 8.34%\n- Process risk component = Total - Systematic = 16.04% - 8.34% = 7.70%\n\nThis is correct.\n\n**Statement C**: As portfolio size grows infinitely large\nAs portfolio size increases, k_t^Δ approaches 0 (process risk vanishes). Therefore:\nSR_t", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 411, "Question": "### Background\n\n**Research Question.** Under what conditions does an optimal portfolio exist for a robust utility maximization problem in a one-period market?\n\n**Setting and Environment.** We consider a one-period market with `d` stocks and initial capital `x`. An agent with utility function `U` faces model uncertainty represented by a set of measures `\\mathcal{P}`. The existence of an optimal portfolio hinges on the geometric properties of the market, induced by a no-arbitrage principle.\n\n**Variables and Parameters.**\n- `\\Delta S`: Random `\\mathbb{R}^d`-valued vector of stock price changes.\n- `h`: A deterministic portfolio vector in `\\mathbb{R}^d`.\n- `x`: Initial capital.\n- `\\mathcal{P}`: A convex set of probability measures.\n- `\\mathcal{P}\\text{-q.s.}`: A property holds `\\mathcal{P}`-quasi-surely, i.e., outside a set of measure zero for all `P \\in \\mathcal{P}`.\n- `\\mathrm{supp}_{\\mathcal P}(\\Delta S)`: The smallest closed set `A` such that `P(\\Delta S \\in A) = 1` for all `P \\in \\mathcal{P}`.\n- `L`: The linear subspace `\\mathrm{span~supp}_{\\mathcal P}(\\Delta S)`.\n\n---\n\n### Data / Model Specification\n\nThe set of admissible portfolios is defined as `D_{x}:=\\{h\\in\\mathbb{R}^{d}:x+h\\Delta S\\geq0 \\; \\mathcal{P}\\mathrm{-q.s.}\\}`. The set of non-redundant admissible portfolios is `K_x = D_x \\cap L`.\nThe market is assumed to satisfy the no-arbitrage condition `NA(\\mathcal{P})`: for any `h \\in \\mathbb{R}^d`,\n  \nh\\Delta S\\geq0 \\; \\mathcal{P}\\mathrm{-q.s.} \\quad \\implies \\quad h\\Delta S=0 \\; \\mathcal{P}\\mathrm{-q.s.} \\quad \\text{(Eq. (1))}\n \nThe existence of an optimal portfolio `\\hat{h}` for the problem `\\sup_{h\\in D_{x}}\\inf_{P\\in{\\mathcal{P}}}E_{P}[U(x+h\\Delta S)]` is guaranteed by showing the objective function attains its supremum on the compact set `K_x`.\n\n---\n\n### Question\n\nSelect all conditions or properties that are essential for the argument that an optimal portfolio exists.\n", "Options": {"A": "The integrability condition `E_{P}[U^{+}(x+h\\Delta S)] < \\infty` is used to establish that the objective function `h \\mapsto \\inf_{P\\in\\mathcal{P}}E_{P}[U(x+h\\Delta S)]` is upper semicontinuous.", "B": "The set of measures `\\mathcal{P}` must be dominated by a single reference measure `P^*` to allow for the use of standard integration theory.", "C": "The no-arbitrage condition `NA(\\mathcal{P})` is used to prove that the set `K_x` is bounded, and therefore compact.", "D": "The concavity of the utility function `U` is required to ensure the objective function is well-defined and finite."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests the student's understanding of the complete logical chain of the existence proof in Theorem 2.2. It uses a 'Premise/assumption packaging' strategy to isolate the key components of the argument. Distractor C is a subtle 'True but Irrelevant' fact for this specific part of the proof (concavity is important, but not for the compactness/semicontinuity argument itself), while Distractor D presents a 'Conceptual Opposite' to the paper's main setting, which is explicitly non-dominated.", "qid": "411", "question": "### Background\n\n**Research Question.** Under what conditions does an optimal portfolio exist for a robust utility maximization problem in a one-period market?\n\n**Setting and Environment.** We consider a one-period market with `d` stocks and initial capital `x`. An agent with utility function `U` faces model uncertainty represented by a set of measures `\\mathcal{P}`. The existence of an optimal portfolio hinges on the geometric properties of the market, induced by a no-arbitrage principle.\n\n**Variables and Parameters.**\n- `\\Delta S`: Random `\\mathbb{R}^d`-valued vector of stock price changes.\n- `h`: A deterministic portfolio vector in `\\mathbb{R}^d`.\n- `x`: Initial capital.\n- `\\mathcal{P}`: A convex set of probability measures.\n- `\\mathcal{P}\\text{-q.s.}`: A property holds `\\mathcal{P}`-quasi-surely, i.e., outside a set of measure zero for all `P \\in \\mathcal{P}`.\n- `\\mathrm{supp}_{\\mathcal P}(\\Delta S)`: The smallest closed set `A` such that `P(\\Delta S \\in A) = 1` for all `P \\in \\mathcal{P}`.\n- `L`: The linear subspace `\\mathrm{span~supp}_{\\mathcal P}(\\Delta S)`.\n\n---\n\n### Data / Model Specification\n\nThe set of admissible portfolios is defined as `D_{x}:=\\{h\\in\\mathbb{R}^{d}:x+h\\Delta S\\geq0 \\; \\mathcal{P}\\mathrm{-q.s.}\\}`. The set of non-redundant admissible portfolios is `K_x = D_x \\cap L`.\nThe market is assumed to satisfy the no-arbitrage condition `NA(\\mathcal{P})`: for any `h \\in \\mathbb{R}^d`,\n  \nh\\Delta S\\geq0 \\; \\mathcal{P}\\mathrm{-q.s.} \\quad \\implies \\quad h\\Delta S=0 \\; \\mathcal{P}\\mathrm{-q.s.} \\quad \\text{(Eq. (1))}\n \nThe existence of an optimal portfolio `\\hat{h}` for the problem `\\sup_{h\\in D_{x}}\\inf_{P\\in{\\mathcal{P}}}E_{P}[U(x+h\\Delta S)]` is guaranteed by showing the objective function attains its supremum on the compact set `K_x`.\n\n---\n\n### Question\n\nSelect all conditions or properties that are essential for the argument that an optimal portfolio exists.\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to identify which conditions are essential for proving that an optimal portfolio exists in this robust utility maximization framework.\n\nThe key insight is that the existence proof relies on showing that the objective function attains its supremum on the compact set K_x. Let me analyze each option:\n\n**Option A**: The integrability condition E_P[U^+(x+hΔS)] < ∞ is indeed used to establish upper semicontinuity of the objective function h ↦ inf_{P∈P} E_P[U(x+hΔS)]. Upper semicontinuity is crucial because it ensures that the supremum of a upper semicontinuous function on a compact set is attained. This is a fundamental result in optimization theory.\n\n**Option B**: While domination by a reference measure can be useful in some contexts, it's not essential for this existence argument. The proof can work with more general sets of measures P, as long as they satisfy the required properties. The compactness and continuity arguments don't inherently require domination.\n\n**Option C**: The no-arbitrage condition NA(P) is absolutely essential. It ensures that if h·ΔS ≥ 0 P-q.s., then h·ΔS = 0 P-q.s. This condition is used to prove that K_x is bounded. Without boundedness, K_x wouldn't be compact (since it's a closed subset of the finite-dimensional space L), and we couldn't guarantee the existence of an optimal portfolio.\n\n**Option D**: While concavity of U is common in utility maximization problems and has nice properties, it's not strictly necessary for the existence argument. The existence proof relies on compactness of K_x and upper semicontinuity of the objective function, which can be established without concavity. Concavity might help with uniqueness or other properties, but not with mere existence.\n\nTherefore, the essential conditions are A and C.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 278, "Question": "### Background\n\n**Research Question.** This study investigates why the pass-through of exchange rate changes to investment good prices varies so significantly across industries. A key empirical challenge is to distinguish between competing explanations for this variation.\n\n**Setting / Data-Generating Environment.** The analysis uses panel data for various industrial sectors across 12 OECD countries. The paper documents a strong positive correlation between an industry's estimated exchange rate pass-through and its reliance on imported capital goods.\n\n---\n\n### Data / Model Specification\n\nThe paper reports a simple correlation of 0.87 between an industry's exchange rate pass-through and its share of imported investment goods (`S_i`). However, it also notes that industries with high pass-through are often classified as 'high-technology' (`H_i`). This raises a question of whether the import share channel is causal or merely correlated with the true driver (technology level).\n\n**Table 1: Pass-Through and Import Shares for Selected Industries**\n\n| Industry                                    | Predicted Price Impact of 10% Depreciation | Percent of Investment Goods Imported (`S_i`) |\n| :------------------------------------------ | :----------------------------------------- | :------------------------------------------- |\n| Transport, storage and communication        | 3.15%                                      | 51.0%                                        |\n| Finance, insurance, real estate & business  | 0.24%                                      | 2.3%                                         |\n\n*Source: Adapted from the paper's Table 2.*\n\n---\n\n### Question\n\nBased on the principles of causal inference and the context provided, which of the following statements accurately describe the challenge of identifying the true causal mechanism behind high pass-through?", "Options": {"A": "The high correlation of 0.87 is sufficient evidence to conclude that a higher import share (`S_i`) causes higher pass-through, making the role of technology level (`H_i`) secondary.", "B": "A plausible economic reason for technology level (`H_i`) to independently cause higher pass-through is that specialized high-tech capital goods often lack domestic substitutes, making demand for them less price-elastic.", "C": "For technology level (`H_i`) to be a valid confounder of the relationship between import share (`S_i`) and pass-through, high-technology industries must systematically have different import shares than low-technology industries.", "D": "In a regression of pass-through on both `S_i` and `H_i`, a statistically significant coefficient on `H_i` would prove that the import share channel is entirely spurious."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of causal inference, specifically the concept of confounding, which was central to the original QA problem. It uses the 'Premise/assumption packaging' strategy by asking for the necessary conditions for confounding. Options A and B correctly state the two conditions for confounding in this context. Distractor C represents a classic 'correlation is not causation' error. Distractor D presents an overly strong conclusion, a common misinterpretation of regression results where multiple factors can have partial effects.", "qid": "278", "question": "### Background\n\n**Research Question.** This study investigates why the pass-through of exchange rate changes to investment good prices varies so significantly across industries. A key empirical challenge is to distinguish between competing explanations for this variation.\n\n**Setting / Data-Generating Environment.** The analysis uses panel data for various industrial sectors across 12 OECD countries. The paper documents a strong positive correlation between an industry's estimated exchange rate pass-through and its reliance on imported capital goods.\n\n---\n\n### Data / Model Specification\n\nThe paper reports a simple correlation of 0.87 between an industry's exchange rate pass-through and its share of imported investment goods (`S_i`). However, it also notes that industries with high pass-through are often classified as 'high-technology' (`H_i`). This raises a question of whether the import share channel is causal or merely correlated with the true driver (technology level).\n\n**Table 1: Pass-Through and Import Shares for Selected Industries**\n\n| Industry                                    | Predicted Price Impact of 10% Depreciation | Percent of Investment Goods Imported (`S_i`) |\n| :------------------------------------------ | :----------------------------------------- | :------------------------------------------- |\n| Transport, storage and communication        | 3.15%                                      | 51.0%                                        |\n| Finance, insurance, real estate & business  | 0.24%                                      | 2.3%                                         |\n\n*Source: Adapted from the paper's Table 2.*\n\n---\n\n### Question\n\nBased on the principles of causal inference and the context provided, which of the following statements accurately describe the challenge of identifying the true causal mechanism behind high pass-through?", "model_evaluation_result": {"model_raw_response": "Looking at this causal inference problem, I need to evaluate each statement about identifying the true causal mechanism behind exchange rate pass-through.\n\nA) This statement claims the 0.87 correlation is \"sufficient evidence\" for causation. This is incorrect - correlation does not imply causation, especially when there's a potential confounding variable (technology level) explicitly mentioned in the setup.\n\nB) This provides a plausible economic mechanism for why technology level could independently affect pass-through: high-tech capital goods often lack domestic substitutes, making demand less price-elastic and thus increasing pass-through. This is a valid economic reasoning that explains an independent causal pathway.\n\nC) This correctly identifies a necessary condition for confounding. For technology level (H_i) to confound the relationship between import share (S_i) and pass-through, it must be associated with both variables. Specifically, high-tech and low-tech industries must have systematically different import shares. This is a fundamental requirement in causal inference for a variable to be a confounder.\n\nD) This statement is incorrect. A significant coefficient on H_i in a regression would show that technology level has an independent association with pass-through after controlling for import share, but it wouldn't prove the import share channel is \"entirely spurious.\" Both variables could have genuine causal effects.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 418, "Question": "### Background\n\n**Research Question.** How do solvency capital requirements that explicitly model both process and systematic risk compare to a simpler standard formula that only captures systematic risk, and how does this comparison depend on portfolio size?\n\n**Setting and Environment.** The analysis considers a portfolio of immediate life annuities issued to a cohort of retirees aged `s=65`. Capital is calculated using two methods: (1) a full stochastic approach that models the entire liability distribution, and (2) the Solvency II QIS5 standard approach, which uses a deterministic shock. The comparison is made for portfolios of varying cohort sizes at different valuation times `t` (years since retirement).\n\n**Variables and Parameters.**\n\n*   `SR`: Solvency Ratio, defined as `SC_t / V_t`.\n*   `l(s+t)`: Cohort size (number of policies).\n*   `t`: Time of valuation (years since retirement age `s=65`).\n*   `SC_t`: Solvency capital at time `t`.\n*   `V_t`: Actuarial reserve (best estimate liability) at time `t`.\n*   `SCR_t`: Solvency Capital Requirement from the QIS5 standard formula.\n*   `k_t^Δ`: Coefficient of variation of the portfolio's random liability under a systematic shock.\n\n---\n\n### Data / Model Specification\n\nThe numerical experiment compares the VaR Solvency Ratio (`VaR SR`) from a stochastic internal model against the limiting ratio from the Solvency II standard approach (`Limiting SR = SCR_t / V_t`). The stochastic model accounts for both process risk (random fluctuations) and systematic risk (a permanent 20% decrease in mortality rates), while the standard approach only accounts for the systematic risk shock.\n\nFor large portfolios, the liability distribution is approximated by a normal distribution. The VaR Solvency Ratio at a confidence level of `1-ε` is calculated as:\n\n  \nSR_t^{VaR} = \\frac{SCR_t + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot V_t^\\Delta}{V_t} = \\left( \\frac{SCR_t}{V_t} \\right) + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot \\left( \\frac{V_t^\\Delta}{V_t} \\right) \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ⁻¹` is the inverse of the standard normal CDF. The confidence level is 99.5%, for which `Φ⁻¹(0.995) = 2.576`.\n\n**Table 1: Simulation Results for Longevity Risk**\n\n| Cohort Size `l(s+t)` | Valuation Time `t` | Coeff. of Variation `k_t^Δ` | `V_t^Δ / V_t` Ratio | Limiting SR (QIS5) | Calculated VaR SR |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 100 | 0 | 1.977% | 1.0551 | 5.51% | 10.88% |\n| 1,000 | 0 | 0.625% | 1.0551 | 5.51% | 7.20% |\n| 100,000 | 0 | 0.063% | 1.0551 | 5.51% | 5.68% |\n| 100 | 10 | 2.760% | 1.0834 | 8.34% | 16.04% |\n| 1,000 | 10 | 0.873% | 1.0834 | 8.34% | 10.78% |\n| 100,000 | 10 | 0.087% | 1.0834 | 8.34% | 8.58% |\n\n---\n\n### Question\n\nBased on the data for valuation time `t=0` in **Table 1**, select all of the following statements that are correct interpretations of the relationship between portfolio size, risk, and capital requirements.", "Options": {"A": "An increase in cohort size from 1,000 to 100,000 primarily reduces systematic risk.", "B": "For the smallest portfolio (100 lives), the capital required for process risk (the difference between VaR SR and Limiting SR) is a significant component of the total capital requirement.", "C": "As the cohort size increases, the coefficient of variation `k_t^Δ` decreases, reflecting the diversification of process risk.", "D": "The Solvency II standard formula (Limiting SR) provides a conservative capital estimate for portfolios of all sizes."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test the core interpretations from the original QA problem's first question. Options A and B are independently correct statements derived from the table and the provided context. Distractor C (Conceptual Opposite) targets the misunderstanding that the standard formula is always sufficient. Distractor D (Conceptual Opposite) targets the common confusion between diversifiable process risk and non-diversifiable systematic risk.", "qid": "418", "question": "### Background\n\n**Research Question.** How do solvency capital requirements that explicitly model both process and systematic risk compare to a simpler standard formula that only captures systematic risk, and how does this comparison depend on portfolio size?\n\n**Setting and Environment.** The analysis considers a portfolio of immediate life annuities issued to a cohort of retirees aged `s=65`. Capital is calculated using two methods: (1) a full stochastic approach that models the entire liability distribution, and (2) the Solvency II QIS5 standard approach, which uses a deterministic shock. The comparison is made for portfolios of varying cohort sizes at different valuation times `t` (years since retirement).\n\n**Variables and Parameters.**\n\n*   `SR`: Solvency Ratio, defined as `SC_t / V_t`.\n*   `l(s+t)`: Cohort size (number of policies).\n*   `t`: Time of valuation (years since retirement age `s=65`).\n*   `SC_t`: Solvency capital at time `t`.\n*   `V_t`: Actuarial reserve (best estimate liability) at time `t`.\n*   `SCR_t`: Solvency Capital Requirement from the QIS5 standard formula.\n*   `k_t^Δ`: Coefficient of variation of the portfolio's random liability under a systematic shock.\n\n---\n\n### Data / Model Specification\n\nThe numerical experiment compares the VaR Solvency Ratio (`VaR SR`) from a stochastic internal model against the limiting ratio from the Solvency II standard approach (`Limiting SR = SCR_t / V_t`). The stochastic model accounts for both process risk (random fluctuations) and systematic risk (a permanent 20% decrease in mortality rates), while the standard approach only accounts for the systematic risk shock.\n\nFor large portfolios, the liability distribution is approximated by a normal distribution. The VaR Solvency Ratio at a confidence level of `1-ε` is calculated as:\n\n  \nSR_t^{VaR} = \\frac{SCR_t + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot V_t^\\Delta}{V_t} = \\left( \\frac{SCR_t}{V_t} \\right) + \\Phi^{-1}(1-\\varepsilon) \\cdot k_t^\\Delta \\cdot \\left( \\frac{V_t^\\Delta}{V_t} \\right) \\quad \\text{(Eq. (1))}\n \n\nwhere `Φ⁻¹` is the inverse of the standard normal CDF. The confidence level is 99.5%, for which `Φ⁻¹(0.995) = 2.576`.\n\n**Table 1: Simulation Results for Longevity Risk**\n\n| Cohort Size `l(s+t)` | Valuation Time `t` | Coeff. of Variation `k_t^Δ` | `V_t^Δ / V_t` Ratio | Limiting SR (QIS5) | Calculated VaR SR |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 100 | 0 | 1.977% | 1.0551 | 5.51% | 10.88% |\n| 1,000 | 0 | 0.625% | 1.0551 | 5.51% | 7.20% |\n| 100,000 | 0 | 0.063% | 1.0551 | 5.51% | 5.68% |\n| 100 | 10 | 2.760% | 1.0834 | 8.34% | 16.04% |\n| 1,000 | 10 | 0.873% | 1.0834 | 8.34% | 10.78% |\n| 100,000 | 10 | 0.087% | 1.0834 | 8.34% | 8.58% |\n\n---\n\n### Question\n\nBased on the data for valuation time `t=0` in **Table 1**, select all of the following statements that are correct interpretations of the relationship between portfolio size, risk, and capital requirements.", "model_evaluation_result": {"model_raw_response": "Looking at the data for t=0 in Table 1, I need to analyze each statement:\n\n**Statement A**: \"An increase in cohort size from 1,000 to 100,000 primarily reduces systematic risk.\"\n- The Limiting SR (which represents systematic risk only) remains constant at 5.51% for all cohort sizes\n- Systematic risk affects all policies simultaneously and cannot be diversified away\n- This statement is **incorrect**\n\n**Statement B**: \"For the smallest portfolio (100 lives), the capital required for process risk (the difference between VaR SR and Limiting SR) is a significant component of the total capital requirement.\"\n- For 100 lives: VaR SR = 10.88%, Limiting SR = 5.51%\n- Process risk component = 10.88% - 5.51% = 5.37%\n- This represents 49.4% of the total capital requirement (5.37%/10.88%)\n- This statement is **correct**\n\n**Statement C**: \"As the cohort size increases, the coefficient of variation k_t^Δ decreases, reflecting the diversification of process risk.\"\n- From the data: 100 lives → k_t^Δ = 1.977%, 1,000 lives → k_t^Δ = 0.625%, 100,000 lives → k_t^Δ = 0.063%\n- The coefficient of variation clearly decreases as cohort size increases\n- This reflects the law of large numbers - process risk diversifies away with larger portfolios\n- This statement is **correct**\n\n**Statement D**: \"The Solvency II standard formula (Limiting SR) provides a conservative capital estimate for portfolios of all sizes.\"\n- Limiting SR = 5.51% for all sizes\n- VaR SR values: 100 lives = 10.88%, 1,000 lives = 7.20%, 100,000 lives = 5.68%\n- The standard formula underestimates capital requirements for smaller portfolios (100 and 1,000 lives)\n- This statement is **incorrect**\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 385, "Question": "### Background\n\n**Research Question.** What is the long-run equilibrium relationship between the U.S. Treasury term premium and foreign holdings, accounting for the dynamic interactions between official and private investors?\n\n**Setting.** A cointegrated Vector Autoregression (VAR) model is used to analyze the long-run equilibrium relationship among the 5-year term premium (`TP_t`), the stock of foreign official holdings (`FOI_t`), and the stock of foreign private holdings (`FPVT_t`). If the variables are cointegrated, the system can be represented as a Vector Error Correction Model (VECM).\n\n### Data / Model Specification\n\nThe VECM contains an error-correction term `\\mathbf{\\Pi} \\cdot \\mathbf{X}_{t-1}`, where `\\mathbf{X}_{t} = (TP_t, FOI_t, FPVT_t)'`. The matrix `\\mathbf{\\Pi}` can be decomposed as `\\mathbf{\\Pi} = \\alpha \\beta'`, where `\\beta'` defines the long-run equilibrium relationship and `\\alpha` is a vector of loading factors representing the speed of adjustment to deviations from this equilibrium.\n\n**Table 1. VAR Long-Run Coefficients and Adjustment Speeds (12 Lags)**\n\n| | Coefficient | t-statistic |\n| :--- | :---: | :---: |\n| **Cointegrating Vector (`\\beta`)** | | |\n| Term premium (`TP_t`) | 1 (normalized) | - |\n| Foreign official (`FOI_t`) | 0.046 | 5.782 |\n| Foreign private (`FPVT_t`) | 0.061 | 2.883 |\n| | | |\n| **Loading Factors (`\\alpha`)** | | |\n| Term premium equation | -0.481 | -5.831 |\n| Foreign official equation | -0.540 | -2.378 |\n| Foreign private equation | 0.096 | 0.274 |\n\n### Question\n\nBased on the VECM results presented in Table 1, which of the following statements accurately describe the long-run dynamics between the term premium and foreign holdings?", "Options": {"A": "The loading factor of -0.481 for the term premium equation indicates a rapid and stable adjustment process, where approximately 48% of any deviation from the long-run equilibrium is corrected in the following month.", "B": "The smaller long-run effect of foreign official inflows compared to short-run estimates is primarily because foreign *official* investors tend to reverse their initial purchases over time.", "C": "The negative loading factor for the term premium equation (-0.481) indicates an unstable, divergent path for the term premium, while the positive factor for the private holdings equation (0.096) implies stability.", "D": "The long-run equilibrium relationship is `TP_t = -0.046 * FOI_t - 0.061 * FPVT_t`, implying that in the long run, both official and private foreign holdings put similar downward pressure on the term premium."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the interpretation of a VECM, converted from a Table QA problem. It uses an **Atomic Decomposition** strategy to test understanding of the cointegrating vector, loading factors, and the economic story reconciling short- and long-run effects. **Correct options (A, B)** test the ability to formulate the long-run equation from the `β` coefficients and to interpret the `α` loading factor for speed and stability. **Distractor C (Reconciliation Error)** presents an incorrect economic mechanism for the dampened long-run effect, misattributing it to official reversals instead of private offsetting. **Distractor D (Adjustment Speed Misinterpretation)** targets a fundamental error in VECM theory, incorrectly associating a negative loading factor with instability.", "qid": "385", "question": "### Background\n\n**Research Question.** What is the long-run equilibrium relationship between the U.S. Treasury term premium and foreign holdings, accounting for the dynamic interactions between official and private investors?\n\n**Setting.** A cointegrated Vector Autoregression (VAR) model is used to analyze the long-run equilibrium relationship among the 5-year term premium (`TP_t`), the stock of foreign official holdings (`FOI_t`), and the stock of foreign private holdings (`FPVT_t`). If the variables are cointegrated, the system can be represented as a Vector Error Correction Model (VECM).\n\n### Data / Model Specification\n\nThe VECM contains an error-correction term `\\mathbf{\\Pi} \\cdot \\mathbf{X}_{t-1}`, where `\\mathbf{X}_{t} = (TP_t, FOI_t, FPVT_t)'`. The matrix `\\mathbf{\\Pi}` can be decomposed as `\\mathbf{\\Pi} = \\alpha \\beta'`, where `\\beta'` defines the long-run equilibrium relationship and `\\alpha` is a vector of loading factors representing the speed of adjustment to deviations from this equilibrium.\n\n**Table 1. VAR Long-Run Coefficients and Adjustment Speeds (12 Lags)**\n\n| | Coefficient | t-statistic |\n| :--- | :---: | :---: |\n| **Cointegrating Vector (`\\beta`)** | | |\n| Term premium (`TP_t`) | 1 (normalized) | - |\n| Foreign official (`FOI_t`) | 0.046 | 5.782 |\n| Foreign private (`FPVT_t`) | 0.061 | 2.883 |\n| | | |\n| **Loading Factors (`\\alpha`)** | | |\n| Term premium equation | -0.481 | -5.831 |\n| Foreign official equation | -0.540 | -2.378 |\n| Foreign private equation | 0.096 | 0.274 |\n\n### Question\n\nBased on the VECM results presented in Table 1, which of the following statements accurately describe the long-run dynamics between the term premium and foreign holdings?", "model_evaluation_result": {"model_raw_response": "Looking at the VECM results in Table 1, I need to evaluate each statement about the long-run dynamics.\n\n**Statement A**: The loading factor of -0.481 for the term premium equation with t-statistic of -5.831 (highly significant) indicates that when the term premium deviates from its long-run equilibrium, it adjusts back toward equilibrium. The negative sign is correct for error correction - it means the term premium decreases when above equilibrium and increases when below. The interpretation that ~48% of deviation is corrected each period is accurate. ✓\n\n**Statement B**: This statement claims that foreign official investors reverse their purchases over time, but the VECM results don't provide evidence for this. The loading factor for foreign official equation is -0.540, which is about adjustment to equilibrium, not about reversing purchases. This statement makes an unsupported claim. ✗\n\n**Statement C**: This misinterprets the loading factors. A negative loading factor (like -0.481) indicates stable adjustment back to equilibrium, not instability. The positive factor for private holdings (0.096) with an insignificant t-statistic (0.274) suggests that private holdings don't significantly adjust to equilibrium deviations. The statement has the stability implications backwards. ✗\n\n**Statement D**: From the cointegrating vector with TP normalized to 1, the long-run relationship is:\nTP_t + 0.046×FOI_t + 0.061×FPVT_t = 0\n\nRearranging: TP_t = -0.046×FOI_t - 0.061×FPVT_t\n\nThis is exactly what the statement says. Both coefficients are negative and statistically significant (t-stats > 2), indicating that both types of foreign holdings put downward pressure on the term premium in the long run. ✓\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 283, "Question": "### Background\n\nThe paper argues that traditional, decentralized capital budgeting systems fail because they ignore system-wide effects like positive externalities and synergies, which are central to the value of organizational capabilities. A division maximizing its own profit will underinvest from the firm's perspective.\n\n### Data / Model Specification\n\nA firm has two divisions, A and B. The total value created by their investment decisions is:\n\n  \nV_{Firm} = (\\pi_A - I_A)1_A + (\\pi_B - I_B)1_B + E_{BA}1_B + E_{AB}1_A + S \\cdot 1_A \\cdot 1_B\n \n\nwhere `I` is investment cost, `π` is standalone profit, `E` represents a positive externality (e.g., `E_AB` is the benefit to B from A's investment), `S` is a synergy from joint investment, and `1` is an indicator for investing.\n\nConsider the following numerical scenario:\n- Costs: `I_A = 20`, `I_B = 15`\n- Standalone Profits: `π_A = 18`, `π_B = 12`\n- Externalities: `E_AB = 4` (from A to B), `E_BA = 5` (from B to A)\n- Synergy: `S = 6`\n\n### Question\n\nBased on the provided model and numerical scenario, select all of the following statements that are correct.", "Options": {"A": "If Division A invests but Division B does not, the total value created for the firm is 2.", "B": "The decentralized decision-making process fails to account for a total of 15 in value from externalities and synergies when both projects are undertaken.", "C": "From a centralized (firm-wide) perspective, the optimal decision is for both divisions to invest, yielding a total firm value of 10.", "D": "If both divisions act independently to maximize their own standalone profit (`π - I`), both will choose not to invest."}, "Answer": ["B", "C", "D"], "pi_justification": "This item uses the 'Computational Judgment' strategy to test understanding of why decentralized capital budgeting fails. It requires students to perform calculations from both a decentralized and a centralized perspective and compare the outcomes. Option A is correct: A's standalone NPV is 18-20=-2; B's is 12-15=-3. Both reject. Option B is correct: The firm-wide value of investing in both is (-2) + (-3) + 4 + 5 + 6 = 10, which is the maximum possible value. Option D is correct: The ignored value is E_AB + E_BA + S = 4 + 5 + 6 = 15. Distractor C is a 'Step-Omission Error'; the value of investing in A only is (π_A - I_A) + E_AB = (18-20) + 4 = 2. Wait, this makes C correct. Let me re-evaluate. A's NPV is -2, B's is -3. Both reject. Correct. Centralized value for both is (-2)+(-3)+4+5+6 = 10. Correct. Ignored value is 4+5+6=15. Correct. Value for A only is (18-20) + E_AB = -2 + 4 = 2. Correct. This means A, B, C, and D are all correct. This is not a good design. I will redesign the item to have fewer correct answers and better distractors. Let's change the numbers. New scenario: I_A=20, π_A=18, I_B=15, π_B=16, E_AB=3, E_BA=5, S=6. A's NPV = -2 (Reject). B's NPV = +1 (Invest). Decentralized outcome is (Reject, Invest). Firm value = (16-15) + E_BA = 1+5=6. Centralized: A only = (18-20)+E_AB = -2+3=1. B only = (16-15)+E_BA = 1+5=6. Both = (-2)+(1)+3+5+6 = 13. Optimal is Both. This is a better scenario. Let's re-write the options based on this. A: B invests, A rejects. Correct. B: Optimal is both, value 13. Correct. C: Value of A only is 1. Correct. D: Ignored value is 3+5+6=14. Correct. Still too many correct answers. I will revert to the original numbers and use the Inversion Protocol. The single best outcome is investing in both. I will ask what is NOT a valid conclusion. The original correct answers A, B, D will become distractors. C will become a distractor. I need to create new options that are incorrect. Let's try again from scratch with the original numbers. A: Both reject. Correct. B: Optimal is both, value 10. Correct. C: Value of A only is (18-20)+E_AB = 2. Correct. D: Ignored value is 15. Correct. This is a flawed question. I will create a new one. Let's focus on the subsidies part of the original QA. This is more complex and allows for better distractors. I will scrap the current item and build a new one based on QA2, part 3. \n\n**REVISED ITEM:**\n\n json\n{\n  \"question_type\": \"Multiple Response\",\n  \"question\": \"### Background\\n\\nThe paper argues that traditional, decentralized capital budgeting systems fail because they ignore system-wide effects. This coordination failure can sometimes be solved if a central planner (HQ) provides subsidies to align divisional incentives with firm-wide goals.\\n\\n### Data / Model Specification\\n\\nA firm has two divisions, A and B. Consider the following numerical scenario:\\n- Costs: `I_A = 12`, `I_B = 11`\\n- Standalone Profits: `π_A = 10`, `π_B = 8`\\n- Externalities: `E_AB = 3` (from A to B), `E_BA = 4` (from B to A)\\n- Synergy: `S = 5`\\n\\nIn this scenario, decentralized divisions will not invest, but the firm-optimal decision is for both to invest, creating a total value of 7. HQ wants to design minimum subsidies `(T_A, T_B)` to make (Invest, Invest) a Nash Equilibrium, meaning each division finds it profitable to invest *given the other is investing*. A division's total payoff is its standalone profit, plus any incoming externality, plus any subsidy.\\n\\n### Question\\n\\nBased on the provided data, select all of the following statements that are correct regarding the design of this subsidy scheme.\",\n  \"options\": [\n    {\n      \"value\": \"A\",\n      \"text\": \"To incentivize Division A to invest (assuming B invests), HQ must provide a subsidy `T_A` that is greater than 2.\"\n    },\n    {\n      \"value\": \"B\",\n      \"text\": \"Division A is willing to invest without a subsidy if it knows Division B is investing.\"\n    },\n    {\n      \"value\": \"C\",\n      \"text\": \"To incentivize Division B to invest (assuming A invests), HQ must provide a subsidy `T_B` that is greater than 0.\"\n    },\n    {\n      \"value\": \"D\",\n      \"text\": \"The primary risk of this scheme is that divisions may overstate their investment costs (`I_A`, `I_B`) to secure larger subsidies from HQ.\"\n    }\n  ],\n  \"answer\": [\n    \"B\",\n    \"C\"\n  ],\n  \"pi_justification\": \"This item uses a 'Computational Judgment' strategy to assess a nuanced concept from the paper: using internal mechanisms to solve coordination failures. It requires students to calculate the conditions for a Nash Equilibrium in a subsidized game. Option B is correct: A's payoff if B invests is `π_A + E_BA = 10 + 4 = 14`, which is greater than its cost `I_A = 12`. No subsidy is needed. Option C is correct: B's payoff if A invests is `π_B + E_AB = 8 + 3 = 11`, which is exactly equal to its cost `I_B = 11`. Any infinitesimally positive subsidy makes investing the strictly better option. Distractor A is a 'Sign Error', as no subsidy is needed for A. Distractor D is a plausible but incorrect 'Conceptual Distractor'; the paper and standard mechanism design theory suggest the key risk is understating standalone payoffs (`π`), not overstating costs, to justify the need for a subsidy.\"\n}\n ", "qid": "283", "question": "### Background\n\nThe paper argues that traditional, decentralized capital budgeting systems fail because they ignore system-wide effects like positive externalities and synergies, which are central to the value of organizational capabilities. A division maximizing its own profit will underinvest from the firm's perspective.\n\n### Data / Model Specification\n\nA firm has two divisions, A and B. The total value created by their investment decisions is:\n\n  \nV_{Firm} = (\\pi_A - I_A)1_A + (\\pi_B - I_B)1_B + E_{BA}1_B + E_{AB}1_A + S \\cdot 1_A \\cdot 1_B\n \n\nwhere `I` is investment cost, `π` is standalone profit, `E` represents a positive externality (e.g., `E_AB` is the benefit to B from A's investment), `S` is a synergy from joint investment, and `1` is an indicator for investing.\n\nConsider the following numerical scenario:\n- Costs: `I_A = 20`, `I_B = 15`\n- Standalone Profits: `π_A = 18`, `π_B = 12`\n- Externalities: `E_AB = 4` (from A to B), `E_BA = 5` (from B to A)\n- Synergy: `S = 6`\n\n### Question\n\nBased on the provided model and numerical scenario, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement based on the given model and numerical values.\n\nGiven values:\n- I_A = 20, I_B = 15\n- π_A = 18, π_B = 12\n- E_AB = 4, E_BA = 5\n- S = 6\n\nLet me evaluate each scenario:\n\n**Statement A**: If Division A invests but Division B does not (1_A = 1, 1_B = 0):\nV_Firm = (18 - 20)×1 + (12 - 15)×0 + 5×0 + 4×1 + 6×1×0\nV_Firm = -2 + 0 + 0 + 4 + 0 = 2\nThis is correct.\n\n**Statement B**: When both projects are undertaken, the externalities and synergies are:\n- E_AB × 1_A = 4 × 1 = 4\n- E_BA × 1_B = 5 × 1 = 5\n- S × 1_A × 1_B = 6 × 1 × 1 = 6\nTotal = 4 + 5 + 6 = 15\nThis is correct.\n\n**Statement C**: If both divisions invest (1_A = 1, 1_B = 1):\nV_Firm = (18 - 20)×1 + (12 - 15)×1 + 5×1 + 4×1 + 6×1×1\nV_Firm = -2 + (-3) + 5 + 4 + 6 = 10\nThis is correct.\n\n**Statement D**: Under decentralized decision-making, each division only considers its standalone profit:\n- Division A: π_A - I_A = 18 - 20 = -2 < 0 → Won't invest\n- Division B: π_B - I_B = 12 - 15 = -3 < 0 → Won't invest\nThis is correct.\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 432, "Question": "### Background\n\n**Research Question.** What is the causal effect of an underwriter's reputation on the degree of IPO underpricing, after accounting for the non-random matching between issuers and underwriters?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage least-squares (2SLS) regression to analyze IPO underpricing. The key challenge is that high-quality firms may preferentially match with high-reputation underwriters, creating an endogeneity problem.\n\n**Variables & Parameters.**\n\n*   `Underpricing_i`: The percentage difference between the first-day closing price and the offer price for IPO `i`.\n*   `Reputation_i`: The endogenous measure of underwriter reputation. The primary measure is the `Fragmented ranking Equally weighted` score.\n*   `X_i`: A vector of exogenous control variables for firm `i` (e.g., firm age, offer size, VC-backing).\n*   `Z_i`: The instrumental variable for `Reputation_i`, defined as the number of underwriters that have underwritten European IPOs in the same 2-digit industry as firm `i` in the past five years.\n\n---\n\n### Data / Model Specification\n\nThe study estimates the following structural model:\n\n  \n\\text{Underpricing}_i = \\beta_0 + \\beta_1 \\text{Reputation}_i + \\Gamma' X_i + \\epsilon_i\n \n\nDue to endogeneity, `Reputation_i` is instrumented using `Z_i` in a 2SLS framework.\n\n**Table 1: 2SLS Regression of Underpricing on Underwriter Reputation (Model 1)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| **Second Stage** | | |\n| Underwriter Reputation | | |\n| (Fragmented ranking Equally weighted) | -0.04 | (-6.08) |\n\n*Source: Adapted from Table 7, Model 1 in the paper.*\n\n---\n\n### The Question\n\nSuppose a critic asserts that the true causal effect of underwriter reputation on underpricing is zero (i.e., `β_1 = 0`). Given the study's finding of a highly significant negative coefficient (-0.04, t=-6.08) from the 2SLS estimation, which of the following conclusions logically follow from the critic's premise?", "Options": {"A": "The significant 2SLS result implies that the key identifying assumption—the exclusion restriction—is violated.", "B": "The finding proves that the instrument is irrelevant and fails the first-stage regression.", "C": "The instrument (`Z_i`) must have a direct negative causal effect on `Underpricing_i`, likely through a channel like increased pre-IPO competition and information discovery.", "D": "This outcome suggests that a simple OLS regression would have provided a less biased estimate of the effect."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Reverse-Reasoning strategy to test a deep understanding of instrumental variable assumptions. By positing that the study's conclusion is wrong (true effect is zero), it forces the candidate to deduce the logical implications for the model's validity, specifically the violation of the exclusion restriction. Distractors target common confusions between IV relevance and exogeneity (C) and the nature of endogeneity bias (D).", "qid": "432", "question": "### Background\n\n**Research Question.** What is the causal effect of an underwriter's reputation on the degree of IPO underpricing, after accounting for the non-random matching between issuers and underwriters?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage least-squares (2SLS) regression to analyze IPO underpricing. The key challenge is that high-quality firms may preferentially match with high-reputation underwriters, creating an endogeneity problem.\n\n**Variables & Parameters.**\n\n*   `Underpricing_i`: The percentage difference between the first-day closing price and the offer price for IPO `i`.\n*   `Reputation_i`: The endogenous measure of underwriter reputation. The primary measure is the `Fragmented ranking Equally weighted` score.\n*   `X_i`: A vector of exogenous control variables for firm `i` (e.g., firm age, offer size, VC-backing).\n*   `Z_i`: The instrumental variable for `Reputation_i`, defined as the number of underwriters that have underwritten European IPOs in the same 2-digit industry as firm `i` in the past five years.\n\n---\n\n### Data / Model Specification\n\nThe study estimates the following structural model:\n\n  \n\\text{Underpricing}_i = \\beta_0 + \\beta_1 \\text{Reputation}_i + \\Gamma' X_i + \\epsilon_i\n \n\nDue to endogeneity, `Reputation_i` is instrumented using `Z_i` in a 2SLS framework.\n\n**Table 1: 2SLS Regression of Underpricing on Underwriter Reputation (Model 1)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| **Second Stage** | | |\n| Underwriter Reputation | | |\n| (Fragmented ranking Equally weighted) | -0.04 | (-6.08) |\n\n*Source: Adapted from Table 7, Model 1 in the paper.*\n\n---\n\n### The Question\n\nSuppose a critic asserts that the true causal effect of underwriter reputation on underpricing is zero (i.e., `β_1 = 0`). Given the study's finding of a highly significant negative coefficient (-0.04, t=-6.08) from the 2SLS estimation, which of the following conclusions logically follow from the critic's premise?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze what conclusions follow if we accept the critic's premise that the true causal effect of underwriter reputation on underpricing is zero (β₁ = 0), while the 2SLS estimation shows a highly significant negative coefficient.\n\nLet me work through each option:\n\n**Option A**: If the true effect is zero but 2SLS shows a significant negative effect, this strongly suggests the exclusion restriction is violated. The exclusion restriction requires that the instrument (Z_i) affects the outcome (Underpricing) only through the endogenous variable (Reputation). If this assumption holds and the true effect is zero, we shouldn't see a significant coefficient. The significant result implies Z_i must have a direct effect on Underpricing, violating the exclusion restriction.\n\n**Option B**: This doesn't follow. The significant 2SLS result actually suggests the instrument is relevant (passes the first stage). If the instrument were irrelevant, we wouldn't get precise estimates in the second stage.\n\n**Option C**: This follows logically from A. If the exclusion restriction is violated and we're seeing a negative coefficient, the instrument must have a direct negative effect on underpricing. The suggested mechanism (increased pre-IPO competition and information discovery) is plausible - more underwriters in an industry could lead to better price discovery and less underpricing.\n\n**Option D**: This doesn't follow. OLS suffers from the original endogeneity problem (selection bias). Even if 2SLS is biased due to exclusion restriction violation, there's no reason to believe OLS would provide a less biased estimate.\n\nTherefore, options A and C logically follow from the critic's premise.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 402, "Question": "### Background\n\n**Research Question.** This case examines the behavioral motivation behind the use of stop-loss orders, distinguishing between the 'learning hypothesis' (stop-loss use teaches better habits) and the 'self-control hypothesis' (biased investors use stop losses as a pre-commitment device).\n\n**Setting / Data-Generating Environment.** The analysis uses two distinct empirical tests. The first is a transaction-level analysis comparing trades closed via stop-loss to manual trades. The second is a user-level analysis on a subsample of data that *excludes all stop-loss transactions*, comparing the underlying manual trading behavior of stop-loss users versus non-users.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator (dummy=1 for loss).\n- `TGI(t)`: Trading Gain Indicator (dummy=1 for gain).\n- `SL_Transaction`: A dummy variable equal to 1 for a roundtrip position closed by a stop-loss order.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who use stop losses.\n\n---\n\n### Data / Model Specification\n\nResults from the two key analyses are presented in Table 1 and Table 2.\n\n**Table 1: Transaction-Level Analysis of Stop-Loss Executions**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.5502*** | | \n| TLI × SL_Transaction | 2.1546*** | | \n| TGI | | 1.7695*** |\n| TGI × SL_Transaction | | 0.5225*** |\n\n**Table 2: User-Level Analysis on Manual (Non-Stop-Loss) Transactions**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.5610*** | | \n| TLI × SL_User | 0.9555** | | \n| TGI | | 1.8918*** |\n| TGI × SL_User | | 0.8419*** |\n\n*Notes: *** denotes significance at 1%, ** at 5%.*\n\n---\n\n### Question\n\nBased on the results in Table 1 and Table 2, select all of the following statements that are correct.", "Options": {"A": "The results in Table 2 support the 'self-control hypothesis' by showing that in their manual trades, stop-loss users are even more reluctant to sell losers than non-users are.", "B": "The results in Table 1 show that while stop-loss transactions significantly increase the propensity to sell losers, the combined hazard ratio for selling a loser remains below 1.", "C": "The results in Table 2 support the 'learning hypothesis' by showing that stop-loss users are less prone to the disposition effect in their manual trades.", "D": "The results in Table 1 demonstrate that for transactions closed via a stop-loss, the disposition effect is inverted, with the conditional probability of selling a loser becoming greater than the conditional probability of selling a winner."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item synthesizes results from two different tables to test a core theoretical question. It uses a 'Computational Judgment' and 'Scenario Application' strategy. Option A requires calculating the combined hazard ratios from Table 1 (`0.5502 * 2.1546 = 1.185` for losers; `1.7695 * 0.5225 = 0.925` for winners) and correctly interpreting the inversion. Option B requires interpreting the significant interaction term in Table 2 and connecting it to the correct behavioral theory. Distractor C is the 'Conceptual Opposite' of B. Distractor D is a 'Step-Omission Error', as it correctly notes the increase but fails to perform the multiplication which shows the combined hazard is > 1.", "qid": "402", "question": "### Background\n\n**Research Question.** This case examines the behavioral motivation behind the use of stop-loss orders, distinguishing between the 'learning hypothesis' (stop-loss use teaches better habits) and the 'self-control hypothesis' (biased investors use stop losses as a pre-commitment device).\n\n**Setting / Data-Generating Environment.** The analysis uses two distinct empirical tests. The first is a transaction-level analysis comparing trades closed via stop-loss to manual trades. The second is a user-level analysis on a subsample of data that *excludes all stop-loss transactions*, comparing the underlying manual trading behavior of stop-loss users versus non-users.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator (dummy=1 for loss).\n- `TGI(t)`: Trading Gain Indicator (dummy=1 for gain).\n- `SL_Transaction`: A dummy variable equal to 1 for a roundtrip position closed by a stop-loss order.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who use stop losses.\n\n---\n\n### Data / Model Specification\n\nResults from the two key analyses are presented in Table 1 and Table 2.\n\n**Table 1: Transaction-Level Analysis of Stop-Loss Executions**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.5502*** | | \n| TLI × SL_Transaction | 2.1546*** | | \n| TGI | | 1.7695*** |\n| TGI × SL_Transaction | | 0.5225*** |\n\n**Table 2: User-Level Analysis on Manual (Non-Stop-Loss) Transactions**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.5610*** | | \n| TLI × SL_User | 0.9555** | | \n| TGI | | 1.8918*** |\n| TGI × SL_User | | 0.8419*** |\n\n*Notes: *** denotes significance at 1%, ** at 5%.*\n\n---\n\n### Question\n\nBased on the results in Table 1 and Table 2, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the tables and analyzing each statement:\n\n**Table 1 Analysis (Transaction-Level):**\n- For losses: Base hazard ratio = 0.5502, with stop-loss interaction = 2.1546\n- Combined effect for stop-loss losses: 0.5502 × 2.1546 = 1.1857\n- For gains: Base hazard ratio = 1.7695, with stop-loss interaction = 0.5225\n- Combined effect for stop-loss gains: 1.7695 × 0.5225 = 0.9246\n\n**Table 2 Analysis (User-Level, Manual Trades Only):**\n- For losses: Base hazard ratio = 0.5610, with SL_User interaction = 0.9555\n- Combined effect for SL users' losses: 0.5610 × 0.9555 = 0.5361\n- For gains: Base hazard ratio = 1.8918, with SL_User interaction = 0.8419\n- Combined effect for SL users' gains: 1.8918 × 0.8419 = 1.5927\n\n**Evaluating each statement:**\n\nA) **Correct** - In Table 2, stop-loss users have a lower hazard ratio for selling losers (0.5361) compared to non-users (0.5610), meaning they are MORE reluctant to sell losers in their manual trades. This supports the self-control hypothesis.\n\nB) **Incorrect** - The combined hazard ratio for selling a loser via stop-loss is 1.1857, which is above 1, not below.\n\nC) **Incorrect** - Table 2 shows stop-loss users actually exhibit MORE disposition effect in manual trades (they're more reluctant to sell losers), contradicting the learning hypothesis.\n\nD) **Correct** - In Table 1, for stop-loss transactions: hazard ratio for losses (1.1857) > hazard ratio for gains (0.9246), showing an inverted disposition effect.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 427, "Question": "### Background\n\n**Research Question.** How precise are the theoretical bounds on lifetime functionals when applied to both a theoretical mortality law and a real-world life table, and what do the results reveal about the practical severity of model risk from fractional age assumptions?\n\n**Setting.** An actuary has developed a theoretical framework for bounding the value of life-contingent claims for any lifetime `X` that has an Increasing Force of Mortality (IFR) and is consistent with a given life table. The framework produces a stochastic lower bound `L` (based on a Constant Force of Mortality assumption) and a stochastic upper bound `U` (the supremum bound). The key result is that for any valid lifetime `X`, `L ≤_st X ≤_st U`.\n\n**Variables and Parameters.**\n\n*   `e_x`: Expected future lifetime for a person aged `x` (units: years).\n*   `Ā_x`: Present value of a continuous life annuity for a person aged `x`, with interest rate `i=0.06` (dimensionless value).\n*   `E[h(L)]`: The value of a functional calculated using the stochastic lower bound `L`.\n*   `E[h(U)]`: The value of a functional calculated using the stochastic upper bound `U`.\n*   `E[h(U_μ*)]`: The value of a functional calculated using a tighter upper bound that requires optimizing over possible mortality rates at integer ages.\n\n---\n\n### Data / Model Specification\n\nThe following tables present calculated bounds for two scenarios: a theoretical lifetime distribution from Makeham’s law (where the true value is known) and a real-world German life table from 1999/2001.\n\n**Table 1.** Bounds for `e_30` and `Ā_30` under Makeham's Law (`i=0.06`)\n\n| | `e_30` (years) | `Ā_30` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 45.05944407 | 0.10554796 |\n| **True Value** | **45.06687713** | **0.10550557** |\n| `E[h(U_μ*)]` | 45.07059946 | 0.10548412 |\n| `E[h(U)]` | 45.08173562 | 0.10542090 |\n\n**Table 2.** Bounds for `e_50` and `Ā_50` using German Life Table 1999/2001 (`i=0.06`)\n\n| | `e_50` (years) | `Ā_50` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 27.27856559 | 0.24740336 |\n| `E[h(U_μ*)]` | 27.28957960 | 0.24723758 |\n| `E[h(U)]` | 27.29933908 | 0.24708628 |\n\nFor any increasing function `f(X)` (like `e_x`), the stochastic ordering `L ≤_st X ≤_st U` implies `E[f(L)] ≤ E[f(X)] ≤ E[f(U)]`. For any decreasing function `g(X)`, the implication is reversed: `E[g(L)] ≥ E[g(X)] ≥ E[g(U)]`.\n\n---\n\n### Question\n\nThe numerical results for the annuity `Ā_x` in both tables show `E[Ā_x(L)] > E[Ā_x(U)]`. This contradicts the theoretical expectation from the stochastic order `L ≤_st U`, which implies `E[f(L)] ≤ E[f(U)]` for any increasing function `f(X)`. Select all of the following statements that correctly diagnose this contradiction.", "Options": {"A": "The discrepancy is minor and likely stems from a numerical precision error in the integration routine used to calculate the annuity values, rather than a fundamental conceptual mistake.", "B": "The observed numerical pattern (`E[value(L)] > E[value(U)]`) would be correct if the annuity were a decreasing function of lifetime. A plausible error is that the authors misclassified the annuity's functional form when interpreting the results.", "C": "The present value of a continuous life annuity is an increasing function of lifetime `X`, meaning the theoretical relationship `E[Ā_x(L)] ≤ E[Ā_x(U)]` must hold. The reported numbers violate this property.", "D": "The paper's core crossing criterion is likely invalid for financial instruments, as non-linear discounting can break the conditions required for stochastic ordering."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses deep conceptual understanding by asking the user to diagnose a contradiction between theory and reported data. It uses a **Reverse-Reasoning** strategy. The correct options require identifying the correct theoretical property of the annuity and pinpointing the likely source of the error in the paper's application. Distractor C is a **Conceptual (Almost Right)** error that incorrectly blames the paper's core theorem. Distractor D is a **Conceptual** error that mischaracterizes a fundamental sign error as a minor numerical issue.", "qid": "427", "question": "### Background\n\n**Research Question.** How precise are the theoretical bounds on lifetime functionals when applied to both a theoretical mortality law and a real-world life table, and what do the results reveal about the practical severity of model risk from fractional age assumptions?\n\n**Setting.** An actuary has developed a theoretical framework for bounding the value of life-contingent claims for any lifetime `X` that has an Increasing Force of Mortality (IFR) and is consistent with a given life table. The framework produces a stochastic lower bound `L` (based on a Constant Force of Mortality assumption) and a stochastic upper bound `U` (the supremum bound). The key result is that for any valid lifetime `X`, `L ≤_st X ≤_st U`.\n\n**Variables and Parameters.**\n\n*   `e_x`: Expected future lifetime for a person aged `x` (units: years).\n*   `Ā_x`: Present value of a continuous life annuity for a person aged `x`, with interest rate `i=0.06` (dimensionless value).\n*   `E[h(L)]`: The value of a functional calculated using the stochastic lower bound `L`.\n*   `E[h(U)]`: The value of a functional calculated using the stochastic upper bound `U`.\n*   `E[h(U_μ*)]`: The value of a functional calculated using a tighter upper bound that requires optimizing over possible mortality rates at integer ages.\n\n---\n\n### Data / Model Specification\n\nThe following tables present calculated bounds for two scenarios: a theoretical lifetime distribution from Makeham’s law (where the true value is known) and a real-world German life table from 1999/2001.\n\n**Table 1.** Bounds for `e_30` and `Ā_30` under Makeham's Law (`i=0.06`)\n\n| | `e_30` (years) | `Ā_30` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 45.05944407 | 0.10554796 |\n| **True Value** | **45.06687713** | **0.10550557** |\n| `E[h(U_μ*)]` | 45.07059946 | 0.10548412 |\n| `E[h(U)]` | 45.08173562 | 0.10542090 |\n\n**Table 2.** Bounds for `e_50` and `Ā_50` using German Life Table 1999/2001 (`i=0.06`)\n\n| | `e_50` (years) | `Ā_50` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 27.27856559 | 0.24740336 |\n| `E[h(U_μ*)]` | 27.28957960 | 0.24723758 |\n| `E[h(U)]` | 27.29933908 | 0.24708628 |\n\nFor any increasing function `f(X)` (like `e_x`), the stochastic ordering `L ≤_st X ≤_st U` implies `E[f(L)] ≤ E[f(X)] ≤ E[f(U)]`. For any decreasing function `g(X)`, the implication is reversed: `E[g(L)] ≥ E[g(X)] ≥ E[g(U)]`.\n\n---\n\n### Question\n\nThe numerical results for the annuity `Ā_x` in both tables show `E[Ā_x(L)] > E[Ā_x(U)]`. This contradicts the theoretical expectation from the stochastic order `L ≤_st U`, which implies `E[f(L)] ≤ E[f(U)]` for any increasing function `f(X)`. Select all of the following statements that correctly diagnose this contradiction.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze why the numerical results show `E[Ā_x(L)] > E[Ā_x(U)]` when theory predicts the opposite.\n\nFirst, let me understand what we're dealing with:\n- `Ā_x` is the present value of a continuous life annuity\n- We have stochastic ordering: `L ≤_st X ≤_st U`\n- For increasing functions, this implies `E[f(L)] ≤ E[f(U)]`\n- For decreasing functions, this implies `E[g(L)] ≥ E[g(U)]`\n\nThe key question is: Is `Ā_x` an increasing or decreasing function of lifetime?\n\nA continuous life annuity pays a continuous stream of payments until death. Its present value is:\n`Ā_x = ∫₀^T e^(-δt) dt` where T is the remaining lifetime and δ is the force of interest.\n\nThis integral equals `(1 - e^(-δT))/δ`, which is clearly an increasing function of T (lifetime). A longer lifetime means more payments received, hence higher present value.\n\nGiven that `Ā_x` is increasing in lifetime, we should have `E[Ā_x(L)] ≤ E[Ā_x(U)]`. But the data shows the opposite pattern in both tables.\n\nAnalyzing each option:\n\nA) The differences are small (around 0.4-1.2% relative error), consistent with numerical precision issues in complex integration routines. This is plausible.\n\nB) The observed pattern would indeed be correct if the annuity were decreasing. Since the authors might have misclassified the functional form when interpreting results, this explains the contradiction.\n\nC) This correctly identifies that `Ā_x` is increasing and that the reported numbers violate the theoretical property.\n\nD) The stochastic ordering is based on general probability theory, not specific to financial instruments. The discounting is exponential (linear in log terms), not non-linear. This option is incorrect.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 301, "Question": "### Background\n\n**Research Question.** This case evaluates the potential impact of sample selection criteria on the validity of empirical findings concerning the relationship between CEO compensation and analyst forecasts.\n\n**Setting / Data-Generating Environment.** The study constructs its sample by merging ExecuComp, I/B/E/S, and COMPUSTAT data for U.S. firms from 1993–2003. A key filter is requiring a firm to have at least three analyst forecasts.\n\n**Variables & Parameters.**\n- `ACCURACY_it`: Analyst forecast accuracy for firm `i` in year `t`.\n- `OPTIONS_it`: Level of CEO options pay for firm `i` in year `t`.\n- `IN_SAMPLE_it`: A binary variable, equal to 1 if firm `i` is in the final sample, and 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nTo correct for potential selection bias arising from the requirement of having at least three analysts, a researcher proposes using a Heckman two-stage model. The goal is to get an unbiased estimate of the effect of `OPTIONS` on `ACCURACY`.\n\n---\n\n### Question\n\nWhich of the following are necessary and correct steps in specifying and interpreting this Heckman model? Select all that apply.", "Options": {"A": "The first stage should be a probit or logit model where the dependent variable is `IN_SAMPLE_it`.", "B": "The second-stage regression of `ACCURACY_it` on `OPTIONS_it` and other controls must include the inverse Mills ratio, calculated from the first stage, as an additional regressor.", "C": "The first-stage model must include at least one instrumental variable that is correlated with being in the sample but does not have a direct causal effect on `ACCURACY_it`.", "D": "The second-stage regression of `ACCURACY_it` on `OPTIONS_it` and other controls must include the predicted probability of being in the sample, `P(IN_SAMPLE_it=1)`, as an additional regressor."}, "Answer": ["A", "B", "C"], "pi_justification": "This item tests the procedural knowledge of specifying the Heckman two-stage correction for selection bias, a form of Scenario Application. Options A, B, and C correctly describe the three core components: the first-stage selection model, the need for a valid instrument, and the inclusion of the inverse Mills ratio in the second stage. Option D is a high-fidelity distractor representing a common procedural error (Step-Omission Error), where the predicted probability is incorrectly used instead of the inverse Mills ratio.", "qid": "301", "question": "### Background\n\n**Research Question.** This case evaluates the potential impact of sample selection criteria on the validity of empirical findings concerning the relationship between CEO compensation and analyst forecasts.\n\n**Setting / Data-Generating Environment.** The study constructs its sample by merging ExecuComp, I/B/E/S, and COMPUSTAT data for U.S. firms from 1993–2003. A key filter is requiring a firm to have at least three analyst forecasts.\n\n**Variables & Parameters.**\n- `ACCURACY_it`: Analyst forecast accuracy for firm `i` in year `t`.\n- `OPTIONS_it`: Level of CEO options pay for firm `i` in year `t`.\n- `IN_SAMPLE_it`: A binary variable, equal to 1 if firm `i` is in the final sample, and 0 otherwise.\n\n---\n\n### Data / Model Specification\n\nTo correct for potential selection bias arising from the requirement of having at least three analysts, a researcher proposes using a Heckman two-stage model. The goal is to get an unbiased estimate of the effect of `OPTIONS` on `ACCURACY`.\n\n---\n\n### Question\n\nWhich of the following are necessary and correct steps in specifying and interpreting this Heckman model? Select all that apply.", "model_evaluation_result": {"model_raw_response": "To determine the correct steps for specifying a Heckman two-stage model, I need to recall the key components of this procedure for correcting selection bias.\n\nThe Heckman two-stage model consists of:\n\n1. **First stage (Selection equation)**: A probit model predicting the probability of being selected into the sample\n2. **Second stage (Outcome equation)**: The regression of interest, augmented with a correction term\n\nLet me evaluate each option:\n\n**Option A**: \"The first stage should be a probit or logit model where the dependent variable is `IN_SAMPLE_it`.\"\n- This is CORRECT. The first stage models the selection process using a probit model (traditionally probit, though logit can be used) with the binary selection indicator as the dependent variable.\n\n**Option B**: \"The second-stage regression... must include the inverse Mills ratio, calculated from the first stage, as an additional regressor.\"\n- This is CORRECT. The inverse Mills ratio (lambda) is the key correction term in the Heckman model. It's calculated from the first-stage probit results and included in the second stage to correct for selection bias.\n\n**Option C**: \"The first-stage model must include at least one instrumental variable that is correlated with being in the sample but does not have a direct causal effect on `ACCURACY_it`.\"\n- This is CORRECT. For identification of the Heckman model, you need at least one exclusion restriction - a variable that affects selection but not the outcome directly. This provides the necessary variation to identify the selection correction.\n\n**Option D**: \"The second-stage regression... must include the predicted probability of being in the sample, `P(IN_SAMPLE_it=1)`, as an additional regressor.\"\n- This is INCORRECT. The Heckman model uses the inverse Mills ratio, not the predicted probability directly. The inverse Mills ratio is a transformation of the predicted probability that accounts for the truncation of the distribution.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 443, "Question": "### Background\n\n**Research Question.** How does an investor optimally liquidate inventory when market liquidity switches between high and low states?\n\n**Setting.** The market liquidity, parameterized by `\\lambda`, follows a two-state Markov chain: an \"active\" state (0, high `\\lambda_0`) and a \"slow\" state (1, low `\\lambda_1`), with `\\lambda_0 > \\lambda_1`. The investor observes the current state and adjusts their strategy. The analysis is in the continuous, infinite-horizon fluid limit for a power-law LOB where `\\Lambda(s) = \\lambda s^{-\\alpha}`.\n\n**Result.** A key finding is that the optimal spread is *higher* in the active (high liquidity) state than in the slow state. This seems paradoxical.\n\n---\n\n### Question\n\nSelect all of the following statements that provide a valid economic justification for why the investor optimally posts a higher, less aggressive spread precisely when the market is most liquid (the active state).", "Options": {"A": "The continuation value of holding inventory is higher in the active state, as the potential for future high-rate revenue generation is greater.", "B": "The higher spread is needed to offset the greater risk of the market switching from the active to the slow state.", "C": "The investor becomes more risk-averse in the active state and therefore demands a higher premium for each share sold.", "D": "Selling a share in the active state means forfeiting a valuable option to exploit favorable market conditions, thus requiring higher immediate compensation (a higher spread)."}, "Answer": ["A", "D"], "pi_justification": "REPLACE with Multiple Choice (Score: 9.0). This item assesses the deep economic reasoning behind the paradoxical result from the regime-switching model. It uses a Premise Packaging strategy, asking for the components of the correct 'option value of liquidity' argument. Distractor C is a 'True but Irrelevant' or 'Conceptual Misattribution' error, as risk aversion is not the primary driver in this model. Distractor D is a plausible but incorrect causal link, misinterpreting the role of the transition rates.", "qid": "443", "question": "### Background\n\n**Research Question.** How does an investor optimally liquidate inventory when market liquidity switches between high and low states?\n\n**Setting.** The market liquidity, parameterized by `\\lambda`, follows a two-state Markov chain: an \"active\" state (0, high `\\lambda_0`) and a \"slow\" state (1, low `\\lambda_1`), with `\\lambda_0 > \\lambda_1`. The investor observes the current state and adjusts their strategy. The analysis is in the continuous, infinite-horizon fluid limit for a power-law LOB where `\\Lambda(s) = \\lambda s^{-\\alpha}`.\n\n**Result.** A key finding is that the optimal spread is *higher* in the active (high liquidity) state than in the slow state. This seems paradoxical.\n\n---\n\n### Question\n\nSelect all of the following statements that provide a valid economic justification for why the investor optimally posts a higher, less aggressive spread precisely when the market is most liquid (the active state).", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to identify which statements correctly explain why an investor would post a higher spread in the active (high liquidity) state.\n\nLet me analyze each option:\n\n**Option A**: \"The continuation value of holding inventory is higher in the active state, as the potential for future high-rate revenue generation is greater.\"\n\nThis makes economic sense. In the active state with high λ₀, the investor can execute more trades at favorable prices. The inventory becomes more valuable because it represents more future trading opportunities. Therefore, the investor requires higher compensation (higher spread) to part with each share.\n\n**Option B**: \"The higher spread is needed to offset the greater risk of the market switching from the active to the slow state.\"\n\nThis is also valid. When in the active state, there's a risk of transitioning to the slow state where trading conditions worsen. The investor needs compensation for this transition risk, which justifies a higher spread in the active state.\n\n**Option C**: \"The investor becomes more risk-averse in the active state and therefore demands a higher premium for each share sold.\"\n\nThis doesn't make economic sense. There's no reason why an investor's risk preferences would change based on market liquidity states. Risk aversion is typically a characteristic of the investor, not the market state.\n\n**Option D**: \"Selling a share in the active state means forfeiting a valuable option to exploit favorable market conditions, thus requiring higher immediate compensation (a higher spread).\"\n\nThis is essentially a restatement of option A using real options language. In the active state, holding inventory provides a valuable option to trade in favorable conditions. Selling now means giving up this option, which requires compensation through a higher spread.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 319, "Question": "### Background\n\nThe study investigates the factors driving a remuneration committee's choice between different long-term incentive instruments, primarily executive stock options versus Long-Term Incentive Plans (LTIPs) based on relative Total Shareholder Return (TSR).\n\n### Data / Model Specification\n\n> **Executive Share Option:** Payoff is `max(S_T - K, 0)`, where `S_T` is the stock price at maturity and `K` is the strike price. The reward is based on the absolute increase in share price.\n\n> **Relative TSR LTIP:** Payoff depends on the company's TSR ranking against a peer group. This filters out common market-wide or industry-wide stock price movements.\n\n> **Evidence (Consultant):** \"...option schemes amongst public companies are now back in favour, as it were.\"\n\n> **Evidence (CEO & NXD):** Committee members' choices were influenced by their \"personal wishes\" and whether a scheme was the \"flavour of the moment.\"\n\n### Question\n\nWhich of the following statements represent valid conclusions or applications of the principles discussed regarding the choice between executive stock options and relative TSR LTIPs?\n\nSelect all that apply.", "Options": {"A": "An executive stock option effectively filters out broad market trends, rewarding managers only for firm-specific performance.", "B": "A grant of at-the-money stock options is economically equivalent to a grant of restricted shares under an LTIP, provided their grant-date fair values are identical.", "C": "The evidence suggests that mimetic isomorphism—adopting practices that are currently 'in favour'—is a significant factor in the choice of incentive schemes.", "D": "A relative TSR LTIP is designed to reward managers for outperforming peers, meaning it can pay out even if the absolute share price falls, as long as it falls less than the peer group average."}, "Answer": ["C", "D"], "pi_justification": "This item assesses understanding of the different incentive properties of options vs. LTIPs and the institutional factors influencing their selection. It uses a 'Reverse-Reasoning' strategy, asking for valid conclusions from the provided facts. Option A correctly describes the function of relative performance evaluation in an LTIP. Option C correctly identifies the institutional theory concept (isomorphism) described by the interviewees. Distractor B is a 'Conceptual Opposite', as options reward absolute performance, not relative. Distractor D is a sophisticated 'Conceptual Distractor' that equates accounting value with economic substance, ignoring the fundamental differences in payoff structure and risk exposure, a key distinction.", "qid": "319", "question": "### Background\n\nThe study investigates the factors driving a remuneration committee's choice between different long-term incentive instruments, primarily executive stock options versus Long-Term Incentive Plans (LTIPs) based on relative Total Shareholder Return (TSR).\n\n### Data / Model Specification\n\n> **Executive Share Option:** Payoff is `max(S_T - K, 0)`, where `S_T` is the stock price at maturity and `K` is the strike price. The reward is based on the absolute increase in share price.\n\n> **Relative TSR LTIP:** Payoff depends on the company's TSR ranking against a peer group. This filters out common market-wide or industry-wide stock price movements.\n\n> **Evidence (Consultant):** \"...option schemes amongst public companies are now back in favour, as it were.\"\n\n> **Evidence (CEO & NXD):** Committee members' choices were influenced by their \"personal wishes\" and whether a scheme was the \"flavour of the moment.\"\n\n### Question\n\nWhich of the following statements represent valid conclusions or applications of the principles discussed regarding the choice between executive stock options and relative TSR LTIPs?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) An executive stock option effectively filters out broad market trends, rewarding managers only for firm-specific performance.\nThis is FALSE. Executive stock options pay based on absolute share price increase (max(S_T - K, 0)). They do NOT filter out market trends - if the entire market rises, the option becomes more valuable regardless of firm-specific performance.\n\nB) A grant of at-the-money stock options is economically equivalent to a grant of restricted shares under an LTIP, provided their grant-date fair values are identical.\nThis is FALSE. Stock options and restricted shares have fundamentally different payoff structures. Options have asymmetric payoffs (limited downside, unlimited upside) while restricted shares have symmetric exposure to stock price movements. Even with identical grant-date fair values, they are not economically equivalent.\n\nC) The evidence suggests that mimetic isomorphism—adopting practices that are currently 'in favour'—is a significant factor in the choice of incentive schemes.\nThis is TRUE. The evidence directly supports this - the consultant notes that \"option schemes amongst public companies are now back in favour\" and the CEO & NXD evidence indicates choices were influenced by what was the \"flavour of the moment.\" This clearly demonstrates mimetic isomorphism (copying popular practices).\n\nD) A relative TSR LTIP is designed to reward managers for outperforming peers, meaning it can pay out even if the absolute share price falls, as long as it falls less than the peer group average.\nThis is TRUE. The background states that relative TSR LTIP \"filters out common market-wide or industry-wide stock price movements\" and payoff \"depends on the company's TSR ranking against a peer group.\" This means it rewards relative performance, so a company that declines less than peers would still rank well and receive a payout.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 415, "Question": "### Background\n\n**Research Question.** Do firms strategically time their equity issues to occur after public disclosures, and do firm characteristics associated with information asymmetry predict this timing?\n\n**Setting / Data-Generating Environment.** The study examines the timing of seasoned equity issues relative to quarterly earnings releases. Two competing hypotheses are considered:\n- **Hypothesis 1 (Adverse Selection Timing)**: Firms issue after releases to minimize information asymmetry.\n- **Alternative Hypothesis (Operating Cycle)**: Issue clustering is driven by institutional factors (e.g., board meetings) that align with the quarterly reporting cycle, not by information asymmetry.\n\n**Variables & Parameters.**\n- `D_I`: The number of days between the equity issue and the *preceding* earnings release.\n- `ln(size)`: Log of the market value of the firm's equity.\n- `ln(issue size)`: Log of the dollar value of the primary share issue.\n\n### Data / Model Specification\n\nA one-sided Wilcoxon rank-sum test confirms that issues cluster after earnings releases (results not shown). To distinguish between the competing hypotheses, the following regression is estimated:\n  \nD_I = \\beta_0 + \\beta_1 \\ln(\\text{size}) + \\beta_2 \\ln(\\text{issue size}) + \\epsilon \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Regression of Issue Timing on Firm and Issue Size**\n\n| Dependent Variable | Constant | ln(size) | ln(issue size) | N |\n|:-------------------|:---------|:---------|:---------------|:--|\n| `D_I`              | 40.263*  | 0.321*   | -0.397*        | 563|\n|                    | (1.273)  | (0.060)  | (0.030)        |   |\n\n*White heteroskedasticity-robust standard errors in parentheses. *Significant at 1% level.*\n\n### Question\n\nThe study aims to distinguish the 'Adverse Selection Timing' hypothesis from the 'Operating Cycle' hypothesis. Based on the provided results, select all statements that accurately reflect the evidence and the logic of the identification strategy.", "Options": {"A": "The negative coefficient on `ln(issue size)` supports the adverse selection hypothesis by suggesting that for larger issues, where potential 'lemons' costs are higher, firms have a stronger incentive to issue quickly after a release.", "B": "The 'Operating Cycle' hypothesis predicts that the coefficient on `ln(size)` should be positive and the coefficient on `ln(issue size)` should be negative.", "C": "The positive coefficient on `ln(size)` supports the adverse selection hypothesis by suggesting that larger firms, which face less information asymmetry, have less incentive to issue immediately after an earnings release.", "D": "The fact that issues cluster after earnings releases is sufficient on its own to reject the 'Operating Cycle' hypothesis."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the identification strategy used to distinguish the adverse selection hypothesis from the operating cycle alternative. It uses a 'premise packaging' strategy. Options A and B correctly state the nuanced predictions of the adverse selection model. Distractor C tests a common logical flaw, confusing correlation with causation (clustering is consistent with both hypotheses). Distractor D presents a conceptual opposite for the alternative hypothesis's prediction (it predicts zero coefficients).", "qid": "415", "question": "### Background\n\n**Research Question.** Do firms strategically time their equity issues to occur after public disclosures, and do firm characteristics associated with information asymmetry predict this timing?\n\n**Setting / Data-Generating Environment.** The study examines the timing of seasoned equity issues relative to quarterly earnings releases. Two competing hypotheses are considered:\n- **Hypothesis 1 (Adverse Selection Timing)**: Firms issue after releases to minimize information asymmetry.\n- **Alternative Hypothesis (Operating Cycle)**: Issue clustering is driven by institutional factors (e.g., board meetings) that align with the quarterly reporting cycle, not by information asymmetry.\n\n**Variables & Parameters.**\n- `D_I`: The number of days between the equity issue and the *preceding* earnings release.\n- `ln(size)`: Log of the market value of the firm's equity.\n- `ln(issue size)`: Log of the dollar value of the primary share issue.\n\n### Data / Model Specification\n\nA one-sided Wilcoxon rank-sum test confirms that issues cluster after earnings releases (results not shown). To distinguish between the competing hypotheses, the following regression is estimated:\n  \nD_I = \\beta_0 + \\beta_1 \\ln(\\text{size}) + \\beta_2 \\ln(\\text{issue size}) + \\epsilon \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Regression of Issue Timing on Firm and Issue Size**\n\n| Dependent Variable | Constant | ln(size) | ln(issue size) | N |\n|:-------------------|:---------|:---------|:---------------|:--|\n| `D_I`              | 40.263*  | 0.321*   | -0.397*        | 563|\n|                    | (1.273)  | (0.060)  | (0.030)        |   |\n\n*White heteroskedasticity-robust standard errors in parentheses. *Significant at 1% level.*\n\n### Question\n\nThe study aims to distinguish the 'Adverse Selection Timing' hypothesis from the 'Operating Cycle' hypothesis. Based on the provided results, select all statements that accurately reflect the evidence and the logic of the identification strategy.", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement based on the research design and results.\n\nThe study examines whether firms time equity issues after earnings releases due to information asymmetry concerns (Adverse Selection Timing hypothesis) or due to institutional factors like board meetings (Operating Cycle hypothesis).\n\nKey findings:\n- Issues cluster after earnings releases (Wilcoxon test)\n- Regression shows: D_I = 40.263 + 0.321*ln(size) - 0.397*ln(issue size)\n- D_I measures days between issue and preceding earnings release\n\nAnalyzing each option:\n\nA) The negative coefficient on ln(issue size) (-0.397) means larger issues occur closer to earnings releases (smaller D_I). This supports the adverse selection hypothesis because firms with larger issues (higher potential information asymmetry costs) would want to issue when information asymmetry is lowest (right after earnings releases). ✓\n\nB) The Operating Cycle hypothesis suggests timing is driven by institutional factors unrelated to information asymmetry. It would not predict any systematic relationship between firm/issue characteristics and timing based on information asymmetry considerations. This statement incorrectly attributes specific predictions to the Operating Cycle hypothesis. ✗\n\nC) The positive coefficient on ln(size) (0.321) means larger firms issue further from earnings releases (larger D_I). This supports the adverse selection hypothesis because larger firms typically have less information asymmetry, so they have less need to time issues close to earnings releases. ✓\n\nD) The clustering alone cannot distinguish between hypotheses since both predict clustering - one due to information asymmetry, the other due to institutional factors. The regression analysis examining how firm characteristics affect timing is what allows discrimination between hypotheses. ✗\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 346, "Question": "### Background\n\n**Research Question.** This case investigates how to use principal component analysis (PCA) to distill predictive signals from a set of highly correlated moving average predictors.\n\n**Setting and Sample.** The analysis uses monthly excess returns on the S&P 500. Predictors are the first principal components extracted from two distinct sets of `MADP` variables: short-term lags (2-10 days) and long-term lags (20-200 days).\n\n### Data / Model Specification\n\nThe principal component predictive regression using the first component is:\n  \nr_{t+1} = \\alpha + \\beta_1 F_{1,t} + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Principal Component Regression Results**\n\n| Predictor `F_{1,t}` | In-Sample `R^2` (%) | Out-of-Sample `R_{os}^2` (%) |\n|:---|---:|---:|\n| `F_{1,t}^{\\text{short}}` (from short-term MADPs) | 2.75 | 2.62 |\n| `F_{1,t}^{\\text{long}}` (from long-term MADPs) | 0.01 | -1.13 |\n\n\n### Question\n\nRegarding the use of principal component analysis (PCA) in this context, select all statements that are methodologically sound or correctly interpret the empirical results in Table 1.", "Options": {"A": "A negative `R_{os}^2` for `F_{1,t}^{\\text{long}}` indicates that the model has significant power to predict market downturns.", "B": "A primary motivation for using PCA is to create a single, powerful predictor from a set of highly correlated `MADP` variables, thus avoiding multicollinearity issues.", "C": "The stark contrast in `R_{os}^2` between `F_{1,t}^{\\text{short}}` (2.62%) and `F_{1,t}^{\\text{long}}` (-1.13%) provides robust evidence that the predictive signal is concentrated in very recent price movements.", "D": "The first principal component `F_{1,t}^{\\text{short}}` is interpreted as the simple arithmetic average of the `MADP(2)` through `MADP(10)` predictors."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a mix of **Premise/Assumption Packaging** (Option A) and **Atomic Decomposition** (Option B) to test the rationale for the method and the interpretation of its results. Option A assesses understanding of why PCA is used. Option B assesses the ability to draw a clear conclusion from the results table. Distractors target common misunderstandings: C confuses a principal component with a simple average (Conceptual Error), and D misinterprets the meaning of a negative out-of-sample R-squared (it means the model is worse than the historical mean, not that it predicts negative outcomes).", "qid": "346", "question": "### Background\n\n**Research Question.** This case investigates how to use principal component analysis (PCA) to distill predictive signals from a set of highly correlated moving average predictors.\n\n**Setting and Sample.** The analysis uses monthly excess returns on the S&P 500. Predictors are the first principal components extracted from two distinct sets of `MADP` variables: short-term lags (2-10 days) and long-term lags (20-200 days).\n\n### Data / Model Specification\n\nThe principal component predictive regression using the first component is:\n  \nr_{t+1} = \\alpha + \\beta_1 F_{1,t} + \\varepsilon_{t+1} \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Principal Component Regression Results**\n\n| Predictor `F_{1,t}` | In-Sample `R^2` (%) | Out-of-Sample `R_{os}^2` (%) |\n|:---|---:|---:|\n| `F_{1,t}^{\\text{short}}` (from short-term MADPs) | 2.75 | 2.62 |\n| `F_{1,t}^{\\text{long}}` (from long-term MADPs) | 0.01 | -1.13 |\n\n\n### Question\n\nRegarding the use of principal component analysis (PCA) in this context, select all statements that are methodologically sound or correctly interpret the empirical results in Table 1.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) A negative `R_{os}^2` for `F_{1,t}^{\\text{long}}` indicates that the model has significant power to predict market downturns.\nThis is incorrect. A negative out-of-sample R² means the model performs worse than using the historical mean as a forecast. It indicates poor predictive performance, not an ability to predict market downturns.\n\nB) A primary motivation for using PCA is to create a single, powerful predictor from a set of highly correlated `MADP` variables, thus avoiding multicollinearity issues.\nThis is correct. PCA is commonly used to extract principal components from correlated predictors, reducing dimensionality and addressing multicollinearity problems in regression analysis.\n\nC) The stark contrast in `R_{os}^2` between `F_{1,t}^{\\text{short}}` (2.62%) and `F_{1,t}^{\\text{long}}` (-1.13%) provides robust evidence that the predictive signal is concentrated in very recent price movements.\nThis is correct. The positive out-of-sample R² for short-term MADPs versus the negative R² for long-term MADPs suggests that predictive power comes from recent price movements rather than longer-term trends.\n\nD) The first principal component `F_{1,t}^{\\text{short}}` is interpreted as the simple arithmetic average of the `MADP(2)` through `MADP(10)` predictors.\nThis is incorrect. The first principal component is a weighted linear combination of the original variables where weights are determined by maximizing variance, not a simple arithmetic average.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 401, "Question": "### Background\n\n**Research Question.** This case investigates the relative importance of three investor characteristics—sophistication, experience (proxied by age), and the use of stop-loss orders—in mitigating the disposition effect. The analysis aims to determine which factors remain significant when all are considered simultaneously.\n\n**Setting / Data-Generating Environment.** The analysis uses a full Cox proportional hazard model including all three characteristics and their interactions with the trading gain/loss indicators. This allows for a direct comparison of the marginal contribution of each factor, controlling for the others.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator, a dummy variable equal to 1 if a position is at a loss.\n- `Soph`: A time-invariant dummy variable equal to 1 for sophisticated investors (warrant traders).\n- `Age`: A continuous variable defined as `(investor's actual age - 18) / 10`.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who have used a stop-loss order at least once.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a comprehensive model with multiple interaction terms. The key results from this full specification are summarized in Table 1.\n\n**Table 1: Combined Model of Predictive Variables**\n\n| Variable | Hazard Ratio (Loss Model) |\n| :--- | :--- |\n| TLI | 0.3948*** |\n| TLI × Sophistication | 1.039 |\n| TLI × Age | 1.1054*** |\n| TLI × SL_User | 1.1700*** |\n\n*Notes: *** denotes significance at the 1% level.*\n\n---\n\n### Question\n\nIn the full model presented in Table 1, the interaction `TLI × Sophistication` is statistically insignificant. However, in a simpler model that excluded `Age` and `SL_User`, this same interaction was found to be significant. Select all statistically valid explanations for this change in significance.", "Options": {"A": "The significance in the simpler model was likely influenced by omitted variable bias, as `Sophistication` may have been capturing the correlated effect of `Age` (experience), which is now controlled for directly.", "B": "The paper notes that sophisticated investors tend to be older, suggesting a correlation between the `Sophistication` and `Age` variables. Including both in the model can cause multicollinearity, inflating standard errors and potentially rendering one of the correlated terms insignificant.", "C": "The Proportional Hazards assumption was violated for the `Sophistication` variable in the full model but not the simple model, invalidating the result.", "D": "The change in significance indicates that the effect of sophistication on selling losers is entirely mediated by stop-loss use, meaning sophisticated investors only sell losers more often because they use stop-losses."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret a common statistical artifact: a change in coefficient significance when new control variables are added. It uses a 'Reverse-Reasoning' strategy. Options A and B represent the two canonical explanations for this phenomenon (omitted variable bias and multicollinearity), both of which are correct. Distractor C is a 'Factual Error' distractor, as the paper states the assumption was violated for `Age`, not `Sophistication`. Distractor D presents a plausible but unsupported causal story ('True but Irrelevant/Unsupported'), a common error in interpretation.", "qid": "401", "question": "### Background\n\n**Research Question.** This case investigates the relative importance of three investor characteristics—sophistication, experience (proxied by age), and the use of stop-loss orders—in mitigating the disposition effect. The analysis aims to determine which factors remain significant when all are considered simultaneously.\n\n**Setting / Data-Generating Environment.** The analysis uses a full Cox proportional hazard model including all three characteristics and their interactions with the trading gain/loss indicators. This allows for a direct comparison of the marginal contribution of each factor, controlling for the others.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator, a dummy variable equal to 1 if a position is at a loss.\n- `Soph`: A time-invariant dummy variable equal to 1 for sophisticated investors (warrant traders).\n- `Age`: A continuous variable defined as `(investor's actual age - 18) / 10`.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who have used a stop-loss order at least once.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a comprehensive model with multiple interaction terms. The key results from this full specification are summarized in Table 1.\n\n**Table 1: Combined Model of Predictive Variables**\n\n| Variable | Hazard Ratio (Loss Model) |\n| :--- | :--- |\n| TLI | 0.3948*** |\n| TLI × Sophistication | 1.039 |\n| TLI × Age | 1.1054*** |\n| TLI × SL_User | 1.1700*** |\n\n*Notes: *** denotes significance at the 1% level.*\n\n---\n\n### Question\n\nIn the full model presented in Table 1, the interaction `TLI × Sophistication` is statistically insignificant. However, in a simpler model that excluded `Age` and `SL_User`, this same interaction was found to be significant. Select all statistically valid explanations for this change in significance.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify statistically valid explanations for why the TLI × Sophistication interaction became insignificant when Age and SL_User were added to the model.\n\nLet me analyze each option:\n\n**Option A**: This describes omitted variable bias - when Age and SL_User were excluded from the simpler model, the Sophistication variable may have been capturing their effects. Once these variables are included directly, Sophistication loses its apparent significance. This is a classic and statistically valid explanation for changes in coefficient significance.\n\n**Option B**: The paper explicitly states that sophisticated investors tend to be older, indicating correlation between Sophistication and Age. When correlated variables are included together, multicollinearity can inflate standard errors, making previously significant coefficients insignificant. This is also a statistically valid explanation.\n\n**Option C**: While violation of the Proportional Hazards assumption could affect results, there's no evidence provided that this assumption was tested or that it differs between models. Without such evidence, this is speculative rather than a valid explanation based on the information given.\n\n**Option D**: This suggests complete mediation by stop-loss use. However, if sophistication's effect were entirely mediated by stop-loss use, we would expect the main effect of Sophistication (not shown) to also become insignificant. More importantly, complete mediation is a very specific claim that would require additional statistical tests beyond what's presented. The change in significance of an interaction term alone doesn't prove complete mediation.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 273, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical foundation and empirical implementation of the Empirical Average Cost of Capital (EACC), a novel measure derived from accounting data based on a fundamental microeconomic equilibrium condition.\n\n**Setting.** The model considers a firm in a long-run equilibrium where the marginal benefit of capital equals its marginal cost. The analysis links this economic principle to observable financial statement variables and operationalizes it via a simple time-series regression.\n\n---\n\n### Data / Model Specification\n\nThe model is founded on the Marshallian equilibrium condition, which in dollar terms is:\n\n  \n\\text{Realized Return on Capital} = \\text{Required Return on Capital} \n \n\nThe EACC model proposes that the realized dollar return on capital can be proxied by `NOPAT` and that the required return can be expressed as `K` times `Total Capital`. The model assumes the firm operates in an equilibrium where economic profit is zero. Empirically, the EACC is estimated via the following bivariate regression model:\n\n  \n\\text{NOPAT}_{i,t} = K_i (\\text{TOTAL CAPITAL}_{i,t-1}) + e_{i,t} \n\\quad \\text{(Eq. 1)}\n \n\n---\n\nAccording to the paper's theoretical framework, which of the following statements are valid assumptions or direct implications of the EACC model?", "Options": {"A": "If a firm consistently generates positive economic profit (e.g., due to a sustainable competitive advantage), its EACC estimate `K` will be an upwardly biased estimate of its true WACC.", "B": "The model's use of lagged total capital (`TOTAL CAPITAL_{i,t-1}`) as the regressor completely eliminates the possibility of endogeneity in the OLS estimation of `K_i`.", "C": "The assumption of zero economic profit is necessary to interpret the estimated coefficient `K` as being theoretically equivalent to the firm's WACC.", "D": "The EACC model requires fewer subjective analyst inputs (e.g., risk-free rate, equity beta, market portfolio return) compared to a conventional WACC calculation using the CAPM."}, "Answer": ["A", "C", "D"], "pi_justification": "This item assesses the user's understanding of the core theoretical underpinnings and assumptions of the EACC model. It uses a 'Reverse-Reasoning' strategy by asking for the necessary pre-conditions and implications of the model. Option A tests the critical link between zero economic profit and the interpretation of K. Option B tests the ability to reason about what happens when a key assumption is violated. Option C tests the user's knowledge of the model's practical advantages as described in the paper and appendix. The distractor (D) is a 'Conceptual Distractor' (Almost Right); while using lagged variables can mitigate some endogeneity issues, it does not eliminate all sources, such as omitted variable bias where the omitted variable is serially correlated, a common issue in finance.", "qid": "273", "question": "### Background\n\n**Research Question.** This problem explores the theoretical foundation and empirical implementation of the Empirical Average Cost of Capital (EACC), a novel measure derived from accounting data based on a fundamental microeconomic equilibrium condition.\n\n**Setting.** The model considers a firm in a long-run equilibrium where the marginal benefit of capital equals its marginal cost. The analysis links this economic principle to observable financial statement variables and operationalizes it via a simple time-series regression.\n\n---\n\n### Data / Model Specification\n\nThe model is founded on the Marshallian equilibrium condition, which in dollar terms is:\n\n  \n\\text{Realized Return on Capital} = \\text{Required Return on Capital} \n \n\nThe EACC model proposes that the realized dollar return on capital can be proxied by `NOPAT` and that the required return can be expressed as `K` times `Total Capital`. The model assumes the firm operates in an equilibrium where economic profit is zero. Empirically, the EACC is estimated via the following bivariate regression model:\n\n  \n\\text{NOPAT}_{i,t} = K_i (\\text{TOTAL CAPITAL}_{i,t-1}) + e_{i,t} \n\\quad \\text{(Eq. 1)}\n \n\n---\n\nAccording to the paper's theoretical framework, which of the following statements are valid assumptions or direct implications of the EACC model?", "model_evaluation_result": {"model_raw_response": "Looking at each statement carefully:\n\n**A) If a firm consistently generates positive economic profit (e.g., due to a sustainable competitive advantage), its EACC estimate `K` will be an upwardly biased estimate of its true WACC.**\n\nThis is TRUE. The model assumes zero economic profit equilibrium where Realized Return = Required Return. If a firm consistently earns positive economic profit, then Realized Return > Required Return. Since EACC estimates K from the realized returns (NOPAT/Capital), it will capture this higher realized return, making K > true WACC.\n\n**B) The model's use of lagged total capital (`TOTAL CAPITAL_{i,t-1}`) as the regressor completely eliminates the possibility of endogeneity in the OLS estimation of `K_i`.**\n\nThis is FALSE. While using lagged values can help reduce simultaneity bias, it doesn't \"completely eliminate\" all endogeneity concerns. There could still be issues like omitted variable bias or measurement error that cause endogeneity.\n\n**C) The assumption of zero economic profit is necessary to interpret the estimated coefficient `K` as being theoretically equivalent to the firm's WACC.**\n\nThis is TRUE. The model's theoretical foundation relies on the equilibrium condition where Realized Return = Required Return (WACC). Without the zero economic profit assumption, the estimated K would reflect the realized return rather than the required return, and thus wouldn't equal WACC.\n\n**D) The EACC model requires fewer subjective analyst inputs (e.g., risk-free rate, equity beta, market portfolio return) compared to a conventional WACC calculation using the CAPM.**\n\nThis is TRUE. The EACC model estimates K directly from accounting data (NOPAT and Total Capital) through regression. In contrast, CAPM-based WACC requires subjective inputs like the risk-free rate, equity beta, and market risk premium.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 286, "Question": "### Background\n\nOption pricing models require a volatility input. This can be estimated from past asset returns (historical volatility) or backed out from current option prices (implied volatility).\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the differences between historical and implied volatility? Select all that apply.", "Options": {"A": "Historical volatility is a statistical measure calculated as the standard deviation of an asset's past returns.", "B": "Implied volatility is considered forward-looking because it reflects the market's current expectation of future price dispersion.", "C": "Historical volatility is derived from current option prices, making it a real-time measure of market sentiment.", "D": "Implied volatility is always a more stable and less noisy measure than historical volatility."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the fundamental definitions distinguishing the two core volatility concepts in the paper. It replaces Q1 of the source QA. Distractors are designed by swapping the definitions (B) or introducing plausible but incorrect properties (D).", "qid": "286", "question": "### Background\n\nOption pricing models require a volatility input. This can be estimated from past asset returns (historical volatility) or backed out from current option prices (implied volatility).\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the differences between historical and implied volatility? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) Historical volatility is a statistical measure calculated as the standard deviation of an asset's past returns.\n- This is correct. Historical volatility is indeed calculated as the standard deviation of past returns over a specified period.\n\nB) Implied volatility is considered forward-looking because it reflects the market's current expectation of future price dispersion.\n- This is correct. Implied volatility is extracted from current option prices and represents the market's expectation of future volatility.\n\nC) Historical volatility is derived from current option prices, making it a real-time measure of market sentiment.\n- This is incorrect. Historical volatility is derived from past asset returns, not option prices. It's implied volatility that comes from option prices.\n\nD) Implied volatility is always a more stable and less noisy measure than historical volatility.\n- This is incorrect. Implied volatility can actually be quite volatile and noisy, especially during market stress or around significant events. It's not necessarily more stable than historical volatility.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 436, "Question": "### Background\n\n**Research Question.** A central challenge in empirical asset pricing is accurately measuring a firm's exposure to risk factors, such as exchange rate fluctuations. Standard models, like the augmented CAPM proposed by Jorion, often find surprisingly weak evidence of such exposure. This paper argues that this weak evidence may be a statistical artifact stemming from the model's failure to account for the correlation between exchange rate movements and overall stock market returns.\n\n**Setting / Data-Generating Environment.** The analysis compares two methods for estimating firm-level currency exposure for European firms from 1999-2011: the standard Jorion model and a three-step orthogonalization procedure. The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Model 1: The Standard Jorion Model**\nThis model estimates the currency exposure coefficient `\\beta_{1i}`.\n\n**Model 2: The Three-Step Orthogonalization Procedure**\nThis model estimates the currency exposure coefficient `\\beta_{is}` using orthogonal risk factors.\n\n**Table 1. Comparison of Final Estimated Currency Exposure (Euro Stoxx TMI)**\n\n| Currency | Model | Avg. `\\beta` (significant firms) | Total Significant (%) |\n|:---|:---|:---:|:---:|\n| **CHF** | Orthogonalized | 1.88 | 52.74% |\n| | Jorion (Augmented) | 1.05 | 11.34% |\n\n### Question\n\nUsing the results for the CHF/Euro exchange rate in Table 1, select all statements that correctly quantify and interpret the impact of using the Orthogonalized model compared to the standard Jorion model.", "Options": {"A": "The average exposure coefficient (`\\beta`) for significantly affected firms is substantially larger in the Orthogonalized model (1.88) than in the Jorion model (1.05), suggesting the Jorion model understates not just the prevalence but also the magnitude of the exposure.", "B": "The results suggest that the Jorion model is more conservative and likely provides a more reliable estimate of currency exposure by avoiding the complexities of the three-step procedure.", "C": "The Orthogonalized model identifies a proportion of significantly exposed firms (52.74%) that is more than four times greater than the proportion identified by the Jorion model (11.34%).", "D": "The Jorion model finds that over 20% of firms are significantly exposed to CHF risk, which is only moderately lower than the Orthogonalized model's finding."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Computational Judgment and Atomic Decomposition strategies. It tests the ability to extract specific data from a table, perform a simple calculation (ratio), and draw the correct conclusion about the methodological comparison. Distractors are based on a Numerical Error (C) and a Conceptual Opposite to the paper's findings (D).", "qid": "436", "question": "### Background\n\n**Research Question.** A central challenge in empirical asset pricing is accurately measuring a firm's exposure to risk factors, such as exchange rate fluctuations. Standard models, like the augmented CAPM proposed by Jorion, often find surprisingly weak evidence of such exposure. This paper argues that this weak evidence may be a statistical artifact stemming from the model's failure to account for the correlation between exchange rate movements and overall stock market returns.\n\n**Setting / Data-Generating Environment.** The analysis compares two methods for estimating firm-level currency exposure for European firms from 1999-2011: the standard Jorion model and a three-step orthogonalization procedure. The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Model 1: The Standard Jorion Model**\nThis model estimates the currency exposure coefficient `\\beta_{1i}`.\n\n**Model 2: The Three-Step Orthogonalization Procedure**\nThis model estimates the currency exposure coefficient `\\beta_{is}` using orthogonal risk factors.\n\n**Table 1. Comparison of Final Estimated Currency Exposure (Euro Stoxx TMI)**\n\n| Currency | Model | Avg. `\\beta` (significant firms) | Total Significant (%) |\n|:---|:---|:---:|:---:|\n| **CHF** | Orthogonalized | 1.88 | 52.74% |\n| | Jorion (Augmented) | 1.05 | 11.34% |\n\n### Question\n\nUsing the results for the CHF/Euro exchange rate in Table 1, select all statements that correctly quantify and interpret the impact of using the Orthogonalized model compared to the standard Jorion model.", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to compare the results between the Orthogonalized model and the Jorion model for CHF/Euro exchange rate exposure.\n\nLet me analyze each option:\n\n**Option A**: \n- Orthogonalized model: Average β = 1.88 for significant firms\n- Jorion model: Average β = 1.05 for significant firms\n- 1.88 is indeed substantially larger than 1.05 (about 79% higher)\n- This correctly states that the Jorion model understates both prevalence and magnitude\n- **This is CORRECT**\n\n**Option B**:\n- The background explicitly states that weak evidence from the Jorion model \"may be a statistical artifact\" due to not accounting for correlation between exchange rates and stock market returns\n- The orthogonalization procedure addresses this statistical issue\n- There's no evidence suggesting the Jorion model is more reliable; rather, it appears to have a methodological limitation\n- **This is INCORRECT**\n\n**Option C**:\n- Orthogonalized model: 52.74% of firms significantly exposed\n- Jorion model: 11.34% of firms significantly exposed\n- Ratio: 52.74% ÷ 11.34% = 4.65\n- This is indeed more than four times greater\n- **This is CORRECT**\n\n**Option D**:\n- The Jorion model finds 11.34% of firms significantly exposed\n- This is not \"over 20%\" - it's actually just over 11%\n- **This is INCORRECT**\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 352, "Question": "### Background\nA study investigates the counterintuitive hypothesis that stricter public (i.e., government-enforced) regulations against self-dealing may be associated with lower corporate valuations. The analysis uses a sample of 4,634 firms from 22 countries.\n\n### Data / Model Specification\nThe hypothesis is tested using the following OLS regression model:\n\n  \n\\text{Tobin's Q} = \\alpha + \\dots + \\beta_{2}\\mathrm{PUBLIC} + \\dots + \\text{Controls} + \\varepsilon \\quad \\text{(Eq. 1)}\n \n\nWhere `Tobin's Q` is a proxy for corporate valuation and `PUBLIC` is a country-level index measuring the strictness of public control of self-dealing (e.g., fines, prison terms).\n\n**Table 1: Regression Results for Public Control**\n\n| Variable | Coefficient (Model 3) | t-statistic |\n| :------- | :-------------------- | :---------- |\n| `PUBLIC` | -0.42                 | -3.60       |\n\n*Source: Table 6, Model 3 of the source paper.*\n\n---\n\nGiven the significant negative coefficient on `PUBLIC`, which of the following are plausible economic rationales for why investors might perceive stricter government-enforced anti-self-dealing regulations as value-destroying? Select all that apply.", "Options": {"A": "Stricter public regulations empower minority shareholders to file lawsuits, increasing litigation risk and associated legal costs for the firm.", "B": "Empowering government agencies with the ability to levy fines or initiate proceedings creates opportunities for corruption or political extraction by officials.", "C": "Government agencies may lack industry-specific expertise, leading to inefficient, overly broad rules that stifle legitimate, value-creating transactions.", "D": "Public control regulations are a strong complement to an already efficient judicial system, creating redundant layers of costly oversight."}, "Answer": ["B", "C"], "pi_justification": "This item uses a Reverse-Reasoning strategy. Given the empirical result (a negative effect of public control), the candidate must identify plausible underlying causes. Options B and C represent two distinct, valid economic arguments for this negative effect: bureaucratic inefficiency/compliance costs and the risk of political extraction. Option A is a distractor (Almost Right) that confuses public control (government enforcement) with private control (shareholder lawsuits). Option D is a Conceptual Opposite; in a country with an efficient judiciary, public control is more likely a costly *substitute*, not a complement, making its net effect negative.", "qid": "352", "question": "### Background\nA study investigates the counterintuitive hypothesis that stricter public (i.e., government-enforced) regulations against self-dealing may be associated with lower corporate valuations. The analysis uses a sample of 4,634 firms from 22 countries.\n\n### Data / Model Specification\nThe hypothesis is tested using the following OLS regression model:\n\n  \n\\text{Tobin's Q} = \\alpha + \\dots + \\beta_{2}\\mathrm{PUBLIC} + \\dots + \\text{Controls} + \\varepsilon \\quad \\text{(Eq. 1)}\n \n\nWhere `Tobin's Q` is a proxy for corporate valuation and `PUBLIC` is a country-level index measuring the strictness of public control of self-dealing (e.g., fines, prison terms).\n\n**Table 1: Regression Results for Public Control**\n\n| Variable | Coefficient (Model 3) | t-statistic |\n| :------- | :-------------------- | :---------- |\n| `PUBLIC` | -0.42                 | -3.60       |\n\n*Source: Table 6, Model 3 of the source paper.*\n\n---\n\nGiven the significant negative coefficient on `PUBLIC`, which of the following are plausible economic rationales for why investors might perceive stricter government-enforced anti-self-dealing regulations as value-destroying? Select all that apply.", "model_evaluation_result": {"model_raw_response": "To answer this question, I need to identify plausible economic rationales for why stricter government-enforced anti-self-dealing regulations might be perceived as value-destroying (given the negative coefficient on PUBLIC).\n\nLet me analyze each option:\n\nA) Stricter public regulations empower minority shareholders to file lawsuits, increasing litigation risk and associated legal costs for the firm.\n- This is plausible. Increased litigation risk and legal costs would reduce firm value from investors' perspective.\n\nB) Empowering government agencies with the ability to levy fines or initiate proceedings creates opportunities for corruption or political extraction by officials.\n- This is plausible. If government officials can use enforcement powers for rent-seeking or political purposes, this creates uncertainty and potential costs that would reduce firm value.\n\nC) Government agencies may lack industry-specific expertise, leading to inefficient, overly broad rules that stifle legitimate, value-creating transactions.\n- This is plausible. Overly broad or poorly designed regulations due to lack of expertise could prevent firms from engaging in beneficial transactions, reducing value.\n\nD) Public control regulations are a strong complement to an already efficient judicial system, creating redundant layers of costly oversight.\n- This is NOT plausible as a rationale for value destruction. If regulations complement an efficient judicial system, they would likely enhance rather than destroy value. The statement itself suggests redundancy is costly, but complementarity typically implies synergy, not redundancy.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 424, "Question": "### Background\n\n**Research Question.** This case examines the paper's core empirical framework for establishing an intertemporal stock-to-bond volatility (ISBV) relation. It tests whether lagged equity volatility has incremental predictive power for future bond futures volatility, after controlling for known predictors from within the bond market.\n\n### Data / Model Specification\n\nThe primary predictive regression model is:\n\n  \n\\sigma_{t,t+21}^{TmSt} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TmSt} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\nwhere `\\sigma^{TmSt}` is the log realized volatility of a Treasury futures contract, `\\sigma^{ST}` is the lagged log realized stock volatility, and `\\sigma_{t-1,t-22}^{TmSt}` and `PrComp_{j,t-1}` are control variables.\n\n**Table 1. Predictive Regression Results for 30-yr T-bond Futures Volatility (Full Period 1997:10-2013:06)**\n\n| Panel | Stock Volatility Measure | `\\hat{\\gamma}_{2}` (t-stat) | `R^2` |\n| :--- | :--- | :--- | :--- |\n| A | Daily S&P 500 Returns | 0.189 (5.92) | 58.8% |\n| B | 5-minute SPY Returns | 0.230 (7.01) | 60.3% |\n\n### Question\n\nBased on the regression specification and the results in Table 1, which of the following statements are valid econometric interpretations? Select all that apply.", "Options": {"A": "The larger coefficient `\\hat{\\gamma}_{2}` and higher `R^2` in Panel B versus Panel A is consistent with the idea that using a higher-frequency volatility measure reduces attenuation bias from measurement error.", "B": "The high `R^2` value of 58.8% in Panel A indicates that multicollinearity between the regressors is not a concern for the validity of the model.", "C": "The use of a higher-frequency (5-minute) stock volatility measure in Panel B introduces additional noise, which biases the `\\hat{\\gamma}_{2}` coefficient downward compared to Panel A.", "D": "The significance of `\\hat{\\gamma}_{2}` in the full model (Eq. 1) supports the claim that stock volatility offers 'incremental' information beyond that contained in the bond market's own past volatility and the state of the yield curve."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the core regression design and interpretation. It uses an Atomic Decomposition strategy, testing the concept of 'incremental information' (from Q1 of the source) and 'measurement error' (from Q2 of the source). Correct option A defines the key takeaway from the full specification. Correct option B correctly applies the concept of attenuation bias. Distractor C makes a false claim about the relationship between R-squared and multicollinearity. Distractor D presents the conceptual opposite of the paper's argument about measurement error.", "qid": "424", "question": "### Background\n\n**Research Question.** This case examines the paper's core empirical framework for establishing an intertemporal stock-to-bond volatility (ISBV) relation. It tests whether lagged equity volatility has incremental predictive power for future bond futures volatility, after controlling for known predictors from within the bond market.\n\n### Data / Model Specification\n\nThe primary predictive regression model is:\n\n  \n\\sigma_{t,t+21}^{TmSt} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TmSt} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\nwhere `\\sigma^{TmSt}` is the log realized volatility of a Treasury futures contract, `\\sigma^{ST}` is the lagged log realized stock volatility, and `\\sigma_{t-1,t-22}^{TmSt}` and `PrComp_{j,t-1}` are control variables.\n\n**Table 1. Predictive Regression Results for 30-yr T-bond Futures Volatility (Full Period 1997:10-2013:06)**\n\n| Panel | Stock Volatility Measure | `\\hat{\\gamma}_{2}` (t-stat) | `R^2` |\n| :--- | :--- | :--- | :--- |\n| A | Daily S&P 500 Returns | 0.189 (5.92) | 58.8% |\n| B | 5-minute SPY Returns | 0.230 (7.01) | 60.3% |\n\n### Question\n\nBased on the regression specification and the results in Table 1, which of the following statements are valid econometric interpretations? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this econometric question, I need to evaluate each statement based on the regression specification and results.\n\n**Statement A**: The larger coefficient and higher R² in Panel B (using 5-minute SPY returns) versus Panel A (using daily S&P 500 returns) is consistent with measurement error theory. Higher-frequency data typically provides more accurate volatility estimates, reducing measurement error. When the independent variable has less measurement error, this reduces attenuation bias (bias toward zero), leading to larger coefficient estimates. This is a valid interpretation.\n\n**Statement B**: This is incorrect. A high R² indicates the model explains a large proportion of variance in the dependent variable, but it does not indicate whether multicollinearity is present or absent. Multicollinearity concerns the correlation among independent variables, not the overall explanatory power of the model.\n\n**Statement C**: This contradicts standard measurement theory. Higher-frequency data generally provides more precise volatility estimates, not noisier ones. The empirical evidence shows the opposite - Panel B has a larger coefficient, not a downward-biased one. This statement is invalid.\n\n**Statement D**: The regression includes controls for lagged bond volatility (σ^TmSt_{t-1,t-22}) and yield curve factors (PrComp_{j,t-1}). The significant coefficient on stock volatility (γ₂) in this full model specification means stock volatility adds predictive power beyond what these bond market variables already provide. This supports the \"incremental information\" claim and is a valid interpretation.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 378, "Question": "### Background\n\n**Research Question.** What are the welfare and financial stability trade-offs of implementing an optimal macroprudential LTV policy, particularly regarding its impact on quantity volatility versus price volatility?\n\n**Setting.** The analysis compares three regimes in a DSGE model: no macroprudential policy, a non-reciprocal policy, and the optimal reciprocal policy. A key finding is that in the presence of financial frictions, the relevant terms for welfare are not the levels and volatilities of total consumption or housing but the consumption and housing gaps between constrained borrowers and unconstrained savers. Policies that manage to effectively close these gaps (or their variance) will be welfare enhancing.\n\n**Variables & Parameters.**\n- `φ_H`, `φ_F`: Policy response coefficients for domestic and foreign LTV rules.\n- `σ_bT`: Standard deviation of total borrowing (quantity stability).\n- `σ_q`: Standard deviation of house prices (price stability).\n- `σ_c,gap`, `σ_h,gap`: Standard deviation of the consumption and housing gaps between borrowers and savers (welfare indicators).\n- **Welfare Gain**: The consumption equivalent (CE) percentage gain from a policy relative to the no-policy baseline.\n\n---\n\n### Data / Model Specification\n\nThe paper models macroprudential policy as a set of counter-cyclical rules where loan-to-value (LTV) ratios are tightened in response to rising domestic house prices:\n  \nm_{H t}=m_{H}(q_{t})^{-\\phi_{H}}, \\quad m_{F t}=m_{F}(q_{t})^{-\\phi_{F}}\n \nReciprocity implies that `φ_F > 0`. The social planner optimizes the choice of `φ_H` and `φ_F` to maximize social welfare. The results of this optimization and the resulting model moments are presented below.\n\n**Table 1: Optimal Macroprudential Policy**\n\n| φ_H (Optimal Domestic) | φ_F (Optimal Foreign) | Welfare Gain (CE) |\n| :--- | :--- | :--- |\n| 8.5 | 0.2 | 1.32% |\n\n**Table 2: Financial Stability and Welfare Moments**\n\n| Policy Regime | σ_bT (Total Borrowing) | σ_c,gap (Consumption Gap) | σ_h,gap (Housing Gap) | σ_q (House Price) | Welfare Gain (CE) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| No Macropru | 5.160 | 2.315 | 7.961 | 1.753 | — |\n| Macropru - No Reciprocity | 3.818 | 0.352 | 1.890 | 1.775 | 0.97% |\n| Optimal Macropru (Reciprocal) | 0.104 | 0.337 | 0.815 | 2.235 | 1.32% |\n\n---\n\n### Question\n\nBased on the provided context and data in Table 1 and Table 2, which of the following statements accurately describe the effects and trade-offs of the \"Optimal Macropru\" policy compared to the \"No Macropru\" baseline?", "Options": {"A": "The optimal policy enhances financial stability by reducing the volatility of both credit quantities (`σ_bT`) and house prices (`σ_q`).", "B": "The policy successfully stabilizes the quantity of total borrowing (`σ_bT`) but does so at the cost of significantly increasing house price volatility (`σ_q`).", "C": "The primary channel for the welfare gain is the reduction in house price volatility, which makes housing more accessible to constrained borrowers.", "D": "The policy achieves its welfare gain primarily by dramatically reducing the volatility of the housing gap (`σ_h,gap`) between borrowers and savers."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize quantitative results from a table with the paper's theoretical framework. It uses an 'atomic decomposition' strategy, breaking down the complex policy evaluation into distinct, verifiable claims about the welfare mechanism and a key policy trade-off. Distractors are designed as 'Conceptual Opposites' that target common misinterpretations of the results, such as assuming stability must apply to both prices and quantities, or misidentifying the source of welfare gains.", "qid": "378", "question": "### Background\n\n**Research Question.** What are the welfare and financial stability trade-offs of implementing an optimal macroprudential LTV policy, particularly regarding its impact on quantity volatility versus price volatility?\n\n**Setting.** The analysis compares three regimes in a DSGE model: no macroprudential policy, a non-reciprocal policy, and the optimal reciprocal policy. A key finding is that in the presence of financial frictions, the relevant terms for welfare are not the levels and volatilities of total consumption or housing but the consumption and housing gaps between constrained borrowers and unconstrained savers. Policies that manage to effectively close these gaps (or their variance) will be welfare enhancing.\n\n**Variables & Parameters.**\n- `φ_H`, `φ_F`: Policy response coefficients for domestic and foreign LTV rules.\n- `σ_bT`: Standard deviation of total borrowing (quantity stability).\n- `σ_q`: Standard deviation of house prices (price stability).\n- `σ_c,gap`, `σ_h,gap`: Standard deviation of the consumption and housing gaps between borrowers and savers (welfare indicators).\n- **Welfare Gain**: The consumption equivalent (CE) percentage gain from a policy relative to the no-policy baseline.\n\n---\n\n### Data / Model Specification\n\nThe paper models macroprudential policy as a set of counter-cyclical rules where loan-to-value (LTV) ratios are tightened in response to rising domestic house prices:\n  \nm_{H t}=m_{H}(q_{t})^{-\\phi_{H}}, \\quad m_{F t}=m_{F}(q_{t})^{-\\phi_{F}}\n \nReciprocity implies that `φ_F > 0`. The social planner optimizes the choice of `φ_H` and `φ_F` to maximize social welfare. The results of this optimization and the resulting model moments are presented below.\n\n**Table 1: Optimal Macroprudential Policy**\n\n| φ_H (Optimal Domestic) | φ_F (Optimal Foreign) | Welfare Gain (CE) |\n| :--- | :--- | :--- |\n| 8.5 | 0.2 | 1.32% |\n\n**Table 2: Financial Stability and Welfare Moments**\n\n| Policy Regime | σ_bT (Total Borrowing) | σ_c,gap (Consumption Gap) | σ_h,gap (Housing Gap) | σ_q (House Price) | Welfare Gain (CE) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| No Macropru | 5.160 | 2.315 | 7.961 | 1.753 | — |\n| Macropru - No Reciprocity | 3.818 | 0.352 | 1.890 | 1.775 | 0.97% |\n| Optimal Macropru (Reciprocal) | 0.104 | 0.337 | 0.815 | 2.235 | 1.32% |\n\n---\n\n### Question\n\nBased on the provided context and data in Table 1 and Table 2, which of the following statements accurately describe the effects and trade-offs of the \"Optimal Macropru\" policy compared to the \"No Macropru\" baseline?", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 2, I need to compare the \"Optimal Macropru (Reciprocal)\" policy with the \"No Macropru\" baseline:\n\n**No Macropru baseline:**\n- σ_bT = 5.160\n- σ_c,gap = 2.315\n- σ_h,gap = 7.961\n- σ_q = 1.753\n\n**Optimal Macropru:**\n- σ_bT = 0.104 (massive decrease from 5.160)\n- σ_c,gap = 0.337 (large decrease from 2.315)\n- σ_h,gap = 0.815 (large decrease from 7.961)\n- σ_q = 2.235 (increase from 1.753)\n\nEvaluating each option:\n\nA) False - While credit quantity volatility (σ_bT) decreases dramatically, house price volatility (σ_q) actually increases, not decreases.\n\nB) True - The policy reduces σ_bT from 5.160 to 0.104 (98% reduction) while increasing σ_q from 1.753 to 2.235 (27% increase). This shows a clear trade-off.\n\nC) False - House price volatility increases under the optimal policy, so this cannot be the primary channel for welfare gains.\n\nD) True - The housing gap volatility (σ_h,gap) drops dramatically from 7.961 to 0.815 (90% reduction). The background explicitly states that \"Policies that manage to effectively close these gaps (or their variance) will be welfare enhancing.\"\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 257, "Question": "### Background\n\n**Research Question.** How can one construct a standardized measure of net institutional trading that isolates active buying or selling decisions from confounding price effects and aggregate fund flows?\n\n### Data / Model Specification\n\nThe unadjusted Trade Ratio is defined as:\n  \nTR(i,t) = \\frac{P(i,t) - S(i,t)}{P(i,t) + S(i,t)} \\quad \\text{(Eq. 1)}\n \nwhere `P(i,t)` and `S(i,t)` are non-negative share counts of purchases and sales, respectively.\n\nTo control for the influence of aggregate fund flows, an adjusted measure is constructed:\n  \n\\text{Adjusted } TR(i,t) = TR(i,t) - \\overline{TR(t)} \\quad \\text{(Eq. 2)}\n \nwhere `\\overline{TR(t)}` is the cross-sectional average of `TR(i,t)` across all stocks in quarter `t`.\n\n---\n\nWhich of the following statements about the Trade Ratio (`TR`) and its adjustment are mathematically or conceptually correct?\n", "Options": {"A": "The `TR(i,t)` for a stock will be exactly zero only if institutions did not trade the stock at all during the quarter.", "B": "If institutions experience a large net fund inflow in quarter `t`, `\\overline{TR(t)}` will likely be positive, and a stock with an `Adjusted TR(i,t)` near zero was likely bought at a rate similar to the market average.", "C": "The unadjusted `TR(i,t)` is mathematically bounded between -1 and +1, inclusive.", "D": "The adjustment in Eq. (2) is designed to isolate active stock selection decisions from the mechanical trading effects of market-wide net fund inflows or outflows to institutions."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the user's understanding of the construction and interpretation of the paper's core metric, the Adjusted Trade Ratio. It combines mathematical properties with conceptual purpose.\nDepth Strategy: Computational Judgment. The user must evaluate the mathematical properties and conceptual underpinnings of the provided equations.\nDistractor Logic:\n- A (Correct): This is a provable mathematical property of the ratio, as `P-S` cannot be larger in magnitude than `P+S`.\n- B (Correct): This is the stated conceptual purpose of the adjustment, as explained in the paper.\n- C (Correct): This correctly interprets the meaning of the adjustment term `\\overline{TR(t)}` and the resulting `Adjusted TR` in a specific scenario (net fund inflow).\n- D (Incorrect): This is a Step-Omission Error. `TR(i,t)` is zero if the numerator is zero, which occurs when `P(i,t) = S(i,t)`. This means net trading is zero, but total trading (`P+S`) could be very large. The ratio is undefined, not zero, if no trading occurs (`P=S=0`).", "qid": "257", "question": "### Background\n\n**Research Question.** How can one construct a standardized measure of net institutional trading that isolates active buying or selling decisions from confounding price effects and aggregate fund flows?\n\n### Data / Model Specification\n\nThe unadjusted Trade Ratio is defined as:\n  \nTR(i,t) = \\frac{P(i,t) - S(i,t)}{P(i,t) + S(i,t)} \\quad \\text{(Eq. 1)}\n \nwhere `P(i,t)` and `S(i,t)` are non-negative share counts of purchases and sales, respectively.\n\nTo control for the influence of aggregate fund flows, an adjusted measure is constructed:\n  \n\\text{Adjusted } TR(i,t) = TR(i,t) - \\overline{TR(t)} \\quad \\text{(Eq. 2)}\n \nwhere `\\overline{TR(t)}` is the cross-sectional average of `TR(i,t)` across all stocks in quarter `t`.\n\n---\n\nWhich of the following statements about the Trade Ratio (`TR`) and its adjustment are mathematically or conceptually correct?\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The TR(i,t) for a stock will be exactly zero only if institutions did not trade the stock at all during the quarter.\n- TR(i,t) = (P(i,t) - S(i,t))/(P(i,t) + S(i,t))\n- TR(i,t) = 0 when P(i,t) = S(i,t) (equal purchases and sales)\n- If institutions did not trade at all, both P(i,t) = 0 and S(i,t) = 0, making TR(i,t) = 0/0 which is undefined, not zero\n- Therefore, this statement is FALSE\n\nB) If institutions experience a large net fund inflow in quarter t, TR̄(t) will likely be positive, and a stock with an Adjusted TR(i,t) near zero was likely bought at a rate similar to the market average.\n- Net fund inflows would lead to more purchases than sales across stocks, making TR̄(t) > 0\n- If Adjusted TR(i,t) = TR(i,t) - TR̄(t) ≈ 0, then TR(i,t) ≈ TR̄(t)\n- This means the stock's trading pattern matches the market average\n- This statement is TRUE\n\nC) The unadjusted TR(i,t) is mathematically bounded between -1 and +1, inclusive.\n- When P(i,t) = 0: TR(i,t) = -S(i,t)/S(i,t) = -1\n- When S(i,t) = 0: TR(i,t) = P(i,t)/P(i,t) = +1\n- Since P(i,t) and S(i,t) are non-negative, the ratio (P-S)/(P+S) must be between -1 and +1\n- This statement is TRUE\n\nD) The adjustment in Eq. (2) is designed to isolate active stock selection decisions from the mechanical trading effects of market-wide net fund inflows or outflows to institutions.\n- By subtracting the cross-sectional average", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 27, "Question": "### Background\n\n**Research Question.** Can a trading rule based on identifying properties with large negative residuals from a hedonic pricing model generate superior risk-adjusted returns, and what does this imply for real estate market efficiency?\n\n**Setting.** An empirical test using portfolios of apartment buildings sorted by their hedonic pricing error (`e`). Performance is evaluated by comparing raw returns and calculating Sharpe ratios against the total sample of properties.\n\n**Variables and Parameters.**\n- `e`: Estimation error from the hedonic model, used for sorting properties into portfolios.\n- `Mean Excess Return`: Average annualized return from appreciation minus a maturity-matched risk-free rate.\n- `Std. Dev.`: Standard deviation of excess returns.\n- `Sharpe Ratio`: A measure of risk-adjusted return, calculated as `Mean Excess Return / Std. Dev.`.\n\n---\n\n### Data / Model Specification\n\nProperties purchased between 1971 and 1980 were sorted into portfolios based on the estimation error `e` at the time of purchase. The two key portfolios for comparison are the \"undervalued\" portfolio, identified by the trading rule (`e ≤ -0.10`), and the portfolio of all properties in the sample.\n\n**Table 1.** Return and Risk Statistics for Portfolios (Purchases 1971-1980).\n\n| Portfolio | Mean Excess Return (%) | Std. Dev. (%) |\n| :--- | :--- | :--- |\n| Total sample | 12.0 | 13.6 |\n| `e ≤ -0.10` | 24.6 | 33.0 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following statements are correct?", "Options": {"A": "The higher raw return of the 'undervalued' portfolio (`e ≤ -0.10`) was accompanied by disproportionately higher risk (standard deviation), leading to its lower risk-adjusted performance.", "B": "The total sample portfolio has a Sharpe ratio of approximately 0.88.", "C": "The 'undervalued' portfolio (`e ≤ -0.10`) delivered a superior risk-adjusted return, as measured by the Sharpe ratio, compared to the total sample.", "D": "The 'undervalued' portfolio (`e ≤ -0.10`) generated a higher raw mean excess return than the total sample."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses the ability to distinguish between raw and risk-adjusted performance, a central theme of the paper. It uses a **Computational Judgment** strategy, requiring the user to calculate Sharpe ratios from provided data to evaluate the options. Option A is a direct data interpretation. Options C and D require calculation and synthesis. The key distractor (B) represents the naive conclusion an investor might draw by ignoring risk, a classic conceptual error.", "qid": "27", "question": "### Background\n\n**Research Question.** Can a trading rule based on identifying properties with large negative residuals from a hedonic pricing model generate superior risk-adjusted returns, and what does this imply for real estate market efficiency?\n\n**Setting.** An empirical test using portfolios of apartment buildings sorted by their hedonic pricing error (`e`). Performance is evaluated by comparing raw returns and calculating Sharpe ratios against the total sample of properties.\n\n**Variables and Parameters.**\n- `e`: Estimation error from the hedonic model, used for sorting properties into portfolios.\n- `Mean Excess Return`: Average annualized return from appreciation minus a maturity-matched risk-free rate.\n- `Std. Dev.`: Standard deviation of excess returns.\n- `Sharpe Ratio`: A measure of risk-adjusted return, calculated as `Mean Excess Return / Std. Dev.`.\n\n---\n\n### Data / Model Specification\n\nProperties purchased between 1971 and 1980 were sorted into portfolios based on the estimation error `e` at the time of purchase. The two key portfolios for comparison are the \"undervalued\" portfolio, identified by the trading rule (`e ≤ -0.10`), and the portfolio of all properties in the sample.\n\n**Table 1.** Return and Risk Statistics for Portfolios (Purchases 1971-1980).\n\n| Portfolio | Mean Excess Return (%) | Std. Dev. (%) |\n| :--- | :--- | :--- |\n| Total sample | 12.0 | 13.6 |\n| `e ≤ -0.10` | 24.6 | 33.0 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "I need to analyze the data in Table 1 and evaluate each statement.\n\nGiven data:\n- Total sample: Mean Excess Return = 12.0%, Std. Dev. = 13.6%\n- e ≤ -0.10 portfolio: Mean Excess Return = 24.6%, Std. Dev. = 33.0%\n\nLet me calculate the Sharpe ratios:\n- Total sample Sharpe ratio = 12.0 / 13.6 = 0.882\n- e ≤ -0.10 Sharpe ratio = 24.6 / 33.0 = 0.745\n\nNow evaluating each option:\n\nA) The statement says the undervalued portfolio had lower risk-adjusted performance due to disproportionately higher risk. \n- The undervalued portfolio's Sharpe ratio (0.745) is indeed lower than the total sample's (0.882)\n- The risk increased by 33.0/13.6 = 2.43x while return only increased by 24.6/12.0 = 2.05x\n- This is correct.\n\nB) The total sample portfolio has a Sharpe ratio of approximately 0.88.\n- Calculation: 12.0 / 13.6 = 0.882 ≈ 0.88\n- This is correct.\n\nC) The undervalued portfolio delivered superior risk-adjusted return compared to total sample.\n- Sharpe ratio of undervalued (0.745) < Sharpe ratio of total sample (0.882)\n- This is incorrect.\n\nD) The undervalued portfolio generated higher raw mean excess return than total sample.\n- Undervalued: 24.6% > Total sample: 12.0%\n- This is correct.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 366, "Question": "### Background\n\n**Research Question:** The paper's central theory predicts that in a competitive innovation race, firms with a higher relative innovation output (i.e., 'leaders') will have lower systematic risk (beta) than their less successful rivals ('laggards'). This question assesses the empirical evidence for this prediction using both portfolio-level and firm-level tests.\n\n**Empirical Strategy:** The authors use patent data to construct a measure of a firm's relative innovation output—its share of patents within a specific technology field. They first test the theory using simple portfolio sorts and then move to more formal firm-level panel regressions, controlling for a range of known determinants of beta.\n\n### Data / Model Specification\n\nThe primary firm-level regression specification is:\n\n  \n\\beta_{i t}=\\lambda_{0}+\\lambda_{1}\\bar{y}_{i t}+\\lambda_{2}X_{i t}+FE+\\epsilon_{i t}\n\\quad \\text{(Eq. (1))}\n \n\nwhere `β_it` is the market beta of firm `i` at time `t`, `y_it` is the firm's normalized share of innovation output (z-score), `X_it` is a vector of firm-level controls (e.g., size, leverage, book-to-market), and `FE` represents fixed effects.\n\n**Table 1** below presents a summary of key empirical results from the paper. Panel A shows results from sorting innovating firms into portfolios based on their patent share. Panel B shows results from the firm-level regression in Eq. (1).\n\n**Table 1: Empirical Results on Innovation Output and Systematic Risk**\n\n| Panel / Specification | Dependent Variable | Key Metric | Leaders | Middle | M - L Diff. |\n| :--- | :--- | :--- | :---: | :---: | :---: |\n| **A: Portfolio Sorts** | Portfolio Returns | Market Beta (`β_MKT`) | 0.937*** | 1.196*** | 0.259*** |\n| | | Std. Error | (0.0126) | (0.0373) | (0.0366) |\n| | | | | | |\n| **B: Firm-Level Regressions** | Firm Beta (`β_it`) | Coefficient on `y_it` (`λ_1`) | | | |\n| Baseline (Patent Counts) | `β_it` | `λ_1` | -0.0713*** | | |\n| | | Std. Error | (0.0110) | | |\n| Robustness (Citation-Weighted) | `β_it` | `λ_1` | -0.0469*** | | |\n| | | Std. Error | (0.00749) | | |\n\n*Notes: Results are based on Tables 2 and 3 in the original paper. *** denotes significance at the 1% level. The regression results in Panel B control for firm characteristics and industry × time fixed effects.*\n\n### Question\n\nBased on the provided data and the paper's analysis, which of the following statements are supported by the evidence?", "Options": {"A": "Using a citation-weighted measure of innovation output reveals a much larger economic effect on beta compared to using simple patent counts, indicating that patent quality is the primary driver of the risk reduction.", "B": "The difference in market beta between the 'Middle' and 'Leader' innovation portfolios is approximately 0.26, and this difference is statistically significant at the 1% level.", "C": "The portfolio of 'Leader' firms exhibits a higher cost of capital than the 'Middle' portfolio, reflecting the higher risk associated with maintaining a dominant innovation position.", "D": "The firm-level regression results suggest that a one-standard-deviation increase in a firm's normalized share of patents is associated with a decrease in its market beta of approximately 0.07."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item replaces a Table QA problem, converting its core quantitative interpretations into a multiple-choice format. It uses an **atomic decomposition** strategy, breaking down the original multi-part question into distinct, verifiable statements. The assessment target is the user's ability to accurately interpret both portfolio-level and regression-level evidence from the provided table (**Computational Judgment**). Distractor C is a **Conceptual Opposite**, directly contradicting the paper's main finding about the relationship between leadership and cost of capital. Distractor D is a **Numerical Misinterpretation**, forcing a careful comparison of the coefficients in Panel B and correctly identifying that the citation-weighted effect is smaller in magnitude (-0.0469) not larger.", "qid": "366", "question": "### Background\n\n**Research Question:** The paper's central theory predicts that in a competitive innovation race, firms with a higher relative innovation output (i.e., 'leaders') will have lower systematic risk (beta) than their less successful rivals ('laggards'). This question assesses the empirical evidence for this prediction using both portfolio-level and firm-level tests.\n\n**Empirical Strategy:** The authors use patent data to construct a measure of a firm's relative innovation output—its share of patents within a specific technology field. They first test the theory using simple portfolio sorts and then move to more formal firm-level panel regressions, controlling for a range of known determinants of beta.\n\n### Data / Model Specification\n\nThe primary firm-level regression specification is:\n\n  \n\\beta_{i t}=\\lambda_{0}+\\lambda_{1}\\bar{y}_{i t}+\\lambda_{2}X_{i t}+FE+\\epsilon_{i t}\n\\quad \\text{(Eq. (1))}\n \n\nwhere `β_it` is the market beta of firm `i` at time `t`, `y_it` is the firm's normalized share of innovation output (z-score), `X_it` is a vector of firm-level controls (e.g., size, leverage, book-to-market), and `FE` represents fixed effects.\n\n**Table 1** below presents a summary of key empirical results from the paper. Panel A shows results from sorting innovating firms into portfolios based on their patent share. Panel B shows results from the firm-level regression in Eq. (1).\n\n**Table 1: Empirical Results on Innovation Output and Systematic Risk**\n\n| Panel / Specification | Dependent Variable | Key Metric | Leaders | Middle | M - L Diff. |\n| :--- | :--- | :--- | :---: | :---: | :---: |\n| **A: Portfolio Sorts** | Portfolio Returns | Market Beta (`β_MKT`) | 0.937*** | 1.196*** | 0.259*** |\n| | | Std. Error | (0.0126) | (0.0373) | (0.0366) |\n| | | | | | |\n| **B: Firm-Level Regressions** | Firm Beta (`β_it`) | Coefficient on `y_it` (`λ_1`) | | | |\n| Baseline (Patent Counts) | `β_it` | `λ_1` | -0.0713*** | | |\n| | | Std. Error | (0.0110) | | |\n| Robustness (Citation-Weighted) | `β_it` | `λ_1` | -0.0469*** | | |\n| | | Std. Error | (0.00749) | | |\n\n*Notes: Results are based on Tables 2 and 3 in the original paper. *** denotes significance at the 1% level. The regression results in Panel B control for firm characteristics and industry × time fixed effects.*\n\n### Question\n\nBased on the provided data and the paper's analysis, which of the following statements are supported by the evidence?", "model_evaluation_result": {"model_raw_response": "Looking at the empirical results in Table 1:\n\n**Option A**: The citation-weighted coefficient is -0.0469 compared to -0.0713 for patent counts. The citation-weighted measure shows a SMALLER effect, not larger. This statement is incorrect.\n\n**Option B**: From Panel A, the M - L Diff. is 0.259*** (significant at 1% level). This matches the statement about the difference being approximately 0.26 and statistically significant. This statement is correct.\n\n**Option C**: Leaders have beta = 0.937 while Middle has beta = 1.196. Lower beta means lower systematic risk and therefore lower cost of capital (according to CAPM). Leaders have LOWER cost of capital, not higher. This statement is incorrect.\n\n**Option D**: The baseline coefficient λ₁ = -0.0713*** means a one-standard-deviation increase in normalized patent share is associated with a 0.0713 decrease in beta, which is approximately 0.07. This statement is correct.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 447, "Question": "### Background\n\nA central theme in household finance is understanding how consumers choose between fixed-rate mortgages (FRMs) and adjustable-rate mortgages (ARMs). A baseline model proposed by Koijen, Van Hemert, and Van Nieuwerburgh (KHN) posits a simple linear relationship between the aggregate ARM share and the \"Household Decision Rule\" (HDR), a proxy for the long-term bond risk premium. This paper extends that model by hypothesizing that borrowers' sensitivity to the HDR is not constant, but varies over time with their perceived risk. This perceived risk is driven by two main factors: the current level of short-term interest rates and the state of the housing market.\n\n### Data / Model Specification\n\nTo capture this time-varying sensitivity, the coefficients of the baseline model are made conditional on the one-year treasury yield (`short_t`) and a dummy variable for a declining housing market (`down_t`, which equals 1 if the Case-Shiller home price index fell in the previous month, and 0 otherwise). This results in the following full interaction model:\n\n  \nARM_{t}=\\alpha_{0}+\\alpha_{1}short_{t}+\\alpha_{2}down_{t}+\\beta_{0}HDR_{t}+\\beta_{1}(HDR_{t} \\times short_{t}) +\\beta_{2}(HDR_{t} \\times down_{t})+\\varepsilon_{t}\n \nEq. (1)\n\nwhere `ARM_t` is the percentage share of new mortgages that are ARMs. The model is estimated using monthly data from the Federal Housing Finance Agency's Monthly Interest Rate Survey (MIRS) from February 1987 to September 2008. Summary statistics and regression results for this sample are provided in Table 1 and Table 2 below.\n\n**Table 1: Summary Statistics & Correlations (MIRS Sample, 1987-2008)**\n\n| Variable | Mean | Std. Dev. | Min | Max |\n| :--- | :--- | :--- | :--- | :--- |\n| ARM (%) | 25.75 | 13.76 | 4.00 | 69.00 |\n| HDR (%) | 1.11 | 0.98 | -0.86 | 3.73 |\n| short_t (%) | 4.90 | 2.02 | 1.00 | 9.56 |\n| down_t | 0.25 | 0.43 | 0 | 1 |\n\n**Table 2: ARM Choice Model Regression Results (MIRS Sample)**\n\n| Variable | (1) Simple Model | (2) Full Model |\n| :--- | :--- | :--- |\n| Intercept | 15.28*** | 5.13* |\n| | (1.74) | (2.68) |\n| HDR_t | 9.41*** | 5.09** |\n| | (1.54) | (2.55) |\n| short_t | | 2.16** |\n| | | (0.59) |\n| down_t | | -0.30 |\n| | | (2.25) |\n| HDR_t x short_t | | 1.01* |\n| | | (0.53) |\n| HDR_t x down_t | | -5.06** |\n| | | (2.06) |\n| | | |\n| N | 260 | 260 |\n| Adj. R² | 0.448 | 0.736 |\n\n*Notes: Newey-West standard errors (12 lags) in parentheses. Significance codes: *p<0.1; **p<0.05; ***p<0.01.*\n\n### Question\n\nUsing the results from the full model (Table 2) and the mean `short_t` from Table 1, which of the following conclusions about the marginal effect of `HDR_t` on the ARM share are correct? Select all that apply.", "Options": {"A": "In a normal housing market (`down_t` = 0), with the short-term rate at its sample mean, a one-percentage-point increase in `HDR_t` is associated with an approximate 10.04 percentage point increase in the ARM share.", "B": "The analysis shows that the sensitivity of ARM choice to the HDR is more than halved during a declining housing market compared to a normal one (at the mean short-term rate).", "C": "The marginal effect of `HDR_t` on the ARM share is constant at its direct coefficient of 5.09, as this is the primary effect listed in the regression table.", "D": "During a declining housing market (`down_t` = 1), the marginal effect of `HDR_t` on the ARM share increases to approximately 15.10 at the mean short-term rate."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to calculate and interpret a conditional marginal effect from an interaction model, a key quantitative skill. The strategy is Computational Judgment. Correct options (A, B) require performing the calculation correctly for one scenario (A: 5.09 + 1.01*4.90 ≈ 10.04) and then comparing the results across two scenarios (B: 10.04 vs. 10.04 - 5.06 = 4.98). Distractor (C) is a Step-Omission Error, where the student fails to include the interaction terms in the marginal effect calculation. Distractor (D) is a Sign Error, where the student incorrectly adds instead of subtracts the effect of the `down_t` interaction term.", "qid": "447", "question": "### Background\n\nA central theme in household finance is understanding how consumers choose between fixed-rate mortgages (FRMs) and adjustable-rate mortgages (ARMs). A baseline model proposed by Koijen, Van Hemert, and Van Nieuwerburgh (KHN) posits a simple linear relationship between the aggregate ARM share and the \"Household Decision Rule\" (HDR), a proxy for the long-term bond risk premium. This paper extends that model by hypothesizing that borrowers' sensitivity to the HDR is not constant, but varies over time with their perceived risk. This perceived risk is driven by two main factors: the current level of short-term interest rates and the state of the housing market.\n\n### Data / Model Specification\n\nTo capture this time-varying sensitivity, the coefficients of the baseline model are made conditional on the one-year treasury yield (`short_t`) and a dummy variable for a declining housing market (`down_t`, which equals 1 if the Case-Shiller home price index fell in the previous month, and 0 otherwise). This results in the following full interaction model:\n\n  \nARM_{t}=\\alpha_{0}+\\alpha_{1}short_{t}+\\alpha_{2}down_{t}+\\beta_{0}HDR_{t}+\\beta_{1}(HDR_{t} \\times short_{t}) +\\beta_{2}(HDR_{t} \\times down_{t})+\\varepsilon_{t}\n \nEq. (1)\n\nwhere `ARM_t` is the percentage share of new mortgages that are ARMs. The model is estimated using monthly data from the Federal Housing Finance Agency's Monthly Interest Rate Survey (MIRS) from February 1987 to September 2008. Summary statistics and regression results for this sample are provided in Table 1 and Table 2 below.\n\n**Table 1: Summary Statistics & Correlations (MIRS Sample, 1987-2008)**\n\n| Variable | Mean | Std. Dev. | Min | Max |\n| :--- | :--- | :--- | :--- | :--- |\n| ARM (%) | 25.75 | 13.76 | 4.00 | 69.00 |\n| HDR (%) | 1.11 | 0.98 | -0.86 | 3.73 |\n| short_t (%) | 4.90 | 2.02 | 1.00 | 9.56 |\n| down_t | 0.25 | 0.43 | 0 | 1 |\n\n**Table 2: ARM Choice Model Regression Results (MIRS Sample)**\n\n| Variable | (1) Simple Model | (2) Full Model |\n| :--- | :--- | :--- |\n| Intercept | 15.28*** | 5.13* |\n| | (1.74) | (2.68) |\n| HDR_t | 9.41*** | 5.09** |\n| | (1.54) | (2.55) |\n| short_t | | 2.16** |\n| | | (0.59) |\n| down_t | | -0.30 |\n| | | (2.25) |\n| HDR_t x short_t | | 1.01* |\n| | | (0.53) |\n| HDR_t x down_t | | -5.06** |\n| | | (2.06) |\n| | | |\n| N | 260 | 260 |\n| Adj. R² | 0.448 | 0.736 |\n\n*Notes: Newey-West standard errors (12 lags) in parentheses. Significance codes: *p<0.1; **p<0.05; ***p<0.01.*\n\n### Question\n\nUsing the results from the full model (Table 2) and the mean `short_t` from Table 1, which of the following conclusions about the marginal effect of `HDR_t` on the ARM share are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "I need to calculate the marginal effect of HDR_t on the ARM share under different conditions using the full model results.\n\nFrom the full model (Table 2), the equation is:\nARM_t = 5.13 + 5.09*HDR_t + 2.16*short_t - 0.30*down_t + 1.01*(HDR_t × short_t) - 5.06*(HDR_t × down_t)\n\nThe marginal effect of HDR_t on ARM_t is:\n∂ARM_t/∂HDR_t = 5.09 + 1.01*short_t - 5.06*down_t\n\nFrom Table 1, the mean of short_t is 4.90%.\n\nLet me evaluate each option:\n\n**Option A**: When down_t = 0 and short_t = 4.90:\nMarginal effect = 5.09 + 1.01*4.90 - 5.06*0 = 5.09 + 4.95 = 10.04\nThis is correct.\n\n**Option B**: \n- Normal housing market (down_t = 0): Marginal effect = 5.09 + 1.01*4.90 = 10.04\n- Declining housing market (down_t = 1): Marginal effect = 5.09 + 1.01*4.90 - 5.06*1 = 10.04 - 5.06 = 4.98\n- The ratio is 4.98/10.04 ≈ 0.496, which means the sensitivity is approximately halved.\nThis is correct.\n\n**Option C**: The marginal effect is not constant at 5.09. It varies with short_t and down_t due to the interaction terms.\nThis is incorrect.\n\n**Option D**: When down_t = 1 and short_t = 4.90:\nMarginal effect = 5.09 + 1.01*4.90 - 5.06*1 = 10.04 - 5.06 = 4.98\nThis is not", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 435, "Question": "### Background\n\n**Research Question.** A central challenge in empirical asset pricing is accurately measuring a firm's exposure to risk factors, such as exchange rate fluctuations. Standard models, like the augmented CAPM proposed by Jorion, often find surprisingly weak evidence of such exposure. This paper argues that this weak evidence may be a statistical artifact stemming from the model's failure to account for the correlation between exchange rate movements and overall stock market returns, which are both driven by common macroeconomic news.\n\n**Setting / Data-Generating Environment.** The analysis compares two methods for estimating firm-level currency exposure for European firms from 1999-2011: the standard Jorion model and a three-step orthogonalization procedure designed to address the correlation issue. The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Model 1: The Standard Jorion Model**\nAn augmented CAPM specification regresses a firm's stock return (`r_{it}`) on the contemporaneous exchange rate return (`r_{st}`) and the market portfolio return (`r_{mt}`).\n  \nr_{\\mathrm{it}}=\\beta_{0i}+\\beta_{1i}r_{\\mathrm{st}}+\\beta_{2i}r_{\\mathrm{mt}}+\\mu_{\\mathrm{it}} \\quad \\text{(Eq. (1))}\n \n\n**Model 2: The Three-Step Orthogonalization Procedure**\nThis procedure first isolates unexpected shocks to the exchange rate and the market, and then estimates exposure using these orthogonal shocks.\n- **Step 1:** Isolate unexpected exchange rate returns (`\\hat{\\varepsilon}_{st}`) by regressing `r_{st}` on lagged macroeconomic control variables.\n- **Step 2:** Isolate unexpected market returns (`\\hat{e}_{mt}^M`) by regressing `r_{mt}` on the same controls *and* the unexpected exchange rate returns `\\hat{\\varepsilon}_{st}` from Step 1.\n- **Step 3:** Estimate firm-level exposure by regressing `r_{it}` on the orthogonal shocks from the first two steps:\n  \nr_{\\mathrm{{it}}}=\\alpha_{i,0}+\\dots+\\beta_{\\mathrm{{im}}}\\widehat{e_{\\mathrm{{mt}}}^{M}}+\\beta_{\\mathrm{{is}}}\\widehat{\\varepsilon_{\\mathrm{{st}}}}+\\gamma_{\\mathrm{{it}}} \\quad \\text{(Eq. (2))}\n \n\n**Table 1. Second-Stage Regression Results (Euro Stoxx TMI)**\nThis table shows the results from regressing the market return (`r_{mt}`) on macroeconomic controls and the unexpected exchange rate shock (`\\hat{\\varepsilon}_{st}`) from Step 1.\n\n| | **Second Stage (for Market Return `r_{mt}` )** | | | |\n|:---|:---:|:---:|:---:|:---:|\n| **Currency Shock Used** | **JPY** | **USD** | **GBP** | **CHF** |\n| `\\hat{\\varepsilon}_{st}` Coefficient | 0.43*** | 0.21 | 0.27 | 1.18*** |\n\n*Notes: Significance levels: *** 1%. Coefficients for control variables are omitted.* \n\n### Question\n\nBased on the provided models and data, select all statements that accurately describe the methodological problem with the standard Jorion model and the logic of the orthogonalization procedure.", "Options": {"A": "The Jorion model's primary flaw is its failure to include lagged macroeconomic control variables, which leads to omitted variable bias.", "B": "The orthogonalization procedure assumes that the raw market return (`r_{mt}`) is a suitable proxy for the market risk factor in the final regression, as it captures all systematic risks.", "C": "The purpose of including `\\hat{\\varepsilon}_{st}` in the second-stage regression is to ensure that the resulting market shock residual (`\\hat{e}_{mt}^M`) is, by construction, statistically orthogonal to the exchange rate shock.", "D": "Based on Table 1, the significant coefficient of 1.18 on the CHF shock (`\\hat{\\varepsilon}_{st}`) in the second-stage regression provides direct evidence that market returns and exchange rate shocks are correlated, violating a key assumption of the Jorion model."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition strategy to test understanding of the paper's core methodological critique. It assesses the ability to connect empirical evidence from Table 1 to a theoretical flaw (Option A) and understand the mechanics of the proposed solution (Option B). Distractors are designed based on a Conceptual Opposite (C) and a misinterpretation of the paper's main argument (D).", "qid": "435", "question": "### Background\n\n**Research Question.** A central challenge in empirical asset pricing is accurately measuring a firm's exposure to risk factors, such as exchange rate fluctuations. Standard models, like the augmented CAPM proposed by Jorion, often find surprisingly weak evidence of such exposure. This paper argues that this weak evidence may be a statistical artifact stemming from the model's failure to account for the correlation between exchange rate movements and overall stock market returns, which are both driven by common macroeconomic news.\n\n**Setting / Data-Generating Environment.** The analysis compares two methods for estimating firm-level currency exposure for European firms from 1999-2011: the standard Jorion model and a three-step orthogonalization procedure designed to address the correlation issue. The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Model 1: The Standard Jorion Model**\nAn augmented CAPM specification regresses a firm's stock return (`r_{it}`) on the contemporaneous exchange rate return (`r_{st}`) and the market portfolio return (`r_{mt}`).\n  \nr_{\\mathrm{it}}=\\beta_{0i}+\\beta_{1i}r_{\\mathrm{st}}+\\beta_{2i}r_{\\mathrm{mt}}+\\mu_{\\mathrm{it}} \\quad \\text{(Eq. (1))}\n \n\n**Model 2: The Three-Step Orthogonalization Procedure**\nThis procedure first isolates unexpected shocks to the exchange rate and the market, and then estimates exposure using these orthogonal shocks.\n- **Step 1:** Isolate unexpected exchange rate returns (`\\hat{\\varepsilon}_{st}`) by regressing `r_{st}` on lagged macroeconomic control variables.\n- **Step 2:** Isolate unexpected market returns (`\\hat{e}_{mt}^M`) by regressing `r_{mt}` on the same controls *and* the unexpected exchange rate returns `\\hat{\\varepsilon}_{st}` from Step 1.\n- **Step 3:** Estimate firm-level exposure by regressing `r_{it}` on the orthogonal shocks from the first two steps:\n  \nr_{\\mathrm{{it}}}=\\alpha_{i,0}+\\dots+\\beta_{\\mathrm{{im}}}\\widehat{e_{\\mathrm{{mt}}}^{M}}+\\beta_{\\mathrm{{is}}}\\widehat{\\varepsilon_{\\mathrm{{st}}}}+\\gamma_{\\mathrm{{it}}} \\quad \\text{(Eq. (2))}\n \n\n**Table 1. Second-Stage Regression Results (Euro Stoxx TMI)**\nThis table shows the results from regressing the market return (`r_{mt}`) on macroeconomic controls and the unexpected exchange rate shock (`\\hat{\\varepsilon}_{st}`) from Step 1.\n\n| | **Second Stage (for Market Return `r_{mt}` )** | | | |\n|:---|:---:|:---:|:---:|:---:|\n| **Currency Shock Used** | **JPY** | **USD** | **GBP** | **CHF** |\n| `\\hat{\\varepsilon}_{st}` Coefficient | 0.43*** | 0.21 | 0.27 | 1.18*** |\n\n*Notes: Significance levels: *** 1%. Coefficients for control variables are omitted.* \n\n### Question\n\nBased on the provided models and data, select all statements that accurately describe the methodological problem with the standard Jorion model and the logic of the orthogonalization procedure.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to evaluate each statement about the methodological issues with the Jorion model and the logic of the orthogonalization procedure.\n\nLet me analyze each option:\n\n**Option A**: \"The Jorion model's primary flaw is its failure to include lagged macroeconomic control variables...\"\n- This is incorrect. The research question explicitly states the primary flaw is the model's \"failure to account for the correlation between exchange rate movements and overall stock market returns.\" The issue isn't about omitted lagged variables, but about contemporaneous correlation between the regressors.\n\n**Option B**: \"The orthogonalization procedure assumes that the raw market return (r_mt) is a suitable proxy...\"\n- This is incorrect. The orthogonalization procedure specifically does NOT use the raw market return in the final regression. Instead, it uses the orthogonalized market shock (ê_mt^M) from Step 2, which has been purged of its correlation with the exchange rate shock.\n\n**Option C**: \"The purpose of including ε̂_st in the second-stage regression is to ensure that the resulting market shock residual (ê_mt^M) is, by construction, statistically orthogonal to the exchange rate shock.\"\n- This is correct. By including ε̂_st as a regressor in Step 2, the residual from that regression (ê_mt^M) will be orthogonal to ε̂_st by the properties of OLS regression. This is the key innovation of the three-step procedure.\n\n**Option D**: \"Based on Table 1, the significant coefficient of 1.18 on the CHF shock...\"\n- This is correct. The significant coefficient shows that exchange rate shocks help explain market returns, demonstrating empirical correlation between these variables. This correlation violates the implicit assumption in the Jorion model that these regressors capture independent sources of risk.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 446, "Question": "### Background\n\nA central theme in household finance is understanding how consumers choose between fixed-rate mortgages (FRMs) and adjustable-rate mortgages (ARMs). A baseline model proposed by Koijen, Van Hemert, and Van Nieuwerburgh (KHN) posits a simple linear relationship between the aggregate ARM share and the \"Household Decision Rule\" (HDR), a proxy for the long-term bond risk premium. This paper extends that model by hypothesizing that borrowers' sensitivity to the HDR is not constant, but varies over time with their perceived risk. This perceived risk is driven by two main factors: the current level of short-term interest rates and the state of the housing market.\n\n### Data / Model Specification\n\nTo capture this time-varying sensitivity, the coefficients of the baseline model are made conditional on the one-year treasury yield (`short_t`) and a dummy variable for a declining housing market (`down_t`, which equals 1 if the Case-Shiller home price index fell in the previous month, and 0 otherwise). This results in the following full interaction model:\n\n  \nARM_{t}=\\alpha_{0}+\\alpha_{1}short_{t}+\\alpha_{2}down_{t}+\\beta_{0}HDR_{t}+\\beta_{1}(HDR_{t} \\times short_{t}) +\\beta_{2}(HDR_{t} \\times down_{t})+\\varepsilon_{t}\n \nEq. (1)\n\nwhere `ARM_t` is the percentage share of new mortgages that are ARMs. The model is estimated using monthly data from the Federal Housing Finance Agency's Monthly Interest Rate Survey (MIRS) from February 1987 to September 2008. Summary statistics and regression results for this sample are provided in Table 1 and Table 2 below.\n\n**Table 1: Summary Statistics & Correlations (MIRS Sample, 1987-2008)**\n\n| Variable | Mean | Std. Dev. | Min | Max |\n| :--- | :--- | :--- | :--- | :--- |\n| ARM (%) | 25.75 | 13.76 | 4.00 | 69.00 |\n| HDR (%) | 1.11 | 0.98 | -0.86 | 3.73 |\n| short_t (%) | 4.90 | 2.02 | 1.00 | 9.56 |\n| down_t | 0.25 | 0.43 | 0 | 1 |\n\n| | ARM_t | HDR_t | short_t | down_t |\n| :--- | :--- | :--- | :--- | :--- |\n| **ARM_t** | 1.00 | | | |\n| **HDR_t** | 0.67 | 1.00 | | |\n| **short_t** | 0.50 | 0.02 | 1.00 | |\n| **down_t** | -0.27 | -0.17 | -0.06 | 1.00 |\n\n**Table 2: ARM Choice Model Regression Results (MIRS Sample)**\n\n| Variable | (1) Simple Model | (2) Full Model |\n| :--- | :--- | :--- |\n| Intercept | 15.28*** | 5.13* |\n| | (1.74) | (2.68) |\n| HDR_t | 9.41*** | 5.09** |\n| | (1.54) | (2.55) |\n| short_t | | 2.16** |\n| | | (0.59) |\n| down_t | | -0.30 |\n| | | (2.25) |\n| HDR_t x short_t | | 1.01* |\n| | | (0.53) |\n| HDR_t x down_t | | -5.06** |\n| | | (2.06) |\n| | | |\n| N | 260 | 260 |\n| Adj. R² | 0.448 | 0.736 |\n\n*Notes: Newey-West standard errors (12 lags) in parentheses. Significance codes: *p<0.1; **p<0.05; ***p<0.01.*\n\n### Question\n\nBased on the provided data and regression results, which of the following statements are valid interpretations consistent with the paper's theory of time-varying perceived risk? Select all that apply.", "Options": {"A": "The coefficient of -5.06 on `HDR_t x down_t` suggests that during a down market, borrowers become more sensitive to the economic incentives (the risk premium) of choosing an ARM.", "B": "The coefficient of -0.30 on `down_t` indicates that a declining housing market has a direct, statistically significant negative impact on the ARM share.", "C": "The negative correlation of -0.27 between `ARM_t` and `down_t` provides initial, model-free evidence that borrowers are less likely to choose ARMs during housing market downturns.", "D": "The significant positive coefficient of 1.01 on the `HDR_t x short_t` interaction term implies that borrowers' sensitivity to the HDR increases when short-term rates are higher."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret both bivariate correlations and multivariate regression coefficients, particularly interaction terms, in the context of the paper's core theory. The strategy is Atomic Decomposition, breaking down the multifaceted interpretation from the original QA into distinct, verifiable statements. Correct options (A, B) accurately link statistical results to theoretical claims. Distractor (C) targets a common error: ignoring statistical significance (the p-value for the `down_t` coefficient is high, given the standard error of 2.25). Distractor (D) is a Conceptual Opposite, misinterpreting the sign of the interaction term to mean 'more sensitive' instead of 'less sensitive'.", "qid": "446", "question": "### Background\n\nA central theme in household finance is understanding how consumers choose between fixed-rate mortgages (FRMs) and adjustable-rate mortgages (ARMs). A baseline model proposed by Koijen, Van Hemert, and Van Nieuwerburgh (KHN) posits a simple linear relationship between the aggregate ARM share and the \"Household Decision Rule\" (HDR), a proxy for the long-term bond risk premium. This paper extends that model by hypothesizing that borrowers' sensitivity to the HDR is not constant, but varies over time with their perceived risk. This perceived risk is driven by two main factors: the current level of short-term interest rates and the state of the housing market.\n\n### Data / Model Specification\n\nTo capture this time-varying sensitivity, the coefficients of the baseline model are made conditional on the one-year treasury yield (`short_t`) and a dummy variable for a declining housing market (`down_t`, which equals 1 if the Case-Shiller home price index fell in the previous month, and 0 otherwise). This results in the following full interaction model:\n\n  \nARM_{t}=\\alpha_{0}+\\alpha_{1}short_{t}+\\alpha_{2}down_{t}+\\beta_{0}HDR_{t}+\\beta_{1}(HDR_{t} \\times short_{t}) +\\beta_{2}(HDR_{t} \\times down_{t})+\\varepsilon_{t}\n \nEq. (1)\n\nwhere `ARM_t` is the percentage share of new mortgages that are ARMs. The model is estimated using monthly data from the Federal Housing Finance Agency's Monthly Interest Rate Survey (MIRS) from February 1987 to September 2008. Summary statistics and regression results for this sample are provided in Table 1 and Table 2 below.\n\n**Table 1: Summary Statistics & Correlations (MIRS Sample, 1987-2008)**\n\n| Variable | Mean | Std. Dev. | Min | Max |\n| :--- | :--- | :--- | :--- | :--- |\n| ARM (%) | 25.75 | 13.76 | 4.00 | 69.00 |\n| HDR (%) | 1.11 | 0.98 | -0.86 | 3.73 |\n| short_t (%) | 4.90 | 2.02 | 1.00 | 9.56 |\n| down_t | 0.25 | 0.43 | 0 | 1 |\n\n| | ARM_t | HDR_t | short_t | down_t |\n| :--- | :--- | :--- | :--- | :--- |\n| **ARM_t** | 1.00 | | | |\n| **HDR_t** | 0.67 | 1.00 | | |\n| **short_t** | 0.50 | 0.02 | 1.00 | |\n| **down_t** | -0.27 | -0.17 | -0.06 | 1.00 |\n\n**Table 2: ARM Choice Model Regression Results (MIRS Sample)**\n\n| Variable | (1) Simple Model | (2) Full Model |\n| :--- | :--- | :--- |\n| Intercept | 15.28*** | 5.13* |\n| | (1.74) | (2.68) |\n| HDR_t | 9.41*** | 5.09** |\n| | (1.54) | (2.55) |\n| short_t | | 2.16** |\n| | | (0.59) |\n| down_t | | -0.30 |\n| | | (2.25) |\n| HDR_t x short_t | | 1.01* |\n| | | (0.53) |\n| HDR_t x down_t | | -5.06** |\n| | | (2.06) |\n| | | |\n| N | 260 | 260 |\n| Adj. R² | 0.448 | 0.736 |\n\n*Notes: Newey-West standard errors (12 lags) in parentheses. Significance codes: *p<0.1; **p<0.05; ***p<0.01.*\n\n### Question\n\nBased on the provided data and regression results, which of the following statements are valid interpretations consistent with the paper's theory of time-varying perceived risk? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement against the data and regression results provided.\n\nLet me analyze each option:\n\n**Option A**: The coefficient of -5.06 on `HDR_t x down_t` suggests that during a down market, borrowers become more sensitive to the economic incentives (the risk premium) of choosing an ARM.\n\nThis is incorrect. The negative coefficient means that during down markets, the effect of HDR on ARM choice is reduced (5.09 - 5.06 = 0.03), not increased. This suggests borrowers become LESS sensitive to the risk premium during down markets, not more sensitive.\n\n**Option B**: The coefficient of -0.30 on `down_t` indicates that a declining housing market has a direct, statistically significant negative impact on the ARM share.\n\nThis is incorrect. The coefficient is -0.30 with a standard error of 2.25, which means it is not statistically significant (the t-statistic would be approximately -0.13). The statement claims it is statistically significant, which is false.\n\n**Option C**: The negative correlation of -0.27 between `ARM_t` and `down_t` provides initial, model-free evidence that borrowers are less likely to choose ARMs during housing market downturns.\n\nThis is correct. The correlation table shows a correlation of -0.27 between ARM_t and down_t. This negative correlation is indeed \"model-free\" evidence (just looking at the raw correlation) that ARM shares tend to be lower when the housing market is declining.\n\n**Option D**: The significant positive coefficient of 1.01 on the `HDR_t x short_t` interaction term implies that borrowers' sensitivity to the HDR increases when short-term rates are higher.\n\nThis is correct. The coefficient is 1.01 with a standard error of 0.53, which is statistically significant at the 10% level (marked with *). The positive interaction term means that when short-term rates are higher, the effect of HDR on ARM choice increases (from 5.09 to 5.09 + 1.01×short_t), indicating increased sensitivity.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 206, "Question": "### Background\n\n**Research Question.** Why is it necessary to use multiple, complementary tests like the Augmented Dickey-Fuller (ADF) and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) to establish the non-stationarity of a time series?\n\n**Setting / Data-Generating Environment.** The ADF test has a null hypothesis of a unit root (non-stationarity), while the KPSS test has a null hypothesis of stationarity. The paper notes that standard unit root tests like ADF have \"low power against stable autoregressive alternatives with roots near unity.\" The KPSS test statistic `η_u` is also known to be sensitive to the choice of a lag parameter `L`, which is used to estimate the long-run variance `S²(L)` in the denominator of the test statistic.\n\n---\n\n### Question\n\nSelect ALL of the following statements that correctly describe the properties, limitations, and mechanics of the ADF and KPSS tests.", "Options": {"A": "The KPSS test statistic, `η_u`, is sensitive to the choice of the lag parameter `L`; it typically decreases as `L` increases because the long-run variance estimator in the denominator, `S²(L)`, grows larger by incorporating more positive autocorrelation terms.", "B": "Choosing a larger lag parameter `L` in the KPSS test is always better as it increases the test statistic `η_u`, making it more likely to correctly detect non-stationarity.", "C": "The ADF test is preferred for its high power in distinguishing between a true unit root and a stationary process with a root close to one, a weakness of the KPSS test.", "D": "The KPSS test is used to complement the ADF test because ADF tests often have low power, meaning they may fail to reject the unit root null hypothesis even when the series is a highly persistent but stationary process."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule).\nThis item assesses a deep conceptual understanding of the statistical theory behind unit root testing. It uses a 'Premise/Assumption Packaging' strategy to test knowledge of the tests' properties. Option A addresses the critical concept of statistical power, which motivates the use of the KPSS test. Option B tests the mechanical understanding of how the KPSS statistic is constructed and why it is sensitive to the lag parameter. Distractors C and D are 'Conceptual Opposites' that directly contradict the correct statements, targeting fundamental misunderstandings of power and test construction.", "qid": "206", "question": "### Background\n\n**Research Question.** Why is it necessary to use multiple, complementary tests like the Augmented Dickey-Fuller (ADF) and Kwiatkowski-Phillips-Schmidt-Shin (KPSS) to establish the non-stationarity of a time series?\n\n**Setting / Data-Generating Environment.** The ADF test has a null hypothesis of a unit root (non-stationarity), while the KPSS test has a null hypothesis of stationarity. The paper notes that standard unit root tests like ADF have \"low power against stable autoregressive alternatives with roots near unity.\" The KPSS test statistic `η_u` is also known to be sensitive to the choice of a lag parameter `L`, which is used to estimate the long-run variance `S²(L)` in the denominator of the test statistic.\n\n---\n\n### Question\n\nSelect ALL of the following statements that correctly describe the properties, limitations, and mechanics of the ADF and KPSS tests.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) This is correct. The KPSS test statistic η_u = T^(-2)∑S_t²/S²(L), where S²(L) is the long-run variance estimator. As L increases, S²(L) typically increases because it incorporates more positive autocorrelation terms, which causes η_u to decrease. This sensitivity to L is a well-known property of the KPSS test.\n\nB) This is incorrect. Increasing L doesn't increase the test statistic - it typically decreases it (as explained in A). Moreover, there's a bias-variance tradeoff in choosing L: too small and you underestimate the long-run variance; too large and you introduce more estimation error. There's no simple \"larger is always better\" rule.\n\nC) This is incorrect. The passage explicitly states that ADF tests have \"low power against stable autoregressive alternatives with roots near unity.\" This is actually a weakness of the ADF test, not a strength. The ADF test struggles to distinguish between true unit roots and highly persistent stationary processes.\n\nD) This is correct. The passage directly supports this by noting ADF's low power problem. When a series is highly persistent but stationary (root close to but less than 1), the ADF test may fail to reject its null hypothesis of a unit root. The KPSS test, with its null of stationarity, provides a complementary perspective - if both tests fail to reject their nulls, it suggests the series may be in this ambiguous region.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 332, "Question": "### Background\n\n**Research Question.** How does the number of long-run equilibrium relationships (cointegrating vectors) among a set of international stock prices relate to the number of common driving trends, the degree of market integration, and long-term portfolio diversification benefits?\n\n**Setting.** Consider a system of `p` international stock markets. The log stock price in each market is integrated of order one, I(1), meaning its first difference is stationary. The system is characterized by its cointegrating rank, `r`.\n\n**Variables and Parameters.**\n- `X_t`: A `p x 1` vector of log stock prices `(x_{1t}, ..., x_{pt})'` at time `t`.\n- `p`: The number of stock markets in the system.\n- `r`: The cointegrating rank, representing the number of stationary long-run relationships among the `p` stock prices (`0 ≤ r ≤ p-1`).\n- `p-r`: The number of common stochastic trends driving the system.\n\n---\n\n### Data / Model Specification\n\nThe core concepts are:\n1.  **Cointegration:** A set of `p` variables, `X_t`, all individually I(1), are cointegrated with rank `r` if there exist `r` linearly independent combinations of these variables that are stationary, I(0).\n2.  **Common Trends:** If the cointegrating rank is `r`, the system of `p` non-stationary prices is driven by a smaller number, `p-r`, of common stochastic trends.\n\n---\n\n### Question\n\nIn a system of `p` international stock markets, a finding of maximum cointegration (`r = p-1`) has several important implications. Which of the following statements are correct implications of `r = p-1`? (Select all that apply)", "Options": {"A": "The markets exhibit a high degree of long-run integration.", "B": "The system is driven by a single common stochastic trend.", "C": "The system is driven by `p` distinct stochastic trends.", "D": "The markets offer significant long-term diversification benefits."}, "Answer": ["A", "B"], "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.5). This item replaces part of the original QA problem. It assesses the core financial and statistical interpretation of a key cointegration scenario (`r=p-1`). The use of Multiple Choice allows for testing multiple facets of the concept simultaneously. Distractors are designed based on classic conceptual opposites: Distractor B (`p` trends) describes the `r=0` case, and Distractor D (high diversification benefits) is the financial opposite of high integration.", "qid": "332", "question": "### Background\n\n**Research Question.** How does the number of long-run equilibrium relationships (cointegrating vectors) among a set of international stock prices relate to the number of common driving trends, the degree of market integration, and long-term portfolio diversification benefits?\n\n**Setting.** Consider a system of `p` international stock markets. The log stock price in each market is integrated of order one, I(1), meaning its first difference is stationary. The system is characterized by its cointegrating rank, `r`.\n\n**Variables and Parameters.**\n- `X_t`: A `p x 1` vector of log stock prices `(x_{1t}, ..., x_{pt})'` at time `t`.\n- `p`: The number of stock markets in the system.\n- `r`: The cointegrating rank, representing the number of stationary long-run relationships among the `p` stock prices (`0 ≤ r ≤ p-1`).\n- `p-r`: The number of common stochastic trends driving the system.\n\n---\n\n### Data / Model Specification\n\nThe core concepts are:\n1.  **Cointegration:** A set of `p` variables, `X_t`, all individually I(1), are cointegrated with rank `r` if there exist `r` linearly independent combinations of these variables that are stationary, I(0).\n2.  **Common Trends:** If the cointegrating rank is `r`, the system of `p` non-stationary prices is driven by a smaller number, `p-r`, of common stochastic trends.\n\n---\n\n### Question\n\nIn a system of `p` international stock markets, a finding of maximum cointegration (`r = p-1`) has several important implications. Which of the following statements are correct implications of `r = p-1`? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze what happens when we have maximum cointegration (r = p-1) in a system of p international stock markets.\n\nKey relationships:\n- Cointegrating rank = r\n- Number of markets = p\n- Number of common stochastic trends = p - r\n\nWhen r = p-1 (maximum cointegration):\n- Number of common stochastic trends = p - (p-1) = 1\n\nLet me evaluate each option:\n\n**Option A: \"The markets exhibit a high degree of long-run integration\"**\n- With r = p-1, we have p-1 cointegrating relationships among p markets\n- This means p-1 stationary linear combinations exist\n- All markets are bound together by these long-run relationships\n- This indicates HIGH integration\n- ✓ CORRECT\n\n**Option B: \"The system is driven by a single common stochastic trend\"**\n- Number of common trends = p - r = p - (p-1) = 1\n- Exactly one common stochastic trend drives all p markets\n- ✓ CORRECT\n\n**Option C: \"The system is driven by p distinct stochastic trends\"**\n- This would only be true if r = 0 (no cointegration)\n- With r = p-1, we have only 1 common trend, not p\n- ✗ INCORRECT\n\n**Option D: \"The markets offer significant long-term diversification benefits\"**\n- With only one common trend driving all markets, they move together in the long run\n- Long-term diversification requires markets to have independent trends\n- Maximum cointegration means MINIMAL diversification benefits\n- ✗ INCORRECT\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 293, "Question": "### Background\n\n**Research Question.** How can a tail risk bound be constructed for aggregate claims when the severity distribution is heavy-tailed, possessing only a finite number of moments and no moment generating function?\n\n**Setting / Data-Generating Environment.** We consider a collective risk model where the claim size distribution `F(y)` is potentially heavy-tailed. The exponential Lundberg-type bounds are not applicable. An alternative framework based on Pareto-type bounds is needed.\n\n**Variables & Parameters.**\n- `F(y)`: The CDF of claim sizes.\n- `\\phi`: A parameter of the claim frequency distribution.\n- `r`: A positive real number indicating the highest finite moment of the claim size distribution.\n- `\\kappa`: A parameter determining the scale of the Pareto bound.\n\n---\n\n### Data / Model Specification\n\nThis approach uses a Pareto-type function for the bound, `\\overline{B}(x) = (1+\\kappa x)^{-r}`. The parameter `\\kappa > 0` is chosen to satisfy the integral constraint:\n  \n\\phi^{-1} = \\int_{0}^{\\infty} (1+\\kappa y)^{r} \\mathrm{d}F(y) \\quad \\text{(Eq. (1))}\n \nThe resulting Pareto bound on the aggregate claims tail is:\n  \n\\overline{G}(x) \\leq \\phi^{-1}(1-p_{0})(1+\\kappa x)^{-r} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the paper's framework for Pareto bounds, select all of the following statements that are correct.", "Options": {"A": "For the case where `r=2`, the parameter `\\kappa` is given by the positive root of the quadratic equation `(E[Y^2])\\kappa^2 + (2E[Y])\\kappa + (1 - \\phi^{-1}) = 0`.", "B": "The Pareto bound is suitable for heavy-tailed claim distributions that may lack a moment generating function, whereas the classical exponential bound requires the existence of one.", "C": "For the case where `r=2`, the parameter `\\kappa` is given by the positive root of the linear equation `(2E[Y])\\kappa + (1 - \\phi^{-1}) = 0`.", "D": "For extreme events (large `x`), the Pareto bound `(1+\\kappa x)^{-r}` decays faster than the exponential bound `e^{-\\kappa x}`, implying lower capital requirements for heavy-tailed risks."}, "Answer": ["A", "B"], "pi_justification": "This item assesses understanding of the conceptual basis and parameter derivation for the Pareto bound framework, using an Atomic Decomposition strategy. Correct option (A) accurately contrasts the assumptions of Pareto and exponential bounds. Correct option (B) correctly identifies the quadratic equation for `\\kappa` when `r=2`. Distractor (C) makes a false claim about the relative decay rates and capital implications (Conceptual Opposite). Distractor (D) presents an incorrect equation for `\\kappa` that would arise from ignoring the `y^2` term in the expansion (Step-Omission Error).", "qid": "293", "question": "### Background\n\n**Research Question.** How can a tail risk bound be constructed for aggregate claims when the severity distribution is heavy-tailed, possessing only a finite number of moments and no moment generating function?\n\n**Setting / Data-Generating Environment.** We consider a collective risk model where the claim size distribution `F(y)` is potentially heavy-tailed. The exponential Lundberg-type bounds are not applicable. An alternative framework based on Pareto-type bounds is needed.\n\n**Variables & Parameters.**\n- `F(y)`: The CDF of claim sizes.\n- `\\phi`: A parameter of the claim frequency distribution.\n- `r`: A positive real number indicating the highest finite moment of the claim size distribution.\n- `\\kappa`: A parameter determining the scale of the Pareto bound.\n\n---\n\n### Data / Model Specification\n\nThis approach uses a Pareto-type function for the bound, `\\overline{B}(x) = (1+\\kappa x)^{-r}`. The parameter `\\kappa > 0` is chosen to satisfy the integral constraint:\n  \n\\phi^{-1} = \\int_{0}^{\\infty} (1+\\kappa y)^{r} \\mathrm{d}F(y) \\quad \\text{(Eq. (1))}\n \nThe resulting Pareto bound on the aggregate claims tail is:\n  \n\\overline{G}(x) \\leq \\phi^{-1}(1-p_{0})(1+\\kappa x)^{-r} \\quad \\text{(Eq. (2))}\n \n\n---\n\n### Question\n\nBased on the paper's framework for Pareto bounds, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each statement based on the Pareto bounds framework described.\n\nFor the integral constraint in Eq. (1):\nφ^(-1) = ∫₀^∞ (1+κy)^r dF(y)\n\nLet me expand this for r=2:\nφ^(-1) = ∫₀^∞ (1+κy)² dF(y)\nφ^(-1) = ∫₀^∞ (1 + 2κy + κ²y²) dF(y)\nφ^(-1) = 1 + 2κE[Y] + κ²E[Y²]\n\nRearranging:\nκ²E[Y²] + 2κE[Y] + (1 - φ^(-1)) = 0\n\nThis confirms statement A is correct and statement C is incorrect (C incorrectly omits the κ² term).\n\nStatement B correctly identifies that Pareto bounds are suitable for heavy-tailed distributions without MGFs, while exponential bounds require MGF existence. This is the key motivation for using Pareto bounds in this framework.\n\nFor statement D, comparing decay rates as x→∞:\n- Pareto bound: (1+κx)^(-r) ~ x^(-r) (polynomial decay)\n- Exponential bound: e^(-κx) (exponential decay)\n\nSince exponential functions decay faster than any polynomial, the Pareto bound actually decays slower than the exponential bound. This means the Pareto bound is less tight for large x, implying higher (not lower) capital requirements. Statement D is incorrect.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 422, "Question": "### Background\n\n**Research Question.** This case examines the paper's challenge to a central tenet of standard Affine Term Structure Models (ATSMs): the idea that the volatility of the yield curve should be fully explained ('spanned') by information within the yield curve itself. The analysis tests whether an external variable—lagged equity market volatility—has incremental predictive power.\n\n### Data / Model Specification\n\nThe full predictive regression model for the volatility of a principal component (`PCk`) is:\n\n  \n\\sigma_{t,t+21}^{PCk} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{PCk} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\nHere, `\\sigma^{ST}` is the lagged log realized stock volatility, and `PrComp_{j,t-1}` are the lagged values of the principal components themselves, which serve as proxies for the state of the yield curve.\n\n**Table 1. Predictive Power of Stock Volatility for Yield Curve Component Volatility (Full Period 1997:10-2013:06)**\n\n| Dependent Variable | `\\hat{\\gamma}_{2}` (t-stat) |\n| :--- | :--- |\n| `\\sigma^{PC1}` (Level Volatility) | 0.222 (5.99) |\n| `\\sigma^{PC2}` (Slope Volatility) | 0.352 (8.36) |\n\n*Note: Results are for the full specification in Eq. (1).*\n\n### Question\n\nBased on the results in Table 1 and the principles of Affine Term Structure Models (ATSMs), which of the following conclusions are supported? Select all that apply.", "Options": {"A": "The evidence indicates that stock market volatility primarily affects the level of the yield curve (PC1), with no significant spillover to the yield curve's slope (PC2).", "B": "The statistical significance of `\\hat{\\gamma}_{2}` for an external variable (`\\sigma^{ST}`) after controlling for yield curve factors (`PrComp`) violates the 'spanning condition' implied by standard ATSMs.", "C": "The fact that stock volatility predicts volatility in two orthogonal components (level and slope) suggests that equity market risk is a pervasive, systemic factor for the bond market, not a narrow one.", "D": "The results confirm the core prediction of standard ATSMs that the bond market is informationally self-contained, with its volatility fully explained by its own past state."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the paper's main theoretical challenge to Affine Term Structure Models. It uses an Atomic Decomposition strategy, converting the first two questions of the source QA into distinct, verifiable statements. Correct option A defines the violation of the spanning condition. Correct option B provides the economic interpretation of the effect on orthogonal components. Distractor C is a direct conceptual opposite of the paper's conclusion. Distractor D is factually incorrect according to Table 1.", "qid": "422", "question": "### Background\n\n**Research Question.** This case examines the paper's challenge to a central tenet of standard Affine Term Structure Models (ATSMs): the idea that the volatility of the yield curve should be fully explained ('spanned') by information within the yield curve itself. The analysis tests whether an external variable—lagged equity market volatility—has incremental predictive power.\n\n### Data / Model Specification\n\nThe full predictive regression model for the volatility of a principal component (`PCk`) is:\n\n  \n\\sigma_{t,t+21}^{PCk} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{PCk} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\nHere, `\\sigma^{ST}` is the lagged log realized stock volatility, and `PrComp_{j,t-1}` are the lagged values of the principal components themselves, which serve as proxies for the state of the yield curve.\n\n**Table 1. Predictive Power of Stock Volatility for Yield Curve Component Volatility (Full Period 1997:10-2013:06)**\n\n| Dependent Variable | `\\hat{\\gamma}_{2}` (t-stat) |\n| :--- | :--- |\n| `\\sigma^{PC1}` (Level Volatility) | 0.222 (5.99) |\n| `\\sigma^{PC2}` (Slope Volatility) | 0.352 (8.36) |\n\n*Note: Results are for the full specification in Eq. (1).*\n\n### Question\n\nBased on the results in Table 1 and the principles of Affine Term Structure Models (ATSMs), which of the following conclusions are supported? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the results in Table 1 and the context of Affine Term Structure Models (ATSMs):\n\n**Analysis of each option:**\n\nA) **Incorrect** - The coefficient for slope volatility (PC2) is 0.352 with t-stat 8.36, which is actually larger and more significant than the coefficient for level volatility (PC1) at 0.222 with t-stat 5.99. Both are highly significant, contradicting the claim of \"no significant spillover to the yield curve's slope.\"\n\nB) **Correct** - The spanning condition in standard ATSMs implies that all information relevant for bond pricing should be contained within the yield curve itself. The fact that an external variable (stock volatility) has significant predictive power even after controlling for yield curve factors (the PrComp variables) directly violates this condition.\n\nC) **Correct** - Stock volatility significantly predicts volatility in both PC1 (level) and PC2 (slope), which are orthogonal components by construction of principal components. This broad impact across multiple dimensions of the yield curve suggests equity market risk is indeed a pervasive, systemic factor affecting the entire bond market structure.\n\nD) **Incorrect** - This is the opposite of what the results show. The significant coefficients on stock volatility demonstrate that bond market volatility is NOT fully explained by its own past state, directly contradicting the core prediction of standard ATSMs about informational self-containment.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 18, "Question": "### Background\n\n**Research Question.** How can the price of a call option on a sum of correlated log-normal variables, `Y(k)`, be analytically approximated using convex bounds?\n\n**Setting.** The value of the reference portfolio `Y(k)` is a sum of correlated log-normal random variables. Pricing an option on `Y(k)` requires calculating `E[(Y(k) - d)_+]`, which is analytically intractable. The method of comonotonic approximation provides a lower bound `Y(k)^l` and an upper bound `Y(k)^u`.\n\n### Data / Model Specification\n\nThe bounds are established using the concept of convex ordering (`\\leq_{cx}`), where `A \\leq_{cx} B` means `E[\\nu(A)] \\leq E[\\nu(B)]` for any convex function `\\nu(x)`. The paper establishes the relationship:\n\n  \nY(k)^l \\leq_{cx} Y(k) \\leq_{cx} Y(k)^u \\quad \\text{(Eq. 1)}\n \n\nThis ordering implies relationships between their expected values and variances:\n\n  \nE[Y(k)^l] = E[Y(k)] = E[Y(k)^u] \\quad \\text{(Eq. 2)}\n \n\n  \n\\mathrm{Var}[Y(k)^l] \\leq \\mathrm{Var}[Y(k)] \\leq \\mathrm{Var}[Y(k)^u] \\quad \\text{(Eq. 3)}\n \n\nThe quality of the lower bound approximation `Y(k)^l` depends on how well a single conditioning variable `\\Lambda(k)` captures the randomness of the components of `Y(k)`.\n\n---\n\nBased on the theoretical properties of these convex bounds, which of the following statements are **INCORRECT**?\n", "Options": {"A": "The payoff function of a European call option, `f(x) = (x-d)_+`, is a convex function, which is why these bounds are applicable for option pricing.", "B": "The equality of the expected values in Eq. (2) implies that the bounds `Y(k)^l` and `Y(k)^u` will always provide a tight approximation for the price of a call option on `Y(k)`.", "C": "For any convex function `\\nu`, the inequality `E[\\nu(Y(k)^l)] \\leq E[\\nu(Y(k)^u)]` must hold.", "D": "If the underlying components of `Y(k)` become perfectly correlated (comonotonic), then `\\mathrm{Var}[Y(k)^l] = \\mathrm{Var}[Y(k)] = \\mathrm{Var}[Y(k)^u]`."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the theoretical underpinnings of the paper's core methodology. The assessment objective inherently leads to multiple true statements and only a few false ones, making it a good candidate for the Single-Correct-Answer Inversion Protocol. The stem is inverted to ask for incorrect statements.\n- **Option A (Distractor - True):** This follows directly from the transitivity of the convex order in Eq. (1). It is a correct statement, thus an incorrect answer to the inverted question.\n- **Option B (Distractor - True):** The convexity of the call payoff is the fundamental reason this entire framework is useful for option pricing. It is a correct statement, thus an incorrect answer.\n- **Option C (Correct - False Statement):** If `Y(k)` becomes comonotonic, then `Y(k)` is equivalent to `Y(k)^u` in distribution, so `Var[Y(k)] = Var[Y(k)^u]`. However, `Y(k)^l` is a conditional expectation, and its variance will only equal `Var[Y(k)]` if the conditioning variable `\\Lambda(k)` perfectly explains `Y(k)`, which is not guaranteed. Therefore, the three-way equality is not necessarily true.\n- **Option D (Correct - False Statement):** This is a critical conceptual error. Option prices depend on the entire distribution, especially the variance (and higher moments), not just the mean. Equal means do not guarantee tight option price bounds. The variance inequalities in Eq. (3) show that the distributions can be very different, leading to wide option price bounds despite having the same mean.", "qid": "18", "question": "### Background\n\n**Research Question.** How can the price of a call option on a sum of correlated log-normal variables, `Y(k)`, be analytically approximated using convex bounds?\n\n**Setting.** The value of the reference portfolio `Y(k)` is a sum of correlated log-normal random variables. Pricing an option on `Y(k)` requires calculating `E[(Y(k) - d)_+]`, which is analytically intractable. The method of comonotonic approximation provides a lower bound `Y(k)^l` and an upper bound `Y(k)^u`.\n\n### Data / Model Specification\n\nThe bounds are established using the concept of convex ordering (`\\leq_{cx}`), where `A \\leq_{cx} B` means `E[\\nu(A)] \\leq E[\\nu(B)]` for any convex function `\\nu(x)`. The paper establishes the relationship:\n\n  \nY(k)^l \\leq_{cx} Y(k) \\leq_{cx} Y(k)^u \\quad \\text{(Eq. 1)}\n \n\nThis ordering implies relationships between their expected values and variances:\n\n  \nE[Y(k)^l] = E[Y(k)] = E[Y(k)^u] \\quad \\text{(Eq. 2)}\n \n\n  \n\\mathrm{Var}[Y(k)^l] \\leq \\mathrm{Var}[Y(k)] \\leq \\mathrm{Var}[Y(k)^u] \\quad \\text{(Eq. 3)}\n \n\nThe quality of the lower bound approximation `Y(k)^l` depends on how well a single conditioning variable `\\Lambda(k)` captures the randomness of the components of `Y(k)`.\n\n---\n\nBased on the theoretical properties of these convex bounds, which of the following statements are **INCORRECT**?\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The payoff function of a European call option, `f(x) = (x-d)_+`, is a convex function, which is why these bounds are applicable for option pricing.\n- This is CORRECT. The call option payoff is indeed convex, which allows the convex ordering to be applied.\n\nB) The equality of the expected values in Eq. (2) implies that the bounds `Y(k)^l` and `Y(k)^u` will always provide a tight approximation for the price of a call option on `Y(k)`.\n- This is INCORRECT. Equal expected values do not guarantee tight bounds for option prices. The option price depends on the entire distribution, not just the mean. The bounds can be quite wide despite having equal means, especially when the variances differ significantly (as shown in Eq. 3).\n\nC) For any convex function `\\nu`, the inequality `E[\\nu(Y(k)^l)] \\leq E[\\nu(Y(k)^u)]` must hold.\n- This is CORRECT. This follows directly from the convex ordering relationship `Y(k)^l \\leq_{cx} Y(k)^u`.\n\nD) If the underlying components of `Y(k)` become perfectly correlated (comonotonic), then `\\mathrm{Var}[Y(k)^l] = \\mathrm{Var}[Y(k)] = \\mathrm{Var}[Y(k)^u]`.\n- This is CORRECT. When the components are comonotonic, the lower bound becomes exact because a single conditioning variable can perfectly capture all the randomness. In this case, all three random variables have the same distribution.\n\nB", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 88, "Question": "### Background\n\nThe study performs a robustness check by examining a sample of firms that existed during two opposing legislative periods: the 1991 Employment Contracts Act (which weakened unions) and the 2000 Employment Relations Act (which strengthened them). The analysis focuses on the long-run Average Cumulative Abnormal Returns (ACARs) for the same firms across these two events.\n\n### Data / Model Specification\n\nThe study reports two key findings for this restricted sample of firms:\n1.  The ACAR around the anti-union 1991 legislation was significantly positive.\n2.  The Pearson correlation between the ACARs from the 1991 and 2000 periods was negative and statistically significant: `ρ(ACAR_1991, ACAR_2000) = -0.2507` (p-value = 0.068).\n\n---\n\nBased on this price-reversal test across two opposing legislative eras, which of the following inferences are supported?", "Options": {"A": "Firms that benefited most from the anti-union law in 1991 were also the ones harmed most by the pro-union law in 2000.", "B": "The correlation result implies that firms that benefited from the 1991 law also benefited from the 2000 law, indicating a persistent 'quality' factor.", "C": "The results help refute the alternative explanation that the negative returns in 2000 were due to a persistent, long-term decline in the industries of unionized firms.", "D": "The test provides an out-of-sample validation of the main findings, suggesting that union power is a causal driver of firm valuation."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to understand the causal logic of using an opposing historical event as a natural experiment and to correctly interpret the meaning of a negative correlation in this context. Depth Strategy: Reverse-Reasoning. Given the empirical result (negative correlation across opposing treatments), the user must identify all plausible causal inferences that can be drawn. Distractor Logic: D is a conceptual opposite. It correctly identifies the idea of a persistent factor but misinterprets the sign of the correlation, leading to a conclusion that is the exact opposite of what the data show.", "qid": "88", "question": "### Background\n\nThe study performs a robustness check by examining a sample of firms that existed during two opposing legislative periods: the 1991 Employment Contracts Act (which weakened unions) and the 2000 Employment Relations Act (which strengthened them). The analysis focuses on the long-run Average Cumulative Abnormal Returns (ACARs) for the same firms across these two events.\n\n### Data / Model Specification\n\nThe study reports two key findings for this restricted sample of firms:\n1.  The ACAR around the anti-union 1991 legislation was significantly positive.\n2.  The Pearson correlation between the ACARs from the 1991 and 2000 periods was negative and statistically significant: `ρ(ACAR_1991, ACAR_2000) = -0.2507` (p-value = 0.068).\n\n---\n\nBased on this price-reversal test across two opposing legislative eras, which of the following inferences are supported?", "model_evaluation_result": {"model_raw_response": "Looking at the data and findings:\n\n- 1991 Employment Contracts Act (weakened unions): positive ACARs\n- 2000 Employment Relations Act (strengthened unions): negative correlation with 1991 ACARs\n- ρ(ACAR_1991, ACAR_2000) = -0.2507 (p-value = 0.068)\n\nThe negative correlation means firms that had positive abnormal returns in 1991 (when unions were weakened) tended to have negative abnormal returns in 2000 (when unions were strengthened), and vice versa.\n\nAnalyzing each option:\n\nA) This is supported. The negative correlation indicates firms that benefited from anti-union legislation in 1991 were harmed by pro-union legislation in 2000.\n\nB) This is incorrect. The negative correlation shows the opposite - firms that benefited in 1991 were harmed in 2000, not that they benefited in both periods.\n\nC) This is supported. If unionized firms were in long-term declining industries, we would expect consistent negative returns across both periods. The reversal pattern (positive in 1991, then negative correlation in 2000) contradicts this alternative explanation.\n\nD) This is supported. The reversal pattern when union power changes in opposite directions provides strong evidence that union power causally affects firm value, not just correlation.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 433, "Question": "### Background\n\n**Research Question.** What are the key empirical differences between firms listing on main versus second-tier European stock markets, and what do these differences imply about market structure and the role of financial intermediaries?\n\n**Setting / Data-Generating Environment.** The analysis covers 3,776 Initial Public Offerings (IPOs) from 1995-2010 across four major European economies. The data distinguishes between listings on main markets (e.g., London's Official List) and various second-tier markets (e.g., London's Alternative Investment Market, or AIM).\n\n**Variables & Parameters.**\n\n*   `Age`: The age of the issuing firm at the time of the IPO (years).\n*   `Offer size`: The total proceeds from the IPO, adjusted for inflation to 2010 purchasing power (€ millions).\n*   `Secondary offer`: The ratio of shares sold by existing shareholders to the total shares offered in the IPO (dimensionless, expressed as %).\n*   `VC`: An indicator variable equal to 1 if the firm is venture-capital backed, and 0 otherwise (dimensionless, expressed as % of firms in a given market).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Descriptive Statistics of IPOs in Europe, by Listing Market (Selected Markets)**\n\n| Market | IPOs (No.) | Age (median, yrs) | Offer size (median, €m) | Secondary offer (median, %) | VC (% backed) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **London** | | | | | |\n| Official List | 459 | 6.0 | 70.0 | 11.2 | 65.7 |\n| AIM | 1,666 | 2.0 | 7.7 | 0.0 | 61.9 |\n| **Milan** | | | | | |\n| MTA | 149 | 29.0 | 103.0 | 17.4 | 23.5 |\n\n*Source: Adapted from the paper's Table 1.* The paper posits that underwriters act as certifying agents, producing information to alleviate asymmetries between firm insiders and outsiders.\n\n---\n\n### The Question\n\nBased on the data in Table 1 and the concept of underwriter certification, select all statements that are supported by the evidence.", "Options": {"A": "Due to the large volume of IPOs on AIM, economies of scale likely lead to lower average underwriter fees (as a percentage of proceeds) compared to Milan's MTA.", "B": "Firms listing on London's AIM are typically at an earlier life-cycle stage than those on the Official List, as evidenced by their lower median age, smaller offer sizes, and near-zero median secondary offerings.", "C": "The underwriter's certification role is likely more critical for AIM-listed firms because their characteristics (e.g., younger age) are associated with greater information asymmetry for potential investors.", "D": "The high proportion of VC-backed firms on AIM (61.9%) reduces information asymmetry, making the underwriter's certification role less important than on Milan's MTA (23.5% VC-backed)."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize descriptive statistics with financial theory. It uses an Atomic Decomposition strategy, breaking down the original QA into testable propositions about firm characteristics on second-tier markets and the resulting implications for the underwriter's certification role. Distractors are based on a Conceptual Opposite argument regarding fees (C) and a plausible but incorrect theoretical inference about VC backing (D).", "qid": "433", "question": "### Background\n\n**Research Question.** What are the key empirical differences between firms listing on main versus second-tier European stock markets, and what do these differences imply about market structure and the role of financial intermediaries?\n\n**Setting / Data-Generating Environment.** The analysis covers 3,776 Initial Public Offerings (IPOs) from 1995-2010 across four major European economies. The data distinguishes between listings on main markets (e.g., London's Official List) and various second-tier markets (e.g., London's Alternative Investment Market, or AIM).\n\n**Variables & Parameters.**\n\n*   `Age`: The age of the issuing firm at the time of the IPO (years).\n*   `Offer size`: The total proceeds from the IPO, adjusted for inflation to 2010 purchasing power (€ millions).\n*   `Secondary offer`: The ratio of shares sold by existing shareholders to the total shares offered in the IPO (dimensionless, expressed as %).\n*   `VC`: An indicator variable equal to 1 if the firm is venture-capital backed, and 0 otherwise (dimensionless, expressed as % of firms in a given market).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Descriptive Statistics of IPOs in Europe, by Listing Market (Selected Markets)**\n\n| Market | IPOs (No.) | Age (median, yrs) | Offer size (median, €m) | Secondary offer (median, %) | VC (% backed) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **London** | | | | | |\n| Official List | 459 | 6.0 | 70.0 | 11.2 | 65.7 |\n| AIM | 1,666 | 2.0 | 7.7 | 0.0 | 61.9 |\n| **Milan** | | | | | |\n| MTA | 149 | 29.0 | 103.0 | 17.4 | 23.5 |\n\n*Source: Adapted from the paper's Table 1.* The paper posits that underwriters act as certifying agents, producing information to alleviate asymmetries between firm insiders and outsiders.\n\n---\n\n### The Question\n\nBased on the data in Table 1 and the concept of underwriter certification, select all statements that are supported by the evidence.", "model_evaluation_result": {"model_raw_response": "Looking at the data and analyzing each statement:\n\n**Statement A**: The data only shows IPO volumes and does not provide any information about underwriter fees. We cannot infer anything about economies of scale or fee structures from the given information. **Not supported.**\n\n**Statement B**: The data clearly shows:\n- AIM firms have median age of 2.0 years vs 6.0 years for Official List\n- AIM firms have median offer size of €7.7m vs €70.0m for Official List  \n- AIM firms have median secondary offering of 0.0% vs 11.2% for Official List\n\nThese characteristics (younger, smaller offerings, no secondary sales) are consistent with earlier-stage firms. **Supported.**\n\n**Statement C**: Younger firms (AIM median age 2.0 years) typically have:\n- Shorter operating histories\n- Less established track records\n- Greater uncertainty about future prospects\n\nThese factors create greater information asymmetry between insiders and potential investors, making the underwriter's certification role more critical for reducing this asymmetry. **Supported.**\n\n**Statement D**: This statement incorrectly assumes that VC backing reduces the need for underwriter certification. In reality:\n- VC backing and underwriter certification serve complementary roles\n- VCs may have inside information that still needs to be credibly communicated to public investors\n- The high VC backing on AIM (61.9%) doesn't eliminate information asymmetry issues\n- The younger age of AIM firms (2.0 years) suggests greater information asymmetry regardless of VC backing\n\n**Not supported.**\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 357, "Question": "### Background\n\n**Research Question.** How can econometric methods address concerns about causality in the relationship between firm characteristics and stock returns following deregulation?\n\n**Setting and Sample.** The study uses a cross-sectional OLS regression to link commercial bank characteristics (like size and `SEC20` status) to abnormal returns. However, this approach is vulnerable to critiques of endogeneity and omitted variable bias, which could be addressed by more advanced methods like Instrumental Variables (IV) or panel data with Fixed Effects (FE).\n\n### Data / Model Specification\n\nThe paper estimates the following cross-sectional model via OLS:\n\n  \n\\mathrm{CAR}_{i} = \\beta_{0} + \\beta_{1}\\mathrm{Ln}(\\mathrm{SIZE}_{i}) + \\beta_{2}\\mathrm{SEC}20_{i} + \\text{controls}_i + e_{i} \\quad \\text{(Eq. (1))}\n \n\nConcerns exist that `SEC20_i` is endogenous (correlated with the error term `e_i`) and that time-invariant unobserved variables (e.g., managerial quality) bias the estimates.\n\n---\n\n### Question\n\nRegarding the potential endogeneity of `SEC20` status and the use of alternative estimation methods, select all correct statements.", "Options": {"A": "A valid instrumental variable for `SEC20` status must be correlated with a bank's choice to have a subsidiary but must not be independently correlated with the bank's abnormal returns from deregulation.", "B": "The main advantage of a firm fixed-effects model is its ability to provide unbiased estimates for time-invariant characteristics like `SEC20` status.", "C": "A primary limitation of using a firm fixed-effects panel model for this analysis is that it cannot estimate the coefficient on a time-invariant variable like `SEC20` status.", "D": "The exclusion restriction for an instrumental variable requires it to be highly correlated with the abnormal return `CAR_i`."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses knowledge of advanced causal inference techniques (IV and FE) as they apply to the paper's research design. It uses the 'Premise/Assumption Packaging' strategy, asking the user to identify correct statements about the conditions and limitations of these methods. Options A and B correctly state the exclusion restriction for IV and a key limitation of FE, respectively. Distractor C is a 'Conceptual Opposite' of option B. Distractor D is 'Almost Right' but confuses the relevance condition with the exclusion restriction for an IV.", "qid": "357", "question": "### Background\n\n**Research Question.** How can econometric methods address concerns about causality in the relationship between firm characteristics and stock returns following deregulation?\n\n**Setting and Sample.** The study uses a cross-sectional OLS regression to link commercial bank characteristics (like size and `SEC20` status) to abnormal returns. However, this approach is vulnerable to critiques of endogeneity and omitted variable bias, which could be addressed by more advanced methods like Instrumental Variables (IV) or panel data with Fixed Effects (FE).\n\n### Data / Model Specification\n\nThe paper estimates the following cross-sectional model via OLS:\n\n  \n\\mathrm{CAR}_{i} = \\beta_{0} + \\beta_{1}\\mathrm{Ln}(\\mathrm{SIZE}_{i}) + \\beta_{2}\\mathrm{SEC}20_{i} + \\text{controls}_i + e_{i} \\quad \\text{(Eq. (1))}\n \n\nConcerns exist that `SEC20_i` is endogenous (correlated with the error term `e_i`) and that time-invariant unobserved variables (e.g., managerial quality) bias the estimates.\n\n---\n\n### Question\n\nRegarding the potential endogeneity of `SEC20` status and the use of alternative estimation methods, select all correct statements.", "model_evaluation_result": {"model_raw_response": "Looking at each statement about endogeneity and estimation methods:\n\nA) This correctly describes the two key requirements for a valid instrumental variable:\n- Relevance: Must be correlated with the endogenous variable (SEC20 status)\n- Exclusion restriction: Must affect the outcome (CAR) only through the endogenous variable, not directly\n\nThis statement is **correct**.\n\nB) This is incorrect. Fixed effects models eliminate time-invariant unobserved heterogeneity, but they cannot provide estimates for time-invariant variables themselves. The coefficients on time-invariant variables are not identified in FE models.\n\nThis statement is **incorrect**.\n\nC) This accurately identifies a key limitation of fixed effects models. Since SEC20 status appears to be time-invariant in this cross-sectional setting, a firm fixed-effects model would absorb this variable along with the fixed effects, making it impossible to estimate its coefficient.\n\nThis statement is **correct**.\n\nD) This misstates the exclusion restriction. The exclusion restriction requires that the instrument NOT be correlated with the outcome variable (CAR) except through its effect on the endogenous variable. High correlation with CAR would violate the exclusion restriction.\n\nThis statement is **incorrect**.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 413, "Question": "### Background\n\n**Research Question:** This case requires an interpretation of diagnostic tests for a BEKK-MVGARCH model to assess its adequacy. Model diagnostics are crucial for determining if the model's assumptions are met and its conclusions are reliable.\n\n### Data / Model Specification\n\nThe model consists of a VAR(1) mean equation and a BEKK(1,1) variance equation. After estimation, Ljung-Box Q-statistics are calculated for the standardized residuals (`Q(12)`) and squared standardized residuals (`Q²(12)`) to test for remaining serial correlation.\n\n- A significant `Q(12)` statistic suggests the mean equation is misspecified.\n- A significant `Q²(12)` statistic suggests the variance equation is misspecified.\n\n**Table 1: BEKK-MVGARCH Diagnostic Results**\n\n| Market | Test | Statistic | p-value |\n| :--- | :--- | :--- | :--- |\n| Shanghai | `Q(12)` | 48.73 | 0.00 |\n| | `Q²(12)` | 7.08 | 0.93 |\n| Hong Kong | `Q(12)` | 24.51 | 0.22 |\n| | `Q²(12)` | 41.10 | 0.01 |\n\n### Question\n\nBased on the diagnostic results in **Table 1**, select all of the following statements that are correct interpretations of model misspecification.\n", "Options": {"A": "The mean equation for the Shanghai market is misspecified.", "B": "The mean equation for the Hong Kong market is misspecified.", "C": "The variance equation for the Shanghai market is misspecified.", "D": "The variance equation for the Hong Kong market is misspecified."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the ability to correctly interpret standard diagnostic tests for a time series model. It uses a select-all-that-apply format to test for comprehensive understanding. The correct answers (A, D) correspond to the statistically significant p-values (0.00 and 0.01). The distractors (B, C) correspond to the insignificant p-values and represent a failure to correctly apply the decision rule for hypothesis testing.", "qid": "413", "question": "### Background\n\n**Research Question:** This case requires an interpretation of diagnostic tests for a BEKK-MVGARCH model to assess its adequacy. Model diagnostics are crucial for determining if the model's assumptions are met and its conclusions are reliable.\n\n### Data / Model Specification\n\nThe model consists of a VAR(1) mean equation and a BEKK(1,1) variance equation. After estimation, Ljung-Box Q-statistics are calculated for the standardized residuals (`Q(12)`) and squared standardized residuals (`Q²(12)`) to test for remaining serial correlation.\n\n- A significant `Q(12)` statistic suggests the mean equation is misspecified.\n- A significant `Q²(12)` statistic suggests the variance equation is misspecified.\n\n**Table 1: BEKK-MVGARCH Diagnostic Results**\n\n| Market | Test | Statistic | p-value |\n| :--- | :--- | :--- | :--- |\n| Shanghai | `Q(12)` | 48.73 | 0.00 |\n| | `Q²(12)` | 7.08 | 0.93 |\n| Hong Kong | `Q(12)` | 24.51 | 0.22 |\n| | `Q²(12)` | 41.10 | 0.01 |\n\n### Question\n\nBased on the diagnostic results in **Table 1**, select all of the following statements that are correct interpretations of model misspecification.\n", "model_evaluation_result": {"model_raw_response": "Looking at the diagnostic results:\n\nFor Shanghai:\n- Q(12): Statistic = 48.73, p-value = 0.00 (significant at any conventional level)\n- Q²(12): Statistic = 7.08, p-value = 0.93 (not significant)\n\nFor Hong Kong:\n- Q(12): Statistic = 24.51, p-value = 0.22 (not significant)\n- Q²(12): Statistic = 41.10, p-value = 0.01 (significant at conventional levels)\n\nBased on the interpretation rules:\n- A significant Q(12) statistic (low p-value) suggests mean equation misspecification\n- A significant Q²(12) statistic (low p-value) suggests variance equation misspecification\n\nTherefore:\n- Shanghai has a significant Q(12) with p-value = 0.00 → mean equation is misspecified\n- Hong Kong has a significant Q²(12) with p-value = 0.01 → variance equation is misspecified\n\nA,D", "model_answer": ["A", "D"], "error": null}}
{"ID": 431, "Question": "### Background\n\n**Research Question.** What is the causal effect of an underwriter's reputation on the degree of IPO underpricing, after accounting for the non-random matching between issuers and underwriters?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage least-squares (2SLS) regression to analyze IPO underpricing. The key challenge is that high-quality firms may preferentially match with high-reputation underwriters, creating an endogeneity problem.\n\n**Variables & Parameters.**\n\n*   `Underpricing_i`: The percentage difference between the first-day closing price and the offer price for IPO `i`.\n*   `Reputation_i`: The endogenous measure of underwriter reputation. The primary measure is the `Fragmented ranking Equally weighted` score.\n*   `X_i`: A vector of exogenous control variables for firm `i` (e.g., firm age, offer size, VC-backing).\n*   `Z_i`: The instrumental variable for `Reputation_i`, defined as the number of underwriters that have underwritten European IPOs in the same 2-digit industry as firm `i` in the past five years.\n\n---\n\n### Data / Model Specification\n\nThe study estimates the following structural model:\n\n  \n\\text{Underpricing}_i = \\beta_0 + \\beta_1 \\text{Reputation}_i + \\Gamma' X_i + \\epsilon_i\n \n\nDue to endogeneity, `Reputation_i` is instrumented using `Z_i` in a 2SLS framework.\n\n**Table 1: 2SLS Regression of Underpricing on Underwriter Reputation (Model 1)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| **Second Stage** | | |\n| Underwriter Reputation | | |\n| (Fragmented ranking Equally weighted) | -0.04 | (-6.08) |\n| Ln(Offer Size) | -0.03 | (-5.46) |\n| VC-backed | 0.09 | (1.29) |\n| ... (other controls) | ... | ... |\n| **First Stage** | | |\n| Industry Underwriter Av. (Instrument `Z_i`) | 0.04 | (1.73) |\n\n*Source: Adapted from Table 7, Model 1 in the paper. The dependent variable in the first stage is Underwriter Reputation.* The `underwriter certification hypothesis` posits that reputable underwriters reduce information asymmetry, leading to lower underpricing.\n\n---\n\n### The Question\n\nBased on the provided information, select all statements that correctly describe the 2SLS model's setup, assumptions, and results.", "Options": {"A": "The instrument `Z_i` violates the exclusion restriction because a higher availability of underwriters directly reduces underpricing through increased competition, independent of the chosen underwriter's reputation.", "B": "The primary endogeneity concern is that high-reputation underwriters might take on riskier IPOs to earn higher fees, leading to an upward bias (i.e., less negative) in an OLS estimate of the `Reputation` coefficient.", "C": "The instrument `Z_i` (availability of industry-experienced underwriters) shows evidence of relevance, as its positive first-stage coefficient (0.04) is economically intuitive and at least marginally statistically significant (t=1.73).", "D": "If an OLS regression of `Underpricing` on `Reputation` were run, the coefficient on `Reputation` would likely be biased downwards (i.e., more negative than the true causal effect) due to unobserved issuer quality being positively correlated with `Reputation` and negatively correlated with `Underpricing`."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of the core components of a 2SLS estimation in the context of IPO underpricing. It uses an Atomic Decomposition strategy to test distinct concepts: the direction of omitted variable bias and the criteria for instrument relevance. Distractors are designed to represent a plausible alternative theory of bias (C) and a misapplication of an advanced critique as a statement of fact (D).", "qid": "431", "question": "### Background\n\n**Research Question.** What is the causal effect of an underwriter's reputation on the degree of IPO underpricing, after accounting for the non-random matching between issuers and underwriters?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage least-squares (2SLS) regression to analyze IPO underpricing. The key challenge is that high-quality firms may preferentially match with high-reputation underwriters, creating an endogeneity problem.\n\n**Variables & Parameters.**\n\n*   `Underpricing_i`: The percentage difference between the first-day closing price and the offer price for IPO `i`.\n*   `Reputation_i`: The endogenous measure of underwriter reputation. The primary measure is the `Fragmented ranking Equally weighted` score.\n*   `X_i`: A vector of exogenous control variables for firm `i` (e.g., firm age, offer size, VC-backing).\n*   `Z_i`: The instrumental variable for `Reputation_i`, defined as the number of underwriters that have underwritten European IPOs in the same 2-digit industry as firm `i` in the past five years.\n\n---\n\n### Data / Model Specification\n\nThe study estimates the following structural model:\n\n  \n\\text{Underpricing}_i = \\beta_0 + \\beta_1 \\text{Reputation}_i + \\Gamma' X_i + \\epsilon_i\n \n\nDue to endogeneity, `Reputation_i` is instrumented using `Z_i` in a 2SLS framework.\n\n**Table 1: 2SLS Regression of Underpricing on Underwriter Reputation (Model 1)**\n\n| Variable | Coefficient | (t-statistic) |\n| :--- | :--- | :--- |\n| **Second Stage** | | |\n| Underwriter Reputation | | |\n| (Fragmented ranking Equally weighted) | -0.04 | (-6.08) |\n| Ln(Offer Size) | -0.03 | (-5.46) |\n| VC-backed | 0.09 | (1.29) |\n| ... (other controls) | ... | ... |\n| **First Stage** | | |\n| Industry Underwriter Av. (Instrument `Z_i`) | 0.04 | (1.73) |\n\n*Source: Adapted from Table 7, Model 1 in the paper. The dependent variable in the first stage is Underwriter Reputation.* The `underwriter certification hypothesis` posits that reputable underwriters reduce information asymmetry, leading to lower underpricing.\n\n---\n\n### The Question\n\nBased on the provided information, select all statements that correctly describe the 2SLS model's setup, assumptions, and results.", "model_evaluation_result": {"model_raw_response": "Looking at this 2SLS model setup, I need to evaluate each statement about the model's assumptions, setup, and implications.\n\n**Statement A**: Claims the instrument violates exclusion restriction because more underwriters directly reduce underpricing through competition.\n- This is a valid concern. If having more industry-experienced underwriters available increases competition and directly lowers underpricing (regardless of which underwriter is chosen), then the instrument affects the outcome through a channel other than the endogenous variable.\n- This would violate the exclusion restriction.\n\n**Statement B**: Claims the endogeneity concern is that high-reputation underwriters take riskier IPOs for higher fees.\n- This contradicts the background information, which states: \"high-quality firms may preferentially match with high-reputation underwriters\"\n- The actual concern is reverse causation where better firms choose better underwriters, not that reputable underwriters choose riskier firms.\n\n**Statement C**: Claims the instrument shows evidence of relevance.\n- The first-stage coefficient is 0.04 with t=1.73\n- While t=1.73 is below conventional significance levels, it's marginally significant\n- The positive coefficient makes economic sense: more available underwriters → higher reputation scores\n- The statement correctly describes this as \"at least marginally statistically significant\"\n\n**Statement D**: Claims OLS would produce downward bias (more negative coefficient).\n- If high-quality firms (with lower underpricing) select high-reputation underwriters, then:\n  - Reputation and unobserved quality are positively correlated\n  - Unobserved quality and underpricing are negatively correlated\n- This creates negative bias in OLS, making the coefficient more negative than the true effect\n- The 2SLS coefficient (-0.04) being less negative than what OLS would show is consistent with this bias story\n\nTherefore, statements A, C, and D correctly describe aspects of the model.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 414, "Question": "### Background\n\n**Research Question.** Is the magnitude of the stock price drop at a seasoned equity offering (SEO) announcement related to the timing of the announcement relative to the firm's last public information release?\n\n**Setting / Data-Generating Environment.** The analysis begins with an event study to establish the baseline price reaction to SEOs. It then uses a weighted least squares (WLS) regression to test if the magnitude of this reaction varies with issue timing. The sample is split into two scenarios, with this question focusing on the primary scenario:\n- **Case 1:** An earnings release is followed by an issue announcement, which is followed by the issue (no intervening release).\n\n**Variables & Parameters.**\n- `Announcement Return`: Two-day abnormal return around the SEO announcement, calculated as the firm's return minus the return on an equal-weighted market portfolio.\n- `Days_Announce`: The number of calendar days between the issue announcement and the previous earnings release.\n\n### Data / Model Specification\n\n**Table 1. Average Abnormal Returns Around SEO Announcement Day (%)**\n\n| Day relative to event | Announcement Abnormal Return | Announcement t-statistic |\n|:---------------------|:-----------------------------|:-------------------------|\n| -2                   | -0.19                        | -1.84                    |\n| -1                   | -2.26                        | -20.64                   |\n| 0                    | -0.43                        | -3.74                    |\n| 1                    | 0.11                         | 1.00                     |\n\n*Source: Adapted from the original paper's Table 2. Day 0 is the date of the announcement in the Wall Street Journal.*\n\nThe following regression model is estimated for the **Case 1** subsample:\n  \n\\text{Announcement Return}_{i} = \\beta_0 + \\beta_1 \\times \\text{Days\\_Announce}_{i} + \\epsilon_i \\quad \\text{(Eq. (1))}\n \n\n**Table 2. WLS Regression of Announcement Return on Timing (Case 1 only)**\n\n| Dependent Variable | Explanatory Variable | Constant `(x10^-2)` | Timing Coeff. `(x10^-3)` | N |\n|:-------------------|:---------------------|:--------------------|:-------------------------|:--|\n| Announcement Return| `Days_Announce`      | -2.251** (0.306)    | -0.147* (0.085)          | 435|\n\n*White heteroskedasticity-robust standard errors in parentheses. *Significant at 10% level. **Significant at 1% level.*\n\n### Question\n\nBased on the provided data and the theory of time-varying adverse selection, select all of the following statements that are supported by the evidence.", "Options": {"A": "The regression results indicate that while the timing of an announcement is statistically significant at the 10% level, its economic impact on the price drop is negligible.", "B": "The potential for errors-in-variables bias in the regression suggests that the reported coefficient `β̂₁` likely overstates the true negative relationship between announcement timing and price reaction.", "C": "The regression coefficient on `Days_Announce` implies that a 30-day delay in announcing an issue is associated with an additional price drop of approximately 0.44%.", "D": "The two-day cumulative abnormal return (days -1 and 0) associated with an SEO announcement is approximately -2.69%, consistent with a 'lemons market' interpretation."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the paper's core pricing result (Hypothesis 3). It uses an 'atomic decomposition' strategy, breaking the finding into testable statements about the baseline price drop (A), the economic magnitude of the timing effect (B), and the implication of an econometric critique (C/D). Distractor C presents the conceptual opposite of attenuation bias (which biases toward zero, thus understating the effect). Distractor D misinterprets the paper's conclusion on economic significance.", "qid": "414", "question": "### Background\n\n**Research Question.** Is the magnitude of the stock price drop at a seasoned equity offering (SEO) announcement related to the timing of the announcement relative to the firm's last public information release?\n\n**Setting / Data-Generating Environment.** The analysis begins with an event study to establish the baseline price reaction to SEOs. It then uses a weighted least squares (WLS) regression to test if the magnitude of this reaction varies with issue timing. The sample is split into two scenarios, with this question focusing on the primary scenario:\n- **Case 1:** An earnings release is followed by an issue announcement, which is followed by the issue (no intervening release).\n\n**Variables & Parameters.**\n- `Announcement Return`: Two-day abnormal return around the SEO announcement, calculated as the firm's return minus the return on an equal-weighted market portfolio.\n- `Days_Announce`: The number of calendar days between the issue announcement and the previous earnings release.\n\n### Data / Model Specification\n\n**Table 1. Average Abnormal Returns Around SEO Announcement Day (%)**\n\n| Day relative to event | Announcement Abnormal Return | Announcement t-statistic |\n|:---------------------|:-----------------------------|:-------------------------|\n| -2                   | -0.19                        | -1.84                    |\n| -1                   | -2.26                        | -20.64                   |\n| 0                    | -0.43                        | -3.74                    |\n| 1                    | 0.11                         | 1.00                     |\n\n*Source: Adapted from the original paper's Table 2. Day 0 is the date of the announcement in the Wall Street Journal.*\n\nThe following regression model is estimated for the **Case 1** subsample:\n  \n\\text{Announcement Return}_{i} = \\beta_0 + \\beta_1 \\times \\text{Days\\_Announce}_{i} + \\epsilon_i \\quad \\text{(Eq. (1))}\n \n\n**Table 2. WLS Regression of Announcement Return on Timing (Case 1 only)**\n\n| Dependent Variable | Explanatory Variable | Constant `(x10^-2)` | Timing Coeff. `(x10^-3)` | N |\n|:-------------------|:---------------------|:--------------------|:-------------------------|:--|\n| Announcement Return| `Days_Announce`      | -2.251** (0.306)    | -0.147* (0.085)          | 435|\n\n*White heteroskedasticity-robust standard errors in parentheses. *Significant at 10% level. **Significant at 1% level.*\n\n### Question\n\nBased on the provided data and the theory of time-varying adverse selection, select all of the following statements that are supported by the evidence.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The coefficient on Days_Announce is -0.147 × 10^-3 = -0.000147, which is statistically significant at the 10% level. For a 30-day delay, this would mean an additional price drop of 30 × 0.000147 = 0.00441 or 0.44%. This is not economically negligible, so A is incorrect.\n\nB) Errors-in-variables bias typically causes attenuation bias, meaning the estimated coefficient is biased toward zero. Since β̂₁ is negative, the true coefficient would likely be even more negative (further from zero) if measurement error exists. Therefore, the reported coefficient would understate, not overstate, the true negative relationship. B is incorrect.\n\nC) The coefficient is -0.147 × 10^-3 = -0.000147. For a 30-day delay: 30 × (-0.000147) = -0.00441 or -0.44%. This represents an additional price drop of approximately 0.44%. C is correct.\n\nD) From Table 1, the two-day cumulative abnormal return is:\nDay -1: -2.26%\nDay 0: -0.43%\nTotal: -2.26% + (-0.43%) = -2.69%\n\nThis negative reaction is consistent with adverse selection theory and the 'lemons market' interpretation, where SEO announcements signal negative information about the firm. D is correct.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 306, "Question": "### Background\n\n**Research Question.** To further distinguish between the 'monitoring' and 'selection' hypotheses, the analysis turns to the dynamics of diversification decisions. Do institutions proactively influence firms' decisions to diversify ('ex-ante' monitoring), or do they reactively trade shares after a decision has been made ('ex-post' selection)?\n\n**Setting.** The analysis uses two predictive regressions. The first models the firm's decision to change its diversification level. The second models the subsequent change in institutional ownership.\n\n**Variables & Parameters.**\n*   `Change in Geo Div`: Change in geographic diversification.\n*   `Change in Instl Ownership`: Change in institutional ownership.\n*   `Ln(Relative q Ratio)_{t-1}`: A lagged measure of the firm's market valuation relative to its peers. Higher values indicate higher perceived quality.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Predicting Changes in Diversification (MSA-based)**\n| Dependent var: Change in Geo Div | (1) |\n| :--- | :---: |\n| Ln(Relative q Ratio)<sub>t-1</sub> | **0.065** |\n| | (2.36)** |\n| Ln(Relative q Ratio)<sub>t-1</sub> * | **-0.049** |\n| Change in Instl Ownership<sub>t-1</sub> | (1.94)* |\n\n**Table 2. Predicting Changes in Institutional Ownership (Region-based)**\n| Dependent var: Change in Instl Ownership | (2) |\n| :--- | :---: |\n| Change in Geo Div<sub>t-1</sub> | **-0.008** |\n| | (0.29) |\n\n*Note: Tables show selected coefficients from regressions with controls. t-statistics are in parentheses.* \n\n---\n\n### Question\n\nThe provided tables analyze the dynamics of diversification decisions and investor responses. Which of the following statements accurately describe the chronological evidence for proactive monitoring? Select all that apply.", "Options": {"A": "There is no statistically significant relationship between a firm's past changes in diversification and subsequent changes in institutional ownership, which is inconsistent with a reactive 'voting with their feet' selection model.", "B": "Firms with higher relative valuations (`Ln(Relative q Ratio)`) are significantly less likely to diversify, suggesting they focus on their core strengths.", "C": "The interaction effect in Table 1 shows that institutional investors encourage high-quality firms to diversify more, likely to leverage their superior management skills.", "D": "For firms with high relative valuations, an increase in institutional ownership is associated with a lower likelihood of subsequent geographic diversification, suggesting an ex-ante moderating influence."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to construct a chronological narrative of cause and effect from two dynamic regressions. It uses an Atomic Decomposition strategy to separate the 'ex-ante' influence on firm decisions from the 'ex-post' investor reaction. Distractor (A) is a 'Conceptual Opposite' of the main effect in Table 1, while distractor (C) represents a 'Sign Error' in interpreting the crucial interaction term.", "qid": "306", "question": "### Background\n\n**Research Question.** To further distinguish between the 'monitoring' and 'selection' hypotheses, the analysis turns to the dynamics of diversification decisions. Do institutions proactively influence firms' decisions to diversify ('ex-ante' monitoring), or do they reactively trade shares after a decision has been made ('ex-post' selection)?\n\n**Setting.** The analysis uses two predictive regressions. The first models the firm's decision to change its diversification level. The second models the subsequent change in institutional ownership.\n\n**Variables & Parameters.**\n*   `Change in Geo Div`: Change in geographic diversification.\n*   `Change in Instl Ownership`: Change in institutional ownership.\n*   `Ln(Relative q Ratio)_{t-1}`: A lagged measure of the firm's market valuation relative to its peers. Higher values indicate higher perceived quality.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Predicting Changes in Diversification (MSA-based)**\n| Dependent var: Change in Geo Div | (1) |\n| :--- | :---: |\n| Ln(Relative q Ratio)<sub>t-1</sub> | **0.065** |\n| | (2.36)** |\n| Ln(Relative q Ratio)<sub>t-1</sub> * | **-0.049** |\n| Change in Instl Ownership<sub>t-1</sub> | (1.94)* |\n\n**Table 2. Predicting Changes in Institutional Ownership (Region-based)**\n| Dependent var: Change in Instl Ownership | (2) |\n| :--- | :---: |\n| Change in Geo Div<sub>t-1</sub> | **-0.008** |\n| | (0.29) |\n\n*Note: Tables show selected coefficients from regressions with controls. t-statistics are in parentheses.* \n\n---\n\n### Question\n\nThe provided tables analyze the dynamics of diversification decisions and investor responses. Which of the following statements accurately describe the chronological evidence for proactive monitoring? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the tables and analyzing each statement:\n\n**Statement A**: Table 2 shows that the coefficient for Change in Geo Div (t-1) on Change in Instl Ownership is -0.008 with a t-statistic of 0.29, which is not statistically significant. This means past diversification changes don't significantly predict future institutional ownership changes, which is indeed inconsistent with a reactive selection model where institutions would \"vote with their feet\" after diversification decisions.\n\n**Statement B**: Table 1 shows Ln(Relative q Ratio)_{t-1} has a coefficient of 0.065 (t-stat 2.36**), which is positive and significant. This means firms with higher relative valuations are MORE likely to diversify, not less likely. This statement is incorrect.\n\n**Statement C**: The interaction term in Table 1 is -0.049 (t-stat 1.94*), which is negative. This means that when institutional ownership increases, it reduces the positive effect of high valuation on diversification. This is the opposite of encouraging diversification - it's actually discouraging it. This statement is incorrect.\n\n**Statement D**: The interaction term coefficient of -0.049 shows that for high-valuation firms, increases in institutional ownership are associated with reduced subsequent diversification. This suggests institutions are proactively moderating diversification tendencies in high-quality firms, which is consistent with ex-ante monitoring. This statement is correct.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 425, "Question": "### Background\n\n**Research Question.** This case examines the structure of the intertemporal stock-to-bond volatility (ISBV) relation across different bond maturities and potential econometric challenges to its identification.\n\n### Data / Model Specification\n\nThe primary predictive regression model is:\n\n  \n\\sigma_{t,t+21}^{TmSt} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TmSt} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Predictive Regression Results for Bond Futures Volatility (Full Period 1997:10-2013:06)**\n\n| Panel | Dependent Var. | `\\hat{\\gamma}_{2}` (t-stat) |\n| :--- | :--- | :--- |\n| A | `\\sigma^{TB}` (30-yr) | 0.189 (5.92) |\n| B | `\\sigma^{TN}` (10-yr) | 0.154 (4.05) |\n\n*Note: All results are for the full specification in Eq. (1).*\n\n### Question\n\nWhich of the following conclusions about the structure of volatility spillovers and potential econometric issues are supported by the evidence and model specification? Select all that apply.", "Options": {"A": "A key threat to identifying the unique effect of `\\gamma_{2}` would be strong multicollinearity between stock volatility (`\\sigma^{ST}`) and the yield curve state variables (`PrComp`), as they may be driven by a common risk factor.", "B": "The use of heteroscedasticity- and autocorrelation-consistent (HAC) standard errors in the estimation of Eq. (1) eliminates any potential problems caused by multicollinearity among the regressors.", "C": "The results in Table 1 imply that volatility spillovers from the stock market are uniform across the entire yield curve.", "D": "The larger `\\hat{\\gamma}_{2}` coefficient for 30-year bonds compared to 10-year bonds suggests that the long end of the yield curve is more sensitive to volatility spillovers from the equity market."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the more advanced interpretations from the 'Apex' part of the original QA. It uses an Atomic Decomposition strategy to test two distinct concepts: the term structure of spillovers (Q3a from source) and a key econometric challenge (Q3b from source). Correct option A correctly interprets the difference in coefficients across maturities. Correct option B identifies multicollinearity as a valid threat to identification. Distractor C is a direct contradiction of the evidence in Table 1. Distractor D confuses the purpose of HAC standard errors (for serial correlation/heteroscedasticity) with the problem of multicollinearity.", "qid": "425", "question": "### Background\n\n**Research Question.** This case examines the structure of the intertemporal stock-to-bond volatility (ISBV) relation across different bond maturities and potential econometric challenges to its identification.\n\n### Data / Model Specification\n\nThe primary predictive regression model is:\n\n  \n\\sigma_{t,t+21}^{TmSt} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TmSt} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\sum_{j=1}^{3}\\lambda_{j}PrComp_{j,t-1} + \\varepsilon_{t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1. Predictive Regression Results for Bond Futures Volatility (Full Period 1997:10-2013:06)**\n\n| Panel | Dependent Var. | `\\hat{\\gamma}_{2}` (t-stat) |\n| :--- | :--- | :--- |\n| A | `\\sigma^{TB}` (30-yr) | 0.189 (5.92) |\n| B | `\\sigma^{TN}` (10-yr) | 0.154 (4.05) |\n\n*Note: All results are for the full specification in Eq. (1).*\n\n### Question\n\nWhich of the following conclusions about the structure of volatility spillovers and potential econometric issues are supported by the evidence and model specification? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each option based on the model specification and results provided.\n\n**Option A**: This correctly identifies a key econometric concern. If stock volatility and yield curve variables (PrComp) are highly correlated due to common risk factors, multicollinearity would make it difficult to isolate the unique effect of γ₂. This is a valid threat to identification.\n\n**Option B**: This is incorrect. HAC standard errors address heteroscedasticity and autocorrelation in the error terms, but they do not solve multicollinearity problems among regressors. Multicollinearity affects coefficient estimation itself, not just standard errors.\n\n**Option C**: This is not supported by the evidence. The results show different γ₂ coefficients for different maturities (0.189 for 30-year vs 0.154 for 10-year), indicating that spillovers are NOT uniform across the yield curve.\n\n**Option D**: This is supported by the evidence. The 30-year bond has γ₂ = 0.189 while the 10-year bond has γ₂ = 0.154. The larger coefficient for the longer maturity bond suggests greater sensitivity to equity market volatility spillovers at the long end of the yield curve.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 409, "Question": "### Background\n\n**Research Question.** This case examines the methodology for estimating the causal effect of a mandatory increase in corporate board independence on a firm's Corporate Social Responsibility (CSR) engagement, focusing on the difference-in-differences (DiD) research design.\n\n**Setting.** The analysis centers on the Sarbanes-Oxley (SOX) Act of 2002, which mandated that listed firms have a majority of independent directors. This regulatory shock created a quasi-natural experiment, dividing firms into a \"treatment\" group (those forced to change) and a \"control\" group (those already compliant).\n\n**Variables & Parameters.**\n\n*   `CSR_{it}`: The CSR score for firm `i` in year `t`.\n*   `post-SOX_t`: A binary variable equal to 1 for years after 2002, and 0 otherwise.\n*   `noncompliant_i`: A time-invariant binary variable equal to 1 if firm `i` did not have a majority of independent directors in 2002 (the \"treatment group\"), and 0 otherwise (the \"control group\").\n\n---\n\n### Data / Model Specification\n\nThe following difference-in-differences (DiD) model is estimated with firm fixed effects (`α_i`):\n\n  \nCSR_{it} = \\alpha_i + \\beta_1 \\text{post-SOX}_t + \\beta_3 (\\text{post-SOX}_t \\times \\text{noncompliant}_i) + \\gamma'\\text{controls}_{it} + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Summary Statistics for Compliant and Noncompliant Firms (Pre-SOX)**\n\n| Variable | Compliant | Noncompliant | Difference | t-statistic |\n| :--- | :--- | :--- | :--- | :--- |\n| CSR score | 0.397 | 0.137 | 0.260 | 2.142 |\n| R&D expense/total assets | 0.028 | 0.021 | 0.007 | 3.078 |\n\n**Table 2: Main DiD Regression Results (Firm Fixed Effects)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `Noncompliant × post-SOX` (β₃) | -0.591 | (-4.90) |\n| `post-SOX` (β₁) | 0.414 | (6.74) |\n\n---\n\n### Question\n\nThe tables below present key data and the main difference-in-differences (DiD) results. Based on the model specification and the data, which of the following statements are valid interpretations or conclusions? Select all that apply.", "Options": {"A": "The total net change in CSR for a typical noncompliant firm following the SOX mandate is an estimated decrease of 0.177 points.", "B": "The coefficient on `post-SOX` (β₁ = 0.414) shows that compliant firms, on average, increased their CSR score by 0.414 points after SOX, representing the secular trend in the absence of the treatment.", "C": "The significant pre-SOX differences between compliant and noncompliant firms shown in Table 1 strongly validate the parallel trends assumption required for the DiD model.", "D": "The coefficient on the interaction term (β₃ = -0.591) represents the estimated causal effect of the SOX mandate, indicating that noncompliant firms reduced their CSR score by 0.591 points relative to the change observed in compliant firms."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses a deep understanding of the difference-in-differences methodology, from interpreting coefficients to performing calculations. It uses a 'Computational Judgment' strategy for one option and 'Atomic Decomposition' for others. The distractor (D) targets a fundamental and common misconception about the parallel trends assumption, making it highly diagnostic.", "qid": "409", "question": "### Background\n\n**Research Question.** This case examines the methodology for estimating the causal effect of a mandatory increase in corporate board independence on a firm's Corporate Social Responsibility (CSR) engagement, focusing on the difference-in-differences (DiD) research design.\n\n**Setting.** The analysis centers on the Sarbanes-Oxley (SOX) Act of 2002, which mandated that listed firms have a majority of independent directors. This regulatory shock created a quasi-natural experiment, dividing firms into a \"treatment\" group (those forced to change) and a \"control\" group (those already compliant).\n\n**Variables & Parameters.**\n\n*   `CSR_{it}`: The CSR score for firm `i` in year `t`.\n*   `post-SOX_t`: A binary variable equal to 1 for years after 2002, and 0 otherwise.\n*   `noncompliant_i`: A time-invariant binary variable equal to 1 if firm `i` did not have a majority of independent directors in 2002 (the \"treatment group\"), and 0 otherwise (the \"control group\").\n\n---\n\n### Data / Model Specification\n\nThe following difference-in-differences (DiD) model is estimated with firm fixed effects (`α_i`):\n\n  \nCSR_{it} = \\alpha_i + \\beta_1 \\text{post-SOX}_t + \\beta_3 (\\text{post-SOX}_t \\times \\text{noncompliant}_i) + \\gamma'\\text{controls}_{it} + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Summary Statistics for Compliant and Noncompliant Firms (Pre-SOX)**\n\n| Variable | Compliant | Noncompliant | Difference | t-statistic |\n| :--- | :--- | :--- | :--- | :--- |\n| CSR score | 0.397 | 0.137 | 0.260 | 2.142 |\n| R&D expense/total assets | 0.028 | 0.021 | 0.007 | 3.078 |\n\n**Table 2: Main DiD Regression Results (Firm Fixed Effects)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `Noncompliant × post-SOX` (β₃) | -0.591 | (-4.90) |\n| `post-SOX` (β₁) | 0.414 | (6.74) |\n\n---\n\n### Question\n\nThe tables below present key data and the main difference-in-differences (DiD) results. Based on the model specification and the data, which of the following statements are valid interpretations or conclusions? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this difference-in-differences analysis, I need to evaluate each statement based on the model specification and results.\n\n**Statement A**: \"The total net change in CSR for a typical noncompliant firm following the SOX mandate is an estimated decrease of 0.177 points.\"\n\nFor noncompliant firms after SOX:\n- They experience the general post-SOX trend: β₁ = 0.414\n- Plus the differential treatment effect: β₃ = -0.591\n- Total effect = 0.414 + (-0.591) = -0.177\n\nThis is correct.\n\n**Statement B**: \"The coefficient on `post-SOX` (β₁ = 0.414) shows that compliant firms, on average, increased their CSR score by 0.414 points after SOX, representing the secular trend in the absence of the treatment.\"\n\nIn a DiD model with firm fixed effects, β₁ captures the time trend for the control group (compliant firms). This represents the counterfactual trend - what would have happened in the absence of treatment. This interpretation is correct.\n\n**Statement C**: \"The significant pre-SOX differences between compliant and noncompliant firms shown in Table 1 strongly validate the parallel trends assumption required for the DiD model.\"\n\nThis is incorrect. The parallel trends assumption requires that treatment and control groups would have followed parallel paths in the absence of treatment. Pre-treatment level differences don't validate this - we would need to see similar pre-treatment trends (changes over time), not just compare levels at one point. The firm fixed effects control for these level differences, but Table 1 provides no information about whether trends were parallel.\n\n**Statement D**: \"The coefficient on the interaction term (β₃ = -0.591) represents the estimated causal effect of the SOX mandate, indicating that noncompliant firms reduced their CSR score by 0.591 points relative to the change observed in compliant firms.\"\n\nThis is the correct interpretation of the DiD interaction coefficient. It represents the differential effect of the treatment on the treated group relative to the control group's change.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 428, "Question": "### Background\n\nThe study uses time-series regressions on U.S. annual data (1946-64) to analyze the link between corporate plant and equipment (P&E) expenditures, external financing, and interest rates. The author highlights a puzzle where interest rates are positively correlated with investment, contrary to traditional theory.\n\n### Data / Model Specification\n\nThe author argues that a positive correlation between interest rates and expenditures may reflect shifts in credit demand rather than movements along a stable demand curve: \"higher interest rates may...denote increased finance and increased expenditures if the demand for credit shifts to the right.\"\n\n**Table 1: Selected Regressions for P&E Expenditures (based on Table 6 of the paper)**\n\n| Eq. | Constant | `SAVE` | `BONDS` | `STOCK` | `RATE_govt` | `RATE_corp` | R² |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| (1) | 1.1430 | 0.4946** | 0.9561* | | | | 0.945 |\n| | (1.64) | (5.56) | (2.18) | | | | |\n| (2) | 0.4524 | 0.5148** | | 1.5292* | | | 0.941 |\n| | (0.54) | (6.88) | | (2.20) | | | |\n| (3) | -2.8148** | 0.8555** | | | 3.7115* | | 0.941 |\n| | (-2.23) | (17.05) | | | (2.03) | | |\n| (4) | -0.6286 | 0.7522** | | | | 3.0392** | 0.942 |\n*Notes: t-statistics in parentheses. ** significant at 1%, * significant at 5%. `SAVE` is gross saving, `BONDS` is corporate bonds issued, `STOCK` is corporate stock issued, `RATE_govt` is long-term government bond yield, and `RATE_corp` is long-term corporate bond yield.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following statements are valid interpretations supporting the paper's central thesis?\n\nSelect all that apply.", "Options": {"A": "The statistically significant positive coefficients on `BONDS` and `STOCK` in Eq. (1) and Eq. (2) demonstrate that external financial flows are a key determinant of P&E expenditures.", "B": "The significant positive coefficients on `RATE_govt` and `RATE_corp` in Eq. (3) and Eq. (4) suggest that periods of high investment demand are shifting the credit demand curve rightward, increasing both equilibrium investment and interest rates.", "C": "The consistently high significance of the `SAVE` variable across all equations indicates that internal funds are the only truly important driver of P&E spending, making external finance secondary.", "D": "The results in Eq. (3) and Eq. (4) imply that the true relationship between interest rates and investment is positive, contradicting the traditional theory of interest rates as a cost of capital."}, "Answer": ["A", "B"], "pi_justification": "Assessment Target: This item assesses the user's ability to interpret regression results in the context of the paper's core argument. It tests understanding of both the 'financial flow' linkage and the 'interest rate puzzle'.\nDepth Strategy: Reverse-Reasoning. Given the empirical results (the regression table), the user must identify the theoretical conclusions that are plausibly supported by them.\nDistractor Logic:\n- A (Correct): Directly tests the interpretation of the financial flow variables as supporting the paper's thesis.\n- B (Correct): Correctly interprets the positive interest rate coefficient as evidence of demand shifts, which is the author's central explanation for the puzzle.\n- C (Conceptual Distractor - Almost Right): This is a subtle misinterpretation. The author does not claim the *true* structural relationship is positive, but that the *observed* relationship is positive due to an identification problem (simultaneous shifts in supply and demand). The underlying demand curve is still presumed to be downward sloping.\n- D (Conceptual Distractor - Conceptual Opposite): While `SAVE` is significant, the significance of `BONDS` and `STOCK` directly contradicts the idea that external finance is secondary. This distractor forces the user to evaluate the importance of multiple variables, not just the most significant one.", "qid": "428", "question": "### Background\n\nThe study uses time-series regressions on U.S. annual data (1946-64) to analyze the link between corporate plant and equipment (P&E) expenditures, external financing, and interest rates. The author highlights a puzzle where interest rates are positively correlated with investment, contrary to traditional theory.\n\n### Data / Model Specification\n\nThe author argues that a positive correlation between interest rates and expenditures may reflect shifts in credit demand rather than movements along a stable demand curve: \"higher interest rates may...denote increased finance and increased expenditures if the demand for credit shifts to the right.\"\n\n**Table 1: Selected Regressions for P&E Expenditures (based on Table 6 of the paper)**\n\n| Eq. | Constant | `SAVE` | `BONDS` | `STOCK` | `RATE_govt` | `RATE_corp` | R² |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| (1) | 1.1430 | 0.4946** | 0.9561* | | | | 0.945 |\n| | (1.64) | (5.56) | (2.18) | | | | |\n| (2) | 0.4524 | 0.5148** | | 1.5292* | | | 0.941 |\n| | (0.54) | (6.88) | | (2.20) | | | |\n| (3) | -2.8148** | 0.8555** | | | 3.7115* | | 0.941 |\n| | (-2.23) | (17.05) | | | (2.03) | | |\n| (4) | -0.6286 | 0.7522** | | | | 3.0392** | 0.942 |\n*Notes: t-statistics in parentheses. ** significant at 1%, * significant at 5%. `SAVE` is gross saving, `BONDS` is corporate bonds issued, `STOCK` is corporate stock issued, `RATE_govt` is long-term government bond yield, and `RATE_corp` is long-term corporate bond yield.*\n\n---\n\nBased on the regression results in **Table 1**, which of the following statements are valid interpretations supporting the paper's central thesis?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results and the paper's thesis about the positive correlation between interest rates and investment reflecting shifts in credit demand rather than movements along a stable demand curve.\n\nLet me evaluate each option:\n\n**Option A**: The positive and statistically significant coefficients on BONDS (0.9561*) and STOCK (1.5292*) in equations (1) and (2) do demonstrate that external financial flows are positively associated with P&E expenditures. This supports the paper's view that external financing is an important determinant of investment.\n\n**Option B**: The positive and statistically significant coefficients on RATE_govt (3.7115*) and RATE_corp (3.0392**) in equations (3) and (4) align with the paper's central argument. The author explicitly states that \"higher interest rates may...denote increased finance and increased expenditures if the demand for credit shifts to the right.\" This is exactly what these results suggest - periods of high investment demand shift the credit demand curve rightward, leading to both higher interest rates and higher investment.\n\n**Option C**: While SAVE is highly significant across all equations, this doesn't support the claim that internal funds are the \"only truly important driver.\" The significance of external financing variables (BONDS and STOCK) contradicts this interpretation.\n\n**Option D**: This misinterprets the results. The paper isn't claiming that the \"true relationship\" between interest rates and investment is positive in the traditional sense. Rather, it's explaining why we observe a positive correlation - due to shifts in the demand curve, not movements along it. The traditional theory (negative relationship along a stable demand curve) isn't being contradicted; it's being contextualized.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 330, "Question": "### Background\n\n**Research Question.** How do changes in a firm's growth prospects and capital intensity differentially affect its cash flow-based versus its earnings-based valuation multiples?\n\n**Setting.** A simulation using the Staples, Inc. financial model where different value drivers are changed one at a time, each calibrated to produce a 10% increase in the firm's enterprise value (EV). The model assumes that investments in working capital and property, plant, and equipment (PPE) scale with revenue.\n\n**Variables and Parameters.**\n- `Enterprise Value (EV)`: The market value of the firm's operating assets.\n- `Unlevered Free Cash Flow (UFCF)`: Cash flow available to all capital providers after all operating expenses and investments.\n- `EBIT`: Earnings Before Interest and Taxes.\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization.\n\n---\n\n### Data / Model Specification\n\nThe percentage change in a multiple is calculated as:\n\n  \n(1 + \\% \\Delta \\text{Multiple}) = \\frac{(1 + \\% \\Delta \\text{Numerator})}{(1 + \\% \\Delta \\text{Denominator})} \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Simulation of Changes in Growth and Capital Expenditure**\n| Scenario | Multiple Type | % Change in Numerator (EV) | % Change in Denominator | % Change in Multiple |\n| :--- | :--- | :--- | :--- | :--- |\n| **Increase Initial Revenue Growth** | Unlevered Free Cash Flow | +10.0% | -4.3% | +14.9% |\n| | EBIT | +10.0% | +2.3% | +7.5% |\n| **Reduce Net PPE Growth (CapEx)** | Unlevered Free Cash Flow | +10.0% | +10.4% | -0.3% |\n| | EBITDA | +10.0% | 0.0% | +10.0% |\n\n---\n\n### Question\n\nBased on the simulation results in **Table 1**, which of the following conclusions about the situational reliability of different valuation multiples are supported?", "Options": {"A": "For comparing two firms with similar capital intensity but different near-term growth rates, an earnings-based multiple like EV/EBIT is likely more stable and reliable than the EV/UFCF multiple.", "B": "The EV/Unlevered Free Cash Flow multiple is a reliable metric for comparing firms that differ primarily in their capital intensity, as changes in CapEx affect both the numerator (EV) and the denominator (UFCF) by similar percentages.", "C": "The EV/EBITDA multiple is the most robust metric for comparing firms with different capital expenditure requirements because its denominator is unaffected by depreciation policies.", "D": "The EV/Unlevered Free Cash Flow multiple is highly sensitive to changes in revenue growth, as the necessary investments in working capital and PPE can cause the UFCF denominator to decrease even as firm value increases."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Premise/Assumption Packaging strategy to test the paper's nuanced conclusion that no single multiple is always superior. It requires synthesizing contrasting simulation results to understand the trade-offs. Options A, B, and C are atomic decompositions of the core argument. Distractor (D) targets a common practitioner misconception (confusing an unaffected denominator with a robust multiple), making it a high-fidelity distractor.", "qid": "330", "question": "### Background\n\n**Research Question.** How do changes in a firm's growth prospects and capital intensity differentially affect its cash flow-based versus its earnings-based valuation multiples?\n\n**Setting.** A simulation using the Staples, Inc. financial model where different value drivers are changed one at a time, each calibrated to produce a 10% increase in the firm's enterprise value (EV). The model assumes that investments in working capital and property, plant, and equipment (PPE) scale with revenue.\n\n**Variables and Parameters.**\n- `Enterprise Value (EV)`: The market value of the firm's operating assets.\n- `Unlevered Free Cash Flow (UFCF)`: Cash flow available to all capital providers after all operating expenses and investments.\n- `EBIT`: Earnings Before Interest and Taxes.\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization.\n\n---\n\n### Data / Model Specification\n\nThe percentage change in a multiple is calculated as:\n\n  \n(1 + \\% \\Delta \\text{Multiple}) = \\frac{(1 + \\% \\Delta \\text{Numerator})}{(1 + \\% \\Delta \\text{Denominator})} \\quad \\text{(Eq. 1)}\n \n\n**Table 1: Simulation of Changes in Growth and Capital Expenditure**\n| Scenario | Multiple Type | % Change in Numerator (EV) | % Change in Denominator | % Change in Multiple |\n| :--- | :--- | :--- | :--- | :--- |\n| **Increase Initial Revenue Growth** | Unlevered Free Cash Flow | +10.0% | -4.3% | +14.9% |\n| | EBIT | +10.0% | +2.3% | +7.5% |\n| **Reduce Net PPE Growth (CapEx)** | Unlevered Free Cash Flow | +10.0% | +10.4% | -0.3% |\n| | EBITDA | +10.0% | 0.0% | +10.0% |\n\n---\n\n### Question\n\nBased on the simulation results in **Table 1**, which of the following conclusions about the situational reliability of different valuation multiples are supported?", "model_evaluation_result": {"model_raw_response": "Looking at the simulation results in Table 1, I need to evaluate each conclusion against the data.\n\n**Option A:** When initial revenue growth increases:\n- EV/UFCF multiple changes by +14.9% (highly sensitive)\n- EV/EBIT multiple changes by +7.5% (less sensitive)\n\nThis supports the claim that EV/EBIT is more stable than EV/UFCF when comparing firms with different growth rates. ✓\n\n**Option B:** When Net PPE Growth (CapEx) is reduced:\n- EV increases by +10.0%\n- UFCF increases by +10.4%\n- Resulting multiple change is only -0.3%\n\nThe similar percentage changes (10.0% vs 10.4%) result in a nearly unchanged multiple, supporting the claim that EV/UFCF is reliable for comparing firms with different capital intensity. ✓\n\n**Option C:** When Net PPE Growth (CapEx) is reduced:\n- EV/EBITDA multiple changes by +10.0%\n- The denominator (EBITDA) shows 0% change\n\nWhile EBITDA is unaffected by depreciation, the 10% change in the multiple indicates it's not particularly \"robust\" - it changes proportionally with EV. The claim overstates the robustness. ✗\n\n**Option D:** When initial revenue growth increases:\n- EV increases by +10.0%\n- UFCF decreases by -4.3%\n- Multiple increases by +14.9%\n\nThis confirms that higher growth requires more investment in working capital and PPE, reducing UFCF even as firm value increases, making the multiple highly sensitive. ✓\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 397, "Question": "### Background\n\nThe well-posedness of the Thiele PDE model for a surplus-linked insurance reserve depends on a set of technical and economic assumptions about the coefficient functions.\n\n### Data / Model Specification\n\nThe paper presents Hypothesis 1, which includes the following core assumptions:\n\n*   **(i)(a) Uniform Ellipticity:** There exists a `\\pi_0 > 0` such that `\\pi(t,x) \\ge \\pi_0`, where `\\pi(t,x)` is the share of surplus invested in a risky asset. This ensures the coefficient of the `\\partial_x^2` term in the PDE, `(1/2)\\pi(t,x)^2\\sigma^2x^2`, is strictly positive.\n*   **(iii)(a) Linear Dividend Bound:** There exists a `k > 0` such that `0 \\le \\delta^j(t,x) \\le kx`, where `\\delta^j(t,x)` are dividend payments.\n*   **(iii)(c) Surplus Non-negativity:** `x + c^{jk}(t) - \\delta^{jk}(t,x) \\ge 0`, ensuring the surplus remains non-negative after cash flows from state transitions.\n\nThe paper notes that the uniform ellipticity assumption can be relaxed to `\\pi(x) \\ge 0`, which makes the PDE degenerate where `\\pi(x) = 0`.\n\n---\n\nWhich of the following statements represent valid mathematical or economic consequences of these assumptions, or valid critiques of their realism?\n", "Options": {"A": "A Constant Proportion Portfolio Insurance (CPPI) strategy, where risky asset allocation can become zero if the surplus hits a floor, would violate the uniform ellipticity assumption.", "B": "The surplus non-negativity assumption (iii)(c) ensures that the arguments of the reserve function `V` remain within its domain (`\\mathbb{R}_+`) after state transitions.", "C": "A dividend policy that pays a fixed amount (e.g., $1000) as long as the surplus `x` is above a threshold would violate the linear dividend bound assumption (iii)(a).", "D": "The uniform ellipticity assumption (i)(a) ensures the PDE is non-degenerate parabolic, which is a standard condition for the existence of a smooth, classical solution."}, "Answer": ["A", "B", "C", "D"], "pi_justification": "Assessment Target: This item assesses the ability to connect the paper's formal mathematical assumptions to their technical purpose (e.g., ensuring non-degeneracy) and their economic realism. Strategy: Scenario Application. The student must apply the abstract assumptions to concrete financial strategies (CPPI, fixed dividends) to evaluate their validity. All four options are correct statements drawn from the original QA problem, requiring the student to understand the role of each assumption and identify realistic scenarios where they might fail.", "qid": "397", "question": "### Background\n\nThe well-posedness of the Thiele PDE model for a surplus-linked insurance reserve depends on a set of technical and economic assumptions about the coefficient functions.\n\n### Data / Model Specification\n\nThe paper presents Hypothesis 1, which includes the following core assumptions:\n\n*   **(i)(a) Uniform Ellipticity:** There exists a `\\pi_0 > 0` such that `\\pi(t,x) \\ge \\pi_0`, where `\\pi(t,x)` is the share of surplus invested in a risky asset. This ensures the coefficient of the `\\partial_x^2` term in the PDE, `(1/2)\\pi(t,x)^2\\sigma^2x^2`, is strictly positive.\n*   **(iii)(a) Linear Dividend Bound:** There exists a `k > 0` such that `0 \\le \\delta^j(t,x) \\le kx`, where `\\delta^j(t,x)` are dividend payments.\n*   **(iii)(c) Surplus Non-negativity:** `x + c^{jk}(t) - \\delta^{jk}(t,x) \\ge 0`, ensuring the surplus remains non-negative after cash flows from state transitions.\n\nThe paper notes that the uniform ellipticity assumption can be relaxed to `\\pi(x) \\ge 0`, which makes the PDE degenerate where `\\pi(x) = 0`.\n\n---\n\nWhich of the following statements represent valid mathematical or economic consequences of these assumptions, or valid critiques of their realism?\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\n**Statement A**: CPPI strategies allocate to risky assets based on a cushion (surplus above a floor). When the cushion hits zero, the risky allocation becomes zero, making π(x) = 0. This would indeed violate the uniform ellipticity assumption π(t,x) ≥ π₀ > 0. ✓\n\n**Statement B**: The assumption x + c^{jk}(t) - δ^{jk}(t,x) ≥ 0 directly ensures that after transitions involving cash flows c^{jk} and dividends δ^{jk}, the resulting surplus remains non-negative. This keeps the argument of V within ℝ₊ (non-negative reals). ✓\n\n**Statement C**: A fixed dividend payment (e.g., $1000 when x > threshold) would violate the linear bound δ^j(t,x) ≤ kx. For small surplus values just above the threshold, no finite k could satisfy 1000 ≤ kx. ✓\n\n**Statement D**: The uniform ellipticity assumption π(t,x) ≥ π₀ > 0 ensures the coefficient (1/2)π(t,x)²σ²x² of the second-order term is strictly positive, making the PDE uniformly parabolic. This is indeed a standard condition for classical solution existence. ✓\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 408, "Question": "### Background\n\n**Research Question.** This case dissects the economic mechanism behind the finding that increased board independence leads to a reduction in Corporate Social Responsibility (CSR). It investigates the relationship between CSR and firm risk to build a cohesive narrative supporting the agency cost hypothesis.\n\n**Setting.** The 'risk-mitigation/agency cost hypothesis' posits that managers, who are more risk-averse than well-diversified shareholders, may over-invest in CSR activities to reduce firm-specific risk and protect their personal human and financial capital. An independent board, representing shareholders, would act to curtail such value-destroying over-investment.\n\n**Variables & Parameters.**\n\n*   `CSR score`: The composite measure, defined as `CSR strengths - CSR concerns`.\n*   `Total risk`: Standard deviation of daily stock returns.\n*   `Idiosyncratic risk`: Standard deviation of residuals from a market model regression.\n*   `Systematic risk`: The market beta (`β`) from a market model regression.\n\n---\n\n### Data / Model Specification\n\n**Table 1: The Effect of CSR on Firm Risk**\n\nThis table shows results from regressing three different measures of firm risk on the composite CSR score, including firm and year fixed effects.\n\n| Dependent Variable | `CSR` Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| Total risk | -0.004 | (-4.48) |\n| Idiosyncratic risk | -0.003 | (-3.25) |\n| Systematic risk | -0.019 | (-6.13) |\n\n---\n\n### Question\n\nThe 'risk-mitigation/agency cost hypothesis' provides a rationale for why managers might over-invest in CSR. Based on the theory and the empirical evidence in Table 1, which of the following statements are correct? Select all that apply.", "Options": {"A": "According to the agency cost hypothesis, independent boards cut CSR because any form of risk reduction, including systematic risk reduction, is inherently value-destroying for shareholders.", "B": "The agency conflict motivating CSR over-investment stems from managers' aversion to firm-specific (idiosyncratic) risk, which well-diversified shareholders are insulated from.", "C": "The data shows that higher CSR scores are associated with statistically significant reductions in total, idiosyncratic, and systematic firm risk.", "D": "The evidence suggests that CSR activities primarily reduce systematic (market) risk, with no significant effect on idiosyncratic risk."}, "Answer": ["B", "C"], "pi_justification": "This item tests the understanding of the core theoretical mechanism (agency cost of risk aversion) and the ability to link it to supporting empirical evidence. It combines 'Premise Packaging' (testing the theory) and 'Atomic Decomposition' (testing the data). Distractors test for incomplete reading of the table (C) and a misunderstanding of the nuance of shareholder value (D).", "qid": "408", "question": "### Background\n\n**Research Question.** This case dissects the economic mechanism behind the finding that increased board independence leads to a reduction in Corporate Social Responsibility (CSR). It investigates the relationship between CSR and firm risk to build a cohesive narrative supporting the agency cost hypothesis.\n\n**Setting.** The 'risk-mitigation/agency cost hypothesis' posits that managers, who are more risk-averse than well-diversified shareholders, may over-invest in CSR activities to reduce firm-specific risk and protect their personal human and financial capital. An independent board, representing shareholders, would act to curtail such value-destroying over-investment.\n\n**Variables & Parameters.**\n\n*   `CSR score`: The composite measure, defined as `CSR strengths - CSR concerns`.\n*   `Total risk`: Standard deviation of daily stock returns.\n*   `Idiosyncratic risk`: Standard deviation of residuals from a market model regression.\n*   `Systematic risk`: The market beta (`β`) from a market model regression.\n\n---\n\n### Data / Model Specification\n\n**Table 1: The Effect of CSR on Firm Risk**\n\nThis table shows results from regressing three different measures of firm risk on the composite CSR score, including firm and year fixed effects.\n\n| Dependent Variable | `CSR` Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| Total risk | -0.004 | (-4.48) |\n| Idiosyncratic risk | -0.003 | (-3.25) |\n| Systematic risk | -0.019 | (-6.13) |\n\n---\n\n### Question\n\nThe 'risk-mitigation/agency cost hypothesis' provides a rationale for why managers might over-invest in CSR. Based on the theory and the empirical evidence in Table 1, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement against the agency cost hypothesis and the empirical evidence in Table 1.\n\nLet me analyze each option:\n\n**Option A**: \"According to the agency cost hypothesis, independent boards cut CSR because any form of risk reduction, including systematic risk reduction, is inherently value-destroying for shareholders.\"\n\nThis is incorrect. The agency cost hypothesis specifically focuses on managers' aversion to firm-specific (idiosyncratic) risk, not all forms of risk reduction. Systematic risk reduction could actually be valuable to shareholders since they cannot diversify it away.\n\n**Option B**: \"The agency conflict motivating CSR over-investment stems from managers' aversion to firm-specific (idiosyncratic) risk, which well-diversified shareholders are insulated from.\"\n\nThis is correct. The background explicitly states that managers are \"more risk-averse than well-diversified shareholders\" and may over-invest in CSR to \"reduce firm-specific risk.\" Well-diversified shareholders can eliminate idiosyncratic risk through diversification, but managers cannot diversify their human capital tied to the firm.\n\n**Option C**: \"The data shows that higher CSR scores are associated with statistically significant reductions in total, idiosyncratic, and systematic firm risk.\"\n\nThis is correct. Table 1 shows negative coefficients for CSR on all three risk measures:\n- Total risk: -0.004 (t-stat = -4.48)\n- Idiosyncratic risk: -0.003 (t-stat = -3.25)\n- Systematic risk: -0.019 (t-stat = -6.13)\n\nAll t-statistics exceed conventional significance thresholds (|t| > 2.58 for 1% significance level).\n\n**Option D**: \"The evidence suggests that CSR activities primarily reduce systematic (market) risk, with no significant effect on idiosyncratic risk.\"\n\nThis is incorrect. While systematic risk has the largest coefficient (-0.019), the effect on idiosyncratic risk (-0.003, t-stat = -3.25) is also statistically significant.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 373, "Question": "### Background\n\n**Research Question.** To determine if the effect of national industrial policy on corporate financialization is conditional on local institutional factors, specifically regional market development and local government financial capacity.\n\n**Theoretical Framework.** The effectiveness of a top-down national policy may depend on the capacity of local institutions to implement it.\n\n### Data / Model Specification\n\nThe analysis uses a regression model with an interaction term:\n  \nFINRATIO_{i,t+1} = \\beta_{0} + \\beta_{1} IP1_{it} + \\beta_{2} (IP1_{it} \\times Moderator_{it}) + \\dots + e_{i,t+1} \\quad \\text{(Eq. 1)}\n \n-   `FINRATIO`: The ratio of financial assets to total assets.\n-   `IP1`: A dummy variable for industrial policy support.\n-   `MARKET`: A dummy variable = 1 for firms in high-marketization regions.\n-   `DEFICIT`: A measure of local government financial capacity (higher value = greater capacity).\n\n**Table 1. Moderating Effect of Marketization (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | 0.0131 | (1.63) |\n| `IP1 × MARKET` | -0.0035*** | (-3.24) |\n\n**Table 2. Moderating Effect of Local Gov't Financial Capacity (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | -0.0182*** | (-9.82) |\n| `IP1 × DEFICIT` | -0.0076*** | (-4.36) |\n\n*Note: *** denotes p < .01.*\n\n### Question\n\nBased on the regression results for moderating effects, select all of the following statements that are correct.", "Options": {"A": "In high-marketization regions (`MARKET=1`), industrial policy has a statistically significant negative effect on `FINRATIO`.", "B": "The effect of industrial policy on financialization is significantly more pronounced (i.e., more negative) in regions with greater local government financial capacity.", "C": "The results in Table 2 imply that the total effect of industrial policy is `Total Effect = -0.0182 + (-0.0076 × DEFICIT)`.", "D": "For firms in low-marketization regions (`MARKET=0`), the estimated effect of industrial policy on `FINRATIO` is positive but not statistically significant."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Scenario Application strategy to test the interpretation of interaction terms. Students must calculate and interpret conditional effects. Correct options A and B require correct interpretation of an interaction term and a main effect's significance, respectively. Distractors are designed to catch common errors: C involves a sign error in calculating the total effect for a subgroup, and D involves a sign error when constructing the total effect equation from the regression coefficients.", "qid": "373", "question": "### Background\n\n**Research Question.** To determine if the effect of national industrial policy on corporate financialization is conditional on local institutional factors, specifically regional market development and local government financial capacity.\n\n**Theoretical Framework.** The effectiveness of a top-down national policy may depend on the capacity of local institutions to implement it.\n\n### Data / Model Specification\n\nThe analysis uses a regression model with an interaction term:\n  \nFINRATIO_{i,t+1} = \\beta_{0} + \\beta_{1} IP1_{it} + \\beta_{2} (IP1_{it} \\times Moderator_{it}) + \\dots + e_{i,t+1} \\quad \\text{(Eq. 1)}\n \n-   `FINRATIO`: The ratio of financial assets to total assets.\n-   `IP1`: A dummy variable for industrial policy support.\n-   `MARKET`: A dummy variable = 1 for firms in high-marketization regions.\n-   `DEFICIT`: A measure of local government financial capacity (higher value = greater capacity).\n\n**Table 1. Moderating Effect of Marketization (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | 0.0131 | (1.63) |\n| `IP1 × MARKET` | -0.0035*** | (-3.24) |\n\n**Table 2. Moderating Effect of Local Gov't Financial Capacity (Dep. Var: `FINRATIO`)**\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `IP1` | -0.0182*** | (-9.82) |\n| `IP1 × DEFICIT` | -0.0076*** | (-4.36) |\n\n*Note: *** denotes p < .01.*\n\n### Question\n\nBased on the regression results for moderating effects, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results, I need to evaluate each statement:\n\n**Statement A**: In high-marketization regions (MARKET=1), the total effect of industrial policy is:\n- IP1 effect + IP1×MARKET effect = 0.0131 + (-0.0035) = 0.0096\n- This is positive, not negative, so Statement A is incorrect.\n\n**Statement B**: From Table 2, the interaction term IP1×DEFICIT has a coefficient of -0.0076***, which is negative and significant. Since higher DEFICIT means greater financial capacity, this negative interaction means the effect of industrial policy becomes more negative (more pronounced) in regions with greater local government financial capacity. Statement B is correct.\n\n**Statement C**: The total effect formula from Table 2 is:\n- Total Effect = β₁ + β₂×DEFICIT = -0.0182 + (-0.0076×DEFICIT)\n- This matches the statement exactly. Statement C is correct.\n\n**Statement D**: For firms in low-marketization regions (MARKET=0), the effect is just the IP1 coefficient from Table 1:\n- Effect = 0.0131 with t-statistic = 1.63\n- This is positive and not statistically significant (no significance stars shown). Statement D is correct.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 377, "Question": "### Background\n\nTo control for unobserved differences between firms, a study estimates the following panel regression model for corporate investment (`INV`):\n\n  \nINV_{it} = \\beta_0 + \\beta_1 Q_{it-1} + \\beta_2 CF_{it-1} + \\text{FirmFE}_i + \\text{YearFE}_t + \\varepsilon_{it}\n \n\nWhere `FirmFE_i` represents firm fixed effects.\n\n### Question\n\nWhich of the following statements are valid implications of including firm fixed effects (`FirmFE_i`) in this model? (Select all that apply)", "Options": {"A": "The model controls for time-invariant firm characteristics, such as a firm's foundational corporate culture or its long-term strategic industry position.", "B": "The model fails to control for time-varying omitted variables, such as a sudden change in CEO effectiveness, that could affect both investment and cash flow.", "C": "The model effectively eliminates all potential sources of endogeneity bias in the estimation of β₁ and β₂.", "D": "The model's coefficients are identified based on how a firm's investment changes over time as its own Q and cash flow change."}, "Answer": ["A", "B", "D"], "pi_justification": "This item assesses a nuanced understanding of the power and limitations of firm fixed effects, a cornerstone identification strategy. It uses a Conceptual Judgment strategy. Correct options (A), (B), and (D) accurately describe what fixed effects do (identify from within-variation), what they control for (time-invariant heterogeneity), and what they do not control for (time-varying heterogeneity). Distractor (C) represents a classic, high-frequency misconception: the belief that fixed effects are a cure-all for endogeneity. This format forces the user to evaluate each statement's validity independently.", "qid": "377", "question": "### Background\n\nTo control for unobserved differences between firms, a study estimates the following panel regression model for corporate investment (`INV`):\n\n  \nINV_{it} = \\beta_0 + \\beta_1 Q_{it-1} + \\beta_2 CF_{it-1} + \\text{FirmFE}_i + \\text{YearFE}_t + \\varepsilon_{it}\n \n\nWhere `FirmFE_i` represents firm fixed effects.\n\n### Question\n\nWhich of the following statements are valid implications of including firm fixed effects (`FirmFE_i`) in this model? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at this panel regression model with firm fixed effects, I need to evaluate each statement about the implications of including FirmFE_i.\n\nLet me analyze each option:\n\n**Option A**: \"The model controls for time-invariant firm characteristics, such as a firm's foundational corporate culture or its long-term strategic industry position.\"\n- This is a core property of firm fixed effects. They absorb any firm-specific characteristics that don't change over time, including foundational culture, industry position, management philosophy, etc. **TRUE**\n\n**Option B**: \"The model fails to control for time-varying omitted variables, such as a sudden change in CEO effectiveness, that could affect both investment and cash flow.\"\n- Firm fixed effects only control for time-invariant characteristics. They cannot control for factors that vary over time within a firm, like CEO changes, productivity shocks, or changing market conditions. This is a key limitation. **TRUE**\n\n**Option C**: \"The model effectively eliminates all potential sources of endogeneity bias in the estimation of β₁ and β₂.\"\n- This is false. While firm fixed effects address one source of endogeneity (time-invariant omitted variables), many other sources remain: time-varying omitted variables, reverse causality, measurement error, etc. Fixed effects are not a panacea for all endogeneity issues. **FALSE**\n\n**Option D**: \"The model's coefficients are identified based on how a firm's investment changes over time as its own Q and cash flow change.\"\n- This correctly describes the \"within-firm\" variation that fixed effects models exploit. The coefficients are identified from deviations from firm-specific means over time. **TRUE**\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 457, "Question": "### Background\nIn the paper's model, a target firm and an “insider” acquirer form a coalition to maximize their joint surplus in a future takeover contest against N “outsiders.” They achieve this by having the target sell an equity stake of size `α*` to the insider. The optimal stake size, `α*`, is determined by the trade-off between extracting surplus from outsiders and ensuring outsiders are willing to pay an investigation cost, *c*, to participate. A larger stake `α` makes the insider a more aggressive bidder, which helps the coalition extract surplus but reduces the outsiders' expected payoff, potentially violating their participation constraint.\n\n### Data / Model Specification\nThe model yields several key predictions about the optimal stake size and its pricing:\n1.  The optimal stake size, `α*`, is increasing in the probability of a takeover, *s*, and decreasing in the outsiders’ cost of investigation, *c*.\n2.  The optimal stake size, `α*`, decreases with any positive stochastic shift in the insider’s synergy value distribution, F_I(·).\n3.  Assuming disagreement between the target and insider would result in a standard second-price auction with no stake, the equity stake `α*` will always sell for a discount to the target’s post-trade market price.\n\n### Question\nAccording to the model's explanation for the stake price discount (Proposition 3), select ALL statements that are consistent with the paper's reasoning.", "Options": {"A": "A third party would be willing to pay less per share for the target's equity after the insider acquires its stake, justifying the initial discount.", "B": "The insider's overbidding provides a 'public good' to the target's original shareholders by raising the expected sale price of the firm.", "C": "The stake is sold at a discount to compensate the insider for the privately costly act of overbidding in states where it wins the auction.", "D": "The discount primarily reflects the information asymmetry between the target, which knows the takeover probability, and the insider."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the economic rationale behind the stake pricing prediction. It uses an Atomic Decomposition strategy, breaking the explanation into the insider's cost (A) and the shareholders' benefit (C). Distractors include a plausible but incorrect alternative theory from corporate finance (B: information asymmetry) and a direct contradiction of a subtle point in the text (D: third-party valuation), testing for deep comprehension beyond surface-level reading.", "qid": "457", "question": "### Background\nIn the paper's model, a target firm and an “insider” acquirer form a coalition to maximize their joint surplus in a future takeover contest against N “outsiders.” They achieve this by having the target sell an equity stake of size `α*` to the insider. The optimal stake size, `α*`, is determined by the trade-off between extracting surplus from outsiders and ensuring outsiders are willing to pay an investigation cost, *c*, to participate. A larger stake `α` makes the insider a more aggressive bidder, which helps the coalition extract surplus but reduces the outsiders' expected payoff, potentially violating their participation constraint.\n\n### Data / Model Specification\nThe model yields several key predictions about the optimal stake size and its pricing:\n1.  The optimal stake size, `α*`, is increasing in the probability of a takeover, *s*, and decreasing in the outsiders’ cost of investigation, *c*.\n2.  The optimal stake size, `α*`, decreases with any positive stochastic shift in the insider’s synergy value distribution, F_I(·).\n3.  Assuming disagreement between the target and insider would result in a standard second-price auction with no stake, the equity stake `α*` will always sell for a discount to the target’s post-trade market price.\n\n### Question\nAccording to the model's explanation for the stake price discount (Proposition 3), select ALL statements that are consistent with the paper's reasoning.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which statements are consistent with the model's explanation for why the equity stake α* sells at a discount.\n\nFrom the background, the key insight is that the insider and target form a coalition to maximize joint surplus. The insider acquires a stake that makes them bid more aggressively, which helps extract surplus from outsiders in the takeover contest.\n\nLet me analyze each option:\n\nA) \"A third party would be willing to pay less per share for the target's equity after the insider acquires its stake, justifying the initial discount.\"\n- This suggests the discount reflects a decrease in the target's value to outsiders after the stake sale. This is consistent with the model where the insider's stake makes them a tougher competitor, reducing outsiders' expected profits.\n\nB) \"The insider's overbidding provides a 'public good' to the target's original shareholders by raising the expected sale price of the firm.\"\n- This correctly identifies that the insider's aggressive bidding benefits all target shareholders by driving up the takeover price. This is a key mechanism in the model.\n\nC) \"The stake is sold at a discount to compensate the insider for the privately costly act of overbidding in states where it wins the auction.\"\n- This explains the discount as compensation to the insider for bidding above their private value when they win. This aligns with the coalition's surplus-sharing arrangement where the discount compensates the insider for their role in extracting surplus from outsiders.\n\nD) \"The discount primarily reflects the information asymmetry between the target, which knows the takeover probability, and the insider.\"\n- The model assumes the coalition (target and insider) jointly maximizes surplus, suggesting aligned information rather than asymmetry as the primary driver of the discount.\n\nBased on the model's logic where the discount facilitates the coalition's surplus extraction strategy, options A, B, and C are all consistent with the paper's reasoning.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 348, "Question": "### Background\n\n**Research Question.** This case investigates the paper's central empirical claim: that a country's underlying \"financial structure\"—specifically, the sensitivity of money demand to inflation—is a more powerful determinant of its average inflation rate than traditional fiscal variables. The analysis traces the proposed causal chain from estimated money demand parameters to a country's theoretical inflation-tax capacity, and finally to its observed inflation outcomes.\n\n**Variables & Parameters.**\n- `α`: The estimated semi-elasticity of currency demand with respect to inflation. A higher `α` means demand is more sensitive to inflation.\n- `π*`: The estimated revenue-maximizing inflation rate, a measure of a country's inflation-tax capacity.\n- `π`: The observed average inflation rate.\n- `ln(·)`: The natural logarithm transformation of a variable.\n\n---\n\n### Data / Model Specification\n\nThe paper's central hypothesis is a causal chain: low `α` (inelastic demand) ⇒ high `π*` (high tax capacity) ⇒ high observed average inflation `π`. This chain is tested by examining correlations between these variables.\n\n**Table 1. Correlation Coefficients (in Natural Logarithms)**\n\n|                     | `ln(α)` | `ln(π*)` | `ln(π)` |\n| :------------------ | :---------- | :--------- | :------------------- |\n| `ln(α)`             | 1.0         |            |                      |\n| `ln(π*)`            | -0.580      | 1.0        |                      |\n| `ln(π)`             | -0.802      | 0.816      | 1.0                  |\n\n---\n\n### Question\n\nBased on the provided data and the paper's financial structure hypothesis, select all statements that are supported by the evidence in **Table 1**.", "Options": {"A": "The data indicate that a higher sensitivity of currency demand to inflation (higher `α`) is associated with a greater capacity to levy the inflation tax (higher `π*`).", "B": "The financial structure hypothesis is primarily supported by the strong positive correlation between the semi-elasticity of currency demand (`ln(α)`) and the observed inflation rate (`ln(π)`).", "C": "The strong positive correlation (0.816) between `ln(π*)` and `ln(π)` suggests that countries with a higher inflation-tax capacity systematically exhibit higher average inflation rates in practice.", "D": "The strong negative correlation (-0.580) between `ln(α)` and `ln(π*)` indicates that countries where currency demand is less sensitive to inflation (lower `α`) have a greater theoretical capacity to raise revenue through inflation (higher `π*`)."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the user's ability to interpret the paper's core empirical evidence from correlation coefficients. It uses an Atomic Decomposition strategy, breaking the paper's central causal argument into two independently verifiable statements. Distractors are based on common interpretation errors: a Sign Error (C) and a Conceptual Opposite (D), ensuring high discriminability.", "qid": "348", "question": "### Background\n\n**Research Question.** This case investigates the paper's central empirical claim: that a country's underlying \"financial structure\"—specifically, the sensitivity of money demand to inflation—is a more powerful determinant of its average inflation rate than traditional fiscal variables. The analysis traces the proposed causal chain from estimated money demand parameters to a country's theoretical inflation-tax capacity, and finally to its observed inflation outcomes.\n\n**Variables & Parameters.**\n- `α`: The estimated semi-elasticity of currency demand with respect to inflation. A higher `α` means demand is more sensitive to inflation.\n- `π*`: The estimated revenue-maximizing inflation rate, a measure of a country's inflation-tax capacity.\n- `π`: The observed average inflation rate.\n- `ln(·)`: The natural logarithm transformation of a variable.\n\n---\n\n### Data / Model Specification\n\nThe paper's central hypothesis is a causal chain: low `α` (inelastic demand) ⇒ high `π*` (high tax capacity) ⇒ high observed average inflation `π`. This chain is tested by examining correlations between these variables.\n\n**Table 1. Correlation Coefficients (in Natural Logarithms)**\n\n|                     | `ln(α)` | `ln(π*)` | `ln(π)` |\n| :------------------ | :---------- | :--------- | :------------------- |\n| `ln(α)`             | 1.0         |            |                      |\n| `ln(π*)`            | -0.580      | 1.0        |                      |\n| `ln(π)`             | -0.802      | 0.816      | 1.0                  |\n\n---\n\n### Question\n\nBased on the provided data and the paper's financial structure hypothesis, select all statements that are supported by the evidence in **Table 1**.", "model_evaluation_result": {"model_raw_response": "Looking at the correlations in Table 1 and the paper's hypothesis:\n\nThe hypothesis states: low α (inelastic demand) ⇒ high π* (high tax capacity) ⇒ high observed average inflation π\n\nLet me evaluate each option:\n\nA) States higher α is associated with higher π*. But the correlation between ln(α) and ln(π*) is -0.580 (negative), meaning higher α is associated with LOWER π*, not higher. This contradicts the statement. **FALSE**\n\nB) States the hypothesis is supported by positive correlation between ln(α) and ln(π). But the correlation is -0.802 (strongly negative), not positive. **FALSE**\n\nC) The correlation between ln(π*) and ln(π) is 0.816 (strongly positive), which does indicate that countries with higher inflation-tax capacity exhibit higher average inflation rates. This supports the hypothesis. **TRUE**\n\nD) The correlation between ln(α) and ln(π*) is -0.580 (negative), which means when α is lower, π* tends to be higher. This is exactly what the statement says and aligns with the hypothesis that less sensitive currency demand (lower α) leads to greater inflation-tax capacity (higher π*). **TRUE**\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 343, "Question": "### Background\n\n**Research Question.** This case investigates the portfolio implications of stock return predictability from moving averages and the likely impact of arbitrage activity on such signals.\n\n**Setting and Sample.** A mean-variance investor uses a predictive regression to form expectations and allocate capital between the S&P 500 and a risk-free asset.\n\n### Data / Model Specification\n\nThe investor uses the `MADP(3)` predictive regression to form expectations about the next period's excess return, `E_t[r_{t+1}]`:\n  \nE_t[r_{t+1}] = \\hat{\\alpha} + \\hat{\\beta} \\cdot MADP_t(3) \\quad \\text{(Eq. 1)}\n \nThe optimal weight, `w_t^*`, allocated to the S&P 500 for an investor with risk aversion `γ` and a constant conditional return variance `σ^2` is given by:\n  \nw_t^* = \\frac{E_t[r_{t+1}]}{\\gamma \\sigma^2} \\quad \\text{(Eq. 2)}\n \nEmpirical results show that the `MADP(3)` predictor had significant out-of-sample predictive power, measured by `R_{os}^2 > 0`.\n\n### Question\n\nConsidering the investor's portfolio choice problem and the principles of market efficiency, select all of the following statements that are true.", "Options": {"A": "If other large funds begin to aggressively trade on this signal, the predictive power of the model, as measured by out-of-sample `R_{os}^2`, is expected to decline over time.", "B": "The optimal weight `w_t^*` is inversely proportional to the estimated intercept `\\hat{\\alpha}` from the predictive regression.", "C": "The optimal weight `w_t^*` is a positive linear function of the predictor's current value, `MADP_t(3)`.", "D": "The optimal weight `w_t^*` is directly proportional to the investor's risk aversion `γ`."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a **Scenario Application** strategy. It tests the application of portfolio choice theory (deriving properties of the optimal weight) and the economic concept of limits to arbitrage. Option A requires substituting Eq. 1 into Eq. 2 and interpreting the result. Option B is a conceptual question about market efficiency. Distractors target common errors in interpreting the portfolio weight formula: C incorrectly states the role of `γ` (Conceptual Opposite), and D incorrectly states the role of `\\hat{\\alpha}` (Formula Misuse).", "qid": "343", "question": "### Background\n\n**Research Question.** This case investigates the portfolio implications of stock return predictability from moving averages and the likely impact of arbitrage activity on such signals.\n\n**Setting and Sample.** A mean-variance investor uses a predictive regression to form expectations and allocate capital between the S&P 500 and a risk-free asset.\n\n### Data / Model Specification\n\nThe investor uses the `MADP(3)` predictive regression to form expectations about the next period's excess return, `E_t[r_{t+1}]`:\n  \nE_t[r_{t+1}] = \\hat{\\alpha} + \\hat{\\beta} \\cdot MADP_t(3) \\quad \\text{(Eq. 1)}\n \nThe optimal weight, `w_t^*`, allocated to the S&P 500 for an investor with risk aversion `γ` and a constant conditional return variance `σ^2` is given by:\n  \nw_t^* = \\frac{E_t[r_{t+1}]}{\\gamma \\sigma^2} \\quad \\text{(Eq. 2)}\n \nEmpirical results show that the `MADP(3)` predictor had significant out-of-sample predictive power, measured by `R_{os}^2 > 0`.\n\n### Question\n\nConsidering the investor's portfolio choice problem and the principles of market efficiency, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) If other large funds begin to aggressively trade on this signal, the predictive power of the model, as measured by out-of-sample R²ₒₛ, is expected to decline over time.\n\nThis is TRUE. When many investors trade on the same signal, their collective actions tend to eliminate the profit opportunity. As they buy when the signal predicts positive returns and sell when it predicts negative returns, prices adjust more quickly to incorporate the information, reducing the signal's predictive power. This is a fundamental principle of market efficiency - profitable trading strategies tend to self-destruct as more capital pursues them.\n\nB) The optimal weight w*ₜ is inversely proportional to the estimated intercept α̂ from the predictive regression.\n\nThis is FALSE. From equations (1) and (2):\n- E_t[r_{t+1}] = α̂ + β̂ · MADP_t(3)\n- w*ₜ = E_t[r_{t+1}]/(γσ²) = (α̂ + β̂ · MADP_t(3))/(γσ²)\n\nThe optimal weight increases when α̂ increases (holding other factors constant), so they are directly proportional, not inversely proportional.\n\nC) The optimal weight w*ₜ is a positive linear function of the predictor's current value, MADP_t(3).\n\nThis is TRUE. From w*ₜ = (α̂ + β̂ · MADP_t(3))/(γσ²), we can see that w*ₜ is a linear function of MADP_t(3) with slope β̂/(γσ²). Since the model has significant predictive power (R²ₒₛ > 0), β̂ must be non-zero. Given that this is a profitable trading strategy, β̂ is positive (higher MADP values predict higher returns), making the relationship a positive linear function.\n\nD) The optimal weight w*ₜ is directly proportional to the investor's risk aversion γ.\n\nThis is FALSE. From w*ₜ = E_t[r", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 448, "Question": "### Background\n\n**Research Question.** This case examines whether the behavior of hedgers and speculators in commodity markets is stable over time or if it shifts between distinct regimes. It investigates the econometric methods used to model such shifts and explores the implications for market dynamics.\n\n**Setting.** A model for futures returns is extended to a two-state Markov-Switching (MS) framework. The parameters governing the influence of hedging and speculative risk factors, as well as speculator risk preferences, are allowed to differ depending on an unobserved, latent market state, `s_t`, which captures shifts between market regimes.\n\n### Data / Model Specification\n\nThe Markov-Switching model for futures returns is:\n  \nr_{f,t}=e_{0s_{t}}-(b_{s_{t}}^{H}/d_{s_{t}}^{S})\\sigma_{r_H,t}^2+(e_{s_{t}}^{S}/d_{s_{t}}^{S})h_{r_{f},t}^{2}+c_{1s_{t}}r_{f,t-1}+u_{r_{f},s_{t}t} \n \nwhere `\\sigma_{r_H,t}^2` is the variance of the optimally hedged portfolio, `h_{r_f,t}^2` is the conditional variance of futures returns, and `u_{r_{f},s_t,t} \\sim N(0, \\sigma_{s_t}^2)`. The unobserved state `s_t` follows a first-order Markov chain.\n\nSpeculator demand is modeled such that the sign of the parameter `e^S` reflects risk preferences: `e^S > 0` for risk-averse speculators and `e^S < 0` for risk-seeking speculators. The parameter `d^S` is assumed to be positive.\n\n**Table 1. Identification and Estimation of Market Regimes**\n\n| Panel A: Correlation of Regime 1 Probability with Market Variables ||\n|:---|:---:|\n| Correlated Variable | Silver |\n| `r_{f,t}` (Futures Return) | 0.114 |\n| `\\sigma_{r_{f,t}}` (Return Std. Dev.) | -0.718 |\n\n| Panel B: Markov-Switching Estimates for Copper and Soybeans |||||\n| Parameter | Copper (St=1) | Copper (St=2) | Soybeans (St=1) | Soybeans (St=2) |\n| `P(st != st-1)` (Transition Prob.) | 0.021 | 0.056 | 0.034 | 0.084 |\n| `(e_{s_t}^S/d_{s_t}^S)` | 2.979 | 11.315 | 4.368 | -8.901 |\n\n*Source: Adapted from Tables 6 and 7 in the source paper. State 1 is identified as the low-volatility state and State 2 as the high-volatility state.*\n\n### Question\n\nBased on the data and model specification provided, select all of the following statements that are correct interpretations of the results.", "Options": {"A": "The estimates for Copper in Panel B show that speculators are risk-averse in the low-volatility state but become risk-seeking in the high-volatility state.", "B": "The estimates for Soybeans in Panel B indicate that speculators switch from risk-averse behavior in the low-volatility state to risk-seeking behavior in the high-volatility state.", "C": "For Silver, the correlations in Panel A suggest that Regime 1 is best characterized as a 'bearish, low-volatility' state.", "D": "For Copper, the transition probabilities in Panel B imply that the expected duration of the high-volatility regime (State 2) is approximately 18 working days."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the 'atomic decomposition' strategy to convert a multi-part QA problem into a single, high-fidelity multi-select question. It tests three distinct skills: interpreting correlation coefficients for regime identification (Option A), interpreting model parameters for state-dependent behavior (Options B & D), and performing a simple calculation based on transition probabilities (Option C). This makes it a 'Computational Judgment' item. Distractors are designed based on common errors: A misinterprets the sign of the return correlation (Conceptual Opposite); D misinterprets the sign of the coefficient for Copper in State 2, which remains positive (Almost Right).", "qid": "448", "question": "### Background\n\n**Research Question.** This case examines whether the behavior of hedgers and speculators in commodity markets is stable over time or if it shifts between distinct regimes. It investigates the econometric methods used to model such shifts and explores the implications for market dynamics.\n\n**Setting.** A model for futures returns is extended to a two-state Markov-Switching (MS) framework. The parameters governing the influence of hedging and speculative risk factors, as well as speculator risk preferences, are allowed to differ depending on an unobserved, latent market state, `s_t`, which captures shifts between market regimes.\n\n### Data / Model Specification\n\nThe Markov-Switching model for futures returns is:\n  \nr_{f,t}=e_{0s_{t}}-(b_{s_{t}}^{H}/d_{s_{t}}^{S})\\sigma_{r_H,t}^2+(e_{s_{t}}^{S}/d_{s_{t}}^{S})h_{r_{f},t}^{2}+c_{1s_{t}}r_{f,t-1}+u_{r_{f},s_{t}t} \n \nwhere `\\sigma_{r_H,t}^2` is the variance of the optimally hedged portfolio, `h_{r_f,t}^2` is the conditional variance of futures returns, and `u_{r_{f},s_t,t} \\sim N(0, \\sigma_{s_t}^2)`. The unobserved state `s_t` follows a first-order Markov chain.\n\nSpeculator demand is modeled such that the sign of the parameter `e^S` reflects risk preferences: `e^S > 0` for risk-averse speculators and `e^S < 0` for risk-seeking speculators. The parameter `d^S` is assumed to be positive.\n\n**Table 1. Identification and Estimation of Market Regimes**\n\n| Panel A: Correlation of Regime 1 Probability with Market Variables ||\n|:---|:---:|\n| Correlated Variable | Silver |\n| `r_{f,t}` (Futures Return) | 0.114 |\n| `\\sigma_{r_{f,t}}` (Return Std. Dev.) | -0.718 |\n\n| Panel B: Markov-Switching Estimates for Copper and Soybeans |||||\n| Parameter | Copper (St=1) | Copper (St=2) | Soybeans (St=1) | Soybeans (St=2) |\n| `P(st != st-1)` (Transition Prob.) | 0.021 | 0.056 | 0.034 | 0.084 |\n| `(e_{s_t}^S/d_{s_t}^S)` | 2.979 | 11.315 | 4.368 | -8.901 |\n\n*Source: Adapted from Tables 6 and 7 in the source paper. State 1 is identified as the low-volatility state and State 2 as the high-volatility state.*\n\n### Question\n\nBased on the data and model specification provided, select all of the following statements that are correct interpretations of the results.", "model_evaluation_result": {"model_raw_response": "Looking at this Markov-Switching model and the data provided, I need to evaluate each statement:\n\n**Statement A (Copper):** The parameter (e^S/d^S) for Copper is positive in both states (2.979 in State 1 and 11.315 in State 2). Since d^S is assumed positive, this means e^S > 0 in both states. According to the model specification, e^S > 0 indicates risk-averse speculators. Therefore, speculators are risk-averse in both states, not switching to risk-seeking. This statement is incorrect.\n\n**Statement B (Soybeans):** For Soybeans, (e^S/d^S) is 4.368 in State 1 (positive) and -8.901 in State 2 (negative). With d^S > 0, this means:\n- State 1: e^S > 0 → risk-averse speculators\n- State 2: e^S < 0 → risk-seeking speculators\nThis confirms speculators switch from risk-averse to risk-seeking behavior. This statement is correct.\n\n**Statement C (Silver):** Panel A shows for Silver:\n- Correlation with futures returns (r_f,t): 0.114 (positive but weak)\n- Correlation with return volatility (σ_r_f,t): -0.718 (strong negative)\n\nThe strong negative correlation with volatility confirms Regime 1 is a low-volatility state. However, the correlation with returns is only 0.114, which is too weak to characterize the regime as \"bearish\" (which would require a strong negative correlation). This statement is incorrect.\n\n**Statement D (Copper):** The transition probability P(st ≠ st-1) for State 2 is 0.056, meaning the probability of staying in State 2 is 1 - 0.056 = 0.944. The expected duration is 1/(1-0.944) = 1/0.056 ≈ 17.86 ≈ 18 working days. This statement is correct.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 333, "Question": "### Background\n\n**Research Question.** How can two different non-stationary regression specifications, both designed to test for forward rate unbiasedness and using cointegrated regressors, produce diametrically opposed results? This question explores how simultaneity bias can persist in non-stationary settings and how its magnitude is critically dependent on the choice of regressors.\n\n**Setting.** An econometric analysis of two non-stationary models. The first, used by Barnhart and Szakmary, regresses the spot rate `S_t` on the lagged forward rate `F_{t-1}` and the lagged spot rate `S_{t-1}`. The second, used by Frenkel, regresses `S_t` on `F_{t-1}` and the second lag of the forward rate, `F_{t-2}`. Under the null hypothesis of unbiasedness, the true coefficients `[\\beta_1, \\beta_2]` are `[1, 0]`.\n\n**Variables & Parameters.**\n- `S_t`, `S_{t-1}`: Log spot exchange rates.\n- `F_{t-1}`, `F_{t-2}`: Log forward exchange rates.\n- `\\eta_t`: Innovation to the spot rate process, `S_t - S_{t-1}`.\n- `e_{t-1}`: Innovation to the forward rate process, `F_{t-1} - S_{t-1}`.\n- `\\beta_1`, `\\beta_2`: Regression coefficients.\n\n---\n\n### Data / Model Specification\n\nThe analysis relies on the properties of the underlying innovations to the spot and forward rate data generating processes (DGPs).\n\n**Table 1. Summary Statistics of DGP Residuals (Japan)**\n| Statistic | Spot Rate Innovation (`\\eta_t`) | Forward Rate Innovation (`e_{t-1}`) |\n| :--- | :--- | :--- |\n| Variance | 0.0011155 | 0.0000115 |\n| Covariance `cov(e_{t-1}, \\eta_t)` | | -0.000014 |\n\n**Asymptotic Limits of OLS Estimators:**\n- **Barnhart/Szakmary/McCallum Specification (`S_t` on `F_{t-1}`, `S_{t-1}`):**\n      \n    [\\hat{\\beta}_1, \\hat{\\beta}_2] \\to [1, 0] + \\frac{\\mathrm{cov}(\\eta_{t},e_{t-1})-\\mathrm{var}(e_{t-1})}{\\mathrm{var}(e_{t-1})}[1, -1] \\quad \\text{(Eq. (1))}\n     \n- **Frenkel Specification (`S_t` on `F_{t-1}`, `F_{t-2}`):**\n      \n    [\\hat{\\beta}_1, \\hat{\\beta}_2] \\to [1, 0] + \\frac{\\mathrm{cov}(\\eta_{t},e_{t-1})-\\mathrm{var}(e_{t-1})}{2\\mathrm{var}(e_{t-1})+\\mathrm{var}(\\eta_{t})-2\\mathrm{cov}(\\eta_{t},e_{t-1})}[1, -1] \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nBased on the provided data and formulas for the Japanese Yen, which of the following statements are correct? Select all that apply.", "Options": {"A": "The theoretical asymptotic limit for `\\hat{\\beta}_2` in the Barnhart/Szakmary/McCallum specification (Eq. (1)) is approximately -2.22.", "B": "The theoretical asymptotic limit for `\\hat{\\beta}_1` in the Barnhart/Szakmary/McCallum specification (Eq. (1)) is approximately -1.22, indicating severe bias.", "C": "The negligible bias in the Frenkel specification (Eq. (2)) is due to its bias formula's denominator being dominated by the large spot rate error variance, `var(\\eta_t)`.", "D": "The large bias in the Barnhart/Szakmary/McCallum specification (Eq. (1)) is primarily caused by the large variance of the spot rate innovation (`var(\\eta_t)`) appearing in the denominator of its bias formula."}, "Answer": ["B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform calculations based on theoretical formulas and interpret the results in the context of the paper's argument. It uses a Computational Judgment strategy with atomic decomposition. Correct options A and B test the calculation for one model and the conceptual reason for the result in the other. Distractor C is based on a common sign error in calculation (the correct value is approx +2.22). Distractor D presents a conceptual opposite regarding the source of the bias.", "qid": "333", "question": "### Background\n\n**Research Question.** How can two different non-stationary regression specifications, both designed to test for forward rate unbiasedness and using cointegrated regressors, produce diametrically opposed results? This question explores how simultaneity bias can persist in non-stationary settings and how its magnitude is critically dependent on the choice of regressors.\n\n**Setting.** An econometric analysis of two non-stationary models. The first, used by Barnhart and Szakmary, regresses the spot rate `S_t` on the lagged forward rate `F_{t-1}` and the lagged spot rate `S_{t-1}`. The second, used by Frenkel, regresses `S_t` on `F_{t-1}` and the second lag of the forward rate, `F_{t-2}`. Under the null hypothesis of unbiasedness, the true coefficients `[\\beta_1, \\beta_2]` are `[1, 0]`.\n\n**Variables & Parameters.**\n- `S_t`, `S_{t-1}`: Log spot exchange rates.\n- `F_{t-1}`, `F_{t-2}`: Log forward exchange rates.\n- `\\eta_t`: Innovation to the spot rate process, `S_t - S_{t-1}`.\n- `e_{t-1}`: Innovation to the forward rate process, `F_{t-1} - S_{t-1}`.\n- `\\beta_1`, `\\beta_2`: Regression coefficients.\n\n---\n\n### Data / Model Specification\n\nThe analysis relies on the properties of the underlying innovations to the spot and forward rate data generating processes (DGPs).\n\n**Table 1. Summary Statistics of DGP Residuals (Japan)**\n| Statistic | Spot Rate Innovation (`\\eta_t`) | Forward Rate Innovation (`e_{t-1}`) |\n| :--- | :--- | :--- |\n| Variance | 0.0011155 | 0.0000115 |\n| Covariance `cov(e_{t-1}, \\eta_t)` | | -0.000014 |\n\n**Asymptotic Limits of OLS Estimators:**\n- **Barnhart/Szakmary/McCallum Specification (`S_t` on `F_{t-1}`, `S_{t-1}`):**\n      \n    [\\hat{\\beta}_1, \\hat{\\beta}_2] \\to [1, 0] + \\frac{\\mathrm{cov}(\\eta_{t},e_{t-1})-\\mathrm{var}(e_{t-1})}{\\mathrm{var}(e_{t-1})}[1, -1] \\quad \\text{(Eq. (1))}\n     \n- **Frenkel Specification (`S_t` on `F_{t-1}`, `F_{t-2}`):**\n      \n    [\\hat{\\beta}_1, \\hat{\\beta}_2] \\to [1, 0] + \\frac{\\mathrm{cov}(\\eta_{t},e_{t-1})-\\mathrm{var}(e_{t-1})}{2\\mathrm{var}(e_{t-1})+\\mathrm{var}(\\eta_{t})-2\\mathrm{cov}(\\eta_{t},e_{t-1})}[1, -1] \\quad \\text{(Eq. (2))}\n     \n\n---\n\n### Question\n\nBased on the provided data and formulas for the Japanese Yen, which of the following statements are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "I need to calculate the asymptotic limits for both specifications using the provided data and formulas.\n\nGiven data:\n- var(η_t) = 0.0011155\n- var(e_{t-1}) = 0.0000115\n- cov(e_{t-1}, η_t) = -0.000014\n\nFirst, let me calculate the bias term that appears in both formulas:\ncov(η_t, e_{t-1}) - var(e_{t-1}) = -0.000014 - 0.0000115 = -0.0000255\n\n**For Barnhart/Szakmary/McCallum specification (Eq. 1):**\nThe bias multiplier is:\n[cov(η_t, e_{t-1}) - var(e_{t-1})]/var(e_{t-1}) = -0.0000255/0.0000115 = -2.217\n\nSo:\n- β̂₁ → 1 + (-2.217)(1) = -1.217 ≈ -1.22\n- β̂₂ → 0 + (-2.217)(-1) = 2.217 ≈ 2.22\n\n**For Frenkel specification (Eq. 2):**\nThe denominator is:\n2var(e_{t-1}) + var(η_t) - 2cov(η_t, e_{t-1})\n= 2(0.0000115) + 0.0011155 - 2(-0.000014)\n= 0.000023 + 0.0011155 + 0.000028\n= 0.0011665\n\nThe bias multiplier is:\n-0.0000255/0.0011665 = -0.0219\n\nSo:\n- β̂₁ → 1 + (-0.0219)(1) = 0.978\n- β̂₂ → 0 + (-0.0219)(-1) = 0.0219\n\nNow checking each option:\n\nA) The asymptotic limit for β̂₂ in", "model_answer": ["A"], "error": null}}
{"ID": 65, "Question": "### Background\n\n**Research Question.** How are an insurer's optimal dynamic reinsurance and dividend policies jointly determined as the solution to a stochastic optimal control problem?\n\n**Setting.** The insurer's problem is formulated as a free-boundary problem. Below a surplus barrier `x_b`, the value function `v(x)` satisfies a Hamilton-Jacobi-Bellman (HJB) equation. The insurer chooses the retention proportion `q` to maximize the value function.\n\n**Variables and Parameters.**\n- `v(x)`: The value function at surplus `x`.\n- `q`: The retention proportion, the control variable for reinsurance.\n- `v'(x), v''(x)`: First and second derivatives of the value function.\n- `θ, η, a, b, β`: Standard model parameters.\n\n---\n\n### Data / Model Specification\n\nIn the continuation region `0 < x < x_b`, the value function `v(x)` solves the HJB equation:\n\n  \n\\beta v(x) = \\operatorname*{max}_{0\\leq q\\leq1}\\left[(\\theta-(1-q)\\eta)a v^{\\prime}(x)+\\frac{1}{2}q^{2}b^{2}v^{\\prime\\prime}(x)\\right] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the HJB equation in **Eq. (1)**, which of the following statements about the optimal reinsurance strategy `q*(x)` and its economic interpretation are correct? Assume an interior solution for `q*` and that the value function `v(x)` is concave. Select all that apply.", "Options": {"A": "The term `v'(x)` in the formula represents the total value of the firm, while `v''(x)` represents its growth rate.", "B": "The optimal retention `q*(x)` is inversely proportional to the insurer's absolute risk aversion, `ARA(x) = -v''(x)/v'(x)`.", "C": "An increase in the cost of reinsurance, `η`, leads the insurer to retain a smaller proportion of its risk (a lower `q*`).", "D": "The formula for the unconstrained optimal retention is `q*(x) = - (ηa/b²) * (v'(x)/v''(x))`."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item assesses the derivation and economic interpretation of the optimal control. It uses the 'Atomic Decomposition' strategy. Option B is the correct mathematical derivation from the HJB equation. Option A is the correct economic interpretation of that formula in terms of risk aversion. Distractor C (Conceptual Opposite) states the incorrect relationship between reinsurance cost and retention. Distractor D (Formula Misuse) provides incorrect definitions for the derivatives of the value function.", "qid": "65", "question": "### Background\n\n**Research Question.** How are an insurer's optimal dynamic reinsurance and dividend policies jointly determined as the solution to a stochastic optimal control problem?\n\n**Setting.** The insurer's problem is formulated as a free-boundary problem. Below a surplus barrier `x_b`, the value function `v(x)` satisfies a Hamilton-Jacobi-Bellman (HJB) equation. The insurer chooses the retention proportion `q` to maximize the value function.\n\n**Variables and Parameters.**\n- `v(x)`: The value function at surplus `x`.\n- `q`: The retention proportion, the control variable for reinsurance.\n- `v'(x), v''(x)`: First and second derivatives of the value function.\n- `θ, η, a, b, β`: Standard model parameters.\n\n---\n\n### Data / Model Specification\n\nIn the continuation region `0 < x < x_b`, the value function `v(x)` solves the HJB equation:\n\n  \n\\beta v(x) = \\operatorname*{max}_{0\\leq q\\leq1}\\left[(\\theta-(1-q)\\eta)a v^{\\prime}(x)+\\frac{1}{2}q^{2}b^{2}v^{\\prime\\prime}(x)\\right] \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Question\n\nBased on the HJB equation in **Eq. (1)**, which of the following statements about the optimal reinsurance strategy `q*(x)` and its economic interpretation are correct? Assume an interior solution for `q*` and that the value function `v(x)` is concave. Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the HJB equation and analyzing each statement:\n\n**Statement A**: \"The term `v'(x)` in the formula represents the total value of the firm, while `v''(x)` represents its growth rate.\"\n- This is incorrect. `v'(x)` represents the marginal value of surplus (how value changes with surplus), not total value. `v''(x)` represents the curvature of the value function, not growth rate.\n\n**Statement B**: \"The optimal retention `q*(x)` is inversely proportional to the insurer's absolute risk aversion, `ARA(x) = -v''(x)/v'(x)`.\"\n- To find the optimal q*, we take the first-order condition:\n  ∂/∂q[(θ-(1-q)η)av'(x) + (1/2)q²b²v''(x)] = 0\n  ηav'(x) + qb²v''(x) = 0\n  q* = -ηav'(x)/(b²v''(x))\n- Since ARA(x) = -v''(x)/v'(x), we have:\n  q* = ηa/(b²·ARA(x))\n- This shows q* is indeed inversely proportional to ARA(x). This is correct.\n\n**Statement C**: \"An increase in the cost of reinsurance, `η`, leads the insurer to retain a smaller proportion of its risk (a lower `q*`).\"\n- From q* = -ηav'(x)/(b²v''(x)), and since v(x) is concave (v''(x) < 0) and v'(x) > 0:\n  - The ratio -v'(x)/v''(x) is positive\n  - Therefore, ∂q*/∂η > 0\n- An increase in η leads to higher q*, not lower. This is incorrect.\n\n**Statement D**: \"The formula for the unconstrained optimal retention is `q*(x) = - (ηa/b²) * (v'(x)/v''(x))`.\"\n- From our derivation above, this is exactly", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 454, "Question": "### Background\n\nThis paper investigates whether a firm's reluctance to cut dividends, a behavior termed \"dividend stickiness,\" serves as a credible signal to investors about the firm's future earnings prospects. The study proposes that managers who are confident in their firm's future earnings are more likely to avoid cutting dividends even when current earnings fall. This behavior, if understood by the market, should be associated with higher future earnings persistence, stronger future earnings growth, and a higher valuation multiple placed on the firm's current earnings.\n\n### Data / Model Specification\n\nThe study tests three main hypotheses using regression analysis:\n1.  **Earnings Persistence (H1)**: Tested with the model:\n      \n    EPS_{i,t+1}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}DS_{i,t}+\\alpha_{3}EPS_{i,t}*DS_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n     \n    **Eq. (1)**\n2.  **Earnings Growth (H2)**: Tested with the model:\n      \n    EG_{i,(t+1,t)}=\\alpha_{0}+\\alpha_{1}DS_{i,t}+\\alpha_{2}Size_{i,t}+\\alpha_{3}ROA_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n     \n    **Eq. (2)**\n3.  **Pricing Multiples (H3)**: Tested with an Ohlson valuation model:\n      \n    P_{i,t}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}BV_{i,t}+\\alpha_{3}DS_{i,t-1}+\\alpha_{4}EPS_{i,t}*DS_{i,t-1} + \\text{Controls} + \\varepsilon_{i,t}\n     \n    **Eq. (3)**\n\nIn the models, `DS_Dummy` is a binary variable equal to 1 for firms with sticky dividends.\n\n**Table 1: Asymmetric Response of Dividends to Earnings Change (Total Sample)**\n\n| Earnings change | Median of dividend change (%) |\n| :--- | :--- |\n| Increase (+) | 0.678 |\n| Decrease (-) | -0.389 |\n\n**Table 2: Impact on Earnings Persistence (H1)**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.675*** |\n| DS_Dummyt | 0.207*** |\n| **EPS\\*DS_Dummy** | **0.031*** |\n\n**Table 3: Impact on Earnings Growth (H2)**\n\n| Variable | (1) 1 year |\n| :--- | :--- |\n| **DS_Dummy** | **0.078*** |\n| Size | 0.049*** |\n\n**Table 4: Impact on Pricing Multiples (H3)**\n\n| Variable | (1) DS_dummy |\n| :--- | :--- |\n| **EPSt** | **5.041*** |\n| BVt | 1.582*** |\n| **EPS\\*DSt-1** | **0.834** |\n\n\n### Question\n\nThe paper tests three hypotheses regarding the signaling content of dividend stickiness. Based on the provided definitions and regression results, which of the following empirical findings are directly supported by the tables? Select all that apply.", "Options": {"A": "The evidence supports Hypothesis 1, showing that the earnings persistence for firms with sticky dividends is significantly higher than for firms without.", "B": "The regression results for Hypothesis 3 indicate that the market values the earnings of firms with sticky dividends more highly, with the earnings multiple increasing by approximately 16.5% for such firms.", "C": "The total sample data shows a strong asymmetry: the median dividend increase for a 1% earnings rise (0.678%) is substantially larger in magnitude than the median dividend decrease for a 1% earnings fall (-0.389%).", "D": "The `DS_Dummy` variable has a direct, positive, and significant effect on one-year-ahead earnings (`EPSt+1`), independent of current earnings levels."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the comprehensive understanding of the paper's central empirical results across all three of its main hypotheses. It uses an Atomic Decomposition strategy, presenting key findings as separate statements. The item requires both calculation/estimation (asymmetry in A, percentage increase in B) and careful interpretation of regression coefficients (D). The distractor (C) is a 'True but Misleading' statement; while the coefficient on DS_Dummyt in Table 2 is significant, the core test of H1 relies on the interaction term, assessing a reader's focus on the paper's central argument.", "qid": "454", "question": "### Background\n\nThis paper investigates whether a firm's reluctance to cut dividends, a behavior termed \"dividend stickiness,\" serves as a credible signal to investors about the firm's future earnings prospects. The study proposes that managers who are confident in their firm's future earnings are more likely to avoid cutting dividends even when current earnings fall. This behavior, if understood by the market, should be associated with higher future earnings persistence, stronger future earnings growth, and a higher valuation multiple placed on the firm's current earnings.\n\n### Data / Model Specification\n\nThe study tests three main hypotheses using regression analysis:\n1.  **Earnings Persistence (H1)**: Tested with the model:\n      \n    EPS_{i,t+1}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}DS_{i,t}+\\alpha_{3}EPS_{i,t}*DS_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n     \n    **Eq. (1)**\n2.  **Earnings Growth (H2)**: Tested with the model:\n      \n    EG_{i,(t+1,t)}=\\alpha_{0}+\\alpha_{1}DS_{i,t}+\\alpha_{2}Size_{i,t}+\\alpha_{3}ROA_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n     \n    **Eq. (2)**\n3.  **Pricing Multiples (H3)**: Tested with an Ohlson valuation model:\n      \n    P_{i,t}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}BV_{i,t}+\\alpha_{3}DS_{i,t-1}+\\alpha_{4}EPS_{i,t}*DS_{i,t-1} + \\text{Controls} + \\varepsilon_{i,t}\n     \n    **Eq. (3)**\n\nIn the models, `DS_Dummy` is a binary variable equal to 1 for firms with sticky dividends.\n\n**Table 1: Asymmetric Response of Dividends to Earnings Change (Total Sample)**\n\n| Earnings change | Median of dividend change (%) |\n| :--- | :--- |\n| Increase (+) | 0.678 |\n| Decrease (-) | -0.389 |\n\n**Table 2: Impact on Earnings Persistence (H1)**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.675*** |\n| DS_Dummyt | 0.207*** |\n| **EPS\\*DS_Dummy** | **0.031*** |\n\n**Table 3: Impact on Earnings Growth (H2)**\n\n| Variable | (1) 1 year |\n| :--- | :--- |\n| **DS_Dummy** | **0.078*** |\n| Size | 0.049*** |\n\n**Table 4: Impact on Pricing Multiples (H3)**\n\n| Variable | (1) DS_dummy |\n| :--- | :--- |\n| **EPSt** | **5.041*** |\n| BVt | 1.582*** |\n| **EPS\\*DSt-1** | **0.834** |\n\n\n### Question\n\nThe paper tests three hypotheses regarding the signaling content of dividend stickiness. Based on the provided definitions and regression results, which of the following empirical findings are directly supported by the tables? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the empirical findings in the tables:\n\n**Option A**: The evidence does support H1. Table 2 shows that the interaction term EPS*DS_Dummy has a coefficient of 0.031***, which is positive and significant. This means firms with sticky dividends have higher earnings persistence (0.675 + 0.031 = 0.706) compared to firms without (0.675).\n\n**Option B**: Table 4 shows the earnings coefficient for non-sticky firms is 5.041 and the interaction term EPS*DSt-1 is 0.834. The earnings multiple for sticky dividend firms is 5.041 + 0.834 = 5.875. The percentage increase is 0.834/5.041 = 16.5%. This is supported.\n\n**Option C**: Table 1 shows median dividend change is +0.678% when earnings increase but -0.389% when earnings decrease. The magnitude comparison is incorrect in the option - 0.678% is actually smaller in absolute magnitude than 0.389%, not larger.\n\n**Option D**: Table 2 shows DS_Dummy has a coefficient of 0.207***, which is indeed positive and significant. This represents the direct effect of DS_Dummy on EPSt+1, independent of the interaction with current earnings.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 460, "Question": "### Background\n\n**Research Question.** After establishing the statistical significance of various factors on sovereign risk, the study seeks to determine their relative economic importance. Specifically, is the impact of global risk appetite larger or smaller than that of traditional country-specific fundamentals?\n\n**Setting / Data-Generating Environment.** The analysis uses standardized coefficients from a dynamic panel model. Standardization involves normalizing each variable (dependent and independent) by subtracting its mean and dividing by its standard deviation. This allows for a unit-free comparison of the explanatory variables' influence.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t`.\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t`.\n- `IR`\\u208_it\\u208_: International reserves / imports ratio for country `i` at quarter `t`.\n- Standardized Coefficient (`\\theta`): The change in the dependent variable, measured in standard deviations, resulting from a one standard deviation change in an independent variable.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents standardized coefficients to assess the relative economic importance of different factors on the Brady bond yield spread (BBY).\n\n**Table 1: Estimated Standardized Coefficients for BBY**\n\n| | Standardized Coefficient |\n| :--- | :--- |\n| **Dependent Variable** | **Brady bond stripped yield spread** |\n| One-lagged dep. Var. | 0.6374 (7.899)** |\n| Risk appetite index (RAI) | **-0.2083 (-3.355)**** |\n| Real GDP growth (GDP) | -0.1599 (-2.271)** |\n| International reserves (IR) | **-0.1688 (-2.325)**** |\n| Change in real exchange rate | -0.1333 (-1.943)** |\n| Inflation rate | 0.0808 (1.016) |\n\n*Notes: t-statistics in parentheses. ** denotes significance at the 5% level or better.* \n\n---\n\nWhich of the following statements are valid conclusions based on these results and the methodology?", "Options": {"A": "A one-standard-deviation increase in International Reserves (IR) is predicted to cause the BBY spread to decrease by 0.1688 basis points.", "B": "The use of standardized coefficients is necessary for a meaningful comparison of economic importance because the explanatory variables are measured in different units and have different natural scales of variation.", "C": "The large coefficient on the lagged dependent variable (0.6374) implies that the short-run impacts shown in the table are identical to the long-run impacts of each variable.", "D": "A one-standard-deviation shock to the Risk Appetite Index (RAI) has a larger immediate impact on the BBY spread than a one-standard-deviation shock to International Reserves (IR)."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the interpretation of standardized coefficients and the understanding of their methodological purpose. It uses an Atomic Decomposition strategy. Distractors include a classic Unit Error (C) confusing standard deviations with basis points, and a Conceptual Opposite (D) that misinterprets the implications of dynamic adjustment in the model, subtly testing a key limitation of the analysis.", "qid": "460", "question": "### Background\n\n**Research Question.** After establishing the statistical significance of various factors on sovereign risk, the study seeks to determine their relative economic importance. Specifically, is the impact of global risk appetite larger or smaller than that of traditional country-specific fundamentals?\n\n**Setting / Data-Generating Environment.** The analysis uses standardized coefficients from a dynamic panel model. Standardization involves normalizing each variable (dependent and independent) by subtracting its mean and dividing by its standard deviation. This allows for a unit-free comparison of the explanatory variables' influence.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t`.\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t`.\n- `IR`\\u208_it\\u208_: International reserves / imports ratio for country `i` at quarter `t`.\n- Standardized Coefficient (`\\theta`): The change in the dependent variable, measured in standard deviations, resulting from a one standard deviation change in an independent variable.\n\n---\n\n### Data / Model Specification\n\nTable 1 presents standardized coefficients to assess the relative economic importance of different factors on the Brady bond yield spread (BBY).\n\n**Table 1: Estimated Standardized Coefficients for BBY**\n\n| | Standardized Coefficient |\n| :--- | :--- |\n| **Dependent Variable** | **Brady bond stripped yield spread** |\n| One-lagged dep. Var. | 0.6374 (7.899)** |\n| Risk appetite index (RAI) | **-0.2083 (-3.355)**** |\n| Real GDP growth (GDP) | -0.1599 (-2.271)** |\n| International reserves (IR) | **-0.1688 (-2.325)**** |\n| Change in real exchange rate | -0.1333 (-1.943)** |\n| Inflation rate | 0.0808 (1.016) |\n\n*Notes: t-statistics in parentheses. ** denotes significance at the 5% level or better.* \n\n---\n\nWhich of the following statements are valid conclusions based on these results and the methodology?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the standardized coefficients methodology and the results presented.\n\nLet me analyze each option:\n\n**Option A**: \"A one-standard-deviation increase in International Reserves (IR) is predicted to cause the BBY spread to decrease by 0.1688 basis points.\"\n\nThis is incorrect. The coefficient -0.1688 is a standardized coefficient, meaning it represents the change in BBY measured in standard deviations, not in basis points. A one-standard-deviation increase in IR leads to a 0.1688 standard deviation decrease in BBY.\n\n**Option B**: \"The use of standardized coefficients is necessary for a meaningful comparison of economic importance because the explanatory variables are measured in different units and have different natural scales of variation.\"\n\nThis is correct. The background explicitly states that standardization \"allows for a unit-free comparison of the explanatory variables' influence.\" Variables like GDP growth (%), inflation rate (%), reserves/imports ratio, and risk appetite index all have different units and scales. Standardization puts them on a common scale for comparison.\n\n**Option C**: \"The large coefficient on the lagged dependent variable (0.6374) implies that the short-run impacts shown in the table are identical to the long-run impacts of each variable.\"\n\nThis is incorrect. In dynamic models with lagged dependent variables, short-run and long-run effects differ. The presence of a significant lagged dependent variable (0.6374) indicates persistence, meaning shocks have cumulative effects over time. Long-run effects would need to be calculated separately using the formula: long-run effect = short-run effect / (1 - coefficient on lagged dependent variable).\n\n**Option D**: \"A one-standard-deviation shock to the Risk Appetite Index (RAI) has a larger immediate impact on the BBY spread than a one-standard-deviation shock to International Reserves (IR).\"\n\nThis is correct. The standardized coefficient for RAI is -0.2083 (in absolute value: 0.2083), while for IR it is -0.1688 (in absolute value: 0.1688). Since 0.2083 > 0.1688, RAI has a larger immediate impact.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 453, "Question": "### Background\n\nA central question in corporate finance is whether dividend policy contains information about a firm's future prospects. This paper investigates if managerial reluctance to cut dividends, termed \"dividend stickiness,\" serves as a credible signal of future earnings quality. The primary empirical finding is a positive correlation between dividend stickiness and future earnings persistence. However, a key challenge to interpreting this correlation as a causal signal is the issue of self-selection bias: managers of firms with genuinely better (but unobserved) future prospects may be more likely to adopt a sticky dividend policy. The correlation would then be spurious rather than a result of the policy signaling information.\n\nTo strengthen the causal claim, the paper employs two distinct strategies: (1) a cross-sectional analysis to rule out the alternative \"catering theory\" of dividends, and (2) a propensity score matching (PSM) analysis to mitigate selection bias based on observable firm characteristics.\n\n### Data / Model Specification\n\nThe baseline test for the signaling hypothesis uses the following one-year-ahead earnings persistence model:\n\n  \nEPS_{i,t+1}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}DS_{i,t}+\\alpha_{3}EPS_{i,t}*DS_{i,t}+\\alpha_{4}SI_{i,t}+\\alpha_{5}SI_{i,t}*EPS_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n \n**Eq. (1)**\n\nwhere `EPS` is earnings per share, `DS` is a measure of dividend stickiness (`DS_Dummy` = 1 for sticky firms), and `SI` is a dummy for special items. The coefficient of interest is `α3`, which captures the incremental effect of dividend stickiness on earnings persistence.\n\n**Catering Theory Analysis**: This theory posits that managers smooth dividends to cater to investor demand, not to signal. This demand is proxied by the Dividend Premium (`DP`), where `DP=1` indicates high catering incentives. The sample is split into high (`DP=1`) and low (`DP=0`) catering groups, and Eq. (1) is estimated for each.\n\n**Propensity Score Matching (PSM) Analysis**: To address self-selection, a logistic model estimates the probability (propensity score) of a firm having a sticky dividend policy based on observable characteristics. Sticky firms (treatment) are then matched with non-sticky firms (control) that have the closest propensity score. Eq. (1) is then re-estimated on this matched sample.\n\n**Table 1: Baseline OLS Results for Earnings Persistence**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.675*** |\n| **EPS\\*DS** | **0.031*** |\n| Observations | 59,038 |\n\n**Table 2: Cross-Sectional Analysis by Catering Incentives**\n\n| | DP-high (DP=1) | DP-low (DP=0) |\n| :--- | :--- | :--- |\n| EPSt | 0.666*** | 0.688*** |\n| **EPS\\*DS_Dummy** | **0.019** | **0.039*** |\n\n**Table 3: PSM Results for Earnings Persistence**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.751*** |\n| **EPS\\*DS** | **0.045** |\n| Observations | 18,984 |\n\n\n### Question\n\nThe paper investigates whether dividend stickiness is a credible signal of future earnings, using several econometric strategies to address potential biases. Based on the provided models and results, which of the following statements are valid interpretations of the evidence? Select all that apply.", "Options": {"A": "The PSM results in Table 3 reveal that once observable differences are accounted for, the signaling effect of dividend stickiness disappears, suggesting the baseline OLS result was entirely due to self-selection.", "B": "The cross-sectional analysis in Table 2 provides strong support for the catering theory, showing that the reluctance to cut dividends is most informative when investor demand for dividends is high.", "C": "The Propensity Score Matching (PSM) analysis, which controls for selection on observable firm characteristics, confirms that a positive relationship between dividend stickiness and earnings persistence remains.", "D": "The signaling effect of dividend stickiness on earnings persistence is statistically significant in the sub-sample of firms with low catering incentives, but not in the sub-sample with high catering incentives."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize evidence from multiple, advanced econometric analyses (sub-group and PSM) to evaluate a paper's core causal claim. It uses an Atomic Decomposition strategy, breaking down the paper's identification strategy into distinct, verifiable conclusions. Distractors are designed as Conceptual Opposites (C and D), directly contradicting the evidence in the tables to test for careful reading and interpretation of the paper's main efforts to establish causality.", "qid": "453", "question": "### Background\n\nA central question in corporate finance is whether dividend policy contains information about a firm's future prospects. This paper investigates if managerial reluctance to cut dividends, termed \"dividend stickiness,\" serves as a credible signal of future earnings quality. The primary empirical finding is a positive correlation between dividend stickiness and future earnings persistence. However, a key challenge to interpreting this correlation as a causal signal is the issue of self-selection bias: managers of firms with genuinely better (but unobserved) future prospects may be more likely to adopt a sticky dividend policy. The correlation would then be spurious rather than a result of the policy signaling information.\n\nTo strengthen the causal claim, the paper employs two distinct strategies: (1) a cross-sectional analysis to rule out the alternative \"catering theory\" of dividends, and (2) a propensity score matching (PSM) analysis to mitigate selection bias based on observable firm characteristics.\n\n### Data / Model Specification\n\nThe baseline test for the signaling hypothesis uses the following one-year-ahead earnings persistence model:\n\n  \nEPS_{i,t+1}=\\alpha_{0}+\\alpha_{1}EPS_{i,t}+\\alpha_{2}DS_{i,t}+\\alpha_{3}EPS_{i,t}*DS_{i,t}+\\alpha_{4}SI_{i,t}+\\alpha_{5}SI_{i,t}*EPS_{i,t} + \\text{Controls} + \\varepsilon_{i,t+1}\n \n**Eq. (1)**\n\nwhere `EPS` is earnings per share, `DS` is a measure of dividend stickiness (`DS_Dummy` = 1 for sticky firms), and `SI` is a dummy for special items. The coefficient of interest is `α3`, which captures the incremental effect of dividend stickiness on earnings persistence.\n\n**Catering Theory Analysis**: This theory posits that managers smooth dividends to cater to investor demand, not to signal. This demand is proxied by the Dividend Premium (`DP`), where `DP=1` indicates high catering incentives. The sample is split into high (`DP=1`) and low (`DP=0`) catering groups, and Eq. (1) is estimated for each.\n\n**Propensity Score Matching (PSM) Analysis**: To address self-selection, a logistic model estimates the probability (propensity score) of a firm having a sticky dividend policy based on observable characteristics. Sticky firms (treatment) are then matched with non-sticky firms (control) that have the closest propensity score. Eq. (1) is then re-estimated on this matched sample.\n\n**Table 1: Baseline OLS Results for Earnings Persistence**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.675*** |\n| **EPS\\*DS** | **0.031*** |\n| Observations | 59,038 |\n\n**Table 2: Cross-Sectional Analysis by Catering Incentives**\n\n| | DP-high (DP=1) | DP-low (DP=0) |\n| :--- | :--- | :--- |\n| EPSt | 0.666*** | 0.688*** |\n| **EPS\\*DS_Dummy** | **0.019** | **0.039*** |\n\n**Table 3: PSM Results for Earnings Persistence**\n\n| Variable | (1) DS_Dummy |\n| :--- | :--- |\n| EPSt | 0.751*** |\n| **EPS\\*DS** | **0.045** |\n| Observations | 18,984 |\n\n\n### Question\n\nThe paper investigates whether dividend stickiness is a credible signal of future earnings, using several econometric strategies to address potential biases. Based on the provided models and results, which of the following statements are valid interpretations of the evidence? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the tables and interpreting the results:\n\n**Table 1 (Baseline OLS)**: Shows EPS*DS coefficient of 0.031*** (highly significant), indicating dividend stickiness is associated with higher earnings persistence.\n\n**Table 2 (Cross-sectional by Catering Incentives)**:\n- DP-high (DP=1): EPS*DS_Dummy = 0.019 (no stars, not significant)\n- DP-low (DP=0): EPS*DS_Dummy = 0.039*** (highly significant)\n\nThis shows the signaling effect is significant only when catering incentives are low, not high.\n\n**Table 3 (PSM Results)**: Shows EPS*DS coefficient of 0.045 (no stars, not significant at conventional levels).\n\nEvaluating each option:\n\nA) FALSE - The PSM coefficient (0.045) is positive and actually larger than the baseline (0.031), though not statistically significant. This doesn't suggest the effect \"disappears\" or was \"entirely due to self-selection.\"\n\nB) FALSE - The results show the opposite. The signaling effect is stronger and significant when catering incentives are LOW (DP=0), not high.\n\nC) FALSE - While the relationship remains positive, it's not statistically significant in the PSM analysis (no stars on the 0.045 coefficient).\n\nD) TRUE - Table 2 clearly shows the signaling effect is significant in the low catering incentive subsample (0.039***) but not in the high catering incentive subsample (0.019, no stars).\n\nD", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 404, "Question": "### Background\n\n**Research Question.** This paper's central hypothesis is that market liquidity enhances efficiency by facilitating arbitrage. This is tested by examining if the predictability of short-horizon returns from past order flow is weaker on more liquid days. The analysis is conducted across three tick-size regimes which induced secular changes in liquidity.\n\n**Setting and Sample.** The study uses 5-minute interval data for a value-weighted portfolio of NYSE stocks from 1993-2002.\n\n### Data / Model Specification\n\n**Variables:**\n- `Return_t`: The 5-minute portfolio return.\n- `OIB$t-1`: The dollar order imbalance from the prior 5-minute interval, calculated as (buyer-initiated volume - seller-initiated volume) / total volume.\n- `ILD_t`: An \"Illiquid Day\" dummy variable. It is constructed for each tick-size regime by first linearly detrending the daily effective spread, then setting `ILD_t = 1` for days when the residual is at least one standard deviation above the mean, and 0 otherwise.\n\n**Baseline Model:** The paper first establishes baseline predictability:\n  \nReturn_t = \\alpha + \\beta \\cdot OIB\\$_{t-1} + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n**Interaction Model:** The main hypothesis is tested with an interaction term:\n  \nReturn_t = \\alpha + \\beta_1 OIB\\$_{t-1} + \\beta_2 (OIB\\$_{t-1} \\cdot ILD_t) + \\epsilon_t \\quad \\text{(Eq. 2)}\n \n\n**Table 1. Summary Statistics and Regression Results**\n\n| Panel A: Summary Statistic (Full Sample) | Value |\n| :--- | :--- |\n| Standard Deviation of `OIB$` | 0.251 |\n\n| Panel B: Baseline Regression (Eq. 1, Full Sample) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| `OIB$t-1` | 0.0259 | 31.40 |\n\n| Panel C: Interaction Regression (Eq. 2, Sixteenths Regime) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| `OIB$t-1` (β₁) | 0.0148 | 7.92 |\n| `OIB$t-1 * ILD_t` (β₂) | 0.0517 | 9.53 |\n\n### Question\n\nBased on the provided model specifications and results, which of the following statements are correct?", "Options": {"A": "In the Sixteenths Regime, the marginal effect of lagged order imbalance (`OIB$t-1`) on returns is more than four times stronger on illiquid days (`ILD_t=1`) compared to liquid days (`ILD_t=0`).", "B": "The highly significant positive coefficient on the interaction term (`β₂`) supports the hypothesis that illiquidity impedes market efficiency by allowing the price impact of order flow to persist longer.", "C": "Based on the full-sample baseline regression, a one-standard-deviation increase in `OIB$t-1` predicts a 0.0259 basis point increase in the five-minute return.", "D": "In the Sixteenths Regime, the marginal effect of lagged order imbalance on returns during an illiquid day is 0.0517."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the interpretation of an interaction model, combining Computational Judgment with conceptual understanding. It uses an Atomic Decomposition strategy. Option A requires a multi-step calculation of marginal effects. Option C tests the conceptual takeaway. Distractor B is a classic Step-Omission Error in interpreting interaction terms (using β₂ instead of β₁+β₂). Distractor D is a Numerical/Unit Error, confusing a regression coefficient with its calculated economic magnitude.", "qid": "404", "question": "### Background\n\n**Research Question.** This paper's central hypothesis is that market liquidity enhances efficiency by facilitating arbitrage. This is tested by examining if the predictability of short-horizon returns from past order flow is weaker on more liquid days. The analysis is conducted across three tick-size regimes which induced secular changes in liquidity.\n\n**Setting and Sample.** The study uses 5-minute interval data for a value-weighted portfolio of NYSE stocks from 1993-2002.\n\n### Data / Model Specification\n\n**Variables:**\n- `Return_t`: The 5-minute portfolio return.\n- `OIB$t-1`: The dollar order imbalance from the prior 5-minute interval, calculated as (buyer-initiated volume - seller-initiated volume) / total volume.\n- `ILD_t`: An \"Illiquid Day\" dummy variable. It is constructed for each tick-size regime by first linearly detrending the daily effective spread, then setting `ILD_t = 1` for days when the residual is at least one standard deviation above the mean, and 0 otherwise.\n\n**Baseline Model:** The paper first establishes baseline predictability:\n  \nReturn_t = \\alpha + \\beta \\cdot OIB\\$_{t-1} + \\epsilon_t \\quad \\text{(Eq. 1)}\n \n**Interaction Model:** The main hypothesis is tested with an interaction term:\n  \nReturn_t = \\alpha + \\beta_1 OIB\\$_{t-1} + \\beta_2 (OIB\\$_{t-1} \\cdot ILD_t) + \\epsilon_t \\quad \\text{(Eq. 2)}\n \n\n**Table 1. Summary Statistics and Regression Results**\n\n| Panel A: Summary Statistic (Full Sample) | Value |\n| :--- | :--- |\n| Standard Deviation of `OIB$` | 0.251 |\n\n| Panel B: Baseline Regression (Eq. 1, Full Sample) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| `OIB$t-1` | 0.0259 | 31.40 |\n\n| Panel C: Interaction Regression (Eq. 2, Sixteenths Regime) | Coefficient | t-Statistic |\n| :--- | :--- | :--- |\n| `OIB$t-1` (β₁) | 0.0148 | 7.92 |\n| `OIB$t-1 * ILD_t` (β₂) | 0.0517 | 9.53 |\n\n### Question\n\nBased on the provided model specifications and results, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the regression results provided.\n\n**Statement A**: \"In the Sixteenths Regime, the marginal effect of lagged order imbalance (`OIB$t-1`) on returns is more than four times stronger on illiquid days (`ILD_t=1`) compared to liquid days (`ILD_t=0`).\"\n\nFrom the interaction regression:\n- On liquid days (ILD_t=0): marginal effect = β₁ = 0.0148\n- On illiquid days (ILD_t=1): marginal effect = β₁ + β₂ = 0.0148 + 0.0517 = 0.0665\n- Ratio: 0.0665/0.0148 = 4.49\n\nThis is indeed more than 4 times stronger. **Statement A is correct.**\n\n**Statement B**: \"The highly significant positive coefficient on the interaction term (`β₂`) supports the hypothesis that illiquidity impedes market efficiency by allowing the price impact of order flow to persist longer.\"\n\nThe positive and significant β₂ (0.0517, t=9.53) means return predictability from lagged order flow is stronger on illiquid days. This supports the hypothesis that illiquidity impedes efficiency. **Statement B is correct.**\n\n**Statement C**: \"Based on the full-sample baseline regression, a one-standard-deviation increase in `OIB$t-1` predicts a 0.0259 basis point increase in the five-minute return.\"\n\nThe coefficient is 0.0259, and the standard deviation of OIB$ is 0.251. The predicted effect would be:\n0.0259 × 0.251 = 0.0065 (not 0.0259)\n\nThe statement incorrectly states the effect as 0.0259 basis points. **Statement C is incorrect.**\n\n**Statement D**: \"In the Sixteenths Regime, the marginal effect of lagged order imbalance on returns during an illiquid day is 0.0517.\"\n\nThe marginal effect on illiquid days is β₁ + β₂ = 0.0148 + ", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 36, "Question": "### Background\n\nAn MLP neural network is trained to forecast one-day-ahead returns. The training process involves minimizing an error function, but the stopping criterion is based on trading performance to prevent overfitting.\n\n### Data / Model Specification\n\nThe MLP is trained by adjusting its weights to minimize the Mean Squared Error (MSE) on the training data:\n\n  \nE = \\frac{1}{T}\\sum_{t=1}^{T}(y_{t}-\\tilde{y}_{t})^{2} \\quad \\text{(Eq. (1))}\n \n\nHowever, the paper states: \"Our networks... will stop training when the profit of our forecasts in the test sub-period is maximized.\" A simple trading rule generates profit `π_t = sign(ỹ_t) ⋅ y_t`.\n\n---\n\nWhich of the following statements correctly identify potential issues or valid characteristics related to this training and evaluation methodology?\n", "Options": {"A": "The profit function `π_t` is generally non-differentiable with respect to the network's output `ỹ_t`, making it unsuitable as a direct objective function for gradient-based training methods like backpropagation.", "B": "The MSE objective function penalizes large forecast errors more heavily than the profit function, which only depends on the forecast's sign.", "C": "Using test set profit as an early stopping criterion helps mitigate overfitting by selecting a model based on its generalization performance on an economic, rather than purely statistical, metric.", "D": "The set of network weights that minimizes test set MSE is guaranteed to be the same set of weights that maximizes test set profit."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the candidate's understanding of the conflict between statistical and economic objective functions in machine learning for finance, including concepts of optimization and overfitting. Strategy: Reverse-Reasoning (identifying valid statements about the methodology). Distractor Logic: Option B is correct because the `sign()` function's derivative is zero almost everywhere, providing no gradient for optimization. Option C correctly contrasts the symmetric, magnitude-sensitive nature of MSE with the sign-dependent profit function. Option D correctly identifies the purpose of the early stopping rule. Option A is a 'Conceptual Opposite' distractor; it is the core mismatch that the question explores. Minimizing large errors (MSE) is not the same as maximizing correct directional bets (profit), so the optimal weights are not guaranteed to be the same.", "qid": "36", "question": "### Background\n\nAn MLP neural network is trained to forecast one-day-ahead returns. The training process involves minimizing an error function, but the stopping criterion is based on trading performance to prevent overfitting.\n\n### Data / Model Specification\n\nThe MLP is trained by adjusting its weights to minimize the Mean Squared Error (MSE) on the training data:\n\n  \nE = \\frac{1}{T}\\sum_{t=1}^{T}(y_{t}-\\tilde{y}_{t})^{2} \\quad \\text{(Eq. (1))}\n \n\nHowever, the paper states: \"Our networks... will stop training when the profit of our forecasts in the test sub-period is maximized.\" A simple trading rule generates profit `π_t = sign(ỹ_t) ⋅ y_t`.\n\n---\n\nWhich of the following statements correctly identify potential issues or valid characteristics related to this training and evaluation methodology?\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The profit function π_t = sign(ỹ_t) · y_t is indeed non-differentiable with respect to ỹ_t because the sign function has a discontinuity at zero (its derivative is undefined at ỹ_t = 0 and zero elsewhere). This makes it unsuitable for gradient-based optimization methods like backpropagation, which require differentiable objective functions. ✓\n\nB) MSE = (y_t - ỹ_t)² penalizes errors quadratically - larger errors contribute disproportionately more to the loss. In contrast, the profit function π_t = sign(ỹ_t) · y_t only cares about whether the forecast has the correct sign (direction), not the magnitude of the error. A small error that gets the sign wrong results in negative profit, while a large error with the correct sign still yields positive profit. ✓\n\nC) Using test set profit as an early stopping criterion is a form of validation that directly measures the economic objective (trading profit) rather than just statistical fit. This can help prevent overfitting to the training data's statistical properties while ensuring the model performs well on the actual task of interest. ✓\n\nD) This statement is false. MSE minimization focuses on accurate magnitude prediction, while profit maximization only requires correct directional prediction. A model could have high MSE (poor magnitude predictions) but still achieve maximum profit if it consistently predicts the correct direction. Conversely, a model with low MSE might not maximize profit if it makes small errors that frequently result in wrong directional calls. ✗\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 305, "Question": "### Background\n\n**Research Question.** Does the presence of institutional investors mitigate the geographic diversification discount in REITs, and if so, is this effect driven by their ability to select superior firms ('selection hypothesis') or their power to influence management ('monitoring hypothesis')?\n\n**Setting.** The analysis uses two complementary empirical strategies. First, an interaction model tests whether the diversification discount varies with the level of ownership by different investor types. Second, a subsample analysis examines the discount across firms sorted by characteristics of their institutional ownership base.\n\n**Variables & Parameters.**\n*   `Tobin's q`: The dependent variable, measuring firm value (standardized).\n*   `GeographicDiversification` (`GeoDiv`): Measure of geographic diversification (standardized).\n*   `ActiveInstl Ownership`: Ownership by active institutions (e.g., investment advisors, mutual funds) (standardized).\n*   `Active Instl Number`: The number of distinct active institutions holding shares.\n*   `Instl Ownership Herf`: The Herfindahl Index of institutional ownership concentration. A higher value means more concentrated ownership.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Valuation with Ownership Interactions (Region-based)**\n| Dependent variable: Tobin's q | (1) |\n| :--- | :---: |\n| GeographicDiversification | **-0.168** |\n| | (2.49)** |\n| GeoDiv * Active Instl Ownership | **0.090** |\n| | (2.33)** |\n| Geo Div * Passive Instl Ownership | 0.023 |\n| | (0.52) |\n\n*Note: Table presents selected coefficients from a regression of Tobin's q on diversification, ownership types, and their interactions. All variables are standardized.* \n\n**Table 2. Diversification Discount Across Subgroups (Region-based)**\n| | Low Subsample | High Subsample | F-test p-value (Low vs. High) |\n| :--- | :---: | :---: | :---: |\n| **Panel A: Sort by Active Instl Number** | | |\n| `Geographic Diversification` Coeff. | -0.154 | -0.145* | 0.745 |\n| **Panel B: Sort by Instl Ownership Herf** | | |\n| `Geographic Diversification` Coeff. | -0.233*** | 0.019 | 0.013 |\n\n*Note: Table shows the coefficient on `Geographic Diversification` from separate regressions on subsamples split by the median of the sorting variable.* \n\n---\n\n### Question\n\nBased on the evidence presented in Table 1 and Table 2, which of the following conclusions about the 'monitoring' and 'selection' hypotheses are supported by the data? Select all that apply.", "Options": {"A": "The diversification discount is statistically significant only in the subsample of firms with low institutional ownership concentration, which supports the monitoring hypothesis.", "B": "The magnitude of the diversification discount does not significantly differ based on the number of active institutional investors, which weakens the selection hypothesis.", "C": "The interaction effect in Table 1 (GeoDiv * Active Instl Ownership) conclusively proves the monitoring hypothesis by itself, as active investors are by definition monitors.", "D": "Firms with highly concentrated institutional ownership (high Herfindahl) exhibit a larger diversification discount, suggesting that concentrated owners are risk-averse."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize evidence from both interaction-term and subsample-split regressions to distinguish between the 'monitoring' and 'selection' hypotheses. It uses an Atomic Decomposition strategy, breaking the paper's core causal argument into distinct, verifiable claims. Distractor (B) is an 'Almost Right' overstatement, while distractor (D) is a 'Conceptual Opposite' that directly contradicts the findings in Table 2, Panel B.", "qid": "305", "question": "### Background\n\n**Research Question.** Does the presence of institutional investors mitigate the geographic diversification discount in REITs, and if so, is this effect driven by their ability to select superior firms ('selection hypothesis') or their power to influence management ('monitoring hypothesis')?\n\n**Setting.** The analysis uses two complementary empirical strategies. First, an interaction model tests whether the diversification discount varies with the level of ownership by different investor types. Second, a subsample analysis examines the discount across firms sorted by characteristics of their institutional ownership base.\n\n**Variables & Parameters.**\n*   `Tobin's q`: The dependent variable, measuring firm value (standardized).\n*   `GeographicDiversification` (`GeoDiv`): Measure of geographic diversification (standardized).\n*   `ActiveInstl Ownership`: Ownership by active institutions (e.g., investment advisors, mutual funds) (standardized).\n*   `Active Instl Number`: The number of distinct active institutions holding shares.\n*   `Instl Ownership Herf`: The Herfindahl Index of institutional ownership concentration. A higher value means more concentrated ownership.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Valuation with Ownership Interactions (Region-based)**\n| Dependent variable: Tobin's q | (1) |\n| :--- | :---: |\n| GeographicDiversification | **-0.168** |\n| | (2.49)** |\n| GeoDiv * Active Instl Ownership | **0.090** |\n| | (2.33)** |\n| Geo Div * Passive Instl Ownership | 0.023 |\n| | (0.52) |\n\n*Note: Table presents selected coefficients from a regression of Tobin's q on diversification, ownership types, and their interactions. All variables are standardized.* \n\n**Table 2. Diversification Discount Across Subgroups (Region-based)**\n| | Low Subsample | High Subsample | F-test p-value (Low vs. High) |\n| :--- | :---: | :---: | :---: |\n| **Panel A: Sort by Active Instl Number** | | |\n| `Geographic Diversification` Coeff. | -0.154 | -0.145* | 0.745 |\n| **Panel B: Sort by Instl Ownership Herf** | | |\n| `Geographic Diversification` Coeff. | -0.233*** | 0.019 | 0.013 |\n\n*Note: Table shows the coefficient on `Geographic Diversification` from separate regressions on subsamples split by the median of the sorting variable.* \n\n---\n\n### Question\n\nBased on the evidence presented in Table 1 and Table 2, which of the following conclusions about the 'monitoring' and 'selection' hypotheses are supported by the data? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the evidence in Tables 1 and 2:\n\n**Table 1 Analysis:**\n- Geographic diversification has a negative coefficient (-0.168**), indicating a diversification discount\n- The interaction term GeoDiv * Active Instl Ownership is positive (0.090**) and significant, suggesting active institutional ownership mitigates the discount\n- The passive institutional ownership interaction is not significant\n\n**Table 2 Analysis:**\n- Panel A (sorted by Active Instl Number): The diversification discount coefficients are similar between low (-0.154) and high (-0.145*) subsamples, with no significant difference (p=0.745)\n- Panel B (sorted by Instl Ownership Herf): The low concentration subsample shows a significant discount (-0.233***), while the high concentration subsample shows no discount (0.019), with a significant difference (p=0.013)\n\n**Evaluating each option:**\n\nA) TRUE - The data shows the diversification discount is statistically significant only in the low concentration subsample (-0.233***) but not in the high concentration subsample (0.019). This supports monitoring, as concentrated owners can better monitor management.\n\nB) TRUE - Panel A shows no significant difference in the diversification discount between firms with few vs. many active institutional investors (p=0.745). If selection were driving the results, we'd expect firms with more active investors to show less discount.\n\nC) FALSE - The interaction effect alone doesn't prove monitoring. It could reflect either selection (active investors choosing better-diversified firms) or monitoring (active investors improving firm management). Additional evidence is needed.\n\nD) FALSE - The data shows the opposite: firms with highly concentrated ownership (high Herfindahl) have NO diversification discount (0.019), while firms with dispersed ownership show a large discount (-0.233***).\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 120, "Question": "### Background\n\n**Research Question.** Has the efficiency of Indonesian banks converged over time, and how did the Asian financial crisis and subsequent reforms affect the speed of this convergence?\n\n**Setting / Data-Generating Environment.** Panel data analysis of Indonesian bank efficiency is used to test for `β`-convergence (laggards catching up) before and after the 1997-1999 financial crisis.\n\n**Variables & Parameters.**\n\n*   `$\\ln(\\hat{\\hat{\\rho}}_{j,t})$`: Log of the bias-corrected efficiency score for bank `j` at time `t`.\n*   `$\\theta$`: The `β`-convergence parameter. A negative value indicates catch-up.\n\n---\n\n### Data / Model Specification\n\nThe model for `β`-convergence is given by:\n\n  \n\\ln\\hat{\\hat{\\rho}}_{j,t} - \\ln\\hat{\\hat{\\rho}}_{j,t-1} = \\phi + \\theta \\ln\\hat{\\hat{\\rho}}_{j,t} + \\nu_{j,t} \\quad \\text{(Eq. 1)}\n \n\nThis implies an autoregressive process for log efficiency: `$\\ln(\\hat{\\hat{\\rho}}_{j,t}) = c + \\lambda_1 \\ln(\\hat{\\hat{\\rho}}_{j,t-1})$`, where `$\\lambda_1 = 1/(1-\\theta)$`.\n\n**Table 1. `β`-Convergence Results (Model 2)**\n\n| Period | Parameter | Coefficient | P-value |\n| :--- | :--- | :--- | :--- |\n| 2000-2007 | `$\\theta$` | -0.8249 | 0.0000 |\n\n---\n\n### Question\n\nThe speed of `β`-convergence can be measured by the half-life (`$T_{1/2}$`), the time it takes for half of an efficiency gap to dissipate. The formula is `$T_{1/2} = -\\ln(2) / \\ln(\\lambda_1)$`, where `$\\lambda_1 = 1/(1-\\theta)$`. Using the estimated `$\\theta = -0.8249$` for the post-crisis period from Table 1, select all true statements.", "Options": {"A": "A smaller absolute value of `$\\theta$` (e.g., -0.5 instead of -0.8249) would imply a shorter half-life and faster convergence.", "B": "The value of `$\\lambda_1$` implies that, on average, an inefficient bank closes less than 30% of its efficiency gap each year.", "C": "The implied coefficient on the lagged efficiency term, `$\\lambda_1$`, is approximately 0.548.", "D": "The calculated half-life of an efficiency gap is approximately 1.15 years."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Computational Judgment strategy to test the quantitative implications of the convergence parameter. It breaks the calculation into an intermediate step (calculating `$\\lambda_1$`) and a final step (calculating the half-life), allowing for multiple correct assertions. Distractor C is a step-omission error (misinterpreting `$\\lambda_1$`). Distractor D is a conceptual opposite regarding the relationship between the parameter's magnitude and the speed of convergence.", "qid": "120", "question": "### Background\n\n**Research Question.** Has the efficiency of Indonesian banks converged over time, and how did the Asian financial crisis and subsequent reforms affect the speed of this convergence?\n\n**Setting / Data-Generating Environment.** Panel data analysis of Indonesian bank efficiency is used to test for `β`-convergence (laggards catching up) before and after the 1997-1999 financial crisis.\n\n**Variables & Parameters.**\n\n*   `$\\ln(\\hat{\\hat{\\rho}}_{j,t})$`: Log of the bias-corrected efficiency score for bank `j` at time `t`.\n*   `$\\theta$`: The `β`-convergence parameter. A negative value indicates catch-up.\n\n---\n\n### Data / Model Specification\n\nThe model for `β`-convergence is given by:\n\n  \n\\ln\\hat{\\hat{\\rho}}_{j,t} - \\ln\\hat{\\hat{\\rho}}_{j,t-1} = \\phi + \\theta \\ln\\hat{\\hat{\\rho}}_{j,t} + \\nu_{j,t} \\quad \\text{(Eq. 1)}\n \n\nThis implies an autoregressive process for log efficiency: `$\\ln(\\hat{\\hat{\\rho}}_{j,t}) = c + \\lambda_1 \\ln(\\hat{\\hat{\\rho}}_{j,t-1})$`, where `$\\lambda_1 = 1/(1-\\theta)$`.\n\n**Table 1. `β`-Convergence Results (Model 2)**\n\n| Period | Parameter | Coefficient | P-value |\n| :--- | :--- | :--- | :--- |\n| 2000-2007 | `$\\theta$` | -0.8249 | 0.0000 |\n\n---\n\n### Question\n\nThe speed of `β`-convergence can be measured by the half-life (`$T_{1/2}$`), the time it takes for half of an efficiency gap to dissipate. The formula is `$T_{1/2} = -\\ln(2) / \\ln(\\lambda_1)$`, where `$\\lambda_1 = 1/(1-\\theta)$`. Using the estimated `$\\theta = -0.8249$` for the post-crisis period from Table 1, select all true statements.", "model_evaluation_result": {"model_raw_response": "I need to analyze the β-convergence model and calculate the relevant parameters.\n\nGiven information:\n- θ = -0.8249 (from Table 1)\n- λ₁ = 1/(1-θ)\n- Half-life formula: T₁/₂ = -ln(2)/ln(λ₁)\n\nFirst, let me calculate λ₁:\nλ₁ = 1/(1-θ) = 1/(1-(-0.8249)) = 1/(1+0.8249) = 1/1.8249 ≈ 0.548\n\nNow, let me calculate the half-life:\nT₁/₂ = -ln(2)/ln(λ₁) = -ln(2)/ln(0.548) = -0.693/(-0.601) ≈ 1.15 years\n\nLet me evaluate each option:\n\nA) \"A smaller absolute value of θ (e.g., -0.5 instead of -0.8249) would imply a shorter half-life and faster convergence.\"\n- If θ = -0.5: λ₁ = 1/(1-(-0.5)) = 1/1.5 = 0.667\n- T₁/₂ = -ln(2)/ln(0.667) = -0.693/(-0.405) ≈ 1.71 years\n- This is a LONGER half-life, not shorter. So this statement is FALSE.\n\nB) \"The value of λ₁ implies that, on average, an inefficient bank closes less than 30% of its efficiency gap each year.\"\n- λ₁ ≈ 0.548 means about 54.8% of the previous period's efficiency level persists\n- This means 1 - 0.548 = 0.452 or about 45.2% of the gap is closed each year\n- Since 45.2% > 30%, this statement is FALSE.\n\nC) \"The implied coefficient on the lagged efficiency term, λ₁, is approximately 0.548.\"\n- As calculated above: λ", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 451, "Question": "Background\n\nStandard risk models, such as 'Normal VaR', assume that asset returns follow a normal distribution. A normal distribution is symmetric (skewness = 0) and has a kurtosis of 3.\n\nData / Model Specification\n\nThe paper analyzes daily returns for Canadian crude oil and reports two key statistical properties:\n1.  The sample kurtosis is 16.2.\n2.  The return distribution 'skews to the left' (i.e., has negative skewness).\n\n---\n\nQuestion\n\nGiven these two empirical findings, which of the following are valid critiques of using a standard Normal VaR model for this market? (Select all that apply)", "Options": {"A": "The model would systematically overestimate the average daily return of crude oil.", "B": "The model would be overly conservative, systematically overstating the potential for extreme losses.", "C": "The model would fail to capture the asymmetric nature of risk, where large losses are more likely or more severe than large gains.", "D": "The model would underestimate the frequency and magnitude of extreme price movements because the actual distribution has 'fat tails' (leptokurtosis)."}, "Answer": ["C", "D"], "pi_justification": "Assessment Target: Tests the ability to interpret kurtosis and skewness and critique a model based on violations of its assumptions. Strategy: Select-all-valid consequences. Distractor Logic: (A) Correct: Kurtosis > 3 implies fat tails (leptokurtosis), meaning a normal model underestimates extreme events. (B) Correct: Negative skewness means the left (loss) tail is fatter/longer than the right, an asymmetry a normal model ignores. (C) Conceptual Confusion: Incorrectly links higher-order moments (kurtosis, skew) to the first moment (mean/average return). (D) Conceptual Opposite: The normal model would be insufficiently conservative, underestimating risk, not overestimating it.", "qid": "451", "question": "Background\n\nStandard risk models, such as 'Normal VaR', assume that asset returns follow a normal distribution. A normal distribution is symmetric (skewness = 0) and has a kurtosis of 3.\n\nData / Model Specification\n\nThe paper analyzes daily returns for Canadian crude oil and reports two key statistical properties:\n1.  The sample kurtosis is 16.2.\n2.  The return distribution 'skews to the left' (i.e., has negative skewness).\n\n---\n\nQuestion\n\nGiven these two empirical findings, which of the following are valid critiques of using a standard Normal VaR model for this market? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "To determine the valid critiques of using a Normal VaR model for Canadian crude oil returns, I need to analyze how the empirical findings differ from the normal distribution assumptions.\n\nGiven empirical findings:\n- Sample kurtosis = 16.2 (much higher than 3)\n- Negative skewness (left-skewed distribution)\n\nNormal distribution assumptions:\n- Kurtosis = 3\n- Skewness = 0 (symmetric)\n\nLet me evaluate each option:\n\nA) \"The model would systematically overestimate the average daily return of crude oil.\"\n- This is about the mean/expected return, not about risk measurement\n- Skewness affects the shape of the distribution, not necessarily the mean\n- VaR models focus on tail risk, not average returns\n- This is NOT a valid critique\n\nB) \"The model would be overly conservative, systematically overstating the potential for extreme losses.\"\n- High kurtosis (16.2 vs 3) means fat tails, so extreme events are MORE likely\n- Negative skewness means the left tail (losses) is longer/fatter\n- A normal distribution would UNDERESTIMATE, not overstate, extreme losses\n- This is NOT a valid critique\n\nC) \"The model would fail to capture the asymmetric nature of risk, where large losses are more likely or more severe than large gains.\"\n- The data shows negative skewness (left-skewed)\n- This means the loss tail is longer/fatter than the gain tail\n- Normal distribution assumes symmetry (skewness = 0)\n- The model would indeed fail to capture this asymmetry\n- This IS a valid critique\n\nD) \"The model would underestimate the frequency and magnitude of extreme price movements because the actual distribution has 'fat tails' (leptokurtosis).\"\n- Kurtosis of 16.2 >> 3 indicates leptokurtosis (fat tails)\n- Fat tails mean extreme events occur more frequently than normal distribution predicts\n- Normal VaR would underestimate both frequency and magnitude of extreme moves\n- This IS a valid critique\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 158, "Question": "### Background\n\nA study investigates whether incumbent banks use branch network expansion as a strategy to deter the entry of competitors, particularly multimarket (MM) banks. The analysis uses a two-stage model. The first stage models the entry decisions of firms, while the second stage models the number of branches an incumbent firm operates. To address the endogeneity of market structure (i.e., the number of competitors), the second-stage model is estimated in two ways: an \"Unadjusted\" version that ignores endogeneity and an \"Adjusted\" version that includes correction terms derived from the first stage.\n\n### Data / Model Specification\n\nThe key output from the second-stage branching model for incumbent multimarket banks is presented below.\n\n**Table 1: Second-Stage Marginal Effects on Number of Branches per Multimarket Bank**\n\n| Variable | Unadjusted Model (Biased) | Adjusted Model (Corrected) |\n| :--- | :---: | :---: |\n| Effect of one additional MM Competitor | -0.048 | 0.159 |\n| | (-3.79) | (5.85) |\n| Coefficient on MM market presence unobservable | --- | -0.100 |\n| | | (-8.57) |\n\n*Note: t-statistics are in parentheses.* \n\n---\n\n### Question\n\nBased on the results in **Table 1**, select all statements that are supported by the paper's analysis.", "Options": {"A": "The negative coefficient on the 'MM market presence unobservable' is consistent with a story of strategic deterrence, where incumbents preemptively build more branches in markets that are otherwise prime targets for MM bank entry.", "B": "The sign reversal for the 'Effect of one additional MM Competitor' from negative to positive after adjustment suggests that a naive analysis would incorrectly conclude that MM competition suppresses branch expansion.", "C": "The negative coefficient on the 'MM market presence unobservable' in the adjusted model indicates that markets inherently attractive to MM banks are also inherently unattractive for building branches.", "D": "The positive adjusted effect of MM competitors implies that MM banks likely collude to mutually expand their branch networks when present in the same market."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the student's ability to interpret the core results of the endogeneity correction. It uses an 'Atomic Decomposition' strategy, breaking the complex finding into two key, independently verifiable interpretations. Distractor (B) presents a conceptual opposite, misinterpreting the strategic interaction as a simple negative correlation in market characteristics. Distractor (D) offers an alternative, incorrect strategic explanation (collusion vs. competition) for the observed positive coefficient.", "qid": "158", "question": "### Background\n\nA study investigates whether incumbent banks use branch network expansion as a strategy to deter the entry of competitors, particularly multimarket (MM) banks. The analysis uses a two-stage model. The first stage models the entry decisions of firms, while the second stage models the number of branches an incumbent firm operates. To address the endogeneity of market structure (i.e., the number of competitors), the second-stage model is estimated in two ways: an \"Unadjusted\" version that ignores endogeneity and an \"Adjusted\" version that includes correction terms derived from the first stage.\n\n### Data / Model Specification\n\nThe key output from the second-stage branching model for incumbent multimarket banks is presented below.\n\n**Table 1: Second-Stage Marginal Effects on Number of Branches per Multimarket Bank**\n\n| Variable | Unadjusted Model (Biased) | Adjusted Model (Corrected) |\n| :--- | :---: | :---: |\n| Effect of one additional MM Competitor | -0.048 | 0.159 |\n| | (-3.79) | (5.85) |\n| Coefficient on MM market presence unobservable | --- | -0.100 |\n| | | (-8.57) |\n\n*Note: t-statistics are in parentheses.* \n\n---\n\n### Question\n\nBased on the results in **Table 1**, select all statements that are supported by the paper's analysis.", "model_evaluation_result": {"model_raw_response": "Looking at the results in Table 1, I need to analyze what the coefficients tell us about incumbent bank behavior and the relationship between competition and branch expansion.\n\nThe key findings are:\n1. The unadjusted model shows a negative effect (-0.048) of MM competitors on branches\n2. The adjusted model shows a positive effect (0.159) after correcting for endogeneity\n3. The adjustment includes a negative coefficient (-0.100) on \"MM market presence unobservable\"\n\nLet me evaluate each option:\n\n**Option A**: The negative coefficient on 'MM market presence unobservable' (-0.100) represents factors that make a market attractive to MM banks but are unobserved by the researcher. The negative sign means that in markets with high unobserved attractiveness to MM banks, incumbents operate MORE branches (negative effect on a positive unobservable). This is consistent with strategic deterrence - incumbents build more branches precisely in markets that would otherwise attract MM entry. ✓\n\n**Option B**: The unadjusted model shows -0.048 (negative), while the adjusted model shows +0.159 (positive). This sign reversal demonstrates that ignoring endogeneity leads to the wrong conclusion. Without adjustment, one would think MM competition reduces branching, when in fact it increases it. ✓\n\n**Option C**: This misinterprets the coefficient. The negative coefficient means incumbents build MORE branches (not fewer) in markets attractive to MM banks, which contradicts this statement. ✗\n\n**Option D**: The positive adjusted effect simply means actual MM competition leads to more branches by incumbents. This could be due to competitive response, not necessarily collusion. The paper's framework is about strategic deterrence, not collusion. ✗\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 444, "Question": "### Background\n\n**Research Question.** How can the theoretical predictions of the maturity mismatch model—that capital flow volatility negatively impacts investment primarily in economies with underdeveloped financial markets—be tested empirically?\n\n**Setting & Data.** The analysis uses a panel dataset for 16 emerging markets (EMs) and 11 small open advanced economies (AEs) to identify the impact of capital flow volatility on investment.\n\n### Data / Model Specification\n\n**Volatility Measure Construction.**\nThe key independent variable, capital flow volatility, is constructed in a two-step process. First, an AR(1) model is estimated on monthly gross capital inflows (`flow_{i,m}`) to extract the unpredictable component (`e_{i,m}`):\n\n  \nflow_{i,m} = \\rho_{0,i} + \\rho_{1,i} flow_{i,m-1} + e_{i,m} \\quad \\text{(Eq. (1))}\n \n\nSecond, volatility (`Vol_{i,m}`) is calculated as the 12-month trailing standard deviation of the estimated residuals (`\\widehat{e}_{i,m}`), normalized by trend GDP:\n\n  \nVol_{i,m} = \\frac{\\sqrt{\\frac{1}{12}\\sum_{j=0}^{11}\\widehat{e}_{i,m-j}^{2}}}{Trend~GDP_{i,m}} \\quad \\text{(Eq. (2))}\n \n\n**Key Empirical Results.**\nThe paper's core findings come from panel regressions of investment on lagged capital flows and their volatility. A summary of key results is provided in Table 1.\n\n**Table 1: Summary of Panel Regression Results on Investment**\n| Specification | Sample | Key Regressor | Estimated Coefficient |\n| :--- | :--- | :--- | :--- |\n| 1 | Emerging Markets | Volatility (Total Portfolio) | -1.581*** |\n| 2 | Advanced Economies | Volatility (Total Portfolio) | 0.107 |\n*Note: `***` denotes statistical significance at the 1% level.* \n\n### Question\n\nThe paper's empirical strategy relies on a specific construction of the volatility measure and a comparison between emerging and advanced economies. Based on the information provided, which of the following statements accurately describe the rationale or interpretation of this approach?", "Options": {"A": "Normalizing volatility by trend GDP (Eq. 2) is done to amplify the measure's effect in smaller economies, making it easier to achieve statistical significance.", "B": "The results primarily indicate that advanced economies are more resilient to shocks simply because their economies are larger and more diversified.", "C": "The starkly different coefficients for volatility in emerging markets (-1.581***) versus advanced economies (0.107) support the hypothesis that a structural feature, like the prevalence of maturity mismatch, is necessary for volatility to negatively impact investment.", "D": "The use of residuals from an AR(1) model (Eq. 1) is intended to isolate the unpredictable component of capital flows, which represents genuine uncertainty for firms."}, "Answer": ["C", "D"], "pi_justification": "This item assesses the student's understanding of the foundational empirical methodology. It uses an Atomic Decomposition strategy to test two key concepts: the rationale for variable construction (isolating shocks) and the interpretation of the primary cross-country evidence. Distractor C (Conceptual Opposite) presents an incorrect rationale for normalization. Distractor D (True but Irrelevant) offers a plausible but less precise explanation that misses the paper's core mechanism, testing for careful interpretation.", "qid": "444", "question": "### Background\n\n**Research Question.** How can the theoretical predictions of the maturity mismatch model—that capital flow volatility negatively impacts investment primarily in economies with underdeveloped financial markets—be tested empirically?\n\n**Setting & Data.** The analysis uses a panel dataset for 16 emerging markets (EMs) and 11 small open advanced economies (AEs) to identify the impact of capital flow volatility on investment.\n\n### Data / Model Specification\n\n**Volatility Measure Construction.**\nThe key independent variable, capital flow volatility, is constructed in a two-step process. First, an AR(1) model is estimated on monthly gross capital inflows (`flow_{i,m}`) to extract the unpredictable component (`e_{i,m}`):\n\n  \nflow_{i,m} = \\rho_{0,i} + \\rho_{1,i} flow_{i,m-1} + e_{i,m} \\quad \\text{(Eq. (1))}\n \n\nSecond, volatility (`Vol_{i,m}`) is calculated as the 12-month trailing standard deviation of the estimated residuals (`\\widehat{e}_{i,m}`), normalized by trend GDP:\n\n  \nVol_{i,m} = \\frac{\\sqrt{\\frac{1}{12}\\sum_{j=0}^{11}\\widehat{e}_{i,m-j}^{2}}}{Trend~GDP_{i,m}} \\quad \\text{(Eq. (2))}\n \n\n**Key Empirical Results.**\nThe paper's core findings come from panel regressions of investment on lagged capital flows and their volatility. A summary of key results is provided in Table 1.\n\n**Table 1: Summary of Panel Regression Results on Investment**\n| Specification | Sample | Key Regressor | Estimated Coefficient |\n| :--- | :--- | :--- | :--- |\n| 1 | Emerging Markets | Volatility (Total Portfolio) | -1.581*** |\n| 2 | Advanced Economies | Volatility (Total Portfolio) | 0.107 |\n*Note: `***` denotes statistical significance at the 1% level.* \n\n### Question\n\nThe paper's empirical strategy relies on a specific construction of the volatility measure and a comparison between emerging and advanced economies. Based on the information provided, which of the following statements accurately describe the rationale or interpretation of this approach?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement about the empirical strategy's rationale and interpretation.\n\nA) \"Normalizing volatility by trend GDP (Eq. 2) is done to amplify the measure's effect in smaller economies, making it easier to achieve statistical significance.\"\nThis is incorrect. Normalization by trend GDP is a standard practice to make volatility measures comparable across countries of different sizes. It doesn't amplify effects in smaller economies - it actually scales them appropriately. The purpose is comparability, not manipulation of statistical significance.\n\nB) \"The results primarily indicate that advanced economies are more resilient to shocks simply because their economies are larger and more diversified.\"\nThis is incorrect. The paper specifically tests the maturity mismatch hypothesis related to financial market development, not just economy size or diversification. The differential impact is attributed to structural features of financial markets, not merely economic scale.\n\nC) \"The starkly different coefficients for volatility in emerging markets (-1.581***) versus advanced economies (0.107) support the hypothesis that a structural feature, like the prevalence of maturity mismatch, is necessary for volatility to negatively impact investment.\"\nThis is correct. The large negative and statistically significant coefficient for emerging markets contrasted with the small, insignificant coefficient for advanced economies directly supports the theoretical prediction that underdeveloped financial markets (which lead to maturity mismatch) are necessary for capital flow volatility to harm investment.\n\nD) \"The use of residuals from an AR(1) model (Eq. 1) is intended to isolate the unpredictable component of capital flows, which represents genuine uncertainty for firms.\"\nThis is correct. The AR(1) model removes the predictable component of capital flows, leaving only the unexpected shocks in the residuals. This unpredictable component represents true uncertainty that firms cannot anticipate, making it the relevant measure for testing how volatility affects investment decisions.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 362, "Question": "### Background\n\n**Research Question.** How can Monte Carlo simulation be used to price options under stochastic volatility, particularly when the correlation `ρ` between the stock price and volatility is non-zero?\n\n**Setting / Data-Generating Environment.** To price an option when `ρ≠0`, one must simulate the joint path of the stock price `S` and variance `V` in a risk-neutral world. The simulation can be improved with variance reduction techniques.\n\n---\n\n### Data / Model Specification\n\nThe joint processes are discretized using two independent standard normal random variates, `uᵢ` and `vᵢ`:\n  \nS_{i} = S_{i-1} \\exp\\left[ (r - V_{i-1}/2)\\Delta t + \\sqrt{V_{i-1}\\Delta t} \\cdot u_i \\right] \\quad \\text{(Eq. (1))}\n \n  \nV_{i} = V_{i-1} \\exp\\left[ (\\mu - \\xi^2/2)\\Delta t + \\xi\\sqrt{\\Delta t} \\cdot (\\rho u_i + \\sqrt{1-\\rho^2}v_i) \\right] \\quad \\text{(Eq. (2))}\n \nTwo variance reduction techniques mentioned are:\n1.  **Antithetic Variates:** Using the opposite random shocks (e.g., `-uᵢ`, `-vᵢ`) to generate a second, negatively correlated price estimate.\n2.  **Control Variates:** Using a related variable with a known mean (like a standard B-S option price) that is highly correlated with the variable of interest to reduce variance.\n\n---\n\n### Question\n\nWhen implementing the Monte Carlo simulation for the `ρ≠0` case, which of the following statements about the procedure and its refinement are correct? (Select all that apply)", "Options": {"A": "The shock to the variance process in Eq. (2) is constructed using two independent normal variates (`uᵢ`, `vᵢ`) to ensure the final shocks to `S` and `V` have the specified correlation `ρ`.", "B": "The control variate technique is effective because the primary driver of the option's value (the stock price path, driven by `uᵢ`) is shared between the stochastic volatility price and the standard B-S price, inducing high positive correlation.", "C": "The antithetic variate technique involves using shocks `-uᵢ` and `-vᵢ` because this generates a new price estimate that is independent of the original, reducing variance when averaged.", "D": "The discretized stock price `Sᵢ` in Eq. (1) depends on the correlated shock `ρuᵢ + sqrt(1-ρ²)vᵢ`."}, "Answer": ["A", "B"], "pi_justification": "This item tests the procedural details of the advanced simulation. It is a 'select-all-valid' task. Option A tests the construction of correlated shocks. Option B tests the statistical intuition for the control variate method. Distractors are based on a 'Conceptual Opposite' error for variance reduction (C) and a 'Formula Misuse' error in applying the shocks (D).", "qid": "362", "question": "### Background\n\n**Research Question.** How can Monte Carlo simulation be used to price options under stochastic volatility, particularly when the correlation `ρ` between the stock price and volatility is non-zero?\n\n**Setting / Data-Generating Environment.** To price an option when `ρ≠0`, one must simulate the joint path of the stock price `S` and variance `V` in a risk-neutral world. The simulation can be improved with variance reduction techniques.\n\n---\n\n### Data / Model Specification\n\nThe joint processes are discretized using two independent standard normal random variates, `uᵢ` and `vᵢ`:\n  \nS_{i} = S_{i-1} \\exp\\left[ (r - V_{i-1}/2)\\Delta t + \\sqrt{V_{i-1}\\Delta t} \\cdot u_i \\right] \\quad \\text{(Eq. (1))}\n \n  \nV_{i} = V_{i-1} \\exp\\left[ (\\mu - \\xi^2/2)\\Delta t + \\xi\\sqrt{\\Delta t} \\cdot (\\rho u_i + \\sqrt{1-\\rho^2}v_i) \\right] \\quad \\text{(Eq. (2))}\n \nTwo variance reduction techniques mentioned are:\n1.  **Antithetic Variates:** Using the opposite random shocks (e.g., `-uᵢ`, `-vᵢ`) to generate a second, negatively correlated price estimate.\n2.  **Control Variates:** Using a related variable with a known mean (like a standard B-S option price) that is highly correlated with the variable of interest to reduce variance.\n\n---\n\n### Question\n\nWhen implementing the Monte Carlo simulation for the `ρ≠0` case, which of the following statements about the procedure and its refinement are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) **Correct**. The variance process in Eq. (2) uses the construction `ρu_i + √(1-ρ²)v_i` where `u_i` and `v_i` are independent standard normal variates. This construction ensures that the shock to variance has correlation `ρ` with the shock to the stock price (which uses `u_i`).\n\nB) **Correct**. The control variate technique works well here because both the stochastic volatility option price and the Black-Scholes price share the same stock price path driven by `u_i`. This creates high positive correlation between them, making the Black-Scholes price an effective control variate.\n\nC) **Incorrect**. Antithetic variates generate negatively correlated (not independent) price estimates. The technique works by creating paths that move in opposite directions, which when averaged together reduce variance through negative correlation, not independence.\n\nD) **Incorrect**. Looking at Eq. (1), the stock price `S_i` depends only on `u_i`, not on the correlated shock `ρu_i + √(1-ρ²)v_i`. The correlated shock appears only in the variance equation (Eq. 2).\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 311, "Question": "### Background\n\n**Research Question.** How did the composition of foreign direct investment (FDI) into Argentina after its 1991 reforms drive a fundamental shift in corporate ownership, and what were the implications for corporate governance?\n\n**Setting / Data-Generating Environment.** The analysis focuses on Argentina during its decade of intense economic liberalization (1990-1999). Sweeping reforms attracted a massive inflow of foreign capital, leading to a dramatic change in the ownership structure of the country's top corporations, which had previously been dominated by domestic families and the state. This shift from concentrated family/state ownership to control by foreign corporations and investment funds had profound effects on prevailing corporate governance practices.\n\n### Data / Model Specification\n\nThe composition of foreign investment in Argentina from 1990 to 1999 is detailed in Table 1.\n\n**Table 1: Foreign Investment in Argentina from 1990–1999 (in millions of dollars)**\n\n| Category      | 1990-1996 | 1997   | 1998   | 1999    | Total      |\n|:--------------|:----------|:-------|:-------|:--------|:-----------|\n| M&A           | 8,131.3   | 11,792.5 | 9,120.7  | 19,507.5  | 48,552.0   |\n| Privatizations| 8,763.0   | 1,111.2  | 442.5    | 5,926.7   | 16,243.3   |\n| Greenfield    | 5,552.4   | 4,143.3  | 6,263.9  | 5,194.7   | 21,154.3   |\n| Expansion     | 18,258.0  | 7,453.5  | 7,670.6  | 7,272.6   | 40,654.7   |\n| **Total**     | 40,704.7  | 24,500.5 | 23,497.6 | 37,901.5  | 126,604.3  |\n\nThis influx of capital resulted in a significant ownership transition, as exemplified by the cases in Table 2.\n\n**Table 2: Examples of Ownership Transition in Top Argentine Corporations**\n\n| Corporation     | Sector       | Pre-1991 Controlling Shareholders | Post-1991 Controlling Shareholders |\n|:----------------|:-------------|:----------------------------------|:-----------------------------------|\n| YPF             | Oil & Gas    | State                             | Repsol (Spain)                     |\n| Banco Francés   | Banking      | Domestic Families                 | Banco Bilbao Vizcaya (Spain)       |\n| Norte           | Supermarkets | Domestic                          | Promodes (Carrefour France)        |\n\n### Question\n\nBased on the provided data and context, select ALL statements that are factually correct and supported by the analysis.", "Options": {"A": "Greenfield investments, which involve building new facilities from scratch, represented over 20% of the total foreign investment.", "B": "Investment in Privatizations was the single largest category of foreign investment, indicating the primary focus was on acquiring state assets.", "C": "The combined investment in Mergers & Acquisitions (M&A) and Expansion accounted for more than two-thirds of the total foreign investment from 1990-1999.", "D": "Investment related to Privatizations constituted less than 15% of the total foreign capital inflow during the period."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform accurate calculations from tabular data and draw direct, first-order conclusions. It uses a Computational Judgment strategy. The correct options (A, C) are derived from straightforward percentage calculations (70.5% and 12.8%, respectively). Distractor B presents a plausible but incorrect interpretation, testing for careful reading against the data. Distractor D is based on a common numerical misjudgment (the correct value is 16.7%), creating a high-fidelity numerical distractor.", "qid": "311", "question": "### Background\n\n**Research Question.** How did the composition of foreign direct investment (FDI) into Argentina after its 1991 reforms drive a fundamental shift in corporate ownership, and what were the implications for corporate governance?\n\n**Setting / Data-Generating Environment.** The analysis focuses on Argentina during its decade of intense economic liberalization (1990-1999). Sweeping reforms attracted a massive inflow of foreign capital, leading to a dramatic change in the ownership structure of the country's top corporations, which had previously been dominated by domestic families and the state. This shift from concentrated family/state ownership to control by foreign corporations and investment funds had profound effects on prevailing corporate governance practices.\n\n### Data / Model Specification\n\nThe composition of foreign investment in Argentina from 1990 to 1999 is detailed in Table 1.\n\n**Table 1: Foreign Investment in Argentina from 1990–1999 (in millions of dollars)**\n\n| Category      | 1990-1996 | 1997   | 1998   | 1999    | Total      |\n|:--------------|:----------|:-------|:-------|:--------|:-----------|\n| M&A           | 8,131.3   | 11,792.5 | 9,120.7  | 19,507.5  | 48,552.0   |\n| Privatizations| 8,763.0   | 1,111.2  | 442.5    | 5,926.7   | 16,243.3   |\n| Greenfield    | 5,552.4   | 4,143.3  | 6,263.9  | 5,194.7   | 21,154.3   |\n| Expansion     | 18,258.0  | 7,453.5  | 7,670.6  | 7,272.6   | 40,654.7   |\n| **Total**     | 40,704.7  | 24,500.5 | 23,497.6 | 37,901.5  | 126,604.3  |\n\nThis influx of capital resulted in a significant ownership transition, as exemplified by the cases in Table 2.\n\n**Table 2: Examples of Ownership Transition in Top Argentine Corporations**\n\n| Corporation     | Sector       | Pre-1991 Controlling Shareholders | Post-1991 Controlling Shareholders |\n|:----------------|:-------------|:----------------------------------|:-----------------------------------|\n| YPF             | Oil & Gas    | State                             | Repsol (Spain)                     |\n| Banco Francés   | Banking      | Domestic Families                 | Banco Bilbao Vizcaya (Spain)       |\n| Norte           | Supermarkets | Domestic                          | Promodes (Carrefour France)        |\n\n### Question\n\nBased on the provided data and context, select ALL statements that are factually correct and supported by the analysis.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to calculate percentages for each category relative to the total foreign investment of $126,604.3 million.\n\nLet me verify each statement:\n\n**Statement A**: Greenfield investments = $21,154.3 million\nPercentage = ($21,154.3 / $126,604.3) × 100 = 16.7%\nThis is less than 20%, so Statement A is FALSE.\n\n**Statement B**: Privatizations = $16,243.3 million\nThis is not the largest category. M&A ($48,552.0 million) is the largest, so Statement B is FALSE.\n\n**Statement C**: M&A + Expansion = $48,552.0 + $40,654.7 = $89,206.7 million\nPercentage = ($89,206.7 / $126,604.3) × 100 = 70.5%\nThis is more than two-thirds (66.67%), so Statement C is TRUE.\n\n**Statement D**: Privatizations = $16,243.3 million\nPercentage = ($16,243.3 / $126,604.3) × 100 = 12.8%\nThis is less than 15%, so Statement D is TRUE.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 445, "Question": "### Background\n\n**Research Question.** A key empirical challenge is to distinguish the paper's proposed 'Maturity Mismatch' (MM) channel from an alternative 'Demand Uncertainty' (DU) channel. The DU hypothesis posits that capital flow volatility merely proxies for uncertainty about future demand for the country's exports, which would also dampen investment.\n\n**Setting & Data.** To distinguish these channels, the analysis disaggregates total portfolio flows into debt and equity flows for the emerging market sample.\n\n### Data / Model Specification\n\n**Key Empirical Results.**\nThe paper estimates the impact of the volatility of debt flows and equity flows separately on investment. A summary of these results is provided in Table 1.\n\n**Table 1: Disaggregated Regression Results on Investment (Emerging Markets)**\n| Specification | Key Regressor | Estimated Coefficient |\n| :--- | :--- | :--- |\n| 1 | Volatility (Debt Flows) | -3.162*** |\n| 2 | Volatility (Equity Flows) | 0.315 |\n*Note: `***` denotes statistical significance at the 1% level.* \n\n### Question\n\nBased on the competing hypotheses and the results in Table 1, which of the following conclusions are supported?", "Options": {"A": "The positive coefficient on equity flow volatility (0.315), though insignificant, suggests that equity market uncertainty might be beneficial for investment.", "B": "The empirical findings provide strong evidence against the Demand Uncertainty channel being the primary driver, as the volatility of both flow types would be expected to capture such uncertainty.", "C": "The Maturity Mismatch hypothesis predicts that only the volatility of debt flows should negatively impact investment, as equity financing does not create rollover risk.", "D": "The results imply that both financial (debt) and real (demand) uncertainty channels are at play, as the test is unable to definitively separate their effects."}, "Answer": ["B", "C"], "pi_justification": "This item assesses the ability to interpret a 'horse race' regression designed to distinguish between competing causal mechanisms, a core skill in empirical analysis. It uses a Reverse-Reasoning strategy where the student must infer which hypothesis is supported by the evidence. Options A and B correctly state the prediction of the target hypothesis and the conclusion from the evidence. Distractor C (Step-Omission Error) tests for the common error of over-interpreting insignificant coefficients. Distractor D (Conceptual Opposite) presents a false conclusion of equivalence between channels, contrary to the differentiating evidence.", "qid": "445", "question": "### Background\n\n**Research Question.** A key empirical challenge is to distinguish the paper's proposed 'Maturity Mismatch' (MM) channel from an alternative 'Demand Uncertainty' (DU) channel. The DU hypothesis posits that capital flow volatility merely proxies for uncertainty about future demand for the country's exports, which would also dampen investment.\n\n**Setting & Data.** To distinguish these channels, the analysis disaggregates total portfolio flows into debt and equity flows for the emerging market sample.\n\n### Data / Model Specification\n\n**Key Empirical Results.**\nThe paper estimates the impact of the volatility of debt flows and equity flows separately on investment. A summary of these results is provided in Table 1.\n\n**Table 1: Disaggregated Regression Results on Investment (Emerging Markets)**\n| Specification | Key Regressor | Estimated Coefficient |\n| :--- | :--- | :--- |\n| 1 | Volatility (Debt Flows) | -3.162*** |\n| 2 | Volatility (Equity Flows) | 0.315 |\n*Note: `***` denotes statistical significance at the 1% level.* \n\n### Question\n\nBased on the competing hypotheses and the results in Table 1, which of the following conclusions are supported?", "model_evaluation_result": {"model_raw_response": "Looking at the empirical results and the competing hypotheses:\n\nThe key finding is that debt flow volatility has a significant negative effect on investment (-3.162***), while equity flow volatility has an insignificant positive effect (0.315).\n\nLet me evaluate each option:\n\nA) This is incorrect. The positive coefficient is statistically insignificant, so we cannot conclude that equity market uncertainty is beneficial for investment. This would be an overinterpretation of insignificant results.\n\nB) This is correct. Under the Demand Uncertainty (DU) channel, both debt and equity flow volatility should proxy for the same underlying demand uncertainty and thus have similar negative effects on investment. The stark difference in results (significant negative for debt, insignificant for equity) provides evidence against DU being the primary driver.\n\nC) This is correct. The Maturity Mismatch (MM) hypothesis specifically predicts that debt flows create rollover risk due to maturity mismatches, while equity financing (which has no maturity) does not create such risks. The results showing only debt flow volatility matters aligns perfectly with the MM prediction.\n\nD) This is incorrect. The results actually help distinguish between the channels rather than showing both are at play. The differential impact of debt versus equity flow volatility supports MM over DU.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 416, "Question": "### Background\n\n**Research Question.** Are the earnings releases that precede a seasoned equity offering (SEO) systematically different in their information content and news valence compared to other earnings releases?\n\n**Setting / Data-Generating Environment.** The study analyzes abnormal returns and return volatility around quarterly earnings releases for firms that conduct an SEO, comparing the quarters before the issue to the quarters after. The theory predicts that firms with good news will delay their issue until after the news is released, and this incentive is strongest when the news is most informative.\n\n**Variables & Parameters.**\n- `Average abnormal return`: The average two-day cumulative abnormal return for firms around an earnings release.\n- `Average excess variance ratio`: A measure of the informativeness of an earnings release, where higher values indicate abnormally high volatility and information content.\n\n### Data / Model Specification\n\n**Table 1. Abnormal Returns and Informativeness at Earnings Release Dates**\n\n| Quarters relative to issue day | -2 | -1 | 1 | 2 |\n|:------------------------------|:-----|:-----|:------|:------|\n| Average abnormal return (%)   | 1.20 | 0.33 | 0.81  | -0.10 |\n| (t-statistic)                 | (6.10)| (1.85)| (4.77)|(-0.52)|\n| Average excess variance ratio | 0.61 | 0.79 | 0.53  | 0.52  |\n| (t-statistic)                 | (4.06)| (5.06)| (5.02)| (4.96)|\n\n*Source: Adapted from the original paper's Table 5.*\n\n### Question\n\nThe theory of time-varying adverse selection predicts that firms self-select when to issue equity based on their private information. Based on the results in Table 1, select all conclusions that are consistent with this self-selection mechanism.", "Options": {"A": "The significantly positive 'Average abnormal return' in quarters -2 and -1 suggests that the sample of issuing firms is disproportionately composed of those that chose to wait for good news to become public before issuing.", "B": "The high 'Average excess variance ratio' across all quarters indicates that earnings releases for issuing firms are generally uninformative.", "C": "The results for quarters 1 and 2 (post-issue) show significantly negative abnormal returns, suggesting the market views the equity issue as a negative long-term signal.", "D": "The high 'Average excess variance ratio' in quarters -2 and -1 indicates that the earnings releases preceding an issue are not only positive but also more informative than typical releases, consistent with firms having the greatest incentive to delay for high-impact news."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses comprehension of the self-selection mechanism underlying Hypothesis 2. It uses an 'atomic decomposition' strategy, testing the interpretation of both the first moment (returns) and second moment (variance) of the data. Options A and B correctly link the data to the theory of self-selection based on news valence and informativeness. Distractor C is a factual error based on the table (post-issue returns are not significantly negative). Distractor D is a conceptual opposite, misinterpreting the meaning of a high variance ratio.", "qid": "416", "question": "### Background\n\n**Research Question.** Are the earnings releases that precede a seasoned equity offering (SEO) systematically different in their information content and news valence compared to other earnings releases?\n\n**Setting / Data-Generating Environment.** The study analyzes abnormal returns and return volatility around quarterly earnings releases for firms that conduct an SEO, comparing the quarters before the issue to the quarters after. The theory predicts that firms with good news will delay their issue until after the news is released, and this incentive is strongest when the news is most informative.\n\n**Variables & Parameters.**\n- `Average abnormal return`: The average two-day cumulative abnormal return for firms around an earnings release.\n- `Average excess variance ratio`: A measure of the informativeness of an earnings release, where higher values indicate abnormally high volatility and information content.\n\n### Data / Model Specification\n\n**Table 1. Abnormal Returns and Informativeness at Earnings Release Dates**\n\n| Quarters relative to issue day | -2 | -1 | 1 | 2 |\n|:------------------------------|:-----|:-----|:------|:------|\n| Average abnormal return (%)   | 1.20 | 0.33 | 0.81  | -0.10 |\n| (t-statistic)                 | (6.10)| (1.85)| (4.77)|(-0.52)|\n| Average excess variance ratio | 0.61 | 0.79 | 0.53  | 0.52  |\n| (t-statistic)                 | (4.06)| (5.06)| (5.02)| (4.96)|\n\n*Source: Adapted from the original paper's Table 5.*\n\n### Question\n\nThe theory of time-varying adverse selection predicts that firms self-select when to issue equity based on their private information. Based on the results in Table 1, select all conclusions that are consistent with this self-selection mechanism.", "model_evaluation_result": {"model_raw_response": "Looking at the data and the self-selection theory:\n\nThe theory predicts that firms with good news will delay their SEO until after the news is released, especially when the news is most informative.\n\nAnalyzing each option:\n\nA) **Correct** - The significantly positive abnormal returns in quarters -2 (1.20%, t=6.10) and -1 (0.33%, t=1.85) before the SEO indicate that issuing firms tend to have good earnings news preceding their equity offerings. This is consistent with firms waiting for positive news to become public before issuing.\n\nB) **Incorrect** - High excess variance ratios indicate that earnings releases are MORE informative (higher volatility and information content), not uninformative.\n\nC) **Incorrect** - Quarter 1 shows a positive abnormal return (0.81%, t=4.77), not negative. Quarter 2 shows -0.10% but with t=-0.52, which is not statistically significant.\n\nD) **Correct** - The excess variance ratios are indeed high in quarters -2 (0.61, t=4.06) and -1 (0.79, t=5.06), indicating these earnings releases are particularly informative. Combined with the positive abnormal returns, this supports the theory that firms have the strongest incentive to delay issuance when they have high-impact positive news.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 369, "Question": "### Background\n\n**Research Question.** How does the choice of payment method in M&A deals, particularly the use of earnouts and stock consideration, serve as a mechanism to mitigate information asymmetry problems like adverse selection and moral hazard, especially in the context of cross-border acquisitions (CBAs)?\n\n**Setting.** M&A transactions are fraught with information asymmetry, where the target firm's managers have superior knowledge about its true value and prospects. Acquirers must design contracts to protect themselves from overpaying (adverse selection) and to ensure sellers remain motivated post-acquisition (moral hazard).\n\n**Variables and Concepts.**\n- **Adverse Selection:** The pre-contractual risk that a seller with private information will exploit this advantage, leading the buyer to purchase an overvalued asset (a 'lemon').\n- **Moral Hazard:** The post-contractual risk that one party will change their behavior (e.g., shirk) after the contract is signed, to the detriment of the other party.\n- **Earnout:** A contract where a portion of the payment is deferred and made conditional on the target's post-merger performance.\n- **CBA (Cross-Border Acquisition):** An acquisition where the target is domiciled in a different country than the acquirer, assumed to involve higher information asymmetry.\n\n---\n\n### The Question\n\nIn the context of M&A, an earnout's deferred, performance-contingent payment is a contractual mechanism designed to mitigate information asymmetry. Which of the following problems does this mechanism directly address? (Select all that apply)", "Options": {"A": "Moral Hazard, by incentivizing key personnel to remain and perform well post-acquisition to meet performance targets.", "B": "Adverse Selection, by forcing sellers who claim high future value to have a portion of their payment depend on achieving that value.", "C": "Acquirer's Liquidity Constraints, by allowing the acquirer to pay a significant portion of the deal price later using future cash flows.", "D": "Principal-Agent Problems within the Acquirer's Firm, by ensuring the acquiring managers do not overpay for empire-building purposes."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the foundational understanding of the dual role of an earnout in mitigating information asymmetry, directly testing concepts from Q1 and Q2 of the source QA. The distractors are designed to be high-fidelity: Option C is a true but irrelevant benefit of deferred payments, testing if the user can distinguish between financing and informational roles. Option D misapplies agency theory to the wrong party in the transaction.", "qid": "369", "question": "### Background\n\n**Research Question.** How does the choice of payment method in M&A deals, particularly the use of earnouts and stock consideration, serve as a mechanism to mitigate information asymmetry problems like adverse selection and moral hazard, especially in the context of cross-border acquisitions (CBAs)?\n\n**Setting.** M&A transactions are fraught with information asymmetry, where the target firm's managers have superior knowledge about its true value and prospects. Acquirers must design contracts to protect themselves from overpaying (adverse selection) and to ensure sellers remain motivated post-acquisition (moral hazard).\n\n**Variables and Concepts.**\n- **Adverse Selection:** The pre-contractual risk that a seller with private information will exploit this advantage, leading the buyer to purchase an overvalued asset (a 'lemon').\n- **Moral Hazard:** The post-contractual risk that one party will change their behavior (e.g., shirk) after the contract is signed, to the detriment of the other party.\n- **Earnout:** A contract where a portion of the payment is deferred and made conditional on the target's post-merger performance.\n- **CBA (Cross-Border Acquisition):** An acquisition where the target is domiciled in a different country than the acquirer, assumed to involve higher information asymmetry.\n\n---\n\n### The Question\n\nIn the context of M&A, an earnout's deferred, performance-contingent payment is a contractual mechanism designed to mitigate information asymmetry. Which of the following problems does this mechanism directly address? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify which information asymmetry problems earnouts directly address.\n\nLet me analyze each option:\n\n**Option A - Moral Hazard:** Earnouts create post-acquisition incentives by tying payment to future performance. This directly addresses moral hazard by ensuring sellers (often key personnel who remain) are motivated to work hard and achieve performance targets rather than shirking after the initial payment.\n\n**Option B - Adverse Selection:** Earnouts act as a screening mechanism. By making payment contingent on future performance, they force sellers to \"put their money where their mouth is.\" Sellers with truly valuable firms will accept earnouts, while those with overvalued \"lemons\" will be reluctant to accept performance-contingent payment. This directly addresses the adverse selection problem.\n\n**Option C - Liquidity Constraints:** While earnouts do defer payment, this is a financing benefit, not a mechanism to address information asymmetry. The question specifically asks about information asymmetry problems.\n\n**Option D - Principal-Agent Problems within Acquirer:** This refers to internal governance issues within the acquiring firm (managers vs. shareholders), not the information asymmetry between acquirer and target that earnouts are designed to address.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 459, "Question": "### Background\n\n**Research Question.** This study investigates whether market-assessed sovereign risk (bond spreads) and agency-assessed sovereign risk (credit ratings) are driven by the same factors, specifically testing the differential impact of global investor risk appetite.\n\n**Setting / Data-Generating Environment.** The analysis uses a quarterly panel dataset for five emerging market countries from 1992-1997. The core methodology is a dynamic panel regression with country fixed effects, estimated for two different dependent variables: market-based Brady bond spreads and agency-based credit ratings.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t` (basis points).\n- `IIR`\\u208_it\\u208_: Institutional Investor Sovereign Ratings for country `i` at quarter `t` (ratings scale).\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t` (dimensionless index).\n- Pure Contagion: The portion of co-movement in sovereign risk premia that cannot be explained by changes in economic fundamentals or other common shocks.\n\n---\n\n### Data / Model Specification\n\nThe paper's central argument is that the Risk Appetite Index (RAI) explains both the divergence between market and agency risk measures and the phenomenon of \"pure contagion.\" The core finding is that RAI significantly affects market-based Brady bond spreads (BBY) but has no statistically significant effect on agency-based ratings (IIR).\n\n---\n\nBased on this central finding, which of the following statements accurately describe the mechanisms of divergence and contagion as proposed by the paper?", "Options": {"A": "The paper concludes that contagion is primarily a 'spill-over' effect, where a crisis in one country directly worsens the economic fundamentals of another, rather than a 'pure contagion' effect.", "B": "The model supports the idea of 'pure contagion' by showing that a common global factor (RAI) can induce co-movement in the bond spreads of different countries, even if their economic fundamentals are unchanged and uncorrelated.", "C": "The model implies that agency ratings (IIR) are the primary transmission mechanism for market contagion, as they are more stable than bond spreads.", "D": "A divergence between market-based spreads (BBY) and agency ratings (IIR) is explained by their differential sensitivity; a global risk-off shock (fall in RAI) would widen spreads while leaving ratings, which are based on fundamentals, unchanged."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect empirical results to the paper's core theoretical claims about contagion and divergence. It uses a Premise/Assumption Packaging strategy. Distractors test for confusion between the paper's main point and alternative explanations of contagion (D) or a direct misreading of the results' implications (C).", "qid": "459", "question": "### Background\n\n**Research Question.** This study investigates whether market-assessed sovereign risk (bond spreads) and agency-assessed sovereign risk (credit ratings) are driven by the same factors, specifically testing the differential impact of global investor risk appetite.\n\n**Setting / Data-Generating Environment.** The analysis uses a quarterly panel dataset for five emerging market countries from 1992-1997. The core methodology is a dynamic panel regression with country fixed effects, estimated for two different dependent variables: market-based Brady bond spreads and agency-based credit ratings.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t` (basis points).\n- `IIR`\\u208_it\\u208_: Institutional Investor Sovereign Ratings for country `i` at quarter `t` (ratings scale).\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t` (dimensionless index).\n- Pure Contagion: The portion of co-movement in sovereign risk premia that cannot be explained by changes in economic fundamentals or other common shocks.\n\n---\n\n### Data / Model Specification\n\nThe paper's central argument is that the Risk Appetite Index (RAI) explains both the divergence between market and agency risk measures and the phenomenon of \"pure contagion.\" The core finding is that RAI significantly affects market-based Brady bond spreads (BBY) but has no statistically significant effect on agency-based ratings (IIR).\n\n---\n\nBased on this central finding, which of the following statements accurately describe the mechanisms of divergence and contagion as proposed by the paper?", "model_evaluation_result": {"model_raw_response": "Looking at the core finding that RAI significantly affects market-based Brady bond spreads (BBY) but has no statistically significant effect on agency-based ratings (IIR), I need to evaluate each statement:\n\nA) This contradicts the paper's central argument. The paper specifically investigates \"pure contagion\" - co-movement that cannot be explained by changes in economic fundamentals. The finding that RAI affects spreads but not ratings supports pure contagion, not fundamental spillovers.\n\nB) This accurately describes the paper's mechanism. Since RAI affects bond spreads across countries but not ratings (which are based on fundamentals), this demonstrates that a common global factor can induce co-movement in spreads even when fundamentals remain unchanged - the definition of pure contagion.\n\nC) This is incorrect. Agency ratings (IIR) are not affected by RAI and therefore cannot be the transmission mechanism for contagion. The paper shows that market-based spreads, not ratings, are the channel through which contagion operates.\n\nD) This correctly captures the divergence mechanism. When global risk appetite falls (RAI decreases), market-based spreads widen due to their sensitivity to investor sentiment, while agency ratings remain stable because they focus on fundamentals and are not affected by RAI.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 426, "Question": "### Background\n\n**Research Question.** How precise are the theoretical bounds on lifetime functionals when applied to both a theoretical mortality law and a real-world life table, and what do the results reveal about the practical severity of model risk from fractional age assumptions?\n\n**Setting.** An actuary has developed a theoretical framework for bounding the value of life-contingent claims for any lifetime `X` that has an Increasing Force of Mortality (IFR) and is consistent with a given life table. The framework produces a stochastic lower bound `L` (based on a Constant Force of Mortality assumption) and a stochastic upper bound `U` (the supremum bound). The key result is that for any valid lifetime `X`, `L ≤_st X ≤_st U`.\n\n**Variables and Parameters.**\n\n*   `e_x`: Expected future lifetime for a person aged `x` (units: years).\n*   `Ā_x`: Present value of a continuous life annuity for a person aged `x`, with interest rate `i=0.06` (dimensionless value).\n*   `E[h(L)]`: The value of a functional calculated using the stochastic lower bound `L`.\n*   `E[h(U)]`: The value of a functional calculated using the stochastic upper bound `U`.\n*   `E[h(U_μ*)]`: The value of a functional calculated using a tighter upper bound that requires optimizing over possible mortality rates at integer ages.\n\n---\n\n### Data / Model Specification\n\nThe following tables present calculated bounds for two scenarios: a theoretical lifetime distribution from Makeham’s law (where the true value is known) and a real-world German life table from 1999/2001.\n\n**Table 1.** Bounds for `e_30` and `Ā_30` under Makeham's Law (`i=0.06`)\n\n| | `e_30` (years) | `Ā_30` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 45.05944407 | 0.10554796 |\n| **True Value** | **45.06687713** | **0.10550557** |\n| `E[h(U_μ*)]` | 45.07059946 | 0.10548412 |\n| `E[h(U)]` | 45.08173562 | 0.10542090 |\n\n**Table 2.** Bounds for `e_50` and `Ā_50` using German Life Table 1999/2001 (`i=0.06`)\n\n| | `e_50` (years) | `Ā_50` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 27.27856559 | 0.24740336 |\n| `E[h(U_μ*)]` | 27.28957960 | 0.24723758 |\n| `E[h(U)]` | 27.29933908 | 0.24708628 |\n\nFor any increasing function `f(X)` (like `e_x`), the stochastic ordering `L ≤_st X ≤_st U` implies `E[f(L)] ≤ E[f(X)] ≤ E[f(U)]`. For any decreasing function `g(X)`, the implication is reversed: `E[g(L)] ≥ E[g(X)] ≥ E[g(U)]`.\n\n---\n\n### Question\n\nBased on the provided data and the theoretical framework, which of the following statements are correct?", "Options": {"A": "The width of the `[E[h(L)], E[h(U)]]` bounding interval for the expected future lifetime of a 30-year-old (`e_30`) under Makeham's law is less than 0.025 years.", "B": "According to the German Life Table results in Table 2, the lower bound `E[h(L)]` for the annuity value `Ā_50` is 0.24723758.", "C": "The uncertainty in the expected future lifetime for a 50-year-old (`e_50`) based on the German Life Table, as captured by the `[E[h(L)], E[h(U)]]` interval, is approximately one week, representing a financially insignificant risk for large-scale pension fund management.", "D": "The bounding interval for the annuity value `Ā_30` under Makeham's law is wider than the interval for the expected lifetime `e_30`, indicating greater model risk for financial valuations than for longevity projections."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform precise calculations from tabular data and interpret the results in a practical context. It uses an **Atomic Decomposition** strategy, breaking down the original problem into two distinct, verifiable claims. Distractor C is a **Numerical (Step-Omission Error)** type, testing careful reading of the table by using a value from the wrong row. Distractor D is a **Conceptual** error based on a faulty numerical comparison, testing whether the user can correctly calculate and compare the relative widths of the bounds.", "qid": "426", "question": "### Background\n\n**Research Question.** How precise are the theoretical bounds on lifetime functionals when applied to both a theoretical mortality law and a real-world life table, and what do the results reveal about the practical severity of model risk from fractional age assumptions?\n\n**Setting.** An actuary has developed a theoretical framework for bounding the value of life-contingent claims for any lifetime `X` that has an Increasing Force of Mortality (IFR) and is consistent with a given life table. The framework produces a stochastic lower bound `L` (based on a Constant Force of Mortality assumption) and a stochastic upper bound `U` (the supremum bound). The key result is that for any valid lifetime `X`, `L ≤_st X ≤_st U`.\n\n**Variables and Parameters.**\n\n*   `e_x`: Expected future lifetime for a person aged `x` (units: years).\n*   `Ā_x`: Present value of a continuous life annuity for a person aged `x`, with interest rate `i=0.06` (dimensionless value).\n*   `E[h(L)]`: The value of a functional calculated using the stochastic lower bound `L`.\n*   `E[h(U)]`: The value of a functional calculated using the stochastic upper bound `U`.\n*   `E[h(U_μ*)]`: The value of a functional calculated using a tighter upper bound that requires optimizing over possible mortality rates at integer ages.\n\n---\n\n### Data / Model Specification\n\nThe following tables present calculated bounds for two scenarios: a theoretical lifetime distribution from Makeham’s law (where the true value is known) and a real-world German life table from 1999/2001.\n\n**Table 1.** Bounds for `e_30` and `Ā_30` under Makeham's Law (`i=0.06`)\n\n| | `e_30` (years) | `Ā_30` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 45.05944407 | 0.10554796 |\n| **True Value** | **45.06687713** | **0.10550557** |\n| `E[h(U_μ*)]` | 45.07059946 | 0.10548412 |\n| `E[h(U)]` | 45.08173562 | 0.10542090 |\n\n**Table 2.** Bounds for `e_50` and `Ā_50` using German Life Table 1999/2001 (`i=0.06`)\n\n| | `e_50` (years) | `Ā_50` (value) |\n| :--- | :--- | :--- |\n| `E[h(L)]` | 27.27856559 | 0.24740336 |\n| `E[h(U_μ*)]` | 27.28957960 | 0.24723758 |\n| `E[h(U)]` | 27.29933908 | 0.24708628 |\n\nFor any increasing function `f(X)` (like `e_x`), the stochastic ordering `L ≤_st X ≤_st U` implies `E[f(L)] ≤ E[f(X)] ≤ E[f(U)]`. For any decreasing function `g(X)`, the implication is reversed: `E[g(L)] ≥ E[g(X)] ≥ E[g(U)]`.\n\n---\n\n### Question\n\nBased on the provided data and the theoretical framework, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "Looking at the data and theoretical framework:\n\nFor option A:\n- From Table 1, for e₃₀ under Makeham's law:\n  - E[h(L)] = 45.05944407\n  - E[h(U)] = 45.08173562\n  - Width = 45.08173562 - 45.05944407 = 0.02229155 years\n  - This is less than 0.025 years ✓\n\nFor option B:\n- From Table 2, the lower bound E[h(L)] for Ā₅₀ is 0.24740336, not 0.24723758\n- The value 0.24723758 is actually E[h(U_μ*)] ✗\n\nFor option C:\n- From Table 2, for e₅₀:\n  - Width = 27.29933908 - 27.27856559 = 0.02077349 years\n  - Converting to weeks: 0.02077349 × 52 ≈ 1.08 weeks\n  - This is approximately one week\n- However, the claim about \"financially insignificant risk\" is subjective and not supported by the data ✗\n\nFor option D:\n- For e₃₀: width = 0.02229155 years\n- For Ā₃₀: width = |0.10554796 - 0.10542090| = 0.00012706\n- Since e₃₀ is in years and Ā₃₀ is dimensionless, direct comparison is inappropriate\n- The statement about \"wider interval\" cannot be validated from the given information ✗\n\nA", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 458, "Question": "### Background\n\n**Research Question.** This study investigates whether market-assessed sovereign risk (bond spreads) and agency-assessed sovereign risk (credit ratings) are driven by the same factors, specifically testing the differential impact of global investor risk appetite.\n\n**Setting / Data-Generating Environment.** The analysis uses a quarterly panel dataset for five emerging market countries from 1992-1997. The core methodology is a dynamic panel regression with country fixed effects, estimated for two different dependent variables: market-based Brady bond spreads and agency-based credit ratings.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t` (basis points).\n- `IIR`\\u208_it\\u208_: Institutional Investor Sovereign Ratings for country `i` at quarter `t` (ratings scale).\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t` (dimensionless index).\n- `X`\\u208_it\\u208_: Vector of country-specific economic fundamentals for country `i` at quarter `t`.\n\n---\n\n### Data / Model Specification\n\nThe core regression model is specified as:\n\n  \nY_{it} = \\beta_{0i} + \\beta_{1}Y_{it-1} + \\beta_{2} \\mathrm{RAI}_{t} + \\Gamma' X_{it} + \\varepsilon_{it} \n \n\nwhere `Y`\\u208_it\\u208_ is the measure of country risk (`BBY`\\u208_it\\u208_ or `IIR`\\u208_it\\u208_). The estimated coefficients for this model are presented in Table 1 below.\n\n**Table 1: Estimated Country Risk Measures**\n\n| | (1) | (2) | (3) |\n| :--- | :--- | :--- | :--- |\n| **Dependent Variable** | **BBY** | **BBY** | **IIR** |\n| One-lagged dep. Var. | 0.6694 (8.859)** | 0.7344 (10.017)** | 0.9568 (65.623)** |\n| Risk appetite index (RAI) | | **-1.9785 (-3.630)**** | -0.0002 (-0.150) |\n| Real GDP growth | -14.6289 (-2.525)** | -11.7498 (-2.133)** | 0.0856 (6.718)** |\n| International reserves | -1.3429 (-3.158)** | -0.8628 (-2.048)** | 0.0036 (3.774)** |\n| Change in real exchange rate | -23.8800 (-2.862)** | -22.7968 (-2.902)** | -0.0660 (-3.125)** |\n| Inflation rate | 0.0963 (2.435)** | 0.9711 (2.610)** | -0.0002 (-2.104)** |\n\n*Notes: t-statistics in parentheses. ** denotes significance at the 5% level or better. BBY is Brady bond stripped yield spread. IIR is Institutional Investor Sovereign Ratings. Column (1) excludes RAI, Column (2) includes it.* \n\n---\n\nBased on the model specification and the estimation results in Table 1, which of the following statements are correct interpretations of the findings?", "Options": {"A": "The estimated effect of the Risk Appetite Index (RAI) on Institutional Investor Sovereign Ratings (IIR) is statistically significant at conventional levels (e.g., 5%).", "B": "A 10-point increase in the Risk Appetite Index (RAI) is associated with a decrease in the Brady bond yield spread (BBY) of approximately 19.8 basis points.", "C": "An increase in investor risk appetite (a higher RAI) is associated with an increase in market-assessed sovereign risk (a higher BBY).", "D": "The estimated effect of the Risk Appetite Index (RAI) on the Brady bond yield spread (BBY) is statistically significant at conventional levels (e.g., 5%)."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret regression coefficients from a table, covering magnitude, statistical significance, and direction. It uses an Atomic Decomposition strategy to create two correct options from the core findings. Distractors are based on common errors: misinterpreting statistical significance (C) and a sign error in the economic interpretation (D).", "qid": "458", "question": "### Background\n\n**Research Question.** This study investigates whether market-assessed sovereign risk (bond spreads) and agency-assessed sovereign risk (credit ratings) are driven by the same factors, specifically testing the differential impact of global investor risk appetite.\n\n**Setting / Data-Generating Environment.** The analysis uses a quarterly panel dataset for five emerging market countries from 1992-1997. The core methodology is a dynamic panel regression with country fixed effects, estimated for two different dependent variables: market-based Brady bond spreads and agency-based credit ratings.\n\n**Variables & Parameters.**\n- `BBY`\\u208_it\\u208_: Brady bond stripped yield spread for country `i` at quarter `t` (basis points).\n- `IIR`\\u208_it\\u208_: Institutional Investor Sovereign Ratings for country `i` at quarter `t` (ratings scale).\n- `RAI`\\u208_t\\u208_: Risk Appetite Index at quarter `t` (dimensionless index).\n- `X`\\u208_it\\u208_: Vector of country-specific economic fundamentals for country `i` at quarter `t`.\n\n---\n\n### Data / Model Specification\n\nThe core regression model is specified as:\n\n  \nY_{it} = \\beta_{0i} + \\beta_{1}Y_{it-1} + \\beta_{2} \\mathrm{RAI}_{t} + \\Gamma' X_{it} + \\varepsilon_{it} \n \n\nwhere `Y`\\u208_it\\u208_ is the measure of country risk (`BBY`\\u208_it\\u208_ or `IIR`\\u208_it\\u208_). The estimated coefficients for this model are presented in Table 1 below.\n\n**Table 1: Estimated Country Risk Measures**\n\n| | (1) | (2) | (3) |\n| :--- | :--- | :--- | :--- |\n| **Dependent Variable** | **BBY** | **BBY** | **IIR** |\n| One-lagged dep. Var. | 0.6694 (8.859)** | 0.7344 (10.017)** | 0.9568 (65.623)** |\n| Risk appetite index (RAI) | | **-1.9785 (-3.630)**** | -0.0002 (-0.150) |\n| Real GDP growth | -14.6289 (-2.525)** | -11.7498 (-2.133)** | 0.0856 (6.718)** |\n| International reserves | -1.3429 (-3.158)** | -0.8628 (-2.048)** | 0.0036 (3.774)** |\n| Change in real exchange rate | -23.8800 (-2.862)** | -22.7968 (-2.902)** | -0.0660 (-3.125)** |\n| Inflation rate | 0.0963 (2.435)** | 0.9711 (2.610)** | -0.0002 (-2.104)** |\n\n*Notes: t-statistics in parentheses. ** denotes significance at the 5% level or better. BBY is Brady bond stripped yield spread. IIR is Institutional Investor Sovereign Ratings. Column (1) excludes RAI, Column (2) includes it.* \n\n---\n\nBased on the model specification and the estimation results in Table 1, which of the following statements are correct interpretations of the findings?", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each statement based on the regression results.\n\n**Statement A**: The RAI coefficient for IIR (column 3) is -0.0002 with a t-statistic of -0.150. This t-statistic is well below 1.96 (the critical value for 5% significance), so this effect is NOT statistically significant. Statement A is incorrect.\n\n**Statement B**: The RAI coefficient for BBY (column 2) is -1.9785. This means a 1-point increase in RAI is associated with a 1.9785 basis point decrease in BBY. Therefore, a 10-point increase in RAI would be associated with a 10 × 1.9785 = 19.785 ≈ 19.8 basis point decrease in BBY. Statement B is correct.\n\n**Statement C**: The RAI coefficient for BBY is negative (-1.9785), meaning higher RAI is associated with lower BBY. Since BBY measures sovereign risk (higher spread = higher risk), this means higher investor risk appetite is associated with LOWER market-assessed sovereign risk, not higher. Statement C is incorrect.\n\n**Statement D**: The RAI coefficient for BBY (column 2) is -1.9785 with a t-statistic of -3.630, which is marked with ** indicating significance at the 5% level or better. The absolute value of the t-statistic (3.630) exceeds the critical value of 1.96. Statement D is correct.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 420, "Question": "### Background\n\n**Research Question.** This case examines whether the documented intertemporal stock-to-bond volatility (ISBV) relation is an episodic phenomenon driven by a Flight-to-Quality (FTQ) mechanism, which predicts the relationship should be strongest during periods of high market stress.\n\n**Setting and Sample.** The analysis uses a predictive regression for bond volatility estimated separately on subperiods classified as 'High Stress' (around NBER recessions) and 'Low Stress' (sustained low VIX). A separate sorting analysis characterizes the market environment following months of extremely high stock volatility.\n\n### Data / Model Specification\n\n**Test 1: State-Dependent Regressions**\nThe model below is estimated on four separate subperiods:\n\n  \n\\sigma_{t,t+21}^{TB} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TB} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\varepsilon_{t}\n \n\nwhere `\\sigma^{TB}` is the log realized volatility of 30-year T-bond futures and `\\sigma^{ST}` is the log realized volatility of S&P 500 futures.\n\n**Table 1. Subperiod Regression Results for 30-year T-bond Futures Volatility**\n\n| Subperiod | Stress Level | `\\hat{\\gamma}_{1}` (t-stat) | `\\hat{\\gamma}_{2}` (t-stat) | `R^2` |\n| :--- | :--- | :--- | :--- | :--- |\n| I. 1993-1996 | Low | 0.447 (4.11) | 0.024 (0.33) | 21.7% |\n| II. 2001-2002 | High | 0.141 (0.82) | 0.304 (3.69) | 23.5% |\n| III. 2004-2007 | Low | 0.579 (5.79) | -0.016 (-0.14) | 36.4% |\n| IV. 2007-2010 | High | 0.325 (1.52) | 0.176 (1.94) | 40.3% |\n\n**Test 2: Sorting Analysis**\nMonths are sorted based on lagged stock volatility. The table below shows the median characteristics of the subsequent month for the top decile (highest preceding volatility).\n\n**Table 2. Market Characteristics Following Highest 10% Realized Stock Volatility**\n\n| Variable | Median Value |\n| :--- | :--- |\n| Subsequent Stock Volatility (`StFt`) | 28.13 |\n| Subsequent T-bond Volatility (`TB`) | 13.30 |\n| Subsequent Stock-Bond Correlation (`pStFt,TB`) | -0.32 |\n\n### Question\n\nBased on the evidence in Table 1 and Table 2, which of the following statements accurately characterize the Intertemporal Stock-to-Bond Volatility (ISBV) relation and the Flight-to-Quality (FTQ) dynamic? Select all that apply.", "Options": {"A": "The market environment following a high stock volatility shock is characterized by the joint occurrence of persistent stock volatility, elevated bond volatility, and a strongly negative stock-bond correlation.", "B": "The FTQ dynamic is characterized by a positive correlation between stock and bond returns as investors flee to the quality of government bonds.", "C": "The predictive power of lagged stock volatility (γ₂) for future bond volatility is statistically significant and economically large during high-stress periods, but insignificant during low-stress periods.", "D": "In high-stress periods, the bond market's own lagged volatility (γ₁) is a more powerful predictor of future bond volatility than lagged stock volatility (γ₂)."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the core finding that the ISBV relation is state-dependent. It uses an Atomic Decomposition strategy, breaking down the original QA into two key verifiable claims: one about the regression coefficients (from Q1 of the source) and one about the market environment (from Q2 of the source). Distractor C misinterprets the relative importance of predictors in Table 1 for high-stress periods. Distractor D presents a conceptual opposite for the stock-bond correlation during FTQ.", "qid": "420", "question": "### Background\n\n**Research Question.** This case examines whether the documented intertemporal stock-to-bond volatility (ISBV) relation is an episodic phenomenon driven by a Flight-to-Quality (FTQ) mechanism, which predicts the relationship should be strongest during periods of high market stress.\n\n**Setting and Sample.** The analysis uses a predictive regression for bond volatility estimated separately on subperiods classified as 'High Stress' (around NBER recessions) and 'Low Stress' (sustained low VIX). A separate sorting analysis characterizes the market environment following months of extremely high stock volatility.\n\n### Data / Model Specification\n\n**Test 1: State-Dependent Regressions**\nThe model below is estimated on four separate subperiods:\n\n  \n\\sigma_{t,t+21}^{TB} = \\gamma_{0} + \\gamma_{1}\\sigma_{t-1,t-22}^{TB} + \\gamma_{2}\\sigma_{t-1,t-22}^{ST} + \\varepsilon_{t}\n \n\nwhere `\\sigma^{TB}` is the log realized volatility of 30-year T-bond futures and `\\sigma^{ST}` is the log realized volatility of S&P 500 futures.\n\n**Table 1. Subperiod Regression Results for 30-year T-bond Futures Volatility**\n\n| Subperiod | Stress Level | `\\hat{\\gamma}_{1}` (t-stat) | `\\hat{\\gamma}_{2}` (t-stat) | `R^2` |\n| :--- | :--- | :--- | :--- | :--- |\n| I. 1993-1996 | Low | 0.447 (4.11) | 0.024 (0.33) | 21.7% |\n| II. 2001-2002 | High | 0.141 (0.82) | 0.304 (3.69) | 23.5% |\n| III. 2004-2007 | Low | 0.579 (5.79) | -0.016 (-0.14) | 36.4% |\n| IV. 2007-2010 | High | 0.325 (1.52) | 0.176 (1.94) | 40.3% |\n\n**Test 2: Sorting Analysis**\nMonths are sorted based on lagged stock volatility. The table below shows the median characteristics of the subsequent month for the top decile (highest preceding volatility).\n\n**Table 2. Market Characteristics Following Highest 10% Realized Stock Volatility**\n\n| Variable | Median Value |\n| :--- | :--- |\n| Subsequent Stock Volatility (`StFt`) | 28.13 |\n| Subsequent T-bond Volatility (`TB`) | 13.30 |\n| Subsequent Stock-Bond Correlation (`pStFt,TB`) | -0.32 |\n\n### Question\n\nBased on the evidence in Table 1 and Table 2, which of the following statements accurately characterize the Intertemporal Stock-to-Bond Volatility (ISBV) relation and the Flight-to-Quality (FTQ) dynamic? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the evidence in Tables 1 and 2, I need to evaluate each statement about the ISBV relation and FTQ dynamic.\n\n**Statement A**: Table 2 shows that following high stock volatility shocks (top decile), we observe:\n- Persistent stock volatility (median 28.13)\n- Elevated bond volatility (median 13.30)\n- Strongly negative stock-bond correlation (median -0.32)\n\nThis accurately describes the market environment. ✓\n\n**Statement B**: This states FTQ involves a positive correlation between stock and bond returns. However, Table 2 shows a negative correlation (-0.32) during these periods. Flight-to-Quality means investors sell risky assets (stocks) and buy safe assets (bonds), creating negative correlation. ✗\n\n**Statement C**: From Table 1:\n- High stress periods (II & IV): γ₂ = 0.304 (t=3.69) and 0.176 (t=1.94) - significant\n- Low stress periods (I & III): γ₂ = 0.024 (t=0.33) and -0.016 (t=-0.14) - insignificant\n\nThis shows lagged stock volatility predicts bond volatility only during high-stress periods. ✓\n\n**Statement D**: In high-stress periods:\n- Period II: γ₁ = 0.141 (t=0.82) vs γ₂ = 0.304 (t=3.69)\n- Period IV: γ₁ = 0.325 (t=1.52) vs γ₂ = 0.176 (t=1.94)\n\nIn both cases, γ₁ is not statistically significant while γ₂ is significant (or marginally so). This contradicts the statement. ✗\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 468, "Question": "### Background\n\nAn insurer seeks to minimize the Value-at-Risk (`VaR`) of its total risk exposure, `T_I(X)`, by choosing a ceded loss function `I(x)`. The reinsurance premium includes a safety loading `θ`. A key finding (Theorem 3.2(i)) is that under certain conditions, the optimal strategy is to purchase no reinsurance at all, i.e., `I*(x) = 0`.\n\n### Data / Model Specification\n\nThe condition for forgoing reinsurance is:\n\n  \np \\leq \\theta^{*} \\quad \\Leftrightarrow \\quad \\theta \\geq \\frac{p}{1-p} \\quad \\text{(Eq. 1)}\n \n\nWhere `p` is the VaR confidence level and `θ* = θ / (1+θ)`.\n\nThe objective function being minimized is:\n\n  \n\\mathsf{VaR}_{T_{I}(X)}(p) = \\mathsf{VaR}_{X}(p) - I(\\mathsf{VaR}_{X}(p)) + (1+\\theta)\\mathbb{E}[I(X)] \\quad \\text{(Eq. 2)}\n \n\n### Question\n\nAccording to the model and its underlying logic, which of the following statements are valid criticisms or interpretations of the recommendation to purchase no reinsurance (`I*(x)=0`) when the condition in Eq. 1 is met? Select all that apply.", "Options": {"A": "The term `p/(1-p)` in the condition represents the statistical odds of a loss *exceeding* the `p`-quantile, meaning reinsurance is unattractive if its markup exceeds these odds.", "B": "This 'optimal' strategy can be dangerously misleading in practice because the VaR-minimization framework is blind to the magnitude of catastrophic losses in the `(1-p)` tail, potentially exposing the insurer to insolvency risk.", "C": "Switching to a TVaR-minimization framework would likely lead to the same 'no reinsurance' conclusion under the same condition, as both risk measures are based on the `p`-quantile.", "D": "The recommendation is economically sound within the model because when `θ` is sufficiently high, the premium cost `(1+θ)E[I(X)]` systematically outweighs the benefit of reducing the loss at the single point `VaR_X(p)`."}, "Answer": ["B", "D"], "pi_justification": "This item assesses the ability to interpret a key theorem and critically evaluate its practical implications. It uses a 'Reverse-Reasoning' strategy by asking for valid interpretations and critiques of a given result. Option A correctly explains the economic logic *within* the VaR model. Option B provides a valid and crucial critique of the model's limitations, highlighting the myopia of VaR. Option C is a 'Conceptual Distractor' (Almost Right); `p/(1-p)` represents the odds of the loss *not* exceeding the quantile. Option D is a 'Conceptual Opposite' distractor; a TVaR framework is sensitive to the tail and would likely lead to a different, more prudent conclusion.", "qid": "468", "question": "### Background\n\nAn insurer seeks to minimize the Value-at-Risk (`VaR`) of its total risk exposure, `T_I(X)`, by choosing a ceded loss function `I(x)`. The reinsurance premium includes a safety loading `θ`. A key finding (Theorem 3.2(i)) is that under certain conditions, the optimal strategy is to purchase no reinsurance at all, i.e., `I*(x) = 0`.\n\n### Data / Model Specification\n\nThe condition for forgoing reinsurance is:\n\n  \np \\leq \\theta^{*} \\quad \\Leftrightarrow \\quad \\theta \\geq \\frac{p}{1-p} \\quad \\text{(Eq. 1)}\n \n\nWhere `p` is the VaR confidence level and `θ* = θ / (1+θ)`.\n\nThe objective function being minimized is:\n\n  \n\\mathsf{VaR}_{T_{I}(X)}(p) = \\mathsf{VaR}_{X}(p) - I(\\mathsf{VaR}_{X}(p)) + (1+\\theta)\\mathbb{E}[I(X)] \\quad \\text{(Eq. 2)}\n \n\n### Question\n\nAccording to the model and its underlying logic, which of the following statements are valid criticisms or interpretations of the recommendation to purchase no reinsurance (`I*(x)=0`) when the condition in Eq. 1 is met? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each statement about the VaR-minimization framework and the condition for purchasing no reinsurance.\n\nLet me examine each option:\n\n**Option A**: \"The term `p/(1-p)` in the condition represents the statistical odds of a loss *exceeding* the `p`-quantile...\"\n\nThis is correct. If p is the confidence level (e.g., 0.95), then (1-p) is the probability of exceeding the p-quantile (e.g., 0.05). The ratio p/(1-p) represents the odds of not exceeding vs exceeding the VaR threshold. The condition states that if the safety loading θ exceeds these odds, reinsurance becomes unattractive under VaR minimization.\n\n**Option B**: \"This 'optimal' strategy can be dangerously misleading in practice because the VaR-minimization framework is blind to the magnitude of catastrophic losses in the `(1-p)` tail...\"\n\nThis is a valid criticism. VaR only considers the threshold value at the p-quantile and ignores what happens beyond that point. An insurer following this \"optimal\" strategy could face catastrophic losses in the tail that could lead to insolvency, even though the VaR metric suggested no reinsurance was optimal.\n\n**Option C**: \"Switching to a TVaR-minimization framework would likely lead to the same 'no reinsurance' conclusion...\"\n\nThis is incorrect. TVaR (Tail Value at Risk) considers the expected value of losses beyond the VaR threshold, making it sensitive to tail risk. Under TVaR minimization, reinsurance would likely remain attractive even with high safety loadings because it would reduce the expected tail losses.\n\n**Option D**: \"The recommendation is economically sound within the model because when `θ` is sufficiently high, the premium cost `(1+θ)E[I(X)]` systematically outweighs the benefit...\"\n\nThis is a valid interpretation within the VaR framework. The objective function shows that when θ is high enough (satisfying the condition), the premium cost term dominates the benefit of reducing the loss at VaR_X(p), making I*(x)=0 optimal under this specific risk measure.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 437, "Question": "### Background\n\n**Research Question.** After establishing a robust method for measuring currency exposure, the paper investigates how this exposure varies across different types of firms. The analysis explores heterogeneity based on firms' international activities and their industry sector, revealing several counter-intuitive and economically significant patterns.\n\n**Setting / Data-Generating Environment.** The analysis uses firm-level exchange rate exposure coefficients (`\\beta_{is}`) estimated from the third stage of the orthogonalized model for firms in the Euro Stoxx TMI index (1999-2011). The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Table 1. Exchange Rate Exposure (`\\beta_{is}`) by Firm Characteristics**\n\n| | Avg. `\\beta_{is}` (significant firms) | Total Significant (%) |\n|:---|:---:|:---:|\n| **Panel A: Overall Results** | | |\n| JPY | 0.82 | 27.60% |\n| USD | 0.68 | 13.04% |\n| **Panel B: By International Involvement (for USD Shocks)** | | |\n| MNCs (High Exporters) | 0.47 | 10% |\n| Domestic Firms | 0.76 | 13% |\n| **Panel C: By Industry (for CHF Shocks)** | | |\n| Financials | 1.90 | 72% |\n| Non-Financials | 1.88 | 49% |\n\n*Notes: MNCs are Multinational Corporations. `\\beta_{is}` is the average of significant coefficients.* \n\n### Question\n\nBased on the results in Table 1, select all statements that correctly link an empirical finding to its proposed theoretical explanation in the paper.", "Options": {"A": "The finding in Panel B that MNCs have lower average USD exposure (`\\beta_{is}`=0.47) than domestic firms (`\\beta_{is}`=0.76) is explained by the argument that MNCs engage in more effective corporate hedging, reducing their net stock price sensitivity.", "B": "The predominantly positive `\\beta_{is}` coefficients in Panel A suggest that the typical European firm benefits from a stronger Euro, consistent with being a net importer or having net foreign currency liabilities.", "C": "The higher percentage of financial firms with significant CHF exposure (Panel C) is because they are primarily net exporters of financial services, making them vulnerable to a stronger Euro.", "D": "The results in Panel B imply that domestic firms have greater underlying gross economic exposure to currency fluctuations than MNCs."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Atomic Decomposition and Reverse-Reasoning strategies. It tests the ability to connect distinct empirical patterns from the table with their corresponding theoretical explanations (net importer hypothesis, corporate hedging). Distractors are designed using a Conceptual Opposite explanation (C) and a subtle misinterpretation of the hedging argument (D).", "qid": "437", "question": "### Background\n\n**Research Question.** After establishing a robust method for measuring currency exposure, the paper investigates how this exposure varies across different types of firms. The analysis explores heterogeneity based on firms' international activities and their industry sector, revealing several counter-intuitive and economically significant patterns.\n\n**Setting / Data-Generating Environment.** The analysis uses firm-level exchange rate exposure coefficients (`\\beta_{is}`) estimated from the third stage of the orthogonalized model for firms in the Euro Stoxx TMI index (1999-2011). The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Table 1. Exchange Rate Exposure (`\\beta_{is}`) by Firm Characteristics**\n\n| | Avg. `\\beta_{is}` (significant firms) | Total Significant (%) |\n|:---|:---:|:---:|\n| **Panel A: Overall Results** | | |\n| JPY | 0.82 | 27.60% |\n| USD | 0.68 | 13.04% |\n| **Panel B: By International Involvement (for USD Shocks)** | | |\n| MNCs (High Exporters) | 0.47 | 10% |\n| Domestic Firms | 0.76 | 13% |\n| **Panel C: By Industry (for CHF Shocks)** | | |\n| Financials | 1.90 | 72% |\n| Non-Financials | 1.88 | 49% |\n\n*Notes: MNCs are Multinational Corporations. `\\beta_{is}` is the average of significant coefficients.* \n\n### Question\n\nBased on the results in Table 1, select all statements that correctly link an empirical finding to its proposed theoretical explanation in the paper.", "model_evaluation_result": {"model_raw_response": "Looking at the empirical findings and their theoretical explanations:\n\n**Panel A Analysis:** The positive β coefficients (0.82 for JPY, 0.68 for USD) indicate that when the Euro appreciates (positive return), stock prices increase. This is consistent with firms being net importers or having foreign currency liabilities, as a stronger Euro makes imports cheaper or reduces the Euro value of foreign debt.\n\n**Panel B Analysis:** MNCs show lower USD exposure (β=0.47) compared to domestic firms (β=0.76). This suggests MNCs engage in more hedging activities that reduce their net exposure, not that domestic firms have greater gross exposure. The data shows net exposure after hedging, not underlying economic exposure.\n\n**Panel C Analysis:** The high percentage of financial firms with significant CHF exposure (72% vs 49% for non-financials) likely reflects their role as financial intermediaries with CHF-denominated assets and liabilities, not because they are net exporters of financial services.\n\nEvaluating each option:\n- A) ✓ Correctly links lower MNC exposure to more effective hedging\n- B) ✓ Correctly interprets positive coefficients as consistent with net importer status\n- C) ✗ Incorrectly explains financial sector exposure as due to service exports\n- D) ✗ Incorrectly infers gross exposure differences from net exposure data\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 403, "Question": "### Background\n\n**Research Question.** This paper investigates two distinct facets of market efficiency. The first is the deviation of prices from a random walk, often attributed to trading-related noise. The second is *informational efficiency*, which concerns the degree to which prices incorporate private information. This question explores how a secular increase in market liquidity, driven by reductions in the minimum tick size, affected both types of efficiency.\n\n**Setting and Sample.** The study examines size-sorted portfolios of NYSE stocks across three tick-size regimes: Eighths (1993-1997), Sixteenths (1997-2001), and Decimal (2001-2002).\n\n### Data / Model Specification\n\n**Metric 1: Random Walk Deviation.** The deviation from a random walk is measured by a variance ratio:\n  \nVR = \\frac{q \\cdot \\text{Var}(\\text{5-minute returns})}{\\text{Var}(\\text{open-to-close returns})} \\quad \\text{(Eq. 1)}\n \nwhere `q` is the number of 5-minute intervals in a trading day. For a perfect random walk, `VR = 1`. A `VR > 1` suggests negative serial correlation in short-horizon returns, often caused by microstructure noise like bid-ask bounce.\n\n**Metric 2: Informational Efficiency.** Following French and Roll (1986), informational efficiency is assessed by jointly analyzing two statistics:\n1.  The **Informational Variance Ratio** (`VR_Info`), defined as the ratio of per-hour open-to-close return variance to per-hour close-to-open return variance. An increase in `VR_Info` can be caused by either (i) more private information being incorporated during trading hours, or (ii) more mispricing (e.g., noise trading, overreactions) during trading hours.\n2.  The **First-Order Daily Return Autocorrelation** (`ρ_1`), which serves as a proxy for mispricing. Positive autocorrelation is consistent with investor underreaction or slow price adjustment.\n\n**Table 1. Efficiency Metrics by Tick Regime**\n\n| | Eighths | Sixteenths | Decimals |\n| :--- | :--- | :--- | :--- |\n| **Panel A: Random Walk Variance Ratio (`VR`)** | | | |\n| Large firms | 1.21 | 1.21 | 1.10 |\n| Mid-cap firms | 1.81 | 1.82 | 1.36* |\n| Small firms | 2.28 | 2.38 | 1.79* |\n| **Panel B: Informational Variance Ratio (`VR_Info`)** | | | |\n| Small firms | 8.95 | 15.33 | 22.16* |\n| **Panel C: Daily Return Autocorrelation (`ρ_1`)** | | | |\n| Small firms | 0.2121 | 0.0814* | 0.0339* |\n| (p-value for `ρ_1=0`) | (0.000) | (0.014) | (0.458) |\n\n*An asterisk (*) indicates the value in the Decimals/Sixteenths regime is statistically different from the Eighths regime at the 5% level.*\n\n### Question\n\nBased on the provided data for Small Firms and the paper's reasoning, which of the following conclusions are supported?", "Options": {"A": "The significant decline in daily return autocorrelation (`ρ_1`) from a highly significant 0.2121 to an insignificant 0.0339 suggests that market mispricing diminished in the more liquid decimal regime.", "B": "The decline in the random walk variance ratio (`VR`) from 2.28 to 1.79 indicates that prices for small firms moved further away from a random walk benchmark.", "C": "The sharp increase in the informational variance ratio (`VR_Info`) from 8.95 to 22.16 provides unambiguous evidence of increased mispricing during trading hours.", "D": "The joint observation of a rising informational variance ratio (`VR_Info`) and a falling autocorrelation (`ρ_1`) supports the conclusion that increased incorporation of private information, rather than increased mispricing, drove the rise in trading-hour volatility."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize multiple empirical findings to support a nuanced conclusion about informational efficiency. It uses an Atomic Decomposition strategy, breaking the paper's complex argument into verifiable statements. Distractor C is a Conceptual Opposite (interpreting a move towards the benchmark of 1 as a move away). Distractor D represents a Step-Omission Error (failing to use the autocorrelation data from Panel C to resolve the ambiguity of the variance ratio in Panel B).", "qid": "403", "question": "### Background\n\n**Research Question.** This paper investigates two distinct facets of market efficiency. The first is the deviation of prices from a random walk, often attributed to trading-related noise. The second is *informational efficiency*, which concerns the degree to which prices incorporate private information. This question explores how a secular increase in market liquidity, driven by reductions in the minimum tick size, affected both types of efficiency.\n\n**Setting and Sample.** The study examines size-sorted portfolios of NYSE stocks across three tick-size regimes: Eighths (1993-1997), Sixteenths (1997-2001), and Decimal (2001-2002).\n\n### Data / Model Specification\n\n**Metric 1: Random Walk Deviation.** The deviation from a random walk is measured by a variance ratio:\n  \nVR = \\frac{q \\cdot \\text{Var}(\\text{5-minute returns})}{\\text{Var}(\\text{open-to-close returns})} \\quad \\text{(Eq. 1)}\n \nwhere `q` is the number of 5-minute intervals in a trading day. For a perfect random walk, `VR = 1`. A `VR > 1` suggests negative serial correlation in short-horizon returns, often caused by microstructure noise like bid-ask bounce.\n\n**Metric 2: Informational Efficiency.** Following French and Roll (1986), informational efficiency is assessed by jointly analyzing two statistics:\n1.  The **Informational Variance Ratio** (`VR_Info`), defined as the ratio of per-hour open-to-close return variance to per-hour close-to-open return variance. An increase in `VR_Info` can be caused by either (i) more private information being incorporated during trading hours, or (ii) more mispricing (e.g., noise trading, overreactions) during trading hours.\n2.  The **First-Order Daily Return Autocorrelation** (`ρ_1`), which serves as a proxy for mispricing. Positive autocorrelation is consistent with investor underreaction or slow price adjustment.\n\n**Table 1. Efficiency Metrics by Tick Regime**\n\n| | Eighths | Sixteenths | Decimals |\n| :--- | :--- | :--- | :--- |\n| **Panel A: Random Walk Variance Ratio (`VR`)** | | | |\n| Large firms | 1.21 | 1.21 | 1.10 |\n| Mid-cap firms | 1.81 | 1.82 | 1.36* |\n| Small firms | 2.28 | 2.38 | 1.79* |\n| **Panel B: Informational Variance Ratio (`VR_Info`)** | | | |\n| Small firms | 8.95 | 15.33 | 22.16* |\n| **Panel C: Daily Return Autocorrelation (`ρ_1`)** | | | |\n| Small firms | 0.2121 | 0.0814* | 0.0339* |\n| (p-value for `ρ_1=0`) | (0.000) | (0.014) | (0.458) |\n\n*An asterisk (*) indicates the value in the Decimals/Sixteenths regime is statistically different from the Eighths regime at the 5% level.*\n\n### Question\n\nBased on the provided data for Small Firms and the paper's reasoning, which of the following conclusions are supported?", "model_evaluation_result": {"model_raw_response": "Looking at the data for Small Firms across the three tick regimes:\n\n**Option A**: The data shows ρ₁ declined from 0.2121 (p=0.000, highly significant) in Eighths to 0.0339 (p=0.458, not significant) in Decimals. This substantial decline in autocorrelation, which the paper identifies as a proxy for mispricing, supports the conclusion that market mispricing diminished. ✓\n\n**Option B**: The VR declined from 2.28 to 1.79. Since VR = 1 for a perfect random walk, and VR > 1 indicates deviation from random walk due to negative serial correlation, a decline from 2.28 to 1.79 means prices moved *closer* to a random walk, not further away. ✗\n\n**Option C**: While VR_Info increased from 8.95 to 22.16, the paper states that such an increase can be caused by either (i) more private information incorporation OR (ii) more mispricing. The evidence is not unambiguous - we need the autocorrelation data to distinguish between these explanations. ✗\n\n**Option D**: This correctly interprets the joint evidence. The rising VR_Info (8.95 to 22.16) combined with falling ρ₁ (0.2121 to 0.0339) allows us to distinguish between the two possible causes. Since mispricing decreased (as shown by falling autocorrelation), the rise in trading-hour volatility must be due to increased incorporation of private information. ✓\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 466, "Question": "### Background\n\nAn insurer with an initial random loss `X` chooses a ceded loss function `I(X)` to transfer risk to a reinsurer. The insurer pays a premium `π_I(X)` and is left with a total post-reinsurance risk exposure, `T_I(X)`. The goal is to minimize a risk measure of this final exposure. The insurer is concerned with solvency during catastrophic events and is comparing two different risk management objectives: minimizing Value-at-Risk (VaR) versus minimizing Tail Value-at-Risk (TVaR).\n\n### Data / Model Specification\n\nThe insurer's total risk exposure is its retained loss plus the premium paid:\n\n  \nT_I(X) = X - I(X) + (1+\\theta)\\mathbb{E}[I(X)] \\quad \\text{(Eq. 1)}\n \n\nWhere `θ > 0` is the reinsurer's safety loading factor.\n\nValue-at-Risk (`VaR`) at confidence level `p` is the loss threshold that will not be exceeded with probability `p`.\n\nTail Value-at-Risk (`TVaR`) at confidence level `p` is the expected loss, conditional on the loss exceeding the `VaR` at that same confidence level `p`.\n\n### Question\n\nFrom a risk management perspective, which of the following statements accurately contrast the implications of a VaR-minimizing strategy versus a TVaR-minimizing strategy for the insurer? Select all that apply.", "Options": {"A": "If the insurer's objective switches from minimizing VaR to minimizing TVaR, the optimal upper limit of reinsurance coverage (`b*` in a two-layer contract `I(x) = (x-a)_+ - (x-b)_+`) would tend to increase to provide more protection against extreme tail events.", "B": "A TVaR-minimizing strategy is considered more conservative because it accounts for the average severity of losses in the tail, whereas a VaR-minimizing strategy is indifferent to the magnitude of losses beyond the `p`-quantile.", "C": "A VaR-minimizing strategy might lead to an optimal contract that leaves the insurer exposed to rare but enormous losses, as long as the `p`-quantile of the total risk exposure is minimized.", "D": "The optimal ceded loss function `I*(X)` under VaR minimization will always cede more of the extreme tail losses (for very large `X`) compared to the `I*(X)` under TVaR minimization."}, "Answer": ["A", "B", "C"], "pi_justification": "This item assesses the core conceptual comparison between VaR and TVaR, which is central to the paper's motivation. It uses a 'Scenario Application' strategy by asking for the practical implications of choosing one risk measure over the other. Option A correctly defines the key difference. Option B correctly applies this logic to contract design, predicting the effect on the coverage limit. Option D correctly identifies a major weakness of the VaR framework. Option C is a 'Conceptual Opposite' distractor; TVaR, not VaR, incentivizes ceding more of the extreme tail.", "qid": "466", "question": "### Background\n\nAn insurer with an initial random loss `X` chooses a ceded loss function `I(X)` to transfer risk to a reinsurer. The insurer pays a premium `π_I(X)` and is left with a total post-reinsurance risk exposure, `T_I(X)`. The goal is to minimize a risk measure of this final exposure. The insurer is concerned with solvency during catastrophic events and is comparing two different risk management objectives: minimizing Value-at-Risk (VaR) versus minimizing Tail Value-at-Risk (TVaR).\n\n### Data / Model Specification\n\nThe insurer's total risk exposure is its retained loss plus the premium paid:\n\n  \nT_I(X) = X - I(X) + (1+\\theta)\\mathbb{E}[I(X)] \\quad \\text{(Eq. 1)}\n \n\nWhere `θ > 0` is the reinsurer's safety loading factor.\n\nValue-at-Risk (`VaR`) at confidence level `p` is the loss threshold that will not be exceeded with probability `p`.\n\nTail Value-at-Risk (`TVaR`) at confidence level `p` is the expected loss, conditional on the loss exceeding the `VaR` at that same confidence level `p`.\n\n### Question\n\nFrom a risk management perspective, which of the following statements accurately contrast the implications of a VaR-minimizing strategy versus a TVaR-minimizing strategy for the insurer? Select all that apply.", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement about VaR vs TVaR minimization strategies for insurance risk management.\n\nFirst, let me clarify the key differences between VaR and TVaR:\n- VaR at level p is just the p-quantile of the loss distribution\n- TVaR at level p is the expected loss conditional on exceeding the VaR threshold\n- TVaR captures the severity of tail losses, while VaR only captures the threshold\n\nNow analyzing each option:\n\n**Option A**: When switching from VaR to TVaR minimization, the optimal upper limit b* would tend to increase.\n- TVaR considers the average severity of losses beyond the VaR threshold\n- To minimize TVaR, the insurer would want more protection against extreme tail events\n- This means extending coverage further into the tail (higher b*)\n- This statement is TRUE\n\n**Option B**: TVaR is more conservative because it accounts for average severity in the tail, while VaR is indifferent to magnitude beyond the p-quantile.\n- This is a fundamental property of these risk measures\n- VaR only cares about the threshold value at the p-quantile\n- TVaR averages all losses beyond that threshold\n- This statement is TRUE\n\n**Option C**: VaR minimization might leave the insurer exposed to rare but enormous losses as long as the p-quantile is minimized.\n- VaR optimization focuses solely on the p-quantile\n- It doesn't penalize extreme losses beyond that point\n- This can create moral hazard where extreme tail risks are ignored\n- This statement is TRUE\n\n**Option D**: VaR minimization will always cede more extreme tail losses than TVaR minimization.\n- This is actually backwards\n- TVaR minimization cares about reducing extreme tail losses\n- VaR minimization might retain extreme tail losses if it helps minimize the p-quantile\n- This statement is FALSE\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 463, "Question": "### Background\n\n**Research Question.** How can we test for 'leakage' in global climate policy, where divestment from carbon-intensive firms by investors in one region is offset by increased investment from another?\n\n**Setting.** A study finds that after the Paris Agreement (PA), overall investor participation in High-Carbon (HC) firms decreased. However, the researchers note that \"investors from BRIC countries increased their participation in HC firms,\" while anecdotal evidence suggests EU investors decreased their participation.\n\n**Data / Model Specification.**\nThe original model was:\n\n  \nY_{it} = \\alpha_i + \\delta_t + \\beta_1 (HC_i \\times PostPA_t) + \\dots + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nwhere `Y_{it}` is total institutional ownership in firm `i`.\n\n---\n\n### Question\n\nTo formally test the 'leakage' hypothesis, which of the following are valid and necessary modifications to the research design? Select ALL that apply.", "Options": {"A": "Disaggregate the dependent variable by investor domicile, creating separate ownership variables such as `Y_{it,EU}` (ownership by EU investors) and `Y_{it,BRIC}` (ownership by BRIC investors).", "B": "Run separate regressions for subsamples of investors (e.g., one for EU investors, one for BRIC investors) and test if the coefficient on `HC x PostPA` is negative for the EU group and positive for the BRIC group.", "C": "Add firm fixed effects for BRIC-domiciled firms and EU-domiciled firms to the original model to control for regional differences.", "D": "Add a new interaction term, `BRIC_i × PostPA_t`, to the original regression, where `BRIC_i` is a dummy for firms domiciled in BRIC countries."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item assesses the ability to extend an econometric model to test a more nuanced hypothesis. It uses a Premise/Assumption Packaging strategy, asking for the correct modeling steps. Distractors (B and D) represent common but incorrect specifications that misidentify the source of heterogeneity (investor vs. firm), making them high-fidelity 'Formula Misuse' errors.", "qid": "463", "question": "### Background\n\n**Research Question.** How can we test for 'leakage' in global climate policy, where divestment from carbon-intensive firms by investors in one region is offset by increased investment from another?\n\n**Setting.** A study finds that after the Paris Agreement (PA), overall investor participation in High-Carbon (HC) firms decreased. However, the researchers note that \"investors from BRIC countries increased their participation in HC firms,\" while anecdotal evidence suggests EU investors decreased their participation.\n\n**Data / Model Specification.**\nThe original model was:\n\n  \nY_{it} = \\alpha_i + \\delta_t + \\beta_1 (HC_i \\times PostPA_t) + \\dots + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\nwhere `Y_{it}` is total institutional ownership in firm `i`.\n\n---\n\n### Question\n\nTo formally test the 'leakage' hypothesis, which of the following are valid and necessary modifications to the research design? Select ALL that apply.", "model_evaluation_result": {"model_raw_response": "To test for 'leakage' in global climate policy, we need to examine whether divestment by one group of investors (EU) is offset by increased investment from another group (BRIC). Let me analyze each option:\n\nA) Disaggregate the dependent variable by investor domicile, creating separate ownership variables such as `Y_{it,EU}` and `Y_{it,BRIC}`.\n- This is VALID and NECESSARY. To test leakage, we must track ownership by different investor groups separately. This allows us to see if EU ownership decreases while BRIC ownership increases in HC firms post-PA.\n\nB) Run separate regressions for subsamples of investors and test if the coefficient on `HC x PostPA` is negative for EU and positive for BRIC.\n- This is VALID and represents one approach to test the hypothesis. By running separate regressions with disaggregated dependent variables, we can directly test opposing effects for different investor groups.\n\nC) Add firm fixed effects for BRIC-domiciled firms and EU-domiciled firms to control for regional differences.\n- This is NOT valid for testing the leakage hypothesis. The leakage concerns investor domicile, not firm domicile. Adding firm regional fixed effects doesn't help identify differential investment behavior by investor origin.\n\nD) Add a new interaction term, `BRIC_i × PostPA_t`, where `BRIC_i` is a dummy for firms domiciled in BRIC countries.\n- This is NOT valid for testing leakage. This tests whether BRIC-domiciled firms experienced different ownership changes post-PA, but leakage is about investor behavior (where investors are from), not firm location.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 400, "Question": "### Background\n\n**Research Question.** This case investigates the relative importance of three investor characteristics—sophistication, experience (proxied by age), and the use of stop-loss orders—in mitigating the disposition effect. The analysis aims to determine which factors remain significant when all are considered simultaneously.\n\n**Setting / Data-Generating Environment.** The analysis uses a full Cox proportional hazard model including all three characteristics and their interactions with the trading gain/loss indicators. This allows for a direct comparison of the marginal contribution of each factor, controlling for the others.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator, a dummy variable equal to 1 if a position is at a loss.\n- `TGI(t)`: Trading Gain Indicator, a dummy variable equal to 1 if a position is at a gain.\n- `Soph`: A time-invariant dummy variable equal to 1 for sophisticated investors (warrant traders).\n- `Age`: A continuous variable defined as `(investor's actual age - 18) / 10`. Hazard ratios reflect the effect of a 10-year increase in age.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who have used a stop-loss order at least once.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a comprehensive model with multiple interaction terms. The key results from this full specification are summarized in Table 1.\n\n**Table 1: Combined Model of Predictive Variables**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.3948*** | | \n| TGI | | 2.3777*** |\n| TLI × Sophistication | 1.039 | | \n| TGI × Sophistication | | 0.8987*** |\n| TLI × Age | 1.1054*** | | \n| TGI × Age | | 0.9356*** |\n| TLI × SL_User | 1.1700*** | | \n| TGI × SL_User | | 0.7409*** |\n\n*Notes: *** denotes significance at the 1% level.*\n\n---\n\n### Question\n\nConsider a 38-year-old, non-sophisticated investor (for whom the `Age` variable = 2.0 and `Soph` = 0). Based on the model results in Table 1, select all of the following statements that are correct.", "Options": {"A": "The disposition ratio (HR_winner / HR_loser) for this investor, if they are *not* a stop-loss user, is approximately 4.31.", "B": "Becoming a stop-loss user reduces this investor's disposition ratio by more than 35%.", "C": "If this investor becomes a stop-loss user, their conditional probability of selling a losing stock becomes lower than if they were not a user.", "D": "For this investor, being a stop-loss user has a larger percentage impact on their propensity to sell winners than it does on their propensity to sell losers."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to perform a multi-step calculation using the Cox model's hazard ratios. It uses a 'Computational Judgment' strategy, requiring students to calculate intermediate and final values to verify the provided statements. Options A and B are correct calculations derived from the source QA. Distractor C is a 'Sign Error' misconception, as the hazard ratio for selling losers *increases*. Distractor D is an 'Almost Right' distractor requiring a careful comparison of percentage changes (a 25.9% decrease for winners vs. a 17.0% increase for losers), a common point of confusion.", "qid": "400", "question": "### Background\n\n**Research Question.** This case investigates the relative importance of three investor characteristics—sophistication, experience (proxied by age), and the use of stop-loss orders—in mitigating the disposition effect. The analysis aims to determine which factors remain significant when all are considered simultaneously.\n\n**Setting / Data-Generating Environment.** The analysis uses a full Cox proportional hazard model including all three characteristics and their interactions with the trading gain/loss indicators. This allows for a direct comparison of the marginal contribution of each factor, controlling for the others.\n\n**Variables & Parameters.**\n- `TLI(t)`: Trading Loss Indicator, a dummy variable equal to 1 if a position is at a loss.\n- `TGI(t)`: Trading Gain Indicator, a dummy variable equal to 1 if a position is at a gain.\n- `Soph`: A time-invariant dummy variable equal to 1 for sophisticated investors (warrant traders).\n- `Age`: A continuous variable defined as `(investor's actual age - 18) / 10`. Hazard ratios reflect the effect of a 10-year increase in age.\n- `SL_User`: A time-invariant dummy variable equal to 1 for investors who have used a stop-loss order at least once.\n\n---\n\n### Data / Model Specification\n\nThe study estimates a comprehensive model with multiple interaction terms. The key results from this full specification are summarized in Table 1.\n\n**Table 1: Combined Model of Predictive Variables**\n\n| Variable | Hazard Ratio (Loss Model) | Hazard Ratio (Gain Model) |\n| :--- | :--- | :--- |\n| TLI | 0.3948*** | | \n| TGI | | 2.3777*** |\n| TLI × Sophistication | 1.039 | | \n| TGI × Sophistication | | 0.8987*** |\n| TLI × Age | 1.1054*** | | \n| TGI × Age | | 0.9356*** |\n| TLI × SL_User | 1.1700*** | | \n| TGI × SL_User | | 0.7409*** |\n\n*Notes: *** denotes significance at the 1% level.*\n\n---\n\n### Question\n\nConsider a 38-year-old, non-sophisticated investor (for whom the `Age` variable = 2.0 and `Soph` = 0). Based on the model results in Table 1, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "I need to analyze the hazard ratios for a 38-year-old non-sophisticated investor and calculate the disposition ratios with and without stop-loss usage.\n\nGiven information:\n- Age = 38, so Age variable = (38-18)/10 = 2.0\n- Soph = 0 (non-sophisticated)\n- Need to compare scenarios with SL_User = 0 and SL_User = 1\n\n**Step 1: Calculate hazard ratios without stop-loss (SL_User = 0)**\n\nFor losses (HR_loser):\n- Base TLI effect: 0.3948\n- TLI × Age effect: 1.1054^2 = 1.2219\n- Combined: 0.3948 × 1.2219 = 0.4826\n\nFor gains (HR_winner):\n- Base TGI effect: 2.3777\n- TGI × Age effect: 0.9356^2 = 0.8754\n- Combined: 2.3777 × 0.8754 = 2.0817\n\nDisposition ratio = HR_winner / HR_loser = 2.0817 / 0.4826 = 4.314\n\n**Step 2: Calculate hazard ratios with stop-loss (SL_User = 1)**\n\nFor losses (HR_loser):\n- Base effects: 0.3948 × 1.2219 = 0.4826\n- SL_User effect: 1.1700\n- Combined: 0.4826 × 1.1700 = 0.5646\n\nFor gains (HR_winner):\n- Base effects: 2.3777 × 0.8754 = 2.0817\n- SL_User effect: 0.7409\n- Combined: 2.0817 × 0.7409 = 1.5423\n\nDisposition ratio = HR_winner / HR_loser = 1.5423 / 0.5646 = 2.732\n\n**Step 3: Evaluate each statement**\n\nA) Disposition ratio without stop-loss = 4.314 ", "model_answer": ["A"], "error": null}}
{"ID": 297, "Question": "### Background\n\n**Research Question.** How do real-world frictions like transaction costs and differences in asset characteristics affect the viability of sophisticated trading strategies, particularly at institutional scale?\n\n**Setting.** A large fund attempts to implement the ABMA size/BM rotation strategy. The paper's reported `\\alpha_{net}` of 2.62% only accounts for effective bid-ask spreads and does not include the price impact of large trades, which can be a significant cost for large asset managers.\n\n---\n\n### Data / Model Specification\n\nConsider a $1 billion fund implementing the ABMA size/BM strategy. The strategy longs/shorts portfolios composed entirely of small-cap stocks. Assume the following parameters:\n- **AUM:** $1,000,000,000\n- **Quarterly Turnover:** 40%\n- **Trade Concentration:** Trades are concentrated in 100 small-cap stocks.\n- **Average Daily Volume (ADV):** $5,000,000 per stock.\n- **Price Impact Model:** The one-way cost per trade is `PI = \\lambda \\cdot (Q / ADV)`, where `Q` is the trade size in dollars and `\\lambda=0.1` is a price impact parameter.\n\n---\n\n### Question\n\nGiven the parameters above, which of the following statements about the additional price impact costs faced by the fund are correct?", "Options": {"A": "The one-way price impact cost for an average trade is 4.0%.", "B": "After accounting for these price impact costs, the strategy's reported net alpha of 2.62% would remain positive.", "C": "The total quarterly price impact cost for the fund amounts to 1.6% of its AUM.", "D": "The total annualized price impact cost is approximately 3.2%."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to apply a theoretical model of transaction costs to a practical scenario, a **Computational Judgment** task. It requires a multi-step calculation to evaluate the viability of a strategy at institutional scale. Distractor C is a 'Step-Omission Error' (incorrect annualization), and Distractor D is a 'Conceptual Opposite' that ignores the calculation's outcome, testing whether the student can connect their computation to a final conclusion.", "qid": "297", "question": "### Background\n\n**Research Question.** How do real-world frictions like transaction costs and differences in asset characteristics affect the viability of sophisticated trading strategies, particularly at institutional scale?\n\n**Setting.** A large fund attempts to implement the ABMA size/BM rotation strategy. The paper's reported `\\alpha_{net}` of 2.62% only accounts for effective bid-ask spreads and does not include the price impact of large trades, which can be a significant cost for large asset managers.\n\n---\n\n### Data / Model Specification\n\nConsider a $1 billion fund implementing the ABMA size/BM strategy. The strategy longs/shorts portfolios composed entirely of small-cap stocks. Assume the following parameters:\n- **AUM:** $1,000,000,000\n- **Quarterly Turnover:** 40%\n- **Trade Concentration:** Trades are concentrated in 100 small-cap stocks.\n- **Average Daily Volume (ADV):** $5,000,000 per stock.\n- **Price Impact Model:** The one-way cost per trade is `PI = \\lambda \\cdot (Q / ADV)`, where `Q` is the trade size in dollars and `\\lambda=0.1` is a price impact parameter.\n\n---\n\n### Question\n\nGiven the parameters above, which of the following statements about the additional price impact costs faced by the fund are correct?", "model_evaluation_result": {"model_raw_response": "I need to calculate the price impact costs for this fund implementing the ABMA strategy.\n\nGiven parameters:\n- AUM: $1,000,000,000\n- Quarterly turnover: 40%\n- Trades concentrated in 100 small-cap stocks\n- Average Daily Volume (ADV): $5,000,000 per stock\n- Price Impact Model: PI = λ × (Q/ADV), where λ = 0.1\n\nStep 1: Calculate quarterly trading volume\nQuarterly trading volume = AUM × Quarterly turnover = $1,000,000,000 × 0.40 = $400,000,000\n\nStep 2: Calculate average trade size per stock\nSince trades are concentrated in 100 stocks:\nAverage trade size per stock (Q) = $400,000,000 / 100 = $4,000,000\n\nStep 3: Calculate one-way price impact cost\nPI = λ × (Q/ADV) = 0.1 × ($4,000,000 / $5,000,000) = 0.1 × 0.8 = 0.08 = 8.0%\n\nThis eliminates option A (which states 4.0%).\n\nStep 4: Calculate total quarterly price impact cost\nSince turnover is 40% quarterly, this represents one-way trading.\nTotal quarterly PI cost = 40% × 8.0% = 3.2% of AUM\n\nThis eliminates option C (which states 1.6%).\n\nStep 5: Calculate annualized price impact cost\nAnnualized PI cost = 3.2% × 4 quarters = 12.8%\n\nThis eliminates option D (which states 3.2%).\n\nStep 6: Check if net alpha remains positive\nOriginal net alpha = 2.62%\nAnnualized price impact cost = 12.8%\nAlpha after price impact = 2.62% - 12.8% = -10.18%\n\nThis eliminates option B (the strategy would have negative alpha).\n\nNone of the options are correct.\n\n\"\"", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 314, "Question": "### Background\n\n**Research Question.** A researcher aims to test for money endogeneity using time-series data. However, valid causal inference requires addressing two key econometric challenges: potential structural breaks in the data due to shifts in policy regimes, and the non-stationarity of the economic variables.\n\n**Setting / Data-Generating Environment.** The analysis uses quarterly data for the United Kingdom from 1975 to 2006. The researcher suspects a structural break in 1992:Q4. The key variables, Bank Loans (`BL`) and Money Supply (`MS`), are found to be integrated of order one, I(1).\n\n**Variables & Parameters.**\n- `I(1)`: Integrated of order one; a non-stationary time series whose first difference is stationary.\n- `UK 1`: The sub-period from 1975:3 to 1992:3.\n- `UK 2`: The sub-period from 1992:4 to 2006:2.\n\n---\n\n### Data / Model Specification\n\n**1. Structural Break Test:** The Chow test is used to check for a structural break at 1992:Q4. The null hypothesis is parameter stability.\n\n**Table 1: Chow Breakpoint Test Results for UK**\n\n| Test | Statistic | Significance |\n| :--- | :--- | :--- |\n| F-statistic | 152.28 | *** |\n| Log Likelihood Ratio | 156.68 | *** |\n\n*Note: *** indicates significance at the 1% level.*\n\n**2. Cointegration Test:** After splitting the sample, the Johansen cointegration test is run on each sub-period to check for a stable long-run relationship between `BL` and `MS`. The null hypothesis is no cointegration.\n\n**Table 2: Johansen Cointegration Test Results for BL and MS**\n\n| Sample | Trace Test | Max Eigenvalue Test | Conclusion |\n| :--- | :--- | :--- | :--- |\n| UK 1 | 12.01 | 8.15 | No Cointegration |\n| UK 2 | 28.58*** | 24.88*** | Cointegration |\n\n*Note: *** indicates significance at the 1% level.*\n\n---\n\n### Question\n\nBased on the diagnostic tests for the UK time-series data, select **all** statements that represent a correct methodological conclusion or procedural step.", "Options": {"A": "The Johansen test results (Table 2) imply that a Vector Error-Correction Model (VECM) is the appropriate specification for the UK 2 period, while a VAR in first differences should be used for the UK 1 period.", "B": "The highly significant Chow test statistics (Table 1) provide strong evidence of a structural break, making it necessary to split the sample to avoid biased inferences from pooling two distinct data-generating processes.", "C": "For the UK 2 period, the absence of cointegration means that any observed relationship between Bank Loans and Money Supply is likely spurious.", "D": "According to the results, a single VECM model applied to the full 1975-2006 sample is the most powerful way to test for a long-run relationship."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the user's understanding of the correct econometric workflow for time-series analysis with structural breaks. It uses a Premise Packaging strategy, asking the user to select all valid methodological steps. Correct options (A, B) confirm the interpretation of diagnostic tests and the subsequent choice of model. Distractors are based on clear methodological errors: a Conceptual Opposite (C: misreading the cointegration result for UK 2) and a Step-Omission Error (D: ignoring the structural break test result).", "qid": "314", "question": "### Background\n\n**Research Question.** A researcher aims to test for money endogeneity using time-series data. However, valid causal inference requires addressing two key econometric challenges: potential structural breaks in the data due to shifts in policy regimes, and the non-stationarity of the economic variables.\n\n**Setting / Data-Generating Environment.** The analysis uses quarterly data for the United Kingdom from 1975 to 2006. The researcher suspects a structural break in 1992:Q4. The key variables, Bank Loans (`BL`) and Money Supply (`MS`), are found to be integrated of order one, I(1).\n\n**Variables & Parameters.**\n- `I(1)`: Integrated of order one; a non-stationary time series whose first difference is stationary.\n- `UK 1`: The sub-period from 1975:3 to 1992:3.\n- `UK 2`: The sub-period from 1992:4 to 2006:2.\n\n---\n\n### Data / Model Specification\n\n**1. Structural Break Test:** The Chow test is used to check for a structural break at 1992:Q4. The null hypothesis is parameter stability.\n\n**Table 1: Chow Breakpoint Test Results for UK**\n\n| Test | Statistic | Significance |\n| :--- | :--- | :--- |\n| F-statistic | 152.28 | *** |\n| Log Likelihood Ratio | 156.68 | *** |\n\n*Note: *** indicates significance at the 1% level.*\n\n**2. Cointegration Test:** After splitting the sample, the Johansen cointegration test is run on each sub-period to check for a stable long-run relationship between `BL` and `MS`. The null hypothesis is no cointegration.\n\n**Table 2: Johansen Cointegration Test Results for BL and MS**\n\n| Sample | Trace Test | Max Eigenvalue Test | Conclusion |\n| :--- | :--- | :--- | :--- |\n| UK 1 | 12.01 | 8.15 | No Cointegration |\n| UK 2 | 28.58*** | 24.88*** | Cointegration |\n\n*Note: *** indicates significance at the 1% level.*\n\n---\n\n### Question\n\nBased on the diagnostic tests for the UK time-series data, select **all** statements that represent a correct methodological conclusion or procedural step.", "model_evaluation_result": {"model_raw_response": "Looking at the diagnostic test results:\n\n**Table 1 (Chow Test):** Both F-statistic (152.28) and Log Likelihood Ratio (156.68) are highly significant at the 1% level (***), strongly rejecting the null hypothesis of parameter stability. This indicates a structural break at 1992:Q4.\n\n**Table 2 (Johansen Cointegration Test):**\n- UK 1 (1975:3-1992:3): No significance markers, indicating failure to reject the null of no cointegration\n- UK 2 (1992:4-2006:2): Both tests significant at 1% (***), rejecting the null and indicating cointegration exists\n\nEvaluating each option:\n\n**Option A:** ✓ CORRECT\n- UK 2 has cointegration → VECM is appropriate (captures long-run relationship)\n- UK 1 has no cointegration → VAR in first differences is appropriate (since variables are I(1))\n\n**Option B:** ✓ CORRECT\n- The highly significant Chow test provides strong evidence of structural break\n- Splitting the sample is necessary to avoid pooling two different data-generating processes\n\n**Option C:** ✗ INCORRECT\n- UK 2 shows **presence** of cointegration (not absence)\n- With cointegration, the relationship is not spurious but represents a genuine long-run equilibrium\n\n**Option D:** ✗ INCORRECT\n- The structural break makes a single model inappropriate\n- Different cointegration properties in each sub-period require different model specifications\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 208, "Question": "### Background\n\nThe analysis compares summary statistics for FHA loan vintages (2007-2009) compiled using two different data structures: a **Borrower-Based** framework and a **Mortgage-Based** framework. The key distinction is the treatment of internal FHA-to-FHA refinances.\n\n*   **Borrower Data:** Tracks a borrower across all their FHA loans. A \"Prepay\" event occurs only when the borrower exits the FHA system entirely (e.g., sells the house). A \"Default\" is recorded for the borrower's original vintage, regardless of intervening refinances.\n*   **Mortgage Data:** Treats each loan as a separate entity. A \"Prepay\" event includes both true exits and internal FHA refinances. A default is attributed to the vintage of the specific loan that defaulted.\n\n### Data / Model Specification\n\n**Table 1: FHA Sustainability—Borrower vs. Mortgage Data (%)**\n\n| Vintage | Borrower Data: Default | Borrower Data: Prepay | Mortgage Data: Default | Mortgage Data: Prepay |\n| :--- | :--- | :--- | :--- | :--- |\n| 2007 | 42.7 | 10.9 | 32.3 | 32.7 |\n| 2008 | 29.6 | 11.2 | 21.0 | 37.0 |\n\n*Notes: Data as of September 2011. Default is ever reaching 90-days delinquent.*\n\n### Question\n\nBased on the data for the **2007 vintage** in Table 1, which of the following statements are correct quantitative deductions?", "Options": {"A": "Approximately two-thirds (66.7%) of the events recorded as \"Prepay\" in the mortgage data are actually internal refinances where credit risk remains with the FHA.", "B": "The rate of internal refinances for the 2007 vintage is 32.7%.", "C": "The mortgage-based framework understates the true default rate attributable to the 2007 cohort by 10.4 percentage points.", "D": "The mortgage-based framework overstates the true prepayment rate by a factor of 3.0."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: The item assesses the ability to perform multi-step calculations and interpret the quantitative discrepancies between the two data frameworks, which is the paper's core empirical finding. Depth Strategy: Computational Judgment. Distractor Logic: Option A requires calculating the ratio `32.7 / 10.9`. Option B requires a two-step calculation: first finding the rate of internal refinances (`32.7 - 10.9 = 21.8`), then finding its proportion of the total recorded prepayments (`21.8 / 32.7`). Option C requires a simple subtraction (`42.7 - 32.3`). Option D is a 'Formula Misuse' distractor, incorrectly equating the total mortgage prepay rate with the internal refinance rate, omitting the subtraction of true prepayments.", "qid": "208", "question": "### Background\n\nThe analysis compares summary statistics for FHA loan vintages (2007-2009) compiled using two different data structures: a **Borrower-Based** framework and a **Mortgage-Based** framework. The key distinction is the treatment of internal FHA-to-FHA refinances.\n\n*   **Borrower Data:** Tracks a borrower across all their FHA loans. A \"Prepay\" event occurs only when the borrower exits the FHA system entirely (e.g., sells the house). A \"Default\" is recorded for the borrower's original vintage, regardless of intervening refinances.\n*   **Mortgage Data:** Treats each loan as a separate entity. A \"Prepay\" event includes both true exits and internal FHA refinances. A default is attributed to the vintage of the specific loan that defaulted.\n\n### Data / Model Specification\n\n**Table 1: FHA Sustainability—Borrower vs. Mortgage Data (%)**\n\n| Vintage | Borrower Data: Default | Borrower Data: Prepay | Mortgage Data: Default | Mortgage Data: Prepay |\n| :--- | :--- | :--- | :--- | :--- |\n| 2007 | 42.7 | 10.9 | 32.3 | 32.7 |\n| 2008 | 29.6 | 11.2 | 21.0 | 37.0 |\n\n*Notes: Data as of September 2011. Default is ever reaching 90-days delinquent.*\n\n### Question\n\nBased on the data for the **2007 vintage** in Table 1, which of the following statements are correct quantitative deductions?", "model_evaluation_result": {"model_raw_response": "Looking at the 2007 vintage data:\n- Borrower Data: Default = 42.7%, Prepay = 10.9%\n- Mortgage Data: Default = 32.3%, Prepay = 32.7%\n\nLet me analyze each option:\n\n**Option A**: The mortgage data shows 32.7% prepayments, while borrower data shows 10.9% true exits. The difference (32.7% - 10.9% = 21.8%) represents internal refinances. The proportion of internal refinances to total mortgage prepayments is 21.8/32.7 = 66.7%. ✓\n\n**Option B**: The rate of internal refinances is the difference between mortgage prepayments and borrower prepayments: 32.7% - 10.9% = 21.8%, not 32.7%. ✗\n\n**Option C**: The borrower-based default rate (42.7%) captures all defaults from the 2007 cohort, while the mortgage-based rate (32.3%) misses some defaults that occur after refinancing. The difference is 42.7% - 32.3% = 10.4 percentage points. ✓\n\n**Option D**: The mortgage-based prepayment rate is 32.7% and the true prepayment rate (borrower exits) is 10.9%. The ratio is 32.7/10.9 = 3.0. ✓\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 455, "Question": "### Background\n\nIn econometric analyses using instrumental variables, such as the Generalized Method of Moments (GMM), it is crucial to validate the model's specification. The Hansen J-test of overidentifying restrictions is a key diagnostic tool for this purpose.\n\n---\n\nWhich of the following statements about the Hansen J-test are correct? (Select all that apply)", "Options": {"A": "Under the null hypothesis, the test statistic follows a standard normal distribution.", "B": "The test is applicable only when the number of instruments is strictly greater than the number of endogenous variables being instrumented.", "C": "The null hypothesis is that the instruments are jointly valid (i.e., uncorrelated with the error term).", "D": "A low p-value (e.g., < 0.05) provides confidence that the instruments are valid."}, "Answer": ["B", "C"], "pi_justification": "This item assesses knowledge of a fundamental econometric specification test. The strategy is factual recall. The distractors are designed based on high-frequency misconceptions: B (Conceptual Opposite: a high p-value is desired), C (Correct: this is the definition of 'overidentifying'), and D (Formula Misuse: the statistic follows a chi-squared, not normal, distribution).", "qid": "455", "question": "### Background\n\nIn econometric analyses using instrumental variables, such as the Generalized Method of Moments (GMM), it is crucial to validate the model's specification. The Hansen J-test of overidentifying restrictions is a key diagnostic tool for this purpose.\n\n---\n\nWhich of the following statements about the Hansen J-test are correct? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at each statement about the Hansen J-test:\n\nA) Under the null hypothesis, the test statistic follows a standard normal distribution.\nThis is incorrect. The Hansen J-test statistic follows a chi-squared distribution with degrees of freedom equal to the number of overidentifying restrictions (number of instruments minus number of parameters), not a standard normal distribution.\n\nB) The test is applicable only when the number of instruments is strictly greater than the number of endogenous variables being instrumented.\nThis is correct. The Hansen J-test is a test of overidentifying restrictions, which means there must be more instruments than endogenous variables to be instrumented. If the model is exactly identified (equal numbers), there are no overidentifying restrictions to test.\n\nC) The null hypothesis is that the instruments are jointly valid (i.e., uncorrelated with the error term).\nThis is correct. The Hansen J-test's null hypothesis is that all instruments are valid, meaning they satisfy the exclusion restriction (uncorrelated with the error term) and are correctly excluded from the structural equation.\n\nD) A low p-value (e.g., < 0.05) provides confidence that the instruments are valid.\nThis is incorrect. A low p-value leads to rejection of the null hypothesis, which means the instruments are NOT valid. A high p-value (failing to reject the null) is what provides support for instrument validity.\n\nB,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 442, "Question": "### Background\n\n**Research Question.** How does an investor optimally liquidate inventory when market liquidity switches between high and low states?\n\n**Setting.** The market liquidity, parameterized by `\\lambda`, follows a two-state Markov chain: an \"active\" state (0, high `\\lambda_0`) and a \"slow\" state (1, low `\\lambda_1`), with `\\lambda_0 > \\lambda_1`. The investor observes the current state and adjusts their strategy. The analysis is in the continuous, infinite-horizon fluid limit for a power-law LOB where `\\Lambda(s) = \\lambda s^{-\\alpha}`.\n\n**Variables & Parameters.**\n- `u(x)`: Value function when in the active state (0) with inventory `x`.\n- `w(x)`: Value function when in the slow state (1) with inventory `x`.\n- `s_0^*(x), s_1^*(x)`: Optimal spreads in the active and slow states, respectively.\n\n---\n\n### Data / Model Specification\n\nThe value functions `u(x)` and `w(x)` solve a system of coupled ODEs. The solution has the properties that `u(x) > w(x)` and the marginal value is also higher in the active state, `u_x(x) > w_x(x)`. The optimal spread is proportional to the marginal value: `s^* \\propto u_x` or `s^* \\propto w_x`.\n\n---\n\n### Question\n\nBased on the model's specification and results, select all of the following statements that are correct descriptions of the optimal strategy and its underlying values.", "Options": {"A": "The marginal value of inventory is lower in the active state than in the slow state, i.e., `u_x(x) < w_x(x)`.", "B": "To take advantage of high liquidity, the investor posts a lower (more aggressive) spread in the active state.", "C": "The value function in the active state is higher than in the slow state, i.e., `u(x) > w(x)`.", "D": "The optimal spread is higher in the active state than in the slow state, i.e., `s_0^*(x) > s_1^*(x)`."}, "Answer": ["C", "D"], "pi_justification": "REPLACE with Multiple Choice (Score: 9.0). This item assesses the core results of the regime-switching model. It uses an Atomic Decomposition strategy to test understanding of the relationships between value functions, marginal values, and the paradoxical optimal spread. Distractor C is a Conceptual Opposite of a key premise. Distractor D represents the intuitive but incorrect reasoning that the paradox is designed to challenge, making it a high-fidelity misconception.", "qid": "442", "question": "### Background\n\n**Research Question.** How does an investor optimally liquidate inventory when market liquidity switches between high and low states?\n\n**Setting.** The market liquidity, parameterized by `\\lambda`, follows a two-state Markov chain: an \"active\" state (0, high `\\lambda_0`) and a \"slow\" state (1, low `\\lambda_1`), with `\\lambda_0 > \\lambda_1`. The investor observes the current state and adjusts their strategy. The analysis is in the continuous, infinite-horizon fluid limit for a power-law LOB where `\\Lambda(s) = \\lambda s^{-\\alpha}`.\n\n**Variables & Parameters.**\n- `u(x)`: Value function when in the active state (0) with inventory `x`.\n- `w(x)`: Value function when in the slow state (1) with inventory `x`.\n- `s_0^*(x), s_1^*(x)`: Optimal spreads in the active and slow states, respectively.\n\n---\n\n### Data / Model Specification\n\nThe value functions `u(x)` and `w(x)` solve a system of coupled ODEs. The solution has the properties that `u(x) > w(x)` and the marginal value is also higher in the active state, `u_x(x) > w_x(x)`. The optimal spread is proportional to the marginal value: `s^* \\propto u_x` or `s^* \\propto w_x`.\n\n---\n\n### Question\n\nBased on the model's specification and results, select all of the following statements that are correct descriptions of the optimal strategy and its underlying values.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the optimal liquidation strategy when market liquidity switches between high and low states.\n\nGiven information:\n- Active state (0): high liquidity λ₀\n- Slow state (1): low liquidity λ₁, where λ₀ > λ₁\n- Value functions: u(x) for active state, w(x) for slow state\n- Key results: u(x) > w(x) and u_x(x) > w_x(x)\n- Optimal spread is proportional to marginal value\n\nLet me evaluate each option:\n\n**Option A**: States u_x(x) < w_x(x)\nThis directly contradicts the given information that u_x(x) > w_x(x). \n**FALSE**\n\n**Option B**: Lower spread in active state to take advantage of high liquidity\nSince s* ∝ u_x or s* ∝ w_x, and we know u_x(x) > w_x(x), this means s₀*(x) > s₁*(x). So the spread is actually higher in the active state, not lower.\n**FALSE**\n\n**Option C**: u(x) > w(x)\nThis is explicitly stated in the problem.\n**TRUE**\n\n**Option D**: s₀*(x) > s₁*(x)\nSince optimal spread is proportional to marginal value, and u_x(x) > w_x(x), we have s₀*(x) > s₁*(x).\n**TRUE**\n\nThe key insight is that in the active (high liquidity) state, the investor has higher marginal value of inventory because they can more easily liquidate in the future. This leads them to be more patient and post higher spreads, waiting for better prices rather than rushing to liquidate.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 391, "Question": "### Background\n\n**Research Question.** Is the initial underpricing of a Seasoned Equity Offering (SEO) a credible signal of the issuing firm's superior future performance?\n\n**Setting.** The signaling hypothesis of underpricing posits that high-quality firms are willing to underprice their shares to signal private information about strong future prospects. This implies a positive relationship between initial underpricing and subsequent stock performance.\n\n**Variables & Parameters.**\n- `R_1`: Offer-to-close return, a measure of initial underpricing (percentage).\n- `Adjusted BHR_{+60}`: Market-adjusted buy-and-hold return for 60 trading days after the offer (percentage).\n- `CRETP60`: The cumulative raw return from the offer to 60 trading days subsequent to the offer.\n\n---\n\n### Data / Model Specification\n\nThe study examines the link between underpricing and performance using both cross-sectional comparisons and regression analysis.\n\n**Table 1. SEO Underpricing and Performance by REIT Property Type**\n\n| REIT Type | Initial Underpricing (`R_1`, %) | 60-Day Adjusted BHR (%) |\n| :--- | :--- | :--- |\n| Office | 1.35* | 2.47* |\n| Malls | 0.25 | -0.10 |\n\n*Source: Synthesized from Table 2 and Table 7 in the source document. An asterisk (*) denotes statistical significance.*\n\n**Table 2. Regression of Underpricing on Post-Offer Returns**\n\n| Dependent Variable | Independent Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- |\n| Underpricing (`R_1`) | CRETP60 | 0.077 | (4.85)* |\n\n*Source: Synthesized from Model 4, Table 8 in the source document. An asterisk (*) denotes statistical significance.*\n\n---\n\n### Question\n\nThe signaling hypothesis suggests that underpricing is a credible signal of a firm's superior future performance. Select all empirical findings presented that are consistent with this hypothesis.", "Options": {"A": "The study finds that all REIT property types with significant underpricing had negative subsequent market-adjusted returns, contradicting the signaling hypothesis.", "B": "The regression coefficient linking underpricing to future returns was positive but not statistically significant, providing only weak support for the signaling hypothesis.", "C": "Regression analysis for the full sample shows a statistically significant positive relationship between the initial underpricing (`R_1`) and the subsequent 60-day raw return (`CRETP60`).", "D": "In a cross-sectional comparison, Office REITs exhibited high initial underpricing and strong positive 60-day market-adjusted returns, while Malls REITs showed low underpricing and negative returns."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to synthesize two different forms of evidence (cross-sectional and regression) supporting the signaling hypothesis. It uses an **Atomic Decomposition** strategy, presenting each piece of evidence as a separate, verifiable statement. Correct options (A, B) accurately describe the results from Table 1 and Table 2. Distractors C and D are **Factual Errors** that directly contradict the data presented in the tables, testing for careful reading and interpretation.", "qid": "391", "question": "### Background\n\n**Research Question.** Is the initial underpricing of a Seasoned Equity Offering (SEO) a credible signal of the issuing firm's superior future performance?\n\n**Setting.** The signaling hypothesis of underpricing posits that high-quality firms are willing to underprice their shares to signal private information about strong future prospects. This implies a positive relationship between initial underpricing and subsequent stock performance.\n\n**Variables & Parameters.**\n- `R_1`: Offer-to-close return, a measure of initial underpricing (percentage).\n- `Adjusted BHR_{+60}`: Market-adjusted buy-and-hold return for 60 trading days after the offer (percentage).\n- `CRETP60`: The cumulative raw return from the offer to 60 trading days subsequent to the offer.\n\n---\n\n### Data / Model Specification\n\nThe study examines the link between underpricing and performance using both cross-sectional comparisons and regression analysis.\n\n**Table 1. SEO Underpricing and Performance by REIT Property Type**\n\n| REIT Type | Initial Underpricing (`R_1`, %) | 60-Day Adjusted BHR (%) |\n| :--- | :--- | :--- |\n| Office | 1.35* | 2.47* |\n| Malls | 0.25 | -0.10 |\n\n*Source: Synthesized from Table 2 and Table 7 in the source document. An asterisk (*) denotes statistical significance.*\n\n**Table 2. Regression of Underpricing on Post-Offer Returns**\n\n| Dependent Variable | Independent Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- | :--- |\n| Underpricing (`R_1`) | CRETP60 | 0.077 | (4.85)* |\n\n*Source: Synthesized from Model 4, Table 8 in the source document. An asterisk (*) denotes statistical significance.*\n\n---\n\n### Question\n\nThe signaling hypothesis suggests that underpricing is a credible signal of a firm's superior future performance. Select all empirical findings presented that are consistent with this hypothesis.", "model_evaluation_result": {"model_raw_response": "Looking at the empirical findings presented:\n\nFrom Table 1:\n- Office REITs: Initial underpricing = 1.35%*, 60-Day Adjusted BHR = 2.47%* (both statistically significant)\n- Mall REITs: Initial underpricing = 0.25%, 60-Day Adjusted BHR = -0.10% (neither statistically significant)\n\nFrom Table 2:\n- Regression shows coefficient of 0.077 with t-statistic of 4.85* (statistically significant) for the relationship between underpricing (R_1) and CRETP60\n\nEvaluating each option:\n\nA) False - Office REITs had significant underpricing (1.35%*) and positive subsequent returns (2.47%*), not negative returns.\n\nB) False - The regression coefficient (0.077) was statistically significant with t-statistic of 4.85*, not insignificant.\n\nC) True - The regression shows a coefficient of 0.077 with t-statistic of 4.85*, indicating a statistically significant positive relationship between initial underpricing and subsequent 60-day raw returns.\n\nD) True - Office REITs had high underpricing (1.35%*) with strong positive returns (2.47%*), while Mall REITs had low underpricing (0.25%) with negative returns (-0.10%), consistent with the signaling hypothesis.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 108, "Question": "### Background\n\nCorporate governance literature distinguishes between the **monitoring role** and the **advisory role** of outside directors. The monitoring role focuses on constraining managerial misbehavior, while the advisory role involves providing expert counsel to improve strategic decisions.\n\n### Data / Model Specification\n\nThe paper's hypotheses link these roles to corporate debt policy:\n*   **Hypothesis 2 (H2):** Posits a correlation between the proportion of outside directors and corporate debt, which relates to the board's **monitoring** function. A substitutive relationship (negative correlation) implies strong board monitoring replaces the need for debt's disciplinary role.\n*   **Hypothesis 3 (H3):** Posits that outside directors magnify the negative correlation between tax aggressiveness and debt. This is explained by the board's **advisory** function, where expert directors provide better counsel on making optimal capital structure adjustments in response to changes in the firm's non-debt tax shields.\n\n---\n\nSelect all statements that correctly apply these theoretical concepts.\n", "Options": {"A": "According to the logic of H3, a board with more outside directors is expected to react more strongly to an increase in non-debt tax shields by pushing for a larger reduction in corporate debt.", "B": "A board vetoing a CEO's value-destroying pet project is an example of its advisory role.", "C": "The finding of a negative correlation between outside directors and debt supports the view that these two governance mechanisms are substitutes for monitoring management.", "D": "If a firm has valuable growth options, a skilled advisory board would likely push for higher debt levels to fund these options, counteracting the debt-substitution effect."}, "Answer": ["A", "C"], "pi_justification": "This item assesses the understanding of the distinct theoretical roles of a board (monitoring vs. advisory) as presented in the paper. It uses a Scenario Application strategy. Option B correctly links the empirical finding (negative correlation) to the substitutive monitoring theory. Option C correctly interprets the paper's core argument for H3: better advice from outside directors leads to a more pronounced (magnified) debt-substitution effect. Option A is a Conceptual Opposite distractor; vetoing a project is a classic monitoring action, not an advisory one. Option D is also a Conceptual Opposite; the presence of valuable growth options increases the cost of debt (underinvestment risk), meaning a skilled board would push for *less* debt, reinforcing, not counteracting, the debt-substitution effect.", "qid": "108", "question": "### Background\n\nCorporate governance literature distinguishes between the **monitoring role** and the **advisory role** of outside directors. The monitoring role focuses on constraining managerial misbehavior, while the advisory role involves providing expert counsel to improve strategic decisions.\n\n### Data / Model Specification\n\nThe paper's hypotheses link these roles to corporate debt policy:\n*   **Hypothesis 2 (H2):** Posits a correlation between the proportion of outside directors and corporate debt, which relates to the board's **monitoring** function. A substitutive relationship (negative correlation) implies strong board monitoring replaces the need for debt's disciplinary role.\n*   **Hypothesis 3 (H3):** Posits that outside directors magnify the negative correlation between tax aggressiveness and debt. This is explained by the board's **advisory** function, where expert directors provide better counsel on making optimal capital structure adjustments in response to changes in the firm's non-debt tax shields.\n\n---\n\nSelect all statements that correctly apply these theoretical concepts.\n", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) This correctly describes H3's logic. The hypothesis states that outside directors magnify the negative correlation between tax aggressiveness and debt through their advisory function. When non-debt tax shields increase (a form of tax aggressiveness), expert outside directors would advise larger debt reductions to optimize the capital structure.\n\nB) This is incorrect. Vetoing a value-destroying project is a monitoring function (constraining managerial misbehavior), not an advisory function (providing expert counsel).\n\nC) This correctly interprets H2. A negative correlation between outside directors and debt suggests they are substitutes - strong board monitoring reduces the need for debt's disciplinary role.\n\nD) This is incorrect. Firms with valuable growth options typically avoid high debt to maintain financial flexibility and avoid underinvestment problems. A skilled advisory board would recognize this and not push for higher debt levels.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 452, "Question": "### Background\n\n**Research Question.** This question assesses the ability to identify research settings where the paper's \"signaling critique\" is most applicable.\n\n**Setting.** The signaling critique argues that an identification strategy (like IV or RD) may be misleading for a decision-maker if it estimates a partial causal effect while ignoring a plausible signaling channel.\n\n---\n\n### Data / Model Specification\n\nThe signaling critique is deemed applicable if two conditions are met:\n1.  The main explanatory (causal) variable is a choice variable of a unitary decision-maker (e.g., a CFO or board) who may have private information.\n2.  The main dependent (outcome) variable is mediated by the beliefs of a counterparty (e.g., investors, rating agencies, consumers).\n\n---\n\n### Question\n\nSelect all of the following hypothetical research questions where the signaling critique would apply, based on the two criteria provided.\n", "Options": {"A": "What is the causal effect of a firm's board voluntarily destaggering its election cycle on the firm's stock price (Tobin's Q)?", "B": "What is the causal effect of a CEO's decision to initiate a large, unannounced share repurchase program on the firm's credit default swap spreads?", "C": "What is the causal effect of an exogenous, industry-wide shock to input costs on firms' inventory management policies?", "D": "What is the causal effect of a firm's location in an earthquake-prone zone on its insurance premiums?"}, "Answer": ["A", "B"], "pi_justification": "This item assesses the application of the two-part rule for the signaling critique. It is a 'select-all-valid' task. Options (A) and (C) both meet the criteria: they feature a discretionary choice by a firm's leadership (board decision, CEO decision) and an outcome mediated by market beliefs (stock price, CDS spreads). Option (B) fails the first criterion, as location is largely not a discretionary choice intended to signal quality. Option (D) fails the first criterion, as an industry-wide shock is not a unilateral firm choice. The distractors test the ability to distinguish discretionary choices from exogenous states or shocks.", "qid": "452", "question": "### Background\n\n**Research Question.** This question assesses the ability to identify research settings where the paper's \"signaling critique\" is most applicable.\n\n**Setting.** The signaling critique argues that an identification strategy (like IV or RD) may be misleading for a decision-maker if it estimates a partial causal effect while ignoring a plausible signaling channel.\n\n---\n\n### Data / Model Specification\n\nThe signaling critique is deemed applicable if two conditions are met:\n1.  The main explanatory (causal) variable is a choice variable of a unitary decision-maker (e.g., a CFO or board) who may have private information.\n2.  The main dependent (outcome) variable is mediated by the beliefs of a counterparty (e.g., investors, rating agencies, consumers).\n\n---\n\n### Question\n\nSelect all of the following hypothetical research questions where the signaling critique would apply, based on the two criteria provided.\n", "model_evaluation_result": {"model_raw_response": "To determine where the signaling critique applies, I need to check if each option meets both criteria:\n1. The main explanatory variable is a choice variable of a unitary decision-maker who may have private information\n2. The main dependent variable is mediated by the beliefs of a counterparty\n\nLet me analyze each option:\n\n**Option A**: Board voluntarily destaggering → Stock price (Tobin's Q)\n- Criterion 1: ✓ (Board's voluntary decision to destagger is a choice by a unitary decision-maker who may have private information about firm prospects)\n- Criterion 2: ✓ (Stock price/Tobin's Q is determined by investors' beliefs about firm value)\n- **Applies**\n\n**Option B**: CEO's unannounced share repurchase → Credit default swap spreads\n- Criterion 1: ✓ (CEO's decision to initiate repurchase is a choice by a unitary decision-maker who may have private information)\n- Criterion 2: ✓ (CDS spreads reflect credit market participants' beliefs about default risk)\n- **Applies**\n\n**Option C**: Exogenous industry shock → Inventory management\n- Criterion 1: ✗ (An exogenous shock is not a choice variable)\n- Criterion 2: ✗ (Inventory management is an internal operational decision, not mediated by counterparty beliefs)\n- **Does not apply**\n\n**Option D**: Earthquake-prone location → Insurance premiums\n- Criterion 1: ✗ (Geographic location is not a choice variable in this context)\n- Criterion 2: Could be argued either way, but criterion 1 already fails\n- **Does not apply**\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 467, "Question": "### Background\n\nAn insurer's goal is to minimize the Tail Value-at-Risk (TVaR) of its post-reinsurance total risk, `T_I(X)`. The derivation of the objective function for this optimization problem is a key step in finding the optimal reinsurance contract, `I(X)`.\n\n### Data / Model Specification\n\nThe TVaR of the total risk `T_I(X)` can be expressed as:\n\n  \n\\mathrm{TVaR}_{T_{I}(X)}(p) = C(p) + (1+\\theta)\\int_{0}^{\\mathrm{VaR}_{X}(p)}I(x)dF_{X}(x) + \\delta\\int_{\\mathrm{VaR}_{X}(p)}^{\\infty}I(x)dF_{X}(x) \\quad \\text{(Eq. 1)}\n \n\nWhere:\n- `C(p)` is a term that depends on the confidence level `p` and the loss distribution `F_X(x)`, but not on the choice of `I(x)`.\n- `θ > 0` is the safety loading factor.\n- `VaR_X(p)` is the Value-at-Risk of the initial loss `X`.\n- The parameter `δ` is defined as:\n\n  \n\\delta = 1+\\theta - \\frac{1}{1-p} \\quad \\text{(Eq. 2)}\n \n\nThis derivation relies on the assumption that the ceded loss `I(X)` and the retained loss `X - I(X)` are comonotonic (i.e., they always move in the same direction).\n\n### Question\n\nBased on the structure of the TVaR objective function in Eq. 1 and the definition of `δ` in Eq. 2, which of the following conclusions are valid? Select all that apply.", "Options": {"A": "If `δ < 0`, the insurer has an incentive to cede more risk for losses `x` that fall in the extreme tail (i.e., `x > VaR_X(p)`), because the marginal benefit of TVaR reduction outweighs the marginal premium cost for those losses.", "B": "If a reinsurance contract introduces basis risk (e.g., the payout `I(X,Y)` depends on an external index `Y`), the comonotonicity assumption is violated, and the derivation in Eq. 1 would fail because the TVaR of the retained loss would be greater than the sum of the individual TVaRs.", "C": "The parameter `δ` represents the net marginal cost of ceding one unit of expected loss in the region below the `p`-quantile (i.e., for `x < VaR_X(p)`).", "D": "If the safety loading `θ` is very low and the confidence level `p` is very high (e.g., `p=0.99`), it is more likely that `δ` will be negative."}, "Answer": ["A", "D"], "pi_justification": "This item assesses the student's ability to interpret the mathematical structure of the TVaR objective function, a core analytical result of the paper. It uses a 'Reverse-Reasoning' strategy, asking for valid conclusions from the given formula. Option A correctly interprets the economic trade-off governed by `δ`. Option C requires 'Computational Judgment' to assess how the inputs `θ` and `p` affect the sign of `δ`. Option B is a 'Conceptual Distractor' (Almost Right), as `δ` applies to the tail *above* the quantile, not below. Option D is also 'Almost Right' but contains a sign error; basis risk introduces a diversification benefit, so the TVaR of the retained loss would be *less than* the sum, not greater.", "qid": "467", "question": "### Background\n\nAn insurer's goal is to minimize the Tail Value-at-Risk (TVaR) of its post-reinsurance total risk, `T_I(X)`. The derivation of the objective function for this optimization problem is a key step in finding the optimal reinsurance contract, `I(X)`.\n\n### Data / Model Specification\n\nThe TVaR of the total risk `T_I(X)` can be expressed as:\n\n  \n\\mathrm{TVaR}_{T_{I}(X)}(p) = C(p) + (1+\\theta)\\int_{0}^{\\mathrm{VaR}_{X}(p)}I(x)dF_{X}(x) + \\delta\\int_{\\mathrm{VaR}_{X}(p)}^{\\infty}I(x)dF_{X}(x) \\quad \\text{(Eq. 1)}\n \n\nWhere:\n- `C(p)` is a term that depends on the confidence level `p` and the loss distribution `F_X(x)`, but not on the choice of `I(x)`.\n- `θ > 0` is the safety loading factor.\n- `VaR_X(p)` is the Value-at-Risk of the initial loss `X`.\n- The parameter `δ` is defined as:\n\n  \n\\delta = 1+\\theta - \\frac{1}{1-p} \\quad \\text{(Eq. 2)}\n \n\nThis derivation relies on the assumption that the ceded loss `I(X)` and the retained loss `X - I(X)` are comonotonic (i.e., they always move in the same direction).\n\n### Question\n\nBased on the structure of the TVaR objective function in Eq. 1 and the definition of `δ` in Eq. 2, which of the following conclusions are valid? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each option based on the TVaR objective function and the definition of δ.\n\nLet me start by understanding the structure:\n- The objective is to minimize TVaR of T_I(X)\n- δ = 1 + θ - 1/(1-p)\n- The TVaR formula has two integral terms with coefficients (1+θ) and δ\n\n**Option A Analysis:**\nIf δ < 0, then in the second integral (for x > VaR_X(p)), increasing I(x) would decrease the TVaR since we're minimizing and the coefficient is negative. This means the insurer benefits from ceding more risk in the extreme tail region. This is correct.\n\n**Option B Analysis:**\nThe comonotonicity assumption is crucial for the TVaR decomposition. If basis risk is introduced (where payout depends on an external index Y), the ceded and retained losses are no longer comonotonic. However, the statement that \"TVaR of the retained loss would be greater than the sum of individual TVaRs\" is incorrect. TVaR is subadditive for general risks, meaning TVaR(A+B) ≤ TVaR(A) + TVaR(B). The derivation would fail, but not for the reason stated.\n\n**Option C Analysis:**\nLooking at the first integral (for x < VaR_X(p)), the coefficient is (1+θ), not δ. The parameter δ applies to the region above VaR_X(p), not below it. This statement is incorrect.\n\n**Option D Analysis:**\nLet's check when δ < 0:\n- δ = 1 + θ - 1/(1-p) < 0\n- This means 1 + θ < 1/(1-p)\n\nFor p = 0.99: 1/(1-0.99) = 100\nSo we need 1 + θ < 100, or θ < 99\n\nThis is satisfied for any reasonable safety loading (typically θ is much smaller than 99). So with low θ and high p, δ is indeed more likely to be negative. This is correct.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 396, "Question": "### Background\n\nThe solution to the transformed Thiele PDE system can be expressed as a 'mild solution' using the Duhamel formula. This integral representation is crucial for the paper's analysis as it does not require the reserve function to be classically differentiable.\n\n### Data / Model Specification\n\nThe transformed Thiele PDE system is:\n  \n\\partial_{\\tau}{\\bf V}(\\tau) = \\left({\\mathcal{A}}(\\tau)-r\\right){\\bf V}(\\tau) + {\\bf T}(\\tau){\\bf V}(\\tau) + e^{r\\tau}\\beta(\tau), \\quad {\\bf V}(0)=0\n \nThe mild solution is given by the Duhamel formula:\n  \n\\mathbf{V}(\\tau)=\\int_{0}^{\\tau}\\mathbf{G}(\\tau,s)\\left[\\mathbf{T}(s)\\mathbf{V}(s)+e^{-r(\\tau-s)}\\beta(s)\\right]d s \\quad \\text{(Eq. 1)}\n \nHere, `\\mathbf{G}(\\tau, s)` is the evolution system (propagator) generated by the operator `\\mathcal{A}(\\tau) - rI`.\n\n---\n\nWhich of the following statements accurately describe the properties and interpretation of the Duhamel formula (Eq. 1) and its components?\n", "Options": {"A": "The term `\\mathbf{T}(s)\\mathbf{V}(s)` represents the instantaneous liability generated at time `s` due to insurance risk (state transitions).", "B": "The evolution operator `\\mathbf{G}(\\tau,s)` propagates the liability generated at time `s` forward to time `\\tau`, accounting for financial risk and the time value of money.", "C": "The solution is termed a 'mild solution' because it satisfies the integral equation (Eq. 1) but does not require the reserve `\\mathbf{V}` to be differentiable, unlike a 'classical solution'.", "D": "The operator `\\mathbf{G}(\\tau,s)` is generated by the full operator `\\mathcal{A}(\\tau) + \\mathbf{T}(\\tau) - rI`, combining both financial and insurance risks into a single propagator."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the student's ability to interpret the economic meaning of each component of the Duhamel formula, which is the paper's primary analytical tool. It also tests the distinction between 'mild' and 'classical' solutions. Strategy: Reverse-Reasoning. Given the solution formula, the student must identify the correct roles of its constituent parts. Distractor Logic: Option D is a 'Conceptual Distractor (Almost Right)'. It incorrectly combines `\\mathcal{A}` and `\\mathbf{T}` into the generator for `\\mathbf{G}`, whereas the paper's formulation treats `\\mathbf{T}` as part of the inhomogeneity term inside the integral, which is a crucial structural feature of the solution method.", "qid": "396", "question": "### Background\n\nThe solution to the transformed Thiele PDE system can be expressed as a 'mild solution' using the Duhamel formula. This integral representation is crucial for the paper's analysis as it does not require the reserve function to be classically differentiable.\n\n### Data / Model Specification\n\nThe transformed Thiele PDE system is:\n  \n\\partial_{\\tau}{\\bf V}(\\tau) = \\left({\\mathcal{A}}(\\tau)-r\\right){\\bf V}(\\tau) + {\\bf T}(\\tau){\\bf V}(\\tau) + e^{r\\tau}\\beta(\tau), \\quad {\\bf V}(0)=0\n \nThe mild solution is given by the Duhamel formula:\n  \n\\mathbf{V}(\\tau)=\\int_{0}^{\\tau}\\mathbf{G}(\\tau,s)\\left[\\mathbf{T}(s)\\mathbf{V}(s)+e^{-r(\\tau-s)}\\beta(s)\\right]d s \\quad \\text{(Eq. 1)}\n \nHere, `\\mathbf{G}(\\tau, s)` is the evolution system (propagator) generated by the operator `\\mathcal{A}(\\tau) - rI`.\n\n---\n\nWhich of the following statements accurately describe the properties and interpretation of the Duhamel formula (Eq. 1) and its components?\n", "model_evaluation_result": {"model_raw_response": "Looking at the Duhamel formula and the transformed Thiele PDE system, I need to analyze each statement:\n\n**Statement A**: The term `\\mathbf{T}(s)\\mathbf{V}(s)` represents the instantaneous liability generated at time `s` due to insurance risk (state transitions).\n- This is correct. In the PDE, `\\mathbf{T}(\\tau)` represents the transition/insurance risk operator. The term `\\mathbf{T}(s)\\mathbf{V}(s)` in the integral represents how state transitions at time `s` contribute to the liability.\n\n**Statement B**: The evolution operator `\\mathbf{G}(\\tau,s)` propagates the liability generated at time `s` forward to time `\\tau`, accounting for financial risk and the time value of money.\n- This is correct. The text explicitly states that `\\mathbf{G}(\\tau, s)` is the evolution system generated by `\\mathcal{A}(\\tau) - rI`. This operator propagates values from time `s` to time `\\tau`, with the `-r` term accounting for discounting/time value of money.\n\n**Statement C**: The solution is termed a 'mild solution' because it satisfies the integral equation (Eq. 1) but does not require the reserve `\\mathbf{V}` to be differentiable, unlike a 'classical solution'.\n- This is correct. The background explicitly states that \"This integral representation is crucial for the paper's analysis as it does not require the reserve function to be classically differentiable.\" This is the defining characteristic of a mild solution.\n\n**Statement D**: The operator `\\mathbf{G}(\\tau,s)` is generated by the full operator `\\mathcal{A}(\\tau) + \\mathbf{T}(\\tau) - rI`, combining both financial and insurance risks into a single propagator.\n- This is incorrect. The text clearly states that `\\mathbf{G}(\\tau, s)` is generated by `\\mathcal{A}(\\tau) - rI`, not by `\\mathcal{A}(\\tau) + \\mathbf{T}(\\tau) - rI`.", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 438, "Question": "### Background\n\n**Research Question.** After establishing a robust method for measuring currency exposure, the paper investigates how this exposure varies across different types of firms, including by industry and country of origin.\n\n**Setting / Data-Generating Environment.** The analysis uses firm-level exchange rate exposure coefficients (`\\beta_{is}`) estimated from the third stage of the orthogonalized model for firms in the Euro Stoxx TMI index (1999-2011). The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Table 1. Exchange Rate Exposure (`\\beta_{is}`) by Industry (for CHF Shocks)**\n\n| | Avg. `\\beta_{is}` (significant firms) | Total Significant (%) |\n|:---|:---:|:---:|\n| Financials | 1.90 | 72% |\n| Non-Financials | 1.88 | 49% |\n\n*Notes: `\\beta_{is}` is the average of significant coefficients.* \n\n### Question\n\nA portfolio manager notes that the Spanish stock market is composed of approximately 28% financial firms and 72% non-financial firms. Using the average significant `\\beta_{is}` coefficients for the CHF/Euro rate from Table 1, which of the following conclusions are arithmetically and conceptually correct?", "Options": {"A": "The expected value-weighted average exposure for the Spanish market is simply the average of the two coefficients, (1.90 + 1.88) / 2 = 1.89, because the weights are nearly equal.", "B": "This bottom-up calculation, showing a high aggregate exposure, supports the paper's finding that country-level risk is heavily influenced by industrial composition, particularly the concentration of high-exposure financial firms.", "C": "The calculation shows that the high exposure of financial firms is largely offset by the lower exposure of non-financials, leading to a moderate overall market exposure.", "D": "The expected value-weighted average exposure (`\\bar{\\beta}_{is}`) for the Spanish market is approximately 1.89."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the Computational Judgment strategy. It requires the user to perform a multi-step calculation (weighted average) based on data from the table and then draw a conclusion that synthesizes this calculation with the paper's broader argument about country-level heterogeneity. Distractors are based on a Formula Misuse error (C) and a Conceptual Opposite interpretation of the result (D).", "qid": "438", "question": "### Background\n\n**Research Question.** After establishing a robust method for measuring currency exposure, the paper investigates how this exposure varies across different types of firms, including by industry and country of origin.\n\n**Setting / Data-Generating Environment.** The analysis uses firm-level exchange rate exposure coefficients (`\\beta_{is}`) estimated from the third stage of the orthogonalized model for firms in the Euro Stoxx TMI index (1999-2011). The exchange rate is quoted as foreign currency per Euro, so a positive return implies Euro appreciation.\n\n### Data / Model Specification\n\n**Table 1. Exchange Rate Exposure (`\\beta_{is}`) by Industry (for CHF Shocks)**\n\n| | Avg. `\\beta_{is}` (significant firms) | Total Significant (%) |\n|:---|:---:|:---:|\n| Financials | 1.90 | 72% |\n| Non-Financials | 1.88 | 49% |\n\n*Notes: `\\beta_{is}` is the average of significant coefficients.* \n\n### Question\n\nA portfolio manager notes that the Spanish stock market is composed of approximately 28% financial firms and 72% non-financial firms. Using the average significant `\\beta_{is}` coefficients for the CHF/Euro rate from Table 1, which of the following conclusions are arithmetically and conceptually correct?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to calculate the value-weighted average exposure for the Spanish stock market and evaluate which conclusions are correct.\n\nGiven information:\n- Spanish market composition: 28% financial firms, 72% non-financial firms\n- Average β for financial firms: 1.90\n- Average β for non-financial firms: 1.88\n\nLet me calculate the value-weighted average:\nValue-weighted average = (0.28 × 1.90) + (0.72 × 1.88)\n                      = 0.532 + 1.3536\n                      = 1.8856\n                      ≈ 1.89\n\nNow evaluating each option:\n\n**Option A**: Claims the average is (1.90 + 1.88)/2 = 1.89 because weights are \"nearly equal\"\n- This is incorrect. The weights are 28% and 72%, which are not nearly equal\n- The arithmetic is wrong - it should be a weighted average, not a simple average\n- Even though the final number (1.89) happens to be close to the correct answer, the reasoning is fundamentally flawed\n\n**Option B**: States this calculation shows high aggregate exposure influenced by industrial composition\n- This is correct. The calculation does show high exposure (1.89 is substantial)\n- It correctly identifies that country-level risk depends on industrial composition\n- The presence of high-exposure financial firms (even at 28%) contributes significantly\n\n**Option C**: Claims financial firms' high exposure is \"largely offset\" by non-financials\n- This is incorrect. Both sectors have very similar high exposures (1.90 vs 1.88)\n- There's minimal offsetting effect when both coefficients are nearly identical\n- The overall exposure remains high, not moderate\n\n**Option D**: States the value-weighted average is approximately 1.89\n- This is correct based on my calculation above\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 393, "Question": "### Background\n\n**Research Question.** How are Seasoned Equity Offerings (SEOs) priced relative to prevailing market prices, and which economic theories best explain the observed patterns?\n\n**Setting.** Two main theories compete to explain SEO pricing. The model of Parsons and Raviv (P&R) suggests that due to rationing risk, the pre-offer market price will be higher than the offer price (`P_{-1} > P_0`), and the post-offer price will also be higher than the offer price (`P_1 > P_0`). The model of Loderer, Sheehan, and Kadlec (LSK) argues that to avoid commissions, the offer price must be higher than the pre-offer price (`P_0 > P_{-1}`). Both models predict `P_1 > P_0`.\n\n**Variables & Parameters.**\n- `R_0`: The close-to-offer return, `(P_0 / P_{-1} - 1) * 100`.\n- `R_1`: The offer-to-close return, `(P_1 / P_0 - 1) * 100`.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Average SEO Returns for REITs (1991-1996)**\n\n| Return Type | Sample Size | Raw Return (%) | t-Statistic |\n| :--- | :--- | :--- | :--- |\n| Close-to-Offer (`R_0`) | 178 | -1.14 | -7.45 |\n| Offer-to-Close (`R_1`) | 178 | 0.74 | 4.57 |\n\n*Source: Synthesized from Table 2 in the source document.*\n\n---\n\n### Question\n\nThe study tests two competing theories of SEO pricing using the data in **Table 1**. Based on these results, select all correct conclusions.", "Options": {"A": "The finding of a significantly negative close-to-offer return (`R_0` = -1.14%) contradicts the Loderer, Sheehan, and Kadlec (LSK) model.", "B": "The finding of a significantly negative close-to-offer return (`R_0` = -1.14%) is consistent with the Parsons and Raviv (P&R) model.", "C": "The significantly positive offer-to-close return (`R_1` = 0.74%) is a unique prediction of the LSK model and contradicts the P&R model.", "D": "The empirical results are inconclusive, as they fail to provide clear support for either the P&R or LSK model."}, "Answer": ["A", "B"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the ability to adjudicate between competing theories using empirical evidence. It uses an **Atomic Decomposition** strategy, where each option is a discrete statement about the consistency between theory and data. Correct options (A, B) accurately reflect that the negative R_0 supports P&R and refutes LSK. Distractor C is a **Factual Error**, as both models predict a positive R_1. Distractor D is a **Factual Error** because the results clearly favor one model over the other.", "qid": "393", "question": "### Background\n\n**Research Question.** How are Seasoned Equity Offerings (SEOs) priced relative to prevailing market prices, and which economic theories best explain the observed patterns?\n\n**Setting.** Two main theories compete to explain SEO pricing. The model of Parsons and Raviv (P&R) suggests that due to rationing risk, the pre-offer market price will be higher than the offer price (`P_{-1} > P_0`), and the post-offer price will also be higher than the offer price (`P_1 > P_0`). The model of Loderer, Sheehan, and Kadlec (LSK) argues that to avoid commissions, the offer price must be higher than the pre-offer price (`P_0 > P_{-1}`). Both models predict `P_1 > P_0`.\n\n**Variables & Parameters.**\n- `R_0`: The close-to-offer return, `(P_0 / P_{-1} - 1) * 100`.\n- `R_1`: The offer-to-close return, `(P_1 / P_0 - 1) * 100`.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Average SEO Returns for REITs (1991-1996)**\n\n| Return Type | Sample Size | Raw Return (%) | t-Statistic |\n| :--- | :--- | :--- | :--- |\n| Close-to-Offer (`R_0`) | 178 | -1.14 | -7.45 |\n| Offer-to-Close (`R_1`) | 178 | 0.74 | 4.57 |\n\n*Source: Synthesized from Table 2 in the source document.*\n\n---\n\n### Question\n\nThe study tests two competing theories of SEO pricing using the data in **Table 1**. Based on these results, select all correct conclusions.", "model_evaluation_result": {"model_raw_response": "Looking at the data and theoretical predictions:\n\n**Data shows:**\n- R₀ (close-to-offer return) = -1.14% (highly significant, t = -7.45)\n- R₁ (offer-to-close return) = 0.74% (highly significant, t = 4.57)\n\n**This means:**\n- P₀ < P₋₁ (since R₀ is negative)\n- P₁ > P₀ (since R₁ is positive)\n\n**Theoretical predictions:**\n- P&R model: P₋₁ > P₀ and P₁ > P₀\n- LSK model: P₀ > P₋₁ and P₁ > P₀\n\n**Analysis of each option:**\n\nA) **Correct** - LSK predicts P₀ > P₋₁, but the data shows P₀ < P₋₁ (negative R₀). This directly contradicts LSK.\n\nB) **Correct** - P&R predicts P₋₁ > P₀, and the data confirms this with the negative R₀.\n\nC) **Incorrect** - Both models predict P₁ > P₀, so the positive R₁ is not unique to LSK and does not contradict P&R.\n\nD) **Incorrect** - The results clearly support P&R and contradict LSK, so they are not inconclusive.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 310, "Question": "### Background\nAn insurer is analyzing the risk profile of a specific fleet of vehicles. The fleet constitutes 50% of the traffic ($\\rho^{\\Phi}=0.5$) and operates under a moderately aggressive driving configuration, denoted $\\xi^{2a}$. The insurer uses a sophisticated microscopic traffic simulation model to project the aggregate annual loss, $L$, for this fleet.\n\n### Data / Model Specification\nAn insurer is deciding between two premium principles to price this fleet's policy:\n\n(a) **Variance Principle:** Premium = E(L) + $\\alpha \\cdot \\sqrt{\\text{Var(L)}}$, with a risk loading factor $\\alpha = 0.25$.\n(b) **Capital Cost Principle:** Premium = E(L) + $\\beta \\cdot (\\text{ES}_{0.95}(\\text{L}) - \\text{E(L)})$, with a cost-of-capital factor $\\beta = 0.10$.\n\nTable 1 presents the simulation results for the aggregate annual loss $L$ under various model specifications. All values are based on 10,000 independent samples of $L$.\n\n**Table 1: Statistical Functionals of L for $\\rho^{\\Phi}=0.5$ and $\\xi^{2a}$**\n| | **Gamma** | | | **Log-normal** | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** |\n| **E(L)** | 1577.8 | 1571.5 | 1578.4 | 1581.7 | 1576.2 | 1582.7 |\n| **Var(L)** | 160,179.5 | 247,943.9 | 626,190.6 | 162,067.5 | 247,069.8 | 614,713.4 |\n| **ES$_{0.95}$(L)** | 2468.9 | 2748.0 | 3628.9 | 2491.4 | 2772.5 | 3838.4 |\n\n### Question\nAn insurer uses the data for the Gamma distribution with $c_v=1.0$ from Table 1 to calculate the annual premium. Based on the two pricing principles provided, which of the following conclusions are valid?", "Options": {"A": "Using the Capital Cost Principle with a cost-of-capital factor $\\beta = 0.10$, the risk loading component of the premium (the amount added to E(L)) is greater than 115.", "B": "The premium calculated using the Capital Cost Principle ($\\beta=0.10$) is higher than the premium calculated using the Variance Principle ($\\alpha=0.25$).", "C": "Using the Variance Principle with a risk loading factor $\\alpha = 0.25$, the calculated premium is less than 1700.", "D": "If the insurer switched to the Log-normal model ($c_v=1.0$), the premium from the Variance Principle would decrease because Var(L) is slightly lower."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses multi-step computational judgment and comparison of actuarial pricing models. It uses a 'Computational Judgment' strategy, requiring the user to calculate premiums under two different frameworks and compare the results. Distractors are based on common errors: a direct comparison error (C) and a conceptual error where a correct premise leads to a false conclusion (D).", "qid": "310", "question": "### Background\nAn insurer is analyzing the risk profile of a specific fleet of vehicles. The fleet constitutes 50% of the traffic ($\\rho^{\\Phi}=0.5$) and operates under a moderately aggressive driving configuration, denoted $\\xi^{2a}$. The insurer uses a sophisticated microscopic traffic simulation model to project the aggregate annual loss, $L$, for this fleet.\n\n### Data / Model Specification\nAn insurer is deciding between two premium principles to price this fleet's policy:\n\n(a) **Variance Principle:** Premium = E(L) + $\\alpha \\cdot \\sqrt{\\text{Var(L)}}$, with a risk loading factor $\\alpha = 0.25$.\n(b) **Capital Cost Principle:** Premium = E(L) + $\\beta \\cdot (\\text{ES}_{0.95}(\\text{L}) - \\text{E(L)})$, with a cost-of-capital factor $\\beta = 0.10$.\n\nTable 1 presents the simulation results for the aggregate annual loss $L$ under various model specifications. All values are based on 10,000 independent samples of $L$.\n\n**Table 1: Statistical Functionals of L for $\\rho^{\\Phi}=0.5$ and $\\xi^{2a}$**\n| | **Gamma** | | | **Log-normal** | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** | **$c_v=0.5$** | **$c_v=1.0$** | **$c_v=2.0$** |\n| **E(L)** | 1577.8 | 1571.5 | 1578.4 | 1581.7 | 1576.2 | 1582.7 |\n| **Var(L)** | 160,179.5 | 247,943.9 | 626,190.6 | 162,067.5 | 247,069.8 | 614,713.4 |\n| **ES$_{0.95}$(L)** | 2468.9 | 2748.0 | 3628.9 | 2491.4 | 2772.5 | 3838.4 |\n\n### Question\nAn insurer uses the data for the Gamma distribution with $c_v=1.0$ from Table 1 to calculate the annual premium. Based on the two pricing principles provided, which of the following conclusions are valid?", "model_evaluation_result": {"model_raw_response": "I need to calculate the premiums using both principles for the Gamma distribution with cv=1.0, then evaluate each statement.\n\nFrom Table 1, for Gamma distribution with cv=1.0:\n- E(L) = 1571.5\n- Var(L) = 247,943.9\n- ES₀.₉₅(L) = 2748.0\n\n**Variance Principle calculation:**\nPremium = E(L) + α × √Var(L)\nPremium = 1571.5 + 0.25 × √247,943.9\nPremium = 1571.5 + 0.25 × 497.94\nPremium = 1571.5 + 124.48\nPremium = 1695.98\n\n**Capital Cost Principle calculation:**\nPremium = E(L) + β × (ES₀.₉₅(L) - E(L))\nPremium = 1571.5 + 0.10 × (2748.0 - 1571.5)\nPremium = 1571.5 + 0.10 × 1176.5\nPremium = 1571.5 + 117.65\nPremium = 1689.15\n\nNow evaluating each option:\n\n**Option A:** Risk loading component using Capital Cost Principle = 117.65\nThis is greater than 115. ✓\n\n**Option B:** Capital Cost premium (1689.15) vs Variance Principle premium (1695.98)\nCapital Cost premium is lower, not higher. ✗\n\n**Option C:** Variance Principle premium = 1695.98\nThis is less than 1700. ✓\n\n**Option D:** For Log-normal with cv=1.0:\n- Var(L) = 247,069.8 (slightly lower than Gamma's 247,943.9)\n- Premium would be 1576.2 + 0.25 × √247,069.8 = 1576.2 + 0.25 × 497.06 = 1700.47\nThe premium would actually increase (from 1695.98 to 1700.47), not decrease. ✗\n\nA,", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 464, "Question": "### Background\n\n**Research Question.** In designing early warning systems for financial distress, why might simple, transparent metrics be more effective than complex, statistically sophisticated models?\n\n**Setting.** A financial stability authority compares a simple indicator (`I_S`) with a complex model (`I_C`) based on their historical predictive accuracy for financial crises, measured by the Area Under the Curve (AUC).\n\n**Table 1: Hypothetical Performance of Early Warning Indicators**\n\n| Indicator Type | Number of Parameters | In-Sample AUC (1980-2007) | Out-of-Sample AUC (2008-2020) |\n| :--- | :--- | :--- | :--- |\n| Simple (`I_S`) | 1 | 0.75 | 0.72 |\n| Complex (`I_C`) | 50+ | 0.92 | 0.65 |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the principles of robust policymaking, select ALL of the following statements that are correct.", "Options": {"A": "The complex model's (`I_C`) superior in-sample performance and poor out-of-sample performance is a classic sign of overfitting, where the model has learned sample-specific noise rather than the true underlying signal.", "B": "Beyond statistical performance, a simple indicator is often preferred by regulators because it is easier to communicate to the public and financial industry, which enhances policy legitimacy and predictability.", "C": "The simple model's (`I_S`) better out-of-sample performance suggests it is more robust and has captured a more generalizable relationship for predicting crises.", "D": "In the event of a structural break not seen in the historical data, the complex model (`I_C`) would likely be more reliable because its multiple parameters can adapt more quickly to the new environment."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests core concepts in statistical modeling and the political economy of regulation. It uses an Atomic Decomposition strategy to convert the original QA into verifiable statements. The distractor (D) is a high-fidelity 'Conceptual Opposite' that targets a common misunderstanding about model complexity and robustness to structural change.", "qid": "464", "question": "### Background\n\n**Research Question.** In designing early warning systems for financial distress, why might simple, transparent metrics be more effective than complex, statistically sophisticated models?\n\n**Setting.** A financial stability authority compares a simple indicator (`I_S`) with a complex model (`I_C`) based on their historical predictive accuracy for financial crises, measured by the Area Under the Curve (AUC).\n\n**Table 1: Hypothetical Performance of Early Warning Indicators**\n\n| Indicator Type | Number of Parameters | In-Sample AUC (1980-2007) | Out-of-Sample AUC (2008-2020) |\n| :--- | :--- | :--- | :--- |\n| Simple (`I_S`) | 1 | 0.75 | 0.72 |\n| Complex (`I_C`) | 50+ | 0.92 | 0.65 |\n\n---\n\n### Question\n\nBased on the data in **Table 1** and the principles of robust policymaking, select ALL of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1 and applying principles of robust policymaking:\n\n**Option A**: The complex model shows AUC of 0.92 in-sample but drops to 0.65 out-of-sample, while the simple model maintains similar performance (0.75 to 0.72). This dramatic deterioration in the complex model's out-of-sample performance is indeed a classic sign of overfitting - the model with 50+ parameters has fitted to noise in the training data rather than capturing the true underlying relationships. ✓\n\n**Option B**: Simple indicators are indeed preferred by regulators for communication reasons. A single-parameter model is much easier to explain to the public, financial institutions, and other stakeholders. This transparency enhances policy legitimacy (people understand and trust what they can comprehend) and predictability (market participants can anticipate regulatory responses). This is a well-established principle in financial regulation. ✓\n\n**Option C**: The simple model shows better out-of-sample performance (0.72 vs 0.65) and much more stable performance across samples (dropping only 0.03 vs 0.27). This demonstrates that the simple model has captured a more robust, generalizable relationship that continues to work in new data. ✓\n\n**Option D**: This is incorrect. Complex models with many parameters are typically less robust to structural breaks, not more. With 50+ parameters fitted to historical data, the complex model is likely to fail when faced with fundamentally new patterns. Simple models, by capturing only the most essential relationships, tend to be more robust to structural changes. The table already shows the complex model's poor generalization to new data. ✗\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 228, "Question": "### Background\n\n**Research Question.** How can the theoretical framework for solving partial integro-differential equations (PIDEs) be applied to the practical problem of pricing and hedging a catastrophe (CAT) bond, an instrument exposed to both non-hedgeable insurance risk and hedgeable market risk?\n\n**Setting / Data-Generating Environment.** The price of a CAT bond depends on a three-dimensional state process `X = (Z^1, Z^2, L)`, where `Z^1` is a factor driving the loss intensity, `Z^2` is a factor driving the interest rate, and `L` is the cumulative loss process. The goal is to price the bond and construct a mean-variance optimal hedge for its interest rate exposure using a traded zero-coupon bond.\n\n### Data / Model Specification\n\nThe state process `X_t = (Z^1_t, Z^2_t, L_t)` follows the SDE system under a risk-neutral measure `Q`:\n  \n\\mathrm{d}Z_{t}^{1} = a_{1}(b_{1}-Z_{t}^{1})\\mathrm{d}t+\\sqrt{1-\\rho^{2}}\\sigma_{1}\\mathrm{d}W_{t}^{1}+\\rho\\sigma_{1}\\mathrm{d}W_{t}^{2}+\\gamma^{Z}\\mathrm{d}L_{t} \\quad \\text{(Eq. (1))}\n \n  \n\\mathrm{d}Z_{t}^{2} = a_{2}(b_{2}-Z_{t}^{2})\\mathrm{d}t+\\sigma_{2}\\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (2))}\n \nwhere `W^1, W^2` are independent Brownian motions. The cumulative loss `L_t` is a compound Poisson process with intensity `λ(t,Z_t^1)` and jump size distribution `μ(du)`. The short rate is `r_t = r(Z^2_t)`. The CAT bond has a terminal payoff at time `T` given by:\n  \ng(l) = F - \\left( (l-K_1)^+ - (l-K_2)^+ \\right) \\quad \\text{(Eq. (3))}\n \nwhere `F` is the face value and `K_1`, `K_2` are the attachment and exhaustion points.\n\n### Question\n\nBased on the model specification, select ALL of the following statements that are mathematically correct descriptions of the system's dynamics or risk factors.", "Options": {"A": "A value of `ρ=0` would imply that the CAT bond has no exposure to financial market risk, as the loss intensity factor `Z^1` would be independent of the interest rate factor `Z^2`.", "B": "The parameter `γ^Z` models a self-excitation effect, where a loss event directly increases the loss intensity factor `Z^1`, making subsequent losses more probable.", "C": "The cross-diffusion term in the generator `L_t` for the state process `X`, which captures the instantaneous covariance between the diffusion parts of `Z^1` and `Z^2`, is `ρσ₁σ₂φ_{z₁z₂}`.", "D": "The diffusion term (second-order derivative term) for the `Z^1` component in the generator `L_t` is `(1/2)(1-ρ²)σ₁²φ_{z₁z₁}`."}, "Answer": ["B", "C"], "pi_justification": "Assessment Target: This item tests the user's ability to correctly interpret the model's SDEs to derive components of the associated PIDE generator and to understand the financial meaning of key risk parameters. Strategy: Atomic Decomposition. The original QA question's parts on generator derivation and parameter interpretation are broken down into independent, verifiable statements. Distractor Logic: (C) is a Step-Omission Error, as it incorrectly calculates the variance of `dZ^1` by ignoring the `dW^2` component. (D) is a Conceptual: Almost Right error; `ρ=0` removes correlation between the *shocks* but does not remove the bond's fundamental exposure to interest rates via discounting.", "qid": "228", "question": "### Background\n\n**Research Question.** How can the theoretical framework for solving partial integro-differential equations (PIDEs) be applied to the practical problem of pricing and hedging a catastrophe (CAT) bond, an instrument exposed to both non-hedgeable insurance risk and hedgeable market risk?\n\n**Setting / Data-Generating Environment.** The price of a CAT bond depends on a three-dimensional state process `X = (Z^1, Z^2, L)`, where `Z^1` is a factor driving the loss intensity, `Z^2` is a factor driving the interest rate, and `L` is the cumulative loss process. The goal is to price the bond and construct a mean-variance optimal hedge for its interest rate exposure using a traded zero-coupon bond.\n\n### Data / Model Specification\n\nThe state process `X_t = (Z^1_t, Z^2_t, L_t)` follows the SDE system under a risk-neutral measure `Q`:\n  \n\\mathrm{d}Z_{t}^{1} = a_{1}(b_{1}-Z_{t}^{1})\\mathrm{d}t+\\sqrt{1-\\rho^{2}}\\sigma_{1}\\mathrm{d}W_{t}^{1}+\\rho\\sigma_{1}\\mathrm{d}W_{t}^{2}+\\gamma^{Z}\\mathrm{d}L_{t} \\quad \\text{(Eq. (1))}\n \n  \n\\mathrm{d}Z_{t}^{2} = a_{2}(b_{2}-Z_{t}^{2})\\mathrm{d}t+\\sigma_{2}\\mathrm{d}W_{t}^{2} \\quad \\text{(Eq. (2))}\n \nwhere `W^1, W^2` are independent Brownian motions. The cumulative loss `L_t` is a compound Poisson process with intensity `λ(t,Z_t^1)` and jump size distribution `μ(du)`. The short rate is `r_t = r(Z^2_t)`. The CAT bond has a terminal payoff at time `T` given by:\n  \ng(l) = F - \\left( (l-K_1)^+ - (l-K_2)^+ \\right) \\quad \\text{(Eq. (3))}\n \nwhere `F` is the face value and `K_1`, `K_2` are the attachment and exhaustion points.\n\n### Question\n\nBased on the model specification, select ALL of the following statements that are mathematically correct descriptions of the system's dynamics or risk factors.", "model_evaluation_result": {"model_raw_response": "Looking at the model specification, I need to analyze each statement about the dynamics and risk factors.\n\n**Statement A**: Claims that ρ=0 implies no exposure to financial market risk.\n- When ρ=0, the diffusion term for Z^1 becomes: dZ^1_t = a_1(b_1-Z^1_t)dt + σ_1dW^1_t + γ^Z dL_t\n- The diffusion term for Z^2 is: dZ^2_t = a_2(b_2-Z^2_t)dt + σ_2dW^2_t\n- With ρ=0, there's no common Brownian motion between Z^1 and Z^2, making their diffusion parts independent\n- However, Z^1 still affects the loss intensity λ(t,Z^1_t), and the CAT bond price depends on both the discount factor (through r(Z^2)) and expected losses (through Z^1)\n- The bond still has exposure to interest rate risk through Z^2, so this statement is **incorrect**\n\n**Statement B**: About the self-excitation effect of γ^Z.\n- From Eq. (1): dZ^1_t includes the term γ^Z dL_t\n- When a jump occurs in L_t, it causes an immediate jump in Z^1_t by amount γ^Z times the jump size\n- Since λ(t,Z^1_t) depends on Z^1_t, an increase in Z^1_t increases the loss intensity\n- This creates a self-excitation mechanism where losses make future losses more likely\n- This statement is **correct**\n\n**Statement C**: About the cross-diffusion term in the generator.\n- The diffusion parts of Z^1 and Z^2 are:\n  - Z^1: √(1-ρ²)σ_1dW^1 + ρσ_1dW^2\n  - Z^2: σ_2dW^2\n- The instantaneous covariance is:\n  - Cov(√(1-ρ²)σ_1dW^1 ", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 441, "Question": "### Background\n\n**Research Question.** This case evaluates the empirical and economic value of the Hadamard Exponential (HE) extension to Conditional Autoregressive Wishart (CAW) models. It investigates whether the statistical significance of the HE parameter translates into superior out-of-sample performance for a practical risk management application.\n\n**Setting.** Forecasts from various covariance models for 29 DJIA stocks are evaluated. The analysis compares in-sample statistical fit with out-of-sample economic performance, measured by the risk of a Global Minimum Variance Portfolio (GMVP). The COV family of models specifies the conditional covariance matrix `S_t` directly.\n\n**Variables and Parameters.**\n*   `S_t`: The `n x n` conditional covariance matrix.\n*   `S_hat_{t,b}`: The `b`-step ahead forecast of `S_t`.\n*   `w_{t,b}`: The `n x 1` vector of GMVP weights.\n*   `j_n`: An `n x 1` vector of ones.\n*   `h`: The forecast horizon in days (1, 5, or 22).\n*   `φ_A`: The Hadamard Exponential parameter, which makes the impact of covariance shocks time-varying and dependent on past correlations.\n*   `a_i`, `b_i`: Asset-specific parameters for the rank-one (R1) models, grouped into 3 clusters.\n\n---\n\n### Data / Model Specification\n\nThe weight vector for the GMVP is computed using the forecast covariance matrix `S_hat_{t,b}`:\n  \n\\widehat{w}_{t,b} = \\frac{\\widehat{S}_{t,b}^{-1} j_n}{j_n' \\widehat{S}_{t,b}^{-1} j_n}\n\n (Eq. (1))\n \nModel performance is evaluated based on in-sample fit (Table 1) and the annualized standard deviation of the out-of-sample GMVP returns constructed using these weights (Table 2). Lower standard deviation is better.\n\n**Table 1: In-Sample Estimation Results for COV Models (29 stocks)**\n| Parameters | S       | S-Pt    | S-Rt    | R1      | R1-Pt   | R1-Rt   |\n|:-----------|:--------|:--------|:--------|:--------|:--------|:--------|\n| **ΦA**     |         | **0.061**   | **0.072**   |         | **0.055**   | **0.070**   |\n|            |         | **(0.015)** | **(0.012)** |         | **(0.014)** | **(0.013)** |\n| Log-lik    | -93522.1| -93464.4| -93428.6| -93485.7| -93442.2| -93409.4|\n\n*Note: Robust standard errors in parentheses. S-Pt/Rt are scalar HE models; R1-Pt/Rt are rank-one HE models.* \n\n**Table 2: Annualized Standard Deviations of GMVP Returns (%)**\n| Model        | h=1  | h=5  | h=22 |\n|--------------|:----:|:----:|:----:|\n| **COV-S**    | **8.10** | **7.53** | **7.63** |\n| **COV-S-Pt** | **8.08** | **7.51** | **7.60** |\n| **COV-S-Rt** | **8.07** | **7.49** | **7.60** |\n| **EWMA**     | **8.03** | 8.05 | **7.87** |\n\n*Note: Bold values identify models in the 95% Model Confidence Set. EWMA is the Exponentially Weighted Moving Average benchmark.* \n\n---\n\n### Question\n\nBased on the in-sample estimation results in **Table 1** and the out-of-sample economic performance in **Table 2**, select ALL statements that are supported by the data.", "Options": {"A": "The COV-S-Rt model delivers the best economic performance at the 1-day forecast horizon, achieving the lowest GMVP risk of all models shown in Table 2.", "B": "Despite its statistical significance, the HE extension (e.g., COV-S-Rt) fails to improve upon the simple COV-S model in terms of GMVP risk at any of the reported forecast horizons.", "C": "The HE extension in the COV-S-Rt model provides a tangible, though modest, economic benefit over the standard COV-S model at the 5-day forecast horizon, reducing the GMVP standard deviation from 7.53% to 7.49%.", "D": "The Hadamard Exponential parameter (`ΦA`) is highly statistically significant in the COV-S-Rt model, with a t-statistic of 6.0, indicating strong in-sample explanatory power."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to connect in-sample statistical significance with out-of-sample economic performance. It uses an Atomic Decomposition strategy, breaking down the original QA problem's conclusions into independently verifiable statements. Correct options A and B test direct calculation (t-stat) and data comparison (GMVP risk reduction). Distractor C is a Conceptual Opposite, directly contradicting the data for h=1 and h=5. Distractor D is an 'Almost Right' error, as COV-S-Rt performs well at h=1 but is outperformed by the EWMA benchmark, testing for careful reading.", "qid": "441", "question": "### Background\n\n**Research Question.** This case evaluates the empirical and economic value of the Hadamard Exponential (HE) extension to Conditional Autoregressive Wishart (CAW) models. It investigates whether the statistical significance of the HE parameter translates into superior out-of-sample performance for a practical risk management application.\n\n**Setting.** Forecasts from various covariance models for 29 DJIA stocks are evaluated. The analysis compares in-sample statistical fit with out-of-sample economic performance, measured by the risk of a Global Minimum Variance Portfolio (GMVP). The COV family of models specifies the conditional covariance matrix `S_t` directly.\n\n**Variables and Parameters.**\n*   `S_t`: The `n x n` conditional covariance matrix.\n*   `S_hat_{t,b}`: The `b`-step ahead forecast of `S_t`.\n*   `w_{t,b}`: The `n x 1` vector of GMVP weights.\n*   `j_n`: An `n x 1` vector of ones.\n*   `h`: The forecast horizon in days (1, 5, or 22).\n*   `φ_A`: The Hadamard Exponential parameter, which makes the impact of covariance shocks time-varying and dependent on past correlations.\n*   `a_i`, `b_i`: Asset-specific parameters for the rank-one (R1) models, grouped into 3 clusters.\n\n---\n\n### Data / Model Specification\n\nThe weight vector for the GMVP is computed using the forecast covariance matrix `S_hat_{t,b}`:\n  \n\\widehat{w}_{t,b} = \\frac{\\widehat{S}_{t,b}^{-1} j_n}{j_n' \\widehat{S}_{t,b}^{-1} j_n}\n\n (Eq. (1))\n \nModel performance is evaluated based on in-sample fit (Table 1) and the annualized standard deviation of the out-of-sample GMVP returns constructed using these weights (Table 2). Lower standard deviation is better.\n\n**Table 1: In-Sample Estimation Results for COV Models (29 stocks)**\n| Parameters | S       | S-Pt    | S-Rt    | R1      | R1-Pt   | R1-Rt   |\n|:-----------|:--------|:--------|:--------|:--------|:--------|:--------|\n| **ΦA**     |         | **0.061**   | **0.072**   |         | **0.055**   | **0.070**   |\n|            |         | **(0.015)** | **(0.012)** |         | **(0.014)** | **(0.013)** |\n| Log-lik    | -93522.1| -93464.4| -93428.6| -93485.7| -93442.2| -93409.4|\n\n*Note: Robust standard errors in parentheses. S-Pt/Rt are scalar HE models; R1-Pt/Rt are rank-one HE models.* \n\n**Table 2: Annualized Standard Deviations of GMVP Returns (%)**\n| Model        | h=1  | h=5  | h=22 |\n|--------------|:----:|:----:|:----:|\n| **COV-S**    | **8.10** | **7.53** | **7.63** |\n| **COV-S-Pt** | **8.08** | **7.51** | **7.60** |\n| **COV-S-Rt** | **8.07** | **7.49** | **7.60** |\n| **EWMA**     | **8.03** | 8.05 | **7.87** |\n\n*Note: Bold values identify models in the 95% Model Confidence Set. EWMA is the Exponentially Weighted Moving Average benchmark.* \n\n---\n\n### Question\n\nBased on the in-sample estimation results in **Table 1** and the out-of-sample economic performance in **Table 2**, select ALL statements that are supported by the data.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Tables 1 and 2, I need to evaluate each statement:\n\nA) \"The COV-S-Rt model delivers the best economic performance at the 1-day forecast horizon, achieving the lowest GMVP risk of all models shown in Table 2.\"\n- Looking at Table 2, h=1 column: COV-S-Rt has 8.07%, but EWMA has 8.03%\n- Therefore, COV-S-Rt is NOT the lowest, so this is FALSE\n\nB) \"Despite its statistical significance, the HE extension (e.g., COV-S-Rt) fails to improve upon the simple COV-S model in terms of GMVP risk at any of the reported forecast horizons.\"\n- Comparing COV-S vs COV-S-Rt in Table 2:\n  - h=1: 8.10% vs 8.07% (improvement)\n  - h=5: 7.53% vs 7.49% (improvement)\n  - h=22: 7.63% vs 7.60% (improvement)\n- The HE extension DOES improve at all horizons, so this is FALSE\n\nC) \"The HE extension in the COV-S-Rt model provides a tangible, though modest, economic benefit over the standard COV-S model at the 5-day forecast horizon, reducing the GMVP standard deviation from 7.53% to 7.49%.\"\n- Table 2 shows exactly this: COV-S has 7.53% and COV-S-Rt has 7.49% at h=5\n- This is TRUE\n\nD) \"The Hadamard Exponential parameter (`ΦA`) is highly statistically significant in the COV-S-Rt model, with a t-statistic of 6.0, indicating strong in-sample explanatory power.\"\n- Table 1 shows ΦA = 0.072 with standard error (0.012)\n- t-statistic = 0.072/0.012 = 6.0\n- This is TRUE\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 271, "Question": "### Background\n\nA theoretical argument in the paper suggests that a sufficient condition for increased finalization speed to *decrease* Payments Per Claim Finalized (PPCF) is that the underlying PPCF curve is a strictly concave function of time. This theoretical condition would explain the consistently negative `γ_k` coefficients found in the empirical 'See-Saw' model, which indicate that higher finalization speed is associated with lower average payments.\n\n### Data / Model Specification\n\nTo test this, the author examines the fitted PPCF values from the model for a typical runoff pattern and calculates their second differences. A persistently concave function would exhibit consistently negative second differences. The results are shown in Table 1.\n\n**Table 1: PPCF for a Typical Runoff of Claim Numbers**\n\n| Development Year | Fitted PPCF | Second Difference of Fitted PPCF |\n| :--- | :--- | :--- |\n| 0 | 12,568 | |\n| 1 | 8,106 | +2,981 |\n| 2 | 9,587 | -2,593 |\n| 3 | 8,475 | -865 |\n| 4 | 6,498 | +5,921 |\n| 5 | 10,442 | -1,837 |\n| 6 | 12,549 | -1,814 |\n| 7 | 12,842 | +1,725 |\n| 8 | 14,860 | +2,556 |\n| 9 | 19,434 | -2,493 |\n| 10 | 21,515 | |\n\nThe paper investigates the apparent contradiction between the theoretical condition (concavity) and the model's empirical results. Based on the provided data and the paper's analysis, which of the following conclusions are valid? (Select all that apply)", "Options": {"A": "The author suggests the negative `γ_k` coefficients might be explained by other factors, such as a secular trend of increasing claim sizes over accident years that is negatively correlated with finalization speeds.", "B": "The data in Table 1 strongly supports the theory that the PPCF curve is a strictly concave function of development year, as evidenced by the predominantly negative second differences.", "C": "The fluctuating signs in the 'Second Difference' column of Table 1 show that the fitted PPCF curve is not persistently concave, which appears to contradict the sufficient condition that would theoretically guarantee the observed negative `γ_k` coefficients.", "D": "The paper concludes that because the concavity condition is violated, the 'See-Saw' model is fundamentally flawed and its results are unreliable."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize theoretical arguments with empirical evidence, a key part of the original QA's part 3. It uses the 'Atomic Decomposition' strategy to test two distinct parts of the paper's conclusion. Option A tests the correct interpretation of the second-difference data in Table 1. Option B tests the understanding of the author's proposed resolution to the apparent contradiction. Distractor C is a 'Conceptual Opposite' error, misreading the evidence in the table. Distractor D presents an 'Incorrect Conclusion'; the author explicitly argues the model remains useful as a linear approximation even if the underlying theory is incomplete.", "qid": "271", "question": "### Background\n\nA theoretical argument in the paper suggests that a sufficient condition for increased finalization speed to *decrease* Payments Per Claim Finalized (PPCF) is that the underlying PPCF curve is a strictly concave function of time. This theoretical condition would explain the consistently negative `γ_k` coefficients found in the empirical 'See-Saw' model, which indicate that higher finalization speed is associated with lower average payments.\n\n### Data / Model Specification\n\nTo test this, the author examines the fitted PPCF values from the model for a typical runoff pattern and calculates their second differences. A persistently concave function would exhibit consistently negative second differences. The results are shown in Table 1.\n\n**Table 1: PPCF for a Typical Runoff of Claim Numbers**\n\n| Development Year | Fitted PPCF | Second Difference of Fitted PPCF |\n| :--- | :--- | :--- |\n| 0 | 12,568 | |\n| 1 | 8,106 | +2,981 |\n| 2 | 9,587 | -2,593 |\n| 3 | 8,475 | -865 |\n| 4 | 6,498 | +5,921 |\n| 5 | 10,442 | -1,837 |\n| 6 | 12,549 | -1,814 |\n| 7 | 12,842 | +1,725 |\n| 8 | 14,860 | +2,556 |\n| 9 | 19,434 | -2,493 |\n| 10 | 21,515 | |\n\nThe paper investigates the apparent contradiction between the theoretical condition (concavity) and the model's empirical results. Based on the provided data and the paper's analysis, which of the following conclusions are valid? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to analyze the second differences of the fitted PPCF values to determine which conclusions are valid.\n\nThe second differences show the following pattern:\n- Year 1: +2,981\n- Year 2: -2,593\n- Year 3: -865\n- Year 4: +5,921\n- Year 5: -1,837\n- Year 6: -1,814\n- Year 7: +1,725\n- Year 8: +2,556\n- Year 9: -2,493\n\nFor a function to be strictly concave, all second differences would need to be negative. However, the data shows both positive and negative values, with 4 positive and 5 negative second differences.\n\nAnalyzing each option:\n\nA) This is a valid conclusion. Since the theoretical condition (concavity) is not met but the model still shows negative γ_k coefficients, the author would logically suggest alternative explanations for this phenomenon.\n\nB) This is incorrect. The second differences are not predominantly negative - they fluctuate between positive and negative values, clearly contradicting strict concavity.\n\nC) This is a valid conclusion. The fluctuating signs directly demonstrate that the PPCF curve is not persistently concave, which contradicts the theoretical sufficient condition.\n\nD) This is not a valid conclusion. The violation of the sufficient condition doesn't mean the model is flawed - it simply means that concavity is not the explanation for the negative γ_k coefficients. The model can still be valid with other explanations.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 38, "Question": "### Background\n\n**Research Question:** This case investigates the novel information content of the ECB's 2014 Comprehensive Assessment (CA) from a \"production side\" perspective. The goal is to quantify how much of the supervisory-imposed capital curtailments from the Asset Quality Review (AQR) and Stress Test Exercise (STE) can be explained by publicly available information, thereby inferring the supervisor's implicit decision rule.\n\n**Setting and Sample:** The analysis uses a cross-section of 122 European banks that participated in the 2014 CA. Two competing hypotheses for the supervisor's behavior are considered: a **risk-based approach** (harsher stance on riskier banks) and **regulatory capture** (lenient stance on larger, more relevant banks).\n\n### Data / Model Specification\n\nThe following OLS regression models are estimated to explain the capital curtailments (in bps):\n  \nAQR_{i} = \\beta_{0} + \\beta_{1}(Leverage)_{i} + \\beta_{2}(NPE.R)_{i} + \\beta_{3}(Tot.Ass. (ln))_{i} + ... + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n \n  \nSTE_{i} = \\alpha_{0} + \\alpha_{1}(Leverage)_{i} + \\alpha_{2}(NPE.R)_{i} + ... + \\vartheta_{i} \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Regression and Summary Statistics**\n\n| | **Panel A: Regression Results** | | **Panel B: Summary Statistics** |\n| :--- | :--- | :--- | :--- | :--- |\n| **Variable** | **AQR Coeff. (p-val)** | **STE Coeff. (p-val)** | **Variable** | **Std. Dev.** |\n| Leverage | 1.67*** (0.0001) | 12.19** (0.0116) | Non-Performing Exp. (NPE.R) | 9.09% |\n| Total Assets (ln) | -7.02** (0.0195) | -3.86 (0.8503) | | |\n| Non-Performing Exp. (NPE.R) | 463.00*** (0.0021) | 1558.30*** (0.0010) | | |\n| **R-squared** | **0.60** | **0.47** | | |\n\n*Source: Adapted from Tables 2 and 3 in the source paper. `***` and `**` denote significance at 1% and 5% levels.*\n\n### Question\n\nBased on the provided models and results in **Table 1**, which of the following statements are correct?", "Options": {"A": "The R-squared of 0.60 for the AQR model suggests that the upper bound for new, idiosyncratic information revealed by the AQR is 40% of the total cross-sectional variation.", "B": "A one-standard-deviation increase in `NPE.R` (9.09%) is associated with an increase of approximately 42.1 bps in the STE curtailment.", "C": "The evidence regarding the supervisor's motives in the AQR is ambiguous; the significant positive coefficient on `Leverage` supports a risk-based approach, whereas the significant negative coefficient on `Total Assets (ln)` is consistent with regulatory capture.", "D": "The significant positive coefficient on `Leverage` in the AQR model indicates that the supervisor was more lenient towards highly levered banks, consistent with regulatory capture."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests multiple skills required for interpreting regression analysis: conceptual understanding of R-squared (Option A), inference about competing hypotheses from coefficient signs (Option B), and a direct calculation of economic significance. It combines **Conceptual Judgment** and **Computational Judgment**. Distractors are designed to trap common errors: using the wrong coefficient for a calculation (**Formula Misuse**, Option C) and misinterpreting a coefficient's sign in the context of the stated hypotheses (**Conceptual Opposite**, Option D).", "qid": "38", "question": "### Background\n\n**Research Question:** This case investigates the novel information content of the ECB's 2014 Comprehensive Assessment (CA) from a \"production side\" perspective. The goal is to quantify how much of the supervisory-imposed capital curtailments from the Asset Quality Review (AQR) and Stress Test Exercise (STE) can be explained by publicly available information, thereby inferring the supervisor's implicit decision rule.\n\n**Setting and Sample:** The analysis uses a cross-section of 122 European banks that participated in the 2014 CA. Two competing hypotheses for the supervisor's behavior are considered: a **risk-based approach** (harsher stance on riskier banks) and **regulatory capture** (lenient stance on larger, more relevant banks).\n\n### Data / Model Specification\n\nThe following OLS regression models are estimated to explain the capital curtailments (in bps):\n  \nAQR_{i} = \\beta_{0} + \\beta_{1}(Leverage)_{i} + \\beta_{2}(NPE.R)_{i} + \\beta_{3}(Tot.Ass. (ln))_{i} + ... + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n \n  \nSTE_{i} = \\alpha_{0} + \\alpha_{1}(Leverage)_{i} + \\alpha_{2}(NPE.R)_{i} + ... + \\vartheta_{i} \\quad \\text{(Eq. (2))}\n \n\n**Table 1: Regression and Summary Statistics**\n\n| | **Panel A: Regression Results** | | **Panel B: Summary Statistics** |\n| :--- | :--- | :--- | :--- | :--- |\n| **Variable** | **AQR Coeff. (p-val)** | **STE Coeff. (p-val)** | **Variable** | **Std. Dev.** |\n| Leverage | 1.67*** (0.0001) | 12.19** (0.0116) | Non-Performing Exp. (NPE.R) | 9.09% |\n| Total Assets (ln) | -7.02** (0.0195) | -3.86 (0.8503) | | |\n| Non-Performing Exp. (NPE.R) | 463.00*** (0.0021) | 1558.30*** (0.0010) | | |\n| **R-squared** | **0.60** | **0.47** | | |\n\n*Source: Adapted from Tables 2 and 3 in the source paper. `***` and `**` denote significance at 1% and 5% levels.*\n\n### Question\n\nBased on the provided models and results in **Table 1**, which of the following statements are correct?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the regression results and summary statistics provided.\n\n**Statement A**: The R-squared of 0.60 for the AQR model suggests that the upper bound for new, idiosyncratic information revealed by the AQR is 40% of the total cross-sectional variation.\n\nThis is correct. An R-squared of 0.60 means that 60% of the variation in AQR capital curtailments can be explained by publicly available information (the independent variables). Therefore, at most 40% (1 - 0.60) of the variation could be due to new, private information revealed by the supervisor through the AQR.\n\n**Statement B**: A one-standard-deviation increase in NPE.R (9.09%) is associated with an increase of approximately 42.1 bps in the STE curtailment.\n\nLet me calculate: 1558.30 × 0.0909 = 141.65 bps\nThis is not approximately 42.1 bps, so this statement is incorrect.\n\n**Statement C**: The evidence regarding the supervisor's motives in the AQR is ambiguous; the significant positive coefficient on Leverage supports a risk-based approach, whereas the significant negative coefficient on Total Assets (ln) is consistent with regulatory capture.\n\nThis is correct. The positive coefficient on Leverage (1.67***) means higher leverage leads to higher capital curtailments, which is consistent with a risk-based approach (harsher on riskier banks). The negative coefficient on Total Assets (-7.02**) means larger banks receive lower capital curtailments, which is consistent with regulatory capture (more lenient on larger, systemically important banks). These conflicting signals make the evidence ambiguous.\n\n**Statement D**: The significant positive coefficient on Leverage in the AQR model indicates that the supervisor was more lenient towards highly levered banks, consistent with regulatory capture.\n\nThis is incorrect. A positive coefficient (1.67***) means higher leverage leads to higher (not lower) capital curtailments. This indicates the supervisor was harsher (not more lenient) on highly levered banks, which is consistent with a risk-based approach, not regulatory capture.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 339, "Question": "### Background\n\n**Research Question.** How can the theoretical properties of the mortgage risk model be used to justify and assess the validity of using a simpler infinite-horizon calculation to approximate a more complex finite-term risk?\n\n**Setting.** The paper proposes using the infinite-horizon survival probability, `\\psi^\\infty(u)`, which is often analytically tractable, as an approximation for the finite-time survival probability, `\\psi(u,t)`, which is relevant for fixed-term loans but harder to compute. The validity of this approximation depends on the rate of convergence between the two.\n\n**Variables and Parameters.**\n\n*   `\\psi(u,t)`: Probability of repaying loan `u` before time `t`.\n*   `\\psi^\\infty(u)`: Probability of ever repaying loan `u` (`\\lim_{t\\to\\infty} \\psi(u,t)`).\n*   `W`: Random variable for the inter-arrival time between disasters.\n*   `M_W(\\theta) = \\mathbf{E}[e^{\\theta W}]`: The moment generating function (MGF) of `W`.\n*   `X`: Random variable for disaster severity.\n\n---\n\n### Data / Model Specification\n\nThe paper's key theoretical result on convergence is:\n\n**Theorem:** Assume there exists `\\theta_0 > 0` such that the MGF `M_W(\\theta)` exists for `\\theta < \\theta_0`. Then, the convergence is exponentially fast. For any `u>0`, there exists `C(u)>0` such that:\n\n  \n0 \\le \\psi^{\\infty}(u) - \\psi(u,t) \\le C(u)e^{-\\xi t} \\quad \\text{(Eq. 1)}\n \n\nfor some decay rate `\\xi > 0`.\n\n---\n\n### Question\n\nSuppose the assumption that the MGF of `W` exists for some `\\theta > 0` is violated because the inter-arrival time `W` follows a heavy-tailed distribution (e.g., Pareto). Select all of the following statements that are true consequences of this violation.\n", "Options": {"A": "The convergence of `\\psi(u,t)` to `\\psi^\\infty(u)` would be polynomial, not exponential.", "B": "The survival probability `\\psi(u,t)` would fail to converge to `\\psi^\\infty(u)` as `t` goes to infinity.", "C": "The expected time between disasters, `E[W]`, would be infinite.", "D": "Using the infinite-horizon default probability would likely lead to a significant underestimation of the true 30-year default risk."}, "Answer": ["A", "D"], "pi_justification": "This item assesses understanding of the theoretical underpinnings of the paper's approximation method. It uses the **Scenario Application** strategy by asking for the consequences of violating a key assumption. The options are designed to discriminate between precise understanding and common misconceptions. Correct options (A, C) state the true consequences (slower convergence, risk underestimation). Distractors include a conceptual opposite (B: 'no convergence' vs. 'slower convergence') and a subtle confusion (D: equating heavy tails with an infinite mean, which is not necessarily true or the core issue).", "qid": "339", "question": "### Background\n\n**Research Question.** How can the theoretical properties of the mortgage risk model be used to justify and assess the validity of using a simpler infinite-horizon calculation to approximate a more complex finite-term risk?\n\n**Setting.** The paper proposes using the infinite-horizon survival probability, `\\psi^\\infty(u)`, which is often analytically tractable, as an approximation for the finite-time survival probability, `\\psi(u,t)`, which is relevant for fixed-term loans but harder to compute. The validity of this approximation depends on the rate of convergence between the two.\n\n**Variables and Parameters.**\n\n*   `\\psi(u,t)`: Probability of repaying loan `u` before time `t`.\n*   `\\psi^\\infty(u)`: Probability of ever repaying loan `u` (`\\lim_{t\\to\\infty} \\psi(u,t)`).\n*   `W`: Random variable for the inter-arrival time between disasters.\n*   `M_W(\\theta) = \\mathbf{E}[e^{\\theta W}]`: The moment generating function (MGF) of `W`.\n*   `X`: Random variable for disaster severity.\n\n---\n\n### Data / Model Specification\n\nThe paper's key theoretical result on convergence is:\n\n**Theorem:** Assume there exists `\\theta_0 > 0` such that the MGF `M_W(\\theta)` exists for `\\theta < \\theta_0`. Then, the convergence is exponentially fast. For any `u>0`, there exists `C(u)>0` such that:\n\n  \n0 \\le \\psi^{\\infty}(u) - \\psi(u,t) \\le C(u)e^{-\\xi t} \\quad \\text{(Eq. 1)}\n \n\nfor some decay rate `\\xi > 0`.\n\n---\n\n### Question\n\nSuppose the assumption that the MGF of `W` exists for some `\\theta > 0` is violated because the inter-arrival time `W` follows a heavy-tailed distribution (e.g., Pareto). Select all of the following statements that are true consequences of this violation.\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze what happens when the MGF of W doesn't exist for any θ > 0, which occurs with heavy-tailed distributions like Pareto.\n\nLet me examine each option:\n\n**Option A**: When the MGF doesn't exist for θ > 0, we lose the exponential tail decay that enables exponential convergence. Heavy-tailed distributions typically lead to polynomial convergence rates instead. This is a well-established result in probability theory - the convergence rate is directly related to the tail behavior of the distribution.\n\n**Option B**: This is incorrect. The convergence of ψ(u,t) to ψ^∞(u) depends on whether the underlying stochastic process reaches the absorbing state (loan repayment) with probability 1, not on the existence of the MGF. Even with heavy-tailed inter-arrival times, convergence can still occur, just at a slower rate.\n\n**Option C**: This is incorrect. A Pareto distribution can have finite mean depending on its shape parameter α. For example, if W ~ Pareto(α, x_m) with α > 1, then E[W] = αx_m/(α-1), which is finite. The MGF can fail to exist even when the mean is finite.\n\n**Option D**: This requires careful analysis. With polynomial convergence (say |ψ^∞(u) - ψ(u,t)| ≤ C/t^α), the error for t=30 years would be approximately C/30^α. For typical polynomial rates (α between 0.5 and 2), this gives errors ranging from ~18% to ~0.1% of C. Whether this constitutes \"significant underestimation\" depends on the specific parameters, but given that financial applications often require high precision and that C could be substantial, this would likely represent a significant error in practice.\n\nTherefore, the true consequences are A and D.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 462, "Question": "### Background\n\n**Research Question.** What is the causal effect of international climate policy announcements on investor participation in carbon-intensive firms?\n\n**Setting.** The study uses a global sample of publicly traded firms to analyze changes in investor holdings around two key events: the signing of the Paris Agreement (PA) and the subsequent announcement of the US withdrawal. Firms are classified into High-Carbon (HC) and Low-Carbon (LC) groups.\n\n**Data / Model Specification.**\nThe study employs a multi-period Difference-in-Differences (DiD) regression model:\n\n  \nY_{it} = \\alpha_i + \\delta_t + \\beta_1 (HC_i \\times PostPA_t) + \\beta_2 (HC_i \\times PostUSW_t) + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\n- `Y_{it}`: Institutional investor ownership in firm `i` at time `t`.\n- `HC_i`: A dummy for High-Carbon firms (treatment group).\n- `PostPA_t`, `PostUSW_t`: Dummies for the periods after the two events.\n\n**Table 1: Stylized DiD Regression Results**\n\n| Variable | Coefficient Estimate |\n| :--- | :--- |\n| `HC x PostPA` (`\\beta_1`) | -0.05*** |\n| `HC x PostUSW` (`\\beta_2`) | +0.04** |\n\n*Note: *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nBased on the provided model and results, select ALL of the following statements that are correct.", "Options": {"A": "The coefficient `β1 = -0.05` implies that after the Paris Agreement, investor ownership in High-Carbon firms fell by 5 percentage points relative to the change in ownership for Low-Carbon firms.", "B": "The study's causal interpretation relies on the 'parallel trends' assumption: that absent the policy events, ownership trends in High-Carbon and Low-Carbon firms would have evolved similarly.", "C": "The results suggest that investors react to the perceived credibility of global climate policy, divesting when the policy seems strong and reinvesting when its enforcement appears weakened.", "D": "The positive coefficient `β2 = +0.04` indicates that the US withdrawal completely erased the initial effect of the Paris Agreement, returning ownership in HC firms to its pre-agreement trend."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests the interpretation of DiD regression results and the understanding of the key identifying assumption. It uses an Atomic Decomposition strategy. The distractor (C) is a subtle 'Almost Right' error, testing for precise interpretation of the coefficient magnitudes rather than a general impression.", "qid": "462", "question": "### Background\n\n**Research Question.** What is the causal effect of international climate policy announcements on investor participation in carbon-intensive firms?\n\n**Setting.** The study uses a global sample of publicly traded firms to analyze changes in investor holdings around two key events: the signing of the Paris Agreement (PA) and the subsequent announcement of the US withdrawal. Firms are classified into High-Carbon (HC) and Low-Carbon (LC) groups.\n\n**Data / Model Specification.**\nThe study employs a multi-period Difference-in-Differences (DiD) regression model:\n\n  \nY_{it} = \\alpha_i + \\delta_t + \\beta_1 (HC_i \\times PostPA_t) + \\beta_2 (HC_i \\times PostUSW_t) + \\epsilon_{it} \\quad \\text{(Eq. (1))}\n \n\n- `Y_{it}`: Institutional investor ownership in firm `i` at time `t`.\n- `HC_i`: A dummy for High-Carbon firms (treatment group).\n- `PostPA_t`, `PostUSW_t`: Dummies for the periods after the two events.\n\n**Table 1: Stylized DiD Regression Results**\n\n| Variable | Coefficient Estimate |\n| :--- | :--- |\n| `HC x PostPA` (`\\beta_1`) | -0.05*** |\n| `HC x PostUSW` (`\\beta_2`) | +0.04** |\n\n*Note: *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nBased on the provided model and results, select ALL of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The coefficient β₁ = -0.05 represents the differential change in institutional investor ownership for High-Carbon firms relative to Low-Carbon firms after the Paris Agreement. This is the standard interpretation of a DiD coefficient, so this statement is correct.\n\nB) The parallel trends assumption is fundamental to the validity of any DiD design. It assumes that in the absence of treatment (the policy events), the outcome variable would have followed parallel trends between treatment and control groups. This is essential for causal interpretation, so this statement is correct.\n\nC) The results show that investors reduced holdings in HC firms after the Paris Agreement (β₁ = -0.05) and increased holdings after the US withdrawal (β₂ = +0.04). This pattern is consistent with investors responding to the perceived strength/credibility of climate policy - divesting when policy appears strong and reinvesting when enforcement is weakened. This statement is correct.\n\nD) While β₂ = +0.04 shows a partial reversal, it does not completely offset β₁ = -0.05. The net effect after both events is -0.05 + 0.04 = -0.01, meaning there's still a 1 percentage point reduction relative to the pre-agreement baseline. The statement claiming \"complete erasure\" is incorrect.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 244, "Question": "### Background\n\n**Research Question.** How can one construct a robust econometric framework to test the hypothesis that the market prices of macroeconomic risks are not constant, but vary predictably over the business cycle?\n\n**Setting.** The study's empirical core is a Generalized Method of Moments (GMM) framework designed to jointly model and estimate the relationship between asset returns and the conditional moments of industrial production.\n\n### Data / Model Specification\n\n1.  **Variable Construction:** The model uses two key industrial production variables:\n    *   The realized production growth to be priced, `YP_t`, is the 12-month log growth of *seasonally adjusted* industrial production.\n    *   The business cycle proxy, `IPL_t`, is the 12-month log growth of *seasonally unadjusted* industrial production, lagged two months (`IPL_{t-2}`).\n\n2.  **GMM System:** The model is estimated as a system of equations:\n\n      \n    e_{1t} = YP_t - \\phi_0 - \\sum_{j} \\phi_j IPL_{t-j} \\quad \\text{(Eq. 1)}\n     \n\n      \n    e_{2t} = r_{mt} - \\eta_{0s} - (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) YP_t - (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) e_{1t}^2 \\quad \\text{(Eq. 2)}\n     \n\n    where `r_{mt}` is the stock return and `e_{1t}^2` proxies for production volatility.\n\n### Question\n\nRegarding the study's GMM estimation framework and its underlying assumptions, which of the following statements accurately describe potential statistical problems and the methodological choices made to address them?", "Options": {"A": "Estimating the system jointly via GMM is preferred to a two-step OLS approach primarily to avoid the \"generated regressors problem,\" which would lead to underestimated standard errors.", "B": "The joint GMM estimation is necessary because stock returns (`r_mt`) are endogenous with respect to production growth (`YP_t`).", "C": "If the production growth series `YP_t` were non-stationary (contained a unit root), the GMM estimates would be invalid due to the risk of spurious regression.", "D": "The main consequence of using a \"weak instrument\" (`IPL_{t-2}`) would be heteroskedasticity in the pricing errors, which GMM corrects automatically."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 10.0). This item assesses knowledge of advanced econometric issues central to the paper's methodology. It uses a 'Reverse-Reasoning' strategy, asking for the problems that justify the methods used. Correct options (A, B) identify two critical issues: the generated regressors problem and the stationarity assumption. Distractor (C) misidentifies the consequence of a classic econometric problem (weak instruments), a 'Formula Misuse' type of error. Distractor (D) offers a plausible but incorrect primary motivation for the specific GMM vs. two-step OLS choice, making it a strong 'Almost Right' distractor.", "qid": "244", "question": "### Background\n\n**Research Question.** How can one construct a robust econometric framework to test the hypothesis that the market prices of macroeconomic risks are not constant, but vary predictably over the business cycle?\n\n**Setting.** The study's empirical core is a Generalized Method of Moments (GMM) framework designed to jointly model and estimate the relationship between asset returns and the conditional moments of industrial production.\n\n### Data / Model Specification\n\n1.  **Variable Construction:** The model uses two key industrial production variables:\n    *   The realized production growth to be priced, `YP_t`, is the 12-month log growth of *seasonally adjusted* industrial production.\n    *   The business cycle proxy, `IPL_t`, is the 12-month log growth of *seasonally unadjusted* industrial production, lagged two months (`IPL_{t-2}`).\n\n2.  **GMM System:** The model is estimated as a system of equations:\n\n      \n    e_{1t} = YP_t - \\phi_0 - \\sum_{j} \\phi_j IPL_{t-j} \\quad \\text{(Eq. 1)}\n     \n\n      \n    e_{2t} = r_{mt} - \\eta_{0s} - (\\lambda_{0s} + \\lambda_{1s} IPL_{t-2}) YP_t - (\\kappa_{0s} + \\kappa_{1s} IPL_{t-2}) e_{1t}^2 \\quad \\text{(Eq. 2)}\n     \n\n    where `r_{mt}` is the stock return and `e_{1t}^2` proxies for production volatility.\n\n### Question\n\nRegarding the study's GMM estimation framework and its underlying assumptions, which of the following statements accurately describe potential statistical problems and the methodological choices made to address them?", "model_evaluation_result": {"model_raw_response": "Looking at this GMM framework for testing time-varying market prices of macroeconomic risks, I need to evaluate each statement about potential statistical problems and methodological choices.\n\n**Statement A**: This correctly identifies a key advantage of joint GMM estimation. In a two-step approach, first estimating equation (1) to get ê₁ₜ, then using ê₁ₜ² in equation (2) would create a \"generated regressors problem.\" The standard errors in the second step would be underestimated because they wouldn't account for the estimation uncertainty from the first step. Joint GMM properly accounts for this by estimating both equations simultaneously.\n\n**Statement B**: This is incorrect. The GMM system doesn't treat stock returns as endogenous with respect to production growth. In equation (2), r_mt is the dependent variable and YP_t is an explanatory variable. The joint estimation is needed for efficiency and to handle the generated regressor issue, not because of endogeneity between these variables.\n\n**Statement C**: This correctly identifies a serious concern. If YP_t contained a unit root (was non-stationary), the regression relationships would be spurious unless the variables were cointegrated. GMM doesn't solve non-stationarity problems - the estimates would indeed be invalid. This is why proper time series testing for stationarity is crucial before applying GMM.\n\n**Statement D**: This is incorrect on multiple counts. Weak instruments primarily cause bias and inconsistency in instrumental variables estimation, not heteroskedasticity. Moreover, GMM doesn't \"automatically correct\" for heteroskedasticity - it requires appropriate weighting matrices and robust standard errors to handle heteroskedasticity.\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 450, "Question": "### Background\n\n**Research Question.** This case evaluates the derivation and practical effectiveness of different hedging strategies by comparing their ability to reduce portfolio risk.\n\n**Setting.** An investor holds a position in a cash commodity and wishes to hedge the price risk using futures. The return on a hedged portfolio is `r_{H,t} = r_{c,t} - \\beta r_{f,t}`. The performance of different strategies is measured by the resulting standard deviation of this portfolio.\n\n### Data / Model Specification\n\nThe time-varying optimal minimum-variance hedge ratio is given by `\\beta_t^* = \\operatorname{Cov}_t(r_{c,t}, r_{f,t}) / \\operatorname{Var}_t(r_{f,t})`. The paper employs a Constant Conditional Correlation (CCC) GARCH model where:\n*   `\\operatorname{Var}_t(r_{f,t}) = h_{r_f,t}^2`\n*   `\\operatorname{Cov}_t(r_{c,t}, r_{f,t}) = \\rho_{r_c r_f} h_{r_c,t} h_{r_f,t}`\n\n**Table 1. Comparison of Hedging Strategy Performance for Crude Oil**\n\n| Strategy | Hedge Ratio ($\\beta$) | Portfolio Std. Dev. |\n|:---|:---:|:---:|\n| CCC-GARCH | 0.74 (average) | 0.016322 |\n| OLS | 0.70 | 0.016416 |\n| Naïve | 1.00 | 0.018017 |\n\n*Source: Adapted from Table 5 in the source paper.*\n\n### Question\n\nBased on the theory and data provided, select all of the following statements that are correct.", "Options": {"A": "The explicit formula for the dynamic hedge ratio (`\\beta_t^*`) derived from the CCC-GARCH specification is `\\rho_{r_c r_f} (h_{r_c,t} / h_{r_f,t})`.", "B": "The primary conceptual advantage of a dynamic hedge over a static OLS hedge is its ability to adapt the hedge ratio to time-varying conditional volatilities and correlations.", "C": "For Crude Oil, the data shows that the dynamic CCC-GARCH hedge reduces portfolio risk (standard deviation) by approximately 9.4% compared to the Naïve hedge.", "D": "The data for Crude Oil demonstrates that the static OLS hedge provides a greater risk reduction than the dynamic CCC-GARCH hedge."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses 'atomic decomposition' to test multiple facets of the hedging analysis: theoretical derivation, conceptual understanding, and empirical calculation. This makes it a 'Computational Judgment' item. Option A tests the derivation of the dynamic hedge ratio formula. Option C tests the ability to calculate percentage risk reduction from the table. Option B is a conceptual distractor that is almost correct but the CCC-GARCH model assumes *constant* correlation, so it cannot adapt to time-varying correlations (Almost Right). Option D is a direct contradiction of the data in the table (Conceptual Opposite). This structure creates a robust multi-select question with two correct, independent statements.", "qid": "450", "question": "### Background\n\n**Research Question.** This case evaluates the derivation and practical effectiveness of different hedging strategies by comparing their ability to reduce portfolio risk.\n\n**Setting.** An investor holds a position in a cash commodity and wishes to hedge the price risk using futures. The return on a hedged portfolio is `r_{H,t} = r_{c,t} - \\beta r_{f,t}`. The performance of different strategies is measured by the resulting standard deviation of this portfolio.\n\n### Data / Model Specification\n\nThe time-varying optimal minimum-variance hedge ratio is given by `\\beta_t^* = \\operatorname{Cov}_t(r_{c,t}, r_{f,t}) / \\operatorname{Var}_t(r_{f,t})`. The paper employs a Constant Conditional Correlation (CCC) GARCH model where:\n*   `\\operatorname{Var}_t(r_{f,t}) = h_{r_f,t}^2`\n*   `\\operatorname{Cov}_t(r_{c,t}, r_{f,t}) = \\rho_{r_c r_f} h_{r_c,t} h_{r_f,t}`\n\n**Table 1. Comparison of Hedging Strategy Performance for Crude Oil**\n\n| Strategy | Hedge Ratio ($\\beta$) | Portfolio Std. Dev. |\n|:---|:---:|:---:|\n| CCC-GARCH | 0.74 (average) | 0.016322 |\n| OLS | 0.70 | 0.016416 |\n| Naïve | 1.00 | 0.018017 |\n\n*Source: Adapted from Table 5 in the source paper.*\n\n### Question\n\nBased on the theory and data provided, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) The explicit formula for the dynamic hedge ratio (`β_t*`) derived from the CCC-GARCH specification is `ρ_{r_c r_f} (h_{r_c,t} / h_{r_f,t})`.\n\nGiven:\n- β_t* = Cov_t(r_{c,t}, r_{f,t}) / Var_t(r_{f,t})\n- Var_t(r_{f,t}) = h²_{r_f,t}\n- Cov_t(r_{c,t}, r_{f,t}) = ρ_{r_c r_f} h_{r_c,t} h_{r_f,t}\n\nSubstituting:\nβ_t* = (ρ_{r_c r_f} h_{r_c,t} h_{r_f,t}) / h²_{r_f,t} = ρ_{r_c r_f} (h_{r_c,t} / h_{r_f,t})\n\nThis is correct. ✓\n\nB) The primary conceptual advantage of a dynamic hedge over a static OLS hedge is its ability to adapt the hedge ratio to time-varying conditional volatilities and correlations.\n\nThis accurately describes the key advantage of dynamic hedging strategies like CCC-GARCH over static strategies. The dynamic hedge ratio adjusts as market conditions change. This is correct. ✓\n\nC) For Crude Oil, the data shows that the dynamic CCC-GARCH hedge reduces portfolio risk (standard deviation) by approximately 9.4% compared to the Naïve hedge.\n\nCalculation: (0.018017 - 0.016322) / 0.018017 × 100% = 9.41%\n\nThis is correct. ✓\n\nD) The data for Crude Oil demonstrates that the static OLS hedge provides a greater risk reduction than the dynamic CCC-GARCH hedge.\n\nFrom the table:\n- CCC-GARCH: 0.016322\n- OLS: 0.016416\n\nSince 0.016322 < 0.016416, the CCC-GARCH hedge actually provides", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 251, "Question": "### Background\n\nThe paper proposes that a key feature of a derivative is its embedded leverage (`Ω`), which is the return magnification it offers relative to its underlying asset. The central hypothesis is that leverage-constrained investors bid up the prices of securities with high `Ω`, leading to lower subsequent risk-adjusted returns.\n\n### Data / Model Specification\n\n**Table 1: Summary Statistics for Embedded Leverage (1996-2018)**\n\n| Sample | Mean `Ω` |\n| :--- | :--- |\n| Equity Options | 7.02 |\n| Index Options | 14.61 |\n\n**Table 2: Embedded Leverage and Returns for Extreme Index Option Portfolios**\n\n| Portfolio Category | Maturity | `|Δ|` Range | Avg. `Ω` | Avg. Delta-Hedged Excess Return (%) |\n| :--- | :--- | :--- | :--- | :--- |\n| High-Leverage | < 1 month | 0.00 - 0.20 | 42.16 | -29.23% |\n| Low-Leverage | > 12 months | 0.80 - 1.00 | 3.27 | +0.01% |\n\n### Question\n\nBased on the empirical evidence presented in Table 1 and Table 2, select all of the following statements that are true.", "Options": {"A": "On average, options on broad market indexes tend to have higher embedded leverage than options on individual equities.", "B": "Securities with higher embedded leverage are associated with lower subsequent risk-adjusted returns.", "C": "Securities with higher embedded leverage are associated with higher subsequent risk-adjusted returns.", "D": "On average, options on individual equities tend to have higher embedded leverage than options on broad market indexes."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the student's ability to interpret empirical results from tables, replacing the 'interpretation' part of the original QA. It is a 'select-all-valid' task. The correct options (A, B) are direct conclusions from Table 2 and Table 1, respectively. The distractors (C, D) are the direct conceptual opposites of the correct statements, designed to catch misinterpretations of the data or the paper's main hypothesis.", "qid": "251", "question": "### Background\n\nThe paper proposes that a key feature of a derivative is its embedded leverage (`Ω`), which is the return magnification it offers relative to its underlying asset. The central hypothesis is that leverage-constrained investors bid up the prices of securities with high `Ω`, leading to lower subsequent risk-adjusted returns.\n\n### Data / Model Specification\n\n**Table 1: Summary Statistics for Embedded Leverage (1996-2018)**\n\n| Sample | Mean `Ω` |\n| :--- | :--- |\n| Equity Options | 7.02 |\n| Index Options | 14.61 |\n\n**Table 2: Embedded Leverage and Returns for Extreme Index Option Portfolios**\n\n| Portfolio Category | Maturity | `|Δ|` Range | Avg. `Ω` | Avg. Delta-Hedged Excess Return (%) |\n| :--- | :--- | :--- | :--- | :--- |\n| High-Leverage | < 1 month | 0.00 - 0.20 | 42.16 | -29.23% |\n| Low-Leverage | > 12 months | 0.80 - 1.00 | 3.27 | +0.01% |\n\n### Question\n\nBased on the empirical evidence presented in Table 1 and Table 2, select all of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "Looking at the data provided:\n\nFrom Table 1:\n- Equity Options have mean Ω = 7.02\n- Index Options have mean Ω = 14.61\n\nThis clearly shows that index options have higher embedded leverage than equity options on average.\n\nFrom Table 2:\n- High-Leverage portfolio (Ω = 42.16) has average delta-hedged excess return of -29.23%\n- Low-Leverage portfolio (Ω = 3.27) has average delta-hedged excess return of +0.01%\n\nThis demonstrates that securities with higher embedded leverage have lower subsequent risk-adjusted returns.\n\nTherefore:\n- Statement A is TRUE (14.61 > 7.02)\n- Statement B is TRUE (higher Ω associated with lower returns)\n- Statement C is FALSE (contradicts the data)\n- Statement D is FALSE (opposite of what the data shows)\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 461, "Question": "### Background\n\n**Research Question.** How can stress tests that focus solely on the banking sector misrepresent systemic risk by ignoring contagion from the non-bank financial sector, such as investment funds?\n\n**Setting.** A regulator conducts a stress test on a representative bank. The analysis compares a 'banks-only' scenario with an 'integrated' scenario that also models the behavior of investment funds holding similar assets, specifically government bonds.\n\n**Variables and Parameters.**\n- `A`: Total assets.\n- `L`: Total liabilities.\n- `E`: Equity capital (`E = A - L`).\n- `CR`: Capital Ratio (`CR = E / A`).\n- `A_{GB}`: Assets held in government bonds.\n- `A_{Loans}`: Assets held in loans.\n- `ε_L`: Loss rate on the loan portfolio under stress (dimensionless).\n- `ε_{GB}`: Loss rate on the government bond portfolio due to fire sales (dimensionless).\n\n---\n\n### Data / Model Specification\n\nConsider the stylized balance sheet of a representative bank before the stress test.\n\n**Table 1: Representative Bank Balance Sheet (Pre-Stress)**\n\n| Assets | Value | Liabilities & Equity | Value |\n| :--- | :--- | :--- | :--- |\n| Government Bonds | 400 | Deposits | 920 |\n| Loans | 600 | Equity | 80 |\n| **Total Assets** | **1000** | **Total Liab. & Equity** | **1000** |\n\n**Scenario 1 (Banks-Only):** A severe recession is simulated. The shock is a direct credit loss on the bank's loan portfolio. The loss rate `ε_L` is 10%.\n\n**Scenario 2 (Integrated):** The same recession also causes massive outflows from investment funds. These funds are forced to sell their large holdings of government bonds, the same type held by the bank. This fire sale causes the market price of these bonds to fall. The loss rate `ε_{GB}` is 15%.\n\n---\n\n### Question\n\nBased on the provided data and scenarios, select ALL of the following statements that are true.", "Options": {"A": "The bank's initial capital ratio is 8.0%.", "B": "Under the 'Banks-Only' scenario (Scenario 1), the bank's capital ratio falls to approximately 2.13%, and the bank remains solvent.", "C": "The additional fire-sale shock from investment funds reduces the bank's final capital ratio by approximately one percentage point, a magnitude consistent with the general findings of Sydow et al.", "D": "In the integrated scenario (combining Scenario 1 and 2), the bank becomes insolvent with a final capital ratio of approximately -4.55%."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Computational Judgment strategy to test the core calculations from the original QA problem. The options are designed using atomic decomposition, breaking the original multi-part question into independently verifiable statements. The distractor (C) is a conceptual error that tempts the user to apply a qualitative finding from the paper to a stylized example where the effect is deliberately amplified, testing whether they performed the calculation or are relying on memory.", "qid": "461", "question": "### Background\n\n**Research Question.** How can stress tests that focus solely on the banking sector misrepresent systemic risk by ignoring contagion from the non-bank financial sector, such as investment funds?\n\n**Setting.** A regulator conducts a stress test on a representative bank. The analysis compares a 'banks-only' scenario with an 'integrated' scenario that also models the behavior of investment funds holding similar assets, specifically government bonds.\n\n**Variables and Parameters.**\n- `A`: Total assets.\n- `L`: Total liabilities.\n- `E`: Equity capital (`E = A - L`).\n- `CR`: Capital Ratio (`CR = E / A`).\n- `A_{GB}`: Assets held in government bonds.\n- `A_{Loans}`: Assets held in loans.\n- `ε_L`: Loss rate on the loan portfolio under stress (dimensionless).\n- `ε_{GB}`: Loss rate on the government bond portfolio due to fire sales (dimensionless).\n\n---\n\n### Data / Model Specification\n\nConsider the stylized balance sheet of a representative bank before the stress test.\n\n**Table 1: Representative Bank Balance Sheet (Pre-Stress)**\n\n| Assets | Value | Liabilities & Equity | Value |\n| :--- | :--- | :--- | :--- |\n| Government Bonds | 400 | Deposits | 920 |\n| Loans | 600 | Equity | 80 |\n| **Total Assets** | **1000** | **Total Liab. & Equity** | **1000** |\n\n**Scenario 1 (Banks-Only):** A severe recession is simulated. The shock is a direct credit loss on the bank's loan portfolio. The loss rate `ε_L` is 10%.\n\n**Scenario 2 (Integrated):** The same recession also causes massive outflows from investment funds. These funds are forced to sell their large holdings of government bonds, the same type held by the bank. This fire sale causes the market price of these bonds to fall. The loss rate `ε_{GB}` is 15%.\n\n---\n\n### Question\n\nBased on the provided data and scenarios, select ALL of the following statements that are true.", "model_evaluation_result": {"model_raw_response": "I need to analyze each statement based on the given data and scenarios.\n\nGiven data:\n- Total Assets (A) = 1000\n- Total Liabilities (L) = 920\n- Equity (E) = 80\n- Government Bonds (A_GB) = 400\n- Loans (A_Loans) = 600\n- Loss rate on loans (ε_L) = 10%\n- Loss rate on government bonds (ε_GB) = 15%\n\n**Statement A: The bank's initial capital ratio is 8.0%**\nInitial CR = E / A = 80 / 1000 = 0.08 = 8.0%\n✓ TRUE\n\n**Statement B: Under 'Banks-Only' scenario, CR falls to ~2.13% and bank remains solvent**\nScenario 1 losses = ε_L × A_Loans = 0.10 × 600 = 60\nNew equity = 80 - 60 = 20\nNew assets = 1000 - 60 = 940\nNew CR = 20 / 940 = 0.0213 ≈ 2.13%\nSince equity > 0, bank remains solvent\n✓ TRUE\n\n**Statement C: Additional fire-sale shock reduces CR by ~1 percentage point**\nScenario 2 (integrated) losses:\n- Loan losses = 60 (same as Scenario 1)\n- Government bond losses = ε_GB × A_GB = 0.15 × 400 = 60\n- Total losses = 60 + 60 = 120\nNew equity = 80 - 120 = -40\nNew assets = 1000 - 120 = 880\nNew CR = -40 / 880 = -0.0455 ≈ -4.55%\n\nDifference in CR = -4.55% - 2.13% = -6.68 percentage points\n✗ FALSE (The reduction is ~6.68 percentage points, not ~1)\n\n**Statement D: In integrated scenario, bank becomes insolvent with CR ≈ -4.55%**\nFrom calculation above", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 465, "Question": "### Background\n\n**Research Question.** In a financial system vulnerable to fire-sale contagion, what is the minimum amount of capital that a regulator must inject to halt the contagion, and can this amount be determined analytically?\n\n**Setting.** A fire-sale contagion process is modeled in discrete rounds. An initial shock causes some banks to sell assets (`S_1`). These sales depress prices, causing mark-to-market losses (`ΔA_t`) on other banks, which in turn leads to further sales (`S_{t+1}`).\n\n**Data / Model Specification.**\nThe propagation of fire sales is modeled by the linear relationship:\n\n  \nS_{t+1} = \\beta (\\lambda S_t) = \\phi S_t \\quad \\text{(Eq. (1))}\n \n\nwhere `λ` is the price impact of sales, `β` is the sensitivity of sales to losses, and `φ = βλ` is the **contagion coefficient**.\n\nA regulator can make a capital injection `K` to prevent the fire sale from starting. The optimal (minimum) injection `K*` is the amount just sufficient to prevent the first round of sales (`S_1=0`).\n\n---\n\n### Question\n\nBased on the provided fire-sale model, select ALL of the following statements that are correct.", "Options": {"A": "The minimum capital injection `k_i` needed to restore a single distressed bank to the regulatory leverage threshold is equal to the full amount of that bank's asset loss, `ΔA_{i,0}`.", "B": "The fire-sale spiral is self-sustaining and explosive if the contagion coefficient `φ = βλ` is greater than or equal to 1.", "C": "As the contagion coefficient `φ` approaches 1 from below, the total cumulative asset sales `S_total` from an initial shock `S_1` approaches infinity.", "D": "The total optimal injection `K*` is typically smaller than the total initial loss `ΔA_0` because the injection surgically targets only the 'excess leverage' of distressed banks, while well-capitalized banks can absorb their losses without intervention."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests the understanding of a dynamic fire-sale model and the logic of optimal bailouts. It uses an Atomic Decomposition strategy to test insights from all parts of the original QA. The distractor (B) is a high-fidelity 'Conceptual Opposite' that targets a critical and common misconception about the size of required capital injections, making it highly discriminative.", "qid": "465", "question": "### Background\n\n**Research Question.** In a financial system vulnerable to fire-sale contagion, what is the minimum amount of capital that a regulator must inject to halt the contagion, and can this amount be determined analytically?\n\n**Setting.** A fire-sale contagion process is modeled in discrete rounds. An initial shock causes some banks to sell assets (`S_1`). These sales depress prices, causing mark-to-market losses (`ΔA_t`) on other banks, which in turn leads to further sales (`S_{t+1}`).\n\n**Data / Model Specification.**\nThe propagation of fire sales is modeled by the linear relationship:\n\n  \nS_{t+1} = \\beta (\\lambda S_t) = \\phi S_t \\quad \\text{(Eq. (1))}\n \n\nwhere `λ` is the price impact of sales, `β` is the sensitivity of sales to losses, and `φ = βλ` is the **contagion coefficient**.\n\nA regulator can make a capital injection `K` to prevent the fire sale from starting. The optimal (minimum) injection `K*` is the amount just sufficient to prevent the first round of sales (`S_1=0`).\n\n---\n\n### Question\n\nBased on the provided fire-sale model, select ALL of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at each statement carefully:\n\n**Statement A**: The minimum capital injection `k_i` needed to restore a single distressed bank to the regulatory leverage threshold is equal to the full amount of that bank's asset loss, `ΔA_{i,0}`.\n\nThis is incorrect. The minimum injection needed is only the amount required to bring the bank back to the regulatory threshold, not the full loss. If a bank has some buffer above the threshold, it can absorb part of the loss without needing injection.\n\n**Statement B**: The fire-sale spiral is self-sustaining and explosive if the contagion coefficient `φ = βλ` is greater than or equal to 1.\n\nThis is correct. From equation (1), S_{t+1} = φS_t. If φ ≥ 1, then each round of sales is at least as large as the previous round, creating a non-converging (explosive) spiral.\n\n**Statement C**: As the contagion coefficient `φ` approaches 1 from below, the total cumulative asset sales `S_total` from an initial shock `S_1` approaches infinity.\n\nThis is correct. The total sales form a geometric series: S_total = S_1 + φS_1 + φ²S_1 + ... = S_1/(1-φ) for φ < 1. As φ → 1⁻, the denominator approaches 0, so S_total → ∞.\n\n**Statement D**: The total optimal injection `K*` is typically smaller than the total initial loss `ΔA_0` because the injection surgically targets only the 'excess leverage' of distressed banks, while well-capitalized banks can absorb their losses without intervention.\n\nThis is correct. The optimal injection only needs to prevent banks from breaching regulatory thresholds. Banks with sufficient capital buffers can absorb losses without triggering fire sales, so the regulator doesn't need to compensate their losses.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 295, "Question": "### Background\n\n**Research Question.** Does the statistical out-of-sample (OOS) predictability from combination forecasts translate into economically significant, risk-adjusted, net-of-cost returns for an investor?\n\n**Setting.** The paper evaluates the real-time performance of an industry-rotation strategy based on combination forecasts. First, the statistical accuracy of the forecasts is measured using the out-of-sample R-squared (`R_OS^2`). Second, the economic performance of a long-short trading strategy based on these forecasts is evaluated using the Fama-French-Carhart four-factor model, with a focus on net-of-costs alpha (`\\alpha_{net}`). Finally, the benefit to a mean-variance investor is assessed by calculating the maximum attainable Sharpe Ratio (SR).\n\n**Variables and Parameters.**\n- `R_OS^2`: The out-of-sample R-squared, measuring the percentage reduction in mean squared forecast error relative to a historical mean forecast.\n- `\\alpha_{net}`: The annualized abnormal return of a strategy after accounting for transaction costs and exposure to the MKT, SMB, HML, and MOM factors.\n- `\\beta_{MKT}, \\beta_{SMB}, \\beta_{HML}, \\beta_{MOM}`: Loadings on the Fama-French-Carhart factors.\n- `SR`: The annualized maximum net-of-costs Sharpe Ratio of a portfolio.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the key out-of-sample results for the 44 industry portfolios from 1980:1 to 2015:4.\n\n**Table 1. Average Out-of-Sample `R_OS^2` Statistics (%)**\n\n| Forecast Method | ABMA | BMi (Own BM) |\n| :--- | :--- | :--- |\n| Avg `R_OS^2`    | 2.10 | -3.27        |\n\n*Source: Table III in the original paper. ABMA is a combination forecast method. BMi is the benchmark forecast using only an industry's own BM ratio.*\n\n**Table 2. Performance of Long-Short Industry-Rotation Strategies**\n\n| Strategy | `\\alpha_{net}` (%) | `\\beta_{MKT}` | `\\beta_{SMB}` | `\\beta_{HML}` | `\\beta_{MOM}` |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| L-S ABMA | 8.15*** | 0.15* | -0.36*** | -0.05 | 0.62*** |\n| L-S BMi | -0.31 | 0.11 | -0.17* | 0.34*** | -0.10** |\n\n*Source: Table IV in the original paper. L-S ABMA is the long-short strategy based on the ABMA combination forecast. L-S BMi is the benchmark strategy. `***` denotes statistical significance.*\n\n**Table 3. Ex-Post Mean-Variance Efficient Portfolios (Net of Costs)**\n\n| Universe | MKT | SMB | HML | MOM | L-S ABMA | SR |\n| :--- | :-: | :-: | :-: | :-: | :---: | :-: |\n| FF4      | 0.38| 0.00| 0.41| 0.21|       | 0.71|\n| FF4+ABMA | 0.29| 0.03| 0.44| 0.01| 0.23  | 0.86|\n\n*Source: Table V in the original paper. FF4 refers to the universe of the four Fama-French-Carhart factors. Weights represent the tangency portfolio composition.*\n\n---\n\n### Question\n\nBased on the evidence presented in Tables 1, 2, and 3, which of the following conclusions about the ABMA combination forecast strategy are supported?", "Options": {"A": "Despite a significant loading on the momentum (MOM) factor, the `L-S ABMA` strategy is not redundant, as it generates significant alpha *after* accounting for momentum exposure.", "B": "Adding the `L-S ABMA` strategy to the investment universe expands the efficient frontier, increasing the maximum attainable Sharpe Ratio and making the MOM factor largely redundant in the optimal portfolio.", "C": "The `L-S ABMA` strategy's profitability is primarily driven by a traditional value premium, as shown by its large, positive loading on the HML factor.", "D": "The statistical out-of-sample predictability of the ABMA forecast, evidenced by a positive average `R_OS^2`, translates into a statistically significant, positive net-of-costs alpha (`\\alpha_{net}`)."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the paper's core findings across statistical, risk-adjusted, and portfolio-level evidence. It uses an **Atomic Decomposition** strategy, breaking down the paper's main conclusion into three independently verifiable statements. Option B serves as a high-fidelity 'Conceptual Opposite' distractor, testing a common misinterpretation of a strategy based on book-to-market ratios (expecting a value effect) against the actual factor loadings provided.", "qid": "295", "question": "### Background\n\n**Research Question.** Does the statistical out-of-sample (OOS) predictability from combination forecasts translate into economically significant, risk-adjusted, net-of-cost returns for an investor?\n\n**Setting.** The paper evaluates the real-time performance of an industry-rotation strategy based on combination forecasts. First, the statistical accuracy of the forecasts is measured using the out-of-sample R-squared (`R_OS^2`). Second, the economic performance of a long-short trading strategy based on these forecasts is evaluated using the Fama-French-Carhart four-factor model, with a focus on net-of-costs alpha (`\\alpha_{net}`). Finally, the benefit to a mean-variance investor is assessed by calculating the maximum attainable Sharpe Ratio (SR).\n\n**Variables and Parameters.**\n- `R_OS^2`: The out-of-sample R-squared, measuring the percentage reduction in mean squared forecast error relative to a historical mean forecast.\n- `\\alpha_{net}`: The annualized abnormal return of a strategy after accounting for transaction costs and exposure to the MKT, SMB, HML, and MOM factors.\n- `\\beta_{MKT}, \\beta_{SMB}, \\beta_{HML}, \\beta_{MOM}`: Loadings on the Fama-French-Carhart factors.\n- `SR`: The annualized maximum net-of-costs Sharpe Ratio of a portfolio.\n\n---\n\n### Data / Model Specification\n\nThe following tables summarize the key out-of-sample results for the 44 industry portfolios from 1980:1 to 2015:4.\n\n**Table 1. Average Out-of-Sample `R_OS^2` Statistics (%)**\n\n| Forecast Method | ABMA | BMi (Own BM) |\n| :--- | :--- | :--- |\n| Avg `R_OS^2`    | 2.10 | -3.27        |\n\n*Source: Table III in the original paper. ABMA is a combination forecast method. BMi is the benchmark forecast using only an industry's own BM ratio.*\n\n**Table 2. Performance of Long-Short Industry-Rotation Strategies**\n\n| Strategy | `\\alpha_{net}` (%) | `\\beta_{MKT}` | `\\beta_{SMB}` | `\\beta_{HML}` | `\\beta_{MOM}` |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| L-S ABMA | 8.15*** | 0.15* | -0.36*** | -0.05 | 0.62*** |\n| L-S BMi | -0.31 | 0.11 | -0.17* | 0.34*** | -0.10** |\n\n*Source: Table IV in the original paper. L-S ABMA is the long-short strategy based on the ABMA combination forecast. L-S BMi is the benchmark strategy. `***` denotes statistical significance.*\n\n**Table 3. Ex-Post Mean-Variance Efficient Portfolios (Net of Costs)**\n\n| Universe | MKT | SMB | HML | MOM | L-S ABMA | SR |\n| :--- | :-: | :-: | :-: | :-: | :---: | :-: |\n| FF4      | 0.38| 0.00| 0.41| 0.21|       | 0.71|\n| FF4+ABMA | 0.29| 0.03| 0.44| 0.01| 0.23  | 0.86|\n\n*Source: Table V in the original paper. FF4 refers to the universe of the four Fama-French-Carhart factors. Weights represent the tangency portfolio composition.*\n\n---\n\n### Question\n\nBased on the evidence presented in Tables 1, 2, and 3, which of the following conclusions about the ABMA combination forecast strategy are supported?", "model_evaluation_result": {"model_raw_response": "Looking at the data systematically:\n\nFrom Table 1:\n- ABMA has a positive R_OS^2 of 2.10%, indicating statistical out-of-sample predictability\n- BMi has a negative R_OS^2 of -3.27%, indicating it performs worse than the historical mean\n\nFrom Table 2:\n- L-S ABMA has α_net = 8.15% (statistically significant)\n- L-S ABMA has β_MOM = 0.62*** (significant positive loading on momentum)\n- L-S ABMA has β_HML = -0.05 (small, insignificant loading on value factor)\n- L-S BMi has α_net = -0.31 (insignificant)\n\nFrom Table 3:\n- Adding L-S ABMA to the FF4 universe increases SR from 0.71 to 0.86\n- In the optimal portfolio with ABMA, the MOM weight drops from 0.21 to 0.01\n- L-S ABMA receives a weight of 0.23 in the optimal portfolio\n\nEvaluating each option:\n\nA) TRUE - The L-S ABMA strategy has a significant momentum loading (0.62***) but still generates significant alpha (8.15***) after accounting for this exposure. This shows it's not redundant.\n\nB) TRUE - The SR increases from 0.71 to 0.86 when adding L-S ABMA, and the MOM weight drops from 0.21 to 0.01, indicating ABMA makes MOM largely redundant in the optimal portfolio.\n\nC) FALSE - The HML loading is -0.05 (small and insignificant), not large and positive. The strategy is not driven by value premium.\n\nD) TRUE - ABMA has positive R_OS^2 (2.10%) showing statistical predictability, and this translates to significant positive α_net (8.15***).\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 288, "Question": "### Background\n\n**Research Question.** How can a research design be improved to distinguish between insiders trading on specific foreknowledge of a listing versus trading on the firm's general good performance that makes it *eligible* for listing?\n\n**Setting.** A key challenge in the study is this identification problem. The observed pre-listing buying could be a reaction to the firm's strong underlying performance, which is also the reason the firm decides to list.\n\n---\n\nTo address the alternative hypothesis that insider buying is driven solely by general 'good performance' rather than specific listing information, a researcher proposes a difference-in-differences approach. This involves creating a control group of high-performing OTC firms that did *not* apply for listing and comparing their insider trading to the listing firms (the treatment group).\n\nWhich of the following are valid components or predictions of such a refined research design? Select all that apply.", "Options": {"A": "The key test statistic would be the difference in Mean Abnormal Net Purchasers between the listing firms and the control firms (`MANP_Listing - MANP_Control`).", "B": "If the 'good performance' hypothesis were the sole driver of trading, the test statistic (`MANP_Listing - MANP_Control`) would be expected to be statistically indistinguishable from zero.", "C": "The control group should be matched to the treatment group on characteristics like industry, firm size, and especially recent stock performance.", "D": "If the 'good performance' hypothesis were the sole driver of trading, the test statistic (`MANP_Listing - MANP_Control`) would be expected to be significantly negative."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: This item assesses the user's understanding of econometric research design, specifically the logic of a difference-in-differences approach used to solve an identification problem (confounding variables). Depth Strategy: Scenario Application. The user must apply the principles of experimental design to the specific scenario of disentangling two competing hypotheses from the paper. Distractor Logic: Option D is a 'Conceptual Opposite' distractor. If good performance were the only driver, both groups would have similar abnormal buying, leading to a difference near zero, not a negative one. The correct options correctly identify the test statistic (A), the null hypothesis prediction (B), and the proper construction of a control group (C).", "qid": "288", "question": "### Background\n\n**Research Question.** How can a research design be improved to distinguish between insiders trading on specific foreknowledge of a listing versus trading on the firm's general good performance that makes it *eligible* for listing?\n\n**Setting.** A key challenge in the study is this identification problem. The observed pre-listing buying could be a reaction to the firm's strong underlying performance, which is also the reason the firm decides to list.\n\n---\n\nTo address the alternative hypothesis that insider buying is driven solely by general 'good performance' rather than specific listing information, a researcher proposes a difference-in-differences approach. This involves creating a control group of high-performing OTC firms that did *not* apply for listing and comparing their insider trading to the listing firms (the treatment group).\n\nWhich of the following are valid components or predictions of such a refined research design? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this difference-in-differences research design to distinguish between insider trading based on listing information versus general good performance:\n\n**A) The key test statistic would be the difference in Mean Abnormal Net Purchasers between the listing firms and the control firms (`MANP_Listing - MANP_Control`).**\n- This is correct. In a difference-in-differences design, the key test statistic is indeed the difference between the treatment group (listing firms) and control group (non-listing firms) in their outcome variable (MANP).\n\n**B) If the 'good performance' hypothesis were the sole driver of trading, the test statistic (`MANP_Listing - MANP_Control`) would be expected to be statistically indistinguishable from zero.**\n- This is correct. If insider buying is driven only by good performance (not listing information), then both high-performing listing firms and high-performing non-listing firms should show similar insider trading patterns. The difference would be zero.\n\n**C) The control group should be matched to the treatment group on characteristics like industry, firm size, and especially recent stock performance.**\n- This is correct. Proper matching is essential for a valid difference-in-differences design. The control firms need to be similar to treatment firms in all relevant characteristics except for the listing decision. Recent stock performance is particularly important here since the alternative hypothesis is about \"good performance.\"\n\n**D) If the 'good performance' hypothesis were the sole driver of trading, the test statistic (`MANP_Listing - MANP_Control`) would be expected to be significantly negative.**\n- This is incorrect. If good performance drives insider trading equally in both groups, the difference would be zero, not negative. There's no reason why non-listing firms would have higher insider buying than listing firms under the good performance hypothesis.\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 284, "Question": "### Background\n\nThe paper posits that an incumbent firm's willingness to introduce a new product that cannibalizes an existing one can be a powerful strategy to deter market entry. However, this threat is often 'time-inconsistent' and not credible unless the firm makes a prior, irreversible commitment that changes its own incentives.\n\n### Data / Model Specification\n\nAn Incumbent (I) faces a potential Entrant (E). The payoffs are `(Π_I, Π_E)`.\n- If E Stays Out: `(100, 0)`\n- If E Enters & I Accommodates: `(60, 20)`\n- If E Enters & I Fights (cannibalizes): `(40, -10)`\n\nBefore the game begins, the Incumbent can make an irreversible investment `K` in the 'capacity to cannibalize'. This investment changes the Incumbent's payoff from fighting from 40 to `40 + B`, where `B` is a 'boldness' payoff from the investment.\n\n### Question\n\nUsing backward induction and the concept of Subgame Perfect Nash Equilibrium (SPNE), select all of the following statements that are correct.", "Options": {"A": "In the absence of the commitment investment (`K`), the SPNE outcome is that the Entrant enters and the Incumbent accommodates.", "B": "The maximum investment `K` the Incumbent would be willing to make to achieve credibility is 40.", "C": "For the threat to 'Fight' to become credible, the boldness payoff `B` must be greater than 20.", "D": "If the Incumbent makes the threat to fight credible, its net gain compared to the no-investment outcome is 60."}, "Answer": ["A", "B", "C"], "pi_justification": "This item uses a 'Computational Judgment' strategy, applying game theory to formalize the paper's argument about credible commitment and cannibalization. Option A is correct: without investment, if E enters, I compares Accommodate (60) vs. Fight (40) and chooses Accommodate. E anticipates this, compares Enter (20) vs. Stay Out (0), and chooses Enter. Option B is correct: for the threat to be credible, the payoff from fighting must exceed accommodating: `40 + B > 60`, which implies `B > 20`. Option D is correct: the value of successful deterrence is the difference between the monopoly payoff (100) and the duopoly payoff (60), which is 40. The firm would pay up to this amount for the investment `K`. Distractor C is a 'Formula Misuse' error; the net gain is 100 - 60 = 40, not 60.", "qid": "284", "question": "### Background\n\nThe paper posits that an incumbent firm's willingness to introduce a new product that cannibalizes an existing one can be a powerful strategy to deter market entry. However, this threat is often 'time-inconsistent' and not credible unless the firm makes a prior, irreversible commitment that changes its own incentives.\n\n### Data / Model Specification\n\nAn Incumbent (I) faces a potential Entrant (E). The payoffs are `(Π_I, Π_E)`.\n- If E Stays Out: `(100, 0)`\n- If E Enters & I Accommodates: `(60, 20)`\n- If E Enters & I Fights (cannibalizes): `(40, -10)`\n\nBefore the game begins, the Incumbent can make an irreversible investment `K` in the 'capacity to cannibalize'. This investment changes the Incumbent's payoff from fighting from 40 to `40 + B`, where `B` is a 'boldness' payoff from the investment.\n\n### Question\n\nUsing backward induction and the concept of Subgame Perfect Nash Equilibrium (SPNE), select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "I need to analyze this game using backward induction and SPNE concepts.\n\nFirst, let me understand the game structure:\n- Stage 1: Incumbent decides whether to invest K (which gives boldness payoff B when fighting)\n- Stage 2: Entrant decides whether to Enter or Stay Out\n- Stage 3: If Entry occurs, Incumbent decides whether to Accommodate or Fight\n\n**Analyzing without investment (K=0, B=0):**\n\nUsing backward induction:\n- At Stage 3 (if Entry occurs): Incumbent chooses between Accommodate (60) or Fight (40). Since 60 > 40, Incumbent will Accommodate.\n- At Stage 2: Knowing Incumbent will Accommodate, Entrant compares Stay Out (0) vs Enter (20). Since 20 > 0, Entrant will Enter.\n- SPNE without investment: (Enter, Accommodate) with payoffs (60, 20)\n\nSo **Option A is correct**.\n\n**Analyzing with investment:**\n\nFor Fight to be credible, Incumbent must prefer Fight over Accommodate after Entry:\n- Fight payoff with investment: 40 + B\n- Accommodate payoff: 60\n- Need: 40 + B > 60, so B > 20\n\nSo **Option C is correct**.\n\n**Maximum investment K:**\n\nIf B > 20 makes Fight credible, then:\n- Entrant anticipates Fight after Entry, getting -10\n- So Entrant Stays Out, giving Incumbent 100\n- Incumbent's net payoff = 100 - K\n\nFor this to be worthwhile:\n- Need: 100 - K ≥ 60 (the no-investment outcome)\n- So: K ≤ 40\n\nMaximum K = 40, so **Option B is correct**.\n\n**Net gain from credible threat:**\n\n- With credible threat: Incumbent gets 100 - K\n- Without investment: Incumbent gets 60\n- Net gain = (100 - K) - 60 = 40 - K\n\nIf K = 0, net gain would be 40. But K must be positive to generate B > 20.\nThe statement says net gain is 60, which would require K =", "model_answer": ["A", "B", "C"], "error": null}}
{"ID": 238, "Question": "### Background\n\nIn the multivariate Unobserved Components (UC) model, the information share (`IS_j`) for market `j` can be derived under the identifying assumption that the idiosyncratic noise covariance matrix, `Ω`, is diagonal.\n\n### Data / Model Specification\n\nThe derived formula for the information share is:\n\n  \nI S_{j}=\\frac{\\beta_{j}^{2}/\\omega_{j}^{2}}{1/\\sigma^{2}+\\sum_{i}\\beta_{i}^{2}/\\omega_{i}^{2}} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n*   `β_j` is the sensitivity of market `j`'s price innovation to the efficient price innovation `r_t`.\n*   `ω_j^2` is the variance of market `j`'s idiosyncratic noise.\n*   `σ^2` is the variance of the efficient price innovation `r_t`.\n\n---\n\nConsider the behavior of the information shares in this system under certain limiting conditions. Which of the following statements are mathematically and economically correct interpretations of **Eq. (1)**?", "Options": {"A": "If all markets have identical parameters (`β_j = β` and `ω_j^2 = ω` for all `j`), then each of the `N` markets will have an information share of `1/N`.", "B": "If a single market `k` becomes infinitely noisy (`ω_k^2 → ∞`), its information share `IS_k` approaches 0.", "C": "A market's information share is directly proportional to its \"signal-to-noise\" ratio, defined as `β_j^2 / ω_j^2`.", "D": "If a single market `k` becomes perfectly efficient (`ω_k^2 → 0`), its information share `IS_k` approaches 1, and the shares of all other markets approach 0."}, "Answer": ["B", "C", "D"], "pi_justification": "This item assesses a deep conceptual understanding of the paper's central theoretical result (Theorem 1) by requiring the user to analyze its properties in limiting cases. It uses a Scenario Application strategy with hypothetical scenarios. Options A and B test the correct analysis of the formula as a market becomes perfectly efficient or infinitely noisy, respectively. Option D correctly identifies the 'signal-to-noise' interpretation. Option C is a plausible numerical distractor; while the shares would be equal, they would not sum to 1 due to the `1/σ^2` term in the denominator, so `IS_j ≠ 1/N` unless `1/σ^2` is zero, which is not generally true.", "qid": "238", "question": "### Background\n\nIn the multivariate Unobserved Components (UC) model, the information share (`IS_j`) for market `j` can be derived under the identifying assumption that the idiosyncratic noise covariance matrix, `Ω`, is diagonal.\n\n### Data / Model Specification\n\nThe derived formula for the information share is:\n\n  \nI S_{j}=\\frac{\\beta_{j}^{2}/\\omega_{j}^{2}}{1/\\sigma^{2}+\\sum_{i}\\beta_{i}^{2}/\\omega_{i}^{2}} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n*   `β_j` is the sensitivity of market `j`'s price innovation to the efficient price innovation `r_t`.\n*   `ω_j^2` is the variance of market `j`'s idiosyncratic noise.\n*   `σ^2` is the variance of the efficient price innovation `r_t`.\n\n---\n\nConsider the behavior of the information shares in this system under certain limiting conditions. Which of the following statements are mathematically and economically correct interpretations of **Eq. (1)**?", "model_evaluation_result": {"model_raw_response": "Looking at the formula for information share:\n\n$$IS_j = \\frac{\\beta_j^2/\\omega_j^2}{1/\\sigma^2 + \\sum_i \\beta_i^2/\\omega_i^2}$$\n\nLet me analyze each option:\n\n**Option A**: If all markets have identical parameters (β_j = β and ω_j² = ω for all j), then:\n- Numerator for any market j: β²/ω²\n- Denominator: 1/σ² + N(β²/ω²)\n- IS_j = (β²/ω²)/(1/σ² + N(β²/ω²))\n\nThis does NOT equal 1/N in general. It would only equal 1/N if 1/σ² = 0, which is impossible since σ² > 0.\n\n**Option B**: If ω_k² → ∞ for market k:\n- The term β_k²/ω_k² → 0\n- So IS_k → 0/(positive denominator) = 0 ✓\n\n**Option C**: The information share formula shows that IS_j depends on the ratio β_j²/ω_j² in the numerator. While there's a relationship, it's not directly proportional because:\n- The denominator also contains β_j²/ω_j²\n- Other markets' signal-to-noise ratios affect the denominator\n- The term 1/σ² in the denominator prevents direct proportionality\n\n**Option D**: If ω_k² → 0 for market k:\n- The term β_k²/ω_k² → ∞\n- IS_k → (β_k²/ω_k²)/(1/σ² + β_k²/ω_k² + other terms)\n- As β_k²/ω_k² → ∞, this approaches (β_k²/ω_k²)/(β_k²/ω_k²) = 1\n- For other markets j ≠ k: IS_j → (finite)/(∞) = ", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 386, "Question": "### Background\n\n**Research Question.** To study the impact of capital flows on bond yields, one must first isolate the component of the yield that reflects risk compensation from the component that reflects expectations of future monetary policy. This requires precise definitions of bond risk premia.\n\n**Setting.** The analysis considers two alternative measures of the risk premium on an `n`-period zero-coupon U.S. Treasury bond: an ex-ante, model-based measure (the term premium) and an ex-post, model-free measure (realized excess return).\n\n### Data / Model Specification\n\nThe **term premium** (`TP`) is defined as the difference between the long-term yield and the average of expected future short rates:\n\n  \nTP_{t}^{(n)} \\equiv R_{t}^{(n)} - \\frac{1}{n}\\sum_{i=0}^{n-1}E_{t}(r_{t+i}) \\quad \\text{(Eq. 1)}\n \n\nwhere `R_t^{(n)}` is the `n`-period yield and `r_t` is the one-period short rate.\n\nThe **realized one-period excess return** (`D`) from buying an `n`-period bond and selling it one period later is:\n\n  \nD_{t+1}^{(n)} = \\ln(P_{t+1}^{(n-1)}) - \\ln(P_t^{(n)}) - r_t \\quad \\text{(Eq. 2)}\n \n\nwhere `P_t^{(n)}` is the bond's price.\n\nThe theoretical link between the two measures is:\n\n  \nTP_{t}^{(n)} = \\frac{1}{n} \\sum_{i=1}^{n-1} E_{t}(D_{t+i}^{(n-i+1)}) \\quad \\text{(Eq. 3)}\n \n\n### Question\n\nBased on the provided definitions and theoretical relationships between the term premium (`TP`) and realized excess return (`D`), select all of the following statements that are correct.", "Options": {"A": "The term premium (`TP`) is a \"model-free\" measure because it is directly calculated from bond yields, while the realized excess return (`D`) is \"model-dependent\" because it relies on the log-price transformation.", "B": "The realized excess return (`D`) is an \"ex-post\" and \"model-free\" measure because it is calculated after the fact using only observable market prices, making it a valuable tool for checking the robustness of findings based on the term premium.", "C": "The term premium (`TP`) is considered an \"ex-ante\" and \"model-dependent\" measure because it reflects future expectations and requires a theoretical model to estimate the unobservable path of future short rates.", "D": "The theoretical link in Eq. (3) implies that the Expectations Hypothesis (`TP = 0`) holds if and only if all expected one-period excess returns are zero (`E_t[D_{t+i}^{(k)}] = 0`), reflecting the economic principle of risk-neutrality towards duration risk."}, "Answer": ["B", "C", "D"], "pi_justification": "This item was converted from a General QA problem based on a high suitability score (9.0), reflecting its highly structured concepts and potential for high-fidelity distractors. The item uses an **Atomic Decomposition** strategy to test the theoretical definitions underpinning the paper's dependent variables. **Correct options (A, B, C)** cover the distinct properties of the two risk premium measures and the core theoretical result that unifies them (the conclusion of the original QA's proof). This structure efficiently assesses a broad range of foundational knowledge. **Distractor D (Conceptual Swap)** is a classic high-fidelity distractor that directly swaps the key attributes (\"model-dependent\" vs. \"model-free\") of the two concepts.", "qid": "386", "question": "### Background\n\n**Research Question.** To study the impact of capital flows on bond yields, one must first isolate the component of the yield that reflects risk compensation from the component that reflects expectations of future monetary policy. This requires precise definitions of bond risk premia.\n\n**Setting.** The analysis considers two alternative measures of the risk premium on an `n`-period zero-coupon U.S. Treasury bond: an ex-ante, model-based measure (the term premium) and an ex-post, model-free measure (realized excess return).\n\n### Data / Model Specification\n\nThe **term premium** (`TP`) is defined as the difference between the long-term yield and the average of expected future short rates:\n\n  \nTP_{t}^{(n)} \\equiv R_{t}^{(n)} - \\frac{1}{n}\\sum_{i=0}^{n-1}E_{t}(r_{t+i}) \\quad \\text{(Eq. 1)}\n \n\nwhere `R_t^{(n)}` is the `n`-period yield and `r_t` is the one-period short rate.\n\nThe **realized one-period excess return** (`D`) from buying an `n`-period bond and selling it one period later is:\n\n  \nD_{t+1}^{(n)} = \\ln(P_{t+1}^{(n-1)}) - \\ln(P_t^{(n)}) - r_t \\quad \\text{(Eq. 2)}\n \n\nwhere `P_t^{(n)}` is the bond's price.\n\nThe theoretical link between the two measures is:\n\n  \nTP_{t}^{(n)} = \\frac{1}{n} \\sum_{i=1}^{n-1} E_{t}(D_{t+i}^{(n-i+1)}) \\quad \\text{(Eq. 3)}\n \n\n### Question\n\nBased on the provided definitions and theoretical relationships between the term premium (`TP`) and realized excess return (`D`), select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at each statement carefully:\n\n**Statement A**: Claims TP is \"model-free\" and D is \"model-dependent\"\n- This is incorrect. The background explicitly states that TP is the \"model-based measure\" and D is the \"model-free measure\"\n- TP requires estimating unobservable expected future short rates E_t(r_{t+i}), which needs a model\n- D only uses observable market prices and yields\n\n**Statement B**: Claims D is \"ex-post\" and \"model-free\"\n- This is correct. The background confirms D is the \"ex-post, model-free measure\"\n- D is calculated after the fact using only observable prices: ln(P_{t+1}^{(n-1)}) - ln(P_t^{(n)}) - r_t\n- It's valuable for robustness checks precisely because it doesn't rely on model assumptions\n\n**Statement C**: Claims TP is \"ex-ante\" and \"model-dependent\"\n- This is correct. The background states TP is the \"ex-ante, model-based measure\"\n- TP reflects future expectations through E_t(r_{t+i})\n- Calculating these expectations requires a theoretical model since future short rates are unobservable\n\n**Statement D**: Links Expectations Hypothesis to risk-neutrality\n- This is correct. From Eq. (3): TP_t^{(n)} = (1/n) Σ E_t(D_{t+i}^{(n-i+1)})\n- If TP = 0 (Expectations Hypothesis), then all E_t[D_{t+i}^{(k)}] = 0\n- Zero expected excess returns means investors don't require compensation for duration risk, which is the definition of risk-neutrality\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 320, "Question": "### Background\n\n**Research Question.** This study investigates the causal pathway through which board gender diversity affects board effectiveness, testing whether board processes—the internal dynamics and working structures of the board—act as a mediating mechanism.\n\n**Setting / Data-Generating Environment.** The analysis uses cross-sectional survey data from CEOs of 201 Norwegian firms, collected in 2003. A hierarchical multiple regression analysis is used to test for mediation.\n\n**Variables & Parameters.**\n- `Ratio of Women Directors` (X): The independent variable, measured as the percentage of female directors on the board.\n- `Board Processes` (M): A set of mediating variables measured via survey. The two key mediators found to be significant are `Board Development Activities` (formal structures like evaluations and instructions) and `Board Conflict` (perceived disagreements and interpersonal friction).\n- `Board Strategic Control` (Y): The dependent variable, a survey measure of the board's effectiveness in monitoring long-term, qualitative issues like corporate strategy, CSR, and human resources.\n\n---\n\n### Data / Model Specification\n\nThe study's central finding is that the effect of the `Ratio of Women Directors` on `Board Strategic Control` is fully mediated by `Board Processes`. This is established via a three-step hierarchical regression procedure summarized in the stylized table below.\n\n**Table 1: Stylized Hierarchical Regression for Mediation on Strategic Control**\n| | (1) Strategic Control | (2) Strategic Control |\n|:---|:---:|:---:|\n| **Ratio of Women Directors (X)** | **0.45*** | 0.05 |\n| | (0.18) | (0.17) |\n| **Board Processes (M)** | | **0.70*** |\n| | | (0.15) |\n| Control Variables | Included | Included |\n| N | 201 | 201 |\n\n*Note: * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the provided background and **Table 1**, select all statements that represent valid interpretations or critiques of the study's mediation analysis.", "Options": {"A": "The results in Table 1 suggest that the positive effect of a higher ratio of women directors on strategic control is not direct, but rather operates indirectly through improvements in board processes (e.g., reduced conflict, more development activities).", "B": "The study's use of a large sample size (N=201) and control variables effectively mitigates the risk of common method bias, which is primarily a small-sample problem.", "C": "The decrease of the coefficient on 'Ratio of Women Directors' from 0.45 to 0.05 indicates that board processes are a confounding variable that invalidates the initial relationship.", "D": "A major limitation of the study is potential omitted variable bias; for instance, an unobserved factor like a progressive corporate culture could independently drive both the appointment of women directors and the adoption of better board processes."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret a full mediation result from a regression table and identify a key econometric threat to its causal validity. It uses an 'atomic decomposition' strategy, breaking down the original QA's insights into distinct, testable propositions. Option A tests the core interpretation of mediation. Option B tests the understanding of omitted variable bias. Distractor C presents a common misinterpretation of a mediator as a confounder. Distractor D targets a misconception about how to solve common method bias (it is a data collection issue, not a sample size issue).", "qid": "320", "question": "### Background\n\n**Research Question.** This study investigates the causal pathway through which board gender diversity affects board effectiveness, testing whether board processes—the internal dynamics and working structures of the board—act as a mediating mechanism.\n\n**Setting / Data-Generating Environment.** The analysis uses cross-sectional survey data from CEOs of 201 Norwegian firms, collected in 2003. A hierarchical multiple regression analysis is used to test for mediation.\n\n**Variables & Parameters.**\n- `Ratio of Women Directors` (X): The independent variable, measured as the percentage of female directors on the board.\n- `Board Processes` (M): A set of mediating variables measured via survey. The two key mediators found to be significant are `Board Development Activities` (formal structures like evaluations and instructions) and `Board Conflict` (perceived disagreements and interpersonal friction).\n- `Board Strategic Control` (Y): The dependent variable, a survey measure of the board's effectiveness in monitoring long-term, qualitative issues like corporate strategy, CSR, and human resources.\n\n---\n\n### Data / Model Specification\n\nThe study's central finding is that the effect of the `Ratio of Women Directors` on `Board Strategic Control` is fully mediated by `Board Processes`. This is established via a three-step hierarchical regression procedure summarized in the stylized table below.\n\n**Table 1: Stylized Hierarchical Regression for Mediation on Strategic Control**\n| | (1) Strategic Control | (2) Strategic Control |\n|:---|:---:|:---:|\n| **Ratio of Women Directors (X)** | **0.45*** | 0.05 |\n| | (0.18) | (0.17) |\n| **Board Processes (M)** | | **0.70*** |\n| | | (0.15) |\n| Control Variables | Included | Included |\n| N | 201 | 201 |\n\n*Note: * indicates significance at the 5% level.*\n\n---\n\n### Question\n\nBased on the provided background and **Table 1**, select all statements that represent valid interpretations or critiques of the study's mediation analysis.", "model_evaluation_result": {"model_raw_response": "Looking at this mediation analysis, I need to evaluate each statement based on the hierarchical regression results and research design.\n\nLet me analyze each option:\n\n**Option A**: The table shows that the coefficient for Ratio of Women Directors drops from 0.45 (significant) in Model 1 to 0.05 (non-significant) in Model 2 when Board Processes is added. Meanwhile, Board Processes has a strong significant coefficient (0.70). This pattern indicates full mediation - the effect of women directors on strategic control operates through board processes rather than directly. This is a valid interpretation.\n\n**Option B**: This statement is incorrect. Common method bias occurs when data for multiple variables comes from the same source (here, CEO surveys), potentially inflating correlations. This bias is not primarily about sample size - it can occur in large samples too. The study's N=201 doesn't mitigate this risk.\n\n**Option C**: This misinterprets the results. The coefficient reduction indicates mediation, not confounding. In mediation, the mediator (board processes) is part of the causal pathway from X to Y. A confounder would be a variable that affects both X and Y independently, which isn't what's being claimed here.\n\n**Option D**: This is a valid critique. The cross-sectional design cannot rule out that an unmeasured variable (like progressive corporate culture) might influence both the appointment of women directors and the implementation of better board processes. This would create spurious relationships that could be mistaken for mediation.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 429, "Question": "### Background\n\nThe paper proposes a heuristic test to assess the substitutability of different credit sources for financing specific expenditures, with implications for the effectiveness of selective credit controls. The test compares the t-statistic of an aggregate financial variable (`t_agg`) to that of the best single component (`t_comp`).\n\n### Data / Model Specification\n\nThe decision rule is as follows:\n- If `t_agg / t_comp > 1`, it suggests high substitutability, making selective controls less effective.\n- If `t_agg / t_comp < 1`, it suggests low substitutability (a \"one-to-one relation\"), making selective controls more effective.\n\n**Table 1: Tests of Substitutability for Household Expenditures (based on Table 8, Annual Data)**\n\n| Expenditure Category | Best Component Instrument | Test Ratio (`t_agg / t_comp`) | Author's Interpretation |\n| :--- | :--- | :---: | :--- |\n| Residential Construction | 1-to-4 family mortgages | 0.7304 | Low Substitutability |\n| Consumer Durables | Consumer credit | 1.007 | High Substitutability |\n\n---\n\nBased on the data and logic presented, which of the following conclusions or policy implications are valid?\n\nSelect all that apply.", "Options": {"A": "The results suggest that a selective credit control policy targeting mortgage origination would be a relatively direct and effective tool for influencing residential construction expenditures.", "B": "The test ratio for Residential Construction being less than 1 implies that adding non-mortgage credit flows to the regression model adds more statistical noise than explanatory power.", "C": "The test ratio for Consumer Durables being greater than 1 proves that consumer credit and mortgage credit are perfect substitutes in a formal econometric sense.", "D": "A policy restricting only consumer credit would likely be ineffective at curbing durable goods spending, as households could substitute towards other financing like mortgage credit."}, "Answer": ["A", "B", "D"], "pi_justification": "Assessment Target: This item assesses the ability to interpret the paper's heuristic test for credit substitutability and draw correct policy conclusions from it.\nDepth Strategy: Scenario Application. The user must apply the general findings from the table to specific, hypothetical policy scenarios.\nDistractor Logic:\n- A (Correct): This is the correct policy implication for the Consumer Durables result (ratio > 1), indicating high substitutability means controls can be circumvented.\n- B (Correct): This is the correct policy implication for the Residential Construction result (ratio < 1), indicating a dedicated financing channel that is vulnerable to targeted controls.\n- C (Correct): This correctly interprets the statistical logic behind the heuristic. A ratio < 1 means the aggregate variable's t-statistic is lower, which happens when the added variables (other credit flows) are noisy and irrelevant for explaining the specific expenditure.\n- D (Conceptual Distractor - Overstatement): This distractor uses the term 'proves' and 'perfect substitutes in a formal econometric sense'. The paper's test is explicitly a heuristic, not a formal F-test for coefficient equality. This option is incorrect because it overstates the power and formality of the method used.", "qid": "429", "question": "### Background\n\nThe paper proposes a heuristic test to assess the substitutability of different credit sources for financing specific expenditures, with implications for the effectiveness of selective credit controls. The test compares the t-statistic of an aggregate financial variable (`t_agg`) to that of the best single component (`t_comp`).\n\n### Data / Model Specification\n\nThe decision rule is as follows:\n- If `t_agg / t_comp > 1`, it suggests high substitutability, making selective controls less effective.\n- If `t_agg / t_comp < 1`, it suggests low substitutability (a \"one-to-one relation\"), making selective controls more effective.\n\n**Table 1: Tests of Substitutability for Household Expenditures (based on Table 8, Annual Data)**\n\n| Expenditure Category | Best Component Instrument | Test Ratio (`t_agg / t_comp`) | Author's Interpretation |\n| :--- | :--- | :---: | :--- |\n| Residential Construction | 1-to-4 family mortgages | 0.7304 | Low Substitutability |\n| Consumer Durables | Consumer credit | 1.007 | High Substitutability |\n\n---\n\nBased on the data and logic presented, which of the following conclusions or policy implications are valid?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each option based on the test methodology and results presented.\n\nThe key decision rule is:\n- Test ratio > 1 → high substitutability → selective controls less effective\n- Test ratio < 1 → low substitutability → selective controls more effective\n\nFor Residential Construction: ratio = 0.7304 (< 1) → low substitutability\nFor Consumer Durables: ratio = 1.007 (> 1) → high substitutability\n\nEvaluating each option:\n\n**Option A**: The ratio of 0.7304 < 1 indicates low substitutability between mortgage credit and other credit sources for residential construction. This means households cannot easily substitute other credit for mortgages when financing home construction. Therefore, targeting mortgage origination would be effective. ✓\n\n**Option B**: A ratio < 1 means the aggregate variable performs worse than the single best component. This indicates that adding other credit components to mortgages in the model reduces explanatory power, which is consistent with adding statistical noise rather than useful information. ✓\n\n**Option C**: The ratio of 1.007 indicates high substitutability, but this doesn't prove \"perfect substitutes in a formal econometric sense.\" Perfect substitution would require much stronger evidence than a test ratio slightly above 1. The word \"proves\" is too strong, and \"perfect\" substitution is an extreme claim not supported by the data. ✗\n\n**Option D**: With a ratio of 1.007 > 1, there is high substitutability between consumer credit and other credit sources for durable goods. This means households can substitute other forms of credit (like mortgages) if consumer credit is restricted, making a selective policy targeting only consumer credit ineffective. ✓\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 118, "Question": "### Background\n\nTo model the pass-through between different interest rates, one must have a theoretical framework for how rates are related and how their dynamics should be modeled based on their statistical properties. The analysis considers a conceptual framework for decomposing interest rate spreads and modeling the dynamic adjustment between two rates, `r_t` and `i_t`, based on whether they are stationary or cointegrated non-stationary processes.\n\n### Data / Model Specification\n\nThe market structure of interest rates posits that the spread between a riskier, longer-term rate (`r_t`) and a less-risky, shorter-term rate (`i_t`) can be decomposed as:\n  \nr_{t}=i_{t}+\\gamma+\\phi \\quad \\text{(Eq. 1)}\n \nwhere `\\gamma` is the term premium and `\\phi` is the risk premium.\n\nIf `r_t` and `i_t` are non-stationary (I(1)) but cointegrated, their dynamic relationship can be modeled with an Error Correction Model (ECM) of the form:\n  \n\\Delta{r_{t}}=c+\\beta_{0}\\Delta{i_{t}}+\\gamma({r_{t-1}}-{i_{t-1}}) \\quad \\text{(Eq. 2)}\n \n\n### Question\n\nBased on the theoretical framework presented in Eq. (1) and Eq. (2), select all of the following statements that are correct.", "Options": {"A": "The coefficient `\\beta_0` in Eq. (2) measures the speed at which `r_t` returns to its long-run equilibrium with `i_t` after a shock.", "B": "For the dynamic system in Eq. (2) to be stable, the error correction parameter `\\gamma` must be a positive value between 0 and 1.", "C": "For the two non-stationary rates `r_t` and `i_t` to be cointegrated, the combined premium `\\gamma + \\phi` must be a stationary process.", "D": "The cointegrating vector that makes the spread `r_t - i_t` stationary is `[1, -1]`."}, "Answer": ["C", "D"], "pi_justification": "This item assesses foundational knowledge of cointegration and Error Correction Models. It uses a 'select-all-valid' format to efficiently test multiple concepts. Distractor B confuses the short-run pass-through (`\\beta_0`) with the speed of adjustment (`\\gamma`), a classic misconception. Distractor D introduces a sign error, as the adjustment parameter `\\gamma` must be negative for stability, another common point of confusion.", "qid": "118", "question": "### Background\n\nTo model the pass-through between different interest rates, one must have a theoretical framework for how rates are related and how their dynamics should be modeled based on their statistical properties. The analysis considers a conceptual framework for decomposing interest rate spreads and modeling the dynamic adjustment between two rates, `r_t` and `i_t`, based on whether they are stationary or cointegrated non-stationary processes.\n\n### Data / Model Specification\n\nThe market structure of interest rates posits that the spread between a riskier, longer-term rate (`r_t`) and a less-risky, shorter-term rate (`i_t`) can be decomposed as:\n  \nr_{t}=i_{t}+\\gamma+\\phi \\quad \\text{(Eq. 1)}\n \nwhere `\\gamma` is the term premium and `\\phi` is the risk premium.\n\nIf `r_t` and `i_t` are non-stationary (I(1)) but cointegrated, their dynamic relationship can be modeled with an Error Correction Model (ECM) of the form:\n  \n\\Delta{r_{t}}=c+\\beta_{0}\\Delta{i_{t}}+\\gamma({r_{t-1}}-{i_{t-1}}) \\quad \\text{(Eq. 2)}\n \n\n### Question\n\nBased on the theoretical framework presented in Eq. (1) and Eq. (2), select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each statement based on the theoretical framework provided.\n\nLet me examine each option:\n\n**Option A**: \"The coefficient `β₀` in Eq. (2) measures the speed at which `r_t` returns to its long-run equilibrium with `i_t` after a shock.\"\n\nThis is incorrect. In the ECM equation (Eq. 2), `β₀` is the coefficient on `Δi_t` and represents the short-run or contemporaneous effect of changes in `i_t` on changes in `r_t`. The speed of adjustment to equilibrium is measured by `γ`, not `β₀`.\n\n**Option B**: \"For the dynamic system in Eq. (2) to be stable, the error correction parameter `γ` must be a positive value between 0 and 1.\"\n\nThis is incorrect. For stability in an ECM, the error correction parameter must be negative (typically between -1 and 0). When `r_t` is above its equilibrium with `i_t`, we need `γ` to be negative so that `Δr_t` is negative, bringing `r_t` back down toward equilibrium.\n\n**Option C**: \"For the two non-stationary rates `r_t` and `i_t` to be cointegrated, the combined premium `γ + φ` must be a stationary process.\"\n\nThis is correct. From Eq. (1), we have `r_t - i_t = γ + φ`. For `r_t` and `i_t` to be cointegrated, their linear combination must be stationary. Therefore, `γ + φ` must be stationary (I(0)).\n\n**Option D**: \"The cointegrating vector that makes the spread `r_t - i_t` stationary is `[1, -1]`.\"\n\nThis is correct. The cointegrating vector `[1, -1]` applied to `[r_t, i_t]` gives us `1·r_t + (-1)·i_t = r_t - i_t`, which equals `γ + φ` according to Eq. (1). As established in option C, this must be", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 380, "Question": "### Background\n\n**Research Question:** How are the parameters of a MAR-GARCH model estimated, and how can the model's specification be validated, especially when standard tests for the number of regimes are invalid?\n\n**Setting / Data-Generating Environment:** A researcher has fitted a MAR-GARCH model via Maximum Likelihood (ML) and needs to perform diagnostic checks to assess its adequacy. This involves understanding both the conditions for standard inference and the methods for testing for remaining misspecification in the conditional mean and variance.\n\n### Data / Model Specification\n\nDiagnostic checks are based on the standardized residual, `\\hat{e}_t`, which under the null of correct specification is a martingale difference sequence with unit variance.\n  \ne_{t}=\\frac{y_{t}-E_{t-1}(y_{t})}{\\sqrt{\\operatorname{Var}_{t-1}(y_{t})}} \\quad \\text{(Eq. (1))}\n \nWithin the GARCH specification, `u_{i,t}` refers to the regime-specific unscaled residual, `y_t - E(y_t | regime=i)`.\n\n### Question\n\nA researcher has estimated a MAR-GARCH model and is performing diagnostic checks. Which of the following statements about the diagnostic process are correct? Select all that apply.", "Options": {"A": "To test for an asymmetric volatility response (leverage effect), one could add the term `γ * u_{i,t-1} * I(u_{i,t-1}<0)` to the conditional variance equation and test the null `H_0: γ = 0`.", "B": "Significant serial correlation in the standardized residuals, `\\hat{e}_t`, would suggest that the autoregressive order (`p`) of the conditional mean is too low.", "C": "Even if the model is correctly specified, the standardized residuals `e_t` are not expected to be normally distributed.", "D": "Significant serial correlation in the squared standardized residuals, `\\hat{e}_t^2`, would suggest that the GARCH component of the model is misspecified."}, "Answer": ["B", "C", "D"], "pi_justification": "This Multiple Choice item assesses the practical application of diagnostic testing for MAR-GARCH models. It combines key insights from the paper regarding residual analysis and test formulation. Option A tests the standard interpretation of GARCH diagnostics. Option B is a high-fidelity distractor with a subtle error in the test specification (omitting the square on the residual), testing close reading. Option C tests a key property of the model's residuals. Option D tests the standard interpretation of conditional mean diagnostics. This format efficiently covers multiple distinct but related concepts.", "qid": "380", "question": "### Background\n\n**Research Question:** How are the parameters of a MAR-GARCH model estimated, and how can the model's specification be validated, especially when standard tests for the number of regimes are invalid?\n\n**Setting / Data-Generating Environment:** A researcher has fitted a MAR-GARCH model via Maximum Likelihood (ML) and needs to perform diagnostic checks to assess its adequacy. This involves understanding both the conditions for standard inference and the methods for testing for remaining misspecification in the conditional mean and variance.\n\n### Data / Model Specification\n\nDiagnostic checks are based on the standardized residual, `\\hat{e}_t`, which under the null of correct specification is a martingale difference sequence with unit variance.\n  \ne_{t}=\\frac{y_{t}-E_{t-1}(y_{t})}{\\sqrt{\\operatorname{Var}_{t-1}(y_{t})}} \\quad \\text{(Eq. (1))}\n \nWithin the GARCH specification, `u_{i,t}` refers to the regime-specific unscaled residual, `y_t - E(y_t | regime=i)`.\n\n### Question\n\nA researcher has estimated a MAR-GARCH model and is performing diagnostic checks. Which of the following statements about the diagnostic process are correct? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) To test for an asymmetric volatility response (leverage effect), one could add the term `γ * u_{i,t-1} * I(u_{i,t-1}<0)` to the conditional variance equation and test the null `H_0: γ = 0`.\n\nThis is correct. The leverage effect refers to the empirical observation that negative shocks tend to increase volatility more than positive shocks of the same magnitude. Adding a term like `γ * u_{i,t-1} * I(u_{i,t-1}<0)` to the variance equation captures this asymmetry, where I(·) is an indicator function. Testing γ = 0 tests whether this asymmetric effect is present.\n\nB) Significant serial correlation in the standardized residuals, `\\hat{e}_t`, would suggest that the autoregressive order (`p`) of the conditional mean is too low.\n\nThis is correct. If the standardized residuals show serial correlation, it means the conditional mean equation hasn't fully captured the dynamics in the data. Under correct specification, standardized residuals should be a martingale difference sequence with no serial correlation. Serial correlation suggests more lags are needed in the conditional mean specification.\n\nC) Even if the model is correctly specified, the standardized residuals `e_t` are not expected to be normally distributed.\n\nThis is correct. GARCH models do not require the standardized residuals to be normally distributed. The model can be correctly specified with non-normal innovations. What matters is that they form a martingale difference sequence with unit variance.\n\nD) Significant serial correlation in the squared standardized residuals, `\\hat{e}_t^2`, would suggest that the GARCH component of the model is misspecified.\n\nThis is correct. The squared standardized residuals test for remaining conditional heteroskedasticity. If there's serial correlation in `\\hat{e}_t^2`, it indicates that the variance equation hasn't fully captured the volatility dynamics, suggesting the GARCH specification needs adjustment (perhaps higher order GARCH terms or different specification).\n\nA,B,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 141, "Question": "### Background\n\n**Research Question.** How do specific assumptions about the claim size distribution, such as being exponential, affect the analytical form of the Gerber-Shiu function, and what are the limitations of such assumptions?\n\n**Setting / Data-Generating Environment.** We consider a semi-Markovian risk model where claim sizes `Y_j` in each state `j` can be modeled by different distributions. The choice of distribution has significant analytical and practical consequences.\n\n**Variables & Parameters.**\n- `~b(s)`: Laplace transform of the claim size density.\n- `ε_k`: Exponents in the solution, determined as roots of a characteristic equation.\n\n---\n\n### Data / Model Specification\n\nThe analytical solution for the Gerber-Shiu function often involves finding roots of the generalized Lundberg equation:\n  \n\\det(\\mathbf{I} - \\tilde{\\mathbf{k}}(\\delta - cs) \\mathbf{P} \\tilde{\\mathbf{b}}(s)) = 0\n \nThe validity and form of this analysis depend critically on the properties of the claim size distributions and their Laplace transforms `~b(s)`.\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the role and limitations of claim size distribution assumptions in this model? (Select all that apply)", "Options": {"A": "Assuming exponential claim sizes is analytically convenient because their rational Laplace transforms often reduce complex integral equations to algebraic problems.", "B": "The exponential distribution is empirically well-suited for modeling insurance portfolios with high exposure to catastrophic events.", "C": "Phase-type distributions are useful because they are the only class of distributions capable of representing heavy-tailed phenomena.", "D": "The method of finding exponential decay rates `ε_k` is valid only if the claim size distributions' Laplace transforms exist for positive real arguments."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item tests the understanding of the trade-offs and mathematical requirements of using specific claim size distributions. It assesses foundational knowledge about the model's assumptions. Distractors include a conceptual opposite (B: exponential is ill-suited for catastrophes) and a common misconception about phase-type distributions (D: they are dense but not the only way to model heavy tails, and are not inherently heavy-tailed).", "qid": "141", "question": "### Background\n\n**Research Question.** How do specific assumptions about the claim size distribution, such as being exponential, affect the analytical form of the Gerber-Shiu function, and what are the limitations of such assumptions?\n\n**Setting / Data-Generating Environment.** We consider a semi-Markovian risk model where claim sizes `Y_j` in each state `j` can be modeled by different distributions. The choice of distribution has significant analytical and practical consequences.\n\n**Variables & Parameters.**\n- `~b(s)`: Laplace transform of the claim size density.\n- `ε_k`: Exponents in the solution, determined as roots of a characteristic equation.\n\n---\n\n### Data / Model Specification\n\nThe analytical solution for the Gerber-Shiu function often involves finding roots of the generalized Lundberg equation:\n  \n\\det(\\mathbf{I} - \\tilde{\\mathbf{k}}(\\delta - cs) \\mathbf{P} \\tilde{\\mathbf{b}}(s)) = 0\n \nThe validity and form of this analysis depend critically on the properties of the claim size distributions and their Laplace transforms `~b(s)`.\n\n---\n\n### Question\n\nWhich of the following statements accurately describe the role and limitations of claim size distribution assumptions in this model? (Select all that apply)", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) Exponential distributions have Laplace transforms of the form 1/(1+s/λ), which are rational functions. This property does make the generalized Lundberg equation algebraic rather than transcendental, simplifying the analysis significantly. This statement is correct.\n\nB) Exponential distributions have light tails and decay exponentially, making them poor models for catastrophic events which typically exhibit heavy-tailed behavior. This statement is incorrect.\n\nC) Phase-type distributions are dense in the class of distributions on positive reals and can approximate many distributions well, but they cannot represent truly heavy-tailed distributions (those without finite moments). They have exponentially decaying tails. This statement is incorrect.\n\nD) For the method of finding exponential decay rates ε_k to work, we need the Laplace transforms to exist in a neighborhood of the origin extending into the positive real axis. This is necessary for the analytical continuation and root-finding procedures. This statement is correct.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 449, "Question": "### Background\n\n**Research Question.** This case examines how the equilibrium futures return can be modeled as an outcome of the interaction between two representative agents: hedgers, who are sensitive to portfolio risk, and speculators, who are sensitive to expected returns and risk.\n\n**Setting.** A futures market is populated by hedgers and speculators. Hedger demand for futures depends on the variance of an optimally hedged portfolio. Speculator demand depends on the expected return and variance of futures contracts. The market clears daily.\n\n### Data / Model Specification\n\nThe testable relationship for the realized futures return, `r_{f,t}`, derived from the market clearing condition is:\n  \nr_{f,t} = e_0 - \\frac{b^H}{d^S}\\sigma_{r_{H},t}^{2} + \\frac{e^S}{d^S}\\sigma_{r_{f},t}^{2} + u_{r_{f},t} \n \nwhere `\\sigma_{r_H,t}^2` is the variance of the optimally hedged portfolio and `\\sigma_{r_f,t}^2` is the variance of futures returns.\n\nBehavioral assumptions on parameters:\n*   `b^H`: Hedger sensitivity. `b^H > 0` for consumers; `b^H < 0` for producers.\n*   `d^S`: Speculator sensitivity to expected return (assumed positive).\n*   `e^S`: Speculator sensitivity to futures variance. `e^S > 0` for risk-averse; `e^S < 0` for risk-loving.\n\n**Table 1. Empirical Results for Selected Futures Markets**\n\n| Commodity | Coefficient `-(b^H/d^S)` | Coefficient `(e^S/d^S)` | Relative Importance of Speculative Drivers |\n|:---|:---:|:---:|:---:|\n| Crude Oil | 4.217 | 2.401 | 1.30 |\n| Soybeans | -19.695 | -5.916 | 1.21 |\n\n*Source: Adapted from Tables 2, 3, and 4 in the source paper.*\n\n### Question\n\nBased on the model and empirical results in Table 1, select all of the following statements that are correct characterizations of the markets.", "Options": {"A": "The 'Relative Importance' metric for Crude Oil (1.30) indicates that futures returns in this market are more sensitive to hedging risk (`\\sigma_{r_H,t}^2`) than to speculative risk (`\\sigma_{r_f,t}^2`).", "B": "The negative sign on the `-(b^H/d^S)` coefficient for Soybeans indicates that the market is dominated by producers.", "C": "In the Soybeans market, the results imply that hedgers are predominantly consumers and speculators are, on average, risk-loving.", "D": "In the Crude Oil market, the results imply that hedgers are predominantly producers and speculators are, on average, risk-averse."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses the 'atomic decomposition' strategy to test the core interpretation of the paper's behavioral model. It is a 'Reverse-Reasoning' item, requiring the user to infer underlying agent characteristics from estimated coefficients. Options A and B test the correct interpretation for two different markets, providing two independent correct answers. Distractors are based on high-fidelity errors: D makes a sign error in interpreting the `-(b^H/d^S)` coefficient (a negative coefficient implies `-b^H < 0`, so `b^H > 0`, which means consumers dominate). C misinterprets the 'Relative Importance' metric, which shows the opposite (Conceptual Opposite).", "qid": "449", "question": "### Background\n\n**Research Question.** This case examines how the equilibrium futures return can be modeled as an outcome of the interaction between two representative agents: hedgers, who are sensitive to portfolio risk, and speculators, who are sensitive to expected returns and risk.\n\n**Setting.** A futures market is populated by hedgers and speculators. Hedger demand for futures depends on the variance of an optimally hedged portfolio. Speculator demand depends on the expected return and variance of futures contracts. The market clears daily.\n\n### Data / Model Specification\n\nThe testable relationship for the realized futures return, `r_{f,t}`, derived from the market clearing condition is:\n  \nr_{f,t} = e_0 - \\frac{b^H}{d^S}\\sigma_{r_{H},t}^{2} + \\frac{e^S}{d^S}\\sigma_{r_{f},t}^{2} + u_{r_{f},t} \n \nwhere `\\sigma_{r_H,t}^2` is the variance of the optimally hedged portfolio and `\\sigma_{r_f,t}^2` is the variance of futures returns.\n\nBehavioral assumptions on parameters:\n*   `b^H`: Hedger sensitivity. `b^H > 0` for consumers; `b^H < 0` for producers.\n*   `d^S`: Speculator sensitivity to expected return (assumed positive).\n*   `e^S`: Speculator sensitivity to futures variance. `e^S > 0` for risk-averse; `e^S < 0` for risk-loving.\n\n**Table 1. Empirical Results for Selected Futures Markets**\n\n| Commodity | Coefficient `-(b^H/d^S)` | Coefficient `(e^S/d^S)` | Relative Importance of Speculative Drivers |\n|:---|:---:|:---:|:---:|\n| Crude Oil | 4.217 | 2.401 | 1.30 |\n| Soybeans | -19.695 | -5.916 | 1.21 |\n\n*Source: Adapted from Tables 2, 3, and 4 in the source paper.*\n\n### Question\n\nBased on the model and empirical results in Table 1, select all of the following statements that are correct characterizations of the markets.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze each statement based on the model equation and empirical results.\n\nThe model equation is:\nr_{f,t} = e_0 - (b^H/d^S)σ_{r_H,t}^2 + (e^S/d^S)σ_{r_f,t}^2 + u_{r_{f},t}\n\nKey behavioral assumptions:\n- b^H > 0 for consumers; b^H < 0 for producers\n- d^S > 0 (always positive)\n- e^S > 0 for risk-averse; e^S < 0 for risk-loving speculators\n\nLet me analyze each option:\n\n**Option A**: The Relative Importance metric for Crude Oil is 1.30. This appears to be the ratio of the absolute values of the coefficients: |4.217|/|2.401| ≈ 1.76 or |2.401|/|4.217| ≈ 0.57. Neither equals 1.30 exactly, but if we consider measurement error or rounding, it's likely |coefficient on hedging risk|/|coefficient on speculative risk|. Since 1.30 > 1, this indicates futures returns are indeed more sensitive to hedging risk than speculative risk. **TRUE**\n\n**Option B**: For Soybeans, -(b^H/d^S) = -19.695\nSince d^S > 0, we have -b^H/d^S = -19.695\nTherefore, b^H/d^S = 19.695 > 0\nThis means b^H > 0, indicating consumers dominate (not producers). **FALSE**\n\n**Option C**: For Soybeans:\n- From Option B analysis: b^H > 0, so hedgers are predominantly consumers ✓\n- (e^S/d^S) = -5.916 < 0\n- Since d^S > 0, this means e^S < 0, indicating risk-loving speculators ✓\n**TRUE**\n\n**Option D**: For Crude Oil:\n- -(b^H/d^S) =", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 63, "Question": "### Background\n\n**Research Question.** How sensitive are an insurer's optimal dividend and reinsurance policies to the magnitude of the penalty associated with financial ruin?\n\n**Setting.** A numerical example illustrating the theoretical predictions that a higher ruin penalty `P` makes the insurer more conservative. The table shows the optimal policy thresholds (`x_1`, `x_b`) and the initial risk retention `q*(0)` for a range of ruin penalty values `P`.\n\n**Variables and Parameters.**\n- `P`: The penalty for ruin. `P < 0` indicates a salvage value (units: currency).\n- `x_1`: The surplus threshold separating partial and full reinsurance (units: currency).\n- `x_b`: The optimal dividend barrier (units: currency).\n- `q*(0)`: The optimal retention proportion at infinitesimally small surplus (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe following table presents numerical solutions for the optimal policy under the parameterization: `a=1`, `b=0.2`, `θ=0.1`, `β=0.05`, and `η=0.15`.\n\n**Table 1: The effect of `P` on `x_1`, `x_b`, and `q*(0)`**\n\n| P    | x_1   | x_b   | q*(0) |\n|:-----|:------|:------|:------|\n| -1.0 | 0.080 | 0.651 | 0.863 |\n| -0.5 | 0.192 | 0.763 | 0.724 |\n| 0.0  | 0.263 | 0.834 | 0.667 |\n| 0.5  | 0.313 | 0.885 | 0.637 |\n| 1.0  | 0.352 | 0.924 | 0.620 |\n| 5.0  | 0.514 | 1.085 | 0.582 |\n| 10.0 | 0.606 | 1.177 | 0.574 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following statements accurately describe how the insurer's strategy changes as the ruin penalty `P` increases from a salvage value (`P=-1.0`) to a high penalty (`P=10.0`)? Select all that apply.", "Options": {"A": "The initial retention proportion `q*(0)` decreases by more than 30%, indicating a more cautious stance at low surplus levels.", "B": "The region of partial reinsurance, defined by the threshold `x_1`, shrinks as the penalty `P` increases.", "C": "The size of the surplus region where the insurer operates with full risk retention (`x_b - x_1`) remains constant across all values of `P`.", "D": "The dividend barrier `x_b` decreases as the penalty `P` rises, as the firm tries to pay out dividends faster to avoid the penalty."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses quantitative interpretation and policy synthesis. It uses the 'Atomic Decomposition' strategy by breaking down the original QA's findings into distinct, verifiable statements. Options A and B are correct calculations/observations from the table. Distractor C (Conceptual Opposite) posits an incorrect relationship for `x_b`. Distractor D (Conceptual Opposite) incorrectly describes the trend in `x_1`, which actually expands dramatically.", "qid": "63", "question": "### Background\n\n**Research Question.** How sensitive are an insurer's optimal dividend and reinsurance policies to the magnitude of the penalty associated with financial ruin?\n\n**Setting.** A numerical example illustrating the theoretical predictions that a higher ruin penalty `P` makes the insurer more conservative. The table shows the optimal policy thresholds (`x_1`, `x_b`) and the initial risk retention `q*(0)` for a range of ruin penalty values `P`.\n\n**Variables and Parameters.**\n- `P`: The penalty for ruin. `P < 0` indicates a salvage value (units: currency).\n- `x_1`: The surplus threshold separating partial and full reinsurance (units: currency).\n- `x_b`: The optimal dividend barrier (units: currency).\n- `q*(0)`: The optimal retention proportion at infinitesimally small surplus (dimensionless).\n\n---\n\n### Data / Model Specification\n\nThe following table presents numerical solutions for the optimal policy under the parameterization: `a=1`, `b=0.2`, `θ=0.1`, `β=0.05`, and `η=0.15`.\n\n**Table 1: The effect of `P` on `x_1`, `x_b`, and `q*(0)`**\n\n| P    | x_1   | x_b   | q*(0) |\n|:-----|:------|:------|:------|\n| -1.0 | 0.080 | 0.651 | 0.863 |\n| -0.5 | 0.192 | 0.763 | 0.724 |\n| 0.0  | 0.263 | 0.834 | 0.667 |\n| 0.5  | 0.313 | 0.885 | 0.637 |\n| 1.0  | 0.352 | 0.924 | 0.620 |\n| 5.0  | 0.514 | 1.085 | 0.582 |\n| 10.0 | 0.606 | 1.177 | 0.574 |\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following statements accurately describe how the insurer's strategy changes as the ruin penalty `P` increases from a salvage value (`P=-1.0`) to a high penalty (`P=10.0`)? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to analyze how each variable changes as P increases from -1.0 to 10.0.\n\nLet me check each option:\n\n**Option A**: The initial retention proportion q*(0) decreases by more than 30%\n- At P = -1.0: q*(0) = 0.863\n- At P = 10.0: q*(0) = 0.574\n- Percentage decrease = (0.863 - 0.574)/0.863 × 100% = 33.5%\nThis is indeed more than 30%, so Option A is correct.\n\n**Option B**: The region of partial reinsurance (threshold x₁) shrinks as P increases\n- The threshold x₁ actually increases from 0.080 to 0.606 as P goes from -1.0 to 10.0\n- A larger x₁ means the region of partial reinsurance (from 0 to x₁) expands, not shrinks\nOption B is incorrect.\n\n**Option C**: The size of the surplus region with full risk retention (xb - x₁) remains constant\nLet me calculate xb - x₁ for each P value:\n- P = -1.0: 0.651 - 0.080 = 0.571\n- P = -0.5: 0.763 - 0.192 = 0.571\n- P = 0.0: 0.834 - 0.263 = 0.571\n- P = 0.5: 0.885 - 0.313 = 0.572\n- P = 1.0: 0.924 - 0.352 = 0.572\n- P = 5.0: 1.085 - 0.514 = 0.571\n- P = 10.0: 1.177 - 0.606 = 0.571\n\nThe difference remains essentially constant at 0.571-0.572, so Option C is correct.\n\n**Option D**: The dividend barrier xb decreases as P rises\n- Looking at the data, xb increases from 0.651 to ", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 456, "Question": "### Background\nIn the paper's model, a target firm and an “insider” acquirer form a coalition to maximize their joint surplus in a future takeover contest against N “outsiders.” They achieve this by having the target sell an equity stake of size `α*` to the insider. The optimal stake size, `α*`, is determined by the trade-off between extracting surplus from outsiders and ensuring outsiders are willing to pay an investigation cost, *c*, to participate. A larger stake `α` makes the insider a more aggressive bidder, which helps the coalition extract surplus but reduces the outsiders' expected payoff, potentially violating their participation constraint.\n\n### Data / Model Specification\nThe model yields several key predictions about the optimal stake size:\n1.  The optimal stake size, `α*`, is increasing in the probability of a takeover, *s*, and decreasing in the outsiders’ cost of investigation, *c*.\n2.  The optimal stake size, `α*`, decreases with any positive stochastic shift in the insider’s synergy value distribution, F_I(·).\n3.  Assuming disagreement between the target and insider would result in a standard second-price auction with no stake, the equity stake `α*` will always sell for a discount to the target’s post-trade market price.\n\n### Question\nBased on the model's logic and propositions, select ALL statements that accurately describe the determinants of the optimal stake size, `α*`.", "Options": {"A": "A higher probability of a takeover (*s*) allows the coalition to take a larger optimal stake `α*`.", "B": "If the insider is ex-ante stronger (i.e., has a stochastically dominant synergy distribution), the coalition must take a smaller optimal stake `α*`.", "C": "In a hypothetical scenario where outsiders' investigation costs (*c*) are zero, the optimal stake size `α*` would also be zero.", "D": "A higher investigation cost (*c*) for outsiders requires the coalition to take a larger optimal stake `α*` to compensate."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the understanding of the model's key comparative statics and a critical boundary condition. It uses an Atomic Decomposition strategy to test multiple distinct predictions from the paper. Distractors are designed based on a sign error (B), a common logical slip, and a misunderstanding of a boundary condition (D), which targets a frequent error path where a variable going to zero implies the outcome must also be zero.", "qid": "456", "question": "### Background\nIn the paper's model, a target firm and an “insider” acquirer form a coalition to maximize their joint surplus in a future takeover contest against N “outsiders.” They achieve this by having the target sell an equity stake of size `α*` to the insider. The optimal stake size, `α*`, is determined by the trade-off between extracting surplus from outsiders and ensuring outsiders are willing to pay an investigation cost, *c*, to participate. A larger stake `α` makes the insider a more aggressive bidder, which helps the coalition extract surplus but reduces the outsiders' expected payoff, potentially violating their participation constraint.\n\n### Data / Model Specification\nThe model yields several key predictions about the optimal stake size:\n1.  The optimal stake size, `α*`, is increasing in the probability of a takeover, *s*, and decreasing in the outsiders’ cost of investigation, *c*.\n2.  The optimal stake size, `α*`, decreases with any positive stochastic shift in the insider’s synergy value distribution, F_I(·).\n3.  Assuming disagreement between the target and insider would result in a standard second-price auction with no stake, the equity stake `α*` will always sell for a discount to the target’s post-trade market price.\n\n### Question\nBased on the model's logic and propositions, select ALL statements that accurately describe the determinants of the optimal stake size, `α*`.", "model_evaluation_result": {"model_raw_response": "Looking at each statement against the model's predictions:\n\nA) \"A higher probability of a takeover (*s*) allows the coalition to take a larger optimal stake `α*`.\"\n- This directly matches prediction 1: \"The optimal stake size, `α*`, is increasing in the probability of a takeover, *s*\"\n- ✓ CORRECT\n\nB) \"If the insider is ex-ante stronger (i.e., has a stochastically dominant synergy distribution), the coalition must take a smaller optimal stake `α*`.\"\n- This matches prediction 2: \"The optimal stake size, `α*`, decreases with any positive stochastic shift in the insider's synergy value distribution\"\n- A positive stochastic shift means the insider becomes stronger\n- ✓ CORRECT\n\nC) \"In a hypothetical scenario where outsiders' investigation costs (*c*) are zero, the optimal stake size `α*` would also be zero.\"\n- The model states there's a trade-off: larger stakes help extract surplus but may violate outsiders' participation constraint\n- If investigation costs are zero, outsiders will always participate regardless of stake size\n- This would eliminate the participation constraint, allowing the coalition to maximize stake size, not minimize it\n- ✗ INCORRECT\n\nD) \"A higher investigation cost (*c*) for outsiders requires the coalition to take a larger optimal stake `α*` to compensate.\"\n- Prediction 1 states: \"The optimal stake size, `α*`, is... decreasing in the outsiders' cost of investigation, *c*\"\n- This statement claims the opposite relationship\n- ✗ INCORRECT\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 115, "Question": "### Background\n\nA credit support agency with a limited budget `B` must decide how to allocate funds between direct co-funding (`Q`) and credit guarantees (`φ`) to support productive entrepreneurs. The agency faces a population that includes both productive entrepreneurs and a mass `ρ` of unproductive 'copy-cat' entrepreneurs with riskless projects.\n\n### Data / Model Specification\n\nThe model establishes a 'pecking order' for government support based on the agency's budget `B` relative to a threshold `underline(B)`:\n\n1.  If `B < underline(B)`, the optimal policy is **no co-funding (`Q=0`)** and maximum possible guarantees.\n2.  If `B ≥ underline(B)`, the optimal policy involves **both co-funding (`Q > 0`) and guarantees (`φ > 0`)**.\n\nThe maximum incentive-compatible guarantee is `hat(φ)(Q) = 1 - A/(I-Q)`, which is decreasing in `Q`.\n\n### Question\n\nBased on the model's 'pecking order' theory, which of the following statements are valid conclusions about the optimal policy mix?\n\nSelect all that apply.", "Options": {"A": "In the high-budget regime (`B ≥ underline(B)`), as more co-funding `Q` is provided, the optimal guarantee level `φ` is reduced because the entrepreneur's increased equity makes a powerful guarantee less necessary to secure financing.", "B": "Co-funding is always the superior policy tool because it directly increases an entrepreneur's equity, which avoids the incentive-distorting effects of guarantees.", "C": "When the budget is large (`B ≥ underline(B)`), co-funding is introduced because the maximum useful level of guarantees has been reached, and any additional funds are best used to increase entrepreneurs' equity.", "D": "For a tightly constrained budget (`B < underline(B)`), guarantees are preferred because they are targeted; copy-cats with riskless projects cannot benefit from a guarantee that only pays out on failure, preventing resource leakage."}, "Answer": ["A", "C", "D"], "pi_justification": "Assessment Target: This item assesses comprehension of the paper's central policy prescription: the 'pecking order' of support. It requires understanding why guarantees are preferred with low budgets and how the policy mix shifts with high budgets. Depth Strategy: Scenario Application. The question asks the student to apply the pecking order theory to different budget scenarios (low vs. high) and identify the correct rationales. Distractor Logic: Option D is a conceptual opposite and an oversimplification. It ignores the key problem of resource leakage to copy-cats, which is the entire basis for the pecking order, making it an appealing but incorrect choice for students who miss this nuance.", "qid": "115", "question": "### Background\n\nA credit support agency with a limited budget `B` must decide how to allocate funds between direct co-funding (`Q`) and credit guarantees (`φ`) to support productive entrepreneurs. The agency faces a population that includes both productive entrepreneurs and a mass `ρ` of unproductive 'copy-cat' entrepreneurs with riskless projects.\n\n### Data / Model Specification\n\nThe model establishes a 'pecking order' for government support based on the agency's budget `B` relative to a threshold `underline(B)`:\n\n1.  If `B < underline(B)`, the optimal policy is **no co-funding (`Q=0`)** and maximum possible guarantees.\n2.  If `B ≥ underline(B)`, the optimal policy involves **both co-funding (`Q > 0`) and guarantees (`φ > 0`)**.\n\nThe maximum incentive-compatible guarantee is `hat(φ)(Q) = 1 - A/(I-Q)`, which is decreasing in `Q`.\n\n### Question\n\nBased on the model's 'pecking order' theory, which of the following statements are valid conclusions about the optimal policy mix?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this credit support agency model, I need to analyze each statement based on the given pecking order theory.\n\nThe key facts from the model:\n- Budget constraint B determines policy mix\n- If B < underline(B): only guarantees (Q=0)\n- If B ≥ underline(B): both co-funding (Q>0) and guarantees (φ>0)\n- Maximum incentive-compatible guarantee: hat(φ)(Q) = 1 - A/(I-Q), decreasing in Q\n\nAnalyzing each option:\n\nA) This correctly describes the trade-off in the high-budget regime. As Q increases, the entrepreneur has more equity (I-Q decreases), making hat(φ)(Q) decrease. The model shows φ and Q are substitutes in maintaining incentive compatibility.\n\nB) This contradicts the model's pecking order. If co-funding were always superior, it would be used even with small budgets. But the model explicitly states Q=0 when B < underline(B), showing guarantees are preferred initially.\n\nC) This aligns with the pecking order logic. When B < underline(B), only guarantees are used. Once B ≥ underline(B), co-funding is introduced, suggesting the guarantee tool has reached its limit and additional funds are channeled to co-funding.\n\nD) This explains why guarantees are preferred for small budgets. The model mentions \"copy-cat\" entrepreneurs with riskless projects. Since guarantees only pay on failure, riskless projects cannot benefit, making guarantees a targeted tool that avoids resource leakage.\n\nA,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 440, "Question": "### Background\n\nA large investor with CRRA utility (`R>1`) operates in a two-state market (good state 0, bad state 1). The investor's portfolio `\\pi` and consumption `c` choices influence the intensity of switching between states via a floored affine function:\n  \n\\vartheta^{i,1-i}(\\pi,c) = \\max\\{A^{i}+\\pi^{\\top}B_{\\pi}^{i}+B_{c}^{i}c,C^{i}\\} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Data / Model Specification\n\nConsider the specific case where the investor gains no direct utility from intermediate consumption (`\\varepsilon=0`). However, consumption can still influence the market regime. Assume that in the good state (`i=0`), consumption reduces the intensity of switching to the bad state (i.e., `B_c^0 < 0`).\n\n---\n\n### Question\n\nUnder these circumstances, it can be optimal for the investor to consume a positive amount of wealth, a behavior interpreted as 'market manipulation'. Select all of the following conditions that must be met for this seemingly paradoxical positive consumption to be optimal.", "Options": {"A": "The marginal benefit of consumption on reducing the switching intensity is sufficiently high (i.e., `B_c^0` is sufficiently negative).", "B": "The investor's current portfolio and consumption choices do not already result in the minimum possible switching intensity, `C^0`.", "C": "The perceived difference in value between the good and bad market regimes is sufficiently large.", "D": "The investor's coefficient of relative risk aversion, `R`, is greater than 1."}, "Answer": ["A", "B", "C"], "pi_justification": "Assessment Target: Tests the understanding of the specific conditions required for the paper's novel 'consumption as manipulation' result. Strategy: Scenario Application. The user must apply the theory to identify the set of necessary conditions. Distractor Logic: Options A, B, and D are the three necessary conditions outlined in the paper's analysis. Option C is True but Irrelevant; `R>1` is a standing assumption of the model setup, not a specific trigger condition for this particular effect.", "qid": "440", "question": "### Background\n\nA large investor with CRRA utility (`R>1`) operates in a two-state market (good state 0, bad state 1). The investor's portfolio `\\pi` and consumption `c` choices influence the intensity of switching between states via a floored affine function:\n  \n\\vartheta^{i,1-i}(\\pi,c) = \\max\\{A^{i}+\\pi^{\\top}B_{\\pi}^{i}+B_{c}^{i}c,C^{i}\\} \\quad \\text{(Eq. (1))}\n \n\n---\n\n### Data / Model Specification\n\nConsider the specific case where the investor gains no direct utility from intermediate consumption (`\\varepsilon=0`). However, consumption can still influence the market regime. Assume that in the good state (`i=0`), consumption reduces the intensity of switching to the bad state (i.e., `B_c^0 < 0`).\n\n---\n\n### Question\n\nUnder these circumstances, it can be optimal for the investor to consume a positive amount of wealth, a behavior interpreted as 'market manipulation'. Select all of the following conditions that must be met for this seemingly paradoxical positive consumption to be optimal.", "model_evaluation_result": {"model_raw_response": "I need to analyze when it's optimal for an investor to consume wealth despite gaining no direct utility from consumption (ε=0).\n\nGiven:\n- CRRA utility with R>1\n- Two-state market (good state 0, bad state 1)\n- Switching intensity: θ^{i,1-i}(π,c) = max{A^i + π^T B_π^i + B_c^i c, C^i}\n- ε=0 (no direct utility from consumption)\n- In good state (i=0): B_c^0 < 0 (consumption reduces switching to bad state)\n\nFor positive consumption to be optimal despite ε=0:\n\n**Option A**: The marginal benefit of consumption on reducing switching intensity must be sufficiently high (B_c^0 sufficiently negative).\n- This is necessary. If B_c^0 is not negative enough, the cost of consuming (reducing wealth) won't be offset by the benefit of staying in the good state longer.\n- ✓ CORRECT\n\n**Option B**: The investor must not already be at the minimum switching intensity C^0.\n- This is necessary. If A^0 + π^T B_π^0 + B_c^0 c ≤ C^0, then we're at the floor C^0, and further consumption won't reduce switching intensity.\n- The marginal benefit of consumption is zero once we hit the floor.\n- ✓ CORRECT\n\n**Option C**: The difference in value between good and bad states must be sufficiently large.\n- This is necessary. If the good and bad states are similar in value, there's little benefit to manipulating the switching intensity.\n- The investor needs sufficient incentive to sacrifice wealth through consumption.\n- ✓ CORRECT\n\n**Option D**: R > 1 is required.\n- This is already given in the problem setup, not a condition that \"must be met\" for positive consumption to be optimal.\n- While R > 1 affects the investor's preferences, it's not a specific requirement for the optimality of positive consumption in this context.\n- ✗ NOT CORRECT\n\nA,B,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 410, "Question": "### Background\n\n**Research Question.** This case examines two competing theories regarding the role of Corporate Social Responsibility (CSR) and its relationship with corporate governance. Does effective governance, proxied by board independence, encourage or curtail CSR activities?\n\n**Setting.** The analysis is framed by a fundamental conflict in corporate finance: the potential divergence of interests between a firm's managers and its shareholders. CSR investments are a key area where this conflict may manifest.\n\n---\n\n### Data / Model Specification\n\nThe study evaluates two competing hypotheses regarding the link between board independence and CSR:\n\n1.  **The Conflict Resolution Hypothesis.** This hypothesis posits that CSR is a value-maximizing tool used to resolve costly conflicts among various stakeholders (e.g., employees, customers, community). By mitigating these conflicts, CSR ultimately enhances firm performance and shareholder wealth. Effective governance should therefore promote value-enhancing CSR.\n\n2.  **The Risk-Mitigation / Agency Cost Hypothesis.** This hypothesis argues that managers, whose human and financial capital are poorly diversified and tied to the firm, are more risk-averse than the firm's well-diversified shareholders. To the extent that CSR activities reduce firm-specific risk, managers may over-invest in them to protect their personal wealth, even if such investments are not optimal for shareholders. This over-investment is an agency cost that effective governance should curtail.\n\n---\n\n### Question\n\nThe paper evaluates two competing hypotheses to explain the relationship between corporate governance and CSR. Which of the following statements accurately describe the logic and predictions of these hypotheses? Select all that apply.", "Options": {"A": "The agency conflict described in the paper arises because well-diversified shareholders are more averse to firm-specific risk than undiversified managers.", "B": "According to the logic of Jensen's inequality as applied to managerial risk aversion, a risk-averse manager's expected utility is maximized by maximizing the firm's expected value, regardless of risk.", "C": "The Risk-Mitigation/Agency Cost Hypothesis predicts a negative relationship between board independence and CSR, viewing CSR as a potential over-investment driven by managers' personal risk aversion.", "D": "The Conflict Resolution Hypothesis predicts a positive relationship between board independence and CSR, viewing CSR as a tool to maximize shareholder wealth."}, "Answer": ["C", "D"], "pi_justification": "REPLACED General QA (Suitability Score: 9.0; A=8, B=10). The original question tested recall of two opposing theories, making it highly suitable for conversion. This item uses a 'Premise Packaging' strategy to test the core logic of each hypothesis. Distractors are designed as 'Conceptual Opposites' (C) or misapplications of underlying theory (D), targeting high-frequency errors in corporate finance.", "qid": "410", "question": "### Background\n\n**Research Question.** This case examines two competing theories regarding the role of Corporate Social Responsibility (CSR) and its relationship with corporate governance. Does effective governance, proxied by board independence, encourage or curtail CSR activities?\n\n**Setting.** The analysis is framed by a fundamental conflict in corporate finance: the potential divergence of interests between a firm's managers and its shareholders. CSR investments are a key area where this conflict may manifest.\n\n---\n\n### Data / Model Specification\n\nThe study evaluates two competing hypotheses regarding the link between board independence and CSR:\n\n1.  **The Conflict Resolution Hypothesis.** This hypothesis posits that CSR is a value-maximizing tool used to resolve costly conflicts among various stakeholders (e.g., employees, customers, community). By mitigating these conflicts, CSR ultimately enhances firm performance and shareholder wealth. Effective governance should therefore promote value-enhancing CSR.\n\n2.  **The Risk-Mitigation / Agency Cost Hypothesis.** This hypothesis argues that managers, whose human and financial capital are poorly diversified and tied to the firm, are more risk-averse than the firm's well-diversified shareholders. To the extent that CSR activities reduce firm-specific risk, managers may over-invest in them to protect their personal wealth, even if such investments are not optimal for shareholders. This over-investment is an agency cost that effective governance should curtail.\n\n---\n\n### Question\n\nThe paper evaluates two competing hypotheses to explain the relationship between corporate governance and CSR. Which of the following statements accurately describe the logic and predictions of these hypotheses? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at each option carefully:\n\nA) This statement reverses the relationship. The paper states that managers (whose wealth is tied to the firm) are MORE risk-averse than well-diversified shareholders, not the other way around. This is incorrect.\n\nB) This misrepresents Jensen's inequality. For a risk-averse manager with a concave utility function, Jensen's inequality shows that the expected utility of a risky outcome is less than the utility of the expected outcome. This means risk-averse managers would NOT maximize expected utility by simply maximizing expected firm value while ignoring risk - they would also seek to reduce risk. This is incorrect.\n\nC) This accurately describes the Risk-Mitigation/Agency Cost Hypothesis. The hypothesis argues that risk-averse managers over-invest in CSR to reduce firm-specific risk (protecting their undiversified wealth), which is suboptimal for diversified shareholders. Therefore, effective governance (high board independence) should curtail this over-investment, creating a negative relationship. This is correct.\n\nD) This accurately describes the Conflict Resolution Hypothesis. This hypothesis views CSR as a value-maximizing tool that resolves costly stakeholder conflicts, ultimately enhancing shareholder wealth. Therefore, effective governance (high board independence) should promote such value-enhancing CSR, creating a positive relationship. This is correct.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 365, "Question": "### Background\n\n**Research Question.** Is the immediate market reaction to bad earnings news more severe for growth firms than for value firms (the \"earnings torpedo\" effect), and does this asymmetry depend on whether the news is routine or non-routine?\n\n**Setting and Sample.** The study examines the 3-day market-adjusted abnormal returns (`BH_3`) around management earnings forecast (MEF) announcements for Australian firms. Firms are categorized as 'Growth' or 'Value' based on book-to-market quintiles. MEFs are categorized as 'Good', 'Bad', or 'Neutral' news. Announcements are also classified as 'Routine' (e.g., at an Annual General Meeting) or 'Non-routine' (arising from continuous disclosure obligations).\n\n**Variables and Parameters.**\n- `BH_3`: 3-day market-adjusted buy-and-hold abnormal return.\n- `Growth`, `Value`: Dummy variables for firm classification. The omitted category is intermediate B/M firms.\n- `D_BadA`, `D_GoodA`: Dummy variables for news classification.\n- Control variables include `LSize` (log firm size), `NAN` (number of analysts), and `LMeanrba` (log bid-ask spread).\n- `β_k`: Regression coefficients.\n\n---\n\n### Data / Model Specification\n\nThe study tests for an asymmetric market reaction using a regression model with interaction terms. The omitted baseline category is an intermediate B/M firm issuing neutral news.\n\n  \n\\mathrm{BH}_{3} = \\alpha + \\beta_{1}(\\text{Growth} \\cdot D_{\\mathrm{BadA}}) + \\beta_{3}(\\text{Value} \\cdot D_{\\mathrm{BadA}}) + \\beta_{6}(\\text{Growth} \\cdot D_{\\mathrm{GoodA}}) + \\beta_{8}(\\text{Value} \\cdot D_{\\mathrm{GoodA}}) + \\dots + \\text{Controls} + \\varepsilon\n \n\nHypothesis H1A posits that the reaction to bad news is more negative for growth firms (`|β1| > |β3|`). Hypothesis H1B posits no difference in reaction to good news (`β6 = β8`).\n\n**Table 1: Regression of 3-Day Abnormal Returns (`BH_3`)**\n\n| Variable | Overall | Non-routine |\n| :--- | :--- | :--- |\n| `Growth * D_BadA` (β1) | -0.053*** | -0.074*** |\n| `Value * D_BadA` (β3) | -0.027*** | -0.023 |\n| `Growth * D_GoodA` (β6) | 0.013** | 0.018* |\n| `Value * D_GoodA` (β8) | 0.019** | 0.034* |\n| **Wald Test H1A: |β1|=|β3|** | p = 0.029 | p = 0.007 |\n| **Wald Test H1B: β6=β8** | p = 0.522 | p = 0.344 |\n\n*Controls included but not shown. *** p<0.01, ** p<0.05, * p<0.1. Source: Table 3 of the paper.* \n\n---\n\n### Question\n\nBased on the regression results presented in **Table 1**, which of the following conclusions about the immediate 3-day market reaction (`BH_3`) to management earnings forecasts are supported by the data?", "Options": {"A": "For good news announcements, there is no statistically significant difference in the market's reaction between growth and value firms in either the 'Overall' or 'Non-routine' sample.", "B": "When considering only 'Non-routine' announcements, the market's negative reaction to bad news is statistically significant for growth firms but not for value firms.", "C": "The asymmetric negative reaction to bad news, where growth firms are penalized more severely than value firms, is statistically significant in both the 'Overall' and 'Non-routine' samples.", "D": "The positive reaction to good news for value firms is statistically significant in the 'Overall' sample but not in the 'Non-routine' sample."}, "Answer": ["A", "B", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize and compare regression results across different subsamples ('Overall' vs. 'Non-routine'). It uses an Atomic Decomposition strategy, breaking down the paper's main findings on the 'earnings torpedo' effect into distinct, verifiable statements. The options require careful interpretation of coefficient estimates, significance levels from Wald tests, and individual coefficient significance from stars. Distractor D is a high-fidelity error based on misreading the significance levels across columns.", "qid": "365", "question": "### Background\n\n**Research Question.** Is the immediate market reaction to bad earnings news more severe for growth firms than for value firms (the \"earnings torpedo\" effect), and does this asymmetry depend on whether the news is routine or non-routine?\n\n**Setting and Sample.** The study examines the 3-day market-adjusted abnormal returns (`BH_3`) around management earnings forecast (MEF) announcements for Australian firms. Firms are categorized as 'Growth' or 'Value' based on book-to-market quintiles. MEFs are categorized as 'Good', 'Bad', or 'Neutral' news. Announcements are also classified as 'Routine' (e.g., at an Annual General Meeting) or 'Non-routine' (arising from continuous disclosure obligations).\n\n**Variables and Parameters.**\n- `BH_3`: 3-day market-adjusted buy-and-hold abnormal return.\n- `Growth`, `Value`: Dummy variables for firm classification. The omitted category is intermediate B/M firms.\n- `D_BadA`, `D_GoodA`: Dummy variables for news classification.\n- Control variables include `LSize` (log firm size), `NAN` (number of analysts), and `LMeanrba` (log bid-ask spread).\n- `β_k`: Regression coefficients.\n\n---\n\n### Data / Model Specification\n\nThe study tests for an asymmetric market reaction using a regression model with interaction terms. The omitted baseline category is an intermediate B/M firm issuing neutral news.\n\n  \n\\mathrm{BH}_{3} = \\alpha + \\beta_{1}(\\text{Growth} \\cdot D_{\\mathrm{BadA}}) + \\beta_{3}(\\text{Value} \\cdot D_{\\mathrm{BadA}}) + \\beta_{6}(\\text{Growth} \\cdot D_{\\mathrm{GoodA}}) + \\beta_{8}(\\text{Value} \\cdot D_{\\mathrm{GoodA}}) + \\dots + \\text{Controls} + \\varepsilon\n \n\nHypothesis H1A posits that the reaction to bad news is more negative for growth firms (`|β1| > |β3|`). Hypothesis H1B posits no difference in reaction to good news (`β6 = β8`).\n\n**Table 1: Regression of 3-Day Abnormal Returns (`BH_3`)**\n\n| Variable | Overall | Non-routine |\n| :--- | :--- | :--- |\n| `Growth * D_BadA` (β1) | -0.053*** | -0.074*** |\n| `Value * D_BadA` (β3) | -0.027*** | -0.023 |\n| `Growth * D_GoodA` (β6) | 0.013** | 0.018* |\n| `Value * D_GoodA` (β8) | 0.019** | 0.034* |\n| **Wald Test H1A: |β1|=|β3|** | p = 0.029 | p = 0.007 |\n| **Wald Test H1B: β6=β8** | p = 0.522 | p = 0.344 |\n\n*Controls included but not shown. *** p<0.01, ** p<0.05, * p<0.1. Source: Table 3 of the paper.* \n\n---\n\n### Question\n\nBased on the regression results presented in **Table 1**, which of the following conclusions about the immediate 3-day market reaction (`BH_3`) to management earnings forecasts are supported by the data?", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to evaluate each option against the regression results and Wald tests.\n\n**Option A**: \"For good news announcements, there is no statistically significant difference in the market's reaction between growth and value firms in either the 'Overall' or 'Non-routine' sample.\"\n\n- The Wald Test H1B tests whether β6 = β8 (i.e., whether the reaction to good news is the same for growth and value firms)\n- Overall sample: p = 0.522 (not significant)\n- Non-routine sample: p = 0.344 (not significant)\n- Since both p-values are well above 0.05, we fail to reject the null hypothesis that β6 = β8\n- This supports option A ✓\n\n**Option B**: \"When considering only 'Non-routine' announcements, the market's negative reaction to bad news is statistically significant for growth firms but not for value firms.\"\n\n- Non-routine sample:\n  - Growth * D_BadA (β1) = -0.074*** (significant at p<0.01)\n  - Value * D_BadA (β3) = -0.023 (no significance stars, so not significant)\n- This supports option B ✓\n\n**Option C**: \"The asymmetric negative reaction to bad news, where growth firms are penalized more severely than value firms, is statistically significant in both the 'Overall' and 'Non-routine' samples.\"\n\n- The Wald Test H1A tests whether |β1| = |β3| (testing for asymmetry)\n- Overall sample: p = 0.029 (significant at p<0.05)\n- Non-routine sample: p = 0.007 (significant at p<0.01)\n- Since both p-values are below 0.05, we reject the null hypothesis of equality, confirming the asymmetry is statistically significant\n- This supports option C ✓\n\n**Option D**: \"The positive reaction to good news for value firms is statistically significant in the 'Overall' sample but not in the 'Non-routine' sample.\"\n\n- Value * D_GoodA (β8):\n  - Overall sample: 0.019**", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 259, "Question": "### Background\n\n**Research Question.** What is the empirical journey from a raw correlation to a final, causal estimate of the net economic impact of M&A rumors on deal outcomes?\n\n**Setting / Data-Generating Environment.** The analysis progresses through several stages: (1) examining raw descriptive data, (2) estimating a naive model that ignores endogeneity and selection bias, and (3) estimating a full structural model using Indirect Inference (II) to correct for these biases, and (4) synthesizing the final estimates to calculate the overall economic impact.\n\n**Variables & Parameters.**\n- `Closing`: Binary variable; 1 if a deal is completed, 0 otherwise.\n- `Ln(Price)`: The natural logarithm of the deal's transaction multiple (e.g., EV/Sales).\n- `Leak`: Binary variable; 1 if a deal was subject to a pre-announcement rumor, 0 otherwise.\n- `\\gamma`: The causal effect of a `Leak` on the propensity for `Closing`.\n- `\\kappa`: The causal effect of a `Leak` on `Ln(Price)` for completed deals.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Naive OLS Model for Deal Value, Ln(Price)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.066*** |\n| | (0.005) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 8, estimated on observed prices only. `***` denotes p < 0.01.*\n\n**Table 2: Indirect Inference (II) Structural Model Estimates**\n\n| Equation | Variable | Coefficient (Std. Err.) |\n| :--- | :--- | :--- |\n| **Closing** | **Leak** (`\\hat{\\gamma}`) | **-1.734*** (0.596) |\n| **Ln(Price)** | **Leak** (`\\hat{\\kappa}`) | **0.160*** (0.031) |\n*Source: Abridged from original paper's Table 9. `***` denotes p < 0.01.*\n\n**Table 3: Final Economic Impact Calculation**\n\n| Metric | Value |\n| :--- | :--- |\n| **Relative Rumor Price (Damage)** | **-32.42%*** |\n*Source: Original paper's Table 10. `***` denotes p < 0.01.*\n\n---\n\n### Question\n\nThe paper's main contribution comes from contrasting the naive estimates with corrected estimates from a structural model. Based on the full set of results, select all of the following conclusions that are supported by the paper.", "Options": {"A": "A key finding from the structural model (Table 2) is that rumors *increase* the premium for completed deals by 16.0%, a direct reversal of the conclusion from the naive model (Table 1).", "B": "The structural model's estimate for the effect of a leak on closing (`\\hat{\\gamma}` = -1.734) is smaller in magnitude than the naive probit estimate of -0.904 (not shown), indicating the naive model overstated the negative impact.", "C": "The final net economic impact of -32.42% (Table 3) is driven by the fact that the large negative effect of a rumor on deal completion probability outweighs its positive effect on the price of deals that do close.", "D": "The paper's final conclusion is that the 16.0% price premium for rumored deals makes leaking a profitable strategy for sellers on average, despite the risks."}, "Answer": ["A", "C"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to synthesize the paper's core causal findings, contrasting naive vs. corrected estimates and interpreting the ultimate net economic impact. It uses an 'Atomic Decomposition' strategy to test the key takeaways. Distractor C is a 'Numerical' error, requiring careful comparison of coefficient magnitudes. Distractor D is a 'Conceptual Opposite' that presents a conclusion directly refuted by the paper's final -32.42% damage estimate.", "qid": "259", "question": "### Background\n\n**Research Question.** What is the empirical journey from a raw correlation to a final, causal estimate of the net economic impact of M&A rumors on deal outcomes?\n\n**Setting / Data-Generating Environment.** The analysis progresses through several stages: (1) examining raw descriptive data, (2) estimating a naive model that ignores endogeneity and selection bias, and (3) estimating a full structural model using Indirect Inference (II) to correct for these biases, and (4) synthesizing the final estimates to calculate the overall economic impact.\n\n**Variables & Parameters.**\n- `Closing`: Binary variable; 1 if a deal is completed, 0 otherwise.\n- `Ln(Price)`: The natural logarithm of the deal's transaction multiple (e.g., EV/Sales).\n- `Leak`: Binary variable; 1 if a deal was subject to a pre-announcement rumor, 0 otherwise.\n- `\\gamma`: The causal effect of a `Leak` on the propensity for `Closing`.\n- `\\kappa`: The causal effect of a `Leak` on `Ln(Price)` for completed deals.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Naive OLS Model for Deal Value, Ln(Price)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :--- |\n| **Leak** | **-0.066*** |\n| | (0.005) |\n| *Controls & FEs* | *Included* |\n*Source: Abridged from original paper's Table 8, estimated on observed prices only. `***` denotes p < 0.01.*\n\n**Table 2: Indirect Inference (II) Structural Model Estimates**\n\n| Equation | Variable | Coefficient (Std. Err.) |\n| :--- | :--- | :--- |\n| **Closing** | **Leak** (`\\hat{\\gamma}`) | **-1.734*** (0.596) |\n| **Ln(Price)** | **Leak** (`\\hat{\\kappa}`) | **0.160*** (0.031) |\n*Source: Abridged from original paper's Table 9. `***` denotes p < 0.01.*\n\n**Table 3: Final Economic Impact Calculation**\n\n| Metric | Value |\n| :--- | :--- |\n| **Relative Rumor Price (Damage)** | **-32.42%*** |\n*Source: Original paper's Table 10. `***` denotes p < 0.01.*\n\n---\n\n### Question\n\nThe paper's main contribution comes from contrasting the naive estimates with corrected estimates from a structural model. Based on the full set of results, select all of the following conclusions that are supported by the paper.", "model_evaluation_result": {"model_raw_response": "Looking at the data and conclusions:\n\n**Option A**: The structural model shows κ̂ = 0.160 (16.0% increase in price for completed deals with leaks), while the naive model shows -0.066 (6.6% decrease). This is indeed a direct reversal - from negative to positive effect. ✓\n\n**Option B**: The structural model's γ̂ = -1.734 is described as \"smaller in magnitude than the naive probit estimate of -0.904.\" However, |-1.734| = 1.734 > |-0.904| = 0.904, so the structural estimate is actually larger in magnitude, not smaller. ✗\n\n**Option C**: The structural model shows:\n- Leak effect on closing: γ̂ = -1.734 (large negative effect on completion probability)\n- Leak effect on price if completed: κ̂ = 0.160 (positive 16% premium)\n- Final net impact: -32.42% (negative)\n\nThis negative net impact logically follows from the large negative completion effect outweighing the positive price effect. ✓\n\n**Option D**: With a final net economic impact of -32.42%, leaking is clearly not profitable on average. The statement contradicts the data. ✗\n\nA,C", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 331, "Question": "### Background\n\n**Research Question.** How should an analyst select comparable companies when using a market multiple to estimate the terminal value of a firm, and why are valuation multiples expected to change as a firm matures?\n\n**Setting.** An illustrative valuation of a company that transitions from a high-growth phase (25% revenue growth in Year 1) to a stable, mature phase (2% growth from Year 4 onwards).\n\n**Variables and Parameters.**\n- `EV_t`: Enterprise Value at the beginning of Year `t`.\n- `EBIT_t`: EBIT for Year `t`.\n\n---\n\n### Data / Model Specification\n\nThe constant growth perpetuity formula for firm value is `EV_t = FCF_{t+1} / (r - g)`. This shows that multiples are an increasing function of the growth rate `g`.\n\n**Table 1: Market Multiples for a High-Growth Firm Over Time**\n| Metric | Year 1 | Year 2 | Year 3 | Year 4 | Year 5 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Revenue Growth Rate | 25.0% | 20.0% | 8.0% | 2.0% | 2.0% |\n| EBIT Multiple (EV/EBIT) | 10.9x | 9.6x | 8.4x | 8.0x | 8.0x |\n\n---\n\n### Question\n\nAn analyst is valuing a young, high-growth company. The valuation involves a 5-year explicit forecast period followed by a terminal value calculation, at which point the company is expected to be a mature, stable-growth firm. Based on the principles and data presented, select all appropriate practices for this valuation.", "Options": {"A": "The peer group used to determine the terminal value multiple should consist of companies that are currently mature and exhibit stable, low-growth characteristics.", "B": "The terminal EV/EBIT multiple should be cross-checked for consistency with an implied DCF perpetuity formula, such as `EV/EBIT = (FCF/EBIT) * (1+g) / (r-g)`.", "C": "To ensure consistency, the terminal multiple should be based on the company's own calculated multiple in Year 1 (e.g., 10.9x EV/EBIT).", "D": "The valuation multiples of the company being valued are expected to decline over the forecast period as its growth rate slows."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses a Premise/Assumption Packaging strategy to assess the practical application of the paper's findings to terminal value calculation, a critical valuation step. It tests for understanding of the 'characteristic matching' principle. Options A and B are direct applications of the paper's argument. Option C packages the logic of the original QA's quantitative derivation into a conceptual check, adding depth. Distractor (D) represents the primary valuation error discussed in the paper.", "qid": "331", "question": "### Background\n\n**Research Question.** How should an analyst select comparable companies when using a market multiple to estimate the terminal value of a firm, and why are valuation multiples expected to change as a firm matures?\n\n**Setting.** An illustrative valuation of a company that transitions from a high-growth phase (25% revenue growth in Year 1) to a stable, mature phase (2% growth from Year 4 onwards).\n\n**Variables and Parameters.**\n- `EV_t`: Enterprise Value at the beginning of Year `t`.\n- `EBIT_t`: EBIT for Year `t`.\n\n---\n\n### Data / Model Specification\n\nThe constant growth perpetuity formula for firm value is `EV_t = FCF_{t+1} / (r - g)`. This shows that multiples are an increasing function of the growth rate `g`.\n\n**Table 1: Market Multiples for a High-Growth Firm Over Time**\n| Metric | Year 1 | Year 2 | Year 3 | Year 4 | Year 5 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| Revenue Growth Rate | 25.0% | 20.0% | 8.0% | 2.0% | 2.0% |\n| EBIT Multiple (EV/EBIT) | 10.9x | 9.6x | 8.4x | 8.0x | 8.0x |\n\n---\n\n### Question\n\nAn analyst is valuing a young, high-growth company. The valuation involves a 5-year explicit forecast period followed by a terminal value calculation, at which point the company is expected to be a mature, stable-growth firm. Based on the principles and data presented, select all appropriate practices for this valuation.", "model_evaluation_result": {"model_raw_response": "Looking at this valuation problem, I need to identify which practices are appropriate when valuing a high-growth company that transitions to maturity.\n\nLet me analyze each option:\n\n**Option A**: Using mature, stable-growth companies as peers for the terminal value multiple.\n- This is correct. The terminal value represents the company when it has reached maturity (Year 4+ with 2% growth). The appropriate comparable companies should have similar characteristics to what the target company will be at that time - mature with stable, low growth.\n\n**Option B**: Cross-checking the terminal multiple with the DCF perpetuity formula.\n- This is correct. The formula EV/EBIT = (FCF/EBIT) × (1+g)/(r-g) provides a theoretical foundation for what the multiple should be. Cross-checking ensures internal consistency between the multiple approach and DCF fundamentals.\n\n**Option C**: Using the company's Year 1 multiple (10.9x) for the terminal value.\n- This is incorrect. The Year 1 multiple reflects high growth (25%), while the terminal value should reflect mature, stable growth (2%). Using the high-growth multiple would significantly overvalue the terminal value.\n\n**Option D**: Expecting multiples to decline as growth slows.\n- This is correct. The table shows multiples declining from 10.9x to 8.0x as growth slows from 25% to 2%. This is consistent with valuation theory - the perpetuity formula shows multiples are an increasing function of growth rate g.\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 329, "Question": "### Background\n\n**Research Question.** Why might the popular EV/EBITDA multiple be a less reliable valuation tool than the EV/Unlevered Earnings multiple, particularly when comparing firms with different tax rates or capital intensities?\n\n**Setting.** The analysis compares two key enterprise value multiples: EV/EBITDA and EV/Unlevered Earnings (also known as NOPAT). The goal is to understand their relative sensitivity to underlying value drivers based on a simulation using a financial model for Staples, Inc.\n\n**Variables and Parameters.**\n- `Enterprise Value (EV)`: The market value of the firm's operating assets.\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization.\n- `Unlevered Earnings (NOPAT)`: Net Operating Profit After Tax, defined as `EBIT * (1 - Tax Rate)`.\n- `CapEx`: Capital Expenditures.\n\n---\n\n### Data / Model Specification\n\nSimulation results from the paper show the sensitivity of multiples to permanent changes in value drivers that are calibrated to cause a 10% increase in EV.\n\n**Table 1: Sensitivity of Multiples to Changes in Tax Rate and CapEx**\n| Scenario | % Change in Unlevered Earnings Multiple | % Change in EBITDA Multiple |\n| :--- | :--- | :--- |\n| Income Tax Rate Reduced | +0.6% | +10.0% |\n| CapEx Requirements Reduced | +9.5% | +10.0% |\n\nKey definitions:\n- `EBITDA = EBIT + Depreciation & Amortization`\n- `Unlevered Earnings = EBIT - Taxes on EBIT = EBIT * (1 - τ_c)`\n\n---\n\n### Question\n\nBased on the provided definitions and simulation results in **Table 1**, select all of the following statements that correctly describe the relative vulnerabilities of the EV/EBITDA and EV/Unlevered Earnings multiples.", "Options": {"A": "When valuing a firm with significant Net Operating Losses (NOLs), using the cash tax rate (0%) to calculate NOPAT provides the most comparable multiple to peer firms without NOLs.", "B": "The EV/Unlevered Earnings multiple is relatively stable against tax rate changes because both its numerator (EV) and denominator (Unlevered Earnings) are after-tax measures that move in the same direction.", "C": "The EV/EBITDA multiple is highly sensitive to a firm's capital intensity because the EBITDA denominator ignores capital expenditures and depreciation, while these investments directly affect firm value (the numerator).", "D": "A change in the corporate tax rate significantly impacts the EV/EBITDA multiple but has a minimal effect on the EV/Unlevered Earnings multiple."}, "Answer": ["B", "C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item uses Atomic Decomposition to test the paper's central conclusion about the relative merits of NOPAT vs. EBITDA multiples. It assesses the user's ability to interpret simulation data in light of accounting definitions. Distractor (D) is a high-fidelity conceptual opposite based on an advanced scenario from the original problem (valuing firms with NOLs), testing for deep understanding rather than simple table lookup.", "qid": "329", "question": "### Background\n\n**Research Question.** Why might the popular EV/EBITDA multiple be a less reliable valuation tool than the EV/Unlevered Earnings multiple, particularly when comparing firms with different tax rates or capital intensities?\n\n**Setting.** The analysis compares two key enterprise value multiples: EV/EBITDA and EV/Unlevered Earnings (also known as NOPAT). The goal is to understand their relative sensitivity to underlying value drivers based on a simulation using a financial model for Staples, Inc.\n\n**Variables and Parameters.**\n- `Enterprise Value (EV)`: The market value of the firm's operating assets.\n- `EBITDA`: Earnings Before Interest, Taxes, Depreciation, and Amortization.\n- `Unlevered Earnings (NOPAT)`: Net Operating Profit After Tax, defined as `EBIT * (1 - Tax Rate)`.\n- `CapEx`: Capital Expenditures.\n\n---\n\n### Data / Model Specification\n\nSimulation results from the paper show the sensitivity of multiples to permanent changes in value drivers that are calibrated to cause a 10% increase in EV.\n\n**Table 1: Sensitivity of Multiples to Changes in Tax Rate and CapEx**\n| Scenario | % Change in Unlevered Earnings Multiple | % Change in EBITDA Multiple |\n| :--- | :--- | :--- |\n| Income Tax Rate Reduced | +0.6% | +10.0% |\n| CapEx Requirements Reduced | +9.5% | +10.0% |\n\nKey definitions:\n- `EBITDA = EBIT + Depreciation & Amortization`\n- `Unlevered Earnings = EBIT - Taxes on EBIT = EBIT * (1 - τ_c)`\n\n---\n\n### Question\n\nBased on the provided definitions and simulation results in **Table 1**, select all of the following statements that correctly describe the relative vulnerabilities of the EV/EBITDA and EV/Unlevered Earnings multiples.", "model_evaluation_result": {"model_raw_response": "Looking at the data and definitions provided:\n\nFrom Table 1:\n- When income tax rate is reduced: Unlevered Earnings multiple changes by +0.6%, EBITDA multiple changes by +10.0%\n- When CapEx requirements are reduced: Unlevered Earnings multiple changes by +9.5%, EBITDA multiple changes by +10.0%\n\nLet me analyze each option:\n\nA) This is incorrect. Using a 0% cash tax rate for firms with NOLs would make their NOPAT artificially high (NOPAT = EBIT × (1-0) = EBIT), creating non-comparable multiples with peer firms that pay taxes.\n\nB) This is correct. The data shows only a 0.6% change in the Unlevered Earnings multiple when tax rates change. Both EV and Unlevered Earnings are after-tax measures, so they move together when tax rates change, keeping the multiple relatively stable.\n\nC) This is correct. EBITDA excludes both depreciation and capital expenditures, while these investments directly impact firm value (EV). The 10% change in EBITDA multiple when CapEx changes demonstrates this sensitivity to capital intensity.\n\nD) This is correct. The data clearly shows that a tax rate change causes a 10% change in EV/EBITDA multiple but only a 0.6% change in EV/Unlevered Earnings multiple.\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 276, "Question": "### Background\n\n**Research Question.** What is the joint and incremental value relevance of book value (a stock measure from the balance sheet) and earnings (a flow measure from the income statement) for valuing international banks?\n\n**Setting / Data-Generating Environment.** The analysis uses a pooled sample of 28,786 quarterly observations for 813 international banking institutions during the 1993–2004 period.\n\n### Data / Model Specification\n\nThree pooled OLS regressions are estimated to assess the value relevance of book value per share (`BV`) and earnings per share (`EPS`) for explaining market value per share (`MV`):\n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + e_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} EPS_i + e_{i} \\quad \\text{(Eq. 2)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + \\beta_{2} EPS_i + e_{i} \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Pooled OLS Regressions of Value Relevance**\n\n| Variable          | Model 1 (Eq. 1) | Model 2 (Eq. 2) | Model 3 (Eq. 3) |\n| :---------------- | :-------------- | :-------------- | :-------------- |\n| Intercept         | 18.452***       | 17.160***       | 14.340***       |\n| Book Value (`β₁`) | 0.559***        |                 | 0.462***        |\n| Earnings (`β₂`)   |                 | 0.376***        | 0.241***        |\n| R-squared         | 0.291           | 0.250           | 0.357           |\n\n*Note: *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the regression results in Table 1, select all of the following statements that represent a correct interpretation of the models.", "Options": {"A": "The statistical significance of the Earnings coefficient (0.241) in Model 3 demonstrates that earnings provide incremental value-relevant information beyond what is already contained in book value.", "B": "Because Model 1 (R²=0.291) has a higher R-squared than Model 2 (R²=0.250), book value is a sufficient statistic for firm value, and earnings are redundant.", "C": "In Model 3, the coefficient on Book Value (0.462) is larger than on Earnings (0.241), which proves that the balance sheet is more important than the income statement for valuation.", "D": "The decrease in the Book Value coefficient from 0.559 in Model 1 to 0.462 in Model 3 is consistent with positive omitted variable bias in Model 1, where Book Value was likely capturing some of the positive effect of the omitted Earnings variable."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses core econometric interpretation skills: understanding incremental information content and diagnosing omitted variable bias by comparing nested models. It uses an Atomic Decomposition strategy. Correct options test the concept of incremental explanatory power (A) and the specific mechanics of OVB (B). Distractor C misinterprets R-squared to incorrectly conclude redundancy, while D makes a common error of comparing coefficients across variables with different scales to infer 'importance'.", "qid": "276", "question": "### Background\n\n**Research Question.** What is the joint and incremental value relevance of book value (a stock measure from the balance sheet) and earnings (a flow measure from the income statement) for valuing international banks?\n\n**Setting / Data-Generating Environment.** The analysis uses a pooled sample of 28,786 quarterly observations for 813 international banking institutions during the 1993–2004 period.\n\n### Data / Model Specification\n\nThree pooled OLS regressions are estimated to assess the value relevance of book value per share (`BV`) and earnings per share (`EPS`) for explaining market value per share (`MV`):\n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + e_{i} \\quad \\text{(Eq. 1)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} EPS_i + e_{i} \\quad \\text{(Eq. 2)}\n \n\n  \nMV_i = \\beta_{0} + \\beta_{1} BV_i + \\beta_{2} EPS_i + e_{i} \\quad \\text{(Eq. 3)}\n \n\n**Table 1: Pooled OLS Regressions of Value Relevance**\n\n| Variable          | Model 1 (Eq. 1) | Model 2 (Eq. 2) | Model 3 (Eq. 3) |\n| :---------------- | :-------------- | :-------------- | :-------------- |\n| Intercept         | 18.452***       | 17.160***       | 14.340***       |\n| Book Value (`β₁`) | 0.559***        |                 | 0.462***        |\n| Earnings (`β₂`)   |                 | 0.376***        | 0.241***        |\n| R-squared         | 0.291           | 0.250           | 0.357           |\n\n*Note: *** denotes significance at the 1% level.*\n\n### Question\n\nBased on the regression results in Table 1, select all of the following statements that represent a correct interpretation of the models.", "model_evaluation_result": {"model_raw_response": "Looking at the regression results and analyzing each statement:\n\nA) **Correct**: The Earnings coefficient (0.241) in Model 3 is statistically significant at the 1% level. This significance when Book Value is already included in the model demonstrates that earnings provide additional explanatory power beyond what book value alone captures. This is the definition of incremental value relevance.\n\nB) **Incorrect**: While Model 1 has a higher R² than Model 2, this doesn't mean earnings are redundant. Model 3, which includes both variables, has the highest R² (0.357), indicating that both variables together explain more variance than either alone. This shows that earnings add value beyond book value.\n\nC) **Incorrect**: The relative size of coefficients cannot be directly compared to determine \"importance\" without considering the scales and variances of the variables. Book value and earnings are measured in different units and have different distributions, so coefficient magnitudes alone don't indicate relative importance.\n\nD) **Correct**: The Book Value coefficient decreases from 0.559 (Model 1) to 0.462 (Model 3) when Earnings is added. This pattern is consistent with positive omitted variable bias - in Model 1, the Book Value coefficient was inflated because it was capturing some of the effect of the omitted Earnings variable, which is positively correlated with both Book Value and Market Value.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 407, "Question": "### Background\n\n**Research Question.** This case dissects the economic mechanism behind the finding that increased board independence leads to a reduction in Corporate Social Responsibility (CSR). It investigates which specific types of CSR are cut and their impact on firm value to build a cohesive narrative supporting the agency cost hypothesis.\n\n**Setting.** The analysis builds on a primary finding from a difference-in-differences (DiD) study showing that the Sarbanes-Oxley Act (SOX), by forcing some firms to increase board independence, caused a reduction in their overall CSR engagement. This case examines a series of follow-on tests to understand the nuances of this effect.\n\n**Variables & Parameters.**\n\n*   `CSR score`: The composite measure, defined as `CSR strengths - CSR concerns`.\n*   `CSR strengths`: A count of a firm's positive, proactive CSR activities (e.g., community development, environmental initiatives).\n*   `CSR concerns`: A count of a firm's negative CSR-related incidents (e.g., environmental fines, employee lawsuits).\n*   `Tobin's Q`: A proxy for firm value.\n\n---\n\n### Data / Model Specification\n\nThe analysis relies on results from two separate panel regressions, presented below.\n\n**Table 1: DiD Effect on CSR Strengths and Concerns**\n\nThis table shows the results of the DiD model run separately on the two components of the CSR score.\n\n| Dependent Variable | `Noncompliant × post-SOX` Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| CSR strengths | -0.587 | (-2.98) |\n| CSR concerns | 0.080 | (0.53) |\n\n**Table 2: The Effects of CSR Strengths and Concerns on Firm Value**\n\nThis table shows results from a regression of Tobin's Q on the CSR components, including firm and year fixed effects.\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `CSR strengths` | -0.044 | (-2.89) |\n| `CSR concerns` | 0.009 | (0.47) |\n\n---\n\n### Question\n\nBased on the evidence provided, which of the following statements accurately synthesize the findings regarding the impact of increased board independence on CSR components and firm value? Select all that apply.", "Options": {"A": "The reduction in the composite CSR score is statistically attributable to a significant decrease in CSR strengths.", "B": "CSR strengths are associated with a statistically significant reduction in firm value, as measured by Tobin's Q.", "C": "The findings suggest that CSR concerns, unlike CSR strengths, are positively and significantly associated with firm value.", "D": "The analysis shows that independent boards cause a statistically significant increase in CSR concerns."}, "Answer": ["A", "B"], "pi_justification": "This item assesses the ability to synthesize findings from two separate analyses (CSR component decomposition and valuation). It uses an 'Atomic Decomposition' strategy, breaking the paper's narrative into independently verifiable claims. Distractors are based on misinterpreting statistical significance (C) and the sign of the valuation effect (D), representing common comprehension errors.", "qid": "407", "question": "### Background\n\n**Research Question.** This case dissects the economic mechanism behind the finding that increased board independence leads to a reduction in Corporate Social Responsibility (CSR). It investigates which specific types of CSR are cut and their impact on firm value to build a cohesive narrative supporting the agency cost hypothesis.\n\n**Setting.** The analysis builds on a primary finding from a difference-in-differences (DiD) study showing that the Sarbanes-Oxley Act (SOX), by forcing some firms to increase board independence, caused a reduction in their overall CSR engagement. This case examines a series of follow-on tests to understand the nuances of this effect.\n\n**Variables & Parameters.**\n\n*   `CSR score`: The composite measure, defined as `CSR strengths - CSR concerns`.\n*   `CSR strengths`: A count of a firm's positive, proactive CSR activities (e.g., community development, environmental initiatives).\n*   `CSR concerns`: A count of a firm's negative CSR-related incidents (e.g., environmental fines, employee lawsuits).\n*   `Tobin's Q`: A proxy for firm value.\n\n---\n\n### Data / Model Specification\n\nThe analysis relies on results from two separate panel regressions, presented below.\n\n**Table 1: DiD Effect on CSR Strengths and Concerns**\n\nThis table shows the results of the DiD model run separately on the two components of the CSR score.\n\n| Dependent Variable | `Noncompliant × post-SOX` Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| CSR strengths | -0.587 | (-2.98) |\n| CSR concerns | 0.080 | (0.53) |\n\n**Table 2: The Effects of CSR Strengths and Concerns on Firm Value**\n\nThis table shows results from a regression of Tobin's Q on the CSR components, including firm and year fixed effects.\n\n| Variable | Coefficient | t-statistic |\n| :--- | :--- | :--- |\n| `CSR strengths` | -0.044 | (-2.89) |\n| `CSR concerns` | 0.009 | (0.47) |\n\n---\n\n### Question\n\nBased on the evidence provided, which of the following statements accurately synthesize the findings regarding the impact of increased board independence on CSR components and firm value? Select all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at the tables and analyzing each statement:\n\n**Statement A**: \"The reduction in the composite CSR score is statistically attributable to a significant decrease in CSR strengths.\"\n- Table 1 shows CSR strengths coefficient = -0.587 with t-statistic = -2.98 (statistically significant)\n- Table 1 shows CSR concerns coefficient = 0.080 with t-statistic = 0.53 (not statistically significant)\n- Since CSR score = CSR strengths - CSR concerns, and only the decrease in strengths is significant, this statement is TRUE.\n\n**Statement B**: \"CSR strengths are associated with a statistically significant reduction in firm value, as measured by Tobin's Q.\"\n- Table 2 shows CSR strengths coefficient = -0.044 with t-statistic = -2.89 (statistically significant)\n- The negative coefficient indicates CSR strengths reduce firm value\n- This statement is TRUE.\n\n**Statement C**: \"The findings suggest that CSR concerns, unlike CSR strengths, are positively and significantly associated with firm value.\"\n- Table 2 shows CSR concerns coefficient = 0.009 with t-statistic = 0.47 (not statistically significant)\n- While the association is positive, it is NOT statistically significant\n- This statement is FALSE.\n\n**Statement D**: \"The analysis shows that independent boards cause a statistically significant increase in CSR concerns.\"\n- Table 1 shows CSR concerns coefficient = 0.080 with t-statistic = 0.53 (not statistically significant)\n- The increase is not statistically significant\n- This statement is FALSE.\n\nA,B", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 285, "Question": "### Background\n\nThe paper argues that organizational capabilities like manufacturing flexibility are akin to real options. Traditional Discounted Cash Flow (DCF) analysis, which relies on a single expected path of future cash flows, systematically undervalues such investments because it fails to capture the value of managerial flexibility to adapt to future uncertainty.\n\n### Data / Model Specification\n\nAt `t=0`, a firm can build one of two plants:\n- **Dedicated Plant:** Cost `I_D = 90`. Can only produce Product A.\n- **Flexible Plant:** Cost `I_F = 100`. Can produce Product A or B.\n\nAt `t=1`, the value of producing each product, `V_A` and `V_B`, is revealed. The flexible plant, initially set to make A, can be switched to make B at a cost of `C_S = 10`. The risk-free discount factor is 0.9 (i.e., `1/(1+r)`).\n\n### Question\n\nAccording to a real options framework, which of the following statements are valid critiques or conclusions regarding the valuation of these investments?", "Options": {"A": "If at t=1, `V_A = 120` and `V_B = 125`, the firm should exercise its option and switch production to B.", "B": "An increase in the future uncertainty (volatility) of `V_A` and `V_B` decreases the value of the flexible plant, as it makes planning more difficult.", "C": "The value of the flexibility option embedded in the flexible plant is equivalent to the present value of a European option to exchange asset A for asset B with a strike price of 10.", "D": "A traditional DCF analysis, focused on the expected value of the primary product (A), would favor the dedicated plant over the flexible plant."}, "Answer": ["C", "D"], "pi_justification": "This item uses a 'Scenario Application' strategy to test the paper's core argument about valuing flexibility as a real option. Option A is correct: a static DCF sees the same expected revenue stream but a higher cost for the flexible plant (`100 > 90`), so it would always choose the dedicated plant. Option B is correct: the option's payoff at `t=1` is `max(0, V_B - V_A - C_S)`, which is the definition of an exchange option with a strike price equal to the switching cost (`C_S=10`). Distractor C is a 'Step-Omission Error'; the firm only switches if `V_B - C_S > V_A`. Here, `125 - 10 = 115`, which is less than `V_A = 120`, so the firm should *not* switch. Distractor D is a 'Conceptual Opposite' error; option value increases with volatility, so higher uncertainty makes the flexibility more, not less, valuable.", "qid": "285", "question": "### Background\n\nThe paper argues that organizational capabilities like manufacturing flexibility are akin to real options. Traditional Discounted Cash Flow (DCF) analysis, which relies on a single expected path of future cash flows, systematically undervalues such investments because it fails to capture the value of managerial flexibility to adapt to future uncertainty.\n\n### Data / Model Specification\n\nAt `t=0`, a firm can build one of two plants:\n- **Dedicated Plant:** Cost `I_D = 90`. Can only produce Product A.\n- **Flexible Plant:** Cost `I_F = 100`. Can produce Product A or B.\n\nAt `t=1`, the value of producing each product, `V_A` and `V_B`, is revealed. The flexible plant, initially set to make A, can be switched to make B at a cost of `C_S = 10`. The risk-free discount factor is 0.9 (i.e., `1/(1+r)`).\n\n### Question\n\nAccording to a real options framework, which of the following statements are valid critiques or conclusions regarding the valuation of these investments?", "model_evaluation_result": {"model_raw_response": "Looking at each statement:\n\nA) If at t=1, V_A = 120 and V_B = 125, the firm should exercise its option and switch production to B.\n\nThe firm will switch if V_B - C_S > V_A\n125 - 10 = 115 < 120\nSince 115 < 120, the firm should NOT switch. Statement A is FALSE.\n\nB) An increase in the future uncertainty (volatility) of V_A and V_B decreases the value of the flexible plant, as it makes planning more difficult.\n\nThis contradicts a fundamental principle of option theory. Higher volatility increases option value because it increases the probability of favorable outcomes while losses are limited. The flexible plant contains an embedded option, so higher uncertainty increases its value. Statement B is FALSE.\n\nC) The value of the flexibility option embedded in the flexible plant is equivalent to the present value of a European option to exchange asset A for asset B with a strike price of 10.\n\nThe flexible plant gives the right to switch from producing A to producing B by paying C_S = 10. This is exactly an exchange option where you give up asset A plus 10 to receive asset B. The payoff at t=1 is max(V_B - V_A - 10, 0), which matches the payoff of an exchange option with strike price 10. Statement C is TRUE.\n\nD) A traditional DCF analysis, focused on the expected value of the primary product (A), would favor the dedicated plant over the flexible plant.\n\nTraditional DCF would compare:\n- Dedicated plant: -90 + 0.9 × E[V_A]\n- Flexible plant: -100 + 0.9 × E[V_A]\n\nSince both produce the same expected cash flows from product A, but the flexible plant costs 10 more, traditional DCF would favor the dedicated plant because it ignores the value of the switching option. Statement D is TRUE.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 394, "Question": "### Background\n\n**Research Question.** Why are Seasoned Equity Offerings (SEOs) typically underpriced, resulting in a positive average return from the offer price to the closing price on the offer day (`R_1 > 0`)?\n\n**Setting.** One prominent explanation is the 'winner's curse' hypothesis, which is based on information asymmetry between informed and uninformed investors. Uninformed investors know they are at an informational disadvantage and will only participate in offerings if they expect to make a non-negative return on average.\n\n---\n\n### Question\n\nThe 'winner's curse' is a key concept in information asymmetry models used to explain SEO underpricing. Select all statements that accurately describe the logic or implications of the winner's curse.", "Options": {"A": "The problem is most severe in markets with no information asymmetry, as all investors face the same risk of overpaying.", "B": "The winner's curse implies that it is the *informed* investors who systematically lose money in SEOs because they are cursed with knowing too much.", "C": "To prevent uninformed investors from systematically losing money and withdrawing from the market, issues must be underpriced on average.", "D": "It arises because uninformed investors tend to receive larger allocations of overpriced ('bad') issues and smaller, rationed allocations of underpriced ('good') issues."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item tests the conceptual understanding of the winner's curse, a core theoretical underpinning of the paper. It uses an **Atomic Decomposition** strategy to break the theory into its core premise and its main implication. Correct options (A, B) accurately describe these two components. Distractor C is a **Conceptual Opposite**, misidentifying who is cursed. Distractor D is also a **Conceptual Opposite**, as the curse requires information asymmetry to exist.", "qid": "394", "question": "### Background\n\n**Research Question.** Why are Seasoned Equity Offerings (SEOs) typically underpriced, resulting in a positive average return from the offer price to the closing price on the offer day (`R_1 > 0`)?\n\n**Setting.** One prominent explanation is the 'winner's curse' hypothesis, which is based on information asymmetry between informed and uninformed investors. Uninformed investors know they are at an informational disadvantage and will only participate in offerings if they expect to make a non-negative return on average.\n\n---\n\n### Question\n\nThe 'winner's curse' is a key concept in information asymmetry models used to explain SEO underpricing. Select all statements that accurately describe the logic or implications of the winner's curse.", "model_evaluation_result": {"model_raw_response": "Looking at each statement about the winner's curse in the context of SEO underpricing:\n\nA) \"The problem is most severe in markets with no information asymmetry, as all investors face the same risk of overpaying.\"\nThis is incorrect. The winner's curse specifically arises FROM information asymmetry. Without information asymmetry, there would be no distinction between informed and uninformed investors, and the winner's curse wouldn't exist.\n\nB) \"The winner's curse implies that it is the *informed* investors who systematically lose money in SEOs because they are cursed with knowing too much.\"\nThis is incorrect. The winner's curse affects UNINFORMED investors, not informed ones. Informed investors use their superior information to avoid overpriced offerings and participate more heavily in underpriced ones.\n\nC) \"To prevent uninformed investors from systematically losing money and withdrawing from the market, issues must be underpriced on average.\"\nThis is correct. This captures the core logic of how the winner's curse leads to underpricing. If offerings weren't underpriced on average, uninformed investors would expect negative returns and exit the market.\n\nD) \"It arises because uninformed investors tend to receive larger allocations of overpriced ('bad') issues and smaller, rationed allocations of underpriced ('good') issues.\"\nThis is correct. This describes the mechanism of the winner's curse - informed investors crowd out uninformed investors in good deals but avoid bad deals, leaving uninformed investors with adversely selected allocations.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 430, "Question": "### Background\n\nThe paper develops an integrated expenditure equation that combines financing from internal, non-financial receipts with financing from external, market-determined funds. This model is designed to provide a more complete picture of spending decisions.\n\n### Data / Model Specification\n\nThe proposed integrated expenditure equation is:\n\n  \nN^{ij} = {}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j} + DN^{ij} + LN^{ij} + CN^{ij} + MN^{ij} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `N^{ij}`: Expenditure on product `i` by sector `j`.\n- `{}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j}`: Spending financed by internal receipts (determined by the fixed-coefficient circular flow model).\n- `DN^{ij} + LN^{ij} + CN^{ij} + MN^{ij}`: Spending financed by external funds from equity (D), other asset (L), bank loan (C), and money (M) markets (determined by the financial market model).\n\n---\n\nConsider a scenario where a central bank implements a contractionary monetary policy by selling government securities, which drains reserves from the banking system. According to the logic of the paper's integrated model, which of the following are likely consequences of this policy action?\n\nSelect all that apply.", "Options": {"A": "The overall real expenditure, `N^{ij}`, would be guaranteed to fall by the exact amount that `CN^{ij}` decreases, as all sources of funds are perfect substitutes.", "B": "The bank loan flow term, `CN^{ij}`, would decrease as the supply of bank credit is constrained by the lack of reserves.", "C": "Firms and households seeking to maintain spending levels might attempt to compensate for the reduction in bank loans by increasing their reliance on other financial sources, potentially increasing flows like `LN^{ij}` (selling other assets) or `MN^{ij}` (dishoarding).", "D": "The internal funds component, `{}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j}`, would remain unaffected in the short run, as it depends on past receipts, not current financial market conditions."}, "Answer": ["B", "C", "D"], "pi_justification": "Assessment Target: This item tests the user's understanding of the structure of the integrated expenditure model and their ability to trace the transmission of a monetary policy shock through its components.\nDepth Strategy: Scenario Application. The user must apply the theoretical model to a novel micro-case (a specific monetary policy action) and predict its effects.\nDistractor Logic:\n- A (Correct): This identifies the most direct and primary transmission channel of a reserve-draining operation, which is the constraint on bank lending.\n- B (Correct): This correctly distinguishes between the external finance component, which is immediately affected by market conditions, and the internal finance component, which is based on a sector's own receipts and is therefore more backward-looking.\n- C (Correct): This demonstrates a deeper understanding of the model by acknowledging that agents will attempt to substitute between different sources of external finance in response to a shock in one market.\n- D (Conceptual Distractor - Overstatement/Conceptual Opposite): This is incorrect because it makes two extreme claims: a guaranteed 1-for-1 effect and perfect substitutability. The paper's empirical section suggests substitutability is imperfect and varies by expenditure type. The attempt to substitute (as in option C) does not mean the substitution will be complete or costless, so total expenditure is still expected to fall, but not necessarily by the exact amount of the initial shock to bank loans.", "qid": "430", "question": "### Background\n\nThe paper develops an integrated expenditure equation that combines financing from internal, non-financial receipts with financing from external, market-determined funds. This model is designed to provide a more complete picture of spending decisions.\n\n### Data / Model Specification\n\nThe proposed integrated expenditure equation is:\n\n  \nN^{ij} = {}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j} + DN^{ij} + LN^{ij} + CN^{ij} + MN^{ij} \\quad \\text{(Eq. (1))}\n \n\nWhere:\n- `N^{ij}`: Expenditure on product `i` by sector `j`.\n- `{}_{NN}α \\cdot N_{j} + {}_{EN}α \\cdot E_{j}`: Spending financed by internal receipts (determined by the fixed-coefficient circular flow model).\n- `DN^{ij} + LN^{ij} + CN^{ij} + MN^{ij}`: Spending financed by external funds from equity (D), other asset (L), bank loan (C), and money (M) markets (determined by the financial market model).\n\n---\n\nConsider a scenario where a central bank implements a contractionary monetary policy by selling government securities, which drains reserves from the banking system. According to the logic of the paper's integrated model, which of the following are likely consequences of this policy action?\n\nSelect all that apply.", "model_evaluation_result": {"model_raw_response": "Looking at this integrated expenditure equation and the contractionary monetary policy scenario, I need to analyze each option:\n\n**Option A**: Claims perfect substitutability and exact one-to-one reduction. This is incorrect because:\n- Different funding sources are not perfect substitutes (they have different costs, availability, and constraints)\n- The model shows multiple funding sources that can partially offset each other\n- Real expenditure changes depend on the net effect across all funding sources\n\n**Option B**: Bank loan flow (CN^ij) would decrease. This is correct because:\n- Contractionary policy drains reserves from the banking system\n- Reduced reserves constrain banks' ability to create new loans\n- This directly reduces the bank credit available for expenditure financing\n\n**Option C**: Firms might compensate through other financial sources. This is correct because:\n- The model explicitly includes multiple external funding sources\n- When one source (bank loans) becomes constrained, economic agents naturally seek alternatives\n- LN^ij (selling assets) and MN^ij (using existing money balances) are viable substitutes\n- This is a standard portfolio rebalancing response\n\n**Option D**: Internal funds remain unaffected in the short run. This is correct because:\n- Internal funds ({}_{NN}α · N_j + {}_{EN}α · E_j) depend on past receipts from the circular flow\n- These are predetermined by previous economic activity\n- Monetary policy affects financial markets but doesn't immediately change past receipts\n- Only over time would reduced expenditure affect future internal receipts\n\nB,C,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 221, "Question": "### Background\n\n**Research Question.** How can the differential impact of corporate governance on the risk and performance of Islamic banks, relative to conventional banks, be empirically identified and interpreted through the lens of financial theory?\n\n**Setting.** A panel data regression analysis is conducted to explain bank risk-taking (`Log_Z`) and performance (`ROA`). The study's key insight is that the effect of a corporate governance index (`CGI`) may differ between Islamic and conventional banks.\n\n### Data / Model Specification\n\nThe primary regression model for a dependent variable `Y` (either `Log_Z` or `ROA`) is:\n\n  \nY_{i,t} = \\alpha_0 + \\alpha_1 Islamic_i + \\beta_1 CGI_{i,t-1} + \\beta_2 (Islamic_i \\times CGI_{i,t-1}) + \\text{Controls} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Selected Regression Results (from paper's Tables 4 & 5)**\n| Dependent Variable | `Islamic` (`α₁`) | `CGI` (`β₁`) | `Islamic × CGI` (`β₂`) | `STDROA` |\n| :--- | :---: | :---: | :---: | :---: |\n| `Log_Z` (Risk) | 0.446 (0.346) | 0.105* (0.057) | -0.808** (0.343) | Not in model |\n| `ROA` (Performance) | -0.029** (0.014) | -0.002 (0.008) | 0.070*** (0.026) | Positive & Significant*** |\n*Notes: Robust standard errors in parentheses. *** p<0.01, ** p<0.05, * p<0.10. `Log_Z` is a measure of bank stability (higher value means lower risk). `STDROA` is a proxy for bank risk.*\n\n### Question\n\nBased on the regression model, empirical results, and principles of corporate finance, select all of the following statements that are correct.", "Options": {"A": "For an Islamic bank, a one-unit increase in the Corporate Governance Index (CGI) is associated with a calculated decrease of 0.703 in the Log_Z score, implying higher insolvency risk.", "B": "For a conventional bank, the marginal effect of CGI on the Log_Z score is -0.808, indicating that governance significantly increases its risk.", "C": "The positive and significant coefficient on `STDROA` in the `ROA` regression implies that risk-taking by Islamic banks is generally uncompensated.", "D": "The finding that better-governed Islamic banks take more risk is consistent with the Merton (1977) model, where shareholder value can be increased by raising asset volatility (`σ`) due to the convex payoff from the deposit insurance guarantee."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to calculate and interpret marginal effects from an interaction model and connect empirical findings to foundational corporate finance theory. It uses a combination of Computational Judgment and Scenario Application strategies. Correct options (A, B) require both calculation (A: 0.105 - 0.808) and theoretical synthesis (B). Distractor C reflects a common error of misinterpreting an interaction coefficient as a main effect (Formula Misuse). Distractor D presents a Conceptual Opposite regarding risk compensation.", "qid": "221", "question": "### Background\n\n**Research Question.** How can the differential impact of corporate governance on the risk and performance of Islamic banks, relative to conventional banks, be empirically identified and interpreted through the lens of financial theory?\n\n**Setting.** A panel data regression analysis is conducted to explain bank risk-taking (`Log_Z`) and performance (`ROA`). The study's key insight is that the effect of a corporate governance index (`CGI`) may differ between Islamic and conventional banks.\n\n### Data / Model Specification\n\nThe primary regression model for a dependent variable `Y` (either `Log_Z` or `ROA`) is:\n\n  \nY_{i,t} = \\alpha_0 + \\alpha_1 Islamic_i + \\beta_1 CGI_{i,t-1} + \\beta_2 (Islamic_i \\times CGI_{i,t-1}) + \\text{Controls} + \\varepsilon_{i,t} \\quad \\text{(Eq. (1))}\n \n\n**Table 1: Selected Regression Results (from paper's Tables 4 & 5)**\n| Dependent Variable | `Islamic` (`α₁`) | `CGI` (`β₁`) | `Islamic × CGI` (`β₂`) | `STDROA` |\n| :--- | :---: | :---: | :---: | :---: |\n| `Log_Z` (Risk) | 0.446 (0.346) | 0.105* (0.057) | -0.808** (0.343) | Not in model |\n| `ROA` (Performance) | -0.029** (0.014) | -0.002 (0.008) | 0.070*** (0.026) | Positive & Significant*** |\n*Notes: Robust standard errors in parentheses. *** p<0.01, ** p<0.05, * p<0.10. `Log_Z` is a measure of bank stability (higher value means lower risk). `STDROA` is a proxy for bank risk.*\n\n### Question\n\nBased on the regression model, empirical results, and principles of corporate finance, select all of the following statements that are correct.", "model_evaluation_result": {"model_raw_response": "Looking at this regression analysis, I need to evaluate each statement based on the model specification and results.\n\n**Statement A**: For an Islamic bank (Islamic = 1), the marginal effect of CGI on Log_Z is:\nβ₁ + β₂ = 0.105 + (-0.808) = -0.703\n\nSince Log_Z is a stability measure (higher = lower risk), a decrease of 0.703 implies higher insolvency risk. This is correct.\n\n**Statement B**: For a conventional bank (Islamic = 0), the marginal effect of CGI on Log_Z is simply β₁ = 0.105 (positive, not -0.808). The statement incorrectly claims the effect is -0.808. This is incorrect.\n\n**Statement C**: The positive and significant coefficient on STDROA in the ROA regression indicates a positive relationship between risk and return. This suggests risk-taking is compensated with higher returns, not uncompensated. This is incorrect.\n\n**Statement D**: The Merton (1977) model shows that with deposit insurance, bank equity can be viewed as a call option on assets. Due to the convex payoff structure, shareholders benefit from increased asset volatility. The finding that better-governed Islamic banks take more risk (lower Log_Z) while achieving higher returns (positive effect on ROA) is consistent with this model - governance may enable strategic risk-taking that exploits the option value of deposit insurance. This is correct.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 274, "Question": "### Background\n\n**Research Question.** What institutional factors explain the cross-country variation in the value relevance of bank financial statements, and can these empirical relationships be interpreted causally?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage regression approach using a sample of 813 banks from 38 countries over the period 1993-2004. In the first stage, bank-specific value relevance is estimated. In the second stage, a single cross-sectional regression is run to explain the variation in these estimates.\n\n### Data / Model Specification\n\nIn the second stage, the Extent of Value Relevance (`EVR`), proxied by the coefficient on book value from the first-stage regressions, is modeled as a function of country-level and bank-specific factors:\n\n  \n\\mathrm{EVR}_{i} = a_{0} + b_{1} \\text{Transparency}_i + b_{2} \\text{Legal Environment}_i + b_{3} \\text{Accounting Cluster}_i + ... + \\varepsilon_{i}\n \n\nKey independent variables are defined as:\n*   `EVR`: The bank-specific coefficient on book value from a first-stage regression of market value on book value.\n*   `Legal Environment`: An indicator variable equal to 1 for common law systems and 0 for code law systems.\n*   `Transparency`: A country-level index (CIFAR) measuring the extent of mandatory financial disclosure.\n*   `Accounting Cluster`: An indicator variable equal to 1 for the British-American accounting cluster and 0 otherwise.\n\n**Table 1: Second-Stage Regression Results (Dependent Variable = EVR)**\n\n| Correlated Factor     | Coefficient | P-value |\n| :-------------------- | :---------- | :------ |\n| Transparency          | 0.045       | 0.001   |\n| Corporate environment | 0.042       | 0.011   |\n| Legal environment     | 0.041       | 0.037   |\n| Accounting cluster    | 0.114       | 0.001   |\n\n*Source: Adapted from Table 5, Panel A of the paper.*\n\n**Table 2: Correlation Matrix Snippet**\n\n|                     | Legal Env. | Transparency | Acct. Cluster |\n| :------------------ | :--------- | :----------- | :------------ |\n| **Legal Env.**      | 1.00       |              |               |\n| **Transparency**    | 0.40*      | 1.00         |               |\n| **Acct. Cluster**   | 0.48*      | 0.57*        | 1.00          |\n\n*Source: Adapted from Table 4 of the paper. * denotes significant correlation.*\n\n### Question\n\nBased on the provided regression and correlation results, select all of the following statements that are valid interpretations or policy conclusions.", "Options": {"A": "Since both `Legal Environment` and `Transparency` are statistically significant, a regulator should prioritize changing the legal system as it is a more fundamental institutional feature.", "B": "Holding other factors constant, banks in common law countries have a book value coefficient that is, on average, 0.041 higher than banks in code law countries, a difference that is statistically significant at the 5% level.", "C": "The coefficient on `Accounting Cluster` (0.114) is more than double that of `Legal Environment` (0.041), implying that accounting standards have a more than twice as large causal effect on value relevance.", "D": "For a regulator aiming to improve value relevance, focusing on increasing `Transparency` is a more practical policy lever than changing the `Legal Environment` because disclosure requirements are more readily modifiable than a country's entire legal tradition."}, "Answer": ["B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses the ability to interpret regression coefficients in a policy context. It uses an Atomic Decomposition strategy, breaking down the original QA's interpretation and policy application tasks into distinct, verifiable statements. Correct options require accurate reading of coefficient magnitude/significance (A) and sound reasoning about policy implementability (B). Distractor C tempts a common error of equating coefficient magnitude with causal importance, while D presents a conceptually opposite and impractical policy conclusion.", "qid": "274", "question": "### Background\n\n**Research Question.** What institutional factors explain the cross-country variation in the value relevance of bank financial statements, and can these empirical relationships be interpreted causally?\n\n**Setting / Data-Generating Environment.** The study employs a two-stage regression approach using a sample of 813 banks from 38 countries over the period 1993-2004. In the first stage, bank-specific value relevance is estimated. In the second stage, a single cross-sectional regression is run to explain the variation in these estimates.\n\n### Data / Model Specification\n\nIn the second stage, the Extent of Value Relevance (`EVR`), proxied by the coefficient on book value from the first-stage regressions, is modeled as a function of country-level and bank-specific factors:\n\n  \n\\mathrm{EVR}_{i} = a_{0} + b_{1} \\text{Transparency}_i + b_{2} \\text{Legal Environment}_i + b_{3} \\text{Accounting Cluster}_i + ... + \\varepsilon_{i}\n \n\nKey independent variables are defined as:\n*   `EVR`: The bank-specific coefficient on book value from a first-stage regression of market value on book value.\n*   `Legal Environment`: An indicator variable equal to 1 for common law systems and 0 for code law systems.\n*   `Transparency`: A country-level index (CIFAR) measuring the extent of mandatory financial disclosure.\n*   `Accounting Cluster`: An indicator variable equal to 1 for the British-American accounting cluster and 0 otherwise.\n\n**Table 1: Second-Stage Regression Results (Dependent Variable = EVR)**\n\n| Correlated Factor     | Coefficient | P-value |\n| :-------------------- | :---------- | :------ |\n| Transparency          | 0.045       | 0.001   |\n| Corporate environment | 0.042       | 0.011   |\n| Legal environment     | 0.041       | 0.037   |\n| Accounting cluster    | 0.114       | 0.001   |\n\n*Source: Adapted from Table 5, Panel A of the paper.*\n\n**Table 2: Correlation Matrix Snippet**\n\n|                     | Legal Env. | Transparency | Acct. Cluster |\n| :------------------ | :--------- | :----------- | :------------ |\n| **Legal Env.**      | 1.00       |              |               |\n| **Transparency**    | 0.40*      | 1.00         |               |\n| **Acct. Cluster**   | 0.48*      | 0.57*        | 1.00          |\n\n*Source: Adapted from Table 4 of the paper. * denotes significant correlation.*\n\n### Question\n\nBased on the provided regression and correlation results, select all of the following statements that are valid interpretations or policy conclusions.", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to evaluate each statement based on the regression results and correlation matrix provided.\n\nLet me analyze each option:\n\n**Option A**: \"Since both Legal Environment and Transparency are statistically significant, a regulator should prioritize changing the legal system as it is a more fundamental institutional feature.\"\n\nThis is not a valid interpretation. The regression shows associations, not causal effects. Additionally, changing an entire legal system is highly impractical and there's no evidence that being \"more fundamental\" makes it a better policy target.\n\n**Option B**: \"Holding other factors constant, banks in common law countries have a book value coefficient that is, on average, 0.041 higher than banks in code law countries, a difference that is statistically significant at the 5% level.\"\n\nThis is a valid interpretation. The coefficient is 0.041 with p-value = 0.037, which is < 0.05. Since Legal Environment = 1 for common law and 0 for code law, this correctly describes the regression result.\n\n**Option C**: \"The coefficient on Accounting Cluster (0.114) is more than double that of Legal Environment (0.041), implying that accounting standards have a more than twice as large causal effect on value relevance.\"\n\nThis is not valid. The study uses observational data with a cross-sectional regression. The high correlations between variables (0.48 between Legal Environment and Accounting Cluster) make it impossible to establish causal effects. The coefficients show associations, not causal impacts.\n\n**Option D**: \"For a regulator aiming to improve value relevance, focusing on increasing Transparency is a more practical policy lever than changing the Legal Environment because disclosure requirements are more readily modifiable than a country's entire legal tradition.\"\n\nThis is a valid policy conclusion. It makes a reasonable practical argument about policy implementation - disclosure requirements can indeed be modified more easily than changing an entire legal system. This statement doesn't claim causality but rather makes a practical observation about policy feasibility.\n\nTherefore, the valid interpretations are B and D.\n\nB,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 325, "Question": "## Background\n\n**Research Question.** This case examines the valuation of a large-scale privatization, Deutsche Telekom (DT), using the method of comparables, with a focus on the choice of valuation multiple and the limitations of the approach in a dynamic industry.\n\n**Setting.** In 1996, investment bankers are tasked with pricing the landmark $10 billion privatization of Deutsche Telekom. The company is a state-owned monopoly facing imminent deregulation and major capital expenditures. The valuation relies on comparing DT to other global telecommunications firms.\n\n---\n\n## Data / Model Specification\n\nThe valuation was based on applying a range of peer-group multiples to DT's financial metrics. DT's estimated cash flow per share (CFPS) was $2.60. The paper states that applying a PCF multiple range of 5x to 8x to DT's CFPS yielded a price range of $13 to $21. It also notes that Daimler Benz, another German firm, had 1995 earnings per share of -$11.05 but cash flow per share of $18.64.\n\n**Table 1: Comparables Valuation Data for Deutsche Telekom**\n\n| Company | Year | PCF Multiple |\n| :--- | :--- | :--- |\n| AT&T (U.S.) | 1995 | 6.4x |\n| BCE, Inc (Canada) | 1995 | 5.3x |\n| British Telecom (U.K.) | 1995 | 5.4x |\n| Sprint (U.S.) | 1995 | 5.1x |\n| PTT Nederland (Neth.) | 1996e | 5.1x |\n| Telmex (Mexico) | 1995 | 6.5x |\n\n---\n\n## Question\n\nBased on the provided background and data for the Deutsche Telekom (DT) privatization, select all statements that are correct regarding the valuation methodology and its rationale.", "Options": {"A": "The case of Daimler Benz, which had negative earnings but positive cash flow, illustrates that P/E multiples are generally more reliable than PCF multiples for firms undergoing restructuring.", "B": "The upper end of the valuation range (using an 8.0x PCF multiple) was directly supported by the median PCF multiple of the comparable firms listed in Table 1.", "C": "The valuation range of $13 to $21 per share for DT was calculated by applying a PCF multiple range of 5.0x to 8.0x to the company's estimated cash flow per share of $2.60.", "D": "Analysts likely preferred Price-to-Cash-Flow (PCF) multiples over Price-to-Earnings (P/E) multiples because DT's earnings were expected to be distorted by large non-cash charges and restructuring costs typical of a privatizing state-owned enterprise."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This item assesses understanding of valuation using comparables, specifically the choice of an appropriate multiple (PCF vs. P/E) in a special situation (privatization/restructuring). It uses a combination of Atomic Decomposition and Computational Judgment. Option A tests the conceptual rationale. Option B tests the ability to verify the calculation. Distractor C targets misinterpretation of the provided table data, while D presents a conceptual opposite regarding the lesson from the Daimler Benz example.", "qid": "325", "question": "## Background\n\n**Research Question.** This case examines the valuation of a large-scale privatization, Deutsche Telekom (DT), using the method of comparables, with a focus on the choice of valuation multiple and the limitations of the approach in a dynamic industry.\n\n**Setting.** In 1996, investment bankers are tasked with pricing the landmark $10 billion privatization of Deutsche Telekom. The company is a state-owned monopoly facing imminent deregulation and major capital expenditures. The valuation relies on comparing DT to other global telecommunications firms.\n\n---\n\n## Data / Model Specification\n\nThe valuation was based on applying a range of peer-group multiples to DT's financial metrics. DT's estimated cash flow per share (CFPS) was $2.60. The paper states that applying a PCF multiple range of 5x to 8x to DT's CFPS yielded a price range of $13 to $21. It also notes that Daimler Benz, another German firm, had 1995 earnings per share of -$11.05 but cash flow per share of $18.64.\n\n**Table 1: Comparables Valuation Data for Deutsche Telekom**\n\n| Company | Year | PCF Multiple |\n| :--- | :--- | :--- |\n| AT&T (U.S.) | 1995 | 6.4x |\n| BCE, Inc (Canada) | 1995 | 5.3x |\n| British Telecom (U.K.) | 1995 | 5.4x |\n| Sprint (U.S.) | 1995 | 5.1x |\n| PTT Nederland (Neth.) | 1996e | 5.1x |\n| Telmex (Mexico) | 1995 | 6.5x |\n\n---\n\n## Question\n\nBased on the provided background and data for the Deutsche Telekom (DT) privatization, select all statements that are correct regarding the valuation methodology and its rationale.", "model_evaluation_result": {"model_raw_response": "Looking at each option carefully:\n\nA) The case of Daimler Benz, which had negative earnings but positive cash flow, illustrates that P/E multiples are generally more reliable than PCF multiples for firms undergoing restructuring.\n\nThis is incorrect. Daimler Benz had negative EPS (-$11.05) but positive CFPS ($18.64). With negative earnings, P/E multiples cannot even be calculated meaningfully. This example actually demonstrates why PCF multiples are MORE reliable than P/E multiples for firms undergoing restructuring, not less reliable.\n\nB) The upper end of the valuation range (using an 8.0x PCF multiple) was directly supported by the median PCF multiple of the comparable firms listed in Table 1.\n\nLooking at the PCF multiples in Table 1: 6.4x, 5.3x, 5.4x, 5.1x, 5.1x, 6.5x\nArranging in order: 5.1x, 5.1x, 5.3x, 5.4x, 6.4x, 6.5x\nThe median is (5.3x + 5.4x) / 2 = 5.35x\nThis is nowhere near 8.0x, so this statement is incorrect.\n\nC) The valuation range of $13 to $21 per share for DT was calculated by applying a PCF multiple range of 5.0x to 8.0x to the company's estimated cash flow per share of $2.60.\n\nLet me verify: \n- $2.60 × 5.0 = $13.00 ✓\n- $2.60 × 8.0 = $20.80 ≈ $21.00 ✓\nThis matches the stated range, so this statement is correct.\n\nD) Analysts likely preferred Price-to-Cash-Flow (PCF) multiples over Price-to-Earnings (P/E) multiples because DT's earnings were expected to be distorted by large non-cash charges and restructuring costs typical of a privatizing state-owned enterprise.\n\nThis makes logical sense. The background mentions DT was a state", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 439, "Question": "### Background\n\n**Research Question.** Beyond estimating the average levels of risk, it is important to understand the relationship between different risk exposures. A key question is whether firms that are highly sensitive to systematic market risk (high \"market beta\") are also highly sensitive to exchange rate risk. A simple linear relationship might be expected, but the reality could be more complex due to factors like corporate risk management.\n\n**Setting / Data-Generating Environment.** A quantile regression framework is used to model the relationship between firms' estimated market betas (`\\hat{\\beta}_{im}^M`) and their estimated exchange rate betas (`\\hat{\\beta}_{is}`), which were generated from a prior regression analysis.\n\n### Data / Model Specification\n\nThe paper investigates the relationship between the two estimated risk exposures. The quantile regression technique models the `\\tau`-th conditional quantile of a dependent variable `y` given an independent variable `x`. Its objective function is:\n\n  \n\\min_{\\theta_0, \\theta_1} \\sum_{i=1}^{N} \\rho_{\\tau}(y_i - \\theta_0 - \\theta_1 x_i), \\quad \\text{where } \\rho_{\\tau}(u) = u(\\tau - I(u<0)) \\quad \\text{(Eq. (1))}\n \n\nHere, `y_i = \\hat{\\beta}_{is}` and `x_i = \\hat{\\beta}_{im}^M`. The term `I(u<0)` is an indicator function that is 1 if `u<0` and 0 otherwise. The paper's key finding is that the relationship is non-monotonic.\n\n### Question\n\nThe paper uses quantile regression to analyze the relationship between market risk (`\\hat{\\beta}_{im}^M`) and exchange rate risk (`\\hat{\\beta}_{is}`). Based on the provided information and the paper's findings, select all statements that are methodologically or empirically correct.", "Options": {"A": "The `\\rho_{\\tau}(u)` function in Eq. (1) works by minimizing the sum of squared residuals, which ensures the regression line fits the conditional mean of the data, similar to OLS.", "B": "The analysis shows a simple monotonic relationship, where firms with the highest market risk (`\\hat{\\beta}_{im}^M`) consistently exhibit the highest exchange rate risk (`\\hat{\\beta}_{is}`).", "C": "The key empirical finding is a non-monotonic, inverted U-shaped relationship: exchange rate sensitivity first increases with market sensitivity for defensive firms, then decreases for the most aggressive firms.", "D": "Quantile regression is preferred over OLS because it can model how the relationship between the two risk factors changes at different points of the conditional distribution, which is necessary to detect the non-monotonic pattern."}, "Answer": ["C", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.0). Score A=8, B=10. This item uses Atomic Decomposition to test a comprehensive understanding of the quantile regression analysis. It assesses the ability to recall the main empirical finding (A), understand the methodological rationale (B), and avoid common misconceptions about the technique's mechanics (C) and the empirical results (D). Distractors are based on classic Conceptual Opposites from econometrics and a direct contradiction of the paper's findings.", "qid": "439", "question": "### Background\n\n**Research Question.** Beyond estimating the average levels of risk, it is important to understand the relationship between different risk exposures. A key question is whether firms that are highly sensitive to systematic market risk (high \"market beta\") are also highly sensitive to exchange rate risk. A simple linear relationship might be expected, but the reality could be more complex due to factors like corporate risk management.\n\n**Setting / Data-Generating Environment.** A quantile regression framework is used to model the relationship between firms' estimated market betas (`\\hat{\\beta}_{im}^M`) and their estimated exchange rate betas (`\\hat{\\beta}_{is}`), which were generated from a prior regression analysis.\n\n### Data / Model Specification\n\nThe paper investigates the relationship between the two estimated risk exposures. The quantile regression technique models the `\\tau`-th conditional quantile of a dependent variable `y` given an independent variable `x`. Its objective function is:\n\n  \n\\min_{\\theta_0, \\theta_1} \\sum_{i=1}^{N} \\rho_{\\tau}(y_i - \\theta_0 - \\theta_1 x_i), \\quad \\text{where } \\rho_{\\tau}(u) = u(\\tau - I(u<0)) \\quad \\text{(Eq. (1))}\n \n\nHere, `y_i = \\hat{\\beta}_{is}` and `x_i = \\hat{\\beta}_{im}^M`. The term `I(u<0)` is an indicator function that is 1 if `u<0` and 0 otherwise. The paper's key finding is that the relationship is non-monotonic.\n\n### Question\n\nThe paper uses quantile regression to analyze the relationship between market risk (`\\hat{\\beta}_{im}^M`) and exchange rate risk (`\\hat{\\beta}_{is}`). Based on the provided information and the paper's findings, select all statements that are methodologically or empirically correct.", "model_evaluation_result": {"model_raw_response": "Looking at each option:\n\nA) This is incorrect. The ρ_τ(u) function in quantile regression does not minimize squared residuals. Instead, it uses an asymmetric loss function that weights positive and negative residuals differently depending on the quantile τ. This is fundamentally different from OLS, which minimizes squared residuals to fit the conditional mean.\n\nB) This is incorrect. The passage explicitly states that \"The paper's key finding is that the relationship is non-monotonic.\" This directly contradicts the claim of a simple monotonic relationship.\n\nC) This is correct. The passage states the relationship is non-monotonic, and this option describes a specific non-monotonic pattern (inverted U-shape) where exchange rate sensitivity first increases then decreases with market sensitivity. This is consistent with the paper's finding.\n\nD) This is correct. Quantile regression can model relationships at different points of the conditional distribution (different quantiles), not just the mean. This capability is essential for detecting non-monotonic patterns that might vary across the distribution, which OLS (focusing only on the conditional mean) might miss.\n\nC,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 345, "Question": "### Background\n\n**Research Question.** This case assesses the robustness of a trading strategy's profitability to transaction costs.\n\n**Setting and Sample.** A portfolio manager analyzes the `MADP(3)` strategy. The reduction in a portfolio's average return from trading is modeled as `CostDrag = c × Turnover`, where `c` is the transaction cost and `Turnover` is the average annual trade volume. This cost drag is the sole reason for the difference between the no-cost and 50bps-cost `Δ(cer)`.\n\n### Data / Model Specification\n\nThe economic value of a model is measured by the CER gain, `Δ(cer) = CER_{model} - CER_{benchmark}`.\n\n**Table 1: Economic Evaluation of the MADP(3) Strategy**\n\n| Strategy | `Δ(cer)` (%) (No cost) | `Δ(cer)` (%) (50bps cost) |\n|:---|---:|---:|\n| MADP(3) | 1.64 | 1.39 |\n\n\n### Question\n\nBased on the provided model and data for the `MADP(3)` strategy, select all of the following conclusions that are arithmetically correct.", "Options": {"A": "The breakeven transaction cost `c^*` that would make the net CER gain of the `MADP(3)` strategy equal to zero is 328 basis points.", "B": "If the strategy's turnover were 100% annually, its net `Δ(cer)` with a 50bps cost would be 0.82%.", "C": "The annual cost drag for the `MADP(3)` strategy at a 50bps cost is 1.39%.", "D": "The implied annual turnover of the `MADP(3)` strategy is 50%."}, "Answer": ["A", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Table QA rule). This is a **Computational Judgment** item. It requires the user to perform a multi-step calculation based on the provided data and model to verify quantitative statements. Option A requires calculating turnover from the cost drag (`(0.0164 - 0.0139) / 0.005`). Option B requires using the result from A to find the breakeven cost (`0.0164 / 0.5`). Distractors are based on misreading the table (C mistakes the net gain for the drag) and plausible calculation errors (D uses a simple but incorrect heuristic of halving the gross gain).", "qid": "345", "question": "### Background\n\n**Research Question.** This case assesses the robustness of a trading strategy's profitability to transaction costs.\n\n**Setting and Sample.** A portfolio manager analyzes the `MADP(3)` strategy. The reduction in a portfolio's average return from trading is modeled as `CostDrag = c × Turnover`, where `c` is the transaction cost and `Turnover` is the average annual trade volume. This cost drag is the sole reason for the difference between the no-cost and 50bps-cost `Δ(cer)`.\n\n### Data / Model Specification\n\nThe economic value of a model is measured by the CER gain, `Δ(cer) = CER_{model} - CER_{benchmark}`.\n\n**Table 1: Economic Evaluation of the MADP(3) Strategy**\n\n| Strategy | `Δ(cer)` (%) (No cost) | `Δ(cer)` (%) (50bps cost) |\n|:---|---:|---:|\n| MADP(3) | 1.64 | 1.39 |\n\n\n### Question\n\nBased on the provided model and data for the `MADP(3)` strategy, select all of the following conclusions that are arithmetically correct.", "model_evaluation_result": {"model_raw_response": "I need to analyze each option based on the given data and model.\n\nGiven information:\n- CostDrag = c × Turnover\n- Δ(cer) with no cost = 1.64%\n- Δ(cer) with 50bps cost = 1.39%\n- The cost drag is the sole reason for the difference between no-cost and 50bps-cost Δ(cer)\n\nFirst, let me calculate the cost drag:\nCost drag = Δ(cer) no cost - Δ(cer) with cost = 1.64% - 1.39% = 0.25%\n\nSince CostDrag = c × Turnover, and c = 50bps = 0.50%:\n0.25% = 0.50% × Turnover\nTurnover = 0.25% / 0.50% = 0.5 = 50%\n\nNow let me check each option:\n\nA) Breakeven transaction cost c* where Δ(cer) = 0:\nAt breakeven: Δ(cer) no cost - c* × Turnover = 0\n1.64% - c* × 50% = 0\nc* = 1.64% / 50% = 3.28% = 328 basis points\n✓ This is correct.\n\nB) If turnover were 100%, net Δ(cer) with 50bps cost:\nCost drag = 0.50% × 100% = 0.50%\nNet Δ(cer) = 1.64% - 0.50% = 1.14%\n✗ This gives 1.14%, not 0.82%.\n\nC) Annual cost drag at 50bps cost is 1.39%:\nWe calculated cost drag = 0.25%, not 1.39%\n✗ This is incorrect. 1.39% is the net Δ(cer), not the cost drag.\n\nD) Implied annual turnover is 50%:\nWe calculated Turnover = 50%\n✓ This is correct.\n\nA,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"ID": 412, "Question": "### Background\n\n**Research Question.** How does model uncertainty affect an agent's decision when a potential arbitrage opportunity exists under one model but not another?\n\n**Setting and Environment.** Consider a one-period market where the no-arbitrage condition is violated. There exists a portfolio `h^*` such that `h^*\\Delta S \\ge 0` `\\mathcal{P}`-q.s. The set of models `\\mathcal{P}` contains two specific measures:\n- `P_1`: An 'optimistic' model where `E_{P_1}[h^*\\Delta S] > 0`.\n- `P_2`: A 'pessimistic' model where `E_{P_2}[h^*\\Delta S] < 0`.\n\nAn agent with utility `U(w) = \\log(w)` and initial wealth `x` considers portfolios of the form `h = \\alpha h^*` for `\\alpha \\ge 0`.\n\n---\n\n### Data / Model Specification\n\nThe agent's problem is to choose `\\alpha` to solve the robust optimization problem:\n  \n\\sup_{\\alpha \\ge 0} \\inf_{P \\in \\{P_1, P_2\\}} E_P[\\log(x+\\alpha h^*\\Delta S)]\n \n\n---\n\n### Question\n\nBased on this robust optimization framework, select all correct conclusions about the agent's behavior and the properties of the problem.\n", "Options": {"A": "Under model `P_2`, the agent's expected utility `E_{P_2}[\\log(x+\\alpha h^*\\Delta S)]` is a decreasing function of `\\alpha` for `\\alpha > 0`.", "B": "If the agent only considered model `P_1`, their optimal strategy would be to choose an arbitrarily large `\\alpha` (i.e., `\\alpha \\to \\infty`).", "C": "The optimal strategy is a finite, non-zero `\\alpha^* > 0` that optimally balances the expected gain from `P_1` against the expected loss from `P_2`.", "D": "The presence of the pessimistic measure `P_2` in the robust framework leads to an optimal choice of `\\alpha^* = 0`."}, "Answer": ["A", "B", "D"], "pi_justification": "Judgment: REPLACE with Multiple Choice (Score: 9.5). This item tests the application of the robust (max-min) objective to a scenario where the core no-arbitrage assumption is violated. It uses a 'Scenario Application' strategy. The options are designed to decompose the reasoning via 'Atomic Decomposition': analyzing each model separately (A, B), and then synthesizing them under the `inf` operator (C). Distractor D presents a common but incorrect intuition ('Almost Right') about 'balancing' outcomes, which is not how the max-min criterion works, making it a high-fidelity distractor.", "qid": "412", "question": "### Background\n\n**Research Question.** How does model uncertainty affect an agent's decision when a potential arbitrage opportunity exists under one model but not another?\n\n**Setting and Environment.** Consider a one-period market where the no-arbitrage condition is violated. There exists a portfolio `h^*` such that `h^*\\Delta S \\ge 0` `\\mathcal{P}`-q.s. The set of models `\\mathcal{P}` contains two specific measures:\n- `P_1`: An 'optimistic' model where `E_{P_1}[h^*\\Delta S] > 0`.\n- `P_2`: A 'pessimistic' model where `E_{P_2}[h^*\\Delta S] < 0`.\n\nAn agent with utility `U(w) = \\log(w)` and initial wealth `x` considers portfolios of the form `h = \\alpha h^*` for `\\alpha \\ge 0`.\n\n---\n\n### Data / Model Specification\n\nThe agent's problem is to choose `\\alpha` to solve the robust optimization problem:\n  \n\\sup_{\\alpha \\ge 0} \\inf_{P \\in \\{P_1, P_2\\}} E_P[\\log(x+\\alpha h^*\\Delta S)]\n \n\n---\n\n### Question\n\nBased on this robust optimization framework, select all correct conclusions about the agent's behavior and the properties of the problem.\n", "model_evaluation_result": {"model_raw_response": "Looking at this robust optimization problem, I need to analyze each option carefully.\n\n**Option A**: Under model P₂, we have E_{P₂}[h*ΔS] < 0. For the logarithmic utility:\n- E_{P₂}[log(x + αh*ΔS)] where α > 0\n- As α increases, the term αh*ΔS becomes more negative on average under P₂\n- This reduces the argument of the log function, decreasing expected utility\n- Therefore, this is **correct**\n\n**Option B**: Under only model P₁ where E_{P₁}[h*ΔS] > 0:\n- As α → ∞, we'd have E_{P₁}[log(x + αh*ΔS)] → ∞\n- This would make α → ∞ optimal under P₁ alone\n- This is **correct**\n\n**Option C**: In the robust optimization sup_{α≥0} inf_{P∈{P₁,P₂}} E_P[log(x + αh*ΔS)]:\n- For any α > 0, the worst-case model is P₂ (gives lower utility)\n- So the problem reduces to maximizing E_{P₂}[log(x + αh*ΔS)]\n- Since this is decreasing in α (from Option A), the maximum occurs at α = 0\n- Therefore, there is no finite non-zero α* that balances gains and losses\n- This is **incorrect**\n\n**Option D**: Following the logic from Option C:\n- The robust optimization leads to choosing the smallest feasible α\n- Since α ≥ 0 and E_{P₂}[log(x + αh*ΔS)] is decreasing in α\n- The optimal choice is α* = 0\n- This is **correct**\n\nA,B,D", "model_answer": ["A", "B", "C", "D"], "error": null}}
