[
  {
    "ID": 1,
    "Question": "### Background\n\nA researcher is faced with a dataset where some observations are incomplete. The validity of different statistical methods depends on the assumed mechanism generating the missing data. The primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nUnder which of the following scenarios does the Missing at Random (MAR) assumption hold, but the more restrictive Missing Completely at Random (MCAR) assumption is violated?",
    "Options": {
      "A": "In a survey of income and education, individuals with lower reported education levels are more likely to refuse to answer the income question.",
      "B": "A researcher accidentally spills coffee on a random subset of survey forms, making the income data on them illegible.",
      "C": "In a survey of income, individuals with extremely high incomes are the most likely to refuse to report their income.",
      "D": "In a survey of income, the distribution of reported income is highly skewed to the right."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the crucial distinction between MCAR and MAR assumptions. It uses a Scenario Application strategy, requiring the test-taker to apply abstract definitions to concrete examples. The distractors are designed to probe for common confusions: B represents a pure MCAR case, C represents the more complex MNAR case (a key conceptual neighbor), and D is a true but irrelevant statement about data distribution.",
    "qid": "1",
    "model_evaluation_result": {
      "model_raw_response": "C) In the scenario described in option C, the probability of a response being missing (refusal to report income) is dependent on the observed variable (income level), which violates the MCAR assumption. However, it still satisfies the MAR assumption since the missingness depends only on the observed data (income).",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 2,
    "Question": "### Background\n\nCommon methods for handling missing data include:\n- **Listwise Deletion:** Discarding any observation with one or more missing values.\n- **Single Imputation:** Filling in each missing value with a single plausible value (e.g., a regression prediction) to create one complete dataset.\n- **Multiple Imputation (MI):** Creating *m* complete datasets by filling in missing values with *m* different plausible draws from a predictive distribution, analyzing each dataset, and then combining the results.\n\nThe primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nListwise deletion is a common but problematic method for handling missing data. Under which specific condition will it produce biased parameter estimates, and why is it always considered statistically inefficient?",
    "Options": {
      "A": "It produces bias when the data are Missing at Random (MAR) but not MCAR, and it is always inefficient because it discards information from partially complete cases.",
      "B": "It produces bias when the data are MCAR, and it is inefficient only when the data are MAR.",
      "C": "It is always unbiased as long as the missingness is ignorable (MAR), but it is inefficient because it requires complex calculations.",
      "D": "It produces bias by imputing incorrect values for missing data, and it is inefficient because it can only be used for linear models."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the two primary statistical failures of listwise deletion: the condition for bias and the reason for inefficiency. The distractors are designed to capture common errors: B reverses the conditions (Conceptual Opposite), C makes an incorrect claim about bias under MAR (Almost Right), and D confuses deletion with imputation (Conceptual Conflation).",
    "qid": "2",
    "model_evaluation_result": {
      "model_raw_response": "A) It is a single-letter answer. Here's the output:\n\nA",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 3,
    "Question": "### Background\n\nIn Multiple Imputation (MI), results from *m* analyses of imputed datasets are combined. The total variance ($T$) of the final point estimate is calculated to account for two sources of uncertainty:\n\n1.  **Within-Imputation Variance ($\\overline{U}$):** The average of the estimated variances from each of the *m* analyses. This captures standard **sampling variance**.\n2.  **Between-Imputation Variance ($B$):** The variance of the *m* point estimates across the datasets. This captures **imputation uncertainty**.\n\nThe total variance is given by:\n```latex\nT = \\overline{U} + \\left(1 + \\frac{1}{m}\\right)B \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nSingle imputation creates a complete dataset but yields invalid confidence intervals because it ignores imputation uncertainty. How does Multiple Imputation (MI), using the variance formula provided in Eq. (1), solve this specific problem?",
    "Options": {
      "A": "By using the between-imputation variance ($B$) to explicitly quantify the uncertainty introduced by the missing data, and adding it to the sampling variance ($\\overline{U}$) to calculate the total variance ($T$).",
      "B": "By using the within-imputation variance ($\\overline{U}$) to measure the uncertainty from missing data, while the between-imputation variance ($B$) captures sampling error.",
      "C": "By averaging the point estimates across multiple datasets, which produces a more accurate point estimate than any single imputation.",
      "D": "By creating multiple datasets, which increases the overall sample size and statistical power compared to listwise deletion."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the core inferential advantage of Multiple Imputation over Single Imputation. It uses a Reverse-Reasoning strategy, asking how MI's formal machinery solves a known problem. The distractors are designed based on common errors: B reverses the roles of the variance components (Formula Misuse), C states a true but tangential benefit of MI (Almost Right), and D confuses the benefits of MI over listwise deletion with its specific advantage over single imputation.",
    "qid": "3",
    "model_evaluation_result": {
      "model_raw_response": "A\n\nNote: The answer is A) By using...",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 4,
    "Question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant. According to the paper's theory, the data are marginally uninformative about `\\nu = F(\\lambda|\\psi)`, where `F` is the CDF corresponding to the prior in Eq. (2).\n\n### Question\n\nWhich of the following quantities is a one-to-one function of `\\nu`, implying that all identifying information for it comes entirely from the prior specification?",
    "Options": {
      "A": "The reduced form coefficient on the instrument, `\\pi_2`.",
      "B": "The structural coefficient on the instrument, `\\pi_1 - \\lambda\\pi_2`.",
      "C": "The structural parameter of interest, `\\lambda`.",
      "D": "The instrument's noncentrality parameter, `\\pi_2^2 x'x / \\omega_{22}`."
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: Tests the ability to identify the specific parametric function for which the data are uninformative in the simultaneous equations model. Strategy: Reverse-Reasoning. The user must derive the transformation from `\\nu` to the options. Distractor Logic: A) is a directly identified parameter (Conceptual Opposite). C) is the ultimate parameter of interest, but not the direct transformation of `\\nu` (Almost Right). D) is a component of the prior variance, not the uninformative quantity itself (True but Irrelevant).",
    "qid": "4",
    "model_evaluation_result": {
      "model_raw_response": "C) Thestructural parameter of interest,\\lambda.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 5,
    "Question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant.\n\n### Question\n\nThe prior variance in Eq. (2) is inversely proportional to the instrument's noncentrality parameter, `\\pi_2^2 x'x / \\omega_{22}`. What is the primary justification for this choice, and what does it imply if a researcher observes that the instrument `x_t` is very weak (i.e., `\\pi_2` is close to zero)?",
    "Options": {
      "A": "Justification: Prior uncertainty about the exclusion restriction should decrease as the instrument becomes stronger. Implication: A weak instrument implies the prior on `\\lambda` should be very diffuse (high variance).",
      "B": "Justification: To ensure the posterior distribution is proper. Implication: A weak instrument implies the prior on `\\lambda` must be very concentrated (low variance).",
      "C": "Justification: Prior uncertainty about the exclusion restriction should increase as the instrument becomes stronger. Implication: A weak instrument implies the prior on `\\lambda` should be very concentrated (low variance).",
      "D": "Justification: Computational convenience in the Gibbs sampler. Implication: A weak instrument implies the posterior for `\\lambda` will be improper."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the deep logic behind the prior specification, linking instrument strength to prior uncertainty. Strategy: Scenario Application. The user must apply the logic to the scenario of a weak instrument. Distractor Logic: B) has an incorrect justification and a conceptually opposite implication. C) has a conceptually opposite justification and implication. D) offers a plausible but incorrect justification (computational convenience) and an incorrect implication.",
    "qid": "5",
    "model_evaluation_result": {
      "model_raw_response": "C) Justifiable. If a researcher observes a weak instrument (`\\pi_1` is close zero), the prior on `λ` should be more concentrated (low-variance) to counteract the increased uncertainty about the structural parameter due to the weak instrument.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 6,
    "Question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`. This posterior is constructed by averaging a conditional prior `f(\\lambda|\\psi)` over the posterior of the identified parameters, `f(\\psi|y)`.\n\n### Question\n\nWhat is a key advantage of the Bayesian posterior `f(\\lambda|y)` that is absent from the simple reporting of the estimated set `\\Lambda(\\hat{\\psi})`?",
    "Options": {
      "A": "The Bayesian posterior provides a single point estimate for `\\lambda`, completely resolving the identification problem.",
      "B": "The Bayesian posterior is always uniform over the bounds `\\Lambda(\\hat{\\psi})`, providing a clear non-informative benchmark.",
      "C": "The Bayesian posterior coherently integrates the statistical uncertainty about the identified parameters `\\psi` and provides a probabilistic weighting of values for `\\lambda`.",
      "D": "The Bayesian posterior's support is always narrower than the bounds `\\Lambda(\\hat{\\psi})`, providing more precise inference."
    },
    "Answer": "C",
    "pi_justification": "Assessment Target: Tests the central conceptual difference between a Bayesian posterior and a frequentist estimated set in a partially identified model. Strategy: Conceptual Judgment. Distractor Logic: A) is a Conceptual Opposite; the Bayesian approach does not resolve the identification problem. B) is a common misconception; the posterior is generally not uniform. D) is incorrect; the marginal posterior support is typically the entire parameter space (e.g., [0,1]), which is wider than the conditional bounds.",
    "qid": "6",
    "model_evaluation_result": {
      "model_raw_response": "C) The Bayesien posterior coherely integrates the statistical uncerainty about the identified paramters psi and provides a probablistic weighting of values of lambda.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 7,
    "Question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`.\n\n### Question\n\nAs censoring disappears (the probability of observation `\\psi_1 \\to 1`), the model becomes point-identified. In this limit, the posterior probability that `\\lambda` falls within the estimated set `\\Lambda(\\hat{\\psi})` approaches zero. Why does a 95% Bayesian Highest Posterior Density (HPD) interval not suffer from this issue?",
    "Options": {
      "A": "Because the HPD interval is constructed to contain 95% of the posterior mass by definition, while `\\Lambda(\\hat{\\psi})` is an estimate of a set whose width shrinks to zero.",
      "B": "Because the Bayesian HPD interval's width is fixed by the prior and does not shrink as the data become more informative.",
      "C": "Because the Bayesian HPD interval always converges to the full parameter space, `[0,1]`, as more data is collected.",
      "D": "Because the Bayesian analysis uses a prior that prevents the probability `\\psi_1` from ever reaching 1."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests understanding of the fundamental difference between a Bayesian credible interval and a frequentist set estimate in the limit of point-identification. Strategy: Reverse-Reasoning. The user must explain why one method avoids a paradox. Distractor Logic: B) is a Conceptual Opposite; HPD intervals shrink with more information. C) is also a Conceptual Opposite; they converge to a point, not the whole space. D) describes a possible prior choice but is not the fundamental reason the HPD interval behaves correctly.",
    "qid": "7",
    "model_evaluation_result": {
      "model_raw_response": "B) Because Bayesian HPD intervals' width is fixed and does not change as the data becomes more informative.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 8,
    "Question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), the implied prior correlation between `\\psi` and `\\lambda` is:\n```latex\nCorr(\\psi, \\lambda) = \\frac{1}{\\sqrt{1 + \\tau^2/\\omega^2}} \\quad \\text{(Eq. 1)}\n```\nMarginal learning about the non-identified parameter `\\lambda` is possible if and only if `\\psi` and `\\lambda` are a priori dependent (i.e., their correlation is non-zero).\n\n### Question\n\nWhat is the effect of using an improper prior for `\\beta` (i.e., letting `\\tau^2 \\to \\infty`), and what is the direct consequence for learning about `\\lambda`?",
    "Options": {
      "A": "The prior correlation `Corr(\\psi, \\lambda)` goes to 1, which maximizes marginal learning about `\\lambda`.",
      "B": "The prior correlation `Corr(\\psi, \\lambda)` goes to 0, which has no effect on learning about `\\lambda` as that is determined by the likelihood.",
      "C": "The prior correlation `Corr(\\psi, \\lambda)` becomes undefined, which makes the joint posterior for `(\\psi, \\lambda)` improper.",
      "D": "The prior correlation `Corr(\\psi, \\lambda)` goes to 0, which makes `\\psi` and `\\lambda` a priori independent and prevents any marginal learning about `\\lambda`."
    },
    "Answer": "D",
    "pi_justification": "Assessment Target: Tests the ability to calculate the effect of an improper prior on the key prior dependence and understand its consequence for learning. Strategy: Computational Judgment. The user must evaluate a limit and connect it to the theory. Distractor Logic: A) is the Conceptual Opposite, describing the case where `\\omega^2 \\to \\infty`. B) correctly evaluates the limit but draws an incorrect conclusion about its consequence. C) describes a plausible but incorrect outcome.",
    "qid": "8",
    "model_evaluation_result": {
      "model_raw_response": "D) The priors are improper, so the joint posterior distribution will also be improper, and there will be no marginal learning about the unidentifiable parameter `\\lambda`. This is because the prior distribution for `\\beta`, being improper, does not provide any information about `\\lambda`, and therefore the data cannot learn anything about `\\lambda'.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 9,
    "Question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), marginal learning about the non-identified parameter `\\lambda` is possible because `\\psi` and `\\lambda` are a priori dependent. We consider two scenarios with improper priors.\n\n### Question\n\nA researcher argues for using an improper prior on `\\beta` (`\\tau^2 \\to \\infty`) to be \"non-informative.\" Which statement best critiques this reasoning and correctly contrasts it with using an improper prior on `\\lambda` (`\\omega^2 \\to \\infty`) instead?",
    "Options": {
      "A": "The prior is not non-informative; it dogmatically asserts `\\psi` and `\\lambda` are independent, preventing learning about `\\lambda`. In contrast, an improper prior on `\\lambda` asserts perfect dependence, maximizing learning about `\\lambda`.",
      "B": "The prior is indeed non-informative, allowing the data to speak freely about both `\\beta` and `\\lambda`. An improper prior on `\\lambda` would have the same effect.",
      "C": "The prior is not non-informative because it prevents learning about `\\lambda`. An improper prior on `\\lambda` would also prevent learning about `\\lambda`, but for different reasons.",
      "D": "The prior is not non-informative because it makes the posterior for `\\beta` improper. An improper prior on `\\lambda` is truly non-informative and allows learning about both parameters."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the subtle, paradoxical nature of \"non-informative\" priors in this context. Strategy: Scenario Application. The user must critique one scenario and contrast it with another. Distractor Logic: B) is incorrect on all points. C) has a partially correct critique but an incorrect contrast. D) offers an incorrect critique and an incorrect contrast. The correct answer requires understanding the precise and opposite effects of the two different improper prior specifications.",
    "qid": "9",
    "model_evaluation_result": {
      "model_raw_response": "C) The priors are not non-iniformative because they prevent learning about λ. An improper priors on λ would also prevent leaning about λ, but for different reason.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 10,
    "Question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, what specific linear combination of the regression coefficients are the data marginally uninformative about?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` has rank `K-1` because `X_2 = X_1d`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe identified parameter is `\\psi = \\beta_1 + d\\beta_2 = [I_{K-1}, d] \\beta`. The data are marginally uninformative for any parameter that is a priori independent of `\\psi`. The specific uninformative quantity is `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c = [d', -1]'`. Independence between `\\psi` and `\\alpha` is established by showing their prior covariance is zero.\n\n### Question\n\nWhich calculation correctly demonstrates the a priori independence of `\\psi` and `\\alpha`?",
    "Options": {
      "A": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot (\\underline{Q}^{-1}c)' = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot c'\\underline{Q}^{-1} \\neq 0`",
      "B": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot (c'\\underline{Q}^{-1})' = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot \\underline{Q}^{-1}c = [I_{K-1}, d]c = d-d=0`",
      "C": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot \\underline{Q}^{-1} \\cdot (c'\\underline{Q}^{-1})' = [I_{K-1}, d] \\cdot \\underline{Q}^{-1} \\cdot \\underline{Q}^{-1}c \\neq 0`",
      "D": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot c = d-d=0`, because the `\\underline{Q}` matrices are irrelevant."
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: Tests the core mathematical derivation for the uninformative quantity in the multicollinearity case. Strategy: Computational Judgment. The user must identify the correct algebraic steps. Distractor Logic: A) makes a mistake with the transpose rule for matrices. C) uses `Q⁻¹` instead of `Q` in the covariance formula (Formula Misuse). D) arrives at the right answer but omits the crucial `Q Q⁻¹ = I` cancellation, which is the heart of the proof (Step-Omission Error).",
    "qid": "10",
    "model_evaluation_result": {
      "model_raw_response": "B) ` Cov(\\psi, alpha) = [ I_{K- 1}, d ] * Q * (c'Q^(-1))' = [ I_(K-1), d ] *Q * Q^(-1)c = [ I_K-1, d ] c = d - d = 0`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 11,
    "Question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, how can a researcher's beliefs about what the data cannot inform be used to specify a prior distribution?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` and `X_2 = X_1`, so `\\beta` is a 2x1 vector. The identified parameter is `\\psi = \\beta_1 + \\beta_2`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe general theory states that the data are marginally uninformative about `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c` describes the collinearity. In this case, `d=1`, so `c = [1, -1]'`.\n\n### Question\n\nA researcher believes, based on economic theory, that this particular experiment will be completely uninformative about the *difference* in coefficients, `\\beta_1 - \\beta_2`. To align their prior `\\beta ~ N(\\underline{b}, \\underline{Q})` with this belief, what restriction must be placed on the prior covariance matrix `\\underline{Q} = \\begin{pmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix}`?",
    "Options": {
      "A": "The prior correlation between `\\beta_1` and `\\beta_2` must be zero (`\\rho=0`).",
      "B": "The prior variances of `\\beta_1` and `\\beta_2` must be equal (`\\sigma_1^2 = \\sigma_2^2`).",
      "C": "The prior correlation between `\\beta_1` and `\\beta_2` must be negative one (`\\rho=-1`).",
      "D": "The prior mean of `\\beta_1` must equal the prior mean of `\\beta_2`."
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: Tests the application of the general theory to a specific prior elicitation problem. Strategy: Scenario Application. The user must translate a high-level belief into a concrete mathematical restriction. Distractor Logic: A) and C) represent common but incorrect intuitions about the required prior structure. The belief `\\alpha \\propto \\beta_1 - \\beta_2` requires `c'Q⁻¹ \\propto [1, -1]`, which leads to `q¹¹=q²²` in the precision matrix, implying `σ₁²=σ₂²` in the covariance matrix. A common error is to assume `ρ=0` is the required condition.",
    "qid": "11",
    "model_evaluation_result": {
      "model_raw_response": "A\n\nNote: I'm just answering the question, without providing any additional explanation.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 12,
    "Question": "### Background\n\nIn the model, a group of heterogeneous individuals makes decisions via majority rule. The preferences of the `i`-th member of the group over two public goods, `g` and `f`, are given by:\n\n```latex\nW^{i} \\equiv E\\left\\{ \\sum_{t=1}^{2} \\left[ \\alpha^{i}u(g_{t}) + (1-\\alpha^{i})u(f_{t}) \\right] \\right\\} \\quad \\text{(Eq. 1)}\n```\n\nwhere `α^i` is the individual's preference parameter. This preference structure is crucial for the model's tractability.\n\n### Question\n\nIn this model, what is the primary role of the parameter `α^i` and which resulting property of the preference structure is crucial for applying the median voter theorem?\n",
    "Options": {
      "A": "It represents an individual's risk aversion, and the resulting concavity of preferences allows for a unique optimum.",
      "B": "It acts as a utility weight defining heterogeneous tastes, and the resulting single-crossing property of indifference curves ensures preferences are single-peaked.",
      "C": "It determines an individual's rate of time preference, and the resulting stationarity of preferences ensures that the median voter is stable over time.",
      "D": "It acts as a utility weight defining heterogeneous tastes, and the resulting convexity of the choice set allows for an interior solution."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the understanding of the model's microfoundations, specifically how individual preferences are defined and aggregated. It replaces a portion of a QA problem because the concepts are distinct and allow for high-fidelity distractors. The distractors test for common confusions between preference parameters (α^i), utility function properties (concavity), and the conditions for political aggregation (single-crossing).",
    "qid": "12",
    "model_evaluation_result": {
      "model_raw_response": "B) Itacts as a utilityweight defining heterogeneous tastes,and the resulting singlecrossing property ofindifference curves ensures preferencesare single-peaked.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 13,
    "Question": "### Background\n\nIn the model, the identity of the decisive median voter can change randomly from period to period due to shocks (`ε`) affecting voter participation or eligibility. The median voter's preference, `α^m`, is defined implicitly as the value that splits the voting population in half:\n\n```latex\n\\int_{0}^{\\alpha^{m}(\\varepsilon)} \\gamma(\\alpha, \\varepsilon) d\\alpha - \\frac{1}{2} = 0 \\quad \\text{(Eq. 1)}\n```\n\nwhere `γ(α, ε)` is the probability density function of preferences. The sensitivity of the median voter to a shock is given by:\n\n```latex\n\\frac{d\\alpha^m}{d\\varepsilon} = - \\frac{\\int_{0}^{\\alpha^{m}} \\gamma_{\\varepsilon}(\\alpha, \\varepsilon) d\\alpha}{\\gamma(\\alpha^m, \\varepsilon)} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAccording to the model, a political system is considered more unstable if the identity of the median voter (`α^m`) is highly sensitive to random shocks (`ε`). Given the formula for this sensitivity (Eq. 2), which characteristic of the voter preference distribution `γ(α)` would lead to the greatest political instability?",
    "Options": {
      "A": "A high density of voters at the median, `γ(α^m, ε)`.",
      "B": "A low density of voters at the median, `γ(α^m, ε)`.",
      "C": "A distribution that is unimodal and symmetric around `α = 0.5`.",
      "D": "A distribution where the total number of voters is very large."
    },
    "Answer": "B",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to assess a key testable implication of the model: the link between preference distribution and political instability. It replaces a QA sub-question because it targets a specific, sharp insight perfectly suited for a choice format. The distractors are designed based on a clear error matrix: Conceptual Opposite (A: high density implies stability), True but Irrelevant/Distracting (C: a unimodal distribution implies high median density and thus stability), and Irrelevant (D).",
    "qid": "13",
    "model_evaluation_result": {
      "model_raw_response": "B) A Low density of votersat the median, γ(αm, ε).",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 14,
    "Question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`. For GPRs, this is equivalent to `A_1(\\bar{c}, E) = ... = A_{|N|}(\\bar{c}, E)`. Let this common value be `A(\\bar{c}, E)`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n\n### Question\n\nA Generalized Proportional Rule (GPR) is refined by imposing two axioms: Efficiency (`∑_{i∈N} f_i(c, E) = E`) and Uniform Treatment of Uniforms (UTU), which implies the non-proportional component `A_i` is the same for all entities (`A_i(\\bar{c}, E) = A(\\bar{c}, E)`). Which of the following formulas correctly represents the resulting division rule `f_i(c, E)`?",
    "Options": {
      "A": "```latex\nf_i(c, E) = \\frac{E}{|N|}(1 - \\sum_{k \\in K} W_k(\\bar{c}, E)) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E\n```",
      "B": "```latex\nf_i(c, E) = (E - \\sum_{k \\in K} W_k(\\bar{c}, E) E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E\n```",
      "C": "```latex\nf_i(c, E) = \\frac{E}{|N|} + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} (W_k(\\bar{c}, E) - 1) E\n```",
      "D": "```latex\nf_i(c, E) = \\frac{E}{|N|}(1 - \\sum_{k \\in K} W_k(\\bar{c}, E)) + \\sum_{k \\in K} c_{ik} W_k(\\bar{c}, E) E\n```"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform the multi-step algebraic derivation from Corollary 2, a core task in the original QA problem. It uses a Computational Judgment strategy. The correct option is the unique functional form that results from correctly applying the Efficiency and UTU axioms to the GPR. Distractors are derived from plausible algebraic errors in the derivation, such as mishandling the summation over `|N|` entities (B), incorrectly solving the efficiency constraint (C), or omitting terms from the proportional component (D).",
    "qid": "14",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: I'm assuming that the notation `|N|$ denotes the number of entities, and `E` is the total amount to be divided.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 15,
    "Question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n4.  **Non-negativity:** `f_i(c, E) ≥ 0` for all `i`.\n\n### Question\n\nWhich set of axioms, when imposed on the General Proportional Rule (GPR), is sufficient to characterize the class of Proportional Rules, where the rule takes the form `f_i(c, E) = \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E` with `\\sum_{k \\in K} W_k(\\bar{c}, E) = 1`?",
    "Options": {
      "A": "No Award for Null and Efficiency.",
      "B": "Uniform Treatment of Uniforms and Efficiency.",
      "C": "No Award for Null and Non-negativity.",
      "D": "Efficiency and Non-negativity."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the axiomatic derivation of the Proportional Rule from the GPR, a task in the original QA problem. It uses a Reverse-Reasoning strategy, asking which set of axioms produces a known result. The correct option identifies the precise pair of axioms that force the two key constraints (`A_i=0` from NAN, and `∑W_k=1` from Efficiency). Distractors are other plausible combinations of axioms from the paper that lead to different or incomplete characterizations.",
    "qid": "15",
    "model_evaluation_result": {
      "model_raw_response": "D) Efficiency, Non-negavity. \n\n(Note: This is the correct answer, as efficiency ensures that the total allocation equals the endowment, and non-negativity ensures that each agent receives a non-negative share.)",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 16,
    "Question": "### Background\n\nAn analysis of Indian manufacturing firms produced estimates for an extended Cobb-Douglas production function. The goal is to understand the productivity effects of a firm's own R&D (`LNRND`) and industry-wide R&D spillovers (`LNSRDEXP`).\n\n### Data / Model Specification\n\nThe core log-linear production function is:\n```latex\n\\hat{y}_{i j t}=\\hat{a}_{i j t}+\\alpha\\hat{s}_{j t}+\\beta\\hat{r}_{i j t}+\\gamma\\hat{k}_{i j t}+\\delta\\hat{l}_{i j t}+\\hat{e}_{i j t}\n```\nwhere `y` is value-added, `s` is spillover R&D, `r` is own R&D, `k` is physical capital, and `l` is labor, all in logs.\n\nTwo models were estimated for the \"Overall Industry\": a standard OLS model (Table 1) and a model correcting for first-order serial correlation using a Cochrane-Orcutt transformation (Table 2).\n\n**Table 1: OLS Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.016 (3.80) |\n| LNSRDEXP ($\\alpha$) | 0.093 (8.01) |\n| DW | 0.794 |\n*t-statistics in parentheses.*\n\n**Table 2: Cochrane-Orcutt Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.009 (1.49) |\n| LNSRDEXP ($\\alpha$) | 0.055 (2.58) |\n| DW | 2.170 |\n*t-statistics in parentheses.*\n\n### Question\n\nComparing the results for own R&D (`LNRND`) in Table 1 and Table 2, what is the most accurate econometric explanation for the change in the coefficient's statistical significance?",
    "Options": {
      "A": "The OLS model in Table 1 was correctly specified, and the Cochrane-Orcutt procedure in Table 2 incorrectly reduced the coefficient's significance.",
      "B": "The OLS model in Table 1 suffered from negative serial correlation (DW < 2), which biased the t-statistic for `LNRND` downward. The correction fixed this.",
      "C": "The OLS model in Table 1 suffered from positive serial correlation, which biased the coefficient estimate for `LNRND` upward. The correction in Table 2 removed this bias, revealing a smaller, insignificant effect.",
      "D": "The change in significance is due to correcting for unobserved firm-specific heterogeneity, which was confounding the OLS results."
    },
    "Answer": "C",
    "pi_justification": "This item replaces part of a larger QA problem. It uses a Reverse-Reasoning strategy to assess a core concept: diagnosing and understanding the consequences of serial correlation. The correct answer (C) requires interpreting the Durbin-Watson statistic (0.794 indicates positive serial correlation) and knowing that this biases coefficients and standard errors, leading to spurious significance. Distractor A is a conceptual opposite. Distractor B correctly identifies the DW value but incorrectly states the direction of bias. Distractor D correctly identifies a potential problem (heterogeneity) but attributes the change to the wrong correction, testing if the student can distinguish between serial correlation and heterogeneity corrections.",
    "qid": "16",
    "model_evaluation_result": {
      "model_raw_response": "C) The OLs model in Table I suffered from positiveserial correlation, whichbiased the coefficient estimatefor LNRND upward. Thecorrection in Table  II removed thisbias, revealing a smalleer, insignificant effect.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 17,
    "Question": "### Background\n\nTo account for unobserved firm-specific heterogeneity (e.g., managerial skill), Fixed Effect (FE) and Random Effect (RE) models were estimated for a panel of Indian manufacturing firms. A Hausman test was conducted to choose between the two specifications.\n\n### Data / Model Specification\n\nThe table below presents FE and RE estimates for the \"Overall Industry\" after correcting for serial correlation. The Hausman test yielded a statistic of 53.19.\n\n**Table 1: Fixed vs. Random Effect Models**\n| Variables | Fixed Effect | Random Effect |\n|:---|---:|---:|\n| LNRND ($\\beta$) | 0.007 (1.54) | 0.010 (2.07) |\n| LNSRDEXP ($\\alpha$) | 0.101 (3.34) | 0.122 (5.64) |\n*t-statistics in parentheses.*\n\n### Question\n\nGiven the Hausman test statistic of 53.19 (which is highly significant), which statement provides the correct conclusion and justification?",
    "Options": {
      "A": "The Random Effect model is preferred because it is more efficient and the test fails to reject its validity.",
      "B": "The Fixed Effect model is preferred because the test rejects the null hypothesis that unobserved effects are uncorrelated with the regressors, implying the Random Effect model would be biased.",
      "C": "The Random Effect model is preferred because the test rejects the null hypothesis that the Fixed Effect model is consistent.",
      "D": "The Fixed Effect model is preferred because the coefficients are smaller, indicating they are more conservative and reliable."
    },
    "Answer": "B",
    "pi_justification": "This item replaces part of a larger QA problem and assesses the student's ability to apply a standard specification test. The strategy is Conceptual Judgment. The correct answer (B) requires knowing the null hypothesis of the Hausman test (no correlation between unobserved effects and regressors) and the implication of its rejection (RE is inconsistent, so FE is preferred). Distractor A correctly states that RE is more efficient but ignores the test result, making it 'True but Irrelevant'. Distractor C misstates the null hypothesis and conclusion. Distractor D offers a folk-theory justification ('smaller is better') that is not the basis for the statistical decision.",
    "qid": "17",
    "model_evaluation_result": {
      "model_raw_response": "B) The fixed effect model is preferred... etc.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 18,
    "Question": "### Background\n\nThe final preferred model for the **Light industry** was a Fixed Effect specification. In this industry, private sector R&D accounts for 80% of total industrial R&D expenditure.\n\n### Data / Model Specification\n\n**Table 1: Final Selected Model Estimates for the Light Industry**\n| Variables | Light Industry (Fixed Effect) |\n|:---|---:|\n| LNRND ($\\beta$) | 0.012 (1.16) |\n| LNSRDEXP ($\\alpha$) | 0.357 (5.34) |\n*t-statistics in parentheses.*\n\n### Question\n\nSuppose a policy induces a 10% increase in the R&D expenditures of *all private firms* in the Light industry. Based on the statistically significant effects in the model, what is the estimated total percentage increase in a representative firm's output?",
    "Options": {
      "A": "0.12%",
      "B": "2.86%",
      "C": "2.98%",
      "D": "3.57%"
    },
    "Answer": "B",
    "pi_justification": "This item replaces the most complex part of a larger QA problem, using a Computational Judgment strategy. It tests the ability to perform a multi-step policy counterfactual calculation. The correct answer (B) requires: (1) calculating the increase in the spillover stock (10% * 0.80 = 8%), (2) applying the spillover elasticity (0.357 * 8% = 2.856%), and (3) correctly ignoring the own-R&D effect because its coefficient is statistically insignificant (t=1.16). Distractor A is a Step-Omission Error (only calculates the private return and ignores significance). Distractor C is a Conceptual Error (adds the insignificant private return to the social return). Distractor D is a Calculation Error (forgets to weight the spillover increase by the 80% private share).",
    "qid": "18",
    "model_evaluation_result": {
      "model_raw_response": "B) 3% (2.86%)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 19,
    "Question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of testing the Melitz model's predictions, focusing on why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nThe theoretical change in productivity for a sector `s` in country 1 under the Melitz model is:\n\n```latex\nd\\ln\\Phi_{1s}=\\xi_{1s}d\\ln\\tau_{21s}-\\xi_{2s}d\\ln\\tau_{12s}-\\xi_{3s}d\\ln w_{1}\n```\n(Eq. 1)\n\nwhere `\\xi_{1s}` is the response to import costs `\\tau_{21s}`. Trefler's empirical DiD specification for Canadian productivity is:\n\n```latex\n\\Delta\\ln\\Phi_{s,t}^{C A}=\\beta^{C A}\\Delta\\tau_{s,t}^{C A}+\\beta^{U S}\\Delta\\tau_{s,t}^{U S}+\\gamma_{t}+\\text{Controls}_{st}+\\varepsilon_{st}\n```\n(Eq. 2)\n\nwhere `\\Delta\\tau_{s,t}^{C A}` represents the change in Canadian import tariffs. Trefler finds a statistically significant `\\hat{\\beta}^{CA} < 0`.\n\n### Question\n\nBased on the structurally symmetric Melitz model where the theoretical productivity response to import costs `\\xi_{1}` is positive, what does the model predict for the coefficient `\\beta^{CA}` in Trefler's regression, and why does his actual finding present a puzzle?",
    "Options": {
      "A": "The model predicts `\\beta^{CA} > 0` because productivity and import costs move in the same direction. Trefler's finding of `\\hat{\\beta}^{CA} < 0` is a puzzle because it implies productivity rose most where import tariffs were cut the most, the opposite of the prediction.",
      "B": "The model predicts `\\beta^{CA} < 0` because lower tariffs (`\\Delta\\tau < 0`) increase competition and productivity. Trefler's finding confirms this prediction, so there is no puzzle.",
      "C": "The model predicts `\\beta^{CA}` will be biased due to the wage term `d\\ln w_{1}`. The puzzle is that Trefler's time fixed effects did not adequately control for this general equilibrium shock.",
      "D": "The model predicts `\\beta^{CA} > 0`, but this is because higher tariffs (`\\Delta\\tau > 0`) protect domestic firms, raising their productivity. Trefler's finding of `\\hat{\\beta}^{CA} < 0` is a puzzle because it suggests protection hurts productivity."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to connect a theoretical parameter to its empirical counterpart in a regression context, which was the core of the original QA's first question. It uses a Reverse-Reasoning strategy. Distractor B represents the common but incorrect intuition that the paper refutes. Distractor C conflates the central puzzle with one of its potential explanations (omitted variables). Distractor D correctly identifies the predicted sign of the coefficient but provides an incorrect explanation for the underlying theoretical mechanism.",
    "qid": "19",
    "model_evaluation_result": {
      "model_raw_response": "D) The mode predicts `\\beta^CA > 0,` but this is becaus",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 20,
    "Question": "### Background\n\n**Research Question.** This problem investigates how violating the \"structural symmetry\" assumption in the Melitz model can lead to omitted variable bias, potentially explaining the puzzle of why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nIf coefficients are heterogeneous across sectors, the theoretical difference-in-differences (DiD) comparison for a unilateral import liberalization between a more-liberalized sector A and a less-liberalized sector B is:\n\n```latex\nd\\ln\\Phi_{1A}-d\\ln\\Phi_{1B}=\\xi_{1B}(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})+(\\xi_{1A}-\\xi_{1B})d\\ln\\tau_{21A}-(\\xi_{3A}-\\xi_{3B})d\\ln w_{1}\n```\n(Eq. 1)\n\n### Question\n\nAn econometrician tries to estimate the effect of import tariff cuts on productivity using a simple DiD regression of `(d\\ln\\Phi_{1A}-d\\ln\\Phi_{1B})` on `(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})`. Assume the direct tariff effect is homogeneous (`\\xi_{1A} = \\xi_{1B} = \\xi_1 > 0`), but sensitivity to wages is not. Consider a scenario where:\n1. Unilateral liberalization causes the general equilibrium wage to fall (`d\\ln w_1 < 0`).\n2. The largest tariff cuts (`d\\ln\\tau_{21A}` is most negative) were given to the most labor-intensive sectors, which are also most sensitive to wage changes (`\\xi_{3A} > \\xi_{3B}`).\n\nUnder this scenario, what is the nature of the omitted variable bias, and what is its implication for the estimated coefficient `\\hat{\\beta}`?",
    "Options": {
      "A": "The bias is negative. This could cause the estimated `\\hat{\\beta}` to be negative even if the true causal effect `\\xi_1` is positive, potentially resolving the puzzle.",
      "B": "The bias is positive. This would make the estimated `\\hat{\\beta}` even more positive than the true `\\xi_1`, deepening the puzzle.",
      "C": "The bias is negative, but this would only make the estimated `\\hat{\\beta}` smaller than the true `\\xi_1` while remaining positive, thus failing to resolve the puzzle.",
      "D": "There is no bias because the DiD specification perfectly controls for the wage shock `d\\ln w_1`."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Scenario Application strategy to test the ability to perform a multi-step omitted variable bias analysis, which was the core of the original QA's third question. The distractors are designed based on predictable errors: B results from a sign error in calculating the covariance term of the bias formula; C shows a misunderstanding of how a large bias can flip the sign of an estimate; D incorrectly applies the DiD logic when its key identifying assumption is explicitly violated.",
    "qid": "20",
    "model_evaluation_result": {
      "model_raw_response": "C) Thebias is negative, butt his would only make themestimated β smaller than the truexi1while remaining positive,thus failing to resolve thee puzzle.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 21,
    "Question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe paper proposes a simpler weight, `α₂`, which is derived under the assumption that the covariance between the micro and macro forecasts is zero. The formula is:\n```latex\nα₂ = \\frac{σ₂²}{(z_T'γ - a'X_Tβ)² + σ₁² + σ₂²}\n```\nwhere `σ₁²` and `σ₂²` are the variances of the macro and aggregated micro forecasts, respectively.\n\nIn the formula for the simplified weight `α₂`, what is the correct interpretation of the 'inconsistency' term `(z_T'γ - a'X_Tβ)²`?",
    "Options": {
      "A": "It measures the estimation uncertainty (variance) of the macro forecast.",
      "B": "It quantifies the systematic difference between the macro model's structure and the aggregated micro model's structure.",
      "C": "It reflects the degree of correlation between the micro and macro forecast errors.",
      "D": "It captures the portion of the micro forecast variance that is explained by the macro model."
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: Tests the core economic intuition behind the forecast combination formula, specifically the role of model inconsistency. Strategy: Scenario Application (interpreting a term within a formula). Distractor Logic: (A) Confuses the inconsistency term with the variance term `σ₁²`. (C) Is a Conceptual Opposite, as this formula is derived assuming zero correlation. (D) Is a plausible-sounding but incorrect statistical interpretation (Formula Misuse).",
    "qid": "21",
    "model_evaluation_result": {
      "model_raw_response": "B) It measures systematic difference between macro model's and aggregated micro model structure.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 22,
    "Question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe optimal weight `α₁` on the macro forecast `η̂_{T+1}^{(1)}` is chosen to minimize the mean squared error of the final, updated micro forecast. \n\nIn the paper's 'base scenario', the micro model is perfectly specified and macro data are exact aggregates of micro data. In this idealized case, the optimal weight `α₁` on the macro forecast is zero. What is the direct implication of `α₁ = 0`?",
    "Options": {
      "A": "The macro forecast is biased, while the micro forecast is unbiased.",
      "B": "The micro forecast is a sufficient statistic, containing all relevant information.",
      "C": "The micro and macro forecasts are perfectly correlated, making one redundant.",
      "D": "The variance of the macro forecast is infinitely large compared to the micro forecast."
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: Assesses understanding of the key theoretical conclusion from the idealized base case. Strategy: Reverse-Reasoning (given an outcome, `α₁=0`, infer the underlying condition). Distractor Logic: (A) Plausible but not the direct implication; both are unbiased in the base case. (C) Is Almost Right, but sufficiency is the more precise and fundamental concept. (D) Is a condition that would also lead to `α₁` approaching zero, but it is not the reason in this specific scenario (Conceptual Opposite).",
    "qid": "22",
    "model_evaluation_result": {
      "model_raw_response": "B) The micrforecast is a sufficient statitic, containing all relevent information.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 23,
    "Question": "### Background\n\n**Research Question.** This problem explores alternative approaches to combining micro and macro forecasts.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}`. The paper discusses two main methods:\n1.  A **two-step approach**: First, an optimal combined macro forecast `η̂_{T+1}` is created. Second, the micro forecast is updated to be consistent with this macro total.\n2.  A **one-step GLS approach**: This method frames the problem as a single regression model and finds the `y_{T+1}` that simultaneously best fits both the initial micro and macro forecasts.\n\n### Data / Model Specification\n\nThe one-step GLS approach is formulated as the regression problem:\n```latex\n\\begin{pmatrix} η̂_{T+1}^{(1)} \\\\ ŷ_{T+1}^{(2)} \\end{pmatrix} = \\begin{pmatrix} a' \\\\ I_p \\end{pmatrix} y_{T+1} + \\begin{pmatrix} ε₁ \\\\ ε₂ \\end{pmatrix}\n```\n\nWhat is the key conceptual difference between the one-step GLS approach and the two-step approach?",
    "Options": {
      "A": "The GLS approach is Bayesian, while the two-step approach is frequentist.",
      "B": "The GLS approach assumes forecast errors are normally distributed, while the two-step approach does not.",
      "C": "The GLS approach simultaneously balances deviations from both forecasts, while the two-step approach sequentially imposes the combined macro forecast as a hard constraint.",
      "D": "The GLS approach is only optimal if the forecasts are uncorrelated, while the two-step approach is always optimal."
    },
    "Answer": "C",
    "pi_justification": "Assessment Target: Tests the ability to distinguish between the two main theoretical approaches in the paper. Strategy: Conceptual Judgment. Distractor Logic: The distractors target common but incorrect ways of differentiating statistical models. (A) is an incorrect classification. (B) is an incorrect statement about assumptions, as normality is useful for both. (D) makes an incorrect claim about the optimality conditions of the two methods.",
    "qid": "23",
    "model_evaluation_result": {
      "model_raw_response": "C) The GSL approach simultaneously balances deviatons from both forecasts while the two step approach sequentially imposes combined macro forecast.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 24,
    "Question": "### Background\n\n**Research Question.** This problem explores an alternative, one-step GLS approach to combining micro and macro forecasts and its connection to Bayesian updating.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}` by combining two sources of information: a direct forecast `ŷ_{T+1}^{(2)}` and a forecast of a linear combination of `y_{T+1}`, `η̂_{T+1}^{(1)}`.\n\n### Data / Model Specification\n\nThe problem can be framed in Bayesian terms as follows:\n*   **Prior:** Our initial belief about `y_{T+1}` comes from the micro model: `y_{T+1} ∼ N_p(ŷ_{T+1}^{(2)}, Σ)`.\n*   **Likelihood:** The macro forecast is viewed as data providing a signal about `y_{T+1}`: `η̂_{T+1}^{(1)} | y_{T+1} ∼ N(a'y_{T+1}, σ₁²)`.\n\nThe posterior distribution is then derived by combining the prior and the likelihood using Bayes' rule.\n\nWhen framing the forecast combination problem in these Bayesian terms, how are the initial micro and macro forecasts used?",
    "Options": {
      "A": "The macro forecast serves as the prior belief, which is updated using the micro forecast as the data.",
      "B": "The micro forecast serves as the prior belief, which is updated using the macro forecast as the data.",
      "C": "Both forecasts are combined to form a joint likelihood function, with a flat, uninformative prior.",
      "D": "The micro forecast is the parameter to be estimated, and the macro forecast is the posterior mean."
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: Tests for a sharp understanding of how the Bayesian paradigm is applied in this context. Strategy: Scenario Application (applying Bayesian terminology). Distractor Logic: (A) is the classic Conceptual Opposite error of reversing the roles of prior and data/likelihood. (C) describes an alternative, valid Bayesian approach, but it is not the one specified in the problem stem (True but Irrelevant). (D) demonstrates a confusion of basic Bayesian terms.",
    "qid": "24",
    "model_evaluation_result": {
      "model_raw_response": "B) The micrforecast serves as theprior belief, whichis updated using the macforecast as the data.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 25,
    "Question": "### Background\n\n**Research Question.** This problem investigates how the Rational Expectations Hypothesis (REH) imposes testable cross-equation restrictions within a linear simultaneous equations model.\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\pi_1, \\pi_{1,0}^*$: Scalar reduced-form coefficients.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to lagged exogenous variables.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process. Under REH, the expectation $y_{1,t}^*$ is a linear function of the information set, which in this case simplifies to $y_{1,t}^* = \\psi_0 z_{t-1}$.\n\n---\n\n### Question\n\nWhat is the correct expression for the coefficient $\\psi_0$ that is consistent with the Rational Expectations Hypothesis?",
    "Options": {
      "A": "$\\psi_0 = \\frac{\\pi_1 A_1}{1 - \\pi_{1,0}^*}$",
      "B": "$\\psi_0 = \\frac{\\pi_1 A_1}{1 + \\pi_{1,0}^*}$",
      "C": "$\\psi_0 = \\pi_1 A_1$",
      "D": "$\\psi_0 = \\frac{\\pi_1}{1 - \\pi_{1,0}^*}$"
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item captures the core of the original QA's Q3(a). Assessment Target: Test the ability to apply the general theory of REH restrictions to a specific, simplified model. Strategy: Computational Judgment. Distractor Logic: (B) Sign Error in the denominator, a common algebraic slip. (C) Step-Omission Error, ignoring the feedback term from the expectation itself ($y_{1,t}^* \\pi_{1,0}^*$). (D) Formula Misuse, omitting the persistence parameter $A_1$ from the exogenous process forecast.",
    "qid": "25",
    "model_evaluation_result": {
      "model_raw_response": "A) $\\boxed{A}$ \n\n(Note: The answer is only the letter \"A\", without any additional text.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 26,
    "Question": "### Background\n\n**Research Question.** This problem investigates the economic intuition behind the cross-equation restrictions imposed by the Rational Expectations Hypothesis (REH).\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to the lagged exogenous variable $z_{t-1}$, such that $y_{1,t}^* = \\psi_0 z_{t-1}$.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error and $\\pi_1 > 0$. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process and $0 < A_1 < 1$.\n\n---\n\n### Question\n\nHow does the coefficient $\\psi_0$ change as the persistence of the exogenous process, $A_1$, increases, and what is the economic intuition for this change?",
    "Options": {
      "A": "$\\psi_0$ increases, because a more persistent process means $z_{t-1}$ is a stronger predictor of the future value $z_t$, so rational agents weigh it more heavily.",
      "B": "$\\psi_0$ decreases, because higher persistence in the exogenous process makes the endogenous feedback loop relatively less important.",
      "C": "$\\psi_0$ is unaffected, because $A_1$ is a parameter of the exogenous process and does not enter the structural equation for $y_{1,t}$.",
      "D": "$\\psi_0$ increases, because higher persistence implies greater future volatility, causing agents to react more strongly to any new information."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item captures the core of the original QA's Q3(b) and Q3(c). Assessment Target: Test the economic intuition behind the model's comparative statics. Strategy: Scenario Application. Distractor Logic: (A) is the correct answer, linking persistence to predictive power. (B) is a Conceptual Opposite, suggesting the wrong direction of change with a plausible-sounding but incorrect reason. (C) represents a common error of failing to see cross-equation linkages (True but Irrelevant logic). (D) is Almost Right in direction but provides an incorrect economic mechanism (confusing persistence with volatility).",
    "qid": "26",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: Only the letter of correct option is output.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 27,
    "Question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe contestant's preferences are represented by the CARA utility function:\n\n```latex\nU(W) = -e^{-\\alpha W}\n\\quad \\text{(Eq. 1)}\n```\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round, where `0.5 * stake \\le b \\le stake`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings (Distribution A) to the simulated distribution from a risk-neutral strategy (Distribution B).\n\n**Table 1. Data for Method 2 (Distributional Comparison)**\n\n| Distribution | Mean Winnings (\\$) | Std. Dev. of Winnings (\\$) |\n| :--- | :--- | :--- |\n| A: Actual Sample | `\\mu_A = 4,677` | `\\sigma_A = 4,258` |\n| B: Risk-Neutral Sim. | `\\mu_B = 6,987` | `\\sigma_B = 10,843` |\n\n### Question\n\nUsing the certainty equivalent approximation `CE ≈ μ - (α/2)σ²` and the data in Table 1, calculate the coefficient of absolute risk aversion (`α`) that would make a contestant indifferent between the actual sample distribution of winnings (Distribution A) and the simulated risk-neutral distribution (Distribution B).",
    "Options": {
      "A": "0.000047",
      "B": "0.000023",
      "C": "0.000310",
      "D": "0.701"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to apply a specific formula for risk aversion using data from a table. It uses a Computational Judgment strategy. The distractors are designed to catch common errors: B is a Step-Omission Error (omitting the '2' in the numerator of the rearranged formula); C is a Conceptual Error (confusing the result of Method 2 with the reported result of Method 1); D is a Formula Misuse error (using standard deviation instead of variance in the denominator).",
    "qid": "27",
    "model_evaluation_result": {
      "model_raw_response": "C) ۰٫۰۰۰۳۱۰\n\n(Note: I've written the answer in scientific notation as it was provided in the question)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 28,
    "Question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round. This method yields an average `\\hat{\\alpha} \\approx 0.000310`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings to the simulated distribution from a risk-neutral strategy. This method yields a much smaller `\\hat{\\alpha}`.\n\n### Question\n\nAccording to the paper's analysis, which of the following best explains why Method 1 (final-round bets) yields a much higher estimate of risk aversion (`\\alpha`) than Method 2 (distributional comparison)?",
    "Options": {
      "A": "The final-round bets in Method 1 are heavily censored (many contestants bet the minimum or maximum), which biases the `\\alpha` estimate upwards.",
      "B": "Contestants are more risk-loving in the final round due to the 'house money' effect, which should lead to a *lower* `\\alpha` from Method 1, making the results contradictory.",
      "C": "Method 1 measures behavior only in high-stake final rounds, while Method 2 averages behavior across all rounds where risk-taking is not constant but depends heavily on the current stake, a violation of the CARA model's core assumption.",
      "D": "The risk-neutral strategy in Method 2 is an unrealistic benchmark because it fails to account for contestants' ability to count cards."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the student's ability to synthesize findings and understand the paper's core critique of standard utility theory. It uses a Reverse-Reasoning strategy, asking for the cause of an observed empirical puzzle. Distractor A is 'True but Irrelevant,' as censoring is a technical issue but not the core behavioral explanation for the large discrepancy. Distractor B is a 'Conceptual Opposite,' misstating the implication of the house money effect in this context. Distractor D is 'Almost Right,' mentioning a real detail from the paper (card counting) but misattributing it as the primary cause for the discrepancy in `α` estimates.",
    "qid": "28",
    "model_evaluation_result": {
      "model_raw_response": "C) Method one measures behavior only high-stake finals rounds, while method two averages behavior across rounds where risk taking is not constant.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 29,
    "Question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Banking Sector:** The loan market is contestable, forcing banks to set the lowest feasible lending rate `R_p`. A bank will choose to initiate monitoring (`m > 0`) only if the marginal benefit of doing so exceeds the marginal cost. The marginal type `p*` is the entrepreneur type for whom a bank is indifferent between `m=0` and initiating monitoring.\n\n### Question\n\nGiven the quadratic cost function, what is the expression for the marginal entrepreneur type `p*` that separates the non-monitoring sector (`p ≤ p*`) from the monitoring sector (`p > p*`) in terms of the safe rate `R_0` and the cost parameter `γ`?",
    "Options": {
      "A": "`p* = 1 - sqrt(R_0/γ)`",
      "B": "`p* = 1 + sqrt(R_0/γ)`",
      "C": "`p* = 1 - (R_0/γ)`",
      "D": "`p* = 1 - sqrt(γ/R_0)`"
    },
    "Answer": "A",
    "pi_justification": "Assesses the student's ability to derive the key threshold for the extensive margin of risk. Strategy: Computational Judgment. Distractors are based on common algebraic errors: B (sign error on the square root), C (omitting the square root), and D (inverting the fraction within the root).",
    "qid": "29",
    "model_evaluation_result": {
      "model_raw_response": "B) `1 + sqrt(1/γ)*sqrt(R_0)` (Note: The correct answer is not among the provided options. The correct expression for `p* is actually `1 + (R_₀/γ)`, but it's not present in the options.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 30,
    "Question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Equilibrium Conditions:** For a given safe rate `R_0`, a marginal type `p* = 1 - sqrt(R_0/γ)` separates non-monitoring (`p ≤ p*`) from monitoring (`p > p*`) banks. For `p > p*`, banks choose an optimal monitoring intensity `m_p* > 0` determined by the first-order condition:\n  ```latex\nc''(m_p) - \\frac{R_0}{(1-p+m_p)^2} = 0 \\quad \\text{(Eq. 1)}\n  ```\n\n### Question\n\nUnder the quadratic cost assumption, what is the equilibrium probability of failure (`p - m_p*`) for a monitoring bank lending to an entrepreneur of type `p > p*`?",
    "Options": {
      "A": "`p*`",
      "B": "`p`",
      "C": "`0`",
      "D": "`p - p*`"
    },
    "Answer": "A",
    "pi_justification": "Assesses understanding of the model's key result for the intensive margin of risk. Strategy: Computational Judgment. Distractors represent common conceptual errors: B (confusing failure probability with the entrepreneur's baseline type), C (assuming monitoring eliminates all risk), and D (providing the formula for monitoring intensity `m_p*` instead of the failure probability `p - m_p*`).",
    "qid": "30",
    "model_evaluation_result": {
      "model_raw_response": "B) `B` (p) \n\nNote: This is based on the provided information, and it assumes that the model is correctly specified.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 31,
    "Question": "### Background\n\nAn agent evaluates a bet on the outcome of a two-stage draw. The first draw is from an unambiguous source, while the second is from an ambiguous source. The agent observes the second draw and then evaluates a bet on the first.\n\n*   State space `S = \\{rr, rg, gr, gg\\}`.\n*   `R_1 = \\{rr, rg\\}`: Event that first draw is red.\n*   `R_2 = \\{rr, gr\\}`: Event that second draw is red.\n*   `X = 1_{R_1}`: The random variable being evaluated (a bet on the first draw being red).\n*   `\\mathcal{P} = \\{R_2, G_2\\}`: The information partition, where `G_2` is the event the second draw is green.\n*   The agent's beliefs are captured by a totally monotone capacity `π` with the following non-zero Möbius transform values: `μ_π(rr) = μ_π(gg) = 1/6`, `μ_π(rg) = μ_π(gr) = 1/12`, `μ_π(R_1) = μ_π(G_1) = 1/4`.\n\n---\n\n### Data / Model Specification\n\nThe paper's proposed **proxy update** rule first forms a proxy capacity `π^\\mathcal{P}` and then updates it via `π(A|B) = π^\\mathcal{P}(A ∩ B) / π^\\mathcal{P}(B)`. The Möbius transform of the proxy is given by:\n\n```latex\n\\mu_{\\pi}^{\\mathcal{P}}(A) = \\sum_{B \\in \\mathcal{P}} \\sum_{\\{D : D \\cap B = A\\}} \\frac{|A|}{|D|} \\cdot \\mu_{\\pi}(D) \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nUsing the proxy updating rule, what is the conditional evaluation of the bet `X` after observing a red ball from the second draw, `E(X|R_2)`?",
    "Options": {
      "A": "7/12",
      "B": "5/12",
      "C": "1/3",
      "D": "1/2"
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: This question tests the student's ability to correctly apply the formula for the proxy update rule, the paper's core technical contribution. Strategy: Computational Judgment. The student must perform a multi-step calculation to arrive at a single numerical answer. Distractor Logic: (A) Correct answer. (B) `E(X|G_2)`, the value conditional on the other signal, testing if the student correctly identifies the conditioning event. (C) The result from the alternative prior-by-prior updating rule, testing for conceptual confusion between the two models. (D) The ex-ante value `E(X)`, testing if the student incorrectly assumes the information is uninformative.",
    "qid": "31",
    "model_evaluation_result": {
      "model_raw_response": "B) 1-1/12 = 5/6 * 1/1 = 5/(12)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 32,
    "Question": "### Background\n\nConsider an agent evaluating a prospect where information arrives sequentially. The agent's ex-ante evaluation is `E(X)`. After observing an event `B` from a partition `\\mathcal{P}`, the agent forms a conditional evaluation `E(X|B)`. For the specific example in the paper involving two draws, the ex-ante value of a bet `X` is `E(X) = 1/2`. When updating beliefs using the **prior-by-prior** rule, the conditional evaluations are `E^m(X|R_2) = 1/3` and `E^m(X|G_2) = 1/6`. Both values are strictly less than the ex-ante value, a phenomenon known as \"all news is bad news.\"\n\n---\n\n### Question\n\nWhat is the core reason that the prior-by-prior updating rule is susceptible to the \"all news is bad news\" phenomenon, while the paper's proposed proxy update rule is not?",
    "Options": {
      "A": "Prior-by-prior updating allows the agent to select a different worst-case prior for each possible information event, while proxy updating commits the agent to a single proxy belief structure before conditioning.",
      "B": "The proxy update rule always satisfies the law of iterated expectations, whereas the prior-by-prior rule does not.",
      "C": "Prior-by-prior updating is only defined for totally monotone capacities, while proxy updating works for any capacity.",
      "D": "The proxy update rule transforms the set of priors into a single Bayesian prior before updating, eliminating all ambiguity."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: This question assesses the student's understanding of the fundamental behavioral difference between the proxy update rule and its main alternative, prior-by-prior updating. Strategy: Conceptual distinction. The question asks for the 'why' behind a key difference. Distractor Logic: (A) Correct answer, capturing the re-optimization vs. commitment distinction. (B) Conceptual Opposite: Both rules can violate the law of iterated expectations; this is a central point of the paper. (C) Factual Error: Both rules are typically discussed in the context of totally monotone (or at least supermodular) capacities. (D) Almost Right: The proxy update makes the *partition elements* unambiguous, but it does not necessarily eliminate all ambiguity from the conditional beliefs.",
    "qid": "32",
    "model_evaluation_result": {
      "model_raw_response": "B) The proxyme update rule always satifies the law of itereated expectations, wheras the prior-by prior rule does not.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 33,
    "Question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nBased on the model and results in Table 1, which statement best describes the moderating effect of R&D intensity on the relationship between corruption and a firm's decision to invest?",
    "Options": {
      "A": "R&D-intensive firms are less deterred by corruption, likely due to stronger bargaining power.",
      "B": "R&D-intensive firms are more deterred by corruption because their complex operations are more exposed to bureaucratic hurdles.",
      "C": "Corruption uniformly deters all firms, and R&D intensity has a separate, independent negative effect on investment.",
      "D": "The positive interaction effect shows that R&D-intensive firms are more adept at using bribes to their advantage."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the conceptual understanding of an interaction effect, a key nuance in the paper's findings. It uses a Reverse-Reasoning strategy, asking for the best explanation of an observed statistical pattern. Distractors are designed to be plausible but incorrect: (B) is a plausible alternative mechanism (Conceptual Opposite), (C) ignores the interaction term entirely (Step-Omission Error), and (D) misinterprets the 'bargaining power' mechanism as active participation in corruption rather than resistance to it.",
    "qid": "33",
    "model_evaluation_result": {
      "model_raw_response": "A) R&A-intensive firms are least deterred by corrosion, likely due stronger bargaining power.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 34,
    "Question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nUsing the coefficient estimates in Table 1, at what threshold level of R&D intensity (R&D as a share of sales) does the marginal effect of corruption on the firm's investment propensity become zero?",
    "Options": {
      "A": "9.2%",
      "B": "17.6%",
      "C": "12.3%",
      "D": "The marginal effect is always negative, so no such threshold exists."
    },
    "Answer": "C",
    "pi_justification": "This item tests the ability to translate an econometric model with an interaction term into a precise quantitative prediction. It uses the Computational Judgment strategy. The correct answer requires setting the marginal effect equation ($\\beta_{Corruption} + \\beta_{Interaction} \\times R\\&D = 0$) to zero and solving. Distractors are based on common errors: (B) uses the wrong coefficient in the calculation (Formula Misuse), (A) is a plausible number derived from a different part of the paper (a subtle Formula Misuse), and (D) represents a conceptual failure to understand how a positive interaction term can offset a negative main effect.",
    "qid": "34",
    "model_evaluation_result": {
      "model_raw_response": "B) 12% (or 17.61%)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 35,
    "Question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to one of three groups: `Control`, `Price-Only`, and `Price+IHD`. The `Price+IHD` group was intended to receive an in-home display (IHD) providing real-time usage information. However, due to scheduling and other issues, there was significant non-compliance. While compliance was high in other groups, only 72% of households assigned to the `Price+IHD` group completed the study and had a functioning IHD installed.\n\nResearchers estimated the Intent-to-Treat (ITT) effect, which measures the causal effect of being *assigned* to a treatment group, regardless of compliance.\n\n### Data / Model Specification\n\nThe table below shows the ITT estimates from a difference-in-differences model with household and hour-by-day fixed effects. The dependent variable is the natural log of electricity usage, `ln(kWh)`.\n\n**Table 1: ITT Treatment Effects on ln(kWh) for All Events**\n\n| | Coefficient (Std. Err.) |\n| :--- | :--- |\n| Price-only | -0.038 (0.036) |\n| Price + IHD | -0.137*** (0.046) |\n\n*Notes: *** p<0.01.*\n\n---\n\n### Question\n\nGiven the results in Table 1 and the context of 72% compliance in the `Price+IHD` group, which statement most accurately interprets the Intent-to-Treat (ITT) estimate and its policy relevance?",
    "Options": {
      "A": "The ITT estimate of -0.137 shows the causal effect of the IHD on the 72% of households who complied, indicating that the device itself caused a 13.7% usage reduction for its users.",
      "B": "The ITT estimate of -0.137 is biased due to the 28% non-compliance rate and therefore does not represent a valid causal effect of the program.",
      "C": "The ITT estimate of -0.137 represents the average effect of the program on all households *assigned* to the `Price+IHD` group, making it the relevant parameter for a policymaker considering a large-scale rollout where similar non-compliance is expected.",
      "D": "The ITT estimate of -0.137 shows that being assigned to the `Price+IHD` group caused a 13.7 percentage point decrease in electricity usage, answering the question of the program's effect under perfect compliance."
    },
    "Answer": "C",
    "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.0). It assesses the core concept of interpreting an Intent-to-Treat (ITT) estimate in the presence of non-compliance. The strategy is conceptual interpretation. The correct answer (C) accurately defines ITT and its policy relevance. Distractors are designed based on common misconceptions: (A) incorrectly describes the Treatment-on-the-Treated (ToT) effect; (B) incorrectly claims ITT is biased by non-compliance (it is not biased as an estimate of the effect of *assignment*); (D) uses incorrect units ('percentage point') for a log-level model and incorrectly describes the policy question.",
    "qid": "35",
    "model_evaluation_result": {
      "model_raw_response": "C)󠁧󠁢󠁥󠁮󠁧",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 36,
    "Question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to the `Price+IHD` group, which provided real-time energy usage information. However, only 72% of assigned households ultimately complied and had a functioning device installed. The Intent-to-Treat (ITT) effect—the average effect on all *assigned* households—was estimated to be a 13.7% reduction in electricity usage (-0.137 in log terms).\n\nThe Treatment-on-the-Treated (ToT) effect measures the causal effect only on the sub-population of compliers. It can be calculated by scaling the ITT effect by the compliance rate.\n\n### Data / Model Specification\n\n- **ITT Estimate for `Price+IHD`:** -0.137\n- **Compliance Rate for `Price+IHD`:** 0.72\n\n---\n\n### Question\n\nUsing the provided data, what is the implied Treatment-on-the-Treated (ToT) estimate, and what is the economic intuition for its relationship to the ITT estimate?",
    "Options": {
      "A": "The ToT estimate is -0.099 (-0.137 * 0.72). Its magnitude is smaller than the ITT because it applies to a smaller, more selective group of households.",
      "B": "The ToT estimate is -0.190 (-0.137 / 0.72). Its magnitude is larger than the ITT because the ITT effect is diluted by including non-compliers who had a zero treatment effect.",
      "C": "The ToT estimate is -0.137, the same as the ITT. The estimate is valid for compliers because the initial randomization ensures the groups are comparable.",
      "D": "The ToT estimate is -0.190 (-0.137 / 0.72). Its magnitude is larger than the ITT because the compliers were likely more motivated to save energy from the start."
    },
    "Answer": "B",
    "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.0). It uses the 'Computational Judgment' strategy to test the mathematical and intuitive relationship between ITT and ToT. The correct answer (B) provides the correct calculation (scaling up the ITT by the compliance rate) and the correct intuition (ITT is a diluted average). Distractors are based on predictable errors: (A) uses the wrong formula (multiplication instead of division) and faulty intuition; (C) fails to make any adjustment; (D) arrives at the correct number but provides subtly incorrect intuition, attributing the difference to selection bias rather than the mechanical process of the Wald estimator.",
    "qid": "36",
    "model_evaluation_result": {
      "model_raw_response": "B) The Treatment-on-the-treated (ToT estimate is indeed -0.097 (-0.13 * 0,72). Its Magnitude is larger than ITT because ITT includes both compliers and non-compliers.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 37,
    "Question": "### Background\n\nResearchers conducted a randomized controlled trial (RCT) assigning households to `Control`, `Price-Only`, and `Price+IHD` groups. A check of pre-treatment characteristics revealed a statistically significant difference in average home size between the `Price-Only` and `Control` groups. Furthermore, the study experienced selective attrition: households on a time-of-use (TOU) rate plan were significantly more likely to drop out of the `Price+IHD` group than other households in that group.\n\n---\n\n### Question\n\nGiven the baseline imbalance and selective attrition, which statement correctly assesses the threats to the study's internal validity and the appropriate analytical approach?",
    "Options": {
      "A": "The baseline imbalance proves the randomization failed, and the selective attrition introduces bias; therefore, no causal claims can be made from the experiment.",
      "B": "The selective attrition is the primary threat, which can be solved by comparing outcomes only among the households who completed the study (compliers).",
      "C": "The baseline imbalance is a minor concern that can be addressed by including home size as a control variable, and the selective attrition is addressed by using Intent-to-Treat (ITT) analysis, which preserves the benefits of the original randomization.",
      "D": "The Intent-to-Treat (ITT) principle solves the problem of the baseline imbalance, but the selective attrition requires using a Treatment-on-the-Treated (ToT) analysis to obtain an unbiased estimate."
    },
    "Answer": "C",
    "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.5). It assesses the student's ability to correctly diagnose and propose remedies for two common threats to validity in an RCT: baseline imbalance and selective attrition. The strategy is 'Conceptual Judgment'. The correct answer (C) identifies the standard best practices. Distractors are based on classic misconceptions: (A) is overly pessimistic and dismisses valid remedies; (B) proposes an analysis (naive comparison of compliers) that is biased by selective attrition; (D) incorrectly maps the solutions to the problems (ITT addresses attrition's effect on the 'assignment' estimate, not baseline imbalance).",
    "qid": "37",
    "model_evaluation_result": {
      "model_raw_response": "C) The intent-to-treat principle solves the issue of the baseline balance, and the selectivity of attrition can be addressed using ITT analysis, which maintains the benefits of original randomization.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 38,
    "Question": "### Background\n\nIn an experiment on electricity demand, researchers exposed households to two types of high-price events:\n\n- **Day-Ahead (DA) Events:** Households received 24-hour advance notification of a moderate price increase (e.g., from ~$0.20/kWh to $0.70/kWh).\n- **30-Minute (TM) Events:** Households received 30-minute advance notification of a much larger price increase (e.g., from ~$0.20/kWh to $1.45/kWh).\n\n---\n\n### Question\n\nWhat is the primary behavioral trade-off that this experimental design choice—including both DA and TM events—allows the researchers to investigate?",
    "Options": {
      "A": "The difference between short-run and long-run price elasticity of demand.",
      "B": "Whether households respond more to price increases in the afternoon versus the evening.",
      "C": "The relative importance of the price signal's magnitude versus the household's ability to plan and overcome adjustment costs.",
      "D": "Whether households who own their homes are more price-sensitive than households who rent."
    },
    "Answer": "C",
    "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.5). It assesses the ability to interpret the logic behind a specific experimental design choice. The strategy is 'Conceptual Interpretation'. The correct answer (C) accurately captures the trade-off between the size of the incentive (price) and the constraints on action (time to plan). Distractors represent plausible but incorrect interpretations: (A) confuses within-day variation with the long-run/short-run distinction; (B) focuses on a secondary feature (timing) rather than the core design contrast; (D) describes a potential analysis of heterogeneity, not the primary purpose of the DA vs. TM variation.",
    "qid": "38",
    "model_evaluation_result": {
      "model_raw_response": "C) The primary behavioral trade-offs that this experimental designs allows the researchers to investigates is the relative importance of price signal's magnitudes versus the household ability to plan & overcome adjustment costs.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 39,
    "Question": "### Background\n\n**Research Question.** This problem investigates the dynamic adjustment of the interest rate to a monetary policy shock, testing the paper's proposition of short-run \"over-adjustment\" within the specific Miller-Orr model of money demand.\n\n**Setting / Institutional Environment.** We consider the Miller-Orr \"pure diffusion\" case for a representative individual's money holdings, where cash flows follow a random walk. The monetary authority contracts the money supply by a fraction `\\gamma` through open-market sales of bonds.\n\n### Data / Model Specification\n\nFor the Miller-Orr model with a lower barrier of 0, an upper barrier `h`, and a return point `z`:\n1.  **Optimal Policy:** The optimal return point is `z=h/3`.\n2.  **Average Holdings:** The average money holding is `M = 4h/9`.\n3.  **Over-adjustment Condition:** Over-adjustment of the interest rate occurs if the new long-run average money holding, `M' = 4h'/9`, is less than the amount of money remaining after the short-run adjustment, `(1-\\gamma)M`.\n4.  **Derived Result:** The paper shows that for the Miller-Orr case, the condition for over-adjustment reduces to the following polynomial inequality in `y = h'/h`:\n    ```latex\n    \\frac{1}{2}y^{3}-\\frac{1}{2}y^{2}-\\frac{1}{18}y+\\frac{1}{18}<0\n    ```\n    The roots of this polynomial are at `y=1` and `y=\\pm 1/3`.\n\n### Question\n\nGiven the model and the derived inequality, under what conditions will the interest rate over-adjust in the short run in response to a monetary contraction of size `\\gamma`?",
    "Options": {
      "A": "Over-adjustment occurs for monetary contractions `\\gamma` that are less than 2/3.",
      "B": "Over-adjustment occurs for monetary contractions `\\gamma` that are greater than 2/3.",
      "C": "Over-adjustment occurs for monetary contractions `\\gamma` that are less than 1/3.",
      "D": "Over-adjustment never occurs; the interest rate adjusts smoothly to its new long-run equilibrium."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to solve a mathematical condition and translate the result into an economic conclusion. It uses a Computational Judgment strategy. The correct answer requires solving a polynomial inequality for `y=h'/h` to find `y > 1/3`, and then correctly converting this into a condition on the monetary contraction `\\gamma` using `y = M'/M = (1-\\gamma_{LR})/(1)`. Distractors are based on common errors: (B) is a sign error in solving the inequality; (C) is a unit error, confusing the bound on `y` with the bound on `\\gamma`; (D) represents a plausible but incorrect conceptual belief about model dynamics.",
    "qid": "39",
    "model_evaluation_result": {
      "model_raw_response": "B) Overadjustment occurs for Monetary contractions γ that are greater-than 2/33",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 40,
    "Question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). An 'assignment' is a matrix, $D = (\\delta_{ij})$, where $\\delta_{ij}$ is the proportion of meetings between type-i workers and type-j jobs that result in a match. A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. An assignment is an equilibrium if the $\\delta_{ij}$ values are consistent with these private decisions.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nThe model considers three potential equilibrium assignments, with the 'Exact' assignment defined by $\\delta_{11}=\\delta_{22}=\\delta_{33}=1$ and all other $\\delta_{ij}=0$. Equilibrium reservation values for the 'Exact' and 'Mixed (2, 3)' assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Mixed (2, 3) | 0.323           | 0.870           | 2.313           |\n| Exact        | 0.301           | 0.904           | 2.410           |\n\n### Question\n\nIn the 'Exact' equilibrium, the model specifies that matches between type-2 workers and type-3 jobs are always rejected (i.e., $\\delta_{23}=0$). Is this rejection privately rational for the agents, and why?",
    "Options": {
      "A": "Yes, it is rational because the match output ($a_{23}=3.2$) is less than the sum of the agents' reservation values in the 'Exact' equilibrium ($w_{02}+r_{03}=3.314$).",
      "B": "No, it is irrational because the match output ($a_{23}=3.2$) is greater than the sum of reservation values calculated using the 'Mixed (2, 3)' equilibrium data ($w_{02}+r_{03}=3.183$).",
      "C": "No, it is irrational because the match output ($a_{23}=3.2$) is greater than the sum of the agents' reservation values ($w_{02}+r_{02}=1.808$).",
      "D": "Yes, it is rational because the 'Exact' assignment by definition requires that only diagonal matches ($\\delta_{ii}=1$) are formed."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to apply the model's core equilibrium condition. It uses a Computational Judgment strategy. Distractors are designed based on common errors: B (Lookup Error: using data from the wrong equilibrium), C (Calculation Error: using the wrong reservation value, r_02 instead of r_03), and D (Conceptual Error: confusing the definition of an assignment with the equilibrium condition).",
    "qid": "40",
    "model_evaluation_result": {
      "model_raw_response": "B) No. \n\n(Note: I only output the answer without explanation.)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 41,
    "Question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. The economy can be stuck in an inefficient equilibrium.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nEquilibrium reservation values for two potential assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Exact        | 0.301           | 0.904           | 2.410           |\n| Mixed (1, 2) | 0.293           | 0.870           | 2.581           |\n\n### Question\n\nSuppose the economy is in the 'Exact' equilibrium, where matches between type-1 workers and type-2 jobs are rejected. To shift the economy to the more efficient 'Mixed (1, 2)' equilibrium, a planner must incentivize this match. What is the minimum per-match subsidy required to make the (worker 1, job 2) match privately rational, based on the 'Exact' equilibrium reservation values?",
    "Options": {
      "A": "0.025",
      "B": "-0.025",
      "C": "0.017",
      "D": "0.114"
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to perform a policy-relevant calculation based on the model. It uses a Computational Judgment strategy. Distractors are designed based on plausible errors: B (Sign Error: reporting the private deficit instead of the subsidy needed to overcome it), C (Lookup Error: using reservation values from the target 'Mixed (1, 2)' equilibrium instead of the initial 'Exact' equilibrium), and D (Misapplication: calculating the subsidy for a different, incorrect match, such as (2,3)).",
    "qid": "41",
    "model_evaluation_result": {
      "model_raw_response": "C) 017\n\nNote: The answer is based on the given data and the question.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 42,
    "Question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers three potential equilibrium assignments: 'Mixed (2, 3)', 'Exact', and 'Mixed (1, 2)'. The 'Exact' assignment only allows matches between agents of the same type (e.g., worker 1 with job 1). The 'Mixed (1, 2)' assignment allows matches between types 1 and 2, in addition to same-type matches. Key outcomes are shown in the tables below.\n\n**Table 1: Unemployment Rates**\n| Assignment   | $u_1=v_1$ | $u_2=v_2$ | $u_3=v_3$ |\n| :----------- | :-------- | :-------- | :-------- |\n| Exact        | 0.375     | 0.375     | 0.375     |\n| Mixed (1, 2) | 0.250     | 0.250     | 0.333     |\n\n**Table 2: Aggregate Output**\n| Assignment   | Output, $\\pi$ |\n| :----------- | :------------- |\n| Exact        | 7.50           |\n| Mixed (1, 2) | 7.72           |\n\n### Question\n\nThe 'Mixed (1, 2)' equilibrium yields a higher total output ($\\pi=7.72$) than the 'Exact' equilibrium ($\\pi=7.50$). According to the model's logic, what is the primary reason for this efficiency gain?",
    "Options": {
      "A": "By allowing moderately productive matches between type-1 and type-2 agents, the search pool changes, which allows the highly productive type-3 agents to find each other and match more quickly, increasing the frequency of high-output ($a_{33}=8.0$) matches.",
      "B": "The output from new matches between type-1 and type-2 agents ($a_{12}=1.18$) is substantially greater than the output from the type-1 matches they replace ($a_{11}=1.0$), and this direct gain accounts for most of the difference.",
      "C": "The 'Mixed (1, 2)' equilibrium is more efficient because it forces type-3 workers to accept lower reservation wages, making them form matches more often.",
      "D": "The 'Mixed (1, 2)' equilibrium is more efficient simply because it has lower unemployment rates for a majority of worker types compared to the 'Exact' assignment."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to synthesize information from multiple tables to understand the model's main economic insight. It uses a Reverse-Reasoning strategy. Distractors are designed to test for common misinterpretations: B (True but Incomplete: focusing on a minor effect while ignoring the larger externality on type-3 agents), C (Factually Incorrect: contradicting data from the paper showing reservation wages for type 3 actually increase), and D (Oversimplification: ignoring the crucial compositional effects of matches, which are central to the paper's argument).",
    "qid": "42",
    "model_evaluation_result": {
      "model_raw_response": "D) The \"Mixed (1/2)\" equilibrium is more efficiency because it has a lower unemployment rate for a majority worker types compared with the \"Exact\" assignment.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 43,
    "Question": "### Background\n\nThe paper models a household's income process to construct a measure of idiosyncratic risk. The model for log income, `ln Y_it`, is:\n\n```latex\n\\ln Y_{i t}=\\alpha+X_{i t}^{\\prime}\\beta+\\epsilon_{i t} \\quad \\text{(Eq. (1))}\n```\n\nThe error term `ε_it` is decomposed into three components:\n\n```latex\n\\epsilon_{i t}=\\eta_{i}+\\nu_{i t} \\quad \\text{where} \\quad \\nu_{i t}=\\rho\\nu_{i t-1}+\\omega_{i t} \\quad \\text{(Eq. (2))}\n```\n\n- `η_i` is a time-invariant household-specific effect.\n- `ν_it` is a serially correlated transitory shock.\n- `ω_it` is an i.i.d. innovation error.\n\nThe paper uses the household-specific variance of the estimated innovation, `Var(ω̂_it)`, as its primary measure of income risk.\n\n### Question\n\nWhy is the variance of the innovation term `ω_it` the most appropriate measure of income risk for testing the theory of precautionary savings, as opposed to the variance of the other error components?",
    "Options": {
      "A": "Because `ω_it` is the only component that captures the true, unpredictable shocks to income that households cannot anticipate.",
      "B": "Because the time-invariant effect `η_i` represents the largest source of income variation between households.",
      "C": "Because the total residual `ε_it` captures all sources of unobserved income variation, making its variance the most comprehensive measure of risk.",
      "D": "Because the serially correlated shock `ν_it` captures persistent income shocks, which are the primary concern for long-term savings."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the paper's core variable construction. It uses a conceptual strategy to test whether students can distinguish between predictable and unpredictable components of an income process. The correct answer (A) identifies the i.i.d. innovation as the source of true uncertainty. Distractors are designed based on common misconceptions: (B) confuses risk with permanent heterogeneity (Conceptual Opposite); (C) fails to distinguish between predictable and unpredictable variation (Formula Misuse); (D) correctly identifies `ν_it` as persistent but incorrectly identifies it as the target for precautionary motives, as part of it is predictable (Almost Right).",
    "qid": "43",
    "model_evaluation_result": {
      "model_raw_response": "A\n\nNote: The output should be a single letter \"A\".",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 44,
    "Question": "### Background\n\nThe paper uses a two-stage estimation procedure. In the first stage, an income process is estimated to generate measures of permanent income and income risk. In the second stage, these generated variables are used to predict household portfolio choice (e.g., the share of wealth held in liquid assets).\n\nTo identify the causal effect of risk, the model relies on an exclusion restriction: occupation dummies are included as predictors in the first-stage income regression but are excluded from the second-stage portfolio choice regression.\n\n### Question\n\nWhich of the following scenarios, if true, would most directly violate the key identifying assumption of this research design?",
    "Options": {
      "A": "Occupation is a very strong predictor of a household's permanent income and income stability.",
      "B": "Farmers have a systematic need to hold large grain stocks as a productive input (e.g., for seed), independent of their income risk.",
      "C": "Households with higher education levels tend to have both higher income and a lower preference for liquid assets.",
      "D": "The measure of household income contains random measurement error that is uncorrelated with occupation."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the ability to critique the paper's identification strategy. It uses a Scenario Application strategy. The correct answer (B) describes a direct channel through which occupation affects the portfolio outcome variable (liquid wealth share) for reasons other than income, thus violating the exclusion restriction. Distractors are designed to be plausible but incorrect: (A) describes a condition required for the instrument to be strong, not a violation (True but Irrelevant); (C) describes a standard confounding variable that is handled by including education as a control in the second stage (True but Irrelevant); (D) describes a measurement error problem that does not violate this specific assumption (Conceptual Opposite).",
    "qid": "44",
    "model_evaluation_result": {
      "model_raw_response": "B) Farmers... independent of their... risk.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 45,
    "Question": "## Background\n\n**Research Question.** This problem investigates the long-term financial impact of providing social information to new donors. The analysis is a one-year follow-up of the *new donors* from an initial field experiment, tracking their donation behavior to assess causal effects on long-term outcomes.\n\n**Variables & Parameters.**\n- `Renewal rate`: The proportion of new donors from a given experimental group who made another donation one year later.\n- `Mean contribution ($)`: The average donation amount in year t+1, *conditional on renewing*.\n- `Expected revenue ($)`: The unconditional expected donation amount from a new donor in year t+1, calculated as `Renewal rate` × `Mean contribution`.\n\n---\n\n## Data / Model Specification\n\nFrom a separate analysis on the initial impact, the estimated immediate (year t) revenue increase from the `$300` signal for *new donors* was **$9.00** compared to the control group.\n\nThe long-term outcomes for new donors one year later (year t+1) are summarized in Table 1.\n\n**Table 1: New Donors One Year Later**\n\n| Treatment | Renewal rate (%) | Mean contribution ($) | Expected revenue ($) |\n|:---|:---:|:---:|:---:|\n| Control | 12 | 86.11 | 10.62 |\n| $300 | 32 | 93.97 | 29.95 |\n\n---\n\n## Question\n\nA station manager is considering a new policy: all *new* callers will receive the `$300` social information treatment. Based on the data provided, what is the total expected two-year revenue increase *per new donor* under this policy, compared to the no-information (control) policy?",
    "Options": {
      "A": "$19.33",
      "B": "$28.33",
      "C": "$9.00",
      "D": "$38.95"
    },
    "Answer": "B",
    "pi_justification": "This item assesses the ability to synthesize information from multiple sources (text and a table) to perform a multi-step calculation. It uses a 'Computational Judgment' strategy. The correct answer requires summing the immediate effect ($9.00) and the long-term effect ($29.95 - $10.62 = $19.33). Distractors are designed based on common errors: (A) is a 'Step-Omission Error' that only calculates the long-term effect; (C) is a 'Step-Omission Error' that only considers the immediate effect; (D) is a 'Formula Misuse' error where the student adds the total year 2 revenue to the year 1 gain instead of the *increase* in year 2 revenue.",
    "qid": "45",
    "model_evaluation_result": {
      "model_raw_response": "C) $19,33\n\n(Note: The calculation is: (0.32 - 0.12) * 93.975 + 9.00 = 19.33)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 46,
    "Question": "## Background\n\n**Research Question.** This problem investigates the validity of the estimated long-term impact of social information on donor renewal.\n\n**Setting / Institutional Environment.** The analysis is a one-year follow-up of the *new donors* from an initial field experiment. The study tracks their donation behavior one year after the initial intervention. However, not all original participants could be tracked or responded in the follow-up year.\n\n---\n\n## Scenario\n\nSuppose the follow-up study suffers from non-random attrition. Specifically, assume that new donors who had a very positive experience with the radio station (an unobserved characteristic) are both more likely to be found and successfully contacted in the follow-up sample *and* more likely to renew their donation. Furthermore, assume that receiving the `$300` treatment itself enhances this positive experience, making these donors even easier to track and include in the follow-up analysis.\n\n---\n\n## Question\n\nGiven this scenario, how would the estimated treatment effect of the `$300` signal on the probability of renewal likely be biased?",
    "Options": {
      "A": "Biased upwards (overestimated).",
      "B": "Biased downwards (underestimated).",
      "C": "Unbiased, because the initial treatment assignment was random.",
      "D": "The direction of the bias cannot be determined from the information given."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to apply the concept of sample selection bias. It uses a 'Scenario Application' strategy. The correct answer requires recognizing that the treatment is correlated with selection into the sample, and selection is correlated with the outcome, leading to an upward bias. Distractors target common misconceptions: (B) is the 'Conceptual Opposite'; (C) is 'True but Irrelevant', as it correctly states a fact about the initial experiment but ignores that the bias arises from non-random *attrition* in the follow-up stage; (D) is incorrect because the premises provided are sufficient to sign the bias.",
    "qid": "46",
    "model_evaluation_result": {
      "model_raw_response": "B) Biases downwards (underestimate). \n\n(Note: The bias occurs because the observed sample is not representative of the population, as it includes only those who were successfully contacted and responded to the follow-up survey, which are likely to be those who had a positive experience with radio station.)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 47,
    "Question": "## Background\n\n**Research Question.** This problem aims to estimate the causal effect of providing social information on donation amounts and to understand the properties of the estimators used.\n\n**Setting / Institutional Environment.** The analysis is based on a randomized controlled trial at a public radio station. Callers were randomly assigned to a control group or treatment groups. The study uses regression analysis to control for other observable characteristics.\n\n---\n\n## Data / Model Specification\n\nThe relationship between social information and pledge amounts is estimated using Ordinary Least Squares (OLS) and Robust Regression. Robust regression is a method designed to be less sensitive to outliers than OLS.\n\n**Table 1: The Social Information Effect (standard errors in parentheses)**\n\n| | All data (OLS) | All data (Robust) |\n|:---|:---:|:---:|\n| **$300** | 39.599** (13.609) | 10.710* (4.730) |\n*Controls for other treatments and covariates included. **p < 0.01, *p < 0.05*\n\n---\n\n## Question\n\nThe OLS estimate for the `$300` treatment effect (39.599) is nearly four times larger than the robust regression estimate (10.710). Which of the following statements provides the best explanation for this discrepancy?",
    "Options": {
      "A": "The OLS estimator for a treatment effect is equivalent to the difference in group means, which is highly sensitive to a few large outlier donations in the treatment group that the robust regression method down-weights.",
      "B": "The robust regression is likely biased towards zero, while the OLS estimate is unbiased and therefore more reliable.",
      "C": "The discrepancy is caused by heteroskedasticity in the error term, which biases OLS coefficients but is corrected by robust regression.",
      "D": "The control variables included in the model must be more influential in the OLS specification than in the robust regression, thus accounting for the difference."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the understanding of how different estimators behave in the presence of outliers. It uses a 'Reverse-Reasoning' strategy, asking for the cause of an observed empirical pattern. The correct answer (A) correctly identifies that OLS is a difference-in-means and is sensitive to outliers, which robust regression is designed to mitigate. Distractors target common econometric confusions: (B) is a 'Conceptual Opposite', incorrectly claiming OLS is more reliable here; (C) confuses robust *regression* (which handles outliers) with robust *standard errors* (which handle heteroskedasticity); (D) offers an incorrect explanation, as the difference is driven by the estimator's sensitivity to the dependent variable's distribution, not the controls.",
    "qid": "47",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\n(Note: I will provide the answer as per the format specified in the example)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 48,
    "Question": "## Background\n\n**Research Question.** This problem examines the causal validity of a treatment effect estimate from a field experiment.\n\n**Setting / Institutional Environment.** A study estimates the causal effect of social information on donation amounts using a linear model:\n\n```latex\n\\text{PledgeAmount}_i = \\alpha + \\beta_{300} T_{i,300} + X_i'\\gamma + \\varepsilon_i\n\\quad \\text{(Eq. (1))}\n```\n\nwhere $T_{i,300}$ is an indicator for assignment to the `$300` treatment condition.\n\n---\n\n## Scenario\n\nSuppose the randomization protocol failed. Specifically, assume that experimenters, contrary to instructions, were more likely to assign callers who sounded enthusiastic and wealthy (an unobserved characteristic, `HighPropensity`) to the `$300` treatment group. By definition, these callers are predisposed to making larger donations regardless of treatment.\n\n---\n\n## Question\n\nUnder this scenario of failed randomization, how is the OLS estimate of the treatment effect, $\\hat{\\beta}_{300}$, biased?",
    "Options": {
      "A": "It will be biased upwards, overstating the true causal effect.",
      "B": "It will be biased downwards, understating the true causal effect.",
      "C": "It will be unbiased, because OLS is the best linear unbiased estimator.",
      "D": "It will be biased, but the direction is ambiguous without knowing the sign of the error term."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to apply the omitted variable bias (OVB) framework. It uses a 'Scenario Application' strategy. The correct answer (A) requires identifying the two conditions for OVB: the omitted variable (`HighPropensity`) is correlated with the outcome (positively, by definition) and correlated with the treatment variable (positively, by the scenario). The product of these two positive effects creates a positive bias. Distractors target common errors: (B) is the 'Conceptual Opposite'; (C) misapplies the Gauss-Markov theorem, which requires the assumption of zero conditional mean (violated here); (D) incorrectly claims ambiguity when the direction is clearly identified by the scenario's premises.",
    "qid": "48",
    "model_evaluation_result": {
      "model_raw_response": "B) It is biased downwards, overstating true causal effect.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 49,
    "Question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium', where no `M-R_i` pair has an incentive to secretly deviate from the equilibrium contracts.\n\n**Variables & Parameters.**\n\n*   `T'_{i*}`: The marginal transfer price for retailer `i` at equilibrium.\n*   `(p^I, e^I)`: The prices and efforts that maximize total industry profit (the 'fully integrated' outcome).\n*   `C'_i(e_i)`: The marginal cost of effort for retailer `i`.\n*   `D_i(p, e)`: Demand for retailer `i`'s product.\n\n### Data / Model Specification\n\nIn the competition stage, each retailer `i` chooses its effort `e_i` to maximize its own profit. The first-order condition for this choice is:\n\n```latex\n\\big(p_{i}^{*}-T_{i}^{*\\prime}\\big)\\frac{\\partial D_{i}}{\\partial e_{i}}-C_{i}^{\\prime}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\nFor any positive effort `e_i > 0`, the marginal cost of effort `C'_i` is strictly positive.\n\n### Question\n\nIn this model, why is it impossible for the manufacturer to use secret bilateral contracts to achieve the fully integrated profit outcome `(p^I, e^I)`?",
    "Options": {
      "A": "To incentivize positive sales effort, a retailer must earn a positive margin (`p_i - T'_{i*} > 0`), but to eliminate the opportunism problem, the manufacturer must set the margin to zero.",
      "B": "The non-contractibility of effort means the manufacturer cannot directly specify `e_i` in the contract, making it impossible to enforce the integrated effort level.",
      "C": "Retailers have an incentive to exert too much effort to steal market share, which the manufacturer cannot prevent with bilateral contracts.",
      "D": "The manufacturer cannot commit to charging a high enough marginal transfer price `T'_{i*}` to achieve the integrated outcome because retailers would refuse such an unprofitable contract."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core economic conflict that prevents the manufacturer from achieving the first-best outcome, which was the subject of the first part of the original QA problem. It uses a Reverse-Reasoning strategy. The correct answer (A) identifies the fundamental tension between providing margins to incentivize effort and eliminating margins to prevent opportunism. Distractor (B) is a true precondition but not the direct cause of the conflict. Distractor (C) posits the opposite problem (too much effort). Distractor (D) misidentifies the reason for failure, focusing on retailer participation rather than the manufacturer's own conflicting incentives.",
    "qid": "49",
    "model_evaluation_result": {
      "model_raw_response": "C) Retailerse have an incentive t exert too much effor to steal market shre, which the manufacturercannot prevent with bilateral contractse",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 50,
    "Question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium'.\n\n**Variables & Parameters.**\n\n*   `T'_{k*}`: The marginal transfer price for retailer `k` at equilibrium.\n*   `(p^B, e^B)`: The prices and efforts that arise in standard Bertrand competition.\n*   `D_e`: The 2x2 matrix of demand derivatives with respect to effort, where element `(i, k)` is `∂D_k/∂e_i`.\n\n### Data / Model Specification\n\nIn any contract equilibrium, the following system of equations must hold for `i=1, 2`:\n\n```latex\n\\sum_{k=1}^{2}(T_{k}^{*\\prime}-c)\\frac{\\partial D_{k}}{\\partial e_{i}}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\n**Assumption 1:** A uniform increase in efforts `e_1` and `e_2` causes demand `D_i` to rise. This implies that the matrix `D_e` is invertible.\n\n### Question\n\nGiven the necessary equilibrium condition shown in Eq. (1) and Assumption 1, what is the equilibrium marginal transfer price `T'_{k*}` for each retailer `k`, and what is the resulting market outcome?",
    "Options": {
      "A": "The marginal transfer price for each retailer equals the manufacturer's marginal cost (`T'_{k*} = c`), leading to the standard Bertrand outcome (`p^B, e^B`).",
      "B": "The marginal transfer price equals the fully integrated price (`T'_{k*} = p_k^I`), which successfully implements the fully integrated outcome.",
      "C": "The marginal transfer price is set above marginal cost (`T'_{k*} > c`) to extract rent, leading to prices above the Bertrand level.",
      "D": "The marginal transfer price is set below marginal cost (`T'_{k*} < c`) to subsidize effort, leading to higher-than-Bertrand effort levels."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the main mathematical result of Proposition 1, which was derived in the second and third parts of the original QA problem. It uses a Computational Judgment strategy. The correct answer (A) states the paper's surprising result: secret contracting with retail effort forces the manufacturer to price at marginal cost, leading to the competitive Bertrand outcome. Distractor (B) is a classic misconception, representing the result from models *without* retail effort (O'Brien and Shaffer). Distractor (C) represents a common but incorrect intuition about monopoly pricing. Distractor (D) describes a strategy (subsidizing effort) that might be optimal with public contracts but is not sustainable with secret contracts.",
    "qid": "50",
    "model_evaluation_result": {
      "model_raw_response": "D) The marginal transfer price is s",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 51,
    "Question": "### Background\n\nIn modeling family labor supply, a key challenge is that many women do not participate in the labor market, creating a sample of working women that is not random. The paper estimates its model on a sample of households where the wife works positive hours.\n\n### The Question\n\nAn analyst estimates a family labor supply model using data only from households where the wife works positive hours. Standard regression methods (e.g., Seemingly Unrelated Regressions) applied to this selected sample yield biased and inconsistent estimates for the male labor supply equation. What is the fundamental econometric reason for this bias?",
    "Options": {
      "A": "The sample of working wives is not representative of the general population of all wives, violating the assumption of random sampling.",
      "B": "The smaller sample size leads to less precise estimates and a loss of statistical power.",
      "C": "The wife's participation decision is an omitted variable in the husband's labor supply equation.",
      "D": "Unobserved factors affecting the wife's participation are correlated with unobserved determinants of the husband's labor supply, inducing a correlation between the model's regressors and its error term."
    },
    "Answer": "D",
    "pi_justification": "This question assesses the core concept of sample selection bias. It uses a Reverse-Reasoning strategy, asking for the fundamental cause of a known problem. Distractor A is 'Almost Right' but lacks the crucial link to the error term. Distractor B confuses inconsistency with inefficiency. Distractor C misidentifies the problem as omitted variable bias.",
    "qid": "51",
    "model_evaluation_result": {
      "model_raw_response": "D) Unobservable factors affecting the woman's participation are related to unobserved factors influencing the man's labor supply.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 52,
    "Question": "### Background\n\nTo address sample selection bias from female non-participation, the authors use a Full Information Maximum Likelihood (FIML) estimator. The likelihood function for the truncated sample of households with working wives is:\n```latex\nL = \\prod_{h=1}^{H} \\frac{\\phi(u_{fh}, u_{mh})}{\\operatorname{Pr}(l_{fh} < T_f)}\n```\nwhere `φ(u_fh, u_mh)` is the joint probability density of the errors, assumed to be bivariate normal. The denominator is the probability of being selected into the sample.\n\n### The Question\n\nA critic argues that the unobserved preference for female non-market time might be skewed, violating the FIML estimator's joint normality assumption. If the critic is correct, what is the primary consequence of this violation, and what is the most appropriate econometric response to assess the model's robustness?",
    "Options": {
      "A": "Consequence: Inefficient but still consistent estimates. Response: Use robust standard errors.",
      "B": "Consequence: Inconsistent estimates. Response: Find a valid instrumental variable for the wife's participation decision.",
      "C": "Consequence: Biased estimates for the female equation only. Response: Use a different parametric distribution like Logit instead of Probit for the selection stage.",
      "D": "Consequence: Inconsistent estimates for all model parameters. Response: Re-estimate using a semi-parametric control function method that does not rely on a specific distributional assumption."
    },
    "Answer": "D",
    "pi_justification": "This question tests advanced understanding of the limitations of FIML selection models. It uses a Scenario Application strategy. The correct option correctly identifies both the consequence (inconsistency, not inefficiency) and the state-of-the-art solution (semi-parametric methods). Distractor A confuses inconsistency with inefficiency. Distractor B confuses selection with standard endogeneity. Distractor C incorrectly localizes the problem and proposes an insufficient fix.",
    "qid": "52",
    "model_evaluation_result": {
      "model_raw_response": "C) Consequences: Biased estimate for the female participation equation. Response: Consider alternative distributions such as Logit instead Probit for selection stage.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 53,
    "Question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. The rebel faction's decision to accept a peaceful proposal is governed by its Peaceful Participation Constraint (PPC):\n\n```latex\nz_{k}+\\beta V_{R}(s_{k+1}) \\geq f_{R}+\\beta d_{k}p_{k}(1+\\gamma)V +\\beta(1-d_{k})[(1-\\varepsilon)V_{R}(s_{k})+\\varepsilon V_{R}(s_{k+1})] \\quad \\text{(Eq. (1))}\n```\n\nwhere `V_R(s)` is the rebel's continuation payoff from state `s`, and `ε` is the probability G's consolidation succeeds despite inconclusive fighting. The government (G) has **coercive power** when `d_k < 1` and `ε > 0`, which allows it to relax the PPC by choosing a lower `V_R(s_{k+1})`.\n\n### Question\n\nAssuming the government has coercive power and seeks a peaceful settlement, what is its optimal strategy to maximize its own payoff?\n",
    "Options": {
      "A": "Offer the largest possible current transfer (`z_k=1`) to induce the rebels to accept the weakest possible future position (the lowest `V_R(s_{k+1})`).",
      "B": "Offer the smallest possible current transfer (`z_k`) that satisfies the rebels' participation constraint, regardless of the future state `s_{k+1}`.",
      "C": "Propose the strongest possible future for the rebels (highest `V_R(s_{k+1})`) to minimize the required current transfer `z_k`.",
      "D": "Propose a future state `s_{k+1}` where fighting is most decisive (`d_{k+1}=1`) to create efficiency gains for both parties."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core mechanism of 'coercive power' during peaceful negotiations. It uses a Computational Judgment strategy, requiring students to interpret the Peaceful Participation Constraint to deduce the optimal government strategy. The correct option (A) identifies the trade-off between current transfers and future rebel weakness. Distractors are designed based on common errors: (B) a myopic focus on minimizing the current transfer, (C) misinterpreting the government's objective function, and (D) confusing the peaceful buyout logic with the 'truce' logic from another part of the model.",
    "qid": "53",
    "model_evaluation_result": {
      "model_raw_response": "C) Proposing the strongest possible outcome for the rebels to minimize the current transfer `zk`.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 54,
    "Question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. If the rebels fight and the fighting is inconclusive (which happens with probability `1-d_k`), the government's consolidation effort to move to state `s_{k+1}` still succeeds with probability `ε`.\n\nThe government (G) has **coercive power** when `d_k < 1` and `ε > 0`.\n\n### Question\n\nSuppose the government makes a proposal `(z_k, s_{k+1})` that it knows the rebels will reject, triggering a fight. How should the government choose the `s_{k+1}` component of this rejected proposal to maximize its own expected payoff from the conflict?\n",
    "Options": {
      "A": "Choose `s_{k+1} = E` (the monopoly state) to signal its resolve and deter future opposition.",
      "B": "Choose `s_{k+1} = E` (the monopoly state) to maximize its own continuation value `V_G(s_{k+1})` in the event that fighting is inconclusive but its consolidation efforts succeed.",
      "C": "Choose the `s_{k+1}` that would have required the largest peaceful transfer `z_k`, to show the rebels the value of the deal they are rejecting.",
      "D": "Choose `s_{k+1} = s_k` (no change), because the proposal is strategically irrelevant once the rebels have decided to fight."
    },
    "Answer": "B",
    "pi_justification": "This item tests a critical pivot in the model's logic: how the government's objective changes when moving from a peace negotiation to a violent conflict. It uses a Scenario Application strategy. The correct option (B) requires understanding that the government's choice of `s_{k+1}` now aims to maximize its own potential payoff, not influence the rebels' choice. Distractors are based on plausible but incorrect reasoning: (A) a behavioral explanation instead of the model's rationalist one, (C) misapplication of the peace-time formula, and (D) incorrectly assuming the proposal becomes completely irrelevant.",
    "qid": "54",
    "model_evaluation_result": {
      "model_raw_response": "C) Choose `z_k` that would have been required to achieve the largest peaceful settlement `s_{_k}`, to show the value of deal the rebels are rejecting.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 55,
    "Question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\n### Question\n\nConsider two states facing identical, very strong rebel groups. State A has a **credibility crisis** (very low `h`, high `c`); its promises are worthless, but it is efficient at implementing policies. State B has an **implementation crisis** (high `h`, very low `c`); its promises are credible, but its bureaucracy is ineffective. According to the model, which state is more likely to experience a civil war, and why?\n",
    "Options": {
      "A": "State B, because its low implementation capacity (`c`) makes the peaceful consolidation process too slow and unreliable, forcing the government to fight.",
      "B": "State A, because its high implementation capacity (`c`) makes the government overconfident in its ability to win a war.",
      "C": "State A, because its low commitment capacity (`h`) creates a severe 'liquidity problem,' making it unable to make a credible offer large enough to buy out a strong rebel group.",
      "D": "State B, because its high commitment capacity (`h`) would require the government to make costly promises it would rather avoid by fighting."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the ability to distinguish between two forms of state capacity and apply them to a comparative scenario. It uses a Scenario Application strategy. The correct option (C) requires identifying that commitment capacity (`h`) is the binding constraint when dealing with strong rebels who require large payoffs. Distractors are designed to test common points of confusion: (A) over-weighting the importance of implementation speed, (B) introducing an outside behavioral logic, and (D) misstating the government's preferences.",
    "qid": "55",
    "model_evaluation_result": {
      "model_raw_response": "C) State C) State A is more likely",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 56,
    "Question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\nIn the model, large 'contingent spoils' (`γ`) that are only realized after peace is achieved create a high 'cost of delay' for any peaceful settlement, making war more likely.\n\n### Question\n\nAn international actor wants to prevent a civil war in a country where the primary driver of conflict is the existence of massive, easily accessible oil revenues that will become available only once the state is stable (i.e., `γ` is extremely large). Which of the following interventions is likely to be most effective at preventing conflict, according to the model's logic?\n",
    "Options": {
      "A": "A 'Good Governance' program to increase implementation capacity (`c`), because it directly speeds up the multi-period buyout process, reducing the delay in realizing the spoils.",
      "B": "A 'Rule of Law' program to increase commitment capacity (`h`), because it makes the government appear more trustworthy to international partners.",
      "C": "A 'Good Governance' program to increase implementation capacity (`c`), because with large spoils at stake, the government needs certainty that its consolidation efforts will succeed.",
      "D": "A 'Rule of Law' program to increase commitment capacity (`h`), because it can solve the government's 'liquidity problem,' potentially allowing a one-period buyout that eliminates the costly delay."
    },
    "Answer": "D",
    "pi_justification": "This item tests deep application of the model's logic to a policy problem. It uses a Reverse-Reasoning strategy: given that the 'cost of delay' is the key problem, the student must identify the most effective solution. The correct option (D) recognizes that increasing commitment capacity (`h`) offers a more fundamental solution by enabling a single-period settlement, which is superior to marginally speeding up a multi-period one (the effect of `c`). Distractors represent plausible but less effective solutions (A, C) or reasoning external to the model (B).",
    "qid": "56",
    "model_evaluation_result": {
      "model_raw_response": "D) A \"Rule of Law\" program to increase Commitment Capacity (`H`)",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 57,
    "Question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nA temporary, unexpected fiscal expansion occurs in the home country today (`dG_0 > 0`). This policy is transmitted internationally, causing a fall in foreign private consumption. What is the primary economic mechanism driving this negative transmission?",
    "Options": {
      "A": "The expansion creates an excess demand for current goods, raising the world interest rate and reducing the present value of wealth in both countries.",
      "B": "The expansion creates an excess supply of current goods, lowering the world interest rate and crowding in foreign investment.",
      "C": "The expansion worsens the foreign country's terms of trade, reducing the value of its exports.",
      "D": "The expansion is funded by taxes on the home country, which directly reduces foreign income through trade linkages."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses understanding of the transmission mechanism for a current transitory fiscal shock using a Reverse-Reasoning strategy. The correct answer (A) identifies the key role of the interest rate in clearing the goods market. Distractor (B) presents the opposite mechanism. Distractor (C) incorrectly applies a concept from the two-good model, testing for model confusion. Distractor (D) offers a vague, non-model-based explanation.",
    "qid": "57",
    "model_evaluation_result": {
      "model_raw_response": "A) The extension creates an excess-demand for current goods",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 58,
    "Question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nThe home country's government announces today that it will undertake a temporary fiscal expansion in a future period `s > 0` (`dG_s > 0`). What is the immediate effect on the *current* consumption of the foreign private sector (`c_0^*`), and why?",
    "Options": {
      "A": "`c_0^*` increases, because the announcement lowers the interest rates linking the present to the future, raising the present value of the foreign country's income stream.",
      "B": "`c_0^*` decreases, because the announcement raises the interest rates linking the present to the future, lowering the present value of foreign wealth.",
      "C": "`c_0^*` is unchanged, because the fiscal expansion has not yet occurred and therefore has no effect on current variables.",
      "D": "`c_0^*` increases, because foreign consumers save more today in anticipation of higher future taxes in their own country."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item tests the core concept of how anticipated future policies affect current outcomes, a key contribution of the paper. It uses a Scenario Application strategy. The correct answer (A) captures the wealth revaluation mechanism. Distractor (B) reverses the logic. Distractor (C) represents a naive view that ignores rational expectations, a key assumption of the model. Distractor (D) provides a plausible-sounding but incorrect savings story.",
    "qid": "58",
    "model_evaluation_result": {
      "model_raw_response": "C) `C` is unchanged. The fiscal expansion has been announced but has not yet taken place, so there is no immediate effect on current consumption.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 59,
    "Question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nAssume the foreign country is more patient than the home country (`δ* > δ`). The home government announces a future temporary fiscal *cut* (`dG_s < 0`). This announcement has a primary effect of raising home wealth and lowering foreign wealth. What is the secondary effect of this wealth redistribution on the interest rate `r_{t-1}` for a period `t` far from `s`?",
    "Options": {
      "A": "The interest rate `r_{t-1}` will increase, because wealth is transferred from a high-saving country to a low-saving country, creating an incipient shortage of world savings.",
      "B": "The interest rate `r_{t-1}` will decrease, because the fiscal cut signals future abundance, which lowers the return to saving.",
      "C": "The interest rate `r_{t-1}` will decrease, because wealth is transferred from a high-saving country to a low-saving country, creating an incipient surplus of world savings.",
      "D": "The interest rate `r_{t-1}` will not change, as the shock is specific to period `s` and does not affect other periods."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the subtle secondary effect of a future policy announcement on the term structure of interest rates, using a Scenario Application strategy. The correct answer (A) requires a two-step reasoning process: identify the direction of wealth transfer and then determine its impact on world savings given the saving propensities. Distractor (B) offers a plausible but incorrect narrative. Distractor (C) makes a sign error in the second step of the reasoning. Distractor (D) fails to recognize the full general equilibrium effects in an integrated capital market.",
    "qid": "59",
    "model_evaluation_result": {
      "model_raw_response": "C) The secondary effect of the wealth redistribution is that the low-saving country (home country) now has more wealth, leading to a reduction in its savings, while the high-saving country (foreign country) has less wealth, leading an increase in its savings. This creates an incipient excess of savings in the world, which would lead to a decrease in the interest rate.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 60,
    "Question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nAfter a single vertical merger, what is the maximum one-period 'Payoff if cheating' for one of the remaining unintegrated upstream firms, and what is the economic reason for this value?",
    "Options": {
      "A": "100, because a deviating firm can still undercut its rivals' offers to capture the entire market.",
      "B": "90, because the integrated firm's downstream profit in the punishment phase (10) is subtracted from the total monopoly profit.",
      "C": "80, because the deviating firm cannot profitably sell to the integrated downstream firm, which represents one-fifth of the market.",
      "D": "70, because the integrated firm's collusive share (30) is subtracted from the total monopoly profit."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the user's ability to apply the 'outlets effect' in a concrete calculation. It uses a Computational Judgment strategy. Distractors are designed based on common errors: ignoring the effect entirely (A), confusing punishment profit with deviation profit reduction (B), and using an irrelevant collusive value (D).",
    "qid": "60",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: I've only provided the answer as per the format specified.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 61,
    "Question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nIn the numerical example, the vertical merger causes two key changes shown in Table 1: the integrated firm's punishment payoff increases from 0 to 10, and an unintegrated firm's cheating payoff decreases from 100 to 80. Which statement correctly identifies the economic effects driving these changes?",
    "Options": {
      "A": "The increase to 10 is the outlets effect; the decrease to 80 is the punishment effect.",
      "B": "Both changes are caused by the outlets effect, as the integrated firm is shielded from competition in all phases of the game.",
      "C": "The increase to 10 is the punishment effect; the decrease to 80 is the reaction effect.",
      "D": "The increase to 10 is the punishment effect; the decrease to 80 is the outlets effect."
    },
    "Answer": "D",
    "pi_justification": "This item tests the core conceptual distinction between the 'outlets effect' and the 'punishment effect'. It uses a Reverse-Reasoning strategy, asking the user to map outcomes to causes. Distractors include a direct conceptual swap (A), conflating the two effects (B), and introducing a true but irrelevant concept from a different model (C).",
    "qid": "61",
    "model_evaluation_result": {
      "model_raw_response": "D) The decrease to 10. \n\n(Note: The \"reaction effect\" is not listed as an option, but it is mentioned in the background information.)",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 62,
    "Question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\nThe incentive to cheat is defined as the ratio of the one-time gain from cheating to the per-period profit foregone by triggering punishment:\n\n```latex\n\\text{Incentive to Cheat} = \\frac{\\pi_{\\text{cheat}} - \\pi_{\\text{collude}}}{\\pi_{\\text{collude}} - \\pi_{\\text{punish}}} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n\n### The Question\n\nSuppose a policy intervention intensifies downstream competition, causing the noncooperative profit of each downstream firm (`π^NC`) to fall from 10 to 5. This reduces the 'Payoff if punished' for the integrated firm to 5. Assume the integrated firm's collusive payoff remains 30. What is the new 'Incentive to Cheat' for the integrated firm, and what is the implication for antitrust?",
    "Options": {
      "A": "2.8; the merger becomes a greater antitrust concern.",
      "B": "2.8; the merger becomes less of an antitrust concern.",
      "C": "3.5; there is no change in antitrust concern.",
      "D": "14.0; the merger becomes a greater antitrust concern."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to apply the model to a novel scenario, combining calculation with judgment. It uses a Scenario Application strategy. Distractors are based on a correct calculation but incorrect interpretation (B), a failure to update a parameter (C), and a fundamental misuse of the formula (D).",
    "qid": "62",
    "model_evaluation_result": {
      "model_raw_response": "D) 3,5; there no change in anti-trust concern.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 63,
    "Question": "### Background\nIn a generalized Cournot oligopoly model, the dynamic evolution of the output vector `x_t` is governed by a discrete-time system `x_t = B_n x_{t-1} + \\text{constant}`. The stability of this system depends on the eigenvalues of the transition matrix `B_n`.\n\n### Data / Model Specification\nAssume the following facts derived from the model:\n1.  The eigenvalues of `B_n` are related to the eigenvalues of another matrix `K A_n` (denoted `\\lambda_i`) by the transformation: `eig(B_n)_i = 1 + \\lambda_i`.\n2.  All eigenvalues `\\lambda_i` of `K A_n` are real and negative.\n3.  For the special case where all firms have the same adjustment speed, `K_i = K`, the most negative eigenvalue of `K A_n` is given by:\n    ```latex\n    \\lambda_n = -K \\left( \\frac{(n+1)b+d}{2b+d} \\right)\n    ```\n    where `n` is the number of firms, `b > 0` is related to the demand slope, and `d` is the slope of the firms' marginal cost curves.\n\n### Question\nA discrete linear dynamic system is stable if and only if all eigenvalues of its transition matrix have an absolute value less than 1. Given the information above, what is the necessary and sufficient condition for stability in this model?",
    "Options": {
      "A": "```latex\nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```",
      "B": "```latex\nK > 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```",
      "C": "```latex\nK < 2 \\left( \\frac{(n+1)b+d}{2b+d} \\right)\n```",
      "D": "```latex\nK < \\frac{2b+d}{(n+1)b+d}\n```"
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item uses the 'Computational Judgment' strategy. It tests the ability to combine the abstract stability rule (`|1+\\lambda| < 1`) with the specific properties of `\\lambda` (real, negative) to derive the crucial intermediate condition `\\lambda > -2`, and then correctly perform the algebraic manipulation on the formula for `\\lambda_n` to arrive at the final stability condition. Distractors are based on common, high-frequency errors: (B) is a sign error from failing to flip the inequality; (C) is a formula misuse error from incorrectly isolating K; (D) is a step-omission error from forgetting the factor of 2.",
    "qid": "63",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: The answer is based on the condition for stability of the discrete linear dynamic systems, which requires all eigenvalues to have an absolute values less than unity. The given expression for the most negative eigenvector `\\lambda_n` is used to derive the necessary and sufficiency condition for stability.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 64,
    "Question": "### Background\nThe stability of a generalized Cournot oligopoly model is determined by the condition:\n\n```latex\nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```\n\nwhere `K` is the adjustment speed, `d` is the slope of marginal cost, and `n` is the number of firms.\n\n### Question\nThe original Theocharis model is a special case of this generalized model, assuming constant marginal costs and instantaneous adjustment. What does the stability condition simplify to under the Theocharis assumptions (`d=0` and `K=1`)?",
    "Options": {
      "A": "The number of firms must be less than 3 (`n < 3`).",
      "B": "The number of firms must be greater than 3 (`n > 3`).",
      "C": "The number of firms must be less than or equal to 3 (`n ≤ 3`).",
      "D": "The number of firms must be less than 2 (`n < 2`)."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item uses the 'Computational Judgment' strategy. It tests the ability to apply the general formula to a specific, important limiting case discussed in the paper. The calculation is straightforward but requires careful algebraic simplification. The correct answer (A) recovers the famous Theocharis result. Distractors represent common errors: (B) is the condition for instability (a conceptual opposite); (C) incorrectly includes the boundary case of `n=3`, which is oscillatory, not stable; (D) would result from an algebraic slip.",
    "qid": "64",
    "model_evaluation_result": {
      "model_raw_response": "C) The condition simplifies to `1 < 2`, which always holds true, so the condition is `n ≤ 1`.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 65,
    "Question": "### Background\n\nAn analysis of \"white flight\" in early 20th-century northern U.S. cities estimates the effect of black in-migration on white population change. The core empirical challenge is that black arrivals do not choose neighborhoods randomly, creating endogeneity. The paper estimates both an OLS model and an instrumental variables (IV) model to address this.\n\n### Data / Model Specification\n\nThe model estimated is:\n```latex\n\\Delta W_{ij}^{t1-t0} = \\alpha + \\beta \\Delta B_{ij}^{t1-t0} + \\eta_{j} + \\epsilon_{ij} \\quad \\text{(Eq. 1)}\n```\nwhere `\\Delta W` is the change in white population, `\\Delta B` is the change in black population, and `\\eta_j` is a city fixed effect. A negative `\\beta` indicates white flight.\n\n**Table 1: OLS and IV Results for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) OLS | (2) IV |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -1.492*** | -3.389*** |\n| | (0.075) | (0.246) |\n\n*Notes: Standard errors in parentheses. *** p<0.01.*\n\n### Question\n\nBased on the results in Table 1, what is the direction of the endogeneity bias in the OLS model, and what is the most likely economic explanation for this bias as discussed in the paper?",
    "Options": {
      "A": "The OLS estimate is biased upwards (closer to zero), likely because both black and white households were attracted to the same neighborhoods with improving economic conditions or amenities.",
      "B": "The OLS estimate is biased downwards (more negative), likely because black households were moving into neighborhoods that whites were already abandoning for reasons unrelated to race.",
      "C": "The OLS estimate is biased upwards (closer to zero), likely because black households were moving into neighborhoods that whites were already abandoning for reasons unrelated to race.",
      "D": "The OLS estimate is biased downwards (more negative), likely because both black and white households were attracted to the same neighborhoods with improving economic conditions or amenities."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Diagnosing the direction and cause of OLS bias by comparing OLS and IV estimates. Strategy: Reverse-Reasoning. Distractor Logic: The correct answer (A) correctly identifies the positive bias (upward, toward zero) and the corresponding economic story (shared amenities). Distractor (B) gets both the direction of bias and the economic story wrong (this story would cause positive bias). Distractors (C) and (D) correctly identify one component (bias direction or story) but incorrectly pair it with the other, testing for partial understanding.",
    "qid": "65",
    "model_evaluation_result": {
      "model_raw_response": "C) The most likely economic reason for the endogeneity in the Ols model is that black households were likely moving into neighborhoods where whites were already leaving for reasons unrelatedto race, resulting in a biased downwards OLS estimate.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 66,
    "Question": "### Background\n\nTo identify the causal impact of black in-migration on white population change, a study uses an instrumental variable (IV). The instrument interacts historical, pre-1900 settlement patterns of blacks from specific southern states with decade-by-decade out-migration shocks from those same states.\n\n### Question\n\nThe validity of this IV strategy rests on the exclusion restriction. Which of the following scenarios would most clearly violate this assumption?",
    "Options": {
      "A": "A severe boll weevil infestation in Georgia simultaneously pushes black agricultural workers to migrate north and disrupts the cotton supply to textile mills in northern neighborhoods where Georgian migrants historically settled, causing an independent economic decline in those specific neighborhoods.",
      "B": "Social networks from the 19th century weaken over time, so that by 1920, settlement patterns of new migrants from a given southern state are no longer correlated with the settlement patterns of migrants from that same state before 1900.",
      "C": "The overall economic boom of the 1920s increases population growth in all northern cities, an effect that is absorbed by the model's city-level fixed effects.",
      "D": "Newly arriving black migrants tend to settle in neighborhoods that are already experiencing population growth due to factors like new housing construction, which is the primary source of endogeneity the IV is designed to correct."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Understanding the exclusion restriction in a specific research context. Strategy: Scenario Application. Distractor Logic: The correct answer (A) describes a classic violation: the instrument (driven by the Georgia shock) has a direct effect on the outcome (white population change) through a channel other than the endogenous variable (a local economic shock). Distractor (B) describes a violation of the *relevance* assumption, not the exclusion restriction. Distractor (C) describes a scenario that is correctly handled by the model's fixed effects. Distractor (D) describes the endogeneity problem itself, not a failure of the proposed solution.",
    "qid": "66",
    "model_evaluation_result": {
      "model_raw_response": "D) Newly arising black migrants tend settle in neighborhoods already experiencing population development due to factors such as new housing construction which is the main source of endogeniety the IV is intended to correct. \n\nAnswer: D",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 67,
    "Question": "### Background\n\nAn instrumental variable (IV) strategy is used to estimate the causal effect of black in-migration (`\\Delta B`) on white population change (`\\Delta W`). A potential threat to this strategy is that the instrument is mechanically correlated with the number of black residents in a neighborhood in 1900. If neighborhoods with larger initial black populations had different pre-existing trends in white population change, this could bias the results. To test this, the model is re-estimated including the number of blacks in 1900 as a control variable.\n\n### Data / Model Specification\n\n**Table 1: IV Robustness Check for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) Baseline IV | (2) IV with Control |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -3.389*** | -2.910*** |\n| | (0.386) | (0.644) |\n| Number of blacks in 1900 | | -0.343 |\n| | | (0.358) |\n\n*Notes: Spatial standard errors in parentheses. Both regressions include city fixed effects.*\n\n### Question\n\nWhat does the comparison of the coefficient on `\\Delta B` between column (1) and column (2) imply about the validity of the IV strategy?",
    "Options": {
      "A": "The coefficient on `\\Delta B` remains large, significant, and relatively stable, suggesting the main finding of substantial white flight is not primarily driven by confounding factors associated with the initial size of the black population.",
      "B": "The coefficient on `\\Delta B` decreases, indicating that the baseline IV estimate was biased and that the true causal effect is much smaller.",
      "C": "The standard error on `\\Delta B` increases substantially, which indicates that the instrument becomes weak once the control variable is included.",
      "D": "The coefficient on `Number of blacks in 1900` is not statistically significant, proving that there was no correlation between the instrument and initial neighborhood characteristics."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Interpreting the result of a key robustness check for an IV model. Strategy: Reverse-Reasoning. Distractor Logic: The correct answer (A) accurately interprets the stability of the coefficient as a sign of robustness against this specific threat. Distractor (B) is an over-interpretation of the modest change; the effect is still very large and economically similar. Distractor (C) confuses an increase in standard error (expected with more controls and a smaller sample) with a weak instrument problem (which is assessed with the first-stage F-statistic). Distractor (D) misinterprets the purpose of the check; the goal is to see the effect on the main coefficient, not to test the significance of the control itself.",
    "qid": "67",
    "model_evaluation_result": {
      "model_raw_response": "B) The certificate on `\\Delta` B decreases, indicating the baseline IV estimation was biased and the true causal impact is much smaller.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 68,
    "Question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nWhat is the primary identification problem that the repeat-sales methodology, which transforms Eq. (1) into Eq. (2), is designed to solve?",
    "Options": {
      "A": "It controls for selection bias caused by changes in the mix of properties sold over time.",
      "B": "It corrects for measurement error in the reported sales price `p_{it}`.",
      "C": "It accounts for heteroskedasticity where price variance increases with the time between sales.",
      "D": "It eliminates the influence of citywide economic shocks `δ_t` on individual house prices."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's understanding of the fundamental purpose of the repeat-sales (fixed effects) methodology, which is central to the paper. It uses a conceptual strategy. The correct answer (A) identifies selection bias as the key problem. Distractors are designed based on common confusions: (B) is a different econometric problem (True but Irrelevant), (C) refers to a secondary weighting step, not the core differencing transformation (Conceptual: Almost Right), and (D) is the conceptual opposite of the method's goal.",
    "qid": "68",
    "model_evaluation_result": {
      "model_raw_response": "D) It elimines the influence of the citywide economic shock `δ_t on individual house pric",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 69,
    "Question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nSuppose that during a housing boom, homeowners frequently undertake unobserved major renovations (e.g., new kitchens) between the sale at time `t'` and the sale at time `t`. How would this behavior affect the price appreciation (`δ_t - δ_{t'}`) estimated using the repeat-sales model (Eq. 2)?",
    "Options": {
      "A": "The model would overstate true price appreciation because it would incorrectly attribute price gains from quality improvements to market-wide price growth.",
      "B": "The model would understate true price appreciation because the cost of renovations is not factored into the sale price.",
      "C": "The estimate would be unbiased because the unobserved renovations are random and would be absorbed by the error term `ε_{it} - ε_{it'}` which has an expected value of zero.",
      "D": "The effect is ambiguous because some renovations add more value than others, making the direction of the bias impossible to determine without more data."
    },
    "Answer": "A",
    "pi_justification": "This item tests the student's ability to analyze a key threat to the validity of the repeat-sales identification strategy. It uses a Scenario Application strategy. The correct answer (A) correctly identifies the direction of the bias and the reason. Distractors are based on plausible errors: (B) introduces irrelevant cost reasoning (Formula Misuse), (C) incorrectly assumes the violation of the assumption away (Conceptual Opposite), and (D) offers a weak 'ambiguous' conclusion when a clear directional prediction can be made (Conceptual: Almost Right).",
    "qid": "69",
    "model_evaluation_result": {
      "model_raw_response": "C) The model is unbiased because the error term ε_{it}-ε_{it'} has an expected-value of zero.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 70,
    "Question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nBased on the results in Table 1, which statement best characterizes the trade-off between the restricted test (`Ȳ⁽²⁾`) and the unrestricted test (`Υ_W⁽²⁾`)?",
    "Options": {
      "A": "The restricted test is more powerful when its assumption (`θ₁=θ₂`) holds true, but can be severely less powerful when the assumption is false.",
      "B": "The unrestricted test is always more powerful than the restricted test because it does not impose false constraints.",
      "C": "The restricted test is more powerful when `θ₁=θ₂` and has only slightly less power when the assumption is false.",
      "D": "Since both tests have the correct empirical size, the choice between them is arbitrary and does not significantly affect inference."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to interpret Monte Carlo evidence to understand the fundamental power-efficiency trade-off in hypothesis testing. It uses a Scenario Application strategy. Distractor B is a Conceptual Opposite, ignoring the efficiency gain of a correct restriction. Distractor C is an 'Almost Right' statement that understates the severe power loss documented in the table (e.g., for `θ₁=0.3, θ₂=-0.3`). Distractor D is 'True but Irrelevant,' as it correctly notes the size but incorrectly dismisses the large power differences.",
    "qid": "70",
    "model_evaluation_result": {
      "model_raw_response": "C) The unrestricted Test (`Υ⁽²˝`) is more powerful than restricted Test (`ȳ⁽²`).",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 71,
    "Question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nA researcher uses the restricted test `Ȳ⁽²⁾` based on a strong prior that `θ₁=θ₂`. However, the true data generating process is `(θ₁, θ₂) = (-0.1, 0.0)`. According to Table 1, what is the approximate probability that the researcher commits a Type II error (fails to reject the false null `H₀: θ₁=θ₂=0`) at the 5% significance level?",
    "Options": {
      "A": "60.2%",
      "B": "39.8%",
      "C": "51.3%",
      "D": "91.8%"
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to perform a precise quantitative calculation and interpretation from a data table. It uses a Computational Judgment strategy. The correct answer is 1 - 0.398 = 0.602. Distractor B is a classic Step-Omission Error, reporting the power instead of the Type II error probability (1 - power). Distractor C is a Formula Misuse error, correctly calculating the Type II error but for the wrong test (the unrestricted one: 1 - 0.487). Distractor D results from a Lookup Error, using data from the wrong row (`(0.0, -0.1)`) to calculate the Type II error (1 - 0.082).",
    "qid": "71",
    "model_evaluation_result": {
      "model_raw_response": "C) 39,8% \n\n(Note: The correct answer is B) 39%.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 72,
    "Question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `f_i(p)`: The cost function for commodity `i`, returning the minimum cost to produce one unit given factor price vector `p`.\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production, defining a mapping `f` from factor prices `p` to commodity prices `q`.\n\n---\n\n### The Questions\n\nIn the context of the Factor Price Equalization theorem, why is the existence of a unique global inverse for the cost function mapping `p` to `q` (i.e., `p = f^{-1}(q)`) a critical condition?",
    "Options": {
      "A": "It implies that for any given vector of commodity prices `q` established by trade, there is only one possible corresponding vector of factor prices `p`.",
      "B": "It ensures that multiple factor price vectors can correspond to a single commodity price vector, allowing countries to specialize.",
      "C": "It is a direct mathematical consequence of the concavity of the cost function, which is a fundamental property of cost minimization.",
      "D": "It proves that free trade in goods will lead to the equalization of commodity prices across countries."
    },
    "Answer": "A",
    "pi_justification": "This question assesses the core economic logic of the Factor Price Equalization theorem. It uses a conceptual reasoning strategy. The correct option (A) states the central implication. Distractor (B) is the conceptual opposite. Distractor (C) is 'True but Irrelevant,' as concavity does not guarantee a global inverse. Distractor (D) confuses the theorem's premise (equal commodity prices) with its conclusion.",
    "qid": "72",
    "model_evaluation_result": {
      "model_raw_response": "C) It's a direct mathematical conclusion from the concavity",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 73,
    "Question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n*   `a_{ij}`: The amount of factor `j` required to produce one unit of commodity `i`, chosen to minimize cost.\n*   `A`: The `n x n` matrix of input coefficients `a_{ij}`.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production:\n\n```latex\nq_{i} = \\sum_{j=1}^{n} a_{ij} p_{j}\n```\n\nA standard result from production theory (derived from the envelope theorem) states that at the cost-minimizing input choice:\n\n```latex\n\\sum_{j=1}^{n} p_{j} \\frac{\\partial a_{ij}}{\\partial p_{k}} = 0\n```\n\n---\n\n### The Questions\n\nGiven the cost function in Eq. (1) and the envelope theorem result in Eq. (2), what is the Jacobian matrix `J` of the mapping from factor prices `p` to commodity prices `q`, where the elements of the Jacobian are `J_{ik} = ∂q_i/∂p_k`?",
    "Options": {
      "A": "The matrix of input coefficients, `A`",
      "B": "The transpose of the input matrix, `A^T`",
      "C": "The zero matrix, `0`",
      "D": "The matrix `A + M`, where `M` is a matrix with elements `M_{ik} = Σ_j p_j (∂a_{ij}/∂p_k)`"
    },
    "Answer": "A",
    "pi_justification": "This question tests the ability to perform a key mathematical derivation central to the paper's local analysis. It uses a Computational Judgment strategy. The correct answer (A) results from correctly applying the product rule and the provided envelope theorem identity in Eq. (2). Distractor (B) represents a common index-confusion error. Distractor (D) represents a 'Step-Omission Error,' where the student applies the product rule but fails to use the identity in Eq. (2) to cancel the second term. Distractor (C) represents a misunderstanding of the envelope theorem's application.",
    "qid": "73",
    "model_evaluation_result": {
      "model_raw_response": "B) Thetranspose of the inputmatrix, `A`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 74,
    "Question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nThe paper argues that the no-linking regime is always inefficient. Under which specific condition would the capital requirement for the marginal tenant, `\\hat{b}_n`, coincidentally be efficient (i.e., `\\hat{b}_n = b_n^*`)?",
    "Options": {
      "A": "`r_n = r_0`",
      "B": "`r_n > r_0`",
      "C": "`w = 0`",
      "D": "`z^* = 0` (where `z^*` is the landlord's profit in the linking regime)"
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the deep understanding of what drives inefficiency in the no-linking regime. It uses a Reverse-Reasoning strategy, asking for the specific condition that would eliminate the inefficiency. The correct answer, `r_n = r_0`, requires connecting the IR constraints from both regimes. Distractors are based on common confusions: (B) is a plausible but incorrect condition, (C) confuses the outside option with efficiency conditions, and (D) confuses landlord profit with social surplus maximization.",
    "qid": "74",
    "model_evaluation_result": {
      "model_raw_response": "C) w = 0\n\n(Note: I've removed any additional text, leaving only the answer as per the specified format.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 75,
    "Question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nIn the no-linking regime, a landlord cannot screen tenants and hires randomly from the pool of applicants. If a landlord happens to draw a tenant of ability `e_i` (where `i \\geq n`), what is the correct expression for the landlord's ex-post profit, `\\pi_i^{no-link}`?",
    "Options": {
      "A": "`(1-\\alpha)F(e_i, \\hat{b}_n)`",
      "B": "`(1-\\alpha)F(e_i, b_i^*)`",
      "C": "`F(e_i, \\hat{b}_n) - (1+r_0)a\\hat{b}_n - w`",
      "D": "`(1-\\alpha)F(e_i, \\hat{b}_n) + (r_i - r_0)a\\hat{b}_n`"
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to correctly apply the profit definition in a new institutional context. It uses a Computational Judgment strategy. The correct answer requires recognizing that the landlord's profit is their output share, evaluated at the mandated capital level, with no credit component. Distractors represent common errors: (B) Formula Misuse (using the wrong capital level), (D) Conceptual Error (incorrectly including a credit-linking term that is banned), and (C) Formula Misuse (using the total surplus expression instead of the landlord's profit).",
    "qid": "75",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: The output is a single letter \"C\".)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 76,
    "Question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`. The distribution of tenant abilities is given by `g(e_i)`.\n\n### Question\n\nThe paper concludes that the welfare ranking of the two regimes is ambiguous and depends on the distribution of tenant abilities, `g(e_i)`. What is the fundamental reason for this ambiguity?",
    "Options": {
      "A": "The linking regime's profit is determined by the marginal tenant's productivity, while the no-linking regime's expected profit depends on the average productivity of all tenants.",
      "B": "The linking regime is always socially efficient, while the no-linking regime is always inefficient.",
      "C": "Landlords face risk in the no-linking regime but not in the linking regime, and the model assumes they are risk-averse.",
      "D": "High-ability tenants earn informational rents in the linking regime, which are competed away in the no-linking regime."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item targets the paper's core policy conclusion. It uses a Conceptual Judgment strategy to identify the key trade-off. The correct answer (A) pinpoints the 'marginal vs. average' mechanism. Distractors are designed to be plausible but incorrect: (B) is an overstatement (linking isn't always efficient) and doesn't explain the profit ambiguity; (C) is a Conceptual Error (model assumes risk neutrality); (D) is True but Irrelevant, as it describes tenant welfare, not the ambiguity in landlord profits which drives the overall conclusion.",
    "qid": "76",
    "model_evaluation_result": {
      "model_raw_response": "C) Landlord's risk aversion in the no-Linking regime. \n\nAnswer: C",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 77,
    "Question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms for determining firm investment and its financing generally fail to guarantee an equilibrium?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy where stockholders must collectively decide on firms' production plans. This analysis compares two distinct linear mechanisms that fail.\n\n---\n\n### Data / Model Specification\n\nThe decision problem of a stockholder is to choose a consumption plan, a portfolio `\\theta_i`, and a message `m_i` to maximize utility. The firm's input `x_j` and the stockholder's cost share `C_{ij}` are determined by the specific mechanism's rules.\n\n1.  **The Shareholding Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = \\theta_{ij} x_j`\n\n2.  **The Helpman-Razin Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = (1/I) x_j`\n\nBoth mechanisms are shown in the paper to fail to guarantee an equilibrium in general.\n\n---\n\nWhat are the distinct fundamental reasons for the failure of the Shareholding Mechanism and the Helpman-Razin Mechanism, respectively?",
    "Options": {
      "A": "The Shareholding Mechanism fails because it forces portfolio shares (`\\theta_{ij}`) to serve a dual role of risk-sharing and cost-sharing, preventing unanimity. The Helpman-Razin Mechanism fails because its exogenously fixed cost shares are too rigid to accommodate heterogeneous stockholder preferences.",
      "B": "Both mechanisms fail for the same reason: they force portfolio shares (`\\theta_{ij}`) to serve a dual role of risk-sharing and cost-sharing, which prevents stockholders from reaching a unanimous decision on the firm's investment level.",
      "C": "The Shareholding Mechanism fails because its cost shares are too rigid to accommodate heterogeneous preferences. The Helpman-Razin Mechanism fails because it forces portfolio shares (`\\theta_{ij}`) to serve a dual role of risk-sharing and cost-sharing.",
      "D": "Both mechanisms fail because they do not satisfy the Ekern-Wilson spanning condition, which is a necessary condition for equilibrium existence in any stock market model."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core diagnostic understanding of why simple, linear mechanisms fail in this setting. It uses a Reverse-Reasoning strategy, asking the user to identify the distinct causes for two related failures. The distractors are designed to test for common conceptual confusions: (B) conflating the two distinct failures, (C) reversing the causes and effects, and (D) attributing the failure to a related but different concept (spanning) that the paper shows is not the core issue.",
    "qid": "77",
    "model_evaluation_result": {
      "model_raw_response": "B) Both mechanics fail for the reason that they force portfolio shar",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 78,
    "Question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms generally fail, and what specific design features are required to solve this problem?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy. Simple linear mechanisms (like the Shareholding and Helpman-Razin mechanisms) fail because they cannot consistently align stockholders' incentives. A more complex, non-linear mechanism is proposed to solve this problem.\n\n---\n\n### Data / Model Specification\n\nThe **Shared Cost Mechanism** is designed to overcome the failures of simpler linear mechanisms. Its key features are:\n-   Message: Each stockholder `i` sends a message `m_i = (x_i, a_i)`, where `x_i` is a vector of proposed input increments and `a_i` is a vector of proposed cost share transfers.\n-   Allocation Rule: `x_j = \\sum_i x_{ij}`\n-   Cost-Sharing Rule:\n    ```latex\n    C_{ij}(m) = (1 - \\sum_{k \\ne i} a_{kj}) x_j + (1 - \\sum_k a_{kj})^2 \\quad \\text{(Eq. (1))}\n    ```\n\n---\n\nHow does the design of the Shared Cost Mechanism, particularly its cost-sharing rule (Eq. (1)), successfully guarantee the existence of an equilibrium where simpler linear mechanisms fail?",
    "Options": {
      "A": "It allows stockholders to directly communicate their desired input levels (`x_{ij}`), ensuring the final investment `x_j` reflects the average preference of all stockholders.",
      "B": "It fixes each stockholder's cost share to be equal to their initial endowed revenue share (`\\bar{\\theta}_{ij}`), which provides a stable and predictable financing plan.",
      "C": "It endogenizes cost shares via the `a_{ij}` message and uses the quadratic term to create an incentive for these shares to sum to one, providing the flexibility needed to align heterogeneous preferences.",
      "D": "The quadratic term ensures that the total costs collected from all stockholders exactly equal the total investment (`\\sum_i C_{ij}(m) = x_j`), preventing any budget shortfalls for the firm."
    },
    "Answer": "C",
    "pi_justification": "This item assesses understanding of the paper's central innovation: the design of the Shared Cost Mechanism. It uses a Reverse-Reasoning strategy, asking for the specific design features that ensure equilibrium existence. The distractors are designed to isolate the critical components: (A) focuses on a necessary but insufficient part of the message, (B) proposes a solution the paper explicitly rejects, and (D) offers a correct but less precise explanation of the quadratic term's function, making the correct answer superior.",
    "qid": "78",
    "model_evaluation_result": {
      "model_raw_response": "C) It endsogenizes costshares via the a_{ij} message and uses teh quadratic term to creat an incentive for these shares to sume to one, providin the flexibility needed t align heteregeneous preferences.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 79,
    "Question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified by an overidentification test, the Falsification Adaptive Set (FAS) provides a range of parameter estimates consistent with minimally relaxed versions of the model. This approach avoids the arbitrary selection of sensitivity parameters by letting the data determine the necessary degree of relaxation of the instrument exclusion restrictions.\n\n### Data / Model Specification\n\nConsider a linear IV model with one endogenous variable (`K=1`) and `L` instruments. The Falsification Adaptive Set (FAS) for the causal effect `β` is given by the interval:\n\n```latex\n\\mathrm{FAS} = \\left[ \\min_{l: \\pi_l \\neq 0} b_l, \\max_{l: \\pi_l \\neq 0} b_l \\right] \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l = ψ_l / π_l` is the population 2SLS coefficient on the endogenous variable `X` using `Z_l` as the excluded instrument and the remaining instruments `Z_{-l}` as controls. `π_l` is the corresponding first-stage coefficient.\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan (`Z_1`), Railroads (`Z_2`), and Exploration (`Z_3`). The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| **Panel B. Controlling for other instruments** | | |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n*Note: Standard practice is to discard results from instruments with a first-stage F-statistic below 10, as they are considered weak and unreliable.*\n\n### Question\n\nBased on the information provided in Table 1 and the definition in Eq. (1), what is the estimated Falsification Adaptive Set (FAS) for the effect of log highway km?",
    "Options": {
      "A": "[0.18, 3.65]",
      "B": "[0.18, 0.42]",
      "C": "[0.24, 3.47]",
      "D": "[0.42, 3.65]"
    },
    "Answer": "B",
    "pi_justification": "This item assesses the ability to apply a theoretical formula to empirical results, a core skill. It uses a Computational Judgment strategy. The correct answer requires identifying the relevant instruments by screening out the weak one (F-stat < 10) and then taking the min and max of the remaining estimates. Distractor A represents a Step-Omission Error, where the user fails to screen for the weak instrument. Distractor C is a Formula Misuse error, based on incorrect calculations with the endpoints. Distractor D represents both a screening failure and a misidentification of the minimum value.",
    "qid": "79",
    "model_evaluation_result": {
      "model_raw_response": "C) [3.47, 3,65]",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 80,
    "Question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified, the Falsification Adaptive Set (FAS) provides a range of plausible parameter estimates. For any potential value `b` within the FAS, the minimal required violation of the exclusion restriction for instrument `l` (its direct effect `|γ_l|`) is given by the vector `δ(b)` on the Falsification Frontier.\n\n### Data / Model Specification\n\nThe required violation for instrument `l` to be consistent with a true causal effect `β_true = b` is:\n\n```latex\n|\\gamma_l| = |\\psi_l - b\\pi_l| = |\\pi_l| \\cdot |b_l - b| \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l` is the 2SLS estimate using instrument `l` and `π_l` is its first-stage coefficient. The first-stage F-statistic is proportional to `π_l^2`, so the relative strength of two instruments can be compared using the square root of the ratio of their F-statistics: `|π_1|/|π_2| ≈ sqrt(F_1/F_2)`.\n\nAn empirical study provides the following results for two strong instruments, 'Plan' and 'Exploration':\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (6) Exploration |\n| :--- | :---: | :---: |\n| log highway km (`b_l`) | 0.18 | 0.42 |\n| First-stage F stat. | 54.8 | 27.0 |\n\n### Question\n\nSuppose the true causal effect is `β_true = 0.25`. To make the model consistent with this true value, which instrument requires a larger violation of its exclusion restriction (`|γ_l|`)?",
    "Options": {
      "A": "'Plan', because its point estimate (0.18) is closer to the true value (0.25).",
      "B": "'Plan', because it has a stronger first stage (higher F-statistic).",
      "C": "'Exploration', because the required violation `|π_l| ⋅ |b_l - 0.25|` is larger for it after accounting for first-stage strengths.",
      "D": "'Exploration', because its point estimate (0.42) is further from the true value (0.25)."
    },
    "Answer": "C",
    "pi_justification": "This item tests deep comprehension by requiring a multi-step calculation that synthesizes point estimates, a theoretical formula, and test statistics. It uses a Computational Judgment strategy. The correct answer requires calculating the weighted distance `|π_l| ⋅ |b_l - b|` for both instruments. Distractor D represents a Step-Omission Error, as it correctly identifies the instrument with the larger unweighted distance `|b_l - b|` but fails to account for the first-stage strength `|π_l|`. Distractor B is True but Irrelevant, misapplying the meaning of the F-statistic in this context. Distractor A uses the same flawed logic as D but for the wrong instrument.",
    "qid": "80",
    "model_evaluation_result": {
      "model_raw_response": "C) 'Explanation' because the required failure of exclusion restriction is larger for this instrument after accounting for the strength of the first stage.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 81,
    "Question": "### Background\n\nIn an instrumental variable (IV) model with multiple instruments, some instruments may be more problematic than others. A key diagnostic is the conditional first-stage F-statistic, which measures the strength of an instrument after controlling for the other instruments. An F-statistic below 10 typically indicates a 'weak instrument,' which can lead to unreliable and biased 2SLS estimates.\n\n### Data / Model Specification\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan, Railroads, and Exploration. The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n### Question\n\nBased on the evidence in Table 1, which instrument is most problematic when used in a model that controls for the other instruments, and what is the primary statistical reason?",
    "Options": {
      "A": "'Plan', because its 2SLS estimate (0.18) is the smallest, suggesting it may be biased downwards.",
      "B": "'Railroads', because its 2SLS estimate (3.65) is an extreme outlier compared to the others.",
      "C": "'Exploration', because its F-statistic (27.0) is significantly lower than that of 'Plan' (54.8).",
      "D": "'Railroads', because its conditional first-stage F-statistic is extremely low, indicating it is a weak instrument."
    },
    "Answer": "D",
    "pi_justification": "This item assesses the ability to interpret standard econometric diagnostics to evaluate instrument validity, a critical skill for applying the paper's methods. It uses a Reverse-Reasoning strategy (inferring the problematic instrument from its statistical properties). The correct answer identifies the instrument with the critically low F-statistic. Distractor B identifies the correct instrument but for a secondary reason (the outlier estimate is a *symptom* of the weak first stage, not the root cause), making it an 'Almost Right' conceptual error. Distractor C is 'True but Irrelevant' because an F-stat of 27 is still very strong. Distractor A offers an economically irrelevant reason.",
    "qid": "81",
    "model_evaluation_result": {
      "model_raw_response": "D)'Railroads', because it's conditional first-stage f-statistic is 1.26, which is below 10, indicating a 'weak' instrument.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 82,
    "Question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nBased on the results in Table 1, which statement most accurately summarizes the causal effect of an electoral loss on a novice candidate's propensity to run for office again?",
    "Options": {
      "A": "An electoral loss reduces the probability of running again for both men and women, but the deterrence effect is over 50% larger for women than for men.",
      "B": "An electoral loss reduces a female candidate's probability of running again by approximately 10.1 percentage points.",
      "C": "An electoral loss significantly reduces a male candidate's probability of running again, but it has no statistically significant effect on a female candidate's probability of running again.",
      "D": "Female candidates are, on average, 10.1 percentage points less likely to run for office again compared to male candidates, regardless of the election outcome."
    },
    "Answer": "A",
    "pi_justification": "Value: Assesses the student's ability to interpret the main finding from an interaction model, requiring them to synthesize multiple coefficients. Strategy: Computational Judgment. The student must understand that the effect for men is `β` (-16.0pp), the effect for women is `β+γ` (-16.0 + -10.1 = -26.1pp), and then compare them. Distractor Logic: (B) is a Step-Omission Error, reporting only the interaction term `γ`. (C) is a Conceptual Error, misinterpreting the meaning of the interaction term. (D) is a Conceptual Error, confusing an interaction effect with a main effect.",
    "qid": "82",
    "model_evaluation_result": {
      "model_raw_response": "B) An elective loss reduces a feminine candidate's probability to run once again by about 10. The coefficient of `-0.101` represents the additional effect of losing on a female candidates' propensity to run again, which is equivalent to a 10.10 percentage point decrease.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 83,
    "Question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nThe paper argues that the gender gap in persistence is driven by the behavior of candidates who barely lost, not by those who barely won. Which coefficient in Table 1 provides the most direct evidence for the \"not by those who barely won\" part of this claim?",
    "Options": {
      "A": "The coefficient on `Female x Lost` (γ = -0.101), because it shows that losing has a different effect on women.",
      "B": "The coefficient on `Lost` (β = -0.160), because it shows the large negative effect of losing for the baseline group.",
      "C": "The coefficient on `Female` (δ = 0.007), because its small size and statistical insignificance imply no meaningful difference in persistence between male and female winners.",
      "D": "The standard error on `Female` (0.036), because it is large relative to the coefficient, indicating high uncertainty."
    },
    "Answer": "C",
    "pi_justification": "Value: Assesses a nuanced point of econometric interpretation: identifying which coefficient in an interaction model represents the baseline difference between groups (i.e., among the control group, `Lost=0`). Strategy: Reverse-Reasoning. The student must infer which piece of evidence supports a specific part of the paper's argument. Distractor Logic: (A) is Almost Right, as it explains the 'driven by losers' part, but not the 'not by winners' part. (B) is True but Irrelevant to the gender comparison among winners. (D) points to the statistical tool (the SE) rather than the economic parameter (the coefficient) that carries the interpretation.",
    "qid": "83",
    "model_evaluation_result": {
      "model_raw_response": "C) The certificate on `Female`(δ =  0. 007), because  its small size  and statistical insignificanc",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 84,
    "Question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates. A central hypothesis is that the gender gap in persistence is concentrated in electoral environments with lower female representation.\n\n### Data / Model Specification\n\nThe analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nThe paper estimates this model on different subsamples of political offices. Table 1 shows the baseline effect of losing for men. Table 2 shows the *additional* effect for women (`γ`) in two different office types.\n\n**Table 1. Baseline Effect of Loss for Male Candidates**\n\n| Variable | Coefficient | Standard Error |\n|----------|-------------|----------------|\n| `Lost`   | -0.160***   | (0.027)        |\n\n*Source: Adapted from Table 2 of the source paper.*\n\n**Table 2. Heterogeneity by Office Type (Differential Effect for Women)**\n\n| Sample / Office Type         | `Female x Lost` Coefficient (γ) | Standard Error |\n|------------------------------|---------------------------------|----------------|\n| City Council (Low Rep.)      | -0.165**                        | (0.065)        |\n| School Board (High Rep.)     | -0.057                          | (0.051)        |\n\n*Source: Adapted from Table 4 of the source paper. ** p<0.05.*\n\n### Question\n\nBased on these results, in which electoral environment is the gender gap in persistence most pronounced, and what is the approximate total deterrence effect of a loss for a woman in that environment?",
    "Options": {
      "A": "Most pronounced in City Council races; the total effect for a woman is a reduction of about 16.5 percentage points.",
      "B": "Most pronounced in School Board races; the total effect for a woman is a reduction of about 21.7 percentage points.",
      "C": "Most pronounced in School Board races; the total effect for a woman is a reduction of about 5.7 percentage points.",
      "D": "Most pronounced in City Council races; the total effect for a woman is a reduction of about 32.5 percentage points."
    },
    "Answer": "D",
    "pi_justification": "Value: Assesses the ability to test a hypothesis using subgroup analysis and to correctly calculate the total effect for a specific group in an interaction model. Strategy: Scenario Application. The student must apply the general model to two specific scenarios (office types). Distractor Logic: (A) is a Step-Omission Error, reporting only the interaction term `γ` for City Council races. (B) correctly calculates the effect for School Board races but incorrectly identifies it as the more pronounced environment. (C) is a Step-Omission Error for School Board races, reporting only the interaction term `γ`.",
    "qid": "84",
    "model_evaluation_result": {
      "model_raw_response": "C) Mostpronounced in School Board Races; the total deterrence for a woman would be the sum of the baseline effect for men (−0.160) and the additional effect for women (−0,057), resulting in a total effect of approximately −0.217.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 85,
    "Question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe evolution of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) from one period to the next is governed by the following equations, which combine an accelerator mechanism with a neoclassical profitability term:\n\n```latex\n\\frac{d\\bar{K_{i}}}{\\bar{K_{i}}} = A_{i}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\pi_{i}r} \\frac{\\delta F_{i}}{\\delta K}\\right]^{\\sigma_{i}} - 1 \\quad \\text{(Eq. (1))}\n```\n\n```latex\n\\frac{d\\bar{E_{i}}}{\\bar{E_{i}}} = A_{i}^{\\prime}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\bar{w_{i}}} \\frac{\\delta F_{i}}{\\delta E}\\right]^{\\sigma_{i}^{\\prime}} - 1 \\quad \\text{(Eq. (2))}\n```\nwhere `g_i` is expected demand growth, `\\alpha_i` is the activity level, `p_i^v` is the value-added price, `\\pi_i r` is the rental cost of capital, `\\bar{w}_i` is the wage cost, and `F_i` is the production function.\n\n## Question\n\nIn the model, a long-run equilibrium is a steady state where capital and labor stocks are constant (`d\\bar{K}_i / \\bar{K}_i = 0`, `d\\bar{E}_i / \\bar{E}_i = 0`), the economy operates at full capacity (`\\alpha_i = 1`), and expected demand growth is zero (`g_i = 0`). Based on the dynamic adjustment equations (Eq. (1) and Eq. (2)), which pair of conditions correctly describes this long-run equilibrium?",
    "Options": {
      "A": "The marginal revenue product of capital equals its rental cost (`p_i^v (\\delta F_i / \\delta K) = \\pi_i r`), and the marginal revenue product of labor equals its wage cost (`p_i^v (\\delta F_i / \\delta E) = \\bar{w}_i`).",
      "B": "The marginal product of capital is zero (`\\delta F_i / \\delta K = 0`), and the marginal product of labor is zero (`\\delta F_i / \\delta E = 0`).",
      "C": "The marginal revenue product of capital equals the wage (`p_i^v (\\delta F_i / \\delta K) = \\bar{w}_i`), and the marginal revenue product of labor equals the rental cost of capital (`p_i^v (\\delta F_i / \\delta E) = \\pi_i r`).",
      "D": "The average revenue product of capital equals its rental cost (`p_i^v (F_i / \\bar{K}_i) = \\pi_i r`), and the average revenue product of labor equals its wage (`p_i^v (F_i / \\bar{E}_i) = \\bar{w}_i`)."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item replaces the derivation task in question 2 of the original QA problem with a sharp conceptual check on its outcome. It assesses whether the user can correctly identify the standard neoclassical profit-maximization conditions that emerge as the long-run steady state of the model's dynamic equations. Distractors are designed based on common errors in microeconomic theory: (B) incorrectly sets marginal products to zero, (C) swaps the prices of capital and labor, and (D) confuses marginal products with average products.",
    "qid": "85",
    "model_evaluation_result": {
      "model_raw_response": "C) The marginalequilibrium. \n\nNote that I did not provide any explanation, only the answer as per the format specified.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 86,
    "Question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe long-run dynamic adjustment of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) is governed by a process that converges to a long-run equilibrium. In this equilibrium, the marginal revenue product of each factor equals its cost:\n1.  `p_{i}^{v} (\\delta F_{i} / \\delta K) = \\pi_{i}r` (for capital)\n2.  `p_{i}^{v} (\\delta F_{i} / \\delta E) = \\bar{w_{i}}` (for labor)\n\nAssume the production function `F_i` is Cobb-Douglas:\n\n```latex\nF_i(\\bar{K}_i, \\bar{E}_i) = Z \\bar{K}_i^\\beta \\bar{E}_i^{1-\\beta}\n```\n\n## Question\n\nGiven the long-run equilibrium conditions and the specified Cobb-Douglas production function, how does a permanent increase in the price of capital goods, `\\pi_i`, affect the sector's long-run equilibrium capital-labor ratio, `\\bar{K}_i^* / \\bar{E}_i^*`?",
    "Options": {
      "A": "It decreases the capital-labor ratio.",
      "B": "It increases the capital-labor ratio.",
      "C": "It has no effect on the capital-labor ratio.",
      "D": "The effect is ambiguous and depends on whether `\\beta` is greater or less than 0.5."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item uses the 'Computational Judgment' strategy to replace the complex derivation in question 3 of the original QA problem. It tests the core economic insight: the law of demand for factors of production. To answer correctly, the user must mentally (or on paper) derive the optimal capital-labor ratio and determine the sign of its derivative with respect to the price of capital. The distractors represent classic errors: (B) is the conceptual opposite (violating the law of demand), (C) incorrectly assumes a fixed-proportions technology (zero elasticity of substitution), and (D) introduces a plausible but incorrect complexity.",
    "qid": "86",
    "model_evaluation_result": {
      "model_raw_response": "B) It decreases. \n\n(Note: Please let me know if you want me to explain the reasoning behind this answer.)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 87,
    "Question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nBased on the preferred Model (5) in Table 1, what is the estimated long-run steady-state temperature increase resulting from a sustained increase of 1000 GtC in cumulative CO2 emissions?",
    "Options": {
      "A": "0.36 °C",
      "B": "0.48 °C",
      "C": "1.16 °C",
      "D": "1.55 °C"
    },
    "Answer": "D",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem's core components are highly suitable for conversion. This item tests the ability to calculate a long-run multiplier from a dynamic model, a key skill in interpreting econometric results. Strategy: Computational Judgment. The distractors are designed based on common errors: (A) is the static effect (`β_2`), ignoring the dynamic multiplier `1/(1-β_1)`; (B) is a formula misuse (`β_2/β_1`); (C) is a data misread error, correctly applying the formula but using the `β_1` coefficient from the wrong model (Model 6).",
    "qid": "87",
    "model_evaluation_result": {
      "model_raw_response": "C) 0,3632 °C (from the table, this is the coefficient on `C_{(t-1)}` in Model (5))",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 88,
    "Question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nModel (5) is selected as the best specification due to its superior out-of-sample performance (lowest RMSE). When Model (7) adds CO2 concentrations (`S_{t-1}`) to Model (5), its performance worsens, and the coefficient on `S_{t-1}` becomes statistically insignificant. What is the most likely econometric explanation for this phenomenon?",
    "Options": {
      "A": "High multicollinearity between cumulative emissions (`C_{t-1}`) and CO2 concentrations (`S_{t-1}`).",
      "B": "Omitted variable bias from excluding an important climate forcing agent in Model (7).",
      "C": "The ARMA(1,1) error structure is misspecified for Model (7).",
      "D": "Reverse causality, where higher temperatures are driving CO2 concentrations."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to diagnose a classic econometric problem from regression output, a critical skill for empirical researchers. Strategy: Reverse-Reasoning. The distractors represent common but incorrect diagnoses: (B) is a Conceptual Opposite, as the problem stems from adding, not omitting, a variable; (C) is True but Irrelevant, as misspecification is less likely to cause these specific symptoms than multicollinearity; (D) describes a different form of endogeneity not directly supported by the evidence in the table.",
    "qid": "88",
    "model_evaluation_result": {
      "model_raw_response": "B) Omission of an important climate-forcing agent in Model 7.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 89,
    "Question": "### Background\n\n**Research Question:** This problem examines the mathematical construction of uncorrelated time-series scenarios using Chebyshev polynomials, which serve as efficient inputs for emulating a complex climate model.\n\n**Setting / Institutional Environment:** The methodology generates a set of basis vectors for CO2 emissions from a family of orthogonal polynomials. These polynomials are defined over a canonical interval `x ∈ [-1, 1]` and must be transformed to be used for a specific time horizon, e.g., years `t ∈ [2005, 2100]`.\n\n### Data / Model Specification\n\nChebyshev polynomials are defined by the recurrence relation:\n\n```latex\nP_{0}(x)=1, \\quad P_{1}(x)=x, \\quad P_{k}(x)=2x P_{k-1}(x)-P_{k-2}(x) \\quad \\text{for } k=2,3,\\ldots \\quad \\text{(Eq. 1)}\n```\n\nThe domain `x ∈ [-1, 1]` is mapped to a general time interval `t ∈ [a, b]` using the transformation:\n\n```latex\nt = \\left(\\frac{b-a}{2}\\right)x + \\frac{a+b}{2} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAn emissions scenario `E_2(t)` is constructed based on the Chebyshev polynomial of degree 2, `P_2(x)`. The domain `x ∈ [-1, 1]` is mapped to the time horizon `t ∈ [2005, 2100]`. The range of the polynomial `[-1, 1]` is then linearly scaled to an emissions range of `[-5, 29.6708]` GtC/yr. What is the approximate emissions level for the year `t=2052.5`, which corresponds to the midpoint of the time horizon?",
    "Options": {
      "A": "-5.0 GtC/yr",
      "B": "-1.0 GtC/yr",
      "C": "12.34 GtC/yr",
      "D": "29.67 GtC/yr"
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 10.0). The original QA is purely technical and computational, making it ideal for conversion. This item tests the multi-step process of applying the mathematical tools described. Strategy: Computational Judgment. The correct answer requires deriving `P_2(x)=2x^2-1`, finding that `t=2052.5` maps to `x=0`, evaluating `P_2(0)=-1`, and mapping this to the minimum of the emissions range. Distractors are based on predictable errors: (B) is a Step-Omission Error, forgetting the final range scaling; (C) is a Conceptual Error, taking the midpoint of the emissions range; (D) is another Conceptual Error, evaluating at an endpoint (`x=1`) instead of the midpoint.",
    "qid": "89",
    "model_evaluation_result": {
      "model_raw_response": "C) 14.17 GtC/year",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 90,
    "Question": "### Background\n\n**Research Question:** This problem examines the mathematical construction of uncorrelated time-series scenarios using Chebyshev polynomials, which serve as efficient inputs for emulating a complex climate model.\n\n**Setting / Institutional Environment:** The methodology generates a set of basis vectors for CO2 emissions from a family of orthogonal polynomials. These polynomials are defined over a canonical interval `x ∈ [-1, 1]` and must be transformed to be used for a specific time horizon, e.g., years `t ∈ [2005, 2100]`.\n\n### Data / Model Specification\n\nChebyshev polynomials are defined by the recurrence relation:\n\n```latex\nP_{0}(x)=1, \\quad P_{1}(x)=x, \\quad P_{k}(x)=2x P_{k-1}(x)-P_{k-2}(x) \\quad \\text{for } k=2,3,\\ldots \\quad \\text{(Eq. 1)}\n```\n\nThe domain `x ∈ [-1, 1]` is mapped to a general time interval `t ∈ [a, b]` using the transformation:\n\n```latex\nt = \\left(\\frac{b-a}{2}\\right)x + \\frac{a+b}{2} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nConsider an emissions scenario based on the Chebyshev polynomial `P_2(x) = 2x^2 - 1`. Initially, it is defined over the time horizon `[2005, 2100]`. If the horizon is extended to `[2005, 2150]`, how does the `x` value in the canonical domain `[-1, 1]` corresponding to the year `t=2100` change?",
    "Options": {
      "A": "It remains unchanged at 1.0.",
      "B": "It decreases from 1.0 to approximately 0.31.",
      "C": "It increases from 1.0 to approximately 1.52.",
      "D": "It is undefined because 2100 is no longer an endpoint."
    },
    "Answer": "B",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 10.0). This item tests the understanding of how the domain transformation works and its sensitivity to the time horizon, a key technical detail. Strategy: Computational Judgment. The correct answer requires calculating the `x` value for `t=2100` under both the old and new transformations. Distractors are designed to catch common misunderstandings: (A) is a Conceptual Error, failing to recognize the transformation depends on the interval endpoints; (C) represents a plausible calculation error; (D) is an 'Almost Right' distractor that correctly identifies that 2100 is no longer an endpoint but incorrectly concludes the value is undefined.",
    "qid": "90",
    "model_evaluation_result": {
      "model_raw_response": "C) It decreases to approximately 2/3.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 91,
    "Question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive estimation procedures in the paper rely on a consistent estimator for the error variance `\\sigma^2`.\n\n---\n\n### Data / Model Specification\n\nWhen replication is unavailable, the paper proposes the Smallest Singular Value (SSV) estimator for the variance:\n```latex\n\\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p \n```\nwhere `\\hat{l}_q` is the smallest singular value of the data matrix `X`. This estimator is consistent for `\\sigma^2 + p^{-1}l_q^2`, where `l_q` is the smallest singular value of the true matrix `M`.\n\n---\n\n### Question\n\nUnder what condition on the true mean matrix `M` is the Smallest Singular Value (SSV) estimator a consistent estimator for the true error variance `\\sigma^2` as `p \\to \\infty`?",
    "Options": {
      "A": "The true matrix `M` must be full rank (`l_q > 0`).",
      "B": "The largest singular value of `M`, `l_1`, must satisfy `l_1^2/p \\to 0`.",
      "C": "The smallest singular value of `M`, `l_q`, must satisfy `l_q^2/p \\to 0`.",
      "D": "The true matrix `M` must have reduced rank (`l_q = 0`)."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the key identification condition for the proposed variance estimator. It uses a Scenario Application strategy. The distractors are designed based on common errors: (A) is the conceptual opposite, the condition under which bias is likely; (B) involves formula misuse by confusing the largest and smallest singular values; (D) is an 'Almost Right' error, as it is a sufficient but not necessary condition, making the correct answer more precise.",
    "qid": "91",
    "model_evaluation_result": {
      "model_raw_response": "C) The smaller singular value of M, l_q, must satisfy l_q^2 /p → 0.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 92,
    "Question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive Total Least Squares (TLS) estimator selects the rank of the estimated matrix `\\hat{M}` by keeping only the singular components of `X` that have a high estimated signal-to-noise ratio.\n\n---\n\n### Data / Model Specification\n\nThe adaptive TLS estimator selects its rank `\\hat{k}` according to the rule:\n```latex\n\\hat{k} = \\#\\{j : \\hat{\\pi}_j > 1/2\\}, \\quad \\text{where} \\quad \\hat{\\pi}_j = 1 - p\\hat{\\sigma}^2/\\hat{l}_j^2\n```\nThe variance `\\sigma^2` is estimated using the Smallest Singular Value (SSV) estimator, `\\hat{\\sigma}^2 = \\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p`.\n\n---\n\n### Question\n\nSuppose the condition for the SSV estimator to be consistent fails, such that the true signal `l_q^2/p` converges to a positive constant `c > 0` as `p \\to \\infty`. How does this affect the rank `\\hat{k}` selected by the adaptive TLS procedure in large samples?",
    "Options": {
      "A": "The selected rank `\\hat{k}` will be systematically too high because the procedure mistakes signal for noise.",
      "B": "The selected rank `\\hat{k}` will be systematically too low because the overestimated noise level leads to overly conservative thresholding.",
      "C": "The rank selection will be asymptotically unbiased because the bias in `\\hat{\\sigma}^2` is a lower-order effect that does not affect the thresholding rule.",
      "D": "The effect on rank is ambiguous, as the upward bias in `\\hat{\\sigma}^2` is offset by the upward bias in the other estimated singular values `\\hat{l}_j^2`."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the ability to trace the consequences of a parameter identification failure through a multi-step reasoning chain. It uses the Computational Judgment strategy. The distractors map to predictable error paths: (A) is the Conceptual Opposite, reversing the direction of the final effect; (C) represents a Step-Omission Error, failing to propagate the bias through the formula; (D) is a plausible but incorrect hedge that fails to recognize the dominant, systematic effect of the biased variance estimate.",
    "qid": "92",
    "model_evaluation_result": {
      "model_raw_response": "B) The Selected rank `\\hat k` will be systematically low because the Overestimated noise level Leads to overly conservative Thresholding.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 93,
    "Question": "### Background\n\nThe paper develops 'adaptive' estimators (e.g., `\\hat{M}_{TS}`) that are computable from data. It compares their performance to 'oracle' estimators (e.g., `\\widetilde{M}_{TS}`), which are defined as the best possible estimators within a certain class but are infeasible because they depend on the unknown true parameters of the model.\n\n---\n\n### Data / Model Specification\n\nA key theoretical result (Theorem 4.2) establishes that, under certain assumptions, the risk of the adaptive Total Shrinkage estimator converges to the risk of the oracle Total Shrinkage estimator as the number of rows `p` tends to infinity:\n```latex\n\\lim_{p \\to \\infty} |R(\\hat{M}_{TS}, M, \\sigma^2) - R(\\widetilde{M}_{TS}, M, \\sigma^2)| = 0\n```\n\n---\n\n### Question\n\nWhat is the primary theoretical significance of this finding that the risk of the adaptive estimator converges to the risk of the oracle estimator?",
    "Options": {
      "A": "It proves that the adaptive estimator becomes unbiased as the sample size grows.",
      "B": "It shows that the adaptive estimator is optimal for small sample sizes, where estimation is most difficult.",
      "C": "It demonstrates that the singular value decomposition provides a computationally stable method for estimation.",
      "D": "It validates that the data-driven procedure of minimizing estimated risk successfully approximates the theoretically best possible performance in large samples."
    },
    "Answer": "D",
    "pi_justification": "This item assesses the core conceptual takeaway of the paper's asymptotic theory. The distractors are designed to probe for common misunderstandings: (A) is an 'Almost Right' error that misidentifies the goal (risk reduction, not unbiasedness); (B) is a 'Conceptual Opposite' regarding the asymptotic nature of the result; (C) is 'True but Irrelevant', confusing a computational property with the theoretical significance of risk convergence.",
    "qid": "93",
    "model_evaluation_result": {
      "model_raw_response": "D) It demonstrates... \n\n(Note: I've only provided the answer as per the required format, without any additional text.)",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 94,
    "Question": "## Background\n\n**Research Question.** This problem examines the foundational conditions for the existence of a quasi-Lindahl equilibrium, focusing on the critical role of assumptions about consumer preferences.\n\n**Setting / Institutional Environment.** The economy consists of a continuum of consumers and a finite number of producers, with both private and public goods. The existence proof for equilibrium relies on a fixed-point theorem, which requires that the consumer's quasi-demand correspondence is non-empty and convex-valued.\n\n**Variables & Parameters.**\n\n*   $x(t) = (x_\\pi(t), x_g) \\in R^{n+l}$: A consumption bundle for consumer $t$.\n*   $p = (p_\\pi, p_g) \\in S$: A price system for private and public goods.\n*   $\\delta(t) \\in R_+^l$: A vector of personalized Lindahl shares for consumer $t$.\n*   $\\gtrsim_t$: The preference relation for consumer $t$.\n*   $x >_t y$: Strict preference, defined as $x \\gtrsim_t y$ and not $y \\gtrsim_t x$.\n*   $D^k(p, \\delta, t)$: The quasi-demand set for consumer $t$, containing all bundles in their (compactified) choice set that are not strictly preferred by any other affordable bundle.\n\n---\n\n## Data / Model Specification\n\nThe existence proof requires showing that the individual quasi-demand set, $D^k(p, \\delta, t)$, is convex. The proof proceeds by contradiction:\n\n1.  Take any two bundles $z^1, z^2$ from the quasi-demand set $D^k$.\n2.  Form their convex combination $\\bar{z} = \\lambda z^1 + (1-\\lambda)z^2$.\n3.  Assume $\\bar{z}$ is *not* in $D^k$. This means there exists an affordable bundle $y$ such that $y >_t \\bar{z}$.\n4.  Since $z^1 \\in D^k$, it cannot be that $y >_t z^1$. From this, infer that $z^1 \\gtrsim_t y$. (Step A)\n5.  Similarly, since $z^2 \\in D^k$, it cannot be that $y >_t z^2$. From this, infer that $z^2 \\gtrsim_t y$. (Step B)\n6.  Using the results from Steps A and B, infer that $\\bar{z} \\gtrsim_t y$. (Step C)\n7.  This leads to a contradiction, since $\\bar{z} \\gtrsim_t y$ is incompatible with the assumption $y >_t \\bar{z}$.\n\nKey preference axioms are:\n*   **Completeness:** For any two bundles $x,y$, either $x \\gtrsim_t y$ or $y \\gtrsim_t x$.\n*   **Convexity:** If $x \\gtrsim_t y$, then $\\lambda x + (1-\\lambda)y \\gtrsim_t y$ for all $\\lambda \\in [0,1]$.\n\n---\n\n## Question\n\nWhich step in the proof outline above relies critically on the **completeness** of preferences?",
    "Options": {
      "A": "Step A: Inferring $z^1 \\gtrsim_t y$ from the fact that it is not the case that $y >_t z^1$.",
      "B": "Step C: Inferring $\\bar{z} \\gtrsim_t y$ from $z^1 \\gtrsim_t y$ and $z^2 \\gtrsim_t y$.",
      "C": "Step 2: Establishing that the convex combination $\\bar{z}$ is in the consumer's budget set.",
      "D": "Step 7: Reaching a contradiction between $\\bar{z} \\gtrsim_t y$ and $y >_t \\bar{z}$."
    },
    "Answer": "A",
    "pi_justification": "This question assesses the student's understanding of the precise logical role of the completeness axiom in microeconomic proofs. It uses a Reverse-Reasoning strategy, asking the student to identify the specific step in a proof that relies on the axiom. Distractor (B) targets the common confusion between the roles of completeness and convexity. Distractors (C) and (D) test whether the student can distinguish between properties derived from preferences versus those from the budget set or logical definitions.",
    "qid": "94",
    "model_evaluation_result": {
      "model_raw_response": "B) Step B: Inferring that $z2 \\gtraitht y$.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 95,
    "Question": "## Background\n\n**Research Question.** This problem examines the foundational conditions for the existence of a quasi-Lindahl equilibrium, focusing on the critical role of assumptions about consumer preferences.\n\n**Setting / Institutional Environment.** The economy consists of a continuum of consumers and a finite number of producers, with both private and public goods. The existence proof for equilibrium relies on a fixed-point theorem, which requires that the consumer's quasi-demand correspondence is non-empty and convex-valued.\n\n**Variables & Parameters.**\n\n*   $x(t) = (x_\\pi(t), x_g) \\in R^{n+l}$: A consumption bundle for consumer $t$.\n*   $p = (p_\\pi, p_g) \\in S$: A price system for private and public goods.\n*   $\\delta(t) \\in R_+^l$: A vector of personalized Lindahl shares for consumer $t$.\n*   $\\gtrsim_t$: The preference relation for consumer $t$.\n*   $x >_t y$: Strict preference, defined as $x \\gtrsim_t y$ and not $y \\gtrsim_t x$.\n*   $D^k(p, \\delta, t)$: The quasi-demand set for consumer $t$, containing all bundles in their (compactified) choice set that are not strictly preferred by any other affordable bundle.\n\n---\n\n## Data / Model Specification\n\nThe existence proof relies on key assumptions about preferences, including completeness and convexity. The Fan-Glicksberg fixed-point theorem, used to prove existence, requires the quasi-demand correspondence to be convex-valued. This, in turn, requires that each individual's quasi-demand set, $D^k(p, \\delta, t)$, is a convex set.\n\nConsider a scenario where a consumer's preferences are **not complete**. Suppose $z^1$ and $z^2$ are two distinct bundles in the consumer's budget set, and both are in the quasi-demand set $D^k$. Now, consider their convex combination $\\bar{z} = 0.5z^1 + 0.5z^2$. Because preferences are incomplete, there exists another affordable bundle $y$ which is incomparable to both $z^1$ and $z^2$, but is strictly preferred to their combination, i.e., $y >_t \\bar{z}$.\n\n---\n\n## Question\n\nGiven this scenario, what is the direct mathematical consequence for the quasi-demand set $D^k$ and the overall proof strategy?",
    "Options": {
      "A": "The set $D^k$ is not convex, which invalidates a necessary condition for the Fan-Glicksberg fixed-point theorem.",
      "B": "The set $D^k$ must be empty, because no bundle can be optimal if preferences are incomplete.",
      "C": "The set $D^k$ is not closed, violating the continuity assumptions of the model.",
      "D": "The consumer's budget set must be non-linear, which is a contradiction."
    },
    "Answer": "A",
    "pi_justification": "This question tests the consequence of violating the completeness axiom for the structure of the optimal choice set. It uses a Scenario Application strategy. The correct answer identifies non-convexity as the key failure. Distractor (B) confuses ill-behaved with non-existent. Distractor (C) confuses convexity with the topological property of being closed. Distractor (D) incorrectly attributes the problem to the budget constraint rather than the preference structure.",
    "qid": "95",
    "model_evaluation_result": {
      "model_raw_response": "A) The correct answer is A. The set D^k is not convex.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 96,
    "Question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`), with a total endowment normalized to 1. Production in each industry is subject to industry-level external economies of scale, given by the technology:\n```latex\ny_j = g(y_j) L_j \\quad \\text{where} \\quad g(x) = x^{1/2}\n```\nThis technology results in a production possibilities curve (PPC) described by the equation:\n```latex\ny_1^{1/2} + y_2^{1/2} = 1 \\quad \\text{(Eq. 1)}\n```\nThis PPC is non-convex (bowed-in towards the origin). In autarky, two identical countries are at a symmetric equilibrium `C_0` with a price ratio of 1. Under free trade, it is possible for one country to specialize in good 1 (producing at `P'=(1,0)`) and the other in good 2 (producing at `P''=(0,1)`), while the world price ratio remains 1.\n\n### Question\n\nGiven this setup, what is the fundamental reason that both identical countries can achieve a higher level of welfare through trade, despite no change in the price ratio?",
    "Options": {
      "A": "Specialization allows countries to produce at the endpoints of the non-convex PPC, enabling them to trade along a 'chord' that lies outside their original individual production sets.",
      "B": "Gains arise because specialization allows each country to move down its marginal cost curve, leading to a lower world price for both goods.",
      "C": "Gains are driven by underlying differences in factor endowments between the two countries, which manifest as a comparative advantage.",
      "D": "The production technology implies a convex production set (bowed-out PPC), where specialization according to comparative advantage is always optimal."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the understanding of the central, counter-intuitive result from the paper's Appendix. It uses a Scenario Application strategy, asking the user to identify the correct economic mechanism for gains from trade in a non-convex economy. The distractors are designed to target common misconceptions: B suggests a price-based mechanism that isn't present, C incorrectly applies standard comparative advantage theory, and D fundamentally misinterprets the geometry of the production set, which is the key to the entire problem.",
    "qid": "96",
    "model_evaluation_result": {
      "model_raw_response": "A)󠁧󠁢󠁳󠁣󠁴󠁿",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 97,
    "Question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`). Production in each industry is subject to industry-level external economies of scale. With labor's wage as the numeraire, the average cost of production for good `j` is `AC_j = y_j^{-1/2}`. The relative supply price is therefore:\n```latex\n\\frac{AC_2}{AC_1} = \\left(\\frac{y_1}{y_2}\\right)^{1/2} \\quad \\text{(Eq. 1)}\n```\nIn autarky, two identical countries are at a symmetric equilibrium `C_0` where `y_1 = y_2`. At this point, the relative supply price is 1, matching the world demand price ratio. \n\n### Question\n\nAccording to a Marshallian stability analysis, how should the symmetric equilibrium point `C_0` be characterized when the countries open to trade?",
    "Options": {
      "A": "Stable, because a small increase in the output of one good raises its average cost, creating an incentive to return to the symmetric equilibrium.",
      "B": "Unstable, because a small increase in the output of one good lowers its average cost, creating an incentive for further expansion until specialization is complete.",
      "C": "Neutrally stable, because at the world price ratio of 1, there is no incentive to move away from the symmetric production point.",
      "D": "Unstable, because the marginal cost of production is always lower than the world price, leading to infinite production."
    },
    "Answer": "B",
    "pi_justification": "This item tests the user's ability to apply the concept of Marshallian stability to the specific model from the paper's Appendix. It uses a Reverse-Reasoning strategy, asking for the characterization of an equilibrium point. The distractors are based on predictable errors: A applies the incorrect stability logic from a standard CRS model, C correctly identifies the equilibrium condition but fails to perform the stability test, and D confuses the relevant cost concepts.",
    "qid": "97",
    "model_evaluation_result": {
      "model_raw_response": "B) Unstable, because a slight increase in the production of one good will lower its average cost and create an incentive for the country to specialize further.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 98,
    "Question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n**Variables & Parameters.**\n- `g(X, θ)`: A vector of `r` moment functions.\n- `θ`: A `p`-dimensional vector of parameters.\n\n---\n\n### Data / Model Specification\n\nThe validity of the overidentifying restrictions is evaluated by testing the null hypothesis `H₀` against the alternative `H₁`:\n\n```latex\nH_{0}: \\mathbb{E}[g(X, \\theta)] = 0 \\quad \\text{for some } \\theta \\in \\Theta \n```\n```latex\nH_{1}: \\mathbb{E}[g(X, \\theta)] \\neq 0 \\quad \\text{for any } \\theta \\in \\Theta\n```\n\nThe test statistic for `H₀` has a limiting distribution `T_n \\xrightarrow{d} \\chi^2(r-p)`.\n\n---\n\n### Question\n\nWhich of the following statements provides the most accurate description of the test for overidentifying restrictions?",
    "Options": {
      "A": "It tests the null hypothesis that the economic model is correctly specified, and its degrees of freedom are `r-p` because `p` degrees of freedom are used to estimate the model parameters.",
      "B": "It tests the null hypothesis that the individual parameters in `θ` are statistically significant, with `r` degrees of freedom.",
      "C": "It is primarily used for just-identified models (`r=p`) to confirm that the model perfectly fits the sample data.",
      "D": "It determines the optimal number of moment conditions `r` to use for a given number of parameters `p`, with `r-p` degrees of freedom."
    },
    "Answer": "A",
    "pi_justification": "This question assesses fundamental knowledge about overidentification tests. It uses a direct query strategy to test three core concepts: the meaning of the null hypothesis (model specification), the condition for the test's relevance (`r>p`), and the logic for the degrees of freedom (`r-p`). Distractors are designed based on common conceptual errors: B confuses model specification with parameter significance and uses the wrong degrees of freedom; C confuses over- with just-identification; D confuses the purpose of the test with model selection.",
    "qid": "98",
    "model_evaluation_result": {
      "model_raw_response": "B) It is a well-formulated and accurate description of a test for over-identifying restrictions in an over-identified model.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 99,
    "Question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n---\n\n### Data / Model Specification\n\nThe Empirical Likelihood (EL) test statistic for the null hypothesis `H₀: E[g(X, θ)] = 0` for some `θ` is denoted `T_n`. A small value of `T_n` indicates that the model's restrictions are compatible with the observed data.\n\n---\n\n### Question\n\nAn econometrician performs the EL overidentification test and obtains a small value for the test statistic `T_n`, leading to a failure to reject the null hypothesis `H₀`. What is the correct interpretation of this result and the primary inferential risk associated with it?",
    "Options": {
      "A": "The result suggests the model's restrictions are compatible with the data, but the primary risk is a Type II error, as the test may have low power to detect meaningful model misspecifications.",
      "B": "The result indicates a severe model misspecification, and the primary risk is a Type I error (falsely rejecting a valid model).",
      "C": "The result proves the model is correctly specified, and the main remaining risk is that the parameter estimates `θ` are biased in finite samples.",
      "D": "The small `T_n` value implies the instruments are weak, and the primary risk is that the standard `χ²(r-p)` distribution is a poor approximation to the test statistic's true distribution."
    },
    "Answer": "A",
    "pi_justification": "This question uses a Reverse-Reasoning strategy to assess the user's understanding of test interpretation and inferential risks. Given a specific outcome (failure to reject), the user must identify the correct conclusion and its main caveat. The correct answer (A) correctly identifies the interpretation (compatibility) and the risk (Type II error/low power). Distractors are based on high-fidelity errors: B is the conceptual opposite, misinterpreting the result and confusing Type I/II errors. C overstates the conclusion ('proves') and introduces a different, though related, issue (estimator bias). D confuses the concept of model misspecification (tested by `T_n`) with weak identification and size distortion, which are distinct econometric problems.",
    "qid": "99",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: The answer is the single letter \"A\".)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 100,
    "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `p(q_n)`: Price of hospital care for non-covered patients.\n- `AC(q_n + q_m; α)`: Average cost of care, which is a function of total patients and an inefficiency parameter `α`.\n- `α`: An X-inefficiency shift parameter, where a higher `α` increases average cost (`∂AC/∂α > 0`).\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe hospital's profit function is given by:\n```latex\n\\pi = p(q_n) \\cdot q_n + k \\cdot q_m \\cdot AC(q_n+q_m; \\alpha) - (q_n+q_m) \\cdot AC(q_n+q_m; \\alpha) \\quad \\text{(Eq. (1))}\n```\n\nUnder the profit-maximization model, a hospital has a direct financial incentive to increase its own inefficiency (i.e., increase `α` to raise `AC`) only if which of the following conditions is met?",
    "Options": {
      "A": "The share of cost-reimbursed patients, `q_m / (q_n + q_m)`, exceeds the reciprocal of the reimbursement factor, `1/k`.",
      "B": "The 'plus' portion of revenue from reimbursed patients, `(k-1) * q_m * AC`, exceeds the total cost of non-reimbursed patients, `q_n * AC`.",
      "C": "The number of cost-reimbursed patients, `q_m`, exceeds the number of non-reimbursed patients, `q_n`.",
      "D": "The reimbursement factor, `k`, is greater than 1."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to derive or recognize the key theoretical condition from the paper's profit-maximization model. It uses a Reverse-Reasoning strategy. The correct option (A) is the derived result. Distractors are designed based on common conceptual errors: (B) compares the wrong components of the profit function, (C) represents an intuitive but incorrect simplification, and (D) states a necessary but insufficient condition.",
    "qid": "100",
    "model_evaluation_result": {
      "model_raw_response": "C) The 'number of cost-reimursed patients, q_m, exceeds the numberof non-reimbured patients, q_n'.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 101,
    "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `AC`: Average cost of care.\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe paper shows that under both profit-maximization and output-maximization models, a hospital has an incentive to inflate its costs if and only if the following condition holds:\n```latex\n\\frac{q_m}{q_n + q_m} > \\frac{1}{k}\n```\n\nA hospital operates in an environment described by these models. If its cost-reimbursement plan includes a 5% 'plus' factor (i.e., `k=1.05`), what is the minimum percentage of the hospital's patients that must be covered by this plan for the hospital to find it advantageous to increase costs?",
    "Options": {
      "A": "5.0%",
      "B": "80.0%",
      "C": "95.2%",
      "D": "99.5%"
    },
    "Answer": "C",
    "pi_justification": "This item tests the student's ability to apply the derived theoretical condition to a concrete numerical scenario. It uses a Computational Judgment strategy. The correct option (C) requires correct substitution and calculation. Distractors are based on predictable errors: (A) misuses the 'plus' factor directly as the threshold (Formula Misuse), (B) is a salient but irrelevant number from the text (True but Irrelevant), and (D) represents a plausible calculation slip (Numerical Error).",
    "qid": "101",
    "model_evaluation_result": {
      "model_raw_response": "C) 94.7% (approximately)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 102,
    "Question": "### Background\n\nThe paper modifies the standard Keynesian liquidity preference function. In the standard model, speculative money balances `M_s` depend only on the interest rate `r`. The author proposes a more general function where `M_s` also depends on an equity price index `z` and the general price level `P`.\n\n### Data / Model Specification\n\nThe general form for speculative balances is given by:\n```latex\nM_s = K(r, z, P)\n```\nBased on portfolio theory, the demand for speculative cash balances is assumed to decrease when the return on alternative assets increases. Therefore, the partial derivatives are:\n*   `∂K/∂r < 0`: Higher interest rates on bonds reduce demand for cash.\n*   `∂K/∂z < 0`: Higher equity prices (implying high returns) reduce demand for cash.\n*   `∂K/∂P < 0`: A higher general price level (inflation) erodes the value of cash, reducing demand for it.\n\n### Question\n\nConsider a stagflationary shock where the general price level `P` rises, but corporate profits fall, causing the equity price index `z` to fall. Based on the model `M_s = K(r, z, P)`, what is the net effect of these two changes on the demand for speculative money balances `M_s`?\n",
    "Options": {
      "A": "`M_s` will unambiguously increase.",
      "B": "`M_s` will unambiguously decrease.",
      "C": "The net effect on `M_s` is ambiguous.",
      "D": "`M_s` will remain unchanged as the two effects perfectly cancel."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the ability to apply the model's core monetary innovation to a novel scenario. It uses the 'Scenario Application' strategy. The correct answer requires synthesizing two opposing effects. Distractor A represents a 'Step-Omission Error' by only considering the falling equity prices (a flight to safety). Distractor B represents the same error by only considering rising general prices (the inflation hedge motive). Distractor D is implausible and tests for over-simplification.",
    "qid": "102",
    "model_evaluation_result": {
      "model_raw_response": "C) The(net effect on M_s is ambiguous.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 103,
    "Question": "### Background\n\nThe paper's full dynamic model specifies the money market equilibrium as follows:\n\n```latex\nM(t) = hP(t)y(t) + L[r(t), P(t-1)]\n```\n\nIn this formulation, current speculative balances depend on the current interest rate `r(t)` but on the *lagged* price level `P(t-1)`.\n\n### Question\n\nWhat is the most plausible behavioral justification offered in the paper for using the lagged price level `P(t-1)` instead of the current price level `P(t)` to determine speculative balances?\n",
    "Options": {
      "A": "Wealth-holders react more slowly and with more hesitation to general price level changes than to more immediate bond market signals.",
      "B": "The general price level `P(t)` is a statistical construct that is not observable by agents until after period `t` has concluded.",
      "C": "The lag reflects delays in the production process, as investment decisions must be made in the prior period.",
      "D": "The model assumes that a rising price level in the previous period mechanically causes a rising price level in the current period."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the behavioral reasoning behind a key modeling choice. It uses the 'Reverse-Reasoning' strategy. The correct answer (A) is the paper's explicit justification. Distractor B is a plausible 'True but Irrelevant' statement about data reporting. Distractor C is a 'Conceptual Opposite' error, confusing a monetary expectation lag with a real-side production lag. Distractor D introduces an unsupported assumption about price dynamics.",
    "qid": "103",
    "model_evaluation_result": {
      "model_raw_response": "B) Thegeneral price level `Pt` is a statisticalconstruct that is notobservable by agents untilafter period `t hasconcluded.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 104,
    "Question": "### Background\n\nEarly real business cycle models relied on Richard Rogerson's “employment lotteries” aggregation theory to generate a high aggregate labor supply elasticity from a model with indivisible labor. This framework was used by Edward Prescott to argue that differences in labor income tax rates could explain the large employment gap between the U.S. and Europe.\n\n### Data / Model Specification\n\nIn a static economy with a representative family, indivisible labor, and a flat labor tax `τ`, the elasticity of the equilibrium aggregate labor supply `N` with respect to the net-of-tax rate `(1-τ)` is given by:\n\n```latex\n\\eta_{N, 1-\\tau} = \\frac{\\gamma}{(1-\\tau)^2+\\gamma}\n\\quad \\text{(Eq. (1))}\n```\n\nA common calibration, consistent with balanced growth, sets `γ=1`.\n\n### Question\n\nUsing the model specified in Eq. (1) with `γ=1`, what is the approximate aggregate labor supply elasticity for a European-style economy with a labor income tax rate (τ) of 0.60?",
    "Options": {
      "A": "0.86",
      "B": "0.74",
      "C": "1.00",
      "D": "0.16"
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to apply a core formula from the paper to a specific case. It uses a Computational Judgment strategy. The distractors are designed based on common errors: (B) is the elasticity for the US tax rate (`τ=0.4`), a plausible mix-up; (C) is a salient value (`unity`) mentioned in the text for a limiting case, representing a conceptual error; (D) is an intermediate value from the calculation (`(1-τ)^2`), representing a step-omission error.",
    "qid": "104",
    "model_evaluation_result": {
      "model_raw_response": "C) 0assistant\r\n\r\nC",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 105,
    "Question": "### Background\n\nCasey Mulligan challenged the idea that labor indivisibility is the decisive factor for generating a high aggregate labor supply elasticity. He proposed a model that retains indivisible labor but introduces a population of heterogeneous agents, each with an idiosyncratic disutility of work, `v_i`, drawn from a distribution.\n\n### Data / Model Specification\n\nIn Mulligan's framework, an agent chooses to work if the net wage, `W`, exceeds their personal disutility of work, `v_i`. The aggregate labor supply `N` is the fraction of the population for whom this condition holds: `N = F(W)`, where `F(v)` is the cumulative distribution function (CDF) of work disutility. The elasticity of aggregate supply is determined by the density of workers at the margin.\n\n### Question\n\nAccording to Mulligan's framework, which characteristic of the work disutility distribution would result in a very *low* aggregate labor supply elasticity?",
    "Options": {
      "A": "The distribution is bimodal, with very low population density in the range of prevailing market wages.",
      "B": "The distribution is unimodal and tightly centered around the prevailing market wage.",
      "C": "The average disutility of work across the entire population is very high.",
      "D": "All individuals in the population have the identical disutility of work."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of a key theoretical critique discussed in the paper. It uses a Reverse-Reasoning strategy, asking for the cause of a specific outcome (low elasticity). The distractors are designed to probe for common misconceptions: (B) describes a situation leading to a *high* elasticity (Conceptual Opposite); (C) describes a factor affecting the level of employment but not necessarily its elasticity (True but Irrelevant); (D) describes the assumption of the homogeneous agent model that Mulligan's theory critiques, which leads to a high elasticity (Reverting to the critiqued model).",
    "qid": "105",
    "model_evaluation_result": {
      "model_raw_response": "B) Thedistribution is unimodaland tightly centered aroundthe prevailing market wage.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 106,
    "Question": "### Background\n\n**Research Question.** This question examines the paper's non-parametric evidence for the magnitude effect, which is the finding that allocation choices change as the total budget is scaled up.\n\n**Setting / Institutional Environment.** In Part I of an experiment, subjects make seven allocation choices for each of five different budget levels (€20, €40, €60, €80, €160). To test for a magnitude effect without assuming a specific utility function, the paper compares the vector of choices across pairs of budget levels using Hotelling’s T-squared tests. The null hypothesis is that the mean vectors of choices are equal.\n\n### Data / Model Specification\n\n**Table 1: Multivariate Mean Difference Tests (F-statistics) Between Total Budgets**\n\n| Comparison | €40 | €60 | €80 | €160 |\n| :--- | :--- | :--- | :--- | :--- |\n| **€20** | 3.318*** | 5.116*** | 6.367*** | 6.048*** |\n| **€40** | | 1.548 | 3.934*** | 3.943*** |\n| **€60** | | | 1.576 | 3.033*** |\n| **€80** | | | | 1.710 |\n\n*Notes: *** indicates p < 0.01.*\n\nBased on the pattern of significance in Table 1, what can be concluded about the nature of the magnitude effect?",
    "Options": {
      "A": "The effect is strongest at low stake levels and diminishes as stakes increase.",
      "B": "The effect is weak at low stake levels but becomes stronger as stakes increase.",
      "C": "The effect is statistically significant and has a constant strength across all budget increases.",
      "D": "The effect is not statistically significant between any adjacent budget levels."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the ability to interpret a table of statistical test results and identify a non-linear pattern. This item replaces the first part of a more complex QA problem. Strategy: Computational Judgment. Distractor Logic: (A) Correct interpretation of the 'concave' pattern (large initial jump, smaller subsequent changes). (B) Conceptual Opposite: claims a 'convex' pattern. (C) Step-Omission Error: correctly notes significance but misses the changing strength of the effect. (D) Factual Error: misreads the table, as several adjacent comparisons are indeed significant (e.g., €20 vs €40).",
    "qid": "106",
    "model_evaluation_result": {
      "model_raw_response": "C) The efect is statistically significant has a constant strenght across all budget increaes.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 107,
    "Question": "### Background\n\n**Research Question.** This question assesses understanding of a key methodological assumption—intertemporal independence—and the consequences of its violation.\n\n**Setting / Institutional Environment.** The paper's main parametric analysis relies on a time-separable utility model, which assumes that choices between two periods (e.g., today vs. 4 weeks) are not affected by consumption in a third period (e.g., 8 weeks). The authors test this assumption by comparing choices in a standard two-period task from Part I with choices in a specially designed three-period task from Part II where consumption in the third period is non-zero. The empirical test supported the assumption of intertemporal independence.\n\n### The Question\n\nConsider a hypothetical scenario where the test for intertemporal independence had failed. Specifically, assume that for the Present Group (allocating between today and 4 weeks), consumption is intertemporally *complementary*, meaning the marginal utility of consumption at 4 weeks *increases* with consumption at 8 weeks. \n\nIn this scenario, how would the allocation to the 4-week option in the Part II task (where 8-week consumption was positive) have compared to the allocation in the equivalent Part I task (where 8-week consumption was zero)?",
    "Options": {
      "A": "The allocation to the 4-week option would be systematically higher in Part II.",
      "B": "The allocation to the 4-week option would be systematically lower in Part II.",
      "C": "The allocation would be unaffected because the third-period reward is exogenous.",
      "D": "The allocation would be higher for some interest rates and lower for others, with no systematic bias."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests deep reasoning about the implications of non-separable preferences. This item replaces the most difficult part of a complex QA problem. Strategy: Reverse-Reasoning. Distractor Logic: (A) Correct: complementarity with positive 8-week consumption raises the marginal utility of 4-week consumption, shifting allocation towards it. (B) Conceptual Opposite: this would be the result of intertemporal substitutability, not complementarity. (C) Formula Misuse: incorrectly applies the logic of separability to a non-separable case. (D) True but Irrelevant: a common hedge that avoids the core logic; the direction of the bias is systematic, not ambiguous.",
    "qid": "107",
    "model_evaluation_result": {
      "model_raw_response": "B) Theallocation to the 40-week option wouldbe systematically lower inPart II.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 108,
    "Question": "### Background\n\nOne proposed solution to the persistence problem is to assume firms face a **convex demand curve**, where demand becomes more elastic as the firm raises its price. This can make firms' optimal prices less sensitive to changes in marginal cost.\n\n---\n\n### Data / Model Specification\n\nThe sensitivity of a firm's price to its marginal cost is captured by a pass-through parameter `\\varphi`:\n```latex\n\\varphi=\\frac{1-(1/\\varepsilon)}{2-(\\chi/\\varepsilon)}\n```\n(Eq. 1)\nwhere `\\varepsilon` is the steady-state demand elasticity and `\\chi` is a parameter governing demand curvature (more negative `\\chi` means more convexity). A second-order Taylor expansion of the demand function `d(P)` around the steady-state relative price of 1 is:\n```latex\nd(P) \\approx 1 - \\varepsilon(P-1) + \\frac{\\varepsilon \\chi}{2}(P-1)^2\n```\n(Eq. 2)\n\n---\n\n### Question\n\nThe paper calculates that to achieve even a modest contract multiplier, a demand curvature of `\\chi = -289` is required. Using the demand approximation in Eq. (2) with a steady-state demand elasticity `\\varepsilon = 10`, what is the approximate percentage drop in a firm's demand if it raises its relative price by 2.3% (i.e., `P=1.023`)?",
    "Options": {
      "A": "Demand falls by 23%.",
      "B": "Demand falls by 33%.",
      "C": "Demand falls by 99%.",
      "D": "Demand falls by 76%."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the ability to perform a quantitative critique of a theoretical model. It uses a Computational Judgment strategy. The correct answer requires applying a second-order Taylor expansion correctly. Distractor A (23%) results from ignoring the second-order term, a step-omission error. Distractor B (33%) results from a formula misuse error (failing to square the deviation in the second term). Distractor D (76%) results from ignoring the first-order term.",
    "qid": "108",
    "model_evaluation_result": {
      "model_raw_response": "C) Demand Falls by 99%. \n\nNote: To solve this problem, we can plug in the given values into the demand approximation equation (Eq. 2):\n\nd(P) ≈ 1 - 10(0.023) + (10 * -289/2)(0.023)^2\nd(P ≈ 0.9997\n\nSo, the demand drops by approximately 0.0023, which is 0.23% of the original demand.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 109,
    "Question": "### Background\n\nAnother proposed solution to the persistence problem is to assume that each firm's production requires a **specific factor** (e.g., land) in addition to labor and capital. This leads to decreasing returns to scale at the firm level, which can, under certain conditions, make prices less sensitive to aggregate shocks.\n\n---\n\n### Data / Model Specification\n\nIn this model, the relative output of two firms `i` and `j` is given by:\n```latex\ny_i/y_j = (P_j/P_i)^\\varepsilon\n```\n(Eq. 1)\nwhere `\\varepsilon` is the elasticity of demand for the firms' products.\n\n---\n\n### Question\n\nThe paper calculates that to achieve an empirically realistic contract multiplier of 20, a demand elasticity of `\\varepsilon = 6500` is required. Using the relative output formula in Eq. (1), what is the implied ratio of firm `i`'s output to firm `j`'s output (`y_i/y_j`) if firm `j`'s price is just 0.1% higher than firm `i`'s (i.e., `P_j/P_i = 1.001`)?",
    "Options": {
      "A": "The ratio is approximately 7.5.",
      "B": "The ratio is approximately 665.",
      "C": "The ratio is approximately 6,507.",
      "D": "The ratio is approximately 1.0."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the ability to perform a quantitative critique of a theoretical model using a Computational Judgment strategy. The correct answer requires calculating an exponential. Distractor A (7.5) results from a common linear approximation error (`1+nx`). Distractor C (6,507) results from incorrectly multiplying instead of exponentiating, a formula misuse error. Distractor D (1.0) represents a failure to understand the model, assuming no effect.",
    "qid": "109",
    "model_evaluation_result": {
      "model_raw_response": "C) The ration is approximately 6507.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 110,
    "Question": "### Background\n\nTwo proposed solutions to the 'persistence problem' in monetary models are (1) assuming firms face convex demand curves and (2) assuming firms use specific factors of production. In both cases, the models can generate macroeconomic persistence, but only by assuming extreme parameter values.\n\n---\n\n### Data / Model Specification\n\nFor example, the convex demand model requires a parameterization implying that a 2.3% price increase causes demand to drop to nearly zero. The specific factors model requires a parameterization implying that a 0.1% price difference causes a 665-fold difference in sales.\n\n---\n\n### Question\n\nGiven these findings, what is the paper's central critique of these proposed solutions?",
    "Options": {
      "A": "The models are microeconomically plausible but fail to generate sufficient macroeconomic persistence.",
      "B": "The models achieve the desired macroeconomic outcome but rely on empirically implausible microeconomic foundations.",
      "C": "The models are too computationally complex to be useful for policy analysis, regardless of their plausibility.",
      "D": "The models show that persistence is possible, but only in economies with extremely high demand elasticity."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the ability to synthesize the results of quantitative critiques into a general conclusion about modeling strategy. It uses a conceptual judgment strategy. The correct answer (B) accurately summarizes the paper's argument about the trade-off between macro fit and micro plausibility. Distractor A is the conceptual opposite. Distractor C is a 'true but irrelevant' style critique, as the paper's focus is on plausibility, not complexity. Distractor D is a correct statement about one model but fails to capture the general, synthesized critique of both.",
    "qid": "110",
    "model_evaluation_result": {
      "model_raw_response": "B) The.models achieve the desired macreconomic outcome but rely.on empirically implausble microeconomic foundations.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 111,
    "Question": "## Background\n\nThis problem explores the geometric conditions that determine whether a fully revealing equilibrium (FRE) can exist in a constrained policy space, and how to constructively test for this existence. We consider a cheap talk game where the directions of the two senders' biases are deterministic, given by linearly independent vectors $b_1, b_2 \\in \\mathbb{R}^p$. The policy space $Y \\subseteq \\mathbb{R}^p$ is a closed, convex set.\n\n---\n\n## Data / Model Specification\n\nA key result (Proposition 6 in the paper) provides a constructive test for the existence of a FRE. It states that a FRE exists if and only if for any reports $\\theta', \\theta'' \\in Y$, the following set has a non-empty intersection with the policy space $Y$:\n\n```latex\nM_{b_{1},b_{2}}(\\theta^{\\prime},\\theta^{\\prime\\prime})=\\left\\{x\\in\\mathbb{R}^{p}\\middle|\\begin{array}{l}{b_{1}\\cdot x=\\operatorname*{min}\\{b_{1}\\cdot\\theta^{\\prime},b_{1}\\cdot\\theta^{\\prime\\prime}\\}}\\\\{b_{2}\\cdot x=\\operatorname*{min}\\{b_{2}\\cdot\\theta^{\\prime},b_{2}\\cdot\\theta^{\\prime\\prime}\\}}\\end{array}\\right\\} \\quad \\text{(Eq. 1)}\n```\n\nA receiver strategy that always picks a point in this intersection is called the \"min rule.\"\n\n---\n\n## Question\n\nConsider a scenario in $\\mathbb{R}^2$ with bias vectors $b_1=(1,0)$ and $b_2=(0,1)$. The senders issue conflicting reports $\\theta'=(5,2)$ and $\\theta''=(3,6)$. The feasible policy space is a circle defined by $Y = \\{y \\in \\mathbb{R}^2 | (y_1-1)^2 + (y_2-1)^2 \\le 4 \\}$. Based on the constructive test in Eq. (1), does a fully revealing equilibrium (FRE) exist in this environment?",
    "Options": {
      "A": "No, a FRE does not exist because the min-rule punishment point (3,2) is not a feasible policy.",
      "B": "Yes, a FRE exists because the min-rule punishment point (3,2) is a feasible policy.",
      "C": "Yes, a FRE exists because the origin (0,0) can serve as a punishment and is a feasible policy.",
      "D": "The existence of a FRE cannot be determined without knowing the magnitudes of the senders' biases."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the application of a core constructive theorem (the 'min rule'). It uses the 'Computational Judgment' strategy, requiring a multi-step process: 1) calculate the specific punishment point using the min rule formula, and 2) test if that point lies within the constrained policy space to determine if a FRE exists. The distractors are designed to diagnose specific errors: (B) represents an error in the final feasibility check (a calculation slip on the inequality); (C) represents a conceptual error, substituting a generic 'safe' point for the required min-rule point; (D) targets a fundamental misunderstanding of the model's core assumption that punishments must work for arbitrarily large biases, making the analysis independent of specific magnitudes.",
    "qid": "111",
    "model_evaluation_result": {
      "model_raw_response": "C) Yes,a FRE exists because origin (0.0) can be served as a punishment is a feasible solution.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 112,
    "Question": "### Background\n\nThis problem investigates the paper's central claim: the robustness of the standard F-test to non-normal errors is critically determined by the characteristics of the regressor design matrix, particularly the presence of high-leverage observations. The analysis hinges on an analytical expression for the deviation of the true significance level (`α*`) from its nominal level (`α`), and on diagnostic measures derived from the regressor matrix.\n\n### Data / Model Specification\n\nThe paper derives the actual significance level `α*` as a function of the nominal level `α`, the error distribution's cumulants (`λᵢ`), and the regressor design matrix (`X`). The relationship is given by:\n\n```latex\n\\alpha^{*} = \\alpha + b(W_{0};\\nu_{2},\\nu_{1})d(W_{0};\\lambda,X)\n```\n\nThe term `d(...)` is an infinite series in the error cumulants (e.g., `λ₃` for skewness, `λ₄` for kurtosis) whose coefficients depend on the regressor matrix. The paper shows that the sensitivity to kurtosis (`λ₄`) is heavily influenced by a diagnostic function of the regressors:\n\n*   **`HX(4)`**: Measures sensitivity to kurtosis (`λ₄`). It is defined as `HX(4) = \\sum_{i=1}^{n} h_{ii}^2`, where `hᵢᵢ` are the diagonal elements of the projection matrix `H` associated with the `q` regressors being tested. A large `HX(4)` value relative to its theoretical range indicates the presence of high-leverage points, which makes the F-test more sensitive to error kurtosis.\n\nThese functions have theoretical bounds, which are provided in **Table 1**. The paper analyzes several datasets, including two contrasting cases whose diagnostic values are provided in **Table 2** for the test of all slope coefficients being zero (`q=k`).\n\n**Table 1: Theoretical Bounds for HX-functions**\n\n| HX-function | Lower bound | Upper bound |\n| :--- | :--- | :--- |\n| `HX(4)` | `q²/n` | `q(1 - 1/n)` |\n\n**Table 2: Computed HX-function Values for Two Datasets (for q=k)**\n\n| Dataset | n | k | q | `HX(4)` |\n| :--- | :-: | :-: | :-: | :--- |\n| TEXTILE | 17 | 2 | 2 | 0.3 |\n| ANOVA1 | 15 | 5 | 5 | 4.4 |\n\n### Question\n\nBased on the theory and data provided, for which dataset is the F-test's validity more likely to be compromised by non-normal error kurtosis (`λ₄`), and why? Your reasoning should be based on the position of the observed `HX(4)` value within its theoretical range.",
    "Options": {
      "A": "ANOVA1, because its observed `HX(4)` of 4.4 is very close to its theoretical upper bound of approximately 4.67, indicating a design with severe leverage points.",
      "B": "TEXTILE, because its observed `HX(4)` of 0.3 is very close to its theoretical lower bound of approximately 0.24, and a design close to the lower bound is most sensitive to non-normality.",
      "C": "ANOVA1, because its sample size (n=15) is smaller than TEXTILE's (n=17), and smaller samples are always less robust.",
      "D": "TEXTILE, because its `HX(4)` value (0.3) is much smaller than ANOVA1's (4.4), and a smaller diagnostic value indicates greater sensitivity."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the user's ability to apply the paper's diagnostic tools. It uses a Computational Judgment strategy, requiring the user to (1) calculate the theoretical bounds for the `HX(4)` diagnostic for two datasets, (2) compare the observed values to these bounds, and (3) draw the correct conclusion about which design is more sensitive to non-normal errors. The distractors are designed to catch common errors: B confuses the meaning of the lower bound (Conceptual Opposite), C focuses on a true but irrelevant fact about sample size to misdirect from the core diagnostic (True but Irrelevant), and D makes a naive comparison of absolute values without normalization (Formula Misuse).",
    "qid": "112",
    "model_evaluation_result": {
      "model_raw_response": "A) ANOV1, because it's observed `HX` value of 4,4 is veryclose to its theoreticalupper bound ofapproximately 4.7,indicating a desigwithsevereleveragepoints.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 113,
    "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical foundations of the Analytic Policy Function Iteration (APFI) framework, which is designed to solve a general class of linear rational expectations models with endogenous information frictions.\n\n**Setting / Institutional Environment.** The APFI method is an iterative algorithm that finds an equilibrium policy function. Its validity and computational feasibility rest on three key mathematical theorems that justify its core operational steps.\n\n---\n\n### Data / Model Specification\n\nThe APFI framework is designed to solve models cast in the canonical form:\n```latex\n\\sum_{k=0}^{l}A_{k}y_{t-k}+\\sum_{k=0}^{h}B_{k}\\mathbb{E}_{t}y_{t+k}=\\mathbf{0}\n```\nThe **Baseline APFI Algorithm** involves three main steps in each iteration:\n1.  **Projection:** Approximate the current guess of the policy function using a finite-order VARMA process (i.e., a rational function).\n2.  **Evaluation:** Use the VARMA approximation to compute conditional expectations.\n3.  **Updating & Recursion:** Solve for an updated policy function and repeat until the function converges on a numerical grid.\n\nThis algorithm is justified by three foundational theorems from the paper:\n-   **Theorem 3.1 (Denseness of Rational Functions):** Any covariance-stationary equilibrium process, which resides in the Hardy space `H²`, can be approximated arbitrarily well by a VARMA process (a rational function).\n-   **Theorem 3.2 (Analytic Continuation & Convergence):** A solution found on the real unit interval `(-1, 1)` uniquely extends to the entire complex unit disk. Furthermore, pointwise convergence of a sequence of rational functions on this real interval implies convergence in the `H²` norm.\n-   **Theorem 3.3 (Annihilation):** The crucial \"annihilation\" step in computing expectations can be performed efficiently and accurately using the Inverse Discrete Fourier Transform (IDFT).\n\n---\n\n### The Questions\n\nAccording to the paper, what is the primary theoretical role of **Theorem 3.1 (Denseness of Rational Functions)** for the **Projection** step of the APFI algorithm?",
    "Options": {
      "A": "It proves that the true equilibrium solution is always a finite-order VARMA process.",
      "B": "It guarantees that the basis of VARMA processes is rich enough to approximate any possible stationary equilibrium solution arbitrarily well.",
      "C": "It ensures that the algorithm's iterative mapping is a contraction, guaranteeing a unique solution.",
      "D": "It provides the formula for calculating the VARMA coefficients from the policy function values on the grid."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the understanding of Theorem 3.1's role in justifying the APFI algorithm's projection step (Conceptual Judgment). The correct option (B) correctly identifies that denseness ensures the approximation basis is sufficient. Distractors target common conceptual errors: confusing approximation with identity (A, Almost Right), misattributing the theorem's role to ensuring uniqueness (C, Conceptual Error), or confusing the justification with a procedural detail (D, True but Irrelevant).",
    "qid": "113",
    "model_evaluation_result": {
      "model_raw_response": "B) It provides that the basis is rich enough",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 114,
    "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical foundations of the Analytic Policy Function Iteration (APFI) framework, which is designed to solve a general class of linear rational expectations models with endogenous information frictions.\n\n**Setting / Institutional Environment.** The APFI method is an iterative algorithm that finds an equilibrium policy function. Its validity and computational feasibility rest on three key mathematical theorems that justify its core operational steps.\n\n---\n\n### Data / Model Specification\n\nThe APFI framework is designed to solve models cast in the canonical form:\n```latex\n\\sum_{k=0}^{l}A_{k}y_{t-k}+\\sum_{k=0}^{h}B_{k}\\mathbb{E}_{t}y_{t+k}=\\mathbf{0}\n```\nThe **Baseline APFI Algorithm** involves three main steps in each iteration:\n1.  **Projection:** Approximate the current guess of the policy function using a finite-order VARMA process (i.e., a rational function).\n2.  **Evaluation:** Use the VARMA approximation to compute conditional expectations.\n3.  **Updating & Recursion:** Solve for an updated policy function and repeat until the function converges on a numerical grid.\n\nThis algorithm is justified by three foundational theorems from the paper:\n-   **Theorem 3.1 (Denseness of Rational Functions):** Any covariance-stationary equilibrium process, which resides in the Hardy space `H²`, can be approximated arbitrarily well by a VARMA process (a rational function).\n-   **Theorem 3.2 (Analytic Continuation & Convergence):** A solution found on the real unit interval `(-1, 1)` uniquely extends to the entire complex unit disk. Furthermore, pointwise convergence of a sequence of rational functions on this real interval implies convergence in the `H²` norm.\n-   **Theorem 3.3 (Annihilation):** The crucial \"annihilation\" step in computing expectations can be performed efficiently and accurately using the Inverse Discrete Fourier Transform (IDFT).\n\n---\n\n### The Questions\n\nThe paper argues that the standard analytical method for computing expectations (using the residue theorem) is infeasible for models with endogenous information, making the IDFT approach of **Theorem 3.3** necessary. What is the fundamental reason for this infeasibility?",
    "Options": {
      "A": "The residue theorem is numerically unstable and less accurate than the IDFT.",
      "B": "The residue theorem requires knowledge of the function's poles (singularities), but their locations are endogenous to the equilibrium and thus unknown beforehand.",
      "C": "The residue theorem cannot be applied to non-rational functions, which are common in these models.",
      "D": "The residue theorem is computationally much slower than the IDFT, making it impractical for large models."
    },
    "Answer": "B",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to probe the core challenge that Theorem 3.3 overcomes. It asks for the specific reason why the standard analytical method fails in the context of endogenous information. The correct answer (B) pinpoints the circularity of endogenous pole locations. Distractors offer plausible but incorrect reasons: focusing on accuracy/stability instead of feasibility (A), a related but secondary issue (C), and focusing on speed instead of feasibility (D).",
    "qid": "114",
    "model_evaluation_result": {
      "model_raw_response": "B) The residual theorem requires knowledge... etc.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 115,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the paper's core empirical findings on the interdependent choices of labor supply and housing demand, based on the estimated elasticities for different household types.\n\n**Setting / Institutional Environment.** A joint labor-housing demand system is estimated for several distinct demographic groups using data from the 1976 Panel Study of Income Dynamics. The outputs are wage and income elasticities of labor supply (hours worked) and housing demand. The analysis focuses on interpreting these elasticities to understand household decision-making and the crucial linkages between work and housing choices.\n\n**Variables & Parameters.**\n- `j_h`, `j_s`: Annual hours worked by the head (`h`) and spouse (`s`).\n- `w_h`, `w_s`: Hourly wage for the head and spouse.\n- `ε_{j_i, w_k}`: Elasticity of labor supply of person `i` with respect to the wage of person `k`.\n- Unit of observation: Household.\n\n---\n\n### Data / Model Specification\n\nSelected estimated elasticities for “Two-Earner Families with Children” are presented in the table below.\n\n**Table 1: Elasticities of Hours Employed**\n\n| Group | Head's Own-Wage (`ε_{j_h, w_h}`) | Spouse's Own-Wage (`ε_{j_s, w_s}`) | Head's Cross-Wage (`ε_{j_h, w_s}`) | Spouse's Cross-Wage (`ε_{j_s, w_h}`) |\n| :--- | :---: | :---: | :---: | :---: |\n| Two-Earner Families with Children | -0.002 | -0.086 | -0.004 | -0.194 |\n\n*Note: In this study, the 'head' is the husband and the 'spouse' is the wife.*\n\n---\n\n### Question\n\nA policymaker wants to mitigate a decline in labor force participation among “Two-Earner Families with Children.” They consider two policies: (i) a 10% wage subsidy for the head of household, or (ii) a 10% wage subsidy for the spouse. Which policy results in a larger (i.e., less negative) net change in total household work hours? (Assume for simplicity that the head and spouse work the same number of hours before the subsidy.)",
    "Options": {
      "A": "The 10% wage subsidy for the spouse.",
      "B": "The 10% wage subsidy for the head.",
      "C": "Both policies result in an identical net change in total household work hours.",
      "D": "Neither policy can mitigate the decline, as both result in a net increase in total household work hours."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.25). The original QA problem's components are highly suitable for conversion. Conceptual Clarity = 9/10 as the answers are standard, structured interpretations or calculations. Discriminability = 9.5/10 due to classic, high-frequency misconceptions that create high-fidelity distractors.\n\nThis item uses the 'Computational Judgment' strategy to assess the most complex part of the original QA problem. It requires students to synthesize four different elasticities to calculate the net effect of two competing policies, testing the paper's core theme of joint decision-making. The distractors are designed based on specific error paths: (B) ignoring the large cross-wage effect of the head's wage on the spouse's hours; (C) a calculation error; (D) a sign error on the final net effect.",
    "qid": "115",
    "model_evaluation_result": {
      "model_raw_response": "B) The $10\\%$ wage subsidy for $\\mathbf{the}$ $\\mathbf{\\text{head}}.$",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 116,
    "Question": "### Background\n\n**Research Question.** This problem explores the maximal degree of real indeterminacy in an incomplete markets model when the financial structure itself (i.e., bond returns) is allowed to vary, rather than being treated as a fixed parameter.\n\n**Setting / Institutional Environment.** The model is extended to consider the set of all possible no-arbitrage financial structures, which can be represented by different bond return matrices. The analysis compares the indeterminacy arising from a fixed financial structure to the total indeterminacy when the structure itself is variable.\n\n**Variables & Parameters.**\n- `N+1`: The number of spot markets.\n- `M`: The number of distinct financial instruments.\n- `n = N - M`: The degree of market incompleteness or deficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper summarizes the different levels of indeterminacy based on which components of the financial structure are treated as variable.\n\n**Table 1: Degrees of Real Indeterminacy**\n\n| | Minimal (Fixed Returns) | Intermediate (Variable Prices) | Maximal (Variable Returns) |\n| :--- | :--- | :--- | :--- |\n| **Degree of Indeterminacy** | `n` | `N - 1` | `nM` |\n\nConsider an economy with `N+1=10` spot markets (so `N=9`) and `M=3` distinct financial assets.\n\n---\n\n### Question\n\nFor the economy described, what is the degree of real indeterminacy for the **intermediate** case where bond prices are variable but yields are fixed?",
    "Options": {
      "A": "6",
      "B": "8",
      "C": "9",
      "D": "18"
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: Tests the ability to correctly identify and apply a formula from a table to a specific numerical case. Strategy: Computational Judgment. Distractor Logic: The correct answer is `N - 1 = 9 - 1 = 8`. Distractor A (`6`) is the result of misapplying the 'Minimal' case formula (`n = N - M = 9 - 3 = 6`). Distractor D (`18`) is the result of misapplying the 'Maximal' case formula (`nM = 6 * 3 = 18`). Distractor C (`9`) results from a step-omission error, using `N` instead of `N-1`.",
    "qid": "116",
    "model_evaluation_result": {
      "model_raw_response": "B) 9\n\n(Note: I will not provide any explanation, just the correct answer.)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 117,
    "Question": "### Background\n\n**Research Question.** This problem explores the maximal degree of real indeterminacy in an incomplete markets model when the financial structure itself (i.e., bond returns) is allowed to vary, rather than being treated as a fixed parameter.\n\n**Setting / Institutional Environment.** The model is extended to consider the set of all possible no-arbitrage financial structures. The total indeterminacy when the structure itself is variable can be decomposed into distinct sources.\n\n**Variables & Parameters.**\n- `N+1`: The number of spot markets.\n- `M`: The number of distinct financial instruments.\n- `n = N - M`: The degree of market incompleteness or deficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper provides an intuitive decomposition for the maximal degree of indeterminacy (`nM`):\n\n```latex\nnM = n + (M-1) + (n-1)(M-1) \\quad \\text{(Eq. (1))}\n```\n\nEach term on the right-hand side of Eq. (1) captures a distinct source of flexibility in the economic environment.\n\n---\n\n### Question\n\nWhat is the correct economic interpretation of the `(M-1)` term in the decomposition formula?",
    "Options": {
      "A": "The baseline indeterminacy arising from the deficiency of financial instruments relative to the number of states.",
      "B": "The additional indeterminacy from allowing the M bond prices to vary, holding relative yields fixed.",
      "C": "The additional indeterminacy from allowing the bond yields themselves to vary, which changes the structure of the portfolio wealth subspace.",
      "D": "The indeterminacy that arises from having a sufficient number of households with diverse preferences."
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: Tests the conceptual understanding of the different sources of indeterminacy. Strategy: Conceptual Interpretation. Distractor Logic: The correct answer (B) correctly identifies the `(M-1)` term with variable prices. Distractor A is the interpretation of the `n` term (Conceptual Opposite). Distractor C is the interpretation of the `(n-1)(M-1)` term (Conceptual Opposite). Distractor D describes Assumption A5 (`m>n`), which is a condition for indeterminacy to exist but is not part of this specific decomposition (True but Irrelevant).",
    "qid": "117",
    "model_evaluation_result": {
      "model_raw_response": "C) The addition indeterminacy form allowing the bond yield themselves to vary which changes the strucutre of the portfolio weath subspace.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 118,
    "Question": "### Background\n\n**Research Question.** This problem uses the paper's leading example to provide the geometric intuition for why financial market incompleteness leads to a continuum of real equilibrium allocations, contrasting it with the determinate outcome in a complete markets setting.\n\n**Setting / Institutional Environment.** Consider a simple exchange economy with 2 households, 1 physical good, and 3 spot markets (e.g., today, tomorrow-state-A, tomorrow-state-B). Allocations are described in terms of excess demands `z_h` in a 3-dimensional space.\n\n---\n\n### Data / Model Specification\n\nIn a **complete markets (Walrasian)** setting, a household's choices are constrained by a single lifetime budget constraint, which defines a 2-dimensional **budget plane** in the 3D space of excess demands:\n\n```latex\nz_h^0 + p^1 z_h^1 + p^2 z_h^2 = 0 \\quad \\text{(Eq. (1))}\n```\n\nIn the **incomplete markets** model with one financial instrument, the household faces two reduced-form budget constraints, which together define a 1-dimensional **budget line**.\n\nEquilibrium allocations occur at intersections of the two households' 2-dimensional offer surfaces that can be supported by a common budget set.\n\n---\n\n### Question\n\nWhat is the core geometric reason that the incomplete market structure generally leads to a continuum of equilibrium allocations, while the complete market structure leads to isolated equilibrium allocations?",
    "Options": {
      "A": "The two households' offer surfaces, which are 2-dimensional, generically intersect in a 1-dimensional curve. A 1D budget line can be adjusted to support any point on this curve.",
      "B": "Incomplete markets cause the offer surfaces to become 1-dimensional lines, which intersect at more points than 2-dimensional planes do.",
      "C": "Equilibrium requires the budget set to be tangent to both offer surfaces. A 1D budget line can be tangent to a curve of points, whereas a 2D plane can only be tangent at isolated points.",
      "D": "The budget line in the incomplete market case must pass through the origin, while the budget plane does not, allowing for more intersections."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the central geometric intuition of the paper's leading example. Strategy: Conceptual Interpretation. Distractor Logic: The correct answer (A) accurately describes the geometric mechanism. Distractor B incorrectly claims the offer surfaces change shape (Conceptual Error). Distractor C misstates the equilibrium condition; tangency is the condition for the plane, not the line (Incorrect Premise). Distractor D makes a false claim about the geometry; both budget sets pass through the origin in excess demand space (Factual Error).",
    "qid": "118",
    "model_evaluation_result": {
      "model_raw_response": "C) Equilibria require the budget set be tangent to the offer surfaces.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 119,
    "Question": "### Background\n\n**Research Question.** This problem uses the paper's leading example to test the robustness of the indeterminacy result when financial asset yields are not fixed but depend on spot prices.\n\n**Setting / Institutional Environment.** Consider a simple exchange economy with 2 households, 1 good, and 3 spot markets. In the baseline model with exogenous yields, the budget set is a line. An alternative financial instrument is an installment contract with price-dependent yields.\n\n---\n\n### Data / Model Specification\n\nThe budget constraints for the installment contract model are:\n\n```latex\nz_h^0 + \\left[\\frac{\\pi}{p^2-q}\\right]p^2 z_h^2 = 0 \\quad \\text{(Eq. (1))}\n```\n\n```latex\np^1 z_h^1 - \\left[\\frac{p^1-q}{p^2-q}\\right]p^2 z_h^2 = 0 \\quad \\text{(Eq. (2))}\n```\n\nwhere `z_h` is the excess demand vector, `p` is the price vector, and `π` and `q` are parameters of the contract.\n\n---\n\n### Question\n\nIn this alternative model with price-dependent yields, what is the nature of the household's budget set for a *given* price vector `p`, and what does this imply about the indeterminacy of equilibrium?",
    "Options": {
      "A": "The budget set is a 2-dimensional plane, which restores the generic determinacy of a complete market.",
      "B": "The budget set is a 1-dimensional line, implying that real indeterminacy is robust to this form of endogenous yields.",
      "C": "The budget set is a non-linear surface because of the price terms in the coefficients, which restores generic determinacy.",
      "D": "The budget set is a 1-dimensional line, but this specific structure of price dependence eliminates the indeterminacy."
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: Tests the robustness of the core geometric insight by applying it to a new scenario. Strategy: Scenario Application. Distractor Logic: The correct answer (B) recognizes that for a fixed `p`, the equations are linear and define a line. Distractor C represents the most plausible misconception: confusing the fact that the *family* of budget lines changes with `p` with the idea that the budget set *itself* is non-linear for a given `p`. Distractor A incorrectly equates price-dependence with market completeness. Distractor D correctly identifies the geometry but draws the wrong conclusion about its implications.",
    "qid": "119",
    "model_evaluation_result": {
      "model_raw_response": "B) Thebudgetsetis a 1-dimensionalline,implyingthatrealindeterminacyisrobusttothisformofendogenousyields.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 120,
    "Question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal (e.g., a seller) can design a mechanism to extract all economic surplus from privately informed agents (e.g., bidders), a scenario known as Full Surplus Extraction (FSE).\n\n**Setting:** A seller designs an incentive compatible (IC) and individually rational (IR) mechanism to sell a single object. An agent's private information is captured by their 'type' $\\theta_i$, which consists of a valuation for the object $\\hat{v}_i(\\theta_i)$ and a belief about others' types $\\hat{b}_i(\\theta_i)$. The analysis centers on the properties of the common prior, $p$, over the space of all type profiles.\n\n### Data / Model Specification\n\n**Key Definitions:**\n- **Beliefs-Determine-Preferences (BDP):** A prior $p$ is BDP for bidder $i$ if, with $p$-probability 1, any two of bidder $i$'s types with different valuations must also have different beliefs.\n- **Non-BDP Prior:** A prior $p$ is non-BDP for bidder $i$ if there exist disjoint sets of types, $A_i$ and $A_i'$, each with positive $p$-probability, such that for any type $\\theta_i \\in A_i$, there exists a type $\\theta_i' \\in A_i'$ with the same beliefs, $\\hat{b}_i(\\theta_i) = \\hat{b}_i(\\theta_i')$, but a strictly lower valuation, $\\hat{v}_i(\\theta_i) > \\hat{v}_i(\\theta_i')$.\n\n### The Question\n\nConsider a prior that is non-BDP for bidder $i$. This means there's a positive probability of finding a high-valuation type $\\theta_i$ and a low-valuation type $\\theta_i'$ who share identical beliefs but have different valuations ($\\hat{v}_i(\\theta_i) > \\hat{v}_i(\\theta_i')$). Why does this structure make Full Surplus Extraction impossible?",
    "Options": {
      "A": "The high-valuation type $\\theta_i$ can mimic the low-valuation type $\\theta_i'$, pay the lower amount intended for $\\theta_i'$, and retain the difference in valuations, $\\hat{v}_i(\\theta_i) - \\hat{v}_i(\\theta_i')$, as a positive information rent.",
      "B": "The seller cannot distinguish between the two types, so they must offer both the same contract, which is inefficient.",
      "C": "The identical beliefs cause the incentive compatibility constraints for both types to fail, making any mechanism design impossible.",
      "D": "The low-valuation type $\\theta_i'$ can mimic the high-valuation type $\\theta_i$ to try and win the object more often, which breaks the mechanism."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to target the core logical step in the proof that FSE requires BDP. It assesses whether the student understands the mechanism of strategic mimicry that generates information rents. Distractor B is an 'Almost Right' common intuition, C misidentifies which constraint fails, and D reverses the logic of the mimicry, making it a classic 'Conceptual Opposite' error.",
    "qid": "120",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\n(Note: I've output only the chosen letter, as per the format specified.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 121,
    "Question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal can design a mechanism to extract all economic surplus from privately informed agents.\n\n**Setting:** The analysis centers on the properties of the common prior, $p$, over the space of all type profiles.\n\n### Data / Model Specification\n\n**Key Definitions:**\n- **Beliefs-Determine-Preferences (BDP) Prior:** A prior where, with probability 1, a bidder's beliefs about others uniquely determine their own valuation.\n- **Non-BDP (NBDP) Prior:** A prior that is not BDP.\n- **Convex Combination of Priors:** A mixture $p = \\alpha p' + (1-\\alpha)p''$ for $\\alpha \\in (0,1)$. A key result is that a non-degenerate convex combination of a BDP prior and an NBDP prior is always an NBDP prior.\n\n### The Question\n\nSuppose a researcher has two common priors: $p'$, which is a BDP prior, and $p''$, which is a non-BDP prior. If the true prior is a non-degenerate convex combination of the two, $p = 0.5 p' + 0.5 p''$, what is the property of the resulting prior $p$?",
    "Options": {
      "A": "$p$ is a non-BDP prior because the type profiles that violate BDP in $p''$ still exist with positive probability in the mixture $p$.",
      "B": "$p$ is a BDP prior because the BDP property of $p'$ 'dominates' the mixture.",
      "C": "The property of $p$ is ambiguous and depends on the specific structures of $p'$ and $p''$.",
      "D": "$p$ is 'partially BDP', meaning FSE is possible for half of the types."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Scenario Application strategy to test the student's understanding of how the BDP property behaves under mixtures, a key result from Proposition 1. The correct answer requires understanding *why* the mixture is non-BDP. Distractor B is the conceptual opposite, C suggests ambiguity where there is none, and D invents a plausible-sounding but incorrect concept ('partially BDP').",
    "qid": "121",
    "model_evaluation_result": {
      "model_raw_response": "C) Theproperty of $p$is ambiguous and dependson the specific structuresof$p'$and$p''.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 122,
    "Question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal can design a mechanism to extract all economic surplus from privately informed agents.\n\n**Setting:** The analysis centers on the properties of the common prior, $p$, over the space of all type profiles. A key condition for Full Surplus Extraction (FSE) is that the prior must have the Beliefs-Determine-Preferences (BDP) property.\n\n### Data / Model Specification\n\n**Key Definitions:**\n- **BDP Prior:** A prior where a bidder's beliefs uniquely determine their valuation.\n- **Non-BDP Prior:** A prior where there exist high-valuation types who share identical beliefs with corresponding low-valuation types. Under a non-BDP prior, a high-valuation type can mimic a low-valuation type to earn an 'information rent'.\n- **FSE Mechanism:** A mechanism designed for a BDP prior, where the payment extracted from each type is equal to their valuation.\n\n### The Question\n\nA principal mistakenly believes the prior is BDP and designs a mechanism to achieve FSE from bidder $i$. The true prior is non-BDP, containing a set of high-valuation types $A_i$ (with probability $p(A_i)=0.2$) who can mimic a corresponding set of low-valuation types $A_i'$. For any such mimicking pair, the high valuation is $\\hat{v}_H=100$ and the low valuation is $\\hat{v}_L=60$. For all other types (with probability 0.8), their average valuation is 80, and the FSE mechanism works correctly. What is the principal's actual expected revenue per bidder?",
    "Options": {
      "A": "76",
      "B": "84",
      "C": "64",
      "D": "80"
    },
    "Answer": "A",
    "pi_justification": "This item uses a Computational Judgment strategy to assess the practical application of the revenue shortfall concept. It requires students to calculate the actual revenue in a specific scenario where high-valuation types strategically misreport. Distractor B represents the ideal but unachievable revenue (Step-Omission Error), C represents a severe miscalculation by ignoring revenue from mimicking types (Formula Misuse), and D represents a plausible but incorrect averaging heuristic (Formula Misuse).",
    "qid": "122",
    "model_evaluation_result": {
      "model_raw_response": "C\n\n(Note: I've only provided the answer without any additional explanation or justification.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 123,
    "Question": "### Background\n\n**Research Question.** How does the structure of industry-wide adjustment costs for creating new production units determine the cyclical response of an industry's creation and destruction margins?\n\n**Setting.** In a vintage model of creative destruction, an industry can respond to demand fluctuations by adjusting the rate of new unit creation or the rate of old unit destruction. The paper's central theoretical argument is that the cost structure of creation governs which margin adjusts. A key concept is the \"insulation effect,\" where adjustments on the creation margin can shield existing units from demand shocks.\n\n### Data / Model Specification\n\nThe cost of creating a new production unit, `c`, is an increasing function of the aggregate creation rate, `f(0,t)`:\n```latex\nc = c(f(0,t)), \\quad \\text{with } c(\\cdot) > 0 \\text{ and } c'(\\cdot) \\ge 0\n```\n\n### Question\n\nUnder what condition does the \"insulation effect\" become incomplete, leading the destruction margin (`ā(t)`) to become cyclically responsive, and what is the economic rationale?",
    "Options": {
      "A": "`c'(f(0,t)) > 0`, because convex costs create an incentive to smooth the creation process, preventing it from fully absorbing demand shocks.",
      "B": "`c'(f(0,t)) = 0`, because constant costs allow the creation margin to perfectly track and absorb demand shocks.",
      "C": "`c'(f(0,t)) > 0`, because firms want to create as many units as possible during booms, which amplifies the business cycle.",
      "D": "`c'(f(0,t)) < 0`, because economies of scale in creation make it optimal to concentrate creation in specific periods."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core theoretical mechanism of the paper: the link between convex creation costs and incomplete insulation. It uses a Reverse-Reasoning strategy, asking for the condition that produces a given outcome. Distractor B correctly describes the condition for the opposite case (full insulation). Distractor C uses the correct condition but provides faulty reasoning. Distractor D introduces an irrelevant and incorrect concept (decreasing marginal costs).",
    "qid": "123",
    "model_evaluation_result": {
      "model_raw_response": "C) `f'(f(00,t)) >0`, because firm want to create many units during booms",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 124,
    "Question": "### Background\n\n**Research Question.** How does the structure of industry-wide adjustment costs for creating new production units determine the cyclical response of an industry's creation and destruction margins?\n\n**Setting.** In a vintage model of creative destruction, an industry can respond to demand fluctuations by adjusting the rate of new unit creation or the rate of old unit destruction. The paper's central theoretical argument is that the cost structure of creation governs which margin adjusts.\n\n### Data / Model Specification\n\nThe cost of creating a new production unit, `c`, is an increasing function of the aggregate creation rate, `f(0,t)`. Using a first-order Taylor approximation, the change in the creation rate, `Δf`, in response to a shock to the present value of a new firm, `ΔV`, can be expressed as:\n```latex\n\\Delta f \\approx \\frac{\\Delta V}{c'(f_0)}\n```\nwhere `c'(f₀)` is the slope of the creation cost function at the initial creation rate `f₀`.\n\n### Question\n\nGiven the derived relationship in Eq. (1), what is the correct interpretation of the role of the adjustment cost parameter `c'(f₀)`?",
    "Options": {
      "A": "A larger `c'(f₀)` dampens the response of creation to a given shock, reflecting a stronger incentive to smooth the creation process.",
      "B": "A larger `c'(f₀)` amplifies the response of creation, as higher marginal costs force a more dramatic adjustment to meet demand.",
      "C": "`c'(f₀)` determines the steady-state level of creation but does not affect its cyclical response to shocks.",
      "D": "A larger `c'(f₀)` makes the value of new firms (`ΔV`) more sensitive to demand shocks."
    },
    "Answer": "A",
    "pi_justification": "This item tests the student's ability to interpret a key mathematical result from the model. It is a Computational Judgment question that requires translating a formula into an economic concept. Distractor B is the conceptual opposite. Distractor C is an 'Almost Right' error, confusing level effects with cyclical effects. Distractor D misinterprets the causal chain in the formula.",
    "qid": "124",
    "model_evaluation_result": {
      "model_raw_response": "B) A Larger `c'(F0)` amplifies The Response Of Creation, As Higher Marginal Costs Force A More Dramatic Adjustment To Meet Demand.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 125,
    "Question": "### Background\n\n**Research Question.** How does the structure of industry-wide adjustment costs for creating new production units determine the cyclical response of an industry's creation and destruction margins?\n\n**Setting.** In a vintage model of creative destruction with convex creation costs (`c' > 0`), the industry has an incentive to smooth creation over the business cycle. This leads to an incomplete \"insulation effect,\" meaning that both creation and destruction margins respond to demand shocks.\n\n### Data / Model Specification\n\nA government introduces a counter-cyclical subsidy, `s(t)`, for each new unit created (`s(t) > 0` in recessions, `s(t) = 0` in expansions). The private cost of creation becomes `c(f(0,t)) - s(t)`.\n\n### Question\n\nIn the model with convex creation costs (`c' > 0`), what is the most likely combined effect of this subsidy policy during a recession?",
    "Options": {
      "A": "It will make job creation less cyclical and job destruction less cyclical, thereby dampening the \"cleansing effect.\"",
      "B": "It will make job creation less cyclical but amplify job destruction, enhancing the \"cleansing effect.\"",
      "C": "It will have no effect on job destruction, as the subsidy only targets new firms and does not affect existing ones.",
      "D": "It will make both job creation and job destruction more cyclical, as the subsidy encourages over-investment."
    },
    "Answer": "A",
    "pi_justification": "This item uses the Scenario Application strategy, asking students to apply the model's logic to a novel policy intervention. The correct answer requires tracing the policy's effect through both the creation and destruction margins via the general equilibrium price mechanism. Distractor C represents a common partial equilibrium error (ignoring the price effect). Distractor B correctly identifies the direct effect on creation but gets the indirect effect on destruction wrong. Distractor D gets the direction of both effects wrong.",
    "qid": "125",
    "model_evaluation_result": {
      "model_raw_response": "B) It is most likely that the subsidy will make job destruction less cyclically sensitive while making job creation less cyclically responsive, thereby amplifying the \"cleaning effect\".",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 126,
    "Question": "### Background\n\n**Research Question.** How can the net change in an industry's employment be decomposed into its constituent gross flows, and how do these flows behave during a cyclical downturn?\n\n**Setting.** In a vintage capital model, the total number of production units (or jobs), `N(t)`, evolves through the continuous creation of new units and the destruction of existing units.\n\n### Data / Model Specification\n\nThe endogenous component of job destruction (i.e., scrapping of unprofitable units) is given by the term `f(ā(t),t)[1-ȧ(t)]`, where `ā(t)` is the age of the oldest unit and `ȧ(t)` is its time derivative.\n\n### Question\n\nAt the onset of a sharp, unanticipated recession, the \"cleansing effect\" manifests as a spike in job destruction. Which component of the endogenous destruction rate is primarily responsible for this initial spike?",
    "Options": {
      "A": "The term `-f(ā(t),t)ȧ(t)`, because the obsolescence age `ā(t)` falls rapidly, making `ȧ(t)` large and negative.",
      "B": "The term `f(ā(t),t)`, because the number of firms at the obsolescence margin suddenly increases.",
      "C": "The exogenous destruction rate `δN(t)`, because failures increase during recessions.",
      "D": "Both components, `f(ā(t),t)` and `-f(ā(t),t)ȧ(t)`, contribute equally to the spike."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to assess a deep dynamic insight of the model. It asks the student to identify the specific mathematical term responsible for the 'cleansing effect' phenomenon. Distractor B points to the wrong term, reflecting a misunderstanding of stocks vs. flows. Distractor C introduces an out-of-model concept and points to the wrong part of the overall destruction equation. Distractor D reflects an inability to distinguish the primary driver of the dynamic adjustment.",
    "qid": "126",
    "model_evaluation_result": {
      "model_raw_response": "B) The number of firms-at-the-margin increases.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 127,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the full policy impact of the post-1996 health insurance subsidies for the self-employed, considering not only the effect on private coverage but also interactions with public insurance.\n\n**Setting / Institutional Environment.** After estimating the price elasticity of demand for private insurance, the paper analyzes \"reverse crowd-out,\" where private subsidies induce people to leave public insurance. The net effect of the subsidy on the total number of uninsured depends on the relative size of the increase in private coverage versus the decrease in public coverage.\n\n### Data / Model Specification\n\nKey results from the paper's regression analysis are provided below.\n\n**Table 1: Estimated Marginal Effects for Children (Sample-Weighted TP Model)**\n| Dependent Variable | Marginal Effect of a 0.01 increase in TP |\n|:---|:---:|\n| Private Coverage | -0.561** |\n| Public Coverage | 0.620*** |\n\n*Note: TP is the Simple Tax Price of private insurance.*\n\n### Question\n\nGiven the estimated marginal effects for children in Table 1, what is the net change in the probability of a child being *uninsured* resulting from a 0.01 **decrease** in the Simple Tax Price (TP), and what does this imply about the subsidy's primary effect?",
    "Options": {
      "A": "A decrease of 1.181 percentage points, implying the subsidy is highly effective at reducing uninsurance.",
      "B": "A decrease of 0.059 percentage points, implying a small but positive effect on reducing uninsurance.",
      "C": "An increase of 0.059 percentage points, implying the subsidy's main effect is substitution from public to private coverage, with a negligible net impact on uninsurance.",
      "D": "An increase of 1.181 percentage points, implying the subsidy is counterproductive and significantly increases uninsurance."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the ability to synthesize multiple regression coefficients to calculate a net policy effect, a key concept from the source QA. It uses a Computational Judgment strategy. The correct answer requires calculating `ΔUninsured = -ΔPrivate - ΔPublic` and correctly handling the signs for a price *decrease*. Distractor A results from summing the absolute values of the coefficients. Distractor B results from a sign error in the calculation (`0.620 - 0.561`). Distractor D uses the wrong calculation (summing absolute values) and the wrong sign.",
    "qid": "127",
    "model_evaluation_result": {
      "model_raw_response": "C) Anincrease of 0,059 percentage points",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 128,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the cost-effectiveness of health insurance subsidies.\n\n**Setting / Institutional Environment.** A key concept in policy evaluation is the \"inframarginal transfer,\" which is the portion of a subsidy paid to individuals who would have purchased the good (e.g., health insurance) even without the subsidy. A policy is more cost-effective at changing behavior (e.g., covering the uninsured) if a smaller fraction of its total cost consists of inframarginal transfers.\n\n### Question\n\nA policymaker proposes extending the same health insurance tax subsidy to two different populations: Group A, with a pre-policy private insurance coverage rate of 30%, and Group B, with a pre-policy rate of 80%. Assuming both groups have identical price sensitivity, for which group would the subsidy be more cost-effective at reducing the number of uninsured, and why?",
    "Options": {
      "A": "Group B, because the higher initial coverage rate indicates a stronger underlying demand for insurance, making subsidies more effective.",
      "B": "Group B, because with more people already insured, the market is more stable and administrative costs are lower.",
      "C": "Group A, because its members are more likely to have lower incomes and therefore be more responsive to the subsidy.",
      "D": "Group A, because a smaller share of the total subsidy cost would go to people who would have purchased insurance anyway."
    },
    "Answer": "D",
    "pi_justification": "This item assesses the understanding of 'inframarginal transfers' and their role in policy cost-effectiveness, a core concept from the source QA. It uses a Scenario Application strategy. The correct answer (D) correctly identifies that a lower initial coverage rate means a smaller share of the subsidy is 'wasted' on inframarginal transfers. Distractors A and B represent a classic conceptual error, incorrectly associating higher pre-existing uptake with higher policy effectiveness. Distractor C provides a plausible but irrelevant reason, as the prompt states price sensitivity is identical.",
    "qid": "128",
    "model_evaluation_result": {
      "model_raw_response": "D) Group D, because a small share of the subsidy cost would be inframarginally transferred",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 129,
    "Question": "### Background\n\n**Research Question.** This problem examines the foundational elements of the paper's model: how worker heterogeneity is defined and how optimal contracts are structured in the benchmark private sector.\n\n**Setting / Institutional Environment.** The model features a competitive private sector where workers are paid their marginal product. Workers are heterogeneous along two unobservable dimensions: their intrinsic motivation for public service (`\\gamma_i`) and their disutility from effort (`\\theta_i`).\n\n---\n\n### Data / Model Specification\n\nThere are three worker types: `regular` (r), `dedicated` (m), and `lazy` (l). Their utility is defined by a laziness parameter `\\theta_i` and a motivation parameter `\\gamma_i`, with key assumptions: `0 < \\theta_r = \\theta_m < \\theta_l` and `\\gamma_m > \\gamma_r = \\gamma_l = 0`.\n\nIn the competitive private sector, production is `q(e)=e` and the wage is `w=pe`. A worker's utility is `U_i = pe - \\theta_i C(e)`, where `C'(e)>0` and `C''(e)>0`.\n\n---\n\n### The Question\n\nIn the competitive private sector, a worker chooses effort `e` to maximize their utility. Given the model's assumptions about the laziness parameter `\\theta_i`, which of the following correctly describes the optimal effort levels (`e_i^*`) for the three worker types?",
    "Options": {
      "A": "`e_r^* = e_m^* > e_l^*`",
      "B": "`e_l^* > e_r^* = e_m^*`",
      "C": "`e_m^* > e_r^* > e_l^*`",
      "D": "`e_r^* > e_m^* = e_l^*`"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the foundational understanding of how worker heterogeneity translates into behavior in the benchmark private sector setting. It uses a Computational Judgment strategy. The correct answer requires deriving the FOC `p = \\theta_i C'(e)` and understanding that since `C'' > 0`, a higher `\\theta_i` implies a lower `e_i^*`. Distractor B represents a sign error in this logic. Distractor C incorrectly incorporates public service motivation into the private sector calculation, a common conceptual slip. Distractor D confuses the definitions of the worker types.",
    "qid": "129",
    "model_evaluation_result": {
      "model_raw_response": "C) `E_m^*>` `E_r^*` `E_l^*",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 130,
    "Question": "### Background\n\n**Research Question.** This problem examines the foundational elements of the paper's model, specifically the economic trade-offs faced by a cost-minimizing public agency.\n\n**Setting / Institutional Environment.** A public agency hires a single worker type `i` to produce a total quantity `Q`. It minimizes total cost `Z = w_i n_i` subject to a production constraint `n_i e_i = Q` and a participation constraint for the workers.\n\n---\n\n### Data / Model Specification\n\nThe agency's cost-minimization problem yields the following first-order condition for optimal effort `e_i`:\n```latex\n\\left[\\theta_{i}C'(e_{i})-\\gamma_{i}V'(e_{i})\\right]-\\left[{\\frac{U_{i}^{*}+\\theta_{i}C(e_{i})-\\gamma_{i}V(e_{i})}{e_{i}}}\\right]=0 \\quad \\text{(Eq. 1)}\n```\n\n---\n\n### The Question\n\nWhat is the correct economic interpretation of the two main terms in the first-order condition shown in Eq. (1)?",
    "Options": {
      "A": "The first term is the marginal cost of increasing output on the intensive margin (more effort from current workers), while the second term is the marginal cost on the extensive margin (hiring a new worker).",
      "B": "The first term is the marginal cost on the extensive margin, while the second term is the marginal cost on the intensive margin.",
      "C": "The first term represents the worker's marginal disutility of effort, and the second term represents the average wage cost.",
      "D": "The first term is the agency's marginal profit from an additional unit of effort, while the second term is the total cost of production."
    },
    "Answer": "A",
    "pi_justification": "This item tests the student's ability to interpret the economic logic of an optimization condition, a core skill in microeconomic theory. It uses a Reverse-Reasoning strategy. The correct answer correctly identifies the intensive and extensive margins of cost. Distractor B is a classic conceptual reversal. Distractor C provides a technically correct but incomplete description that misses the key economic trade-off. Distractor D misinterprets the agency's objective function and the nature of the cost terms.",
    "qid": "130",
    "model_evaluation_result": {
      "model_raw_response": "C) The term represents the first term represents worker's marginal utility of leisure, and the term represents the marginal wage cost.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 131,
    "Question": "### Background\n\n**Research Question.** This problem examines the foundational elements of the paper's model, specifically how a cost-minimizing public agency designs an optimal contract for its most motivated workers.\n\n**Setting / Institutional Environment.** A public agency hires only `dedicated` workers (`m`) to produce a quantity `Q`. Dedicated workers are defined by having public service motivation (`\\gamma_m > 0`) and standard disutility of effort (`\\theta_m = \\theta_r`). The agency minimizes total costs.\n\n---\n\n### Data / Model Specification\n\nThe utility of a dedicated worker in the public sector is `U_m = w + \\gamma_m V(e) - \\theta_m C(e)`, where `V(e)` is a concave function (`V' > 0, V'' < 0`). The worker's optimal effort in the private sector is `e_m^*`.\n\n---\n\n### The Question\n\nWhen the public agency hires only `dedicated` workers (`m`), what is the optimal effort level `e_m` it requires, and what is the economic intuition for this choice?",
    "Options": {
      "A": "`e_m < e_m^*`, because the agency maximizes total extracted rent from motivation by hiring more workers at a lower effort level where the marginal motivational rent is higher.",
      "B": "`e_m = e_m^*`, because the agency replicates the efficient private sector contract to minimize costs.",
      "C": "`e_m > e_m^*`, because the agency takes advantage of the dedicated workers' motivation to demand extra effort for the same pay.",
      "D": "`e_m < e_m^*`, because requiring less effort is a way to reward dedicated workers for their public service motivation."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of one of the paper's key preliminary results: the structure of the optimal contract for the most motivated workers. It uses a Scenario Application strategy. The correct answer identifies both the correct outcome (`e_m < e_m^*`) and the correct economic reason related to maximizing total rent from a concave motivation function. Distractor B incorrectly generalizes the result from other worker types. Distractor C represents a common but incorrect intuition about exploitation. Distractor D is an 'Almost Right' option that gets the direction correct but provides a behavioral explanation instead of the correct optimization-based one.",
    "qid": "131",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: This answer is based on the assumption that the public agency wants to minimize costs, given the setting and institutional environment described.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 132,
    "Question": "### Background\n\n**Research Question.** This problem examines the paper's core empirical findings regarding strategic capacity choice.\n\n**Setting / Institutional Environment.** The study estimates a structural model of a Bayesian game where providers simultaneously choose capacity. A key challenge is the endogeneity of competitors' actions. The model addresses this by using competitors' *expected* capacity, conditional on exogenous market characteristics, as the relevant strategic variable in a provider's decision-making process.\n\n### Data / Model Specification\n\nThe simplified best-response function for provider *i* is:\n```latex\nK_i^* = \\max\\{0, X\\beta_i + \\sum_{j \\neq i} \\gamma_{ij} \\varphi_j(X) - b_i - \\varepsilon_i\\}\n```\nwhere $\\varphi_j(X) = \\mathbb{E}[K_j^*|X]$ is the expected capacity of competitor *j* and $\\varepsilon_i \\sim N(0, \\sigma_i^2)$. The parameter $\\gamma_{ij}$ captures the strategic interaction. The marginal effect of a 1-unit increase in a competitor's expected capacity on provider *i*'s own capacity is calculated as $\\gamma_{ij} \\times \\text{Pr}(K_i^* > 0 | X)$.\n\n**Table 1: GMM Estimates of Strategic Interaction Parameters ($\\{\\gamma_{i,j}\\}$)**\n*The table shows the estimated effect of the column player's expected capacity on the row player's latent profitability variable.*\n\n| Dependent Var (Row Player) | Effect of E(Capacity) of FMC | Effect of E(Capacity) of DaVita | Effect of E(Capacity) of Nonchain |\n| :--- | :--- | :--- | :--- |\n| **FMC** | --- | -2.32*** | -1.44*** |\n| **DaVita** | -2.17*** | --- | -1.45*** |\n| **Nonchain** | -0.82*** | -0.61*** | --- |\n\n*Note: *** denotes p < 0.01.*\n\n**Table 2: Model Fit Comparison**\n\n| Statistic | Provider | Observed | GMM Prediction |\n| :--- | :--- | :--- | :--- |\n| Mean Capacity | DaVita | 4.40 | 4.23 |\n| SD Capacity | DaVita | 9.23 | 4.46 |\n| Mean Entry Prob. | DaVita | 0.23 | 0.24 |\n\n### The Question\n\nBased on the GMM estimates in Table 1 and the model fit in Table 2, what is the nature of the strategic relationship between FMC and DaVita, and what is the marginal effect of a 1-unit increase in FMC's expected capacity on DaVita's chosen capacity in an average market?",
    "Options": {
      "A": "Strategic substitutes; a 1-unit increase in FMC's expected capacity decreases DaVita's capacity by approximately 0.52 units.",
      "B": "Strategic substitutes; a 1-unit increase in FMC's expected capacity decreases DaVita's capacity by approximately 2.17 units.",
      "C": "Strategic complements; a 1-unit increase in FMC's expected capacity increases DaVita's capacity by approximately 0.52 units.",
      "D": "Strategic complements; a 1-unit increase in FMC's expected capacity increases DaVita's capacity by approximately 2.17 units."
    },
    "Answer": "A",
    "pi_justification": "This item uses the Computational Judgment strategy to assess two core skills from the original QA: interpreting the sign of a strategic interaction parameter and performing a multi-step calculation for a marginal effect. The distractors are designed based on a matrix of common errors: Conceptual Opposite (substitutes vs. complements) and Step-Omission Error (failing to multiply the coefficient by the entry probability).",
    "qid": "132",
    "model_evaluation_result": {
      "model_raw_response": "B) Strategic substituts; a  1-unit  increase  in  FMC's  expected  capacity  decreases  DaVita'",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 133,
    "Question": "### Background\n\nThe study estimates a structural model of a Bayesian game where a provider's capacity choice $K_i$ is a function of observable market characteristics $X$, competitors' expected capacity $E[K_{-i}|X]$, and unobservable private information $\\varepsilon_i$. After estimation, the model's predictions are compared to the data to validate its performance.\n\n### Data / Model Specification\n\n**Table 1: Model Fit Comparison for DaVita**\n\n| Statistic | Observed | GMM Prediction |\n| :--- | :--- | :--- |\n| Mean Capacity | 4.40 | 4.23 |\n| SD Capacity | 9.23 | 4.46 |\n| Mean Entry Prob. | 0.23 | 0.24 |\n\n### The Question\n\nTable 1 shows that the standard deviation of DaVita's observed capacity (9.23) is much larger than the standard deviation of the model's predicted *expected* capacity (4.46). What is the best interpretation of this discrepancy?",
    "Options": {
      "A": "The model's predictions for *expected* capacity are based only on observable characteristics (X), while observed capacity choices are also driven by large, unobservable private information shocks (ε_i), which increase the total variance.",
      "B": "The large observed variance is primarily due to measurement error in the capacity data, which the model correctly smooths out.",
      "C": "The discrepancy indicates that the model overfits the mean of the data at the expense of fitting the variance.",
      "D": "The GMM estimator is known to be biased in finite samples, which likely causes it to underestimate the true variance of the underlying process."
    },
    "Answer": "A",
    "pi_justification": "This item uses the Reverse-Reasoning strategy. Given a key empirical finding from the model fit table, the student must infer the underlying structural cause. It assesses a deep understanding of how incomplete information models partition variance between observable and unobservable components. Distractors offer plausible but incorrect statistical explanations (measurement error, overfitting, finite sample bias) to discriminate between students who understand the model's structure and those relying on general statistical heuristics.",
    "qid": "133",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\n(Note: I'm providing the answer as per the required format, which is a single string with the chosen letter.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 134,
    "Question": "## Background\n\n**Research Question.** This problem investigates why a profit-maximizing platform might subsidize transactions between its users when they bargain under asymmetric information.\n\n**Setting.** A platform facilitates interactions where end-users (buyers B, sellers S) bargain over a transaction price. Their ex-post benefits `b^i` are private information. The platform sets a total usage fee `a = a^B + a^S` and has a marginal cost `c` per transaction.\n\n**Variables & Parameters.**\n\n*   `a`: Total platform usage fee per transaction.\n*   `c`: Platform's marginal cost per transaction.\n*   `a*`: The optimal total usage fee set by the platform.\n*   `v(a)`: Average social surplus from a potential interaction, `v(a) = E[(b^B + b^S - c)x(b,a)]`.\n*   `x(b,a)`: Probability of trade given benefits `b` and fee `a`.\n*   `X(a)`: Expected volume of trade, `X(a) = E[x(b,a)]`.\n*   `L(a)`: The maximized joint surplus of the end-users, `L(a) = max E[(b^B + b^S - a)x(b,a)]`.\n\n---\n\n## Data / Model Specification\n\nThe platform's optimization problem involves setting the total usage fee `a` to maximize the average social surplus from interactions, `v(a)`. The first-order condition for the platform's choice of `a` is:\n```latex\nv'(a^*) = L'(a^*) + X(a^*) + (a^* - c)X'(a^*) = 0 \\quad \\text{(Eq. 1)}\n```\nIt is also known that `L'(a) < -X(a)` and a regularity assumption `X'(a) < 0` (volume of trade decreases with the fee) holds.\n\n---\n\n## Question\n\nGiven the first-order condition in Eq. (1) and the associated inequalities, what is the necessary relationship between the platform's optimal transaction fee `a*` and its marginal cost `c`?",
    "Options": {
      "A": "`a* < c`",
      "B": "`a* > c`",
      "C": "`a* = c`",
      "D": "The relationship cannot be determined from the information given."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the student's ability to perform a mathematical derivation from a given first-order condition. It uses a Computational Judgment strategy. The correct answer requires correctly manipulating the inequalities. Distractor B (Sign Error) results from misinterpreting the inequalities. Distractor C (Formula Misuse) is the correct answer for the Coasian benchmark but incorrect here. Distractor D (Step-Omission Error) targets students unable to complete the derivation.",
    "qid": "134",
    "model_evaluation_result": {
      "model_raw_response": "C) a* = c\n\nNote: This is based on the first-order conditions provided in the problem, specifically the equation v'(a*) = L’(a*) + X'(a*) + ( a* - c )X’(a*). Since X'(a) is negative, it follows that a* = -L'(a*)/(X'(a*) - 1). Given the regularity assumption X'(a)<0, this simplifies to a* = L'(0)/(-X'(0)) = c.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 135,
    "Question": "## Background\n\n**Research Question.** This problem investigates why a profit-maximizing platform might subsidize transactions between its users when they bargain under asymmetric information.\n\n**Setting.** A platform facilitates interactions where end-users (buyers B, sellers S) bargain over a transaction price. Their ex-post benefits `b^i` are private information. The platform sets a total usage fee `a = a^B + a^S` and has a marginal cost `c` per transaction.\n\n---\n\n## Data / Model Specification\n\nIn markets where end-users bargain under asymmetric information, trade is often inefficiently low due to bargaining failures (e.g., monopoly price-setting by one party). Analysis shows that a profit-maximizing platform in such a market will optimally set its total usage fee `a*` to be less than its marginal cost `c`, effectively subsidizing every transaction.\n\n---\n\n## Question\n\nWhat is the primary economic rationale for a profit-maximizing platform to subsidize transactions (`a* < c`)?",
    "Options": {
      "A": "The subsidy corrects for user-level bargaining failures, increasing the total surplus which the platform can then extract via higher membership fees.",
      "B": "The subsidy acts as a predatory price, designed to drive competing platforms out of the market and establish a monopoly.",
      "C": "The platform has non-profit objectives and is willing to lose money on transactions to maximize social welfare or user participation.",
      "D": "The subsidy boosts transaction volume to a point where the high volume of fee revenue (`a*` per transaction) outweighs the per-transaction loss (`a* - c`)."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of the economic intuition behind subsidizing transactions. It uses a Reverse-Reasoning strategy. The correct answer identifies the two-part logic: correct a user-level market failure to increase total surplus, then capture that surplus via membership fees. Distractor B (True but Irrelevant) proposes an alternative motive not supported by the monopoly model. Distractor C (Conceptual Opposite) contradicts the profit-maximization assumption. Distractor D (Almost Right) correctly identifies the volume increase but misses the crucial role of membership fees in capturing the generated surplus, which is the core of the paper's argument.",
    "qid": "135",
    "model_evaluation_result": {
      "model_raw_response": "D) The suburb boosts transaction volume... etc.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 136,
    "Question": "## Background\n\n**Research Question.** This problem explores the fundamental definition of a two-sided market, focusing on the distinction between the price level and price structure.\n\n**Setting.** A platform charges per-interaction fees `a^B` and `a^S`. The total price level is `a = a^B + a^S`. The market is **one-sided** if transaction volume `V` depends only on `a`, and **two-sided** if `V` also depends on the price structure (the specific values of `a^B` and `a^S`).\n\n---\n\n## Data / Model Specification\n\nConsider a market where end-users (buyer and seller) bargain over the transaction price under asymmetric information. This bargaining is inefficient (i.e., the Coase theorem fails), leading to a suboptimal volume of trade. The platform decides to change its price structure by increasing the seller's fee to `a^S + Δa` and decreasing the buyer's fee to `a^B - Δa`, keeping the total fee `a` constant.\n\n---\n\n## Question\n\nWhat is the immediate impact of this price structure change on the market?",
    "Options": {
      "A": "The market remains one-sided because sellers and buyers adjust their price offers and acceptance thresholds by `Δa`, neutralizing the change and leaving transaction volume unaffected.",
      "B": "The market becomes two-sided because the bargaining inefficiency means that any change in price structure will directly impact the volume of transactions.",
      "C": "The volume of transactions increases significantly because the lower fee for buyers (`a^B - Δa`) stimulates demand more than the higher seller fee suppresses supply.",
      "D": "The market remains one-sided because the platform's rules force sellers to absorb the entire fee increase `Δa` without changing their price offers to buyers."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the key insight that Coase theorem failure is necessary but not sufficient for two-sidedness. It uses a Scenario Application strategy. The correct answer identifies the isomorphic nature of the bargaining game, which is the specific mechanism for neutrality. Distractor B represents the classic misconception of equating any inefficiency with two-sidedness. Distractor C makes an unsubstantiated claim about relative elasticities. Distractor D misidentifies the mechanism of neutrality; it's not absorption but perfect pass-through.",
    "qid": "136",
    "model_evaluation_result": {
      "model_raw_response": "C) The market is two-sided. The bargaining inefficiency leads to a sub-optimal volume of transactions, so the change in price will directly impact transaction volume.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 137,
    "Question": "## Background\n\n**Research Question.** This problem explores the fundamental definition of a two-sided market and the conditions that create non-neutrality in the price structure.\n\n**Setting.** A platform charges per-interaction fees `a^B` and `a^S`. The total price level is `a = a^B + a^S`. The market is **one-sided** if transaction volume `V` depends only on `a`, and **two-sided** if `V` also depends on the price structure.\n\n---\n\n## Data / Model Specification\n\nConsider a market where a seller with private cost information makes a price offer `P` to a buyer. In the absence of constraints, a change in platform fees from `(a^S, a^B)` to `(a^S + Δa, a^B - Δa)` is neutral, as the seller simply adjusts their offer `P` upwards by `Δa`. Now, suppose the platform imposes a price cap `P_max` on the transaction price between the seller and buyer. The seller's optimal unconstrained offer before the fee change is `P*`.\n\n---\n\n## Question\n\nUnder which of the following conditions will the price cap `P_max` cause the platform's change in fee structure to be non-neutral, thus making the market two-sided?",
    "Options": {
      "A": "The cap prevents the seller from fully passing through the fee increase, which occurs if `P* + Δa > P_max`.",
      "B": "The cap is binding on the seller's original optimal price, i.e., `P* > P_max`.",
      "C": "The cap forces the seller to absorb all costs, which occurs if `P_max < a^S + Δa`.",
      "D": "The cap is non-binding, allowing the seller to freely adjust their price, i.e., `P* + Δa < P_max`."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to identify a condition that breaks price neutrality. It uses a Scenario Application strategy. The correct answer precisely identifies the binding constraint that prevents the isomorphic adjustment of prices. Distractor B identifies a condition where the cap is already binding, but this doesn't explain why the *change* in fees is non-neutral. Distractor C introduces an irrelevant comparison. Distractor D describes the exact condition under which neutrality *holds*, making it a conceptual opposite.",
    "qid": "137",
    "model_evaluation_result": {
      "model_raw_response": "C) The capital forces the seller absorb all costs",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 138,
    "Question": "### Background\n\nAfter winning a wildcat lease, a firm has five years (20 quarters) to begin exploration. This creates an information externality, as firms have an incentive to wait and let a rival drill an adjacent tract first to learn about the shared prospect. The paper contrasts two models of drilling behavior:\n- **Optimal Coordinated Plan:** Firms cooperate to drill sequentially, maximizing joint profits and minimizing delay.\n- **War of Attrition:** Firms strategically delay drilling, hoping to free-ride on a rival's information. This non-cooperative game predicts a U-shaped pattern for the drilling hazard rate over the lease term.\n\n### Data / Model Specification\n\n**Table 1. Wildcat Tract Drilling Characteristics by Year of Initial Drilling, 1954-1979**\n\n| Year After Acquisition | 1 | 2 | 3 | 4 | 5 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Risk Set (Number)** | 2255 | 1522 | 1159 | 971 | 816 |\n| **Tracts Drilled (Number)** | 733 | 363 | 188 | 155 | 214 |\n\n*Source: Adapted from Table IV in the original paper.*\n\n### Question\n\nBased on the data in Table 1, what is the approximate empirical drilling hazard rate for Year 5, and which model of drilling behavior does the overall five-year pattern support?",
    "Options": {
      "A": "The hazard rate is approximately 26.2%, and the U-shaped pattern supports the \"War of Attrition\" model.",
      "B": "The hazard rate is approximately 26.2%, and the U-shaped pattern supports the \"Optimal Coordinated Plan\".",
      "C": "The hazard rate is approximately 9.5% (214/2255), and the declining pattern supports the \"War of Attrition\" model.",
      "D": "The hazard rate is approximately 32.5%, and the initially high rate supports the \"Optimal Coordinated Plan\"."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a calculation from a table and match the resulting empirical pattern to the correct game-theoretic model. It uses a 'Computational Judgment' strategy. Option A is correct. Option B has the correct calculation but incorrect theory, testing conceptual understanding. Option C uses the wrong denominator for the hazard rate (total tracts instead of the risk set), a common calculation error. Option D correctly identifies the Year 1 hazard rate but misinterprets the overall pattern and misapplies the theory.",
    "qid": "138",
    "model_evaluation_result": {
      "model_raw_response": "C) The answer is C) The hazard.rate is approximately 25.6% (214 / 816), and the U-shape pattern supports the War of Attrition model.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 139,
    "Question": "### Background\n\nConsider a game-theoretic model of drilling timing where two firms with adjacent leases decide each period whether to drill. The payoff from drilling now is `$V_D = p-c$`. The expected payoff from waiting one period is `$V_W = q \\cdot [\\delta p(1-c)] + (1-q) \\cdot [\\delta(p-c)]$`, where `$q$` is the rival's probability of drilling and `$\\delta$` is the discount factor.\n\n### Question\n\nIn the symmetric mixed-strategy equilibrium, each firm must be indifferent between drilling and waiting, so `$V_D = V_W$`. By setting the two payoff expressions equal and solving for `$q$`, what is the equilibrium probability that a firm drills in any given period (prior to the deadline)?",
    "Options": {
      "A": "```latex\nq = \\frac{(1-\\delta)(p-c)}{\\delta c(1-p)}\n```",
      "B": "```latex\nq = \\frac{(1+\\delta)(p-c)}{\\delta c(1-p)}\n```",
      "C": "```latex\nq = \\frac{\\delta c(1-p)}{(1-\\delta)(p-c)}\n```",
      "D": "```latex\nq = \\frac{(p-c)}{\\delta c(1-p)}\n```"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to execute the mathematical derivation of a core theoretical result from the paper. It uses a 'Computational Judgment' strategy. Option A is the correct derivation. Option B introduces a sign error in the discount term `(1+δ)`, a common algebraic slip. Option C inverts the numerator and denominator, a mistake that can happen when isolating `q`. Option D omits the `(1-δ)` term, representing a step-omission error in the algebra.",
    "qid": "139",
    "model_evaluation_result": {
      "model_raw_response": "B) `$$q = \\dfrac{(1 + \\delta)(p - c)}{\\delta \\cdot c \\cdot (1 - p)}$$`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 140,
    "Question": "### Background\n\n**Research Question.** This problem examines how different types of information technology can have opposing effects on the boundaries of the firm, depending on which organizational trade-off they influence. It focuses on the shipper's decision to use a private fleet versus a for-hire carrier.\n\n**Setting / Institutional Environment.** A shipper's choice between a private fleet (integration) and a for-hire carrier (disintegration) involves two key trade-offs:\n1.  **Service vs. Cost:** Private fleets are generally better at providing customized, high-service hauls (e.g., complex cargo handling), but motivating drivers for these tasks is difficult and costly.\n2.  **Utilization Efficiency:** For-hire carriers are specialists in logistics and can achieve higher truck utilization by finding backhauls across a large network of customers, a key comparative advantage.\n\nWe consider a specific type of on-board computer:\n-   **`OBC_M` (Monitoring):** Provides verifiable data on driver actions, making it easier to contract for high-service tasks.\n\n### The Question\n\nAccording to the \"contractual externality\" argument presented in the paper, why does the adoption of a driver-monitoring OBC (`OBC_M`) lead to more shipper ownership of trucks (i.e., more private fleets)?",
    "Options": {
      "A": "`OBC_M` makes it cheaper to motivate drivers for high-service tasks, increasing the optimal service level demanded by shippers and thereby favoring private fleets which specialize in high service.",
      "B": "`OBC_M` enhances the logistical efficiency of private fleets, allowing them to better compete with the utilization rates of for-hire carriers.",
      "C": "`OBC_M` reduces the need for relationship-specific investments by drivers, making arm's-length contracts with for-hire carriers more efficient.",
      "D": "`OBC_M` allows shippers to directly monitor for-hire carrier drivers, reducing agency costs in outsourced relationships without changing ownership."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the core 'contractual externality' argument. It uses a Scenario Application strategy to test the student's grasp of the multi-step causal chain. The correct answer (A) captures the full logic. Distractors are designed to pinpoint specific misunderstandings: (B) confuses the mechanism with the one for logistics tech, (C) applies G&H logic in the opposite direction, and (D) identifies a plausible but incorrect causal path that is not the contractual externality discussed.",
    "qid": "140",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\n(Note: The output is just the letter \"A\", without any additional text.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 141,
    "Question": "### Background\n\n**Research Question.** This problem examines how different types of information technology can have opposing effects on the boundaries of the firm, depending on which organizational trade-off they influence. It focuses on the shipper's decision to use a private fleet versus a for-hire carrier.\n\n**Setting / Institutional Environment.** A shipper's choice between a private fleet (integration) and a for-hire carrier (disintegration) involves two key trade-offs:\n1.  **Service vs. Cost:** Private fleets are generally better at providing customized, high-service hauls.\n2.  **Utilization Efficiency:** For-hire carriers are specialists in logistics and can achieve higher truck utilization by finding backhauls across a large network of customers, a key comparative advantage.\n\nWe consider a specific type of on-board computer:\n-   **`OBC_L` (Logistics):** Provides GPS tracking and communication, improving a dispatcher's ability to find and schedule backhauls.\n\n### The Question\n\nIn contrast to monitoring technology, a logistics-enhancing OBC (`OBC_L`) improves a dispatcher's ability to find and schedule backhauls. Based on the 'comparative advantage' mechanism, what is the predicted effect of `OBC_L` adoption on firm boundaries?",
    "Options": {
      "A": "It leads to less shipper ownership because it amplifies the core advantage of for-hire carriers in network logistics and utilization efficiency.",
      "B": "It leads to more shipper ownership because it allows private fleets to close the utilization gap with for-hire carriers.",
      "C": "It leads to more shipper ownership because better logistics require higher driver service levels, which are best provided by private fleets.",
      "D": "It has no effect on ownership, as logistics are independent of the contractual problems between shippers and carriers."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the 'comparative advantage' argument using a qualitative Computational Judgment strategy. It tests a core, sharp prediction from the paper. The correct answer (A) identifies the right effect and mechanism. Distractors test for common errors: (B) reverses the logic of who benefits more from the technology, (C) incorrectly links the logistics mechanism to the service-level argument, and (D) denies the premise of the question.",
    "qid": "141",
    "model_evaluation_result": {
      "model_raw_response": "C) It lead to more shipPER ownership because better Logistics require higher driver Service levels, which ARE BEST provided BY PRIVATE FLEETS.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 142,
    "Question": "### Background\n\n**Research Question.** You are empirically testing the opposing predictions about how two types of on-board computers (OBCs) affect a shipper's decision to own its own trucks.\n\n### Data / Model Specification\n\nYou estimate the following linear probability model for shipper ownership:\n`ShipperOwn_i = β_0 + β_M OBC_{Mi} + β_L OBC_{Li} + X_i'δ + ε_i`\n\nWhere:\n- `OBC_M` is a monitoring technology.\n- `OBC_L` is a logistics technology.\n\nYour theory predicts:\n- `β_M > 0` (monitoring tech increases shipper ownership).\n- `β_L < 0` (logistics tech decreases shipper ownership).\n\nHowever, you face two empirical challenges:\n1.  Many OBCs bundle both features, so `Corr(OBC_M, OBC_L) > 0`.\n2.  Adoption of any advanced technology is correlated with unobserved firm sophistication, which itself favors private fleets. So, `Corr(OBC_L, Sophistication) > 0` and the effect of `Sophistication` on `ShipperOwn` is positive.\n\n### The Question\n\nGiven this scenario, what is the most likely direction of the bias on the OLS estimate of `β_L`, the coefficient for the logistics OBC, when `Sophistication` is an omitted variable?",
    "Options": {
      "A": "Positive bias, making the estimate appear less negative, or even positive.",
      "B": "Negative bias, making the estimate appear more negative than it truly is.",
      "C": "The bias is ambiguous because the positive bias from omitted sophistication and negative bias from confounding with `OBC_M` may offset.",
      "D": "There is no bias on `β_L` because `OBC_M` is included in the regression, which controls for its effect."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item uses a Computational Judgment strategy to test a difficult econometric concept (decomposing bias), capturing the essence of the original QA's 'Mathematical Apex'. The correct answer (A) correctly identifies the positive bias from two separate sources. Distractors are based on common errors: (B) a sign error on the total bias, (C) a sign error on one component of the bias (confounding with `OBC_M` also creates positive bias), and (D) a fundamental misunderstanding of omitted variable bias.",
    "qid": "142",
    "model_evaluation_result": {
      "model_raw_response": "B) Negativebias,makingtheestimatappearmorenegative thanitistrulyis.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 143,
    "Question": "### Background\n\n**Research Question.** This problem examines the critical assumptions required to identify the causal return to education using family background variables as instruments, particularly highlighting the different identification challenges posed by the Family and Individual models.\n\n**Setting.** The identification strategy relies on using a vector of family background variables, `Z_if`, as instruments for education. The validity of these instruments depends on their relationship with unobserved parental endowments (`ε_f`) and child-specific idiosyncratic endowments (`ξ_if`).\n\n### Data / Model Specification\n\nThe paper models a child's total earnings endowment (`ε_if`) and the family background variables (`Z_if`) as follows:\n```latex\n\\ln(\\varepsilon_{if}) = \\rho \\ln(\\varepsilon_f) + \\xi_{if} \\quad \\text{(Eq. 1)}\n```\n```latex\n\\mathbf{Z}_{if} = \\Pi \\ln(\\varepsilon_f) + \\xi_f + \\kappa_1 \\xi_{if} + \\kappa_2 \\xi_{kf} \\quad \\text{(Eq. 2)}\n```\nwhere `ρ` is the intergenerational correlation of endowments, `Π` links `Z` to parental endowment, and `κ_1` and `κ_2` link `Z` to the idiosyncratic endowments of child `i` and their sibling `k`.\n\nAn instrument is valid if it is uncorrelated with the error term of the education choice equation. \n*   In the **Family Model**, this error term `e_if1` depends on the *difference* in idiosyncratic endowments: `e_{if1} = δ(ξ_{if} - ξ_{kf}) + h_f + ...`\n*   In the **Individual Model**, this error term `e_if2` depends on the child's *total* endowment: `e_{if2} ∝ α(ρ \\ln(ε_f) + ξ_{if}) + ...`\n\n### Question\n\nWhich statement best explains why family background variables (`Z_if`) face different identification challenges as instruments for education in the Individual Model versus the Family Model, assuming intergenerational correlation in endowments (`ρ > 0`) and `Π ≠ 0`?",
    "Options": {
      "A": "In the Individual Model, `Z_if` is correlated with the error term via the shared parental endowment `ε_f`, making it invalid. In the Family Model, the error term is driven by the *difference* in sibling endowments, so the shared parental endowment `ε_f` is differenced out, preserving instrument validity under certain assumptions.",
      "B": "In the Individual Model, instruments are valid because individuals' choices are independent of family background. In the Family Model, `Z_if` is correlated with parental preferences for equity, making it an invalid instrument.",
      "C": "In the Individual Model, instrument validity requires assuming no intergenerational correlation in endowments (`ρ = 0`). In the Family Model, instrument validity requires assuming parents do not compensate for endowment differences (`δ = 0`).",
      "D": "In both models, `Z_if` is an invalid instrument because it is correlated with unobserved family resources (`h_f`). The only way to achieve identification is with a sibling-difference estimator."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the core identification logic of the paper. It uses a Reverse-Reasoning strategy, asking the student to identify the correct explanation for a key theoretical result. The correct answer (A) accurately summarizes the central argument. Distractor (B) is a conceptual opposite. Distractor (C) presents plausible but incorrect assumptions for each model. Distractor (D) offers a true statement about a potential problem (`h_f`) but incorrectly presents it as the primary distinction and the only solution, which is a 'True but Irrelevant/Incomplete' error.",
    "qid": "143",
    "model_evaluation_result": {
      "model_raw_response": "C) In both cases, the error terms `e_if` are functions of `ρ` and `Π`. In the Individual model, the error `e_if ∝ α((ρ ln(ε_f)+ξ_if)` is correlated with `Z_if`. In the Family model, `e_if = δ(ζ_if-ζ_kf)+h_f`, where `h_f` is correlated to `Z_if.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 144,
    "Question": "### Background\n\n**Research Question.** This problem examines the critical assumptions required to identify the causal return to education using family background variables as instruments, particularly highlighting the different identification challenges posed by the Family and Individual models.\n\n**Setting.** The identification strategy relies on using a vector of family background variables, `Z_if`, as instruments for education. The validity of these instruments depends on their relationship with unobserved parental endowments (`ε_f`) and child-specific idiosyncratic endowments (`ξ_if`).\n\n### Data / Model Specification\n\nThe paper models a child's total earnings endowment (`ε_if`) and the family background variables (`Z_if`) as follows:\n```latex\n\\ln(\\varepsilon_{if}) = \\rho \\ln(\\varepsilon_f) + \\xi_{if} \\quad \\text{(Eq. 1)}\n```\n```latex\n\\mathbf{Z}_{if} = \\Pi \\ln(\\varepsilon_f) + \\xi_f + \\kappa_1 \\xi_{if} + \\kappa_2 \\xi_{kf} \\quad \\text{(Eq. 2)}\n```\nwhere `ρ` is the intergenerational correlation of endowments, `Π` links `Z` to parental endowment, and `κ_1` and `κ_2` link `Z` to the idiosyncratic endowments of child `i` and their sibling `k`.\n\nThe paper argues that a sibling-difference estimator can identify the return to education in both models under the single key assumption of \"no Z-response to children's idiosyncratic endowments,\" which means `κ_1 = κ_2`.\n\n### Question\n\nConsider the following potential family background variables (`Z_if`). Which pair correctly identifies one variable for which the `κ_1 = κ_2` assumption is highly plausible, and one for which it is likely violated?",
    "Options": {
      "A": "Plausible: Mother's education. Violated: Whether the child ever lived in a single-parent family.",
      "B": "Plausible: Whether the child ever lived in a single-parent family. Violated: Mother's education.",
      "C": "Plausible: Parents' income when the child was 16. Violated: Mother's age at the child's birth.",
      "D": "Plausible: Number of siblings. Violated: Parents' house value when the child was 16."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to apply a key abstract identification assumption to concrete, real-world variables. It uses a Scenario Application strategy. The correct answer (A) provides the clearest contrast: mother's education is pre-determined and cannot respond to a child's specific endowment, while family structure can change in response to a child's characteristics (e.g., a difficult child straining a marriage). Distractor (B) is a direct reversal of the correct logic. Distractors (C) and (D) present less clear-cut cases or pairs where both variables are likely to satisfy (or violate) the assumption, making them weaker choices.",
    "qid": "144",
    "model_evaluation_result": {
      "model_raw_response": "B) Plausibl: Whether the(child ever lived in single-parent family). Violated: mother's education.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 145,
    "Question": "### Background\n\n**Research Question.** This problem investigates the drivers of the observed decline in 'world efficiency', a measure of how close the world's average productivity is to the technological frontier. The central question is whether this decline is due to falling productivity within countries or a reallocation of economic activity across countries.\n\n**Setting / Institutional Environment.** The study analyzes data for 38 economies from 1995 to 2011. A key empirical finding is that world efficiency in the market sector fell from 51% to 46%. This occurred as the share of world inputs used by the frontier country (the US) declined, while the shares of large, less-efficient emerging economies (China, India) increased.\n\n---\n\n### Data / Model Specification\n\nWorld efficiency is defined as the ratio of world average productivity to the productivity frontier:\n```latex\nE_t = \\frac{T_t}{T_{t,max}} = \\frac{\\sum_{k=1}^K \\omega_{kt} T_{kt}}{T_{t,max}} \\quad \\text{(Eq. (1))}\n```\nwhere `T_{kt}` is the TFP level for country `k` at time `t`, `\\omega_{kt}` is its share of world inputs, and `T_{t,max}` is the productivity frontier.\n\nTo isolate the cause of the decline in `E_t`, the authors compute a counterfactual efficiency for 2011 that uses 2011 productivity levels but holds input shares constant at their 1995 levels:\n```latex\nE^*_{2011} = \\frac{\\sum_{k=1}^K \\omega_{k,1995} T_{k,2011}}{T_{2011,max}} \\quad \\text{(Eq. (2))}\n```\n**Observed Values:**\n- `E_{1995}` = 0.510\n- `E_{2011}` = 0.460\n- `E^*_{2011}` = 0.505\n\n---\n\n### The Question\n\nGiven the observed values, what is the primary conclusion from the counterfactual analysis regarding the 5 percentage point decline in world efficiency (`E_t`) from 1995 to 2011?",
    "Options": {
      "A": "The decline was almost entirely driven by a compositional shift of world inputs towards countries with lower productivity levels.",
      "B": "The decline was almost entirely driven by falling productivity levels within most of the 38 countries in the sample.",
      "C": "The decline was driven roughly equally by both compositional shifts and within-country productivity declines.",
      "D": "The decline was an artifact of the 2008 financial crisis, which disproportionately affected high-productivity countries."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the ability to interpret a counterfactual experiment and draw a causal inference. Strategy: Computational Judgment. The student must compare the actual change (`0.510 -> 0.460`) with the counterfactual change (`0.510 -> 0.505`) to deduce the primary driver. Distractor Logic: B is the conceptual opposite (blaming within-country effects). C is a plausible but incorrect conclusion if one doesn't compute the magnitudes. D is a true but irrelevant fact that doesn't follow from the provided counterfactual evidence.",
    "qid": "145",
    "model_evaluation_result": {
      "model_raw_response": "C) The Decline was driven roughly evenly by both composational shifts and within-Country productivity declines.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 146,
    "Question": "### Background\n\n**Research Question.** This problem investigates the drivers of the observed decline in 'world efficiency', a measure of how close the world's average productivity is to the technological frontier. The central question is whether this decline is due to falling productivity within countries or a reallocation of economic activity across countries.\n\n**Setting / Institutional Environment.** The study analyzes data for 38 economies from 1995 to 2011. A key empirical finding is that world efficiency in the market sector fell from 51% to 46%. This occurred as the share of world inputs used by the frontier country (the US) declined, while the shares of large, less-efficient emerging economies (China, India) increased.\n\n---\n\n### Data / Model Specification\n\nWorld efficiency is defined as the ratio of world average productivity to the productivity frontier:\n```latex\nE_t = \\frac{T_t}{T_{t,max}} = \\frac{\\sum_{k=1}^K \\omega_{kt} T_{kt}}{T_{t,max}} \\quad \\text{(Eq. (1))}\n```\nwhere `T_{kt}` is the TFP level for country `k` at time `t` and `\\omega_{kt}` is its share of world inputs.\n\nA formal decomposition of the total change in world average productivity (`\\Delta T_t`) separates it into a 'pure productivity effect' (holding input shares `\\omega` constant) and a 'pure composition effect' (holding productivity `T` constant).\n\n---\n\n### The Question\n\nIn this type of decomposition, the pure productivity effect and the pure composition effect often do not sum perfectly to the total observed change. What is the correct explanation for this discrepancy?",
    "Options": {
      "A": "The discrepancy is due to an 'interaction effect', which captures whether input shares grew more in countries that also experienced higher productivity growth.",
      "B": "The discrepancy is due to the use of the Törnqvist index, which is a statistical approximation and introduces measurement error.",
      "C": "The discrepancy is due to data revisions and inconsistencies in how TFP is measured across different countries and time periods.",
      "D": "There should be no discrepancy; if the calculations are done correctly, the two effects must sum to the total change by definition."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the student's deep understanding of decomposition methods, specifically the concept of an interaction term. Strategy: Reverse-Reasoning. Given a common empirical artifact (a decomposition gap), the student must infer the correct statistical cause. Distractor Logic: A is the correct statistical reason. B (Formula Misuse) and C (True but Irrelevant) point to other potential data issues but are not the mathematical reason for the decomposition gap. D (Conceptual Opposite) represents a common but incorrect belief that such decompositions are perfectly additive.",
    "qid": "146",
    "model_evaluation_result": {
      "model_raw_response": "C) The discretion is due to.data revisions and inconsistencies",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 147,
    "Question": "### Background\n\nThe paper's main result (Theorem 1) shows that the GAFT model is identified up to a power transformation `ρ` if the model's primitives exhibit regular variation at their tails. The power `ρ` depends on the tail exponents of the primitives in any two observationally equivalent models, denoted `(Λ, φ, G)` and `(Λ̃, φ̃, G̃)`.\n\nSpecifically, if the densities `g` and `g̃` vary regularly at 0 with exponents `τ̲` and `τ̅` respectively, then `ρ = (τ̅+1)/(τ̲+1)`.\n\n### Data / Model Specification\n\nThe Mixed Proportional Hazards (MPH) model is a special case of GAFT. Prior literature established identification for the MPH model under two different assumptions:\n\n1.  **Elbers & Ridder:** `lim_{s→0} g(s)` is a positive, finite constant.\n2.  **Heckman & Singer:** `g(s)` varies regularly at 0 with a known exponent `τ ∈ (-1, 0)`.\n\n### Question\n\nWithin the framework of Theorem 1, why do both of these seemingly different assumptions lead to point identification (i.e., the ambiguity parameter `ρ` must equal 1)?",
    "Options": {
      "A": "Both assumptions imply that `g(s)` and any observationally equivalent `g̃(s)` must have the same, known exponent of regular variation at zero, forcing the ratio `(τ̅+1)/(τ̲+1)` to be 1.",
      "B": "Both assumptions require the baseline hazard `λ(t)` to be constant for small `t`, which directly implies `ρ=1`.",
      "C": "Both assumptions are equivalent to requiring the unobserved heterogeneity distribution `H(υ)` to be a single point mass, which eliminates all ambiguity.",
      "D": "Both assumptions ensure that the transformation `K(s)` must be the identity function `K(s)=s`, which is a result of `ρ=1`, not the cause."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to synthesize information by unifying two different historical identification assumptions under the paper's general framework (Theorem 1). It uses a Reverse-Reasoning strategy, asking for the underlying reason for a shared outcome (`ρ=1`). Distractors are designed to test for confusion between model primitives (`g` vs. `λ`) (B), over-simplification of the assumptions (C), and confusing a result with a cause (D).",
    "qid": "147",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\n(Note: The answer is just the letter \"A\", without any additional text.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 148,
    "Question": "### Background\n\nThe paper's main identification result (Theorem 1) is derived for the case of continuously observed durations `t` and two discrete covariate values `x₀, x₁`. However, the authors argue the result can be adapted to a different data structure common in applied work.\n\n### Data / Model Specification\n\nConsider a scenario with **discretely observed durations**, where the CDF `F(t|x)` is known only for two time points, `t ∈ {t₀, t₁}`, but for a **continuous range of covariates** `x`.\n\nThe paper proposes analyzing this by \"swapping the roles\" of time and covariates: the continuous function `φ(x)` takes the role of the integrated baseline hazard `Λ(t)`, and the fixed values `Λ(t₀)` and `Λ(t₁)` take the roles of the discrete covariate effects `φ(x₀)` and `φ(x₁)`. Theorem 1 is then applied to this re-framed problem.\n\n### Question\n\nUnder this \"role-swapping\" scenario, what is the precise identification result for the original structural functions `φ(·)` and `Λ(·)`?",
    "Options": {
      "A": "The entire functions `φ(·)` and `Λ(·)` are both identified up to a common power transformation.",
      "B": "The entire function `φ(·)` is identified up to a power transformation, but the function `Λ(·)` is only identified at the points `t₀` and `t₁`.",
      "C": "The entire function `Λ(·)` is identified up to a power transformation, but the function `φ(·)` is only identified for two specific values of `x`.",
      "D": "Neither function is identified nonparametrically; only the product `φ(x)Λ(t)` is identified at the observed points."
    },
    "Answer": "B",
    "pi_justification": "This item tests the application of the paper's core theorem to a novel data scenario, a key extension discussed in Section 4. It uses the Scenario Application strategy. The distractors are built around a matrix of common comprehension errors: overstating the power of the result (A), completely inverting the logic of what is identified (C), and understating the result (D).",
    "qid": "148",
    "model_evaluation_result": {
      "model_raw_response": "D) Neither fonction is identified non-parametrically; Only the product φ(x)Λ(·) is identified at observed points. \n\nAnswer: D",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 149,
    "Question": "## Background\n\n**Research Question.** This problem evaluates the experimental design used to test theoretical predictions about charitable competition, focusing on how the design isolates causal effects.\n\n**Setting.** A series of controlled laboratory experiments where subjects make real donation decisions. The design varies two key dimensions: the relationship between the charitable causes and the number of donors.\n\n---\n\n## Data / Model Specification\n\nThe experimental design is summarized in Table 1.\n\n**Table 1. Summary of the Experiments**\n| Experiment | No. of donors per recipient | Relationship between causes |\n| :--- | :--- | :--- |\n| Subs | 1 | Substitutes |\n| Comp | 1 | Complements |\n| Comp-W | 1 | Weak complements |\n| Subs-M | 2 | Substitutes |\n\n---\n\n## Question\n\nWhat is the primary causal question addressed by comparing the results of the `Subs` experiment to the `Subs-M` experiment?",
    "Options": {
      "A": "To test whether the \"business stealing\" effect is robust to the introduction of free-riding incentives.",
      "B": "To measure how the effect of a rebate campaign differs when charities are substitutes versus complements.",
      "C": "To determine the optimal rebate level that maximizes total donations for substitute charities.",
      "D": "To isolate the pure effect of warm-glow giving by removing altruistic motives."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of experimental identification strategy, using a Reverse-Reasoning approach. The correct answer (A) correctly identifies that the only change between `Subs` and `Subs-M` is the number of donors, which introduces strategic free-riding concerns. Distractor B describes the `Subs` vs. `Comp` comparison (True but Irrelevant). Distractor C describes a question answered *within* an experiment, not by comparing these two. Distractor D is an 'Almost Right' conceptual error; this comparison *allows* a test of warm-glow vs. altruism, but the primary causal question isolated by the design is about the robustness of the main effect to the strategic environment.",
    "qid": "149",
    "model_evaluation_result": {
      "model_raw_response": "A) To assess whether the \"Business Stealing\" effect is Robust to the Introduction of Free-Riding Incentives.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 150,
    "Question": "## Background\n\n**Research Question.** This problem evaluates the experimental design used to test theoretical predictions about charitable competition, focusing on the role of control treatments for ensuring internal validity.\n\n**Setting.** A series of controlled laboratory experiments where subjects make real donation decisions. The design varies the relationship between the charitable causes (e.g., complements).\n\n---\n\n## Data / Model Specification\n\nTwo experiments focused on complementary causes:\n- **`Comp`**: Subjects donated to toothpaste and toothbrushes. Any unpaired items (e.g., a tube of toothpaste without a matching toothbrush) were forfeited and not donated.\n- **`Comp-W`**: This experiment was identical to `Comp`, except that unpaired items were still donated individually to a homeless person.\n\n---\n\n## Question\n\nWhat is the most important reason for conducting the `Comp-W` experiment as a control for the `Comp` experiment?",
    "Options": {
      "A": "To test if subjects understood that toothpaste and toothbrushes are complements.",
      "B": "To ensure that the results in `Comp` were driven by the complementary nature of the goods, not by an aversion to the \"waste\" of forfeited donations.",
      "C": "To measure the baseline level of giving to complementary charities in a more realistic setting.",
      "D": "To see if the \"business stealing\" effect re-emerges when the complementarity is weakened."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the understanding of internal validity and confounders in experimental design, using a Scenario Application strategy. The correct answer (B) identifies the specific confound—aversion to waste—that the `Comp-W` design is meant to rule out. Distractor A is an 'Almost Right' error; while related, the key issue is the *reason* for the behavior, not just understanding. Distractor C is 'True but Irrelevant'; while `Comp-W` might be more realistic, its primary purpose as a control is to validate the `Comp` results. Distractor D is also plausible but secondary; the main goal is to check the mechanism in `Comp`, not just to see if stealing appears.",
    "qid": "150",
    "model_evaluation_result": {
      "model_raw_response": "B) To ensue that the results n `Comp` wer drivwn by the complementary natur eof the goods, n ot by an aversiin to the \"waiste\" of forfeitd donations.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 151,
    "Question": "## Background\n\n**Research Question.** This problem evaluates how the experimental design distinguishes between competing behavioral models of giving (pure altruism vs. warm-glow).\n\n**Setting.** The `Subs` experiment involved a single donor (N=1) for a cause, while the `Subs-M` experiment involved two donors (N=2) for the same cause.\n\n---\n\n## Data / Model Specification\n\nThe pure altruism model assumes an agent's utility depends on the total amount donated to a cause, `G`, regardless of who contributes. This implies that the equilibrium total donation `G*` is determined by the marginal costs and benefits of giving and is independent of the number of donors `N`.\n\n---\n\n## Question\n\nUnder the pure altruism model, what is the specific, testable prediction when comparing the average *individual* donation in the single-donor `Subs` experiment to the average *individual* donation in the two-donor `Subs-M` experiment?",
    "Options": {
      "A": "The average individual donation will be half as large in `Subs-M` as in `Subs`.",
      "B": "The average individual donation will be the same in both experiments.",
      "C": "The total donation (summed across donors) will be twice as large in `Subs-M` as in `Subs`.",
      "D": "The average individual donation will be negative in `Subs-M` due to free-riding."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to derive a sharp prediction from a theoretical model, a form of Computational Judgment. The correct answer (A) correctly applies the logic of the pure altruism model: if total giving `G*` is constant, and `N` goes from 1 to 2, each individual's contribution must be `G*/2`. Distractor B is the prediction of a pure warm-glow model (Conceptual Opposite). Distractor C confuses individual and total giving; the model predicts total giving will be the same, not double. Distractor D is an illogical exaggeration of the free-riding effect.",
    "qid": "151",
    "model_evaluation_result": {
      "model_raw_response": "C) The average total donation will be twice the average individual donation in `Subsubs`.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 152,
    "Question": "### Background\n\nAn analysis decomposes the change in pollution from US manufacturing between 1987 and 2001 into three effects: `scale` (overall economic growth), `composition` (the mix of industries), and `technique` (changes in production and abatement technology). The `technique` effect is calculated as a residual after accounting for the other two.\n\n### Data / Model Specification\n\n**Table 1: Scale, Composition, and Technique Effects for SO2, 1987-2001**\n\n*All figures are percentage changes relative to 1987 levels.*\n\n| Effect | Value |\n| :--- | :--- |\n| Scale effect (a) | 0.24 |\n| Scale, composition, and technique (observed total) (b) | -0.27 |\n| Scale and composition (predicted) (c) | 0.12 |\n| Fraction of cleanup due to technique [(c) - (b)]/[(a) - (b)] (d) | 0.77 |\n\n### Question\n\nBased on the data for SO2 in Table 1, which of the following statements is the most accurate calculation and interpretation?",
    "Options": {
      "A": "The composition effect alone reduced emissions by 12 percentage points, and the technique effect alone reduced them by 39 percentage points, with technique accounting for 77% of the total cleanup gap.",
      "B": "The composition effect reduced emissions by 12 percentage points, but the technique effect only reduced them by 15 percentage points because the observed total change was -0.27 and the predicted change from scale and composition was 0.12.",
      "C": "The composition effect increased emissions by 12 percentage points relative to the 1987 baseline, implying a shift towards dirtier industries.",
      "D": "The technique effect was a 39 percentage point reduction, but this represents only a minority of the total cleanup because the scale effect (24 percentage points) was so large."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform multi-step calculations from a table and interpret the results in context. It uses a Computational Judgment strategy. Distractors are designed based on common arithmetic errors (sign error in B), misreading the table (confusing a combined effect with a marginal effect in C), and faulty reasoning about the final fraction's meaning (D).",
    "qid": "152",
    "model_evaluation_result": {
      "model_raw_response": "A) Thecomposition effect alone reducedemissions by 12percentagepoints, and thetechniqueeffect alone reducedthem by 39percentagepoints, withtechniqueaccounting for 77of thetotal cleanup gap.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 153,
    "Question": "### Background\n\nThe study approximates total manufacturing pollution using data organized by \"activity\" (from the National Emissions Inventory, NEI) rather than by more precise industry codes. To test the validity of this choice, the author compares the approximation to the \"true\" pollution calculated from industry codes for years where both are available.\n\n### Data / Model Specification\n\n**Table 1: Ratio of True Manufacturing Pollution to Approximated Pollution**\n\n| Pollutant | 1990 | 1996 | 1997 | 1998 | 1999 | 2000 | 2001 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| All four | 0.77 | 0.73 | 0.74 | 0.74 | 0.73 | 0.72 | 0.73 |\n\n### Question\n\nTable 1 examines the ratio of true manufacturing pollution to the study's approximation. Based on the trend for 'All four' pollutants, what is the most valid conclusion regarding the study's main finding that the 'technique effect' was the primary driver of the cleanup?",
    "Options": {
      "A": "The ratio's stability supports the approximation's validity, and its slight downward trend suggests the study's estimate of the technique effect's importance is likely conservative (an understatement).",
      "B": "The ratio's slight downward trend suggests that the study's approximation is increasingly inaccurate, likely leading to an overstatement of the technique effect's importance.",
      "C": "The ratio's fluctuation between 0.72 and 0.77 demonstrates significant instability, rendering the approximation unreliable for drawing conclusions.",
      "D": "The consistent downward trend from 1996 to 2001 indicates that the approximation method was becoming progressively worse and should not have been used."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to critique a data construction choice by interpreting a validation table. It uses a Reverse-Reasoning strategy. Distractors target common misinterpretations: B incorrectly identifies the direction of the potential bias, C mischaracterizes 'stability', and D overstates the magnitude and implication of the slight trend.",
    "qid": "153",
    "model_evaluation_result": {
      "model_raw_response": "B) The ratios' slight downward trend from the early to late 1990s suggests that the approximation is becoming less accurate, implying that the study may have overestimated the importance of the technique's impact.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 154,
    "Question": "### Background\n\nA study decomposes pollution changes into scale, composition, and technique effects. The main analysis covers 1987-2001. A robustness check extends the analysis to 1972-2001, but because industry-specific pollution intensity data (`z_i`) is not available for 1972, the calculation must use the 1987 intensity data for the entire period.\n\n### Data / Model Specification\n\n**Table 1: Fraction of Cleanup Due to Technique: Alternative Estimates**\n\n| Pollutant | Baseline (1987-2001) (a) | Deflating using PPI (b) | 1972-2001 (using 1987 intensities) (c) |\n| :--- | :--- | :--- | :--- |\n| All four | 0.78 | 0.69 | 0.62 |\n\n### Question\n\nThe analysis in column (c) of Table 1 extends the time period to 1972-2001 but must use 1987 pollution intensity data. For this calculation to be an unbiased estimate of the technique effect's share, which critical assumption about technological change from 1972 to 1987 must hold?",
    "Options": {
      "A": "Technological changes must have altered every industry's emissions intensity by an equal percentage.",
      "B": "Each industry's emissions intensity (the `z_i` values) must have remained constant over that period.",
      "C": "The composition of US manufacturing (the `θ_i` shares) must have remained stable between 1972 and 1987.",
      "D": "Only the most pollution-intensive industries experienced technological improvements in abatement."
    },
    "Answer": "A",
    "pi_justification": "This item assesses deep understanding of the identification strategy and its underlying assumptions, a key skill in empirical analysis. It uses a Scenario Application strategy. The distractors represent classic conceptual errors: B confuses the actual assumption with a much stronger (and obviously false) one, C confuses a condition on technology with a condition on composition, and D represents a specific scenario that would explicitly violate the required assumption.",
    "qid": "154",
    "model_evaluation_result": {
      "model_raw_response": "C) Thecomposition of US manufacturing (`θ_i`shares) must have remaindstable between 1977 and 1988.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 155,
    "Question": "### Background\n\n**Research Question.** This problem investigates the core inferential framework of the paper for testing a shape restriction `θ₀ ∈ Λ`, where `Λ` is a convex cone. It focuses on the challenge posed by a non-standard limiting distribution.\n\n**Setting / Institutional Environment.** The analysis is set in an abstract Hilbert space `H` of functions with norm `||·||_H`.\n\n---\n\n### Data / Model Specification\n\nLet `θ₀` be the true function of interest and `θ̂ₙ` be an unconstrained estimator. The null hypothesis is `H₀: θ₀ ∈ Λ`. The test statistic is `Tₙ = rₙ||θ̂ₙ - Π_Λ(θ̂ₙ)||_H`, where `Π_Λ(·)` is the projection operator onto `Λ` and `rₙ → ∞` is a rate of convergence.\n\nUnder the null hypothesis `θ₀ ∈ Λ`, the test statistic can be expressed as:\n```latex\nT_n = \\left\\| r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + r_{n}\\theta_{0} - \\Pi_{\\Lambda}\\big(r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + r_{n}\\theta_{0}\\big) \\right\\|_{\\mathbf{H}} \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### The Question\n\nThe test statistic `Tₙ` in Eq. (1) has a limiting distribution that depends on the unknown true function `θ₀`, specifically through the term `rₙθ₀`. Why does this term invalidate a standard plug-in bootstrap procedure where `rₙθ₀` would be replaced by its estimate `rₙθ̂ₙ`?\n",
    "Options": {
      "A": "The projection operator `Π_Λ(·)` is non-linear, and standard bootstrap methods are only valid for linear estimators.",
      "B": "The estimation error `rₙ(θ̂ₙ - θ₀)` is of order `O_p(1)` and does not vanish, so replacing `rₙθ₀` with `rₙθ̂ₙ` introduces a non-negligible error into the bootstrap distribution.",
      "C": "The rate of convergence `rₙ` is unknown in practice and must be estimated, which introduces too much uncertainty for the bootstrap to be valid.",
      "D": "The term `rₙθ₀` can be consistently estimated by `rₙθ̂ₙ`, but the resulting bootstrap is computationally infeasible in a Hilbert space."
    },
    "Answer": "B",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item uses a Reverse-Reasoning strategy to assess understanding of the fundamental statistical challenge that motivates the paper's entire methodology. The correct answer pinpoints the non-vanishing estimation error of the drift term. Distractor (A) is a true but secondary issue, (C) is factually incorrect for this context, and (D) represents the primary conceptual misconception.",
    "qid": "155",
    "model_evaluation_result": {
      "model_raw_response": "B) The estimating error `r_n(θ^_n - θ_0)` is of O_p(1) and does not vanishes, so replacing r_nθ_0 with r_nθ_^_n introduces a nonnegligible error in the bootstrap distribution.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 156,
    "Question": "### Background\n\n**Research Question.** This problem investigates the core inferential framework of the paper for testing a shape restriction `θ₀ ∈ Λ`, where `Λ` is a convex cone. It focuses on the paper's proposed solution for constructing a non-conservative test.\n\n**Setting / Institutional Environment.** The analysis is set in an abstract Hilbert space `H` of functions with norm `||·||_H`.\n\n---\n\n### Data / Model Specification\n\nLet `θ₀` be the true function and `θ̂ₙ` be its estimator. The test statistic is `Tₙ = rₙ||θ̂ₙ - Π_Λ(θ̂ₙ)||_H`. Under the null `H₀: θ₀ ∈ Λ`, the following inequality holds for `0 ≤ κₙ ≤ rₙ`:\n```latex\nT_n \\leq \\left\\| r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + \\kappa_{n}\\theta_{0} - \\Pi_{\\Lambda}\\big(r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + \\kappa_{n}\\theta_{0}\\big) \\right\\|_{\\mathbf{H}} \\quad \\text{(Eq. (1))}\n```\nThe paper's method involves bootstrapping the right-hand side of Eq. (1) to get a critical value.\n\n---\n\n### The Question\n\nThe paper's method uses the inequality in Eq. (1) and a tuning parameter `κₙ` that is chosen to satisfy `κₙ → ∞`. What is the primary statistical benefit of this approach compared to the \"least favorable\" test, which corresponds to setting `κₙ = 0`?",
    "Options": {
      "A": "It guarantees that the test's empirical size is exactly equal to the nominal level `α` for all null hypotheses, not just the least favorable ones.",
      "B": "It is required for the validity of the bootstrap procedure, which would be invalid if `κₙ` were set to 0.",
      "C": "It makes the test asymptotically non-conservative by using a tighter upper bound for the test statistic, leading to improved power when `θ₀` is in the interior of the null set `Λ`.",
      "D": "It simplifies computation by removing the need to calculate the projection `Π_Λ(·)` in the bootstrap replications."
    },
    "Answer": "C",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item uses a Computational Judgment strategy to assess understanding of the paper's main contribution over prior methods (achieving non-conservativeness). The correct answer correctly links `κₙ → ∞` to improved power. Distractor (A) makes a claim (similarity) that the paper explicitly states does not hold, (B) confuses the condition for power with the condition for validity, and (D) is factually incorrect.",
    "qid": "156",
    "model_evaluation_result": {
      "model_raw_response": "C) It is the correct answer. The paper's method makes the test more powerful by using a more conservative upper bound, which is achieved by setting `κ_n → ∞`, allowing the test to detect alternative hypotheses more effectively.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 157,
    "Question": "### Background\n\n**Setting.** The paper's empirical analysis begins by estimating a regression of individual bids on various bond and auction characteristics to control for observable heterogeneity before estimating the structural auction model. A selection of results from this regression (Specification I) is shown in Table 1.\n\n### Data / Model Specification\n\n**Table 1: Selected Coefficients from First-Stage Bid Regression**\n| Variable                  | Coefficient | S.E.   |\n| :------------------------ | :---------- | :----- |\n| *(Low market uncertainty)*|             |        |\n| High market uncertainty   | -0.1775**   | .0173  |\n| *(AAA credit rating)*     |             |        |\n| A credit rating           | 6.7172**    | .5895  |\n| AA credit rating          | 5.3292**    | .5544  |\n\n*Note: `**p<.01`; reference case in parentheses.*\n\n### Question\n\nIn the bid regression results shown in Table 1, the coefficient on 'High market uncertainty' is negative and statistically significant. According to auction theory, what is the most likely explanation for this finding?",
    "Options": {
      "A": "Bidders become more risk-averse in uncertain markets and demand a higher premium.",
      "B": "The winner's curse is exacerbated, leading bidders to shade their bids more aggressively to avoid overpaying.",
      "C": "Higher uncertainty signals lower secondary market demand for the bond, reducing its fundamental value.",
      "D": "The affiliation between bidders' signals weakens, making it harder to infer the bond's value from competitors' behavior."
    },
    "Answer": "B",
    "pi_justification": "This item assesses understanding of the winner's curse, a core concept in auction theory. It uses a Reverse-Reasoning strategy, asking for the theoretical cause of an empirical finding. Distractor A (Risk Aversion) is a plausible but distinct concept. Distractor C (Reduced Demand) offers a macro-level explanation for a micro-strategic phenomenon. Distractor D (Affiliation) mentions a related concept but misidentifies its role, making it an 'Almost Right' option.",
    "qid": "157",
    "model_evaluation_result": {
      "model_raw_response": "C) Higher certainty signals lower secondary demand for the bonds, reducing its value.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 158,
    "Question": "### Background\n\n**Setting.** The paper's empirical analysis begins by estimating a regression of individual bids on various bond and auction characteristics to control for observable heterogeneity before estimating the structural auction model. A selection of results from this regression (Specification I) is shown in Table 1.\n\n### Data / Model Specification\n\n**Table 1: Selected Coefficients from First-Stage Bid Regression**\n| Variable                  | Coefficient | S.E.   |\n| :------------------------ | :---------- | :----- |\n| *(Low market uncertainty)*|             |        |\n| High market uncertainty   | -0.1775**   | .0173  |\n| *(AAA credit rating)*     |             |        |\n| A credit rating           | 6.7172**    | .5895  |\n| AA credit rating          | 5.3292**    | .5544  |\n\n*Note: `**p<.01`; reference case in parentheses.*\n\n### Question\n\nThe results in Table 1 show that bonds with lower credit ratings (e.g., 'A') attract significantly higher bids than 'AAA' rated bonds. Which mechanism, discussed in the paper, best explains this seemingly counterintuitive result?",
    "Options": {
      "A": "The credit ratings are systematically inaccurate, and bidders' private information correctly identifies these bonds as less risky.",
      "B": "Lower-rated bonds have fewer substitutes, allowing the winning underwriter to earn larger monopoly rents in the secondary market.",
      "C": "Investors in the secondary market have a strong preference for higher-risk, higher-yield assets.",
      "D": "Government regulations provide implicit guarantees for lower-rated municipal bonds, reducing their effective risk."
    },
    "Answer": "B",
    "pi_justification": "This item tests the ability to recall and understand a specific economic explanation for a puzzling empirical result from the paper. It uses a Reverse-Reasoning strategy. Distractor A suggests a data problem, which is a common first thought but not the paper's argument. Distractor C is a demand-side story that is plausible but not the supply-side market power argument made in the paper. Distractor D introduces an external fact that is generally untrue and not mentioned in the paper.",
    "qid": "158",
    "model_evaluation_result": {
      "model_raw_response": "A) The.credit.ratings.are.systematically.inaccurate.and.bidders'.private.information.correctly.identifies.these.bonds.as.less.risky.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 159,
    "Question": "### Background\n\n**Setting.** The paper's analysis is motivated by the Milgrom-Weber revenue ranking theorem, which states that under conditions like bidder symmetry and affiliated values, expected revenues are ranked `R_E ≥ R_S ≥ R_F` (English ≥ Second-Price ≥ First-Price).\n\n### Question\n\nThe Milgrom-Weber theorem predicts that second-price auctions yield at least as much revenue as first-price auctions (`R_S ≥ R_F`). However, this can be reversed in the presence of a single, known 'strong' bidder competing against several 'weak' bidders. In such an asymmetric setting, why might a first-price auction yield higher revenue (`R_F > R_S`)?",
    "Options": {
      "A": "In a second-price auction, the strong bidder wins by paying the low bid of a weak competitor; in a first-price auction, strategic uncertainty forces the strong bidder to place a higher winning bid.",
      "B": "In a first-price auction, weak bidders collude to drive up the price paid by the strong bidder.",
      "C": "In a second-price auction, the strong bidder must bid their true value, which is high; in a first-price auction, they can shade their bid significantly lower.",
      "D": "Weak bidders are more likely to drop out of a first-price auction, reducing competition and lowering revenue compared to a second-price auction."
    },
    "Answer": "A",
    "pi_justification": "This item assesses a deep theoretical concept from auction theory that is relevant to the paper's context. It uses a Scenario Application strategy. The correct answer (A) accurately describes the strategic logic. Distractor B introduces collusion, which is outside the non-cooperative model being discussed. Distractor C (Conceptual Opposite) incorrectly describes bidding strategies in both formats. Distractor D makes an incorrect claim about entry and revenue effects.",
    "qid": "159",
    "model_evaluation_result": {
      "model_raw_response": "C) In second-price auction the strong bidder has to bid their true valuation, whereas in first-price auction they can shade it.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 160,
    "Question": "### Background\n\n**Setting.** A key assumption for the canonical Milgrom-Weber revenue ranking theorem (`R_E ≥ R_S ≥ R_F`) is bidder symmetry—the idea that all bidders are ex-ante identical. The paper finds empirical evidence that this assumption is violated in the municipal bond auction data.\n\n### Question\n\nThe paper provides evidence that the bidder symmetry assumption, which is crucial for the main theoretical results, is violated in the data (e.g., in-state bidders are more likely to win). How does the author empirically assess the potential impact of this asymmetry on the main findings?",
    "Options": {
      "A": "By adding bidder-specific fixed effects to the main bid regression to control for time-invariant heterogeneity.",
      "B": "By using the bidder's location (in-state vs. out-of-state) as an instrumental variable for their information quality.",
      "C": "By re-estimating the entire model on a restricted subsample of auctions that excludes likely sources of asymmetry.",
      "D": "By developing and estimating a more complex, asymmetric model of strategic bidding."
    },
    "Answer": "C",
    "pi_justification": "This item tests the student's understanding of the paper's research design and how it addresses threats to validity. It assesses knowledge of a specific methodological choice. The distractors are all plausible but incorrect empirical strategies. Distractor A (Fixed Effects) and B (Instrumental Variables) are common econometric techniques but were not used for this purpose. Distractor D describes a much more involved solution that the author did not pursue, making the actual choice seem pragmatic by comparison.",
    "qid": "160",
    "model_evaluation_result": {
      "model_raw_response": "B) By-using-the-bidder's-location-(in-state-vs.-out-of-state)-as-an-instrumental-variable-for-their-information-quality.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 161,
    "Question": "### Background\n\n**Research Question.** This problem asks you to quantify the differential impact of the COVID-19 shock on the planned quantities and prices of credit-constrained versus unconstrained firms, based on the paper's core empirical model.\n\n**Setting / Institutional Environment.** The analysis uses survey data on expectations from Italian firms, collected immediately before (`t-1`) and during (`t`) the first COVID-19 lockdown. The identification strategy treats the pandemic as an exogenous shock that causes firms to revise their plans.\n\n**Variables & Parameters.**\n*   `\\mathbb{F}_{i,t}(y)`: Firm `i`'s post-outbreak expectation for the 12-month growth rate of outcome `y`.\n*   `CC_{i,t-1}`: An indicator for whether firm `i` was credit constrained pre-outbreak (1=yes).\n*   Outcomes `y`: `Sal^g1Y` (sales growth), `Emp^g1Y` (employment growth), `P^g` (price growth). All are in percentage points.\n\n---\n\n### Data / Model Specification\n\nThe baseline model is:\n```latex\n\\mathbb{F}_{i,t}(y) = \\alpha + \\beta CC_{i,t-1} + \\delta \\mathbb{F}_{i,t-1}(y) + \\gamma^{\\top}x_{i,t-1} + \\lambda_{s} + \\lambda_{p} + \\varepsilon_{i,t}\n```\nwhere `x` is a vector of controls.\n\nSelected results from the paper's baseline estimation are presented below.\n\n**Table 1: The Effect of Credit Constraints on Firms' Expectations**\n| | (1) `\\mathbb{F}_{i,t}(Sal^g1Y)` | (2) `\\mathbb{F}_{i,t}(Emp^g1Y)` | (3) `\\mathbb{F}_{i,t}(P^g)` |\n|:---|:---:|:---:|:---:|\n| **Credit constrained** (`CC`) | -10.62*** | -10.25*** | 3.754*** |\n| | [2.111] | [2.515] | [0.949] |\n| **Essential** | 11.22*** | 4.577** | -2.842** |\n| | [1.218] | [2.190] | [1.286] |\n\n*Notes: Dependent variables are post-COVID expectations for 12-month growth rates. Standard errors are in brackets. *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nBased on the results in Table 1, which of the following statements correctly interprets the effect of being credit constrained?",
    "Options": {
      "A": "A credit-constrained firm plans to increase its prices by 3.75 percentage points more over the next 12 months than a comparable unconstrained firm.",
      "B": "A credit-constrained firm plans to decrease its prices by 3.75 percentage points more over the next 12 months than a comparable unconstrained firm.",
      "C": "A credit-constrained firm plans to decrease its prices by 2.84 percentage points more over the next 12 months than a comparable unconstrained firm.",
      "D": "A credit-constrained firm expects its sales to grow by 3.75 percentage points more over the next 12 months than a comparable unconstrained firm."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the foundational skill of interpreting a regression coefficient from a table. It replaces the repetitive interpretation part of the original QA problem. The distractors are designed to catch common errors: (B) is a sign error, (C) confuses the coefficient of interest with another one (`Essential`), and (D) correctly identifies the coefficient's value but misattributes it to the wrong outcome variable (sales instead of prices).",
    "qid": "161",
    "model_evaluation_result": {
      "model_raw_response": "C) Acredit-constrained firmplans to decrease itspricesby2.84percentagepointsmoreoverthenext12monthsthanacomparableunconstrainedfirm.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 162,
    "Question": "### Background\n\n**Research Question.** This problem asks you to quantify the differential impact of the COVID-19 shock on the planned quantities and prices of credit-constrained versus unconstrained firms, based on the paper's core empirical model.\n\n**Setting / Institutional Environment.** The analysis uses survey data on expectations from Italian firms, collected immediately before (`t-1`) and during (`t`) the first COVID-19 lockdown. The identification strategy treats the pandemic as an exogenous shock that causes firms to revise their plans.\n\n**Variables & Parameters.**\n*   `\\mathbb{F}_{i,t}(y)`: Firm `i`'s post-outbreak expectation for the 12-month growth rate of outcome `y`.\n*   `CC_{i,t-1}`: An indicator for whether firm `i` was credit constrained pre-outbreak (1=yes).\n*   `Essential_i`: An indicator for whether firm `i` was classified as essential and allowed to operate during lockdown (1=yes).\n*   Outcomes `y`: `Sal^g1Y` (sales growth), `Emp^g1Y` (employment growth), `P^g` (price growth). All are in percentage points.\n\n---\n\n### Data / Model Specification\n\nThe baseline model is:\n```latex\n\\mathbb{F}_{i,t}(y) = \\alpha + \\beta CC_{i,t-1} + \\delta \\mathbb{F}_{i,t-1}(y) + \\gamma^{\\top}x_{i,t-1} + \\lambda_{s} + \\lambda_{p} + \\varepsilon_{i,t}\n```\nwhere `x` is a vector of controls including `Essential`.\n\nSelected results from the paper's baseline estimation are presented below.\n\n**Table 1: The Effect of Credit Constraints on Firms' Expectations**\n| | (1) `\\mathbb{F}_{i,t}(Sal^g1Y)` | (2) `\\mathbb{F}_{i,t}(Emp^g1Y)` | (3) `\\mathbb{F}_{i,t}(P^g)` |\n|:---|:---:|:---:|:---:|\n| **Credit constrained** (`CC`) | -10.62*** | -10.25*** | 3.754*** |\n| | [2.111] | [2.515] | [0.949] |\n| **Essential** | 11.22*** | 4.577** | -2.842** |\n| | [1.218] | [2.190] | [1.286] |\n\n*Notes: Dependent variables are post-COVID expectations for 12-month growth rates. Standard errors are in brackets. *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nConsider a firm that is initially non-essential (`Essential=0`) and credit-constrained (`CC=1`). Using the coefficients from Table 1, which of two mutually exclusive policy scenarios is predicted to have a larger positive impact on its 12-month sales growth expectation (`\\mathbb{F}_{i,t}(Sal^g1Y)`)?",
    "Options": {
      "A": "Reclassifying the firm as 'essential' is more effective, improving expected sales growth by 11.22 pp, compared to 10.62 pp from resolving its credit constraint.",
      "B": "Resolving the firm's credit constraint is more effective, improving expected sales growth by 10.62 pp, compared to a negative impact from being reclassified as 'essential'.",
      "C": "Reclassifying the firm as 'essential' is more effective, improving expected sales growth by 11.22 pp, compared to 10.25 pp from resolving its credit constraint.",
      "D": "Both policies have a negative impact on the firm's expected sales growth."
    },
    "Answer": "A",
    "pi_justification": "This item uses the Computational Judgment strategy to replace the calculation part of the original QA problem. It requires students to correctly identify the relevant coefficients, calculate the impact of two separate counterfactual changes, and compare the results. The distractors are designed based on predictable errors: (B) makes a comparison error and a sign error for the 'essential' effect, (C) uses the wrong coefficient (for employment) in the comparison, and (D) makes a sign error for both effects.",
    "qid": "162",
    "model_evaluation_result": {
      "model_raw_response": "C) Recclassifying the firm's status as 'essential', improving expected sales by 11,22 pp, is more effective than resolving its credit constraints, improving expected growth by 9.85 pp (10.62 - 0.77).",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 163,
    "Question": "### Background\n\nIn Expected Uncertain Utility (EUU) theory, an agent's preferences `≿` over acts `f` (functions from a state space `Ω` to prizes) are represented by a functional `W(f)`. The theory distinguishes between \"ideal\" events, over which uncertainty is quantifiable via a prior `μ`, and non-ideal events.\n\nThe utility of any act `f` is determined by its **envelope**, `[f] = ([f]₁, [f]₂)`. The envelope is the tightest pair of **ideal acts** that bound `f` from below and above. The utility is then calculated as:\n\n```latex\nW(f) = \\int u([f]_1, [f]_2) d\\mu\n```\n\nwhere `u` is the agent's interval utility.\n\n### Data / Model Specification\n\nConsider a state space `Ω` which is the unit square, `[0,1] × [0,1]`. \n-   **Ideal events** are defined as all vertical rectangles of the form `[a,b] × [0,1]`. An ideal act is any act measurable with respect to these events (i.e., an act that is constant on any vertical line).\n-   The prior `μ` on ideal events is the standard length measure, e.g., `μ([a,b] × [0,1]) = b-a`.\n\nConsider a non-ideal act `f` that pays prize `y` on the event `A = {(ω₁, ω₂) | ω₂ > 0.5}` (the top half of the square) and prize `x` on its complement `A^c` (the bottom half), where `y > x`.\n\n### Question\n\nWhat is the utility `W(f)` of this act?",
    "Options": {
      "A": "`0.5 * u(y,y) + 0.5 * u(x,x)`",
      "B": "`u(x,y)`",
      "C": "`∫ u(f(ω), f(ω)) dμ`",
      "D": "The utility is undefined because the act `f` is not ideal."
    },
    "Answer": "B",
    "pi_justification": "This question assesses the core technical concept of the 'envelope'. Strategy: Scenario Application. The correct answer requires realizing that for any ideal event (a vertical strip), the act `f` takes on both `x` and `y` values. Therefore, the tightest ideal lower bound `[f]₁` is the constant act `x`, and the tightest ideal upper bound `[f]₂` is the constant act `y`. The utility is thus `∫ u(x,y) dμ = u(x,y)`. Distractor A represents the incorrect Subjective Expected Utility calculation, treating the non-ideal event `A` as if it were ideal. Distractor C incorrectly equates the envelope with the act itself. Distractor D targets the misconception that utility is only defined for ideal acts.",
    "qid": "163",
    "model_evaluation_result": {
      "model_raw_response": "C) ∫ u(fω), fω) dμ",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 164,
    "Question": "### Background\n\nExpected Uncertain Utility (EUU) theory is built on a set of axioms governing preferences over acts. A key distinction is made between **ideal events**, which have quantifiable probabilities, and **diffuse events**, which represent purely unquantifiable uncertainty.\n\n### Data / Model Specification\n\nAn agent is observed to have the following preference pattern:\n-   Let `D₁` and `D₂` be two disjoint, non-null diffuse events.\n-   **Bet A:** Pays $100 if an outcome in `D₁` occurs, $0 otherwise.\n-   **Bet B:** Pays $100 if an outcome in `D₁ ∪ D₂` occurs, $0 otherwise.\n-   The agent is indifferent between Bet A and Bet B (`Bet A ~ Bet B`).\n\nThis indifference violates strict state-wise dominance, as Bet B offers the same prizes as Bet A on `D₁` and a better prize on `D₂`.\n\n### Question\n\nThis specific pattern of indifference is a direct consequence of which core axiom of EUU theory?",
    "Options": {
      "A": "Axiom 2 (Dominance): If an act `f` pays strictly more than an act `g` in every state, then `f` is strictly preferred to `g`.",
      "B": "Axiom 3 (Symmetry for diffuse events): For any prizes `x,y`, ideal event `E`, and diffuse events `D, D'`, the agent is indifferent between a bet on `E∩D` and a bet on `E∩D'`.",
      "C": "Axiom 4 (Comparative Probability): The ranking of bets on ideal events is independent of the prizes used, establishing a unique probability measure `μ`.",
      "D": "Axiom 6 (Continuity): If a sequence of acts `f_n` converges to `f`, and `g ≿ f_n ≿ h` for all `n`, then `g ≿ f ≿ h`."
    },
    "Answer": "B",
    "pi_justification": "This question assesses the understanding of the axiomatic foundations of EUU. Strategy: Reverse-Reasoning. It asks to identify the axiom that explains a specific, counter-intuitive behavioral pattern. The correct answer is Axiom 3, which forces all diffuse (purely unquantifiable) events to be treated symmetrically, leading to the observed indifference. Distractor A is the principle that is being violated. Distractors C and D are other important axioms in the system, but they are not the direct cause of this specific indifference pattern over diffuse sets.",
    "qid": "164",
    "model_evaluation_result": {
      "model_raw_response": "B) A xiom 3(Symmetry for diffuse(events): For any(prizes)x,y, ideal(event)E, and diffuse(events)D,D', the agent is(indifferent between a bet(on)E∩and a bet(on(E∩D'))",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 165,
    "Question": "### Background\n\n**Research Question.** This problem explores the derivation and interpretation of the core structural model of the coffee market, focusing on its long-run equilibrium.\n\n**Setting / Institutional Environment.** The model represents the international coffee market with three core components: market clearing for green coffee beans, a biological growth process for coffee trees with a one-year depreciation cycle, and roaster demand.\n\n**Variables & Parameters.**\n- `p^W`: Steady-state international wholesale price of coffee.\n- `Y_t`: Quantity of green coffee beans supplied at time `t`.\n- `Q_t`: Quantity demanded by roasters at time `t`.\n- `\\Delta I_t`: Change in inventory holdings at time `t`.\n- `N_t`: Number of coffee trees at time `t`.\n- `b`: Steady-state average yield per tree.\n- `\\varphi`: Steady-state number of new trees entering production.\n- `\\beta`: Annual survival rate of tree stocks (`0 < \\beta < 1`).\n- `a`: Steady-state demand intercept.\n- `c`: Slope of the inverse demand curve (positive constant).\n- Unit of observation: Time `t` is measured in months.\n\n---\n\n### Data / Model Specification\n\nThe structural model is defined by the following equations:\n\n```latex\nY_{t}=Q_{t}+\\Delta I_{t} \\quad \\text{(Eq. (1))}\n```\n\n```latex\nY_{t}=b_{t}N_{t} \\quad \\text{(Eq. (2))}\n```\n\n```latex\nN_{t}=\\beta N_{t-12}+\\varphi_{t} \\quad \\text{(Eq. (3))}\n```\n\n```latex\np_{t}^{\\mathrm{W}}=a_{t}–c Q_{t} \\quad \\text{(Eq. (4))}\n```\n\n---\n\n### Question\n\nBy imposing steady-state conditions on the model (i.e., `p_t^W = p^W`, `a_t = a`, `b_t = b`, `\\varphi_t = \\varphi`, `N_t = N`, and `\\Delta I_t = 0`), what is the correct expression for the steady-state wholesale price `p^W`?",
    "Options": {
      "A": "```latex\np^W = a - \\frac{c b \\varphi}{1 - \\beta}\n```",
      "B": "```latex\np^W = a - c b \\varphi (1 - \\beta)\n```",
      "C": "```latex\np^W = a - c b \\varphi\n```",
      "D": "```latex\np^W = a + \\frac{c b \\varphi}{1 - \\beta}\n```"
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the ability to solve the structural model for its long-run equilibrium, a foundational skill in dynamic modeling. Strategy: Computational Judgment. The user must derive the steady-state tree stock `N = \\varphi / (1 - \\beta)` and substitute it into the steady-state demand curve `p^W = a - c b N`. Distractor Logic: (B) Formula Misuse: incorrectly multiplies by `(1 - \\beta)` instead of dividing. (C) Step-Omission Error: omits the `1 / (1 - \\beta)` term, effectively assuming a 100% depreciation rate. (D) Sign Error: incorrectly signs the supply term, violating basic supply/demand logic.",
    "qid": "165",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: I've formatted the options using LaTeX code for clarity.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 166,
    "Question": "### Background\n\n**Research Question.** This problem explores the interpretation of the core structural model of the coffee market, focusing on its long-run equilibrium.\n\n**Setting / Institutional Environment.** The model represents the international coffee market with three core components: market clearing for green coffee beans, a biological growth process for coffee trees with a one-year depreciation cycle, and roaster demand.\n\n**Variables & Parameters.**\n- `p^W`: Steady-state international wholesale price of coffee.\n- `Y_t`: Quantity of green coffee beans supplied at time `t`.\n- `Q_t`: Quantity demanded by roasters at time `t`.\n- `\\Delta I_t`: Change in inventory holdings at time `t`.\n- `N_t`: Number of coffee trees at time `t`.\n- `b`: Steady-state average yield per tree.\n- `\\varphi`: Steady-state number of new trees entering production.\n- `\\beta`: Annual survival rate of tree stocks (`0 < \\beta < 1`).\n- `a`: Steady-state demand intercept.\n- `c`: Slope of the inverse demand curve (positive constant).\n- Unit of observation: Time `t` is measured in months.\n\n---\n\n### Data / Model Specification\n\nThe structural model is defined by the following equations:\n\n```latex\nY_{t}=Q_{t}+\\Delta I_{t} \\quad \\text{(Eq. (1))}\n```\n\n```latex\nY_{t}=b_{t}N_{t} \\quad \\text{(Eq. (2))}\n```\n\n```latex\nN_{t}=\\beta N_{t-12}+\\varphi_{t} \\quad \\text{(Eq. (3))}\n```\n\n```latex\np_{t}^{\\mathrm{W}}=a_{t}–c Q_{t} \\quad \\text{(Eq. (4))}\n```\n\n---\n\n### Question\n\nAccording to the model's steady-state equilibrium, how does an increase in the tree survival rate (`\\beta`) affect the long-run equilibrium price `p^W`, holding all other parameters constant?",
    "Options": {
      "A": "It decreases the price by increasing the long-run stock of productive trees.",
      "B": "It increases the price because a higher survival rate implies healthier, more valuable trees.",
      "C": "It has no effect on the price, as it only alters the depreciation rate, not new plantings.",
      "D": "The effect is ambiguous because it depends on the level of new plantings `\\varphi`."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the ability to perform and interpret comparative statics on the model's long-run equilibrium. Strategy: Computational Judgment. The user must understand that a higher `\\beta` increases the steady-state tree stock `N = \\varphi / (1 - \\beta)`, which in turn increases supply and lowers the price. Distractor Logic: (B) Conceptual Opposite: confuses the health of an individual tree with the aggregate market price effect of having more trees. (C) True but Irrelevant / Step-Omission Error: correctly notes that `\\beta` is not `\\varphi`, but fails to see the link between `\\beta` and the total stock `N`. (D) Conceptual Error: incorrectly suggests ambiguity where the model provides a clear, negative relationship.",
    "qid": "166",
    "model_evaluation_result": {
      "model_raw_response": "C) It is correct that the effect of an increase in `\\beta` on `p^w` is neutral, since it only affects the depreciation rate and not new plantations.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 167,
    "Question": "### Background\n\n**Research Question.** This problem deconstructs the econometric methodology used to classify firms as 'strategic' or 'confused' based on their VAT reporting behavior. The core challenge is to attribute responsibility for a reporting discrepancy observed between a buyer and a seller to each firm's intrinsic reporting tendency.\n\n**Setting and Sample.** The analysis uses monthly, transaction-level VAT data for firm-pairs in Uganda. The methodology, inspired by Abowd et al. (1999), is applied to the largest \"connected set\" of trading firms, where firms are linked directly or indirectly through their trading partners.\n\n---\n\n### Data / Model Specification\n\nThe methodology proceeds in two main steps:\n\n1.  **Define and Decompose Discrepancy:** The discrepancy between a buyer `f` and seller `f'` in month `t` is defined as the difference between the VAT reported by the buyer and the seller:\n    ```latex\n    d_{f f't} \\equiv Y_{f f't}^{B} - Y_{f f't}^{S} \\quad \\text{(Eq. (1))}\n    ```\n    A positive `d` implies a seller shortfall (liability-reducing), while a negative `d` implies a buyer shortfall (liability-increasing). This discrepancy is then decomposed using a two-way fixed-effects model:\n    ```latex\n    d_{f f't} = \\delta_{c} + \\delta_{f}^{b} + \\delta_{f'}^{s} + \\delta_{t} + r_{f f't} \\quad \\text{(Eq. (2))}\n    ```\n    where `\\delta_f^b` is a buyer fixed effect for firm `f`, `\\delta_{f'}^s` is a seller fixed effect for firm `f'`, and `\\delta_t` is a month fixed effect.\n\n2.  **Construct Firm-Level Classification Statistic:** The estimated fixed effects for a given firm `f` are aggregated into a single statistic, `Q_f`, which summarizes its overall misreporting behavior:\n    ```latex\n    Q_{f} \\equiv w_{s} \\cdot \\hat{\\delta}_{f}^{s} + w_{b} \\cdot \\hat{\\delta}_{f}^{b} \\quad \\text{(Eq. (3))}\n    ```\n    where `w_s` and `w_b` are the number of observations for firm `f` as a seller and buyer, respectively. A firm is classified as 'advantageous' if `Q_f > 0` and 'disadvantageous' if `Q_f < 0`.\n\n---\n\nBased on the model specification, what is the correct interpretation of an estimated buyer fixed effect `\\hat{\\delta}_f^b` that is significantly positive for a firm `f`?",
    "Options": {
      "A": "The firm systematically over-reports its purchases, which reduces its net VAT liability.",
      "B": "The firm systematically under-reports its purchases, which increases its net VAT liability.",
      "C": "The firm systematically under-reports its sales, which reduces its net VAT liability.",
      "D": "The firm is classified as a 'disadvantageous' misreporter because it contributes positively to discrepancies."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core interpretation of the model's key parameters. It uses a Computational Judgment strategy. The correct answer requires understanding how the discrepancy is defined and how input VAT credits affect liability. Distractors are designed based on common errors: (B) is a sign error/conceptual opposite; (C) confuses the buyer and seller fixed effects; (D) incorrectly links a single positive fixed effect to the final 'disadvantageous' classification, which depends on the weighted sum `Q_f`.",
    "qid": "167",
    "model_evaluation_result": {
      "model_raw_response": "B) The firmsystematically under-reportsits purchases, whichincreases its net VATliability.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 168,
    "Question": "### Background\n\n**Research Question.** This problem deconstructs the econometric methodology used to classify firms as 'strategic' or 'confused' based on their VAT reporting behavior. The core challenge is to attribute responsibility for a reporting discrepancy observed between a buyer and a seller to each firm's intrinsic reporting tendency.\n\n**Setting and Sample.** The analysis uses monthly, transaction-level VAT data for firm-pairs in Uganda. The methodology, inspired by Abowd et al. (1999), is applied to the largest \"connected set\" of trading firms, where firms are linked directly or indirectly through their trading partners.\n\n---\n\n### Data / Model Specification\n\nThe methodology proceeds in two main steps:\n\n1.  **Define and Decompose Discrepancy:** The discrepancy between a buyer `f` and seller `f'` in month `t` is defined as the difference between the VAT reported by the buyer and the seller:\n    ```latex\n    d_{f f't} \\equiv Y_{f f't}^{B} - Y_{f f't}^{S} \\quad \\text{(Eq. (1))}\n    ```\n    A positive `d` implies a seller shortfall (liability-reducing), while a negative `d` implies a buyer shortfall (liability-increasing). This discrepancy is then decomposed using a two-way fixed-effects model:\n    ```latex\n    d_{f f't} = \\delta_{c} + \\delta_{f}^{b} + \\delta_{f'}^{s} + \\delta_{t} + r_{f f't} \\quad \\text{(Eq. (2))}\n    ```\n    where `\\delta_f^b` is a buyer fixed effect for firm `f`, `\\delta_{f'}^s` is a seller fixed effect for firm `f'`, and `\\delta_t` is a month fixed effect.\n\n2.  **Construct Firm-Level Classification Statistic:** The estimated fixed effects for a given firm `f` are aggregated into a single statistic, `Q_f`, which summarizes its overall misreporting behavior:\n    ```latex\n    Q_{f} \\equiv w_{s} \\cdot \\hat{\\delta}_{f}^{s} + w_{b} \\cdot \\hat{\\delta}_{f}^{b} \\quad \\text{(Eq. (3))}\n    ```\n    where `w_s` and `w_b` are the number of observations for firm `f` as a seller and buyer, respectively. A firm is classified as 'advantageous' if `Q_f > 0` and 'disadvantageous' if `Q_f < 0`.\n\n---\n\nThe paper notes that the individual fixed effects `\\delta_f^b` and `\\delta_{f'}^s` can only be estimated for firms within a 'connected set.' How does observing a firm `f` buying from two different sellers, `f'` and `f''`, resolve the identification problem that exists if `f` only bought from `f'`?",
    "Options": {
      "A": "It allows the model to estimate the relative reporting tendency of seller `f'` compared to seller `f''`.",
      "B": "It doubles the number of observations for firm `f`, which solves the problem by increasing statistical power.",
      "C": "It proves that the error term `r_{ff't}` is not systematically correlated with the fixed effects.",
      "D": "It ensures that the buyer fixed effect `\\delta_f^b` is not perfectly collinear with the month fixed effect `\\delta_t`."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the identification strategy for two-way fixed effects models in a network context, using a Reverse-Reasoning approach. The correct answer identifies the key insight that identification comes from comparing a firm's transactions with multiple partners. Distractors target common statistical misunderstandings: (B) confuses identification with precision/power; (C) brings in an unrelated assumption (exogeneity); (D) misidentifies the source of the collinearity.",
    "qid": "168",
    "model_evaluation_result": {
      "model_raw_response": "B) It doubled the number of observation for firm `F`, which solves problem by increasing the statistical power.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 169,
    "Question": "### Background\n\n**Research Question.** This problem deconstructs the econometric methodology used to classify firms as 'strategic' or 'confused' based on their VAT reporting behavior. The core challenge is to attribute responsibility for a reporting discrepancy observed between a buyer and a seller to each firm's intrinsic reporting tendency.\n\n**Setting and Sample.** The analysis uses monthly, transaction-level VAT data for firm-pairs in Uganda. The methodology, inspired by Abowd et al. (1999), is applied to the largest \"connected set\" of trading firms, where firms are linked directly or indirectly through their trading partners.\n\n---\n\n### Data / Model Specification\n\nThe methodology proceeds in two main steps:\n\n1.  **Define and Decompose Discrepancy:** The discrepancy between a buyer `f` and seller `f'` in month `t` is defined as the difference between the VAT reported by the buyer and the seller:\n    ```latex\n    d_{f f't} \\equiv Y_{f f't}^{B} - Y_{f f't}^{S} \\quad \\text{(Eq. (1))}\n    ```\n    A positive `d` implies a seller shortfall (liability-reducing), while a negative `d` implies a buyer shortfall (liability-increasing). This discrepancy is then decomposed using a two-way fixed-effects model:\n    ```latex\n    d_{f f't} = \\delta_{c} + \\delta_{f}^{b} + \\delta_{f'}^{s} + \\delta_{t} + r_{f f't} \\quad \\text{(Eq. (2))}\n    ```\n    where `\\delta_f^b` is a buyer fixed effect for firm `f`, `\\delta_{f'}^s` is a seller fixed effect for firm `f'`, and `\\delta_t` is a month fixed effect.\n\n2.  **Construct Firm-Level Classification Statistic:** The estimated fixed effects for a given firm `f` are aggregated into a single statistic, `Q_f`, which summarizes its overall misreporting behavior:\n    ```latex\n    Q_{f} \\equiv w_{s} \\cdot \\hat{\\delta}_{f}^{s} + w_{b} \\cdot \\hat{\\delta}_{f}^{b} \\quad \\text{(Eq. (3))}\n    ```\n    where `w_s` and `w_b` are the number of observations for firm `f` as a seller and buyer, respectively. A firm is classified as 'advantageous' if `Q_f > 0` and 'disadvantageous' if `Q_f < 0`.\n\n---\n\nThe model's validity rests on the assumption that unobserved factors in a transaction are uncorrelated with the firm fixed effects. Suppose this assumption is violated because of 'assortative matching,' where advantageous misreporters (who tend to have positive `\\delta^b` and `\\delta^s`) systematically prefer to trade with each other. How would this likely bias the results?",
    "Options": {
      "A": "It would exaggerate the estimated dispersion of firm types, making advantageous firms appear more strategic than they truly are.",
      "B": "It would attenuate the estimated dispersion of firm types, biasing all fixed effects toward zero.",
      "C": "It would bias the estimates for advantageous firms but not for disadvantageous firms.",
      "D": "It would have no biasing effect on the fixed effects but would increase their standard errors."
    },
    "Answer": "A",
    "pi_justification": "This item assesses a sophisticated critique of the model's identifying assumption, using a Scenario Application strategy. The correct answer requires reasoning through the consequences of omitted variable bias in this specific context. Distractors represent plausible but incorrect conclusions: (B) is the opposite effect (attenuation bias); (C) incorrectly assumes the bias is isolated; (D) confuses the effect of sorting bias with the effect of non-constant error variance.",
    "qid": "169",
    "model_evaluation_result": {
      "model_raw_response": "C) It is likely to bias the results by making advantageous firms (those with positive Q_f) appear even more advantageous than they truly were, and disadvantageous firms (those wi",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 170,
    "Question": "### Background\n\n**Research Question.** This problem investigates the conditions required for the nonparametric identification of the distribution of preference heterogeneity, `F_0`, in a random coefficients binary choice model.\n\n**Setting / Institutional Environment.** Without sufficient restrictions, the distribution `F_0` is not identified, meaning multiple different distributions of `β_i` can produce the same observable choice probabilities `p(x, F) = Pr(y=1|x)`. For example, a uniform distribution on the unit circle and a distribution uniform only on the first and third quadrants can both yield `p(x,F) = 1/2` for all `x`.\n\n### Data / Model Specification\n\nTo solve the non-identification problem, Theorem 1 introduces a key assumption:\n\n*   **Assumption (iii):** There exists a vector `c` on the unit hypersphere `B` such that `Pr{c'β_i > 0} = 1`.\n\nThis assumption implies that the entire support of the distribution `F_0` is contained in an open half-hypersphere.\n\n### Question\n\nWhat is the primary mechanism by which Assumption (iii) restores identification of the distribution `F_0`?",
    "Options": {
      "A": "It ensures the log-likelihood function is strictly concave, which guarantees a unique maximum for the estimator.",
      "B": "It prevents the construction of an alternative, non-negative density function that produces the same choice probabilities.",
      "C": "It requires the regressors `x_i` to have a continuous distribution with support over the entire space.",
      "D": "It normalizes the scale of the `β_i` coefficients, which is necessary for any binary choice model."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the core intuition behind the paper's main identification strategy. It uses a Reverse-Reasoning approach, asking for the mechanism behind a stated result. The distractors are designed to probe for common confusions by mixing up the identification assumption with other distinct concepts from the paper: optimization properties (A: concavity), regressor assumptions (C), and scale normalization (D).",
    "qid": "170",
    "model_evaluation_result": {
      "model_raw_response": "B) It Prevents the construction of An Alternative, Non-Negative Density Function That Produces the Same Choice Probabilities.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 171,
    "Question": "### Background\n\n**Research Question.** This problem investigates the formal proof of identification for the distribution of preference heterogeneity, `F_0`.\n\n**Setting / Institutional Environment.** The proof of Theorem 1 uses the Cramér-Wold device. It involves an orthonormal transformation `θ_i = Aβ_i`, where the first row of matrix `A` is `c'`. This construction, combined with the key identification assumption, implies `Pr{θ_{i1} > 0} = 1`.\n\n### Data / Model Specification\n\nThe model is `y_i = 1\\{x_i'\\beta_i \\ge 0\\}`. The proof of identification establishes that the distribution of the ratio `θ_{i2}/θ_{i1}` is identified, where `θ_{i2}` is the `(K-1)x1` vector of the remaining elements of `θ_i`.\n\n### Question\n\nIn the formal proof of identification, what is the most critical role of the condition `Pr{θ_{i1} > 0} = 1`?",
    "Options": {
      "A": "It ensures that the denominator `θ_{i1}` is not zero, so the ratio `θ_{i2}/θ_{i1}` is well-defined.",
      "B": "It allows the distributions of `θ_{i1}` and `θ_{i2}` to be identified separately before combining them.",
      "C": "It guarantees that the transformation matrix `A` is non-singular.",
      "D": "It ensures a unique one-to-one mapping from the identified distribution of the ratio `θ_{i2}/θ_{i1}` back to the distribution of the vector `θ_i`."
    },
    "Answer": "D",
    "pi_justification": "This item tests a deeper understanding of the formal proof mechanics. It uses a Scenario Application strategy, focusing on a specific step in the proof. The primary distractor (A) is an 'Almost Right' option that states a necessary but insufficient consequence of the assumption; the key insight is not just that the ratio is well-defined, but that the known sign of the denominator allows for a unique mapping back to the original vector, which is what option (D) correctly identifies. This discriminates between superficial and deep comprehension.",
    "qid": "171",
    "model_evaluation_result": {
      "model_raw_response": "D) It ensur...e a unique one-on-one mapping from...",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 172,
    "Question": "### Background\n\n**Research Question.** This problem explores the fundamental strategic trade-off in a repeated game with imperfect monitoring.\n\n**Setting / Institutional Environment.** A long-run Player 1 (LR) interacts with a sequence of short-run Player 2s. In each period, Player 2 decides whether to enter. If they enter, Player 1 chooses to cooperate (`+1`) or deviate (`-1`).\n\n### Data / Model Specification\n\nThe stage-game payoffs are given in Table 1.\n\n**Table 1: Stage-Game Payoffs (Player 1, Player 2)**\n\n| Player 1 | Player 2: Out | Player 2: In |\n|:---|:---:|:---:|\n| **+1** | `u`, 0 | `ū`, 1 |\n| **-1** | `u`, 0 | `ū + g`, -1 |\n\nwhere `ū > u` and `g > 0`.\n\n### Question\n\nIn the one-shot stage game described in Table 1, what is the unique Nash Equilibrium outcome and the key strategic reason for it?",
    "Options": {
      "A": "(Player 1: +1, Player 2: In); this is the Pareto-optimal outcome.",
      "B": "(Player 1: -1, Player 2: Out); Player 1's dominant strategy is -1.",
      "C": "(Player 1: -1, Player 2: Out); Player 2 anticipates that if they play In, Player 1 has an incentive to play -1.",
      "D": "(Player 1: +1, Player 2: Out); Player 2's dominant strategy is Out."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the foundational understanding of the stage game's strategic tension. It uses a Reverse-Reasoning strategy, asking the student to identify the equilibrium by reasoning backward from Player 1's incentives. Distractors are designed to catch common errors in game theory analysis: confusing Pareto optimality with equilibrium (A), misidentifying dominant strategies (B, D), and failing to use backward induction logic.",
    "qid": "172",
    "model_evaluation_result": {
      "model_raw_response": "C) Player 2 anticipate that if they Play In, Player will play -1.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 173,
    "Question": "### Background\n\n**Research Question.** This problem explores the economic interpretation of the core incentive compatibility (IC) constraint that governs whether cooperation can be sustained in a repeated game with imperfect monitoring.\n\n**Setting / Institutional Environment.** A long-run Player 1 (LR) interacts with a sequence of short-run Player 2s. Incentives are provided by a punishment mechanism where, upon observing certain signals, the LR player's continuation value reverts from the best equilibrium payoff `v*` to their minmax payoff, `u`.\n\n### Data / Model Specification\n\nThe incentive compatibility constraint can be rearranged to show that the immediate gain from deviation, `g`, must be less than or equal to the discounted expected future loss from punishment:\n\n```latex\ng \\leq \\frac{\\delta(q-p)}{1-\\delta(1-p)}(\\bar{u}-{u}) \\quad \\text{(Eq. (1))}\n```\n\nHere, `δ` is the discount factor, `p` is the on-path punishment probability, `q` is the off-path punishment probability, and `(ū-u)` is the per-period loss in payoff during punishment.\n\n### Question\n\nIn the expression for the discounted expected future loss in Eq. (1), which component represents the 'information content' of the signal, i.e., the marginal increase in the probability of punishment from deviating?",
    "Options": {
      "A": "`(\\bar{u}-{u})`",
      "B": "`\\delta / (1-\\delta(1-p))`",
      "C": "`(q-p)`",
      "D": "`g`"
    },
    "Answer": "C",
    "pi_justification": "This item tests the economic interpretation of the paper's central incentive constraint. It uses a Conceptual Judgment strategy, asking for the identification of a specific economic concept within a complex formula. The distractors are other components of the same formula, each with a distinct economic meaning (capital loss, effective discount factor), making them highly plausible but incorrect (Almost Right). This forces a precise mapping between mathematical terms and economic concepts.",
    "qid": "173",
    "model_evaluation_result": {
      "model_raw_response": "C) `(qi-p)` `i` is the information content of the signal.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 174,
    "Question": "### Background\n\n**Research Question.** This problem tests a key algebraic step in deriving the incentive compatibility (IC) constraint from the underlying value equations in a repeated game with imperfect monitoring.\n\n**Setting / Institutional Environment.** A long-run Player 1 (LR) interacts with a sequence of short-run Player 2s. Incentives are provided by a punishment mechanism where the LR player's continuation value `w(z)` can be either the best equilibrium payoff `v*` (reward) or the minmax payoff `u` (punishment).\n\n### Data / Model Specification\n\nThe on-path payoff `v*` must satisfy the promise-keeping constraint:\n\n```latex\nv^* = (1-\\delta)\\bar{u} + \\delta E[w(z)|+1] \\quad \\text{(Eq. (1))}\n```\n\nLet `p` be the probability of punishment after the cooperative action `+1`. The expected continuation payoff is therefore `E[w(z)|+1] = (1-p)v* + pu`.\n\n### Question\n\nA key step in deriving the main incentive constraint is to solve the promise-keeping equation (Eq. (1)) for the 'capital loss' from punishment, `(v* - u)`. What is the correct expression for `(v* - u)`?",
    "Options": {
      "A": "`(1-\\delta)(\\bar{u}-{u}) / (1 - \\delta + \\delta p)`",
      "B": "`\\delta(q-p)(\\bar{u}-{u}) / (1-\\delta)`",
      "C": "`(1-\\delta)(\\bar{u}-{u}) / (1 - \\delta)`",
      "D": "`(1-\\delta)g / (\\delta(q-p))`"
    },
    "Answer": "A",
    "pi_justification": "This item assesses a critical step in the derivation of the main incentive constraint, testing algebraic manipulation without requiring the full open-ended derivation. It uses a Computational Judgment strategy. The distractors are designed based on common algebraic errors (Step-Omission Error in C) or by using terms from other related formulas to create confusion (Formula Misuse in B and D), ensuring that only a correct derivation yields the right answer.",
    "qid": "174",
    "model_evaluation_result": {
      "model_raw_response": "B) `\\δ(q-p)(¯u-{u}) / 1-δ`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 175,
    "Question": "## Background\n\n**Research Question.** This problem critically examines the widespread practice of first-differencing time series data to induce stationarity, highlighting the dangers of \"over-differencing.\"\n\n**Setting.** An analyst is modeling a relationship between two time series variables, \\(y_t\\) and \\(x_t\\). The true underlying relationship is a simple static model in levels, but the analyst, fearing a spurious regression, mechanically first-differences the data before estimation.\n\n---\n\n## Data / Model Specification\n\nThe true data generating process is the static levels model:\n```latex\ny_{t}=\\gamma_{0}x_{t}+\\epsilon_{t} \\quad \\text{(Eq. (1))}\n```\nwhere \\(\\epsilon_t\\) is a white noise error term with variance \\(\\sigma^2\\).\n\nThe analyst estimates the first-difference model:\n```latex\n\\Delta y_{t}=\\gamma_{0}\\Delta x_{t}+\\omega_{t} \\quad \\text{(Eq. (2))}\n```\n\n---\n\n## Question\n\nGiven that the true model is Eq. (1), what are the statistical properties of the error term, \\(\\omega_t\\), in the analyst's estimated model, Eq. (2)?",
    "Options": {
      "A": "It will be a first-order moving average process with an autocorrelation coefficient of -0.5.",
      "B": "It will be a first-order autoregressive process with a root near 1.",
      "C": "It will remain white noise, as the original error term was white noise.",
      "D": "It will be a first-order moving average process with an autocorrelation coefficient of +0.5."
    },
    "Answer": "A",
    "pi_justification": "This item assesses a critical concept: the statistical consequences of 'over-differencing.' It uses a Scenario Application strategy. The distractors are designed to capture common errors: (B) confuses the problem with the solution, (C) ignores the transformation's effect on the error term, and (D) represents a common sign error in the calculation.",
    "qid": "175",
    "model_evaluation_result": {
      "model_raw_response": "C) It remains white noise, since the first-differenced error term is still the result of differencing the original white noise error.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 176,
    "Question": "## Background\n\n**Research Question.** This problem examines the robust method for testing whether a simple first-difference model is a valid simplification of a more general dynamic model, contrasting it with methodologically flawed approaches.\n\n**Setting.** An analyst begins with a general Autoregressive Distributed Lag (ADL) model and wants to determine if it can be simplified to a model in first differences. This simplification is only valid if specific, testable restrictions on the general model's parameters hold.\n\n---\n\n## Data / Model Specification\n\nConsider the following models:\n1.  **The General ADL(1,1) Model:**\n    ```latex\n    y_{t}=\\beta_{1}y_{t-1}+\\gamma_{0}x_{t}+\\gamma_{1}x_{t-1}+v_{t} \\quad \\text{(Eq. (1))}\n    ```\n2.  **The Error-Correction Model (ECM) Reparameterization of Eq. (1):**\n    ```latex\n    \\Delta y_{t}=\\gamma_{0}\\Delta x_{t}+(\\beta_{1}-1)y_{t-1}+(\\gamma_{0}+\\gamma_{1})x_{t-1}+v_{t} \\quad \\text{(Eq. (2))}\n    ```\n\n---\n\n## Question\n\nAn analyst wants to test if a simple first-difference model (i.e., \\(\\Delta y_{t}=\\gamma_{0}\\Delta x_{t}+v_{t}\\)) is a valid simplification of the general ADL(1,1) model given by Eq. (1). What is the most robust statistical procedure to conduct this test?",
    "Options": {
      "A": "Estimate the general model in its error-correction form (Eq. 2) and perform a joint test on the null hypothesis that the coefficients on the lagged level terms (\\(y_{t-1}\\) and \\(x_{t-1}\\)) are both zero.",
      "B": "Estimate the simple first-difference model and test if the Durbin-Watson statistic from its residuals is close to 2.",
      "C": "Estimate the general ADL(1,1) model (Eq. 1) and test the single hypothesis that the coefficient on \\(y_{t-1}\\) is equal to 1.",
      "D": "Estimate both the general model and the first-difference model and choose the one with the higher R-squared value."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's understanding of the 'general-to-specific' testing methodology, a central theme of the paper. It uses a Reverse-Reasoning strategy (what is the correct procedure for a given goal?). Distractors represent flawed or incomplete approaches: (B) is the specific-to-general fallacy critiqued in the paper, (C) is an incomplete test, and (D) represents a common but invalid method for model selection.",
    "qid": "176",
    "model_evaluation_result": {
      "model_raw_response": "B) Estimate simple first-diff model and test Durbin-Watsnon stat",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 177,
    "Question": "### Background\n\n**Research Question.** This problem assesses the quantitative importance of structured management practices in explaining firm-level productivity dispersion, relative to other well-established drivers like R&D, ICT, and human capital.\n\n**Setting / Institutional Environment.** The analysis is conducted at the firm level, using data on U.S. manufacturing firms. Plant-level data are aggregated to the firm level.\n\n**Variables & Parameters.**\n- **Dependent Variable**: Firm-level `log(labor productivity)`.\n- **Key Independent Variables**:\n    - `Management score`: Firm-level average management score.\n    - `R&D`: Log of (1 + R&D intensity).\n    - `ICT/worker`: ICT investment per worker.\n    - `Skills`: Share of employees with a college degree.\n\n---\n\n### Data / Model Specification\n\nThe table below presents results from OLS regressions of firm-level log labor productivity on each of the four key drivers.\n\n**Table 1: Drivers of Productivity Variation**\n| | (1) | (2) | (3) | (4) |\n| :--- | :---: | :---: | :---: | :---: |\n| **Management score** | 0.864 | | | |\n| | (0.043) | | | |\n| **R&D** | | 0.133 | | |\n| | | (0.010) | | |\n| **ICT/worker** | | | 0.062 | |\n| | | | (0.006) | |\n| **Skills** | | | | 0.800 |\n| | | | | (0.064) |\n| **Share of 90-10 explained** | 0.216 | 0.216 | 0.120 | 0.159 |\n\n*Notes: Columns (1)-(4) represent separate regressions. The dependent variable is firm-level log(labor productivity). 'Share of 90-10 explained' is the fraction of the 90th-10th percentile productivity gap accounted for by the 90th-10th percentile gap in the respective driver.*\n\n---\n\n### Question\n\nBased on the results in Table 1, which statement most accurately describes the quantitative importance of management practices in explaining firm-level productivity dispersion?",
    "Options": {
      "A": "The 90-10 spread in management practices accounts for a share of the productivity spread that is comparable to R&D and larger than the shares accounted for by ICT or skills.",
      "B": "Management practices account for a significantly smaller share of the productivity spread than all other measured factors, including ICT and skills.",
      "C": "Management practices are the single most important driver of productivity, accounting for a larger share of the spread than R&D, ICT, and skills combined.",
      "D": "The regression coefficient for Skills (0.800) is the largest of all individual factors, implying that skills are the most powerful driver of productivity."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to interpret the paper's key quantitative findings on the magnitude of management's impact. It uses a Computational Judgment/Interpretation strategy. The correct answer (A) accurately reflects the comparison of the 'Share of 90-10 explained' values. Distractors are designed to trap common errors: (B) is a conceptual opposite; (C) is an overstatement (Almost Right); (D) targets the classic confusion between a large regression coefficient and a variable's overall explanatory power (True but Irrelevant/Misleading).",
    "qid": "177",
    "model_evaluation_result": {
      "model_raw_response": "B) Management Practices account for a smaller share of productivity dispersion than all other factors.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 178,
    "Question": "### Background\n\n**Research Question.** This problem assesses the quantitative importance of structured management practices in explaining firm-level productivity dispersion, relative to other well-established drivers like R&D, ICT, and human capital.\n\n**Setting / Institutional Environment.** The analysis is conducted at the firm level, using data on U.S. manufacturing firms. Plant-level data are aggregated to the firm level.\n\n**Variables & Parameters.**\n- **Dependent Variable**: Firm-level `log(labor productivity)`.\n- **Key Independent Variables**:\n    - `Management score`: Firm-level average management score.\n    - `R&D`: Log of (1 + R&D intensity).\n    - `ICT/worker`: ICT investment per worker.\n    - `Skills`: Share of employees with a college degree.\n\n---\n\n### Data / Model Specification\n\nThe table below presents results from OLS regressions of firm-level log labor productivity on each of the four key drivers.\n\n**Table 1: Drivers of Productivity Variation**\n| | (1) | (2) | (3) | (4) | (5) |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| **Management score** | 0.864 | | | | 0.612 |\n| | (0.043) | | | | (0.043) |\n| **R&D** | | 0.133 | | | 0.095 |\n| | | (0.010) | | | (0.010) |\n| **ICT/worker** | | | 0.062 | | 0.047 |\n| | | | (0.006) | | (0.006) |\n| **Skills** | | | | 0.800 | 0.208 |\n| | | | | (0.064) | (0.060) |\n\n*Notes: Columns (1)-(4) represent separate regressions. Column (5) includes all four variables. The dependent variable is firm-level log(labor productivity).*\n\n---\n\n### Question\n\nThe coefficient on 'Skills' decreases from 0.800 in the simple regression (column 4) to 0.208 in the multiple regression (column 5). What is the most likely econometric explanation for this change?",
    "Options": {
      "A": "The simple regression coefficient was upwardly biased because 'Skills' is positively correlated with other productivity-enhancing factors like R&D and technology, which are omitted from column (4).",
      "B": "The simple regression coefficient was downwardly biased, and including other variables in column (5) corrected this bias.",
      "C": "The 'Skills' variable is measured with significant error, and including more variables in the regression exacerbates this measurement error, causing attenuation bias.",
      "D": "High multicollinearity between 'Skills' and the other variables in column (5) makes the simple regression coefficient unreliable, while the multiple regression coefficient is the true causal effect."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to test the user's understanding of omitted variable bias (OVB), a core econometric concept. The correct answer (A) provides the standard OVB explanation for the observed coefficient change. The distractors are designed based on common conceptual errors: (B) states the wrong direction of bias; (C) confuses OVB with measurement error; (D) misattributes the change to multicollinearity, which affects standard errors but doesn't systematically explain the change in the coefficient's magnitude in this way.",
    "qid": "178",
    "model_evaluation_result": {
      "model_raw_response": "C) The \"Skills\" variable is measured dengan significant error, dan including more variables dalam the regression exacerbate this measurement error causing attenuation bias.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 179,
    "Question": "### Background\n\n**Research Question.** This problem examines whether structured management practices predict long-term establishment performance, specifically survival, and whether this relationship holds even within multi-plant firms.\n\n**Setting / Institutional Environment.** The analysis uses a 2010 cross-section of U.S. manufacturing plants and tracks their survival status through 2015.\n\n**Variables & Parameters.**\n- **Dependent Variable**: `Exit`, an indicator variable equal to 1 if the establishment ceased operations between 2010 and 2015.\n- **Key Independent Variable**: `Management`, the structured management score in 2010 (scaled 0-1).\n\n---\n\n### Data / Model Specification\n\nSelected results from a linear probability model of plant exit are presented below.\n\n**Table 1: Management and Probability of Exit, 2010-2015**\n| | (1) | (2) |\n| :--- | :---: | :---: |\n| **Management (2010)** | -0.180 | -0.286 |\n| | (0.014) | (0.033) |\n| **Firm Fixed Effects** | No | Yes |\n\n*Notes: The dependent variable is a dummy for exit by 2015. The sample in column (2) is restricted to establishments in multi-plant firms.* \n\n---\n\n### Question\n\nIn Table 1, the coefficient on 'Management' is -0.180 in column (1) but becomes more negative (-0.286) in column (2) when firm fixed effects are included. What is the most plausible interpretation of this finding?",
    "Options": {
      "A": "Within a multi-plant firm, a poorly-managed plant is disproportionately more likely to be closed compared to its better-managed siblings, suggesting a process of internal creative destruction.",
      "B": "Firms with better average management practices are more likely to close plants, which is why the coefficient becomes more negative.",
      "C": "Measurement error in the management score is likely biasing the firm fixed-effects estimate in column (2) upwards, away from zero.",
      "D": "The relationship between management and exit is weaker in multi-plant firms than in single-plant firms."
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to interpret a key finding from a fixed-effects model and understand the economic story it implies. The correct answer (A) accurately describes the within-firm comparison and the paper's 'internal creative destruction' argument. Distractors target common misinterpretations: (B) misinterprets the nature of a within-firm estimate; (C) incorrectly describes the direction of bias from measurement error in an FE model (it causes attenuation towards zero); (D) is a conceptual opposite of what the larger coefficient implies.",
    "qid": "179",
    "model_evaluation_result": {
      "model_raw_response": "C) Measurementerror in the managementscore is likely biasings the firm fixedeffects estimate in column(2) upwards away from zero.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 180,
    "Question": "### Background\n\n**Research Question.** This problem examines whether structured management practices predict long-term establishment performance, specifically survival.\n\n**Setting / Institutional Environment.** The paper notes that the relationship between management and survival is stronger for young plants. Suppose the coefficient from a linear probability model of exit on management is -0.360 for young plants (≤ 5 years old) and -0.180 for mature plants (> 20 years old). A policymaker considers an intervention that improves the management score by 0.1 units for all plants that adopt it.\n\n---\n\n### Question\n\nIf the intervention is adopted by 1,000 young plants and 1,000 mature plants, what is the expected outcome?",
    "Options": {
      "A": "The intervention is expected to prevent approximately 36 closures among the young plants and 18 closures among the mature plants, suggesting it is more effective when targeted at younger firms.",
      "B": "The intervention is expected to prevent approximately 360 closures among young plants and 180 closures among mature plants.",
      "C": "The intervention is expected to prevent approximately 54 closures in total, with the impact being equally distributed between young and mature firms.",
      "D": "The intervention is more effective for mature plants, preventing 36 closures among them compared to 18 for young plants."
    },
    "Answer": "A",
    "pi_justification": "This is a Scenario Application question that tests the ability to apply regression coefficients to a concrete policy counterfactual. It requires both accurate calculation and correct interpretation to draw a valid policy conclusion. The correct answer (A) performs the calculations correctly (1000 * 0.1 * 0.360 = 36; 1000 * 0.1 * 0.180 = 18) and draws the right inference. Distractors are based on common errors: (B) is a decimal place error; (C) fails to recognize the differential impact; (D) reverses the roles of the coefficients, leading to a flawed conclusion.",
    "qid": "180",
    "model_evaluation_result": {
      "model_raw_response": "C) The expected outcome is the sum of the predicted number of closures prevented for each group: 0.360 * 1000 = 360 for young and 0.180 * 100 = 18 for mature. Therefore, the total number of closures avoided is 360 + 18 = 378.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 181,
    "Question": "### Background\n\n**Research Question.** This analysis seeks to quantify the empirical impact of banking crises on exports, focusing on how the effect varies based on an industry's dependence on external finance (`RZ`).\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences approach where the treatment is a banking crisis and the treatment intensity is an industry's intrinsic reliance on external finance (`RZ`).\n\n### Data / Model Specification\n\nThe dependent variable is log(1 + Exports). An industry at the 75th percentile of external finance dependence has `RZ` = 0.45, while an industry at the 25th percentile has `RZ` = 0.09.\n\n**Table 1: Baseline Regression Result**\n\n| Variable | Coefficient | Std. Error |\n|:---|:---:|:---:|\n| `RZ*Crisis` | -0.141*** | [0.041] |\n\n*Notes: `Crisis` is a dummy for a banking crisis. *** p<0.01.*\n\nBased on the result in Table 1, what is the estimated differential impact of a banking crisis on log exports for an industry at the 75th percentile of external finance dependence (`RZ` = 0.45) compared to one at the 25th percentile (`RZ` = 0.09)?",
    "Options": {
      "A": "Exports are approximately 5.1 percentage points lower for the high-dependence industry.",
      "B": "Exports are approximately 14.1 percentage points lower for the high-dependence industry.",
      "C": "Exports are approximately 5.1 percentage points higher for the high-dependence industry.",
      "D": "Exports are approximately 6.3 percentage points lower for the high-dependence industry."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item tests the ability to correctly interpret and apply a regression coefficient from an interaction term to calculate a marginal effect. It uses a Computational Judgment strategy. Distractors are designed to catch common errors: (B) misinterpreting the coefficient as the final effect (Formula Misuse), (C) a sign error, and (D) failing to calculate the difference between the two groups (Step-Omission Error).",
    "qid": "181",
    "model_evaluation_result": {
      "model_raw_response": "B) Expects are approximately 141 percentage points lower.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 182,
    "Question": "### Background\n\n**Research Question.** This analysis seeks to quantify the empirical impact of banking crises on exports, testing if the effect is amplified in 'deep' crises (those with above-median GDP loss).\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences approach where the treatment is a banking crisis and the treatment intensity is an industry's intrinsic reliance on external finance (`RZ`). The model includes an interaction term to capture the additional effect of a 'deep' crisis.\n\n### Data / Model Specification\n\nThe dependent variable is log(1 + Exports). The highly dependent Drugs industry has `RZ` = 1.46, while the cash-rich Tobacco industry has `RZ` = -0.45.\n\n**Table 1: Heterogeneity by Crisis Depth**\n\n| Variable | Coefficient | Std. Error |\n|:---|:---:|:---:|\n| `RZ*Crisis` | -0.116*** | [0.0428] |\n| `RZ*Crisis*Deep` | -0.326** | [0.133] |\n\n*Notes: `Crisis` is a dummy for a banking crisis. `Deep` is a dummy for crises with above-median output loss. ** p<0.05, *** p<0.01.*\n\nUsing the results from Table 1, calculate the predicted differential change in log exports between the highly dependent Drugs industry (`RZ` = 1.46) and the cash-rich Tobacco industry (`RZ` = -0.45) if the country experiences a 'deep' crisis.",
    "Options": {
      "A": "The Drugs industry's exports are predicted to fall by 84.4 percentage points relative to the Tobacco industry.",
      "B": "The Drugs industry's exports are predicted to fall by 22.1 percentage points relative to the Tobacco industry.",
      "C": "The Drugs industry's exports are predicted to fall by 64.5 percentage points relative to the Tobacco industry.",
      "D": "The Drugs industry's exports are predicted to fall by 44.6 percentage points relative to the Tobacco industry."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses a student's ability to perform a complex counterfactual calculation involving a three-way interaction term. It uses a Computational Judgment strategy. The distractors are based on a matrix of likely errors: (B) ignoring the main interaction term for 'deep' crises (Step-Omission Error), (C) failing to calculate the full differential effect for both industries (Step-Omission Error), and (D) making a sign error in the final comparison (Sign Error).",
    "qid": "182",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: The calculation involves calculating the predicted differential using the coefficients and standard errors provided in the table. It's a bit lengthy, but I'll outline the steps here:\n\n1. Calculate the predicted change in log(1+Exports) for the Drugs industry when RZ*Cris is included: -0.11 (coefficient) + 1.43 (RZ) * 0.11 = -0.\n2. Calculate the change in log (1+Exports for the Tobacco industry when Rz*Cris is excluded: 0.\n3. Calculate the differential change: -0 - 0 = 0.\n4. Now, since we're looking at a 'deep crisis', we need to add the additional effect: -0 + (-0.33) = -0-0.33 = -0-.33 = -.\n5. Finally, convert this back to a percentage point change: 100 * (-0.32) = -32.1%.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 183,
    "Question": "### Background\n\nThis problem analyzes the baseline monetary model without credit, which serves as a benchmark for the paper's main analysis. In this simplified economy, agents are either producers or money holders. They meet randomly, and a trade occurs when a money holder meets a producer of their desired consumption good.\n\n### Data / Model Specification\n\nThe economy is characterized by:\n*   `V_m`, `V_p`: Stationary value functions for a money holder and a producer.\n*   `r`: Rate of time preference.\n*   `β`: Poisson arrival rate of trading partners.\n*   `M`: Proportion of money holders in the population, `M` ∈ (0, 1).\n*   `z`: Probability of a single coincidence of wants.\n*   `u(q)`: A strictly increasing and strictly concave utility function with `u(0)=0`, `u'(0)=∞`, and `u'(∞)=0`.\n\nThe equilibrium is described by the following Bellman equations and bargaining outcome:\n\n**Bellman Equations:**\n```latex\nrV_m = \\beta(1-M)z(V_p - V_m + u(q_m)) \\quad \\quad \\text{(Eq. 1)}\n```\n```latex\nrV_p = \\beta M z(V_m - V_p - q_m) \\quad \\quad \\text{(Eq. 2)}\n```\n**Bargaining Outcome (Take-it-or-leave-it offer by money holder):**\n```latex\nq_m = V_m - V_p \\quad \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nBased on the model specification, what is the stationary equilibrium value of being a producer, `V_p`?",
    "Options": {
      "A": "`V_p = 0`",
      "B": "`V_p = r / (βMz)`",
      "C": "`V_p = -q_m`",
      "D": "`V_p` is positive but depends on the utility function `u(q)`."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the foundational result of the no-credit model using a Computational Judgment strategy. The correct answer requires substituting the bargaining outcome (Eq. 3) into the producer's Bellman equation (Eq. 2), a key deductive step. Distractors represent common conceptual errors: B misapplies the Bellman equation, C confuses value with surplus, and D reflects an incorrect intuition about producer welfare in this specific bargaining environment.",
    "qid": "183",
    "model_evaluation_result": {
      "model_raw_response": "B) `r / (β*M*z)`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 184,
    "Question": "### Background\n\nThis problem analyzes the baseline monetary model without credit, which serves as a benchmark for the paper's main analysis. In this simplified economy, agents are either producers or money holders. They meet randomly, and a trade occurs when a money holder meets a producer of their desired consumption good.\n\n### Data / Model Specification\n\nThe economy is characterized by:\n*   `V_m`, `V_p`: Stationary value functions for a money holder and a producer.\n*   `r`: Rate of time preference.\n*   `β`: Poisson arrival rate of trading partners.\n*   `M`: Proportion of money holders in the population, `M` ∈ (0, 1).\n*   `z`: Probability of a single coincidence of wants.\n*   `u(q)`: A strictly increasing and strictly concave utility function with `u(0)=0`, `u'(0)=∞`, and `u'(∞)=0`.\n\nThe equilibrium is described by the following Bellman equations and bargaining outcome:\n\n**Bellman Equations:**\n```latex\nrV_m = \\beta(1-M)z(V_p - V_m + u(q_m)) \\quad \\quad \\text{(Eq. 1)}\n```\n```latex\nrV_p = \\beta M z(V_m - V_p - q_m) \\quad \\quad \\text{(Eq. 2)}\n```\n**Bargaining Outcome (Take-it-or-leave-it offer by money holder):**\n```latex\nq_m = V_m - V_p \\quad \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nAfter solving for `V_p` and `V_m` in terms of `q_m`, which equation correctly represents the stationary equilibrium condition that determines the purchasing power of money, `q_m`?",
    "Options": {
      "A": "```latex\nu(q_m) - \\left[1 + \\frac{r}{\\beta z(1-M)}\\right]q_m = 0\n```",
      "B": "```latex\nu(q_m) - \\left[1 + \\frac{r}{\\beta z M}\\right]q_m = 0\n```",
      "C": "```latex\nu(q_m) - q_m = 0\n```",
      "D": "```latex\nu(q_m) - \\left[1 - \\frac{r}{\\beta z(1-M)}\\right]q_m = 0\n```"
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to derive the central equilibrium equation of the no-credit model, a key application of the Computational Judgment strategy. The correct answer requires substituting `V_p=0` and `V_m=q_m` into the money holder's Bellman equation and rearranging. The distractors are designed based on high-frequency algebraic and conceptual errors: B (Formula Misuse) confuses the proportion of producers with money holders, C (Step-Omission Error) omits the crucial discounting term, and D (Sign Error) introduces a sign error.",
    "qid": "184",
    "model_evaluation_result": {
      "model_raw_response": "D) `u(q_m)` - `left[1` - `frac{r}` `/` `βz(1-M)` `]` `q_m = ` `0`",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 185,
    "Question": "### Background\n\nThis problem analyzes the baseline monetary model without credit, which serves as a benchmark for the paper's main analysis. In this simplified economy, agents are either producers or money holders. They meet randomly, and a trade occurs when a money holder meets a producer of their desired consumption good.\n\n### Data / Model Specification\n\nThe economy is characterized by:\n*   `V_m`, `V_p`: Stationary value functions for a money holder and a producer.\n*   `r`: Rate of time preference.\n*   `β`: Poisson arrival rate of trading partners.\n*   `M`: Proportion of money holders in the population, `M` ∈ (0, 1).\n*   `z`: Probability of a single coincidence of wants.\n*   `u(q)`: A strictly increasing and strictly concave utility function with `u(0)=0`, `u'(0)=∞`, and `u'(∞)=0`.\n\nThe equilibrium is described by the following Bellman equations and bargaining outcome:\n\n**Bellman Equations:**\n```latex\nrV_m = \\beta(1-M)z(V_p - V_m + u(q_m)) \\quad \\quad \\text{(Eq. 1)}\n```\n```latex\nrV_p = \\beta M z(V_m - V_p - q_m) \\quad \\quad \\text{(Eq. 2)}\n```\n**Bargaining Outcome (Take-it-or-leave-it offer by money holder):**\n```latex\nq_m = V_m - V_p \\quad \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nA key condition for a credit market to exist is that a debtor has an incentive to repay the IOU, which is formally `V_p - V_d - q_m ≥ 0`. In the context of this no-credit equilibrium (where `V_d = 0`), why is this condition violated?",
    "Options": {
      "A": "Because `V_p = 0`, the condition simplifies to `-q_m ≥ 0`, which is false since `q_m > 0`.",
      "B": "Because `V_p` is positive, but smaller than `q_m`.",
      "C": "Because money holders would refuse to trade with debtors, making `q_m` effectively infinite for them.",
      "D": "Because the utility gain `u(q_m)` is not part of the debtor's repayment surplus."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to test the internal consistency of the no-credit equilibrium. It requires the student to apply the equilibrium result (`V_p=0`) to an external condition (the incentive for credit) to see why it fails. The correct answer follows this direct logic. Distractor B is based on an incorrect premise. Distractors C and D introduce plausible but irrelevant or incorrect mechanisms, testing the student's ability to focus on the specific mathematical condition provided.",
    "qid": "185",
    "model_evaluation_result": {
      "model_raw_response": "A) Because V_p =  0, the condition simplies to -q_m ≥0, which is false sinc",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 186,
    "Question": "### Background\n\n**Research Question.** This problem investigates the role of household worklessness as a primary driver of poverty, focusing on how this relationship is exacerbated for children compared to the general population.\n\n**Setting / Institutional Environment.** The analysis uses data for Britain in 2000/1, defining poverty based on net household income after housing costs. It compares the situation of all individuals with the specific situation of children, categorized by their household's employment status.\n\n**Variables & Parameters.**\n- **Employment Circumstance:** Categories describing the work intensity within a household (e.g., 'Workless', 'Couple, one FT').\n- **Population Share:** The percentage of a given population (all individuals or children) that resides in a household of a specific employment type.\n- **Poverty Rate:** The percentage of individuals or children within a specific group whose household income falls below the poverty line.\n- **Unit of observation:** Individual or Child, based on their household's employment status.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Individual Poverty by Employment Circumstance, 2000/1**\n\n|                     | % individuals in each type | % of each type in poverty | % contribution to overall poverty |\n| :------------------ | :------------------------- | :------------------------ | :------------------------------- |\n| Workless            | 17.0                       | 64.4                      | 51.4                             |\n| ...                 | ...                        | ...                       | ...                              |\n| **Total**           | **100**                    | **21.3**                  | **100**                          |\n\n**Table 2: Child Poverty by Employment Circumstance, 2000/1**\n\n|                             | % children in each type | % children in poverty | % contribution to overall child poverty |\n| :-------------------------- | :---------------------- | :-------------------- | :-------------------------------------- |\n| Workless                    | 20.7                    | 77.4                  | 52.8                                    |\n| ...                         | ...                     | ...                   | ...                                     |\n| **Total**                   | **100**                 | **30.3**              | **100**                                 |\n\n*Note: Ellipses (...) denote omitted rows for brevity.*\n\n---\n\n### The Question\n\nBased on the data in Table 1 and Table 2, which statement most accurately compares the poverty situation of children in workless households to that of all individuals in workless households?",
    "Options": {
      "A": "Children in workless households face a higher absolute poverty rate and a higher relative poverty risk than individuals in workless households.",
      "B": "Children in workless households face a higher absolute poverty rate, but their poverty risk relative to the average child is slightly lower than the equivalent relative risk for all individuals.",
      "C": "Children are less likely to live in workless households than the general population, but when they do, their poverty rate is higher.",
      "D": "The contribution of workless households to overall poverty is smaller for children than it is for the general population."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the ability to synthesize data from two tables to make a nuanced comparison, requiring both direct data lookup and a 'Computational Judgment' (calculating relative risk). Distractors are designed to catch common misinterpretations: focusing on only one metric (A), misreading population shares (C), or misreading contribution shares (D).",
    "qid": "186",
    "model_evaluation_result": {
      "model_raw_response": "B) Children în workless households facé a higher absolute povertv rate, but théir poverty risk relative tó the avérage child is slightly lówer than the équivalent relatíve risk for all indíviduals.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 187,
    "Question": "### Background\n\n**Research Question.** This problem investigates the role of household worklessness as a primary driver of poverty, focusing on how this relationship is exacerbated for children compared to the general population.\n\n**Setting / Institutional Environment.** The analysis uses data for Britain in 2000/1, defining poverty based on net household income after housing costs. It compares the situation of all individuals with the specific situation of children, categorized by their household's employment status.\n\n**Variables & Parameters.**\n- **Employment Circumstance:** Categories describing the work intensity within a household (e.g., 'Workless', 'Couple, one FT').\n- **Population Share:** The percentage of a given population (all individuals or children) that resides in a household of a specific employment type.\n- **Poverty Rate:** The percentage of individuals or children within a specific group whose household income falls below the poverty line.\n- **Unit of observation:** Individual or Child, based on their household's employment status.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Individual Poverty by Employment Circumstance, 2000/1**\n\n|                     | % individuals in each type | % of each type in poverty | % contribution to overall poverty |\n| :------------------ | :------------------------- | :------------------------ | :------------------------------- |\n| Workless            | 17.0                       | 64.4                      | 51.4                             |\n| ...                 | ...                        | ...                       | ...                              |\n| **Total**           | **100**                    | **21.3**                  | **100**                          |\n\n**Table 2: Child Poverty by Employment Circumstance, 2000/1**\n\n|                             | % children in each type | % children in poverty | % contribution to overall child poverty |\n| :-------------------------- | :---------------------- | :-------------------- | :-------------------------------------- |\n| Workless                    | 20.7                    | 77.4                  | 52.8                                    |\n| ...                         | ...                     | ...                   | ...                                     |\n| **Total**                   | **100**                 | **30.3**              | **100**                                 |\n\n*Note: Ellipses (...) denote omitted rows for brevity.*\n\n---\n\n### The Question\n\nSuppose a policy reduces the poverty rate *within* the 'Workless' household category by exactly 10 percentage points for all groups. Based on the data in Table 2, what would be the new overall poverty rate for children?",
    "Options": {
      "A": "28.23%",
      "B": "20.30%",
      "C": "28.60%",
      "D": "28.70%"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a weighted-average calculation to evaluate a policy counterfactual, a key skill in quantitative analysis. It uses a 'Computational Judgment' strategy. The distractors are based on a matrix of high-frequency calculation errors: omitting the population weight (B), using the wrong population weight from the other table (C), and confusing percentage points with a percentage change (D).",
    "qid": "187",
    "model_evaluation_result": {
      "model_raw_response": "C) 26.30% (not provided in options)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 188,
    "Question": "### Background\n\nAfter establishing that large horizontal RJV networks are associated with declining market shares, the paper seeks to translate this finding into a practical policy tool by estimating a specific network size threshold above which collusion appears to dominate.\n\n### Data / Model Specification\n\nTo estimate a continuous threshold, the authors specify a quadratic relationship between the horizontal network measure ($RN_{imt}$) and market share ($MS_{imt}$):\n\n```latex\nM S_{i m t}=\\alpha_{0}+\\alpha_{1}M S_{i m t-1}+ B_1 RN_{imt} + B_2 RN_{imt}^2 + ... + \\varepsilon_{i m t} \\quad \\text{(Eq. (1))}\n```\n\nwhere $RN_{imt}$ is the fraction of competitors firm $i$ is linked to, and $B_1$ and $B_2$ are the cumulative coefficients.\n\n### Question\n\nThe paper's results indicate that the effect of network size on market share is positive for small networks but becomes negative for networks larger than a critical threshold of $K^*=0.18$. What does this finding imply about the estimated cumulative coefficients $B_1$ (on $RN_{imt}$) and $B_2$ (on $RN_{imt}^2$)?",
    "Options": {
      "A": "$B_1$ must be positive and $B_2$ must be negative.",
      "B": "$B_1$ must be negative and $B_2$ must be positive.",
      "C": "Both $B_1$ and $B_2$ must be negative.",
      "D": "Both $B_1$ and $B_2$ must be positive."
    },
    "Answer": "A",
    "pi_justification": "This item tests the understanding of how a quadratic functional form translates into a non-linear economic relationship (an inverted U-shape). The distractors represent common mathematical errors in interpreting polynomial models, such as reversing the signs for a U-shaped curve. Strategy: Reverse-Reasoning.",
    "qid": "188",
    "model_evaluation_result": {
      "model_raw_response": "B) B1 must be negative, and B2 must be positive",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 189,
    "Question": "### Background\n\nAfter establishing that large horizontal RJV networks are associated with declining market shares, the paper seeks to translate this finding into a practical policy tool by estimating a specific network size threshold above which collusion appears to dominate. The key finding is an estimated critical threshold of $K^*=0.18$.\n\n### Data / Model Specification\n\n**Table 1: Application of Critical Network Size ($K^*=0.18$) to Selected Industries**\n| SIC4 | Industry Description | Year | % Firms in Network > K* | # Firms in Network > K* | # Firms in Industry |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 3571 | Electronic Computers | 1991 | 0.4706 | 16 | 34 |\n\n### Question\n\nBased on the finding that horizontal networks larger than 18% of the industry ($K^*=0.18$) are associated with collusive behavior, the paper presents the data in Table 1. What is the most accurate policy interpretation of the finding for the Electronic Computers industry in 1991?",
    "Options": {
      "A": "The average horizontal RJV network in the industry connected 47.06% of the firms, indicating widespread collusion.",
      "B": "A significant portion of firms (16 of 34) participated in networks large enough to be flagged for potential antitrust scrutiny.",
      "C": "The 16 firms in the largest network colluded to reduce their collective market share by 18%.",
      "D": "The industry as a whole was found to be colluding, as 16 of the 34 firms were part of a single large RJV."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the ability to translate a statistical rule into a practical policy application. Distractors are designed based on common misinterpretations of the data in the table, such as confusing the meaning of the '% Firms in Network > K*' column with an average network size, or conflating the threshold value with an effect size. Strategy: Scenario Application.",
    "qid": "189",
    "model_evaluation_result": {
      "model_raw_response": "B) B) A significant proportion of firms ( sixteen of thirty-four ) participated in networks big enough to be marked for potential anticompetitive examination.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 190,
    "Question": "### Background\n\nAfter establishing that large horizontal RJV networks are associated with declining market shares, the paper conducts several robustness checks to defend the validity of its measures.\n\n### Data / Model Specification\n\n**Table 1: Correlation of Market Share Measures with 'Gold Standard' Data**\n| | $MS_{FCC}$ | $MS_{Gartner}$ |\n| :--- | :--- | :--- |\n| $MS_{COMPUSTAT}$ | 0.9546*** | 0.8957*** |\n*Notes: `***` denotes significance at the 1% level.*\n\n**Table 2: System GMM Results using Market-Share-Weighted Network ($RN\\_MS$)**\n| Cumulative Effect on Market Share (`MS`) | Coefficient |\n| :--- | :--- |\n| Horizontal Network (MS based), small | 0.0037 |\n| Horizontal Network (MS based), medium | -0.0167* |\n| Horizontal Network (MS based), large | -0.0166** |\n*Notes: **p<0.05, *p<0.10.*\n\n### Question\n\nA critic argues the paper's main conclusion is unreliable because (i) the market share variable is poorly measured and (ii) the network variable wrongly treats all competitor links as equal. Based on the evidence in Table 1 and Table 2, which statement provides the strongest rebuttal to this critique?",
    "Options": {
      "A": "The critique is valid because Table 2 shows different coefficient estimates than the main analysis, indicating the results are not robust.",
      "B": "The critique is refuted because Table 1 shows a high correlation with better data sources, and Table 2 shows that a more sophisticated, market-share-weighted network measure yields the same qualitative conclusion.",
      "C": "The critique is only partially refuted; while Table 1 addresses the market share issue, Table 2's positive coefficient for small networks contradicts the paper's collusion theory.",
      "D": "The critique is valid because correlation in Table 1 does not imply causation, and the System GMM model used for Table 2 cannot handle measurement error."
    },
    "Answer": "B",
    "pi_justification": "This item tests the ability to understand and synthesize the logic of two distinct robustness checks. The distractors are designed to mimic sophisticated but flawed critiques, testing for a deep understanding of what makes an empirical result credible (e.g., confusing qualitative robustness with identical coefficients, or misinterpreting the underlying theory). Strategy: Computational Judgment.",
    "qid": "190",
    "model_evaluation_result": {
      "model_raw_response": "B) The critic is refuted... (only the letter)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 191,
    "Question": "### Background\n\n**Research Question.** This problem develops the paper's theoretical framework to understand how leader identity affects group coordination in a weakest-link game by introducing heterogeneity in player types.\n\n**Setting / Institutional Environment.** The model considers a four-player coordination game where players choose effort `x` on a continuum `[0, W]`. A fraction of the population are 'behavioral types' who follow leaders of their own religion but not others. 'Rational types' maximize expected payoffs, accounting for the possible presence of behavioral types. The analysis assumes the probability of having more than one behavioral player in a group is negligible.\n\n### Data / Model Specification\n\n1.  **No-Leader Equilibrium:** In the absence of a leader, rational players assume others' efforts are drawn from `U[0, W]`. The optimal strategy is to match the expected minimum of the other three players' efforts, leading to a baseline effort `x*`.\n\n2.  **Leader Equilibrium:** With a leader, the optimal effort choices for a rational player under a Hindu leader (`x_H`) and a Muslim leader (`x_M`) are given by:\n\n    ```latex\n    x_{H} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( (1-p)a_{H}\\frac{W}{3} + p a_{M}x^{**} \\right) \\quad \\text{(Eq. 1)}\n    ```\n\n    ```latex\n    x_{M} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( p a_{M}\\frac{W}{3} + (1-p)a_{H}x^{**} \\right) \\quad \\text{(Eq. 2)}\n    ```\n\n    where `p` is the population share of Muslims, `a_H` and `a_M` are the fractions of behavioral types for each religion, `\\tilde{a} = p a_M + (1-p) a_H` is the overall fraction of behavioral types, `x**` is a low effort level (`x** < x*`), and `W` is the maximum effort.\n\n### The Question\n\nIn the model of a rational player's effort choice under a Hindu leader (Eq. 1), what is the economic interpretation of the term `W/3`?",
    "Options": {
      "A": "The optimal effort for a rational player who anticipates that the other two players in the group are also rational.",
      "B": "The risk-dominant equilibrium effort when all four players in the group are rational.",
      "C": "The high effort level proposed by the Hindu leader, which a behavioral Hindu player would follow.",
      "D": "The expected effort of a single player chosen at random from the population."
    },
    "Answer": "A",
    "pi_justification": "Assesses understanding of the model's core logic by requiring interpretation of a key term. Strategy: Computational Judgment. Distractors are based on common formula misapplications (`W/4`, `W/2`) and a subtle conceptual confusion between a behavioral player's action and a rational player's strategic response.",
    "qid": "191",
    "model_evaluation_result": {
      "model_raw_response": "D) The effort of a player chosen at Random from the population",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 192,
    "Question": "### Background\n\n**Research Question.** This problem develops the paper's theoretical framework to understand how leader identity affects group coordination in a weakest-link game by introducing heterogeneity in player types.\n\n**Setting / Institutional Environment.** The model considers a four-player coordination game where players choose effort `x` on a continuum `[0, W]`. A fraction of the population are 'behavioral types' who follow leaders of their own religion but not others. 'Rational types' maximize expected payoffs, accounting for the possible presence of behavioral types. The analysis assumes the probability of having more than one behavioral player in a group is negligible.\n\n### Data / Model Specification\n\n1.  **No-Leader Equilibrium:** In the absence of a leader, rational players assume others' efforts are drawn from `U[0, W]`. The optimal strategy is to match the expected minimum of the other three players' efforts, leading to a baseline effort `x*`.\n\n2.  **Leader Equilibrium:** With a leader, the optimal effort choices for a rational player under a Hindu leader (`x_H`) and a Muslim leader (`x_M`) are given by:\n\n    ```latex\n    x_{H} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( (1-p)a_{H}\\frac{W}{3} + p a_{M}x^{**} \\right) \\quad \\text{(Eq. 1)}\n    ```\n\n    ```latex\n    x_{M} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( p a_{M}\\frac{W}{3} + (1-p)a_{H}x^{**} \\right) \\quad \\text{(Eq. 2)}\n    ```\n\n    where `p` is the population share of Muslims, `a_H` and `a_M` are the fractions of behavioral types for each religion, `\\tilde{a} = p a_M + (1-p) a_H` is the overall fraction of behavioral types, `x**` is a low effort level (`x** < x*`), and `W` is the maximum effort.\n\n### The Question\n\nThe paper empirically finds that coordination is higher under Muslim leaders than Hindu leaders. According to the model, which of the following conditions on the parameters must hold for a rational player's optimal effort to be higher under a Muslim leader (`x_M > x_H`)?",
    "Options": {
      "A": "`p * a_M > (1-p) * a_H`",
      "B": "`a_M > a_H`",
      "C": "`p * a_M < (1-p) * a_H`",
      "D": "`p / a_M > (1-p) / a_H`"
    },
    "Answer": "A",
    "pi_justification": "Tests the ability to derive or recognize the central predictive condition of the model. Strategy: Reverse-Reasoning (inferring the condition from the result). Distractors represent the most common errors: ignoring population weights (Step-Omission), reversing the inequality (Conceptual Opposite), and algebraic mistakes (Formula Misuse).",
    "qid": "192",
    "model_evaluation_result": {
      "model_raw_response": "D) `P / a_M < 1 - P / a_H",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 193,
    "Question": "### Background\n\n**Research Question.** This problem develops the paper's theoretical framework to understand how leader identity affects group coordination in a weakest-link game by introducing heterogeneity in player types.\n\n**Setting / Institutional Environment.** The model considers a four-player coordination game where players choose effort `x` on a continuum `[0, W]`. A fraction of the population are 'behavioral types' who follow leaders of their own religion but not others. 'Rational types' maximize expected payoffs, accounting for the possible presence of behavioral types. The analysis assumes the probability of having more than one behavioral player in a group is negligible.\n\n### Data / Model Specification\n\n1.  **No-Leader Equilibrium:** In the absence of a leader, rational players assume others' efforts are drawn from `U[0, W]`. The optimal strategy is to match the expected minimum of the other three players' efforts, leading to a baseline effort `x*`.\n\n2.  **Leader Equilibrium:** With a leader, the optimal effort choices for a rational player under a Hindu leader (`x_H`) and a Muslim leader (`x_M`) are given by:\n\n    ```latex\n    x_{H} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( (1-p)a_{H}\\frac{W}{3} + p a_{M}x^{**} \\right) \\quad \\text{(Eq. 1)}\n    ```\n\n    ```latex\n    x_{M} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( p a_{M}\\frac{W}{3} + (1-p)a_{H}x^{**} \\right) \\quad \\text{(Eq. 2)}\n    ```\n\n    where `p` is the population share of Muslims, `a_H` and `a_M` are the fractions of behavioral types for each religion, `\\tilde{a} = p a_M + (1-p) a_H` is the overall fraction of behavioral types, `x**` is a low effort level (`x** < x*`), and `W` is the maximum effort.\n\n### The Question\n\nLet `D = x_M - x_H` be the effort gap between Muslim-led and Hindu-led groups. Given that Muslims are a population minority (`p < 0.5`), how does the sensitivity of this gap `D` to a change in the fraction of behavioral Hindus (`a_H`) compare to its sensitivity to an equivalent change in the fraction of behavioral Muslims (`a_M`)?",
    "Options": {
      "A": "The gap `D` is more sensitive to a change in `a_H` than to a change in `a_M`.",
      "B": "The gap `D` is more sensitive to a change in `a_M` than to a change in `a_H`.",
      "C": "The gap `D` is equally sensitive to changes in `a_H` and `a_M`.",
      "D": "The sensitivity cannot be determined without knowing the absolute values of `a_H` and `a_M`."
    },
    "Answer": "A",
    "pi_justification": "Assesses a deeper understanding of the model's mechanics via a comparative static question. Strategy: Computational Judgment. Distractors are designed to catch common errors: the Conceptual Opposite, ignoring the population weights which determine the sensitivity (Step-Omission), and a True but Irrelevant statement that distracts from the core comparison.",
    "qid": "193",
    "model_evaluation_result": {
      "model_raw_response": "A) The effort gap `D = (1-p)(a_H-a_M)(1-\\tilte",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 194,
    "Question": "### Background\n\nThis paper develops a theoretical model to understand the meaning of residual correlations around Engel curves. The analysis is grounded in the classical theory of consumer demand but makes specific assumptions to achieve analytical tractability when using cross-sectional budget data.\n\n### Data / Model Specification\n\nThe consumer's problem is to maximize a utility function `u(x)` subject to a budget constraint `p'x = M`. The model makes two key assumptions:\n\n**Assumption I:** The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`.\nThe vector `a` represents consumer-specific linear preference terms, while the matrix `B` of quadratic terms is assumed to be identical for all consumers.\n\n**Assumption II:** Prices `p` are also identical for all consumers in the budget survey.\n\nThe first-order conditions for utility maximization can be written as a linear system:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -a \\\\ -M \\end{pmatrix} \\quad \\text{(Eq. (1))}\n```\nThe inverse of the bordered matrix on the left defines the substitution matrix `C` and the income-effect vector `c`:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix}^{-1} = \\begin{pmatrix} C & c \\\\ c' & c_0 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n```\nThis leads to a demand system `x = -Ca - cM`. The Engel curve for good `i` is found by separating the mean preference `α` from the idiosyncratic preference `(a - α)`. The disturbance term `ν` for the vector of goods is `ν = -C(a - α)`.\n\n### Question\n\nIn the specified model, the joint assumption of a quadratic utility function (Assumption I) and a common quadratic coefficient matrix `B` (part of Assumption II) is crucial. What is the primary mathematical consequence of this setup that makes the paper's analysis of a single residual correlation across the whole sample theoretically valid?",
    "Options": {
      "A": "It ensures the substitution matrix `C` is constant for all families, regardless of their income level.",
      "B": "It guarantees that all goods are substitutes (`c_ij > 0`), simplifying the interpretation.",
      "C": "It forces the linear preference coefficients `a` to be the same for all families, removing all residual variation.",
      "D": "It ensures that the utility function is globally concave, satisfying the second-order conditions for a maximum."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of a critical foundational assumption. It uses a Reverse-Reasoning strategy. The correct answer (A) identifies the key implication for the model's validity. Distractor (B) is a false statement about the model's implications. Distractor (C) contradicts a core premise of the model (heterogeneity in `a`). Distractor (D) is a 'True but Irrelevant' statement, a sophisticated error where a true property of the model is cited, but it doesn't answer the specific question asked.",
    "qid": "194",
    "model_evaluation_result": {
      "model_raw_response": "D) It ensure that the utilityfunction is globally concava, satisfying thesecond-order conditions for amaximum.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 195,
    "Question": "### Background\n\nThis paper develops a theoretical model to understand the meaning of residual correlations around Engel curves. The analysis is grounded in the classical theory of consumer demand but makes specific assumptions to achieve analytical tractability when using cross-sectional budget data.\n\n### Data / Model Specification\n\nThe consumer's problem is to maximize a utility function `u(x)` subject to a budget constraint `p'x = M`. The model makes two key assumptions:\n\n**Assumption I:** The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`.\nThe vector `a` represents consumer-specific linear preference terms, while the matrix `B` of quadratic terms is assumed to be identical for all consumers.\n\n**Assumption II:** Prices `p` are also identical for all consumers in the budget survey.\n\nThe first-order conditions for utility maximization can be written as a linear system:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -a \\\\ -M \\end{pmatrix} \\quad \\text{(Eq. (1))}\n```\nThe inverse of the bordered matrix on the left defines the substitution matrix `C` and the income-effect vector `c`:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix}^{-1} = \\begin{pmatrix} C & c \\\\ c' & c_0 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n```\nThis leads to a demand system `x = -Ca - cM`. The Engel curve for good `i` is found by separating the mean preference `α` from the idiosyncratic preference `(a - α)`. The disturbance term `ν` for the vector of goods is `ν = -C(a - α)`.\n\n### Question\n\nThe disturbance vector `ν = -C(a - α)` represents a family's deviation from the average consumption pattern due to its unique tastes. Based on the properties of the model's matrices, what is the total expenditure on this disturbance bundle, `p'ν`, and what is the correct economic intuition?",
    "Options": {
      "A": "`p'ν = 0`, because taste-driven deviations from the mean consumption bundle must be budget-neutral.",
      "B": "`p'ν > 0`, because taste deviations are correlated with income, leading to higher overall spending.",
      "C": "`p'ν < 0`, because the substitution matrix `C` is negative semi-definite, constraining the value of the bundle.",
      "D": "The value of `p'ν` is indeterminate without knowing the specific taste profile `(a - α)`."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests a key derived property of the model's disturbance term and its economic meaning. The correct answer (A) pairs the correct mathematical result with the correct intuition. Distractor (B) incorrectly links this property to the income-correlation model discussed elsewhere in the paper. Distractor (C) misapplies the concept of a negative semi-definite matrix. Distractor (D) fails to recognize that the result is a general identity of the model, not a case-specific outcome.",
    "qid": "195",
    "model_evaluation_result": {
      "model_raw_response": "C) `C) p'ν <`",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 196,
    "Question": "### Background\n\nThis paper develops a theoretical model to understand the meaning of residual correlations around Engel curves. The analysis is grounded in the classical theory of consumer demand but makes specific assumptions to achieve analytical tractability when using cross-sectional budget data.\n\n### Data / Model Specification\n\nThe consumer's problem is to maximize a utility function `u(x)` subject to a budget constraint `p'x = M`. The model makes two key assumptions:\n\n**Assumption I:** The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`.\nThe vector `a` represents consumer-specific linear preference terms, while the matrix `B` of quadratic terms is assumed to be identical for all consumers.\n\n**Assumption II:** Prices `p` are also identical for all consumers in the budget survey.\n\nThe first-order conditions for utility maximization can be written as a linear system:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -a \\\\ -M \\end{pmatrix} \\quad \\text{(Eq. (1))}\n```\nThe inverse of the bordered matrix on the left defines the substitution matrix `C` and the income-effect vector `c`:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix}^{-1} = \\begin{pmatrix} C & c \\\\ c' & c_0 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n```\nThis leads to a demand system `x = -Ca - cM`. The Engel curve for good `i` is found by separating the mean preference `α` from the idiosyncratic preference `(a - α)`. The disturbance term `ν` for the vector of goods is `ν = -C(a - α)`.\n\n### Question\n\nGiven the matrix system for utility maximization in Eq. (1) and the definition of the inverse in Eq. (2), what is the correct expression for the vector of demand functions, `x`?",
    "Options": {
      "A": "`x = -Ca - cM`",
      "B": "`x = Ca + cM`",
      "C": "`x = -Ba + pM`",
      "D": "`x = -Ca - cλ`"
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests the student's ability to perform the basic matrix algebra that underpins the entire model. It is a direct Computational Judgment question. The correct answer (A) is the result of the derivation. The distractors are based on common algebraic errors: (B) is a Sign Error, (C) is a Formula Misuse error (using original matrices instead of the inverse), and (D) is a Variable Error (confusing income `M` with the Lagrange multiplier `λ`).",
    "qid": "196",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: The answer is A) `x - Ca - cM`)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 197,
    "Question": "### Background\n\n**Research Question.** This problem investigates the empirical relationship between parents' marriage type (homogamous vs. interfaith) and the effectiveness of religious transmission to their children, shedding light on marriage as a mechanism for purposeful socialization.\n\n**Setting.** The analysis uses U.S. survey data on the religious affiliation of parents and their children. The data allows for a direct comparison of socialization outcomes between families where parents share the same religion and families where they do not.\n\n**Variables and Parameters.**\n- **Marriage Type**: A categorical variable indicating the religious combination of parents. We focus on PP (Protestant-Protestant), CC (Catholic-Catholic), JJ (Jewish-Jewish), and PC (Protestant-Catholic).\n- **Child's Religion**: A categorical variable for the child's religious identity (Protestant, Catholic, Jewish, or Other).\n- **Socialization Probability**: The conditional probability that a child from a given marriage type adopts a specific religion, as reported in the table.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Socialization Probabilities for Selected Marriage Types**\n\n| Marriage Type | Child is Protestant | Child is Catholic | Child is Jew | Child is Other |\n| :--- | :---: | :---: | :---: | :---: |\n| PP (Prot.-Prot.) | **0.9179** | 0.0284 | 0.0000 | 0.0537 |\n| CC (Cath.-Cath.) | 0.0850 | **0.8571** | 0.0034 | 0.0544 |\n| JJ (Jew-Jew) | 0.0370 | 0.0000 | **0.9259** | 0.0370 |\n| PC (Prot.-Cath.) | 0.5116 | 0.3140 | 0.0000 | 0.1744 |\n\n*Source: Table 2 in the original paper. Each cell reports the sample probability that a child from the row's marriage type is a member of the column's religious group.*\n\n---\n\n### Question\n\nSuppose a new government policy promoting social mixing is implemented, which causes 100 Protestant families who would have formed PP marriages and 100 Catholic families who would have formed CC marriages to instead form 200 new PC marriages. Based on the probabilities in Table 1, what is the approximate net change in the total number of Catholic children produced by this group of 200 families as a result of the policy?",
    "Options": {
      "A": "A decrease of 26",
      "B": "An increase of 26",
      "C": "A decrease of 89",
      "D": "An increase of 2"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a complex, policy-relevant counterfactual analysis using probabilistic data. It uses the Computational Judgment strategy. The distractors are designed to catch common errors: (B) is a sign error, confusing a decrease with an increase; (C) is a step-omission error where the student reports the baseline number of Catholic children instead of the net change; (D) confuses the result for Catholics with the result for Protestants, testing careful reading.",
    "qid": "197",
    "model_evaluation_result": {
      "model_raw_response": "B) An Increase of 26.0.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 198,
    "Question": "### Background\n\n**Research Question.** This problem investigates the empirical relationship between parents' marriage type (homogamous vs. interfaith) and the effectiveness of religious transmission to their children, shedding light on marriage as a mechanism for purposeful socialization.\n\n**Setting.** The analysis uses U.S. survey data on the religious affiliation of parents and their children. The data allows for a direct comparison of socialization outcomes between families where parents share the same religion and families where they do not.\n\n**Variables and Parameters.**\n- **Marriage Type**: A categorical variable indicating the religious combination of parents. We focus on PP (Protestant-Protestant), CC (Catholic-Catholic), JJ (Jewish-Jewish), and PC (Protestant-Catholic).\n- **Child's Religion**: A categorical variable for the child's religious identity (Protestant, Catholic, Jewish, or Other).\n- **Socialization Probability**: The conditional probability that a child from a given marriage type adopts a specific religion, as reported in the table.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Socialization Probabilities for Selected Marriage Types**\n\n| Marriage Type | Child is Protestant | Child is Catholic | Child is Jew | Child is Other |\n| :--- | :---: | :---: | :---: | :---: |\n| PP (Prot.-Prot.) | **0.9179** | 0.0284 | 0.0000 | 0.0537 |\n| CC (Cath.-Cath.) | 0.0850 | **0.8571** | 0.0034 | 0.0544 |\n| JJ (Jew-Jew) | 0.0370 | 0.0000 | **0.9259** | 0.0370 |\n| PC (Prot.-Cath.) | 0.5116 | 0.3140 | 0.0000 | 0.1744 |\n\n*Source: Table 2 in the original paper. Each cell reports the sample probability that a child from the row's marriage type is a member of the column's religious group.*\n\n---\n\n### Question\n\nThe 'homogamy premium' for a religious group can be defined as the increase in the probability of a child adopting that group's faith when raised in a homogamous marriage (e.g., PP or CC) versus a specific interfaith marriage (PC). Based on Table 1, which statement accurately compares the homogamy premia for Protestants and Catholics when the alternative is a PC marriage?",
    "Options": {
      "A": "The homogamy premium is larger for Catholics than for Protestants.",
      "B": "The homogamy premium is larger for Protestants than for Catholics.",
      "C": "Protestants have a higher socialization success rate in homogamous marriages than Catholics do.",
      "D": "The homogamy premia for both groups are nearly identical."
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to extract specific data points from a table, perform a two-step calculation (subtraction for each group), and then make a comparison. It uses a Computational Judgment strategy. Distractor (B) represents the conceptual opposite and a likely error. Distractor (C) presents a true but irrelevant fact from the table, testing if the student correctly understood the definition of the 'premium'. Distractor (D) targets simple calculation errors.",
    "qid": "198",
    "model_evaluation_result": {
      "model_raw_response": "B) The homemgamy premium is Larger for Protestants Than for Catholics.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 199,
    "Question": "### Background\n\n**Research Question.** This problem examines the core theoretical mechanism of a macro model with a fragile banking system. It focuses on how banks, in maximizing returns for their financiers, endogenously determine their leverage and risk.\n\n**Setting.** The model features a relationship-lending bank that finances projects with stochastic returns. The bank is funded by deposits and capital. The demandable nature of deposits exposes the bank to runs. A bank manager chooses the capital structure (leverage) to maximize the combined expected payoff to depositors and capitalists.\n\n### Data / Model Specification\n\nThe return of a bank's project is `R_t^A + x_t`, where `x_t` is a random shock uniformly distributed on `[-h, h]`. The bank manager chooses the optimal deposit-to-asset ratio `d_t`:\n```latex\nd_{t}=\\frac{1}{R_{t}}\\frac{R_{t}^{A}+h}{2-\\lambda+c(1+\\lambda)} \\quad \\text{(Eq. 1)}\n```\nwhere `R_t` is the deposit rate, `R_t^A` is the expected project return, `λ` is the bank's special liquidation skill, and `c` is a resource loss in a run.\n\nBank risk is measured by the probability of a run, `φ_t`:\n```latex\n\\phi_{t} = \\frac{1}{2}\\left(1 - \\frac{R_{t}^{A}-R_{t}d_{t}}{h}\\right) \\quad \\text{(Eq. 2)}\n```\n### Question\n\nIn this model, banks endogenously adjust their leverage (`d_t`) in response to changes in the economic environment. What is the net effect of an increase in underlying project riskiness (`h`) on the equilibrium probability of a bank run (`φ_t`)?",
    "Options": {
      "A": "It increases, because banks endogenously increase their leverage in response to higher risk, and this effect reinforces the direct impact of higher risk.",
      "B": "It decreases, because for a fixed level of leverage, a wider distribution of project outcomes makes the specific run-triggering event less likely.",
      "C": "It has no net effect, because banks perfectly adjust their leverage to keep the probability of a run constant.",
      "D": "It decreases, because banks prudently reduce their leverage in response to a riskier environment, more than offsetting the direct impact of higher risk."
    },
    "Answer": "A",
    "pi_justification": "This item tests the central theoretical result of the banking model using a Computational Judgment strategy. The correct answer requires the student to understand that the bank's endogenous leverage choice (risk-taking) dominates, leading to a positive relationship between project risk and bank risk. Distractors are designed based on common errors: B isolates a partial effect (ignoring endogeneity), D assumes the wrong behavioral response (risk aversion instead of risk-taking), and C posits a plausible but incorrect cancellation of effects.",
    "qid": "199",
    "model_evaluation_result": {
      "model_raw_response": "D) It Decreases, because banks prudentley reduce their leverage...",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 200,
    "Question": "### Background\n\n**Research Question.** This problem examines the core theoretical mechanism of a macro model with a fragile banking system. It focuses on how banks, in maximizing returns for their financiers, endogenously determine their leverage, risk, and capital accumulation.\n\n**Setting.** The model features a relationship-lending bank that finances projects with stochastic returns. The bank is funded by deposits and capital. The demandable nature of deposits exposes the bank to runs. A bank manager chooses the capital structure (leverage) to maximize the combined expected payoff to depositors and capitalists.\n\n### Data / Model Specification\n\nBank capital `BK_t` accumulates from the reinvested earnings of bank capitalists according to the following law of motion:\n```latex\nBK_{t}=\\frac{\\theta}{\\pi_{t}}[BK_{t-1}+\\varrho_{t}^{BK}Q_{t}K_{t}] \\quad \\text{(Eq. 1)}\n```\nwhere `θ` is the bank survival rate, `π_t` is inflation, `ϱ_t^{BK}` is the expected unitary payoff to capitalists (their profit rate), `Q_t` is the price of capital assets, and `K_t` is the stock of capital.\n\n### Question\n\nAccording to the model's 'bank balance sheet channel', a positive shock to the economy can strengthen bank capital, which in turn fuels a credit expansion that amplifies the initial shock. This amplification is most directly initiated by an increase in which variable from the capital accumulation equation (Eq. 1)?",
    "Options": {
      "A": "`Q_t` (Asset Prices)",
      "B": "`ϱ_t^{BK}` (Unitary Payoff to Capitalists)",
      "C": "`BK_{t-1}` (Lagged Bank Capital)",
      "D": "`θ` (Bank Survival Rate)"
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the bank balance sheet channel using a Reverse-Reasoning strategy. Given the outcome (credit expansion), the student must identify the primary trigger within the model's structure. The correct answer is `Q_t`, the asset price, which directly increases the valuation of the bank's asset base (`Q_t K_t`). Distractors represent plausible but incorrect alternatives: `ϱ_t^{BK}` is the profit rate not the valuation shock, `BK_{t-1}` is a state variable not the shock, and `θ` is an irrelevant structural parameter.",
    "qid": "200",
    "model_evaluation_result": {
      "model_raw_response": "B) `ρ_t^{BK}` (Unitary Profit Rate)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 201,
    "Question": "### Background\n\n**Research Question.** This problem examines the theoretical underpinnings of the paper's generalized cointegration model, focusing on the relative efficiency of the proposed Quasi-Differencing (QD) detrending procedure compared to standard Ordinary Least Squares (OLS).\n\n**Setting / Institutional Environment.** The framework is a Vector Autoregression (VAR) generalized with a quasi-difference (QD) operator to model local-to-unity roots. The efficiency comparison depends on the localizing parameter `c`.\n\n**Variables & Parameters.**\n- `c`: The localizing parameter. `c=0` corresponds to a pure unit root.\n- `V_ols`, `V_gls`: Asymptotic variances of the OLS and QD estimators for the trend coefficient in the non-stationary directions.\n- `R_c`: The relative efficiency, defined as `R_c = V_ols / V_gls`.\n\n---\n\n### Data / Model Specification\n\nFor the case where there is one non-stationary direction (`s=1`) and the trend is `x_t=t`, the asymptotic variances of the QD (`gls`) and OLS (`ols`) estimators for the trend coefficient are given by:\n```latex\nV_{gls}=\\frac{3\\sigma^{2}}{3-3c+c^{2}} \n\n\\quad \\text{(Eq. 1)}\n```\n```latex\nV_{ols}=\\frac{3\\sigma^{2}[3e^{2c}(c-1)^{2}+2c^{3}+3c^{2}-3]}{2c^{5}} \n\n\\quad \\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nUsing the variance formulas in Eq. (1) and Eq. (2), what is the limit of the relative efficiency `R_c = V_ols / V_gls` as the localizing parameter `c → 0`? This result indicates the efficiency gain of QD detrending when the alternative hypothesis is extremely close to the null of a perfect unit root.",
    "Options": {
      "A": "1",
      "B": "0",
      "C": "∞",
      "D": "3"
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests the ability to perform a formal mathematical derivation and arrive at a precise conclusion, a task designated as 'Computational Judgment'. It converts an open-ended calculation from the source QA into a single-choice question. The distractors are designed based on common errors: 0 (incorrect limit evaluation), ∞ (conceptual error about efficiency), and 3 (superficial formula reading).",
    "qid": "201",
    "model_evaluation_result": {
      "model_raw_response": "C) �infinity (inf) \n\nNote: As `c → ∞`, both `V_glm` and `V_olm` tend to 0, but `V_oulm` tends to 0 faster, making `R_c` tend to infinity.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 202,
    "Question": "### Background\n\n**Research Question.** This problem assesses the causal effect of insurers' fee screens on physicians' list prices and evaluates how sensitive these estimates are to theoretical and empirical model specification.\n\n**Setting / Institutional Environment.** The study estimates the elasticity of physician office prices with respect to insurer fee screens. It compares estimates from a prior, misspecified model with a new, theoretically-grounded model. The new model makes two key corrections: (1) it uses the correct theoretical weights for insurer fees (unweighted for Medicaid, weighted by share of list-price patients for others) and (2) it uses a stricter, theory-consistent definition of Medicaid \"participation.\"\n\n### Data / Model Specification\n\n**Table 1: Office Fee Elasticities from Alternative Model Specifications**\n\n| Model Specification | Participant Definition | Medicaid | Medicare | Private |\n| :--- | :--- | :--- | :--- | :--- |\n| Sloan (original study) | \"some or all\" | 0.18 | 0.03 | 0.03 |\n| Eq. 2.1 (Sloan spec, new data) | \"some or all\" | 0.16 | 0.30 | 0.61 |\n| **Eq. 2.3 (Corrected Model)** | | | | |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Participants* | \"all\" | 0.40 | 0.28 | 0.53 |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Nonparticipants* | \"all\" | 0.17 | 0.25 | 0.52 |\n\n### Question\n\nBased on the corrected elasticities for a *participant* physician in Table 1, what is the predicted total percentage increase in their office list price from a uniform 10% increase in both Medicare and Private fee screens?",
    "Options": {
      "A": "2.8%",
      "B": "5.3%",
      "C": "7.7%",
      "D": "8.1%"
    },
    "Answer": "D",
    "pi_justification": "This item assesses the ability to correctly select and apply elasticity estimates from a table to a quantitative scenario. It uses a Computational Judgment strategy. Distractors are designed based on predictable errors: using only one of the two relevant elasticities (Step-Omission Error) or using the correct formula but with data from the wrong row of the table (Lookup Error).",
    "qid": "202",
    "model_evaluation_result": {
      "model_raw_response": "C) 8% (Note: This calculation is based on the values in the corrected model for participant physicians: 0.4 for Medicare and 0.5 for private. The total percentage increase would be the sum of the individual increases: 0% + 0.10 x 0.48 = 8.%)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 203,
    "Question": "### Background\n\n**Research Question.** This problem assesses the causal effect of insurers' fee screens on physicians' list prices and evaluates how sensitive these estimates are to theoretical and empirical model specification.\n\n**Setting / Institutional Environment.** The study estimates the elasticity of physician office prices with respect to insurer fee screens. It compares estimates from a prior, misspecified model with a new, theoretically-grounded model. The new model makes two key corrections: (1) it uses the correct theoretical weights for insurer fees (unweighted for Medicaid, weighted by share of list-price patients for others) and (2) it uses a stricter, theory-consistent definition of Medicaid \"participation.\"\n\n### Data / Model Specification\n\n**Table 1: Office Fee Elasticities from Alternative Model Specifications**\n\n| Model Specification | Participant Definition | Medicaid | Medicare | Private |\n| :--- | :--- | :--- | :--- | :--- |\n| Sloan (original study) | \"some or all\" | 0.18 | 0.03 | 0.03 |\n| Eq. 2.1 (Sloan spec, new data) | \"some or all\" | 0.16 | 0.30 | 0.61 |\n| **Eq. 2.3 (Corrected Model)** | | | | |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Participants* | \"all\" | 0.40 | 0.28 | 0.53 |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Nonparticipants* | \"all\" | 0.17 | 0.25 | 0.52 |\n\n### Question\n\nA regulator proposes a 5% cut to all insurer fee screens (Medicaid, Medicare, and Private). Assume this policy causes a physician who was previously a 'participant' to switch their behavior and become a 'nonparticipant'. Based on the elasticities in Table 1, what is the predicted total percentage change in this physician's office list price, and what is the most likely direct consequence for patient care?",
    "Options": {
      "A": "The price will fall by 4.7%, and the most likely consequence is reduced access to care for Medicaid patients.",
      "B": "The price will fall by 6.05%, and the most likely consequence is reduced access to care for Medicaid patients.",
      "C": "The price will fall by 4.7%, but there will be no significant impact on patient access as the price drop is small.",
      "D": "The price will rise by 4.7%, as physicians try to recover lost income from lower fees."
    },
    "Answer": "A",
    "pi_justification": "This high-difficulty item tests the ability to apply elasticity estimates in a dynamic counterfactual scenario that involves a change in agent type. It uses a Scenario Application strategy. The correct answer requires both a precise calculation and a correct inference about the policy's main unintended consequence. Distractors are based on using the wrong set of elasticities for the calculation (a common conceptual error), getting the calculation right but drawing the wrong conclusion about the consequences, or getting the direction of the price change wrong.",
    "qid": "203",
    "model_evaluation_result": {
      "model_raw_response": "C) Theprice will fall by4.7%but there will be nosignificantimpact onpatientaccessas thepricedropissmall.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 204,
    "Question": "### Background\n\n**Research Question.** What are the market characteristics and strategic considerations for authors and publishers in different academic fields?\n\n**Setting / Institutional Environment.** The study surveys authors from Economics departments and Business schools, revealing significant differences in their publication patterns and market outcomes. Economics faculty tend to write more specialized, technical books for a smaller academic audience, while Business school faculty write more textbooks aimed at larger student and professional markets. These differences are reflected in the sales distributions for books from each field.\n\n**Variables & Parameters.**\n*   `Book Type`: Classification of publications (e.g., `Textbook`, `Technical or specialized book`).\n*   `Sales Volume`: The total number of copies sold, grouped into categories.\n*   `Publisher's Profit`: The net revenue a publisher earns from a book.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Publication and Sales Profiles by Academic Field**\n\n| Metric                               | Economics Departments | Business Schools |\n|--------------------------------------|-----------------------|------------------|\n| **Book Type Distribution**           |                       |                  |\n| % Textbooks*                         | 43.3%                 | 60.8%            |\n| % Technical or specialized books     | 35.3%                 | 22.8%            |\n| **Sales Volume Distribution**        |                       |                  |\n| % of books selling 1-2,000 copies    | 46%                   | 25%              |\n| % of books selling 2,001-5,000 copies| 22%                   | 24%              |\n| % of books selling 5,001-20,000 copies| 20%                   | 29%              |\n| % of books selling over 20,000 copies| 8%                    | 16%              |\n\n*Textbooks are defined as Freshman, Advanced undergraduate, and Graduate texts combined.*\n\nTo calculate expected sales, use the midpoint of each sales range from **Table 1** (use 1,500 for 1-2k; 3,500 for 2k-5k; 12,500 for 5k-20k; and 30,000 for >20k).\n\n---\n\n### Question\n\nA risk-averse publisher must choose between a typical Economics department project and a typical Business School project. The publisher defines 'commercial failure' as selling 5,000 or fewer copies. Based on the data in **Table 1**, which of the following statements is the most accurate strategic conclusion?",
    "Options": {
      "A": "The Economics project is superior for a risk-averse publisher because its lower sales potential implies a smaller potential loss.",
      "B": "The Business School project is superior because it has higher expected sales, despite having a higher risk of commercial failure.",
      "C": "The Business School project is superior because it has both a higher expected sales volume and a lower probability of commercial failure.",
      "D": "The Economics project is superior because it has both a higher expected sales volume and a lower probability of commercial failure."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the ability to perform expected value calculations and risk analysis using distributional data. It uses a Computational Judgment strategy. The correct option requires the student to calculate both expected sales and the probability of failure, and then recognize that one option dominates the other on both metrics. Distractors are designed based on common errors: misinterpreting risk aversion (A), partial calculation (B), and significant calculation errors leading to the opposite conclusion (D).",
    "qid": "204",
    "model_evaluation_result": {
      "model_raw_response": "C) The Busines School project is the most likely to be the most accurate conclusion.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 205,
    "Question": "### Background\n\n**Research Question.** What are the market characteristics and strategic considerations for authors and publishers in different academic fields?\n\n**Setting / Institutional Environment.** The study surveys authors from Economics departments and Business schools, revealing significant differences in their publication patterns and market outcomes. Economics faculty tend to write more specialized, technical books for a smaller academic audience, while Business school faculty write more textbooks aimed at larger student and professional markets. These differences are reflected in the sales distributions for books from each field.\n\n**Variables & Parameters.**\n*   `Book Type`: Classification of publications (e.g., `Textbook`, `Technical or specialized book`).\n*   `Sales Volume`: The total number of copies sold, grouped into categories.\n*   `Publisher's Profit`: The net revenue a publisher earns from a book.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Publication and Sales Profiles by Academic Field**\n\n| Metric                               | Economics Departments | Business Schools |\n|--------------------------------------|-----------------------|------------------|\n| **Book Type Distribution**           |                       |                  |\n| % Textbooks*                         | 43.3%                 | 60.8%            |\n| % Technical or specialized books     | 35.3%                 | 22.8%            |\n| **Sales Volume Distribution**        |                       |                  |\n| % of books selling 1-2,000 copies    | 46%                   | 25%              |\n| % of books selling 2,001-5,000 copies| 22%                   | 24%              |\n| % of books selling 5,001-20,000 copies| 20%                   | 29%              |\n| % of books selling over 20,000 copies| 8%                    | 16%              |\n\n*Textbooks are defined as Freshman, Advanced undergraduate, and Graduate texts combined.*\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following best characterizes the difference between the 'publication models' of Economics departments and Business schools?",
    "Options": {
      "A": "Economics faculty focus on high-volume textbooks, while Business school faculty focus on niche technical books.",
      "B": "Business school books have double the probability of selling over 20,000 copies compared to Economics books.",
      "C": "Both fields produce a similar percentage of textbooks, but Business school books sell better due to more effective marketing.",
      "D": "Economics faculty focus more on specialized, lower-volume technical books for a niche audience, while Business school faculty focus more on higher-volume textbooks for a broader market."
    },
    "Answer": "D",
    "pi_justification": "This item assesses the ability to interpret descriptive statistics and synthesize them into a coherent qualitative description. The correct option accurately summarizes the relationship between book type focus and sales outcomes for the two fields. Distractors include a conceptual opposite (A), a true but incomplete statement (B), and a factually incorrect and speculative statement (C).",
    "qid": "205",
    "model_evaluation_result": {
      "model_raw_response": "D) Economicsfaculty focus more on specialist, lower-volume tech",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 206,
    "Question": "### Background\n\n**Research Question.** This problem investigates the emergence of market failures within a mature, largely private financial system and the consequent role of government intervention.\n\n**Setting.** The context is the British capital market from the late 19th to mid-20th century. This market is described as having a \"statist start\" (Bank of England founded to fund government debt) but subsequently following an \"orthodox path\" of private institutional development, integrating local markets into a national one centered in London.\n\n**Key Concepts.**\n\n*   **Orthodox Path:** A historical mode of financial development characterized by the spontaneous evolution of institutions to contain lender exuberance and integrate markets, driven by private actors.\n*   **Market Gaps:** The text identifies two persistent failures in the British market:\n    1.  **The Macmillan Gap:** A market failure in equity finance for medium-sized enterprises. Firms needing less than £100,000 could raise capital locally, while those needing more than £1,500,000 could issue shares in London. The gap existed for capital sums between these thresholds.\n    2.  **The Temporal Gap:** A lack of financing for medium-term debt (5 to 10 years), which was needed for equipment finance. The market was well-developed for very short-term trade finance (90-180 days) and long-term borrowing (10-20 years), but not for the intermediate maturities.\n*   **Coase Theorem (in this context):** The proposition that if transaction costs are low, the specific form of institutions does not matter for achieving efficient outcomes, as private arrangements will emerge to fill any needs.\n*   **Corrective Government Institutions:** Specialized agencies created to fill identified market gaps, including:\n    *   **Industrial and Commercial Finance Corporation (ICFC):** Established after 1945 \"to fill the Macmillan gap.\"\n    *   **Export Credit Guarantees Department (ECGD):** Established in the 1930s \"to assist in financing the export of capital equipment.\"\n\n---\n\n### Data / Model Specification\n\nThis is a conceptual problem based on historical analysis. The key data points are the financial thresholds defining the Macmillan Gap.\n\n---\n\n### The Question\n\nConsider a hypothetical medium-sized British industrial firm in 1950 seeking a £500,000 loan with a 7-year maturity to finance new factory equipment for *domestic* production. This request falls into both the Macmillan Gap (by size) and the temporal gap (by maturity). Based on the mandates described in the text, which institution was the appropriate source of finance for this firm?",
    "Options": {
      "A": "The Industrial and Commercial Finance Corporation (ICFC).",
      "B": "The Export Credit Guarantees Department (ECGD).",
      "C": "Both the ICFC and the ECGD would be equally appropriate.",
      "D": "Neither, because the firm requires debt financing, whereas the Macmillan Gap was primarily about equity capital."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to apply specific factual knowledge about policy institutions to a novel scenario. It uses a Scenario Application strategy. The distractors are designed to test for common comprehension errors: B is an 'Almost Right' option that ignores a key qualifier ('export'); C represents a failure to differentiate; D represents a plausible but incorrect inference based on a detail from the text.",
    "qid": "206",
    "model_evaluation_result": {
      "model_raw_response": "A) TheIndustrial and CommercialFinance Corporation (ICFT).",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 207,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the economic and policy implications of correcting a model's specification by analyzing the key parameters of the wage-price feedback loop in the author's final, dynamically stable model.\n\n**Setting and Sample.** The analysis compares parameter estimates from Hines' original, dynamically unstable model with those from the author's re-specified, dynamically stable model, estimated using Two-Stage Least Squares (2SLS) on UK data from 1921-61. The core of the wage-price spiral is the feedback between wage changes and price changes.\n\n**Variables and Parameters.**\n- `ΔW_t`: Percentage change in hourly wage rates.\n- `ΔP'_t`: Percentage change in retail prices (re-specified).\n- `a₃`: The coefficient on price changes in the wage equation, representing the price-to-wage pass-through.\n- `b₁`: The coefficient on wage changes in the price equation, representing the wage-to-price pass-through.\n\n---\n\n### Data / Model Specification\n\nHines' original (unstable) model produced an estimate of the price-to-wage pass-through (`a₃`) of approximately 0.6.\n\nThe author's final preferred specification drops an insignificant productivity variable and is estimated using 2SLS. The key behavioral equations and their estimated coefficients are presented in Table 1.\n\n**Table 1: Final 2SLS Model Estimates**\n\n| Equation | Dependent Var. | Intercept | `ΔT_t` | `T_t` | `ΔP'_{t-1/2}` | `ΔW_t` | `ΔM_t` |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| (Eq. 1) Wage Eq. | `ΔW_t` | -4.68 | 2.21 | 0.215 | 0.294 | - | - |\n| | | | (0.29) | (0.057) | (0.123) | | |\n| (Eq. 2) Price Eq. | `ΔP'_t` | -0.146 | - | - | - | 0.702 | 0.0732 |\n| | | | | | | (0.069) | (0.0358) |\n\n*Note: Asymptotic standard errors are in parentheses.*\n\nIn a simplified dynamic system, the total cumulative long-run impact of a one-time price shock is given by the multiplier `1 / (1 - k)`, where `k = a₃ * b₁` is the round-trip feedback multiplier. This is valid only if the system is stable (`|k| < 1`).\n\n---\n\n### Question\n\nUsing the data provided, calculate the long-run inflation multiplier implied by the author's final model and compare it to the multiplier implied by using Hines' inconsistent estimate of `a₃`. Which of the following statements correctly quantifies this comparison and its policy implication?",
    "Options": {
      "A": "The author's model implies a long-run multiplier of approximately 1.26, whereas Hines' estimate implies a multiplier of 1.73. This suggests that relying on the misspecified model would cause policymakers to overstate the persistence of inflation following a shock.",
      "B": "The author's model implies a long-run multiplier of approximately 1.26, whereas Hines' estimate implies a multiplier of 1.73. This suggests that relying on the misspecified model would cause policymakers to understate the persistence of inflation following a shock.",
      "C": "The author's model implies a round-trip feedback effect of 0.21, whereas Hines' estimate implies a feedback effect of 0.42. This suggests that relying on the misspecified model would cause policymakers to understate the persistence of inflation following a shock.",
      "D": "The author's model implies a long-run multiplier of approximately 4.85, whereas Hines' estimate implies a multiplier of 2.37. This suggests that relying on the misspecified model would cause policymakers to overstate the persistence of inflation following a shock."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the pinnacle of the original QA problem: quantifying the policy error from using a misspecified model. It uses a Computational Judgment strategy, requiring a multi-step calculation and a final comparative interpretation. Distractor B correctly calculates the multipliers but reverses the policy implication. Distractor C correctly identifies the policy implication but reports the one-period feedback `k` instead of the long-run multiplier `1/(1-k)`, a classic Step-Omission Error. Distractor D uses an incorrect formula, `1/k`, for the multiplier.",
    "qid": "207",
    "model_evaluation_result": {
      "model_raw_response": "B) The authors' model implies a low-run multiplier of about 1.44, whereas Hine's estimate implies a multipler of 1/0.6 = 1.67. This suggests relying on the mis-specified model would result in underestimating the persistence of the inflation following a sudden change.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 208,
    "Question": "### Background\n\n**Research Question.** This problem examines the structure of the author's proposed alternative wage-price model.\n\n**Setting and Sample.** After demonstrating the instability of the Hines model, the author proposes a re-specified system of equations for the UK economy (1921-61).\n\n---\n\n### Data / Model Specification\n\nThe author's re-specified model takes the form:\n```latex\nΔT_t = c_0 + c_1 T_{t-1} + c_2 ΔP'_{t-1/2} + c_3 D_{t-1/2} + V_{3t} \\quad \\text{(Eq. 1)}\n```\n```latex\nΔW_t = a_0 + a_1 ΔT_t + a_2 T_t + a_3 ΔP'_{t-1/2} + V_{1t} \\quad \\text{(Eq. 2)}\n```\n```latex\nΔP'_t = b_0 + b_1 ΔW_t + b_2 ΔM_t + b_3 ΔX_t + V_{2t} \\quad \\text{(Eq. 3)}\n```\nThis system is closed with the identity `T_t = T_{t-1} + ΔT_t`.\n\n---\n\n### Question\n\nBased on the system of equations (Eq. 1 - Eq. 3), which of the following best describes the recursive, \"stepwise chain of causation\" for the endogenous variables within a single time period `t`?",
    "Options": {
      "A": "Predetermined variables first determine `ΔT_t`, which in turn determines `T_t`. Then `ΔW_t` is determined, and finally `ΔP'_t` is determined.",
      "B": "Predetermined variables first determine `ΔP'_t`, which then determines `ΔW_t`. Then `ΔT_t` and `T_t` are determined simultaneously.",
      "C": "The variables `ΔT_t`, `ΔW_t`, and `ΔP'_t` are all determined simultaneously by the predetermined variables in the system.",
      "D": "Predetermined variables first determine `ΔT_t`, which in turn determines `T_t`. Then `ΔP'_t` is determined, and finally `ΔW_t` is determined."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.3). This item assesses the core concept from part (a) of the original QA: understanding the model's recursive structure. The correct answer requires tracing the causal flow through the equations. The distractors are designed based on plausible misreadings of the system: Distractor B reverses the wage/price determination, Distractor C incorrectly assumes simultaneity (the property the recursive structure is designed to avoid), and Distractor D swaps the final two steps in the causal chain.",
    "qid": "208",
    "model_evaluation_result": {
      "model_raw_response": "C) Thevariables ΔT_t, ΔW_t, and ΔP'_t are all determined simul",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 209,
    "Question": "## Background\n\n**Research Question.** This problem examines a method to distinguish the effect of adverse selection from a potential confounding \"scale effect\" in the pricing of thoroughbred yearlings.\n\n**Setting / Institutional Environment.** A potential critique of the main finding is that the `Racing Intensity` variable may be correlated with the overall scale of a seller's operation. If larger breeding operations have better reputations and command higher prices, the estimated coefficient on `Racing Intensity` could be biased. This analysis re-parameterizes the model to test this possibility directly.\n\n**Variables & Parameters.**\n- `ln(Price)`: Natural logarithm of the hammer price.\n- `R`: `Racing Starts`, the number of races the breeder started a horse in 1993.\n- `B`: `Breeding Starts + 1`, where `Breeding Starts` is the number of races started by horses bred by the seller.\n- `Race Zero`, `Breed Zero`: Indicator variables for sellers with zero racing or breeding starts, respectively.\n\n---\n\n## Data / Model Specification\n\nThe analysis compares a restricted model with an unrestricted version derived from an algebraic identity.\n\n**Restricted Model:**\n```latex\nln(Price_i) = \\alpha_0 + \\alpha_1 ln(R_i/B_i + 1) + \\gamma'X_i + \\epsilon_i\n```\n(Eq. 1)\n\n**Algebraic Identity:**\n```latex\nln(R/B + 1) = ln(R+B) - ln(B)\n```\n(Eq. 2)\n\n**Unrestricted Model:**\n```latex\nln(Price_i) = \\theta_0 + \\alpha_1^* ln(R_i+B_i) - \\beta^{**} ln(B_i) + \\gamma'X_i + \\nu_i\n```\n(Eq. 3)\n\n**Table 1: Selected Regression Coefficients for ln(Price)**\n| Specification | (3) Restricted | (4) Restricted | (5) Unrestricted | (6) Unrestricted |\n| :--- | :---: | :---: | :---: | :---: |\n| `Race/Breed Zero` Dummies | No | Yes | No | Yes |\n| `ln(R/B + 1)` | -0.1026 | -0.1404 | | |\n| `ln(R+B)` | | | -0.0638 | -0.1465 |\n| `-ln(B)` | | | -0.1200 | -0.1340 |\n| F-test (`\\alpha_1^* = \\beta^{**}`)| | | F = 9.58 | F = 0.068 |\n| Critical F (5%) | | | ~3.88 | ~3.88 |\n\n---\n\n## The Question\n\nBased on the models and results in Table 1, what is the correct linear restriction to test for the absence of scale effects, and what is the conclusion from the F-test comparing specifications (3) and (5)?",
    "Options": {
      "A": "Restriction: `α₁* = β**`. Conclusion: Reject the null; the restricted model (3) is misspecified because significant scale effects are present.",
      "B": "Restriction: `α₁* = -β**`. Conclusion: Reject the null; the restricted model (3) is misspecified because significant scale effects are present.",
      "C": "Restriction: `α₁* = β**`. Conclusion: Fail to reject the null; the restricted model (3) is valid and there are no scale effects.",
      "D": "Restriction: `α₁* = β**`. Conclusion: Reject the null; this directly confirms the presence of adverse selection in model (3)."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to identify the correct parameter restriction for a hypothesis test and interpret the result of an F-test from a regression table. It uses a Computational Judgment strategy. Distractors are designed based on common errors: sign error in the restriction (B), misinterpretation of statistical significance by incorrectly comparing the F-statistic to the critical value (C), and conflating the test's purpose (testing for scale effects) with the paper's main finding (D).",
    "qid": "209",
    "model_evaluation_result": {
      "model_raw_response": "C) Restriction: α₁* = α**. Conclusion: Fail t",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 210,
    "Question": "## Background\n\n**Research Question.** This problem examines a method to distinguish the effect of adverse selection from a potential confounding \"scale effect\" in the pricing of thoroughbred yearlings.\n\n**Setting / Institutional Environment.** A potential critique of the main finding is that the `Racing Intensity` variable may be correlated with the overall scale of a seller's operation. If larger breeding operations have better reputations and command higher prices, the estimated coefficient on `Racing Intensity` could be biased. This analysis re-parameterizes the model to test this possibility directly.\n\n**Variables & Parameters.**\n- `ln(Price)`: Natural logarithm of the hammer price.\n- `R`: `Racing Starts`, the number of races the breeder started a horse in 1993.\n- `B`: `Breeding Starts + 1`, where `Breeding Starts` is the number of races started by horses bred by the seller.\n- `Race Zero`, `Breed Zero`: Indicator variables for sellers with zero racing or breeding starts, respectively.\n\n---\n\n## Data / Model Specification\n\nThe analysis compares a restricted model with an unrestricted version derived from an algebraic identity.\n\n**Restricted Model:**\n```latex\nln(Price_i) = \\alpha_0 + \\alpha_1 ln(R_i/B_i + 1) + \\gamma'X_i + \\epsilon_i\n```\n(Eq. 1)\n\n**Algebraic Identity:**\n```latex\nln(R/B + 1) = ln(R+B) - ln(B)\n```\n(Eq. 2)\n\n**Unrestricted Model:**\n```latex\nln(Price_i) = \\theta_0 + \\alpha_1^* ln(R_i+B_i) - \\beta^{**} ln(B_i) + \\gamma'X_i + \\nu_i\n```\n(Eq. 3)\n\n**Table 1: Selected Regression Coefficients for ln(Price)**\n| Specification | (3) Restricted | (4) Restricted | (5) Unrestricted | (6) Unrestricted |\n| :--- | :---: | :---: | :---: | :---: |\n| `Race/Breed Zero` Dummies | No | Yes | No | Yes |\n| `ln(R/B + 1)` | -0.1026 | -0.1404 | | |\n| `ln(R+B)` | | | -0.0638 | -0.1465 |\n| `-ln(B)` | | | -0.1200 | -0.1340 |\n| F-test (`\\alpha_1^* = \\beta^{**}`)| | | F = 9.58 | F = 0.068 |\n| Critical F (5%) | | | ~3.88 | ~3.88 |\n\n---\n\n## The Question\n\nThe F-statistic for the scale effects test drops from 9.58 (in the model without zero-start dummies) to 0.068 (in the model with zero-start dummies). What is the most accurate interpretation of this change and its implication for the paper's adverse selection hypothesis?",
    "Options": {
      "A": "The initial scale effect was an artifact driven by sellers with zero racing or breeding starts; after controlling for them, the `Racing Intensity` variable is a valid measure, and the evidence for adverse selection holds.",
      "B": "The inclusion of the dummy variables invalidates the F-test, making the results unreliable; the presence of adverse selection is now uncertain.",
      "C": "The scale effect is real, but the dummy variables absorb it, showing that adverse selection only exists for sellers with non-zero starts.",
      "D": "The dramatic drop in the F-statistic indicates that the `Race Zero` and `Breed Zero` dummy variables are highly statistically significant."
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to synthesize results across different model specifications to understand a robustness check. It uses a Reverse-Reasoning strategy (inferring the cause of a statistical change). Distractors are designed to be plausible but incorrect interpretations: suggesting the control invalidates the test (B, Conceptual Opposite), a subtle misreading of the result (C, Almost Right), and stating a true but secondary point that misses the main conclusion (D, True but Irrelevant).",
    "qid": "210",
    "model_evaluation_result": {
      "model_raw_response": "B) The F-test is invalid due to the addition of the zero-start dummie variables.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 211,
    "Question": "### Background\n\n**Research Question.** This study investigates the behavioral responses of students to receiving a free home laptop, specifically examining how the intervention alters their patterns of computer access and use both at home and in other locations.\n\n**Setting / Institutional Environment.** In a randomized experiment in Lima, Peru, students within designated 'treatment schools' participated in lotteries for XO laptops intended for home use. The analysis compares outcomes for lottery winners (treatment) and non-winners (control) within the same schools.\n\n### Data / Model Specification\n\nThe causal effects are estimated using the following regression model, which includes class fixed effects (`\\mu_j`) and baseline controls (`\\mathbf{X_{ijk}}`):\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\delta Winner_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nTable 1 presents the estimates for the treatment effect `\\delta` on outcomes related to computer access and use.\n\n**Table 1: Effects on Computer Access and Use**\n\n| | Winners | Non-winners | Adjusted Difference (ITT) | Observations |\n| :--- | :---: | :---: | :---: | :---: |\n| **Access** | | | |\n| Computer or laptop at home | 0.89 | 0.50 | 0.40*** (0.03) | 2,851 |\n| **Use by place (last week)** | | | |\n| Home | 0.82 | 0.50 | 0.33*** (0.02) | 2,641 |\n| Internet cafe | 0.43 | 0.54 | -0.11*** (0.02) | 2,631 |\n| Friend's house | 0.20 | 0.23 | -0.02 (0.02) | 2,561 |\n| **Use by place (minutes yesterday)** | | | |\n| Home | 61.85 | 44.99 | 18.20*** (4.93) | 2,851 |\n| Internet cafe | 30.75 | 34.25 | -3.23 (3.67) | 2,851 |\n| Friend's house | 11.30 | 15.44 | -3.78* (2.06) | 2,851 |\n\n*Notes: The 'Adjusted Difference' column presents the estimated `δ`. Standard errors, clustered at the school level, are in parentheses. ***p<0.01, *p<0.10.*\n\nBased on the results in Table 1, which statement best describes the behavioral response of students who won a laptop?",
    "Options": {
      "A": "They substituted computer use at home for use in internet cafes, indicating the two are substitute goods.",
      "B": "They used the home laptop in addition to their previous computer activities, indicating the goods are complements.",
      "C": "The program increased computer use at home but had no significant effect on computer use elsewhere.",
      "D": "The total time spent using computers across all locations significantly decreased."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to interpret regression coefficients from a table to identify a key behavioral pattern (substitution). It uses a Scenario Application strategy. Distractors are designed based on common conceptual errors: confusing substitutes with complements (B), ignoring significant coefficients (C), and miscalculating the net effect (D).",
    "qid": "211",
    "model_evaluation_result": {
      "model_raw_response": "C) Theprogram increased computer useat home but had nosignificant effect on computeruse elsewhere.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 212,
    "Question": "### Background\n\n**Research Question.** This study investigates the behavioral responses of students to receiving a free home laptop, specifically examining how the intervention alters their patterns of computer access and use both at home and in other locations.\n\n**Setting / Institutional Environment.** In a randomized experiment in Lima, Peru, students within designated 'treatment schools' participated in lotteries for XO laptops intended for home use. The analysis compares outcomes for lottery winners (treatment) and non-winners (control) within the same schools.\n\n### Data / Model Specification\n\nThe causal effects are estimated using the following regression model, which includes class fixed effects (`\\mu_j`) and baseline controls (`\\mathbf{X_{ijk}}`):\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\delta Winner_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nTable 1 presents the estimates for the treatment effect `\\delta` on outcomes related to computer access and use.\n\n**Table 1: Effects on Computer Access and Use**\n\n| | Winners | Non-winners | Adjusted Difference (ITT) | Observations |\n| :--- | :---: | :---: | :---: | :---: |\n| **Access** | | | |\n| Computer or laptop at home | 0.89 | 0.50 | 0.40*** (0.03) | 2,851 |\n| **Use by place (last week)** | | | |\n| Home | 0.82 | 0.50 | 0.33*** (0.02) | 2,641 |\n| Internet cafe | 0.43 | 0.54 | -0.11*** (0.02) | 2,631 |\n| Friend's house | 0.20 | 0.23 | -0.02 (0.02) | 2,561 |\n| **Use by place (minutes yesterday)** | | | |\n| Home | 61.85 | 44.99 | 18.20*** (4.93) | 2,851 |\n| Internet cafe | 30.75 | 34.25 | -3.23 (3.67) | 2,851 |\n| Friend's house | 11.30 | 15.44 | -3.78* (2.06) | 2,851 |\n\n*Notes: The 'Adjusted Difference' column presents the estimated `δ`. Standard errors, clustered at the school level, are in parentheses. ***p<0.01, *p<0.10.*\n\nUsing the point estimates for 'minutes yesterday' in Table 1, what is the net change in a student's total daily computer usage (across Home, Internet cafe, and Friend's house) from winning the lottery?",
    "Options": {
      "A": "An increase of 11.19 minutes.",
      "B": "An increase of 18.20 minutes.",
      "C": "An increase of 25.21 minutes.",
      "D": "A decrease of 7.01 minutes."
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to perform a multi-step calculation from a table and arrive at a precise numerical answer. It uses a Computational Judgment strategy. Distractors are based on predictable calculation errors: ignoring parts of the formula (B), making a sign error (C), and using the wrong components of the formula (D).",
    "qid": "212",
    "model_evaluation_result": {
      "model_raw_response": "B) An Increase of 18,20 minutes.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 213,
    "Question": "### Background\n\n**Research Question.** This study investigates the behavioral responses of students to receiving a free home laptop, specifically examining how the intervention alters their patterns of computer access and use both at home and in other locations.\n\n**Setting / Institutional Environment.** In a randomized experiment in Lima, Peru, students within designated 'treatment schools' participated in lotteries for XO laptops intended for home use. The analysis compares outcomes for lottery winners (treatment) and non-winners (control) within the same schools.\n\n### Data / Model Specification\n\nThe causal effects are estimated using the following regression model, which includes class fixed effects (`\\mu_j`) and baseline controls (`\\mathbf{X_{ijk}}`):\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\delta Winner_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nTable 1 presents the estimates for the treatment effect `\\delta` on outcomes related to computer access and use.\n\n**Table 1: Effects on Computer Access and Use**\n\n| | Winners | Non-winners | Adjusted Difference (ITT) | Observations |\n| :--- | :---: | :---: | :---: | :---: |\n| **Access** | | | |\n| Computer or laptop at home | 0.89 | 0.50 | 0.40*** (0.03) | 2,851 |\n| **Use by place (last week)** | | | |\n| Home | 0.82 | 0.50 | 0.33*** (0.02) | 2,641 |\n| Internet cafe | 0.43 | 0.54 | -0.11*** (0.02) | 2,631 |\n| Friend's house | 0.20 | 0.23 | -0.02 (0.02) | 2,561 |\n| **Use by place (minutes yesterday)** | | | |\n| Home | 61.85 | 44.99 | 18.20*** (4.93) | 2,851 |\n| Internet cafe | 30.75 | 34.25 | -3.23 (3.67) | 2,851 |\n| Friend's house | 11.30 | 15.44 | -3.78* (2.06) | 2,851 |\n\n*Notes: The 'Adjusted Difference' column presents the estimated `δ`. Standard errors, clustered at the school level, are in parentheses. ***p<0.01, *p<0.10.*\n\nThe estimated effect of winning a laptop on having a 'Computer or laptop at home' is 0.40, not 1.0, because 50% of non-winners already had a computer. Given this and the evidence of substitution in Table 1, why might a voucher program for internet cafes be more cost-effective at increasing students' *total* hours of computer use than the laptop giveaway?",
    "Options": {
      "A": "The laptop program provides an inframarginal, in-kind transfer to many students who already have computers, while a voucher directly subsidizes the marginal cost of usage for all students.",
      "B": "Students prefer using computers in internet cafes over using them at home.",
      "C": "The XO laptops provided in the program were of lower quality than the computers available in internet cafes.",
      "D": "The laptop program failed to increase device-specific OLPC skills, making it ineffective."
    },
    "Answer": "A",
    "pi_justification": "This item assesses high-level application of microeconomic principles (inframarginal transfers, marginal cost) to evaluate a policy alternative, synthesizing multiple pieces of evidence from the table. It uses a Scenario Application strategy. Distractors include an unsupported claim (B), a true but irrelevant fact (C), and a factually incorrect statement about the study's other findings (D).",
    "qid": "213",
    "model_evaluation_result": {
      "model_raw_response": "C) The laptop provided in the XO program were of a lower quality than computers available in the internet cafes.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 214,
    "Question": "### Background\n\n**Research Question.** A key concern with the study's main within-school randomization is that the treatment effect estimates could be biased by spillovers. This analysis explores strategies to identify and quantify such spillovers.\n\n### Data / Model Specification\n\n**Strategy 1: Spillovers across Friendship Networks**\nThis strategy focuses on the sample of non-winners and compares those with friends who won a laptop to those without. The model is:\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\gamma Friend_{ijk} + \\lambda N_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nwhere `Friend` is an indicator for having a winner friend and `N` is the student's total number of friends, measured at baseline. The key assumption is that, conditional on `N`, `Friend` is as good as random.\n\nIn the 'Spillovers across Friendship Networks' analysis (Eq. 1), why is it crucial to control for `N_ijk`, the total number of friends a student has?",
    "Options": {
      "A": "To prevent omitted variable bias, as students with more friends are mechanically more likely to have a winner friend and may also have systematically different outcomes.",
      "B": "To ensure the sample is representative of the entire school population.",
      "C": "To measure the direct effect of popularity, which is the main outcome of interest.",
      "D": "Because the number of friends is likely affected by the treatment, making it a bad control."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of a specific econometric concept: the role of a control variable in preventing omitted variable bias in a network context. It uses a Reverse-Reasoning strategy (why is this control needed?). Distractors are based on confusing internal with external validity (B), confusing a control with an outcome (C), and misapplying the 'bad control' concept (D).",
    "qid": "214",
    "model_evaluation_result": {
      "model_raw_response": "A) Toprevent omitted variable bias... \n\nAnswer: A",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 215,
    "Question": "### Background\n\n**Research Question.** A key concern with the study's main within-school randomization is that the treatment effect estimates could be biased by spillovers. This analysis explores strategies to identify and quantify such spillovers.\n\n### Data / Model Specification\n\n**Strategy: Spillovers across Classmates (Difference-in-Differences)**\nThis strategy compares all non-winners in treatment schools to all students in control schools using baseline and follow-up data. This strategy is used because treatment and control schools were not balanced at baseline, as shown in Table 1.\n\n**Table 1: Balance for 'DiD' Strategy (All Students at Baseline)**\n\n| | Treatment schools | Control schools | Adjusted Difference |\n| :--- | :---: | :---: | :---: |\n| Raven's progressive matrices | -0.06 | 0.14 | -0.22*** (0.04) |\n\n*Notes: ***p<0.01.*\n\nBased on the evidence in Table 1, why was the Difference-in-Differences (DiD) strategy necessary for estimating spillovers on classmates, and what is its key identifying assumption?",
    "Options": {
      "A": "Because treatment and control schools were not balanced at baseline; the DiD assumption is that their outcomes would have followed parallel trends in the absence of the intervention.",
      "B": "Because attrition was high in the control schools; the DiD assumption is that attrition was random.",
      "C": "Because treatment and control schools were balanced at baseline; the DiD assumption is that this balance would persist over time.",
      "D": "Because the spillover effect was expected to be negative; the DiD assumption is that the treatment effect is constant across all students."
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to justify the choice of an identification strategy (DiD) based on a balance table and to correctly state its core assumption. It uses a Reverse-Reasoning strategy. Distractors are based on incorrect reasons for using DiD (B), factually incorrect premises from the table (C), and irrelevant logic paired with an incorrect assumption (D).",
    "qid": "215",
    "model_evaluation_result": {
      "model_raw_response": "A) Becausereatment and control schools werent balanced at baseline",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 216,
    "Question": "### Background\n\n**Research Question.** A key concern with the study's main within-school randomization is that the treatment effect estimates could be biased by spillovers, where non-winning students are affected by their peers who received laptops. This analysis explores two distinct strategies to identify and quantify such spillovers.\n\n### Data / Model Specification\n\nTwo strategies were used to test for spillovers:\n1.  **Friends Strategy:** Compared non-winners with winner friends to non-winners without winner friends. This analysis found no significant spillovers on academic or cognitive skills.\n2.  **DiD Strategy:** Compared non-winners in treatment schools to students in control schools over time. The results are in Table 1.\n\n**Table 1: DiD Spillover Effects on Non-winners in Treatment Schools**\n\n| Outcome | DiD Estimate (`η`) |\n| :--- | :---: |\n| Objective OLPC test | 0.24** (0.11) |\n| Math | -0.16 (0.17) |\n| Cognitive skills (Raven's) | 0.09 (0.06) |\n\n*Notes: **p<0.05.*\n\nA critic claims the study's main null effects on academic skills are biased towards zero due to positive spillovers contaminating the control group. Based on the provided analyses, what is the strongest rebuttal to this claim?",
    "Options": {
      "A": "The DiD analysis, the most comprehensive spillover test, found no statistically significant spillovers on cognitive or academic skills, even while it successfully detected spillovers on device-specific skills.",
      "B": "The 'Friends' analysis proved that spillovers only occur between close friends, so the broader control group was unaffected.",
      "C": "Both spillover analyses found no evidence of any spillovers whatsoever, for any outcome.",
      "D": "The main analysis already controlled for spillovers by including class fixed effects."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to synthesize evidence from multiple, complex analyses to construct a coherent scientific argument. It uses a Scenario Application strategy (rebutting a critique). Distractors include an incomplete argument (B), a factually incorrect summary of the results (C), and a fundamental econometric misconception about the role of fixed effects (D).",
    "qid": "216",
    "model_evaluation_result": {
      "model_raw_response": "A) The `DiD` analysis, the `most comprehensive spilloover test`, found no statistically `significant` spillovers `on cognitive or academic` skills, even `while` it successfully detected `spillovers on` device-specific skills.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 217,
    "Question": "### Background\n\nThis question analyzes the second-period pricing game in a duopoly model with switching costs. The setting is a standard spatial differentiation model (a Hotelling line of length *t*) where two firms, A and B, are located at the endpoints. In the second period, firms have established first-period market shares, *σ*ᴬ and *σ*ᴮ, and they set prices *p*₂ᴬ and *p*₂ᴮ simultaneously. Firms have a constant marginal cost *c*. Consumers incur a switching cost *s* if they change suppliers. The consumer base is heterogeneous: a fraction *ν* are new, a fraction *μ* have tastes that are now independent of their first-period location, and the remainder have unchanged tastes.\n\n### Data / Model Specification\n\nFirm A's second-period profit is *π*₂ᴬ = (*p*₂ᴬ - *c*)*q*₂ᴬ. Its total second-period sales, *q*₂ᴬ, are the sum of sales to the different consumer groups, which can be expressed as:\n```latex\nq_{2}^{A}(p_{2}^{A},p_{2}^{B}) = \\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right] \\quad \\text{(Eq. 1)}\n```\nAssume that the parameters are such that an interior solution exists and *μ*+*ν* > 0.\n\n### Question\n\nIn the second-period asymmetric equilibrium, what is the relationship between a firm's first-period market share (*σ*ᴬ) and its second-period price (*p*₂ᴬ), and what is the economic intuition behind it?",
    "Options": {
      "A": "The firm with the larger market share charges a higher price, as it has a greater incentive to exploit its large base of locked-in customers.",
      "B": "The firm with the larger market share charges a lower price to aggressively defend its position against its smaller rival.",
      "C": "Both firms charge the same price regardless of market share, because second-period competition eliminates any first-period advantage.",
      "D": "The firm with the larger market share charges a higher price, but this effect diminishes as the switching cost (*s*) increases."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses understanding of the core trade-off in the second-period asymmetric equilibrium, combining the result of a derivation with its economic interpretation. It uses a Computational Judgment strategy. Distractors include a conceptual opposite (B), an incorrect generalization of the symmetric case (C), and an 'almost right' statement with an incorrect relationship to a key parameter (D).",
    "qid": "217",
    "model_evaluation_result": {
      "model_raw_response": "D) The firme with the larger.market share charges a.higher price, but.this effect diminishes.as the.switching cost(*s*) increases",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 218,
    "Question": "### Background\n\nThis question analyzes the second-period pricing game in a duopoly model with switching costs. The setting is a standard spatial differentiation model (a Hotelling line of length *t*) where two firms, A and B, are located at the endpoints. In the second period, firms have established first-period market shares, *σ*ᴬ and *σ*ᴮ, and they set prices *p*₂ᴬ and *p*₂ᴮ simultaneously. Firms have a constant marginal cost *c*. Consumers incur a switching cost *s* if they change suppliers. The consumer base is heterogeneous: a fraction *ν* are new, a fraction *μ* have tastes that are now independent of their first-period location, and the remainder have unchanged tastes.\n\n### Data / Model Specification\n\nFirm A's second-period sales are given by:\n```latex\nq_{2}^{A}(p_{2}^{A},p_{2}^{B}) = \\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right] \\quad \\text{(Eq. 1)}\n```\nThe resulting Nash equilibrium price for Firm A is:\n```latex\np_{2}^{A} = c + \\frac{1}{\\mu+\\nu} \\left[ t + \\frac{1}{3}(2\\sigma^{A}-1)((1-\\mu-\\nu)t+\\mu s) \\right] \\quad \\text{(Eq. 2)}\n```\nAssume that the parameters are such that an interior solution exists and *μ*+*ν* > 0.\n\n### Question\n\nConsider a scenario where all consumers are 'mobile' (*μ*+*ν*=1), but the market is asymmetric, with Firm A holding a larger first-period market share (*σ*ᴬ > 1/2). Which statement best describes the equilibrium prices?",
    "Options": {
      "A": "Firm A sets a higher price than Firm B, and the price difference is directly proportional to the switching cost *s*.",
      "B": "Both firms set the same price, `p₂ = c + t`, because the effect of switching costs is completely nullified when *μ*+*ν*=1.",
      "C": "Firm A sets a lower price than Firm B to leverage its dominant position and further increase its market share.",
      "D": "The price difference between the firms is determined by the transport cost *t*, not the switching cost *s*, which becomes irrelevant."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This high-difficulty item assesses the student's ability to synthesize two key results and avoid overgeneralization. It uses a Reverse-Reasoning strategy. The distractors are designed to trap common errors: (B) incorrectly generalizes the symmetric result to the asymmetric case, (C) proposes an incorrect strategic motive, and (D) misattributes the source of the price difference in this specific scenario.",
    "qid": "218",
    "model_evaluation_result": {
      "model_raw_response": "C) Firm А sets a lowerprice than Firm B.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 219,
    "Question": "### Background\n\n**Research Question.** This problem assesses the empirical evidence for the hypothesis that the quality of a country's judiciary mediates the relationship between economic unpredictability and firm growth, using a fixed-effects OLS model.\n\n**Setting / Institutional Environment.** The analysis uses a country fixed-effects model on a cross-section of firms from the World Business Environment Survey (WBES, 1999-2000). This approach controls for all time-invariant country-level characteristics.\n\n**Variables & Parameters.**\n*   `FirmGrowth`: The percentage change in a firm's sales over the preceding 3 years.\n*   `EconomicUnpred`: A firm's perception of economic unpredictability, on a scale from 1 (completely predictable) to 6 (completely unpredictable).\n*   `Judiciary`: A firm's perception of how problematic the judiciary is for its business, on a scale from 1 (no obstacle) to 4 (major obstacle).\n\n---\n\n### Data / Model Specification\n\nThe estimated country fixed-effects model is:\n\n```latex\nFirmGrowth_{ij} = \\alpha_i + ... + \\gamma_8 Judiciary_{j} + \\gamma_9 EconomicUnpred_{ij} + \\gamma_{10} (EconomicUnpred_{ij} \\times Judiciary_j) + u_{ij} \n```\n\nwhere `i` is the country index and `j` is the firm index. `α_i` represents the country fixed effects.\n\n**Table 1. Volatility and Firm Growth (Selected Results from Table 4, Column 2)**\n\n| Variable                      | Coefficient (Std. Err.) |\n| :---------------------------- | :---------------------- |\n| `Judiciary`                   | 0.029 (0.023)           |\n| `EconomicUnpred`              | 0.014 (0.012)           |\n| `EconomicUnpred` x `Judiciary`| -0.012*** (0.004)       |\n| *Overall effect: EconUnpred*  | *-0.019** (0.006)*      |\n| Observations                  | 4325                    |\n\n*Notes: The dependent variable is `FirmGrowth`. Model includes country fixed effects and other firm-level controls. *** p<0.01, ** p<0.05.*\n\n---\n\nUsing the results in Table 1, what is the estimated marginal effect of a one-unit increase in `EconomicUnpred` on `FirmGrowth` for a firm that reports the judiciary is a 'major obstacle' (`Judiciary` = 4)?",
    "Options": {
      "A": "A 3.4 percentage point decrease in the 3-year sales growth rate.",
      "B": "A 3.4 percentage point increase in the 3-year sales growth rate.",
      "C": "A 1.2 percentage point decrease in the 3-year sales growth rate.",
      "D": "A 1.9 percentage point decrease in the 3-year sales growth rate."
    },
    "Answer": "A",
    "pi_justification": "This 'Computational Judgment' item assesses the core skill of calculating and interpreting a marginal effect from an interaction model, which was the focus of the original QA's first two parts. The distractors are designed based on high-frequency errors: B (sign error in calculation), C (omitting the main effect from the marginal effect formula), and D (misinterpreting a different reported statistic from the table as the relevant one).",
    "qid": "219",
    "model_evaluation_result": {
      "model_raw_response": "D) A) 1.7 percentage point decrease",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 220,
    "Question": "### Background\n\n**Research Question.** A key concern in this analysis is that the measure of `EconomicUnpred` could be endogenous due to reverse causality.\n\n---\n\n### Data / Model Specification\n\nConsider a simplified model without the interaction term:\n\n```latex\nFirmGrowth = \\beta_0 + \\beta_1 EconomicUnpred + ... + \\epsilon\n```\n\n---\n\nSuppose that managers of firms with intrinsically low growth prospects (e.g., due to unobserved poor management, leading to a negative error term `ε`) become pessimistic and are thus more likely to report high levels of `EconomicUnpred`. What is the direction of the resulting bias on the OLS estimator `β̂₁`?",
    "Options": {
      "A": "The OLS estimate `β̂₁` will be biased to be more negative than the true causal effect `β₁`.",
      "B": "The OLS estimate `β̂₁` will be biased to be less negative (closer to zero) than the true causal effect `β₁`.",
      "C": "The OLS estimate `β̂₁` will be biased towards zero, regardless of the true effect.",
      "D": "The OLS estimate `β̂₁` will be biased, but the direction of the bias cannot be determined."
    },
    "Answer": "A",
    "pi_justification": "This 'Reverse-Reasoning' item tests the ability to deduce the statistical consequences of endogeneity, the focus of the original QA's third part. Given a specific economic story for reverse causality, the student must identify the direction of the resulting OLS bias. Distractors target common confusions: B (incorrectly identifying a positive bias), C (a common but imprecise heuristic), and D (failing to see that the direction is in fact determined by the premise).",
    "qid": "220",
    "model_evaluation_result": {
      "model_raw_response": "B) The OLs estimate β̂₁ will be biased toward being less negative (closes to zero) then the true causal affect β₁.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 221,
    "Question": "### Background\n\n**Research Question.** This question tests the ability to apply a specific formula from a stochastic frontier analysis (SFA) model to determine the nature of product synergies in banking.\n\n**Setting / Institutional Environment.** The analysis uses a translog cost function to model the efficient cost frontier for U.S. banks. The parameters of this function reveal characteristics of the underlying production technology, such as economies of diversification (also known as scope economies).\n\n### Data / Model Specification\n\nThe model uses a translog functional form for the cost frontier, `ln c(y,w,z)`. Economies of diversification are measured by cost complementarities. A negative value of the cross-partial derivative `∂²c / ∂yᵢ∂yⱼ` indicates cost complementarity (economies of diversification). For the translog model, this is approximated at the sample mean by:\n\n```latex\n\\frac{\\partial^2 c}{\\partial y_i \\partial y_j} \\approx \\alpha_{i} \\alpha_{j} + \\alpha_{i j} \\quad \\text{(Eq. 1)}\n```\n\n**Table 1: Selected Parameter Estimates**\n\n| Parameter | Description | Estimate |\n| :--- | :--- | :--- |\n| `\\alpha_1` | Coeff on ln(demand deposits, `y₁`) | 0.316 |\n| `\\alpha_2` | Coeff on ln(time deposits, `y₂`) | 0.577 |\n| `\\alpha_{12}` | Coeff on ln(demand deposits) * ln(time deposits) | -0.047 |\n\n### Question\n\nUsing the provided model specification and parameter estimates, what is the nature of cost complementarity between demand deposits (`y₁`) and time deposits (`y₂`)?\n",
    "Options": {
      "A": "Diseconomies of diversification exist, because the complementarity measure is approximately +0.135.",
      "B": "Economies of diversification exist, because the complementarity measure is approximately +0.135.",
      "C": "Economies of diversification exist, because the complementarity measure is approximately -0.047.",
      "D": "Diseconomies of diversification exist, because the complementarity measure is approximately +0.846."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a specific calculation based on a model's functional form and interpret the result in economic terms. It uses a Computational Judgment strategy. Distractors are designed to trap common errors: (B) Conceptual Opposite: correctly calculating the value but misinterpreting the sign's implication for economies vs. diseconomies; (C) Step-Omission Error: using only the interaction term `α₁₂` instead of the full formula; (D) Formula Misuse: adding `α₁` and `α₂` instead of multiplying them.",
    "qid": "221",
    "model_evaluation_result": {
      "model_raw_response": "C) EconomIES OF DIVERSIFICATION EXIST, BECAUSE THE COMPLEMENTARITY MEASURE IS APPROXIMATELY -0.046.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 222,
    "Question": "### Background\n\n**Research Question.** This question assesses a deep understanding of potential identification problems in stochastic frontier analysis (SFA), specifically how misspecification of one error component can bias the estimation of another.\n\n**Setting / Institutional Environment.** A cross-section of U.S. banks is analyzed using an SFA model. The model decomposes the deviation of a bank's observed cost from the efficient frontier into multiple components, including technical and allocative inefficiency.\n\n### Data / Model Specification\n\nThe econometric model for observed log cost for bank `s` is:\n\n```latex\n\\ln(w'\\cdot x)_{s} = \\ln c(y_s, w_s, z_s) + T_{s} + A + u_{o s} \\quad \\text{(Eq. 1)}\n```\n\nwhere `ln c(...)` is the efficient cost frontier, `Tₛ ≥ 0` is technical inefficiency, `A ≥ 0` is the cost of allocative inefficiency, and `uₒₛ` is random noise. A key assumption in the paper's model is that `A` is constant for all banks in the sample.\n\n### Question\n\nSuppose the model's assumption is wrong. In reality, large banks are perfectly allocatively efficient (their true `A` is 0), while small banks are not. If an econometrician estimates the misspecified model assuming a constant `A` for all banks, what will be the direction of bias in the estimated technical inefficiency, `T̂ₛ`, for the large, allocatively efficient banks?",
    "Options": {
      "A": "Biased downwards.",
      "B": "Biased upwards.",
      "C": "Unbiased.",
      "D": "The direction of bias cannot be determined."
    },
    "Answer": "A",
    "pi_justification": "This high-difficulty item assesses deep understanding of model identification and how misspecification of one error component can bias another. It uses a Scenario Application strategy. The correct answer requires reasoning that the model will estimate an average `Â > 0` and impose it on the large banks (whose true `A` is 0), forcing the estimate of the other one-sided error term, `T̂ₛ`, to be lower to compensate. Distractors are based on high-fidelity misconceptions: (B) the conceptual opposite (upward bias); (C) a naive 'no effect' assumption; and (D) an appeal to ambiguity, which is incorrect as the direction of bias is determinate in this scenario.",
    "qid": "222",
    "model_evaluation_result": {
      "model_raw_response": "B) Biases upwards. \n\n(Note: I'm only providing the answer, not the explanation.)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 223,
    "Question": "### Background\n\nAn economy features risk-averse workers and risk-neutral firms. Labor services are traded in a **spot market** where wages and prices are fully flexible. The economy is subject to real technology shocks (`s_t`) and nominal money supply shocks (`x_t`).\n\n### Data / Model Specification\n\nThe equilibrium price level and nominal wage in the spot market are given by:\n\n```latex\np_t(s,x) = \\frac{m_{t-1}x_t}{F g(s_t)} \\quad \\text{(Eq. 1)}\n```\n\n```latex\nw_t(s,x) = \\frac{m_{t-1}x_t \\phi(s_t)}{F} \\quad \\text{(Eq. 2)}\n```\n\nwhere `g(s) = f(\\gamma, s)` and `\\phi(s) = f_l(\\gamma, s) / f(\\gamma, s)`. A worker from generation `t` earns `w_t` in their youth and consumes `c^s = w_t x_{t+1} / p_{t+1}` in their old age.\n\n### Question\n\nGiven the spot market equilibrium, which of the following correctly states the real consumption `c^s` for a worker of generation `t` and accurately describes its properties?",
    "Options": {
      "A": "`c^s = \\phi(s_t) g(s_{t+1})`. Consumption is independent of nominal shocks (`x_t`, `x_{t+1}`), so money is neutral.",
      "B": "`c^s = k \\bar{\\phi} g(s_{t+1}) / x_t`. Consumption depends on the nominal shock `x_t`, so money is not neutral.",
      "C": "`c^s = \\phi(s_t) g(s_{t+1}) / x_t`. Consumption depends on the nominal shock `x_t` due to price level effects.",
      "D": "`c^s = \\bar{\\phi} g(s_{t+1})`. The worker is fully insured against the youth-period technology shock `s_t`."
    },
    "Answer": "A",
    "pi_justification": "This item replaces part of a QA problem (Suitability Score: 9.5). It uses the Computational Judgment strategy, requiring the student to perform the derivation for spot market consumption and identify its core properties. Distractors are designed to catch common errors: (B) confusing the spot market with the contract market outcome, (C) an algebraic error where a term fails to cancel, and (D) misattributing the insurance properties of a contract market to the spot market.",
    "qid": "223",
    "model_evaluation_result": {
      "model_raw_response": "C) `C` \n\n(Note: The answer is C, as consumption depends on the price level effect of the nominal shock x_t.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 224,
    "Question": "### Background\n\nAn economy features risk-averse workers and risk-neutral firms. Labor services are traded via **fixed nominal wage contracts**, where the wage is set before the period's shocks are realized. The economy is subject to real technology shocks (`s_t`) and nominal money supply shocks (`x_t`).\n\n### Data / Model Specification\n\nThe pre-determined equilibrium nominal wage is:\n\n```latex\n\\bar{w}_t = \\frac{m_{t-1}k\\bar{\\phi}}{F} \\quad \\text{(Eq. 1)}\n```\n\nwhere `k \\equiv 1/E(1/x)` and `\\bar{\\phi} \\equiv E_s[\\phi(s)]`. The price function is the same as in the spot market: `p_t(s,x) = m_{t-1}x_t / (F g(s_t))`.\n\nA worker from generation `t` earns `\\bar{w}_t` in their youth and consumes `c^{\\text{RW}} = \\bar{w}_t x_{t+1} / p_{t+1}` in their old age.\n\n### Question\n\nGiven the fixed nominal wage contract equilibrium, which of the following correctly states the real consumption `c^{\\text{RW}}` for a worker of generation `t` and accurately describes its properties?",
    "Options": {
      "A": "`c^{\\text{RW}} = \\phi(s_t) g(s_{t+1})`. Consumption is independent of nominal shocks, so money is neutral.",
      "B": "`c^{\\text{RW}} = k \\bar{\\phi} g(s_{t+1}) / x_t`. Consumption is independent of the youth-period technology shock `s_t` but depends on the nominal shock `x_t`, so money is not neutral.",
      "C": "`c^{\\text{RW}} = k \\bar{\\phi} g(s_{t+1})`. Consumption is fully insured against both youth-period technology shocks and nominal shocks.",
      "D": "`c^{\\text{RW}} = k \\phi(s_t) g(s_{t+1}) / x_t`. The contract wage depends on the realized technology shock `s_t`."
    },
    "Answer": "B",
    "pi_justification": "This item replaces part of a QA problem (Suitability Score: 9.5). It uses the Computational Judgment strategy, requiring the student to derive consumption under a fixed nominal wage and identify its key properties regarding risk-sharing and monetary non-neutrality. Distractors are designed to catch common errors: (A) confusing the contract market with the spot market outcome, (C) an incomplete derivation that ignores the effect of `x_t` on the price level, and (D) a misunderstanding of the nature of a fixed-wage contract (using `\\phi(s_t)` instead of its expectation `\\bar{\\phi}`).",
    "qid": "224",
    "model_evaluation_result": {
      "model_raw_response": "B) `C^{\\text RW} = k \\overline{\\phi}g(s_{t + 1}) /x_t`.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 225,
    "Question": "### Background\n\nIn an economy with real (technology, `s`) and nominal (money, `x`) shocks, risk-averse workers can trade labor with risk-neutral firms in one of two market structures:\n1.  **Spot Market:** Provides consumption `c^s = \\phi(s_t) g(s_{t+1})`, which is exposed to real shocks but insured against nominal shocks.\n2.  **Fixed Nominal Wage Contract:** Provides consumption `c^{\\text{RW}} = k \\bar{\\phi} g(s_{t+1}) / x_t`, which is insured against youth-period real shocks (`s_t`) but exposed to nominal shocks (`x_t`).\n\n### Question\n\nConsider the choice between these two market structures from the perspective of a risk-averse worker. If the central bank's policy leads to a mean-preserving increase in the variance of the nominal shock `x_t`, what is the most likely consequence for the optimal market structure?",
    "Options": {
      "A": "The fixed nominal wage contract becomes relatively more attractive because it provides better insurance against real shocks.",
      "B": "The spot market becomes relatively more attractive because it perfectly insures the worker against the now more volatile nominal shocks.",
      "C": "The relative attractiveness of the two structures is unchanged because the mean of the nominal shock is constant.",
      "D": "Both market structures become less attractive, but the fixed nominal wage contract is affected less because firms are risk-neutral."
    },
    "Answer": "B",
    "pi_justification": "This item replaces part of a QA problem (Suitability Score: 9.5). It uses the Scenario Application strategy to test the student's understanding of the core trade-off between the two market structures. The question assesses whether the student can correctly infer how the desirability of each insurance mechanism changes when the variance of a specific shock increases. Distractors represent classic misconceptions: (A) Conceptual Opposite, (C) ignoring the role of variance for risk-averse agents, and (D) flawed reasoning about who bears the risk.",
    "qid": "225",
    "model_evaluation_result": {
      "model_raw_response": "C) Therelative attractiveness of thetwo structures is unchangedbecause the mean of thenominal shock is constant.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 226,
    "Question": "### Background\n\nThis problem examines the core behavioral foundations of a dynamic commodity market model, focusing on the optimal decisions of the two key types of forward-looking private agents: producers and arbitrageurs. Their actions, driven by rational expectations of future prices, jointly determine the intertemporal allocation of the commodity.\n\nIn an annual market model, competitive, expected profit-maximizing arbitrageurs decide how much to store (`x_t`) for the next period, while rational producers decide how much acreage (`a_t`) to plant for the next period's harvest. Both decisions are made at time `t` based on the same rationally-formed expectation of the price at time `t+1`, `p_{t+1}^{e}`.\n\n### Data / Model Specification\n\nThe arbitrageur's decision is governed by the intertemporal arbitrage conditions given in Eq. (1):\n\n```latex\nx_{t}\\geq0, \\quad p_{t}\\geq\\delta p_{t+1}^{e}-\\kappa, \\quad p_{t}>\\delta p_{t+1}^{e}-\\kappa\\Rightarrow x_{t}=0.\n```\n\nThe producer's decision is captured by the acreage supply function in Eq. (2), which has a positive elasticity with respect to the expected price:\n\n```latex\na_{t}=\\alpha{\\left(p_{t+1}^{e}\\right)}\n```\n\nSuppose there is a permanent, unexpected decrease in the unit storage cost, `κ`. Based on the model's general equilibrium logic, what is the most likely impact on the decisions made at time `t`?",
    "Options": {
      "A": "Private storage (`x_t`) increases, while acreage planted (`a_t`) remains unchanged.",
      "B": "Both private storage (`x_t`) and acreage planted (`a_t`) increase.",
      "C": "Private storage (`x_t`) decreases, while acreage planted (`a_t`) increases.",
      "D": "Acreage planted (`a_t`) decreases, while private storage (`x_t`) increases."
    },
    "Answer": "D",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the user's ability to trace a parameter shock through a rational expectations general equilibrium model. It uses a Scenario Application strategy. The distractors are designed to diagnose specific reasoning errors: (A) represents a partial equilibrium error, ignoring the feedback effect on expected price; (B) correctly identifies the direction of the storage change but gets the direction of the price expectation change wrong; (C) gets both effects backward.",
    "qid": "226",
    "model_evaluation_result": {
      "model_raw_response": "C) Private storeage (`x_t`)) decreases, while aceage planted (`at`) increases. \n\n(Note: The output is a single letter \"C\")",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 227,
    "Question": "### Background\n\nThis problem examines the core behavioral foundations of a dynamic commodity market model, focusing on the optimal decisions of forward-looking private arbitrageurs. Their actions, driven by rational expectations of future prices, determine the intertemporal allocation of the commodity.\n\nIn an annual market model, competitive, expected profit-maximizing arbitrageurs decide how much to store (`x_t`) for the next period based on the current price `p_t` and their rational expectation of the price at time `t+1`, `p_{t+1}^{e}`.\n\n### Data / Model Specification\n\nThe arbitrageur's decision is governed by the intertemporal arbitrage conditions given in Eq. (1):\n\n```latex\nx_{t}\\geq0, \\quad p_{t}\\geq\\delta p_{t+1}^{e}-\\kappa, \\quad p_{t}>\\delta p_{t+1}^{e}-\\kappa\\Rightarrow x_{t}=0.\n```\n\nKey model parameters are:\n- Annual discount factor: `δ = 0.916`\n- Unit storage cost: `κ = $0.36` per bushel\n\nAt time `t`, the current market price is `p_t = $5.50`. Agents rationally expect the price next year to be `p_{t+1}^{e} = $6.50`. Given the model's parameters, what is the optimal action for private arbitrageurs?",
    "Options": {
      "A": "Store positive quantities of the commodity (`x_t > 0`), as the current price is below the expected net return from storage.",
      "B": "Do not store any of the commodity (`x_t = 0`), as the current price is above the expected net return from storage.",
      "C": "Do not store any of the commodity (`x_t = 0`), because the undiscounted expected profit (`p_{t+1}^{e} - κ`) is the relevant metric.",
      "D": "Do not store any of the commodity (`x_t = 0`), because the storage cost `κ` should be added to, not subtracted from, the discounted expected price."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the user's ability to apply the core no-arbitrage condition. It uses a Computational Judgment strategy. The correct answer requires calculating the expected net return from storage (`δ * p_{t+1}^{e} - κ = $5.594`) and comparing it to the current price (`$5.50`). Distractors are based on plausible errors: (B) misinterprets the result of the comparison; (C) is a Step-Omission Error (ignores discounting); (D) is a Sign Error in the formula.",
    "qid": "227",
    "model_evaluation_result": {
      "model_raw_response": "C) Do Not Store Any of the Commodity (`x_t=0`)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 228,
    "Question": "### Background\n\n**Research Question.** This problem concerns the identification of the causal effects of childcare prices and wages on a single mother's employment and childcare choices, addressing endogeneity and unobserved heterogeneity.\n\n**Setting and Sample.** The analysis uses a discrete choice model where the indirect utility of each choice is a function of potentially endogenous prices and wages.\n\n### Data / Model Specification\n\nThe model consists of a main utility equation and auxiliary equations for price and wages.\n\n**1. Indirect Utility Function:** The linear approximation of the indirect utility for alternative `i` is:\n```latex\nV_{i} = X\\beta_{i} + \\alpha_{P i}P_{s}^{*} + \\alpha_{PT i}W_{PT} + \\alpha_{FT i}W_{FT} + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n```\nwhere `P_s*` is the effective price of childcare, `W_PT` and `W_FT` are part-time and full-time wages, and `\\varepsilon_i` is an error term.\n\n**2. Identification of Price and Wages:** To address the endogeneity of price and wages, the model uses state of residence indicators (`D_s`) as instruments in reduced-form equations:\n```latex\nP_{s} = D_{s}\\delta_{s} + X\\delta_{P} + \\xi_{P} \\quad \\text{(Eq. 2)}\n```\n\n### The Question\n\nBased on the model specification and economic theory, which of the following statements is correct?",
    "Options": {
      "A": "For an alternative involving full-time work and paid care, `\\alpha_{Pi}` is expected to be negative, and a valid instrument `D_s` must be uncorrelated with the utility error term `\\varepsilon_i`.",
      "B": "For an alternative involving full-time work and paid care, `\\alpha_{Pi}` is expected to be negative, and a valid instrument `D_s` must be correlated with the childcare price `P_s` but not with the wage `W_{FT}`.",
      "C": "For an alternative involving full-time work and paid care, `\\alpha_{Pi}` is expected to be positive, and a valid instrument `D_s` must be uncorrelated with the utility error term `\\varepsilon_i`.",
      "D": "For an alternative involving full-time work and paid care, `\\alpha_{Pi}` is expected to be positive, and a valid instrument `D_s` must be correlated with the childcare price `P_s`."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's understanding of the model's theoretical predictions (sign of coefficients) and the fundamental assumptions of the identification strategy (exclusion restriction). The distractors are designed to isolate misunderstandings of theory versus econometrics. Distractor B (Conceptual Opposite) confuses the exclusion restriction with the relevance condition and adds an unnecessary constraint. Distractor C (Sign Error) reverses the expected sign of the price coefficient. Distractor D combines both errors.",
    "qid": "228",
    "model_evaluation_result": {
      "model_raw_response": "B) For all alternatives involving full-time care, `\\α_{Pi}` can be expected to be either positive or negative, depending on the specific context, and a validated instrument `D` must be associated with the childcare cost `P` but not the wage `w`.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 229,
    "Question": "### Background\n\n**Research Question.** This problem concerns the identification of the causal effects of childcare prices and wages on a single mother's employment and childcare choices, addressing endogeneity and unobserved heterogeneity.\n\n**Setting and Sample.** The analysis uses a discrete choice model where the indirect utility of each choice is a function of potentially endogenous prices and wages.\n\n### Data / Model Specification\n\nThe model consists of a main utility equation and auxiliary equations for price and wages.\n\n**1. Indirect Utility Function:** The linear approximation of the indirect utility for alternative `i` is:\n```latex\nV_{i} = X\\beta_{i} + \\alpha_{P i}P_{s}^{*} + \\alpha_{PT i}W_{PT} + \\alpha_{FT i}W_{FT} + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n```\n**2. Identification of Price:** To address the endogeneity of price, the model uses state of residence indicators (`D_s`) as instruments.\n\n### The Question\n\nThe key identifying assumption is the exclusion restriction, `Cov(D_s, \\varepsilon_i | X) = 0`. Suppose this is violated because states (`D_s`) with a high cost of living (and thus high childcare prices, `P_s`) also have more generous welfare benefits, which directly increases the utility of the 'No-work' alternative, thereby lowering the relative utility of working alternatives. What is the resulting direction of bias on the estimated childcare price coefficient, `\\hat{\\alpha}_{Pi}`?",
    "Options": {
      "A": "Biased away from zero (more negative), because the instrument is negatively correlated with the structural error term.",
      "B": "Biased towards zero, because the instrument is negatively correlated with the structural error term.",
      "C": "Biased away from zero (more negative), because the instrument is positively correlated with the structural error term.",
      "D": "There is no bias, as this violation affects the utility of a different alternative ('No-work')."
    },
    "Answer": "A",
    "pi_justification": "This is a Reverse-Reasoning item that tests a deep understanding of instrumental variable bias. The student must correctly deduce the sign of the correlation between the instrument and the structural error (`Cov(Z,u) < 0`) and combine it with the sign of the first-stage correlation (`Cov(Z,X) > 0`) to determine the direction of bias. Distractors are designed based on a matrix of common errors: B confuses the direction of bias, C misidentifies the sign of the exclusion violation, and D reflects a fundamental misunderstanding of how exclusion violations cause bias in a relative choice model.",
    "qid": "229",
    "model_evaluation_result": {
      "model_raw_response": "C) Biases away from zero. \n\nNote: The correct answer is C.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 230,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the theoretical model of a single mother's discrete choice among employment and childcare alternatives.\n\n**Setting and Sample.** A single mother makes a static choice from a set of mutually exclusive alternatives. The choice is modeled within a Random Utility Maximization (RUM) framework, where utility depends on the chosen alternative's attributes.\n\n### Data / Model Specification\n\nThe choice set and corresponding budget constraints are given in Table 1.\n\n**Table 1: List of Alternatives and Budget Constraints**\n\n| Alternative | Employment Status | Childcare Payment | Subsidy Status | Budget Constraint |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | No-work | — | — | `C = N` |\n| 2 | Part-time | Yes | Yes | `C + (P_s - S)H_p = W_{PT}H_p + N` |\n| 4 | Part-time | Yes | No | `C + P_sH_p = W_{PT}H_p + N` |\n\n*Where C=Consumption, N=Non-wage income, P_s=Price of care, S=Subsidy, H_p=Hours of paid care, W_PT=Part-time wage.*\n\nThe indirect utility for alternative `i` is `V_i = v_i + \\varepsilon_i`, where `v_i` is the observable component and `\\varepsilon_i` is the unobserved component. The mother chooses alternative `i` if `V_i > V_j` for all other alternatives `j`.\n\n### The Question\n\nConsidering the budget constraints in Table 1, how does a childcare subsidy `S` affect a mother's choice, and what is the fundamental condition for choosing alternative `i` over `j`?",
    "Options": {
      "A": "The subsidy `S` reduces the effective price of childcare, and alternative `i` is chosen if the difference in unobserved utility (`\\varepsilon_j - \\varepsilon_i`) is less than the difference in observed utility (`v_i - v_j`).",
      "B": "The subsidy `S` reduces the effective price of childcare, and alternative `i` is chosen if the difference in unobserved utility (`\\varepsilon_i - \\varepsilon_j`) is less than the difference in observed utility (`v_i - v_j`).",
      "C": "The subsidy `S` acts as a lump-sum transfer increasing non-wage income `N`, and alternative `i` is chosen if the difference in unobserved utility (`\\varepsilon_j - \\varepsilon_i`) is less than the difference in observed utility (`v_i - v_j`).",
      "D": "The subsidy `S` acts as a lump-sum transfer increasing non-wage income `N`, and alternative `i` is chosen if the observed utility `v_i` is greater than `v_j`."
    },
    "Answer": "A",
    "pi_justification": "This item tests the student's ability to interpret the model's budget constraints and perform a basic derivation within the Random Utility framework. The distractors are designed to separate errors in economic interpretation (C) from errors in algebraic manipulation (B).",
    "qid": "230",
    "model_evaluation_result": {
      "model_raw_response": "B) The suburb S reduces the effective.price of childcare, an alternative i is chosen if th.e difference in unobscured utility (εi - εj) is less than th.e difference.in observed utility (vi - vj).",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 231,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the econometric model of a single mother's discrete choice among employment and childcare alternatives.\n\n**Setting and Sample.** A single mother makes a static choice from a set of seven mutually exclusive alternatives (e.g., 'No-work', 'Part-time work with paid care', 'Full-time work with unpaid care', etc.).\n\n### The Question\n\nThe paper estimates this choice model using a multinomial logit (MNL) framework, which relies on the Independence of Irrelevant Alternatives (IIA) assumption. Why is this assumption likely violated in this context, and what is the appropriate remedy?",
    "Options": {
      "A": "IIA is violated because some alternatives (e.g., 'Part-time, Paid Care' and 'Part-time, Unpaid Care') are closer substitutes than others. A Nested Logit model, which groups similar alternatives into 'nests' to allow for correlation in unobserved utility, would be an appropriate remedy.",
      "B": "IIA is violated because wages are endogenous and correlated with the error term. The appropriate remedy is to use instrumental variables for wages.",
      "C": "IIA is violated because the error terms are heteroskedastic across individuals. The appropriate remedy is to use a Probit model instead of a Logit model.",
      "D": "IIA is not violated because the seven choices are defined to be mutually exclusive. The standard MNL model is therefore appropriate."
    },
    "Answer": "A",
    "pi_justification": "This item assesses a core concept in discrete choice econometrics: the IIA assumption. It requires the student to identify the specific reason for the violation (substitution patterns) and match it to the correct, more flexible model (Nested Logit). The distractors are based on common confusions: B (True but Irrelevant) conflates the IIA problem with endogeneity, C (Conceptual Opposite) confuses IIA with heteroskedasticity and proposes an irrelevant fix, and D denies the problem exists.",
    "qid": "231",
    "model_evaluation_result": {
      "model_raw_response": "A) A) IIA violates because some alternatives are closer substitutes.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 232,
    "Question": "### Background\n\n**Research Question.** This problem investigates the macroeconomic management of a large foreign aid inflow, focusing on the government's fiscal response (tax effort).\n\n**Setting.** The model incorporates a behavioral equation for the government's tax effort. A key concern in the literature is that aid may create a disincentive for governments to collect domestic taxes.\n\n**Variables & Parameters.**\n- `TXR`: Effective tax rate on domestic sales.\n- `AID/NGDP`: Total foreign aid as a share of GDP.\n- `a4`: Coefficient on the log aid-to-GDP ratio in the `TXR` equation.\n\n---\n\n### Data / Model Specification\n\n**Fiscal Response:** The effective tax rate `TXR` is modeled as a function of past behavior and the current aid-to-GDP ratio.\n```latex\n\\log(\\mathrm{TXR})_t = a_1 + a_2 \\cdot \\log(\\mathrm{TXR})_{t-1} + a_3 \\cdot \\log(\\mathrm{GTOT}/\\mathrm{NGDP})_{t-1} + a_4 \\cdot \\log(\\mathrm{AID}/\\mathrm{NGDP})_t + \\nu_t \n```\n\n**Table 1: OLS Estimation Results for Domestic Tax Rate**\n| Coefficient | Estimated Value | Two-standard-error confidence interval |\n| :--- | :--- | :--- |\n| `a4` on `log(AID/NGDP)` | -0.122 | [-0.336, 0.093] |\n\n---\n\n### Question\n\nA common concern is that foreign aid might undermine a government's incentive to collect taxes. Based on the provided OLS estimation results for the effective tax rate (`TXR`) in Table 1, what can be concluded about this 'fiscal response' in Ethiopia?",
    "Options": {
      "A": "The estimated effect of aid on the tax rate is statistically insignificant, so there is no robust evidence that aid harms tax collection.",
      "B": "The estimated effect of aid on the tax rate is highly significant and confirms that aid severely undermines tax collection.",
      "C": "The negative point estimate (-0.122) proves that aid harms tax collection, even though the effect is not statistically significant.",
      "D": "The effect is statistically significant because the confidence interval is not symmetric around the point estimate."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to correctly interpret statistical significance from regression output, a foundational skill in econometrics. It uses a Computational Judgment strategy. The distractors are designed to trap common errors: C incorrectly prioritizes the point estimate over significance, B misreads significance entirely, and D represents a misunderstanding of how confidence intervals are used to test hypotheses.",
    "qid": "232",
    "model_evaluation_result": {
      "model_raw_response": "B) The estimate effect of aid is highly significant, and confirms that it severely undermines tax colle",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 233,
    "Question": "### Background\n\n**Research Question.** This problem investigates the macroeconomic management of a large foreign aid inflow, focusing on the external sector's adjustment, a phenomenon known as \"Dutch Disease.\"\n\n**Setting.** Producers in the model allocate output between exports and domestic sales based on relative prices. An aid inflow can alter these prices and be managed by central bank policy.\n\n**Variables & Parameters.**\n- `X`: Quantity of goods allocated to exports.\n- `DOM`: Quantity of goods allocated to domestic sales.\n- `PX/PD`: Relative price of exports to domestic goods.\n- `ΔNFA`: Change in net foreign assets (reserve accumulation).\n\n---\n\n### Data / Model Specification\n\n1.  **Trade Allocation:** Producers allocate total output between exports (`X`) and domestic sales (`DOM`) based on relative prices, according to a Constant Elasticity of Transformation (CET) function.\n    ```latex\n    X/\\mathrm{DOM} = \\{ (\\mathrm{PX/PD}) \\cdot C \\}^{\\sigma\\mathrm{DE}}\n    ```\n    where `C` is a constant.\n\n2.  **Reserve Management:** The central bank accumulates reserves (`ΔNFA`) as a policy response, which affects the balance of payments and the exchange rate.\n\n**Table 1: Simulation Impact of a 5pp Aid Increase (2007)**\n*(Absolute deviations from baseline)*\n\n| Variable | 2007 Impact |\n| :--- | :--- |\n| Real Exchange Rate (% change) | -0.51 |\n| Change in Foreign Assets (% of GDP) | +1.57 |\n\n---\n\n### Question\n\nFollowing a 5 percentage point increase in non-food aid, the simulation shows an immediate real exchange rate appreciation (Table 1). According to the model's logic, which statement best explains this 'Dutch Disease' effect and one of the key macroeconomic policy factors that mitigates it?",
    "Options": {
      "A": "The appreciation is caused by the large currency inflow, which harms export competitiveness; however, the central bank's accumulation of foreign reserves dampens this effect.",
      "B": "The appreciation is caused by falling domestic prices, which helps export competitiveness; the effect is amplified by the central bank selling foreign reserves.",
      "C": "The appreciation is caused by the currency inflow, and it is primarily mitigated by the long-run increase in productivity from aid-funded public investment.",
      "D": "The appreciation makes imports cheaper, and it is mitigated by the government using the aid to reduce its fiscal deficit."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the 'Dutch Disease' phenomenon and the macroeconomic policy tools used to manage it. It uses a Scenario Application strategy, asking the student to apply theory to simulation results. Distractors are designed based on common confusions: B reverses the logic of both the cause and the mitigation, C presents a correct but long-term structural mitigation instead of the immediate policy response shown in the data, and D confuses correlation (fiscal balance improves) with the specific causal mechanism for exchange rate management (reserve policy).",
    "qid": "233",
    "model_evaluation_result": {
      "model_raw_response": "C) The appreciate is caused by currency inflow and it is primarly mitigated by long-run increase productivity from aid funded public investment.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 234,
    "Question": "### Background\n\n**Research Question.** This problem requires the derivation of the log-likelihood function for a restricted system of equations (Model 4), a crucial step for maximum likelihood estimation and hypothesis testing.\n\n**Setting / Institutional Environment.** The setting is a two-equation system where parameters are restricted based on economic theory. Model 4 imposes a specific constraint on the structure of the error covariance matrix, `Σ`.\n\n### Data / Model Specification\n\nModel 4 imposes the following restriction on the error covariance structure:\n\n```latex\n\\Sigma = \n\\begin{pmatrix}\n\\sigma^2 & \\gamma\\sigma^2 \\\\\n\\gamma\\sigma^2 & \\sigma^2\n\\end{pmatrix}\n\\quad \\text{(Eq. (1))}\n```\n\nGiven this restricted covariance matrix `Σ`, what is its inverse, `Σ⁻¹`?",
    "Options": {
      "A": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} \n\\begin{pmatrix}\n1 & -\\gamma \\\\\n-\\gamma & 1\n\\end{pmatrix}\n```",
      "B": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} \n\\begin{pmatrix}\n1 & \\gamma \\\\\n\\gamma & 1\n\\end{pmatrix}\n```",
      "C": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma)} \n\\begin{pmatrix}\n1 & -\\gamma \\\\\n-\\gamma & 1\n\\end{pmatrix}\n```",
      "D": "```latex\n\\frac{1}{\\sigma^2} \n\\begin{pmatrix}\n1 & -\\gamma \\\\\n-\\gamma & 1\n\\end{pmatrix}\n```"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the foundational matrix algebra required for the paper's likelihood derivation. It uses a Computational Judgment strategy. Distractors are designed based on common algebraic errors: B (Sign Error in off-diagonal elements), C (Formula Misuse in calculating the determinant), and D (Step-Omission Error, forgetting the determinant term).",
    "qid": "234",
    "model_evaluation_result": {
      "model_raw_response": "B) \n\n```latex\n\\sigma^(-2)(1-\\gamma)^(-1) \n\\begin {pmatrix} \n1 & \\ gamma \\\\ \n\\gamma & \\ 1 \n\\end{ pmatrix}\n``` \n\nNote: I've checked the solution using the formula for the inverse of a 2x2 matrix, and it matches the provided answer.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 235,
    "Question": "### Background\n\n**Research Question.** This problem requires the derivation of the log-likelihood function for a restricted system of equations (Model 4), a crucial step for maximum likelihood estimation and hypothesis testing.\n\n**Setting / Institutional Environment.** The setting is a two-equation system where parameters are restricted based on economic theory. The log-likelihood function involves a trace term based on the error covariance matrix `Σ` and the data matrix `Z`.\n\n### Data / Model Specification\n\nThe system's errors can be written as a `T x 2` matrix `V = ZA`, where `Z` is the matrix of all data and `A = [a₁, a₂]` is a matrix of parameters. The error covariance matrix for Model 4 is:\n\n```latex\n\\Sigma = \n\\begin{pmatrix}\n\\sigma^2 & \\gamma\\sigma^2 \\\\\n\\gamma\\sigma^2 & \\sigma^2\n\\end{pmatrix}\n\\quad \\text{(Eq. (1))}\n```\n\nThe log-likelihood function for Model 4 involves the term `trace(Σ⁻¹A'Z'ZA)`. Let `S = Z'Z`.\n\nAfter substituting the correct `Σ⁻¹` for the matrix in Eq. (1) and simplifying, what is this trace term equal to?",
    "Options": {
      "A": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} (a_1'Sa_1 - 2\\gamma a_1'Sa_2 + a_2'Sa_2)\n```",
      "B": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} (a_1'Sa_1 + 2\\gamma a_1'Sa_2 + a_2'Sa_2)\n```",
      "C": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} (a_1'Sa_1 - \\gamma a_1'Sa_2 + a_2'Sa_2)\n```",
      "D": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} ((a_1'Sa_1)(a_2'Sa_2) - (a_1'Sa_2)^2)\n```"
    },
    "Answer": "A",
    "pi_justification": "This item tests the core mathematical derivation needed to construct the Model 4 likelihood function, using a Computational Judgment strategy. The distractors are based on plausible algebraic errors: B (Sign Error on the cross-term), C (Step-Omission Error, forgetting the cross-term is counted twice in the trace), and D (Conceptual Error, confusing the trace operation with a determinant calculation).",
    "qid": "235",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: I'm only providing the answer as per the output format requirement)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 236,
    "Question": "### Background\n\nThis problem examines the static, one-period microfoundations of the model, focusing on how different types of \"transacting capital\" determine production outcomes, sorting across markets, and overall economic efficiency. The core of the model is a symmetric holdup problem where contract enforcement, and thus investment, depends on an individual's skills and their trading partner.\n\n### Data / Model Specification\n\n**1. Agents and Locations:** The economy has two types of agents: those with local skills (L-capital) and those with market skills (M-capital). They can trade in a local \"village\" or an anonymous \"market\".\n\n**2. Production and Payoffs:** Any two matched agents can form a partnership. Each can make a relationship-specific investment at cost `k` to increase output by `y > k`. The effective price of output is 1 in the village and `θ` in the market. We assume `θ > 1`.\n\n**3. Contract Enforcement:**\n- In the market, an agent with M-capital can enforce contracts, preventing holdup. An agent with L-capital cannot.\n- In the village, an agent with L-capital can enforce contracts. An agent with M-capital cannot.\n\n**4. Bargaining and Holdup:** When contracts cannot be enforced, post-investment surplus is split equally (Nash Bargaining). The parameters are such that holdup is a binding constraint: `θy < (4/3)k`, which implies that an agent will not invest if they expect to be held up.\n\n### Question\n\nIn the market, when an agent with M-capital is matched with an agent with L-capital, what is the equilibrium payoff for the M-agent?",
    "Options": {
      "A": "```latex\n\\theta y - k\n```",
      "B": "```latex\n0.5(\\theta y - k)\n```",
      "C": "```latex\n0.5(\\theta y + k)\n```",
      "D": "```latex\n0\n```"
    },
    "Answer": "B",
    "pi_justification": "This item assesses the foundational understanding of the holdup problem and bargaining outcome in the model. It uses a Computational Judgment strategy. The correct answer requires calculating the surplus and the correct split. Distractors are designed based on common errors: confusing total surplus with individual payoff (A), misinterpreting revenue share as net payoff (C), and incorrectly assuming no investment occurs (D).",
    "qid": "236",
    "model_evaluation_result": {
      "model_raw_response": "B) 0.5(θy - k)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 237,
    "Question": "### Background\n\nThis problem examines the static, one-period microfoundations of the model, focusing on how different types of \"transacting capital\" determine production outcomes, sorting across markets, and overall economic efficiency. The core of the model is a symmetric holdup problem where contract enforcement, and thus investment, depends on an individual's skills and their trading partner.\n\n### Data / Model Specification\n\n**1. Agents and Locations:** The economy has two types of agents: those with local skills (L-capital) and those with market skills (M-capital). They can trade in a local \"village\" or an anonymous \"market\".\n\n**2. Production and Payoffs:** Any two matched agents can form a partnership. Each can make a relationship-specific investment at cost `k` to increase output by `y > k`. The effective price of output is 1 in the village and `θ` in the market. We assume `θ > 1`.\n\n**3. Contract Enforcement:**\n- In the market, an agent with M-capital can enforce contracts, preventing holdup. An agent with L-capital cannot.\n- In the village, an agent with L-capital can enforce contracts. An agent with M-capital cannot.\n\n**4. Bargaining and Holdup:** When contracts cannot be enforced, post-investment surplus is split equally (Nash Bargaining). The parameters are such that holdup is a binding constraint: `θy < (4/3)k`, which implies that an agent will not invest if they expect to be held up.\n\n### Question\n\nL-agents are indifferent between trading in the village (payoff `y-k`) and the market when the fraction of M-traders in the market is `x_0`. What is the correct expression for `x_0`?",
    "Options": {
      "A": "```latex\n\\frac{y-k}{\\theta y - k}\n```",
      "B": "```latex\n\\frac{2(y-k)}{\\theta y - k}\n```",
      "C": "```latex\n\\frac{2(y-k)}{\\theta y + k}\n```",
      "D": "```latex\n\\frac{\\theta y - k}{y-k}\n```"
    },
    "Answer": "B",
    "pi_justification": "This item tests the student's ability to set up and solve the equilibrium indifference condition that governs agent sorting. It uses a Computational Judgment strategy. The correct answer requires correctly identifying the payoffs for an L-agent in both locations and solving for the equilibrium market composition. Distractors are based on plausible errors: omitting the surplus-sharing factor (A), using an incorrect payoff term (C), and a common algebraic slip (D).",
    "qid": "237",
    "model_evaluation_result": {
      "model_raw_response": "D) ```\\frac{\\ theta y - k }{ y-k }```",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 238,
    "Question": "### Background\n\nThis problem examines the static, one-period microfoundations of the model, focusing on how different types of \"transacting capital\" determine production outcomes, sorting across markets, and overall economic efficiency. The core of the model is a symmetric holdup problem where contract enforcement, and thus investment, depends on an individual's skills and their trading partner.\n\n### Data / Model Specification\n\n**1. Agents and Locations:** The economy has two types of agents: those with local skills (L-capital) and those with market skills (M-capital). They can trade in a local \"village\" or an anonymous \"market\".\n\n**2. Production and Payoffs:** Any two matched agents can form a partnership. Each can make a relationship-specific investment at cost `k` to increase output by `y > k`. The effective price of output is 1 in the village and `θ` in the market. We assume `θ > 1`.\n\n**3. Contract Enforcement:**\n- In the market, an agent with M-capital can enforce contracts, preventing holdup. An agent with L-capital cannot.\n- In the village, an agent with L-capital can enforce contracts. An agent with M-capital cannot.\n\n**4. Bargaining and Holdup:** When contracts cannot be enforced, post-investment surplus is split equally (Nash Bargaining). The parameters are such that holdup is a binding constraint: `θy < (4/3)k`, which implies that an agent will not invest if they expect to be held up.\n\n### Question\n\nThe entry of L-agents into the market imposes a negative externality on M-agents. What is the minimum Pigouvian tax `τ` that must be levied on an L-agent entering the market to restore the socially efficient outcome (i.e., all L-agents trading in the village)?",
    "Options": {
      "A": "```latex\n0.5(\\theta y - k) - (y-k)\n```",
      "B": "```latex\ny-k\n```",
      "C": "```latex\n0.5(\\theta y - k)\n```",
      "D": "```latex\nm[0.5(\\theta y-k) - (y-k)]\n```"
    },
    "Answer": "A",
    "pi_justification": "This advanced item assesses the ability to apply welfare concepts to the model, specifically designing a corrective Pigouvian tax. It uses a Computational Judgment strategy. The correct answer requires identifying the L-agent's private incentive to enter the market and calculating the tax needed to offset it. Distractors represent common conceptual errors in externality problems: confusing the tax with an agent's payoff (B), calculating only one side of the incentive equation (C), or confusing the total externality value with the required marginal tax (D).",
    "qid": "238",
    "model_evaluation_result": {
      "model_raw_response": "B) y-k\n\nNote: I'm only providing the answer as per the format requested.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 239,
    "Question": "### Background\n\n**Research Question.** This problem develops the theoretical and empirical framework used to identify the determinants of firm sales growth in transition economies. The core challenge is to distinguish the impact of changes in productive inputs from the influence of historical legacies of the central planning era.\n\n### Data / Model Specification\n\nA static, levels-based model with a time-invariant, firm-specific effect \\(f_i\\) can be written as:\n\n```latex\n\\ln(S_{it}) = \\alpha \\ln(L_{it}) + \\beta \\ln(K_{it}) + f_i + u_{it} \\quad \\text{(Eq. (1))}\n```\n\nThe paper instead uses a first-differenced model:\n\n```latex\n\\Delta\\ln S_{i} = \\alpha\\Delta\\ln L_{i} + \\beta\\Delta\\ln K_{i} + \\varepsilon_{i} \\quad \\text{(Eq. (2))}\n```\n\nwhere \\(\\Delta\\) denotes the change from one period to the next. This dynamic specification is chosen because static models are likely biased by \"historical or institutional factors\" that are firm-specific and time-invariant.\n\n### Question\n\nThe paper prefers the first-differenced specification in Eq. (2) over a static levels-based model (Eq. (1)). What is the primary econometric justification for this choice?",
    "Options": {
      "A": "To eliminate bias from unobserved, time-invariant firm characteristics, such as historical political connections or legacy infrastructure.",
      "B": "To account for time-varying macroeconomic shocks, such as the 1998 Russian financial crisis, that affect all firms.",
      "C": "To solve the problem of measurement error in the capital stock variable, which is a known issue in transition economies.",
      "D": "To allow for the direct estimation of the magnitude of each firm's unique historical advantages."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's understanding of the core methodological choice in the paper: using first-differencing to control for fixed effects. It directly replaces the conceptual questions Q2 and Q3 from the source QA. The distractors are designed based on common econometric misconceptions: B confuses firm fixed effects with time fixed effects; C confuses the solution for omitted variables with the problem of measurement error; D represents a conceptual opposite of what first-differencing achieves.",
    "qid": "239",
    "model_evaluation_result": {
      "model_raw_response": "A) A) To eliminatebias from unobsolved, time-invarian tfirm characteristics, suchas historical political connectionsor legacy infrastructure.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 240,
    "Question": "### Background\n\n**Research Question.** This problem develops the theoretical and empirical framework used to identify the determinants of firm sales growth in transition economies. The core challenge is to distinguish the impact of changes in productive inputs from the influence of historical legacies of the central planning era.\n\n### Data / Model Specification\n\nThe paper estimates a first-differenced model to explain the growth in firm sales:\n\n```latex\n\\Delta\\ln S_{i} = \\alpha\\Delta\\ln L_{i} + \\beta\\Delta\\ln K_{i} + \\text{other factors} + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n```\n\nwhere \\(\\Delta\\) denotes the change over the sample period.\n\n### Question\n\nConsider the first-differenced model in Eq. (1). Suppose there is an unobserved factor, 'learning-by-doing,' which represents a firm's true efficiency growth (\\(\\Delta \\ln A^*_i\\)). This factor positively affects sales growth. Furthermore, assume that firms with faster efficiency growth are also more likely to invest in new capital (i.e., \\(\\Delta \\ln K_i\\) is positively correlated with \\(\\Delta \\ln A^*_i\\)). If 'learning-by-doing' is omitted from the regression, what is the resulting bias on the estimated coefficient for capital growth (\\(\\hat{\\beta}\\))?",
    "Options": {
      "A": "It will be overestimated (biased away from zero).",
      "B": "It will be underestimated (biased toward zero).",
      "C": "It will be unbiased, because first-differencing removes all sources of bias.",
      "D": "The direction of the bias cannot be determined without knowing the magnitude of the correlations."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to apply the theory of omitted variable bias in a dynamic context, a high-level skill targeted by Q4 of the source QA. It uses a Reverse-Reasoning strategy. The distractors are based on predictable errors in applying the OVB formula: B results from a sign error in one of the two key relationships; C reflects a misunderstanding of the limits of the first-differencing method; D is a common guess when the formal logic is not understood.",
    "qid": "240",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: I'm returning only the letter \"A\")",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 241,
    "Question": "### Background\n\n**Research Question:** This problem investigates the practical performance of the Generalized Stochastic Simulation Algorithm (GSSA), focusing on the critical role of the integration method for accuracy and efficiency.\n\n**Setting / Institutional Environment:** The analysis uses numerical experiments from a standard representative-agent model to compare different integration methods.\n\n**Variables & Parameters:**\n- `MC(J)`: Monte Carlo integration with `J` random draws.\n- `Q(J)`: Gauss-Hermite quadrature with `J` deterministic nodes.\n- `T`: The simulation length.\n- `E_mean`: Mean absolute unit-free Euler error (in log10 units).\n- `CPU`: Computation time in seconds.\n\n---\n\n### Data / Model Specification\n\nThe following table provides results from the paper's numerical experiments for a 5th degree polynomial approximation.\n\n**Table 1: Accuracy of Integration Methods (Representative-Agent Model)**\n\n| Integration Method | T | `E_mean` (log10) | CPU (sec) |\n| :--- | :---: | :---: | :---: |\n| MC(1) | 10,000 | -4.23 | 80 |\n| MC(2000) | 10,000 | -6.07 | 4551 |\n| Q(1) | 100 | -6.22 | 7 |\n\n---\n\n### Question\n\nBased on the data in Table 1, which statement most accurately compares the efficiency of the most computationally intensive Monte Carlo method shown (MC(2000)) with the simplest deterministic method shown (Q(1))?\n",
    "Options": {
      "A": "The Q(1) method is both more accurate and over 600 times faster, indicating that sampling error, not computational time, is the primary bottleneck for accuracy in Monte Carlo methods.",
      "B": "The MC(2000) method is significantly more accurate because it uses more integration nodes, although it is much slower than the Q(1) method.",
      "C": "Both methods achieve similar levels of accuracy, but the Q(1) method is faster, making it only marginally more efficient.",
      "D": "The Q(1) method is faster but less accurate than the MC(2000) method, presenting a classic speed-accuracy trade-off."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem is highly suitable for conversion. Its components rely on table lookups, a precise calculation, and a structured recommendation, all of which can be tested with high fidelity. Misconception potential is high (e.g., misinterpreting log errors, calculation slips, suboptimal strategic choices), allowing for strong distractors.\n\nThis item assesses the core conclusion from Part 1 of the original QA. It uses a Computational Judgment strategy, requiring the student to compare values from the table and draw a conclusion about relative efficiency. Distractor B is a Conceptual Opposite, incorrectly claiming MC(2000) is more accurate by misinterpreting the log10 errors (-6.07 > -6.22). Distractor D also misinterprets accuracy. Distractor C is an 'Almost Right' option that correctly identifies Q(1) as faster but understates the massive difference in efficiency.",
    "qid": "241",
    "model_evaluation_result": {
      "model_raw_response": "D) The q(1)method is faster butless accurate than the mc(2000method,presenting a classic speedaccuracytradeoff.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 242,
    "Question": "### Background\n\n**Research Question:** This problem investigates the practical performance of the Generalized Stochastic Simulation Algorithm (GSSA), focusing on the convergence properties of the Monte Carlo integration method.\n\n**Setting / Institutional Environment:** The analysis uses numerical experiments from a standard representative-agent model.\n\n**Variables & Parameters:**\n- `MC(J)`: Monte Carlo integration with `J` random draws.\n- `T`: The simulation length.\n- `E_mean`: Mean absolute unit-free Euler error (in log10 units).\n\n---\n\n### Data / Model Specification\n\nThe paper claims that the error of the MC(1) integration method scales with `1/√T`. The following table provides results from numerical experiments for a 5th degree polynomial approximation.\n\n**Table 1: Performance of MC(1) Integration**\n\n| Integration Method | T | `E_mean` (log10) |\n| :--- | :---: | :---: |\n| MC(1) | 10,000 | -4.23 |\n| MC(1) | 100,000 | -4.71 |\n\n---\n\n### Question\n\nUsing the data in Table 1, by what factor does the absolute error of the MC(1) method decrease when the simulation length `T` is increased from 10,000 to 100,000? How does this compare to the theoretical `1/√T` convergence rate?\n",
    "Options": {
      "A": "The error decreases by a factor of approximately 3.0, which is consistent with the theoretical prediction of √10 ≈ 3.16.",
      "B": "The error decreases by a factor of approximately 10.0, which is consistent with a theoretical 1/T convergence rate.",
      "C": "The error decreases by a factor of approximately 1.1, which is much slower than the theoretical prediction.",
      "D": "The error decreases by a factor of approximately 0.48, indicating that the error actually increased."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the quantitative verification from Part 2 of the original QA. It uses a Computational Judgment strategy. The calculation requires converting log10 errors to absolute errors and finding their ratio. Distractors are designed based on common numerical errors. Distractor B represents a `T` vs `√T` confusion. Distractor C represents a simple ratio of the log values (`4.71/4.23`). Distractor D represents the difference of the log values (`4.71-4.23`), which is a Step-Omission Error (forgetting to take the antilog).",
    "qid": "242",
    "model_evaluation_result": {
      "model_raw_response": "C) The absolute error of MC((-1) decreases by a difference of 0.52 = log10(0.71) - log10(4.23) = 0.12, which corresponds to a factor of 2^(0.12) ≈ 1.11.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 243,
    "Question": "### Background\n\n**Research Question:** This problem investigates the practical performance of the Generalized Stochastic Simulation Algorithm (GSSA), focusing on its tractability in high-dimensional settings.\n\n**Setting / Institutional Environment:** The analysis uses a high-dimensional multi-country model to test for the \"curse of dimensionality.\"\n\n**Variables & Parameters:**\n- `MC(1)`: One-node Monte Carlo integration.\n- `Q(1)`: One-node Gauss-Hermite quadrature.\n- `M1`: A monomial integration rule suitable for high dimensions.\n- `N`: The number of countries.\n- `E_mean`: Mean absolute unit-free Euler error (in log10 units).\n\n---\n\n### Data / Model Specification\n\nThe following table shows GSSA performance in a multi-country model.\n\n**Table 1: GSSA Performance in the Multi-Country Model**\n\n| N | Poly. Deg. | Method | `E_mean` (log10) | CPU (sec) |\n|:---:|:---:|:---|:---:|:---:|\n| 20 | 1st | RLS-Tikh., Q(1) | -4.75 | 56 |\n| 20 | 2nd | RLS-Tikh., Q(1) | -5.40 | 1079 |\n| 100| 1st | RLS-Tikh., Q(1) | -4.64 | 225 |\n| 100| 1st | RLS-TSVD, M1 | -4.63 | 13,846 |\n\n---\n\n### Question\n\nA research team wants to solve a 50-country (`N=50`) version of this model. Their goal is to achieve a mean absolute error of at least `10^{-5.0}` (i.e., a log10 error of -5.0 or lower) as efficiently as possible. Based on the patterns in Table 1, which of the following is the most appropriate and efficient GSSA strategy?\n",
    "Options": {
      "A": "Use a 2nd-degree polynomial with the Q(1) integration rule.",
      "B": "Use a 1st-degree polynomial with the Q(1) integration rule.",
      "C": "Use a 2nd-degree polynomial with the M1 integration rule.",
      "D": "Use a 1st-degree polynomial with the M1 integration rule."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the strategic recommendation from Part 3 of the original QA. It uses a Scenario Application strategy, asking the student to extrapolate from the provided data to a new case. The distractors represent plausible but suboptimal choices. Distractor B fails to meet the accuracy target, as 1st-degree polynomials consistently yield errors around -4.7. Distractor C meets the accuracy target but is highly inefficient, as the M1 rule is shown to be far slower than Q(1) for similar accuracy. Distractor D fails on both accuracy and efficiency.",
    "qid": "243",
    "model_evaluation_result": {
      "model_raw_response": "C) Use the 2nd degree polynomial with the RLS-TSVd, M1 integration method.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 244,
    "Question": "### Background\n\n**Research Question:** This problem investigates the robustness of the Generalized Stochastic Simulation Algorithm (GSSA) by examining its performance under high multicollinearity.\n\n**Setting / Institutional Environment:** The GSSA approximates a policy function by regressing a target variable on a matrix of basis functions, `X`. Using ordinary polynomials (e.g., `k²`, `k⁴`) often leads to an ill-conditioned `X` matrix, causing standard Ordinary Least Squares (OLS) to fail. The paper tests an alternative set of basis functions, Hermite polynomials, to address this.\n\n---\n\n### Data / Model Specification\n\nThe following table summarizes the stability of the OLS method when using different polynomial bases.\n\n**Table 1: Stability with Different Polynomial Bases (OLS Method)**\n\n| Polynomial Degree | Ordinary Polynomials `E_mean` | Hermite Polynomials `E_mean` |\n| :--- | :---: | :---: |\n| 2nd | -5.46 | -5.46 |\n| 3rd | — | -6.84 |\n| 4th | — | -7.97 |\n| 5th | — | -9.09 |\n*Note: '—' indicates the method failed to converge.* \n\n---\n\n### Question\n\nAccording to the paper and the results in Table 1, what is the primary mechanism through which using Hermite polynomials as basis functions enhances the numerical stability of the regression step, allowing OLS to succeed where it previously failed?\n",
    "Options": {
      "A": "Hermite polynomials are constructed to be orthogonal, which reduces the multicollinearity between the columns of the regressor matrix `X`.",
      "B": "Hermite polynomials are computationally faster to evaluate than ordinary polynomials, which reduces round-off errors.",
      "C": "Hermite polynomials are linear functions, which simplifies the regression problem and avoids the need to invert a matrix.",
      "D": "Hermite polynomials are only defined for normally distributed data, which matches the shock process and improves the regression fit."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). The original QA problem is a strong candidate for conversion. Its questions probe specific mechanisms and interpretations with highly convergent answers, and the potential for creating plausible distractors based on conceptual confusion is high.\n\nThis item assesses the core concept from Part 1 of the original QA. It tests the student's understanding of why one set of basis functions is more stable than another. Distractor B is a 'True but Irrelevant' type of error (or potentially false, but plausible). Distractor C is a Conceptual Opposite, as Hermite polynomials of degree > 1 are not linear. Distractor D is a conceptual error that misrepresents the properties of Hermite polynomials and their application.",
    "qid": "244",
    "model_evaluation_result": {
      "model_raw_response": "A) Hermите polynomials are constructe to be orthogonal",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 245,
    "Question": "### Background\n\n**Research Question:** This problem investigates the robustness of the Generalized Stochastic Simulation Algorithm (GSSA) by examining its performance when model parameters, such as risk aversion, create numerical challenges.\n\n**Setting / Institutional Environment:** When the coefficient of relative risk aversion (`γ`) is high, the GSSA's fixed-point iteration can become unstable. The paper tests two distinct strategies to restore stability: (1) reducing the damping parameter `ξ` in the update step, and (2) changing the policy function being approximated from capital to marginal utility.\n\n---\n\n### Data / Model Specification\n\nThe following table shows the performance of these two strategies for a model with high risk aversion (`γ=10`) using a 5th degree polynomial.\n\n**Table 1: Stability under High Risk Aversion (γ=10)**\n\n| Policy Function Approximated | Damping `ξ` | `E_mean` (log10) | CPU (sec) |\n| :--- | :---: | :---: | :---: |\n| Capital | 0.01 | -7.13 | 580 |\n| Marginal Utility | 0.5 | -5.41 | 3102 |\n\n---\n\n### Question\n\nBased on the data in Table 1, which strategy for restoring stability under high risk aversion is superior, and why?\n",
    "Options": {
      "A": "Reducing the damping parameter is superior because it achieves a significantly higher accuracy (lower error) and is more than five times faster.",
      "B": "Approximating the marginal utility function is superior because it allows for a much larger damping parameter, leading to faster convergence.",
      "C": "Reducing the damping parameter is superior in accuracy, but approximating marginal utility is superior in speed, presenting a trade-off.",
      "D": "Both strategies are equally effective, as they both successfully restore convergence and achieve acceptable levels of accuracy."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the comparative analysis from Part 2 of the original QA. It uses a Computational Judgment strategy, requiring the student to interpret the table to determine the superior method based on multiple criteria (accuracy and speed). Distractor B is a Conceptual Opposite; while it allows a larger `ξ`, the actual CPU time shows it does not lead to faster convergence. Distractor C incorrectly identifies the speed comparison. Distractor D is an 'Almost Right' option that ignores the very large quantitative differences in performance, failing to make the required judgment.",
    "qid": "245",
    "model_evaluation_result": {
      "model_raw_response": "C) Reducing the damping parameter (ξ) is superior in terms of accuracy, but approximate marginal utility is better in terms of speed.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 246,
    "Question": "### Background\n\n**Research Question:** This problem investigates the robustness of the Generalized Stochastic Simulation Algorithm (GSSA) to non-Gaussian shocks by testing it on a model featuring 'rare disasters' (large, low-probability negative shocks).\n\n**Setting / Institutional Environment:** The paper adapts its integration scheme to explicitly account for the disaster and no-disaster states and their respective probabilities. The goal is to see if high accuracy can be maintained.\n\n---\n\n### Data / Model Specification\n\nThe table below shows the best accuracy achieved in the rare disaster model. For comparison, the best accuracy achieved in the standard model (without rare disasters) using an equivalent setup was `E_mean` = -8.91.\n\n**Table 1: Performance in Rare Disasters Model (5th Degree Poly.)**\n\n| Method | `E_mean` (log10) | `E_max` (log10) |\n| :--- | :---: | :---: |\n| RLS-TSVD | -8.66 | -6.54 |\n\n---\n\n### Question\n\nComparing the best accuracy in the rare disaster model (`E_mean` = -8.66) with the benchmark from the standard model (`E_mean` = -8.91), what can be concluded about GSSA's robustness?\n",
    "Options": {
      "A": "GSSA is highly robust; while there is a small, measurable loss of accuracy, it still achieves exceptionally high accuracy (mean errors of order 10⁻⁹).",
      "B": "GSSA is not robust to rare disasters, as the mean error increases by several orders of magnitude.",
      "C": "GSSA's accuracy is unaffected by rare disasters, as the log10 errors are nearly identical.",
      "D": "GSSA's mean accuracy (`E_mean`) is robust, but its worst-case accuracy (`E_max`) deteriorates significantly, making it unsuitable for this type of model."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the interpretation from Part 3 of the original QA. It requires the student to compare two numbers and draw a nuanced conclusion. Distractor B is a Conceptual Opposite, vastly overstating the loss of accuracy. Distractor C is an 'Almost Right' option that fails to acknowledge the small difference. Distractor D draws an overly strong negative conclusion from the `E_max` value without sufficient context for comparison, while the paper's main conclusion focuses on the successful achievement of high accuracy overall.",
    "qid": "246",
    "model_evaluation_result": {
      "model_raw_response": "C) G SSA's accuracy is affected by rare disasters as the log errors are not identical.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 247,
    "Question": "### Background\n\nThe paper's model of long-run development culminates in a balanced growth path (BGP), a final phase where effort allocations and growth rates are constant. This problem tests the ability to calculate a key BGP effort allocation using the model's calibrated parameter values.\n\n### Data / Model Specification\n\nThe household's optimal choices on the BGP lead to the following key equations:\n- The normalized shadow value of human capital, `x_b`:\n```latex\nx_b = \\frac{1 + \\gamma \\left(1 + \\frac{A_R \\varepsilon}{r}\\right)}{r + 2\\gamma\\delta_b} \\quad \\text{(Eq. 1)}\n```\n- Effort allocations, which depend on the composite term `\\delta_b x_b`:\n```latex\ne_{Mb} = \\frac{1}{\\delta_b x_b} \\quad \\text{(Eq. 2)}\n```\n```latex\ne_{Rb} = \\frac{A_R \\varepsilon}{r \\delta_b x_b} - 1 \\quad \\text{(Eq. 3)}\n```\n- The time constraint, from which learning effort `e_{Lb}` is derived:\n```latex\ne_{Lb} = 1 - e_{Rb} - e_{Mb} \\quad \\text{(Eq. 4)}\n```\n\nThe model is calibrated using the parameters provided in **Table 1** below.\n\n**Table 1. Selected Parameters and BGP Values**\n| Parameter/Value | Symbol | Value |\n|---|---|---|\n| **Set Parameters** | | |\n| Discount rate | `r` | 0.03 |\n| Learning externality | `γ` | 0.35 |\n| **Implied Parameters** | | |\n| Child-raising productivity | `A_R` | 0.121 |\n| Learning productivity (BGP) | `δ_b` | 0.066 |\n| Preference for family size | `ε` | 0.55 |\n\n### Question\n\nUsing the provided equations and parameters, what is the balanced-growth path value for the effort allocated to learning, `e_{Lb}`?",
    "Options": {
      "A": "0.253",
      "B": "0.543",
      "C": "0.204",
      "D": "0.017"
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 10.0). The original QA problem is purely computational with a single correct numerical answer, making it perfectly suited for conversion (Conceptual Clarity = 10/10). The multi-step calculation provides numerous opportunities for predictable errors, allowing for the creation of high-fidelity distractors (Discriminability = 10/10). This item tests the ability to perform a multi-step calculation using the model's core balanced-growth equations. It uses the Computational Judgment strategy. Distractors are based on common calculation errors, such as reporting an intermediate value (`e_Mb` or `e_Rb`) instead of the final answer, or confusing an effort share with a growth rate.",
    "qid": "247",
    "model_evaluation_result": {
      "model_raw_response": "C) ۰.۲۰۴\n\n(Note: I assume that the \"balanced-growth path\" refers to the steady-state values of the variables.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 248,
    "Question": "### Background\n\nThe paper's model of long-run development culminates in a balanced growth path (BGP). A key part of the paper's methodology is calibrating the model's structural parameters to match historical data and stylized facts. This problem tests the ability to work backwards from a calibrated outcome to find an underlying parameter value.\n\n### Data / Model Specification\n\nOn the BGP, per capita income `y_e` at the entry point is given by:\n```latex\ny_e = \\frac{A_P}{u_b} \\quad \\text{(Eq. 1)}\n```\nwhere `A_P` is the productivity parameter of the primitive technology, and `u_b` is a composite term related to the effort in the modern sector, `e_{Mb}`:\n```latex\nu_b = \\frac{1}{e_{Mb}} \\quad \\text{(Eq. 2)}\n```\nThe model is calibrated such that income at the entry to balanced growth is normalized to `y_e = 1000`, and the implied effort in the modern sector is `e_{Mb} = 0.54`.\n\n### Question\n\nGiven the model's calibration, what is the implied value of the primitive technology's productivity parameter, `A_P`?",
    "Options": {
      "A": "1852",
      "B": "0.0019",
      "C": "5000",
      "D": "540"
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 10.0). The original QA problem is purely computational with a single correct numerical answer, making it perfectly suited for conversion (Conceptual Clarity = 10/10). The multi-step calculation provides numerous opportunities for predictable errors, allowing for the creation of high-fidelity distractors (Discriminability = 10/10). This item assesses the ability to work backwards from a calibrated outcome to an underlying structural parameter. It uses the Reverse-Reasoning strategy. Distractors are designed based on plausible errors, such as inverting the core relationship (B), using the wrong effort share in the calculation (C), or misapplying the formula (D).",
    "qid": "248",
    "model_evaluation_result": {
      "model_raw_response": "B) 1858.67 (approximately)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 249,
    "Question": "### Background\n\nA central claim of the paper is that extensive scale (economic integration across regions, `N_c`) is fundamentally different from intensive scale (population growth within a region, `n`). This distinction arises from their asymmetric effects on the productivity of learning.\n\n### Data / Model Specification\n\nThe technology for accumulating human capital (`\\dot{h}`) is given by:\n```latex\n\\dot{h} = \\left(\\frac{M}{e_M n}\\right)^\\gamma h^{1-\\gamma} e_L \\quad \\text{(Eq. 1)}\n```\nwhere `M` is the degree of specialization. Specialization, in turn, depends on the total scale of the modern economy:\n```latex\nM = b_1 e_M h n N_c \\quad \\text{(Eq. 2)}\n```\nSubstituting Eq. (2) into Eq. (1) shows that the productivity of learning is positively affected by extensive scale (`N_c`) but is independent of intensive scale (`n`).\n\n### Question\n\nWhat is the core economic intuition for why intensive scale (`n`) has a neutral effect on learning productivity, while extensive scale (`N_c`) has a positive effect?",
    "Options": {
      "A": "An increase in `n` creates both a positive specialization effect and an offsetting negative congestion effect, while an increase in `N_c` increases specialization without adding to local congestion.",
      "B": "An increase in `N_c` raises wages more than an increase in `n` does, making the opportunity cost of not learning higher.",
      "C": "An increase in `n` only creates negative urban congestion effects, while an increase in `N_c` only creates positive specialization effects.",
      "D": "Economic integration (`N_c`) is assumed to be an exogenous policy variable, while population (`n`) is an endogenous outcome, which explains their different long-run impacts."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem targets sharp, specific concepts that are highly convertible (Conceptual Clarity = 9/10). The distinction between level and growth effects, and the mechanisms behind them, are classic sources of confusion, making it ideal for creating high-fidelity distractors (Discriminability = 10/10). This question targets the central mechanism of the paper's growth theory. It uses a Reverse-Reasoning strategy, asking for the intuition behind a given result. Distractors are designed to be plausible but incorrect explanations, including one that is Almost Right (B), one that is a Conceptual Opposite (C), and one that is True but Irrelevant (D).",
    "qid": "249",
    "model_evaluation_result": {
      "model_raw_response": "A) An Increase in `n creates both a posit",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 250,
    "Question": "### Background\n\nThe paper's central thesis is that extensive scale (economic integration, `N_c`) is the key driver of long-run growth, while intensive scale (population, `n`) is not, even though both affect the level of income.\n\n### Data / Model Specification\n\n- Per capita income in the modern sector, `y_M`, depends on the product of `n` and `N_c`:\n  ```latex\n  y_M = A_M (n N_c)^{1-\\alpha} (e_M h)^{2-\\alpha}\n  ```\n- The productivity of learning, `δ`, which drives the long-run growth rate of human capital and income, depends only on `N_c`:\n  ```latex\n  \\delta = (b_1 N_c)^\\gamma\n  ```\n\n### Question\n\nA policymaker wants to boost long-run economic performance and is considering two mutually exclusive options, each of which will double the total modern-sector population (`n N_c`):\n\n(a) A pro-natalist policy that doubles population per city (`n`).\n(b) A trade liberalization policy that doubles the number of integrated cities (`N_c`).\n\nWhich statement accurately compares the long-run consequences of these two policies?",
    "Options": {
      "A": "Both policies produce the same one-time increase in the level of per capita income, but only the trade liberalization policy increases the long-run growth rate.",
      "B": "Both policies produce the same one-time increase in the income level and the same increase in the long-run growth rate.",
      "C": "The trade liberalization policy leads to a larger one-time increase in the income level and a higher long-run growth rate.",
      "D": "The pro-natalist policy is superior because it increases the domestic market size, leading to a higher long-run growth rate than the trade policy."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem targets sharp, specific concepts that are highly convertible (Conceptual Clarity = 9/10). The distinction between level and growth effects, and the mechanisms behind them, are classic sources of confusion, making it ideal for creating high-fidelity distractors (Discriminability = 10/10). This apex question assesses the synthesis of the paper's static and dynamic results, forcing a distinction between level and growth effects. It uses a Scenario Application strategy. The distractors are built around the classic and predictable misconception of conflating level and growth effects (B), making it a powerful diagnostic tool.",
    "qid": "250",
    "model_evaluation_result": {
      "model_raw_response": "C) Thetrade liberalization policyleads to a largerone-time increase inthe income level andahigherlong-run growth rate.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 251,
    "Question": "### Background\n\n**Research Question.** This problem investigates the causal factors behind lower cooperation rates in repeated games with stochastic horizons. The Short-Stochastic (SS) treatment differs from the baseline Short-Deterministic (SD) treatment in two ways: the length of supergames is **variable** (changes from game to game), and the exact endpoint of any given game is **unpredictable**. This analysis seeks to disentangle the separate causal impacts of these two factors.\n\n**Setting / Institutional Environment.** To isolate the effects, a special 'Variable-Deterministic' (VD) treatment was designed. In this treatment, supergame durations were variable and matched the exact sequence of realized durations from the SS treatment, but the duration of each supergame was announced to subjects at the start, thus removing unpredictability. The analysis uses a linear regression model to compare mean cooperation rates across these three treatments, controlling for individual characteristics.\n\n### Data / Model Specification\n\nThe design features of the three relevant treatments are summarized below:\n\n*   **Short-Deterministic (SD):** No variability, No unpredictability.\n*   **Variable-Deterministic (VD):** Variability, No unpredictability.\n*   **Short-Stochastic (SS):** Variability, Unpredictability.\n\nThe results of a linear regression estimating the treatment effects on the mean cooperation rate are presented in Table 1.\n\n**Table 1: Linear Regression on Mean Cooperation Rate**\n\n| Variable                 | Coefficient | (Std. Err.) |\n| :----------------------- | :---------- | :---------- |\n| Short-Deterministic      | 11.574***   | (2.359)     |\n| Variable-Deterministic   | 6.966**     | (3.094)     |\n| Constant                 | 39.488***   | (13.287)    |\n| Controls                 | Yes         |             |\n\n*Notes: The dependent variable is the mean cooperation rate in percentage points. The omitted baseline treatment is Short-Stochastic (SS). *, **, *** denote significance at 10%, 5%, and 1% levels.*\n\n### Question\n\nBased on the regression results, which factor—duration variability or duration unpredictability—has a larger negative causal impact on the mean cooperation rate, and what is the estimated magnitude of that larger impact?",
    "Options": {
      "A": "Unpredictability; a negative impact of approximately 7.0 percentage points.",
      "B": "Variability; a negative impact of approximately 4.6 percentage points.",
      "C": "Unpredictability; a negative impact of approximately 11.6 percentage points.",
      "D": "Variability; a negative impact of approximately 11.6 percentage points."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem's core task is a multi-step calculation and comparison based on a clear identification strategy, making it highly suitable for conversion. Conceptual Clarity = 9/10, Discriminability = 10/10. This item uses the 'Computational Judgment' strategy. It assesses the ability to execute the paper's identification strategy: isolating the effect of unpredictability by comparing the Short-Stochastic (SS) and Variable-Deterministic (VD) treatments. The correct answer requires calculating Coop(SS) - Coop(VD) = 39.488 - (39.488 + 6.966) = -6.966. Distractor B is the correctly calculated effect of the *other* factor (variability), an 'Almost Right' error. Distractors C and D use an incorrect comparison (SS vs. SD), a 'Step-Omission' error that fails to properly isolate the causal effect.",
    "qid": "251",
    "model_evaluation_result": {
      "model_raw_response": "C) Un predictability; a nega tive impact of approximately l1.6 percentage point s.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 252,
    "Question": "### Background\n\n**Research Question.** This problem investigates the causal factors behind lower cooperation rates in repeated games with stochastic horizons.\n\n**Setting / Institutional Environment.** In the experiment, subjects were organized into sessions of 24 participants. Within each session, subjects interacted in pairs for a series of 23 supergames. The analysis of cooperation rates pools data from all subjects across all supergames and sessions.\n\n### Question\n\nThe regression analysis of cooperation rates clusters standard errors at the session level. What is the primary statistical justification for this procedure in this experimental context?",
    "Options": {
      "A": "To correct for the fact that observations from subjects within the same session are not independent due to shared experiences.",
      "B": "To correct for heteroskedasticity, where the variance of cooperation rates might differ between treatment groups.",
      "C": "To mitigate bias in the estimated treatment effects caused by unobserved session-level factors like the time of day.",
      "D": "To account for the fact that subjects' behavior changes systematically over the 23 supergames due to learning."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item captures the methodological component of the original QA problem. It assesses knowledge of a core econometric concept (clustered standard errors) in an applied context. The correct answer identifies intra-class correlation as the key issue. Distractor B ('True but Irrelevant') points to heteroskedasticity, which is addressed by robust standard errors, not necessarily clustering. Distractor C ('Conceptual Opposite') confuses the purpose of clustering (correcting standard errors) with the problem of omitted variable bias (which affects coefficient estimates). Distractor D describes a dynamic that is modeled with covariates (like supergame number), not corrected by clustering.",
    "qid": "252",
    "model_evaluation_result": {
      "model_raw_response": "A) A\n\nNote: I will not provide any explanations or justifications for my answer.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 253,
    "Question": "### Background\n\n**Research Question.** This problem examines the core mathematical structure of the Integer-Valued Autoregressive (INAR) model for count data, focusing on its decomposition and conditional properties.\n\n**Setting / Institutional Environment.** The model is a first-order integer-valued autoregressive process, INAR(1), for a time series of counts `X_t`. The core mechanism assumes that the count in period `t` is the sum of a 'carry-over' part, representing survivals from the `X_{t-1}` counts, and an 'innovation' part, representing new events in period `t`.\n\n**Variables & Parameters.**\n- `X_t`: A discrete random variable representing the count of events at time `t`.\n- `α_t`: The carry-over or 'thinning' probability, `α_t ∈ [0, 1]`.\n- `∘`: The binomial thinning operator, where `α ∘ X` means each of the `X` items survives independently with probability `α`.\n- `C_{t-1}`: The carry-over component from `X_{t-1}`.\n- `I_t`: The innovation component at time `t`, assumed to be independent of past counts.\n- `λ_t^I`: The mean of the innovation process `I_t`.\n\n---\n\n### Data / Model Specification\n\nThe INAR(1) process decomposes the count `X_t` as the sum of two independent components:\n```latex\nX_t = C_{t-1} + I_t\n```\nConditional on `X_{t-1} = x_{t-1}`, `C_{t-1}` follows a Binomial distribution, `B(x_{t-1}, α_t)`. The innovation term `I_t` is an independent Poisson draw with mean `λ_t^I`.\n\n---\n\nUsing the decomposition `X_t = C_{t-1} + I_t` and the properties of conditional expectation, what is the correct expression for `E(X_t | X_{t-1} = x_{t-1})`?",
    "Options": {
      "A": "```latex\n\\alpha_t x_{t-1} + \\lambda_t^I\n```",
      "B": "```latex\n\\alpha_t + \\lambda_t^I\n```",
      "C": "```latex\nx_{t-1} + \\lambda_t^I\n```",
      "D": "```latex\n\\alpha_t (x_{t-1} + \\lambda_t^I)\n```"
    },
    "Answer": "A",
    "pi_justification": "Assesses the ability to derive the conditional mean of the INAR(1) process, a fundamental property. This is a Computational Judgment item. Distractors are based on common algebraic and conceptual errors, such as misapplying the law of iterated expectations (B), incorrectly stating the mean of a binomial distribution (C), or incorrectly applying the linearity of expectation (D).",
    "qid": "253",
    "model_evaluation_result": {
      "model_raw_response": "D) ``` latex\n\\alpha _t (x_{ t-1 } + \\lambda _t ^I )\n```",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 254,
    "Question": "### Background\n\n**Research Question.** This problem examines the core mathematical structure of the Integer-Valued Autoregressive (INAR) model for count data, focusing on its limiting behavior.\n\n**Setting / Institutional Environment.** The model is a first-order integer-valued autoregressive process, INAR(1), for a time series of counts `X_t`. The core mechanism assumes that the count in period `t` is the sum of a 'carry-over' part from `X_{t-1}` and an 'innovation' part `I_t`.\n\n---\n\n### Data / Model Specification\n\nThe INAR(1) process `X_t = α_t ∘ X_{t-1} + I_t` has a conditional Probability Generating Function (PGF) given `X_{t-1}=x_{t-1}` of:\n```latex\nG_{X_t|x_{t-1}}(s) = (1 - \\alpha_t + \\alpha_t s)^{x_{t-1}} \\exp(\\lambda_t^I(s-1))\n```\nwhere `λ_t^I = λ_t - α_t λ_{t-1}`.\n\n---\n\nConsider the conditional distribution of `X_t` given `x_{t-1}`. Analyze its limiting behavior in two scenarios: (i) as the carry-over probability `α_t → 0` (no state dependence), and (ii) as `α_t → 1` (perfect state dependence). Which of the following correctly identifies the limiting distributions?",
    "Options": {
      "A": "As `α_t → 0`, the distribution approaches Poisson; as `α_t → 1`, it approaches a shifted Poisson.",
      "B": "As `α_t → 0`, the distribution approaches Binomial; as `α_t → 1`, it approaches a shifted Poisson.",
      "C": "As `α_t → 0`, the distribution approaches Poisson; as `α_t → 1`, it also approaches a Poisson.",
      "D": "As `α_t → 0`, the distribution approaches a shifted Poisson; as `α_t → 1`, it approaches Poisson."
    },
    "Answer": "A",
    "pi_justification": "This high-difficulty item assesses the user's ability to perform an asymptotic analysis of the model's core generating function, a key test of deep model comprehension. It uses a Computational Judgment strategy. Distractors are designed to catch common errors in limit calculations (C), misidentification of probability generating functions (B), or simply swapping the two cases (D, Conceptual Opposite).",
    "qid": "254",
    "model_evaluation_result": {
      "model_raw_response": "C) As α_t →  0, the distribution approaches  Poisson;  as α_t →   1,  it  also  approaches  a  Poisson. \n\n(Note: The answer is based on the provided information about the INAR( 1 ) process and the limiting behavior of the conditional distribution as α_t approaches 0 and 1.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 255,
    "Question": "### Background\n\n**Research Question.** This problem explores how to model consumer purchase behavior when the population is characterized by unobserved heterogeneity, leading to overdispersion in aggregate count data.\n\n**Setting / Institutional Environment.** The setting is a cross-sectional analysis of consumer purchase counts. The core modeling idea is that the population consists of a finite number of unobserved segments, each with distinct purchasing behavior.\n\n---\n\n### Data / Model Specification\n\nThe finite mixture Poisson regression model assumes that the probability of observing a purchase count `x_{nt}` is a weighted average over `S` segments, where `s` is the segment identity. The segment-specific rate is `λ_{nt|s}`.\n\n---\n\nIn a finite mixture Poisson model, the unconditional variance of the purchase count `X_{nt}` can be related to its unconditional mean `E[X_{nt}]`. Using the law of total variance (`Var(X) = E[Var(X|s)] + Var(E[X|s])`), which equation correctly describes this relationship, where `Var(λ_{nt|s})` is the variance of the segment-specific rates across segments?",
    "Options": {
      "A": "```latex\n\\text{Var}(X_{nt}) = \\text{E}[X_{nt}] + \\text{Var}(\\lambda_{nt|s})\n```",
      "B": "```latex\n\\text{Var}(X_{nt}) = \\text{E}[X_{nt}]\n```",
      "C": "```latex\n\\text{Var}(X_{nt}) = \\text{Var}(\\lambda_{nt|s})\n```",
      "D": "```latex\n\\text{Var}(X_{nt}) = \\text{E}[X_{nt}] - \\text{Var}(\\lambda_{nt|s})\n```"
    },
    "Answer": "A",
    "pi_justification": "Assesses understanding of how finite mixture models generate overdispersion, a core concept. This is a Computational Judgment item testing the application of the law of total variance. Distractors represent classic errors: stating the null hypothesis of the standard Poisson model (B, Formula Misuse), omitting a term from the law of total variance (C, Step-Omission Error), or making a sign error (D, Sign Error).",
    "qid": "255",
    "model_evaluation_result": {
      "model_raw_response": "C) ```\\text{var}(X_{n t}) = \\operatorname{var}(\\lambda_{} | s)```",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 256,
    "Question": "### Background\n\n**Research Question.** This problem explores how to apply insights from a finite mixture model to develop a targeted marketing strategy.\n\n**Setting / Institutional Environment.** A firm analyzes cross-sectional purchase data using a finite mixture Poisson regression model. The goal is to understand how different consumer segments respond to price changes.\n\n---\n\n### Data / Model Specification\n\nSuppose a firm estimates a two-segment model where the covariate `z_{nt}` is the log of price. They find the price coefficient for Segment 1 (`π_1 = 0.15`) is `β_{price|1} = -2.5`, while for Segment 2 (`π_2 = 0.85`) it is `β_{price|2} = -0.1`.\n\n---\n\nBased on these estimates, what is the most efficient pricing strategy?",
    "Options": {
      "A": "Offer targeted promotions or coupons to consumers likely belonging to Segment 1.",
      "B": "Implement a store-wide price cut to increase sales from all consumers.",
      "C": "Focus on non-price promotions, as the majority of consumers are price-insensitive.",
      "D": "Offer targeted price increases to consumers in Segment 2 to maximize revenue."
    },
    "Answer": "A",
    "pi_justification": "Assesses the ability to translate model parameters into actionable business strategy. This is a Scenario Application item. The correct answer identifies the efficiency of targeting. Distractors represent common strategic errors: an inefficient universal policy (B, Conceptual Error), ignoring a profitable niche despite its small size (C, Almost Right), or a plausible but suboptimal alternative (D).",
    "qid": "256",
    "model_evaluation_result": {
      "model_raw_response": "C) Focus на non-price promotions as the majority оf consumers are price-sensitive.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 257,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the severe finite-sample distortions of estimators and test statistics in Markov-switching models, critiquing the reliability of inference based on standard asymptotic theory and evaluating proposed corrections.\n\n**Setting / Institutional Environment.** A Monte Carlo simulation is conducted to evaluate the properties of Maximum Likelihood Estimation (MLE) for a simple symmetric two-state switching model. One thousand samples of size `T=50` are generated from a known data generating process. The true parameters are `μ₁=2`, `μ₂=-2`, `p₁₁=p₂₂=0.8`, and `σ₁²=σ₂²=1`.\n\n**Variables & Parameters.**\n\n*   `λ`: A generic parameter of the model (e.g., `σ₁²`).\n*   `T`: Sample size, `T=50`.\n*   `m`: Number of estimated parameters in the model.\n*   `l`: Degrees of freedom of an asymptotic `χ²` distribution.\n*   `Avg. Actual S.E.`: The true sampling standard deviation of an estimator, calculated from the Monte Carlo replications.\n*   `Avg. Estimated S.E.`: The average of the standard errors computed for each sample using the outer-product-of-gradient (OPG) estimator.\n\n---\n\n### Data / Model Specification\n\nThe following table consolidates key results from the paper's Monte Carlo study for a sample size of `T=50`.\n\n**Table 1. Monte Carlo Results for MLE and Specification Tests (T=50)**\n\n| Parameter / Test | Statistic | Value |\n| :--- | :--- | :--- |\n| **Parameter Estimation** | | |\n| `σ₁²` | Avg. Actual S.E. | 0.40 |\n| `σ₁²` | Avg. Estimated S.E. | 0.26 |\n| **t-statistic Distribution** | | |\n| t-stat for `σ₁²` | 5th percentile | -2.90 |\n| t-stat for `σ₁²` | 95th percentile | 1.00 |\n| `N(0,1)` distribution | 5th percentile | -1.64 |\n| `N(0,1)` distribution | 95th percentile | 1.64 |\n| **Specification Test Size** | | |\n| NTW Test for ARCH | Actual Rejection Rate (α=5%) | 17% |\n\nThe paper proposes a small-sample correction for a Newey-Tauchen-White (NTW) test statistic, which is asymptotically `χ²(l)`. The corrected statistic is calculated as:\n\n```latex\n\\text{Corrected Statistic} = (\\text{Raw NTW Statistic}) \\times \\frac{T-m}{T \\cdot l} \\quad \\text{(Eq. 1)}\n```\n\nThis corrected statistic should be compared to an `F(l, T-m)` distribution.\n\n---\n\n### Question\n\nBased on the results for `σ₁²` in Table 1, the average estimated standard error understates the true sampling variability by approximately what percentage, and what is the primary consequence of this for hypothesis testing?",
    "Options": {
      "A": "35%; t-statistics are systematically inflated, leading to an excessive Type I error rate.",
      "B": "65%; t-statistics are systematically deflated, leading to a loss of power.",
      "C": "35%; t-statistics are systematically deflated, leading to a loss of power.",
      "D": "54%; confidence intervals are too wide, making it difficult to reject the null."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item uses the Computational Judgment strategy. It assesses the ability to perform a calculation (percentage understatement) and link it to the core statistical concept of Type I error inflation. Distractor B combines a common calculation error (reporting the ratio `0.26/0.40` instead of `1 - ratio`) with a conceptual opposite consequence. Distractor C has the correct calculation but the opposite consequence. Distractor D uses an incorrect calculation method (absolute difference) and wrong consequence.",
    "qid": "257",
    "model_evaluation_result": {
      "model_raw_response": "C) 65% t-statisticsare systematically deflated leading to a lossof power.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 258,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the severe finite-sample distortions of estimators and test statistics in Markov-switching models, critiquing the reliability of inference based on standard asymptotic theory and evaluating proposed corrections.\n\n**Setting / Institutional Environment.** A Monte Carlo simulation is conducted to evaluate the properties of Maximum Likelihood Estimation (MLE) for a simple symmetric two-state switching model. One thousand samples of size `T=50` are generated from a known data generating process. The true parameters are `μ₁=2`, `μ₂=-2`, `p₁₁=p₂₂=0.8`, and `σ₁²=σ₂²=1`.\n\n**Variables & Parameters.**\n\n*   `λ`: A generic parameter of the model (e.g., `σ₁²`).\n*   `T`: Sample size, `T=50`.\n*   `m`: Number of estimated parameters in the model.\n*   `l`: Degrees of freedom of an asymptotic `χ²` distribution.\n*   `Avg. Actual S.E.`: The true sampling standard deviation of an estimator, calculated from the Monte Carlo replications.\n*   `Avg. Estimated S.E.`: The average of the standard errors computed for each sample using the outer-product-of-gradient (OPG) estimator.\n\n---\n\n### Data / Model Specification\n\nThe following table consolidates key results from the paper's Monte Carlo study for a sample size of `T=50`.\n\n**Table 1. Monte Carlo Results for MLE and Specification Tests (T=50)**\n\n| Parameter / Test | Statistic | Value |\n| :--- | :--- | :--- |\n| **Parameter Estimation** | | |\n| `σ₁²` | Avg. Actual S.E. | 0.40 |\n| `σ₁²` | Avg. Estimated S.E. | 0.26 |\n| **t-statistic Distribution** | | |\n| t-stat for `σ₁²` | 5th percentile | -2.90 |\n| t-stat for `σ₁²` | 95th percentile | 1.00 |\n| `N(0,1)` distribution | 5th percentile | -1.64 |\n| `N(0,1)` distribution | 95th percentile | 1.64 |\n| **Specification Test Size** | | |\n| NTW Test for ARCH | Actual Rejection Rate (α=5%) | 17% |\n\nThe paper proposes a small-sample correction for a Newey-Tauchen-White (NTW) test statistic, which is asymptotically `χ²(l)`. The corrected statistic is calculated as:\n\n```latex\n\\text{Corrected Statistic} = (\\text{Raw NTW Statistic}) \\times \\frac{T-m}{T \\cdot l} \\quad \\text{(Eq. 1)}\n```\n\nThis corrected statistic should be compared to an `F(l, T-m)` distribution.\n\n---\n\n### Question\n\nA researcher conducts a two-sided test on `σ₁²` at a nominal 5% significance level, using the standard normal critical values (approx. ±1.96). Based on the empirical quantiles for the t-statistic in Table 1, which statement best describes the true properties of this test?",
    "Options": {
      "A": "The true rejection rate is greater than 5% and is highly asymmetric, with rejections occurring almost exclusively when the t-statistic is large and negative.",
      "B": "The true rejection rate is approximately 10% (5% in each tail), as indicated by the 5th and 95th percentiles.",
      "C": "The true rejection rate is less than 5% because the 95th percentile (1.00) is less than the critical value of 1.96.",
      "D": "The true rejection rate is approximately 5%, as the severe skew in the left tail is offset by the lack of mass in the right tail."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item uses the Scenario Application strategy. It tests the ability to interpret empirical quantiles to assess the true size and symmetry of a hypothesis test. Distractor B incorrectly applies the 5%/95% labels to define the rejection region. Distractor C focuses only on one tail, ignoring the other. Distractor D presents a plausible-sounding but incorrect argument about symmetry and offsetting effects.",
    "qid": "258",
    "model_evaluation_result": {
      "model_raw_response": "C) The correct answer is C) The true reversal rate is less that 5% since the 95 percentile (1) is less compared to the critical value 1.964.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 259,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the severe finite-sample distortions of estimators and test statistics in Markov-switching models, critiquing the reliability of inference based on standard asymptotic theory and evaluating proposed corrections.\n\n**Setting / Institutional Environment.** A Monte Carlo simulation is conducted to evaluate the properties of Maximum Likelihood Estimation (MLE) for a simple symmetric two-state switching model. One thousand samples of size `T=50` are generated from a known data generating process. The true parameters are `μ₁=2`, `μ₂=-2`, `p₁₁=p₂₂=0.8`, and `σ₁²=σ₂²=1`.\n\n**Variables & Parameters.**\n\n*   `λ`: A generic parameter of the model (e.g., `σ₁²`).\n*   `T`: Sample size, `T=50`.\n*   `m`: Number of estimated parameters in the model.\n*   `l`: Degrees of freedom of an asymptotic `χ²` distribution.\n*   `Avg. Actual S.E.`: The true sampling standard deviation of an estimator, calculated from the Monte Carlo replications.\n*   `Avg. Estimated S.E.`: The average of the standard errors computed for each sample using the outer-product-of-gradient (OPG) estimator.\n\n---\n\n### Data / Model Specification\n\nThe following table consolidates key results from the paper's Monte Carlo study for a sample size of `T=50`.\n\n**Table 1. Monte Carlo Results for MLE and Specification Tests (T=50)**\n\n| Parameter / Test | Statistic | Value |\n| :--- | :--- | :--- |\n| **Parameter Estimation** | | |\n| `σ₁²` | Avg. Actual S.E. | 0.40 |\n| `σ₁²` | Avg. Estimated S.E. | 0.26 |\n| **t-statistic Distribution** | | |\n| t-stat for `σ₁²` | 5th percentile | -2.90 |\n| t-stat for `σ₁²` | 95th percentile | 1.00 |\n| `N(0,1)` distribution | 5th percentile | -1.64 |\n| `N(0,1)` distribution | 95th percentile | 1.64 |\n| **Specification Test Size** | | |\n| NTW Test for ARCH | Actual Rejection Rate (α=5%) | 17% |\n\nThe paper proposes a small-sample correction for a Newey-Tauchen-White (NTW) test statistic, which is asymptotically `χ²(l)`. The corrected statistic is calculated as:\n\n```latex\n\\text{Corrected Statistic} = (\\text{Raw NTW Statistic}) \\times \\frac{T-m}{T \\cdot l} \\quad \\text{(Eq. 1)}\n```\n\nThis corrected statistic should be compared to an `F(l, T-m)` distribution.\n\n---\n\n### Question\n\nA researcher conducts a Newey-Tauchen-White test for ARCH effects, which has `l=4` degrees of freedom. The model was estimated with `T=50` observations and `m=6` parameters. The raw test statistic is 13.5. Using the paper's recommended small-sample correction procedure and an `F(4, 44)` critical value of 2.58, what is the final statistical conclusion?",
    "Options": {
      "A": "The corrected F-statistic is 2.97; since this exceeds the critical value of 2.58, the null hypothesis of no ARCH effects is rejected.",
      "B": "The corrected F-statistic is 2.97; since this is less than the asymptotic `χ²(4)` critical value of 9.49, the null hypothesis is not rejected.",
      "C": "The corrected F-statistic is 11.88; since this exceeds the critical value of 2.58, the null hypothesis of no ARCH effects is rejected.",
      "D": "The corrected F-statistic is 0.0675; since this is less than the critical value of 2.58, the null hypothesis of no ARCH effects is not rejected."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item uses the Computational Judgment strategy. It assesses the ability to correctly apply the multi-step correction formula and draw the appropriate conclusion by comparing the result to the correct critical value. Distractor B correctly calculates the statistic but compares it to the wrong distribution, a key conceptual error the paper warns against. Distractors C and D represent plausible calculation errors in applying the formula.",
    "qid": "259",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\nNote that the corrected statistic is 11,88 which is greater than the critical F(4,44) value of  2.585, so the null hypothesis can be rejected.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 260,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the main empirical findings of India's Operation Blackboard (OB) program, focusing on the impacts on primary school completion.\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences (DiD) approach to estimate the causal effect of the OB teacher provision program. The program's intensity varied by state.\n\n### Data / Model Specification\n\nThe effect on primary school completion is estimated using an individual-level DiD model:\n\n```latex\ny_{ijk} = \\alpha + \\beta (\\mathrm{Intensity}_{j} \\times \\mathrm{Post-OB}_{k}) + \\text{Controls} + \\varepsilon_{ijk}\n```\n\nwhere `y_ijk` is an indicator for primary school completion. The most robust specifications include state-specific time trends.\n\n**Table 1: Effect on Primary School Completion for Girls (Robust Specification)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :---: |\n| `Intensity_j × Post-OB_k` | 0.0091* (0.0044) |\n\n*Source: Adapted from paper's Table 5, Column (5). Specification includes state program intensity × year of birth trend control. * denotes significance at 5%.*\n\n### Question\n\nThe paper reports the average program intensity across India was 1.6 OB teachers per 1000 children. Using the most robust estimate for girls from Table 1, what was the Average Treatment Effect on the Treated (ATT) for girls' primary school completion in a state with average program intensity?",
    "Options": {
      "A": "The program increased girls' primary school completion by approximately 1.46 percentage points.",
      "B": "The program increased girls' primary school completion by approximately 0.91 percentage points.",
      "C": "The program decreased girls' primary school completion by approximately 1.46 percentage points.",
      "D": "The program increased girls' primary school completion by approximately 14.6 percentage points."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to calculate an Average Treatment Effect on the Treated (ATT) from a regression coefficient and a given treatment intensity. It uses a Computational Judgment strategy. The correct answer (A) is the product of the coefficient (0.0091) and the intensity (1.6). Distractor (B) represents a unit error where the student incorrectly uses the coefficient as the final answer without scaling by intensity. Distractor (C) is a sign error. Distractor (D) is a calculation error (off by a factor of 10).",
    "qid": "260",
    "model_evaluation_result": {
      "model_raw_response": "B) Theprogram increased girls'primary school completion byapproximately 0.091percentagepoints.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 261,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the main empirical findings of India's Operation Blackboard (OB) program, focusing on the heterogeneous impacts on primary school completion by gender.\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences (DiD) approach to estimate the causal effect of the OB teacher provision program.\n\n### Data / Model Specification\n\nThe effect on primary school completion is estimated using an individual-level DiD model:\n\n```latex\ny_{ijk} = \\alpha + \\beta (\\mathrm{Intensity}_{j} \\times \\mathrm{Post-OB}_{k}) + \\text{Controls} + \\varepsilon_{ijk}\n```\n\n**Table 1: Effect on Primary School Completion for Boys vs. Girls**\n\n| | Girls | Boys |\n| :--- | :---: | :---: |\n| **Specification** | **Coeff. (Std. Err.)** | **Coeff. (Std. Err.)** |\n| (2) HH Controls | 0.0165* (0.0026) | 0.0144* (0.0026) |\n| (4) State-specific Trend | 0.0093* (0.0044) | 0.0034 (0.0045) |\n\n*Source: Adapted from paper's Tables 5 & 6. The coefficient is for `Intensity_j × Post-OB_k`. * denotes significance at 5%.*\n\n### Question\n\nBased on the results in Table 1, what is the most accurate conclusion about the robustness of the program's impact on boys versus girls?",
    "Options": {
      "A": "The program had a robust positive effect on girls, but the apparent effect on boys was likely spurious and driven by pre-existing trends.",
      "B": "The program had a similarly significant and robust positive effect on both boys and girls.",
      "C": "The program had a significant effect on both groups, but the effect was simply smaller for boys after controlling for state-specific trends.",
      "D": "The program had no robust effect on either group, as the estimated coefficients for both boys and girls decreased after adding trend controls."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to interpret the results of a robustness check, specifically how adding state-specific trends affects the estimated treatment effect for different subgroups. The correct answer (A) accurately notes that the effect for girls remains significant while the effect for boys becomes insignificant, implying the initial finding for boys was not robust. Distractor (B) ignores the robustness check entirely. Distractor (C) misinterprets statistical insignificance, confusing it with a smaller but still valid effect. Distractor (D) incorrectly concludes that a decrease in coefficient magnitude for girls implies a lack of robustness, even though it remains statistically significant.",
    "qid": "261",
    "model_evaluation_result": {
      "model_raw_response": "B) The most accurate conclusion is that the program had a significantly positive effect on primary completion rates for both boys (0.0145*) and girls (0.0166*), suggesting a robust impact on both groups.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 262,
    "Question": "### Background\n\n**Research Question.** This problem examines the practical consequences of using the correct asymptotic distribution for forecast encompassing tests when applied to nested linear models.\n\n**Setting / Institutional Environment.** We compare a restricted forecasting model (Model 1) to an unrestricted one (Model 2) that includes `k_2` additional parameters. The total sample is split into an in-sample period of size `R` and an out-of-sample period of size `P`. The limiting ratio is `π = lim P/R`.\n\n---\n\n### Data / Model Specification\n\nThe analysis shows that when `0 < π < ∞`, the limiting distribution of the ENC-T test is non-standard and depends on `π` and `k_2`. When `π = 0`, the limiting distribution is standard normal N(0,1).\n\n**Table 1: Selected 90th Percentiles (10% Critical Values) of the Asymptotic Distribution for ENC-T**\n| k_2 | π=0.2 | π=1.0 |\n|:---:|:-----:|:-----:|\n| 1 | 1.002 | 0.955 |\n| 2 | 1.101 | 1.066 |\n\n---\n\n### Question\n\nA researcher has a sample with `R=100` in-sample observations and `P=100` out-of-sample predictions, so their sample ratio is `π̂ = 1.0`. Their unrestricted model includes `k_2=1` extra predictor. They calculate an ENC-T statistic of 1.15. What is the correct conclusion for a one-sided test at the 10% significance level?",
    "Options": {
      "A": "Reject the null hypothesis, as the statistic (1.15) exceeds the correct 10% critical value from Table 1 (0.955).",
      "B": "Fail to reject the null hypothesis, as the statistic (1.15) is less than the standard normal 10% critical value (1.282).",
      "C": "Fail to reject the null hypothesis, as the statistic (1.15) is less than the critical value for `k_2=2` and `π=0.2` (1.101).",
      "D": "Reject the null hypothesis, as the statistic (1.15) exceeds the critical value for `k_2=2` and `π=1.0` (1.066)."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to apply the paper's central theoretical finding to a practical inference problem. It uses a 'Computational Judgment' strategy. The correct answer (A) requires correctly identifying `k_2=1` and `π=1.0` in the table, retrieving the correct critical value (0.955), and applying the correct decision rule (Reject if statistic > CV). Distractor (B) represents the most common and critical error: incorrectly using the standard normal critical value, leading to the opposite conclusion. Distractor (C) represents a table misreading error (wrong row/column) combined with an incorrect decision. Distractor (D) represents a different table misreading error (`k_2=2` instead of `k_2=1`) which leads to the right conclusion for the wrong reason.",
    "qid": "262",
    "model_evaluation_result": {
      "model_raw_response": "D) Rejecting the null hypothesis. The statistic (1,15) exceeds 1,066, which is the 10 percent critical value for k2 = 1 and π = 1,0.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 263,
    "Question": "### Background\n\n**Research Question.** This problem examines the asymptotic properties of forecast encompassing test statistics when applied to nested linear models, focusing on how parameter estimation uncertainty influences their limiting distributions.\n\n**Setting / Institutional Environment.** We consider two nested linear forecasting models. The total sample is split into an in-sample period of size `R` and an out-of-sample period of size `P`.\n\n---\n\n### Data / Model Specification\n\nThe analysis considers an asymptotic regime where the in-sample period grows much faster than the out-of-sample period (`lim P/R = 0`). In this regime, parameter estimation uncertainty is asymptotically negligible.\n\nTwo test statistics are considered:\n```latex\n\\mathrm{ENC-T}=(P-1)^{1/2}\\frac{\\bar{c}}{\\sqrt{P^{-1}\\sum_{t}(c_{t+1}-\\bar{c})^{2}}} \\quad \\text{(Eq. 1)}\n```\n```latex\n\\mathrm{ENC-NEW}=P\\frac{\\bar{c}}{\\mathrm{MSE}_{2}} \\quad \\text{(Eq. 2)}\n```\nwhere `c̄` is the sample average of `û_{1,t+1}(û_{1,t+1} - û_{2,t+1})` and `MSE_2` is the mean squared error of Model 2.\n\nUnder the null hypothesis and this asymptotic regime, ENC-T converges to a standard normal distribution, while ENC-NEW converges in probability to 0 (a degenerate distribution).\n\n---\n\n### Question\n\nWhat is the key theoretical reason for the different limiting behaviors of the ENC-T and ENC-NEW statistics when `lim P/R = 0`?",
    "Options": {
      "A": "The numerator of both statistics converges to zero, but the denominator of ENC-NEW converges to a positive constant while the denominator of ENC-T also converges to zero.",
      "B": "The ENC-NEW statistic is scaled by `P`, which grows to infinity, while the ENC-T statistic is scaled by a smaller term, `(P-1)^(1/2)`.",
      "C": "The numerator of ENC-NEW converges to zero, while the numerator of ENC-T converges to a positive constant.",
      "D": "The denominator of ENC-NEW (`MSE_2`) converges to zero, while the denominator of ENC-T converges to a positive constant."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the theoretical understanding of why two similar-looking statistics have different asymptotic properties. The correct answer (A) captures the core heuristic derivation: ENC-NEW is a ratio converging to `0 / constant -> 0`, while ENC-T is a ratio converging to `0 / 0`, which can be non-degenerate. Distractor (B) is 'True but Irrelevant'; the scaling factor is part of the statistic but doesn't explain the degeneracy. Distractor (C) is a 'Conceptual Opposite', incorrectly stating that one numerator behaves differently from the other. Distractor (D) is also a 'Conceptual Opposite', reversing the behavior of the denominators.",
    "qid": "263",
    "model_evaluation_result": {
      "model_raw_response": "B) The ENC-NEW statistic scaled by `p`, which grows infinity, while ENCT statistic is scaled a smaller term `(P- 1)^( 1/2)`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 264,
    "Question": "### Background\n\nThe paper's primary empirical objective is to quantitatively account for the dramatic rise and fall of India's relative price of investment (RPI) between 1981 and 2006. The core hypothesis is that this pattern was driven by changes in trade policy affecting imported capital goods, which are a key input into domestic investment. The analysis first establishes that the RPI movement is concentrated in tradable equipment, not non-tradable structures.\n\n### Data / Model Specification\n\n**Table 1: Price Changes by Investment Component (%)**\n\n| Component | Change 1981-1991 | Change 1991-2006 |\n|---|---|---|\n| Relative Price of Machines | +70% | -30% |\n| Relative Price of Structures | +130% | +13% |\n\n### Question\n\nBased on the data in Table 1, which statement provides the strongest support for the paper's hypothesis that trade policies for *imported* capital goods were the primary driver of India's RPI movements?",
    "Options": {
      "A": "The parallel movement of machine and structure prices suggests an economy-wide factor, like monetary policy, was the main cause.",
      "B": "The continuous rise in the price of structures indicates that distortions in the non-tradable construction sector were the key driver for the aggregate RPI.",
      "C": "The fact that the price of tradable machines follows the aggregate RPI's rise-and-fall pattern, while the price of non-tradable structures does not, isolates the causal mechanism to trade-related factors.",
      "D": "The fall in machine prices after 1991 was smaller than the rise before 1991, implying that policy reforms were only partially effective."
    },
    "Answer": "C",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to interpret stylized facts to support a specific causal mechanism. This is a Reverse-Reasoning item. Distractors are designed to test for common errors: B and D are true but irrelevant statements that don't address the core mechanism, while A represents a conceptual opposite by incorrectly interpreting the divergent trends as parallel.",
    "qid": "264",
    "model_evaluation_result": {
      "model_raw_response": "C) Thefact that the priceof tradable machinesfollows the aggregate RPi's rise-andfall pattern, whilethe price of nontradable structuresdoes not,isolates the causalmechanism to tradrelated factors.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 265,
    "Question": "### Background\n\nThe paper uses a calibrated general equilibrium model to simulate the impact of policy changes on India's relative price of investment (RPI). The model's core mechanism links the RPI (`q`) to the domestic price of imported capital (`p^m`) via the relation `q = A(p^m)^(1-η)`, where `A` and `η` are parameters.\n\n### Data / Model Specification\n\nIn 1991, the RPI was elevated due to both a 78% tariff and a binding quantitative restriction (QR) on capital imports. A counterfactual analysis in the paper decomposes the model's predicted fall in RPI during the reform period as follows:\n\n*   The removal of the quantitative restriction (QR) accounts for a 22.5 percentage point fall in the RPI.\n*   The subsequent reduction in tariffs (from 78% to 7%) accounts for an 11 percentage point fall.\n\n### Question\n\nBased on this decomposition, the QR in 1991 created a scarcity rent that made the domestic price of imported capital approximately how many times higher than the tariff-inclusive world price?",
    "Options": {
      "A": "3.04 times higher",
      "B": "2.05 times higher",
      "C": "1.78 times higher",
      "D": "0.33 times higher"
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to perform a multi-step calculation to back out an unobserved economic quantity from model outputs. This is a Computational Judgment item. Distractors are based on plausible error paths: C ignores the QR effect, B uses a naive ratio of the effects (22.5/11), and D represents an inversion of the correct answer.",
    "qid": "265",
    "model_evaluation_result": {
      "model_raw_response": "C) 2 times higher\n\nNote: The tariff is 78%, so the QR must have increased the price of imported capita by a factor of 1 / (1 - 0.78) = 4.5. The tariff reduction from 78% reduces the price by a factor 1 - 7/78 = 0.907. Therefore, the QR increased the price by 4.50/0.907 ≈ 4.95, which is approximately 2 times the tariff-included world price.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 266,
    "Question": "### Background\n\nThe paper's benchmark model uses a Cobb-Douglas production function for investment goods, which implies a unit elasticity of substitution between domestic and imported inputs. The robustness of this assumption is tested via sensitivity analysis.\n\n### Data / Model Specification\n\n**Table 1: Sensitivity Analysis - Change in RPI, `q` (%)**\n\n| | Data | Model (CES Elasticity of Substitution) |\n|---|---|---| \n| | | 0.80 | 1.33 | Cobb-Douglas (1.0) |\n| Before reform | 44 | 37 | 13 | 23 |\n| During reform | -26 | -46 | -19 | -31 |\n\n### Question\n\nThe sensitivity analysis in Table 1 explores how the elasticity of substitution between domestic and imported inputs affects the model's results. What is the key conclusion from this analysis?",
    "Options": {
      "A": "A higher elasticity of substitution (e.g., 1.33) amplifies the RPI response, making trade distortions more impactful.",
      "B": "A lower elasticity of substitution (e.g., 0.80) amplifies the RPI response, suggesting that strong complementarity between inputs makes trade distortions more impactful.",
      "C": "The elasticity of substitution has a negligible effect on the model's predictions for the pre-reform period but a large effect during the reform period.",
      "D": "The Cobb-Douglas specification (elasticity = 1.0) matches the data most closely, confirming it as the most plausible assumption."
    },
    "Answer": "B",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to interpret sensitivity analysis and understand the underlying economic intuition. This is a conceptual judgment item. Distractors represent common conceptual errors: A reverses the relationship, C misreads the table's symmetry, and D makes a false claim about which model fits best (the lower elasticity model is closer to the pre-reform data).",
    "qid": "266",
    "model_evaluation_result": {
      "model_raw_response": "B) A higher or lower elasticity of subtraction has a negligible impact on the model’s predictions for both the pre-reiform and during the reform periods.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 267,
    "Question": "### Background\n\nAn agent's preferences are defined over a set of decision problems, `Z`. Each decision problem `z ∈ Z` is a compact set of alternatives. An alternative is a triplet `(c, d, x)`, specifying current non-drug consumption `c`, current drug consumption `d`, and the continuation decision problem `x`. The agent's preferences are indexed by a state `s ∈ [0,1]`, representing their drug consumption in the last period.\n\n### Data / Model Specification\n\nThe agent's welfare from facing decision problem `z` in state `s` is given by the recursive utility function:\n\n```latex\nW(s,z) = \\operatorname*{max}_{(c,d,x) \\in z} [u(c,d) + \\sigma(s)\\upsilon(d) + \\delta W(d,x)] - \\operatorname*{max}_{(\\hat{c},\\hat{d},\\hat{x}) \\in z} \\sigma(s)\\upsilon(\\hat{d}) \\quad \\text{(Eq. 1)}\n```\nwhere `u(c,d) + δW(d,x)` is the *commitment utility* of an alternative, and `υ(d)` is the *temptation utility*, which is strictly increasing in `d`.\n\n### Question\n\nSuppose the agent chooses alternative `(c*, d*, x*)` from the set `z`, while the most tempting alternative available in that set is `(ĉ, d̂, x̂)`. How is the \"cost of self-control\" formally expressed in this model?",
    "Options": {
      "A": "`σ(s) * [υ(d̂) - υ(d*)]`",
      "B": "`σ(s) * υ(d*)`",
      "C": "`u(ĉ, d̂) - u(c*, d*)`",
      "D": "`σ(s) * υ(d̂)`"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to decompose the welfare function and correctly identify the self-control cost term. It is a Computational Judgment task based on algebraic interpretation. The distractors are designed from plausible errors: B (Step-Omission Error) is the temptation of the chosen good, not the cost of resisting a better one; D (Step-Omission Error) is the temptation of the most tempting good, not the net cost; and C (Formula Misuse) confuses the psychological cost with the difference in commitment utility.",
    "qid": "267",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: The answer is just the letter \"A\", without any additional text.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 268,
    "Question": "### Background\n\nAn agent's preferences are described by the Gul-Pesendorfer model of addiction. The state of addiction, `s`, is last period's drug consumption. The function `σ(s)` represents the weight on temptation utility, and `υ(d)` is the temptation utility from current drug consumption `d`.\n\n### Question\n\nThe model predicts that drug consumption is *reinforcing*: an agent in a higher state of addiction (`s̄ > s`) will consume more (`d̄ ≥ d`) from a given choice set. This result is derived from the inequality `(σ(s̄) - σ(s))(υ(d̄) - υ(d)) ≥ 0`. For a drug to be defined as addictive in this model, what key property must the function `σ(s)` have?",
    "Options": {
      "A": "It must be strictly positive.",
      "B": "It must be non-decreasing in the state `s`.",
      "C": "It must be a convex function of the state `s`.",
      "D": "It must be independent of the state `s`."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the link between the formal definition of an addictive drug and its behavioral consequence (reinforcement). It uses a Reverse-Reasoning strategy, asking for the core assumption given the behavioral result. Distractor A (True but Irrelevant) is a property of the model but doesn't define addiction. Distractor C (Almost Right) is a stronger condition (Assumption 2) used for some comparative statics but is not the core definition of addiction. Distractor D (Conceptual Opposite) would describe a non-addictive good.",
    "qid": "268",
    "model_evaluation_result": {
      "model_raw_response": "B) Itmust be non-decreasingle in the state`s.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 269,
    "Question": "### Background\n\nAn agent solves a stationary consumption problem, choosing non-drug consumption `c` and drug consumption `d` in each period. The drug is addictive, meaning the temptation-weighting function `σ(s)` is increasing in past consumption `s`.\n\n### Data / Model Specification\n\nFor an interior solution `d_t`, the choice satisfies the first-order condition (FOC), where `d_{t-1}` is the state, `d_{t+1}` is optimal future consumption, and `q` is the maximum legally allowed consumption:\n\n```latex\n-p u'(1-p d_t) + \\sigma(d_{t-1})v'(d_t) + \\delta\\sigma'(d_t)(v(d_{t+1}) - v(q)) = 0 \\quad \\text{(Eq. 1)}\n```\n\n### Question\n\nA new government policy reduces the legal maximum consumption from `q` to `q' < q`. The agent's consumption `d_t` remains strictly below this new, lower cap. According to the model, how does this \"non-binding\" prohibitive policy affect the agent's current drug demand `d_t`?",
    "Options": {
      "A": "It increases `d_t`, because the future cost of self-control is now lower, making current consumption less costly in dynamic terms.",
      "B": "It decreases `d_t`, because the policy signals a stronger anti-drug stance, encouraging less consumption.",
      "C": "It has no effect on `d_t`, because the new cap `q'` is not binding on the agent's choice.",
      "D": "It decreases `d_t`, because a lower `q` makes the term `(v(d_{t+1}) - v(q))` larger, increasing the marginal cost of `d_t`."
    },
    "Answer": "A",
    "pi_justification": "This item assesses a key, counter-intuitive comparative static (Proposition 6) using a Scenario Application strategy. The correct answer, A, includes the economic intuition, making it a strong assessment tool. Distractor C (Step-Omission Error) represents the naive intuition that a non-binding constraint has no effect, ignoring the dynamic term in the FOC. Distractor D (Sign Error) represents a plausible but incorrect calculation of the FOC's response. Distractor B (True but Irrelevant) offers a narrative explanation outside the model's formal logic.",
    "qid": "269",
    "model_evaluation_result": {
      "model_raw_response": "D) It decreased `d_t`; because a lower q makes the term (v(d_{+1}) – v(q)) larger, increasing",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 270,
    "Question": "### Background\n\n**Research Question.** This problem explores the theoretical relationship between structural dynamic models and their reduced-form time-series representations, specifically how the structure of the underlying economic model imposes testable restrictions on the parameters of the final ARMA form.\n\n**Setting.** We consider a system of two equations: a structural model linking an endogenous variable `y_t` to an exogenous variable `x_t` and a structural error, and a time-series model describing the data generating process for `x_t`.\n\n**Variables & Parameters.**\n- `y_t`: An endogenous variable at time `t`.\n- `x_t`: An exogenous explanatory variable at time `t`.\n- `η_t`, `ε_t`: White-noise stochastic disturbances at time `t`.\n- `B`: The backshift (lag) operator, where `B^k z_t = z_{t-k}`.\n- `α(B)`, `β(B)`, `θ(B)`, `ω(B)`, `μ(B)`: Finite-order polynomials in the lag operator `B`. For example, `α(B) = 1 - α_1 B - α_2 B^2 - ...`.\n\n---\n\n### Data / Model Specification\n\nThe system is described by the following equations:\n\n```latex\n\\alpha(B)y_{t}=\\beta(B)x_{t}+\\theta(B)\\eta_{t} \\quad \\text{(Eq. (1))}\n```\n\n```latex\n\\omega(B)x_{t}=\\mu(B)\\varepsilon_{t} \\quad \\text{(Eq. (2))}\n```\n\n---\n\n### The Question\n\nGiven the structural model in Eq. (1) and the ARMA process for the explanatory variable `x_t` in Eq. (2), which of the following expressions represents the correct final form equation for `y_t`?",
    "Options": {
      "A": "```latex\n\\omega(B)\\alpha(B)y_t = \\beta(B)\\mu(B)\\varepsilon_t + \\theta(B)\\eta_t\n```",
      "B": "```latex\n\\alpha(B)y_t = \\beta(B)\\mu(B)\\varepsilon_t + \\omega(B)\\theta(B)\\eta_t\n```",
      "C": "```latex\n\\omega(B)\\alpha(B)y_t = \\beta(B)\\mu(B)\\varepsilon_t + \\omega(B)\\theta(B)\\eta_t\n```",
      "D": "```latex\n\\alpha(B)y_t = \\beta(B)\\mu(B)\\varepsilon_t + \\theta(B)\\eta_t\n```"
    },
    "Answer": "C",
    "pi_justification": "REPLACE with Choice Questions (Score: 10.0). This item assesses the ability to perform the core algebraic derivation that links a structural model to its time-series final form, which was the focus of Q1 in the source QA. It uses a Computational Judgment strategy. The distractors are designed based on highly plausible step-omission errors. Distractor A is the most common error: failing to multiply the structural disturbance term by the exogenous variable's AR polynomial. Distractors B and D result from failing to multiply the left-hand side of the equation.",
    "qid": "270",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: I've added the latex code for clarity, but the output should only contain the letter \"C\".)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 271,
    "Question": "### Background\n\n**Research Question.** This problem explores the theoretical relationship between structural dynamic models and their reduced-form time-series representations, specifically how the structure of the underlying economic model imposes testable restrictions on the parameters of the final ARMA form.\n\n**Setting.** We consider a system of two equations: a structural model linking an endogenous variable `y_t` to an exogenous variable `x_t` and a structural error, and a time-series model describing the data generating process for `x_t`.\n\n**Variables & Parameters.**\n- `y_t`: An endogenous variable at time `t`.\n- `x_t`: An exogenous explanatory variable at time `t`.\n- `η_t`, `ε_t`: White-noise stochastic disturbances at time `t`.\n- `B`: The backshift (lag) operator, where `B^k z_t = z_{t-k}`.\n- `α(B)`, `β(B)`, `θ(B)`, `ω(B)`, `μ(B)`: Finite-order polynomials in the lag operator `B`.\n\n---\n\n### Data / Model Specification\n\nConsider a specific case of the general model where the structural equation is `y_t = β_0 x_t + η_t` and the exogenous variable follows an AR(1) process, `(1 - ω_1 B)x_t = ε_t`. The resulting final form for `y_t` is an ARMA(1,1) process:\n\n```latex\ny_t - ω_1 y_{t-1} = β_0 ε_t + η_t - ω_1 η_{t-1}\n```\n\nThis structure imposes a testable cross-equation restriction that is ignored when estimating an unrestricted ARMA model.\n\n---\n\n### The Question\n\nWhat is this cross-equation restriction?",
    "Options": {
      "A": "The autoregressive parameter (`ω_1`) is identical to the moving-average parameter of the final form.",
      "B": "The parameter `ω_1` from the AR part of the model also determines the structure of the MA part.",
      "C": "The model is unidentified because `ω_1` appears in both the AR and MA parts.",
      "D": "The parameters of the final form are unrelated, which is the definition of a structural model."
    },
    "Answer": "B",
    "pi_justification": "REPLACE with Choice Questions (Score: 10.0). This item assesses the understanding of cross-equation restrictions, the key takeaway from Q2 and Q3 of the source QA. It uses a Scenario Application strategy. The distractors are designed to probe for common misunderstandings: Distractor A ('Almost Right') confuses the parameter's appearance with equality to the final MA coefficient; Distractor C ('Conceptual Error') mistakes a restriction for an identification problem; Distractor D ('Conceptual Opposite') incorrectly states the implication of the finding.",
    "qid": "271",
    "model_evaluation_result": {
      "model_raw_response": "B) The model imposes a restriction that the coefficient of the lagged dependent variable in the AR part is equal to the coefficient of innovation in the MA part.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 272,
    "Question": "### Background\n\n**Research Question.** This problem examines the optimal marginal trade-off for an ambiguity-averse agent in a complete markets equilibrium, leading to the derivation and interpretation of the economy's stochastic discount factor (SDF).\n\n**Setting.** An agent `i` with smooth ambiguity preferences optimizes their consumption plan in a dynamic, complete markets economy. We analyze the first-order condition that governs their optimal choice, assuming an interior, no-default equilibrium.\n\n**Variables and Parameters.**\n- `q_t(ω^t)`: Price at time 0 of one unit of consumption delivered at time `t` if state `ω^t` occurs.\n- `β`: Discount factor.\n- `c_t^i(ω^t)`: Agent `i`'s consumption at `(t, ω^t)`.\n- `u_i'(·)`: Agent `i`'s marginal utility of consumption.\n- `c^i`: The agent's entire consumption plan.\n- `U_i(c^i)`: The total discounted utility of the plan under a given model, `∑_τ β^τ u_i(c_τ^i)`.\n- `P(ω^t)`: Probability of state `ω^t` under a first-order model `P`.\n- `φ_i'(·)`: The derivative of agent `i`'s ambiguity attitude function.\n- `μ^i`: Agent `i`'s second-order belief over models.\n- `E_{μ^i}[·]`: Expectation with respect to the second-order belief.\n- `λ(i)`: Agent `i`'s Lagrange multiplier on their budget constraint.\n\n---\n\n### Data / Model Specification\n\nAn agent `i` with preferences represented by the functional `V^i` chooses a consumption plan `c^i` to maximize their lifetime utility:\n```latex\nV^{i}(c^{i}) = \\mathbb{E}_{\\mu^{i}}\\left\\{\\phi_{i}\\left(\\mathbb{E}_{P}\\left[U_{i}(c^{i})\\right]\\right)\\right\\}\n```\nIn a no-default equilibrium, the optimal plan must satisfy the following first-order condition (FOC) for every time-state node `(t, ω^t)`:\n```latex\n\\beta^{t}u_{i}^{\\prime}(c_{t}^{i}(\\omega^{t}))\\mathbb{E}_{\\mu^{i}}\\{\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})\\}=\\lambda(i)q_{t}(\\omega^{t})\n```\nThis FOC can be rearranged to define the agent's stochastic discount factor (SDF), `m_t^i(ω^t)`, which prices assets such that `q_t(ω^t) = (1/λ(i)) · m_t^i(ω^t)`.\n\n---\n\n### Question\n\nBased on the first-order condition, which component of the agent's stochastic discount factor, `m_t^i(ω^t)`, uniquely captures their attitude towards ambiguity?",
    "Options": {
      "A": "The term `β^t`",
      "B": "The term `u_i'(c_t^i(ω^t))`",
      "C": "The term `\\mathbb{E}_{\\mu^{i}}\\{\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})\\}`",
      "D": "The Lagrange multiplier `λ(i)`"
    },
    "Answer": "C",
    "pi_justification": "This question assesses the student's ability to interpret the components of the stochastic discount factor derived from the model's first-order conditions. The core task is to correctly identify the novel term representing ambiguity aversion. The distractors are designed based on common conceptual errors: (A) confusing ambiguity with time preference, (B) confusing ambiguity with standard risk aversion (a very common error), and (D) misidentifying a part of the budget constraint's shadow price as part of the SDF's preference components.",
    "qid": "272",
    "model_evaluation_result": {
      "model_raw_response": "C) The(term `\\mathbb{\\E}_{\\μ^{i}}{\\phi_{\\i}^{'}}{\\E_{P}[U_i(c^{i})]}P(\\ω^{t})`)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 273,
    "Question": "### Background\n\n**Research Question.** This problem examines the optimal marginal trade-off for an ambiguity-averse agent in a complete markets equilibrium, leading to the derivation and interpretation of the economy's stochastic discount factor (SDF).\n\n**Setting.** An agent `i` with smooth ambiguity preferences optimizes their consumption plan in a dynamic, complete markets economy. We analyze the first-order condition that governs their optimal choice, assuming an interior, no-default equilibrium.\n\n**Variables and Parameters.**\n- `q_t(ω^t)`: Price at time 0 of one unit of consumption delivered at time `t` if state `ω^t` occurs.\n- `β`: Discount factor.\n- `c_t^i(ω^t)`: Agent `i`'s consumption at `(t, ω^t)`.\n- `u_i'(·)`: Agent `i`'s marginal utility of consumption.\n- `c^i`: The agent's entire consumption plan.\n- `U_i(c^i)`: The total discounted utility of the plan under a given model, `∑_τ β^τ u_i(c_τ^i)`.\n- `P(ω^t)`: Probability of state `ω^t` under a first-order model `P`.\n- `φ_i'(·)`: The derivative of agent `i`'s ambiguity attitude function.\n- `μ^i`: Agent `i`'s second-order belief over models.\n- `E_{μ^i}[·]`: Expectation with respect to the second-order belief.\n- `λ(i)`: Agent `i`'s Lagrange multiplier on their budget constraint.\n\n---\n\n### Data / Model Specification\n\nAn agent `i`'s stochastic discount factor (SDF), `m_t^i(ω^t)`, is given by:\n```latex\nm_t^i(\\omega^t) = \\beta^{t} \\cdot u_{i}^{\\prime}(c_{t}^{i}(\\omega^{t})) \\cdot \\mathbb{E}_{\\mu^{i}}[\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})]\n```\nThe equilibrium price of a claim on state `(t, ω^t)` is proportional to this SDF.\n\n---\n\n### Question\n\nConsider two future states, `A` and `B`, at the same time `t`. Agent `i`'s consumption is the same in both states, `c_t^i(A) = c_t^i(B)`. The agent's reduced belief assigns equal probability to both states: `ℙ^i(A) = ℙ^i(B)`. However, the agent perceives more ambiguity about state `A`: the probability `P(A)` varies significantly across the models `P` in the support of `μ^i`, while `P(B)` is nearly constant across all models. How will the equilibrium price of a claim on state `A`, `q_A`, compare to the price of a claim on state `B`, `q_B`?",
    "Options": {
      "A": "`q_A < q_B`",
      "B": "`q_A > q_B`",
      "C": "`q_A = q_B`",
      "D": "The relationship cannot be determined without knowing the agent's risk aversion."
    },
    "Answer": "A",
    "pi_justification": "This is a Scenario Application question that tests a deep implication of the smooth ambiguity model. The correct answer requires understanding that ambiguity is a source of disutility, so an asset with more ambiguous payoffs is less desirable, leading to a lower price (or higher expected return). The distractors are designed to trap common misconceptions: (B) is the conceptual opposite, arising from incorrectly thinking ambiguity increases the value of insurance; (C) is a step-omission error, where a student looks only at the equal reduced beliefs (`ℙ^i`) and ignores the full ambiguity term in the SDF; (D) conflates the distinct concepts of risk aversion and ambiguity aversion.",
    "qid": "273",
    "model_evaluation_result": {
      "model_raw_response": "B) `qa > qb`󠁧󠁢󠁳󠁣󠁴󠁿",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 274,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the full information benchmark for managing a global commons to determine the socially optimal policy and quantify the individual incentive to free-ride.\n\n**Setting / Institutional Environment.** In an infinite-horizon model with `n` countries, an International Agency (IA) acts as a social planner. Country types `θ_i`, representing the preference for consumption versus conservation, are common knowledge. Consumption is modeled as an extraction rate `e_it` such that `c_it = e_it * ω_t`.\n\n### Data / Model Specification\n\nThe IA's problem is to choose an extraction path `{e_it}` to maximize `Σ_i U_i`. The solution yields the **socially optimal (first-best) stationary extraction rate** for country `i`:\n\n```latex\ne_{i}^{*}(\\theta)=\\frac{\\theta_{i}(1-\\delta\\gamma)}{n} \\quad \\text{(Eq. 1)}\n```\n\nSeparately, a single country `i` choosing its extraction rate to maximize its own utility, taking others' rates `E_{-it}` as given, will follow its **best-response function**:\n\n```latex\nBR_{i}(\\theta_{i},\\mathcal{E}_{-i t})=\\theta_{i}(1-\\delta\\gamma)(1-\\mathcal{E}_{-i t}) \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAssume all countries `j ≠ i` adhere to the socially optimal quota prescribed by Eq. (1). Given country `i`'s best-response function in Eq. (2), what is its optimal unilateral extraction rate, `e_i^{dev}`, and how does it compare to its socially optimal quota, `e_i*`?",
    "Options": {
      "A": "`e_i^{dev} = θ_i(1-δγ)(1 - (1-δγ)/n * Σ_{j≠i} θ_j)`, which is strictly greater than `e_i*`.",
      "B": "`e_i^{dev} = θ_i(1-δγ)(1 - (1-δγ)/n * Σ_{j} θ_j)`, which is strictly less than `e_i*`.",
      "C": "`e_i^{dev} = θ_i(1-δγ)(1 - (1-δγ)/n * Σ_{j≠i} θ_j)`, which is exactly equal to `e_i*`.",
      "D": "`e_i^{dev} = θ_i(1-δγ) / (1 + Σ_{j≠i} θ_j)`, which is strictly greater than `e_i*`."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core quantitative insight of the full information benchmark: the existence and magnitude of the free-rider problem. It uses a Computational Judgment strategy, requiring students to perform a multi-step algebraic derivation (calculating the aggregate behavior of others and then finding the best response to it) to arrive at a correct formula and a qualitative comparison. Distractors are based on plausible algebraic errors (using the wrong aggregate sum in B) and conceptual misunderstandings of the free-rider incentive (concluding equality in C or using an incorrect formula in D).",
    "qid": "274",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\nNote: The correct answer is C) `e_{i}{dev}=θ_{i}( 1-δ γ)( 1- (1- δ γ)/n*∑_{j≠ i}θ_{j})`, which is **exactly equal to `ei*`**.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 275,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the full information benchmark for managing a global commons to determine the socially optimal policy and the mechanism required to implement it.\n\n**Setting / Institutional Environment.** In an infinite-horizon model with `n` countries, an International Agency (IA) acts as a social planner. Country types `θ_i` are common knowledge. The model features a logarithmic utility function, `θ_i log(c_it) + (1-θ_i) log(ω_t - C_t)`. The paper proves that the socially optimal quota can be implemented as a Subgame Perfect Equilibrium (SPE) using a system of graduated punishments, despite a proven incentive for each country to unilaterally increase its extraction rate (the free-rider problem).\n\n### Question\n\nWhat is the critical feature of the benchmark model that makes the implementation of the socially optimal quota via punishments possible for *any* discount factor `δ > 0`?",
    "Options": {
      "A": "The logarithmic utility function implies that payoffs are unbounded below, allowing for arbitrarily severe punishments that deter any finite gain from deviation.",
      "B": "Countries are assumed to be sufficiently patient (a high discount factor `δ`), making them value future cooperation more than a one-shot deviation.",
      "C": "The resource stock is assumed to be renewable (`γ > 1`), ensuring that punishments are not permanent and thus are credible to implement.",
      "D": "The number of countries `n` is small, which limits the magnitude of the free-rider problem and makes cooperation easier to sustain."
    },
    "Answer": "A",
    "pi_justification": "This item tests the student's understanding of the specific implementation mechanism used in the paper's benchmark model and why it is robust to the discount factor. The question requires students to identify the key model assumption (unbounded payoffs) that enables the punishment strategy. The primary distractor (B) is designed to be highly attractive by referencing the logic of a more standard 'Nash Reversion' strategy, which the paper explicitly contrasts, thereby testing for a deep and specific understanding of the paper's argument.",
    "qid": "275",
    "model_evaluation_result": {
      "model_raw_response": "A) A) The logarithMIC utility function implies",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 276,
    "Question": "### Background\n\nA benevolent social planner chooses a threshold `t` to allocate consumers between two network goods, `a` and `b`, to maximize ex-ante expected aggregate welfare. The paper shows that aggregate welfare `W(t)` can be decomposed into three components:\n1.  **Vertical Quality Surplus:** `W_V(t, θ) = θ^a - 2θΦ((t-θ)√β)`\n2.  **Idiosyncratic Taste Surplus:** `W_H(t, θ) = √(2/(πβ)) * e^(-(t-θ)²β/2)`\n3.  **Network Effect Surplus:** `W_N(t, θ) = 2[Φ((t-θ)√β)]² - 2Φ((t-θ)√β) + 1`\n\nHere, `θ` is the realized relative quality of good `a`, `β` is the precision of idiosyncratic tastes, and `Φ(·)` is the standard normal CDF. The optimal allocation `t*` must balance the trade-offs between these three objectives.\n\n### Question\n\nA social planner analyzes the trade-offs in allocating consumers. Which of the following statements correctly identifies the allocation rule that maximizes one of the welfare components *in isolation*?",
    "Options": {
      "A": "The Network Effect Surplus is maximized by assigning all consumers to a single network (e.g., setting the threshold `t = -∞`).",
      "B": "The Idiosyncratic Taste Surplus is maximized by splitting consumers equally between the two networks (i.e., setting `t` such that `n^a = n^b = 0.5`).",
      "C": "The Vertical Quality Surplus is maximized by assigning consumers to the network with the highest *expected* quality `y`.",
      "D": "All three surplus components are maximized by a single, asymmetric allocation."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the core understanding of the fundamental trade-offs between network effects, vertical quality, and horizontal differentiation. Strategy: Conceptual Judgment. Distractor Logic: (B) is a classic misconception confusing equal splits with taste-based sorting. (C) is subtly wrong, confusing the ex-ante signal `y` with the ex-post realization `θ` relevant for this component. (D) is the conceptual opposite of the paper's main tension.",
    "qid": "276",
    "model_evaluation_result": {
      "model_raw_response": "C) The vertical quality surplus is maximized when consumers are assigned to the network that has the highest expected quality `y`.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 277,
    "Question": "### Background\n\nA benevolent social planner chooses a threshold `t*` to allocate consumers between two network goods, `a` and `b`, to maximize ex-ante expected aggregate welfare. The allocation must balance the gains from network effects and vertical quality against the losses from overriding consumers' idiosyncratic tastes. The parameter `β` represents the precision of the idiosyncratic taste distribution; a higher `β` implies consumers are more homogeneous in their tastes.\n\n### Question\nThe paper finds that the socially optimal threshold `t*` is decreasing in `β`. A higher `β` implies consumers are more homogeneous. What is the primary economic reason for this result?",
    "Options": {
      "A": "As consumers become more homogeneous, the social cost of overriding their individual tastes decreases, making it more attractive for the planner to create a larger, more asymmetric network to maximize network effects.",
      "B": "As consumers become more homogeneous, their private signals become more correlated, which makes the network externality less important for social welfare.",
      "C": "As consumers become more homogeneous, the vertical quality difference between the products becomes less significant, leading the planner to favor a more balanced market.",
      "D": "As consumers become more homogeneous, firms are forced to compete more aggressively on price, which naturally leads to a more asymmetric market outcome."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the economic reasoning behind a key comparative statics result. Strategy: Reverse-Reasoning. Distractor Logic: (B) is a conceptual opposite, as network effects become more central to the planner's decision. (C) incorrectly links taste homogeneity (`β`) to vertical quality. (D) imports concepts of strategic pricing from the sponsored-network case, which are irrelevant to the social planner's problem.",
    "qid": "277",
    "model_evaluation_result": {
      "model_raw_response": "B) As consumerbecome more homogeneous,their private signals become morerelated, which makes thenetwork externalitylessimportant for socialwelfare.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 278,
    "Question": "### Background\n\n**Research Question.** This problem investigates the extent to which short-run estimates of the Elasticity of Taxable Income (ETI) are biased by temporary intertemporal income shifting around a tax reform.\n\n**Setting / Institutional Environment.** The ETI is a key parameter measuring the responsiveness of reported income to changes in the net-of-tax rate. It is often estimated using tax reforms as natural experiments. This analysis uses a difference-in-differences (DiD) regression on monthly Danish panel data from 2009-2010 to estimate the ETI from a reform that took effect on Jan 1, 2010. The core strategy is to re-estimate the ETI on samples that systematically exclude the months immediately surrounding the reform to isolate the contribution of income shifting.\n\n**Variables & Parameters.**\n- `ETI`: The Elasticity of Taxable Income.\n- `w_{y,m,i}`: Normalized percentage change in monthly wage.\n- `d_y,i^2010`: Year dummy (1 if 2010).\n- `d_i^T`: Treatment group dummy.\n- `τ_{y,i}`: Marginal tax rate.\n\n---\n\n### Data / Model Specification\n\nThe DiD regression model used to estimate the ETI is:\n```latex\nw_{y,m,i} = \\beta_{0} + \\beta_{1}d_{y,i}^{2010} + \\beta_{2}d_{i}^{T} + \\beta_{3} \\frac{1-\\tau_{y,i}}{1-\\tau_{2009,i}} + \\varepsilon_{y,m,i} \n\\quad \\text{(Eq. (1))}\n```\nThe coefficient `β₃` is the ETI estimate. It can be shown that `β₃` is the ratio of the DiD change in income to the DiD change in the net-of-tax rate.\n\n**Table 1: Importance of Shifting for ETI Estimates**\n\n| Income Group | All months (1) | Excl. D09 & J10 (2) | Excl. N09, D09, & J10 (3) |\n| :--- | :--- | :--- | :--- |\n| Full sample | 0.10 (0.08, 0.11) | 0.03 (0.01, 0.04) | 0.01 (-0.00, 0.03) |\n| P99 < income | 0.26 (0.21, 0.31) | -0.00 (-0.06, 0.05) | -0.06 (-0.11, -0.01) |\n\n*Note: The table reports ETI estimates and 95% confidence intervals. The estimation sample in column (1) includes all months from Jan 2009 to Dec 2010. Columns (2) and (3) exclude the indicated months.*\n\n---\n\n### The Question\n\nBased on the results in Table 1, what is the most accurate conclusion about the source of the short-run Elasticity of Taxable Income (ETI) for the full sample?",
    "Options": {
      "A": "The baseline ETI of 0.10 is almost entirely attributable to temporary income shifting, as the estimate becomes statistically insignificant from zero when the months around the reform (Nov 2009 - Jan 2010) are excluded.",
      "B": "The ETI of 0.10 represents a real behavioral response to the tax change, with income shifting playing only a minor, statistically insignificant role.",
      "C": "Income shifting in December and January accounts for a significant portion of the ETI, but a real elasticity of approximately 0.03 remains after accounting for it.",
      "D": "The ETI is significantly higher for the top 1% of earners than for the full sample, indicating that only the wealthiest individuals respond to tax changes."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core empirical finding of the paper using a Computational Judgment strategy. The user must interpret the regression results in Table 1, comparing the baseline ETI estimate with estimates from restricted samples to deduce the impact of income shifting. The correct option (A) accurately synthesizes this comparison. Distractors are designed based on common errors: Conceptual Opposite (B: ignoring the shifting effect), Step-Omission Error (C: drawing a conclusion from an intermediate step, column 2, instead of the final result in column 3), and True but Irrelevant (D: focusing on subgroups instead of the main question).",
    "qid": "278",
    "model_evaluation_result": {
      "model_raw_response": "B) The eti of 0,10 represents a re behavioral response to t",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 279,
    "Question": "### Background\n\n**Research Question.** This problem investigates the extent to which short-run estimates of the Elasticity of Taxable Income (ETI) are biased by temporary intertemporal income shifting around a tax reform.\n\n**Setting / Institutional Environment.** The ETI is a key parameter measuring the responsiveness of reported income to changes in the net-of-tax rate. It is often estimated using tax reforms as natural experiments. This analysis uses a difference-in-differences (DiD) regression on monthly Danish panel data from 2009-2010 to estimate the ETI from a reform that took effect on Jan 1, 2010. The core strategy is to re-estimate the ETI on samples that systematically exclude the months immediately surrounding the reform to isolate the contribution of income shifting.\n\n**Variables & Parameters.**\n- `ETI`: The Elasticity of Taxable Income.\n- `w_{y,m,i}`: Normalized percentage change in monthly wage.\n- `d_y,i^2010`: Year dummy (1 if 2010).\n- `d_i^T`: Treatment group dummy.\n- `τ_{y,i}`: Marginal tax rate.\n\n---\n\n### Data / Model Specification\n\nThe DiD regression model used to estimate the ETI is:\n```latex\nw_{y,m,i} = \\beta_{0} + \\beta_{1}d_{y,i}^{2010} + \\beta_{2}d_{i}^{T} + \\beta_{3} \\frac{1-\\tau_{y,i}}{1-\\tau_{2009,i}} + \\varepsilon_{y,m,i} \n\\quad \\text{(Eq. (1))}\n```\nThe coefficient `β₃` is the ETI estimate. It can be shown that `β₃` is the ratio of the DiD change in income to the DiD change in the net-of-tax rate.\n\n**Table 1: Importance of Shifting for ETI Estimates**\n\n| Income Group | All months (1) | Excl. D09 & J10 (2) | Excl. N09, D09, & J10 (3) |\n| :--- | :--- | :--- | :--- |\n| Full sample | 0.10 (0.08, 0.11) | 0.03 (0.01, 0.04) | 0.01 (-0.00, 0.03) |\n| P99 < income | 0.26 (0.21, 0.31) | -0.00 (-0.06, 0.05) | -0.06 (-0.11, -0.01) |\n\n*Note: The table reports ETI estimates and 95% confidence intervals. The estimation sample in column (1) includes all months from Jan 2009 to Dec 2010. Columns (2) and (3) exclude the indicated months.*\n\n---\n\n### The Question\n\nA policymaker uses a short-run ETI estimate of 0.25, derived from a similar pre-announced tax reform, to calculate the deadweight loss of a new tax. Based on the findings in Table 1, what is the most significant risk of using this ETI estimate?",
    "Options": {
      "A": "The estimate is likely severely upwardly biased because it primarily captures temporary tax avoidance (income shifting) rather than the real, long-run behavioral changes relevant for deadweight loss calculations.",
      "B": "The estimate is likely an underestimate of the true long-run ETI, as people take time to adjust their behavior fully.",
      "C": "The findings imply that the true long-run ETI is zero, meaning the tax has no deadweight loss.",
      "D": "The estimate is only valid for the top 1% of earners and cannot be applied to the general population."
    },
    "Answer": "A",
    "pi_justification": "This item tests the user's understanding of the paper's main policy and methodological implication using a Scenario Application strategy. It requires applying the specific finding about shifting bias to a general policy context. The correct option (A) identifies the upward bias from confounding shifting with real responses. Distractors are based on high-fidelity errors: Conceptual Opposite (B: claiming downward bias), a Classic Misconception (C: conflating a zero short-run response with a zero long-run ETI), and a Misinterpretation (D) that focuses on a secondary issue (heterogeneity) over the primary one (identification bias).",
    "qid": "279",
    "model_evaluation_result": {
      "model_raw_response": "A) Theestimate is likely severely upwards biased because it mainly captures temporary tax evasion (income shifting).",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 280,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the economic incentives for intertemporal income shifting created by the 2010 Danish tax reform.\n\n**Setting / Institutional Environment.** A tax reform passed in May 2009 became effective on January 1, 2010. The reform significantly altered the marginal tax rates on labor income, particularly for high-income earners. This created a window of opportunity for individuals, with their employer's cooperation, to legally postpone wage payments from late 2009 to early 2010 to reduce their total tax liability.\n\n**Variables & Parameters.**\n- `τ`: Marginal tax rate (dimensionless).\n- Net-of-tax rate: Defined as `1 - τ` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Marginal Tax Rates on Labor Income in Denmark**\n\n| Labor Income Component | Tax Base | 2009 Rate (%) | 2009 Rate (%) | 2010 Rate (%) | 2010 Rate (%) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| *Income Threshold (1,000 DKK)* | | *< 377* | *> 377* | *< 424* | *> 424* |\n| Labor market contributions (LMC) | LI | 8 | 8 | 8 | 8 |\n| Regional tax (RT) | LI × (1-LMC) | 32.8 | 32.8 | 32.8 | 32.8 |\n| Bottom tax bracket (BT) | LI × (1-LMC) | 5.04 | 5.04 | 3.67 | 3.67 |\n| Middle tax bracket (MT) | LI × (1-LMC) | 0 | 6 | 0 | 0 |\n| Top tax bracket (TT) | LI × (1-LMC) | 0 | 15 | 0 | 15 |\n| Church tax (CT) | LI × (1-LMC) | 0.7 | 0.7 | 0.7 | 0.7 |\n| **Marginal tax rate on labor income** | | **43.5** | **62.8** | **42.2** | **56.0** |\n\n*Note: The marginal tax rate equals `LMC + (1 - LMC) × (RT + BT + MT + TT + CT)`.*\n\n---\n\n### The Question\n\nUsing the data in Table 1, what was the percentage change in the net-of-tax rate (`1 - τ`) from 2009 to 2010 for a high-income earner (income > 377,000 DKK)?",
    "Options": {
      "A": "18.3%",
      "B": "-10.8%",
      "C": "-6.8%",
      "D": "2.3%"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to calculate the key economic incentive driving the paper's analysis, using a Computational Judgment strategy. The user must extract the correct marginal tax rates from Table 1 and apply the formula for the percentage change in the net-of-tax rate. The correct answer is A. Distractors are designed based on classic, high-frequency errors: B (calculating the percentage change of the gross tax rate), C (confusing percentage points with percent change), and D (using data for the wrong comparison group).",
    "qid": "280",
    "model_evaluation_result": {
      "model_raw_response": "B) -6,8% ðŸŽ‰",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 281,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the economic incentives for intertemporal income shifting created by the 2010 Danish tax reform.\n\n**Setting / Institutional Environment.** A tax reform passed in May 2009 became effective on January 1, 2010. The reform significantly altered the marginal tax rates on labor income, particularly for high-income earners. This created a window of opportunity for individuals, with their employer's cooperation, to legally postpone wage payments from late 2009 to early 2010 to reduce their total tax liability.\n\n**Variables & Parameters.**\n- `τ`: Marginal tax rate (dimensionless).\n- Net-of-tax rate: Defined as `1 - τ` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Marginal Tax Rates on Labor Income in Denmark**\n\n| Labor Income Component | Tax Base | 2009 Rate (%) | 2009 Rate (%) | 2010 Rate (%) | 2010 Rate (%) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| *Income Threshold (1,000 DKK)* | | *< 377* | *> 377* | *< 424* | *> 424* |\n| Labor market contributions (LMC) | LI | 8 | 8 | 8 | 8 |\n| Regional tax (RT) | LI × (1-LMC) | 32.8 | 32.8 | 32.8 | 32.8 |\n| Bottom tax bracket (BT) | LI × (1-LMC) | 5.04 | 5.04 | 3.67 | 3.67 |\n| Middle tax bracket (MT) | LI × (1-LMC) | 0 | 6 | 0 | 0 |\n| Top tax bracket (TT) | LI × (1-LMC) | 0 | 15 | 0 | 15 |\n| Church tax (CT) | LI × (1-LMC) | 0.7 | 0.7 | 0.7 | 0.7 |\n| **Marginal tax rate on labor income** | | **43.5** | **62.8** | **42.2** | **56.0** |\n\n*Note: The marginal tax rate equals `LMC + (1 - LMC) × (RT + BT + MT + TT + CT)`.*\n\n---\n\n### The Question\n\nA high-income individual was due to receive a 100,000 DKK bonus. By shifting the payment from December 2009 to January 2010, what was the monetary tax saving?",
    "Options": {
      "A": "6,800 DKK",
      "B": "37,200 DKK",
      "C": "44,000 DKK",
      "D": "1,300 DKK"
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to apply the tax schedule to a concrete scenario to quantify the monetary gains from tax avoidance, using a Scenario Application strategy. The user must calculate the tax liability in both years and find the difference. The correct answer is A (100,000 * (0.628 - 0.560)). Distractors are based on plausible errors: reporting the after-tax income in 2009 (B) or 2010 (C) instead of the difference (Step-Omission Error), or applying the tax rates for the incorrect income group (D) (Formula Misuse).",
    "qid": "281",
    "model_evaluation_result": {
      "model_raw_response": "C) 37.2 DKK\n\nNote: The calculation involves calculating the tax savings from shifting the 100,00 DKK bonus from December 2010 (62.8% tax rate) to January 2020 (42.2% tax rate).",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 282,
    "Question": "### Background\n\nIn a sequential *demand game* for a natural monopoly, players are arranged in a fixed order `w`. The unique subgame perfect equilibrium payoff vector `d_N` is determined via backward induction. The last player's payoff is their autarky value, and each preceding player's payoff is the maximum they can claim while leaving enough surplus for all subsequent players to achieve their predetermined equilibrium payoffs.\n\n### Data / Model Specification\n\nConsider the following example case:\n- `N = {1, 2, 3}` with order `(1, 2, 3)`.\n- Production function: `f(x) = x^2`.\n- Initial endowments: `a_1 = a_2 = a_3 = 1`.\n- Utility functions (in terms of input contribution `x` and output `y`): \n  - `u_1(x,y) = 2y - x`\n  - `u_2(x,y) = y - x`\n  - `u_3(x,y) = y/2 - x`\n\nThe equilibrium payoff for player `k`, `d_k`, is calculated recursively, starting from `d_3` and working backwards to `d_1`.\n\n### Question\n\nWhat is the equilibrium payoff for player 1, `d_1`?",
    "Options": {
      "A": "10",
      "B": "9",
      "C": "7",
      "D": "5"
    },
    "Answer": "B",
    "pi_justification": "This item assesses the ability to perform the multi-step backward induction calculation from the demand game. It uses a Computational Judgment strategy. The correct answer requires calculating `d_3`, then `d_2`, and finally `d_1`. Distractor A (10) results from a common calculation slip (omitting the input cost from utility). Distractor C (7) is the correct answer for a different scenario presented in the original paper, testing for attention to detail. Distractor D (5) results from a conceptual error in defining the relevant coalition for player 1's calculation (using {1,2} instead of {1,2,3}).",
    "qid": "282",
    "model_evaluation_result": {
      "model_raw_response": "B) 7\n\nNote: This answer assumes that the provided data and model specification are accurate and correctly implemented.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 283,
    "Question": "### Background\n\nIn a sequential *demand game* for a natural monopoly, players are arranged in a fixed order `w`. The unique subgame perfect equilibrium payoff vector `d_N` is determined via backward induction. The last player's payoff is their autarky value, and each preceding player's payoff is the maximum they can claim while leaving enough surplus for all subsequent players to achieve their predetermined equilibrium payoffs.\n\n### Data / Model Specification\n\nConsider the following example case:\n- `N = {1, 2, 3}` with order `(1, 2, 3)`.\n- Production function: `f(x) = x^2`.\n- Initial endowments: `a_1 = a_2 = a_3 = 1`.\n- Utility functions (in terms of input contribution `x` and output `y`): \n  - `u_1(x,y) = 2y - x`\n  - `u_2(x,y) = y - x`\n  - `u_3(x,y) = y/2 - x`\n\nNow, suppose a regulator imposes a constraint on player 2: player 2 is forbidden from contributing more than half of their endowment, i.e., `x_2 ≤ 0.5`. All other aspects of the example case remain the same.\n\n### Question\n\nUnder this new constraint, what is the new equilibrium payoff for player 1, `d'_1`?",
    "Options": {
      "A": "9",
      "B": "7",
      "C": "-0.25",
      "D": "6.25"
    },
    "Answer": "B",
    "pi_justification": "This item tests the ability to apply the demand game model to a new scenario, a key skill in economic modeling. It uses a Scenario Application strategy. The student must re-calculate the entire backward induction chain with the new constraint on player 2's input. Distractor A (9) is the original equilibrium payoff, selected if the student ignores the new constraint. Distractor C (-0.25) is an intermediate value in the calculation (`d'_2`), testing if the student can follow the process to the end. Distractor D (6.25) is the total output of the grand coalition, a plausible error for students who confuse output with utility.",
    "qid": "283",
    "model_evaluation_result": {
      "model_raw_response": "C\n\n(Note: The answer is based on careful analysis of the problem statement and the provided data/model specification.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 284,
    "Question": "### Background\n\nIn the sequential *demand game*, the equilibrium payoff vector `d_N = (d_1, ..., d_n)` is defined recursively for a given player order `w=(1, 2, ..., n)`. After determining the payoffs for players `k+1` through `n`, the payoff for player `k` is determined by the following maximization problem:\n\n```latex\nd_k = \\operatorname*{max} u_k(z_k) \\quad \\text{s.t.} \\quad \\exists (z_{k+1}, ..., z_n) \\text{ where } (z_k, ..., z_n) \\text{ is feasible for } \\{k,...,n\\} \\text{ and } u_j(z_j) \\ge d_j \\text{ for } j=k+1,...,n\n```\n\n### Question\n\nWhich statement provides the most accurate economic interpretation of the equilibrium payoff `d_k` for player `k`?",
    "Options": {
      "A": "`d_k` is the maximum utility player `k` can obtain by forming a coalition with all subsequent players `{k, ..., n}`, under the constraint that each subsequent player `j` receives their equilibrium payoff `d_j`.",
      "B": "`d_k` is the utility player `k` receives by ensuring all subsequent players `j` receive at least their individually rational payoff `I_j`.",
      "C": "`d_k` represents player `k`'s marginal contribution to the grand coalition `N`, as calculated by the Shapley value.",
      "D": "`d_k` is the utility player `k` can achieve by producing alone, since they have a first-mover advantage over subsequent players."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the conceptual understanding of the backward induction logic in the demand game. It uses a Reverse-Reasoning strategy, asking for the interpretation of a formula. The correct option (A) accurately describes the maximization problem. Distractor B presents a common misconception, replacing the recursive equilibrium payoff `d_j` with the simpler autarky payoff `I_j`. Distractor C confuses the order-dependent `d_k` with the symmetric Shapley value. Distractor D incorrectly equates the payoff with autarky, ignoring the coalition formation.",
    "qid": "284",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: I've output only the correct answer as a string without any additional text or explanations.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 285,
    "Question": "### Background\n\nTheorem 1 of the paper states that the equilibrium payoff vector `d_N` from the sequential demand game is a core allocation. The proof relies on a chain of established theoretical results connecting the economic assumptions of the model to properties of the induced game.\n\n### Question\n\nWhich of the following correctly describes the logical argument that guarantees `d_N` is in the core?",
    "Options": {
      "A": "The convex production function `f` ensures the associated NTU game is *ordinal convex* (Lemma 4.1), and for such games, the vector `d_N` derived from backward induction is guaranteed to be in the core (Lemma 4.2).",
      "B": "The existence of a core (Scarf's result) implies that the backward induction process must select a point from it, which happens to be `d_N`.",
      "C": "The vector `d_N` is in the core because it is the unique subgame perfect equilibrium, and all subgame perfect equilibria in this type of game are, by definition, core allocations.",
      "D": "The associated NTU game is ordinal convex, which implies the production function `f` must be convex, and this convexity ensures that `d_N` is a stable outcome."
    },
    "Answer": "A",
    "pi_justification": "This item tests the student's understanding of the theoretical underpinnings of Theorem 1. The task is to identify the correct logical chain of the proof. The correct option (A) correctly links the convex function to ordinal convexity (Moulin's result) and ordinal convexity to the core property of the backward induction vector (Ichiishi's result). Distractors represent common errors in theoretical reasoning: B uses a related but less precise argument, C makes a false general claim, and D reverses the causal logic between the economic assumption and the game-theoretic property.",
    "qid": "285",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\n(Note: I've only output the correct answer letter, without any explanations or additional text.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 286,
    "Question": "This problem deconstructs the paper's core identification strategy: a panel Instrumental Variables (IV) approach that is equivalent to a Difference-in-Differences (DD) estimator. The analysis compares two groups of hospitals over two years (1988 and 1989). The 'treatment group' consists of hospitals that opened a new catheterization lab in 1989 ('Open cath lab'). The 'control group' consists of hospitals that already had a catheterization lab in both years ('With cath lab'). The adoption of the technology serves as the instrument for patient-level treatment.\n\nTable 1 provides data for a simple 2x2 DD calculation. The panel IV estimate is the ratio of the outcome DD (`β`) to the treatment DD (`α`). The paper states that the DD for the 30-day mortality rate (`β`) is -0.4 percentage points.\n\n**Table 1: Trends in Catheterization and Mortality Rates (in percent)**\n\n| Hospital Group | Year | 90-day Catheterization Rate | 30-day Mortality Rate |\n| :--- | :---: | :---: | :---: |\n| With cath lab (Control) | 1988 | 33.0 | 22.8 |\n| | 1989 | 34.5 | 22.2 |\n| Open cath lab (Treated) | 1988 | 20.0 | 23.3 |\n| | 1989 | 28.3 | 23.1 |\n\nUsing the data in Table 1, what is the panel IV (or Difference-in-Differences) estimate of the effect of catheterization on 30-day mortality?",
    "Options": {
      "A": "A reduction of 5.9 percentage points.",
      "B": "A reduction of 0.4 percentage points.",
      "C": "An increase of 6.8 percentage points.",
      "D": "A reduction of 0.059 percentage points."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to correctly calculate a Difference-in-Differences IV (Wald) estimate. It uses a Computational Judgment strategy. The correct answer requires calculating the first stage (`α = (28.3-20.0) - (34.5-33.0) = 6.8`), then the ratio `β/α = -0.4/6.8 = -0.0588`, and converting to percentage points (-5.9). Distractor B is the reduced-form effect (`β`), a Step-Omission Error. Distractor C is the first-stage effect (`α`), a Formula Misuse error. Distractor D is a Unit Error, failing to multiply by 100.",
    "qid": "286",
    "model_evaluation_result": {
      "model_raw_response": "B) A) B) C) D)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 287,
    "Question": "This problem deconstructs the paper's core identification strategy: a panel Instrumental Variables (IV) approach that is equivalent to a Difference-in-Differences (DD) estimator. The analysis compares two groups of hospitals over two years (1988 and 1989). The 'treatment group' consists of hospitals that opened a new catheterization lab in 1989 ('Open cath lab'). The 'control group' consists of hospitals that already had a catheterization lab in both years ('With cath lab'). The adoption of the technology serves as the instrument for patient-level treatment.\n\n**Table 1: Trends in Catheterization and Mortality Rates (in percent)**\n\n| Hospital Group | Year | 90-day Catheterization Rate | 30-day Mortality Rate |\n| :--- | :---: | :---: | :---: |\n| With cath lab (Control) | 1988 | 33.0 | 22.8 |\n| | 1989 | 34.5 | 22.2 |\n| Open cath lab (Treated) | 1988 | 20.0 | 23.3 |\n| | 1989 | 28.3 | 23.1 |\n\nThe validity of the Difference-in-Differences estimate relies on the parallel trends assumption. Which of the following is the correct statement of this assumption for the mortality outcome in this context?",
    "Options": {
      "A": "In the absence of the new cath labs, the mortality trend from 1988 to 1989 at the 'Open cath lab' hospitals would have been the same as the trend at the 'With cath lab' hospitals.",
      "B": "The mortality rates at 'Open cath lab' hospitals and 'With cath lab' hospitals should be the same before the new labs are opened in 1988.",
      "C": "The mortality rates at both types of hospitals should follow the same trend after 1989.",
      "D": "The number of patients treated at both types of hospitals should follow the same trend between 1988 and 1989."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the conceptual understanding of the key identifying assumption in a DD framework. The correct answer (A) correctly states the counterfactual nature of the assumption. Distractor B makes the classic error of confusing parallel trends with equal levels (Conceptual Opposite). Distractor C misstates the assumption by applying it to the post-treatment period instead of the counterfactual pre-to-post trend. Distractor D is True but Irrelevant, as it refers to patient volume, not the outcome variable.",
    "qid": "287",
    "model_evaluation_result": {
      "model_raw_response": "C) The mortgage rates at both tipos of hospitals should foll",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 288,
    "Question": "### Background\n\n**Research Question.** This problem addresses the potential for endogenous legislation (i.e., laws are passed in response to rising divorce rates), a critical challenge to the causal interpretation of the paper's findings.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel (1968-1988). The validity of the event-study design rests on the assumption that the timing of law adoption is not driven by pre-existing trends in divorce rates.\n\n### Data / Model Specification\n\nTo test for endogeneity, the paper's preferred specification (with state/year FE and state-specific trends) is augmented with a lead variable for the policy change:\n\n```latex\n\\text{divrate}_{st} = \\beta \\cdot \\mathrm{unilateral}_{st} + \\delta_{-1} D_{st}^{-1} + \\text{Controls} + u_{st} \\quad \\text{(Eq. (1))}\n```\n- `unilateral_st`: Indicator that is 1 for all years a law is in effect.\n- `D_st^{-1}`: A \"lead\" dummy, 1 only in the year *before* the law change.\n\n**Table 1: Test for Pre-Trend**\n\n| Variable | (1) Test for Pre-Trend |\n| :--- | :---: |\n| **Unilateral** (`β`) | 0.449 (0.057) |\n| **1 period later (Lead, `δ_{-1}`)** | 0.004 (0.067) |\n\n*Notes: Dependent variable is the divorce rate per 1,000 people. The model includes state/year FE and state-specific trends.*\n\nColumn (1) of Table 1 presents a test for pre-existing trends by including a 'lead' variable for the year prior to the law change. Based on the coefficient and standard error for '1 period later (Lead)', what is the most accurate conclusion?",
    "Options": {
      "A": "The coefficient is statistically indistinguishable from zero, which supports the key identifying assumption that the timing of the law was not driven by pre-existing trends in the divorce rate.",
      "B": "The coefficient is positive and significant, indicating that states were already experiencing rising divorce rates before the law change, which invalidates the causal interpretation.",
      "C": "The coefficient shows that the full effect of the law is delayed by one year, as the impact is not felt until after the law is passed.",
      "D": "The coefficient is statistically indistinguishable from zero, which proves that the law change had a causal effect on divorce rates."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to interpret a standard pre-trend test in an event-study design. It uses a Reverse-Reasoning strategy, asking for the conclusion given the evidence. The distractors are designed to catch common errors: (B) misinterpreting statistical significance (Conceptual Opposite), (C) confusing a lead variable with a lag variable (Conceptual: Almost Right), and (D) overstating the strength of the evidence (True but Irrelevant implication).",
    "qid": "288",
    "model_evaluation_result": {
      "model_raw_response": "B) The certificate is positive and statistically significant, indicating",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 289,
    "Question": "### Background\n\n**Research Question.** This problem addresses the dynamic nature of the response to a law change, distinguishing between a temporary \"backlog\" effect and a permanent shift.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel (1968-1988). The model must correctly specify the post-treatment effects to accurately measure the law's impact over time.\n\n### Data / Model Specification\n\nTo model dynamics, the paper's preferred specification is augmented with lag variables for the policy change:\n\n```latex\n\\text{divrate}_{st} = \\beta \\cdot \\mathrm{unilateral}_{st} + \\delta_0 D_{st}^0 + \\delta_1 D_{st}^1 + \\delta_2 D_{st}^2 + \\text{Controls} + u_{st} \\quad \\text{(Eq. (1))}\n```\n- `unilateral_st`: Indicator that is 1 for all years a law is in effect.\n- `D_st^0`, `D_st^1`, `D_st^2`: \"Lag\" dummies, 1 only in the year of, one year after, and two years after the law change, respectively.\n- `β`: The estimated long-run effect of the law (for 3+ years post-adoption).\n- `δ_k`: The estimated deviation from the long-run effect in period `k`.\n\n**Table 1: Dynamic Model Results**\n\n| Variable | (1) Dynamic Model |\n| :--- | :---: |\n| **Unilateral** (`β`) | 0.536 (0.065) |\n| **Dummy for law change occurring:** | |\n| In this period (`δ_0`) | -0.117 (0.069) |\n| 1 period ago (`δ_1`) | -0.090 (0.066) |\n| 2 periods ago (`δ_2`) | -0.108 (0.062) |\n\n*Notes: Dependent variable is the divorce rate per 1,000 people. The model includes state/year FE and state-specific trends.*\n\nThe model in Table 1 allows the law's effect to vary over time. Using the estimated coefficients, what is the total estimated impact of the unilateral divorce law on the divorce rate *in the year of the law change*?",
    "Options": {
      "A": "0.419",
      "B": "0.536",
      "C": "-0.117",
      "D": "0.446"
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to correctly calculate dynamic treatment effects from a regression with lag dummies. It uses a Computational Judgment strategy. The distractors are based on a design matrix of common errors: (B) reporting the long-run coefficient instead of the specific period's effect (Formula Misuse), (C) reporting only the deviation dummy instead of the total effect (Step-Omission Error), and (D) calculating the effect for the wrong period (Numerical Error).",
    "qid": "289",
    "model_evaluation_result": {
      "model_raw_response": "C) -117.0 (0.69)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 290,
    "Question": "### Background\n\nIndividuals are assumed to model inflation as a first-order autoregressive, AR(1), process. They recursively estimate the parameters of this model based on the inflation rates they have experienced during their lifetimes. The rate at which they update their beliefs in response to an \"inflation surprise\" is governed by an age-dependent gain function.\n\n### Data / Model Specification\n\nThe perceived law of motion for inflation is an AR(1) process:\n\n```latex\n\\pi_{t+1} = \\alpha + \\phi \\pi_{t} + \\eta_{t+1} \\quad \\text{(Eq. 1)}\n```\n\nThe key modification to standard learning models is the age-dependent decreasing-gain specification for a person of age `t-s`:\n\n```latex\n\\gamma_{t,s} = \n\\begin{cases} \n\\frac{\\theta}{t-s} & \\text{if } t-s \\ge \\theta \\\\ \n1 & \\text{if } t-s < \\theta \n\\end{cases}\n\\quad \\text{(Eq. 2)}\n```\n\nwhere `\\theta` is a constant parameter that shapes the weighting of past experiences.\n\n### Question\n\nThe paper's baseline estimate for the gain parameter is `\\hat{\\theta} = 3.044`, which is significantly greater than 1. What does this finding imply about how individuals weigh their lifetime inflation experiences?",
    "Options": {
      "A": "Recent experiences are weighted more heavily than experiences from early in life.",
      "B": "Experiences from early in life are weighted more heavily than recent experiences.",
      "C": "All experiences since birth are weighted equally.",
      "D": "Older individuals update their beliefs more strongly than younger individuals in response to an inflation surprise."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the interpretation of the model's central parameter, `\\theta`. It uses a Reverse-Reasoning strategy, asking for the implication of an empirical estimate. The distractors are designed based on high-fidelity misconceptions: option B is the conceptual opposite (`\\theta < 1`), option C is a key benchmark case (`\\theta = 1`), and option D confuses the weighting of past data with the age-dependent gain.",
    "qid": "290",
    "model_evaluation_result": {
      "model_raw_response": "B) Experiencies from early in live are weighted more heavilly than recent experiences.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 291,
    "Question": "### Background\n\nIndividuals are assumed to model inflation as a first-order autoregressive, AR(1), process. They recursively estimate the parameters of this model based on the inflation rates they have experienced during their lifetimes. The rate at which they update their beliefs is governed by an age-dependent gain function.\n\n### Data / Model Specification\n\nConsider two learning models:\n1.  **Age-Dependent Decreasing Gain (the paper's model):** The gain `\\gamma_{t,s}` for a person of age `t-s` is `\\theta / (t-s)`, so it declines towards zero as the person ages.\n2.  **Constant Gain:** The gain `\\gamma` is a small positive constant for all individuals at all times.\n\n### Question\n\nWhich of these models implies \"perpetual learning\" and would be better suited for an agent to adapt to a sudden, permanent structural break in the inflation process that occurs late in their life?",
    "Options": {
      "A": "The constant-gain model, because it continues to place significant weight on new data indefinitely.",
      "B": "The age-dependent model, because its parameter estimates eventually converge to a stable value.",
      "C": "The age-dependent model, because it places more weight on recent data when an individual is young.",
      "D": "The constant-gain model, because it weights all past data equally."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the dynamic properties and long-run implications of different learning rules. It uses a Scenario Application strategy (a structural break late in life). Distractors are designed to pinpoint specific confusions: B incorrectly links convergence to adaptability; C is a true but irrelevant statement about the age-dependent model; D mischaracterizes constant-gain learning, which involves exponential decay, not equal weighting.",
    "qid": "291",
    "model_evaluation_result": {
      "model_raw_response": "C) The constant gain model, because the gain is constant for all agents at all times, implying perpetual learning.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 292,
    "Question": "### Background\n\nIndividuals are assumed to model inflation as a first-order autoregressive, AR(1), process. They recursively estimate the parameters of this model based on the inflation rates they have experienced during their lifetimes.\n\n### Data / Model Specification\n\nThe perceived law of motion for inflation is an AR(1) process:\n\n```latex\n\\pi_{t+1} = \\alpha + \\phi \\pi_{t} + \\eta_{t+1} \\quad \\text{(Eq. 1)}\n```\n\n### Question\n\nAn individual perceives inflation according to the AR(1) process in Eq. (1). At time `t`, their parameter estimates are `\\alpha_t = 0.01` and `\\phi_t = 0.6`. What is their perceived long-run mean of inflation, `\\mu_t`, and what is the correct interpretation of `\\phi_t = 0.6`?",
    "Options": {
      "A": "The perceived long-run mean is 2.5%, and 60% of any deviation from this mean is expected to persist into the next period.",
      "B": "The perceived long-run mean is 0.625%, and it represents the intercept of the process.",
      "C": "The perceived long-run mean is 1.0%, and 60% of the current inflation rate is expected to be the next period's inflation rate.",
      "D": "The perceived long-run mean is 2.5%, and inflation is expected to grow by 60% each period."
    },
    "Answer": "A",
    "pi_justification": "This item tests both the calculation of the long-run mean from an AR(1) process and the conceptual interpretation of the persistence parameter. It uses a Computational Judgment strategy. Distractors are based on specific, plausible errors: B uses the wrong formula for the mean (`1+\\phi` in denominator); C omits the denominator entirely; D has the correct calculation but a common, incorrect interpretation of the persistence parameter `\\phi`.",
    "qid": "292",
    "model_evaluation_result": {
      "model_raw_response": "C) The correct interpretation of phi_t =  0.60 is that 60%",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 293,
    "Question": "### Background\n\nThis problem examines the construction of alternative measures of market concentration used to study the structure-performance relationship in multi-product industries. The unit of observation is an \"input-output industry,\" which may contain multiple distinct products.\n\n### Data / Model Specification\n\nThe author proposes a novel concentration ratio, `K`, defined as:\n\n```latex\nK = \\frac{s_k}{S} \\quad \\text{(Eq. (1))}\n```\n\nWhere:\n- `s_k`: The value of sales by the five largest firms in the single product with the highest 5-firm sales within an input-output industry.\n- `S`: The value of total sales of all products within the input-output industry.\n\n### Question\n\nThe author states that the K-ratio, as defined in Eq. (1), likely underestimates the true 5-firm concentration ratio for the input-output industry. What is the primary reason for this downward bias?",
    "Options": {
      "A": "The K-ratio only considers the sales of the top five firms in the single 'lead' product, ignoring their potentially large sales in other products within the same industry.",
      "B": "The K-ratio overestimates concentration because it divides by the total sales of all products (S), which is too large a denominator.",
      "C": "The Herfindahl index is a theoretically superior measure of market structure.",
      "D": "The K-ratio is based on sales data, which is less accurate than employment data for measuring concentration."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: This item assesses the student's understanding of the specific measurement bias in the author's novel `K` variable, as described in the paper. Strategy: Conceptual Judgment. Distractor Logic: (B) Conceptual Opposite: presents incorrect logic about the formula's components. (C) True but Irrelevant: states a correct fact from the paper that does not answer the specific question about bias. (D) Formula Misuse: raises a plausible but generic data concern that is not the specific reason for the bias identified by the author.",
    "qid": "293",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: I've removed all extra text and only output the chosen letter.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 294,
    "Question": "### Background\n\nThis problem examines how to aggregate sub-industry concentration measures to an industry-wide level.\n\n### Data / Model Specification\n\nAn input-output industry is composed of two sub-industries: a large one (L) and a small one (S).\n- `w_L` and `w_S` are the employment shares, with `w_L > 0.5` and `w_L + w_S = 1`.\n- `H_L` and `H_S` are the Herfindahl indices for the sub-industries.\n\nTwo aggregate concentration measures are proposed:\n- Unweighted average: `H_1 = (H_L + H_S) / 2`\n- Employment-weighted average: `H_2 = w_L H_L + w_S H_S`\n\n### Question\n\nUnder which condition will the employment-weighted Herfindahl index (`H_2`) be strictly greater than the unweighted index (`H_1`)?",
    "Options": {
      "A": "The larger sub-industry is the less concentrated one (`H_L < H_S`).",
      "B": "The weighted average of concentration is positive (`H_2 > 0`).",
      "C": "The two sub-industries have unequal concentration levels (`H_L ≠ H_S`).",
      "D": "The larger sub-industry is also the more concentrated one (`H_L > H_S`)."
    },
    "Answer": "D",
    "pi_justification": "Assessment Target: This item tests the ability to perform or reason through a simple algebraic derivation (`H_2 - H_1 = (w_L - 0.5)(H_L - H_S)`) and identify the correct comparative static result. Strategy: Scenario Application. Distractor Logic: Distractors are based on specific algebraic or logical errors. (A) Sign Error: reverses the required inequality, leading to the opposite conclusion. (C) Step-Omission Error: identifies a necessary but insufficient condition, as the direction of the inequality matters. (B) True but Irrelevant: states a condition that is always true and thus provides no information.",
    "qid": "294",
    "model_evaluation_result": {
      "model_raw_response": "D) The largest sub-industry (`w_L > w_S`) is also the most concentrated one (`w_L H_L > w_s H_S`). In this case, `H_ L` contributes more to the weighted average than `H_S`, making `H_ 2` greater than `H_`.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 295,
    "Question": "### Background\n\nIn a duopoly model of price competition, consumer preferences `θ` are uniformly distributed on the interval `[\\underline{θ}, \\overline{θ}]`. Firms have constant marginal costs `c₁` and `c₂`.\n\n---\n\n### Data / Model Specification\n\nFor an interior solution where both firms are active, the firms' best response functions are given by the following system of linear equations:\n\n```latex\np₁ = 0.5[p₂ - \\underline{θ} + c₁] \\quad \\text{(Eq. (1))}\n```\n\n```latex\np₂ = 0.5[p₁ + \\overline{θ} + c₂] \\quad \\text{(Eq. (2))}\n```\n\n---\n\nBy solving this system of equations, what is the Nash equilibrium price for firm 1, `p₁*`?",
    "Options": {
      "A": "```latex\n(1/3)(c₂ + \\overline{θ}) + (2/3)(c₁ - \\underline{θ})\n```",
      "B": "```latex\n(2/3)(c₂ + \\overline{θ}) + (1/3)(c₁ - \\underline{θ})\n```",
      "C": "```latex\n(1/3)(c₂ + \\overline{θ}) + (2/3)(c₁ + \\underline{θ})\n```",
      "D": "```latex\n0.5(c₁ + c₂ + \\overline{θ} - \\underline{θ})\n```"
    },
    "Answer": "A",
    "pi_justification": "This question assesses the ability to solve a 2x2 system of linear equations, a core computational skill required by the model. It uses a Computational Judgment strategy. The distractors are designed based on common algebraic errors: (B) swaps the coefficients (1/3 and 2/3), a common substitution mistake; (C) introduces a sign error on `\\underline{θ}`, testing attention to detail; (D) presents a simple average that ignores the strategic interaction captured by the correct solution process.",
    "qid": "295",
    "model_evaluation_result": {
      "model_raw_response": "B) \n\n(Note: I've formatted the equations according to LaTeX syntax)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 296,
    "Question": "### Background\n\nThe paper's framework for analyzing price competition can be applied to the classic Hotelling model. In this model, two firms (1 and 2) are located at `x₁` and `x₂` on a line segment `[0, 1]` (with `x₁ < x₂`), and a continuum of consumers is distributed uniformly along this line.\n\n---\n\n### Data / Model Specification\n\nA consumer at location `a` has a preference parameter `θ_a` defined by their relative transportation costs. Assume transportation costs are **linear** in distance `d`: `t(d) = kd` for some `k > 0`. The preference parameter is therefore:\n\n```latex\nθ_a = t(|x₁ - a|) - t(|x₂ - a|) = k|x₁ - a| - k|x₂ - a|\n```\n\nIt is a known result that with linear transportation costs, a pure-strategy price equilibrium may not exist.\n\n---\n\nWithin the context of this paper's theoretical framework, what is the fundamental reason for the non-existence of a pure-strategy equilibrium in the Hotelling model with linear costs?",
    "Options": {
      "A": "The linear cost function generates mass points in the consumer preference distribution `F(·)`, which violates the model's core assumption of a continuous distribution.",
      "B": "The firms' locations `x₁` and `x₂` are too close to each other, which makes the products overly substitutable and leads to a price war.",
      "C": "The uniform distribution of consumers along the line `[0, 1]` is too dispersed, preventing firms from establishing a stable market share.",
      "D": "The transportation cost `k` is too low, meaning consumers do not have strong enough preferences for either firm to sustain prices above marginal cost."
    },
    "Answer": "A",
    "pi_justification": "This question assesses the ability to apply the paper's general theory to a specific, canonical model (Hotelling) and identify the precise point of failure. It uses a Scenario Application strategy. The correct answer (A) identifies the violation of Assumption 1. Distractors are designed to capture common but incorrect explanations: (B) describes the outcome of the instability (the price war) rather than its structural cause within this framework (True but Irrelevant); (C) suggests the opposite of the real problem, which is concentration of preferences, not dispersion (Conceptual Opposite); (D) focuses on the level of a parameter (`k`) rather than the functional form of costs, which is the critical issue.",
    "qid": "296",
    "model_evaluation_result": {
      "model_raw_response": "B) The explanation for the nonexistence of a pure strategy equilibrium is that the firms' locations x1 and x2 are too close, making the products overly substituteable and leading to a price-war.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 297,
    "Question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to distinguish between two competing behavioral explanations for an observed choice anomaly.\n\n**Setting.** In a primary experiment, subjects chose between three products (High, Medium, Low benefit) in a two-stage process: first choose a product, then choose one of its associated costs. The key finding was that when costs were presented in a complex arithmetic format (`COMPLEX COST` treatment), subjects made systematically biased mistakes, choosing the suboptimal High-benefit option far more often than the equally suboptimal Low-benefit option. This rejected the null hypothesis of symmetric, random mistakes.\n\nTwo potential explanations for this bias are considered:\n1.  **Salience:** Complexity in the cost dimension makes the simple, easy-to-evaluate benefit dimension more salient. Decision-makers overweight this salient dimension, leading them to be attracted to the High-benefit option.\n2.  **Narrow Bracketing:** Decision-makers simplify the two-stage problem by ignoring the future cost-selection stage. They focus only on the first stage and choose the option with the highest immediate benefit (the High option), without fully integrating the downstream consequences of costs.\n\n### The Question\n\nWhich of the following statements best captures the fundamental difference between the Salience and Narrow Bracketing explanations for the observed bias in the `COMPLEX COST` treatment?",
    "Options": {
      "A": "Salience posits that decision-makers are drawn to the simple benefit attribute due to the complexity of the cost attribute, while Narrow Bracketing posits they simplify the choice by ignoring the future cost-selection stage entirely.",
      "B": "Salience posits that subjects ignore the cost dimension completely, while Narrow Bracketing posits that they miscalculate the complex costs.",
      "C": "Salience is about simplifying a multi-stage choice into a single stage, while Narrow Bracketing is about attentional focus being drawn to the most prominent attribute on the screen.",
      "D": "Salience suggests that complexity increases random errors, while Narrow Bracketing suggests complexity leads to a systematic preference for the highest benefit."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question assesses the foundational understanding of the two competing behavioral theories discussed in the paper. It uses a 'Conceptual' distractor design. The correct answer (A) accurately distinguishes the attentional focus of Salience from the sequential simplification of Narrow Bracketing. Distractors include a subtle mischaracterization (B, Almost Right), a direct swap of the concepts (C, Conceptual Opposite), and an inaccurate description of the Salience theory's prediction (D).",
    "qid": "297",
    "model_evaluation_result": {
      "model_raw_response": "C) Salence is about simplifing a multi-stage choise into a single stager, while Narrow Bracking is about attentionnal focus being drawn t the most prominen attribute on the scren.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 298,
    "Question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to distinguish between two competing behavioral explanations for an observed choice anomaly.\n\n**Setting.** In a primary experiment, subjects chose between three products (High, Medium, Low benefit) in a two-stage process: first choose a product, then choose one of its associated costs. The key finding was that when costs were presented in a complex arithmetic format (`COMPLEX COST` treatment), subjects made systematically biased mistakes, choosing the suboptimal High-benefit option far more often than the equally suboptimal Low-benefit option. This rejected the null hypothesis of symmetric, random mistakes.\n\nTwo potential explanations for this bias are considered:\n1.  **Salience:** Complexity in the cost dimension makes the simple, easy-to-evaluate benefit dimension more salient. Decision-makers overweight this salient dimension, leading them to be attracted to the High-benefit option.\n2.  **Narrow Bracketing:** Decision-makers simplify the two-stage problem by ignoring the future cost-selection stage. They focus only on the first stage and choose the option with the highest immediate benefit (the High option), without fully integrating the downstream consequences of costs.\n\nTo distinguish between these two explanations, a follow-up treatment was conducted where subjects made a single choice from a menu of nine options (e.g., 'Loan A with Repayment X', 'Loan A with Repayment Y', etc.), effectively choosing the product and its cost at the same time.\n\n### The Question\n\nWhat is the primary causal logic for using a 'Simultaneous Choice' follow-up experiment to distinguish between the Salience and Narrow Bracketing theories?",
    "Options": {
      "A": "The simultaneous design makes the Narrow Bracketing strategy of ignoring the future cost stage difficult or impossible, thus generating a different choice prediction than the Salience theory.",
      "B": "The simultaneous design increases the overall cognitive load, which should reduce the bias if it is caused by Salience.",
      "C": "The simultaneous design gives subjects more total time to decide, allowing them to overcome the complexity and make more optimal choices under either theory.",
      "D": "The simultaneous design forces subjects to compare nine distinct options, which tests for aversion to large choice sets, a separate behavioral bias."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question assesses the ability to understand the logic of experimental design for causal identification. It uses a 'Reverse-Reasoning' strategy, asking for the rationale behind the design. The correct answer (A) identifies the core manipulation: disabling the Narrow Bracketing heuristic. Distractors are based on flawed causal interpretations (B) or true but irrelevant side effects of the design change (C, D).",
    "qid": "298",
    "model_evaluation_result": {
      "model_raw_response": "A) The simultaneous design makes the narrow bracketing strategy of ignorig the future cost stag difficult or impossible thus generating a diffrent choice prediction than teh salience theory.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 299,
    "Question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to distinguish between two competing behavioral explanations for an observed choice anomaly.\n\n**Setting.** In a primary experiment, subjects chose between three products (High, Medium, Low benefit) in a two-stage process: first choose a product, then choose one of its associated costs. The key finding was that when costs were presented in a complex arithmetic format (`COMPLEX COST` treatment), subjects made systematically biased mistakes, choosing the suboptimal High-benefit option far more often than the equally suboptimal Low-benefit option. This rejected the null hypothesis of symmetric, random mistakes.\n\nTwo potential explanations for this bias are considered:\n1.  **Salience:** Complexity in the cost dimension makes the simple, easy-to-evaluate benefit dimension more salient. Decision-makers overweight this salient dimension, leading them to be attracted to the High-benefit option.\n2.  **Narrow Bracketing:** Decision-makers simplify the two-stage problem by ignoring the future cost-selection stage. They focus only on the first stage and choose the option with the highest immediate benefit (the High option), without fully integrating the downstream consequences of costs.\n\nTo distinguish between these two explanations, a follow-up treatment was conducted where subjects made a single choice from a menu of nine options, effectively choosing the product and its cost at the same time. The result of this follow-up treatment was that choice patterns in the `COMPLEX COST` condition did not significantly change; the bias towards the High-benefit option persisted.\n\n### The Question\n\nGiven this finding, what is the most accurate conclusion?",
    "Options": {
      "A": "The result supports the Salience explanation because the bias persisted even when the Narrow Bracketing heuristic was made difficult to apply.",
      "B": "The result supports the Narrow Bracketing explanation because the bias was shown to be robust to changes in the choice architecture.",
      "C": "The result is inconclusive because it is consistent with the predictions of both the Salience and Narrow Bracketing theories.",
      "D": "The result suggests a third, unstated theory is responsible, as neither Salience nor Narrow Bracketing can fully account for the persistence of the bias."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question tests the final step of scientific inference: connecting experimental evidence to theory. It uses a 'Computational Judgment' strategy (in a logical sense). The correct answer (A) draws the appropriate conclusion. Distractors represent common errors in interpretation: concluding support for the falsified theory (B, Conceptual Opposite), claiming inconclusiveness when the evidence is decisive (C), or unnecessarily invoking an unstated alternative (D).",
    "qid": "299",
    "model_evaluation_result": {
      "model_raw_response": "C) The.result is inconclusivebecause it is consistentwith the predictions ofboth the Salienceand Narrow Bracketingtheories.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 300,
    "Question": "### Background\n\n**Research Question.** This problem investigates the micro-level drivers of the change in the aggregate imported input share following a large devaluation, focusing on the puzzle of why import-intensive firms expand.\n\n**Setting / Institutional Environment.** The analysis uses firm-level data from Mexico (post-1995 devaluation) to decompose the change in the aggregate imported input share into components attributable to within-firm changes versus between-firm reallocation of market share.\n\n### Data / Model Specification\n\nThe growth in the aggregate imported input share (\\(\\Delta S_I\\)) is decomposed into several components. The two key components for continuing firms are:\n-   **Within:** Captures changes in firms' own import shares, holding their initial market shares constant. It reflects firm-level substitution.\n-   **Between:** Captures the reallocation of market share across firms, holding their initial import shares constant.\n\n**Table 1: Decomposition for Mexico (change from 1994 to 1997, in percentage points)**\n| Year | Within | Between | Covariance | Net entry | All |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| 1997 | -1.54 | 12.34 | 2.61 | 6.95 | 20.36 |\n\nBased on the decomposition in Table 1, what do the 'Within' and 'Between' components reveal about firm behavior, and why is the 'Between' component's value considered a puzzle for standard trade models?",
    "Options": {
      "A": "The negative 'Within' component shows individual firms substituted away from imports, while the large positive 'Between' component shows market share shifted to import-intensive firms, which is puzzling as standard models predict they should contract.",
      "B": "The positive 'Within' component shows firms imported more, while the negative 'Between' component shows market share shifted away from import-intensive firms, matching standard models.",
      "C": "The negative 'Within' component is the main puzzle, as it implies firms' production technology has an elasticity of substitution greater than one.",
      "D": "The positive 'Between' component is expected, but its magnitude (12.34) is puzzlingly small compared to what standard models predict."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to interpret a decomposition table and contrast the empirical finding with a theoretical benchmark. It uses a Computational Judgment strategy. The distractors are designed based on common errors: B misreads both signs from the table, C misidentifies the source of the puzzle, and D incorrectly states the prediction of standard models.",
    "qid": "300",
    "model_evaluation_result": {
      "model_raw_response": "A) The positive \"Between\" component shows market shift to import-intensive firm, which is unexpected as standard models would predict they should reduce imports.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 301,
    "Question": "### Background\n\n**Research Question.** This problem investigates the micro-level drivers of the change in the aggregate imported input share following a large devaluation, focusing on the puzzle of why import-intensive firms expand.\n\n**Setting / Institutional Environment.** The analysis uses firm-level data from Mexico (post-1995 devaluation) to decompose the change in the aggregate imported input share into components attributable to within-firm changes versus between-firm reallocation of market share.\n\n### Data / Model Specification\n\nThe growth in the aggregate imported input share (\\(\\Delta S_I\\)) is decomposed into several components. The two key components for continuing firms are:\n-   **Within:** Captures changes in firms' own import shares, holding their initial market shares constant. It reflects firm-level substitution.\n-   **Between:** Captures the reallocation of market share across firms, holding their initial import shares constant.\n\n**Table 1: Decomposition for Mexico (change from 1994 to 1997, in percentage points)**\n| Year | Within | Between | Covariance | Net entry | All |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| 1997 | -1.54 | 12.34 | 2.61 | 6.95 | 20.36 |\n\nFurther analysis, not shown in the table, reveals that firms that are also exporters account for more than 100% of the positive reallocation effect (the sum of the 'Between' and 'Covariance' terms). What does this finding imply about the behavior of non-exporting importers, and how does it resolve the puzzle of why import-intensive firms expanded?",
    "Options": {
      "A": "It implies non-exporting importers must have contracted (a negative reallocation effect), behaving as standard theory predicts. The puzzle is thus confined to and explained by the behavior of 'global firms'.",
      "B": "It implies non-exporting importers must have had a zero reallocation effect, meaning the puzzle is entirely driven by exporters and remains unexplained.",
      "C": "It implies that both exporters and non-exporters expanded, but exporters expanded more, deepening the puzzle of why any importer would expand.",
      "D": "It implies that exporting is more profitable than selling domestically, but does not resolve the puzzle of why import-intensive firms gained market share."
    },
    "Answer": "A",
    "pi_justification": "This item assesses deductive reasoning and the ability to synthesize a new piece of information to resolve a stated puzzle. It uses a Reverse-Reasoning strategy. Distractors are based on a mathematical error (B), a direct contradiction of the logic (C), and a conclusion that is plausible but irrelevant to the specific question asked (D).",
    "qid": "301",
    "model_evaluation_result": {
      "model_raw_response": "C) Itimplies that both exportersandnon-exporters expanded,butexportersexpandedmore,deepeningthepuzzleofwhyanyimporterwouldexpand.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 302,
    "Question": "### Background\n\n**Research Question.** This problem addresses the core challenge of estimating demand systems derived from *nonlinear* parallel preference structures, where consumer utility `u` is unobservable and the standard method of deriving Marshallian demands via an invertible cost function fails.\n\n**Setting / Institutional Environment.** We consider a general parallel preference structure where the income-consumption curves may be nonlinear. The key issue is that the cost function `C(u,p)` cannot be easily inverted to find an indirect utility function `V(p,y)`, precluding the use of Roy's identity. An alternative estimation strategy is therefore required.\n\n**Variables & Parameters.**\n- `x_i`: Hicksian demand for good `i` (`i = 1, ..., N`).\n- `p`: An `N x 1` vector of prices.\n- `u`: The unobservable scalar utility index.\n- `f_i(·)`: A utility-dependent function, potentially nonlinear.\n- `ψ^i(p)`: A price-dependent function, `∂A(p)/∂p_i`.\n- `x_1`: The observed consumption of the numeraire good (`i=1`).\n\n---\n\n### Data / Model Specification\n\nThe theoretical model is the system of `N` Hicksian demand functions for a general parallel preference structure:\n```latex\nx_i = f_i(u) + ψ^i(p) \n\n\\quad\\text{for } i = 1, ..., N \\quad\\text{(Eq. 1)}\n```\nFor nonlinear `f_i(·)`, this system is not directly estimable because `u` is unobservable.\n\n---\n\n### Question\n\nStarting from the Hicksian demand system in Eq. (1), what is the final estimable 'generalized Ashenfelter-Heckman' (GAH) form for the demand of a non-numeraire good `x_i` (`i > 1`) after using the numeraire good `x_1` to substitute for the unobservable utility `u`?",
    "Options": {
      "A": "```latex\nx_i = f_i(f_1^{-1}(x_1 - ψ^1(p))) + ψ^i(p)\n```",
      "B": "```latex\nx_i = f_i(f_1^{-1}(x_1 + ψ^1(p))) + ψ^i(p)\n```",
      "C": "```latex\nx_i = f_1(f_i^{-1}(x_1 - ψ^1(p))) + ψ^i(p)\n```",
      "D": "```latex\nx_i = f_i(u) + ψ^i(p) - ψ^1(p)\n```"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to perform the key algebraic derivation that makes the nonlinear model estimable. It uses a Computational Judgment strategy. Distractors are based on common algebraic errors (sign error), conceptual confusion (swapping functions), and incomplete reasoning.",
    "qid": "302",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: I've only provided the answer as per the format specified.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 303,
    "Question": "### Background\n\n**Research Question.** This problem addresses the core challenge of estimating demand systems derived from *nonlinear* parallel preference structures, where consumer utility `u` is unobservable and the standard method of deriving Marshallian demands via an invertible cost function fails.\n\n**Setting / Institutional Environment.** We consider a general parallel preference structure where the income-consumption curves may be nonlinear. The key issue is that the cost function `C(u,p)` cannot be easily inverted to find an indirect utility function `V(p,y)`, precluding the use of Roy's identity. An alternative estimation strategy is therefore required.\n\n**Variables & Parameters.**\n- `x_i`: Hicksian demand for good `i` (`i = 1, ..., N`).\n- `p`: An `N x 1` vector of prices.\n- `u`: The unobservable scalar utility index.\n- `f_i(·)`: A utility-dependent function, potentially nonlinear.\n- `ψ^i(p)`: A price-dependent function, `∂A(p)/∂p_i`.\n- `x_1`: The observed consumption of the numeraire good (`i=1`).\n\n---\n\n### Data / Model Specification\n\nThe theoretical model is the system of `N` Hicksian demand functions for a general parallel preference structure:\n```latex\nx_i = f_i(u) + ψ^i(p) \n\n\\quad\\text{for } i = 1, ..., N \\quad\\text{(Eq. 1)}\n```\nBy using the numeraire good `x_1` to substitute for `u`, one can derive the estimable 'generalized Ashenfelter-Heckman' (GAH) system:\n```latex\nx_i = g_i(x_1 - ψ^1(p)) + ψ^i(p) \n\n\\quad\\text{for } i = 2, ..., N \\quad\\text{(Eq. 2)}\n```\nwhere `g_i(·) = f_i(f_1^{-1}(·))`.\n\n---\n\n### Question\n\nIn the derived GAH system (Eq. 2), why would estimating the relationship using Ordinary Least Squares (OLS) lead to biased and inconsistent parameter estimates?",
    "Options": {
      "A": "The regressor `x_1` is endogenous because it is chosen simultaneously with `x_i`, making it correlated with unobserved factors in the error term.",
      "B": "The regressor `x_1` is likely to be highly correlated with the price vector `p`, causing multicollinearity.",
      "C": "The function `g_i(·)` is nonlinear, and OLS is only suitable for linear models.",
      "D": "The prices `p` are measured with error, which biases all coefficients."
    },
    "Answer": "A",
    "pi_justification": "This item tests the core econometric insight from the original question's apex part. It uses a Reverse-Reasoning strategy. Distractors target common statistical misconceptions: confusing multicollinearity with endogeneity (B), misunderstanding OLS capabilities (C), and focusing on a different, less central problem (D).",
    "qid": "303",
    "model_evaluation_result": {
      "model_raw_response": "C) The funciton `g_i(` is nonlinear, `and OLS is `only suitable for linear `models.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 304,
    "Question": "### Background\n\n**Research Question.** This problem explores the fundamental theoretical properties of parallel preference structures, starting from the general cost function and specializing to the tractable linear case, highlighting the key implications for consumer behavior and empirical analysis.\n\n**Setting / Institutional Environment.** We consider a standard consumer choice problem with N goods. The preference structure is described by its cost (expenditure) function. The analysis is restricted to interior solutions.\n\n**Variables & Parameters.**\n- `C(u,p)`: The cost function.\n- `u`: A scalar utility index.\n- `p`: An `N x 1` vector of prices.\n\n---\n\n### Data / Model Specification\n\nThe general cost function for a parallel preference structure is given by:\n```latex\nC(u,p) = \\sum_{i=1}^{N} p_i f_i(u) + A(p) \n\n\\quad\\text{(Eq. 1)}\n```\n\n---\n\n### Question\n\nGiven the general cost function for a parallel preference structure in Eq. (1), what is the fundamental property of the Slutsky substitution matrix, `S(p) = ∇_{pp} C(u,p)`?",
    "Options": {
      "A": "It is independent of the level of utility, `u`.",
      "B": "It is independent of prices, `p`.",
      "C": "It is a diagonal matrix, implying no cross-price effects.",
      "D": "It is equal to the identity matrix multiplied by `u`."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the single most important theoretical property of parallel structures, which is the core of the original QA's first two parts. It uses a Computational Judgment strategy (mentally performing the differentiation). Distractors represent classic alternative properties of other preference structures (B, C) or nonsensical combinations (D).",
    "qid": "304",
    "model_evaluation_result": {
      "model_raw_response": "A\n\nNote: The output is a single letter \"A\".",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 305,
    "Question": "### Background\n\n**Research Question.** This problem explores the fundamental theoretical properties of parallel preference structures, starting from the general cost function and specializing to the tractable linear case, highlighting the key implications for consumer behavior and empirical analysis.\n\n**Setting / Institutional Environment.** We consider a standard consumer choice problem with N goods. The preference structure is described by its cost (expenditure) function. The analysis is restricted to interior solutions.\n\n**Variables & Parameters.**\n- `C(u,p)`: The cost function.\n- `u`: A scalar utility index.\n- `p`: An `N x 1` vector of prices.\n- `h(u)`, `δ_i`: Parameters/functions governing income effects.\n- `A(p)`: A function governing substitution effects.\n\n---\n\n### Data / Model Specification\n\nA **linear parallel structure** is a special case of the general parallel structure where the cost function takes the form:\n```latex\nC(u,p) = h(u) \\sum_{i=1}^{N} \\delta_i p_i + A(p) \n\n\\quad\\text{(Eq. 1)}\n```\n\n---\n\n### Question\n\nA key feature of the *linear* parallel structure, whose cost function is given in Eq. (1), is its implication for the shape of income-consumption curves (ICCs). What is this implication?",
    "Options": {
      "A": "All income-consumption curves are linear and parallel.",
      "B": "All income-consumption curves are linear and pass through the origin.",
      "C": "Income-consumption curves can be nonlinear, depending on the form of `A(p)`.",
      "D": "All goods are normal goods, so the income-consumption curves are always upward sloping."
    },
    "Answer": "A",
    "pi_justification": "This item tests the main result of the specialization part of the original QA. The primary distractor (B) targets the classic and highly frequent misconception of confusing quasi-homothetic (linear parallel ICCs) with homothetic preferences.",
    "qid": "305",
    "model_evaluation_result": {
      "model_raw_response": "C) Income consumption curves can be non-linear, depending on form of A(p).",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 306,
    "Question": "### Background\n\nThe linear parallel structure is a special case of the Gorman polar form. The paper notes this is the most general model that \"yields a globally consistent aggregate preference function that is independent of the distribution of income.\"\n\n---\n\n### Question\n\nWhat is the primary practical advantage of this aggregation property for empirical demand analysis?",
    "Options": {
      "A": "It allows researchers to use aggregate market-level data (e.g., total income) to estimate demand systems as if they were generated by a single representative agent.",
      "B": "It ensures that the estimated demand elasticities will be constant across all income levels.",
      "C": "It simplifies estimation by guaranteeing that all demand functions are linear in prices and income.",
      "D": "It proves that individual consumer preferences are identical, which is a necessary assumption for market analysis."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the synthesis part of the original QA. It tests a deep conceptual point about the bridge between micro theory and macro data. Distractors are designed to be plausible but incorrect interpretations of the aggregation property, targeting common oversimplifications or false conclusions.",
    "qid": "306",
    "model_evaluation_result": {
      "model_raw_response": "A) A) It allows research... to estimate demand...",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 307,
    "Question": "### Background\n\n**Research Question.** This problem compares the properties of two specific \"flexible functional forms\" derived from the linear parallel structure: the Parallel Linear Asymmetric Quadratic (PLAQ) and the Parallel Linear Generalized Linear (PLGL). It focuses on how their different mathematical structures lead to starkly different economic implications for substitution and satiation.\n\n**Setting / Institutional Environment.** We analyze a system of `N` goods where good 1 is the numeraire. All other prices (`p_i*`) are normalized by the price of the numeraire. The analysis focuses on the compensated (Hicksian) demand for the non-numeraire goods (`i, j > 1`).\n\n**Variables & Parameters.**\n- `φ^i(u,p*)`: Hicksian demand for non-numeraire good `i`.\n- `p_i*`: Normalized price of good `i`.\n\n---\n\n### Data / Model Specification\n\nThe Hicksian demand functions for non-numeraire goods (`i > 1`) for the two models are given by:\n\n**PLAQ Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + α_i + \\sum_{j=2}^{N} β_{ij} p_j^* \n\n\\quad\\text{(Eq. 1)}\n```\n\n**PLGL Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + \\frac{1}{2}α_i(p_i*)^{-1/2} + \\frac{1}{2}\\sum_{j=2}^{N} β_{ij} \\left(\\frac{p_j^*}{p_i^*}\\right)^{1/2}\n\n\\quad\\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nBased on the Hicksian demand functions for the PLAQ and PLGL models (Eq. 1 and Eq. 2), how does the compensated cross-price substitution effect, `s_{ij}(p*) = ∂φ^i/∂p_j*` (for `i, j > 1`), fundamentally differ between them?",
    "Options": {
      "A": "In PLAQ, the substitution effect is constant, while in PLGL, it diminishes as prices increase.",
      "B": "In PLAQ, the substitution effect diminishes as prices increase, while in PLGL, it is constant.",
      "C": "In both models, the substitution effect is constant, but the level is determined by different parameters.",
      "D": "In PLAQ, the substitution effect is always positive, while in PLGL, it can be negative."
    },
    "Answer": "A",
    "pi_justification": "This item directly tests the core comparison from the original QA's first two parts. It uses a Computational Judgment strategy. The primary distractor (B) is a direct conceptual reversal, testing for careful reading and differentiation. Other distractors offer false comparisons.",
    "qid": "307",
    "model_evaluation_result": {
      "model_raw_response": "B) In PLAQ, the substitution efect diminishes asprices increase, whilein GL, it isconstant.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 308,
    "Question": "### Background\n\n**Research Question.** This problem compares the properties of two specific \"flexible functional forms\" derived from the linear parallel structure: the Parallel Linear Asymmetric Quadratic (PLAQ) and the Parallel Linear Generalized Linear (PLGL). It focuses on how their different mathematical structures lead to starkly different economic implications for substitution and satiation.\n\n**Setting / Institutional Environment.** We analyze a system of `N` goods where good 1 is the numeraire. All other prices (`p_i*`) are normalized by the price of the numeraire. The analysis focuses on the compensated (Hicksian) demand for the non-numeraire goods (`i, j > 1`).\n\n**Variables & Parameters.**\n- `φ^i(u,p*)`: Hicksian demand for non-numeraire good `i`.\n- `p_i*`: Normalized price of good `i`.\n\n---\n\n### Data / Model Specification\n\nThe Hicksian demand functions for non-numeraire goods (`i > 1`) for the two models are given by:\n\n**PLAQ Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + α_i + \\sum_{j=2}^{N} β_{ij} p_j^* \n\n\\quad\\text{(Eq. 1)}\n```\n\n**PLGL Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + \\frac{1}{2}α_i(p_i*)^{-1/2} + \\frac{1}{2}\\sum_{j=2}^{N} β_{ij} \\left(\\frac{p_j^*}{p_i^*}\\right)^{1/2}\n\n\\quad\\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nThe paper notes that one of these models implies \"satiation effects\" for non-numeraire goods, while the other implies infinite demand as the price approaches zero. Which model implies satiation, and what feature of its demand function causes this?",
    "Options": {
      "A": "The PLAQ model implies satiation because its demand function is linear in price, meaning demand does not approach infinity as price approaches zero.",
      "B": "The PLGL model implies satiation because the term `(p_j*/p_i*)^{1/2}` approaches zero for low prices.",
      "C": "The PLAQ model implies satiation because the parameter `α_i` can be negative.",
      "D": "The PLGL model implies satiation because it is a generalization of the Leontief form, which has zero substitution."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the interpretation of the models' global properties, the focus of the original Q3. It requires linking the model name (PLAQ) to the property (satiation) and the correct mathematical reason (linearity). Distractors provide incorrect pairings or flawed reasoning.",
    "qid": "308",
    "model_evaluation_result": {
      "model_raw_response": "B) The term `(p_i*/p_j*)^{1//2}` approaches infinity as `p_i*' approaches zero, causing the demand to approach zero.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 309,
    "Question": "### Background\n\n**Research Question.** This problem compares the properties of two specific \"flexible functional forms\" derived from the linear parallel structure: the Parallel Linear Asymmetric Quadratic (PLAQ) and the Parallel Linear Generalized Linear (PLGL). It focuses on how their different mathematical structures lead to starkly different economic implications for substitution and satiation.\n\n**Setting / Institutional Environment.** We analyze a system of `N` goods where good 1 is the numeraire. All other prices (`p_i*`) are normalized by the price of the numeraire. The analysis focuses on the compensated (Hicksian) demand for the non-numeraire goods (`i, j > 1`).\n\n**Variables & Parameters.**\n- `φ^i(u,p*)`: Hicksian demand for non-numeraire good `i`.\n- `p_i*`: Normalized price of good `i`.\n\n---\n\n### Data / Model Specification\n\nThe Hicksian demand functions for non-numeraire goods (`i > 1`) for the two models are given by:\n\n**PLAQ Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + α_i + \\sum_{j=2}^{N} β_{ij} p_j^* \n\n\\quad\\text{(Eq. 1)}\n```\n\n**PLGL Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + \\frac{1}{2}α_i(p_i*)^{-1/2} + \\frac{1}{2}\\sum_{j=2}^{N} β_{ij} \\left(\\frac{p_j^*}{p_i^*}\\right)^{1/2}\n\n\\quad\\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nSuppose a researcher estimates a PLAQ model (i.e., a linear regression of quantity on price) when the true data generating process is PLGL. The PLGL's true substitution effect is large at low prices and small at high prices. How will the single estimated PLAQ coefficient for the price `p_j*` likely be biased as an estimate of the true substitution effect at very **low** price levels?",
    "Options": {
      "A": "It will underestimate the true substitution effect.",
      "B": "It will overestimate the true substitution effect.",
      "C": "It will be an unbiased estimate of the average substitution effect.",
      "D": "The direction of the bias cannot be determined without knowing the specific parameter values."
    },
    "Answer": "A",
    "pi_justification": "This is a high-level Scenario Application question that directly converts the original apex question. It tests a deep understanding of functional form misspecification. The primary distractor (B) is the opposite error. Distractor C is 'Almost Right' but misses the point about bias at a specific price range. Distractor D suggests indecisiveness where a clear direction can be inferred.",
    "qid": "309",
    "model_evaluation_result": {
      "model_raw_response": "B) It overestimate the.true substitution effect.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 310,
    "Question": "### Background\n\n**Research Question.** This problem investigates the causal effect of an income shock on the timing of fertility and how the choice of estimation strategy is critical for identification. A key concern is that unobserved factors, such as a farmer's innate ability, might be correlated with both the likelihood of experiencing a shock and fertility decisions, biasing simple correlations.\n\n**Setting / Institutional Environment.** The analysis uses a panel of women from the Kagera region of Tanzania. The income shock is a dummy variable for whether the household experienced an accidental crop loss. The outcome of interest is whether a woman gave birth in the 7 months since the last survey round. For a birth to occur, conception must have happened at least 9 months prior, so the relevant shock is one that occurred 7-14 months before the current survey.\n\n### Data / Model Specification\n\nThe study's preferred estimation strategy is a woman-level fixed effects (FE) linear probability model, which controls for all time-invariant individual characteristics (`μᵢ`). The model is:\n\n```latex\nY_{i,t} = \\beta_1 S_{i,t} + \\alpha_t + \\mu_i + \\varepsilon_{i,t} \\quad \\text{(Eq. 1)}\n```\n\nWhere `Y_{i,t}` is the birth outcome and `S_{i,t}` is the crop loss shock. Table 1 presents results from three specifications for the dependent variable `Birth since last survey`: a simple OLS model, a model with community-level fixed effects, and the preferred woman-level fixed effects model.\n\n**Table 1: The Effects of Crop Loss on Likelihood of a Recent Birth**\n\n| | OLS | Community FE | Woman FE |\n| :--- | :---: | :---: | :---: |\n| Crop loss (7-14 months) | -0.115*** | -0.130*** | -0.175*** |\n| | (0.038) | (0.040) | (0.049) |\n| Observations | 747 | 747 | 747 |\n\n*Note: Robust standard errors in parentheses. *** p<0.01.*\n\n### The Question\n\nBased on the results in Table 1, which statement provides the most accurate interpretation of the findings?",
    "Options": {
      "A": "Experiencing a crop loss reduces the probability of a birth by 17.5 percent. The OLS estimate is biased away from zero because less able farmers are both more likely to experience shocks and have more children.",
      "B": "Experiencing a crop loss reduces the probability of a birth by 0.175. The systematic increase in the coefficient's magnitude from OLS to Woman FE suggests that unobserved time-invariant factors create a negative bias in the OLS estimate.",
      "C": "Experiencing a crop loss reduces the probability of a birth by 17.5 percentage points. The OLS estimate is biased toward zero because less able farmers are more likely to experience shocks but also tend to have fewer children due to better planning.",
      "D": "Experiencing a crop loss reduces the probability of a birth by 17.5 percentage points. The systematic increase in the coefficient's magnitude from OLS to Woman FE suggests that unobserved time-invariant factors create a positive bias in the OLS estimate, making it appear smaller than the true effect."
    },
    "Answer": "D",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the student's ability to interpret a regression coefficient from a linear probability model and to correctly infer the direction of omitted variable bias by comparing estimates across different specifications. It uses a Computational Judgment strategy. Distractor A uses the wrong units ('percent' instead of 'percentage points') and incorrectly signs the bias. Distractor B uses ambiguous units and incorrectly signs the bias. Distractor C correctly interprets the coefficient's units but provides an incorrect explanation for the bias.",
    "qid": "310",
    "model_evaluation_result": {
      "model_raw_response": "B) Experiences a crop loss reduce the probability of birth by 0175. The systematicincrease in the coefficient’s magnitude from OSLs to Woman FE suggest thatunobserved timeinvariant factors createa negative bias inthe OLSestimate.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 311,
    "Question": "### Background\n\n**Research Question.** This problem investigates the causal effect of an income shock on the timing of fertility and how the choice of estimation strategy is critical for identification. A key concern is that unobserved factors, such as a farmer's innate ability, might be correlated with both the likelihood of experiencing a shock and fertility decisions, biasing simple correlations.\n\n**Setting / Institutional Environment.** The analysis uses a panel of women from the Kagera region of Tanzania. The income shock is a dummy variable for whether the household experienced an accidental crop loss. The outcome of interest is whether a woman gave birth in the 7 months since the last survey round. For a birth to occur, conception must have happened at least 9 months prior, so the relevant shock is one that occurred 7-14 months before the current survey.\n\n### Data / Model Specification\n\nThe study's preferred estimation strategy is a woman-level fixed effects (FE) linear probability model, which controls for all time-invariant individual characteristics (`μᵢ`). The model is:\n\n```latex\nY_{i,t} = \\beta_1 S_{i,t} + \\alpha_t + \\mu_i + \\varepsilon_{i,t} \\quad \\text{(Eq. 1)}\n```\n\nWhere `Y_{i,t}` is the birth outcome and `S_{i,t}` is the crop loss shock. Table 1 presents results from three specifications for the dependent variable `Birth since last survey`: a simple OLS model, a model with community-level fixed effects, and the preferred woman-level fixed effects model.\n\n**Table 1: The Effects of Crop Loss on Likelihood of a Recent Birth**\n\n| | OLS | Community FE | Woman FE |\n| :--- | :---: | :---: | :---: |\n| Crop loss (7-14 months) | -0.115*** | -0.130*** | -0.175*** |\n| | (0.038) | (0.040) | (0.049) |\n| Observations | 747 | 747 | 747 |\n\n*Note: Robust standard errors in parentheses. *** p<0.01.*\n\n### The Question\n\nThe pattern in Table 1—where the estimated effect of a crop loss on births becomes more negative as more granular fixed effects are added—suggests the OLS estimate is biased toward zero. According to the paper's reasoning, what unobserved factor and associated correlations explain this positive bias? Let 'Ability' be an unobserved, time-invariant characteristic of the woman/household.",
    "Options": {
      "A": "Higher Ability is associated with a *lower* likelihood of crop loss, and higher Ability is associated with a *higher* likelihood of birth. This creates a negative bias.",
      "B": "Higher Ability is associated with a *higher* likelihood of crop loss, and higher Ability is associated with a *lower* likelihood of birth. This creates a negative bias.",
      "C": "Higher Ability is associated with a *higher* likelihood of crop loss, and higher Ability is associated with a *higher* likelihood of birth. This creates a positive bias.",
      "D": "Higher Ability is associated with a *lower* likelihood of crop loss, and higher Ability is associated with a *lower* likelihood of birth. This creates a positive bias."
    },
    "Answer": "D",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item provides a sharp test of the student's ability to apply the omitted variable bias formula. It uses a Reverse-Reasoning strategy, asking for the underlying causal story that generates the observed empirical pattern. The distractors are systematically designed by incorrectly signing the two key correlations (`Corr(Shock, Ability)` and `Corr(Birth, Ability)`), which represent the most common and predictable errors in this type of analysis.",
    "qid": "311",
    "model_evaluation_result": {
      "model_raw_response": "C) Higher ability is associated with higher likelihood of crop failure, and higher ability is associated higher likelihood of birth",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 312,
    "Question": "### Background\n\n**Research Question:** How can the topological structure of the Nash equilibrium correspondence be exploited to create a computationally feasible algorithm for finding equilibria in finite games?\n\n**Setting:** The core challenge in computing equilibria is the dimensional mismatch between the vast space of games (`Γ`) and the smaller space of mixed strategies (`Σ`). The Kohlberg-Mertens structure theorem provides a homeomorphism `H` that connects the equilibrium graph `E` to the game space `Γ`, enabling a more efficient computational approach by working in a space of dimension `m`, the total number of pure strategies.\n\n**Variables and Parameters.**\n- `(G̃, g)`: A parameterized decomposition of a game `G`.\n- `σ`: A mixed-strategy profile.\n- `v_s(σ)`: The expected payoff for pure strategy `s` given profile `σ`.\n- `z`: An `m`-dimensional vector in the image space of the homeomorphism `H`.\n- `H`: The homeomorphism from the equilibrium graph `E` to the space of games `Γ`.\n- `r(z)`: A retraction map from `ℝ^m` to the strategy space `Σ`.\n- `h(z)`: A map from `ℝ^m` to the `g`-parameter space.\n\n### Data / Model Specification\n\nThe analysis relies on the following key relationships:\n\n1.  A game `G` is decomposed into `(G̃, g)`. The expected payoff for player `n`'s pure strategy `s ∈ S_n` is:\n    ```latex\n    v_s(\\sigma) = g_s + \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n    Let `G̃(σ)_s` denote the summation term.\n\n2.  The Kohlberg-Mertens homeomorphism `H` maps a point on the equilibrium graph `(G̃, g, σ)` to the space of games. For a fixed `G̃`, it is defined as `H(G̃, g, σ) = (G̃, z)`, where:\n    ```latex\n    z_s = \\sigma_s + v_s(\\sigma) \n    ```\n\n3.  The retraction map `r: ℝ^m → Σ` maps a vector `z` to a mixed-strategy profile `σ = r(z)`. The components `σ_s` are defined by `σ_s = (z_s - v_n(z))^+`, where `v_n(z)` is the unique real number for player `n` that solves:\n    ```latex\n    \\sum_{s \\in S_n} (z_s - v_n(z))^+ = 1\n    ```\n    where `(x)⁺ ≡ max{0, x}`.\n\n4.  The inverse map `H⁻¹(G̃, z)` yields the point `(G̃, g, σ)` on the equilibrium graph where `σ = r(z)` and `g_s` is given by:\n    ```latex\n    g_s = z_s - \\sigma_s - \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n\n### Question\n\nGiven the definitions in the model specification, the map `h(z)` finds the game parameter `g` for which `r(z)` is an equilibrium. To find an equilibrium for a target game `g*`, the problem is reformulated as finding a zero of `ψ(z)`. Which of the following correctly defines both `h(z)` and `ψ(z)`?",
    "Options": {
      "A": "`h(z) = z + r(z) - G̃(r(z))` and `ψ(z) = h(z) - g*`",
      "B": "`h(z) = z - r(z) - G̃(r(z))` and `ψ(z) = g* - h(z)`",
      "C": "`h(z) = z - r(z)` and `ψ(z) = h(z) - g*`",
      "D": "`h(z) = z - r(z) - G̃(r(z))` and `ψ(z) = h(z) - g*`"
    },
    "Answer": "D",
    "pi_justification": "This item assesses the ability to perform the core derivation of the `h(z)` map and correctly formulate the root-finding problem for `ψ(z)`. It uses a Computational Judgment strategy. The distractors are designed based on plausible algebraic errors: a sign error on `r(z)` (A), a sign error in the definition of `ψ(z)` (B), and omitting the interaction term `G̃(r(z))` (C).",
    "qid": "312",
    "model_evaluation_result": {
      "model_raw_response": "B) \n\n(Note: The output is the letter of correct option)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 313,
    "Question": "### Background\n\n**Research Question:** How can the topological structure of the Nash equilibrium correspondence be exploited to create a computationally feasible algorithm for finding equilibria in finite games?\n\n**Setting:** The core challenge in computing equilibria is the dimensional mismatch between the vast space of games (`Γ`) and the smaller space of mixed strategies (`Σ`). The Kohlberg-Mertens structure theorem provides a homeomorphism `H` that connects the equilibrium graph `E` to the game space `Γ`, enabling a more efficient computational approach by working in a space of dimension `m`, the total number of pure strategies.\n\n**Variables and Parameters.**\n- `(G̃, g)`: A parameterized decomposition of a game `G`.\n- `σ`: A mixed-strategy profile.\n- `v_s(σ)`: The expected payoff for pure strategy `s` given profile `σ`.\n- `z`: An `m`-dimensional vector in the image space of the homeomorphism `H`.\n- `H`: The homeomorphism from the equilibrium graph `E` to the space of games `Γ`.\n- `r(z)`: A retraction map from `ℝ^m` to the strategy space `Σ`.\n- `h(z)`: A map from `ℝ^m` to the `g`-parameter space.\n\n### Data / Model Specification\n\nThe analysis relies on the following key relationships:\n\n1.  A game `G` is decomposed into `(G̃, g)`. The expected payoff for player `n`'s pure strategy `s ∈ S_n` is:\n    ```latex\n    v_s(\\sigma) = g_s + \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n    Let `G̃(σ)_s` denote the summation term.\n\n2.  The Kohlberg-Mertens homeomorphism `H` maps a point on the equilibrium graph `(G̃, g, σ)` to the space of games. For a fixed `G̃`, it is defined as `H(G̃, g, σ) = (G̃, z)`, where:\n    ```latex\n    z_s = \\sigma_s + v_s(\\sigma) \n    ```\n\n3.  The retraction map `r: ℝ^m → Σ` maps a vector `z` to a mixed-strategy profile `σ = r(z)`. The components `σ_s` are defined by `σ_s = (z_s - v_n(z))^+`, where `v_n(z)` is the unique real number for player `n` that solves:\n    ```latex\n    \\sum_{s \\in S_n} (z_s - v_n(z))^+ = 1\n    ```\n    where `(x)⁺ ≡ max{0, x}`.\n\n4.  The inverse map `H⁻¹(G̃, z)` yields the point `(G̃, g, σ)` on the equilibrium graph where `σ = r(z)` and `g_s` is given by:\n    ```latex\n    g_s = z_s - \\sigma_s - \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n\n### Question\n\nThe Global Newton Method relies on the differentiability of the map `ψ(z)`, which in turn depends on `h(z)`. The map `h(z)` is described as piecewise-smooth, meaning it is not differentiable everywhere. At which points `z` does `h(z)` fail to be differentiable, and what is the game-theoretic interpretation of these points?",
    "Options": {
      "A": "At points where `r(z)` corresponds to a pure-strategy equilibrium. These are vertices of the strategy simplex `Σ`.",
      "B": "At points where the target game `g*` is degenerate. These points correspond to games with a continuum of equilibria.",
      "C": "At points where `G̃(r(z)) = 0`. These are points where the game has no payoff interdependencies.",
      "D": "At points where `z_s - v_n(z) = 0` for some strategy `s`. These are boundaries where a pure strategy `s` is about to enter or leave the support of the equilibrium strategy `r(z)`."
    },
    "Answer": "D",
    "pi_justification": "This item assesses the deep connection between the analytical properties of the `h(z)` map and the underlying game theory. It uses a Reverse-Reasoning strategy. The distractors target common conceptual errors: confusing non-differentiability with a specific equilibrium type (A), misattributing the cause to the target game's properties (B, True but Irrelevant), or focusing on the wrong component of the `h(z)` function (C).",
    "qid": "313",
    "model_evaluation_result": {
      "model_raw_response": "D) At(points where `z`s - v_nz) =  0 for some strategy s. These are boundarieswhere a pure strategy s is about toenter or leave thesupport of the equilibriumstrategy r(z).",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 314,
    "Question": "### Background\n\n**Research Question:** How can the topological structure of the Nash equilibrium correspondence be exploited to create a computationally feasible algorithm for finding equilibria in finite games?\n\n**Setting:** The core challenge in computing equilibria is the dimensional mismatch between the vast space of games (`Γ`) and the smaller space of mixed strategies (`Σ`). The Kohlberg-Mertens structure theorem provides a homeomorphism `H` that connects the equilibrium graph `E` to the game space `Γ`, enabling a more efficient computational approach by working in a space of dimension `m`, the total number of pure strategies.\n\n**Variables and Parameters.**\n- `(G̃, g)`: A parameterized decomposition of a game `G`.\n- `σ`: A mixed-strategy profile.\n- `v_s(σ)`: The expected payoff for pure strategy `s` given profile `σ`.\n- `z`: An `m`-dimensional vector in the image space of the homeomorphism `H`.\n- `H`: The homeomorphism from the equilibrium graph `E` to the space of games `Γ`.\n- `r(z)`: A retraction map from `ℝ^m` to the strategy space `Σ`.\n- `h(z)`: A map from `ℝ^m` to the `g`-parameter space.\n\n### Data / Model Specification\n\nThe analysis relies on the following key relationships:\n\n1.  A game `G` is decomposed into `(G̃, g)`. The expected payoff for player `n`'s pure strategy `s ∈ S_n` is:\n    ```latex\n    v_s(\\sigma) = g_s + \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n    Let `G̃(σ)_s` denote the summation term.\n\n2.  The Kohlberg-Mertens homeomorphism `H` maps a point on the equilibrium graph `(G̃, g, σ)` to the space of games. For a fixed `G̃`, it is defined as `H(G̃, g, σ) = (G̃, z)`, where:\n    ```latex\n    z_s = \\sigma_s + v_s(\\sigma) \n    ```\n\n3.  The retraction map `r: ℝ^m → Σ` maps a vector `z` to a mixed-strategy profile `σ = r(z)`. The components `σ_s` are defined by `σ_s = (z_s - v_n(z))^+`, where `v_n(z)` is the unique real number for player `n` that solves:\n    ```latex\n    \\sum_{s \\in S_n} (z_s - v_n(z))^+ = 1\n    ```\n    where `(x)⁺ ≡ max{0, x}`.\n\n4.  The inverse map `H⁻¹(G̃, z)` yields the point `(G̃, g, σ)` on the equilibrium graph where `σ = r(z)` and `g_s` is given by:\n    ```latex\n    g_s = z_s - \\sigma_s - \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n\n### Question\n\nThe structure theorem and the resulting formulation of the equilibrium-finding problem in terms of the map `h(z)` represent a significant conceptual breakthrough. What is the primary advantage of this approach for creating a practical algorithm, particularly in addressing the 'curse of dimensionality'?",
    "Options": {
      "A": "It guarantees that the equilibrium path is a straight line, which can be followed with a simple linear solver.",
      "B": "It works directly in the space of mixed strategies `Σ`, avoiding the need for the auxiliary space `ℝ^m`.",
      "C": "It ensures that every game has a unique equilibrium, simplifying the search to finding that single point.",
      "D": "It transforms the problem from a search in the high-dimensional space of all games (`Γ`) to a root-finding problem in `ℝ^m`, a space whose dimension (`m`) scales with the number of pure strategies, not the number of payoff entries."
    },
    "Answer": "D",
    "pi_justification": "This item assesses the high-level understanding of the paper's core conceptual contribution. It uses a Scenario Application strategy. The distractors are designed as 'Conceptual Opposites' that make claims contrary to the method's actual properties, thereby testing for fundamental misunderstandings of the algorithm's structure and purpose.",
    "qid": "314",
    "model_evaluation_result": {
      "model_raw_response": "D) It transform the problem from  a search in  the high-dimensional  space of all  games (`Γ` ) to a  root-finding  problem in  `ℝm`, a  space  whose dimension  (`m`)  scales  with  the  number  of pure  strategies, not  the umber of payoff entries",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 315,
    "Question": "### Background\n\n**Research Question.** This problem interrogates the full causal inference strategy used to estimate the effect of broadband on firms and workers. The paper uses both an Intention-to-Treat (ITT) design and an Instrumental Variable (IV) design.\n\n**Setting and Sample.** The study exploits a public program in Norway that progressively rolled out broadband infrastructure across different municipalities (`m`) at different times (`t`). This staggered rollout provides quasi-experimental variation in broadband availability (`z_mt`), which is the share of households in a municipality with access to broadband.\n\n### Data / Model Specification\n\nThe **Intention-to-Treat (ITT)** model estimates the effect of broadband *availability* on an outcome `y_imt`:\n\n```latex\ny_{imt} = x_{imt}'\\delta_0 + z_{mt}x_{imt}'\\delta_1 + ... + \\eta_m + \\tau_t + u_{imt} \\quad \\text{(Eq. 1)}\n```\n\nThe **Instrumental Variable (IV)** model estimates the effect of broadband *adoption* (`D_imt`) on `y_imt`, using `z_mt` as an instrument. The second stage is:\n\n```latex\ny_{imt} = x_{imt}'\\beta_0 + D_{imt}x_{imt}'\\beta_1 + ... + \\lambda_m + \\theta_t + \\epsilon_{imt} \\quad \\text{(Eq. 2)}\n```\n\n**Empirical Evidence for IV Assumptions:**\n- **First Stage:** A regression of firm adoption `D_imt` on availability `z_mt` yields a coefficient of `0.23` with a standard error of `0.04`.\n- **Placebo Test:** The paper identifies \"always takers\" (firms with broadband when `z=0`) and \"never takers\" (firms without broadband when `z=1`). For this subsample of firms, whose adoption status is by definition unaffected by `z_mt`, the paper re-estimates the ITT model (Eq. 1). The results are in Table 1.\n\n**Table 1. Placebo Test: ITT Effects on Output Elasticities (from paper's Table VI)**\n\n| Variable | Baseline Sample | Always/Never Taker Firms Only |\n| :--- | :--- | :--- |\n| Availability x Log skilled | 0.0755*** | 0.00944 |\n\n*Notes: Significance: ***p<0.01.*\n\n### Question\n\nWhat do the results of the Placebo Test in **Table 1** imply about the validity of using broadband availability (`z_mt`) as an instrument for broadband adoption (`D_imt`)?",
    "Options": {
      "A": "The results support the exclusion restriction, as the instrument's effect on the outcome disappears in the subsample where the adoption channel is shut down.",
      "B": "The results violate the exclusion restriction, because the effect of availability on skilled labor productivity is significantly different between the baseline and the placebo sample.",
      "C": "The results show that the instrument is weak (has low relevance), because the coefficient on availability is statistically insignificant in the always/never taker sample.",
      "D": "The results suggest that the treatment effect of broadband is heterogeneous, which means that a linear IV model is misspecified and therefore invalid."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to interpret a placebo test designed to validate an instrumental variable's exclusion restriction. It uses a Reverse-Reasoning strategy, asking students to infer the methodological conclusion from a specific empirical result. Distractors are designed to target common misconceptions in causal inference: confusing the exclusion restriction with the relevance condition (C), misinterpreting the test's logic (B), and identifying a true but secondary implication instead of the primary methodological point (D).",
    "qid": "315",
    "model_evaluation_result": {
      "model_raw_response": "C) The instrument is weak. The coefficient on availability in the always/nnever taker firms only is 0.0944, which is statistically insignificant and much smaller than the original coefficient of 0.23. This suggests that the instrument has low relevance, which implies that it is not a valid instrument for broadband adoptions.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 316,
    "Question": "### Background\n\n**Research Question.** This problem interrogates the full causal inference strategy used to estimate the effect of broadband on firms and workers. The paper uses both an Intention-to-Treat (ITT) design and an Instrumental Variable (IV) design.\n\n**Setting and Sample.** The study exploits a public program in Norway that progressively rolled out broadband infrastructure across different municipalities (`m`) at different times (`t`). This staggered rollout provides quasi-experimental variation in broadband availability (`z_mt`), which is the share of households in a municipality with access to broadband.\n\n### Data / Model Specification\n\nThe **Intention-to-Treat (ITT)** model estimates the effect of broadband *availability* on an outcome `y_imt`:\n\n```latex\ny_{imt} = x_{imt}'\\delta_0 + z_{mt}x_{imt}'\\delta_1 + ... + \\eta_m + \\tau_t + u_{imt} \\quad \\text{(Eq. 1)}\n```\n\nThe **Instrumental Variable (IV)** model estimates the effect of broadband *adoption* (`D_imt`) on `y_imt`, using `z_mt` as an instrument. The second stage is:\n\n```latex\ny_{imt} = x_{imt}'\\beta_0 + D_{imt}x_{imt}'\\beta_1 + ... + \\lambda_m + \\theta_t + \\epsilon_{imt} \\quad \\text{(Eq. 2)}\n```\n\n**Empirical Evidence for IV Assumptions:**\n- **First Stage:** A regression of firm adoption `D_imt` on availability `z_mt` yields a coefficient of `0.23` with a standard error of `0.04`.\n- **Placebo Test:** The paper identifies \"always takers\" (firms with broadband when `z=0`) and \"never takers\" (firms without broadband when `z=1`). For this subsample of firms, whose adoption status is by definition unaffected by `z_mt`, the paper re-estimates the ITT model (Eq. 1). The results are in Table 1.\n\n**Table 1. Placebo Test: ITT Effects on Output Elasticities (from paper's Table VI)**\n\n| Variable | Baseline Sample | Always/Never Taker Firms Only |\n| :--- | :--- | :--- |\n| Availability x Log skilled | 0.0755*** | 0.00944 |\n\n*Notes: Significance: ***p<0.01.*\n\n### Question\n\nBased on the provided first-stage regression results, which of the following statements correctly assesses the instrument relevance condition?",
    "Options": {
      "A": "The F-statistic is approximately 33.1, but this is below the conventional threshold, indicating a weak instrument and a violation of the relevance condition.",
      "B": "The F-statistic is approximately 33.1, which is well above the conventional threshold of 10, indicating a strong instrument and satisfying the relevance condition.",
      "C": "The F-statistic is 5.75, which is below the conventional threshold of 10, indicating a weak instrument and a violation of the relevance condition.",
      "D": "The relevance condition cannot be assessed with the first-stage results; it can only be satisfied if the placebo test in Table 1 shows a significant effect."
    },
    "Answer": "B",
    "pi_justification": "This item tests the student's ability to perform a standard diagnostic check for an instrumental variable: assessing instrument relevance. It uses a Computational Judgment strategy, requiring a two-step process of calculation (F-statistic) and evaluation (comparison to a rule of thumb). Distractors are based on a precise error matrix: a calculation error (forgetting to square the t-statistic, C), a judgment error (misinterpreting the threshold, A), and a fundamental conceptual error about IV diagnostics (D).",
    "qid": "316",
    "model_evaluation_result": {
      "model_raw_response": "B) The first-stage regression coefficient of `D_im`t on `z_mt`=0.23 has a standard error `SE`=0.04. The F-statistics is calculated as the ratio of the explained variance to the unexplained variance. Since the coefficient is significant and the standard error is small, the F-statistic would be large, indicating a significant relationship between `D_im`t and `z_mt`. Therefore, the instrument is relevant, and the correct statement is B) The F statistic is approximately 5.76 (0.23/0.04^2), which is well-above the conventional threshold.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 317,
    "Question": "### Background\n\n**Research Question.** This question examines the descriptive evidence supporting the paper's central theory that the make-or-buy decision for clinical trials is driven by the trade-off between 'knowledge production' and 'data production'.\n\n**Setting / Institutional Environment.** The paper's theory posits that firms insource knowledge-intensive projects and outsource data-intensive projects. The drug development process is sequential, with early phases (I-III) being more uncertain and knowledge-intensive than post-approval Phase IV trials. The paper notes that the available data exclude 'turnkey' outsourced trials, where the CRO handles all aspects, which may understate the true difference in outsourcing rates between phases.\n\n### Data / Model Specification\n\nThe following table provides descriptive statistics on outsourcing patterns across different trial phases.\n\n**Table 1: Distribution of Outsourcing Across Trial Phase**\n\n| Trial Phase | Mean Outsourcing Level | Freq. |\n|:---|---:|---:|\n| PHASE I Overall | 0.242 | 1,663 |\n| PHASE I—Oncology | 0.094 | 213 |\n| PHASE II | 0.139 | 1,594 |\n| PHASE III | 0.148 | 2,985 |\n| PHASE IV | 0.204 | 584 |\n\n### Question\n\nAssume the true number of Phase III trials was 10% higher than observed in Table 1, and that these missing trials were all 'turnkey' outsourced projects. Based on this assumption, what would be the adjusted outsourcing rate for Phase III trials?",
    "Options": {
      "A": "15.6%",
      "B": "22.5%",
      "C": "24.8%",
      "D": "14.8%"
    },
    "Answer": "B",
    "pi_justification": "Assessment Target: This item tests the ability to perform a multi-step counterfactual calculation based on descriptive data. Strategy: Computational Judgment. Distractor Logic: Distractors are based on predictable errors. (A) is the correctly calculated adjusted rate for Phase II, not Phase III. (C) results from a common denominator error (dividing the new number of outsourced trials by the old total number of trials). (D) represents a conceptual error where the rate is assumed to be unchanged.",
    "qid": "317",
    "model_evaluation_result": {
      "model_raw_response": "C) 25.3% (Note: 148% of 2,685 = 24.75%)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 318,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the bias in regulatory assessments of firm conduct when the regulator uses an incomplete or static measure of marginal cost in a dynamic setting with capital adjustment costs.\n\n**Setting.** A regulator observes a firm's price and can estimate a proxy for its marginal cost, but this proxy omits a forward-looking component related to future investment plans, which are private information to the firm.\n\n**Variables and Parameters.**\n- `x_t`: Level of the productive input (capital) at time `t`.\n- `I_t`: Gross investment at time `t`.\n- `c(I)`: A strictly convex cost function for investment, `c''(I) > 0`.\n- `f'(x)`: Marginal product of input `x`.\n- `\\mathcal{MC}_t^d`: True dynamic marginal cost of output at time `t`.\n- `r`: Interest rate.\n- `\\delta`: Depreciation rate.\n- `\\eta`: Capital survival rate, `\\eta = 1 - \\delta`.\n\n---\n\n### Data / Model Specification\n\nThe true dynamic marginal cost of output is:\n```latex\n\\mathcal{MC}_{t}^{d}=\\frac{(1+r)[1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})]}{f^{\\prime}(x_{t})} \n```\nThis can be decomposed as `\\mathcal{MC}_{t}^{d} = \\widehat{MC}_{t} + \\widehat{E}_{t}`, where `\\widehat{MC}_{t} = \\frac{(r+\\delta)[1+c'(I_{t-1})]}{f'(x_t)}` is an observable proxy and `\\widehat{E}_{t}` is an unobservable error term.\n\n---\n\n### Question\n\nGiven the decomposition `\\mathcal{MC}_{t}^{d} = \\widehat{MC}_{t} + \\widehat{E}_{t}`, what is the correct formula for the error term `\\widehat{E}_{t}` and what is its sign during a period of investment expansion (i.e., `I_t > I_{t-1}`)?",
    "Options": {
      "A": "`\\widehat{E}_{t} = \\frac{\\eta[c'(I_{t-1}) - c'(I_t)]}{f'(x_t)}`; Sign is negative.",
      "B": "`\\widehat{E}_{t} = \\frac{\\eta[c'(I_{t-1}) - c'(I_t)]}{f'(x_t)}`; Sign is positive.",
      "C": "`\\widehat{E}_{t} = \\frac{\\eta[c'(I_{t}) - c'(I_{t-1})]}{f'(x_t)}`; Sign is positive.",
      "D": "`\\widehat{E}_{t} = \\frac{(r+\\delta)[c'(I_{t-1}) - c'(I_t)]}{f'(x_t)}`; Sign is negative."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to perform a key algebraic derivation and apply a convexity assumption to determine the direction of bias in marginal cost measurement. It uses a Computational Judgment strategy. The distractors are designed to catch common errors: a sign error from misinterpreting convexity (B), an algebraic slip in the derivation (C), and using an incorrect but plausible term from a related formula (D).",
    "qid": "318",
    "model_evaluation_result": {
      "model_raw_response": "B) $\\widehat{ET} = \\dfrac{\\eta [c'(I_t) - c'(It_1)]}{f(x_t)}$; Sign is Positive.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 319,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the bias in regulatory assessments of firm conduct when the regulator uses an incomplete or static measure of marginal cost in a dynamic setting with capital adjustment costs.\n\n**Setting.** A regulator observes a firm's price and can estimate a proxy for its marginal cost, but this proxy omits a forward-looking component related to future investment plans, which are private information to the firm.\n\n**Variables and Parameters.**\n- `I_t`: Gross investment at time `t`.\n- `c(I)`: A strictly convex cost function for investment, `c''(I) > 0`.\n- `P_t`: Price of output at time `t`.\n- `\\mathcal{MC}_t^d`: True dynamic marginal cost of output at time `t`.\n- `\\widehat{MC}_t`: An observable but incomplete proxy for marginal cost.\n- `RN`: The Ramsey Number, `RN = \\frac{P-MC}{P}|\\varepsilon_D|`, which is 0 for a first-best firm and 1 for a pure monopolist.\n\n---\n\n### Data / Model Specification\n\nThe true dynamic marginal cost `\\mathcal{MC}_t^d` can be decomposed as `\\mathcal{MC}_{t}^{d} = \\widehat{MC}_{t} + \\widehat{E}_{t}`, where `\\widehat{E}_{t}` is an unobservable error term. During a period of investment expansion (`I_t > I_{t-1}`), the error term `\\widehat{E}_{t}` is negative.\n\n---\n\n### Question\n\nA profit-maximizing monopolist, whose true Ramsey Number is 1, is observed by a regulator during a period of rapid investment expansion (`I_t > I_{t-1}`). The regulator calculates an estimated Ramsey Number, `\\widehat{RN}_t`, using the proxy `\\widehat{MC}_t`. Which of the following correctly describes the value of `\\widehat{RN}_t` and the implication for regulatory practice?",
    "Options": {
      "A": "`\\widehat{RN}_t < 1`. This implies that a low calculated Ramsey Number is not sufficient evidence to rule out monopolistic behavior.",
      "B": "`\\widehat{RN}_t > 1`. This implies that regulators might incorrectly accuse a firm of excessive pricing during expansion.",
      "C": "`\\widehat{RN}_t < 1`. This implies that the firm is likely behaving competitively, as the number is closer to the first-best value of 0.",
      "D": "`\\widehat{RN}_t = 1`. This implies that the proxy measure is unbiased and accurately reflects the firm's monopolistic behavior."
    },
    "Answer": "A",
    "pi_justification": "This item tests the student's ability to synthesize the result from the previous step into a final regulatory conclusion, assessing the core policy takeaway of this part of the paper. It uses a Computational Judgment strategy. The distractors are designed around key conceptual errors: getting the direction of the bias wrong (B), getting the calculation right but the policy implication wrong (C), and incorrectly assuming the proxy is unbiased (D).",
    "qid": "319",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: This answer is based on the given model specification and the description of the scenario. The correct answer is C) `\\wide hat{RN}_ t < 1` This implies that firm is likely to behave competitively, because the proxy measure of Ramsey number is closer the first-best, which is 1 for the monopolist.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 320,
    "Question": "### Background\n\nThe paper contrasts two models of earnings determination. The true data generating process is assumed to be a random coefficients model where returns to skills can differ by geographic labor market. Individual earnings `yᵢⱼ` are determined by individual attributes `Xᵢ` with market-specific returns `βᵢⱼ`:\n```latex\ny_{ij} = X_{i}β_{ij} + ε_{i} \\quad \\text{(Eq. (1))}\n```\nThe expected returns in a market `j`, `β̄ⱼ`, are a function of observable market characteristics `Lⱼ`:\n```latex\nE(β_{ij}) = β̄_{j} = L_{j}Λ + ω_{j} \\quad \\text{(Eq. (2))}\n```\nAn alternative, misspecified “gross standardization” model is proposed, which assumes constant returns `β̄` and adds market characteristics `Lⱼ` directly to the earnings equation:\n```latex\ny_{ij} = X_{i}β̄ + L_{j}Λ^{*} + ν_{i} \\quad \\text{(Eq. (3))}\n```\n\n### Question\n\nAccording to the framework presented, the “gross standardization” model in Eq. (3) is misspecified because it omits a key term present in the true data generating process implied by Eq. (1) and Eq. (2). What is the nature of this omitted term, and under what condition does its omission become a more severe problem?",
    "Options": {
      "A": "An interaction term between individual attributes and market characteristics (`XᵢLⱼ`). The problem is more severe when the variance of market characteristics (`Var(Lⱼ)`) is large.",
      "B": "A squared term for individual attributes (`Xᵢ²`) to capture non-linear returns. The problem is more severe when returns to experience are highly non-linear.",
      "C": "An interaction term between individual attributes and market characteristics (`XᵢLⱼ`). The problem is more severe when the variance of the returns to those attributes across markets (`Var(β̄ⱼ)`) is large.",
      "D": "A market-level fixed effect. The problem is more severe when there are large unobserved differences in productivity across markets."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the student's ability to identify the specific form of misspecification (an omitted interaction term) in a random coefficients context. It uses a Reverse-Reasoning strategy, asking for the cause of the model's failure. Distractor A correctly identifies the term but provides an incorrect condition for the severity of the bias (Almost Right). Distractors B and D point to other plausible but incorrect forms of misspecification (True but Irrelevant; Conceptual Opposite).",
    "qid": "320",
    "model_evaluation_result": {
      "model_raw_response": "C) Aninteraction term between individualattributes and market characteristics (XⱼLⱽ). The problem ismore severe when thevariance of the returnsto those attributes acrossmarkets (Var(ⱼ)) is large.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 321,
    "Question": "### Background\n\nThe author's proposed empirical strategy is a two-step procedure: (1) Estimate market-specific returns `β̂ⱼ` for each market `j` by running the regression `yᵢⱼ = Xᵢβⱼ + εᵢ`. (2) Use these estimates as the dependent variable in a second-stage regression to explain the returns: `β̂ⱼ = LⱼΛ + ωⱼ`. This is known as a 'generated regressor' problem.\n\n### Question\n\nWhat is the primary econometric problem this two-step procedure creates for statistical inference on the coefficients `Λ` in the second stage, and what is the appropriate correction?",
    "Options": {
      "A": "Endogeneity, because `Lⱼ` is correlated with the second-stage error term. The appropriate correction is to use an instrumental variable (IV) for `Lⱼ`.",
      "B": "Heteroskedasticity, because the first-stage estimation error in `β̂ⱼ` has a variance that differs across markets `j`. The appropriate correction is to use Weighted Least Squares (WLS).",
      "C": "Sample selection bias, because markets `j` are not randomly selected. The appropriate correction is a Heckman selection model.",
      "D": "Measurement error in the dependent variable `β̂ⱼ`. This leads to larger standard errors but does not bias the estimates of `Λ`, so no correction is needed."
    },
    "Answer": "B",
    "pi_justification": "This item assesses knowledge of a specific, advanced econometric issue: the consequences of using a generated regressor as a dependent variable. It uses a Computational Judgment strategy, asking for the correct diagnosis and solution. Distractor A confuses the issue with endogeneity (Conceptual: Classic Confusion). Distractor C points to a different, irrelevant problem (True but Irrelevant). Distractor D correctly identifies the source as measurement error but incorrectly states the consequences and the need for correction, making it a subtle but critical error (Conceptual: Almost Right).",
    "qid": "321",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: I will only provide the answer as per the output format)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 322,
    "Question": "### Background\n\nAn industry has `n` symmetric incumbent firms and one potential entrant. The equilibrium outcome depends on the fixed entry cost, `D`, relative to two critical thresholds, `D_1` and `D_2`.\n\n### Data / Model Specification\n\nLet `ξ_k` be the unique symmetric Cournot equilibrium output for each firm in a market with `k` identical firms.\n\n1.  **Accommodated Entry Threshold (`D_1`):** This is the operating profit an entrant would earn if, upon its entry, all `n+1` firms play the standard `(n+1)`-firm Cournot equilibrium.\n    ```latex\n    D_1 = ξ_{n+1}φ((n+1)ξ_{n+1}) - c(ξ_{n+1})\n    ```\n\n2.  **Deterred Entry Threshold (`D_2`):** This is the maximum possible operating profit an entrant could earn if the `n` incumbents are committed to producing their `n`-firm Cournot equilibrium quantities, `ξ_n`.\n    ```latex\n    D_2 = \\max_{q≥0} [qφ(nξ_n + q) - c(q)]\n    ```\n\n### Question\n\nWhich statement best describes the economic interpretation of the strategic beliefs underlying the thresholds `D_1` and `D_2`?",
    "Options": {
      "A": "`D_1` represents the entrant's profit if incumbents accommodate entry by reducing output to the `(n+1)`-firm level, while `D_2` represents the entrant's maximum profit if incumbents deter entry by maintaining the `n`-firm output level.",
      "B": "`D_1` represents the entrant's profit if incumbents deter entry by maintaining the `n`-firm output level, while `D_2` represents the entrant's maximum profit if incumbents accommodate entry by reducing output to the `(n+1)`-firm level.",
      "C": "`D_1` represents the total profit of an incumbent firm after entry is accommodated, while `D_2` represents the total profit of an incumbent firm if entry is successfully deterred.",
      "D": "`D_1` is the limit price that incumbents must set to prevent entry, while `D_2` is the monopoly profit the entrant could earn if all incumbents exited the market."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core conceptual understanding of the two critical entry cost thresholds. It uses a Reverse-Reasoning strategy, asking for the underlying strategic assumptions given the definitions. Distractor B is the primary conceptual opposite, swapping the definitions. Distractor C incorrectly shifts the perspective from the entrant's potential profit to the incumbent's profit. Distractor D introduces related but incorrect concepts from industrial organization (limit pricing, monopoly profit) that are true but irrelevant here.",
    "qid": "322",
    "model_evaluation_result": {
      "model_raw_response": "B) `A) D_1` Represents the entrant\\'s profit if incumbants accommodate entry by redcing output to the (n+1)-firm level, whil`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 323,
    "Question": "### Background\n\nAn industry has `n` symmetric incumbent firms and one potential entrant. The equilibrium outcome depends on the fixed entry cost, `D`, relative to two critical thresholds, `D_1` and `D_2`.\n\n### Data / Model Specification\n\nLet `ξ_k` be the unique symmetric Cournot equilibrium output for each firm in a market with `k` identical firms.\n\n1.  **Accommodated Entry Threshold (`D_1`):** This is the operating profit an entrant would earn if, upon its entry, all `n+1` firms play the standard `(n+1)`-firm Cournot equilibrium.\n\n2.  **Deterred Entry Threshold (`D_2`):** This is the maximum possible operating profit an entrant could earn if the `n` incumbents are committed to producing their `n`-firm Cournot equilibrium quantities, `ξ_n`.\n\nIt is established that `D_1 < D_2`.\n\n### Question\n\nIn the \"low cost\" region where the entry cost `D` is less than `D_1`, what is the noncooperative equilibrium outcome?",
    "Options": {
      "A": "The entrant enters with certainty (`δ* = 1`), and all `n+1` firms produce the `(n+1)`-firm Cournot quantity, `ξ_{n+1}`.",
      "B": "The entrant stays out (`δ* = 0`), and the `n` incumbents produce the `n`-firm Cournot quantity, `ξ_n`.",
      "C": "The entrant randomizes its entry decision (`0 < δ* < 1`), and incumbents produce a quantity between `ξ_n` and `ξ_{n+1}`.",
      "D": "The entrant enters with certainty (`δ* = 1`), but the `n` incumbents continue to produce their `n`-firm Cournot quantity, `ξ_n`."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to apply the model's central result to a specific scenario. It uses a Scenario Application strategy. The correct answer identifies the pure strategy equilibrium for the low-cost case. Distractor B is the conceptual opposite (the high-cost outcome). Distractor C describes the intermediate-cost outcome. Distractor D presents an internally inconsistent scenario (entry occurs, but incumbents play a non-best-response), representing a common comprehension slip.",
    "qid": "323",
    "model_evaluation_result": {
      "model_raw_response": "A) The enrant enters with certainy (`δ*` = 1), and all n+1 firms produce the (n+1)-firm Cournot quantiy, `ξ_(n+1)`.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 324,
    "Question": "### Background\n\nConsider a standard `n`-firm symmetric Cournot oligopoly. Each firm `j` chooses an output quantity `q_j` to maximize its profit. A symmetric noncooperative equilibrium is an output `ξ_k` such that if all `k` firms produce `ξ_k`, no firm has an incentive to deviate.\n\n### Data / Model Specification\n\n*   **Profit:** `π_j = q_j φ(∑_{i=1}^{n} q_i) - c(q_j)`\n*   **Costs:** `c'(q) > 0`, `c''(q) ≥ 0` (positive, non-decreasing marginal cost).\n*   **Demand:** `φ'(q) < 0`, `φ''(q) ≤ 0` (downward sloping, linear or concave demand).\n\n### Question\n\nIf the number of firms in the industry increases from `n` to `m` (where `m > n`), how do the equilibrium individual firm output (`ξ`) and total industry output (`Q`) change?",
    "Options": {
      "A": "Individual output `ξ` decreases, and total output `Q` increases.",
      "B": "Individual output `ξ` increases, and total output `Q` increases.",
      "C": "Individual output `ξ` decreases, and total output `Q` decreases.",
      "D": "Individual output `ξ` increases, and total output `Q` decreases."
    },
    "Answer": "A",
    "pi_justification": "This item assesses knowledge of the fundamental comparative statics of the Cournot model, which is a prerequisite for understanding the entry game. It tests a sharp, factual conclusion. The distractors represent the three possible combinations of sign errors, targeting the common misconception that more firms might lead to lower total output, or that firms might produce more individually to compete.",
    "qid": "324",
    "model_evaluation_result": {
      "model_raw_response": "C\n\n(Note: I've output only the answer letter \"C\")",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 325,
    "Question": "### Background\n\nConsider a standard `n`-firm symmetric Cournot oligopoly. Each firm `j` chooses an output quantity `q_j` to maximize its profit. A symmetric noncooperative equilibrium is an output `ξ_n` such that if all `n` firms produce `ξ_n`, no firm has an incentive to deviate.\n\n### Data / Model Specification\n\n*   **Profit:** `π_j = q_j φ(∑_{i=1}^{n} q_i) - c(q_j)`\n*   **Costs:** `c'(q) > 0`, `c''(q) ≥ 0` (positive, non-decreasing marginal cost).\n*   **Demand:** `φ'(q) < 0`, `φ''(q) ≤ 0` (downward sloping, linear or concave demand).\n\n### Question\n\nAs the number of firms `n` in this symmetric Cournot model approaches infinity, to what value does the equilibrium market price, `P_n = φ(nξ_n)`, converge?",
    "Options": {
      "A": "Marginal cost at zero output, `c'(0)`.",
      "B": "The monopoly price.",
      "C": "Zero.",
      "D": "Average variable cost at the equilibrium output."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the classic result that the Cournot equilibrium converges to the perfectly competitive outcome. It tests a sharp, factual conclusion. Distractor B is the conceptual opposite (the `n=1` case). Distractor C is a plausible but incorrect guess. Distractor D targets the common confusion between marginal and average cost, a classic misconception.",
    "qid": "325",
    "model_evaluation_result": {
      "model_raw_response": "C) Zero. \n\n(Note: I did not write any explanation, just the correct answer)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 326,
    "Question": "### Background\n\n**Research Question.** This problem involves the application of the bootstrap threshold test to a real-world climate science dataset and the interpretation of the resulting nonlinear model.\n\n**Setting.** The monthly time series of the Greenland ice sheet mass balance is analyzed. After testing for nonlinearity, a Threshold Autoregressive Moving Average (TARMA) model is fitted to capture the dynamics.\n\n### Data / Model Specification\n\nTable 1 presents the results of the supLM tests for a threshold effect in the detrended time series, using an AR order of 13 (selected by AIC) and varying the delay parameter `d`.\n\n**Table 1: sLM Test Results for Greenland Ice Sheet Mass Balance (AR order=13)**\n\n| d   | sLMa (statistic) | sLMi (p-value) | sLMw (p-value) | sLMh (p-value) |\n| :-- | :--------------- | :------------- | :------------- | :------------- |\n| 1   | 33.5             | 0.042          | 0.102          | 0.236          |\n| 3   | 48.9             | 0.000          | 0.001          | 0.026          |\n| 6   | 62.5             | 0.000          | 0.000          | 0.001          |\n\nBased on the test results, a TARMA model was estimated with `d=6`:\n\n```latex\nX_t = 0.46 X_{t-1} + 0.75 \\varepsilon_{t-1} + \\text{RegimeComponent}_t + \\varepsilon_t\n```\nwhere:\n- If `X_{t-6} \\leq -5.86` (Lower Regime): `\\text{RegimeComponent}_t = 56.39 + 0.21 X_{t-6} + 0.30 X_{t-11}`\n- If `X_{t-6} > -5.86` (Upper Regime): `\\text{RegimeComponent}_t = -24.14 + 0.21 X_{t-12} - ...`\n\nBased on the test results in Table 1 (for d=6) and the estimated model, what is the correct statistical conclusion at the 1% significance level and the corresponding climatological interpretation of the model's regimes?",
    "Options": {
      "A": "Reject the null hypothesis of linearity; the Lower Regime (positive intercept) represents winter accumulation and the Upper Regime (negative intercept) represents summer melt.",
      "B": "Reject the null hypothesis of linearity; the Lower Regime (positive intercept) represents summer melt and the Upper Regime (negative intercept) represents winter accumulation.",
      "C": "Fail to reject the null hypothesis of linearity; the Lower Regime (positive intercept) represents winter accumulation and the Upper Regime (negative intercept) represents summer melt.",
      "D": "Fail to reject the null hypothesis of linearity; the Lower Regime (positive intercept) represents summer melt and the Upper Regime (negative intercept) represents winter accumulation."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to perform a standard hypothesis test using a p-value and then interpret the parameters of the resulting model in a real-world context. It uses a **Computational Judgment** strategy. The distractors are designed to isolate specific errors: (B) misinterprets the model parameters (conceptual opposite), (C) misinterprets the p-value (classic statistical error), and (D) combines both errors.",
    "qid": "326",
    "model_evaluation_result": {
      "model_raw_response": "B) Rejecting the null hypothesis, the Lower Regimes represents summer melt, and the Upper regime represents winter accumulation, because the p-values for sLMi and sLMw are both less than 0.01, indicating that the null hypothesis can be rejected at the 0.1% significance levels.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 327,
    "Question": "### Background\n\n**Research Question.** This problem involves the application of a fitted nonlinear model to a counterfactual scenario to understand its dynamic properties.\n\n**Setting.** A Threshold Autoregressive Moving Average (TARMA) model was fitted to the Greenland ice sheet mass balance data to capture its asymmetric seasonal cycle.\n\n### Data / Model Specification\n\nThe estimated TARMA model is:\n\n```latex\nX_t = 0.46 X_{t-1} + 0.75 \\varepsilon_{t-1} + \\text{RegimeComponent}_t + \\varepsilon_t\n```\nwhere:\n- If `X_{t-6} \\leq -5.86` (Lower Regime): `\\text{RegimeComponent}_t = 56.39 + 0.21 X_{t-6} + 0.30 X_{t-11}`\n- If `X_{t-6} > -5.86` (Upper Regime): `\\text{RegimeComponent}_t = -24.14 + 0.21 X_{t-12} - ...`\n\nConsider a counterfactual scenario where an unusually severe summer melt season leaves the ice sheet mass balance at `X_{t-6} = -150`. According to the model, which regime will be active six months later, and what does this imply about the system's stability?",
    "Options": {
      "A": "The Lower Regime will be active, implying a stabilizing, mean-reverting feedback.",
      "B": "The Upper Regime will be active, implying a stabilizing, mean-reverting feedback.",
      "C": "The Lower Regime will be active, implying an unstable, explosive feedback.",
      "D": "The Upper Regime will be active, implying an unstable, explosive feedback."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to apply the estimated model to a new scenario and draw conclusions about the system's dynamics. It uses a **Scenario Application** strategy. The distractors are designed based on a matrix of plausible errors: (B) a numerical sign error in applying the threshold rule, (C) a conceptual error in interpreting the feedback mechanism (conceptual opposite), and (D) a combination of both errors.",
    "qid": "327",
    "model_evaluation_result": {
      "model_raw_response": "C) The UpperRegime will beactive, implying anunstable, explosive feedback",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 328,
    "Question": "### Background\n\n**Research Question.** This problem examines the econometric challenges of testing for a threshold effect in an autoregressive model.\n\n**Setting.** In testing a linear AR model against a Threshold AR (TAR) model, the threshold parameter, `r`, is not identified under the null hypothesis of linearity.\n\n### Data / Model Specification\n\nThe Threshold Autoregressive TAR(p) model is:\n```latex\nX_{t}=\\phi_{0}+\\sum_{i=1}^{p}\\phi_{i}X_{t-i}+\\left(\\psi_{0}+\\sum_{i=1}^{p}\\psi_{i}X_{t-i}\\right)I(X_{t-d}\\leq r)+\\varepsilon_{t} \\quad \\text{(Eq. 1)}\n```\nThe null hypothesis of a linear AR(p) model is `H_0: \\boldsymbol{\\psi} = \\mathbf{0}`. To handle the unidentified parameter `r`, the test is constructed as the supremum of the Lagrange Multiplier (LM) statistics over a range of possible values for `r`:\n```latex\nT_{n}=\\operatorname*{sup}_{r\\in[r_{L},r_{U}]}T_{n}(r) \\quad \\text{(Eq. 2)}\n```\nIn testing for a threshold in the model given by Eq. (1), why is the `supLM` test statistic `T_n` in Eq. (2) used, and what is the nature of its asymptotic distribution under i.i.d. errors?",
    "Options": {
      "A": "The `sup` is taken because `r` is unknown under H₀; the resulting distribution is non-standard because it is the maximum of a stochastic process, not a single χ² variable.",
      "B": "The `sup` is taken because `r` is unknown under H₀; the resulting distribution is a χ² distribution with more degrees of freedom to account for the search over `r`.",
      "C": "The `sup` is taken to increase the test's power; the distribution is a standard χ² because the LM principle is robust to nuisance parameters.",
      "D": "The `sup` is taken because the model is nonlinear; the distribution is non-standard because the bootstrap is required for all nonlinear models."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses understanding of the fundamental 'nuisance parameter' problem and its solution via a `sup` test, a core concept in modern econometrics. It uses a **Reverse-Reasoning** strategy. The distractors target key misconceptions: (B) incorrectly assuming the distribution remains χ² (a classic error), (C) misstating the reason for the `sup` test and the nature of the distribution, and (D) providing a true but irrelevant premise with a faulty conclusion.",
    "qid": "328",
    "model_evaluation_result": {
      "model_raw_response": "A) The sup is taken because r is unknown under Ho; the resultingdistribution is non-standardbecause it is themaximum of a stochasticprocess, not a singles χ² variable.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 329,
    "Question": "### Background\n\n**Research Question.** This problem examines a key condition for the validity of standard asymptotic tests.\n\n**Setting.** For a fixed threshold `r`, the standard Lagrange Multiplier (LM) statistic `T_n(r)` for a threshold effect converges to a `χ^2` distribution under the assumption of i.i.d. errors.\n\n### Data / Model Specification\n\nThe Threshold Autoregressive TAR(p) model is:\n```latex\nX_{t}=\\phi_{0}+\\sum_{i=1}^{p}\\phi_{i}X_{t-i}+\\left(\\psi_{0}+\\sum_{i=1}^{p}\\psi_{i}X_{t-i}\\right)I(X_{t-d}\\leq r)+\\varepsilon_{t} \\quad \\text{(Eq. 1)}\n```\nWhat is the primary reason the `χ^2` result for the test statistic `T_n(r)` fails if the errors `ε_t` follow a GARCH process (i.e., are conditionally heteroskedastic)?",
    "Options": {
      "A": "Conditional heteroskedasticity violates the information matrix equality, so the score's variance is no longer the inverse of the Hessian.",
      "B": "The GARCH errors are not normally distributed, which is a requirement for the χ² test.",
      "C": "The parameter estimates under the null hypothesis become biased and inconsistent under GARCH.",
      "D": "The GARCH process makes the observed series Xₜ non-stationary, which violates a key assumption of the test."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses a crucial concept in robust inference: why standard tests fail under heteroskedasticity. The distractors are designed to be plausible but incorrect explanations common among students: (B) confuses heteroskedasticity with non-normality, (C) confuses inconsistency with the effects of GARCH on OLS, and (D) confuses conditional variance dynamics with the stationarity of the process level.",
    "qid": "329",
    "model_evaluation_result": {
      "model_raw_response": "A) A) Conditional heterostedasticity violates...",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 330,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the first step of the paper's in-sample empirical evidence: establishing the number of long-run relationships (the cointegrating rank) in a system of spot and forward exchange rates.\n\n**Setting / Institutional Environment.** The analysis uses a five-variable system (`n=5`) comprising the log spot exchange rate and four log forward exchange rates (`j=4`). The empirical strategy involves a Vector Error Correction Model (VECM). The number of cointegrating relationships (`r`) is determined using the Johansen test procedure. The number of common stochastic trends is `n-r`.\n\n### Data / Model Specification\n\nThe Johansen test for cointegrating rank produces two statistics, the Trace statistic and the λ-max statistic. The testing procedure is sequential. One starts from the null hypothesis of the lowest rank and proceeds until a null hypothesis is not rejected.\n\n**Table 1. Tests of Cointegrating Rank for Dollar-Mark**\n\n| | λ-max Statistic | 5% Critical Value | Trace Statistic | 5% Critical Value |\n|:---|:---:|:---:|:---:|:---:|\n| H₀: r ≤ 4 | 0.196 | 3.762 | 0.196 | 3.762 |\n| H₀: r ≤ 3 | 16.245 | 14.069 | 16.441 | 15.410 |\n\n### Question\n\nBased on the Trace statistic results in Table 1 for the Dollar-Mark system, what is the concluded cointegrating rank `r`?",
    "Options": {
      "A": "r = 3, because the null hypothesis H₀: r ≤ 3 is rejected.",
      "B": "r = 4, because H₀: r ≤ 3 is rejected, but H₀: r ≤ 4 is not rejected.",
      "C": "r = 5, because all tested null hypotheses up to r ≤ 4 are rejected.",
      "D": "r ≤ 3, because the test for H₀: r ≤ 4 is not significant."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the procedural knowledge of interpreting a sequential Johansen cointegration test, a cornerstone of the paper's empirical methodology. It uses a Computational Judgment strategy. The correct answer requires applying the sequential testing logic correctly. Distractor A represents the common error of stopping at the first rejected hypothesis without considering the next step. Distractor C incorrectly interprets the test results. Distractor D confuses the final non-rejection with the conclusion for the rank.",
    "qid": "330",
    "model_evaluation_result": {
      "model_raw_response": "B) r=4, because Ho: r≤3 is rejected but Ho: r ≤4 is notrejected.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 331,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the complete in-sample empirical evidence for the paper's theoretical framework, which posits that spot and forward exchange rates are cointegrated and that the forward premiums form the basis of these long-run relationships.\n\n**Setting / Institutional Environment.** After establishing a cointegrating rank of `r=4` for the five-variable Dollar-Mark system, the paper tests whether the four forward premiums are a valid basis for this cointegration space. It then estimates the Vector Error Correction Model (VECM) to analyze the adjustment dynamics.\n\n### Data / Model Specification\n\n**Table 1. Test of Forward Premiums as Cointegration Basis (Dollar-Mark)**\n\n| | χ²(4) | Marginal Significance Level |\n|:---|:---:|:---:|\n| **Null Hypothesis** | 5.32 | 26% |\n\n*Null Hypothesis: The four forward premiums comprise a basis for the cointegration space.*\n\n**Table 2. VECM Estimates for `Δs_t` Equation (Dollar-Mark)**\n\n| Explanatory Variable | Coeff. |\n|:---|:---:|\n| `(s - f_4)_{t-1}` | -0.981 |\n| `(s - f_{13})_{t-1}` | -0.743 |\n| `(s - f_{26})_{t-1}` | -0.376 |\n| `(s - f_{52})_{t-1}` | -0.061 |\n\n### Question\n\nBased on the combined evidence from Table 1 and Table 2 for the Dollar-Mark, which statement provides the most accurate summary of the in-sample findings?",
    "Options": {
      "A": "The theory is supported: the forward premiums are a valid cointegrating basis, and the negative coefficients on the error correction terms imply that when the spot rate is above the forward rate, the dollar is expected to depreciate.",
      "B": "The theory is rejected: the test in Table 1 fails to reject the null hypothesis, and the coefficients in Table 2 are all negative, which is inconsistent with market efficiency.",
      "C": "The theory is partially supported: the forward premiums are a valid cointegrating basis, but the negative coefficients imply a momentum effect where a spot rate above the forward rate predicts further dollar appreciation.",
      "D": "The theory is rejected: the test in Table 1 shows with a 26% significance level that the forward premiums are not a valid basis for the long-run relationships."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to synthesize results from two different empirical tests to form a coherent conclusion, a key skill in empirical analysis. It uses a Scenario Application strategy. The correct answer (A) correctly interprets the failure to reject the null in Table 1 and the economic meaning of the negative adjustment coefficients. Distractor B incorrectly interprets 'failing to reject'. Distractor C correctly interprets Table 1 but makes a 'Conceptual Opposite' error in interpreting the negative coefficients as momentum instead of error correction. Distractor D misinterprets the meaning of the marginal significance level.",
    "qid": "331",
    "model_evaluation_result": {
      "model_raw_response": "B) Theory is rejected: The test in Table...",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 332,
    "Question": "### Background\n\n**Research Question.** This problem tests the quantitative application of the estimated Vector Error Correction Model (VECM), which links deviations from long-run equilibrium (the forward premiums) to short-run changes in the spot exchange rate.\n\n**Setting / Institutional Environment.** The VECM for the change in the spot rate (`Δs_t`) includes several lagged error correction terms, `(s - f_h)_{t-1}`, which represent the previous period's deviation of the spot rate from the h-week forward rate.\n\n### Data / Model Specification\n\nThe estimated VECM equation for the change in the Dollar-Mark spot rate (`Δs_t`) includes the following terms:\n\n**Table 1. VECM Estimates for `Δs_t` Equation (Dollar-Mark)**\n\n| Explanatory Variable | Coeff. | SE |\n|:---|:---:|:---:|\n| `(s - f_4)_{t-1}` | -0.981 | 0.213 |\n| `(s - f_{13})_{t-1}` | -0.743 | 0.202 |\n\n### Question\n\nSuppose that in week `t-1`, the 4-week forward premium for the Dollar-Mark was `(s - f_4)_{t-1} = -0.02` (a 2% forward discount on the dollar). Assume all other explanatory variables in the `Δs_t` equation were zero. Based on the estimates in Table 1, what is the predicted change in the log spot rate, `Δs_t`, for the next week?",
    "Options": {
      "A": "+0.01962",
      "B": "-0.01962",
      "C": "+0.01486",
      "D": "-0.00426"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a precise calculation using estimated model parameters, a fundamental skill for applying empirical models. It uses a Computational Judgment strategy. The correct answer (A) is found by multiplying the coefficient by the given value (`-0.981 * -0.02`). Distractor B is a 'Sign Error', a very common mistake in such calculations. Distractor C is a 'Formula Misuse' error, resulting from using the wrong coefficient (for the 13-week premium). Distractor D is another 'Formula Misuse' error, resulting from incorrectly using the standard error instead of the coefficient.",
    "qid": "332",
    "model_evaluation_result": {
      "model_raw_response": "B) - 0.0196² (0.981 * -0.020)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 333,
    "Question": "### Background\n\nAn auction is held with two local bidders (1 and 2) and one global bidder (3) for two items (A and B). Bidder 1 values only item A at `$v_1$`, bidder 2 values only item B at `$v_2$`, and bidder 3 values only the package AB at `$V$`. The allocation is based on bids: the global bidder wins if `$B > b_1+b_2$`, otherwise the local bidders win their respective items.\n\n### Data / Model Specification\n\nIn the BCV (Bidder-optimal Core-selecting with Vickrey-minimizing) auction, the payment rule for a local bidder `$i$` (when local bidders win) is:\n\n```latex\n\\hat{h}_{i}(b) = \\operatorname*{max}(0,B-b_{-i}) + \\frac{1}{2}(B - \\operatorname*{max}(0,B-b_{i}) - \\operatorname*{max}(0,B-b_{-i})) \\quad \\text{(Eq. 1)}\n```\n\nIn a standard Vickrey auction, a bidder's payment is independent of their own bid. By contrast, the BCV payment rule in Eq. (1) creates a strategic incentive to 'shade' bids below true values. Which component of the formula is directly responsible for this incentive?",
    "Options": {
      "A": "`$\\operatorname*{max}(0,B-b_{-i})$`",
      "B": "`$-\\frac{1}{2}\\operatorname*{max}(0,B-b_{i})$`",
      "C": "`$\\frac{1}{2}B$`",
      "D": "The allocation rule condition `$b_1+b_2 \\ge B$`"
    },
    "Answer": "B",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to analyze a formal mathematical expression and connect it to economic incentives. It uses a direct identification strategy. Distractor A is the Vickrey-like part of the payment, a plausible but incorrect source of the *deviation* from Vickrey incentives. Distractor C isolates a term that does not depend on the bidder's own bid `$b_i$`. Distractor D is true but irrelevant, confusing the condition for the rule with the rule's content.",
    "qid": "333",
    "model_evaluation_result": {
      "model_raw_response": "B) `-frac{1/2*max(0,B-b_i)}`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 334,
    "Question": "### Background\n\nAn auction is held with two local bidders (1 and 2) and one global bidder (3) for two items (A and B). Bidder 1 values only item A at `$v_1$`, bidder 2 values only item B at `$v_2$`, and bidder 3 values only the package AB at `$V$`. Local bidders' values are i.i.d. `$U[0,1]$`, and the global bidder's value is `$U[0,2]$`. The allocation is based on bids: the global bidder wins if `$B > b_1+b_2$`, otherwise the local bidders win.\n\n### Data / Model Specification\n\nIn the Bayesian Nash Equilibrium of the BCV auction, the global bidder bids truthfully (`$B(V)=V$`), but local bidders strategically shade their bids according to:\n\n```latex\nb_i(v_i) = \\operatorname*{max}(0, v_i - \\alpha) \\quad \\text{(Eq. 1)}\n```\nwhere `$\\alpha = 3-2\\sqrt{2} > 0$`.\n\nThis bid-shading can lead to an inefficient allocation where the global bidder wins even though the sum of the local bidders' true values is higher. Under which of the following conditions on the true values (`$v_1, v_2, V$`) does this specific type of inefficiency occur?",
    "Options": {
      "A": "`$V > v_1 + v_2$`",
      "B": "`$v_1 + v_2 > V > v_1 + v_2 - 2\\alpha$`",
      "C": "`$v_1 + v_2 - 2\\alpha > V$`",
      "D": "`$V > v_1 + v_2 + 2\\alpha$`"
    },
    "Answer": "B",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item tests the ability to apply an equilibrium strategy to determine its consequences for allocative efficiency. It uses a Computational Judgment strategy. Distractor A represents the condition for efficiency, a conceptual opposite. Distractor C represents the condition under which the local bidders win, failing to identify the inefficiency region. Distractor D introduces a sign error on the bid-shading term, a plausible numerical slip.",
    "qid": "334",
    "model_evaluation_result": {
      "model_raw_response": "C) `v1 + v2 -  2alpha > V`",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 335,
    "Question": "### Background\n\nThe BCV auction was designed to produce outcomes that are stable with respect to bids (`core*`). However, when bidder values are private information, its performance is poor compared to the standard Vickrey auction.\n\n### Data / Model Specification\n\n| Metric | BCV vs. Vickrey |\n|:---|:---|\n| Relative Avg. Efficiency | 98% |\n| Relative Avg. Revenue | 91% |\n| Relative Avg. Distance to Core | 126% |\n\n**Table 1:** Performance of BCV Auction Relative to Vickrey Auction\n\nThe BCV auction was designed to ensure outcomes are in the `core*` (stable with respect to bids), yet Table 1 shows its outcomes are, on average, less efficient, less profitable for the seller, and further from the true `core` than the standard Vickrey auction. What is the best explanation for this paradox?",
    "Options": {
      "A": "The `core*` concept is mathematically flawed and does not guarantee stability even with respect to bids.",
      "B": "The auction fails because local bidders behave irrationally, failing to understand the complex payment rule.",
      "C": "The payment rule, while ensuring stability relative to bids, incentivizes strategic bid-shading. This makes the bids a distorted signal of true value, leading to inefficient allocations that are far from the true `core`.",
      "D": "The Vickrey auction's superior performance is only due to the specific uniform distributions assumed; under other distributions, the BCV auction would be superior."
    },
    "Answer": "C",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to synthesize analysis into a coherent narrative explaining a paradoxical result. It uses a Reverse-Reasoning strategy. Distractor A misattributes the problem to the theory (`core*`) rather than its implementation. Distractor B posits irrationality, a common but incorrect explanation for non-truthful bidding. Distractor D is a 'true but irrelevant' type of argument that attempts to dismiss the paper's main finding as an artifact.",
    "qid": "335",
    "model_evaluation_result": {
      "model_raw_response": "C) The.payment rule, while ensurin",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 336,
    "Question": "### Background\n\nAn infinite-horizon model features a logarithmic utility function, `u(c) = ln(c)`, and a strictly convex production function, `f(x) = x^2`, representing perpetual increasing returns to scale.\n\n### Data / Model Specification\n\nThe stationary optimal policy `g(x)` must satisfy the Euler equation:\n\n```latex\nu'[g(x)] = \\delta u'[g(H(x))] f'(x-g(x)) \\quad \\text{(Eq. 1)}\n```\n\nwhere `H(x) = f(x-g(x))` is the law of motion for capital and `\\delta` is the discount factor. A candidate solution for the policy is a linear function of the state, `g(x) = \\lambda x`.\n\n### Question\n\nFor what values of the discount factor `\\delta` does an economically meaningful interior solution of the form `g(x) = \\lambda x` (i.e., where `0 < \\lambda < 1`) exist?",
    "Options": {
      "A": "`\\delta < 1/2`",
      "B": "`\\delta \\ge 1/2`",
      "C": "An interior solution of this form exists for all `\\delta \\in (0, 1)`.",
      "D": "An interior solution of this form is never optimal due to the convexity of `f(x)`."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to solve the model for its key parameter constraint. It uses the Computational Judgment strategy, requiring the student to solve the Euler equation for `\\lambda` and then derive the condition on `\\delta`. Distractor B is the conceptual opposite, representing the parameter region where the interior solution fails. Distractor C represents a failure to recognize the constraint imposed by the increasing returns. Distractor D offers a plausible but incorrect theoretical objection.",
    "qid": "336",
    "model_evaluation_result": {
      "model_raw_response": "C) An inner solution of this kind exists for all delta in (0,1).",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 337,
    "Question": "### Background\n\nAn infinite-horizon model features a logarithmic utility function, `u(c) = ln(c)`, and a strictly convex production function, `f(x) = x^2`, representing perpetual increasing returns to scale.\n\n### Data / Model Specification\n\nFor discount factors `\\delta < 1/2`, the optimal consumption policy is `g(x) = (1-2\\delta)x`.\n\n### Question\n\nGiven the optimal policy, what are the dynamic properties of the non-zero steady-state capital stock `\\bar{x}`?",
    "Options": {
      "A": "The steady state is `\\bar{x} = 1/(4\\delta^2)` and it is unstable.",
      "B": "The steady state is `\\bar{x} = 1/(4\\delta^2)` and it is stable.",
      "C": "The steady state is `\\bar{x} = 1/(2\\delta)` and it is unstable.",
      "D": "The steady state is `\\bar{x} = 1/(2\\delta)` and it is stable."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to analyze the dynamics of the system given the policy function. It uses the Computational Judgment strategy, requiring a two-step calculation: first, find the steady state `\\bar{x}`, and second, evaluate the derivative of the law of motion at `\\bar{x}` to determine stability. The distractor design matrix covers both possible errors: Distractor B has the correct steady state but incorrect stability judgment (a common conceptual error). Distractor C has an incorrect steady state (from an algebraic slip) but the correct stability judgment. Distractor D is incorrect on both counts.",
    "qid": "337",
    "model_evaluation_result": {
      "model_raw_response": "B) The steadystate is `\\bar {x} =  1/( 4 \\delta ^ 2 )` and it isstable.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 338,
    "Question": "### Background\n\nThe selection of Econometric Society (ES) Fellows is a multi-stage process. First, candidates are nominated, either by the Nominating Committee or by existing Fellows. Since 2012, the Nominating Committee has had an explicit mandate to put forward highly qualified female candidates. Second, the full body of Fellows votes on the slate of nominees to elect new Fellows.\n\n### Data / Model Specification\n\nTable 1 provides descriptive statistics on nominations and elections for the 2012-2019 period. Table 2 provides coefficients from logit models estimated on the population of active economists (Panel A) and the sample of nominees (Panel B) for the same period. The models control for a comprehensive vector of publication and citation records.\n\n**Table 1: Summary of Nomination and Election Rates, 2012-2019**\n| Probability / Share | Value |\n|:---|---:|\n| `P(F|N)`: Fraction Females among Nominees | 12.6% |\n| `P(F|N_c)`: Fraction Females among Committee Nominees | 22.7% |\n| `P(E|N_c)`: Election Rate for Committee Nominees | 50.7% |\n| `P(E|N_f)`: Election Rate for Fellow's Nominees | 12.4% |\n\n*Source: Adapted from Table V of the paper.*\n\n**Table 2: Logit Coefficients for Nomination and Election, 2012-2019**\n| Model (Dependent Variable) | Coef. on `Female` | Coef. on `Committee Nominee` |\n|:---|:---:|:---:|\n| **Panel A: Population at Risk** | |\n| 1. Nomination by Committee | 1.209 (0.252) | - |\n| 2. Nomination by Fellows | 0.280 (0.288) | - |\n| **Panel B: Sample of Nominees** | |\n| 3. Selection as Fellow, Cond. on Nom. | 0.639 (0.478) | 2.723 (0.290) |\n\n*Note: Standard errors in parentheses. All models include a full set of controls for publications and citations. Source: Adapted from Table VI, column 3.*\n\n### Question\n\nBased on the data provided, what is the most accurate conclusion regarding the sharp increase in the share of female ES Fellows in the 2012-2019 period?",
    "Options": {
      "A": "The Nominating Committee's explicit preference for nominating qualified women, amplified by a strong voter preference for Committee-endorsed candidates, was the primary driver.",
      "B": "The primary driver was the growing pool of qualified female economists, which led to a proportional increase in nominations from both the Committee and the general body of Fellows.",
      "C": "The general body of Fellows, not the Nominating Committee, began showing a strong preference for nominating and electing female candidates after 2012.",
      "D": "The main reason was that voters began strongly preferring any candidate nominated by the Committee, and the Committee happened to nominate a few more women by chance."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Synthesizing descriptive statistics and regression results to identify a key mechanism. Strategy: Computational Judgment. Distractor Logic: (B) represents a 'pipeline' explanation that is refuted by the regression evidence of a preference shift. (C) incorrectly assigns the preference shift to the Fellows instead of the Committee. (D) correctly identifies the voter endorsement effect but understates the Committee's active, preferential role in nominating women, which is evident from the large coefficient in Model 1.",
    "qid": "338",
    "model_evaluation_result": {
      "model_raw_response": "C) Thegeneral body of Fellaws, not the nominating committee, began showing strong preference for nominaing and electing femalw candidates after 20",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 339,
    "Question": "### Background\n\nAnalysis of Econometric Society (ES) Fellow elections reveals a strong \"endorsement effect\": after controlling for qualifications, candidates nominated by the Nominating Committee are far more likely to be elected. This effect is captured by the coefficient on a `Committee Nominee` indicator in a logit model of election probability.\n\n### Data / Model Specification\n\nThe probability of election for a nominee with characteristics `X` is given by a logit model. The log-odds of election for a female Committee nominee are determined by:\n\n`log-odds = β_0 + β_F * Female + β_C * CommitteeNominee + X'γ`\n\nKey coefficients for the 2012-2019 period are:\n*   `β_F` (Female): 0.639\n*   `β_C` (Committee Nominee): 2.723\n\nAssume the baseline probability of election for a male non-Committee nominee with average qualifications `X` is `P_0 = Λ(β_0 + X'γ) = 0.10`.\n\n### Question\n\nSuppose the ES removes the identifier for Committee nominees on the ballot, causing the endorsement effect to disappear (i.e., the coefficient `β_C` becomes zero). Given the baseline probability, by approximately how much would the election probability for a female Committee nominee with the same average qualifications change?",
    "Options": {
      "A": "Decrease by 59 percentage points",
      "B": "Decrease by 76 percentage points",
      "C": "Decrease by 17 percentage points",
      "D": "Increase by 59 percentage points"
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Applying the logit model to perform a counterfactual calculation. Strategy: Computational Judgment. Distractor Logic: The correct answer requires calculating the baseline log-odds from the 10% probability (which is -2.197), calculating the 'status quo' probability (76.2%), calculating the 'counterfactual' probability (17.4%), and finding the difference (-58.8pp). (B) is the original probability, not the change. (C) is the counterfactual probability, not the change. (D) is a sign error.",
    "qid": "339",
    "model_evaluation_result": {
      "model_raw_response": "C) Decreases by 17percentage points.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 340,
    "Question": "### Background\n\nA study analyzes the probability of an active economist being selected as an Econometric Society (ES) Fellow. It uses a logit model to estimate the effect of gender, controlling for an extensive set of career achievements like cumulative publications and citations.\n\n### Data / Model Specification\n\nThe table below shows key logit coefficients from the most comprehensive model specification for two different time periods.\n\n**Table 1: Logit Coefficients for Selection as ES Fellow**\n| Variable | 1933-1979 | 2000-2019 |\n|:---|---:|---:|\n| Female | -1.412 | 0.931 |\n| # Papers in *Econometrica* | 0.243 | 0.301 |\n\n*Source: Adapted from Table IV of the paper.*\n\n### Question\n\nUsing the coefficients provided for the 2000-2019 period, how can the estimated premium for being a female candidate be interpreted?",
    "Options": {
      "A": "As an advantage equivalent to having approximately three additional publications in *Econometrica*.",
      "B": "As an advantage equivalent to having approximately 0.32 additional publications in *Econometrica*.",
      "C": "As a penalty equivalent to having approximately three fewer publications in *Econometrica*.",
      "D": "As proof that the selection process was unbiased, since the number of women in the profession had increased."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Interpreting the magnitude of a regression coefficient by benchmarking it against another continuous variable. Strategy: Computational Judgment. Distractor Logic: The correct answer is the ratio of the 'Female' coefficient to the 'Econometrica' coefficient (0.931 / 0.301 ≈ 3.09). (B) is an inversion error (0.301 / 0.931). (C) is a sign error. (D) is a conceptual error, confusing the paper's finding of a conditional premium with a statement about the pipeline of candidates.",
    "qid": "340",
    "model_evaluation_result": {
      "model_raw_response": "B) As a premium equivalent to having 0.31 additional publications in Econometrica.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 341,
    "Question": "### Background\n\nA study finds that in the 2000-2019 period, female economists had a higher probability of being selected as an Econometric Society Fellow than male economists with identical, observable publication and citation records. This conclusion relies on a 'selection on observables' assumption: that after controlling for the rich set of observable achievements (`X`), gender is not correlated with any remaining unobserved determinants of election.\n\n### Question\n\nA potential critique of the study is that, due to higher publishing barriers, female scholars with the same publication record (`X`) as male scholars may have higher unobserved 'true quality' (`Q`). If this is true, how would this unobserved quality bias the estimated coefficient on `Female` for the 2000-2019 period and what is the implication for the paper's conclusion?",
    "Options": {
      "A": "The coefficient is biased upwards, which weakens the conclusion that there is a female premium.",
      "B": "The coefficient is biased upwards, which strengthens the conclusion that there is a female premium.",
      "C": "The coefficient is biased downwards, which weakens the conclusion that there is a female premium.",
      "D": "The coefficient is biased downwards, which strengthens the conclusion that there is a female premium."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Applying the logic of omitted variable bias (OVB) to a specific research context. Strategy: Scenario Application. Distractor Logic: The correct reasoning is: 1) Unobserved quality `Q` is positively correlated with election. 2) Conditional on `X`, `Q` is positively correlated with `Female`. 3) The product of these two effects is positive, so the bias on the `Female` coefficient is positive (upward). 4) An upward bias means the estimated premium is an overestimate of the true premium, thus weakening the conclusion. The distractors represent a 2x2 matrix of common errors: getting the bias direction wrong (C, D) or getting the implication wrong (B).",
    "qid": "341",
    "model_evaluation_result": {
      "model_raw_response": "B) The coefficent is biased upwards which strengthens the conlcusion that there is female premium.󠁧󠁢󠁥󠁮󠁧",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 342,
    "Question": "### Background\n\n**Research Question.** This question examines the paper's strategy for identifying the causal effect of Employment Protection Legislation (EPL) deregulation on the labor share, focusing on a key threat to identification.\n\n**Setting / Institutional Environment.** The analysis uses a panel of 26 advanced economies from 1970-2013. A key concern is the 'crisis-induces-reform' hypothesis, which suggests that liberalizing reforms are often enacted during economic downturns, potentially biasing the estimated effect of the reform.\n\n### Data / Model Specification\n\nThe paper estimates the aggregate dynamic response of the labor share to a reform using a country-level model. Table 1 presents results from the baseline model and a model that adds the lagged unemployment rate to control for economic crises.\n\n**Table 1: Country-Level Analysis – Labor Share Results**\n\n| Specification | Impact | 1y | 2y | 3y | 4y | 5y |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| **Baseline** | -0.26* | -0.60* | -0.95** | -0.70** | -0.75* | -0.87** |\n| | (0.13) | (0.34) | (0.39) | (0.32) | (0.40) | (0.35) |\n| **Controlling for lagged unemployment** | -0.23 | -0.62* | -0.94** | -0.74** | -0.75* | -0.84** |\n| | (0.14) | (0.33) | (0.37) | (0.33) | (0.39) | (0.33) |\n\n*Notes: Coefficients are the estimated effect in percentage points for a liberalizing reform. Standard errors in parentheses. *, **, *** indicate significance at the 90%, 95%, and 99% level.*\n\n### Question\n\nConsidering the 'crisis-induces-reform' hypothesis, what is the likely direction of the omitted variable bias in the 'Baseline' estimate, and what does the comparison with the 'Controlling for lagged unemployment' result imply about the causal interpretation of the reform's effect?",
    "Options": {
      "A": "The hypothesis implies a positive bias, understating the reform's negative effect. The robustness check supports a causal interpretation because the coefficient remains negative.",
      "B": "The hypothesis implies a negative bias, overstating the reform's negative effect. The robustness check supports a causal interpretation because the 5-year coefficient remains stable and significant (-0.87 vs. -0.84).",
      "C": "The hypothesis implies a negative bias, overstating the reform's negative effect. The robustness check undermines the causal interpretation because the impact coefficient (-0.23) is no longer statistically significant.",
      "D": "The hypothesis is not a threat to the baseline model because country fixed effects already control for economic crises."
    },
    "Answer": "B",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to test the student's ability to identify a specific endogeneity threat ('crisis-induces-reform'), predict the direction of the resulting bias, and correctly interpret a robustness check designed to mitigate it. Distractors are based on a sign error for the bias (A), a misinterpretation of the robustness check (C), and a conceptual error about the power of fixed effects (D).",
    "qid": "342",
    "model_evaluation_result": {
      "model_raw_response": "B) The crisis-induces-reforms hypothesis implies a bias in the opposite direction than what is stated in the other options.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 343,
    "Question": "### Background\n\n**Research Question.** This question assesses the ability to apply the paper's main empirical finding to a concrete scenario.\n\n**Setting / Institutional Environment.** The analysis uses a panel of 26 advanced economies from 1970-2013 to estimate the effect of Employment Protection Legislation (EPL) deregulation on the labor share.\n\n### Data / Model Specification\n\nThe paper estimates the aggregate dynamic response of the labor share to a reform using a country-level model. The baseline results are presented in Table 1.\n\n**Table 1: Country-Level Analysis – Baseline Labor Share Results**\n\n| Specification | Impact | 1y | 2y | 3y | 4y | 5y |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| **Baseline** | -0.26* | -0.60* | -0.95** | -0.70** | -0.75* | -0.87** |\n| | (0.13) | (0.34) | (0.39) | (0.32) | (0.40) | (0.35) |\n\n*Notes: Coefficients are the estimated effect in percentage points for a liberalizing reform. Standard errors in parentheses. *, **, *** indicate significance at the 90%, 95%, and 99% level.*\n\n### Question\n\nSuppose a country's labor share fell by a total of 7.0 percentage points over the study period. During this time, it implemented a net of two major liberalizing EPL reforms. Based on the 5-year estimate from the baseline model in Table 1, what percentage of this country's total labor share decline can be attributed to these EPL reforms?",
    "Options": {
      "A": "12.4%",
      "B": "1.74%",
      "C": "24.9%",
      "D": "28.6%"
    },
    "Answer": "C",
    "pi_justification": "This item uses a Computational Judgment strategy to assess the student's ability to apply a key empirical finding to a concrete scenario. It requires extracting the correct coefficient, performing a multi-step calculation, and interpreting the result. Distractors are designed to catch common errors: omitting a step (A, forgetting to multiply by 2 reforms), misinterpreting the final unit (B, reporting the percentage point change), and misusing the formula (D).",
    "qid": "343",
    "model_evaluation_result": {
      "model_raw_response": "B) 17.4% (Note: To calculate the answer, we need to multiply the 5th column value (-0.87) from the table by 2, since there were two EPL reforms, and then divide the result by the total decline in labor share (7.0). This gives us 1.87 / 7.00 = 0.267 or approximately 26.7%. The closest option is B) 1/74%.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 344,
    "Question": "### Background\n\nStandard evolutionary models with mutations, such as those by Kandori, Mailath, and Rob (KMR), often yield a unique long-run prediction by assuming a uniform mutation rate \\(\\varepsilon\\) across all states. This paper critiques that assumption by allowing mutation rates to be state-dependent. The core insight is that the relative probability of mutations, not just the number of mutations, determines long-run stability.\n\nThis problem explores a specific 2x2 coordination game where state-dependent mutations can reverse the standard KMR prediction.\n\n### Data / Model Specification\n\nA population of \\(N\\) agents repeatedly plays the following coordination game:\n```latex\n          Player 2\n          1      2\nPlayer 1\n   1    (8, 8) (0, 4)\n   2    (4, 0) (6, 6)\n```\nThe system is modeled as a Markov process where the state \\(i \\in S = \\{0, 1, ..., N\\}\\) is the number of agents playing strategy 2. The system has two absorbing states corresponding to the strict Nash equilibria: state 0 (all play strategy 1) and state \\(N\\) (all play strategy 2).\n\nThere is an integer threshold \\(i^* < N/2\\). Without mutations:\n*   The set \\(\\{0, 1, ..., i^*-1\\}\\) is the basin of attraction for state 0.\n*   The set \\(\\{i^*, ..., N\\}\\) is the basin of attraction for state \\(N\\).\n\nMutations are introduced with state-dependent rates:\n*   In the basin of attraction for state 0 (states \\(0, ..., i^*-1\\)), the mutation rate is \\(\\varepsilon^\\alpha\\).\n*   In the basin of attraction for state \\(N\\) (states \\(i^*, ..., N\\)), the mutation rate is \\(\\varepsilon\\).\n\nHere, \\(\\varepsilon > 0\\) is a small baseline rate and \\(\\alpha > 0\\) is a constant. If \\(\\alpha > 1\\), mutations are less likely when the system is near the Pareto-efficient equilibrium (state 0).\n\nThe ratio of the long-run probabilities (invariant distribution mass) of being in the two absorbing states is given by:\n```latex\n\\frac{q_{0}^{\\prime}}{q_{N}^{\\prime}}=\\varepsilon^{N-i^{*}+1-\\alpha i^{*}}\\frac{k_{1}[1+f_{1}(\\varepsilon)]}{k_{2}[1+f_{2}(\\varepsilon)]} \\quad \\text{(Eq. (1))}\n```\nwhere \\(k_1, k_2\\) are positive constants and \\(f_1(\\varepsilon), f_2(\\varepsilon)\\) are functions that go to zero as \\(\\varepsilon \\to 0\\).\n\n### Question\n\nThe standard KMR model (where \\(\\alpha=1\\)) selects state \\(N\\) because its basin of attraction is larger. This paper's model shows this can be reversed if \\(\\alpha\\) is large enough. Based on Eq. (1), what is the correct expression for the critical threshold \\(\\alpha^*\\) and the intuition for why a high \\(\\alpha\\) favors state 0?",
    "Options": {
      "A": "\\(\\alpha^* = (N-i^*+1)/i^*\\). A high \\(\\alpha\\) makes mutations out of state 0's smaller basin prohibitively rare, making it 'deeper' and more stable.",
      "B": "\\(\\alpha^* = i* / (N-i*+1)\\). A high \\(\\alpha\\) makes mutations into state 0's basin more frequent, increasing its stability.",
      "C": "\\(\\alpha^* = (N-i^*+1)/i^*\\). A high \\(\\alpha\\) increases the size of state 0's basin of attraction, making it more stable.",
      "D": "\\(\\alpha^* = (N-i^*+1)/i* - 1\\). A high \\(\\alpha\\) makes players in state 0 experiment more, which paradoxically reinforces the equilibrium."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses both the derivation of the critical threshold \\(\\alpha^*\\) and the core economic intuition behind the paper's example. It combines 'Computational Judgment' with conceptual understanding. Distractors are designed to separate these skills: (B) uses an inverted formula from a common algebraic error, (C) has the correct formula but a classic conceptual error (confusing mutation probability/depth with basin size), and (D) has an incorrect formula and faulty reasoning.",
    "qid": "344",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: This answer is based on the provided information and my understanding of the problem. It is possible that I may have made a mistake.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 345,
    "Question": "### Background\n\nStandard evolutionary models with mutations, such as those by Kandori, Mailath, and Rob (KMR), often yield a unique long-run prediction by assuming a uniform mutation rate \\(\\varepsilon\\) across all states. This paper critiques that assumption by allowing mutation rates to be state-dependent. The core insight is that the relative probability of mutations, not just the number of mutations, determines long-run stability.\n\n### Data / Model Specification\n\nA population of \\(N\\) agents repeatedly plays the following coordination game:\n```latex\n          Player 2\n          1      2\nPlayer 1\n   1    (8, 8) (0, 4)\n   2    (4, 0) (6, 6)\n```\nThe system is modeled as a Markov process where the state \\(i \\in S = \\{0, 1, ..., N\\}\\) is the number of agents playing strategy 2. The system has two absorbing states corresponding to the strict Nash equilibria: state 0 (all play strategy 1) and state \\(N\\) (all play strategy 2).\n\nThere is an integer threshold \\(i^* < N/2\\). Without mutations:\n*   The set \\(\\{0, 1, ..., i^*-1\\}\\) is the basin of attraction for state 0.\n*   The set \\(\\{i^*, ..., N\\}\\) is the basin of attraction for state \\(N\\).\n\nMutations are introduced with state-dependent rates:\n*   In the basin of attraction for state 0 (states \\(0, ..., i^*-1\\)), the mutation rate is \\(\\varepsilon^\\alpha\\).\n*   In the basin of attraction for state \\(N\\) (states \\(i^*, ..., N\\)), the mutation rate is \\(\\varepsilon\\).\n\nHere, \\(\\varepsilon > 0\\) is a small baseline rate and \\(\\alpha > 0\\) is a constant. The critical threshold for \\(\\alpha\\) to select state 0 is \\(\\alpha^* = (N-i^*+1)/i^*\\).\n\n### Question\n\nSuppose the payoff for mutual cooperation on strategy 1 is increased to \\((8+\\delta, 8+\\delta)\\). This change does not affect \\(i^*\\), but the reduced incentive to experiment increases \\(\\alpha\\) according to the relationship \\(\\alpha(\\delta) = \\alpha_0 + \\gamma\\delta\\) for constants \\(\\alpha_0, \\gamma > 0\\). Assume the system initially selects state \\(N\\) (i.e., \\(\\alpha_0 < \\alpha^*\\)). What is the minimum payoff increase, \\(\\delta_{min}\\), required to reverse the equilibrium selection and make state 0 the unique long-run outcome?",
    "Options": {
      "A": "\\(\\frac{1}{\\gamma} \\left( \\frac{N-i^{*}+1}{i^{*}} - \\alpha_0 \\right)\\)",
      "B": "\\(\\frac{1}{\\gamma} \\left( \\alpha_0 - \\frac{N-i^{*}+1}{i^{*}} \\right)\\)",
      "C": "\\(\\gamma \\left( \\frac{N-i^{*}+1}{i^{*}} - \\alpha_0 \\right)\\)",
      "D": "\\(\\frac{1}{\\gamma} \\left( \\frac{N-i^{*}+1}{i^{*}} \\right)\\)"
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests the ability to apply the model to a novel scenario, a 'Scenario Application' strategy requiring a multi-step 'Computational Judgment'. The distractors are derived from a matrix of plausible errors: (B) a sign error from mishandling the inequality, (C) a formula misuse error (multiplying by \\(\\gamma\\) instead of dividing), and (D) a step-omission error (failing to subtract the initial \\(\\alpha_0\\)). This assesses a deeper understanding beyond simple derivation.",
    "qid": "345",
    "model_evaluation_result": {
      "model_raw_response": "C) γ( N-i*+1 )-α0",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 346,
    "Question": "### Background\n\nA firm is characterized by its capital stock `k` and an idiosyncratic productivity shock `z`. In each period, it makes optimal investment and exit decisions to maximize its market value. External financing is costly.\n\n### Data / Model Specification\n\nThe value of a firm, `p(k,z;w)`, is the solution to the Bellman equation:\n```latex\np(k,z;w) = \\operatorname*{max}_{k'\\geq0} \\Biggl\\{ \\pi(k,z;w) - i(k,k') - \\lambda(a) + \\beta E_{z'|z}[\\operatorname*{max}\\{(1-\\delta)k', p(k',z';w)\\}] \\Biggr\\} \n```\nwhere `a = i(k,k') - π(k,z;w)` is the amount of external finance, and `i(k,k') = k' - (1-δ)k` is investment. The financing cost function `λ(a)` is zero for `a ≤ 0` and positive and increasing for `a > 0` (`λ'(a) > 0`).\n\n### Question\n\nConsider a firm that finds it optimal to invest to a new capital stock `k'` such that it must rely on external funds (i.e., the amount of external finance `a > 0`). Which of the following correctly describes the firm's first-order condition (FOC) for `k'` and its exit rule for the subsequent period?",
    "Options": {
      "A": "FOC: `1 + λ'(a) = βE[∂p(k',z')/∂k']`; Exit Rule: Exit if `E[p(k',z')] < (1-δ)k'`",
      "B": "FOC: `1 = βE[∂p(k',z')/∂k']`; Exit Rule: Exit if `E[p(k',z')] < (1-δ)k'`",
      "C": "FOC: `1 + λ(a) = βE[∂p(k',z')/∂k']`; Exit Rule: Exit if `E[p(k',z')] < (1-δ)k'`",
      "D": "FOC: `1 + λ'(a) = βE[∂p(k',z')/∂k']`; Exit Rule: Exit if `E[p(k',z')] < 0`"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the user's understanding of the model's core microfoundations, combining the interpretation of the exit condition and the derivation of the investment FOC. It uses a 'Computational Judgment' strategy, asking the user to identify the correct optimality conditions. Distractors are designed to target specific, high-frequency errors in dynamic programming: B) ignoring the financing friction (Conceptual Opposite), C) misapplying the chain rule in the FOC (Formula Misuse), and D) misidentifying the firm's outside option as zero instead of its liquidation value (Conceptual Opposite).",
    "qid": "346",
    "model_evaluation_result": {
      "model_raw_response": "C) F OC: `1+λ(a)=βE[∈p(k', z')/∈k']`;Exit Rule: Exitif `E[p (k', z')] < ( 1-δ )k'`",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 347,
    "Question": "### Background\n\nIn a general equilibrium model with heterogeneous firms, a free-entry condition for new firms must hold, which determines the market wage rate `w`. \n\n### Data / Model Specification\n\nThe equilibrium wage rate `w` is determined by the free-entry condition, which states that if entry is positive, the expected value of a new firm (which starts with `k=0`) must be zero:\n```latex\n\\int p(0,z;w)\\varphi(d z) = 0 \n```\nwhere `φ(z)` is the distribution of initial productivity draws for entrants.\n\nThe model's structure implies the following property for the value function:\n**Proposition:** The value function `p(k,z;w)` is continuous and strictly decreasing in the wage rate `w`.\n\n### Question\n\nSuppose the government introduces a lump-sum tax `T > 0` that must be paid by any firm upon entry. How would this policy affect the economy's stationary equilibrium wage rate `w*`, and what is the economic intuition?",
    "Options": {
      "A": "The wage `w*` will decrease, because the tax makes entry less attractive, reducing labor demand and thus lowering the wage required to restore the zero-profit entry condition.",
      "B": "The wage `w*` will increase, because firms pass the cost of the tax on to workers in the form of lower net profits, which must be offset by higher wages to attract labor.",
      "C": "The wage `w*` will not change, because the tax is a fixed cost that only affects the entry decision, not the operating decisions of incumbent firms who set the market wage.",
      "D": "The wage `w*` will decrease, because the tax reduces the cash flow of entering firms, leaving them with less money to pay worker salaries."
    },
    "Answer": "A",
    "pi_justification": "This item tests the user's ability to reason about the model's general equilibrium properties, directly targeting the logic of the original QA's apex question. It uses a 'Scenario Application' strategy, asking for the effect of a new policy (a tax). The distractors are designed based on common errors in equilibrium analysis: B) a simple sign error with flawed reasoning (Sign Error), C) incorrectly concluding no effect by ignoring the entry margin (Conceptual Opposite), and D) providing the correct outcome but with flawed, partial-equilibrium reasoning instead of the correct market-clearing logic (Almost Right).",
    "qid": "347",
    "model_evaluation_result": {
      "model_raw_response": "A) The answer is A. \n\n(Note: This response contains only the letter \"A\", as per the output format specified.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 348,
    "Question": "### Background\n\n**Research Question.** This problem explores the asymptotic theory of the Fully Modified OLS (FM-OLS) estimator, contrasting the standard theory with the more refined fixed-b theory to reveal limitations of the standard approach.\n\n**Setting.** The FM-OLS estimator, $\\widehat{\\theta}^+$, applies two transformations to correct for issues in OLS estimation of cointegrating regressions. Its properties can be analyzed using two different asymptotic frameworks.\n\n### Data / Model Specification\n\nUnder **standard asymptotic theory**, which assumes consistent estimation of long-run variance matrices, the FM-OLS estimator has a clean, zero-mean Gaussian mixture distribution:\n\n```latex\nA^{-1}(\\widehat{\\theta}^{+}-\\theta) \\Rightarrow \\left(\\int B_{v}^{*}(r)B_{v}^{*}(r)^{\\prime}d r\\right)^{-1}\\int B_{v}^{*}(r)d B_{u\\cdot v}(r) \\quad \\text{(Eq. (1))}\n```\n\nUnder **fixed-b asymptotic theory**, which treats the bandwidth-to-sample-size ratio $b=M/T$ as fixed, the estimator has a more complex distribution that includes bias terms:\n\n```latex\nA^{-1}(\\widehat{\\theta}_{b}^{+}-\\theta) \\Rightarrow \\left(\\int B_{v}^{*}(r)B_{v}^{*}(r)^{\\prime}d r\\right)^{-1} \\times\\left(\\int B_{v}^{*}(r)d B_{u v}^{b}(r)+\\mathcal{B}_{1}-\\mathcal{B}_{2}\\right) \\quad \\text{(Eq. (2))}\n```\n\n### Question\n\nBased on a comparison of the standard (Eq. (1)) and fixed-b (Eq. (2)) asymptotic results, what is the primary critique of the standard FM-OLS theory revealed by the fixed-b analysis?",
    "Options": {
      "A": "The standard theory is misleading because it assumes perfect estimation of the nuisance parameters ($\\Omega, \\Delta$), while the fixed-b theory correctly models their sampling variability, which in turn creates a residual asymptotic bias ($\\mathcal{B}_1, \\mathcal{B}_2$).",
      "B": "The fixed-b result proves that the standard asymptotic theory is fundamentally flawed, as the two distributions are irreconcilable.",
      "C": "The standard theory fails to account for the fact that different kernel functions (e.g., Bartlett vs. Quadratic Spectral) will produce different estimates in finite samples.",
      "D": "The standard theory is invalid because its assumption that the bandwidth ratio $b \\to 0$ implies that no lags can be used for long-run variance estimation."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to synthesize the paper's core theoretical critique of FM-OLS. It uses a Reverse-Reasoning strategy, asking for the main implication of the fixed-b result. The correct option (A) identifies the fundamental issue: the practical impact of estimating nuisance parameters, which standard theory ignores. Distractors are designed to be plausible but incorrect interpretations: (B) is a conceptual opposite about reconcilability, (C) identifies a true but superficial consequence rather than the root cause, and (D) reflects a common misunderstanding of asymptotic limits.",
    "qid": "348",
    "model_evaluation_result": {
      "model_raw_response": "A) The primary critique of standard FM-OL",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 349,
    "Question": "### Background\n\n**Research Question.** This problem assesses the performance of machine learning models for one-year-ahead local conflict prediction and benchmarks this performance against standard econometric models to quantify the value added of more complex algorithms.\n\n**Setting and Sample.** The analysis uses a municipality-level panel dataset from Colombia, with out-of-sample predictions generated for each year from 1998 to 2005. The performance of a full-featured machine learning ensemble is compared against several simpler models.\n\n### Data / Model Specification\n\n- **Unit of Observation:** Colombian municipality-year.\n- **Outcome Variable:** `Five or more violent events`: Indicator (1/0) for a “hot spot.” Occurs in ~8% of municipality-years.\n- **Model:** `EBMA`: An Ensemble Bayesian Model Average of four machine learning algorithms (including LASSO, Random Forests) using a full set of over 310 predictors.\n- `AUC`: Area Under the ROC Curve, a performance metric (0.5 = random, 1.0 = perfect).\n\nTable 1 presents the out-of-sample AUC for the EBMA model predicting conflict in Colombia.\n\n**Table 1: Performance of EBMA Model (Colombia, AUC)**\n\n| | EBMA |\n| :--- | :---: |\n| **Colombia (attacks and clashes, 1998-2005)** | |\n| Five or more violent events | 0.915 |\n\n### Question\n\nThe Area Under the ROC Curve (AUC) is a key performance metric. Based on the results for the EBMA model in Table 1, what is the precise probabilistic interpretation of the AUC value of 0.915 for predicting `Five or more violent events`?",
    "Options": {
      "A": "If one municipality that will experience 5+ events and one that will not are chosen at random, there is a 91.5% probability the model assigns a higher risk score to the one that will experience 5+ events.",
      "B": "The model correctly classifies whether a municipality will experience 5+ events 91.5% of the time.",
      "C": "Of all municipalities the model predicts will experience 5+ events, 91.5% of them actually do.",
      "D": "The model successfully identifies 91.5% of all municipalities that actually experience 5+ events."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the foundational knowledge of interpreting the AUC metric, a core concept in classification model evaluation. It uses a Conceptual Judgment strategy. The distractors are designed based on classic, high-frequency misconceptions, specifically confusing AUC with other common but distinct performance metrics like accuracy (B), precision (C), and sensitivity (D).",
    "qid": "349",
    "model_evaluation_result": {
      "model_raw_response": "B) The mode correctly classifieswhether a municipality will experiencenine 5+ eventsg 91.50 of the time.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 350,
    "Question": "### Background\n\n**Research Question.** This problem assesses the performance of machine learning models for one-year-ahead local conflict prediction and benchmarks this performance against standard econometric models to quantify the value added of more complex algorithms.\n\n**Setting and Sample.** The analysis uses a municipality-level panel dataset from Colombia, with out-of-sample predictions generated for each year from 1998 to 2005. The performance of a full-featured machine learning ensemble is compared against several simpler models.\n\n### Data / Model Specification\n\n- **Unit of Observation:** Colombian municipality-year.\n- **Outcome Variable:** `Any violent event`: Indicator (1/0) for at least one attack or clash.\n- **Models:**\n  - `Lagged Predictand`: A simple AR(1) model using only a single lag of the outcome indicator.\n  - `Location FE Only`: An OLS model where the prediction for a location is its historical mean level of violence (i.e., its estimated location fixed effect).\n- `AUC`: Area Under the ROC Curve, a performance metric (0.5 = random, 1.0 = perfect).\n\nTable 1 presents the out-of-sample AUC for various models predicting conflict in Colombia.\n\n**Table 1: Performance Versus Benchmarks (Colombia, AUC)**\n\n| | Lagged Predictand | Location FE Only |\n| :--- | :---: | :---: |\n| **Colombia (attacks and clashes, 1998-2005)** | |\n| Any violent event | 0.743 | 0.828 |\n\n### Question\n\nTable 1 compares the performance of a simple autoregressive model (`Lagged Predictand`) with a model based on a location's historical average risk (`Location FE Only`). Based on the results for predicting `Any violent event`, what does the superior performance of the Location FE model (AUC 0.828 vs. 0.743) imply about the nature of local conflict risk?",
    "Options": {
      "A": "Long-term, persistent spatial risk factors are substantially more important for prediction than short-term temporal persistence.",
      "B": "Year-to-year persistence is a stronger predictor of conflict than a location's historical average risk.",
      "C": "Machine learning models like EBMA can outperform simpler benchmarks.",
      "D": "Neither a location's history nor its recent past are useful for predicting conflict."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item tests the ability to interpret a direct comparison of two benchmark models and draw a conclusion central to the paper's main argument. It uses a Computational Judgment strategy (interpreting table values to make a judgment). The distractors include the conceptual opposite (B), a true but irrelevant statement that distracts from the specific comparison being made (C), and a direct misinterpretation of the data (D).",
    "qid": "350",
    "model_evaluation_result": {
      "model_raw_response": "B) Year-by-year persistence is stronger than a location’s historical average risk.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 351,
    "Question": "### Background\n\n**Research Question.** This problem assesses the performance of machine learning models for one-year-ahead local conflict prediction and benchmarks this performance against standard econometric models to quantify the value added of more complex algorithms.\n\n**Setting and Sample.** The analysis uses a municipality-level panel dataset from Colombia, with out-of-sample predictions generated for each year from 1998 to 2005. The performance of a full-featured machine learning ensemble is compared against several simpler models.\n\n### Data / Model Specification\n\nThe paper notes that with over 310 predictors, a standard OLS model performs poorly (AUC=0.821) compared to a LASSO model (AUC=0.845) for predicting `Any violent event`. This is because LASSO uses a penalty term to prevent overfitting.\n\n### Question\n\nWhat is the primary mechanism by which the LASSO penalty achieves this?",
    "Options": {
      "A": "It shrinks the coefficients of less important predictors, forcing many of them to be exactly zero, which simplifies the model.",
      "B": "It shrinks coefficients towards zero but does not perform variable selection, retaining all predictors in the model.",
      "C": "It sequentially builds decision rules and prunes branches that do not improve predictive accuracy.",
      "D": "It identifies complex non-linear relationships and interaction effects between predictors."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses understanding of the fundamental mechanism of LASSO regularization, a key technique mentioned in the paper. It uses a Reverse-Reasoning strategy (given the method, infer its mechanism). The distractors are designed to be highly plausible by describing the mechanisms of other common and related machine learning/statistical methods: Ridge regression (B), decision trees (C), and a more general goal of other ML algorithms (D).",
    "qid": "351",
    "model_evaluation_result": {
      "model_raw_response": "A) Itshrinks the coefficientsof less important predictors,forging many of themto be exactly zero,simplifying the model.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 352,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the empirical gains from using an asymptotically efficient estimator (2S2SLS) versus a consistent but inefficient one (NL2SLS) in a structural macroeconomic model. It further explores how estimator choice can impact policy conclusions and how to test for systemic model misspecification.\n\n**Setting / Institutional Environment.** The analysis uses limited-information (single-equation) instrumental variable methods to estimate the parameters of Taylor's (1979) model of the U.S. economy. The model's structural equations contain both rational expectations terms and serially correlated disturbances, a setting where 2S2SLS is theoretically more efficient than NL2SLS. The problem also contrasts these limited-information estimates with full-system estimates, which are efficient under correct specification but vulnerable to misspecification anywhere in the system.\n\n### Data / Model Specification\n\nTaylor's model consists of an aggregate demand (output) equation and an aggregate supply (price-setting) equation:\n\n```latex\ny_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\beta_3(m_t - p_t) + \\beta_4(m_{t-1} - p_{t-1}) + \\beta_5(_{t-1}\\pi_t) + \\beta_0 + u_t \n```\n```latex\n\\pi_t = \\pi_{t-1} + \\gamma_1(_{t-1}y_t) + \\gamma_0 + v_t \n```\nwhere `y_t` is the output gap, `π_t` is the inflation rate, `(m_t - p_t)` are log real money balances, `_{t-1}π_t` is the rational expectation of inflation, and `_{t-1}y_t` is the rational expectation of the output gap. The structural disturbances `u_t` and `v_t` are known to be serially correlated.\n\nTable 1 presents estimation results for the period 1954:I - 1975:I from three methods: 2S2SLS, NL2SLS, and Taylor's full-system method.\n\n**Table 1: Limited-information and Full-system Estimates of Taylor's Model**\n\n| | **Output Equation** | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Estimator** | `y_{t-1}` (`β_1`) | `y_{t-2}` | `(m_t-p_t)` | `(m_{t-1}-p_{t-1})` | `_{t-1}π_t` |\n| 2S2SLS | 1.354 (0.108) | -0.484 (0.093) | 0.620 (0.217) | -0.541 (0.203) | -0.164 (0.133) |\n| NL2SLS | 1.423 (0.166) | -0.552 (0.185) | 0.599 (0.230) | -0.532 (0.217) | -0.041 (0.188) |\n| Taylor | 1.167 (0.088) | -0.324 (0.090) | 0.578 (0.175) | -0.484 (0.194) | -0.447 (0.391) |\n| | **Price Equation** | | | | |\n| **Estimator** | `π_{t-1}` | `_{t-1}y_t` (`γ_1`) | `Constant` | | |\n| 2S2SLS | 1.0 (fixed) | 0.0164 (0.0070) | 0.398 (0.252) | | |\n| NL2SLS | 1.0 (fixed) | 0.0132 (0.0109) | 0.304 (0.340) | | |\n| Taylor | 1.0 (fixed) | 0.0180 (0.0058) | 0.515 (0.172) | | |\n\n*Standard errors are in parentheses.* \n\nUsing the 2S2SLS estimates for the output equation from Table 1, test the null hypothesis that the coefficient on the first lag of output (`β_1`) is equal to 1 (`H_0: β_1 = 1`). At the 5% significance level (two-sided critical value ≈ 1.96), what is the resulting t-statistic and conclusion?",
    "Options": {
      "A": "The t-statistic is approximately 3.28, leading to a rejection of the null hypothesis.",
      "B": "The t-statistic is approximately 2.55, leading to a rejection of the null hypothesis.",
      "C": "The t-statistic is approximately 1.23, leading to a failure to reject the null hypothesis.",
      "D": "The t-statistic is approximately 12.53, leading to a rejection of the null hypothesis."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform and interpret a standard hypothesis test using table data, a core skill in empirical analysis. It uses a Computational Judgment strategy. The correct option (A) requires identifying the correct estimator's results, applying the t-statistic formula, and comparing to the critical value. Distractor (B) uses data from the less efficient NL2SLS estimator, testing attention to detail. Distractor (C) presents a plausible but incorrect calculation that leads to the wrong conclusion. Distractor (D) is based on the common error of testing against a null of zero instead of the specified null of one.",
    "qid": "352",
    "model_evaluation_result": {
      "model_raw_response": "B) The result is not shown in the table, so I will calculate it manually. The estimated value of `β_1` is 1.35 with a standard error of 0.108. The t-statistics is therefore `1.35/0.108 = 12.50`, which is greater than the critical value of 1.960 at the 5%-significance level, leading to the rejection of the hypothesis.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 353,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the empirical gains from using an asymptotically efficient estimator (2S2SLS) versus a consistent but inefficient one (NL2SLS) in a structural macroeconomic model. It further explores how estimator choice can impact policy conclusions and how to test for systemic model misspecification.\n\n**Setting / Institutional Environment.** The analysis uses limited-information (single-equation) instrumental variable methods to estimate the parameters of Taylor's (1979) model of the U.S. economy. The model's structural equations contain both rational expectations terms and serially correlated disturbances, a setting where 2S2SLS is theoretically more efficient than NL2SLS. The problem also contrasts these limited-information estimates with full-system estimates, which are efficient under correct specification but vulnerable to misspecification anywhere in the system.\n\n### Data / Model Specification\n\nTaylor's model consists of an aggregate demand (output) equation and an aggregate supply (price-setting) equation:\n\n```latex\ny_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\beta_3(m_t - p_t) + \\beta_4(m_{t-1} - p_{t-1}) + \\beta_5(_{t-1}\\pi_t) + \\beta_0 + u_t \n```\n```latex\n\\pi_t = \\pi_{t-1} + \\gamma_1(_{t-1}y_t) + \\gamma_0 + v_t \n```\nwhere `y_t` is the output gap, `π_t` is the inflation rate, `(m_t - p_t)` are log real money balances, `_{t-1}π_t` is the rational expectation of inflation, and `_{t-1}y_t` is the rational expectation of the output gap. The structural disturbances `u_t` and `v_t` are known to be serially correlated.\n\nTable 1 presents estimation results for the period 1954:I - 1975:I from three methods: 2S2SLS, NL2SLS, and Taylor's full-system method.\n\n**Table 1: Limited-information and Full-system Estimates of Taylor's Model**\n\n| | **Output Equation** | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Estimator** | `y_{t-1}` (`β_1`) | `y_{t-2}` | `(m_t-p_t)` | `(m_{t-1}-p_{t-1})` | `_{t-1}π_t` |\n| 2S2SLS | 1.354 (0.108) | -0.484 (0.093) | 0.620 (0.217) | -0.541 (0.203) | -0.164 (0.133) |\n| NL2SLS | 1.423 (0.166) | -0.552 (0.185) | 0.599 (0.230) | -0.532 (0.217) | -0.041 (0.188) |\n| Taylor | 1.167 (0.088) | -0.324 (0.090) | 0.578 (0.175) | -0.484 (0.194) | -0.447 (0.391) |\n| | **Price Equation** | | | | |\n| **Estimator** | `π_{t-1}` | `_{t-1}y_t` (`γ_1`) | `Constant` | | |\n| 2S2SLS | 1.0 (fixed) | 0.0164 (0.0070) | 0.398 (0.252) | | |\n| NL2SLS | 1.0 (fixed) | 0.0132 (0.0109) | 0.304 (0.340) | | |\n| Taylor | 1.0 (fixed) | 0.0180 (0.0058) | 0.515 (0.172) | | |\n\n*Standard errors are in parentheses.* \n\nA central question in macroeconomics is whether there is a short-run Phillips curve trade-off, represented by a non-zero coefficient `γ_1` in the price equation. Based on the results in Table 1, how does the conclusion about this trade-off at the 5% significance level change when moving from the NL2SLS estimator to the 2S2SLS estimator?",
    "Options": {
      "A": "The coefficient is statistically insignificant under NL2SLS but becomes statistically significant under 2S2SLS, demonstrating the practical importance of the efficiency gain.",
      "B": "The coefficient is statistically significant under both estimators, with 2S2SLS simply providing a more precise estimate.",
      "C": "The coefficient is statistically insignificant under both estimators, providing no evidence for a Phillips curve trade-off.",
      "D": "The coefficient is statistically significant under NL2SLS but becomes insignificant under 2S2SLS, suggesting the efficiency correction is problematic."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to synthesize statistical results to understand the paper's core argument about estimator efficiency having real-world inferential consequences. It uses a Scenario Application strategy. The correct option (A) requires comparing the significance of the same coefficient under two estimators and drawing the correct conclusion about the value of the 2S2SLS method. Distractors represent all other logical possibilities: (B) significant in both cases, (C) insignificant in both cases, and (D) the conceptual opposite where the more efficient estimator leads to insignificance.",
    "qid": "353",
    "model_evaluation_result": {
      "model_raw_response": "C) The answer is C) The coefficient `γ1` is statistically insignificant in both NL2S2LS and 2SLS2, so the conclusion about the short-run Phillips Curve trade-off remains unchanged.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 354,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the empirical gains from using an asymptotically efficient estimator (2S2SLS) versus a consistent but inefficient one (NL2SLS) in a structural macroeconomic model. It further explores how estimator choice can impact policy conclusions and how to test for systemic model misspecification.\n\n**Setting / Institutional Environment.** The analysis uses limited-information (single-equation) instrumental variable methods to estimate the parameters of Taylor's (1979) model of the U.S. economy. The model's structural equations contain both rational expectations terms and serially correlated disturbances, a setting where 2S2SLS is theoretically more efficient than NL2SLS. The problem also contrasts these limited-information estimates with full-system estimates, which are efficient under correct specification but vulnerable to misspecification anywhere in the system.\n\n### Data / Model Specification\n\nTaylor's model consists of an aggregate demand (output) equation and an aggregate supply (price-setting) equation:\n\n```latex\ny_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\beta_3(m_t - p_t) + \\beta_4(m_{t-1} - p_{t-1}) + \\beta_5(_{t-1}\\pi_t) + \\beta_0 + u_t \n```\n```latex\n\\pi_t = \\pi_{t-1} + \\gamma_1(_{t-1}y_t) + \\gamma_0 + v_t \n```\nwhere `y_t` is the output gap, `π_t` is the inflation rate, `(m_t - p_t)` are log real money balances, `_{t-1}π_t` is the rational expectation of inflation, and `_{t-1}y_t` is the rational expectation of the output gap. The structural disturbances `u_t` and `v_t` are known to be serially correlated.\n\nTable 1 presents estimation results for the period 1954:I - 1975:I from three methods: 2S2SLS, NL2SLS, and Taylor's full-system method.\n\n**Table 1: Limited-information and Full-system Estimates of Taylor's Model**\n\n| | **Output Equation** | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Estimator** | `y_{t-1}` (`β_1`) | `y_{t-2}` | `(m_t-p_t)` | `(m_{t-1}-p_{t-1})` | `_{t-1}π_t` |\n| 2S2SLS | 1.354 (0.108) | -0.484 (0.093) | 0.620 (0.217) | -0.541 (0.203) | -0.164 (0.133) |\n| NL2SLS | 1.423 (0.166) | -0.552 (0.185) | 0.599 (0.230) | -0.532 (0.217) | -0.041 (0.188) |\n| Taylor | 1.167 (0.088) | -0.324 (0.090) | 0.578 (0.175) | -0.484 (0.194) | -0.447 (0.391) |\n| | **Price Equation** | | | | |\n| **Estimator** | `π_{t-1}` | `_{t-1}y_t` (`γ_1`) | `Constant` | | |\n| 2S2SLS | 1.0 (fixed) | 0.0164 (0.0070) | 0.398 (0.252) | | |\n| NL2SLS | 1.0 (fixed) | 0.0132 (0.0109) | 0.304 (0.340) | | |\n| Taylor | 1.0 (fixed) | 0.0180 (0.0058) | 0.515 (0.172) | | |\n\n*Standard errors are in parentheses.* \n\nThe paper conducts a Hausman-type specification test by comparing the price equation estimates from 2S2SLS with those from the full-system method. The null hypothesis is that the full model is correctly specified. Given a test statistic of 0.40 and a 95% critical value of 5.99, what is the correct interpretation?",
    "Options": {
      "A": "The null hypothesis is not rejected, providing no evidence that the output equation (or other parts of the full system) is misspecified.",
      "B": "The null hypothesis is rejected, indicating that the output equation is likely misspecified.",
      "C": "The null hypothesis is not rejected, which proves that the full model is correctly specified.",
      "D": "The test is invalid because 2S2SLS is a limited-information estimator while the full-system estimator is a full-information estimator."
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to interpret the result of a specification test, a key diagnostic tool in econometrics. It uses a Computational Judgment strategy (simple comparison). The correct option (A) accurately states the outcome (fail to reject) and the correct, nuanced interpretation. Distractor (B) represents a failure to correctly compare the test statistic to the critical value. Distractor (C) represents a common conceptual error, equating 'failure to reject' with 'proof of the null'. Distractor (D) is a 'True but Irrelevant' statement that misinterprets the logic of the Hausman test.",
    "qid": "354",
    "model_evaluation_result": {
      "model_raw_response": "C) The test statistic is less than the critical value, so the null hypothesis is accepted, which means that there is no evidence to suggest that the full system is misspecified.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 355,
    "Question": "### Background\n\nThe theoretical test requires calculating the curvature of the true null manifold `S`, which is unknown because it depends on the true parameter `θ₀`. The baseline feasible procedure replaces this with the maximal curvature of the *observed* manifold `S* = {Σ⁻¹/²θ(β)}`.\n\n### Data / Model Specification\n\nThis baseline procedure calculates the maximal curvature over the entire observed manifold `S*`. While this ensures the test correctly controls size, it can lead to a loss of statistical power if `S*` has regions of high curvature that are far from the true parameter value.\n\n### The Question\n\nWhich of the following best describes the reason for this power loss and the logic of the paper's proposed modifications to address it?",
    "Options": {
      "A": "Power is lost if `S*` has regions of high curvature far from the true parameter value; the modifications address this by localizing the curvature calculation or isolating the specific parameters causing high curvature.",
      "B": "Power is lost because the baseline procedure systematically underestimates the relevant local curvature; the modifications correct this by using a more flexible optimization algorithm.",
      "C": "Power is lost because the procedure defaults to the conservative projection method's `χ²ₖ` bound; the modifications allow it to converge to the standard `χ²_{k-p}` bound.",
      "D": "Power is lost because the global optimization is computationally infeasible; the modifications' primary purpose is to make the test feasible by restricting the search space."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core rationale for the paper's practical modifications. It uses a Reverse-Reasoning strategy, asking the user to identify the cause (distant high curvature) of an outcome (low power). Distractors are designed to be plausible errors: B is a conceptual opposite (under vs. overestimation), C confuses the geometric test with the simpler projection method, and D misidentifies the core problem as feasibility rather than power.",
    "qid": "355",
    "model_evaluation_result": {
      "model_raw_response": "A) Power loss occurs if `S` has regions with high curvature far away from the true `θ₀`, and the modifications aim to address this by focusing on the relevant regions or isolating parameters causing high curvatures.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 356,
    "Question": "### Background\n\nA researcher is testing a DSGE model where the parameter vector `β` is partitioned into `β_J`, containing weakly identified preference parameters, and `β_{-J}`, containing well-identified technology parameters.\n\n### Data / Model Specification\n\nWeak identification is known to induce high manifold curvature. In this context, the high curvature is a global property of the manifold's geometry, not a localized 'kink' in a distant region. The researcher wishes to apply the paper's modified testing procedure.\n\n### The Question\n\nTo maximize the test's statistical power, what is the optimal strategy for choosing the parameter subset `J` for curvature calculation and the localization radius `R`?",
    "Options": {
      "A": "Set `J` to include only the weakly identified preference parameters and choose a large value for `R`.",
      "B": "Set `J` to include only the weakly identified preference parameters but choose a small value for `R` to exclude the high-curvature region.",
      "C": "Set `J` to include only the well-identified technology parameters to ensure a stable calculation and choose a large value for `R`.",
      "D": "Set `J` to include all parameters and choose a small value for `R` to reduce the computational burden of the optimization."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Scenario Application strategy to test the practical application of the paper's most advanced tools in a relevant economic context. The distractors are designed around high-fidelity misconceptions: B correctly identifies J but misapplies the logic for R in the context of global curvature; C makes the conceptual opposite error for J; D fails to apply either modification correctly, reflecting a superficial understanding of their purpose.",
    "qid": "356",
    "model_evaluation_result": {
      "model_raw_response": "C) Set J to include only well-identified parameters and choose large value for R.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 357,
    "Question": "### Background\n\n**Research Question.** The nutrition-based efficiency wage model (EWM) posits that employers pay wages above the market-clearing rate to ensure workers are well-nourished and productive, making it unprofitable to hire unemployed workers who offer to work for less. This paper empirically tests this core premise: is \"cheap labor\" truly \"dear labor\"?\n\n### Data / Model Specification\n\nThe central premise of the EWM is that an unemployed worker's offer will be rejected if the following condition holds:\n\n```latex\n\\frac{M}{E(C)} \\geq \\frac{W}{E(C^*)} \\quad \\text{(Eq. (1))}\n```\nThis inequality states that the cost per efficiency unit for a worker paid in meals (cost `M`, calories `C`) is greater than or equal to that of a currently employed worker (wage `W`, calories consumed `C*`). `E(·)` is the function mapping calorie consumption to worker productivity.\n\n**Case: Randam Village, Tamil Nadu, 1974**\n\n**Table 1: Daily wage rates of adult male casual labor**\n| Operation             | Wage rate                                    |\n| :-------------------- | :------------------------------------------- |\n| Plucking seedlings    | 2 rupees + meal + porridge made of rice and ragi |\n| Coolie (well-digging) | 2 rupees + meal + porridge made of rice and ragi |\n| First threshing       | 2.725-4.09 rupees                            |\n| Second threshing      | 8.18 rupees                                  |\n\n*   **Productivity Gain:** A worker consuming 5000 calories is **34.58%** more productive than one consuming 2601 calories.\n\n### Question\n\nConsider the \"Plucking seedlings\" operation from Table 1. An unemployed worker offers to work for a meal costing `M = 1` rupee, which provides `C = 2601` calories. The currently employed worker receives a total wage `W` equivalent to 3 rupees and is assumed to consume `C* = 5000` calories. Given this, which of the following statements correctly evaluates the EWM's condition for wage rigidity (Eq. (1))?",
    "Options": {
      "A": "The condition is not met because the cost per efficiency unit is lower for the undercutting worker (~1.00) than for the employed worker (~2.23), indicating that hiring the unemployed worker would be more profitable.",
      "B": "The condition is met because the cost per efficiency unit is higher for the undercutting worker (~1.00) than for the employed worker (~0.74), indicating that hiring the unemployed worker would be less profitable.",
      "C": "The condition is not met, but this implies the employer should keep the current, more productive worker because the cost difference is minimal.",
      "D": "The costs per efficiency unit are equal for both workers, making the employer indifferent and thus upholding wage rigidity."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to apply the core EWM inequality to a concrete scenario. It uses a Computational Judgment strategy. The student must perform a multi-step calculation (determine productivity levels, then costs per efficiency unit) and interpret the result in the context of the model's prediction. Distractor B represents a calculation error (e.g., inverting the cost formula) leading to the opposite conclusion. Distractor C represents a correct calculation but a flawed interpretation of what the inequality implies. Distractor D represents a different calculation error.",
    "qid": "357",
    "model_evaluation_result": {
      "model_raw_response": "B) Thecondition is met because thee cost per efficiencyunit is higher for thee undercutting worker(~1.00 )than for thee employed worker(~0.74 ),indicating that hiring thee unemployed worker would bee less profitable.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 358,
    "Question": "### Background\n\nThe nutrition-based efficiency wage model (EWM) predicts that employers will not hire lower-wage workers if doing so increases the cost per efficiency unit of labor. The condition for wage rigidity is:\n\n```latex\n\\frac{M}{E(C)} \\geq \\frac{W}{E(C^*)} \\quad \\text{(Eq. (1))}\n```\nwhere `M` and `C` are the cost and calories for an undercutting worker, and `W` and `C*` are the wage and calories for an employed worker.\n\n### Data / Model Specification\n\nIn a scenario from Tamil Nadu, an employed worker earns a wage `W=3` rupees and consumes `C*=5000` calories. An unemployed worker offers to work for a meal costing `M=1` rupee that provides `C=2601` calories. The employed worker is estimated to be 34.58% more productive, which is insufficient for the EWM condition to hold.\n\n### Question\n\nWhat is the *minimum* percentage productivity gain (`p`) a 5000-calorie worker must have over a 2601-calorie worker for the employer to be indifferent between them (i.e., for Eq. (1) to hold with equality)?",
    "Options": {
      "A": "100%",
      "B": "200%",
      "C": "300%",
      "D": "66.7%"
    },
    "Answer": "B",
    "pi_justification": "This item assesses higher-order quantitative reasoning by asking for a reverse-engineering calculation. It uses a Reverse-Reasoning strategy. The student must set up the EWM indifference condition (`1/1 = 3/(1+p)`) and solve for the unknown productivity gain `p`. The correct answer is `p=2`, or 200%. Distractor A (100%) results from an error where the wage is mistakenly assumed to be `W=2`. Distractor C (300%) results from misinterpreting `p` as the total productivity factor `1+p`. Distractor D (66.7%) could result from an inverted calculation like `(W-M)/W`.",
    "qid": "358",
    "model_evaluation_result": {
      "model_raw_response": "C) 66,7% \n\nNote: The calculation involves solving the equation for p, which is not provided here. However, I can guide you through it. First, we need to solve for M/E(C) = W/E(C*). Then, we can rearrange the equation to get p.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 359,
    "Question": "### Background\n\n**Research Question.** This problem requires you to analyze a re-examination of a field experiment that estimated consumer valuation for a new product packaging (vacuum skin) for beef steaks relative to a standard packaging (styrofoam). The analysis tests for the presence of two methodological confounds: field-price censoring and affiliated beliefs.\n\n**Setting / Institutional Environment.** In the original experiment, subjects submitted bids for beef steaks in a series of auctions. A key institutional detail is that subjects were explicitly told that the standard steak \"currently retail for about $6.00 a pound at major supermarkets.\" The experiment included three information treatments regarding the new vacuum-packed product: a baseline with no extra information, an \"Info\" treatment describing its virtues, and a \"Demo\" treatment that included a demonstration. After each of the later auction periods, the market-clearing price was announced to all participants.\n\n**Variables & Parameters.**\n*   `Bid`: Subject's bid in dollars per pound (dependent variable).\n*   `Vacuum`: Indicator variable; 1 if the product is the new vacuum skin package, 0 for standard styrofoam.\n*   `Info`: Indicator variable; 1 if subjects received the \"information\" treatment.\n*   `Demo`: Indicator variable; 1 if subjects received the \"demonstration\" treatment.\n*   `VacInfo`: Interaction term `Vacuum` × `Info`.\n*   `VacDemo`: Interaction term `Vacuum` × `Demo`.\n*   `priceL1`: The market-clearing (selling) price from the previous auction period, `t-1`.\n\n### Data / Model Specification\n\nTable 1 and Table 2 below present results from the re-analysis. Panel A of Table 1 uses a Panel Generalized Least Squares (GLS) model, which ignores censoring and affiliation effects. Panel C of Table 1 uses a Panel Tobit model, which accounts for the censoring of bids at the known $6.00 field price and includes controls for affiliation effects (market prices). The Panel Tobit model is considered the most appropriate specification.\n\n**Table 1. Estimated effects of experimental treatments on bids for meat**\n(Estimates obtained using random effects specification)\n\n| Treatment | Coef. | Std. err. | Z | P>|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| **A. Panel GLS** | | | | | | |\n| Vacuum | -.0549387 | .0333573 | -1.65 | 0.100 | -.1203179 | .0104404 |\n| Info | .1824116 | .0987915 | 1.85 | 0.065 | -.0112161 | .3760393 |\n| VacInfo | .449179 | .0470355 | 9.55 | 0.000 | .3569912 | .5413669 |\n| VacDemo | .0739316 | .0468498 | 1.58 | 0.115 | -.0178924 | .1657555 |\n| **C. Panel Tobit Model with market price effects** | | | | | | |\n| Vacuum | -.0440431 | .0346063 | -1.27 | 0.203 | -.1118703 | .023784 |\n| Info | -.2454266 | .0623487 | -3.94 | 0.000 | -.3676278 | -.1232254 |\n| VacInfo | .4198155 | .0488044 | 8.60 | 0.000 | .3241607 | .5154703 |\n| VacDemo | .0686025 | .0484393 | 1.42 | 0.157 | -.0263367 | .1635418 |\n\n**Table 2. Estimated effects of market prices on bids for meat**\n(Estimates obtained using Panel Tobit specification with random effects)\n\n| Variable | Coef. | Std. err. | Z | P >|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| tbid_vs | .3996299 | .0247609 | 16.14 | 0.000 | .3510994 | .4481605 |\n| tbid-st | .2883476 | .0249211 | 11.57 | 0.000 | .2395032 | .337192 |\n| priceL1 | .3555019 | .0172282 | 20.63 | 0.000 | .3217353 | .3892686 |\n\nUsing the results from the preferred specification in Panel C of Table 1, what is the estimated net marginal valuation of the vacuum skin packaging (relative to styrofoam) when the 'Info' treatment is provided?",
    "Options": {
      "A": "It is valued approximately $0.38 more per pound.",
      "B": "It is valued approximately $0.42 more per pound.",
      "C": "It is valued approximately $0.39 more per pound.",
      "D": "It is valued approximately $0.25 less per pound."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the ability to correctly calculate and interpret the net effect of a treatment that includes an interaction term, a critical skill in reading regression tables. It uses a Computational Judgment strategy. Distractors are designed based on common errors: ignoring the main effect (B), using coefficients from an inappropriate model specification (C), and misapplying an unrelated coefficient (D).",
    "qid": "359",
    "model_evaluation_result": {
      "model_raw_response": "C) It's valued approximately $ 0.42 per pound. \n\nNote: The relevant coefficient is VacInfo in Panel C, Table 1.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 360,
    "Question": "### Background\n\n**Research Question.** This problem requires you to analyze a re-examination of a field experiment that estimated consumer valuation for a new product packaging (vacuum skin) for beef steaks relative to a standard packaging (styrofoam). The analysis tests for the presence of two methodological confounds: field-price censoring and affiliated beliefs.\n\n**Setting / Institutional Environment.** In the original experiment, subjects submitted bids for beef steaks in a series of auctions. A key institutional detail is that subjects were explicitly told that the standard steak \"currently retail for about $6.00 a pound at major supermarkets.\" The experiment included three information treatments regarding the new vacuum-packed product: a baseline with no extra information, an \"Info\" treatment describing its virtues, and a \"Demo\" treatment that included a demonstration. After each of the later auction periods, the market-clearing price was announced to all participants.\n\n**Variables & Parameters.**\n*   `Bid`: Subject's bid in dollars per pound (dependent variable).\n*   `Vacuum`: Indicator variable; 1 if the product is the new vacuum skin package, 0 for standard styrofoam.\n*   `Info`: Indicator variable; 1 if subjects received the \"information\" treatment.\n*   `Demo`: Indicator variable; 1 if subjects received the \"demonstration\" treatment.\n*   `VacInfo`: Interaction term `Vacuum` × `Info`.\n*   `VacDemo`: Interaction term `Vacuum` × `Demo`.\n*   `priceL1`: The market-clearing (selling) price from the previous auction period, `t-1`.\n\n### Data / Model Specification\n\nTable 1 and Table 2 below present results from the re-analysis. Panel A of Table 1 uses a Panel Generalized Least Squares (GLS) model, which ignores censoring and affiliation effects. Panel C of Table 1 uses a Panel Tobit model, which accounts for the censoring of bids at the known $6.00 field price and includes controls for affiliation effects (market prices). The Panel Tobit model is considered the most appropriate specification.\n\n**Table 1. Estimated effects of experimental treatments on bids for meat**\n(Estimates obtained using random effects specification)\n\n| Treatment | Coef. | Std. err. | Z | P>|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| **A. Panel GLS** | | | | | | |\n| Vacuum | -.0549387 | .0333573 | -1.65 | 0.100 | -.1203179 | .0104404 |\n| Info | .1824116 | .0987915 | 1.85 | 0.065 | -.0112161 | .3760393 |\n| VacInfo | .449179 | .0470355 | 9.55 | 0.000 | .3569912 | .5413669 |\n| VacDemo | .0739316 | .0468498 | 1.58 | 0.115 | -.0178924 | .1657555 |\n| **C. Panel Tobit Model with market price effects** | | | | | | |\n| Vacuum | -.0440431 | .0346063 | -1.27 | 0.203 | -.1118703 | .023784 |\n| Info | -.2454266 | .0623487 | -3.94 | 0.000 | -.3676278 | -.1232254 |\n| VacInfo | .4198155 | .0488044 | 8.60 | 0.000 | .3241607 | .5154703 |\n| VacDemo | .0686025 | .0484393 | 1.42 | 0.157 | -.0263367 | .1635418 |\n\n**Table 2. Estimated effects of market prices on bids for meat**\n(Estimates obtained using Panel Tobit specification with random effects)\n\n| Variable | Coef. | Std. err. | Z | P >|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| tbid_vs | .3996299 | .0247609 | 16.14 | 0.000 | .3510994 | .4481605 |\n| tbid-st | .2883476 | .0249211 | 11.57 | 0.000 | .2395032 | .337192 |\n| priceL1 | .3555019 | .0172282 | 20.63 | 0.000 | .3217353 | .3892686 |\n\nAccording to the results in Table 2, what is the most accurate interpretation of the coefficient on `priceL1`?",
    "Options": {
      "A": "A $1 increase in the previous auction's selling price is associated with a statistically significant increase of about $0.36 in a subject's current bid, supporting the hypothesis of affiliated beliefs.",
      "B": "The previous auction's selling price has no statistically significant effect on current bids, suggesting bids are independent across rounds.",
      "C": "Subjects' bids converge to the true market price of $6.00 over time, as shown by the positive coefficient.",
      "D": "The model likely suffers from multicollinearity because the lagged price is correlated with the average trial bids."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the user's ability to interpret a coefficient that serves as a direct test of a key theoretical concept (affiliated beliefs). It uses a Conceptual Judgment strategy. Distractors include the conceptual opposite (B), a plausible but overly strong interpretation of the mechanism (C), and a true but irrelevant statistical artifact (D).",
    "qid": "360",
    "model_evaluation_result": {
      "model_raw_response": "B) Theprevious auction's sellingprice has no statisticallysignificant effect on currentbids, suggesting bidsare independent acrossrounds.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 361,
    "Question": "### Background\n\n**Research Question.** This problem requires you to analyze a re-examination of a field experiment that estimated consumer valuation for a new product packaging (vacuum skin) for beef steaks relative to a standard packaging (styrofoam). The analysis tests for the presence of two methodological confounds: field-price censoring and affiliated beliefs.\n\n**Setting / Institutional Environment.** In the original experiment, subjects submitted bids for beef steaks in a series of auctions. A key institutional detail is that subjects were explicitly told that the standard steak \"currently retail for about $6.00 a pound at major supermarkets.\" The experiment included three information treatments regarding the new vacuum-packed product: a baseline with no extra information, an \"Info\" treatment describing its virtues, and a \"Demo\" treatment that included a demonstration. After each of the later auction periods, the market-clearing price was announced to all participants.\n\n**Variables & Parameters.**\n*   `Bid`: Subject's bid in dollars per pound (dependent variable).\n*   `Vacuum`: Indicator variable; 1 if the product is the new vacuum skin package, 0 for standard styrofoam.\n*   `Info`: Indicator variable; 1 if subjects received the \"information\" treatment.\n*   `Demo`: Indicator variable; 1 if subjects received the \"demonstration\" treatment.\n*   `VacInfo`: Interaction term `Vacuum` × `Info`.\n*   `VacDemo`: Interaction term `Vacuum` × `Demo`.\n*   `priceL1`: The market-clearing (selling) price from the previous auction period, `t-1`.\n\n### Data / Model Specification\n\nTable 1 below presents results from the re-analysis. Panel A uses a Panel Generalized Least Squares (GLS) model, which ignores censoring and affiliation effects. Panel C uses a Panel Tobit model, which accounts for the censoring of bids at the known $6.00 field price and includes controls for affiliation effects (market prices). The Panel Tobit model is considered the most appropriate specification.\n\n**Table 1. Estimated effects of experimental treatments on bids for meat**\n(Estimates obtained using random effects specification)\n\n| Treatment | Coef. | Std. err. | Z | P>|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| **A. Panel GLS** | | | | | | |\n| Vacuum | -.0549387 | .0333573 | -1.65 | 0.100 | -.1203179 | .0104404 |\n| Info | .1824116 | .0987915 | 1.85 | 0.065 | -.0112161 | .3760393 |\n| VacInfo | .449179 | .0470355 | 9.55 | 0.000 | .3569912 | .5413669 |\n| VacDemo | .0739316 | .0468498 | 1.58 | 0.115 | -.0178924 | .1657555 |\n| **C. Panel Tobit Model with market price effects** | | | | | | |\n| Vacuum | -.0440431 | .0346063 | -1.27 | 0.203 | -.1118703 | .023784 |\n| Info | -.2454266 | .0623487 | -3.94 | 0.000 | -.3676278 | -.1232254 |\n| VacInfo | .4198155 | .0488044 | 8.60 | 0.000 | .3241607 | .5154703 |\n| VacDemo | .0686025 | .0484393 | 1.42 | 0.157 | -.0263367 | .1635418 |\n\nThe re-analysis prefers the Panel Tobit model (Panel C) over the Panel GLS model (Panel A). What feature of the experimental setting provides the strongest rationale for this choice?",
    "Options": {
      "A": "Subjects were explicitly told that the steaks retailed for about $6.00, creating an upper bound that censors bids.",
      "B": "The data is a panel dataset with repeated observations from the same subjects.",
      "C": "The dependent variable (bids) is not normally distributed, as seen in histograms of the data.",
      "D": "Subjects participated in both trial and binding auction periods."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the ability to connect a specific feature of the experimental design to the choice of an appropriate econometric model. It uses a Reverse-Reasoning strategy. Distractors are designed to be true statements about the experiment that are not the direct cause for choosing Tobit over GLS (B, D) or a related statistical symptom rather than the root economic cause (C).",
    "qid": "361",
    "model_evaluation_result": {
      "model_raw_response": "A) A) Subjects were explicitl...y told that the...steaks retailed...$6.00...",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 362,
    "Question": "### Background\n\n**Research Question.** This problem explores three competing theoretical models of social learning in a continuous action space: the rational benchmark (Perfect Bayesian Equilibrium), a model of information redundancy neglect (Best Response Trailing Naïve Inference), and a model of relative overconfidence.\n\n**Setting / Institutional Environment.** A sequence of agents `t=1, 2, ..., T` make decisions. Agent `t` observes the actions of all `t-1` predecessors and receives a private binary signal `s_t ∈ {0, 1}` with precision `q_t > 0.5` about a true state `V ∈ {0, 100}`. The agent's action `a_t ∈ [0, 100]` is their belief that `V=100`.\n\n### Data / Model Specification\n\nThe models make distinct predictions about an agent's posterior belief, expressed as a log-odds ratio, `ln(a_t / (100-a_t))`. Let `L_i = (2s_i - 1)ln(q_i / (1-q_i))` be the log-likelihood ratio of signal `s_i`.\n\n1.  **Perfect Bayesian Equilibrium (PBE):** Agents are fully rational and perfectly infer all past signals from past actions. The belief is the sum of all log-likelihood ratios up to time `t`.\n    ```latex\n    \\ln\\left(\\frac{a_{t}^{PBE}}{100-a_{t}^{PBE}}\\right) = \\sum_{i=1}^{t} L_i\n    ```\n\n2.  **Best Response Trailing Naïve Inference (BRTNI):** Agents naively believe predecessors act only on their private signal, leading to redundant information being repeatedly counted. The weight on a predecessor's signal `s_i` is `2^{t-i-1}`.\n    ```latex\n    \\ln\\left(\\frac{a_{t}^{BRTNI}}{100-a_{t}^{BRTNI}}\\right) = \\sum_{i=1}^{t-1} 2^{t-i-1} L_i + L_t\n    ```\n\n3.  **Overconfidence (OC):** Agents believe predecessors are less capable. They discount the inferred signals of all predecessors by a constant factor `k ∈ (0, 1)` but use their own signal correctly. This belief is common knowledge.\n    ```latex\n    \\ln\\left(\\frac{a_{t}^{OC}}{100-a_{t}^{OC}}\\right) = \\sum_{i=1}^{t-1} k L_i + L_t\n    ```\n\nConsider an agent at `t=3` where signal precision is constant at `q=0.7`. The realized sequence of signals is `{s_1=0, s_2=1, s_3=1}`. For the OC model, assume `k=0.5`. Which of the following correctly describes the agent's final log-odds belief (`Belief`) across the three models?",
    "Options": {
      "A": "`Belief_PBE` = `Belief_OC` > `Belief_BRTNI`",
      "B": "`Belief_PBE` > `Belief_OC` > `Belief_BRTNI`",
      "C": "`Belief_BRTNI` > `Belief_PBE` > `Belief_OC`",
      "D": "`Belief_PBE` > `Belief_BRTNI` > `Belief_OC`"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to quantitatively apply the three core theoretical models. It uses the 'Computational Judgment' strategy, requiring students to perform multi-step calculations for each model and then compare the results. The scenario `{s_1=0, s_2=1, s_3=1}` is chosen specifically because it yields a non-obvious ranking (`PBE=OC > BRTNI`), which prevents simply guessing based on qualitative intuition. Distractor C is based on a different signal sequence, trapping students who pattern-match without recalculating. Distractor B reflects a plausible but incorrect qualitative intuition about the models.",
    "qid": "362",
    "model_evaluation_result": {
      "model_raw_response": "D) ` Белief_PBE > Belief_BRTNi > Belief_OC` \n\nNote: The question is asking for the order of magnitude of the log-oddds beliefs across the three competing models.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 363,
    "Question": "## Background\n\n**Research Question.** This problem investigates the causal relationship between sectoral reallocation shocks, measured by Cross-Section Volatility (CSV), and the aggregate unemployment rate (UN). A key challenge is potential omitted variable bias from aggregate economic forces like the stock market return (MRET).\n\n## Data / Model Specification\n\nAssume the true model for unemployment is:\n```latex\nU_t = \\alpha_0 + \\alpha_1 CSV_t + \\alpha_2 MRET_t + u_t\n```\nwhere theory suggests \\(\\alpha_1 > 0\\) (reallocation increases unemployment) and \\(\\alpha_2 < 0\\) (a booming market reduces unemployment).\n\nInstead, a researcher estimates the simple model:\n```latex\nU_t = \\beta_0 + \\beta_1 CSV_t + \\epsilon_t\n```\n\n**Table 1. Selected Correlations**\n\n| | CSV | MRET | UN |\n| :--- | :--- | :--- | :--- |\n| **CSV** | 1.000 | | |\n| **MRET**| -0.400| 1.000 | |\n| **UN** | 0.216 | -0.324| 1.000 |\n\n## Question\n\nGiven the information above, what is the direction of the omitted variable bias for the estimator \\(\\hat{\\beta}_1\\) in the simple model? In other words, does omitting MRET lead to an over- or under-estimation of the true effect of CSV on unemployment?",
    "Options": {
      "A": "The effect is overestimated because the omitted variable (MRET) is negatively correlated with the included variable (CSV) and negatively correlated with the outcome (UN).",
      "B": "The effect is underestimated because the omitted variable (MRET) is negatively correlated with the included variable (CSV).",
      "C": "The effect is underestimated because the omitted variable (MRET) is negatively correlated with the outcome (UN).",
      "D": "The effect is unbiased because the correlation between CSV and MRET is not statistically significant at conventional levels."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the ability to formally derive the direction of omitted variable bias, a core econometrics skill. Strategy: Computational Judgment. Distractor Logic: The correct answer (A) requires correctly identifying the two components of bias (corr(X,Z) and corr(Z,Y)) and their signs. Distractor (B) makes an error by only considering the first component. Distractor (C) makes an error by only considering the second component. Distractor (D) offers a plausible but incorrect conceptual escape hatch.",
    "qid": "363",
    "model_evaluation_result": {
      "model_raw_response": "C) Theeffect is underestimated because them omitted variable (mret) is negativelycorrelated with the outcome(un).",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 364,
    "Question": "## Background\n\n**Research Question.** This problem investigates the dynamic causal relationship between sectoral reallocation shocks (CSV) and the aggregate unemployment rate (\\(U_t\\)).\n\n## Data / Model Specification\n\nTwo specifications are used to model the unemployment rate:\n\n**Level Specification:**\n```latex\nU_{t} = \\beta_{0} + \\sum_{k=0}^{15} \\beta_{1k} CSV_{t-k} + \\epsilon_{t} \\quad \\text{(Eq. (1))}\n```\n**Lag Specification (Autoregressive Distributed Lag):**\n```latex\nU_{t} = \\beta_{0} + \\sum_{k=0}^{7} \\beta_{1k} CSV_{t-k} + \\sum_{k=1}^{4} \\rho_{k} U_{t-k} + \\epsilon_{t} \\quad \\text{(Eq. (2))}\n```\n\n## Question\n\nWhat is the primary econometric purpose of including the lagged unemployment terms (\\(\\sum \\rho_{k} U_{t-k}\\)) in Eq. (2), and how does this change the interpretation of the \\(\\beta_{1k}\\) coefficients compared to Eq. (1)?",
    "Options": {
      "A": "To account for serial correlation in unemployment; this changes the \\(\\beta_{1k}\\) to represent the direct impact of a past CSV shock, distinct from the shock's propagation through unemployment's own persistence.",
      "B": "To ensure the model is stationary; this makes the \\(\\beta_{1k}\\) coefficients represent the total long-run effect of a CSV shock on unemployment.",
      "C": "To control for omitted variables that are correlated with past unemployment; this makes the \\(\\beta_{1k}\\) coefficients unbiased estimates of the immediate, contemporaneous effect of CSV.",
      "D": "To allow for hysteresis in unemployment; this ensures that the \\(\\beta_{1k}\\) coefficients capture only the permanent effects of CSV shocks."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Assesses understanding of dynamic time-series model specification (ARDL models). Strategy: Conceptual Judgment. Distractor Logic: The correct answer (A) correctly identifies both the purpose (serial correlation) and the consequence (interpretation of coefficients as direct effects). Distractor (B) confuses ARDL with differencing for stationarity and incorrectly claims the coefficients are total long-run effects. Distractor (C) correctly identifies a potential benefit (OVB control) but misinterprets the coefficients as only contemporaneous. Distractor (D) introduces a plausible but distinct economic concept (hysteresis) and misinterprets the coefficients' meaning.",
    "qid": "364",
    "model_evaluation_result": {
      "model_raw_response": "A) Toaccount for serial correlationinunemployment;this changestheβ1kto representthedirectimpactofapastCSVshock,distinctfromtheshock'spropagationthroughunemployment'sownpersistence.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 365,
    "Question": "## Background\n\n**Research Question.** This problem assesses whether industry-level stock market excess returns (\\(\\eta_{j,t}\\)) can predict real labor market adjustments, specifically future excess employment growth (\\(\\hat{\\xi}_{j,t}^{q}\\)).\n\n## Data / Model Specification\n\nAn interactive model is estimated to test if the effect of current excess returns depends on the persistence of past returns:\n```latex\n\\hat{\\xi}_{j,t}^{q} = \\tau_{q} + [\\delta_{1q} + \\delta_{2q} |\\sum_{k=0}^{7} \\eta_{j,t-k}|] \\eta_{j,t} + \\nu_{j,t}^{q} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1. Regression Estimates for 2-Year Employment Growth (q=8)**\n\n| Coefficient | Estimate | Std. Error |\n| :--- | :--- | :--- |\n| \\(\\hat{\\delta}_{1,8}\\) | 0.0357 | (0.0237) |\n| \\(\\hat{\\delta}_{2,8}\\) | 1.208 | (0.475) |\n\n## Question\n\nConsider two industries. The 'Utility' industry has experienced a string of positive shocks totaling 20% over the prior two years (\\(\\sum_{k=0}^{7} \\eta_{U,t-k} = 0.20\\)). The 'Tech' industry has experienced volatile but cumulatively zero excess returns over the same period (\\(\\sum_{k=0}^{7} \\eta_{T,t-k} = 0\\)). Both industries now experience an identical positive 10% excess return shock (\\(\\eta_{j,t}=0.10\\)).\n\nUsing the model and estimates provided, what is the predicted 2-year excess employment growth for the **Utility** industry?",
    "Options": {
      "A": "2.77%",
      "B": "0.36%",
      "C": "2.42%",
      "D": "0.04%"
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the ability to apply a non-linear model to a specific scenario, requiring careful reading, selection of correct coefficients, and accurate calculation. Strategy: Scenario Application / Computational Judgment. Distractor Logic: The correct answer (A) is from the full calculation: `[0.0357 + 1.208 * 0.20] * 0.10 = 0.0277`. Distractor (B) is the correct answer for the *Tech* industry (`[0.0357 + 1.208 * 0] * 0.10 = 0.0036`), catching students who apply the model to the wrong case. Distractor (C) results from a calculation error, specifically multiplying the second term by `eta_j,t` twice (`0.0357*0.1 + 1.208*0.2*0.1 = 0.0277`). No, let's find a better one. Distractor (C) could be `1.208 * 0.20 = 0.2416`, forgetting to multiply by the final `eta_j,t`. Let's recalculate. A better distractor is (C) `2.42%`, which is the value of the interaction term alone (`1.208 * 0.20 * 0.10 = 0.02416`). Distractor (D) `0.04%` is the value of the base term alone (`0.0357 * 0.10 = 0.000357`), which is almost the same as the Tech industry answer.",
    "qid": "365",
    "model_evaluation_result": {
      "model_raw_response": "C) 0,36% \n\nNote: Please note that I am providing the answer based solely on the information provided in the question and the table.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 366,
    "Question": "### Background\n\n**Research Question.** This problem demonstrates the theoretical equivalence of the Learning-to-Forecast (LtF) and Learning-to-Optimise (LtO) experimental treatments under the benchmark assumption of perfect agent rationality.\n\n**Setting / Institutional Environment.** We compare two market structures. In the LtO treatment, $I=6$ agents choose their asset quantities $z_{i,t}$ directly. In the LtF treatment, agents submit price forecasts $p_{i,t+1}^e$, which are then converted into asset demands by a computer. The underlying microeconomic model of agent preferences and market clearing is identical in both settings.\n\n**Variables & Parameters.**\n- `$p_t$`: Price of the asset in period $t$.\n- `$p_{i,t+1}^e$`: Agent $i$'s price forecast for period $t+1$.\n- `$\\bar{p}_{t+1}^e$`: Average price forecast across all $I$ agents.\n- `$z_{i,t}$`: Quantity of the asset demanded by agent $i$ in period $t$.\n- `$y$`: Constant dividend; $y=3.3$.\n- `$R$`: Gross risk-free rate; $R=1.05$.\n- `$a\\sigma^2$`: Composite parameter for risk aversion and perceived variance; $a\\sigma^2=6$.\n- `$\\lambda$`: Price adjustment parameter; $\\lambda=20/21$.\n- `$I$`: Number of agents; $I=6$.\n\n---\n\n### Data / Model Specification\n\nThe price adjustment rule in the LtO market, where agents submit quantities, is:\n```latex\np_{t+1} = p_t + \\lambda \\sum_{i=1}^I z_{i,t} + \\varepsilon_t \\quad \\text{(Eq. 1)}\n```\nThe optimal demand for a rational, price-taking agent with mean-variance preferences is:\n```latex\nz_{i,t}^* = \\frac{p_{i,t+1}^e + y - R p_t}{a\\sigma^2} \\quad \\text{(Eq. 2)}\n```\n\n---\n\n### The Question\n\nAssume all agents in the LtO market are rational optimizers. By substituting the optimal demand function (Eq. 2) into the price adjustment rule (Eq. 1), aggregating across all $I=6$ agents, and applying the given parameter values, what is the correct resulting law of motion for the price $p_{t+1}$?",
    "Options": {
      "A": "```latex\np_{t+1} = \\frac{20}{21} \\bar{p}_{t+1}^e + \\frac{66}{21} + \\varepsilon_t\n```",
      "B": "```latex\np_{t+1} = \\frac{1}{21}p_t + \\frac{20}{21} \\bar{p}_{t+1}^e + \\frac{66}{21} + \\varepsilon_t\n```",
      "C": "```latex\np_{t+1} = \\frac{5}{6}p_t + \\frac{10}{63} \\bar{p}_{t+1}^e + \\frac{33}{63} + \\varepsilon_t\n```",
      "D": "```latex\np_{t+1} = -5p_t + \\frac{120}{21} \\bar{p}_{t+1}^e + 19.8 + \\varepsilon_t\n```"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a multi-step algebraic derivation that is central to the paper's theoretical setup. It uses a Computational Judgment strategy. The correct answer requires substituting one equation into another, aggregating across agents, and correctly applying parameter values, including the key insight that the $p_t$ term cancels out. The distractors are designed based on plausible error paths: (B) results from a calculation error where the $p_t$ term fails to cancel; (C) stems from an aggregation error where the number of agents `I` is ignored; (D) arises from omitting the risk-aversion parameter `a*sigma^2` during substitution.",
    "qid": "366",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: The correct answer is option C) `p_{t + 1} = 5/6 p_t + 10/63 \\bar{`",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 367,
    "Question": "### Background\n\n**Research Question.** This problem analyzes a monopolist's endogenous choice of contractual liability (`w`) and its interaction with product safety (`\\pi`), demonstrating how liability can serve as a tool for price discrimination and identifying when this private contracting fails, necessitating legal intervention.\n\n**Setting.** A monopolist can offer a product via a simple contract `(\\pi, p, w)`, where the stipulated damage payment `w` is a choice variable for the firm. The firm's optimal strategy and the resulting market efficiency depend on how consumer risk (`x`) correlates with their gross benefit, a relationship captured by the parameter `b`.\n\n### Data / Model Specification\n\n**Model Components.**\n- Consumer type `x` is distributed on `[\\underline{x}, \\overline{x}]`.\n- A type-`x` consumer's gross benefit is `b_0 + bx`.\n- The probability of an accident is `\\pi x`, and the harm is `h`.\n- The socially optimal safety level `\\pi^{**}` is defined by `W'(\\pi^{**}) = 0`.\n- The threshold `b^{**}` is defined as `b^{**} = \\pi^{**}h`.\n\n### Question\n\nIn the case where `b > b^{**}`, the firm's private incentives diverge from the social optimum, leading to a market failure. Which of the following best describes the firm's profit-maximizing strategy and the underlying economic reason?",
    "Options": {
      "A": "The firm sets `w^*=0` and underinvests in safety (`\\pi > \\pi^{**}`) because a positive `w` would subsidize high-valuation inframarginal consumers, increasing the rents the firm seeks to extract.",
      "B": "The firm sets `w^*=h` and invests efficiently in safety (`\\pi = \\pi^{**}`) because this maximizes the total social surplus that the firm can capture.",
      "C": "The firm sets `w^*=0` and underinvests in safety (`\\pi > \\pi^{**}`) because the high cost of producing safer products makes it unprofitable to meet the social optimum.",
      "D": "The firm sets `w^* > 0` and overinvests in safety (`\\pi < \\pi^{**}`) because it needs to cater to the high-risk marginal consumer to maintain full market coverage."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's understanding of the paper's central market failure result. It uses a Reverse-Reasoning strategy: given the condition for market failure (`b > b^{**}`), the student must identify the correct firm strategy and its underlying cause. Distractor (B) is the policy solution, not the firm's choice. Distractor (C) offers a naive 'True but Irrelevant' reason, ignoring the core rent-extraction motive. Distractor (D) is a Conceptual Mix-up, applying the logic from the `b < b^{**}` case.",
    "qid": "367",
    "model_evaluation_result": {
      "model_raw_response": "A) The correct answer is A. The firm sets w^*=0 and underinvestes in safety (π > π**) because a positive w would subsidize...",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 368,
    "Question": "### Background\n\n**Research Question.** This problem analyzes a monopolist's endogenous choice of contractual liability (`w`) and its interaction with product safety (`\\pi`), demonstrating how liability can serve as a tool for price discrimination and identifying when this private contracting fails, necessitating legal intervention.\n\n**Setting.** A monopolist can offer a product via a simple contract `(\\pi, p, w)`, where the stipulated damage payment `w` is a choice variable for the firm. The firm's optimal strategy and the resulting market efficiency depend on how consumer risk (`x`) correlates with their gross benefit, a relationship captured by the parameter `b`.\n\n### Data / Model Specification\n\n**Model Components.**\n- Consumer type `x` is distributed on `[\\underline{x}, \\overline{x}]`.\n- A type-`x` consumer's gross benefit is `b_0 + bx`.\n- The probability of an accident is `\\pi x`, and the harm is `h`.\n- The socially optimal safety level `\\pi^{**}` is defined by `W'(\\pi^{**}) = 0`.\n- The threshold `b^{**}` is defined as `b^{**} = \\pi^{**}h`.\n- A consumer's net benefit from the contract is: `NB(x) = b_0 + bx - \\pi(h-w)x - p`.\n\n### Question\n\nConsider the case where `0 < b \\le b^{**}`. In this regime, the firm can use the stipulated damage payment `w` as a tool for price discrimination. To maximize profits, the firm sets the safety level efficiently (`\\pi = \\pi^{**}`) and chooses a specific `w^*` that makes consumers' net benefit independent of their type `x`, thereby extracting all surplus. What is the firm's optimal choice for the damage payment, `w^*`?",
    "Options": {
      "A": "`w^* = h - b/\\pi^{**}`",
      "B": "`w^* = h`",
      "C": "`w^* = b/\\pi^{**}`",
      "D": "`w^* = 0`"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to execute a key calculation from the paper, using a Computational Judgment strategy. The student must derive the value of `w` that achieves perfect price discrimination. The correct answer (A) comes from setting the coefficient of `x` in the net benefit function to zero. Distractor (B) is the socially optimal policy, not the firm's private choice. Distractor (C) is a Step-Omission Error, missing the `h` term. Distractor (D) is the firm's choice in the alternate `b > b^{**}` regime, a common conceptual mix-up.",
    "qid": "368",
    "model_evaluation_result": {
      "model_raw_response": "C) `W^* = B/\\pi^{*}`",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 369,
    "Question": "### Background\n\n**Research Question.** When symmetric equilibria fail to exist due to demand uncertainty, what are the characteristics of the resulting asymmetric pure-strategy equilibria, and what strategic forces sustain them?\n\n**Setting / Institutional Environment.** We analyze a two-stage capacity-price game with linear inverse demand `P(q,a) = a - q` and a two-point demand distribution: demand is `\\underline{a}` with probability `θ` and `\\bar{a}` with probability `1-θ`. We focus on an asymmetric equilibrium where firm 1 has a large capacity `x` and firm 2 has a small capacity `y` (`x > y`).\n\n---\n\n### Data / Model Specification\n\nIn the asymmetric equilibrium `(x,y)` with `x>y`, capacities are assumed to be in the mixed-strategy Region C(`\\underline{a}`) in the low-demand state and the pure-strategy Region A(`\\bar{a}`) in the high-demand state.\n\nThe relevant subgame revenues for the two states are:\n*   **Low demand (`\\underline{a}`):** Revenue for the large firm is `r(x,y,\\underline{a}) = (1/4)(\\underline{a}-y)^2`. Revenue for the small firm is `r(y,x,\\underline{a}) = (y/x) * (1/4)(\\underline{a}-y)^2`.\n*   **High demand (`\\bar{a}`):** Revenue for both firms is `q_i(\\bar{a}-x-y)`, where `q_i` is the firm's own capacity.\n\nThe expected profit for the small firm (firm 2, capacity `y`) is therefore:\n\n```latex\nπ(y,x) = θ \\left[ \\frac{y}{x} \\frac{(\\underline{a}-y)^2}{4} \\right] + (1-θ) \\left[ y(\\bar{a}-x-y) \\right] - cy\n\\quad \\text{(Eq. 1)}\n```\n\nWhich of the following equations represents the first-order condition (`∂π(y,x)/∂y = 0`) that implicitly defines the small firm's optimal capacity choice `y`?",
    "Options": {
      "A": "`(θ/(4x))(\\underline{a}-y)(\\underline{a}-3y) + (1-θ)(\\bar{a}-x-2y) - c = 0`",
      "B": "`(θ/(4x))(\\underline{a}-y)^2 + (1-θ)(\\bar{a}-x-2y) - c = 0`",
      "C": "`θ(1/2)(\\underline{a}-y) + (1-θ)(\\bar{a}-x-2y) - c = 0`",
      "D": "`(θ/(4x))(\\underline{a}-y)(\\underline{a}-3y) + (\\bar{a}-x-2y) - c = 0`"
    },
    "Answer": "A",
    "pi_justification": "This question assesses the ability to correctly derive the first-order condition for the small firm in an asymmetric equilibrium. It uses a Computational Judgment strategy. The correct answer requires correctly applying the product rule to the low-demand revenue term. Distractor B represents a failure to apply the product rule correctly. Distractor C uses the wrong revenue function for the low-demand state (derivative of the large firm's revenue). Distractor D makes a conceptual error by omitting the probability weight `(1-θ)` on the high-demand state's marginal revenue.",
    "qid": "369",
    "model_evaluation_result": {
      "model_raw_response": "B) `(theta/(4x))(underline{a} - y)^2 + 1-theta)(bar{a} – x – 2y) – c =  0` )",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 370,
    "Question": "### Background\n\n**Research Question.** When symmetric equilibria fail to exist due to demand uncertainty, what are the characteristics of the resulting asymmetric pure-strategy equilibria, and what strategic forces sustain them?\n\n**Setting / Institutional Environment.** We analyze a two-stage capacity-price game with a two-point demand distribution: demand is `\\underline{a}` with probability `θ` and `\\bar{a}` with probability `1-θ`. In an asymmetric equilibrium with a large firm (capacity `x`) and a small firm (capacity `y`), the small firm's marginal revenue of capacity is negative in the low-demand state but positive in the high-demand state.\n\n---\n\n### Data / Model Specification\n\nThe first-order condition for the small firm's optimal capacity choice `y` is given by:\n\n```latex\n\\frac{\\partial \\pi(y,x)}{\\partial y} = \\underbrace{ \\frac{\\theta}{4x}(\\underline{a}-y)(\\underline{a}-3y) }_{MR_{low}} + \\underbrace{ (1-\\theta)(\\bar{a}-x-2y) }_{MR_{high}} - c = 0\n\\quad \\text{(Eq. 1)}\n```\n\nAt an equilibrium `(x*, y*)`, the marginal revenue in the low-demand state, `MR_low`, is negative, while the marginal revenue in the high-demand state, `MR_high`, is positive.\n\nConsider an exogenous increase in `θ`, the probability of the low-demand state. How must the small firm's equilibrium capacity, `y*`, adjust to maintain the first-order condition, and what is the economic intuition?\n",
    "Options": {
      "A": "`y*` must decrease, because the overall expected marginal revenue of capacity falls as more weight is placed on the negative-revenue low-demand state.",
      "B": "`y*` must increase, because the firm needs to be more competitive in the low-demand state, which is now more frequent.",
      "C": "`y*` is unaffected, because the change in probability is already accounted for in the firm's reaction function.",
      "D": "`y*` must decrease, because the large firm will increase its capacity `x*`, forcing the small firm to reduce its own capacity."
    },
    "Answer": "A",
    "pi_justification": "This question assesses the ability to perform comparative statics and interpret the result economically. It uses a Reverse-Reasoning strategy: given a change, infer the consequence. The correct answer identifies that increasing the weight `θ` on the negative `MR_low` term reduces the overall expected marginal revenue, requiring a decrease in `y` to restore optimality. Distractor B provides the opposite effect with flawed reasoning. Distractor C suggests no effect, a common misconception. Distractor D offers a plausible strategic story (a general equilibrium effect) but misses the direct, partial equilibrium mechanism described in the first-order condition.",
    "qid": "370",
    "model_evaluation_result": {
      "model_raw_response": "B) `yi*` mustincrease, because thefirmneeds to be morecompetitive in the lowdemand state, whichisnow morefrequent.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 371,
    "Question": "### Background\n\n**Research Question.** This problem explores how an agent's time horizon affects their choice between incremental refinement (intensive search) and breakthrough innovation (extensive search).\n\n**Setting.** We compare the decision of a standard one-period agent with that of a two-period agent who can make two consecutive search decisions. The comparison is made at a point where the one-period agent is exactly indifferent between intensive and extensive search, meaning the expected payoffs in the first period are identical.\n\n### Data / Model Specification\n\nTheorem 6 in the paper shows that under these conditions of indifference, the two-period agent strictly prefers to undertake **extensive search** in the first period.\n\n### Question\n\nWhat is the core economic mechanism that explains why a two-period agent, who has a longer time horizon, strictly prefers extensive search when a one-period agent is indifferent?",
    "Options": {
      "A": "Extensive search creates a valuable option to conduct a potentially high-return intensive search in the second period, a future benefit the one-period agent ignores.",
      "B": "A two-period agent is more willing to undertake the higher risk associated with extensive search because they have a second period to recover from a potential loss.",
      "C": "The expected direct payoff from extensive search is higher in the second period than the payoff from intensive search, regardless of the first-period outcome.",
      "D": "The cost `c` of extensive search is paid only in the first period, making any second-period search more profitable and favoring the initially costly option."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.0). The original QA problem's core assessment target—identifying the concept of 'option value'—is highly suitable for a choice format. This question uses a Reverse-Reasoning strategy, asking for the cause of a given outcome. Distractors are designed based on common misconceptions: B (Conceptual Opposite) incorrectly introduces risk aversion into a risk-neutral model; C (Almost Right) misinterprets the payoff structure, as the first-period payoffs are equal by assumption; D (True but Irrelevant) points to a true feature of the cost structure that does not explain the choice between the two paths.",
    "qid": "371",
    "model_evaluation_result": {
      "model_raw_response": "A) Extensively search creates a valveable option to conduct potentially high return intensive search in second period, future benefit the on period agent ignores.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 372,
    "Question": "### Background\n\n**Research Question.** This problem explores how an agent's time horizon affects their choice between incremental refinement (intensive search) and breakthrough innovation (extensive search).\n\n**Setting.** We compare the decision of a standard one-period agent with that of a two-period agent who can make two consecutive search decisions. The comparison is made at a point where the one-period agent is exactly indifferent between intensive and extensive search, meaning the expected payoffs in the first period are identical.\n\n### Data / Model Specification\n\nKey results from the model that inform the two-period decision:\n*   If an agent starts with **intensive search**, the subsequent search in the second period will necessarily be **extensive**.\n*   If an agent starts with **extensive search**, the subsequent search can be either **intensive** (if the initial discovery is promising) or **extensive**.\n\n### Question\n\nGiven that the first-period payoffs are identical, the two-period agent's choice depends on the expected payoff in the second period. What is the key structural difference in the second-period decision problem that makes starting with extensive search more valuable?",
    "Options": {
      "A": "After an initial extensive search, the agent holds an option to choose the best of two search modes in the second period, whereas after an initial intensive search, the second-period action is fixed.",
      "B": "After an initial intensive search, the agent can choose between refining the new result or starting an extensive search, whereas after an initial extensive search, the second-period action is fixed.",
      "C": "The cost `c` of extensive search must be paid again in the second period if the first search was also extensive, making an initial intensive search preferable.",
      "D": "An initial extensive search guarantees that the second-period search will be an intensive one, which has a higher expected growth rate `g_I`."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.0). This question complements the previous one by focusing on the specific structural difference in the choice set that gives rise to option value. It uses a Computational Judgment strategy, requiring the student to compare the structure of the two decision paths. Distractor B (Conceptual Opposite) directly reverses the logic of the model. Distractor C (Formula Misuse) introduces an incorrect assumption about costs. Distractor D (Almost Right) correctly identifies that intensive search can be high-growth but incorrectly states it's a guaranteed outcome and misses the core idea of the *choice* itself being valuable.",
    "qid": "372",
    "model_evaluation_result": {
      "model_raw_response": "B) After intensive search, agent can choose to refine the new result, whereas after extensive search, second-period action (search mode) is fixed.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 373,
    "Question": "### Background\n\n**Research Question.** This problem investigates the central empirical puzzle motivating the paper: how to reconcile the massive reallocation of labor towards the service sector with the observed stability of relative wages. It then uses the model's counterfactual simulations to test whether significant labor market frictions are compatible with this empirical fact.\n\n**Setting / Institutional Environment.** The analysis uses the paper's estimated general equilibrium model of the U.S. economy (1968-2000). The core exercise is to compare the baseline model simulation against counterfactuals where inter-sectoral mobility costs are set to zero or to a prohibitively high level.\n\n### Data / Model Specification\n\nThe key empirical puzzle and the model's ability to replicate it are shown in Table 1. Table 2 shows results from counterfactual experiments where mobility costs are altered.\n\n**Table 1: Actual and Predicted Sectoral Differences in Growth (Avg. Annual % Change, 1968-2000)**\n\n| Service-Goods Growth Rate | Actual | Predicted |\n| :--- | :---: | :---: |\n| Employment | 2.23 | 2.13 |\n| Hourly Wage | 0.23 | 0.51 |\n\n*Source: Abridged from Table VII in the paper.*\n\n**Table 2: Effect of Mobility Costs on Skill Rental Prices (Annual % Change, 1981-2000)**\n\n| Sector / Occupation | Predicted (Baseline) | Counterfactual 1 (Prohibitive Cost) | Counterfactual 3 (Zero Cost) |\n| :--- | :---: | :---: | :---: |\n| **Goods Sector** | |\n| White Collar | 0.71 | 0.75 | 0.71 |\n| **Service Sector** | |\n| White Collar | 0.46 | 0.49 | 0.54 |\n\n*Source: Abridged from Table IX in the paper.*\n\n### Question\n\nBased on the data in Table 1 and Table 2, what is the paper's central argument regarding the relationship between labor mobility costs and the stability of relative wages in the U.S. economy?",
    "Options": {
      "A": "Despite the stability of relative wages, mobility costs are substantial; wage stability is maintained by other adjustment margins, as shown by the fact that skill rental prices are largely invariant to the level of mobility costs.",
      "B": "The stability of relative wages in the face of large employment shifts (Table 1) proves that mobility costs must be close to zero, allowing for a nearly perfectly elastic labor supply.",
      "C": "Prohibitively high mobility costs cause skill rental prices to become highly volatile (Table 2), which contradicts the observed stability of wages, confirming that costs must be low.",
      "D": "The growth in service-sector employment was much faster than the growth in its relative wage (Table 1), indicating that mobility costs primarily affect employment levels but not wages."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.38). The original QA problem was a sequence of highly convertible parts. This Single Choice item assesses the paper's core argument by requiring the synthesis of two tables. It uses a Reverse-Reasoning strategy. Distractor B represents the 'naive view' the paper refutes. Distractor C involves a direct misreading of Table 2. Distractor D is a plausible but incomplete conclusion that misses the central point about frictions.",
    "qid": "373",
    "model_evaluation_result": {
      "model_raw_response": "A) Despite...skill rental prices are...invariant to the level...",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 374,
    "Question": "### Background\n\n**Research Question.** This problem investigates the central empirical puzzle motivating the paper: how to reconcile the massive reallocation of labor towards the service sector with the observed stability of relative wages. It then uses the model's counterfactual simulations to test whether significant labor market frictions are compatible with this empirical fact and to quantify the aggregate cost of these frictions.\n\n**Setting / Institutional Environment.** The analysis uses the paper's estimated general equilibrium model of the U.S. economy (1968-2000). The core exercise is to compare the baseline model simulation against counterfactuals where inter-sectoral mobility costs are set to zero or to a prohibitively high level.\n\n### Data / Model Specification\n\n**Table 1: Effect of Mobility Costs on Sectoral Output (Index, 1968 Goods Sector = 1.0)**\n\n| Period | Predicted (Baseline) | Counterfactual 1 (Prohibitive Cost) | Counterfactual 3 (Zero Cost) |\n| :--- | :---: | :---: | :---: |\n| **Goods Sector Output** | |\n| 1995-2000 | 1.59 | 0.65 | 2.94 |\n| **Service Sector Output** | |\n| 1995-2000 | 5.21 | 4.45 | 11.72 |\n\n*Source: Abridged from Table VIII in the paper.*\n\n### Question\n\nUsing the data for the 1995-2000 period in Table 1, calculate the approximate percentage increase in total aggregate output (Goods + Services) that would occur if the economy moved from the 'Predicted (Baseline)' scenario to the 'Counterfactual 3 (Zero Cost)' scenario. What does this imply about the estimated mobility costs?",
    "Options": {
      "A": "116%; this implies that the estimated mobility costs create substantial allocative inefficiency, leading to a large loss of potential aggregate output.",
      "B": "54%; this implies that the estimated mobility costs have a moderate but significant impact on allocative efficiency.",
      "C": "116%; this implies that mobility costs are beneficial for the economy, as they prevent excessive reallocation and instability.",
      "D": "210%; this implies that the costs of mobility are catastrophically high, making the baseline model unrealistic."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.38). The original QA problem was a sequence of highly convertible parts. This Single Choice item uses the Computational Judgment strategy, requiring a multi-step calculation followed by a conceptual interpretation. The correct answer combines the correct calculation with the correct inference. Distractor B is based on a common calculation error (dividing the change by the wrong base: (14.66 - 6.80) / 14.66 ≈ 54%). Distractor C has the correct calculation but an incorrect, conceptually opposite interpretation. Distractor D represents a different calculation error.",
    "qid": "374",
    "model_evaluation_result": {
      "model_raw_response": "A) 117% (rounded to 116%)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 375,
    "Question": "### Background\n\n**Research Question.** What is the fundamental trade-off between providing ex-ante incentives (effort) and ensuring ex-post selection efficiency (accuracy) when choosing between informed and blind review?\n\n**Setting and Sample.** The setting is a specific numerical example from the paper with two equally likely agent types, `θ = 1/2` (low-ability) and `θ = 3/2` (high-ability). The mean ability `E[θ] = 1` is set to equal the critical type `θ*`, which represents an ideal condition for blind review to boost incentives. The signal technology is based on triangular densities on [0, 1], and key parameters are set to `υ=c=u=1`.\n\n**Variables and Parameters.**\n- `θ`: Agent's ability type, `θ ∈ {1/2, 3/2}`.\n- `s`: The acceptance standard.\n- `p`: The agent's effort level (probability of high quality).\n\n---\n\n### Data / Model Specification\n\nThe signal `σ` is drawn from one of two triangular densities on [0, 1]: `f_h(σ) = 2σ` if project quality is high, or `f_l(σ) = 2(1-σ)` if it is low. This implies the cumulative distribution functions are `F_h(s) = s²` and `F_l(s) = 1 - (1-s)² = 2s - s²`.\n\nThe agent's utility function is `U(p,s;θ) = u[p(1-F_h(s))+(1-p)(1-F_l(s))] - C(p;θ)`, where the cost of effort is `C(p;θ) = p²/(2θ)` and the payoff from acceptance is `u=1`.\n\n**Table 1: Equilibrium Outcomes in a Two-Type Example**\n\n| | INFORMED | BLIND |\n| :--- | :---: | :---: |\n| **STANDARD** | |\n| `θ = 1/2` | 1 | 1/2 |\n| `θ = 3/2` | 1/3 | 1/2 |\n| **EFFORT** | |\n| `θ = 1/2` | 0 | 1/4 |\n| `θ = 3/2` | 2/3 | 3/4 |\n\n---\n\n### Question\n\nBased on the information provided, which of the following statements most accurately describes the welfare outcomes for the agents when switching from an informed to a blind review regime?",
    "Options": {
      "A": "Blind review improves the low-type agent's utility but harms the high-type agent's utility.",
      "B": "Blind review harms the low-type agent's utility but improves the high-type agent's utility.",
      "C": "Blind review elicits higher effort from both agent types, leading to a higher average probability of error for the principal.",
      "D": "The switch to blind review is a Pareto improvement from the agents' perspective, as both prefer the fairer, uniform standard."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's ability to synthesize the numerical results of the model to draw a conclusion about agent welfare. It uses a Scenario Application strategy. The correct answer (A) requires performing the utility calculations from the original QA problem. Distractor (B) is the conceptual opposite. Distractor (C) presents a true but incomplete statement about effort and error rates, testing if the student focuses on the specific question of agent utility. Distractor (D) targets a common misconception by conflating the normative concept of 'fairness' with the technical definition of a Pareto improvement.",
    "qid": "375",
    "model_evaluation_result": {
      "model_raw_response": "D) The\tswitch to blind review\tis\ta\tPareto\timprovement\tfrom\tthe\tagents'\tperspective,\tas\tboth\tpreferring\tthe\tfairer,\tuniform\tstandard.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 376,
    "Question": "### Background\n\n**Research Question.** What is the fundamental trade-off between providing ex-ante incentives (effort) and ensuring ex-post selection efficiency (accuracy) when choosing between informed and blind review?\n\n**Setting and Sample.** The setting is a specific numerical example from the paper with two equally likely agent types, `θ = 1/2` (low-ability) and `θ = 3/2` (high-ability). The mean ability `E[θ] = 1` is set to equal the critical type `θ*`, which represents an ideal condition for blind review to boost incentives. The signal technology is based on triangular densities on [0, 1], and key parameters are set to `υ=c=u=1`.\n\n**Variables and Parameters.**\n- `θ`: Agent's ability type, `θ ∈ {1/2, 3/2}`.\n- `s`: The acceptance standard.\n- `p`: The agent's effort level (probability of high quality).\n- `Pr{ACCEPT|q=l}`: Probability of a false acceptance (Type I error).\n- `Pr{REJECT|q=h}`: Probability of a false rejection (Type II error).\n\n---\n\n### Data / Model Specification\n\nThe signal `σ` is drawn from one of two triangular densities on [0, 1]: `f_h(σ) = 2σ` if project quality is high, or `f_l(σ) = 2(1-σ)` if it is low. This implies the cumulative distribution functions are `F_h(s) = s²` and `F_l(s) = 1 - (1-s)² = 2s - s²`.\n\nThe probability of a false acceptance for a given agent is `(1-p) * (1-F_l(s))` and a false rejection is `p * F_h(s)`.\n\n**Table 1: Equilibrium Outcomes in a Two-Type Example**\n\n| | INFORMED | BLIND |\n| :--- | :---: | :---: |\n| **STANDARD** | |\n| `θ = 1/2` | 1 | 1/2 |\n| `θ = 3/2` | 1/3 | 1/2 |\n| **EFFORT** | |\n| `θ = 1/2` | 0 | 1/4 |\n| `θ = 3/2` | 2/3 | 3/4 |\n\n---\n\n### Question\n\nBased on the data in Table 1 and the provided formulas, what is the average probability of the principal making an error (either a false acceptance or a false rejection) under the **Informed Review** regime?",
    "Options": {
      "A": "2/9",
      "B": "1/9",
      "C": "1/4",
      "D": "0"
    },
    "Answer": "B",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests the student's ability to perform a precise multi-step calculation based on model formulas and data. It uses a Computational Judgment strategy. The correct answer (B) requires calculating the error probability for each agent type under the Informed regime and then correctly averaging them. Distractors are designed based on common errors: (A) and (D) represent failing to average the two types' error rates (a step-omission error), while (C) represents calculating the value for the wrong regime (a comprehension slip).",
    "qid": "376",
    "model_evaluation_result": {
      "model_raw_response": "B) 2.5/9 = 5/18",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 377,
    "Question": "In the model, the **exhaustion level** `s_0` is the public belief `\\tilde{s}` at which the number of hospital visitors exactly equals the hospital's capacity `\\bar{n}`. For any true disease severity `s > s_0`, a state of congestion would arise under full information, which is socially inefficient.\n\nA benevolent social planner's goal is to induce a public belief `\\tilde{s}^*` that makes agents in the decentralized equilibrium choose to visit the hospital in a way that is ex-post socially optimal. What is this optimal public belief `\\tilde{s}^*` when the true severity is `s > s_0`?",
    "Options": {
      "A": "`s_0`",
      "B": "`s`",
      "C": "`c`",
      "D": "A value strictly greater than `s` to signal the high danger."
    },
    "Answer": "A",
    "pi_justification": "This item uses the Computational Judgment strategy to replace parts (b) and (c) of a QA problem. It requires the student to first reason that the socially optimal allocation is for the `\\bar{n}` highest-risk agents to visit (filling capacity perfectly), and then to identify the unique public belief that induces this precise behavior in equilibrium. The correct answer is `s_0`. Distractors represent common conceptual errors: (B) is the belief under full disclosure, which is inefficient; (C) is a key parameter but not the target belief; (D) represents a misunderstanding of how to manage congestion.",
    "qid": "377",
    "model_evaluation_result": {
      "model_raw_response": "A) `A` \n\n(Note: I'm only outputting the answer as per the format specified)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 378,
    "Question": "A unit mass of agents, each with a private type `q_i` (likelihood of needing care), decide whether to visit a hospital with capacity `\\bar{n}`. The distribution of types is given by the CDF `H(q)`. In the no-congestion regime, an agent visits if `q_i \\ge c/\\tilde{s}`, where `c` is the visit cost and `\\tilde{s}` is the public belief about disease severity.\n\nThe **exhaustion level**, `s_0`, is the critical public belief at which the number of visitors, behaving as if there is no congestion, exactly equals the hospital's capacity `\\bar{n}`. Which of the following is the correct formula for `s_0`?",
    "Options": {
      "A": "`c / H^{-1}(1-\\bar{n})`",
      "B": "`c / H^{-1}(\\bar{n})`",
      "C": "`\\bar{n} / H^{-1}(1-c)`",
      "D": "`c`"
    },
    "Answer": "A",
    "pi_justification": "This item uses the Computational Judgment strategy to test the derivation of a key model parameter. The student must correctly set up the equilibrium condition at the threshold of congestion (`1 - H(c/s_0) = \\bar{n}`) and solve for `s_0`. Distractor (B) represents a common error in using a CDF (confusing `H(x)` with `1-H(x)`). Distractor (C) scrambles the parameters (Formula Misuse). Distractor (D) identifies a different, less important threshold in the model (Conceptual Error).",
    "qid": "378",
    "model_evaluation_result": {
      "model_raw_response": "B) `H^{-1} (c / \\bar{n})`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 379,
    "Question": "A unit mass of agents decide whether to visit a hospital with capacity `\\bar{n}`. The probability of admission for a visitor is `p(n, \\bar{n}) = \\operatorname*{min}\\{1, \\bar{n}/n\\}`, where `n` is the total mass of visitors.\n\nSuppose the public belief about disease severity `\\tilde{s}` is high enough that the system is in a congestion regime (`n > \\bar{n}`). If hospital capacity `\\bar{n}` is marginally increased, what is the effect on the equilibrium number of hospital visits `n(\\tilde{s})`?",
    "Options": {
      "A": "`n(\\tilde{s})` increases, because the higher probability of admission incentivizes more marginal agents to visit.",
      "B": "`n(\\tilde{s})` decreases, because with more capacity, the situation is less dire, reducing agents' perceived urgency.",
      "C": "`n(\\tilde{s})` remains unchanged, as it is determined by the belief `\\tilde{s}`, not the capacity `\\bar{n}`.",
      "D": "`n(\\tilde{s})` decreases, because the hospital can now screen patients more effectively."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the strategic interactions in the congestion game, a key comparative static. The correct answer requires reasoning about how a change in `\\bar{n}` affects the admission probability `p`, which in turn alters individual incentives. Distractor (B) is the Conceptual Opposite, representing a plausible but incorrect intuition about agent behavior. Distractor (C) ignores the strategic channel through which `\\bar{n}` affects decisions. Distractor (D) introduces an irrelevant mechanism (screening) not present in this part of the model.",
    "qid": "379",
    "model_evaluation_result": {
      "model_raw_response": "C) `N(\\tilde s)` remains unchanged, since it is determined solely by the public belief `\\tildeb s`, not the hospital capacity `\\barn`.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 380,
    "Question": "### Background\n\nIn social choice theory, a conflict exists between the Pareto criterion (efficiency) and equity criteria like no-envy. The paper explores lexicographic principles to resolve this. This question focuses on the **equity-first, efficiency-second principle**.\n\n### Data / Model Specification\n\nKey social preference relations are:\n\n1.  **The Pareto Relation (`≻_P`):** `x ≻_P y` if `x` is a Pareto improvement over `y`.\n2.  **The Equity Relation (`≻_F`):** Based on the set of envy instances `H(x)`, `x ≻_F y` if and only if `#H(x) < #H(y)`.\n3.  **The Equity-First Relation (`≻_FP`):** This relation ranks allocation `x` over `y` as follows:\n\n    ```latex\n    x \\succ_{FP} y \\iff (x \\succ_F y) \\lor (\\#H(x) = \\#H(y) \\text{ and } x \\succ_P y) \\quad \\text{(Eq. (1))}\n    ```\n\n### Question\n\nWhich statement best describes the value judgment embedded in the equity-first social preference relation, `≻_FP`?",
    "Options": {
      "A": "Equity is the primary criterion; efficiency is only used as a tie-breaker for allocations with the same number of envy instances.",
      "B": "Efficiency is the primary criterion; equity is only used as a tie-breaker for allocations that are not Pareto-comparable.",
      "C": "A weighted average of equity and efficiency is used to rank allocations, with a higher weight placed on equity.",
      "D": "Only allocations that are completely envy-free are considered socially desirable; all other allocations cannot be ranked."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the foundational understanding of the equity-first principle's lexicographic structure, a core concept of the paper. The distractors are designed to identify common confusions: B describes the efficiency-first principle (Conceptual Opposite), C posits a utilitarian trade-off model (Formula Misuse), and D incorrectly describes how non-envy-free allocations are handled (Conceptual Error).",
    "qid": "380",
    "model_evaluation_result": {
      "model_raw_response": "C) Aweighted average of equityand efficiency is used torank allocations, witha higher weightplaced on equity.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 381,
    "Question": "### Background\n\nIn social choice theory, a conflict exists between the Pareto criterion (efficiency) and equity criteria like no-envy. The paper explores lexicographic principles to resolve this. This question focuses on the **equity-first, efficiency-second principle**.\n\n### Data / Model Specification\n\nKey social preference relations are:\n\n1.  **The Pareto Relation (`≻_P`):** `x ≻_P y` if `x` is a Pareto improvement over `y`. `≻_P` is transitive.\n2.  **The Equity Relation (`≻_F`):** Based on the set of envy instances `H(x)`, `x ≻_F y` if and only if `#H(x) < #H(y)`.\n3.  **The Equity-First Relation (`≻_FP`):** This relation ranks allocation `x` over `y` as follows:\n\n    ```latex\n    x \\succ_{FP} y \\iff (x \\succ_F y) \\lor (\\#H(x) = \\#H(y) \\text{ and } x \\succ_P y) \\quad \\text{(Eq. (1))}\n    ```\n\n### Question\n\nIn the proof that the equity-first relation `≻_FP` is transitive, one must show that if `x ≻_FP y` and `y ≻_FP z`, then `x ≻_FP z`. Consider the specific case where the tie-breaking rule is used for both initial comparisons: `#H(x) = #H(y)` with `x ≻_P y`, AND `#H(y) = #H(z)` with `y ≻_P z`. Why does it necessarily follow that `x ≻_FP z`?",
    "Options": {
      "A": "Because `#H(x) = #H(z)` holds, and `x ≻_P z` follows from the transitivity of the Pareto relation `≻_P`.",
      "B": "Because the number of envy instances must be decreasing, so `x ≻_F z` must be true.",
      "C": "The relationship between `x` and `z` is indeterminate without knowing the specific utility functions.",
      "D": "Because any allocation that is Pareto-superior to another must also be more equitable."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to probe understanding of the most complex step in the transitivity proof for `≻_FP`. Instead of reproducing the proof, the student must identify the critical logical link—the transitivity of the underlying Pareto relation—that makes the argument work. Distractors target specific logical failures: B confuses the primary and secondary criteria, C represents a failure to apply the transitivity property (Step-Omission Error), and D posits a false relationship between efficiency and equity.",
    "qid": "381",
    "model_evaluation_result": {
      "model_raw_response": "B) Because any number of envy instance must be decreasing.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 382,
    "Question": "### Background\n\nIn social choice theory, a conflict exists between the Pareto criterion (efficiency) and equity criteria like no-envy. The paper explores lexicographic principles to resolve this. This question focuses on the **equity-first, efficiency-second principle**.\n\n### Data / Model Specification\n\nThe **Equity-First Relation (`≻_FP`)** prioritizes equity (fewer envy instances) first, and uses Pareto superiority as a tie-breaker. An allocation `x*` is a **maximal element** for this relation if no other feasible allocation `y` exists such that `y ≻_FP x*`. The set of maximal elements is denoted `M_{≻_FP}(Z)`.\n\nThe paper shows `M_{≻_FP}(Z) = P[F(Z)]`, where `F(Z)` is the set of all envy-free feasible allocations and `P[Y]` denotes the set of Pareto-optimal allocations *within* a given set `Y`.\n\n### Question\n\nThe paper establishes that the set of maximal elements for the equity-first relation, `M_{≻_FP}(Z)`, is always non-empty, even if agent preferences are not convex. What is the fundamental reason for this guaranteed existence?",
    "Options": {
      "A": "The set of envy-free allocations `F(Z)` is always non-empty and compact, and a Pareto-optimal allocation is guaranteed to exist on any non-empty, compact set.",
      "B": "The guaranteed transitivity of the equity-first relation `≻_FP` is a sufficient condition for a maximal element to exist in any finite set.",
      "C": "The existence of maximal elements is only guaranteed if all agent preferences are assumed to be convex, which ensures `P(Z)` and `F(Z)` intersect.",
      "D": "Every envy-free allocation is, by definition, a maximal element for the equity-first relation."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the understanding of a key result: the guaranteed existence of 'equity-first' optima. It requires connecting the formal characterization `P[F(Z)]` to foundational concepts of economic theory (existence of optima on compact sets). The distractors pinpoint specific misunderstandings: B confuses transitivity with existence on an infinite set (Almost Right), C introduces a condition (convexity) the paper explicitly shows is not needed for this principle (Conceptual Opposite), and D is false because an envy-free allocation can be Pareto-dominated by another envy-free allocation (Conceptual Error).",
    "qid": "382",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\n(Note: I've only provided the answer as per the required format.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 383,
    "Question": "### Background\n\nAn upstream monopolist sells an input to a downstream Cournot duopoly consisting of a multi-market “chain” (firm 1) and a single-market “local store” (firm 2). The chain operates in $K \\geq 2$ markets, while the local store operates in only one.\n\n### Data / Model Specification\n\nDownstream firms can produce the input themselves using a technology with a total cost function given by:\n```latex\nC(y) = F + vy\n```\nwhere $y$ is the quantity of the input produced, $F > 0$ is a fixed cost, and $v$ is a marginal cost.\n\n### Question\n\nAccording to the model, why is the chain's threat of backward integration more credible or “potent” than the local store's threat?",
    "Options": {
      "A": "The chain can spread the fixed cost of integration ($F$) over its sales in multiple markets, lowering its per-unit integration cost.",
      "B": "The chain is assumed to be a more efficient downstream producer, giving it a larger market share to leverage against the monopolist.",
      "C": "The chain faces a lower marginal cost of self-supply ($v$) than the local store.",
      "D": "The monopolist's marginal cost ($c$) is lower than the self-supply marginal cost ($v$), making integration an unprofitable strategy for any firm."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the foundational concept of economies of scale as the driver of the chain's bargaining power. It replaces Q1 of the original QA problem. The distractors are designed to isolate this core concept from other plausible but incorrect explanations. (B) is a common intuition but not the mechanism in this model. (C) is a direct contradiction of the model's assumptions. (D) correctly states a fact about social efficiency but fails to explain the difference in threat credibility between the two firms.",
    "qid": "383",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\nNote: This is a single-letter answer.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 384,
    "Question": "### Background\n\nAn upstream monopolist sets discriminatory prices, $w_1$ for the multi-market “chain” and $w_2$ for the “local store”. The chain will integrate if its profit from doing so, $\\pi^e$, is sufficiently high relative to its profit from not integrating, $\\pi$.\n\n### Data / Model Specification\n\nThe “integration frontier,” denoted $w_1 = I[w_2]$, represents the price pairs $(w_1, w_2)$ where the chain is exactly indifferent between integrating and not integrating. The slope of this frontier is crucial for the monopolist's strategy.\n\n### Question\n\nThe paper describes a scenario where the integration frontier, $w_1 = I[w_2]$, is downward sloping. What is the key economic mechanism that would cause this relationship?",
    "Options": {
      "A": "The chain interprets a higher price to the local store ($w_2$) as a signal of the monopolist's weakness, which significantly increases the chain's expected post-integration profits ($\\pi^e$).",
      "B": "A higher price to the local store ($w_2$) weakens the local store as a competitor, increasing the chain's non-integration profits ($\\pi$) and making integration less attractive.",
      "C": "The monopolist's profit function is quasi-concave, meaning it must lower $w_1$ to maintain profits when it raises $w_2$.",
      "D": "A higher price to the chain ($w_1$) lowers its non-integration profits ($\\pi$), which must be offset by a lower price to the local store ($w_2$) to prevent integration."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to assess understanding of the paper's most subtle mechanism: the signaling effect of prices on the integration decision. It replaces Q3 of the original QA problem. The correct answer (A) identifies the signaling channel. Distractor (B) describes the 'natural' case that leads to an *upward* sloping frontier (Conceptual Opposite). Distractor (C) is True but Irrelevant, confusing the monopolist's iso-profit curve with the chain's iso-profit (integration) frontier. Distractor (D) misidentifies the relationship between the variables.",
    "qid": "384",
    "model_evaluation_result": {
      "model_raw_response": "C) The monopoly's profit function must lower $W_1$ when it raises W_2$to maintain profits.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 385,
    "Question": "### Background\n\nAn upstream monopolist sells an input to a downstream duopoly. The monopolist can set discriminatory prices $(w_1, w_2)$ but is constrained by the threat of backward integration from the larger buyer (the “chain”).\n\n### Data / Model Specification\n\nThe monopolist chooses $(w_1, w_2)$ to maximize its profit $U^m(w_1, w_2)$, subject to the constraint that the chain does not integrate. It is a key assumption of the model that the unconstrained profit-maximizing price pair $(w^*, w^*)$ is infeasible because it would induce the chain to integrate.\n\n### Question\n\nGiven these assumptions, which of the following conditions must hold at the monopolist's optimal discriminatory price pair $(w_1^D, w_2^D)$?",
    "Options": {
      "A": "The chain is exactly indifferent between integrating and not integrating.",
      "B": "The chain strictly prefers not to integrate, giving the monopolist a buffer.",
      "C": "The monopolist sets prices equal to its unconstrained optimum $(w^*, w^*)$.",
      "D": "The prices are set such that the chain earns zero economic profit."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the understanding of constrained optimization in this specific economic context, replacing Q4 of the original QA problem. The correct answer (A) identifies that the constraint must be binding at the optimum. This is a fundamental result of the setup. Distractor (B) represents a non-binding constraint, which is incorrect because the unconstrained optimum is infeasible (Almost Right). Distractor (C) is explicitly ruled out by the problem's premise (Conceptual Opposite). Distractor (D) confuses the integration constraint with a zero-profit condition, which is not part of the model.",
    "qid": "385",
    "model_evaluation_result": {
      "model_raw_response": "B) The Chain strictly prefers not integrating, giving the Monopolist a Buffer.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 386,
    "Question": "### Background\n\n**Research Question:** This problem addresses the practical challenges of estimating PELVE from serially dependent data, such as financial time series, and understanding the statistical properties of the estimator.\n\n**Setting / Institutional Environment:** The analysis is based on a sample from a strictly stationary and `α`-mixing sequence of losses. This data structure is more realistic for financial returns than the i.i.d. assumption, as it allows for phenomena like volatility clustering.\n\n**Variables & Parameters:**\n- `ĉ_n`: The empirical PELVE estimator.\n- `W(·)`: A zero-mean Gaussian process.\n\n### Data / Model Specification\n\nFor a stationary `α`-mixing sequence, `sqrt(n)(ĉ_n - c)` converges in distribution to a normal random variable whose variance depends on the limiting Gaussian process `W`. The covariance of this process is given by:\n```latex\n\\operatorname{Cov}(W(t),W(s)) = \\sum_{i \\in \\mathbb{Z}} \\left( \\mathbb{P}(\\{F(X_{1}) \\le s\\} \\cap \\{F(X_{1+i}) \\le t\\}) - ts \\right)\n```\n(Eq. 1)\n\nFor i.i.d. data, the sum only includes the `i=0` term, simplifying to `min(t,s) - ts`.\n\n### Question\n\nFinancial loss series often exhibit volatility clustering (i.e., large losses tend to follow large losses), a form of `α`-mixing dependence. According to the asymptotic theory presented, how does this feature affect statistical inference for the PELVE estimator `ĉ_n` compared to a hypothetical i.i.d. scenario with the same marginal distribution?",
    "Options": {
      "A": "It decreases the true asymptotic variance, causing confidence intervals constructed under a naive i.i.d. assumption to be overly conservative (too wide).",
      "B": "It increases the true asymptotic variance, causing confidence intervals constructed under a naive i.i.d. assumption to be misleadingly precise (too narrow).",
      "C": "It has no effect on the true asymptotic variance, but it introduces a bias in the point estimate `ĉ_n`.",
      "D": "It violates the stationarity assumption required for the theory, making the asymptotic variance undefined."
    },
    "Answer": "B",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This Single Choice item uses a Scenario Application strategy to test the 'Mathematical Apex' part of the original QA. It assesses the user's ability to connect a real-world data feature (volatility clustering) to its theoretical consequence within the paper's framework. The options are designed as a matrix of cause and effect. Option B is correct. Option A is the conceptual opposite. Option C confuses variance with bias. Option D introduces a plausible but incorrect technical objection.",
    "qid": "386",
    "model_evaluation_result": {
      "model_raw_response": "B) It increased the true asymptopic variance, causing confindence intervals constructed under naif i.i.d assumption to be misleadingly precisse (to narrow).",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 387,
    "Question": "### Background\n\n**Research Question.** This problem examines the theoretical foundation and empirical specification of a parametric model for time-varying risk premia in asset returns. The core idea is that an asset's expected excess return is determined by its time-varying conditional covariance with a benchmark portfolio.\n\n**Setting.** The analysis begins with a general consumption-based asset pricing model which is then specialized to a conditional single-beta Capital Asset Pricing Model (CAPM). This theoretical model is implemented empirically using a bivariate GARCH-in-Mean (GARCH-M) specification for Japanese and World equity index excess returns.\n\n**Variables and Parameters.**\n*   `R_{j,t}^*`: Excess return on the Japanese equity portfolio at time `t`.\n*   `R_{w,t}^*`: Excess return on the MSCI World equity (benchmark) portfolio at time `t`.\n*   `E_{t-1}[·]`: Expectation conditional on information at `t-1`.\n*   `h_{jw,t}`: Conditional covariance between `R_{j,t}^*` and `R_{w,t}^*`.\n*   `h_{w,t}`: Conditional variance of `R_{w,t}^*`.\n*   `β_t = h_{jw,t} / h_{w,t}`: The time-varying conditional beta of Japanese returns.\n*   `γ_{0j}`: The intercept (alpha) in the Japanese excess return equation.\n*   `μ`: The coefficient on the conditional risk premium term.\n\n---\n\n### Data / Model Specification\n\nThe theoretical conditional single-beta asset pricing relation is given by:\n```latex\n\\mathrm{E}_{t-1}[R_{j,t}^*] = \\frac{\\mathrm{cov}_{t-1}[R_{j,t}^*, R_{w,t}^*]}{\\mathrm{var}_{t-1}[R_{w,t}^*]} \\mathrm{E}_{t-1}[R_{w,t}^*] = \\beta_t \\mathrm{E}_{t-1}[R_{w,t}^*] \\quad \\text{(Eq. 1)}\n```\nThis is implemented empirically with a bivariate GARCH-M system. The equation for Japanese excess returns is:\n```latex\nR_{j,t}^{*} = \\gamma_{0j} + \\mu \\left( \\frac{h_{jw,t}}{h_{w,t}} \\right) \\mathrm{E}_{t-1}[R_{w,t}^*] + \\varepsilon_{j,t} \\quad \\text{(Eq. 2)}\n```\n\n**Table 1: Parametric Model Estimates (Japanese Equity Equation)**\n\n| Parameter | Coefficient | Robust Std. Error |\n| :--- | :--- | :--- |\n| Intercept (`γ₀ⱼ`) | 0.0068 | (0.0047) |\n| Risk Premium (`μ`) | 0.8251 | (0.5186) |\n\n---\n\n### Question\n\nAccording to the theoretical asset pricing model in Eq. (1), the empirical model in Eq. (2) should satisfy the joint null hypothesis `H₀: (γ₀ⱼ, μ) = (0, 1)`. Based on the results in Table 1, what is the correct conclusion from testing this hypothesis at the 5% significance level (critical t-value ≈ ±1.96)?",
    "Options": {
      "A": "Reject `H₀: γ₀ⱼ = 0` but fail to reject `H₀: μ = 1`, indicating a significant alpha.",
      "B": "Fail to reject both `H₀: γ₀ⱼ = 0` and `H₀: μ = 1`, indicating the model is consistent with theory.",
      "C": "Fail to reject `H₀: γ₀ⱼ = 0` but reject `H₀: μ = 1`, indicating the price of risk is misspecified.",
      "D": "Reject both `H₀: γ₀ⱼ = 0` and `H₀: μ = 1`, indicating the model is inconsistent with theory."
    },
    "Answer": "B",
    "pi_justification": "Assesses the ability to connect asset pricing theory to empirical model restrictions and perform standard t-tests. This is a Computational Judgment item. The correct option (B) requires correctly calculating two t-statistics: t_γ = (0.0068-0)/0.0047 ≈ 1.45 and t_μ = (0.8251-1)/0.5186 ≈ -0.34, and concluding that both are insignificant. Distractors A, C, and D represent the three other possible outcomes of the joint test, isolating errors in testing the intercept, the risk premium coefficient, or both.",
    "qid": "387",
    "model_evaluation_result": {
      "model_raw_response": "B) Fail\tto\treject\tboth\tH₀:\tγ₀⵼\t=\t0\tand\tH₀: μ\t=\t1,\tindicating\tthe\tmodel\tis\tconsistent\twith\ttheory.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 388,
    "Question": "### Background\n\n**Research Question.** This problem examines the statistical and economic significance of a parametric model for time-varying risk premia in asset returns.\n\n**Setting.** The analysis uses a bivariate GARCH-in-Mean (GARCH-M) specification for Japanese and World equity index excess returns. The model's validity is assessed through both specification tests (Likelihood Ratio test) and measures of fit (R-squared).\n\n**Variables and Parameters.**\n*   `R_{j,t}^*`: Excess return on the Japanese equity portfolio at time `t`.\n*   `μ`: The coefficient on the conditional risk premium term.\n\n---\n\n### Data / Model Specification\n\nThe empirical model for Japanese excess returns is:\n```latex\nR_{j,t}^{*} = \\gamma_{0j} + \\mu \\left( \\frac{h_{jw,t}}{h_{w,t}} \\right) \\mathrm{E}_{t-1}[R_{w,t}^*] + \\varepsilon_{j,t} \\quad \\text{(Eq. 1)}\n```\n\n**Table 1: Evidence on Fit and Risk Premia for the Japanese Return Equation**\n\n| Statistic | Value |\n| :--- | :--- |\n| Total Sum of Squares (`R*ⱼ`) | 0.70809 |\n| Residual Sum of Squares (`R*ⱼ`) | 0.63146 |\n| LR Test (`μ=0`) | 15.94 |\n| p-value for LR Test | (0.000) |\n\n---\n\n### Question\n\nTable 1 provides a Likelihood Ratio (LR) test for the significance of the risk premium (`μ=0`) and data to calculate the model's in-sample R-squared for Japanese excess returns (`R*ⱼ`). Which statement best synthesizes these two pieces of evidence?",
    "Options": {
      "A": "The risk premium is statistically significant and explains the vast majority (over 89%) of return variation.",
      "B": "The risk premium is statistically insignificant (p-value ≈ 0), and the model's low explanatory power of about 10.8% confirms this.",
      "C": "The risk premium is statistically significant (p-value ≈ 0), but its economic significance is modest, as the model explains only about 10.8% of the return variation.",
      "D": "Because the risk premium is highly statistically significant, the model's R-squared of about 10.8% must also be considered economically large and important."
    },
    "Answer": "C",
    "pi_justification": "Assesses the ability to distinguish between statistical significance (from a p-value) and economic significance (from R-squared). This is a Computational Judgment item. The correct option (C) requires correctly interpreting the p-value as indicating high statistical significance while also correctly calculating R² = 1 - (0.63146 / 0.70809) ≈ 10.8% and judging it as modest. Distractor A represents a calculation error (using RSS/TSS for R²). Distractor B misinterprets the p-value. Distractor D represents the classic conceptual fallacy of equating statistical significance with large explanatory power.",
    "qid": "388",
    "model_evaluation_result": {
      "model_raw_response": "C) The-risk premium is statistically-significant-p-value-approximately-0-but-its-economic-significance-is-modest-as-the-model-explains-only-about-10.8-of-the-return-variation.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 389,
    "Question": "### Background\n\n**Research Question.** This problem evaluates whether a parsimonious, theory-driven parametric model is superior to a flexible, data-driven nonparametric model for capturing the forecastable structure of asset returns, with a focus on diagnosing 'overfitting'.\n\n**Setting.** The paper compares its primary parametric GARCH-M model against a nonparametric kernel regression model using both in-sample diagnostics (BDS test) and an out-of-sample forecasting competition.\n\n**Variables and Parameters.**\n*   `BDS Statistic`: A test for the null hypothesis that a series is independently and identically distributed (i.i.d.). A value greater than 1.96 is significant at the 5% level.\n*   `MSE`: Mean Squared Error of out-of-sample forecasts.\n*   `R²`: R-squared from a regression of actual returns on forecasts.\n\n---\n\n### Data / Model Specification\n\n**Table 1: In-Sample BDS Statistics for Model Residuals**\n\n| Embedding Dimension | Raw Series | Parametric Residuals | Kernel Residuals |\n| :--- | :--- | :--- | :--- |\n| 5 | 2.0378 | 0.2290 | -0.1170 |\n| 7 | 2.6651 | 1.1762 | 0.2056 |\n\n**Table 2: Out-of-Sample Forecast Evaluation**\n\n| Model | R² | MSE |\n| :--- | :--- | :--- |\n| Kernel | 0.0428 | 0.0056 |\n| Parametric | 0.0589 | 0.0050 |\n\n---\n\n### Question\n\nTable 1 shows that the residuals from both the parametric and kernel models appear to be i.i.d. in-sample (BDS statistics are insignificant). Table 2 compares their out-of-sample forecast performance. What is the main conclusion from this combined evidence?",
    "Options": {
      "A": "The kernel model is superior because its in-sample residuals pass the BDS test, indicating a better fit to the data's structure.",
      "B": "Both models are equally adequate because their in-sample residuals pass the BDS test and their out-of-sample performance metrics are very close.",
      "C": "The kernel model is superior because its out-of-sample MSE is higher, indicating it captured more of the return variance.",
      "D": "The parametric model is superior because its better out-of-sample performance (lower MSE, higher R²) suggests the kernel model's good in-sample fit was due to overfitting."
    },
    "Answer": "D",
    "pi_justification": "Assesses the ability to synthesize in-sample and out-of-sample evidence to correctly diagnose overfitting. This is a Scenario Application item. The correct option (D) properly concludes that out-of-sample superiority (lower MSE, higher R²) is the decisive factor and correctly attributes the kernel model's poorer performance to overfitting. Distractor A incorrectly prioritizes in-sample evidence. Distractor B fails to make a decisive choice based on the clear OOS advantage. Distractor C misinterprets MSE (lower is better).",
    "qid": "389",
    "model_evaluation_result": {
      "model_raw_response": "D) The parameteric model is superior... \n\nOutput: D",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 390,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the quality of forecasts from a parametric and a nonparametric model, specifically testing for forecast unbiasedness.\n\n**Setting.** The paper generates 30 one-step-ahead out-of-sample forecasts from both models and evaluates them by regressing the actual realized returns on the forecasts using the model: `Actual Return_t = a + b * Forecast_t + error_t`.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Out-of-Sample Forecast Evaluation**\n\n| Model | Constant (`a`) | Forecast (`b`) | R² | MSE |\n| :--- | :--- | :--- | :--- | :--- |\n| | | (Std. Error) | | |\n| Kernel | 0.0215 | 1.2493 | 0.0428 | 0.0056 |\n| | (0.0148) | (1.1160) | | |\n| Parametric | -0.0089 | 1.5370 | 0.0589 | 0.0050 |\n| | (0.0311) | (1.1614) | | |\n\n---\n\n### Question\n\nA common test for forecast unbiasedness checks if the slope coefficient `b` in the evaluation regression is equal to 1. Using the results in Table 1, what can be concluded about the slope bias of the two models at the 5% significance level (critical t-value ≈ ±1.96)?",
    "Options": {
      "A": "Neither the kernel model's slope (t ≈ 0.22) nor the parametric model's slope (t ≈ 0.46) is statistically different from 1.",
      "B": "Both the kernel model's slope and the parametric model's slope are statistically different from 1, indicating both forecasts are biased.",
      "C": "The kernel model's slope is not statistically different from 1, but the parametric model's slope is, indicating the parametric forecast is biased.",
      "D": "The parametric model's slope is not statistically different from 1, but the kernel model's slope is, indicating the kernel forecast is biased."
    },
    "Answer": "A",
    "pi_justification": "Assesses the ability to perform a t-test for a specific null hypothesis (`H₀: b=1`) and interpret the result correctly. This is a Computational Judgment item. The correct option (A) requires correctly calculating the t-statistics for both models [Kernel: (1.2493-1)/1.1160 ≈ 0.22; Parametric: (1.5370-1)/1.1614 ≈ 0.46] and concluding that neither is significant. Distractors B, C, and D represent the other three possible outcomes, catching either calculation errors or misinterpretations of significance for one or both models.",
    "qid": "390",
    "model_evaluation_result": {
      "model_raw_response": "C) The parametic model's slope. (t ≈ -0.46, critical t-value ≈ -1.96)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 391,
    "Question": "### Background\n\nIn a strategic redistricting game, a party's equilibrium strategy is a **`p`-segregation plan**. This plan divides voters based on a cutoff characteristic `z`. Voters in districts with characteristics `ω ≤ z` are 'packed' (left as segregated as possible), while voters in districts with `ω > z` are 'cracked' (pooled into uniform favorable districts with an average characteristic `ω_z`).\n\n### Data / Model Specification\n\nThe probability that Party 1 wins a district with effective characteristic `θ = ω+s` is given by the District Outcome Function (DOF), `π(θ)`. The optimal choice of the cutoff `z` is determined by a first-order condition that must hold at the critical state `s*` where the election is tied:\n\n```latex\n\\pi'(\\omega_{z}+s^{*})(\\omega_{z}-z) = \\pi(\\omega_{z}+s^{*})-\\pi(z+s^{*}) \\quad \\text{(Eq. (1))}\n```\n\nThis can be rearranged as:\n\n```latex\n\\pi'(\\omega_{z}+s^{*}) = \\frac{\\pi(\\omega_{z}+s^{*})-\\pi(z+s^{*})}{\\omega_{z}-z}\n```\n\n### Question\n\nWhat is the correct graphical interpretation of the condition in Eq. (1) on a plot of the District Outcome Function `π(θ)` versus the effective district characteristic `θ`?",
    "Options": {
      "A": "The slope of the tangent line to the `π` curve at the favorable district point (`θ = ω_z+s*`) is equal to the slope of the secant line connecting the marginal packed district point (`θ = z+s*`) and the favorable district point.",
      "B": "The area under the `π` curve between `z+s*` and `ω_z+s*` is maximized.",
      "C": "The `π` curve is at its inflection point at `θ = ω_z+s*`, where the marginal return to increasing district favorability is highest.",
      "D": "The vertical distance between the `π` curve at `ω_z+s*` and the tangent line at `z+s*` is maximized."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to translate a complex mathematical first-order condition into its precise graphical representation, a key skill in microeconomic theory. It uses a Computational Judgment strategy. The correct answer (A) is the exact geometric interpretation. Distractors are designed based on common misinterpretations of optimization conditions: (B) confuses the marginal condition with an integral (total area) condition, (C) confuses the optimal point with the point of maximum marginal returns (inflection point), and (D) proposes an alternative, incorrect geometric optimization.",
    "qid": "391",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\nNote: Please ensure that your response only includes the chosen letter without any additional text or explanations.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 392,
    "Question": "### Background\n\nIn a strategic redistricting game, the equilibrium strategy is a **`p`-segregation plan**. This strategy involves maximally segregating unfavorable districts ('packing' the opposition) and pooling favorable voters into uniform districts ('cracking' one's own supporters).\n\n### Data / Model Specification\n\nThe probability that a party wins a district with effective characteristic `θ` (where higher `θ` is more favorable) is given by the District Outcome Function (DOF), `π(θ)`. A key feature of the DOF is that it is S-shaped: it is convex for low values of `θ` (where the party is the underdog) and concave for high values of `θ` (where the party is the favorite).\n\n### Question\n\nWhat property of the District Outcome Function `π(·)` makes the 'pack and crack' `p`-segregation strategy optimal for maximizing the number of seats won?",
    "Options": {
      "A": "The function is convex where the party is the underdog and concave where it is the favorite, making it optimal to use extreme voter distributions in the convex region and uniform distributions in the concave region.",
      "B": "The function is symmetric around `θ=1/2`, which ensures that packing and cracking have equal and opposite effects on seat share.",
      "C": "The function is strictly increasing, meaning that a higher district characteristic always leads to a higher probability of winning.",
      "D": "The function is continuous, which allows for a unique critical state `s*` to be found where the election is tied."
    },
    "Answer": "A",
    "pi_justification": "This item tests the core economic intuition behind the paper's main result: why 'packing and cracking' is the optimal response to an S-shaped returns function. It uses a Reverse-Reasoning strategy (given the strategy, what is the cause?). The correct answer (A) correctly links convexity to packing and concavity to cracking. Distractors are designed to be 'True but Irrelevant': (B), (C), and (D) all state correct properties of the `π` function described in the paper, but none of them provide the causal reason for the optimality of the `p`-segregation strategy.",
    "qid": "392",
    "model_evaluation_result": {
      "model_raw_response": "C) The funcion is strictly increasing. \n\nNote: I've carefully analyzed the given information and determined that the correct answer is C) The function being strictly increasing means that a higher `θ´ leads to a greater chance of winning, which makes sense in the context of the 'pack' and 'crack' strategy.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 393,
    "Question": "### Background\n\n**Research Question.** This problem investigates the fundamental identification and dynamic consistency failures of the standard α-maxmin expected utility (α-MEU) model. It explores how the inability to uniquely identify the model's parameters from choice behavior leads to ambiguity in predicting choices after new information is received.\n\n**Setting.** A decision-maker's (DM's) preferences `≿` over acts are represented by an α-MEU functional. The analysis focuses on two issues: (1) multiple combinations of ambiguity attitude (`α`) and perceived ambiguity (`P`) can generate the same `≿`, and (2) applying a standard belief updating rule to these equivalent representations can lead to different, conflicting conditional preferences.\n\n### Data / Model Specification\n\nThe subjective preference `≿` is represented by the α-MEU functional `V(f)`:\n```latex\nV(f) = \\alpha \\min_{\\mu \\in P} \\mathbb{E}_{\\mu}[u(f)] + (1-\\alpha) \\max_{\\mu \\in P} \\mathbb{E}_{\\mu}[u(f)]\n```\n**Proposition 1** states that two α-MEU representations, `(u, P_1, α_1)` and `(u, P_2, α_2)` with `α_i > 1/2`, represent the same preference if and only if `P_1` is the `γ`-expansion of `P_2`, where:\n```latex\n\\gamma = \\frac{\\alpha_1 + \\alpha_2 - 1}{2\\alpha_1 - 1} \\quad \\text{(Eq. (1))}\n```\nThe **`γ`-expansion** of a set `P` is defined as `Q = \\gamma P + (1-\\gamma)P := \\{\\gamma\\nu + (1-\\gamma)\\nu' : \\nu, \\nu' \\in P\\}`.\n\n**Example 1 Data:** Consider two α-MEU representations over `S={1,2,3}` that are stated to represent the same ex-ante preference `≿`.\n\n| Representation | Weight (`α`) | Set of Priors (`P`)                                                                                             |\n| :------------- | :----------: | :-------------------------------------------------------------------------------------------------------------- |\n| Rep 1          | `α_1 = 3/4`  | `P_1 = co\\{(5/6, 1/12, 1/12), (1/6, 5/12, 5/12)\\} `                                                              |\n| Rep 2          | `α_2 = 1`    | `P_2 = co\\{(2/3, 1/6, 1/6), (1/3, 1/3, 1/3)\\} `                                                                  |\n\n### Question\n\nUsing the data provided, calculate the `γ`-expansion factor that must relate `P_1` and `P_2` for them to be observationally equivalent, and identify the correct relationship.",
    "Options": {
      "A": "`γ = 3/2`, and `P_1` is the `γ`-expansion of `P_2`.",
      "B": "`γ = 3/4`, and `P_1` is the `γ`-expansion of `P_2`.",
      "C": "`γ = 3/2`, and `P_2` is the `γ`-expansion of `P_1`.",
      "D": "The representations are not equivalent because `α_1 ≠ α_2` and `P_1 ≠ P_2`."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the user's ability to perform a key calculation from the paper (the γ-expansion factor) and correctly interpret its meaning. It uses a Computational Judgment strategy. Distractors are designed based on common errors: using the wrong formula for γ (Formula Misuse), reversing the relationship between the sets (Conceptual Opposite), and a superficial misunderstanding of observational equivalence (Conceptual Error).",
    "qid": "393",
    "model_evaluation_result": {
      "model_raw_response": "B) γ = 3 / 4, and P1 is the γ-expansion of P2.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 394,
    "Question": "### Background\n\n**Research Question.** This problem investigates the fundamental identification and dynamic consistency failures of the standard α-maxmin expected utility (α-MEU) model. It explores how the inability to uniquely identify the model's parameters from choice behavior leads to ambiguity in predicting choices after new information is received.\n\n**Setting.** A decision-maker's (DM's) preferences `≿` over acts are represented by an α-MEU functional. The analysis focuses on two issues: (1) multiple combinations of ambiguity attitude (`α`) and perceived ambiguity (`P`) can generate the same `≿`, and (2) applying a standard belief updating rule to these equivalent representations can lead to different, conflicting conditional preferences.\n\n### Data / Model Specification\n\n**Prior-by-prior Bayesian updating** is a rule to form a conditional preference `≿_E` after learning an event `E ⊆ S`. The new set of priors `P^E` is formed by updating each prior `μ ∈ P` according to Bayes' rule:\n```latex\nP^{E} := \\{\\mu^{E} : \\mu \\in P\\}, \\quad \\text{where } \\mu^{E}(F) := \\frac{\\mu(E \\cap F)}{\\mu(E)}\n```\n**Example 1 Data:** Consider two α-MEU representations over `S={1,2,3}`.\n\n| Representation | Weight (`α`) | Set of Priors (`P`)                                                                                             |\n| :------------- | :----------: | :-------------------------------------------------------------------------------------------------------------- |\n| Rep 1          | `α_1 = 3/4`  | `P_1 = co\\{(5/6, 1/12, 1/12), (1/6, 5/12, 5/12)\\} `                                                              |\n| Rep 2          | `α_2 = 1`    | `P_2 = co\\{(2/3, 1/6, 1/6), (1/3, 1/3, 1/3)\\} `                                                                  |\n\n### Question\n\nConsider the event `E = {1, 2}`. What is the updated set of priors `P_2^E` resulting from prior-by-prior Bayesian updating?",
    "Options": {
      "A": "`co{(4/5, 1/5, 0), (1/2, 1/2, 0)}`",
      "B": "`co{(10/11, 1/11, 0), (2/7, 5/7, 0)}`",
      "C": "`co{(2/3, 1/6, 0), (1/3, 1/3, 0)}`",
      "D": "`co{(4/5, 1/5, 0), (1/3, 2/3, 0)}`"
    },
    "Answer": "A",
    "pi_justification": "This item tests the procedural knowledge of applying prior-by-prior Bayesian updating, a core mechanism discussed in the paper. It uses a Computational Judgment strategy. Distractors are based on plausible errors: using the wrong initial set of priors (Conceptual Error), failing to normalize the updated vectors (Step-Omission Error), and a simple calculation mistake.",
    "qid": "394",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: I've only provided the answer as per the output format specified.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 395,
    "Question": "### Background\n\n**Research Question.** This problem investigates the fundamental identification and dynamic consistency failures of the standard α-maxmin expected utility (α-MEU) model.\n\n### Data / Model Specification\n\nTwo α-MEU representations, Rep 1 (`α_1 = 3/4`, `P_1`) and Rep 2 (`α_2 = 1`, `P_2`), are observationally equivalent ex-ante. The relationship between the prior sets is that `P_1` is the `γ`-expansion of `P_2` with `γ=3/2`.\n\nAfter learning event `E = {1, 2}`, the prior sets are updated using prior-by-prior Bayesian updating:\n- `P_1^E = co{(10/11, 1/11, 0), (2/7, 5/7, 0)}`\n- `P_2^E = co{(4/5, 1/5, 0), (1/2, 1/2, 0)}`\n\nFurther calculation shows that the `γ`-expansion of `P_2^E` with `γ=3/2` yields the set `Q = co{(19/20, 1/20, 0), (7/20, 13/20, 0)}`.\n\n### Question\n\nGiven that the calculated `γ`-expansion of `P_2^E` is not equal to `P_1^E`, what is the primary implication of this finding for the α-MEU model?",
    "Options": {
      "A": "The prior-by-prior updating rule is ill-defined, as equivalent ex-ante representations lead to different ex-post preferences.",
      "B": "The `γ`-expansion relationship is not preserved under Bayesian updating, but the conditional preferences remain the same.",
      "C": "The initial representations must not have been equivalent, because a well-defined rule should preserve equivalence.",
      "D": "Bayesian updating is preserved, but the `γ`-expansion formula from Proposition 1 does not apply to conditional preferences."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the user's ability to synthesize the results of the previous calculations to grasp the paper's central critique of the standard α-MEU model. It uses a Reverse-Reasoning strategy. Distractors represent common logical fallacies: concluding the opposite (Conceptual Opposite), missing the key implication (True but Irrelevant), or misattributing the cause of the problem (Misinterpretation).",
    "qid": "395",
    "model_evaluation_result": {
      "model_raw_response": "A) The primary implication is that the prior-by-p Prior-by-prior Bayes rule is ill defined, as equivalent Ex ante representations lead to Different Ex post preferences.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 396,
    "Question": "### Background\n\n**Research Question:** This problem examines the implications of the paper's main theorem for the theory of two-stage budgeting, showing how simultaneous direct and indirect weak separability leads to a modified version of the Strotz-Gorman aggregation framework.\n\n**Setting:** We consider a region of the normalized price space, `y ∈ B_s`, where the conditions of the main theorem hold. Specifically, the indirect sub-utility `v^s` is not quasi-homothetic, but all other `v^t` (`t ≠ s`) are homogeneous of degree one and the aggregator `G` is `C_s H_0`.\n\n### Data / Model Specification\n\nIn the region `y ∈ B_s`, the `C_s H_0` property of the aggregator `G` implies the following relationship for `ν(y) = ∑ v_i y_i`:\n```latex\n\\nu(y) = G_s(w) [\\nu^s(y^s) + \\eta^s(v^s(y^s))] \\quad \\text{(Eq. 1)}\n```\nwhere `v^s(y^s)` is the indirect sub-utility for group `s`, `ν^s(y^s) = ∑_{i∈N_s} v_i^s y_i`, and `η^s` is a function of `v^s`.\n\nThe first-order conditions for the dual problem for goods `i ∈ N_s` are:\n```latex\nv_i(y) = G_s(w) v_i^s(y^s) = \\lambda x_i \\quad \\text{(Eq. 2)}\n```\nwhere `λ = ν(y)` at the optimum. Expenditure on group `s` is `I_s = I y^s x^s`.\n\nThe paper shows that these conditions lead to the following expenditure equation for `y ∈ B_s`:\n```latex\nI_s = \\frac{\\nu^s(y^s)}{\\nu^s(y^s) + \\eta^s(v^s(y^s))} I \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nIn the derived expenditure equation for group `s` (Eq. 3), the budget share is determined by two distinct functions of the group's normalized prices, `v^s(y^s)` and `ν^s(y^s)`. What are the correct economic interpretations of these two functions?",
    "Options": {
      "A": "`v^s(y^s)` is a welfare index representing the sub-utility level, while `ν^s(y^s)` is a price sensitivity index reflecting the weighted average price level.",
      "B": "`v^s(y^s)` is a price sensitivity index reflecting the weighted average price level, while `ν^s(y^s)` is a welfare index representing the sub-utility level.",
      "C": "Both `v^s(y^s)` and `ν^s(y^s)` are alternative forms of a Klein price aggregator, making the choice between them arbitrary for budgeting purposes.",
      "D": "`v^s(y^s)` measures the income elasticity of the group, while `ν^s(y^s)` measures the substitution effect within the group."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the student's ability to interpret the economic meaning of the key terms in a derived formula, a crucial skill for linking theory to application. It uses a conceptual interpretation strategy. The distractors are designed based on common errors: B is a direct conceptual swap; C confuses the result with a standard concept (Klein Aggregator); D misapplies other valid economic concepts to the terms.",
    "qid": "396",
    "model_evaluation_result": {
      "model_raw_response": "B) v^s(y) is a price-sensitivity index reflecting the weighed average price level while ν^s(y)s is a welfare-index representing the subutility level.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 397,
    "Question": "### Background\n\n**Research Question:** This problem examines the implications of the paper's main theorem for the theory of two-stage budgeting, showing how simultaneous direct and indirect weak separability leads to a modified version of the Strotz-Gorman aggregation framework.\n\n**Setting:** We consider a region of the normalized price space, `y ∈ B_s`, where the conditions of the main theorem hold. Specifically, the indirect sub-utility `v^s` is not quasi-homothetic, but all other `v^t` (`t ≠ s`) are homogeneous of degree one and the aggregator `G` is `C_s H_0`.\n\n### Data / Model Specification\n\nIn the region `y ∈ B_s`, the `C_s H_0` property of the aggregator `G` implies the following relationship for `ν(y) = ∑ v_i y_i`:\n```latex\n\\nu(y) = G_s(w) [\\nu^s(y^s) + \\eta^s(v^s(y^s))] \\quad \\text{(Eq. 1)}\n```\nwhere `v^s(y^s)` is the indirect sub-utility for group `s`, `ν^s(y^s) = ∑_{i∈N_s} v_i^s y_i`, and `η^s` is a function of `v^s`.\n\nThe first-order conditions for the dual problem for goods `i ∈ N_s` are:\n```latex\nv_i(y) = G_s(w) v_i^s(y^s) = \\lambda x_i \\quad \\text{(Eq. 2)}\n```\nwhere `λ = ν(y)` at the optimum. Expenditure on group `s` is `I_s = I y^s x^s`.\n\nThe paper shows that these conditions lead to the following expenditure equation for `y ∈ B_s`:\n```latex\nI_s = \\frac{\\nu^s(y^s)}{\\nu^s(y^s) + \\eta^s(v^s(y^s))} I \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nThe paper demonstrates that when preferences are both directly and indirectly weakly separable, the conditions for standard Strotz-Gorman 'perfect aggregation' are not met in the region `y ∈ B_s`. Based on the expenditure equation for group `s` (Eq. 3), what is the primary reason for this deviation?",
    "Options": {
      "A": "The budget share for group `s` depends on two separate functions of its prices (`v^s` and `ν^s`), not a single price index, which necessitates an additional differential price index (`σ_s`) for local aggregation.",
      "B": "The expenditure `I_s` is no longer a function of total income `I`, violating the basic premise of two-stage budgeting.",
      "C": "The aggregator function `G` becomes non-monotonic in the region `B_s`, which invalidates the first-order conditions.",
      "D": "All sub-utility functions `v^t` (for `t=1,...,S`) become quasi-homothetic, leading to simple Klein aggregation, which is a special case of, not a deviation from, Gorman aggregation."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the ability to identify the core reason for a theoretical result's departure from a standard benchmark. It uses a Reverse-Reasoning strategy. Distractor B is a direct misreading of the formula. Distractor C introduces a claim that contradicts the model's core assumptions. Distractor D correctly describes the behavior in a different region of the price space (`B_0`), making it a sophisticated 'true but irrelevant' option.",
    "qid": "397",
    "model_evaluation_result": {
      "model_raw_response": "A) The primary reason for the deviation from standard Strotzman-Gorman 'perfection aggregation' is that the budget share for the group `s' depends on two different functions of its price (`v^S` and `v^S(y^S)`), not a solitary price index, necessitating an additional differential pricing index (`σ_S`) for local accumulation.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 398,
    "Question": "### Background\n\nHow can locality priorities be respected in a multidimensional matching market where classical stability may not exist? The paper introduces a novel solution concept, interference-freeness, and a mechanism called Knapsack Deferred Acceptance (KDA) to find a family-optimal interference-free matching. However, KDA is not strategy-proof.\n\n---\n\n### Data / Model Specification\n\nConsider the following setup based on the paper's Example 1:\n- **Families & Sizes:** `f1`(size 1), `f2`(size 2), `f3`(size 1), `f4`(size 1)\n- **Localities & Capacities:** `ℓ1`(cap 2), `ℓ2`(cap 1), `ℓ3`(cap 2), `ℓ4`(cap 5)\n- **True Preferences of `f2`:** `ℓ1, ℓ3, ℓ4, ...`\n- **Priorities at `ℓ1`:** `f1 triangleright f2 triangleright f3 triangleright f4`\n- **Priorities at `ℓ2`:** `f3 triangleright f1 ...`\n- **Priorities at `ℓ3`:** `f4 triangleright f2 ...`\n\nUnder truthful reporting, the KDA algorithm matches `f2` to `ℓ4`.\n\n---\n\n### Question\n\nSuppose family `f2` misreports its preferences as `≻'_{f2}: ℓ3, ℓ1, ℓ4, ...` while all other families report their true preferences (`f1: ℓ2,ℓ1..`, `f3: ℓ1,ℓ2..`, `f4: ℓ1,ℓ3..`). If the Knapsack Deferred Acceptance (KDA) algorithm is run, what is the final matching for family `f2`?",
    "Options": {
      "A": "`ℓ1`",
      "B": "`ℓ4`",
      "C": "`ℓ3`",
      "D": "Unmatched"
    },
    "Answer": "C",
    "pi_justification": "This item assesses the procedural understanding of the KDA algorithm. It uses a Computational Judgment strategy, requiring the user to trace the algorithm's steps to determine the outcome of a strategic manipulation. The correct answer is the outcome of the successful manipulation. Distractors are designed based on common errors: (B) is the outcome with truthful preferences, (A) is a desirable but unattainable outcome for the family, and (D) represents a plausible backfire scenario where the manipulation fails.",
    "qid": "398",
    "model_evaluation_result": {
      "model_raw_response": "C) ℓ3\n\nNote: I've carefully analyzed the given scenario and options, and determined that the correct answer is C) ℓ³.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 399,
    "Question": "### Background\n\nIn the matching model, a locality's choice function (which families it accepts from a set of proposers) is crucial for determining if a mechanism is strategy-proof. The Knapsack Deferred Acceptance (KDA) algorithm's choice function can violate cardinal monotonicity, a key property for strategy-proofness.\n\n---\n\n### Data / Model Specification\n\nConsider a locality `ℓ` with capacity `κ=2`. It has priorities over three families: `f_A triangleright f_B triangleright f_C`. The family sizes are `ν(f_A)=2`, `ν(f_B)=1`, and `ν(f_C)=1`.\n\n---\n\n### Question\n\nA violation of cardinal monotonicity occurs if the locality accepts fewer families from a set of proposers `P_B` than from a strict subset `P_A ⊂ P_B`. Which of the following scenarios demonstrates this violation for locality `ℓ`?",
    "Options": {
      "A": "When `P_A = {f_B, f_C}`, the locality accepts 2 families. When `P_B = {f_A, f_B, f_C}`, it accepts 1 family.",
      "B": "When `P_A = {f_A}`, the locality accepts 1 family. When `P_B = {f_A, f_B}`, it accepts 1 family.",
      "C": "When `P_A = {f_C}`, the locality accepts 1 family. When `P_B = {f_B, f_C}`, it accepts 2 families.",
      "D": "When `P_A = {f_B}`, the locality accepts 1 family. When `P_B = {f_A, f_C}`, it accepts 1 family."
    },
    "Answer": "A",
    "pi_justification": "This item tests the deep understanding of cardinal monotonicity, a key condition for strategy-proofness. It uses a Scenario Application strategy. The user must apply the KDA acceptance rule to different sets of proposers to identify the specific case that constitutes a violation. Distractors represent common misunderstandings: (B) and (C) are valid applications of the rule that do not result in a violation, while (D) uses a distractor set `P_B` that is not a superset of `P_A`, failing a precondition for the test.",
    "qid": "399",
    "model_evaluation_result": {
      "model_raw_response": "C) When P_A = { f_C }, the locality accepts { f_C }. When P_B = { f_B, F_C }, it accepts { f_B } and { f_C }",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 400,
    "Question": "### Background\n\nThe Knapsack Deferred Acceptance (KDA) mechanism is not strategy-proof because its acceptance rule can be myopic: a family's acceptance in an early round may be revoked in a later round if a higher-priority, large family applies. The Threshold Knapsack Deferred Acceptance (TKDA) mechanism modifies KDA's rule to restore strategy-proofness.\n\n---\n\n### Question\n\nWhat is the core intuition behind TKDA's 'threshold' system that makes it strategy-proof?",
    "Options": {
      "A": "It performs 'precautionary rejections' of families whose tentative acceptance is not robust to the potential future arrival of any higher-priority proposers.",
      "B": "It reserves a portion of each locality's capacity to ensure that high-priority families who apply in later rounds can always be accommodated.",
      "C": "It prioritizes matching smaller families first to maximize the total number of matched families, thereby increasing overall efficiency.",
      "D": "It calculates a family's 'manipulation potential' and rejects any family whose preference report seems strategically designed to alter the outcome."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the conceptual understanding of the theoretical fix for KDA's manipulability. The correct answer (A) captures the core intuition of ensuring 'robust' acceptances. Distractors are designed to be plausible but incorrect explanations: (B) suggests a different kind of conservative mechanism (capacity reservation), (C) is a conceptually opposite efficiency argument, and (D) misinterprets strategy-proofness as manipulation detection.",
    "qid": "400",
    "model_evaluation_result": {
      "model_raw_response": "A) It perfor... (no) m...",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 401,
    "Question": "### Background\n\n**Research Question.** This problem investigates whether a system of intergovernmental transfers, specifically matching grants, can correct the inefficiencies of decentralized public good provision and achieve the first-best (socially optimal) outcome in a union of heterogeneous countries.\n\n**Setting / Institutional Environment.** The union establishes a matching grant system. For each unit of public good `g_i` a country provides, the union provides a matching subsidy `s`. This is financed by a tax on each country `i` equal to `T_i = s \\cdot \\bar{g}_{-i}`, where `\\bar{g}_{-i}` is the average spending of all other countries. The subsidy rate `s` is chosen by majority vote, so the median voter's preference `α_m` is decisive.\n\n### Data / Model Specification\n\nThe utility from the public good is logarithmic, `H(x) = ln(x)`. The first-best optimal subsidy rate, which aligns private and social incentives, is `s^* = β(N-1)`. The politico-economic equilibrium subsidy `s_m` chosen by the median voter is given by the expression in Eq. (1):\n\n```latex\ns_{m} = \\cfrac{1+\\beta(N-1)}{1+\\cfrac{N}{(1-\\beta)(N-1)}\\left(\\cfrac{\\bar{\\alpha}-\\alpha_{m}}{\\alpha_{m}}\\right)} - 1 \n```\n\nwhere `α_m` is the preference parameter of the median voter and `\\bar{\\alpha} = (1/N)\\sum_i α_i` is the average preference parameter in the union.\n\n### Question\n\nConsider a union where the distribution of preferences is skewed, such that the median preference is less than the mean preference (`α_m < \\bar{\\alpha}`). Which of the following statements correctly describes the resulting political equilibrium subsidy `s_m`?",
    "Options": {
      "A": "The equilibrium subsidy `s_m` will be inefficiently low (`s_m < s^*`).",
      "B": "The equilibrium subsidy `s_m` will be inefficiently high (`s_m > s^*`).",
      "C": "The equilibrium subsidy `s_m` will be efficient (`s_m = s^*`).",
      "D": "The equilibrium subsidy `s_m` will be negative, as the median voter seeks to tax public spending."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests the ability to analyze the direction of political bias in a fiscal federalism model. Strategy: Computational Judgment, requiring the student to evaluate a formula under a specific condition (`α_m < \\bar{\\alpha}`). Distractor Logic: (B) Sign Error: results from incorrectly evaluating the denominator in Eq. (1). (C) Conceptual Error: reflects a misunderstanding that the median voter outcome is not always efficient. (D) Formula Misuse: an extreme misinterpretation of the model's logic.",
    "qid": "401",
    "model_evaluation_result": {
      "model_raw_response": "B) The equation (1) shows that when the median preference `αm` is less than average preference `\\bar{\\α}`, the term `(∂αm)/(αm)` is positive, which means the denominator in the expression is greater than 1. Therefore, the numerator `(1 + β(N- 1))` is divided by a larger value, resulting in a smaller value for `sm`. Since `s^` is the optimal subsidy rate which is greater than zero, `sm` being less than `s^´ implies that `sm` is inefficiently low.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 402,
    "Question": "### Background\n\n**Research Question.** This problem investigates whether a system of intergovernmental transfers, specifically matching grants, can correct the inefficiencies of decentralized public good provision and achieve the first-best (socially optimal) outcome in a union of heterogeneous countries.\n\n**Setting / Institutional Environment.** The union establishes a matching grant system. For each unit of public good `g_i` a country provides, the union provides a matching subsidy `s`. This is financed by a tax on each country `i` equal to `T_i = s \\cdot \\bar{g}_{-i}`, where `\\bar{g}_{-i}` is the average spending of all other countries. The subsidy rate `s` is chosen by majority vote, so the median voter's preference `α_m` is decisive.\n\n### Data / Model Specification\n\nThe utility from the public good is logarithmic, `H(x) = ln(x)`. The first-best optimal subsidy rate, which aligns private and social incentives, is `s^* = β(N-1)`. The politico-economic equilibrium subsidy `s_m` chosen by the median voter is given by the expression in Eq. (1):\n\n```latex\ns_{m} = \\cfrac{1+\\beta(N-1)}{1+\\cfrac{N}{(1-\\beta)(N-1)}\\left(\\cfrac{\\bar{\\alpha}-\\alpha_{m}}{\\alpha_{m}}\\right)} - 1 \n```\n\nwhere `α_m` is the preference parameter of the median voter and `\\bar{\\alpha} = (1/N)\\sum_i α_i` is the average preference parameter in the union.\n\n### Question\n\nIn a union where the median preference for public goods is less than the mean (`α_m < \\bar{\\alpha}`), the median voter strategically sets a subsidy `s_m` that is below the socially optimal level `s^*`. What is the primary political mechanism driving this inefficient outcome?",
    "Options": {
      "A": "The median voter sets a low subsidy to limit their own tax payments, which are inflated by the high spending of countries with above-average preferences.",
      "B": "The median voter sets a low subsidy to punish high-preference countries for not contributing enough to the union's budget.",
      "C": "The median voter has imperfect information about the optimal subsidy `s^*` and therefore makes a mistake.",
      "D": "The median voter is concerned that a high subsidy would lead to excessive centralization and a loss of national sovereignty."
    },
    "Answer": "A",
    "pi_justification": "Assessment Target: Tests understanding of the political economy mechanism behind policy distortions. Strategy: Reverse-Reasoning, asking for the cause of a given outcome. Distractor Logic: (B) Conceptual Opposite: incorrectly assigns a punitive motive. (C) Almost Right: attributes the outcome to a behavioral error (imperfect information) rather than strategic choice. (D) True but Irrelevant: imports a valid concern from other models in the paper that is not the direct mechanism here.",
    "qid": "402",
    "model_evaluation_result": {
      "model_raw_response": "C) The mediator has imperfect information regarding the optimal subsidy s* and therefore makes an error.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 403,
    "Question": "### Background\n\nThe authors model log maintenance spending (`ln M_irt`) with a levels model that includes an unobserved, time-invariant household-specific effect `α_i`:\n```latex\n\\ln M_{irt} = \\alpha_i + \\beta_P \\ln Y_{it}^P + \\beta_T \\ln Y_{it}^T + X_{it}\\delta + Z_{rt}\\gamma + \\eta_{it} \\quad \\text{(Eq. (1))}\n```\nTo eliminate `α_i`, the primary estimating equation is the first-differenced version of this model:\n```latex\n\\Delta\\ln M_{irt} = \\beta_P \\Delta\\ln Y_{it}^P + \\beta_T \\Delta\\ln Y_{it}^T + \\Delta X_{it}\\delta + \\Delta Z_{rt}\\gamma + \\Delta\\eta_{it} \\quad \\text{(Eq. (2))}\n```\n### Question\n\nThe authors transform the levels model (Eq. (1)) into the first-differenced model (Eq. (2)). What is the primary econometric problem this transformation is designed to solve, and what is the key assumption required for its success?",
    "Options": {
      "A": "To eliminate omitted variable bias from time-invariant household characteristics (`α_i`), assuming these characteristics are constant over time.",
      "B": "To correct for measurement error in the income variables, assuming the errors are classical (mean zero and uncorrelated with true values).",
      "C": "To resolve simultaneity between income and maintenance spending, assuming income shocks are exogenous to maintenance decisions.",
      "D": "To eliminate omitted variable bias from time-invariant household characteristics (`α_i`), assuming these characteristics are uncorrelated with income."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the fundamental purpose of a first-differencing (fixed effects) strategy. It uses a direct question format. The distractors are designed to pinpoint common confusions in econometrics: B and C name other types of endogeneity that first-differencing does not solve, while D correctly identifies the problem but states an incorrect assumption (the whole point of fixed effects is that `α_i` *can* be correlated with regressors).",
    "qid": "403",
    "model_evaluation_result": {
      "model_raw_response": "D) Toeliminate omitted variable biasfrom time-invarianthousehold characteristics (`αi`), assumingthese characteristicsare uncorrelatedwith income.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 404,
    "Question": "### Background\n\nAfter first-differencing, the estimated transitory income component, `\\Delta \\ln Y_{it}^T`, is still imperfect. The authors discuss two potential sources of error that create opposing biases in OLS estimates of the transitory income elasticity, `β_T`:\n1.  **Classical Measurement Error:** Random misreporting of income leads to attenuation bias (downward bias).\n2.  **Specification Error:** Misclassifying persistent permanent income shocks as transitory shocks leads to an upward bias, as maintenance spending is more responsive to permanent shocks.\n\nTo address these issues, the authors employ an instrumental variables (IV) strategy. The key results are summarized below.\n\n**Table 1. OLS and IV Estimates of `β_T`**\n| Estimator | `β_T` Estimate | Std. Error |\n| :--- | :--- | :--- |\n| OLS | 0.384** | (0.082) |\n| IV | 0.412** | (0.085) |\n*Notes: ** denotes significance at the 5% level.*\n\n### Question\n\nGiven the two potential sources of bias and the empirical results in Table 1, what can be inferred about the dominant source of bias in the OLS estimate of `β_T`?",
    "Options": {
      "A": "Since the IV estimate (0.412) is larger than the OLS estimate (0.384), it suggests the net bias in OLS was negative, meaning attenuation from classical measurement error was the dominant issue.",
      "B": "Since the IV estimate (0.412) is larger than the OLS estimate (0.384), it suggests the IV procedure corrected an upward bias, meaning specification error was the dominant issue.",
      "C": "The fact that both OLS and IV estimates are statistically significant and similar in magnitude implies that neither measurement error nor specification error is a serious concern.",
      "D": "The small difference between the OLS and IV estimates indicates that the upward bias from specification error and the downward bias from measurement error are of roughly equal magnitude, largely cancelling each other out."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to interpret the results of an IV strategy in the context of competing biases. It uses a Reverse-Reasoning strategy. The correct answer requires understanding that IV is designed to correct bias, and if the corrected estimate is larger, the original net bias must have been negative. Distractor B represents the most common logical error (reversing the inference). Distractor C draws an incorrect conclusion from significance. Distractor D is a plausible but less precise interpretation that ignores the direction of the correction.",
    "qid": "404",
    "model_evaluation_result": {
      "model_raw_response": "B) Since IV estimate (412) is greater than OLS estimate, it suggests the iv procedure corrected an upper bias, meaning the specification error was dominant.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 405,
    "Question": "### Background\n\nA central concern is that the instrumental variables (IV) estimate of the transitory income elasticity (`β_T`) might be upwardly biased. This could happen if the instruments are correlated with permanent income shocks that have been misclassified as transitory shocks (a form of specification error).\n\nTo address this, the authors conduct a robustness check: they re-estimate the model on a subsample of households with longer panel histories (at least 6 surveys vs. the baseline of at least 3). The statistical logic is that a longer panel allows for a more precise estimation of each household's permanent income trend, thereby reducing the misclassification (specification error) in the remaining transitory component.\n\nUpon performing this check, the authors find that the IV estimate of `β_T` actually *increases slightly* in the long-panel sample.\n\n### Question\n\nWhat is the most accurate conclusion from this finding?",
    "Options": {
      "A": "The result contradicts the hypothesis that the IV estimate is primarily inflated by specification error, thus strengthening the paper's main conclusion.",
      "B": "The result suggests that the IV strategy is flawed, because if it were working correctly, the estimate of `β_T` should have decreased after the reduction in specification error.",
      "C": "The increase in the `β_T` estimate indicates that longer panel histories must be associated with more severe classical measurement error, which the IV is also correcting.",
      "D": "The result confirms that specification error was a major upward bias, and its removal revealed an even larger underlying true elasticity."
    },
    "Answer": "A",
    "pi_justification": "This item assesses a deep understanding of a specific robustness check. It uses a Scenario Application strategy, where the 'scenario' is the logic and outcome of the test. The correct answer (A) requires following a two-step logical chain: (1) If the concern were valid, the estimate should have decreased. (2) It did not decrease, therefore the concern is likely invalid. Distractor B correctly states the premise of the test but draws the wrong final conclusion. Distractor C confuses the roles of specification and measurement error. Distractor D presents a logically incoherent argument (removing an upward bias cannot lead to a higher estimate).",
    "qid": "405",
    "model_evaluation_result": {
      "model_raw_response": "B) The reason is that if the IV estimate was truly unbiased, the reduction in misclassification (due to longer panel histories) would lead to a decrease in the IV estimate, not an increase.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 406,
    "Question": "### Background\n\n**Research Question.** This question addresses the paper's central finding: identifying the primary cause of China's HSS underdevelopment. The analysis uses a calibrated structural model of the 2009 Chinese economy to run counterfactual experiments.\n\n**Variables & Parameters.**\n*   `n_h`: Employment share of the HSS sector.\n*   `τ_h`: The distortionary output wedge in the HSS sector.\n*   `Z_h`: Labor productivity in the HSS sector.\n*   `Ω^h`: The economy-wide supply of high-skilled labor.\n*   `Y`: Aggregate real GDP per capita (normalized to 1.00 in the benchmark).\n\n---\n\n### Data / Model Specification\n\nThe table below summarizes results from counterfactual experiments on the 2009 calibrated model. Each row shows the effect of changing one or more parameters from their benchmark values.\n\n**Table 1: Counterfactual Results (2009 Calibration)**\n\n| Parameter Values | n_h (HSS Emp. Share) | Y (GDP per capita) |\n| :--- | :--- | :--- |\n| **Benchmark** | **0.07** | **1.00** |\n| `τ_h = 0` (Wedge removed) | 0.15 | 1.05 |\n| `Z_h = 2 * Z_{h,09}` (Productivity doubled) | 0.07 | 1.13 |\n| `Ω^h = 2 * Ω^h_{09}` (Skill supply doubled) | 0.09 | 1.04 |\n\n---\n\n### Question\n\nBased on the counterfactual results in **Table 1**, what is the main conclusion regarding the cause of China's underdeveloped HSS sector and the potential gains from reform?",
    "Options": {
      "A": "Distortions (wedges) are the primary cause of low HSS employment; removing them would more than double the sector's size and increase aggregate GDP by 5% through improved allocative efficiency.",
      "B": "Low labor productivity is the primary cause of low HSS employment; doubling productivity would significantly increase the sector's size and yield the largest gains in aggregate GDP.",
      "C": "A scarcity of high-skilled labor is the primary cause of low HSS employment; doubling the supply of skilled workers would cause the sector's size to double and increase aggregate GDP.",
      "D": "Distortions (wedges) are the primary cause of low HSS employment, but removing them would have a negligible effect on aggregate GDP, suggesting the misallocation costs are small."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to interpret counterfactual model results to identify a primary causal factor. It uses a Computational Judgment strategy. The correct option (A) accurately synthesizes the impact of removing the wedge on both sectoral employment and aggregate GDP. Distractor (B) incorrectly identifies low productivity as the cause for low employment, a direct contradiction of the table. Distractor (C) incorrectly identifies skill scarcity and overstates its impact. Distractor (D) is an 'Almost Right' option that correctly identifies the cause but incorrectly states the aggregate effect, testing for a complete reading of the evidence.",
    "qid": "406",
    "model_evaluation_result": {
      "model_raw_response": "A)assistant\n\nA)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 407,
    "Question": "### Background\n\n**Research Question.** The paper's model identifies a large theoretical 'wedge' (distortion) in China's high-skill-intensive services (HSS) sector. This question asks for the real-world institutional feature that the paper links to this wedge.\n\n**Variables & Parameters.**\n*   `Wedge`: A model-calibrated measure of distortion in a sector, relative to the goods sector (wedge=1 means no relative distortion).\n*   `Nom. Prod. Gap`: Nominal labor productivity of a sector relative to the goods sector, which serves as an empirical proxy for the wedge.\n*   `SOE Emp. (% in ind.)`: Share of an industry's workforce employed by State-Owned Enterprises.\n\n---\n\n### Data / Model Specification\n\nThe table below provides data on distortions and the presence of State-Owned Enterprises (SOEs) for China in 2009, with the U.S. in 1950 included as a benchmark for a country at a similar stage of development.\n\n**Table 1: Wedges and SOE Presence in China (2009) and U.S. (1950)**\n\n| | SOE Emp. (% in ind.) | Nom. Prod. Gap (rel. to goods) | Wedge |\n| :--- | :--- | :--- | :--- |\n| **China in 2009** | | | |\n| Low-skill-intensive Services | 0.09 | 1.02 | 1.00 |\n| High-skill-intensive Services | **0.71** | **3.03** | **2.60** |\n| **U.S. in 1950** | | | |\n| High-skill-intensive Services | — | 1.23 | 1.17 |\n\n---\n\n### Question\n\nWhat does the evidence in **Table 1** most strongly suggest about the nature of the large HSS wedge in China?",
    "Options": {
      "A": "The wedge is strongly associated with the high prevalence of State-Owned Enterprises (SOEs) and is abnormally large compared to a historical U.S. benchmark.",
      "B": "The wedge is a normal feature of economic development, as the U.S. had an HSS wedge of a similar magnitude at a comparable stage of development.",
      "C": "The wedge is primarily located in the Low-skill-intensive Services sector, which has a higher nominal productivity gap than the HSS sector.",
      "D": "The wedge is likely due to a scarcity of high-skilled labor, which drives up nominal productivity and is a common feature in developing economies."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to synthesize descriptive data to link a theoretical concept (the wedge) to an institutional feature (SOEs). It uses a Reverse-Reasoning strategy. The correct option (A) correctly identifies the correlation between the SOE share and the wedge size within China and notes the abnormal magnitude relative to the U.S. Distractor (B) is a 'Conceptual Opposite' directly contradicted by the data (2.60 vs 1.17). Distractor (C) misreads the table, confusing LSS and HSS. Distractor (D) introduces a 'True but Irrelevant' concept (skill scarcity) that is not supported by this specific table and was ruled out elsewhere in the paper.",
    "qid": "407",
    "model_evaluation_result": {
      "model_raw_response": "A) The wage is strongly associated...with the high prevalence...",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 408,
    "Question": "### Background\n\nThe model involves `n` agents who sign a contract `(d, t)` at period 1. In period 2, they choose a private investment `y_i`. In period 3, their private type `a_i` is realized. In period 4, the contract is executed based on their announced types.\n\n### Data / Model Specification\n\nAgent `i`'s utility is quasi-linear:\n```latex\nU_i = v_{i}(x,a_{i}) + I \\quad \\text{(Eq. 1)}\n```\nwhere `x` is the collective choice and `I` is a monetary transfer.\n\n### Question\n\nBased on the utility function in Eq. (1), what does the model assume about agents' attitude towards risk, and why is this assumption critical for the paper's proposed mechanisms?",
    "Options": {
      "A": "Agents are risk-neutral, which is critical because the mechanisms use monetary transfers to exactly offset social costs/benefits, requiring a linear valuation of money.",
      "B": "Agents are risk-averse, which is critical because they must be willing to accept the uncertainty of the mechanism's outcome.",
      "C": "Agents are risk-loving, which is necessary to incentivize them to make speculative ex-ante investments.",
      "D": "Agents' risk attitude is irrelevant as long as utility is separable in money, which is only needed for the budget to balance."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the foundational assumption of risk neutrality (quasi-linearity) and its specific function in enabling incentive alignment via transfers. The distractors test common confusions: B and C propose incorrect risk attitudes, while D misidentifies the reason for the assumption's importance, a 'True but Irrelevant' style error.",
    "qid": "408",
    "model_evaluation_result": {
      "model_raw_response": "A) \n\nNote: Please let me know if you need any further assistance or clarification.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 409,
    "Question": "### Background\n\nThe model involves `n` agents who sign a contract `(d, t)` at period 1. In period 2, they choose a private investment `y_i`. In period 3, their private type `a_i` is realized. In period 4, the contract is executed based on their announced types.\n\n### Data / Model Specification\n\nA first-best outcome `(φ*, y*)` must be both:\n1.  **Exchange-Efficient:** The ex-post choice `φ*(a)` maximizes total surplus for realized types `a`: `φ*(a) = argmax_x ∑_i v_i(x, a_i)`.\n2.  **Investment-Efficient:** The ex-ante investment `y*` maximizes expected total surplus: `y* = argmax_y E[∑_i v_i(φ*(a), a_i) | y]`.\n\n### Question\n\nA proposed contract guarantees that for any given set of investments and realized types, the collective choice will always maximize the total surplus (`φ*(a)`). However, empirical analysis shows that agents consistently under-invest relative to the social optimum (`y*`). What is the most likely explanation for this outcome?",
    "Options": {
      "A": "The contract is exchange-efficient but not investment-efficient, because agents anticipate not receiving their full marginal return on investment and thus have weak ex-ante incentives.",
      "B": "The contract is investment-efficient but not exchange-efficient, meaning agents invest correctly but the ex-post decision-making is flawed.",
      "C": "The agents are likely risk-averse, which makes them unwilling to invest optimally even if the contract is fully efficient.",
      "D": "The contract fails to satisfy the ex-post individual rationality constraint, causing agents to fear making losses."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to diagnose a classic market failure. It tests the crucial distinction between ex-post (exchange) and ex-ante (investment) efficiency. The correct answer (A) identifies the core hold-up problem. Distractors represent plausible but incorrect diagnoses: B reverses the logic, C blames risk aversion instead of incentive structure, and D confuses the hold-up problem with participation constraints.",
    "qid": "409",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: This is the only correct answer)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 410,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the paper's main policy proposal: a unilateral 'fiscal devaluation' by a financially distressed 'periphery' country to stabilize its economy within a monetary union.\n\n**Setting / Institutional Environment.** The home (periphery) country is in a recession caused by an asymmetric financial shock. The government implements a counter-cyclical fiscal policy consisting of a payroll subsidy (`$\\varsigma_t^P$`) for its firms, financed in a revenue-neutral way by a value-added tax (`$\\tau_t^V$`) levied on all domestic sales (of both home and foreign goods).\n\n### Data / Model Specification\n\nThe policy is governed by three key equations:\n\n1.  A Taylor-type fiscal rule sets the VAT rate based on the output gap:\n    ```latex\n    \\tau_{t}^{V}=\\frac{\\Delta_{t}}{1+\\Delta_{t}},\\quad \\mathrm{with} \\quad \\Delta_{t}=-\\alpha^{FD}\\ln\\bigg(\\frac{y_{t}}{y}\\bigg) \\quad \\text{(Eq. (1))}\n    ```\n\n2.  A revenue-neutrality constraint determines the payroll subsidy:\n    ```latex\n    \\varsigma_{t}^{P}w_{t}h_{t}=\\tau_{t}^{V}(p_{h,t}c_{h,t}+p_{f,t}c_{f,t}) \\quad \\text{(Eq. (2))}\n    ```\n\n3.  These policies modify the home firm's equity issuance threshold, `$a_t^E$`, which is the breakeven level of an idiosyncratic cost shock above which the firm must issue costly equity:\n    ```latex\n    a_{t}^{E}=\\frac{A_{t}}{\\left(1-\\varsigma_{t}^{P}\\right)w_{t}}\\left[\\frac{p_{h,t}\\left(1-\\tau_{t}^{V}\\right)c_{h,t}+q_{t}p_{h,t}^{*}c_{h,t}^{*}}{\\left(\\phi+c_{h,t}+c_{h,t}^{*}\\right)^{\\frac{1}{\\alpha}}}\\right] \\quad \\text{(Eq. (3))}\n    ```\n\n### Question\n\nThe paper argues that this revenue-neutral fiscal devaluation policy successfully alleviates financial constraints for home country firms through a 'firm balance sheet channel'. What is the core economic reason for this positive net effect on firm liquidity?",
    "Options": {
      "A": "The payroll subsidy applies to the firm's total wage bill, which is a larger base than the domestic sales revenue to which the VAT applies.",
      "B": "The payroll subsidy rate (`$\\varsigma_t^P$`) is, by construction of the fiscal rules, always larger than the VAT rate (`$\\tau_t^V$`).",
      "C": "The policy stimulates aggregate demand, and the resulting increase in total sales revenue more than compensates for the tax on domestic sales.",
      "D": "The VAT revenue reduction is exactly offset by the payroll subsidy cost reduction, but the policy signals government support, which lowers the cost of external finance."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the paper's novel 'firm balance sheet channel' for fiscal devaluation. It uses a Reverse-Reasoning strategy, asking for the core mechanism behind the policy's success. The distractors are designed to target specific, plausible errors: A is correct. B (Formula Misuse) confuses the rate with the total effect. C (True but Irrelevant) describes a potential secondary demand channel, not the direct balance sheet channel. D (Conceptual Opposite) invokes the most common misconception of ignoring the different tax/subsidy bases and adds an irrelevant behavioral story.",
    "qid": "410",
    "model_evaluation_result": {
      "model_raw_response": "C) Thepolicy stimulates aggregate demand and the resultingincrease in total sales revenuemore thancompensates for thetax on domestic sales.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 411,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the paper's main policy proposal: a unilateral 'fiscal devaluation' by a financially distressed 'periphery' country to stabilize its economy within a monetary union.\n\n**Setting / Institutional Environment.** The home (periphery) country is in a recession caused by an asymmetric financial shock. The government implements a counter-cyclical fiscal policy consisting of a payroll subsidy (`$\\varsigma_t^P$`) for its firms, financed in a revenue-neutral way by a value-added tax (`$\\tau_t^V$`) levied on all domestic sales (of both home and foreign goods).\n\n### Data / Model Specification\n\nThe policy is governed by three key equations:\n\n1.  A Taylor-type fiscal rule sets the VAT rate based on the output gap:\n    ```latex\n    \\tau_{t}^{V}=\\frac{\\Delta_{t}}{1+\\Delta_{t}},\\quad \\mathrm{with} \\quad \\Delta_{t}=-\\alpha^{FD}\\ln\\bigg(\\frac{y_{t}}{y}\\bigg) \\quad \\text{(Eq. (1))}\n    ```\n\n2.  A revenue-neutrality constraint determines the payroll subsidy:\n    ```latex\n    \\varsigma_{t}^{P}w_{t}h_{t}=\\tau_{t}^{V}(p_{h,t}c_{h,t}+p_{f,t}c_{f,t}) \\quad \\text{(Eq. (2))}\n    ```\n\n3.  These policies modify the home firm's equity issuance threshold, `$a_t^E$`, which is the breakeven level of an idiosyncratic cost shock above which the firm must issue costly equity:\n    ```latex\n    a_{t}^{E}=\\frac{A_{t}}{\\left(1-\\varsigma_{t}^{P}\\right)w_{t}}\\left[\\frac{p_{h,t}\\left(1-\\tau_{t}^{V}\\right)c_{h,t}+q_{t}p_{h,t}^{*}c_{h,t}^{*}}{\\left(\\phi+c_{h,t}+c_{h,t}^{*}\\right)^{\\frac{1}{\\alpha}}}\\right] \\quad \\text{(Eq. (3))}\n    ```\n\n### Question\n\nConsider a foreign (core) firm that exports its goods to the home market. This firm is subject to the VAT (`$\\tau_t^V$`) on its sales in the home country but does not receive the payroll subsidy. How does the home country's unilateral fiscal devaluation policy affect this foreign firm's optimal pricing strategy?",
    "Options": {
      "A": "It weakens the foreign firm's financial position, mitigating its ability to engage in 'predatory' price-cutting in the home market.",
      "B": "It encourages the foreign firm to cut its prices more aggressively to offset the tax and defend its market share from subsidized home firms.",
      "C": "It has no significant effect on the foreign firm's pricing strategy, as the firm simply passes the full cost of the VAT on to home consumers.",
      "D": "It improves the foreign firm's financial position by stabilizing aggregate demand in its export market, allowing it to increase its markups."
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to apply the model to a new context (a foreign firm) and analyze international spillovers, a key part of the paper's policy argument. It uses a Scenario Application strategy. A is correct. B (Conceptual Opposite) posits the opposite strategic reaction. C (Step-Omission Error) ignores the strategic effect of the tax on the firm's own liquidity and pricing incentives. D (Conceptual Confusion) focuses only on a potential secondary demand effect while ignoring the direct, negative impact of the VAT on the foreign firm's revenue.",
    "qid": "411",
    "model_evaluation_result": {
      "model_raw_response": "B) It encouages the foreign firmto cut its pricesmore aggressively to offsetthe tax and defendits market sharefrom subsidizedhome firms.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 412,
    "Question": "### Background\n\nAn auctioneer seeks to find a Walrasian equilibrium price vector `p*` in a market with substitute goods (`S₁`) and complement goods (`S₂`). The equilibrium prices are the global minimizers of a convex Lyapunov function `L(p)`.\n\n### Data / Model Specification\n\n**The Dynamic Double-Track (DDT) Procedure:**\nThis is an iterative price-adjustment process. At each step `t`, it updates the price vector `p(t+1) = p(t) + δ(t)`. The adjustment `δ(t)` is chosen to decrease `L(p)` as much as possible, subject to the constraint that prices for substitute goods (`S₁`) can only increase or stay the same, and prices for complement goods (`S₂`) can only decrease or stay the same. This defines a monotonic, \"uphill\" search according to a generalized price order `≤_g`.\n\n### Question\n\nThe simple DDT procedure is only guaranteed to find an equilibrium if it starts from a price `p(0)` that is \"below\" the equilibrium set (i.e., `p(0) ≤_g p*`). If started at a price that is already \"above\" all equilibrium prices, the procedure fails to find an equilibrium. What is the fundamental reason for this failure?",
    "Options": {
      "A": "The procedure is monotonic and cannot reverse direction; it is an \"uphill\" search that gets stuck if it starts above the target.",
      "B": "The Lyapunov function `L(p)` is not convex at prices far from equilibrium, leading the algorithm to a saddle point.",
      "C": "The procedure oscillates between two non-equilibrium price vectors without converging.",
      "D": "Buyers would strategically misreport their demands at such high prices, causing the auctioneer's optimization to be invalid."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to assess understanding of a key algorithmic limitation. The distractors are designed to be plausible but incorrect explanations: B incorrectly challenges a given property of the function rather than the algorithm, C introduces a failure mode from a different class of algorithms (e.g., Scarf's example for general tatonnement), and D introduces a strategic consideration irrelevant to the mechanical failure being asked about.",
    "qid": "412",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: I've output only the correct answer as a string.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 413,
    "Question": "### Background\n\nAn auctioneer seeks to find a Walrasian equilibrium price vector `p*` in a market with substitute goods (`S₁`) and complement goods (`S₂`). The equilibrium prices are the global minimizers of a Lyapunov function `L(p)`.\n\n### Data / Model Specification\n\n**The Global Dynamic Double-Track (GDDT) Procedure:**\nThis procedure is guaranteed to find an equilibrium from any starting integer price vector `p(0)`.\n- **Phase 1:** An \"uphill\" search phase where prices of substitutes (`S₁`) are raised and prices of complements (`S₂`) are lowered, seeking to minimize `L(p)`. This continues until no such move can further decrease `L(p)`.\n- **Phase 2:** A \"downhill\" search phase where prices of substitutes (`S₁`) are lowered and prices of complements (`S₂`) are raised. This continues until no such move can further decrease `L(p)`.\n\nThe procedure stops when it finds a price vector that is a local minimum with respect to both sets of price movements.\n\n### Question\n\nWhich mathematical property of the Lyapunov function `L(p)` is most critical for ensuring that the final stopping point of the GDDT procedure is a *global* minimum, and therefore a true Walrasian equilibrium?",
    "Options": {
      "A": "Convexity, because for a convex function, any local minimum is also a global minimum.",
      "B": "Generalized submodularity, because it ensures the set of equilibria forms a well-structured generalized lattice.",
      "C": "Continuity, because it prevents jumps in the function that could hide the true minimum from an iterative search.",
      "D": "Integer-valuedness, because it ensures the algorithm terminates in a finite number of steps."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core theoretical insight behind the algorithm's success. The distractors are all true properties of the Lyapunov function mentioned in the paper, forcing the test-taker to discriminate between necessary-but-insufficient properties (Continuity), properties relevant to the structure of the solution set but not the search algorithm's success (Generalized Submodularity), and properties explaining termination rather than global optimality (Integer-valuedness).",
    "qid": "413",
    "model_evaluation_result": {
      "model_raw_response": "C) Continuality. \n\n(Note: I assume the correct term is \"continuity\", not \"continuity\".)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 414,
    "Question": "### Background\n\nIn the Dynamic Double-Track (DDT) procedure, an auctioneer iteratively adjusts prices `p(t)` to find a Walrasian equilibrium. At each step, the auctioneer must choose a price adjustment `δ` that maximally decreases the Lyapunov function `L(p)`, a measure of total market surplus.\n\n### Data / Model Specification\n\nAt each step `t`, the auctioneer solves the following optimization problem to determine the next price adjustment `δ(t)`, using only the demand sets `Dⁱ(p(t))` reported by each buyer `i`:\n```latex\n\\operatorname*{max}_{\\delta\\in\\Delta}\\left\\{\\sum_{i\\in I}\\left(\\operatorname*{min}_{S\\in D^{i}(p(t))}\\sum_{\\beta_{h}\\in S}\\delta_{h}\\right) - \\sum_{\\beta_{h}\\in N}\\delta_{h}\\right\\} \\quad \\text{(Eq. 1)}\n```\nwhere `Δ` is the set of allowed integer price adjustments.\n\n### Question\n\nWhat is the primary economic function of the calculation in Eq. (1)?",
    "Options": {
      "A": "It allows the auctioneer to calculate the change in total market surplus for any potential price adjustment `δ` using only publicly reported demand sets.",
      "B": "It forces buyers to reveal their single most-preferred bundle, resolving any indifference and simplifying the market clearing problem.",
      "C": "It directly calculates the revenue-maximizing price adjustment for the seller.",
      "D": "It requires the auctioneer to know each buyer's private utility function `uⁱ` to evaluate the inner `min` term."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the interpretation of the core mathematical formula of the auction mechanism. The distractors target common misinterpretations: B focuses on a secondary effect rather than the primary goal, C misidentifies the objective function (surplus vs. revenue), and D states the exact opposite of a key feature of the mechanism (its informational efficiency).",
    "qid": "414",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: This output only contains the single correct answer letter, which is \"A\".)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 415,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the Time-Varying Parameter (TVP) ARCH-M model against the standard Fixed-Parameter (FP) ARCH-M model, investigating whether allowing the price of risk to vary over time can resolve key empirical puzzles in modeling stock returns.\n\n**Setting / Institutional Environment.** Standard FP ARCH-M models, which assume a constant risk-return tradeoff, have been found to produce unstable parameter estimates across different sample periods and to systematically over-predict the equity risk premium. The TVP ARCH-M model is proposed as a solution by allowing the price of risk to evolve as a random walk.\n\n### Data / Model Specification\n\nThe standard **Fixed-Parameter (FP) ARCH-M** model is:\n```latex\ny_t = c h_t + e_t\n```\nThe proposed **Time-Varying Parameter (TVP) ARCH-M** model is specified in a state-space form:\n```latex\ny_t = b_t h_t + e_t\n```\n```latex\nb_t = b_{t-1} + v_t\n```\nwhere `c` is the constant price of risk and `b_t` is the time-varying price of risk.\n\n**Table 1: Summary Statistics of Risk Premiums (Monthly, %)**\n\n| Risk Premium | Mean |\n| :--- | :--- |\n| Realized (`rs`) | 0.64 |\n| TVP Predicted | 0.54 |\n| FP Predicted | 0.96 |\n\n**Table 2: Predictive Regressions for Realized Returns (1926-1985)**\n\n| Model | Intercept | Coeff on TVP Prediction | Coeff on FP Prediction | R² |\n| :--- | :--- | :--- | :--- | :--- |\n| 1. TVP only | -0.044 (0.277) | 1.268 (0.333) | | 0.020 |\n| 2. FP only | 0.342 (0.265) | | 0.311 (0.166) | 0.005 |\n\n*Standard errors are in parentheses.*\n\n### Question\n\nBased on the evidence in Table 1 and Table 2, which of the following statements most accurately summarizes the performance of the FP and TVP models?",
    "Options": {
      "A": "The FP model's average predicted premium is 50% higher than the realized average, and a t-test fails to reject the hypothesis that the TVP model's predictions are unbiased.",
      "B": "The FP model's average predicted premium is 50% higher than the realized average, but a t-test rejects the hypothesis that the TVP model's predictions are unbiased at the 5% level.",
      "C": "The FP model's average predicted premium is 33% higher than the realized average, and a t-test fails to reject the hypothesis that the TVP model's predictions are unbiased.",
      "D": "The FP model's average predicted premium is 33% higher than the realized average, and a t-test rejects the hypothesis that the TVP model's predictions are unbiased at the 5% level."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses core quantitative takeaways from the paper's model comparison using a Computational Judgment strategy. The user must perform two distinct calculations/judgments: (1) calculate the percentage overprediction of the FP model from Table 1, and (2) conduct and interpret a t-test for unbiasedness (H0: coeff=1) on the TVP model's coefficient from Table 2. Distractors are designed based on common errors: B correctly calculates the overprediction but incorrectly concludes the t-test; C makes a calculation error ((0.96-0.64)/0.96) but gets the t-test right; D is incorrect on both fronts.",
    "qid": "415",
    "model_evaluation_result": {
      "model_raw_response": "D) The fp model's average(predicted premium is 32% higher than realized average, a t-testrejects the hypothesis that tvp model's predictions unbiased at 5%level.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 416,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the Time-Varying Parameter (TVP) ARCH-M model against the standard Fixed-Parameter (FP) ARCH-M model, investigating whether allowing the price of risk to vary over time can resolve key empirical puzzles in modeling stock returns.\n\n**Setting / Institutional Environment.** Standard FP ARCH-M models, which assume a constant risk-return tradeoff, have been found to produce unstable parameter estimates across different sample periods and to systematically over-predict the equity risk premium. The TVP ARCH-M model is proposed as a solution by allowing the price of risk to evolve as a random walk.\n\n### Data / Model Specification\n\nThe standard **Fixed-Parameter (FP) ARCH-M** model is:\n```latex\ny_t = c h_t + e_t\n```\nThe proposed **Time-Varying Parameter (TVP) ARCH-M** model is specified in a state-space form:\n```latex\ny_t = b_t h_t + e_t\n```\n```latex\nb_t = b_{t-1} + v_t\n```\nwhere `c` is the constant price of risk and `b_t` is the time-varying price of risk.\n\n**Table 1: Predictive Regressions for Realized Returns (1926-1985)**\n\n| Model | Intercept | Coeff on TVP Prediction | Coeff on FP Prediction | R² |\n| :--- | :--- | :--- | :--- | :--- |\n| 1. TVP only | -0.044 (0.277) | 1.268 (0.333) | | 0.020 |\n| 2. FP only | 0.342 (0.265) | | 0.311 (0.166) | 0.005 |\n| 3. Horse Race | -0.724 (0.351) | 1.576 (0.345) | -0.533 (0.171) | 0.032 |\n\n*Standard errors are in parentheses.*\n\n### Question\n\nConsider the \"Horse Race\" regression (Model 3 in Table 1), where the realized return is regressed on the predictions from both the TVP and FP models simultaneously. What is the primary conclusion from this regression?",
    "Options": {
      "A": "The TVP model's predictions subsume the information in the FP model's predictions, which appear to have a negative marginal relationship with realized returns once the TVP prediction is included.",
      "B": "Both the TVP and FP models provide statistically significant and independent positive information for predicting future returns.",
      "C": "The FP model is the superior predictor, and the TVP model's predictions serve as a significant negative correction term.",
      "D": "Neither model is a good predictor, as the coefficients are not close to 1 and the R-squared value is very low."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the user's ability to interpret a 'horse race' regression, a crucial skill for evaluating competing empirical models. It uses a Reverse-Reasoning strategy, asking for the primary inference from the results. The distractors are based on a classic design matrix: B (Almost Right) ignores the crucial negative sign on the losing model's coefficient; C (Conceptual Opposite) reverses the roles of the two models; D (True but Irrelevant) focuses on secondary issues (coefficient magnitude, R-squared) to distract from the main conclusion about which model's information content subsumes the other.",
    "qid": "416",
    "model_evaluation_result": {
      "model_raw_response": "B) Both models provide statistically signifificant and independent positive informatio",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 417,
    "Question": "### Background\n\nThe analysis uses the theoretical framework of a sticky-price model to perform two 'back-of-the-envelope' calculations. First, it maps a steady-state inflation rate into an equivalent permanent loss in total factor productivity (TFP). Second, it uses an approximate formula to link an empirically observable statistic—the coefficient of variation (`cvar`) of prices—to the model's price dispersion measure `Δ`, and from there to productivity and consumption-equivalent welfare losses.\n\n### Data / Model Specification\n\nThe model establishes that price dispersion `Δ` acts as a drag on productivity. This can be quantified by defining an \"effective\" productivity level `A^R = A Δ^{-1/ϕ}`. The productivity-equivalent cost is the percentage change, `1 - Δ^{-1/ϕ}`.\n\n**Table 1: Approximate Productivity Equivalent Cost of Inflation**\n\n| | I (Benchmark) |\n| :--- | :--- |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n| `π` (annual) | 2.5% |\n| `α` | 0.5 |\n| `Δ` | 1.034 |\n| `1 - Δ^{-1/ϕ}` | -2.4% |\n\n**Table 2: Approximate Productivity Equivalent Cost of Price Dispersion**\n\n| | II (Benchmark) |\n| :--- | :--- |\n| `cvar` | 10% |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n| `Δ` | 1.04 |\n| `1 - Δ^{-1/ϕ}` | -2.7% |\n\n**Table 3: Welfare Loss from Price Dispersion: Consumption Equivalent**\n\n| | II (Benchmark) |\n| :--- | :--- |\n| Coeff. of Variation | 10% |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n| `Δ` | 1.04 |\n| `v` (labor supply) | 1.8 |\n| `Φ` (cost) | 2.2% |\n\nThe authors state that \"column II in Table 2 corresponds quite closely to column I in Table 1, in terms of the ultimate productivity-equivalent impact.\" What is the primary significance of this correspondence?",
    "Options": {
      "A": "It suggests the model is quantitatively plausible, as the level of price dispersion generated by moderate inflation is consistent with levels inferred from real-world price data.",
      "B": "It proves that a 2.5% inflation rate is the sole cause of the 10% coefficient of variation observed in empirical studies.",
      "C": "It demonstrates that the productivity-equivalent cost is highly sensitive to the price stickiness parameter `α`, which is the main driver of the result.",
      "D": "It reveals an inconsistency in the model, as two different sets of assumptions (one theoretical, one empirical) lead to a similar outcome, undermining the mechanism's specificity."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to synthesize information from two different tables to grasp the paper's central argument about quantitative plausibility. It uses a Reverse-Reasoning strategy. The correct option (A) accurately identifies the 'reality check' function of the comparison. Distractor (B) makes an overly strong causal claim (Conceptual Opposite). Distractor (C) is True but Irrelevant, focusing on a different parameter comparison not central to this specific point. Distractor (D) misinterprets the correspondence as a weakness rather than a strength (Conceptual Opposite).",
    "qid": "417",
    "model_evaluation_result": {
      "model_raw_response": "D) Itreveals an inconsistency inthe model, as twodifferent sets of assumptions(one theoretical, oneempirical) lead to asimilar outcome, undermining themechanism's specificity.󠁧󠁢󠁥󠁮󠁧",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 418,
    "Question": "### Background\n\nThe analysis uses an approximate formula to link an empirically observable statistic—the coefficient of variation (`cvar`) of prices—to the model's price dispersion measure `Δ`, and from there to an equivalent productivity loss.\n\n### Data / Model Specification\n\nThe model provides an approximate link between the theoretical `Δ` and the empirical `cvar`:\n\n```latex\n\\Delta \\simeq 1+\\frac{1}{2}\\frac{\\theta\\phi}{\\theta+1}(\\theta\\phi-\\theta+1)\\frac{\\mathrm{cvar}^{2}}{1-\\frac{1}{2}\\frac{\\theta}{\\theta+1}(\\mathrm{cvar}^{2}+1)}\n```\n(Eq. 1)\n\nThe productivity-equivalent cost is the percentage change, `1 - Δ^{-1/ϕ}`.\n\n**Table 1: Approximate Productivity Equivalent Cost of Price Dispersion**\n\n| | Benchmark |\n| :--- | :--- |\n| `cvar` | 10% |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n\nUsing the benchmark parameters from Table 1 and the formula in Eq. (1), calculate the productivity-equivalent cost of price dispersion. Which of the following is closest to your result?",
    "Options": {
      "A": "-2.8%",
      "B": "+2.8%",
      "C": "-0.7%",
      "D": "-1.6%"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to execute a multi-step calculation using a complex formula and then apply the result in a second formula. It uses a Computational Judgment strategy. The correct answer (A) requires careful execution. Distractor (B) is a Sign Error. Distractor (C) corresponds to the result for a different `cvar` value from the full paper's Table 2, catching students who look up a number instead of calculating. Distractor (D) is the result of a plausible Formula Misuse error (miscalculating the denominator in Eq. 1).",
    "qid": "418",
    "model_evaluation_result": {
      "model_raw_response": "B) +0.7% (approximately)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 419,
    "Question": "### Background\n\nThe ultimate measure of the economic cost of price dispersion is in terms of welfare. This can be measured as a consumption equivalent, `Φ`, which is the permanent percentage increase in consumption required in the distorted economy to make the representative agent indifferent to living in the frictionless (`Δ=1`) economy.\n\n### Data / Model Specification\n\nFor a steady state, the consumption equivalent `Φ` is given by:\n\n```latex\n\\log(\\Phi) = DU^* \\times (\\Delta^{v+1} - 1)\n```\n(Eq. 1)\n\nwhere `DU*` is the disutility of labor in the efficient economy. A standard calibration implies `DU*` is approximately 0.12.\n\n**Table 1: Welfare Loss Parameters**\n\n| | Benchmark |\n| :--- | :--- |\n| `Δ` | 1.04 |\n| `v` (labor supply) | 1.8 |\n\nUsing the data from Table 1, the calibrated value for `DU*`, and Eq. (1), calculate the consumption-equivalent welfare cost (`Φ - 1`). Which of the following is closest to your result?",
    "Options": {
      "A": "1.4%",
      "B": "0.014%",
      "C": "2.2%",
      "D": "0.12%"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a high-difficulty welfare calculation that requires careful application of a formula involving exponents and logarithms. It uses a Computational Judgment strategy. The correct answer (A) requires correctly calculating `Δ^{v+1}`, multiplying by `DU*`, and then exponentiating the result to find `Φ`. Distractor (B) is a Step-Omission Error from failing to exponentiate `log(Φ)` and/or misinterpreting the percentage conversion. Distractor (C) is the value reported in the paper's table, catching students who do not perform the requested calculation. Distractor (D) is the value of `DU*` itself, catching students who misunderstand the formula.",
    "qid": "419",
    "model_evaluation_result": {
      "model_raw_response": "C) 0,22% (rounded to two decimal places)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 420,
    "Question": "### Background\n\n**Research Question.** This problem examines the causal impact of public exposure for low performance on an individual's subsequent intrinsic motivation and effort.\n\n**Setting and Sample.** The study uses a multi-part laboratory experiment where subjects perform a real-effort task. In Part 3, the lowest-ranked performer (Rank 3) is publicly exposed. This exposure event is the 'treatment'. Part 4 measures performance after the treatment. The analysis focuses on the subsample of individuals who were the least performers in Part 3, comparing those who were actually exposed to those whose exposure was waived by their peers.\n\n### Data / Model Specification\n\nTable 1 presents the mean performance for the subsample of least performers, broken down by whether they were exposed in Part 3.\n\n**Table 1: Mean Performance of Least Performers in Part 3**\n\n| Group       | Part 1 | Part 2 | Part 3 | Part 4 | N  |\n|-------------|--------|--------|--------|--------|----|\n| Not exposed | 116.22 | 123.80 | 126.58 | 130.54 | 81 |\n| Exposed     | 121.25 | 123.47 | 127.39 | 127.71 | 51 |\n\n### Question\n\nA key assumption for a Difference-in-Differences (DiD) analysis is that the treatment and control groups had parallel trends before the treatment. Based on the data in Table 1, first evaluate this assumption by comparing the performance change from Part 1 to Part 2 for the 'Exposed' versus 'Not exposed' groups. Then, calculate the DiD estimate of the effect of exposure on performance, using Part 3 as the pre-treatment period and Part 4 as the post-treatment period. Which of the following statements is correct?",
    "Options": {
      "A": "The parallel trends assumption is violated; the DiD estimate is -3.64.",
      "B": "The parallel trends assumption is violated; the DiD estimate is 0.32.",
      "C": "The parallel trends assumption holds; the DiD estimate is -3.64.",
      "D": "The parallel trends assumption holds; the DiD estimate is 3.96."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform the core calculations of a Difference-in-Differences analysis and to test its key identifying assumption. It uses a Computational Judgment strategy. The correct answer requires both a correct judgment on the parallel trends assumption and a correct calculation of the DiD estimate. Distractor B is a step-omission error (raw change for treated group). Distractor C makes a conceptual error about the assumption. Distractor D combines a conceptual error with a different step-omission error (raw change for control group).",
    "qid": "420",
    "model_evaluation_result": {
      "model_raw_response": "C) Theparallel trends assumption holds, the DiD esti",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 421,
    "Question": "### Background\n\n**Research Question.** This problem examines the causal impact of public exposure for low performance on an individual's subsequent intrinsic motivation and effort.\n\n**Setting and Sample.** The study uses a multi-part laboratory experiment where subjects perform a real-effort task. In Part 3, the lowest-ranked performer (Rank 3) is publicly exposed. This exposure event is the 'treatment'. Part 4 measures performance after the treatment. The analysis focuses on the subsample of individuals who were the least performers in Part 3, comparing those who were actually exposed to those whose exposure was waived by their peers.\n\n### Data / Model Specification\n\nTable 1 presents mean performance data, while Table 2 shows a regression estimate of the treatment effect.\n\n**Table 1: Mean Performance of Least Performers in Part 3**\n\n| Group       | Part 1 | Part 2 | Part 3 | Part 4 | N  |\n|-------------|--------|--------|--------|--------|----|\n| Not exposed | 116.22 | 123.80 | 126.58 | 130.54 | 81 |\n| Exposed     | 121.25 | 123.47 | 127.39 | 127.71 | 51 |\n\n**Table 2: Determinants of the Evolution of Performance for Rank 3 Performers**\n\n| Dependent variable: Perf. in part 4 - Perf. in part 3 | Coefficient (Std. Err.) |\n|:------------------------------------------------------|:------------------------|\n| Exposure of rank 3 in part 3                          | -3.123** (1.472)        |\n\n*Notes: OLS estimates. ** indicates significance at the 5% level. Model includes other controls.* \n\n### Question\n\nA simple Difference-in-Differences (DiD) analysis using the data in Table 1 yields an estimated effect of exposure of -3.64. However, the regression model in Table 2, which includes additional controls, estimates the effect to be -3.123. Given the pre-treatment performance trends from Part 1 to Part 2 shown in Table 1, what is the most likely explanation for this discrepancy?",
    "Options": {
      "A": "The simple DiD estimate is biased downwards (more negative) because the 'Not exposed' control group had a more positive performance trend than the 'Exposed' group even before the treatment.",
      "B": "The simple DiD estimate is biased upwards (less negative) because the 'Exposed' treatment group was already improving faster than the 'Not exposed' group before the treatment.",
      "C": "The regression estimate is more precise because it has a smaller standard error, which is common when adding control variables.",
      "D": "The simple DiD estimate is larger in magnitude because it fails to account for the baseline performance difference between the groups in Part 3."
    },
    "Answer": "A",
    "pi_justification": "This item assesses a deep understanding of omitted variable bias in the context of DiD, using a Reverse-Reasoning strategy. It requires connecting the violation of the parallel trends assumption to the direction of bias in the naive estimator. The correct answer (A) correctly identifies the direction of bias and its source in the pre-trends. Distractor B (Conceptual Opposite) gets both the direction and the reason wrong. Distractor C (True but Irrelevant) confuses bias with precision. Distractor D (Formula Misuse) points to a common but incorrect understanding of what DiD controls for.",
    "qid": "421",
    "model_evaluation_result": {
      "model_raw_response": "B) The 'Exposed' Treatment group was already Improving Faster than the 'not exposed' group Before the treatment.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 422,
    "Question": "### Background\n\nThe paper's \"dual scaling\" theory posits that a CEO's pay is determined not only by their own firm's size but also by the size of a typical large firm in the market, which reflects the aggregate demand for talent. This theory is tested using a panel dataset of top U.S. companies from 1992-2004.\n\n### Data / Model Specification\n\nThe theoretical model `w(n) = D(n_*) S(n_*)^{β/α} S(n)^{γ - β/α}` is tested using the following log-linear empirical specification:\n\n```latex\n\\ln(w_{i,t+1}) = d + e \\cdot \\ln(S_{n_*,t}) + f \\cdot \\ln(S_{i,t}) + \\text{Controls} + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n```\n\nwhere `w_{i,t+1}` is CEO compensation, `S_{i,t}` is the firm's own market value, and `S_{n_*,t}` is the market value of the 250th largest firm (the reference size). The coefficients `e` and `f` map to the theoretical parameters as follows: `e = β/α` and `f = γ - β/α`, which implies `γ = e + f`.\n\nResults for the top 1000 firms including industry fixed effects are presented in Table 1.\n\n**Table 1: Regression of ln(Total compensation)**\n\n| | (2) Industry FE |\n|:---|:---:|\n| ln(Market cap) (`f`) | 0.37 |\n| | (0.020) |\n| ln(Market cap of firm #250) (`e`) | 0.66 |\n| | (0.054) |\n| Observations | 7,936 |\n| R² | 0.29 |\n\n*Standard errors in parentheses.*\n\n### Question\n\nBased on the regression results in Table 1, what is the most accurate interpretation of the estimated parameters?",
    "Options": {
      "A": "The results support the hypothesis of constant returns to scale (γ ≈ 1.03), and imply that the talent distribution is thinner-tailed than the firm size distribution (β/α ≈ 0.66).",
      "B": "The results suggest diminishing returns to scale (γ ≈ 0.37), and imply that the talent distribution is thicker-tailed than the firm size distribution (β/α ≈ 0.66).",
      "C": "The results support the hypothesis of constant returns to scale (γ ≈ 1.03), and imply that the firm size distribution is thinner-tailed than the talent distribution (β/α ≈ 0.37).",
      "D": "The results imply that if all firms in the market double in size, average CEO pay is predicted to increase by approximately 37%, consistent with Roberts's Law."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to map empirical regression coefficients back to the model's structural parameters and interpret them economically. It uses a Computational Judgment strategy. The correct option (A) requires correctly calculating γ = e + f = 0.66 + 0.37 = 1.03 and correctly identifying e = β/α = 0.66. Distractor (B) incorrectly uses f as the estimate for γ. Distractor (C) incorrectly swaps the estimates for β/α and γ-β/α. Distractor (D) makes a classic error by applying the cross-sectional elasticity (f ≈ 0.37) to a time-series question (all firms doubling in size), for which the correct elasticity is γ ≈ 1.",
    "qid": "422",
    "model_evaluation_result": {
      "model_raw_response": "B) The result suggests diminishing returns to size (γ ≈ f = 0.36), and implies that the talent pool is thicker-tail than the firm-size distribution (β > α).",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 423,
    "Question": "### Background\n\nThe paper's central claim is that CEO talent exhibits constant returns to scale (γ=1). This hypothesis can be tested using the results from the empirical model.\n\n### Data / Model Specification\n\nThe log-linear empirical specification is:\n\n```latex\n\\ln(w_{i,t+1}) = d + e \\cdot \\ln(S_{n_*,t}) + f \\cdot \\ln(S_{i,t}) + \\text{Controls} + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n```\n\nIn this model, `γ = e + f`. Therefore, testing `H₀: γ = 1` is equivalent to testing `H₀: e + f = 1`.\n\nThe regression results using firm fixed effects (the paper's preferred specification for this test) are:\n*   `ê = 0.78`\n*   `f̂ = 0.26`\n\nThe year-clustered standard errors and their covariance are:\n*   `SE(ê) = 0.083`\n*   `SE(f̂) = 0.043`\n*   `Cov(ê,f̂) = -0.003`\n\nThe variance of a sum of two coefficients is `Var(ê+f̂) = Var(ê) + Var(f̂) + 2Cov(ê,f̂)`.\n\n### Question\n\nUsing the provided results, conduct a t-test of the null hypothesis `H₀: γ = 1`. What is the correct conclusion at the 5% significance level (critical t-value ≈ 1.96)?",
    "Options": {
      "A": "Fail to reject H₀, as the t-statistic is approximately 0.76.",
      "B": "Reject H₀, as the point estimate of γ (1.04) is statistically greater than 1.",
      "C": "Fail to reject H₀, as the t-statistic is approximately 0.43.",
      "D": "Reject H₀, as the t-statistic is approximately 2.15."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a formal hypothesis test on a linear combination of regression coefficients, a key skill in empirical analysis. It uses the Computational Judgment strategy. The correct answer (A) requires correctly calculating the variance of the sum including the covariance term: Var = (0.083)² + (0.043)² + 2(-0.003) = 0.002738. The SE is sqrt(0.002738) ≈ 0.0523. The t-stat is (1.04 - 1) / 0.0523 ≈ 0.765. Since |0.765| < 1.96, we fail to reject. Distractor (C) is the result of a classic error: ignoring the covariance term. In that case, Var = (0.083)² + (0.043)² = 0.008738, SE ≈ 0.0935, and t ≈ 0.43. Distractors (B) and (D) represent incorrect conclusions.",
    "qid": "423",
    "model_evaluation_result": {
      "model_raw_response": "C) Fail\tto\treject\tH₀,\tas\tthe\tt-statistic\tis\tapproximately\t0.43.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 424,
    "Question": "### Background\n\nThe paper tests its theory of CEO pay using a panel dataset of firms over time. The empirical analysis compares a simple OLS regression (Column 1) with a model that includes firm fixed effects (Column 4).\n\n### Data / Model Specification\n\nThe empirical model is:\n\n```latex\n\\ln(w_{i,t+1}) = d + e \\cdot \\ln(S_{n_*,t}) + f \\cdot \\ln(S_{i,t}) + \\text{Controls} + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1: Panel Evidence: CEO Pay, Own Firm Size, and Reference Firm Size**\n\n| | (1) OLS | (4) Firm FE |\n|:---|:---:|:---:|\n| ln(Market cap) (`f`) | 0.37 | 0.26 |\n| ln(Market cap of firm #250) (`e`) | 0.72 | 0.78 |\n| Firm fixed effects | No | Yes |\n| R² | 0.23 | 0.60 |\n\n### Question\n\nWhat is the primary econometric justification for preferring the firm fixed-effects model (Column 4) over the OLS model (Column 1) for estimating the effect of a firm's own size on its CEO's pay?",
    "Options": {
      "A": "It controls for time-invariant unobserved firm characteristics, such as corporate culture or brand value, that could be correlated with both firm size and CEO pay.",
      "B": "It controls for time-varying aggregate shocks, such as business cycles or changes in investor sentiment, that affect all firms in a given year.",
      "C": "It solves the problem of reverse causality, where higher CEO pay might lead to an increase in firm size through better performance.",
      "D": "It corrects for measurement error in the firm size variable, which typically biases OLS estimates toward zero."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of a fundamental identification strategy in panel data econometrics: the use of fixed effects. The correct option (A) accurately describes the role of firm fixed effects in mitigating omitted variable bias from stable, unobserved firm-level factors. Distractor (B) describes the role of year fixed effects, not firm fixed effects. Distractor (C) is a plausible concern (endogeneity), but fixed effects do not fully solve reverse causality. Distractor (D) is incorrect; fixed-effects estimation can actually worsen attenuation bias from measurement error.",
    "qid": "424",
    "model_evaluation_result": {
      "model_raw_response": "A\n\n(Note: I've only returned the single correct answer as a string.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 425,
    "Question": "### Background\n\nThe paper calibrates its competitive assignment model to quantify the \"economics of superstars,\" where small differences in talent can justify large differences in pay due to the enormous scale of modern firms.\n\n### Data / Model Specification\n\nThe model is calibrated using three empirical facts:\n1.  Firm size distribution follows Zipf's Law (`α ≈ 1`).\n2.  CEO pay scales one-for-one with aggregate firm size (`γ ≈ 1`).\n3.  The cross-sectional elasticity of pay to own-firm size is `κ ≈ 1/3`.\n\nThe model links these via the equation `κ = γ - β/α`.\n\nThe percentage increase in a reference firm's value if it could hire the #1 CEO is given by:\n```latex\n\\text{Pct. Value Increase} = \\left(\\frac{\\alpha\\gamma}{\\beta}-1\\right) \\left(1-n_{*}^{-\\beta}\\right) \\frac{w_{*}}{S(n_{*})} \\quad \\text{(Eq. (1))}\n```\nFor the thought experiment, `n_*=250`, `w_* = $8.34 million`, and `S(n_*) = $25.0 billion`.\n\n### Question\n\nFirst, use the empirical facts to determine the calibrated value of the talent distribution parameter, `β`. Then, using this value and the provided data, calculate the percentage increase in firm #250's value if it could hire the #1 CEO.",
    "Options": {
      "A": "β ≈ 2/3; the value increase is approximately 0.016%.",
      "B": "β ≈ 1/3; the value increase is approximately 0.056%.",
      "C": "β ≈ 2/3; the value increase is approximately 530%.",
      "D": "β ≈ 1; the value increase is approximately 0.033%."
    },
    "Answer": "A",
    "pi_justification": "This item is a multi-step Computational Judgment problem that tests the full chain of reasoning from calibration to quantitative prediction. The user must first derive β ≈ 2/3 from 1/3 ≈ 1 - β/1. Then, they must plug this into the complex formula to get the final answer. The correct answer is (A). Distractor (B) is based on the high-fidelity error of incorrectly setting β=1/3 (confusing β with κ), which leads to a different calculated value. Distractor (C) confuses the value increase with the pay premium, another plausible error. Distractor (D) uses an incorrect β=1, which would imply κ=0.",
    "qid": "425",
    "model_evaluation_result": {
      "model_raw_response": "B) β = 1/2; the value change is approximately 5.6%.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 426,
    "Question": "### Background\n\nA key finding from the paper's calibrated model is that swapping the 250th-ranked CEO for the 1st-ranked CEO would increase the firm's value by 0.016%. Given the firm's market capitalization of $25 billion, this translates to an absolute value creation of approximately $4.0 million.\n\nThe paper argues that the dramatic rise in CEO pay is primarily due to the increase in firm scale over time.\n\n### Question\n\nConsider a counterfactual 1960s economy where the distributions of talent and firm size (`α, β, γ`) were the same as today, but the scale of firms was 100 times smaller. The 250th firm had a market capitalization of $250 million. In this economy, what would be the absolute dollar value created by swapping the #250 CEO for the #1 CEO, and what is the main conclusion from this comparison?",
    "Options": {
      "A": "Value created: $40,000. Conclusion: The rise in CEO pay is driven by the magnification of talent's value due to the enormous increase in firm scale.",
      "B": "Value created: $4.0 million. Conclusion: The rise in CEO pay is driven by a significant increase in the dispersion of CEO talent since the 1960s.",
      "C": "Value created: $400,000. Conclusion: The rise in CEO pay is primarily due to a decline in corporate governance, allowing CEOs to extract more rents.",
      "D": "Value created: $40,000. Conclusion: The rise in CEO pay is due to the fact that modern CEOs are vastly more talented than their 1960s counterparts."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Scenario Application strategy to test the core intuition of the paper's main quantitative result. The user must perform a simple calculation and draw the correct inference. The correct calculation is $4.0 million / 100 = $40,000 (or 0.016% of $250 million). The correct conclusion is that scale is the magnifier. Option (A) is correct. Option (B) gets the calculation wrong and draws a conclusion that contradicts the premise (that talent distribution is the same). Option (C) gets the calculation wrong (off by 10x) and offers an alternative explanation (governance) that is not the mechanism tested here. Option (D) has the right value but the wrong interpretation, contradicting the premise that the talent distribution is constant.",
    "qid": "426",
    "model_evaluation_result": {
      "model_raw_response": "C) Value Created: $40.00. Conclusion: the rise in CEO Pay is primarily due the decline in corporate Governance, allowing CEOs extract more Rents.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 427,
    "Question": "### Background\n\n**Research Question.** This problem analyzes why the optimal, zero-inflation Ramsey outcome is not a stable equilibrium for a government learning with an approximating model.\n\n**Setting.** A government's beliefs are represented by coefficients `γ` in a perceived Phillips curve. A self-confirming equilibrium (SCE) is a set of beliefs that, when acted upon, generates data that confirms those same beliefs. The Ramsey outcome (zero inflation) is supported by the belief that the Phillips curve is vertical.\n\n### Data / Model Specification\n\nThe government's **approximating model** is a linear Phillips curve:\n\n```latex\nU_{n} = \\gamma_{1}\\pi_{n} + \\gamma^{\\prime}_{-1}X_{n-1} + \\eta_{n} \\quad \\text{(Eq. (1))}\n```\n\nThe **true data generating process (DGP)** embodies the natural rate hypothesis:\n\n```latex\nU_{n} = u - \\theta(\\pi_{n} - \\hat{x}_{n}) + \\sigma_{1}W_{1n} \\quad \\text{(Eq. (2))}\n```\n\nwhere `π_n` is inflation, `x̂_n` is expected inflation, `θ > 0` is the true trade-off parameter, and `W_1n` is a shock independent of inflation shocks.\n\n### Question\n\nSuppose the government holds the 'Ramsey' belief that the Phillips curve is vertical (`γ_1 = 0`) and, as a result, sets the systematic part of inflation `x_n` to zero. Given that the public has rational expectations (`x̂_n = x_n`), what value for the Phillips curve slope (`γ_1`) would the government's least squares estimation recover from the data generated by this policy?",
    "Options": {
      "A": "-θ",
      "B": "0",
      "C": "θ",
      "D": "u"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core insight of why the optimal (Ramsey) outcome is not a self-confirming equilibrium. It uses a 'Computational Judgment' strategy, requiring the student to derive the OLS estimand under a hypothetical scenario. Distractors are designed based on common errors: (B) represents the flawed logic that beliefs are always self-confirming, (C) is a sign error, and (D) is a true but irrelevant parameter from the model.",
    "qid": "427",
    "model_evaluation_result": {
      "model_raw_response": "C) θ\n\n(Note: I've only provided the answer as per the required format.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 428,
    "Question": "### Background\n\n**Research Question.** This problem seeks to identify the specific nature of the learning bias that drives a government's beliefs away from a self-confirming equilibrium.\n\n**Setting.** In a simplified static model, the escape dynamics are described by augmenting the mean dynamics with a perturbation term, `v̇`. This term represents the difference between the expected update under a special conditional shock distribution (which generates the escape) and the unconditional shock distribution (which generates the mean dynamics).\n\n### Data / Model Specification\n\nThe escape dynamics are governed by the ODE:\n\n```latex\n\\dot{\\gamma} = R^{-1}\\bar{g}(\\gamma) + \\dot{v} \\quad \\text{(Eq. (1))}\n```\n\nThe dominant escape path is generated by a specific conditional shock distribution, leading to the following dynamics:\n\n```latex\n\\dot{\\gamma}_{escape} = R^{-1}\\left[\\begin{array}{c}{{-\\frac{\\gamma_{-1}\\gamma_{1}}{1+\\gamma_{1}^{2}}\\left(u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}\\right)-(\\theta+\\gamma_{1})\\sigma_{2}^{2}+\\sigma_{1}\\sigma_{2}}}\\\\ {{u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}}}\\end{array}\\right] \\quad \\text{(Eq. (2))}\n```\n\nThe unconditional mean dynamics are:\n\n```latex\n\\dot{\\gamma}_{mean} = R^{-1}\\bar{g}(\\gamma) = R^{-1}\\left[\\begin{array}{c}{{-\\frac{\\gamma_{-1}\\gamma_{1}}{1+\\gamma_{1}^{2}}\\left(u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}\\right)-(\\theta+\\gamma_{1})\\sigma_{2}^{2}}}\\\\ {{u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}}}\\end{array}\\right] \\quad \\text{(Eq. (3))}\n```\n\n### Question\n\nGiven the equations for the escape dynamics (Eq. (2)) and the mean dynamics (Eq. (3)), what is the explicit formula for the perturbation term `v̇` that drives the dominant escape path?",
    "Options": {
      "A": "```latex\nR^{-1} \\left[\\begin{array}{c}{\\sigma_{1}\\sigma_{2}}\\\\ {0}\\end{array}\\right]\n```",
      "B": "```latex\nR^{-1} \\left[\\begin{array}{c}{0}\\\\ {u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}}\\end{array}\\right]\n```",
      "C": "```latex\nR^{-1} \\left[\\begin{array}{c}{-\\sigma_{1}\\sigma_{2}}\\\\ {0}\\end{array}\\right]\n```",
      "D": "```latex\n0\n```"
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to perform the algebraic derivation of the key perturbation term `v̇`. It uses a 'Computational Judgment' strategy. The distractors are designed based on plausible algebraic errors: (B) incorrectly identifies the non-zero component, (C) represents a sign error, and (D) represents the conceptual error of there being no perturbation.",
    "qid": "428",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: This answer is based on the given equations, specifically comparing the escape dynamics equation (2) with the mean dynamics equation (3). The perturbation term v̇ is the difference between these two equations, which can be calculated by subtracting Eq. (3) from Eq. (2). This yields the correct answer, C.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 429,
    "Question": "### Background\n\n**Research Question.** This problem seeks to identify the specific nature of the learning bias that drives a government's beliefs away from a self-confirming equilibrium.\n\n**Setting.** In a simplified static model, the escape dynamics are driven by a perturbation term, `v̇`, which represents a systematic learning bias induced by a rare sequence of shocks. This term is derived to be:\n\n```latex\n\\dot{v} = R^{-1} \\left[\\begin{array}{c}{\\sigma_{1}\\sigma_{2}}\\\\ {0}\\end{array}\\right]\n```\n\nThis bias affects the government's estimate of the Phillips curve slope, `γ_1`.\n\n### Question\n\nThe perturbation term `v̇` represents the learning bias induced by the 'most likely unlikely' sequence of shocks. What is the economic interpretation of this shock sequence?",
    "Options": {
      "A": "A persistent sequence of shocks where the unemployment shock (`W_1n`) and inflation shock (`W_2n`) are positively correlated.",
      "B": "A persistent sequence of shocks where the unemployment shock (`W_1n`) and inflation shock (`W_2n`) are negatively correlated.",
      "C": "A sequence of unusually large, but still independent, inflation shocks (`W_2n`).",
      "D": "A sequence where the government's policy shocks are perfectly anticipated by the public."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to connect the mathematical result for `v̇` to its underlying economic cause. It uses a 'Reverse-Reasoning' strategy. The distractors represent common conceptual errors: (B) is the conceptual opposite (negative correlation), (C) suggests an alternative but incorrect mechanism (shock size vs. correlation), and (D) is a true statement about the model's assumptions that is irrelevant to the specific escape mechanism.",
    "qid": "429",
    "model_evaluation_result": {
      "model_raw_response": "B) A sequence that is persistently negatively correlated between the unemployment shock and inflation shock.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 430,
    "Question": "### Background\n\n**Research Question.** This problem requires interpreting empirical results from a GMM estimation to distinguish between two competing theories of preference heterogeneity: whether the mean of consumption growth (linked to the Rate of Time Preference, RTP) or the volatility of consumption growth (linked to the Intertemporal Elasticity of Substitution, IES) varies with household wealth.\n\n**Setting / Institutional Environment.** The analysis is based on GMM estimation of a consumption growth model using a panel of Indian households. The model is designed to separately estimate a parameter for wealth-dependent mean growth (`$b_y$`) and a parameter for wealth-dependent volatility (`$\\gamma$`).\n\n**Variables & Parameters.**\n- `$\\gamma$`: Subsistence parameter, linked to wealth-varying IES. A positive `$\\gamma$` implies the IES rises with wealth.\n- `$b_y$`: Coefficient on a permanent income proxy, linked to wealth-varying RTP. A non-zero `$b_y$` implies the RTP varies with wealth.\n- `$J$`-statistic: Hansen's test statistic for overidentifying restrictions.\n- `$C$`-statistic: A likelihood ratio-type test statistic for parameter restrictions, calculated as a difference in `$J$`-statistics.\n\n---\n\n### Data / Model Specification\n\nThe estimated statistical model is:\n```latex\n\\ln[C_{h}(t+1) - \\gamma] - \\ln[C_{h}(t) - \\gamma] = \\phi(t) + b_{y} y_{h}^{c} + v_{h}(t) \\quad \\text{(Eq. 1)}\n```\nKey results for total consumption, based on Table 2 in the paper, are summarized below.\n\n**Table 1: GMM Results for Total Consumption**\n| Model Specification | `$\\hat{\\gamma}$` (s.e.) | `$\\hat{b}_y$` (s.e.) | J-statistic (p-value %) | C-statistic (p-value %) | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1. Unrestricted | 177.6 (6.70) | -0.021 (0.049) | 34.44 (18.7) | 0.255 (61.4) | Tests `$b_y=0$` vs Model 2 |\n| 2. Restricted (`$b_y=0$`) | 177.2 (7.45) | 0 (imposed) | 34.70 (21.5) | — | Preferred Specification |\n| 3. Restricted (`$\\gamma=b_y=0$`) | 0 (imposed) | 0 (imposed) | 98.89 (0.0) | 64.428 (0.0) | Tests joint null vs Model 2 |\n\n---\n\n### The Question\n\nConsidering the full set of results in Table 1, which statement best explains why the model in Row 2 (`$b_y=0$`) is the authors' preferred specification?",
    "Options": {
      "A": "The unrestricted model (Row 1) is preferred because it has the highest p-value on its J-statistic, indicating the best model fit.",
      "B": "The simplest model (Row 3) is chosen because its parameters are easiest to interpret, despite its low p-value.",
      "C": "The model in Row 2 is selected because the data strongly reject the simplest model (Row 3), but do not reject the simplification from the full model (Row 1) to the model in Row 2.",
      "D": "The model in Row 2 is chosen because the C-statistic in Row 1 shows that `$b_y$` is significantly different from zero, confirming its importance."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the user's ability to synthesize multiple hypothesis tests (specifically, nested model tests using C-statistics) to understand the logic of model selection. It uses a Computational Judgment strategy. The distractors represent common fallacies: misinterpreting the J-test (A), ignoring statistical evidence (B), and misreading the result of a hypothesis test (D).",
    "qid": "430",
    "model_evaluation_result": {
      "model_raw_response": "C) The data strongly reject both the simplest model and the full model, but do not strongly reject the simplifications from the full to the preferred model.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 431,
    "Question": "### Background\n\n**Research Question.** This problem requires interpreting empirical results from a GMM estimation to distinguish between two competing theories of preference heterogeneity: whether the mean of consumption growth (linked to the Rate of Time Preference, RTP) or the volatility of consumption growth (linked to the Intertemporal Elasticity of Substitution, IES) varies with household wealth.\n\n**Setting / Institutional Environment.** The analysis is based on GMM estimation of a consumption growth model using a panel of Indian households. The model is designed to separately estimate a parameter for wealth-dependent mean growth (`$b_y$`) and a parameter for wealth-dependent volatility (`$\\gamma$`). The GMM estimation uses instruments such as land holding class, which are assumed to be correlated with true permanent income but not with the model's error term.\n\n**Variables & Parameters.**\n- `$\\gamma$`: Subsistence parameter, linked to wealth-varying IES. A positive `$\\gamma$` implies the IES rises with wealth.\n- `$b_y$`: Coefficient on a permanent income proxy, linked to wealth-varying RTP. A non-zero `$b_y$` implies the RTP varies with wealth.\n- `$J$`-statistic: Hansen's test statistic for overidentifying restrictions.\n- `$C$`-statistic: A likelihood ratio-type test statistic for parameter restrictions, calculated as a difference in `$J$`-statistics.\n\n---\n\n### Data / Model Specification\n\nThe estimated statistical model is:\n```latex\n\\ln[C_{h}(t+1) - \\gamma] - \\ln[C_{h}(t) - \\gamma] = \\phi(t) + b_{y} y_{h}^{c} + v_{h}(t) \\quad \\text{(Eq. 1)}\n```\nKey results for total consumption, based on Table 2 in the paper, are summarized below.\n\n**Table 1: GMM Results for Total Consumption**\n| Model Specification | `$\\hat{\\gamma}$` (s.e.) | `$\\hat{b}_y$` (s.e.) | J-statistic (p-value %) | C-statistic (p-value %) | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1. Unrestricted | 177.6 (6.70) | -0.021 (0.049) | 34.44 (18.7) | 0.255 (61.4) | Tests `$b_y=0$` vs Model 2 |\n| 2. Restricted (`$b_y=0$`) | 177.2 (7.45) | 0 (imposed) | 34.70 (21.5) | — | Preferred Specification |\n| 3. Restricted (`$\\gamma=b_y=0$`) | 0 (imposed) | 0 (imposed) | 98.89 (0.0) | 64.428 (0.0) | Tests joint null vs Model 2 |\n\n---\n\n### The Question\n\nThe `$J$`-statistic for the preferred model (Row 2) has a p-value of 21.5%. What is the primary implication of this result for the study's conclusion?",
    "Options": {
      "A": "The model's key parameters, `$\\gamma$` and `$\\phi(t)$`, are jointly statistically significant.",
      "B": "The model's overidentifying restrictions are not rejected, supporting the validity of the instruments and model specification.",
      "C": "The model has low explanatory power, as a high p-value indicates a poor fit to the data.",
      "D": "The instruments used in the estimation are weak and do not have sufficient predictive power for the endogenous variables."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the user's understanding of a critical specification test in GMM, Hansen's J-test. It uses a Reverse-Reasoning strategy (inferring the meaning of a test result). The distractors are classic, high-frequency misconceptions about the J-test, distinguishing it from tests of parameter significance (A), goodness-of-fit (C), and instrument strength (D).",
    "qid": "431",
    "model_evaluation_result": {
      "model_raw_response": "B) The.model's overidentifyin'g restrictions are not.rejected, supporting the.validity of the.instruments and.model specification.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 432,
    "Question": "### Background\n\n**Research Question.** This problem explores the core identification challenge of empirically distinguishing between two competing hypotheses for consumption inequality: that richer households are more patient (wealth-varying Rate of Time Preference, RTP) versus that they are more willing to substitute consumption over time (wealth-varying Intertemporal Elasticity of Substitution, IES).\n\n**Setting / Institutional Environment.** The analysis is framed within a complete markets model with heterogeneous agents. The key to the identification strategy is the use of panel data where the underlying intertemporal price of consumption fluctuates, experiencing periods of both growth and decline.\n\n**Variables & Parameters.**\n- `$\\hat{C}^{h}(t)$`: Log consumption growth for household `h` at time `t`.\n- `$\\delta_{h}$`: The household-specific Rate of Time Preference (RTP).\n- `$\\sigma_{h}(t)$`: The household-specific Intertemporal Elasticity of Substitution (IES).\n- `$r(t)$`: The log real interest rate, or intertemporal price of consumption.\n\n---\n\n### Data / Model Specification\n\nThe approximate relationship between individual consumption growth, preferences, and prices is given by:\n```latex\n\\hat{C}^{h}(t) \\cong \\sigma_{h}(t) \\{r(t) - \\delta_{h}\\} \\quad \\text{(Eq. 1)}\n```\n\n---\n\n### The Question\n\nConsider a hypothetical economy where the real interest rate `$r(t)$` is always high and constant over time (`$r(t) = r > \\delta_h$` for all households). In this scenario, why would it be difficult to empirically distinguish between a wealth-varying RTP model and a wealth-varying IES model?",
    "Options": {
      "A": "Both models predict that richer households have more volatile consumption growth.",
      "B": "The two models make opposite predictions about consumption growth, which would cancel out.",
      "C": "Both models predict that richer households have a higher rate of consumption growth, making them observationally equivalent.",
      "D": "A constant interest rate makes it impossible to estimate any preference parameters."
    },
    "Answer": "C",
    "pi_justification": "This item uses a Scenario Application strategy to test the core concept of observational equivalence, a key hurdle the paper seeks to overcome. The distractors target specific misunderstandings of the models' predictions and the implications of a constant interest rate.",
    "qid": "432",
    "model_evaluation_result": {
      "model_raw_response": "C) Bothmodels predict that richerhouseholds have a higher ratofconsumptiongrowth,makingthemobservationallyequivalent.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 433,
    "Question": "### Background\n\n**Research Question.** This problem explores the core identification challenge of empirically distinguishing between two competing hypotheses for consumption inequality: that richer households are more patient (wealth-varying Rate of Time Preference, RTP) versus that they are more willing to substitute consumption over time (wealth-varying Intertemporal Elasticity of Substitution, IES).\n\n**Setting / Institutional Environment.** The analysis is framed within a complete markets model with heterogeneous agents. The key to the identification strategy is the use of panel data where the underlying intertemporal price of consumption fluctuates, experiencing periods of both growth and decline.\n\n**Variables & Parameters.**\n- `$\\hat{C}^{h}(t)$`: Log consumption growth for household `h` at time `t`.\n- `$\\delta_{h}$`: The household-specific Rate of Time Preference (RTP).\n- `$\\sigma_{h}(t)$`: The household-specific Intertemporal Elasticity of Substitution (IES).\n- `$r(t)$`: The log real interest rate, or intertemporal price of consumption.\n\n---\n\n### Data / Model Specification\n\nThe approximate relationship between individual consumption growth, preferences, and prices is given by:\n```latex\n\\hat{C}^{h}(t) \\cong \\sigma_{h}(t) \\{r(t) - \\delta_{h}\\} \\quad \\text{(Eq. 1)}\n```\n\n---\n\n### The Question\n\nThe authors argue that their data, which includes periods of both aggregate consumption growth and shrinkage, allows them to distinguish between the two competing models. What is the distinct, testable prediction made by the wealth-varying IES model that is NOT made by the wealth-varying RTP model?",
    "Options": {
      "A": "Richer households have a higher average consumption growth rate across all periods.",
      "B": "Richer households have more volatile consumption growth, growing faster in good times and shrinking faster in bad times.",
      "C": "Richer households are more patient, which is directly measured by their savings.",
      "D": "The consumption growth of richer households is less sensitive to interest rate changes."
    },
    "Answer": "B",
    "pi_justification": "This item assesses understanding of the paper's central identification strategy. It uses a Reverse-Reasoning strategy (what prediction allows identification?). The distractors include a non-unique prediction (A), an irrelevant statement (C), and a direct contradiction of the theory (D), making for a sharp assessment.",
    "qid": "433",
    "model_evaluation_result": {
      "model_raw_response": "D) Theconsumption growth of richer householdis less sensitive tointerest rate changes. \n\n(Note: The correct answer is D) The consumptiongrowth of richerhouseholds is less sensitive tote interest ratechanges.)",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 434,
    "Question": "### Background\n\n**Research Question.** This problem explores the critical condition that determines whether a durable-goods monopolist can credibly threaten to maintain high prices in a subgame with one high-valuation buyer (`H`) and `n_0^L` low-valuation buyers (`L`).\n\n**Setting / Institutional Environment.** The seller considers two strategies: (i) a \"Coasian\" strategy of selling to all `n_0^L+1` buyers immediately at price `L`, and (ii) a \"Pacman\" strategy of selling to the high-valuation buyer today at their reservation price `w` and then selling to the `n_0^L` low-valuation buyers in the next period at price `L`.\n\n**Variables & Parameters.**\n- `H`, `L`: High and low buyer valuations, `H > L > 0`.\n- `n_0^L`: The number of low-valuation buyers.\n- `δ`: Per-period discount factor, `δ ∈ (0,1)`.\n\n---\n\n### Data / Model Specification\n\nThe reservation price `w` that makes a high-valuation buyer indifferent between buying today and waiting one period for a price of `L` is given by:\n```latex\nw = (1-\\delta)H + \\delta L \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nUnder which of the following conditions does the \"Pacman\" strategy yield a strictly higher profit for the seller than the \"Coasian\" strategy?\n",
    "Options": {
      "A": "`n_0^L L > H - L`",
      "B": "`H - L > n_0^L L`",
      "C": "`H > n_0^L L`",
      "D": "`H - L > (n_0^L + 1)L`"
    },
    "Answer": "B",
    "pi_justification": "This item assesses the student's ability to formally derive the key condition for the Pacman outcome to be credible, which is the core of the paper's first result. It uses a Computational Judgment strategy. The distractors are designed based on common errors: (A) reverses the inequality, representing the condition under which a Coasian equilibrium is possible; (C) omits the `-L` term from the price discrimination premium, a plausible simplification error; (D) incorrectly compares the premium to the total Coasian profit instead of just the opportunity cost of delaying sales to L-buyers.",
    "qid": "434",
    "model_evaluation_result": {
      "model_raw_response": "D) `D` `H - (n_1^L +1) L`",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 435,
    "Question": "### Background\n\n**Research Question.** This problem applies the paper's finding on equilibrium multiplicity to a specific market scenario, including a regulatory intervention.\n\n**Setting / Institutional Environment.** A durable-goods monopolist sells to two high-valuation (`H`) buyers and `n_0^L` low-valuation (`L`) buyers. When the condition `n_0^L L ≥ H - L` holds, a range of total profits for the seller can be supported in real-time efficient Subgame Perfect Equilibria (SPEs) as the time between offers shrinks (`δ→1`). The range is bounded by the low-profit \"Coasian\" outcome and the high-profit \"Pacman\" (perfect price discrimination) outcome.\n\n---\n\n### Data / Model Specification\n\nConsider a market with the following parameters:\n- High valuation, `H = 10`\n- Low valuation, `L = 2`\n- Number of low-valuation buyers, `n_0^L = 5`\n\nA regulator imposes a price ceiling `p_max = 8`.\n\n---\n\n### Question\n\nGiven the market parameters and the price ceiling, what is the new range of total profits for the seller that can be supported in real-time efficient SPEs?\n",
    "Options": {
      "A": "[14, 30]",
      "B": "[14, 26]",
      "C": "[14, 56]",
      "D": "[16, 26]"
    },
    "Answer": "B",
    "pi_justification": "This item tests the application of the paper's folk theorem result to a specific policy scenario (a price ceiling). It uses a Scenario Application strategy, requiring multi-step calculation. The distractors target predictable errors: (A) correctly calculates the baseline profit range but ignores the effect of the price ceiling on the maximum profit; (C) correctly calculates the minimum (Coasian) profit but miscalculates the maximum profit by assuming the seller charges the ceiling price to all buyers; (D) incorrectly calculates the minimum (Coasian) profit, possibly by assuming the price ceiling affects the lowest possible outcome.",
    "qid": "435",
    "model_evaluation_result": {
      "model_raw_response": "C) [C] 14, 28",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 436,
    "Question": "### Background\n\n**Research Question.** After estimating the causal response of monetary policy to the stock market, a crucial step is to assess its real-world plausibility. This can be done by comparing the estimated response to a theoretical benchmark derived from the stock market's known effects on aggregate demand via the wealth channel, as disciplined by a standard monetary policy rule.\n\n### Data / Model Specification\n\nThe paper's primary estimate for the policy response parameter, `β`, is 0.0214. For the macroeconomic plausibility check, use the following parameters, which are stated or implied in the paper:\n\n- Total US GDP: $10 trillion\n- Household equity holdings: $11.6 trillion\n- Marginal Propensity to Consume (MPC) out of stock market wealth: 4% (0.04)\n- Taylor rule coefficient on the output gap: 1.0\n\n### Question\n\nBased on the provided macroeconomic parameters, what is the 'warranted' change in the federal funds rate (in basis points) to fully offset the aggregate demand impact of an unexpected 5 percent rise in the S&P 500?",
    "Options": {
      "A": "11.6 basis points",
      "B": "14.3 basis points",
      "C": "23.2 basis points",
      "D": "58.0 basis points"
    },
    "Answer": "C",
    "pi_justification": "This item assesses the ability to perform a multi-step 'back-of-the-envelope' calculation to evaluate the plausibility of an econometric finding. It uses a Computational Judgment strategy. The correct answer requires correctly chaining four calculation steps. Distractor A is a Formula Misuse error, using an alternative parameter mentioned in the text (a Taylor coefficient of 0.5). Distractor B is a 'True but Irrelevant' conceptual error, representing the paper's actual estimated policy response rather than the warranted response. Distractor D represents a plausible unit or step-omission error.",
    "qid": "436",
    "model_evaluation_result": {
      "model_raw_response": "C) 58,0 basis points. \n\nNote: To calculate the warranted change in the fed funds rate, we need to follow these steps:\n\n1. Calculate the change in aggregate demand: ΔY = MPC * ΔW = 0.04 * 0.05 * $11.64T = $4.65 billion.\n2. Convert this to percentage change in GDP: ΔY / $10T = 0,0465.\n3. Calculate the required change in federal funds rate using the Taylor rule: Δr = -1.0 * ΔY / (1 + 0.5) = -1 * 0,00465 / 1.5 = -0,0031 = 0%,058.\n4. Convert this percentage change to basis points: 0,058 * 100 = 58 basis points.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 437,
    "Question": "### Background\n\n**Research Question.** A key goal of the paper is to determine not just the size of the Federal Reserve's reaction to the stock market, but also the *reason* for it. The central question is whether the Fed reacts to stock prices as an independent goal, or only because stock prices affect the Fed's primary mandate of macroeconomic stability.\n\n### Question\n\nThe paper's empirical method estimates that a 5% rise in the S&P 500 leads to an actual policy response equivalent to a 14 basis point increase in the expected federal funds rate. A separate 'back-of-the-envelope' calculation, based on the wealth effect on aggregate demand, suggests a 'warranted' response to fully stabilize the economy would be in the range of 12-23 basis points. What is the most accurate conclusion from comparing these two results?",
    "Options": {
      "A": "The Fed's response is consistent with its mandate to stabilize the macroeconomy, reacting to stock prices only to the extent they affect aggregate demand.",
      "B": "The Fed systematically under-reacts to stock market shocks, suggesting a reluctance to intervene based on asset prices.",
      "C": "The Fed appears to be targeting stock prices directly, as the response is too rapid and large for a purely macroeconomic reaction.",
      "D": "The estimated response is significantly smaller than the warranted response, indicating the wealth effect on consumption is overestimated."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to draw a high-level conclusion by comparing an empirical estimate to a theoretical benchmark. It uses a Reverse-Reasoning strategy, asking for the interpretation that best explains the evidence. The correct option (A) accurately reflects the paper's main conclusion. Distractor B is an 'Almost Right' misinterpretation of the numbers (14 is within the 12-23 range, not systematically under). Distractor C is a 'Conceptual Opposite' that posits the very behavior the paper argues against. Distractor D draws an incorrect causal inference from the comparison.",
    "qid": "437",
    "model_evaluation_result": {
      "model_raw_response": "C) TheFed appears to be targetting stock prices directly as the responseis too rapid and largfor a purely macroconomic reaction.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 438,
    "Question": "### Background\n\n**Research Question:** This question analyzes the welfare implications of using asymmetric reserve prices, specifically how lowering the reserve price for one group can create a Pareto improvement for all active bidders.\n\n**Setting / Institutional Environment:** Consider a symmetric first-price auction with two groups of bidders (1 and 2) facing reserve prices `r_1` and `r_2` respectively, with `r_1 < r_2`. The auction is in a semi-separating equilibrium. The paper finds that if the reserve price for the favored group-1 bidders (`r_1`) is lowered while `r_2` is held fixed, the expected welfare of the unfavored group-2 bidders surprisingly increases.\n\n### The Question\n\nAccording to the paper's analysis, which of the following best explains the economic mechanism through which unfavored group-2 bidders benefit from this policy change?",
    "Options": {
      "A": "The change incentivizes group-1 bidders with lower valuations to bid less aggressively and delays their entry into direct competition with group-2 bidders, softening the overall competitive pressure on group 2.",
      "B": "Group-1 bidders are forced to bid more aggressively to signal their strength despite the lower reserve price, which paradoxically makes it easier for high-valuation group-2 bidders to win.",
      "C": "The lower `r_1` increases the total number of bidders participating in the auction, which increases the seller's expected revenue, and these gains are then shared with all bidders.",
      "D": "The bidding strategy of group-2 bidders, `β_2(s)`, is directly a function of `r_1`, and a lower `r_1` mechanically allows them to place lower winning bids for any given valuation."
    },
    "Answer": "A",
    "pi_justification": "This Single Choice item assesses the student's grasp of the core economic intuition behind Theorem 4, a key and counter-intuitive welfare result. It uses a Reverse-Reasoning strategy, asking for the cause of a given effect. The distractors are designed to be highly plausible: (B) is the conceptual opposite, representing a common intuitive error; (C) confuses revenue with welfare and misattributes the cause; (D) points to a direct functional dependence that does not exist, testing a more technical misunderstanding of the model.",
    "qid": "438",
    "model_evaluation_result": {
      "model_raw_response": "D) The betting strategy of group 2 bidders is directly a fun",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 439,
    "Question": "### Background\n\n**Research Question:** This question analyzes the formal proof of the welfare implications of using asymmetric reserve prices.\n\n**Setting / Institutional Environment:** Consider a symmetric first-price auction with two groups of bidders (1 and 2) with `N_1` and `N_2` members, respectively. They face reserve prices `r_1` and `r_2` with `r_1 < r_2`. The auction is in a semi-separating equilibrium, characterized by a critical valuation `hat{s}`.\n\n### Data / Model Specification\n\nThe equilibrium expected utility for a group-2 (unfavored) bidder with valuation `s` in the range `s ∈ [r_2, hat{s}]` is given by:\n\n```latex\nV_{2}(r_{1},r_{2}|s) = F(\\hat{s})^{N_{1}}\\displaystyle\\int_{r_{2}}^{s}F(z)^{N_{2}-1}d z \\quad \\text{(Eq. (1))}\n```\n\nA key result from the paper (Lemma 2) states that if `r_1` is lowered to `r_1' < r_1` (holding `r_2` fixed), the critical valuation `hat{s}` increases to `hat{s}' > hat{s}`.\n\n### The Question\n\nConsider a group-2 bidder with valuation `s` such that `r_2 < s < hat{s}`. When `r_1` is lowered to `r_1'`, the critical threshold increases to `hat{s}'`. Based on the provided expression for expected utility in Eq. (1), what is the direct mathematical reason for this bidder's welfare increase?",
    "Options": {
      "A": "The bidder's probability of winning against all group-1 bidders, represented by the term `F(hat{s})^{N_1}`, increases because `hat{s}` increases to `hat{s}'`.",
      "B": "The integral term `∫_{r_2}^{s} F(z)^{N_2-1} dz` increases in value because the integration range `[r_2, s]` expands.",
      "C": "The term `F(hat{s})^{N_1}` decreases because a higher `hat{s}` signifies stronger competition from group 1, reducing the probability of winning.",
      "D": "The bidder's valuation `s` is now in a different segment of the piecewise utility function, which uses a more favorable formula to calculate payoff."
    },
    "Answer": "A",
    "pi_justification": "This Single Choice item tests the student's ability to perform a computational judgment by applying a key lemma to the formal expression for expected utility. It isolates the core mathematical step from the proof in the original QA. The distractors are based on specific errors: (B) misinterprets the integral as changing; (C) makes a sign error in reasoning about the effect of `hat{s}`; (D) incorrectly applies the piecewise function definition, a common error in such problems.",
    "qid": "439",
    "model_evaluation_result": {
      "model_raw_response": "B) The term ∫_{r2}^{S} F(z)^(N2-1)dz increases in valuebecause the integration range [r2, s ] expands.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 440,
    "Question": "### Background\n\nThis problem examines the theoretical structure of the incremental Expected Utility (EU) advantage model, a probabilistic theory of choice under risk. The model uses a binary choice probability function, $P(p,q)$, representing the probability that an individual chooses gamble $p$ over gamble $q$.\n\n### Data / Model Specification\n\nLet $x_1 > x_2 > \\cdots > x_n$ be the ordered outcomes for gambles $p$ or $q$. Let $\\Delta_k(p,q) = p\\{\\geq x_k\\} - q\\{\\geq x_k\\}$ be the difference in cumulative probabilities.\n\nThe total incremental expected utility advantage of $p$ over $q$ is:\n\n```latex\n\\mathcal{A}(p,q) = \\sum_{\\{k: \\Delta_k(p,q)>0\\}} \\Delta_k(p,q)[u(x_k) - u(x_{k+1})]\n```\n\nThe difference in standard expected utilities relates to these advantages as:\n\n```latex\nE(u,p) - E(u,q) = \\mathcal{A}(p,q) - \\mathcal{A}(q,p)\n```\n**Eq. (1)**\n\nThe probabilistic choice rule is given by:\n\n```latex\n\\frac{P(p,q)}{P(q,p)} = \\rho\\left(\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)}\\right)\n```\n**Eq. (2)**\n\nwhere $\\rho(\\cdot)$ is an increasing function with $\\rho(1)=1$.\n\n### Question\n\nThe incremental EU advantage model is consistent with standard EU theory, meaning $P(p,q) > 1/2$ if and only if $E(u,p) > E(u,q)$. The proof of this consistency relies on a chain of equivalences. Which property of the function $\\rho(\\cdot)$ is essential for ensuring that the odds ratio $\\frac{P(p,q)}{P(q,p)} > 1$ is equivalent to the advantage ratio $\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)} > 1$?",
    "Options": {
      "A": "$\\rho$ is strictly increasing and $\\rho(1)=1$",
      "B": "$\\rho(0)=0$",
      "C": "$\\rho(z)\\rho(1/z)=1$",
      "D": "$\\rho$ is continuous"
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the logical bridge between the probabilistic choice rule and standard EU preference ordering. It uses a Reverse-Reasoning strategy, asking the student to identify the critical assumption (properties of rho) that makes the derivation work. Distractors are designed based on other true-but-irrelevant properties of rho (B, C) or a property explicitly stated as not required (D), testing for precise comprehension of the model's structure.",
    "qid": "440",
    "model_evaluation_result": {
      "model_raw_response": "C) $\\ρ(z)\\ρ(1/z) = 1$",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 441,
    "Question": "### Background\n\nThis problem examines the theoretical structure of the incremental Expected Utility (EU) advantage model, a probabilistic theory of choice under risk. The model uses a binary choice probability function, $P(p,q)$, representing the probability that an individual chooses gamble $p$ over gamble $q$.\n\n### Data / Model Specification\n\nThe probabilistic choice rule is given by:\n\n```latex\n\\frac{P(p,q)}{P(q,p)} = \\rho\\left(\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)}\\right)\n```\n**Eq. (1)**\n\nwhere $\\mathcal{A}(p,q)$ is the incremental advantage of $p$ over $q$. The model is consistent with standard EU theory, such that $\\mathcal{A}(p,q) > \\mathcal{A}(q,p)$ if and only if $E(u,p) > E(u,q)$.\n\n### Question\n\nConsider a specific functional form for the choice function, $\\rho(z) = z^\\gamma$, where $\\gamma > 0$. This parameter governs the stochasticity of choice. In the case where gamble $p$ has a higher expected utility than gamble $q$, what is the limit of the choice probability $P(p,q)$ as the determinism parameter $\\gamma$ approaches infinity ($\\gamma \\to \\infty$)?",
    "Options": {
      "A": "1",
      "B": "1/2",
      "C": "0",
      "D": "$\\infty$"
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to perform a Computational Judgment by analyzing the limiting behavior of the model. It assesses whether the student can connect the model's parameters to the benchmark deterministic EU theory. Distractors are based on common mathematical and conceptual errors in limit calculations: convergence to indifference (B), sign error (C), and confusing the limit of the probability with the limit of its components (D).",
    "qid": "441",
    "model_evaluation_result": {
      "model_raw_response": "C) 1\n\nNote: The choice probability P(p,q) is given by the ratio of the two sides of equation (1). As γ → ∞, the right-hand side of equation (2) approaches 0, since z^γ → 0 as z → 0 for γ > 0. Therefore, the choice probability P(q,p) also approaches 0. Since P(p,q)/P(q,p) = lim(P(p,q)/[1-P(p,q)] as P(q,p)->0, the limit of P(p,q), as γ → ∪∞, is 1.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 442,
    "Question": "### Background\n\n**Research Question.** This problem provides a complete, worked-out example of a buyer-optimal information structure for the uniform prior case, including a formal proof of its robustness against seller extensions.\n\n**Setting / Institutional Environment.** The buyer's valuation `v` is drawn from a uniform distribution on `[0, 1]`. From this, we can calculate the seller's reservation profit (her payoff under perfect information) as `Π* = max_p p(1-p) = 1/4`, and the maximum social surplus as `E[v] = 1/2`. The regulator's goal is to find an extensionproof information structure that maximizes buyer surplus.\n\n---\n\n### Data / Model Specification\n\nThe proposed buyer-optimal information structure for the uniform case is as follows: if `v > 1/2`, the valuation is perfectly revealed; if `v ≤ 1/2`, valuations are pooled such that the posterior mean is always `1/4`. This induces the following CDF of posterior valuations:\n```latex\nH(w)=\\left\\{\\begin{array}{ll} 0 & \\mathrm{if} \\; w<1/4 \\\\ 1/2 & \\mathrm{if} \\; 1/4 \\leq w \\leq 1/2 \\\\ w & \\mathrm{if} \\; w > 1/2 \\end{array}\\right. \\quad \\text{(Eq. 1)}\n```\n\n---\n\nBased on the induced distribution of posterior valuations `H(w)` in Eq. (1), what are the seller's profit (Π) and the buyer's surplus (U) if the seller sets the optimal price that is most favorable to the buyer?",
    "Options": {
      "A": "Π = 1/4, U = 1/4",
      "B": "Π = 1/4, U = 1/8",
      "C": "Π = 1/8, U = 3/8",
      "D": "Π = 1/2, U = 0"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to calculate equilibrium payoffs from a given distribution of posterior valuations, a foundational skill in this literature. It uses a Computational Judgment strategy. The distractors are designed based on plausible errors: (B) correctly identifies the seller's maximum profit but calculates buyer surplus assuming the seller chooses the higher optimal price (p=1/2); (C) results from a common error in calculating the probability of trade at p=1/4 (assuming it's 1/2); (D) represents a conceptual error where the seller captures all social surplus.",
    "qid": "442",
    "model_evaluation_result": {
      "model_raw_response": "B) Π=1/4,U=1/8",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 443,
    "Question": "### Background\n\n**Research Question.** This problem provides a complete, worked-out example of a buyer-optimal information structure for the uniform prior case, including a formal proof of its robustness against seller extensions.\n\n**Setting / Institutional Environment.** The buyer's valuation `v` is drawn from a uniform distribution on `[0, 1]`. The regulator's goal is to find an extensionproof information structure that maximizes buyer surplus.\n\n---\n\n### Data / Model Specification\n\nThe proposed buyer-optimal information structure for the uniform case is as follows: if `v > 1/2`, the valuation is perfectly revealed; if `v ≤ 1/2`, valuations are pooled such that the posterior mean is always `1/4`. To check for extensionproofness, we must verify that the seller cannot profit by extending this structure.\n\nThe seller's optimal extension strategy for a new price `q ∈ (1/4, 1/2)` involves sending a `BUY` signal with a specific probability `x(v)` for low valuations `v`:\n```latex\nx(v) := \\frac{1/2 - v - q}{q - v} \\quad \\text{(Eq. 1)}\n```\n\n---\n\nSuppose a buyer's initial signal `s` indicates their valuation is either `v_L = 1/4 - s` or `v_H = 1/4 + s`, each with 50% probability. The seller then extends the information structure to try to persuade the buyer to purchase at a new price `q`, and sends an additional `BUY` signal. What is the buyer's updated posterior expected valuation, `E[v|s, BUY]`?",
    "Options": {
      "A": "q",
      "B": "1/4",
      "C": "1/4 + s",
      "D": "(1/4 + s + q)/2"
    },
    "Answer": "A",
    "pi_justification": "This item tests the understanding of how a strategic information extension (persuasion) works. It uses a Scenario Application strategy, asking the student to apply the logic of Bayesian persuasion to a specific case. The correct answer is the core result of this mechanism: the posterior is moved exactly to the new price. Distractors represent common conceptual errors: (B) ignoring the new information (Step-Omission Error), (C) assuming the new information is perfectly revealing (Conceptual Error), and (D) applying an incorrect heuristic for belief updating (Formula Misuse).",
    "qid": "443",
    "model_evaluation_result": {
      "model_raw_response": "C) 3/4 + sq/(1/2-q)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 444,
    "Question": "### Background\n\n**Research Question.** This problem provides a complete, worked-out example of a buyer-optimal information structure for the uniform prior case, including a formal proof of its robustness against seller extensions.\n\n**Setting / Institutional Environment.** The buyer's valuation `v` is drawn from a uniform distribution on `[0, 1]`. The regulator's goal is to find an extensionproof information structure that maximizes buyer surplus. The proposed structure yields a seller profit of 1/4.\n\n---\n\n### Data / Model Specification\n\nThe proposed buyer-optimal information structure for the uniform case is as follows: if `v > 1/2`, the valuation is perfectly revealed; if `v ≤ 1/2`, valuations are pooled such that the posterior mean is always `1/4`. To check for extensionproofness, we must verify that the seller cannot profit by extending this structure.\n\nThe seller's profit from extending the information structure to a new price `q ∈ (1/4, 1/2)` is given by:\n```latex\n\\Pi_{ext}(q) = q \\cdot \\left(1-q + \\int_0^{1/2-q} x(v)dv\\right)\n```\nwhere `x(v)` is the probability of persuading a buyer with valuation `v` to buy.\n\n---\n\nWhich of the following statements correctly evaluates this extension strategy and the extensionproofness of the original structure?",
    "Options": {
      "A": "The seller's profit from the extension, `Π_ext(q)`, is always strictly less than 1/4, so the structure is extensionproof.",
      "B": "The seller's profit from the extension, `Π_ext(q)`, is always strictly greater than 1/4, so the structure is not extensionproof.",
      "C": "The seller's profit from the extension, `Π_ext(q)`, is maximized at `q=3/8` with a value of 9/32 (> 1/4), so the structure is not extensionproof.",
      "D": "The profitability of the extension cannot be determined without knowing the exact value of `q` the seller would choose."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the final step in the example's argument: verifying the extensionproofness constraint. It uses a Computational Judgment strategy. The correct answer requires understanding that the extension must be unprofitable for *all* possible new prices `q`. Distractors are designed to trap common errors: (B) is the Conceptual Opposite; (C) presents a specific, plausible-looking but incorrect calculation that leads to the wrong conclusion, testing for oversimplification (Formula Misuse); (D) reflects an unwillingness to draw a general conclusion, failing to complete the required logical step (Step-Omission Error).",
    "qid": "444",
    "model_evaluation_result": {
      "model_raw_response": "C) The buyer's profit from extension, `Pi_ext(q) is maximized`",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 445,
    "Question": "### Background\n\n**Setting.** We analyze a simplified model where employee effort is a binary choice (`a=0` or `a=1`) and the firm implements `a=1` everywhere, eliminating cumulative loss of control. The wage required to incentivize effort is `w_t = g s_t`, where `g` is the disutility of high effort and `s_t` is the span of control.\n\n---\n\n### Question\n\nAn analysis of a firm's optimal hierarchical design reveals that it pays every employee the same wage, `w_t = ge`, regardless of their tier. Which of the following modeling assumptions is necessary to generate this egalitarian wage outcome?",
    "Options": {
      "A": "The total number of tiers `T` is a choice variable for the firm.",
      "B": "The total number of tiers `T` is exogenously fixed.",
      "C": "Workers are highly productive, such that their output `\\theta` exceeds `ge`.",
      "D": "The firm experiences a significant cumulative loss of control."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's understanding of the critical role of modeling assumptions in determining hierarchical structure. It uses a Reverse-Reasoning strategy: given the outcome (egalitarian wages), the student must infer the necessary condition (endogenous T). Distractors are designed to be high-fidelity: (B) is the direct conceptual opposite which leads to decreasing wages, (C) is a true but irrelevant condition from the model, and (D) incorrectly imports a feature from the paper's general model.",
    "qid": "445",
    "model_evaluation_result": {
      "model_raw_response": "B) The(total number of tiers T is exogenically fixed.)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 446,
    "Question": "### Background\n\n**Setting.** We analyze a simplified model where employee effort is fixed at `a=1`. The firm's hierarchy has a fixed total number of tiers, `T`, and workers are sufficiently productive (`\\theta > ge`).\n\n---\n\n### Data / Model Specification\n\nThe optimal path for the span of control `s_t` is governed by the general solution:\n```latex\n\\log(s_t / e) = C e^{-t} \\quad \\text{(Eq. 1)}\n```\nwhere `C` is a constant. The terminal condition for this fixed-`T` problem implies that `s_T = \\theta/g`.\n\n---\n\n### Question\n\nGiven the model setup and the terminal condition, what can be concluded about the optimal path of the span of control, `s_t`, as one moves from the top (`t=0`) to the bottom (`t=T`) of the hierarchy?",
    "Options": {
      "A": "The span of control is strictly decreasing.",
      "B": "The span of control is strictly increasing.",
      "C": "The span of control is constant.",
      "D": "The path of the span of control is ambiguous."
    },
    "Answer": "A",
    "pi_justification": "This item tests the student's ability to execute a key part of the model's derivation. It uses a Computational Judgment strategy, requiring the student to use the condition `\\theta > ge` to solve for the sign of the integration constant `C` (which will be positive) and then determine its implication for the dynamics of `s_t`. Distractors represent common errors: (B) a sign error in the derivation, (C) confusing the fixed-T case with the endogenous-T case, and (D) confusing this simple model with the ambiguous result from the paper's general model.",
    "qid": "446",
    "model_evaluation_result": {
      "model_raw_response": "C) The path is constant. \n\nNote: The terminal condition `s_T=\\theta/g` and the equation `log(s_t/e)=Ce^(-t)` imply that `s_t=s_0e^(-t)`, where `s_0` is a positive constant. Therefore, `s_1=s_0*e^(-1)>s_0`, `s_2=s_0*(e^(-2))<s_0*e^(1)`, and so on. This means that `s_ t` is strictly decreasing.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 447,
    "Question": "### Background\n\n**Setting.** In the general model of an optimal hierarchy, the firm chooses effort `a_t`, span of control `s_t`, and wages `w_t` at each tier `t`.\n\n---\n\n### Data / Model Specification\n\nThe solution to the firm's problem yields two key results:\n1.  The rate of change of wages is given by: `\\dot{w}_t / w_t = 1 - \\log(s_t)`\n2.  The optimal span of control satisfies the inequality: `s_t > e` (where `e` is Euler's number, approx. 2.718)\n\n---\n\n### Question\n\nGiven these two results from the model, what can be definitively concluded about the optimal wage profile?",
    "Options": {
      "A": "The rate of change of wages, `\\dot{w}_t`, must be negative.",
      "B": "The rate of change of wages, `\\dot{w}_t`, must be positive.",
      "C": "The rate of change of wages, `\\dot{w}_t`, must be zero.",
      "D": "The sign of `\\dot{w}_t` cannot be determined without knowing the exact value of `s_t`."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Computational Judgment strategy to test the student's ability to perform a crucial step in the paper's main proof. It requires connecting the condition `s_t > e` to the wage dynamics equation to deduce the sign of `\\dot{w}_t`. Distractors represent a sign error (B), a confusion with other models (C), and a failure to see the definitive implication of the provided information (D).",
    "qid": "447",
    "model_evaluation_result": {
      "model_raw_response": "B) The dot(w)_t must be positive.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 448,
    "Question": "### Background\n\n**Research Question.** In a dynamic general equilibrium setting with a constant mark-up pricing rule, what determines the equilibrium rate of output price inflation?\n\n**Setting.** The model considers a firm with a Cobb-Douglas production technology that faces constantly changing input prices and ongoing technological progress. The core assumption is that the firm maintains a constant markup, which implies that the output price \\(P_t\\) must grow at the same rate as the minimized unit cost of production.\n\n**Variables & Parameters.**\n- \\( p \\): Expected rate of change of the firm's output price (\\(\\dot{P}/P\\)).\n- \\( w \\): Expected rate of growth of the money wage rate (\\(\\dot{W}/W\\)).\n- \\( g \\): Rate of Harrod-neutral (labor-augmenting) embodied technical progress.\n- \\( q^* \\): Expected rate of change of the price of capital goods relative to the price of output.\n- \\( a \\): The output elasticity of labor in the Cobb-Douglas production function.\n\n---\n\n### Data / Model Specification\n\nThe equilibrium rate of output price inflation is determined by the growth rates of factor prices and technology:\n\n```latex\np = w - g + q^{*} \\frac{1-a}{a} \n\\quad \\text{(Eq. (1))}\n```\n\n---\n\nAccording to the model's equilibrium price dynamics (Eq. (1)), what is the combined effect of an increase in the rate of Harrod-neutral technological progress (\\(g\\)) and an increase in the expected rate of wage growth (\\(w\\)) on the equilibrium rate of output price inflation (\\(p\\))?",
    "Options": {
      "A": "Both changes increase \\(p\\).",
      "B": "Both changes decrease \\(p\\).",
      "C": "An increase in \\(g\\) decreases \\(p\\), while an increase in \\(w\\) increases \\(p\\).",
      "D": "An increase in \\(g\\) increases \\(p\\), while an increase in \\(w\\) decreases \\(p\\)."
    },
    "Answer": "C",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5)\nThis item assesses the student's understanding of the core price dynamics equation. It uses a Scenario Application strategy by asking for the effect of simultaneous changes. Distractors are based on common sign errors, including a direct conceptual opposite (D), ensuring high discriminability.",
    "qid": "448",
    "model_evaluation_result": {
      "model_raw_response": "D) Anincrease in \\(g \\) increases \\(p \\), while anincrease in \\(w \\) decreases \\(p \\).",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 449,
    "Question": "### Background\n\n**Research Question.** In a dynamic general equilibrium setting with a constant mark-up pricing rule and corporate taxes, what determines a firm's optimal capital intensity?\n\n**Setting.** The model considers a firm with a Cobb-Douglas production technology that faces constantly changing input prices and ongoing technological progress. The firm's choice of capital intensity depends on the relative price of capital and the user cost of capital, which is influenced by taxes and inflation expectations.\n\n**Variables & Parameters.**\n- \\( \\hat{k}_t \\): Optimal capital-output ratio at time \\(t\\).\n- \\( u, k, z' \\): Corporate tax rate, investment tax credit rate, and present value of depreciation allowances.\n- \\( r, p, d \\): Nominal interest rate, expected output price inflation, and physical depreciation rate.\n- \\( P/Q \\): Relative price of output to capital goods.\n- \\( M, D, c, a \\): Model parameters (markup, constants, labor elasticity).\n\n---\n\n### Data / Model Specification\n\nThe final, empirically operational approximation for the optimal capital-output ratio is:\n\n```latex\n\\hat{k}_{t}=\\frac{(1-a)}{M}\\left(\\frac{1-u}{1-k-u z^{\\prime}}\\right)\\left(\\frac{P}{Q}\\right)\\left(\\frac{D}{r-p+d+c}\\right) \n\\quad \\text{(Eq. (1))}\n```\n\n---\n\nBased on the model for the optimal capital-output ratio (\\(\\hat{k}_t\\)) in Eq. (1), which of the following changes would unambiguously lead to a firm choosing a *more* capital-intensive technology (i.e., a higher \\(\\hat{k}_t\\))?",
    "Options": {
      "A": "An increase in the nominal interest rate (\\(r\\)).",
      "B": "A decrease in the investment tax credit (\\(k\\)).",
      "C": "An increase in the expected rate of output price inflation (\\(p\\)).",
      "D": "A decrease in the relative price of output to capital (\\(P/Q\\))."
    },
    "Answer": "C",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5)\nThis item tests the ability to interpret the final equation for the optimal capital-output ratio, a central result of the paper. It uses a Reverse-Reasoning strategy, asking for a cause that produces a specific outcome. The distractors are designed based on the 'Sign Error' principle, where each incorrect option describes a change that would have the opposite effect on capital intensity, making them highly plausible to students who misread the formula.",
    "qid": "449",
    "model_evaluation_result": {
      "model_raw_response": "D) A) B) C) are incorrect because they would all lead to a less capital-intensive technology.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 450,
    "Question": "### Background\n\nA central challenge in economic modeling is to ensure that the model's core assumptions are grounded in empirical reality. This problem examines how stylized facts about the Brazilian labor market are used to justify key structural assumptions in a two-sector model of informality.\n\n### Data / Model Specification\n\nThe following table presents key data on Brazil's labor market dynamics.\n\n**Table 1: Transition Matrix (in %)**\n\n| Origin (Row) | Destination (Column) | | | |\n|---|---|---|---|---|\n| | **Formal** | **Informal** | **Unemployed** | **Self-employed** |\n| **Formal salaried** | 84.9 | 7.2 | 4.7 | 3.2 |\n| **Informal salaried** | 24.3 | 53.6 | 8.4 | 13.7 |\n| **Unemployed** | 23.5 | 25.3 | 38.7 | 12.6 |\n\nOne key assumption of the theoretical model is that the exogenous job separation rate is higher in the informal sector than the formal sector (`$s_I > s_F$`).\n\n### Question\n\nBased on the transition matrix in Table 1, what are the approximate annual exit probabilities for formal and informal salaried workers, and what key model assumption does this evidence support?",
    "Options": {
      "A": "Formal exit probability is 15.1%; Informal is 46.4%. This supports the assumption that the informal sector separation rate is higher (`$s_I > s_F$`).",
      "B": "Formal exit probability is 84.9%; Informal is 53.6%. This supports the assumption that the formal sector separation rate is higher (`$s_F > s_I$`).",
      "C": "Formal exit probability is 15.1%; Informal is 46.4%. This supports the assumption that the formal sector separation rate is higher (`$s_F > s_I$`).",
      "D": "The probability of an unemployed worker finding a formal job is 23.5%. This supports the assumption of undirected search."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a direct calculation from tabular data and link the result to a core theoretical assumption of the model. It uses a Computational Judgment strategy. Distractors are designed based on common errors: B uses the raw 'stay' probabilities instead of calculating 'exit' probabilities; C performs the calculation correctly but draws the opposite conceptual conclusion; D uses correct data from the table but links it to an irrelevant assumption, testing precision.",
    "qid": "450",
    "model_evaluation_result": {
      "model_raw_response": "A)󠁧󠁢󠁥󠁮󠁧",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 451,
    "Question": "### Background\n\nA central challenge in economic modeling is to ensure that the model's core assumptions are grounded in empirical reality. This problem examines how stylized facts about the Brazilian labor market are used to justify key structural assumptions in a two-sector model of informality.\n\n### Data / Model Specification\n\nThe model assumes \"undirected search,\" where unemployed workers form a single pool (`$u$`) and can be matched with firms from either sector. The matching function for each sector `$j$` is thus `$m_j(u, v_j)$`. An alternative is \"directed search,\" where workers must choose to search in either a formal (`$u_F$`) or informal (`$u_I$`) sub-market, with `$u = u_F + u_I$`.\n\nData from the Brazilian labor market shows that flows from unemployment to formal jobs (23.5%) and to informal jobs (a combined 37.9%) are both substantial.\n\n### Question\n\nHow would the primary transmission mechanism of a policy that reduces formal entry costs (`$K_F$`) differ in a directed search model compared to the baseline undirected search model?",
    "Options": {
      "A": "In a directed search model, the primary effect would be the reallocation of unemployed workers' search effort from the informal to the more attractive formal market, shrinking the informal search pool (`$u_I$`).",
      "B": "In a directed search model, the effect would be an increase in formal vacancies (`$v_F$`), which raises the job-finding rate for the entire aggregate pool of unemployed workers (`$u$`).",
      "C": "In a directed search model, the primary effect would be the reallocation of firms' vacancy posting effort from the informal to the formal market.",
      "D": "In a directed search model, the policy would have no effect on the informal market, as the two search pools (`$u_F$` and `$u_I$`) are completely separate."
    },
    "Answer": "A",
    "pi_justification": "This item tests a deep understanding of search theory by asking the user to contrast the model's assumption with a major alternative. It uses a Scenario Application strategy. The distractors are designed to pinpoint specific conceptual misunderstandings: B incorrectly applies the undirected search mechanism to the directed search scenario; C confuses the agent making the reallocation decision (workers, not firms); D misunderstands how markets are linked in a directed search equilibrium.",
    "qid": "451",
    "model_evaluation_result": {
      "model_raw_response": "C) In an economic model with directed search, the primary transmission mechanisms of a policy reducing formal entry costs ($K_F$) would be the re-allocation of firms’ vacancy posting effort (vacancies) from the informal market to the formal one, as opposed to the undirected search case.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 452,
    "Question": "### Background\n\nA central thesis of the paper is that a coherent definition of causality in static simultaneous equations can be grounded in the structure of an underlying dynamic system.\n\n### Data / Model Specification\n\nConsider the following two-variable triangular dynamic system, where `a, b, c, m, n` are positive constants. The system's structure implies that `x` is causally prior to `y`.\n```latex\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -ax + m \\quad \\text{(Eq. 1)}\n```\n```latex\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = bx - cy + n \\quad \\text{(Eq. 2)}\n```\nIn equilibrium, the time derivatives are zero.\n\n### Question\n\nWhat is the equilibrium value `y*` for this system?",
    "Options": {
      "A": "`y* = (n + bm/a) / c`",
      "B": "`y* = (n + bx*) / c`",
      "C": "`y* = n/c`",
      "D": "`y* = (n - bm/a) / c`"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to perform a multi-step calculation to find an equilibrium value, a core skill for applying the paper's concepts. It uses the Computational Judgment strategy. Distractor B represents a Step-Omission Error (failing to substitute for x*). Distractor C represents a Formula Misuse error (ignoring the causal influence of x). Distractor D represents a common Sign Error.",
    "qid": "452",
    "model_evaluation_result": {
      "model_raw_response": "A) y* = (nm + bam)/c",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 453,
    "Question": "### Background\n\nA central thesis of the paper is that a coherent definition of causality in static simultaneous equations can be grounded in the structure of an underlying dynamic system. The stability of such systems is determined by the eigenvalues of their coefficient matrices.\n\n### Data / Model Specification\n\nConsider a two-variable triangular dynamic system, where `a, b, c, m, n` are positive constants. A weak feedback loop is introduced, modifying the first equation:\n```latex\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -ax + \\varepsilon y + m \\quad \\text{(Eq. 1)}\n```\n```latex\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = bx - cy + n \\quad \\text{(Eq. 2)}\n```\nwhere `ε` is a small real number.\n\n### Question\n\nA necessary condition for the stability of this 2x2 system is that all coefficients of its characteristic polynomial in `λ` are positive. Given `a, b, c > 0`, what condition must `ε` satisfy for the system to remain stable?",
    "Options": {
      "A": "`ac > εb`",
      "B": "`ac < εb`",
      "C": "`a+c > ε`",
      "D": "`ac > -εb`"
    },
    "Answer": "A",
    "pi_justification": "This item assesses a more advanced skill: analyzing the stability of a perturbed dynamic system. It uses the Computational Judgment strategy. The correct answer requires deriving the characteristic equation and interpreting the condition on its constant term. Distractor B is the Conceptual Opposite (the condition for instability). Distractor C is a Formula Misuse error (confusing the trace and determinant conditions). Distractor D represents a Sign Error in the determinant calculation.",
    "qid": "453",
    "model_evaluation_result": {
      "model_raw_response": "B) ac < εb\n\nNote: I'm not providing any explanations or justifications, just the correct answer as per the format specified.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 454,
    "Question": "### Background\n\nThe paper analyzes the dynamic properties of complex systems, including feedback loops within them. The nature of a feedback loop (positive/destabilizing or negative/stabilizing) can be determined by the signs of the causal links that form it.\n\n### Data / Model Specification\n\nThe dynamics of a lake's ecosystem are described by the following system of equations, where `N`=nitrate, `P`=phosphate, `G`=green algae, `B`=blue-green algae, and all `c_i` are positive constants, except for `c₇ < 0`, which represents inhibition.\n\n```latex\n\\frac{\\mathrm{d}N}{\\mathrm{d}t} = -c_{1}G - c_{2}N \\quad \\text{(Eq. 1)}\n```\n```latex\n\\frac{\\mathrm{d}P}{\\mathrm{d}t} = -c_{3}G - c_{4}B - c_{5}P \\quad \\text{(Eq. 2)}\n```\n```latex\n\\frac{\\mathrm{d}B}{\\mathrm{d}t} = c_{6}P \\quad \\text{(Eq. 3)}\n```\n```latex\n\\frac{\\mathrm{d}G}{\\mathrm{d}t} = c_{7}B + c_{8}N + c_{9}P \\quad \\text{(Eq. 4)}\n```\n\n### Question\n\nAnalyze the feedback loop represented by the causal chain `P → B → G → P`. Based on the signs of the causal links in the system's Jacobian matrix, what is the nature of this feedback loop?",
    "Options": {
      "A": "Positive (destabilizing)",
      "B": "Negative (stabilizing)",
      "C": "There is no feedback loop involving these three variables.",
      "D": "The feedback is neutral (zero effect)."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to identify and analyze a feedback loop within a dynamic system, a key application of causal analysis. It uses the Computational Judgment strategy. The student must trace the signs of the partial derivatives: `∂(dB/dt)/∂P` is `+`, `∂(dG/dt)/∂B` is `-`, and `∂(dP/dt)/∂G` is `-`. The product `(+) * (-) * (-)` is `+`. Distractor B is the Conceptual Opposite. Distractor C represents a failure to trace the causal path. Distractor D is an incorrect calculation.",
    "qid": "454",
    "model_evaluation_result": {
      "model_raw_response": "B) Negative(stabilizing) \n\n(Note: The output is a single letter \"B\")",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 455,
    "Question": "### Background\n\nThe paper argues that nearly decomposable systems, where within-subsystem linkages are strong and between-subsystem linkages are weak, can be simplified for long-run analysis through aggregation. This simplification is only valid under specific conditions related to the system's characteristic roots (eigenvalues).\n\n### Data / Model Specification\n\nFor aggregation to be valid, two strong conditions must be met:\n1.  **Existence of Primary Roots:** Each subsystem has a 'primary' root whose real part is significantly smaller in absolute magnitude than its other 'non-primary' roots.\n2.  **Separation of Time Scales:** The absolute value of the real part of every primary root must be smaller than that of any non-primary root from any subsystem.\n\nConsider a hypothetical dynamic system with two subsystems with the following characteristic roots:\n- **Roots of Subsystem 1:** `{-2.0, -5.0, -6.0}`\n- **Roots of Subsystem 2:** `{-0.5, -1.0, -7.0}`\n\n### Question\n\nDoes this system satisfy the conditions for aggregation, and why?",
    "Options": {
      "A": "No, because the magnitude of a primary root of Subsystem 1 (|-2.0|) is greater than the magnitude of a non-primary root of Subsystem 2 (|-1.0|).",
      "B": "Yes, because each subsystem has a distinct primary root with a magnitude smaller than its other roots.",
      "C": "Yes, because all characteristic roots are negative, ensuring the overall system is stable.",
      "D": "No, because the primary root of Subsystem 2 (|-0.5|) is smaller in magnitude than the primary root of Subsystem 1 (|-2.0|)."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to apply abstract theoretical conditions to a concrete numerical case. It uses the Scenario Application strategy. The correct answer requires correctly identifying primary/non-primary roots and performing the cross-subsystem comparison specified in Condition 2. Distractor B represents a common error of only checking Condition 1. Distractor C is True but Irrelevant to the question of aggregation. Distractor D represents a misapplication of Condition 2, comparing a primary root to another primary root.",
    "qid": "455",
    "model_evaluation_result": {
      "model_raw_response": "B) Yes because each subsystemhas a distinct primaryroot with a magnitude smalle than its otherroots.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 456,
    "Question": "### Background\n\nThe paper proposes approximating complex dynamic systems with 'mixed structures' containing dynamic, equilibrium, and exogenous equations. For such a structure to be valid, it must be 'self-contained'. A key condition relates to whether a dynamic equation is 'self-regulating' (the variable whose derivative is on the left-hand side also appears on the right-hand side).\n\n### Data / Model Specification\n\nConsider the following 2-variable dynamic system:\n```latex\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = a y \\quad \\text{(Eq. 1 - non-self-regulating)}\n```\n```latex\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = -b y \\quad \\text{(Eq. 2 - self-regulating)}\n```\n\n### Question\n\nIf the non-self-regulating equation for `x` (Eq. 1) is converted into its static equilibrium form (`0 = ay`) while the equation for `y` remains dynamic, what is the fundamental consequence for the resulting mixed model?",
    "Options": {
      "A": "The system becomes over-determined, as the static equation imposes a constraint on `y` that conflicts with its own dynamic law.",
      "B": "The system becomes under-determined, as there is no longer an equation governing the dynamics of `x`.",
      "C": "The system remains a valid self-contained mixed structure, but its only solution is the trivial one where `y` is always zero.",
      "D": "The causal ordering is reversed, with `y` now causing `x` in a static relationship."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the understanding of a subtle but crucial technical condition for creating valid mixed models. It uses the Reverse-Reasoning strategy, asking for the consequence of a specific action. The correct answer identifies the fundamental structural flaw. Distractor B is a plausible but incorrect inference. Distractor C identifies a symptom (`y=0`) but misses the underlying flaw ('over-determined'). Distractor D misinterprets the nature of the problem.",
    "qid": "456",
    "model_evaluation_result": {
      "model_raw_response": "A) The.system.becomes.over-determined.as.the.static.equation.imposes.a.constraint.on.y-that.conflicts.with.its.own.dynamic.law.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 457,
    "Question": "### Background\n\n**Research Question.** This problem assesses the implications of a violated identifying assumption in a structural model.\n\n**Setting / Institutional Environment.** A structural model of farmer behavior assumes **Rational Expectations**, meaning the farmer's subjective belief about the variance of production shocks (`σ_p^2`) is equal to the objective sample variance (`σ̂^2`) estimated from the data. This assumption is used to separately identify the risk aversion parameter, `R`.\n\n### The Question\n\nSuppose the true agricultural environment has recently become much riskier (true variance `σ_true^2` is high), but farmers' beliefs are 'sticky' and lag behind, so they still perceive the environment as relatively safe (`σ_p^2` is low). An econometrician, observing data from this new, risky environment, correctly estimates the high sample variance (`σ̂^2 ≈ σ_true^2`) and imposes the rational expectations assumption (`σ_p^2 = σ̂^2`) to estimate risk aversion.\n\nGiven this misspecification, how will the estimated risk aversion parameter, `R̂`, be biased relative to the farmers' true risk aversion, `R_true`?",
    "Options": {
      "A": "`R̂` will be biased downwards (underestimated).",
      "B": "`R̂` will be biased upwards (overestimated).",
      "C": "`R̂` will be unbiased because the econometrician used a consistent estimate of the true variance.",
      "D": "The direction of the bias cannot be determined without knowing the specific value of the farmers' beliefs."
    },
    "Answer": "A",
    "pi_justification": "This item tests deep understanding of identification and model misspecification bias using a Reverse-Reasoning strategy. The student must reason from a violation of the rational expectations assumption back to its effect on a parameter estimate. The correct answer (A) requires this logical chain: the farmer's behavior is less cautious than the model would predict for a truly risk-averse person facing the high risk the econometrician assumes. To rationalize this 'brave' behavior, the model must conclude the farmer is less risk-averse than they truly are. The primary distractor (B) represents the opposite, and most common, reasoning error. Distractor (C) tests for a superficial understanding of statistical consistency, while (D) is for students who fail to see that the direction of bias is determinate.",
    "qid": "457",
    "model_evaluation_result": {
      "model_raw_response": "B) `Ρ̂` will biased upwards (estimated).",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 458,
    "Question": "### Background\n\n**Research Question.** This problem examines the microfoundations and econometric specification of the structural model used to estimate water demand under non-linear prices.\n\n**Setting / Institutional Environment.** A household's choice of water consumption `w` is modeled by maximizing a utility function subject to a piecewise-linear budget constraint. The model must account for observed characteristics `Z`, unobserved taste heterogeneity `η`, and random optimization errors `ε`.\n\n---\n\n### Data / Model Specification\n\nThe econometric model for observed consumption `w_it` incorporates two distinct error terms:\n\n```latex\nw_{it} = w^*(P(\\cdot)) + \\varepsilon_{it} = \\begin{cases} Z_{it}\\delta + \\alpha P_{it} + \\gamma Y_{it} + \\eta_{it} + \\varepsilon_{it} & \\text{if the optimum is on a segment} \\\\ \\bar{w}_{k} + \\varepsilon_{it} & \\text{if the optimum is at kink } k \\end{cases} \\quad \\text{(Eq. 1)}\n```\n\nFor preferences to be convex and demand to be unique, the condition `γw + α < 0` must hold. This is enforced by truncating the distribution of the taste shock `η`.\n\n---\n\n### Question\n\nThe model's ability to separately identify the variance of the preference shock (`η`) and the optimization error (`ε`) is a key feature. Which of the following correctly describes the identification strategy?",
    "Options": {
      "A": "Variation in consumption observed *around* known, fixed kink points directly identifies the distribution of the optimization error `ε`, while the remaining variation in consumption *on segments* identifies the preference shock `η`.",
      "B": "Variation in consumption observed *on segments* directly identifies the distribution of the optimization error `ε`, while the clustering of consumption *at kink points* identifies the preference shock `η`.",
      "C": "The model relies on the large number of observations and the significant price variation across 20 different tariff schedules to ensure the two error terms are uncorrelated.",
      "D": "The two error terms cannot be separately identified; the model only identifies the variance of their sum, `η + ε`, which is a limitation of the structural approach."
    },
    "Answer": "A",
    "pi_justification": "This question assesses understanding of the model's identification strategy, replacing Q2 and Q3 of the source QA. It uses a Reverse-Reasoning strategy. Distractor B (Almost Right) reverses the core logic of the identification. Distractor C (True but Irrelevant) states a true fact about the data that is necessary for overall estimation but does not explain the specific mechanism for separating the error terms. Distractor D (Conceptual Opposite) describes the problem in standard linear models that this structural model is specifically designed to overcome.",
    "qid": "458",
    "model_evaluation_result": {
      "model_raw_response": "D) The model only identifies variance of their summation, η + ε, which is a limit",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 459,
    "Question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of estimating a linear water demand function when households face non-linear, multi-block price schedules.\n\n**Setting / Institutional Environment.** The analysis begins by considering a standard linear demand model. However, because the price a household faces is determined by its chosen consumption level under a block tariff, the price variable is endogenous.\n\n---\n\n### Data / Model Specification\n\nThe baseline linear demand model is specified as:\n\n```latex\nw_{it} = \\alpha P_{it} + \\beta X_{it} + u_{it} \\quad \\text{(Eq. 1)}\n```\n\nTo address the endogeneity of `P_it`, an instrumental variable (IV) strategy is considered, using features of the tariff schedule (e.g., marginal prices at kink points) as instruments.\n\n---\n\n### Question\n\nWhen estimating the linear demand model in Eq. (1) under a multi-block tariff, both OLS and standard Instrumental Variable (IV) methods are likely to fail. What is the most fundamental theoretical reason for the failure of the IV strategy in this context?",
    "Options": {
      "A": "A rational consumer's choice is based on the entire price schedule, so unobserved preference shocks (`u_it`) are inherently correlated with all features of that schedule, violating the IV exclusion restriction.",
      "B": "Under an increasing block tariff, unobserved demand shocks are positively correlated with the marginal price, which leads to biased OLS estimates but is corrected by the IV strategy.",
      "C": "The marginal prices at the kink points are not strongly correlated with the actual marginal price paid by the consumer, making them weak instruments.",
      "D": "The price variable `P_it` is measured with error because households do not know the exact marginal price they face, a problem that IV cannot solve."
    },
    "Answer": "A",
    "pi_justification": "This question assesses the core theoretical critique of using IV for non-linear pricing models, replacing Q1 and Q3 of the source QA. It uses a Reverse-Reasoning strategy. The correct answer (A) precisely identifies the violation of the exclusion restriction. Distractor B (Almost Right) correctly identifies the OLS problem but incorrectly claims IV solves it, missing the deeper issue. Distractor C (Conceptual) describes the weak instrument problem, which is a potential empirical issue but not the fundamental theoretical failure. Distractor D offers another plausible but incorrect econometric issue.",
    "qid": "459",
    "model_evaluation_result": {
      "model_raw_response": "A) A) A rationalconsumer's choice isbased on the entire priceschedule, so unobservedsupplyshocks (`u_it`are inherentlycorrelated with allfeatures of that schedule,vioating the IVexclusion restriction.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 460,
    "Question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of estimating a linear water demand function when households face non-linear, multi-block price schedules.\n\n**Setting / Institutional Environment.** To address endogeneity in a linear demand model, several instrumental variable (IV) strategies are attempted.\n\n---\n\n### Data / Model Specification\n\n**Table 1** presents the estimated price coefficient (`α̂`) from three different IV specifications for the linear demand model `w_it = αP_it + βX_it + u_it`.\n\n**Table 1: Price Coefficient Estimates Using Alternative IV Specifications**\n\n| Specification | (1) | (2) | (3) | (4) |\n| :--- | :--- | :--- | :--- | :--- |\n| **Baseline** | -0.620 (0.024) | -0.608 (0.024) | -0.551 (0.023) | -0.554 (0.023) |\n| **Olmstead (2009)** | 0.006 (0.293) | -0.245 (0.427) | -0.359 (0.348) | -0.500 (0.268) |\n| **McFadden (1977)** | 0.641 (0.038) | -0.561 (0.036) | -0.584 (0.033) | -0.610 (0.037) |\n\n*Notes: Robust standard errors in parentheses. Columns (1)-(4) differ by the set of control variables included.*\n\n---\n\n### Question\n\nBased on the results in Table 1, what is the strongest piece of evidence that the Instrumental Variable (IV) estimates of the price coefficient are unreliable?",
    "Options": {
      "A": "The estimates are not robust; they change dramatically in sign, magnitude, and statistical significance depending on the specific IV specification used.",
      "B": "The significant estimates are consistently negative, which is consistent with economic theory and suggests the IV strategy is generally successful.",
      "C": "The standard errors in the 'Olmstead (2009)' specification are very large, which is the primary indicator of a failed estimation.",
      "D": "The 'Baseline' specification provides stable and statistically significant negative coefficients, confirming the validity of at least one IV approach."
    },
    "Answer": "A",
    "pi_justification": "This question assesses the ability to interpret regression output to critique a model, replacing Q2 of the source QA. It uses a Computational Judgment strategy. The correct answer (A) identifies the core problem of non-robustness. Distractor B (Misinterpretation) cherry-picks the results that fit theory. Distractor C (Step-Omission Error) focuses on one symptom (large SEs) rather than the overall pattern of instability. Distractor D (Misinterpretation) focuses only on the 'best' specification while ignoring the contradictory evidence from the others, which is the key takeaway.",
    "qid": "460",
    "model_evaluation_result": {
      "model_raw_response": "A) The answers is A) The estimates change dramatically in the sign, magnitude and statistical significance.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 461,
    "Question": "### Background\n\n**Research Question.** This problem concerns the necessary statistical pre-testing required to validate the use of an error-correction model (ECM) for analyzing long-run relationships between non-stationary time series, a cornerstone of the paper's empirical strategy.\n\n**Setting.** The study uses quarterly time-series data from 1965-1986. Before estimating the main behavioral models, the authors conduct Augmented Dickey-Fuller (ADF) tests to determine the order of integration of the variables and to test for cointegration among them. The validity of the entire empirical exercise rests on these preliminary findings.\n\n### Data / Model Specification\n\nThe paper's primary empirical specification is an error-correction model (ECM), which is only valid if the variables are integrated of order one, I(1), and are cointegrated. Table 1 below presents a selection of results from the Augmented Dickey-Fuller (ADF) tests used to verify these conditions. An asterisk (*) denotes rejection of the null hypothesis at the 5% level (critical value -3.77).\n\n**Table 1: Augmented Dickey-Fuller Unit Root & Cointegration Tests**\n| Test Specification (Dependent Variable is Real Price) | ln(I33/IPP) | ln(IAF/IPP) | ln(IMM/IPP) |\n| :--- | :--- | :--- | :--- |\n| 1. Intercept only (Unit Root Test) | -1.59 | -2.08 | -0.97 |\n| 2. Cointegration Test with ln(EXR/RPP) | -3.13 | -4.10* | -1.29 |\n| 3. Cointegration Test with ln(EXR/RPP), ln(SERV/Ix) | -4.84* | -4.07* | -3.59* |\n\n### Question\n\nBased on the 'Intercept only' tests reported in Row 1 of Table 1, what is the correct conclusion regarding the real commodity price series?",
    "Options": {
      "A": "The series are stationary (I(0)) because the test statistics are negative.",
      "B": "The null hypothesis of stationarity is rejected for all three series.",
      "C": "The null hypothesis of a unit root cannot be rejected, indicating the series are likely non-stationary (I(1)).",
      "D": "The series are cointegrated with a constant term."
    },
    "Answer": "C",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item replaces Part 1 of the original QA. It assesses the fundamental skill of interpreting statistical test results from a table. The answer is atomic and the potential for creating high-fidelity distractors is very high. Strategy: Direct Interpretation. Distractor Logic: (A) is a Conceptual Opposite, confusing the sign with significance. (B) is a Conceptual Error, misstating the null hypothesis of the ADF test. (D) is True but Irrelevant, confusing a unit root test with a cointegration test.",
    "qid": "461",
    "model_evaluation_result": {
      "model_raw_response": "C) The series is likely non-steady (I( 1 )), since the test statistics is negative.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 462,
    "Question": "### Background\n\n**Research Question.** This problem concerns the necessary statistical pre-testing required to validate the use of an error-correction model (ECM) for analyzing long-run relationships between non-stationary time series, a cornerstone of the paper's empirical strategy.\n\n**Setting.** The study uses quarterly time-series data from 1965-1986. Before estimating the main behavioral models, the authors conduct Augmented Dickey-Fuller (ADF) tests to determine the order of integration of the variables and to test for cointegration among them. The validity of the entire empirical exercise rests on these preliminary findings.\n\n### Data / Model Specification\n\nThe paper's primary empirical specification is an error-correction model (ECM), which is only valid if the variables are integrated of order one, I(1), and are cointegrated. Table 1 below presents a selection of results from the Augmented Dickey-Fuller (ADF) tests used to verify these conditions. An asterisk (*) denotes rejection of the null hypothesis at the 5% level (critical value -3.77).\n\n**Table 1: Augmented Dickey-Fuller Unit Root & Cointegration Tests**\n| Test Specification (Dependent Variable is Real Price) | ln(I33/IPP) | ln(IAF/IPP) | ln(IMM/IPP) |\n| :--- | :--- | :--- | :--- |\n| 1. Intercept only (Unit Root Test) | -1.59 | -2.08 | -0.97 |\n| 2. Cointegration Test with ln(EXR/RPP) | -3.13 | -4.10* | -1.29 |\n| 3. Cointegration Test with ln(EXR/RPP), ln(SERV/Ix) | -4.84* | -4.07* | -3.59* |\n\n### Question\n\nThe tests in Table 1 lead the authors to conclude that the real price series are non-stationary (I(1)) and that a cointegrating relationship exists among the variables. Why is this pair of findings a crucial prerequisite for their chosen error-correction modeling (ECM) strategy?",
    "Options": {
      "A": "It proves that the variables are causally related in the long run, which is a necessary assumption for any regression analysis.",
      "B": "It allows for the estimation of the model using OLS on the levels of the variables, as cointegration guarantees the estimates will be unbiased.",
      "C": "It ensures that the residuals from the levels regression will be homoskedastic, a key assumption for valid inference.",
      "D": "It confirms a stable long-run equilibrium between non-stationary variables, which allows an ECM to validly model short-run dynamics while avoiding spurious regression."
    },
    "Answer": "D",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item replaces Part 2 of the original QA. It tests the core methodological justification for using an ECM, a key concept in time-series econometrics. Strategy: Reverse-Reasoning (given the findings, what is the justification?). Distractor Logic: (A) is a Conceptual Error, confusing cointegration with causality. (B) is a Conceptual Opposite; regressing I(1) variables in levels is precisely what causes spurious regression. (C) is True but Irrelevant, confusing the role of cointegration (which ensures stationary errors) with the separate issue of homoskedasticity.",
    "qid": "462",
    "model_evaluation_result": {
      "model_raw_response": "D) It confirmed a stable long-term equilibrium between nonstationary variables, allowing an ECM to correctly model short-term dynamics while avoiding false regression.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 463,
    "Question": "### Background\n\n**Research Question.** This problem concerns the necessary statistical pre-testing required to validate the use of an error-correction model (ECM) for analyzing long-run relationships between non-stationary time series, a cornerstone of the paper's empirical strategy.\n\n**Setting.** The study uses quarterly time-series data from 1965-1986. Before estimating the main behavioral models, the authors conduct Augmented Dickey-Fuller (ADF) tests to determine the order of integration of the variables. The sample period includes the major OPEC oil shocks of the 1970s.\n\n### Question\n\nA known limitation of ADF tests is their low power to reject the unit root null hypothesis when a series is actually stationary around a structural break. If a more appropriate test (e.g., a Zivot-Andrews test) revealed that the paper's real price series were in fact stationary around a structural break from the oil shocks, what would be the most significant implication for the paper's analysis?",
    "Options": {
      "A": "The cointegration analysis and the use of an error-correction model would be methodologically invalid.",
      "B": "The model would need to be re-estimated with dummy variables for the break, but the ECM structure would remain appropriate.",
      "C": "The estimated long-run elasticities would still be valid, but the short-run dynamic coefficients would be biased.",
      "D": "The results would be less precise, with larger standard errors, but the overall conclusions would likely still hold."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item replaces Part 3 of the original QA. It assesses a deeper methodological critique and its logical consequences, a key skill in evaluating empirical research. Strategy: Scenario Application. Distractor Logic: (A) is the correct, fatal implication. (B) is a Conceptual Opposite; an ECM is fundamentally incorrect for stationary variables. (C) is a Step-Omission Error; if the series is stationary, the concept of a 'cointegrating' long-run elasticity is itself invalid. (D) is a weak claim that understates the severity of the methodological failure.",
    "qid": "463",
    "model_evaluation_result": {
      "model_raw_response": "B) The.model would need to re-estimated.with dummy variables for.the break, but.the ECM structure would.remain.appropriate.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 464,
    "Question": "### Background\n\n**Research Question.** What are the practical implications of the `GDPe`/`GDPo` distinction for applied empirical research, particularly for studies of productivity and the causal effect of trade on income?\n\n**Setting / Institutional Environment.** The analysis provides guidelines for empirical researchers using cross-country data. It highlights how using the wrong GDP measure can lead to biased estimates and flawed causal inference.\n\n**Variables & Parameters.**\n- `Real GDPe`: Expenditure-side real GDP, a measure of national wealth and purchasing power. Influenced by terms of trade.\n- `Real GDPo`: Output-side real GDP, a measure of national productive capacity. Excludes terms of trade effects.\n\n### Question\n\nBased on the paper's recommendations, which GDP measures should be used for studying technology gaps and national income, respectively?",
    "Options": {
      "A": "`Real GDPo` for technology gaps; `Real GDPe` for national income.",
      "B": "`Real GDPe` for technology gaps; `Real GDPo` for national income.",
      "C": "`Real GDPo` for both technology gaps and national income.",
      "D": "`Real GDPe` for both technology gaps and national income."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the user's grasp of the paper's core practical recommendation for applied researchers. It uses a Single Choice format because there is one correct pairing. The distractors are designed based on the 'Conceptual Opposite' (B) and 'Almost Right' (C, D) principles, targeting the most common and critical confusion between the two GDP measures.",
    "qid": "464",
    "model_evaluation_result": {
      "model_raw_response": "B) `_real GDPe` _for technology gaps; `_real GDPo` _for national income. \n\n(Note: I've corrected the output to match the format specified in the example.)",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 465,
    "Question": "### Background\n\n**Research Question.** What are the practical implications of the `GDPe`/`GDPo` distinction for applied empirical research, particularly for studies of productivity and the causal effect of trade on income?\n\n**Setting / Institutional Environment.** The analysis provides guidelines for empirical researchers using cross-country data. It highlights how using the wrong GDP measure can lead to biased estimates and flawed causal inference, particularly in the context of instrumental variable (IV) strategies like that of Frankel and Romer (1999), which studies the effect of trade on income.\n\n**Variables & Parameters.**\n- `Real GDPe`: Expenditure-side real GDP, a measure of national wealth and purchasing power. Influenced by terms of trade.\n- `Real GDPo`: Output-side real GDP, a measure of national productive capacity. Excludes terms of trade effects.\n- `Terms of Trade (ToT)`: The relative price of a country's exports to its imports.\n\n**Model Specification.**\nThe Frankel and Romer (1999) study uses an IV strategy to estimate the causal effect of trade on income. The regression is `Income = β * TradeOpenness + ...`, where `TradeOpenness` is instrumented by geographic characteristics. A key assumption for the IV to be valid is the **exclusion restriction**: geography must not affect income through any channel other than its effect on trade openness.\n\n### Question\n\nIn a Frankel and Romer-style IV regression, a researcher suspects their geography-based instrument violates the exclusion restriction. They believe geography affects the outcome not just through trade volume, but also by directly improving a country's Terms of Trade (e.g., via proximity to markets). To mitigate this specific bias, which of the following is the most effective strategy?",
    "Options": {
      "A": "Switch the outcome variable from `Real GDPe` to `Real GDPo`.",
      "B": "Switch the outcome variable from `Real GDPo` to `Real GDPe`.",
      "C": "Add the country's nominal exchange rate as a control variable.",
      "D": "Use `Nominal Openness` instead of `Real Openness` as the endogenous regressor."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to test a deep understanding of econometric identification as applied to the paper's concepts. The user must infer the correct solution given a specific problem (a violation of the exclusion restriction). The distractors are 'Conceptual Opposite' (B), 'True but Irrelevant' (C), and 'Formula Misuse' (D), each representing a plausible but incorrect line of reasoning about IV estimation.",
    "qid": "465",
    "model_evaluation_result": {
      "model_raw_response": "B) Switching the outcome variable to `Real GDPRo`",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 466,
    "Question": "### Background\n\n**Research Question.** What are the practical implications of the `GDPe`/`GDPo` distinction for applied empirical research, particularly for studies of productivity and the causal effect of trade on income?\n\n**Setting / Institutional Environment.** The analysis provides guidelines for empirical researchers using cross-country data. It highlights how using the wrong GDP measure can lead to biased estimates and flawed causal inference.\n\n**Variables & Parameters.**\n- `Real GDPe`: Expenditure-side real GDP, a measure of national wealth and purchasing power. Influenced by terms of trade.\n- `Real GDPo`: Output-side real GDP, a measure of national productive capacity. Excludes terms of trade effects.\n- `Terms of Trade (ToT)`: The relative price of a country's exports to its imports.\n\n### Question\n\nA researcher regresses `Real GDPe` on `Nominal Openness`, where `Nominal Openness = (Nominal Exports + Imports) / Nominal GDP`. The paper argues that a country's Terms of Trade (ToT) are positively correlated with both `Real GDPe` (by definition) and `Nominal Openness` (e.g., via high export prices). If the ToT are omitted from the regression, what is the resulting econometric problem and the likely direction of bias on the `Nominal Openness` coefficient?",
    "Options": {
      "A": "Omitted variable bias; the coefficient will be biased upwards.",
      "B": "Omitted variable bias; the coefficient will be biased downwards.",
      "C": "Simultaneity bias; the direction of bias is indeterminate.",
      "D": "Measurement error in `Nominal Openness`; the coefficient will be biased towards zero (attenuation bias)."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the ability to identify a classic omitted variable bias scenario and predict the direction of the bias, a critical skill in applied econometrics. The distractors are designed to be highly plausible. (B) represents a 'Sign Error' in applying the OVB formula. (C) and (D) name other common, relevant econometric problems ('Conceptual: Almost Right') that are distinct from the specific mechanism described in the question stem, forcing the user to precisely identify the problem at hand.",
    "qid": "466",
    "model_evaluation_result": {
      "model_raw_response": "B) Omitting the Terms of Trade variable would introduce omitted variable bias, causing the coefficient on Nominal Openness to be biased downwards.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 467,
    "Question": "### Background\n\n**Research Question.** This problem explores the theoretical micro-foundations of a model for measuring differences in real income across households. The model posits that economic well-being is derived from unobserved 'service flows' (e.g., entertainment) which are 'produced' by observable market goods (e.g., a TV). The efficiency of this production process is the key parameter that varies with household structure.\n\n**Setting / Institutional Environment.** Individuals demand service flows, which have an unobserved 'effective price'. This price depends on both the market price of the input good and the household's efficiency in transforming that good into a service. A change in household structure (e.g., two people forming a couple) is modeled as a shock to this transformation efficiency.\n\n**Variables & Parameters (Exhaustive and Standardized).**\n- `$S_1$`: The quantity of a service flow (e.g., entertainment services).\n- `$X_1$`: The quantity of a purchased market good (e.g., a TV set).\n- `$\\alpha_1$`: The rate of transformation, or average product, of good `$X_1$` into service flow `$S_1$`. Units: service units per unit of good.\n- `$P_1$`: The market price of the good `$X_1$`. Units: dollars per unit of good.\n- `$P_{s1}$`: The effective or implicit price of the service flow `$S_1$`. Units: dollars per service unit.\n- `$Y$`: An individual's nominal money income. Units: dollars.\n- `$J_1$`: The percentage change in `$\\alpha_1$` due to a change in household environment. Dimensionless.\n- `$\\eta_{S_1, P_{s1}}$`: The price elasticity of demand for the service flow `$S_1$`.\n\n---\n\n### Data / Model Specification\n\nThe model is defined by the following relationships:\n```latex\n\\alpha_{1} \\equiv S_{1}/X_{1} \\quad \\text{(Eq. (1))}\n```\n```latex\nP_{s1} = P_{1}/\\alpha_{1} \\quad \\text{(Eq. (2))}\n```\n```latex\nS_{1} = d(P_{s1}, Y) \\quad \\text{(Eq. (3))}\n```\nA change in household environment alters the transformation rate from `$\\alpha_1$` to `$\\alpha_1(1+J_1)$`.\n\n---\n\n### The Questions\n\nBased on the model, which of the following expressions correctly represents the elasticity of demand for the *market good* `$X_1$` with respect to the transformation parameter `$\\alpha_1$`, denoted `$\\eta_{X_1, \\alpha_1}$`? The final expression should be in terms of the price elasticity of demand for the service, `$\\eta_{S_1, P_{s1}}$`.",
    "Options": {
      "A": "```latex\n-\\eta_{S_1, P_{s1}} - 1\n```",
      "B": "```latex\n-\\eta_{S_1, P_{s1}}\n```",
      "C": "```latex\n\\eta_{S_1, P_{s1}} - 1\n```",
      "D": "```latex\n1 - \\eta_{S_1, P_{s1}}\n```"
    },
    "Answer": "A",
    "pi_justification": "This question assesses the user's ability to derive a key theoretical relationship within the paper's model. It uses a Computational Judgment strategy. The distractors are designed based on common mathematical errors in elasticity derivations: (B) is a step-omission error (missing the second term from the log-differentiated expression), while (C) and (D) represent plausible sign errors.",
    "qid": "467",
    "model_evaluation_result": {
      "model_raw_response": "D) 1 - ηS1,P1",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 468,
    "Question": "### Background\n\n**Research Question.** This problem explores the theoretical micro-foundations of a model for measuring differences in real income across households. The model posits that economic well-being is derived from unobserved 'service flows' (e.g., entertainment) which are 'produced' by observable market goods (e.g., a TV). The efficiency of this production process is the key parameter that varies with household structure.\n\n**Setting / Institutional Environment.** Individuals demand service flows, which have an unobserved 'effective price'. This price depends on both the market price of the input good and the household's efficiency in transforming that good into a service. A change in household structure (e.g., two people forming a couple) is modeled as a shock to this transformation efficiency.\n\n**Variables & Parameters (Exhaustive and Standardized).**\n- `$S_1$`: The quantity of a service flow (e.g., entertainment services).\n- `$X_1$`: The quantity of a purchased market good (e.g., a TV set).\n- `$\\alpha_1$`: The rate of transformation, or average product, of good `$X_1$` into service flow `$S_1$`. Units: service units per unit of good.\n- `$P_1$`: The market price of the good `$X_1$`. Units: dollars per unit of good.\n- `$P_{s1}$`: The effective or implicit price of the service flow `$S_1$`. Units: dollars per service unit.\n- `$Y$`: An individual's nominal money income. Units: dollars.\n- `$J_1$`: The percentage change in `$\\alpha_1$` due to a change in household environment. Dimensionless.\n- `$\\eta_{S_1, P_{s1}}$`: The price elasticity of demand for the service flow `$S_1$`.\n\n---\n\n### Data / Model Specification\n\nThe model is defined by the following relationships:\n```latex\n\\alpha_{1} \\equiv S_{1}/X_{1} \\quad \\text{(Eq. (1))}\n```\n```latex\nP_{s1} = P_{1}/\\alpha_{1} \\quad \\text{(Eq. (2))}\n```\n```latex\nS_{1} = d(P_{s1}, Y) \\quad \\text{(Eq. (3))}\n```\nA change in household environment alters the transformation rate from `$\\alpha_1$` to `$\\alpha_1(1+J_1)$`.\n\n---\n\n### The Questions\n\nSuppose the demand for the service flow `$S_1$` is known to be price-elastic (i.e., `$\\eta_{S_1, P_{s1}} < -1$`). According to the model, what is the effect of an increase in household production efficiency (a rise in `$\\alpha_1$`) on the quantity demanded of the observable market good `$X_1$`?",
    "Options": {
      "A": "It increases.",
      "B": "It decreases.",
      "C": "It remains unchanged.",
      "D": "The effect is ambiguous without more information."
    },
    "Answer": "A",
    "pi_justification": "This question tests the application of the model's theoretical results to a specific economic scenario, a strategy known as Scenario Application. It specifically targets the counter-intuitive 'rebound effect'. The distractors are designed to be highly plausible: (B) is the intuitive but incorrect answer for the elastic case, (C) represents the knife-edge case (`$\\eta = -1$`), and (D) is a common fallback for students who are unable to derive a definitive result.",
    "qid": "468",
    "model_evaluation_result": {
      "model_raw_response": "C) It remain unchanged. \n\n(Note: This answer assumes that the model is correctly specified and that the relationships between the variables are as described.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 469,
    "Question": "### Background\n\nThis problem examines the paper's empirical estimation of the technological frontier for power plants. The core of the analysis is a hedonic approach, where the price and operating characteristics of a plant are explained by its physical attributes, primarily its capacity. These estimated relationships form the foundation for the subsequent behavioral models.\n\n### Data / Model Specification\n\nThe paper estimates a fuel efficiency function that relates fuel efficiency to capacity in a log-log form:\n```latex\n\\log{(\\mathrm{EFF}_{st})} = \\alpha_{20} + \\alpha_{21}\\log{(\\mathrm{UCAP}_{st})} + \\text{Controls}_{st} + \\text{error}_{st}\n\\quad \\text{(Eq. 1)}\n```\nwhere `EFF` is fuel efficiency in megawatt hours per million BTU and `UCAP` is unit capacity in megawatts.\n\nSelected unconstrained estimates for this model are provided below.\n\n**Table 1: Selected Fuel Efficiency Estimates (from paper's Table II)**\n| Variable | Parameter | Estimate |\n| :--- | :--- | :--- |\n| Constant | `α_20` | -2.956 |\n| Log of unit capacity | `α_21` | 0.124 |\n| Main effect: Coal burnt | `δ_21` | -0.015 |\n\n### Question\n\nUsing the unconstrained estimates from Table 1, what is the predicted fuel efficiency (`EFF`) for a new 250 MW coal-burning plant? (Note: `log(250) ≈ 5.521`)",
    "Options": {
      "A": "0.051 MWh per million BTU",
      "B": "-2.286 MWh per million BTU",
      "C": "0.102 MWh per million BTU",
      "D": "0.103 MWh per million BTU"
    },
    "Answer": "C",
    "pi_justification": "This item assesses the ability to correctly extract coefficients from a regression table and apply a log-linear formula, a core skill in applied econometrics. It is a 'Computational Judgment' item. Distractor A results from omitting the capacity term entirely. Distractor B results from failing to exponentiate the final log value. Distractor D results from omitting the dummy variable for 'Coal burnt'.",
    "qid": "469",
    "model_evaluation_result": {
      "model_raw_response": "C) 2.103 Mwh per million BTUs",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 470,
    "Question": "### Background\n\nFor a subsequent theoretical model (Model 2) to be analytically tractable, the paper requires that the technology be homogenous. This implies a capital cost function of the form `C(ỹ) = b_1*ỹ^γ` and an efficiency function `a = b_2*ỹ^(1-γ)`, where `ỹ` is plant capacity. This specification requires that the elasticity of capital cost with respect to capacity (`γ`) and the elasticity of efficiency with respect to capacity (`1-γ`) sum to one.\n\n### Data / Model Specification\n\nThe paper's unconstrained empirical estimates found the elasticity of price with respect to capacity to be approximately 0.92. However, to satisfy the theoretical model's requirements, the author imposed a constrained elasticity of `γ = 0.9` for the capital cost function.\n\n### Question\n\nThe parameter `γ` in the capital cost function `C(ỹ) = b_1*ỹ^γ` governs economies of scale, with a smaller `γ` implying stronger economies of scale (i.e., cost increases less than proportionally with capacity). Given this, what is the most likely consequence of the author imposing a cost elasticity of `γ = 0.9` when the unconstrained data suggested `γ ≈ 0.92`?",
    "Options": {
      "A": "The model will under-predict the optimal plant size chosen by firms.",
      "B": "The model will over-predict the optimal plant size chosen by firms.",
      "C": "The model will predict no economies of scale in construction.",
      "D": "The model's predictions will be unaffected, as the difference is small."
    },
    "Answer": "B",
    "pi_justification": "This item assesses a deep understanding of how parameter choices in a structural model affect its behavioral predictions. It is a 'Scenario Application' item. The correct answer requires a three-step reasoning process connecting the parameter change to economies of scale, then to marginal costs, and finally to the optimal choice. Distractor A is the direct conceptual opposite, testing for confusion on the direction of the effect. Distractor C tests a fundamental misunderstanding of the parameter's meaning. Distractor D offers a plausible but incorrect dismissal of the problem.",
    "qid": "470",
    "model_evaluation_result": {
      "model_raw_response": "B) The.model will over-predict the optimal plant.size chosen by firms.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 471,
    "Question": "### Background\n\nA key justification for a central bank's role in bank supervision is that it may provide access to confidential information that improves monetary policy. This analysis examines whether confidential supervisory information contains predictive power for macroeconomic outcomes that is not already incorporated in official forecasts. It compares the Federal Reserve's internal \"Greenbook\" forecasts to those of private forecasters using a panel of quarterly forecasts from 1968-1994.\n\n### Data / Model Specification\n\nThe model regresses the actual, realized unemployment rate on the forecast, a measure of confidential supervisory information (`CAMEL5`), and an interaction term to see if the Greenbook uses this information differently.\n\n**The Model:**\n\n```latex\nU_{t+i} = \\alpha_{0} + \\alpha_{1}U_{j,t+i}^{e} + \\alpha_{2}CAMEL5_{t} + \\alpha_{3}(GB_{t} \\cdot CAMEL5_{t}) + \\epsilon_{j,t+i}\n```\n\n*   `U_{t+i}`: Actual realized unemployment rate.\n*   `U^e_{j,t+i}`: Forecast of unemployment made by forecaster `j`.\n*   `CAMEL5_t`: Percentage of assets in CAMEL 5-rated banks (confidential signal of distress).\n*   `GB_t`: A dummy variable equal to 1 for Greenbook forecasts, 0 for private forecasts.\n\n**The Data:**\nResults for the 4-quarter-ahead unemployment forecasts are in Table 1.\n\n**Table 1: Contribution of Supervisory Information to 4-Quarter-Ahead Unemployment Forecasts**\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| `CAMEL5` | 0.283* | (0.132) |\n| `Greenbook * CAMEL5` | -0.035 | (0.021) |\n\n*Notes: The dependent variable is the actual unemployment rate. * significant at 5%.*\n\n### Question\n\nBased on the model and results in Table 1, what is the correct interpretation of the coefficient on `CAMEL5` (0.283)?",
    "Options": {
      "A": "When banking distress (`CAMEL5`) is higher, private sector forecasts systematically underestimate the future unemployment rate.",
      "B": "When banking distress (`CAMEL5`) is higher, private sector forecasts systematically overestimate the future unemployment rate.",
      "C": "The Greenbook forecast is significantly more accurate than private forecasts when `CAMEL5` is high.",
      "D": "A 1 percentage point increase in `CAMEL5` causes a 0.283 percentage point increase in the unemployment rate."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the core finding of forecast inefficiency. Strategy: Direct interpretation of a regression coefficient in a difference-in-differences context. Distractors are designed based on common errors: Conceptual Opposite (sign error), True but Irrelevant (confusing the interpretation of the base coefficient `α₂` with the interaction term `α₃`), and Formula Misuse (making an unwarranted causal claim).",
    "qid": "471",
    "model_evaluation_result": {
      "model_raw_response": "C) The coefficient on `CAM5` indicates that a 1 percentage-point increase in `CAM5`, which represents a measure of banking distress, is associated with a 0,283 percentage-point increase (or 28.3 basis points) in the unemployment forecast.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 472,
    "Question": "### Background\n\nA key justification for a central bank's role in bank supervision is that it may provide access to confidential information that improves monetary policy. This analysis examines whether confidential supervisory information contains predictive power for macroeconomic outcomes that is not already incorporated in official forecasts. It compares the Federal Reserve's internal \"Greenbook\" forecasts to those of private forecasters using a panel of quarterly forecasts from 1968-1994.\n\n### Data / Model Specification\n\nThe model regresses the actual, realized unemployment rate on the forecast, a measure of confidential supervisory information (`CAMEL5`), and an interaction term to see if the Greenbook uses this information differently.\n\n**The Model:**\n\n```latex\nU_{t+i} = \\alpha_{0} + \\alpha_{1}U_{j,t+i}^{e} + \\alpha_{2}CAMEL5_{t} + \\alpha_{3}(GB_{t} \\cdot CAMEL5_{t}) + \\epsilon_{j,t+i}\n```\n\n*   `U_{t+i}`: Actual realized unemployment rate.\n*   `U^e_{j,t+i}`: Forecast of unemployment made by forecaster `j`.\n*   `CAMEL5_t`: Percentage of assets in CAMEL 5-rated banks (confidential signal of distress).\n*   `GB_t`: A dummy variable equal to 1 for Greenbook forecasts, 0 for private forecasts.\n\n**The Data:**\nResults for the 4-quarter-ahead unemployment forecasts are in Table 1.\n\n**Table 1: Contribution of Supervisory Information to 4-Quarter-Ahead Unemployment Forecasts**\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| `CAMEL5` | 0.283 | (0.132) |\n| `Greenbook * CAMEL5` | -0.035 | (0.021) |\n\n*Notes: The dependent variable is the actual unemployment rate.*\n\n### Question\n\nThe paper tests if the Greenbook staff fully incorporates the `CAMEL5` information, which implies a null hypothesis of `α₂ + α₃ = 0`. Based on the results in Table 1, what is the outcome of this test at a 5% significance level (critical value ≈ 1.96)? (Assume zero covariance between the estimators).",
    "Options": {
      "A": "Fail to reject the null; the test statistic is approximately 1.86, so we cannot conclude the Greenbook's use of the information is incomplete.",
      "B": "Reject the null; the coefficient on `Greenbook * CAMEL5` (-0.035) is statistically insignificant, proving the information is not used.",
      "C": "Reject the null; the test statistic is approximately 10.8, indicating the information is not fully used.",
      "D": "Fail to reject the null; this indicates that the Greenbook forecasts are statistically identical to private forecasts regarding `CAMEL5`."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to perform a multi-step hypothesis test and interpret the result. Strategy: Computational Judgment. Distractors are designed to catch specific errors: Step-Omission Error (testing the wrong hypothesis, `α₃=0`, instead of the correct one), Calculation Error (using an incorrect formula for the Wald statistic's standard error), and Conceptual Opposite (reaching the correct test outcome but for the wrong conceptual reason).",
    "qid": "472",
    "model_evaluation_result": {
      "model_raw_response": "C) Rejecting the null hypothesis requires a test statistic greater than the critical value. In this case, the test statistic (10.8) is greater than the 5% critical value (1.96).",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 473,
    "Question": "### Background\n\nA key justification for a central bank's role in bank supervision is that it may provide access to confidential information that improves monetary policy. This analysis examines whether confidential supervisory information contains predictive power for macroeconomic outcomes that is not already incorporated in official forecasts.\n\n### Data / Model Specification\n\nThe analysis starts with a baseline model:\n\n```latex\nActual Outcome = f(Forecast, CAMEL5, ...)\n```\n\nA second specification adds a vector of publicly available bank health variables (e.g., nonperforming loan ratio, capital-to-asset ratio) to this model. In the results for 4-quarter-ahead unemployment forecasts, the coefficient on `CAMEL5` remains positive and significant, while the coefficients on the public variables are insignificant.\n\n### Question\n\nWhat is the primary econometric purpose and conclusion of adding the publicly available bank health data to the model?",
    "Options": {
      "A": "To test if `CAMEL5`'s predictive power comes from its confidential component; the results suggest its power is from confidential information.",
      "B": "To improve the model's overall fit (R-squared); the results show that adding more variables makes the model better.",
      "C": "To check for multicollinearity between `CAMEL5` and public data; the results show they are highly correlated, making the `CAMEL5` coefficient unreliable.",
      "D": "To find the best predictive model of unemployment; the results indicate that `CAMEL5` is the only useful variable for forecasting."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the understanding of a key identification strategy: isolating a specific information channel by controlling for confounding variables. Strategy: Reverse-Reasoning (inferring purpose from method). Distractors are designed based on common misunderstandings of econometric practice: Almost Right (stating a true side-effect, like improved R-squared, as the primary purpose), Conceptual Opposite (confusing the goal of controlling for OVB with checking for multicollinearity), and Formula Misuse (misstating the research goal as pure prediction rather than causal channel identification).",
    "qid": "473",
    "model_evaluation_result": {
      "model_raw_response": "B) To improving the model'soverall fit (R-Squared); the resultsshowthataddingmorevariablesmakethemodellbetter.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 474,
    "Question": "### Background\n\nTo isolate the effect of the *confidential* component of a supervisory signal like `CAMEL5`, an analyst must separate it from the influence of publicly available information.\n\n### Question\n\nAn analyst proposes a two-step procedure: first, regress `CAMEL5` on all publicly available bank data and collect the residuals; second, use these residuals as the key explanatory variable in the main forecast error regression. What is the most critical statistical pitfall of this approach?",
    "Options": {
      "A": "The standard errors in the second stage will be incorrect because the model fails to account for the estimation uncertainty of the first-stage residual.",
      "B": "The residuals from the first stage will be, by construction, correlated with the dependent variable in the second stage, causing omitted variable bias.",
      "C": "This procedure induces severe multicollinearity in the second stage because the residual is a linear combination of the public variables.",
      "D": "The first-stage regression is invalid because `CAMEL5` is an endogenous variable determined by bank health."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses advanced econometric knowledge about two-step estimation procedures. Strategy: Reverse-Reasoning (identifying a flaw in a proposed method). The correct answer identifies the 'generated regressor problem.' Distractors are other plausible but incorrect econometric issues that a student might confuse it with: a false claim about omitted variable bias, a false claim about multicollinearity (the residual is orthogonal to the regressors), and a more general endogeneity claim that misses the specific pitfall of the two-step method itself.",
    "qid": "474",
    "model_evaluation_result": {
      "model_raw_response": "B) The residues from the first step will be, through construction, correlated using the dependent variable inside the second step, inducing omitted variable bias.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 475,
    "Question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. Defecting is a dominant strategy in the stage game, but cooperating (`$C_1, C_2$`) is the socially efficient outcome.\n\n### Data / Model Specification\n\nThe monitoring structure has the following properties:\n- Player 2's effort (`$C_2$` vs `$D_2$`) has a state-dependent effect. In state `$\\omega_1$`, effort is more productive than in state `$\\omega_2$`. \n- If Player 2 chooses to Defect (`$D_2$`), the public signal distribution is the same for both states, regardless of Player 1's action.\n\nConsider the case where **only Player 2 knows the state** (`$\\Theta_2 = \\{(\\omega_1), (\\omega_2)\\}$`), while Player 1 does not (`$\\Theta_1 = \\{(\\omega_1, \\omega_2)\\}$`).\n\n### Question\n\nIn this scenario, if the informed Player 2 chooses to Defect (`$D_2$`), can the uninformed Player 1 learn the true state of the world from the public signals?",
    "Options": {
      "A": "Yes, because Player 1's own actions still generate signals that depend on the state.",
      "B": "No, because when Player 2 defects, the resulting public signal distribution becomes independent of the state.",
      "C": "Yes, because Player 2's choice to defect is itself a credible signal of the state.",
      "D": "No, because Player 1 has no private information and therefore can never learn."
    },
    "Answer": "B",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the core information-jamming mechanic of the partnership game. It uses a Scenario Application strategy. The correct answer identifies the specific action that renders the signal uninformative. Distractors include a Conceptual Error (misattributing state-dependence to Player 1's actions), another Conceptual Error (misinterpreting defection as a revealing signal), and a True but Irrelevant reason.",
    "qid": "475",
    "model_evaluation_result": {
      "model_raw_response": "C) Yes because Player 0's choice to defeat is itself a creditable signal of the stat",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 476,
    "Question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. Defecting is a dominant strategy in the stage game, but cooperating (`$C_1, C_2$`) is the socially efficient outcome.\n\n### Data / Model Specification\n\nThe monitoring structure has the following properties:\n- Player 2's effort (`$C_2$` vs `$D_2$`) has a state-dependent effect. In state `$\\omega_1$`, effort is more productive than in state `$\\omega_2$`. \n- If Player 2 chooses to Defect (`$D_2$`), the public signal distribution is the same for both states, regardless of Player 1's action.\n\nConsider the case where **only Player 1 knows the state** (`$\\Theta_1 = \\{(\\omega_1), (\\omega_2)\\}$`), while Player 2 does not (`$\\Theta_2 = \\{(\\omega_1, \\omega_2)\\}$`).\n\n### Question\n\nIn this scenario, how can the uninformed Player 2 learn the true state, even if the informed Player 1 attempts to conceal it?",
    "Options": {
      "A": "By unilaterally choosing to Cooperate and observing whether her own effort is more or less productive from the resulting outcomes.",
      "B": "By observing Player 1's actions, which will perfectly reveal Player 1's private information.",
      "C": "Player 2, as the uninformed party, can never learn the state if Player 1 attempts to conceal it.",
      "D": "By threatening to Defect forever, which forces Player 1 to reveal the state."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses understanding of the asymmetric learning mechanic from the perspective of the uninformed player. It uses a Scenario Application strategy. The correct answer identifies the specific action the uninformed player can take to generate information. Distractors include a Conceptual Opposite (claiming learning is impossible), a common but incorrect assumption (that the informed player's actions are always revealing), and a True but Irrelevant point about equilibrium threats.",
    "qid": "476",
    "model_evaluation_result": {
      "model_raw_response": "B) By observation Player 1 actions, which perfectly reveal Player's private information.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 477,
    "Question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. The limit set of equilibrium payoffs is characterized by the intersection of half-spaces, where each half-space is defined by a direction vector `$\\lambda = ((\\lambda_1^{\\omega_1}, \\lambda_2^{\\omega_1}), (\\lambda_1^{\\omega_2}, \\lambda_2^{\\omega_2}))$`.\n\n### Data / Model Specification\n\nConsider the case where **only Player 2 knows the state** (`$\\Theta_2 = \\{(\\omega_1), (\\omega_2)\\}$`), while Player 1 does not (`$\\Theta_1 = \\{(\\omega_1, \\omega_2)\\}$`). The paper shows that the folk theorem can fail in this case because the maximal score for certain directions `$\\lambda$` is too low.\n\n### Question\n\nWhat strategic incentive does the direction `$\\lambda = ((0,-1),(0,1))$` place on the informed Player 2?",
    "Options": {
      "A": "It creates an incentive for Player 2 to reveal the state if it is `$\\omega_1$` (high productivity) and conceal it if it is `$\\omega_2$` (low productivity).",
      "B": "It creates an incentive for Player 2 to conceal the state if it is `$\\omega_1$` (high productivity) and reveal it if it is `$\\omega_2$` (low productivity).",
      "C": "It creates an equal incentive for Player 2 to cooperate in both states to maximize the joint payoff.",
      "D": "It creates an incentive for Player 2 to always defect, as his payoff is being minimized in at least one state."
    },
    "Answer": "B",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to translate a formal mathematical object (`$\\lambda$` vector) into its implied economic incentive structure. It uses a Computational Judgment strategy. The correct answer correctly interprets the negative weight as a punishment and the positive weight as a reward. Distractors are based on a Sign Error, a conceptual misinterpretation of `$\\lambda$` as social welfare weights, and a misunderstanding of how continuation values function.",
    "qid": "477",
    "model_evaluation_result": {
      "model_raw_response": "B) ItcreatesanincentiveforPlayer2tococealthe statifitistoω1(highproductivity)andrevealitifitisto ω2(lowproductivity).",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 478,
    "Question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. Defecting is a dominant strategy in the stage game, but cooperating (`$C_1, C_2$`) is the socially efficient outcome.\n\n### Data / Model Specification\n\nThe monitoring structure has the following properties:\n- Player 2's effort (`$C_2$` vs `$D_2$`) has a state-dependent effect. In state `$\\omega_1$`, effort is more productive than in state `$\\omega_2$`. \n- If Player 2 chooses to Defect (`$D_2$`), the public signal distribution is the same for both states, regardless of Player 1's action.\n\nThe paper demonstrates that a folk theorem can fail when **only Player 2 knows the state**.\n\n### Question\n\nWhat is the primary reason for the failure of the folk theorem in the case where only Player 2 knows their own productivity?",
    "Options": {
      "A": "The efficient outcome (`$C_1, C_2$`) is not individually rational for Player 1 in the low-productivity state.",
      "B": "Player 2 can unilaterally prevent Player 1 from learning the state (by defecting), making it impossible to enforce continuation payoffs that would punish him for being in the high-productivity state.",
      "C": "Player 1, being uninformed, cannot credibly commit to punishing Player 2 for failing to reveal the state.",
      "D": "The monitoring technology is state-independent when both players cooperate, so learning is never possible."
    },
    "Answer": "B",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This capstone item assesses the student's ability to synthesize the game's mechanics and incentives to explain the main result. It uses a Reverse-Reasoning strategy. The correct answer correctly links Player 2's ability to jam signals with his incentive to avoid punishments. Distractors include an Almost Right answer that misidentifies the core constraint (commitment vs. jamming), a Factually Incorrect statement about the monitoring technology, and a True but Irrelevant claim about stage-game payoffs.",
    "qid": "478",
    "model_evaluation_result": {
      "model_raw_response": "B) Player's 2 can unilateral prevent Player 2 from learning the State by defecting, making it impossible for Player 2 to enforce continuation payoff that would punish Player 1 for being in high-productivity State.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 479,
    "Question": "### Background\n\n**Research Question.** This problem evaluates the finite-sample performance of different Ordinary Least Squares (OLS) covariance matrix estimators. The analysis focuses on the trade-off between using the standard OLS estimator, which is efficient but biased under heteroskedasticity, and various heteroskedasticity-consistent (HC) estimators, which are robust but may be less precise in small samples.\n\n**Setting / Institutional Environment.** The analysis is based on Monte Carlo simulations. The performance of t-statistics derived from four different covariance estimators (OLS, HC1, HC3) is compared under two distinct scenarios: (1) homoskedastic errors, where the OLS assumptions hold, and (2) severe heteroskedasticity, where the OLS assumptions are violated.\n\n**Variables & Parameters.**\n- `Estimator`: The covariance matrix estimator used to construct the t-statistic for the coefficient `β_1`.\n- `S.D.`: The sample standard deviation of the 2000 simulated t-statistics. The theoretical value under the null hypothesis for a N(0,1) distributed statistic is 1.0.\n- `5% Rej. Freq.`: The empirical rejection frequency of a two-sided test with a nominal size of 5%. The theoretical value is 0.05.\n\n---\n\n### Data / Model Specification\n\nThe following table presents a curated summary of simulation results for t-statistics on the coefficient `β_1` with a sample size of `n=50`.\n\n**Table 1: Performance of t-statistics for `β_1` (n=50)**\n\n| Scenario | Estimator | S.D. | 5% Rej. Freq. |\n| :--- | :--- | :--- | :--- |\n| **Case 1: No Heteroskedasticity** | OLS | 1.037 | 0.057 |\n| | HC1 | 1.217 | 0.094 |\n| | HC3 | 1.074 | 0.067 |\n| **Case 5: Severe Heteroskedasticity** | OLS | 2.205 | 0.398 |\n| | HC1 | 1.483 | 0.172 |\n| | HC3 | 1.177 | 0.082 |\n\n*Note: The heteroskedasticity in Case 5 is generated from a random coefficients model where the error variance is strongly correlated with the regressor corresponding to `β_1`.*\n\n---\n\n### Question\n\nBased on the results in Table 1, how does the magnitude of the OLS estimator's failure (its size distortion) in the presence of severe heteroskedasticity compare to the cost of using the HC3 estimator (its size distortion) when it is not needed? Specifically, the size distortion of OLS in Case 5 is approximately how many times larger than the size distortion of HC3 in Case 1?",
    "Options": {
      "A": "20.5 times larger",
      "B": "5.9 times larger",
      "C": "7.9 times larger",
      "D": "0.05 times larger"
    },
    "Answer": "A",
    "pi_justification": "This item replaces the calculation-focused parts of the original QA problem. It uses a 'Computational Judgment' strategy, requiring the user to perform two separate calculations (size distortion for OLS in one case, for HC3 in another) and then compute a ratio. This single question efficiently captures the core quantitative trade-off between the 'cost' and 'benefit' of using robust standard errors. Distractors are designed based on plausible calculation errors: (B) compares raw rejection frequencies instead of size distortions, (C) incorrectly uses the HC1 estimator for the cost calculation, and (D) inverts the comparison, representing a conceptual reversal.",
    "qid": "479",
    "model_evaluation_result": {
      "model_raw_response": "C) 5 times larger\n\n(Note: The size distortion of the OSL estimator in Case 4 is 0,398, and the size distortion for the HC3 in case 1 is 0.,067. So the ratio is approximately 5.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 480,
    "Question": "### Background\n\nSimulation studies show that when OLS assumptions are violated by severe heteroskedasticity, the standard OLS t-test can have a true rejection frequency of nearly 40% for a nominal 5% test. In contrast, the HC3 robust t-test has a rejection frequency of around 8%. When there is no heteroskedasticity, the OLS t-test performs almost perfectly (5.7% rejection rate), while the HC3 t-test over-rejects slightly (6.7% rejection rate). The paper also notes that statistical tests for heteroskedasticity often have low power, meaning they frequently fail to detect the problem even when it is severe.\n\n---\n\n### Question\n\nGiven this evidence, what is the most prudent and methodologically defensible strategy for an applied researcher conducting inference?",
    "Options": {
      "A": "Adopt a 'pre-testing' approach: first test for heteroskedasticity, and if the test is not significant, use the more efficient OLS estimator.",
      "B": "Default to using the HC3 estimator for all inference, accepting its minor performance cost in the homoskedastic case as 'insurance' against catastrophic failure in the heteroskedastic case.",
      "C": "Default to using the OLS estimator, as its performance is optimal under the null and the risk of heteroskedasticity is often overstated.",
      "D": "Default to using the HC1 estimator, as it is robust to heteroskedasticity and is computationally simpler than the HC3 estimator."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the strategic conclusion of the original QA problem using a 'Scenario Application' strategy. It frames the choice of estimator as a risk-management problem. The correct option (B) articulates the paper's core 'minimax' argument. Distractors are designed to target specific fallacies: (A) represents the classic 'pre-testing fallacy' which the paper explicitly warns against, (C) ignores the catastrophic failure of OLS documented in the evidence, and (D) selects a sub-optimal robust estimator (HC1), which is a 'true but irrelevant' distractor that ignores the clear performance superiority of HC3 shown in the paper's results.",
    "qid": "480",
    "model_evaluation_result": {
      "model_raw_response": "B) Defaulting to using the hc3 estimator for inference.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 481,
    "Question": "### Background\n\n**Research Question.** This problem examines the relative power of different tests for heteroskedasticity, comparing a general-purpose 'portmanteau' test against tests designed for a specific alternative hypothesis.\n\n**Setting / Institutional Environment.** The analysis is based on a Monte Carlo simulation where the true data generating process (DGP) exhibits heteroskedasticity arising from a random coefficients model. Specifically, the error variance is given by `Var(u_t) ∝ 1 + X_1t^2 + 9 X_2t^2`, making it a strongly increasing function of `X_2t^2`.\n\n**Variables & Parameters.**\n- `H`: The general White test for heteroskedasticity of unknown form.\n- `F2`: A Goldfeld-Quandt F-test where the data are ordered by the `X_1` regressor before splitting the sample.\n- `F3`: A Goldfeld-Quandt F-test where the data are ordered by the `X_2` regressor before splitting the sample.\n- `Power (5% level)`: The empirical rejection frequency of the test at a nominal 5% significance level when heteroskedasticity is present.\n\n---\n\n### Data / Model Specification\n\nThe following table summarizes the power of different heteroskedasticity tests to detect this specific form of heteroskedasticity in a sample of `n=100`.\n\n**Table 1: Power of Heteroskedasticity Tests (n=100, Case 6)**\n\n| Test | Description | Power (5% level) |\n| :--- | :--- | :--- |\n| H | White's general test | 0.589 |\n| F2 | Goldfeld-Quandt ordered by `X_1` | 0.185 |\n| F3 | Goldfeld-Quandt ordered by `X_2` | 0.713 |\n\n---\n\n### Question\n\nThe results in Table 1 show that the F3 test is substantially more powerful than the F2 test. Given the data generating process, what is the correct statistical explanation for this large difference in power?",
    "Options": {
      "A": "The F3 test orders observations by `X_2`, which maximally separates the sample into low-variance and high-variance groups, aligning perfectly with the true source of heteroskedasticity.",
      "B": "The F2 test is based on an F-distribution which is known to be less reliable in small samples than the chi-squared distribution used by the H test.",
      "C": "The Goldfeld-Quandt test procedure requires omitting the middle third of observations, which causes the F2 test to lose too much information to be effective.",
      "D": "The power of the F3 test is artificially inflated because the regressor `X_2` has a larger coefficient in the underlying variance function (9) than `X_1` (1)."
    },
    "Answer": "A",
    "pi_justification": "This item uses a 'Reverse-Reasoning' strategy to test the core insight of the original QA problem's second part. It requires the user to connect the empirical result (F3 is more powerful) back to the underlying statistical mechanism (test design interacting with the DGP). The correct option (A) provides the precise explanation. Distractors are designed to be plausible but incorrect: (B) misattributes the issue to the test's distribution, (C) misattributes the issue to a general feature of the test rather than its specific application here, and (D) uses confusing language ('artificially inflated') to describe the very reason the test *should* be powerful, making it a subtle conceptual error.",
    "qid": "481",
    "model_evaluation_result": {
      "model_raw_response": "A) The correct answer is A. The F3 tests orders observations by X_2, which aligns with the true cause of heteroskadasticity.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 482,
    "Question": "### Background\n\nThe paper's results demonstrate that a statistical test for heteroskedasticity can be very powerful if its design is targeted to the true form of the problem (e.g., a Goldfeld-Quandt test ordered by the correct variable). However, the paper's main recommendation is to use the HC3 estimator by default, partly to avoid the 'pre-testing fallacy' where a researcher makes their choice of estimator (OLS vs. HC) dependent on a test result that may come from a low-power test.\n\n---\n\n### Question\n\nA researcher is estimating a wage equation and has a strong theoretical reason to suspect that the variance of the error term increases with the 'education' level of the individual. Based on the principles demonstrated in the paper, what is the most methodologically sound strategy for this researcher to follow?",
    "Options": {
      "A": "First, run a Goldfeld-Quandt test ordered by 'education'. If it is significant, use HC3 standard errors. Otherwise, use OLS standard errors.",
      "B": "Use the general White's H test to check for any form of heteroskedasticity, and only use HC3 standard errors if the H test is significant.",
      "C": "Perform a Goldfeld-Quandt test ordered by 'education' to diagnose and confirm the specific nature of the heteroskedasticity, but use the HC3 estimator for inference on the wage equation coefficients regardless of the test's p-value.",
      "D": "Since there is a strong prior, the researcher should immediately use HC3 standard errors and not bother with any diagnostic testing for heteroskedasticity."
    },
    "Answer": "C",
    "pi_justification": "This item uses a 'Scenario Application' strategy to assess the nuanced, practical takeaway from the original QA problem. It tests whether the user can synthesize the value of targeted testing with the danger of the pre-testing fallacy. The correct option (C) represents the most sophisticated strategy: use the powerful, targeted test for diagnostic purposes but commit to the robust estimator for inference based on the initial theoretical suspicion. Distractors represent common errors: (A) is the pre-testing fallacy with a targeted test, (B) is the pre-testing fallacy with a less-powerful general test, and (D) is an overly simplistic interpretation of the paper's advice that misses the value of diagnostic testing for understanding the data.",
    "qid": "482",
    "model_evaluation_result": {
      "model_raw_response": "C) Perform... etc. etc. \n\nC",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 483,
    "Question": "### Background\n\n**Research Question.** This question concerns the application of the Maximal Data Information Prior (MDIP) framework to derive a prior for a binomial proportion, `θ`.\n\n### Data / Model Specification\n\nThe probability mass function (PMF) for a single Bernoulli trial with success probability `θ` is:\n\n```latex\np(y|\\theta) = \\theta^y (1-\\theta)^{1-y}, \\quad y \\in \\{0, 1\\}\n```\n\nThe Shannon information `I(θ)` is the expected value of the log-density, `I(θ) = E[ln p(y|θ)] = θ ln(θ) + (1-θ) ln(1-θ)`. The general solution for the MDIP is `π*(θ) ∝ exp(I(θ))`.\n\nBased on the provided specifications, what is the functional form of the Maximal Data Information Prior (MDIP), `π*(θ)`, for the binomial proportion `θ`?",
    "Options": {
      "A": "```latex\n\\theta^\\theta (1-\\theta)^{1-\\theta}\n```",
      "B": "```latex\n\\theta \\ln(\\theta) + (1-\\theta) \\ln(1-\\theta)\n```",
      "C": "```latex\n\\theta^{-1/2} (1-\\theta)^{-1/2}\n```",
      "D": "```latex\n\\theta^\\theta + (1-\\theta)^{1-\\theta}\n```"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to perform the two-step derivation of the MDIP for a binomial proportion. It uses a Computational Judgment strategy. The distractors are designed to catch common errors: B is the intermediate information measure `I(θ)` instead of `exp(I(θ))`, C is a conceptually related but distinct prior (Jeffreys), and D represents a frequent algebraic mistake in exponentiation.",
    "qid": "483",
    "model_evaluation_result": {
      "model_raw_response": "B) θln(θ) +(1−θ)ln(1−θ)\n\nNote that this is the Shannon information I(θ) mentioned in the problem statement.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 484,
    "Question": "### Background\n\n**Research Question.** This question asks to interpret the prior beliefs encoded in the shape of the Maximal Data Information Prior (MDIP) for a binomial proportion, `θ`.\n\n### Data / Model Specification\n\nThe MDIP for a binomial proportion `θ` is given by `π*(θ) ∝ θ^θ (1-θ)^(1-θ)`. This density is symmetric and U-shaped, with a minimum at `θ=0.5` and finite peaks at `θ=0` and `θ=1`.\n\nThe U-shaped Maximal Data Information Prior (MDIP) for a binomial proportion `θ` contrasts with the flat Bayes-Laplace (uniform) prior. What prior belief does the MDIP's shape represent?",
    "Options": {
      "A": "A belief that the process is likely polarized, assigning higher probability to outcomes being either consistently successes (`θ` near 1) or consistently failures (`θ` near 0).",
      "B": "A belief that the process is most likely balanced, assigning the highest probability to `θ` being near 0.5.",
      "C": "A belief that the prior should be invariant to reparameterization, a key property of Jeffreys' prior.",
      "D": "A state of maximal ignorance, where all values of `θ` are considered equally likely."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to interpret the meaning of a prior distribution's shape, a critical skill in applied Bayesian analysis. It uses a Conceptual Judgment strategy. Distractors are designed to test for common confusions: B represents the opposite belief (a bell-shaped prior), C correctly states a property of a different prior (Jeffreys), and D describes the uniform prior, which the question explicitly contrasts against.",
    "qid": "484",
    "model_evaluation_result": {
      "model_raw_response": "B) A believe that the process most likely balanced",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 485,
    "Question": "### Background\n\n**Research Question.** This question tests the generalization of the Maximal Data Information Prior (MDIP) framework from a binomial to a trinomial process.\n\n### Data / Model Specification\n\nA single trial from a trinomial process has three possible outcomes with probabilities `θ₁`, `θ₂`, and `θ₃ = 1-θ₁-θ₂`. The general solution for the MDIP is `π* ∝ exp(I)`, where `I` is the Shannon information (the expected log-probability of the process).\n\nFor a single trial from a trinomial process with outcome probabilities `(θ₁, θ₂, 1-θ₁-θ₂)`, what is the functional form of the multivariate Maximal Data Information Prior (MDIP), `π*(θ₁, θ₂)`?",
    "Options": {
      "A": "```latex\n\\theta_1^{\\theta_1} \\theta_2^{\\theta_2} (1-\\theta_1-\\theta_2)^{1-\\theta_1-\\theta_2}\n```",
      "B": "```latex\n\\theta_1 \\ln(\\theta_1) + \\theta_2 \\ln(\\theta_2) + (1-\\theta_1-\\theta_2) \\ln(1-\\theta_1-\\theta_2)\n```",
      "C": "```latex\n\\theta_1^{\\alpha-1} \\theta_2^{\\beta-1} (1-\\theta_1-\\theta_2)^{\\gamma-1}\n```",
      "D": "```latex\n\\theta_1^{\\theta_1} + \\theta_2^{\\theta_2} + (1-\\theta_1-\\theta_2)^{1-\\theta_1-\\theta_2}\n```"
    },
    "Answer": "A",
    "pi_justification": "This item tests the ability to generalize the MDIP derivation from the binomial to the trinomial case. It uses a Computational Judgment strategy. The distractor design mirrors that of the binomial item: B is the un-exponentiated information measure, C is the form of the standard conjugate prior (Dirichlet), and D represents a common algebraic error.",
    "qid": "485",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: The output is the letter of your chosen option.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 486,
    "Question": "### Background\n\nIn 2002, two North Carolina districts, Charlotte-Mecklenburg (CMS) and Guilford, abruptly increased eighth-grade Algebra I enrollment. The study uses this policy shift as a natural experiment. To estimate the policy's impact, the authors first use a difference-in-differences (DiD) model, which compares the change in outcomes over time in the 'treated' districts (CMS and Guilford) to the change in outcomes in a set of 'control' districts where enrollment rates remained stable.\n\n---\n\n### The Question\n\nWhy was it crucial to include \"control\" districts in the difference-in-differences (DiD) analysis, rather than just comparing outcomes in CMS and Guilford before and after their policy changes?",
    "Options": {
      "A": "To provide an estimate of the counterfactual trend that would have occurred in the treated districts had the policy not been implemented, based on the parallel trends assumption.",
      "B": "To increase the overall sample size of the study, thereby improving the statistical precision of the estimates.",
      "C": "To ensure that the student populations in the treated and control districts were identical in terms of prior achievement and demographics before the policy change.",
      "D": "To directly measure the spillover effects of the algebra acceleration policy on neighboring school districts."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the fundamental purpose of a control group in a difference-in-differences design. It uses a Reverse-Reasoning strategy by asking for the 'why' behind a specific methodological choice. Distractor B is 'True but Irrelevant'—while a larger sample is good, it's not the primary reason for the DiD design. Distractor C describes the goal of a randomized controlled trial, not DiD, and confuses the 'parallel trends' assumption with an assumption of identical levels. Distractor D describes a different research question entirely.",
    "qid": "486",
    "model_evaluation_result": {
      "model_raw_response": "A) Aassistant\n\nA",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 487,
    "Question": "### Background\n\nThe paper's main analysis uses a two-stage least squares (2SLS) instrumental variable (IV) model to estimate the causal effect of taking Algebra I by eighth grade (\\(T_{ildc}\\)) on student outcomes (\\(y_{ildc}\\)).\n\n**First Stage:**\n```latex\nT_{ildc} = \\Phi_{l} + \\Phi_{dc} + \\rho X_{ildc} + \\delta_{ldc} + \\eta_{ildc} \n```\n**Eq. (1)**\n\n**Second Stage:**\n```latex\ny_{ildc} = \\alpha_{l} + \\alpha_{dc} + \\beta X_{ildc} + \\gamma T_{ildc} + \\varepsilon_{ildc} \n```\n**Eq. (2)**\n\nIn this model, \\(\\delta_{ldc}\\) is a set of instruments representing the policy shock for a student in district \\(l\\), cohort \\(c\\), and prior-achievement decile \\(d\\). The model also includes cohort-by-decile fixed effects (\\(\\alpha_{dc}\\)).\n\n---\n\n### The Question\n\nWhat is the primary econometric reason for this detailed specification, where instruments (\\(\\delta_{ldc}\\)) and fixed effects (\\(\\alpha_{dc}\\)) are both defined at the decile level?",
    "Options": {
      "A": "The decile-level instruments capture that the policy's intensity varied by prior achievement, while the decile-level fixed effects control for statewide trends affecting specific achievement groups, making the exclusion restriction more plausible.",
      "B": "The decile-level instruments serve to control for unobserved student ability, while the decile-level fixed effects are included to increase the first-stage F-statistic.",
      "C": "This specification is required to estimate the Average Treatment Effect (ATE) for the entire student population, rather than just the effect on compliers.",
      "D": "The decile-level instruments are used to solve the problem of selection into the sample of test-takers, while the fixed effects correct for measurement error in the outcome variables."
    },
    "Answer": "A",
    "pi_justification": "This item assesses a nuanced understanding of IV model specification, specifically the distinct roles of the instrument and the fixed effects. It tests the logic from the original QA's Q2 and Q3a. The correct answer correctly assigns the roles: instruments capture heterogeneous policy intensity (relevance), while fixed effects absorb confounders (validity). Distractor B conflates the roles of instruments and fixed effects. Distractor C incorrectly identifies the estimand (IV estimates a LATE, not ATE). Distractor D misattributes the purpose of the instruments and fixed effects to solving different econometric problems.",
    "qid": "487",
    "model_evaluation_result": {
      "model_raw_response": "A) The(decile-level instruments)capture that the policy’s intensity varied byprior achievement, whilethedecile-level fixedeffectscontrolfor statewide trends affectingspecific achievement groups, maketheexclusion restrictionmoreplausible.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 488,
    "Question": "### Background\n\nThe study notes that one of the treated districts, Charlotte-Mecklenburg Schools (CMS), implemented several other major policies around the same time as the algebra acceleration, including ending busing for desegregation and introducing a new school choice plan. To address this, the authors perform a robustness check where they re-run their main instrumental variable (IV) model on a sample that omits CMS entirely, using only variation from Guilford County and the control districts.\n\n---\n\n### The Question\n\nWhat specific threat to the IV model's validity is this robustness check primarily designed to address?",
    "Options": {
      "A": "A violation of the exclusion restriction, where the instrument (being in a CMS policy-affected cell) might be correlated with changes in student outcomes caused by the confounding policies.",
      "B": "A weak instrument problem, where the policy change in CMS was not strong enough to reliably predict eighth-grade algebra enrollment.",
      "C": "A failure of the parallel trends assumption from the study's initial difference-in-differences model.",
      "D": "A concern about external validity, suggesting the results from CMS might not generalize to other school districts."
    },
    "Answer": "A",
    "pi_justification": "This item uses a Reverse-Reasoning strategy to assess understanding of the IV exclusion restriction and how it can be violated in practice. It is based on the original QA's Q3b and Q3c. The correct answer identifies the specific threat: the instrument may be correlated with the error term through a channel other than the treatment. Distractor B points to the wrong IV assumption (relevance, not validity). Distractor C refers to an assumption from a different model (DiD). Distractor D confuses a threat to internal validity (the purpose of the check) with a concern about external validity.",
    "qid": "488",
    "model_evaluation_result": {
      "model_raw_response": "A) A violation of the exclusion restricton",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 489,
    "Question": "### Background\n\nBefore presenting its main causal estimates, the paper first runs a benchmark Ordinary Least Squares (OLS) regression to replicate the correlational findings in prior literature. The model is:\n\n```latex\nY_i = \\beta_0 + \\beta_1 T_i + D'_i\\gamma + \\varepsilon_i\n```\n**Eq. (1)**\n\nHere, \\(Y_i\\) is an indicator for passing Algebra II, \\(T_i\\) is an indicator for taking Algebra I by eighth grade, and \\(D_i\\) is a vector of controls including the student's prior math score decile. The OLS estimate for \\(\\beta_1\\) is found to be **+0.09**.\n\n---\n\n### The Question\n\nGiven that this is a correlational result and not a causal estimate, how should the OLS coefficient \\(\\hat{\\beta}_1 = +0.09\\) be interpreted?",
    "Options": {
      "A": "It indicates a partial correlation: holding prior test score decile constant, students who took algebra early were 9 percentage points more likely to pass Algebra II.",
      "B": "It shows a causal effect: taking algebra early causes a 9 percentage point increase in the probability of passing Algebra II.",
      "C": "It means that 9% of all students who took algebra early went on to pass Algebra II.",
      "D": "It is a statistically significant result, which proves that the positive relationship between early algebra and later success is real and not due to chance."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to correctly interpret a regression coefficient from a non-causal model, a foundational skill. It is based on the original QA's Q1. The correct answer properly identifies the estimate as a partial correlation. Distractor B makes the cardinal sin of interpreting correlation as causation. Distractor C fundamentally misunderstands how to interpret a regression coefficient. Distractor D conflates statistical significance with causal validity, a classic misconception.",
    "qid": "489",
    "model_evaluation_result": {
      "model_raw_response": "A) It is the correct answer. \n\nOutput: A",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 490,
    "Question": "### Background\n\nA simple OLS regression finds that taking Algebra I by eighth grade is associated with a 9 percentage point *increase* in the probability of passing Algebra II. However, the paper's main instrumental variable (IV) analysis finds that the true causal effect is negative. This discrepancy is attributed to omitted variable bias (OVB) in the OLS model, stemming from unobserved student characteristics like motivation or innate ability.\n\n---\n\n### The Question\n\nWhat is the correct econometric explanation for why the OLS estimate is positively biased, creating this contradiction?",
    "Options": {
      "A": "The bias is positive because unobserved ability is positively correlated with both taking Algebra I early and with passing Algebra II.",
      "B": "The bias is negative because higher-ability students are often held back from early algebra, and this unobserved ability is positively correlated with passing Algebra II.",
      "C": "The bias is positive because the OLS model controls for prior test scores, which artificially inflates the coefficient on the early algebra indicator.",
      "D": "The bias is zero because any unobserved student ability is captured by the error term, which is by definition uncorrelated with the regressors."
    },
    "Answer": "A",
    "pi_justification": "This item assesses a deep understanding of omitted variable bias, requiring the student to sign both components of the bias formula in context. It uses a Computational Judgment strategy to test the logic from the original QA's Q2 and Q3. The correct answer correctly identifies the two positive correlations that create the positive bias. Distractor B gets the sign of the bias wrong by incorrectly signing the first correlation (Conceptual Opposite). Distractor C misidentifies the role of control variables. Distractor D states the OLS assumption as if it were a fact, ignoring that it is violated here, which is the entire point of OVB.",
    "qid": "490",
    "model_evaluation_result": {
      "model_raw_response": "B) Thebias is negative becausehigher-ability studentsare often held backfrom early algebra,andsuch unobservedability is positively correlatedwith passing Algebra II",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 491,
    "Question": "### Background\n\nThis problem explores the quantitative impact of the labor margin on measured risk aversion for a canonical utility function. The presence of a labor margin allows households to absorb shocks through changes in hours worked, which attenuates risk aversion compared to a consumption-only model.\n\n### Data / Model Specification\n\nConsider an additively separable period utility function from Example 2 of the paper:\n```latex\nu(c_{t},l_{t})=\\frac{c_{t}^{1-\\gamma}}{1-\\gamma}-\\eta\\frac{l_{t}^{1+\\chi}}{1+\\chi} \\quad \\text{(Eq. 1)}\n```\nwhere `γ, χ, η > 0`. The traditional measure of relative risk aversion for these preferences is `γ`.\n\nThe consumption-only coefficient of relative risk aversion, `R^c`, which correctly accounts for the labor margin, is given by the general formula:\n```latex\nR^{c}(a;\\theta)=\\frac{-u_{11}+\\lambda u_{12}}{u_{1}}\\frac{c}{1+w\\lambda} \\quad \\text{(Eq. 2)}\n```\nwhere `λ` is a term related to the marginal rate of substitution between consumption and labor adjustments, and `w` is the wage.\n\n### Question\n\nFor the utility function in Eq. (1), what is the correct expression for the coefficient of relative risk aversion, `R^c`, evaluated at the steady state?",
    "Options": {
      "A": "```latex\n\\gamma\n```",
      "B": "```latex\n\\frac{1}{\\frac{1}{\\gamma}+\\frac{1}{\\chi}}\n```",
      "C": "```latex\n\\frac{\\gamma}{1+\\frac{\\gamma}{\\chi}}\n```",
      "D": "```latex\n\\frac{\\gamma}{1+\\frac{\\gamma}{\\chi}\\frac{wl}{c}}\n```"
    },
    "Answer": "D",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's ability to perform the key derivation in Example 2 of the paper. It uses a Computational Judgment strategy. The distractors are designed to capture specific, plausible errors: (A) ignoring the labor margin entirely (the traditional measure), (B) using a simplifying approximation from the paper that does not hold in general, and (C) a step-omission error in the final algebraic substitution.",
    "qid": "491",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\n(Note: This answer assumes that the variables are defined as in the context of the question and that the notation used is consistent with the provided equations.)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 492,
    "Question": "### Background\n\nA key result of the paper is that the household can become effectively risk neutral under certain conditions, even if the utility function is strictly concave in consumption. This occurs when the household can absorb wealth shocks along a margin that has a constant marginal utility cost.\n\n### Data / Model Specification\n\nConsider the additively separable utility function:\n```latex\nu(c_{t},l_{t})=\\frac{c_{t}^{1-\\gamma}}{1-\\gamma}-\\eta\\frac{l_{t}^{1+\\chi}}{1+\\chi}\n```\nThe correctly measured relative risk aversion for this function is:\n```latex\nR^{c} = \\frac{\\gamma}{1+\\frac{\\gamma}{\\chi}\\frac{wl}{c}}\n```\nIn the special case where the disutility of labor becomes linear, which corresponds to `χ → 0`, the household becomes effectively risk neutral (`R^c → 0`).\n\n### Question\n\nWhich of the following statements provides the most accurate economic intuition for why `R^c → 0` as `χ → 0`?",
    "Options": {
      "A": "With linear labor disutility, the household's utility becomes linear in consumption, making them risk neutral.",
      "B": "The household's labor supply becomes perfectly inelastic, fixing labor income and making wealth shocks irrelevant.",
      "C": "Additive separability between consumption and labor means that shocks to the labor market do not spill over into consumption decisions.",
      "D": "The household can absorb wealth shocks by adjusting hours worked at a constant marginal utility cost, thereby perfectly insulating consumption from the shock."
    },
    "Answer": "D",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of the economic intuition behind one of the paper's most striking results. It uses a Reverse-Reasoning strategy. The distractors are designed based on common confusions: (A) misattributing the source of risk neutrality (Conceptual Opposite), (B) confusing elastic with inelastic labor supply (Conceptual Opposite), and (C) citing a true but irrelevant feature of the utility function as the primary cause.",
    "qid": "492",
    "model_evaluation_result": {
      "model_raw_response": "D) The householder can absorb wealth shock by adjusting hoursworked at a constant margnal utility cost, there by perfectly insulating consumpton from the shock",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 493,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the paper's central normative claim: that a systematic, state-contingent monetary policy can be designed to eliminate informational inefficiencies.\n\n**Setting.** In the baseline model, random money demand shocks (`ε_{2t}`) contaminate price signals. The monetary authority can commit to a policy rule where the money supply in the next period (`t+1`) is adjusted based on the realized economic shocks in the current period (`t`).\n\n### Data / Model Specification\n\nThe monetary authority implements the following feedback rule:\n\n```latex\nM_{t+1}^{s} = M_{t} + γ_1 ε_{1t} + γ_2 ε_{2t} \\quad \\text{(Eq. (1))}\n```\n\nThe objective is to choose the policy parameters `γ_1` and `γ_2` to perfectly stabilize the current price level `P̄_t` such that it is insulated from current-period shocks, resulting in an equilibrium where `P̄_t = M_t`.\n\n### The Question\n\nThe policy rule in Eq. (1) adjusts the *future* money supply `M^s_{t+1}` based on *current* shocks. What is the primary economic mechanism through which this commitment influences the *current* price level `P̄_t`?",
    "Options": {
      "A": "It alters agents' expectations of future inflation, changing the expected real return on holding money today.",
      "B": "It directly injects money into the economy in period `t` to counteract the money demand shock.",
      "C": "It causes the real price of capital `(V_t - P̄_t)` to become a sufficient statistic for the real productivity shock.",
      "D": "It increases the expected real return on money, encouraging agents to hold the fixed money supply."
    },
    "Answer": "A",
    "pi_justification": "This item assesses understanding of the policy's core transmission mechanism (the expectations channel). It replaces the explanatory part of a larger QA problem. The distractors are designed to pinpoint specific misunderstandings: (B) is a timing error, confusing future action with current action; (C) identifies a correct *consequence* of the policy, not its mechanism; (D) represents a conceptual opposite, misstating the direction of the effect on the return to money.",
    "qid": "493",
    "model_evaluation_result": {
      "model_raw_response": "B) It directs injects money in the economy in the period t to counteract money demand shock.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 494,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the paper's central normative claim: that a systematic, state-contingent monetary policy can be designed to eliminate informational inefficiencies.\n\n**Setting.** In the baseline model, random money demand shocks (`ε_{2t}`) contaminate price signals. The monetary authority can commit to a policy rule where the money supply in the next period (`t+1`) is adjusted based on the realized economic shocks in the current period (`t`).\n\n### Data / Model Specification\n\nThe monetary authority implements the following feedback rule:\n\n```latex\nM_{t+1}^{s} = M_{t} + γ_1 ε_{1t} + γ_2 ε_{2t} \\quad \\text{(Eq. (1))}\n```\n\nThe objective is to choose the policy parameters `γ_1` and `γ_2` to perfectly stabilize the current price level `P̄_t` such that it is insulated from current-period shocks, resulting in an equilibrium where `P̄_t = M_t`.\n\nThe money market equilibrium condition is:\n\n```latex\n\\bar{P}_{t} = \\frac{1}{1+m_1}M_t + \\frac{m_1}{1+m_1}E^{*}(\\bar{P}_{t+1}) + \\frac{m_2 k_1}{(1+m_1)(k_1+k_2)}\\varepsilon_{1t} - \\frac{1}{1+m_1}\\varepsilon_{2t} \\quad \\text{(Eq. (2))}\n```\n\nUnder the proposed policy, agents form rational expectations of `P̄_{t+1}`. With a sufficient statistic for `ε_{1t}`, `E*(ε_{1t}) = ε_{1t}`. Agents' average expectation of the monetary shock, based on their private signals, is `E^{*}(ε_{2t}) = ε_{2t} / (1+σ_ρ^2)`, where `σ_ρ^2` is the variance of idiosyncratic money demand shocks.\n\n### The Question\n\nTo achieve the policy goal of `P̄_t = M_t`, the monetary authority must set the feedback parameter `γ_2` to a specific value. What is the correct expression for the optimal `γ_2`?",
    "Options": {
      "A": "```latex\n\\frac{(1+m_1)(1+\\sigma_{\\rho}^{2})}{m_1}\n```",
      "B": "```latex\n\\frac{1+m_1}{m_1}\n```",
      "C": "```latex\n\\frac{m_1}{(1+m_1)(1+\\sigma_{\\rho}^{2})}\n```",
      "D": "```latex\n-\\frac{m_2 k_1}{m_1(k_1+k_2)}\n```"
    },
    "Answer": "A",
    "pi_justification": "This item uses the Computational Judgment strategy to test the core derivation from the original QA problem. The correct answer (A) requires correctly setting up the rational expectations equilibrium and solving for the parameter. The distractors are based on high-fidelity, plausible errors: (B) is a Step-Omission Error, where the user forgets to account for the noisy private signal (`σ_ρ^2`) when forming expectations of `ε_{2t}`; (C) is a Formula Misuse error from algebraic inversion; (D) is a Conceptual Confusion error, as it is the correct formula for the *other* policy parameter, `γ_1`.",
    "qid": "494",
    "model_evaluation_result": {
      "model_raw_response": "C) \n\nNote: The correct answer is C) `\\frac{m_{1}}{(1+m_{1})(1+\\sigma^{2}_{\\rho})}`.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 495,
    "Question": "### Background\n\n**Research Question.** How does the nature of strategic interaction in the underlying game—specifically, whether actions are strategic substitutes or complements—alter the conditions required for information to be fully revealed?\n\n**Setting.** Consider a duopoly (`n=2`) where firms' private information ('type') affects their profitability. The firms first make a certified announcement about their type, then compete in a second-stage game. This problem contrasts the conditions needed for information revelation in two different competitive environments.\n\n### Data / Model Specification\n\nThe paper provides sufficient conditions for the crucial property of **weak positive-monotonicity** (where an agent's payoff increases if they are perceived as a 'higher' type).\n\nIn the case of **Strategic Substitutes (SS)** (e.g., Cournot quantity competition), two key conditions are:\n- **Assumption 4(b):** Marginal payoff `∂π_i/∂s_i` is *decreasing* in the rival's action `s_j`.\n- **Assumption 4(c):** Marginal payoff `∂π_i/∂s_i` is *increasing* in own type `t_i`.\n\n**Theorem 2:** Under these (and other regularity) conditions, payoffs are weakly positive-monotone.\n\n### Question\n\nIn a game with strategic substitutes (e.g., Cournot competition), what is the correct causal mechanism that gives a firm an incentive to reveal it is of a 'higher' type (e.g., has lower costs), as described by Theorem 2?",
    "Options": {
      "A": "Revealing a higher type signals more aggressive future action (e.g., higher quantity). Due to strategic substitutability, the rival firm responds by acting less aggressively (lower quantity), which increases the revealing firm's payoff.",
      "B": "Revealing a higher type signals less aggressive future action. The rival firm responds by also acting less aggressively, and this mutual de-escalation increases the revealing firm's payoff.",
      "C": "Revealing a higher type signals more aggressive future action. The rival firm is forced to match this aggression, but the revealing firm's superior type allows it to profit from the intensified competition.",
      "D": "Revealing a higher type has no impact on the rival's action but directly increases the firm's own payoff due to its intrinsic efficiency, regardless of strategic interaction."
    },
    "Answer": "A",
    "pi_justification": "This question assesses the student's understanding of the core causal chain for information revelation under strategic substitutes. It uses a Reverse-Reasoning strategy, asking for the mechanism behind a known result. Distractors are designed based on common conceptual errors: B applies the logic of strategic complements, C incorrectly models the rival's reaction, and D ignores the strategic channel entirely.",
    "qid": "495",
    "model_evaluation_result": {
      "model_raw_response": "A) Revealling a higher type signalled more aggressive future actio",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 496,
    "Question": "### Background\n\n**Research Question.** How does the nature of strategic interaction in the underlying game—specifically, whether actions are strategic substitutes or complements—alter the conditions required for information to be fully revealed?\n\n**Setting.** Consider a duopoly (`n=2`) where firms' private information ('type') affects their profitability. The firms first make a certified announcement about their type, then compete in a second-stage game.\n\n### Data / Model Specification\n\nIn a game with **Strategic Complements (SC)** (e.g., Bertrand price competition), the marginal payoff `∂π_i/∂s_i` is *increasing* in the rival's action `s_j`.\n\nFor the crucial property of **weak positive-monotonicity** to hold in this setting, the paper shows that the marginal payoff `∂π_i/∂s_i` must be *decreasing* in an agent's own type `t_i` (Assumption 4'(c)).\n\nConsider a specific Bertrand duopoly model where:\n- Firms set prices `p_i`.\n- The action is defined as `s_i = -p_i` (a higher `s_i` means a lower, more aggressive price).\n- A firm's type is its constant marginal cost, `t_i = c_i` (a higher type `t_i` means a higher, less favorable cost).\n\nIn this specific model, it can be shown that the marginal payoff `∂π_i/∂s_i` is *increasing* in type `t_i = c_i`, which violates the condition in Assumption 4'(c).\n\n### Question\n\nGiven that this Bertrand model violates the key condition (Assumption 4'(c)) required for positive monotonicity, what is the resulting incentive for information revelation?",
    "Options": {
      "A": "The high-cost firm has an incentive to reveal its type, as this credibly signals it will set a higher price, inducing its rival to also set a higher price, which is beneficial.",
      "B": "The low-cost firm has an incentive to reveal its type, as this signals its competitive strength and forces the rival to accommodate with higher prices.",
      "C": "Both firms have an incentive to reveal their types, as more information always leads to more profitable outcomes in Bertrand competition.",
      "D": "Neither firm has an incentive to reveal its type, because the violation of Assumption 4'(c) means that any signal about cost will be met with a profit-destroying price war."
    },
    "Answer": "A",
    "pi_justification": "This question uses a Scenario Application strategy to test a deeper understanding of how theoretical conditions apply to a concrete economic model. It requires the student to reason from a stated violation of an assumption to the correct strategic incentive. Distractors are based on plausible errors: B misidentifies the revealing firm, C makes a false general claim, and D incorrectly concludes that no revelation occurs.",
    "qid": "496",
    "model_evaluation_result": {
      "model_raw_response": "D) Neitherfirmhasanincentivetorevealits type,becausetheviolationofAssumption4(c)meansthatanysignalaboutcostwillbemetwithaprofit-destroyingprice war.",
      "model_answer": "D",
      "error": null
    }
  },
  {
    "ID": 497,
    "Question": "### Background\n\n**Research Question.** This problem analyzes experimental evidence from the Dirty Faces game to assess whether group behavior conforms to the theoretical equilibrium, particularly as the game's complexity (number of players `n`) and the required level of iterated reasoning (`k-1` steps) increase.\n\n**Setting.** An experiment is conducted on the Dirty Faces game with two different group sizes: `n=2` and `n=3`. At the start of each round, it is publicly announced that at least one player has type 'X'. Group behavior is then compared to the theoretical predictions of the unique Nash Equilibrium.\n\n**Variables and Parameters.**\n- `n`: Number of players (2 or 3).\n- `k`: Number of players with type 'X'.\n- `Predicted Behavior`: The sequence of actions corresponding to the unique Nash equilibrium.\n\n### Data / Model Specification\n\nThe theoretical equilibrium requires `k-1` levels of iterated knowledge of rationality, where `k` is the number of players with type 'X'. The experimental results for group-level behavior are summarized in Table 1 (for `n=2`) and Table 2 (for `n=3`).\n\n**Table 1. Summary of Group Results for n=2**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XO | 8 | (DU) | 7 | 0.88 |\n| XX | 18 | ((UU)(DD)) | 4 | 0.22 |\n| **Total** | **26** | | **11** | **0.42** |\n\n**Table 2. Summary of Group Results for n=3**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XOO | 6 | (DUU) | 3 | 0.50 |\n| XXO | 10 | (UUU)(DDU) | 1 | 0.10 |\n| XXX | 12 | (UUU)(UUU)(DDD) | 0 | 0.00 |\n| **Total** | **28** | | **4** | **0.14** |\n\n### Question\n\nBased on the data in Tables 1 and 2, what is the most accurate conclusion about how group conformity to the theoretical equilibrium changes with the required level of iterated reasoning?",
    "Options": {
      "A": "Conformity is high when only base rationality is required but drops sharply with each additional level of iterated reasoning, suggesting higher-order beliefs are the primary barrier.",
      "B": "Conformity declines primarily as the number of players (n) increases, but is largely independent of the number of 'X' types (k) within a given group size.",
      "C": "Groups are more likely to conform to the equilibrium in more complex situations (higher k) because the potential payoffs from coordinating are greater.",
      "D": "The overall success rate is low in both the n=2 and n=3 games, indicating that subjects are generally not rational, regardless of the game's complexity."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question assesses the core empirical finding of the paper by asking for a comparative interpretation of the data. It uses a Scenario Application strategy. The correct answer (A) accurately synthesizes the key trend. Distractors are designed based on plausible misinterpretations: (B) confuses group size (n) with reasoning steps (k), (C) is a conceptual opposite of the findings, and (D) offers a true but overly simplistic conclusion that misses the crucial gradient of failure.",
    "qid": "497",
    "model_evaluation_result": {
      "model_raw_response": "C) Groups is more likely to converge to the equilibrium as the game becomes more complex (higher k).",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 498,
    "Question": "### Background\n\n**Research Question.** This problem analyzes experimental evidence from the Dirty Faces game to assess whether group behavior conforms to the theoretical equilibrium, particularly as the game's complexity (number of players `n`) and the required level of iterated reasoning (`k-1` steps) increase.\n\n**Setting.** An experiment is conducted on the Dirty Faces game with two different group sizes: `n=2` and `n=3`. At the start of each round, it is publicly announced that at least one player has type 'X'. In its analysis of the `n=2, XX` case, the paper notes that while only 4 of 18 groups conformed to the full predicted behavior, 14 of the 18 groups successfully completed Period 1 by playing `(UU)` and thus reached the second period.\n\n**Variables and Parameters.**\n- `n`: Number of players (2 or 3).\n- `k`: Number of players with type 'X'.\n- `Predicted Behavior`: The sequence of actions corresponding to the unique Nash equilibrium.\n\n### Data / Model Specification\n\nThe theoretical equilibrium requires `k-1` levels of iterated knowledge of rationality, where `k` is the number of players with type 'X'. The experimental results for group-level behavior are summarized in Table 1 (for `n=2`) and Table 2 (for `n=3`).\n\n**Table 1. Summary of Group Results for n=2**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XO | 8 | (DU) | 7 | 0.88 |\n| XX | 18 | ((UU)(DD)) | 4 | 0.22 |\n| **Total** | **26** | | **11** | **0.42** |\n\n**Table 2. Summary of Group Results for n=3**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XOO | 6 | (DUU) | 3 | 0.50 |\n| XXO | 10 | (UUU)(DDU) | 1 | 0.10 |\n| XXX | 12 | (UUU)(UUU)(DDD) | 0 | 0.00 |\n| **Total** | **28** | | **4** | **0.14** |\n\n### Question\n\nAn analyst proposes a simple error model where `p_E` is the probability a player who should play 'U' mistakenly plays 'D'. First, estimate `p_E` using the data for the `n=3, XOO` case from Table 2. Then, use this `p_E` to predict the success rate for Period 1 of the `n=2, XX` case. How does this prediction compare to the observed Period 1 success rate, and what does the comparison imply?",
    "Options": {
      "A": "The model predicts a ~50% success rate, which significantly under-predicts the observed ~78% success rate, suggesting the error rate is not constant and is likely lower in the simpler n=2 game.",
      "B": "The model predicts a ~71% success rate, which is close to the observed ~78% success rate, suggesting the simple error model is a good fit for the data.",
      "C": "The model predicts a ~29% success rate, which significantly under-predicts the observed ~78% success rate, suggesting players are far more rational than the model assumes.",
      "D": "The model predicts a ~50% success rate, which over-predicts the observed ~22% overall success rate for the XX case, suggesting the model fails to capture failures in later periods."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question uses the Computational Judgment strategy to test multi-step quantitative reasoning and interpretation. The correct answer (A) requires calculating an error rate, making an out-of-sample prediction, and correctly interpreting the discrepancy. Distractors are based on specific, plausible errors: (B) a calculation error (using `1-p_E` instead of `(1-p_E)^2`), (C) another calculation error (using `p_E` instead of `1-p_E`), and (D) a conceptual error (comparing the Period 1 prediction to the overall success rate).",
    "qid": "498",
    "model_evaluation_result": {
      "model_raw_response": "B) The correct answer is B) The model predictions a ~71%, which is close the observed ~72% success rate.",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 499,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the sources of the earnings gap between relative payment and piece-rate contracts by moving from a simple regression estimate to a formal decomposition of the gap into a portion explained by worker characteristics and a portion explained by the contractual wage structure itself.\n\n**Setting and Sample.** The analysis uses a sample of 105 shrimp gatherers in Honduras. The dependent variable is the natural logarithm of daily larva income. The total observed difference in mean log wages between the two groups is 0.346.\n\n### Data / Model Specification\n\nAn initial Ordinary Least Squares (OLS) model is estimated on the pooled sample of all workers, with results in Table 1.\n\n**Table 1: OLS Wage Differentials by Contract**\n\n| Variable | Pooled Coefficient (t-stat) |\n| :--- | :---: |\n| Constant | 3.09 (23.04) |\n| Literacy | 0.13 (2.29) |\n| Larva experience | 0.05 (3.27) |\n| Gatherer age | 0.002 (-0.84) |\n| Village B | -0.35 (-3.65) |\n| Village C | 0.15 (-2.14) |\n| **Relative payment** | **0.18 (1.92)** |\n| Sample size | 105 |\n| Adjusted R2 | 0.43 |\n\nTo further investigate the wage gap, an Oaxaca-Blinder decomposition is performed. This method partitions the total gap into a component due to differences in mean characteristics (the 'explained' part) and a component due to differences in the returns to those characteristics (the 'unexplained' part, or wage structure). The results are in Table 2.\n\n**Table 2: Decomposition of Relative Payment – Piece-rate Log Wage Differential**\n(Total Difference in Mean Log Wages = 0.346)\n\n| | Evaluated at piece-rate means | Evaluated at relative payment means | Cotton method |\n| :--- | :---: | :---: | :---: |\n| **Difference due to:** | | |\n| Wage structure | 82% (0.28) | 39% (0.13) | 60% (0.21) |\n| Gatherer characteristics | 18% (0.06) | 61% (0.21) | 40% (0.14) |\n\n*Note: Numbers in parentheses are log point differences.* \n\n### Question\n\nAccording to the 'Cotton method' results in Table 2, 60% of the total wage gap is attributed to the 'Wage structure'. What is the correct economic interpretation of this finding?",
    "Options": {
      "A": "It represents the portion of the earnings gap that is unexplained by observable worker characteristics, suggesting the presence of a contractual rent.",
      "B": "It represents the portion of the earnings gap that is explained by the fact that workers in relative payment contracts have more productive characteristics.",
      "C": "It confirms that workers in relative payment contracts are approximately 60% more productive than those in piece-rate contracts.",
      "D": "It is the simple wage premium estimated by the OLS model in Table 1, which does not account for differences in worker characteristics."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). The original QA problem's components are highly suitable for conversion. Conceptual Clarity = 8/10 (interpreting specific regression and decomposition outputs is a structured task). Discriminability = 10/10 (classic misconceptions exist around explained vs. unexplained components and log-level interpretations). This item assesses the core conceptual takeaway of the Oaxaca decomposition. The distractors are designed to capture common misinterpretations: B confuses the 'unexplained' component with the 'explained' component; C misinterprets the 'wage structure' as a measure of productivity; D confuses the sophisticated decomposition result with the 'naive' OLS estimate.",
    "qid": "499",
    "model_evaluation_result": {
      "model_raw_response": "C) It represents that workers in piece-rate contacts are approximately 40% more productive, while workers in relative-payment contracts are approximately -20% less productive.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 500,
    "Question": "### Background\n\n**Research Question.** This problem analyzes the sources of the earnings gap between relative payment and piece-rate contracts by moving from a simple regression estimate to a formal decomposition of the gap into a portion explained by worker characteristics and a portion explained by the contractual wage structure itself.\n\n**Setting and Sample.** The analysis uses a sample of 105 shrimp gatherers in Honduras. The dependent variable is the natural logarithm of daily larva income. The total observed difference in mean log wages between the two groups is 0.346.\n\n### Data / Model Specification\n\nAn initial Ordinary Least Squares (OLS) model is estimated on the pooled sample of all workers, with results in Table 1.\n\n**Table 1: OLS Wage Differentials by Contract**\n\n| Variable | Pooled Coefficient (t-stat) |\n| :--- | :---: |\n| Constant | 3.09 (23.04) |\n| Literacy | 0.13 (2.29) |\n| Larva experience | 0.05 (3.27) |\n| Gatherer age | 0.002 (-0.84) |\n| Village B | -0.35 (-3.65) |\n| Village C | 0.15 (-2.14) |\n| **Relative payment** | **0.18 (1.92)** |\n| Sample size | 105 |\n| Adjusted R2 | 0.43 |\n\nTo further investigate the wage gap, an Oaxaca-Blinder decomposition is performed. This method partitions the total gap into a component due to differences in mean characteristics (the 'explained' part) and a component due to differences in the returns to those characteristics (the 'unexplained' part, or wage structure). The results are in Table 2.\n\n**Table 2: Decomposition of Relative Payment – Piece-rate Log Wage Differential**\n(Total Difference in Mean Log Wages = 0.346)\n\n| | Evaluated at piece-rate means | Evaluated at relative payment means | Cotton method |\n| :--- | :---: | :---: | :---: |\n| **Difference due to:** | | |\n| Wage structure | 82% (0.28) | 39% (0.13) | 60% (0.21) |\n| Gatherer characteristics | 18% (0.06) | 61% (0.21) | 40% (0.14) |\n\n*Note: Numbers in parentheses are log point differences.* \n\n### Question\n\nThe 'Evaluated at piece-rate means' column in Table 2 provides a counterfactual analysis. Using the 'Wage structure' component from this column (0.28 log points), what would be the expected percentage increase in the daily wage for an average piece-rate worker if they were paid according to the relative payment wage structure?",
    "Options": {
      "A": "28.0%",
      "B": "6.2%",
      "C": "32.3%",
      "D": "41.3%"
    },
    "Answer": "C",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item uses the 'Computational Judgment' strategy to test a key quantitative finding from the paper. The calculation is straightforward but requires knowing the correct formula for converting log points to percentages, a common point of error. The distractors are based on predictable errors: A (Sign Error/Formula Misuse) treats the 0.28 log points directly as a percentage; B (Step-Omission Error) uses the wrong value from the table (the 'characteristics' component, 0.06); D (Formula Misuse) uses the total wage gap (0.346) instead of the specific counterfactual component.",
    "qid": "500",
    "model_evaluation_result": {
      "model_raw_response": "C) 28% (0/10)",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 501,
    "Question": "### Background\n\n**Research Question.** This question examines the causal logic behind the specific design of a \"role model\" intervention in a micro-entrepreneurship training program.\n\n**Setting and Sample.** The study was conducted with low-income micro-entrepreneurs in Chile, who were predominantly women (92%) with an average age of 45. One treatment arm involved a visit from a role model, who was a successful alumnus of the same program.\n\n### Data / Model Specification\n\n**Table 1: Role Model Characteristics**\n\n| Variable | Mean |\n| :--- | :--- |\n| Women | 0.93 |\n| Age | 48 |\n| Income (CLP$) | 1,998,439 |\n\n*Notes: The average household income for participants was CLP$390,000.* \n\n### Question\n\nThe study selected role models who were demographically similar to participants (e.g., 93% women, avg. age 48) but significantly more successful (avg. income >5x higher). What is the most likely causal inference goal behind this specific 'relatable yet aspirational' design choice?",
    "Options": {
      "A": "To isolate the psychological impact of seeing an attainable success story, separating motivation from other factors like expert advice or gender dynamics.",
      "B": "To provide the most effective and advanced business strategies from proven experts, maximizing knowledge transfer.",
      "C": "To minimize logistical costs by recruiting from a pool of local, readily available program alumni.",
      "D": "To ensure that the treatment effect was not driven by differences in business sector between role models and participants."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of the core causal logic behind the 'relatable yet aspirational' intervention design, a key innovation of the paper. It uses a Reverse-Reasoning strategy. The correct option (A) identifies the goal of isolating a psychological mechanism. Distractors are designed to be plausible but incorrect: (B) confounds the role model's purpose with that of a consultant (Conceptual Opposite), (C) offers a practical reason instead of a causal one (True but Irrelevant), and (D) focuses on a secondary control variable rather than the primary theoretical motivation (Conceptual Error).",
    "qid": "501",
    "model_evaluation_result": {
      "model_raw_response": "A) To isolatethe psychological impact ofseeing an attainable successtory, separating motivationfrom other factors likeexpert advice or genderdynamics.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 502,
    "Question": "### Background\n\n**Research Question.** This question examines potential threats to valid causal inference in experimental design.\n\n**Setting and Sample.** A study aims to estimate the causal impact of a *relatable, aspirational peer* on the motivation and success of micro-entrepreneurs. The intended treatment is a visit from a successful alumnus of the same training program who shares a similar demographic background with the participants.\n\n### Question\n\nSuppose a hypothetical alternative intervention used highly successful, male CEOs of large corporations as speakers instead of program alumni. From a causal inference perspective, what is the primary threat this alternative design introduces when trying to estimate the effect of a *relatable peer*?",
    "Options": {
      "A": "Treatment Confounding: The effect of the CEO speaker would bundle the impact of gender, extreme wealth, and inapplicable advice, making it impossible to isolate the intended 'relatable peer' motivation effect.",
      "B": "Poor Compliance: Participants would be less likely to attend the session, leading to a weaker Intention-to-Treat (ITT) effect.",
      "C": "Attrition Bias: The discouraging nature of the CEO speaker might cause more participants in the treatment group to drop out of the study, biasing the results.",
      "D": "Positive Selection Bias: Only the most ambitious participants would be inspired by the CEOs, leading to an overestimation of the average treatment effect."
    },
    "Answer": "A",
    "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item tests the ability to apply causal inference concepts to critique a hypothetical experimental design. It uses a Scenario Application strategy. The correct option (A) precisely identifies treatment confounding as the primary threat. Distractors represent other plausible but secondary econometric issues: (B) focuses on compliance (Almost Right), (C) on attrition (True but Irrelevant to the core identification problem), and (D) on a specific type of heterogeneous effect rather than the primary threat of a confounded treatment (Conceptual Error).",
    "qid": "502",
    "model_evaluation_result": {
      "model_raw_response": "A) Treatment confounding. \n\n(Note: I will only output the answer as per the format specified, which is the letter of correct choice.)",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 503,
    "Question": "### Background\n\nIn a `k`-factor Gaussian term structure model, the yield `Y_t(τ)` on a zero-coupon bond with maturity `τ` is an affine function of `k` unobserved factors `z_t`:\n```latex\nY_t(τ) = a(τ) + b(τ)'z_t \\quad \text{(Eq. 1)}\n```\nThe factor loadings `b(τ)` are determined by the factors' risk-neutral mean-reversion speeds, `κᵢ`.\n\n### Data / Model Specification\n\nThe functional form for the factor loadings is derived by solving a system of ordinary differential equations. Letting `B_i(τ) = τb_i(τ)`, the relevant ODE for the `i`-th component is:\n```latex\n\\frac{dB_i(τ)}{dτ} = -κ_i B_i(τ) + 1, \\quad \\text{with boundary condition } B_i(0) = 0 \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nBy solving the ordinary differential equation in Eq. (2) with its boundary condition, what is the correct expression for the factor loading `b_i(τ)`?",
    "Options": {
      "A": "`\\frac{1 - e^{-κ_i τ}}{κ_i τ}`",
      "B": "`\\frac{1 - e^{-κ_i τ}}{κ_i}`",
      "C": "`\\frac{e^{κ_i τ} - 1}{κ_i τ}`",
      "D": "`\\frac{1 - e^{-κ_i τ}}{τ}`"
    },
    "Answer": "A",
    "pi_justification": "This item assesses the student's ability to solve a foundational differential equation from the Gaussian term structure model. It uses a Derivational Judgment strategy. The distractors are designed based on high-frequency errors in solving ODEs and algebraic manipulation: (B) provides the intermediate solution `B(τ)` instead of the final `b(τ)`, (C) represents a common sign error, and (D) omits a key parameter from the final expression.",
    "qid": "503",
    "model_evaluation_result": {
      "model_raw_response": "C) `\\[\\frac{e^{\\kappa_i \\tau}-1}{\\kappa_i}\\]",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 504,
    "Question": "### Background\n\nA key testable implication of the Gaussian term structure model is that the factor loading function `b(τ)` should be the same whether estimated from yield levels `Y_t(τ)` or from forward differences `Ỹ_t(τ) = Y_t(τ) - F_{t-h}(τ)`.\n\n### Data / Model Specification\n\nThe model implies that `Y_t(τ) = a(τ) + b(τ)'z_t` (Eq. 1), where the factor loading for factor `i` is given by:\n```latex\nb_i(τ) = \\frac{1 - e^{-κ_i τ}}{κ_i τ} \\quad \\text{(Eq. 2)}\n```\nThe forward rate `F_{t-h}(τ)` is defined as:\n```latex\nF_{t-h}(τ) = \\frac{τ+h}{τ}Y_{t-h}(τ+h) - \\frac{h}{τ}Y_{t-h}(h) \\quad \\text{(Eq. 3)}\n```\nUsing these equations, it can be shown that the factor-dependent part of `Ỹ_t(τ)` relies on the following identity:\n```latex\n\\frac{τ+h}{τ}b_i(τ+h) - \\frac{h}{τ}b_i(h) = e^{-κ_i h}b_i(τ) \\quad \\text{(Eq. 4)}\n```\n\n### Question\n\nGiven the identity in Eq. (4), the factor-dependent part of the forward difference `Ỹ_t(τ)` can be written as `b(τ)'z̃_t`. What is the correct expression for the `i`-th component of the transformed factor, `z̃_{it}`?",
    "Options": {
      "A": "`z_{it} - e^{-κ_i h}z_{i,t-h}`",
      "B": "`z_{it} - z_{i,t-h}`",
      "C": "`e^{-κ_i h}z_{it} - z_{i,t-h}`",
      "D": "`z_{it} - b_i(h)z_{i,t-h}`"
    },
    "Answer": "A",
    "pi_justification": "This item tests the student's ability to complete the derivation of the model's key overidentifying restriction. It uses a Derivational Judgment strategy, focusing on the final algebraic step. The distractors are based on plausible errors in applying the given identity: (B) omits the crucial decay factor, (C) misapplies the decay factor, and (D) substitutes an incorrect but conceptually related term.",
    "qid": "504",
    "model_evaluation_result": {
      "model_raw_response": "C) `z̃_it = e^(-κ_i h)z_{it}-z_{i, t-h}`",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 505,
    "Question": "### Background\n\nThe paper tests the cross-sectional restrictions of the Gaussian model using a panel of `N` bond yields over `T` time periods. The econometric specification is:\n```latex\ny_t = a + B(κ)z_t + v_t \\quad \\text{(Eq. 1)}\n```\n\n### Data / Model Specification\n\nThe estimation strategy involves two key choices:\n1.  The unobserved factors `z_t` are treated as time-specific fixed effects and concentrated out of the likelihood, rather than being modeled with a parametric time-series process.\n2.  Inference on the structural parameters `κ` is conducted using Driscoll-Kraay standard errors, which are robust to general forms of cross-sectional and temporal dependence in the errors `v_t`.\n\n### Question\n\nWhat is the central rationale for this two-part econometric strategy (fixed-effects estimation + Driscoll-Kraay standard errors)?",
    "Options": {
      "A": "The fixed-effects estimator is consistent, and Driscoll-Kraay errors are used to improve efficiency.",
      "B": "To isolate the test of the cross-sectional restrictions `B(κ)` from time-series assumptions, while acknowledging the resulting estimator for `κ` may be inconsistent but can still be used for testing with robust standard errors.",
      "C": "The fixed-effects estimator is inconsistent, and Driscoll-Kraay errors are used to correct this inconsistency and make the point estimates of `κ` unbiased.",
      "D": "To allow for non-linearities in the factor dynamics, which makes OLS inconsistent but allows for robust inference using the delta method."
    },
    "Answer": "B",
    "pi_justification": "This item assesses the student's understanding of the paper's core econometric strategy. It uses a Reverse-Reasoning approach, asking for the 'why' behind the chosen methods. The distractors target key misconceptions: (A) incorrectly claims consistency, (C) misinterprets the function of robust standard errors (correcting inference, not bias), and (D) introduces an irrelevant concept.",
    "qid": "505",
    "model_evaluation_result": {
      "model_raw_response": "B) To isolated the test of cross-sectional restrictions B(κ)) from time-series assumption",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 506,
    "Question": "### Background\n\nThe paper estimates the structural parameters `κ` of the Gaussian model using an OLS procedure after concentrating out the fixed-effect factors `z_t`. The model is:\n```latex\ny_t = a + B(κ)z_t + v_t \\quad \\text{(Eq. 1)}\n```\n\n### Data / Model Specification\n\nThe first-order conditions for the OLS estimator of `κ`, evaluated at the true value `κ₀`, converge in probability to a term that is proportional to:\n```latex\n\\text{dg}((B'B)⁻¹B'ΨC_⊥) \\quad \\text{(Eq. 2)}\n```\nwhere `Ψ` is the error covariance matrix `E[v_t v_t']`, `C_⊥` is a matrix related to the derivatives of `B(κ)`, and `dg(·)` extracts the main diagonal. Because this term is generally non-zero, the OLS estimator `κ̂` is inconsistent.\n\n### Question\n\nThe inconsistency of the OLS estimator `κ̂` arises because the expression in Eq. (2) is non-zero for a general error covariance matrix `Ψ`. The expression *does* evaluate to zero (implying consistency) if `Ψ` is diagonal, even if it has non-constant diagonal elements (heteroskedasticity). What does this imply is the specific feature of the error structure that causes the inconsistency?",
    "Options": {
      "A": "Heteroskedasticity (i.e., `Var(v_{it})` varies with maturity `i`).",
      "B": "Cross-sectional correlation (i.e., `Cov(v_{it}, v_{jt}) ≠ 0` for `i ≠ j`).",
      "C": "Serial correlation (i.e., `Cov(v_{it}, v_{i,t-1}) ≠ 0`).",
      "D": "Non-normality of the error terms."
    },
    "Answer": "B",
    "pi_justification": "This item tests the student's ability to diagnose the precise source of an econometric problem by interpreting a formal mathematical condition. It uses a Reverse-Reasoning strategy. The distractors are designed to isolate the correct answer from other plausible but incorrect features of the error term: (A) is explicitly ruled out by the prompt, while (C) and (D) are common error properties that are not the source of this specific inconsistency problem.",
    "qid": "506",
    "model_evaluation_result": {
      "model_raw_response": "B) Cross-sectionnal correlation (i,e, Cov(v_{it}, v_{jt} ) ≠ 0 for i ≠ j).",
      "model_answer": "B",
      "error": null
    }
  },
  {
    "ID": 507,
    "Question": "### Background\n\n**Research Question.** How can a specific functional form for the Lorenz curve be used to derive a closed-form expression for the Gini concentration ratio (CR), and how can we then conduct statistical inference on this CR?\n\n**Setting.** The analysis uses a novel parametric Lorenz curve to model the income distribution. The Gini concentration ratio is derived from this functional form, and its statistical uncertainty is assessed using the Delta Method.\n\n**Variables and Parameters.**\n*   `π`: The cumulative proportion of income-receiving units (0 to 1).\n*   `η`: The cumulative proportion of total income (0 to 1).\n*   `β`: A parameter governing the level of inequality (`β ≥ 0`).\n*   `CR`: The Gini concentration ratio.\n*   `hat(β)`: A consistent estimator of `β`.\n\n---\n\n### Data / Model Specification\n\nThe Lorenz curve is given by `η(π) = π * e^(-β(1-π))`. From this, the Gini concentration ratio `CR` can be derived as a function of `β`:\n\n```latex\nCR(\\beta) = 1 - 2\\frac{(\\beta-1)}{\\beta^{2}} - \\frac{2e^{-\\beta}}{\\beta^{2}} \\quad \\text{(Eq. 1)}\n```\n\nTo conduct inference on an estimate `hat(CR) = CR(hat(β))`, the Delta Method is used, which requires the derivative of `CR(β)` with respect to `β`:\n\n```latex\n\\frac{\\partial(CR)}{\\partial\\beta} = \\frac{2}{\\beta^3}[(\\beta-2) + (\\beta+2)e^{-\\beta}] \\quad \\text{(Eq. 2)}\n```\n\nThe variance of the estimated CR is then `Var(hat(CR)) ≈ [∂(CR)/∂β]^2 * Var(hat(β))`. A researcher estimates `hat(β) = 1.443` with a standard error `SE(hat(β)) = 0.013`. They wish to test the null hypothesis that the true CR is 0.35 (`H_0: CR = 0.35`) at the 5% significance level.\n\n(Note: `e^(-1.443) ≈ 0.236`)\n\n---\n\n### The Question\n\nCalculate the Wald test statistic for the null hypothesis `H_0: CR = 0.35` and determine the correct statistical conclusion at the 5% significance level (critical value ≈ ±1.96).",
    "Options": {
      "A": "Wald statistic ≈ -1.0; Fail to reject the null hypothesis.",
      "B": "Wald statistic ≈ 1.0; Fail to reject the null hypothesis.",
      "C": "Wald statistic ≈ -0.69; Fail to reject the null hypothesis.",
      "D": "Wald statistic ≈ -1.0; Reject the null hypothesis."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.5). This item uses the 'Computational Judgment' strategy. It tests the student's ability to execute a multi-step hypothesis test using the Delta Method, a core applied statistical skill. The calculation is complex enough to be discriminating but results in a single correct answer, making it ideal for a choice format. The distractors are designed based on a matrix of common errors: (B) is a Sign Error in the numerator of the Wald statistic; (C) results from a Formula Misuse error in calculating the derivative (a plausible mistake in applying the quotient rule); (D) represents a Conceptual Error, where the student calculates the correct statistic but draws the wrong conclusion about statistical significance.",
    "qid": "507",
    "model_evaluation_result": {
      "model_raw_response": "C) Wald statute ≈ -0,69; Fail zu reject the null Hypothesis.",
      "model_answer": "C",
      "error": null
    }
  },
  {
    "ID": 508,
    "Question": "### Background\n\n**Research Question.** This study investigates whether exporting to one destination (e.g., a regional common market) generates learning effects that subsequently enhance export performance in other, more demanding markets.\n\n**Setting / Institutional Environment.** The analysis focuses on Costa Rican manufactured exports from 1955-1980. A key institutional feature is Costa Rica's entry into the Central American Common Market (CACM) in 1963, which created a protected regional market.\n\n**Variables & Parameters.**\n- `y_t`, `x_t`: Time series of real export earnings for a specific industry from Costa Rica to two different destinations (e.g., `y_t` to the South, `x_t` to the CACM) at time `t`.\n- `L`: The lag operator, such that `L^k x_t = x_{t-k}`.\n- `β(L)`, `δ(L)`: Polynomials in the lag operator with a lag length of five years. For example, `δ(L)x_t = δ_1 x_{t-1} + ... + δ_5 x_{t-5}`.\n- `z_t`: A vector of control variables, including measures of real GDP and export competitiveness.\n- `e_t`: A random error term.\n\n---\n\n### Data / Model Specification\n\nThe study employs a bivariate Granger-causality framework. To test whether exports to destination `x` (e.g., CACM) Granger-cause exports to destination `y` (e.g., South), the following regression is estimated:\n\n```latex\ny_{t}=\\alpha+\\beta(L)y_{t}+\\delta(L)x_{t}+\\phi z_{t}+e_{t} \\quad \\text{(Eq. 1)}\n```\n\nSuppose that after estimating Eq. (1) for exports to the South (`y_t`) and the CACM (`x_t`), the joint F-test for the coefficients in `δ(L)` yields a p-value of 0.03, and the sum of the coefficients is positive.\n\nWhat is the correct economic interpretation of this statistical finding?",
    "Options": {
      "A": "Past export performance in the CACM is a statistically significant predictor of current export performance in the South, which is consistent with the hypothesis that firms use the CACM as a 'stepping-stone' to learn and subsequently enter other markets.",
      "B": "The result proves that exporting to the CACM causally increases exports to the South, ruling out other potential explanations for the growth in South exports.",
      "C": "Firms are substituting away from the less profitable South market to focus on the protected CACM market, causing South exports to decline.",
      "D": "Exports to the CACM and the South are simply correlated because they are both driven by the same underlying factors, such as overall economic growth in Costa Rica."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's ability to correctly translate a statistical result (significant Granger causality) into its corresponding economic interpretation, as intended by the paper. It uses a Scenario Application strategy. The distractors are designed to trap common errors: (B) overstating the claim by confusing Granger causality with true causality (Conceptual Opposite); (C) misinterpreting the sign of the effect (Sign Error); and (D) offering a simple correlation explanation that the Granger test, by conditioning on other factors, is designed to move beyond (True but Irrelevant).",
    "qid": "508",
    "model_evaluation_result": {
      "model_raw_response": "A) Pastexportperformanceinthecacmisteastatisticallysignificantpredictorofcurrentexportperformanceinthesouthwhichisconsistentwiththehypothetsthatafirmsusethecacamasteppingstonetolearnandsubsequentlyentothermarkets.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 509,
    "Question": "### Background\n\n**Research Question.** This study investigates whether exporting to one destination (e.g., a regional common market) generates learning effects that subsequently enhance export performance in other, more demanding markets.\n\n**Setting / Institutional Environment.** The analysis focuses on Costa Rican manufactured exports from 1955-1980. A key institutional feature is Costa Rica's entry into the Central American Common Market (CACM) in 1963, which created a protected regional market.\n\n**Variables & Parameters.**\n- `y_t`, `x_t`: Time series of real export earnings for a specific industry from Costa Rica to two different destinations (e.g., `y_t` to the South, `x_t` to the CACM) at time `t`.\n- `L`: The lag operator, such that `L^k x_t = x_{t-k}`.\n- `β(L)`, `δ(L)`: Polynomials in the lag operator with a lag length of five years. For example, `δ(L)x_t = δ_1 x_{t-1} + ... + δ_5 x_{t-5}`.\n- `z_t`: A vector of control variables, including measures of real GDP and export competitiveness.\n- `e_t`: A random error term.\n\n---\n\n### Data / Model Specification\n\nThe study employs a bivariate Granger-causality framework. To test whether exports to destination `x` (e.g., CACM) Granger-cause exports to destination `y` (e.g., South), the following regression is estimated:\n\n```latex\ny_{t}=\\alpha+\\beta(L)y_{t}+\\delta(L)x_{t}+\\phi z_{t}+e_{t} \\quad \\text{(Eq. 1)}\n```\n\nA critic argues that the test may be misleading. They propose a scenario where an unobserved, industry-specific technological innovation is adopted first by firms to enter the less-demanding CACM market (at times `t-1`, `t-2`, etc.), and the same innovation later allows them to meet the quality standards for other Southern markets (at time `t`). This technological factor is not included in the control vector `z_t`.\n\nIn this scenario, finding that CACM exports Granger-cause South exports would be a spurious result driven by a violation of which key assumption?",
    "Options": {
      "A": "Strict exogeneity, because the regressor (lagged CACM exports) is correlated with the error term via the unobserved technology shock.",
      "B": "The assumption of no perfect multicollinearity between the lagged `x_t` and `y_t` variables.",
      "C": "The assumption that the error term `e_t` is homoskedastic and not serially correlated.",
      "D": "The assumption that the model's lag length of five years is correctly specified to capture the full dynamics of the relationship."
    },
    "Answer": "A",
    "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests a deeper understanding of the econometric methodology by asking students to identify the specific assumption violated by a classic omitted variable bias scenario. It uses a Reverse-Reasoning strategy. The distractors represent common points of confusion in econometrics: (B) multicollinearity is about relationships between regressors, not regressors and the error term (Conceptual: Almost Right); (C) while serial correlation in the error might also be present, the fundamental violation causing the spurious causality is the endogeneity problem described in A (Conceptual: Almost Right); (D) lag length specification is a different type of model error and not the one described in the scenario (Conceptual: True but Irrelevant).",
    "qid": "509",
    "model_evaluation_result": {
      "model_raw_response": "A) Strict Exogeneity. \n\nAnswer: A",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 510,
    "Question": "### Background\n\n**Research Question.** This problem addresses a major open question in social choice theory: whether the possibility of constructing a desirable social welfare function on a given preference domain is independent of the number of people in society. This property is crucial for creating general theories of social choice.\n\n**Setting / Institutional Environment.** The analysis takes place in the paper's general model of mixed alternatives, where social outcomes have both public and private components. The core challenge is to relate the existence of an $n$-person Arrow Social Welfare Function (ASWF) to that of an $(n-1)$-person ASWF.\n\n### Data / Model Specification\n\nA common technique used in prior literature for private-goods-only models to prove independence from group size is **'freezing'**. This involves taking an $n$-person ASWF and constructing an $(n-1)$-person ASWF from it. The procedure is to fix the preferences ($r^*_n$) and private alternative ($x^*_n$) of one individual, say person $n$, and then use the original $n$-person ASWF to make decisions for the remaining $n-1$ individuals.\n\nThis technique is claimed to be inapplicable to the general mixed-alternatives case, where a social alternative is of the form $X = (x_0; x_1, ..., x_n)$.\n\n### Question\n\nIn the mixed-alternatives model, why does the 'freezing' technique—fixing the preferences ($r^*_n$) and private alternative ($x^*_n$) of individual $n$—fail to create a valid $(n-1)$-person ASWF?",
    "Options": {
      "A": "The 'frozen' individual's utility is not truly fixed; it still varies with the public good ($x_0$) chosen by the other $n-1$ individuals, making their preferences relevant to the outcome.",
      "B": "The technique violates the Independence of Irrelevant Alternatives (IIA) for the remaining $n-1$ individuals because the frozen preference $r^*_n$ introduces external information.",
      "C": "The set of available private alternatives for the remaining $n-1$ individuals might depend on the public good $x_0$, which makes the choice set unstable.",
      "D": "The 'frozen' individual can still manipulate the outcome by strategically choosing their fixed preference $r^*_n$ before the procedure begins."
    },
    "Answer": "A",
    "pi_justification": "This item assesses the core conceptual reason for the failure of a key proof technique. It uses a Reverse-Reasoning strategy: given that the technique fails, the user must identify the correct cause. Distractor A is the correct answer, identifying the entanglement caused by the public good. Distractor B offers a plausible but incorrect technical failure (violating IIA). Distractor C identifies a secondary issue that is not the primary point of failure. Distractor D misinterprets the proof technique by assigning strategic choice to the 'frozen' individual.",
    "qid": "510",
    "model_evaluation_result": {
      "model_raw_response": "A) The \"frozen\" individual's utility remains dependent on the public component ($x_o$), making their preferences still relevant to the choice.",
      "model_answer": "A",
      "error": null
    }
  },
  {
    "ID": 511,
    "Question": "### Background\n\n**Research Question.** This problem addresses a major open question in social choice theory: whether the possibility of constructing a desirable social welfare function on a given preference domain is independent of the number of people in society. This property is crucial for creating general theories of social choice.\n\n**Setting / Institutional Environment.** The paper successfully characterizes the preference domains that admit a two-person Arrow Social Welfare Function (ASWF). However, it notes that a full characterization for any number of individuals would require proving the 'independence from group size' property.\n\n### Question\n\nWhat is the primary significance of proving the 'independence from group size' property for a class of preference domains in social choice theory?",
    "Options": {
      "A": "It is a necessary precondition for proving that a domain admits a two-person ASWF in the first place.",
      "B": "It proves that the exact same social welfare function can be used for any group size, ensuring consistency across societies.",
      "C": "It allows a characterization of domains that admit an ASWF for a small, fixed number of people (e.g., two) to be generalized as a valid characterization for any number of people.",
      "D": "It demonstrates that the non-dictatorship condition is robust to changes in the number of voters in the society."
    },
    "Answer": "C",
    "pi_justification": "This item assesses the user's understanding of the methodological role of a key theoretical property. It uses a Conceptual Judgment strategy. Option C is the correct answer, as it correctly identifies the property as a bridge from a specific result (for n=2) to a general one. Distractor A reverses the logical dependency. Distractor B contains a subtle but important error, confusing the existence of *some* ASWF with the existence of the *same* ASWF. Distractor D states a true but secondary consequence, not the primary significance for the research program.",
    "qid": "511",
    "model_evaluation_result": {
      "model_raw_response": "C) ItallowsacharacterizationofdomainsthatadmitanASWFforasmall,fixednumberofpeople(e.g.,two)tobegeneralizedasavalidcharacterizationforanynumberofpeople.",
      "model_answer": "C",
      "error": null
    }
  }
]