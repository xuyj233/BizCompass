{"ID": 8, "Question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), the implied prior correlation between `\\psi` and `\\lambda` is:\n```latex\nCorr(\\psi, \\lambda) = \\frac{1}{\\sqrt{1 + \\tau^2/\\omega^2}} \\quad \\text{(Eq. 1)}\n```\nMarginal learning about the non-identified parameter `\\lambda` is possible if and only if `\\psi` and `\\lambda` are a priori dependent (i.e., their correlation is non-zero).\n\n### Question\n\nWhat is the effect of using an improper prior for `\\beta` (i.e., letting `\\tau^2 \\to \\infty`), and what is the direct consequence for learning about `\\lambda`?", "Options": {"A": "The prior correlation `Corr(\\psi, \\lambda)` goes to 0, which has no effect on learning about `\\lambda` as that is determined by the likelihood.", "B": "The prior correlation `Corr(\\psi, \\lambda)` becomes undefined, which makes the joint posterior for `(\\psi, \\lambda)` improper.", "C": "The prior correlation `Corr(\\psi, \\lambda)` goes to 1, which maximizes marginal learning about `\\lambda`.", "D": "The prior correlation `Corr(\\psi, \\lambda)` goes to 0, which makes `\\psi` and `\\lambda` a priori independent and prevents any marginal learning about `\\lambda`."}, "Answer": "D", "pi_justification": "Assessment Target: Tests the ability to calculate the effect of an improper prior on the key prior dependence and understand its consequence for learning. Strategy: Computational Judgment. The user must evaluate a limit and connect it to the theory. Distractor Logic: A) is the Conceptual Opposite, describing the case where `\\omega^2 \\to \\infty`. B) correctly evaluates the limit but draws an incorrect conclusion about its consequence. C) describes a plausible but incorrect outcome.", "qid": "8", "question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), the implied prior correlation between `\\psi` and `\\lambda` is:\n```latex\nCorr(\\psi, \\lambda) = \\frac{1}{\\sqrt{1 + \\tau^2/\\omega^2}} \\quad \\text{(Eq. 1)}\n```\nMarginal learning about the non-identified parameter `\\lambda` is possible if and only if `\\psi` and `\\lambda` are a priori dependent (i.e., their correlation is non-zero).\n\n### Question\n\nWhat is the effect of using an improper prior for `\\beta` (i.e., letting `\\tau^2 \\to \\infty`), and what is the direct consequence for learning about `\\lambda`?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 9, "Question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), marginal learning about the non-identified parameter `\\lambda` is possible because `\\psi` and `\\lambda` are a priori dependent. We consider two scenarios with improper priors.\n\n### Question\n\nA researcher argues for using an improper prior on `\\beta` (`\\tau^2 \\to \\infty`) to be \"non-informative.\" Which statement best critiques this reasoning and correctly contrasts it with using an improper prior on `\\lambda` (`\\omega^2 \\to \\infty`) instead?", "Options": {"A": "The prior is not non-informative; it dogmatically asserts `\\psi` and `\\lambda` are independent, preventing learning about `\\lambda`. In contrast, an improper prior on `\\lambda` asserts perfect dependence, maximizing learning about `\\lambda`.", "B": "The prior is not non-informative because it prevents learning about `\\lambda`. An improper prior on `\\lambda` would also prevent learning about `\\lambda`, but for different reasons.", "C": "The prior is indeed non-informative, allowing the data to speak freely about both `\\beta` and `\\lambda`. An improper prior on `\\lambda` would have the same effect.", "D": "The prior is not non-informative because it makes the posterior for `\\beta` improper. An improper prior on `\\lambda` is truly non-informative and allows learning about both parameters."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the subtle, paradoxical nature of \"non-informative\" priors in this context. Strategy: Scenario Application. The user must critique one scenario and contrast it with another. Distractor Logic: B) is incorrect on all points. C) has a partially correct critique but an incorrect contrast. D) offers an incorrect critique and an incorrect contrast. The correct answer requires understanding the precise and opposite effects of the two different improper prior specifications.", "qid": "9", "question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), marginal learning about the non-identified parameter `\\lambda` is possible because `\\psi` and `\\lambda` are a priori dependent. We consider two scenarios with improper priors.\n\n### Question\n\nA researcher argues for using an improper prior on `\\beta` (`\\tau^2 \\to \\infty`) to be \"non-informative.\" Which statement best critiques this reasoning and correctly contrasts it with using an improper prior on `\\lambda` (`\\omega^2 \\to \\infty`) instead?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 20, "Question": "### Background\n\n**Research Question.** This problem investigates how violating the \"structural symmetry\" assumption in the Melitz model can lead to omitted variable bias, potentially explaining the puzzle of why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nIf coefficients are heterogeneous across sectors, the theoretical difference-in-differences (DiD) comparison for a unilateral import liberalization between a more-liberalized sector A and a less-liberalized sector B is:\n\n```latex\nd\\ln\\Phi_{1A}-d\\ln\\Phi_{1B}=\\xi_{1B}(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})+(\\xi_{1A}-\\xi_{1B})d\\ln\\tau_{21A}-(\\xi_{3A}-\\xi_{3B})d\\ln w_{1}\n```\n(Eq. 1)\n\n### Question\n\nAn econometrician tries to estimate the effect of import tariff cuts on productivity using a simple DiD regression of `(d\\ln\\Phi_{1A}-d\\ln\\Phi_{1B})` on `(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})`. Assume the direct tariff effect is homogeneous (`\\xi_{1A} = \\xi_{1B} = \\xi_1 > 0`), but sensitivity to wages is not. Consider a scenario where:\n1. Unilateral liberalization causes the general equilibrium wage to fall (`d\\ln w_1 < 0`).\n2. The largest tariff cuts (`d\\ln\\tau_{21A}` is most negative) were given to the most labor-intensive sectors, which are also most sensitive to wage changes (`\\xi_{3A} > \\xi_{3B}`).\n\nUnder this scenario, what is the nature of the omitted variable bias, and what is its implication for the estimated coefficient `\\hat{\\beta}`?", "Options": {"A": "There is no bias because the DiD specification perfectly controls for the wage shock `d\\ln w_1`.", "B": "The bias is positive. This would make the estimated `\\hat{\\beta}` even more positive than the true `\\xi_1`, deepening the puzzle.", "C": "The bias is negative, but this would only make the estimated `\\hat{\\beta}` smaller than the true `\\xi_1` while remaining positive, thus failing to resolve the puzzle.", "D": "The bias is negative. This could cause the estimated `\\hat{\\beta}` to be negative even if the true causal effect `\\xi_1` is positive, potentially resolving the puzzle."}, "Answer": "D", "pi_justification": "This item uses a Scenario Application strategy to test the ability to perform a multi-step omitted variable bias analysis, which was the core of the original QA's third question. The distractors are designed based on predictable errors: B results from a sign error in calculating the covariance term of the bias formula; C shows a misunderstanding of how a large bias can flip the sign of an estimate; D incorrectly applies the DiD logic when its key identifying assumption is explicitly violated.", "qid": "20", "question": "### Background\n\n**Research Question.** This problem investigates how violating the \"structural symmetry\" assumption in the Melitz model can lead to omitted variable bias, potentially explaining the puzzle of why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nIf coefficients are heterogeneous across sectors, the theoretical difference-in-differences (DiD) comparison for a unilateral import liberalization between a more-liberalized sector A and a less-liberalized sector B is:\n\n```latex\nd\\ln\\Phi_{1A}-d\\ln\\Phi_{1B}=\\xi_{1B}(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})+(\\xi_{1A}-\\xi_{1B})d\\ln\\tau_{21A}-(\\xi_{3A}-\\xi_{3B})d\\ln w_{1}\n```\n(Eq. 1)\n\n### Question\n\nAn econometrician tries to estimate the effect of import tariff cuts on productivity using a simple DiD regression of `(d\\ln\\Phi_{1A}-d\\ln\\Phi_{1B})` on `(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})`. Assume the direct tariff effect is homogeneous (`\\xi_{1A} = \\xi_{1B} = \\xi_1 > 0`), but sensitivity to wages is not. Consider a scenario where:\n1. Unilateral liberalization causes the general equilibrium wage to fall (`d\\ln w_1 < 0`).\n2. The largest tariff cuts (`d\\ln\\tau_{21A}` is most negative) were given to the most labor-intensive sectors, which are also most sensitive to wage changes (`\\xi_{3A} > \\xi_{3B}`).\n\nUnder this scenario, what is the nature of the omitted variable bias, and what is its implication for the estimated coefficient `\\hat{\\beta}`?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 2, "Question": "### Background\n\nCommon methods for handling missing data include:\n- **Listwise Deletion:** Discarding any observation with one or more missing values.\n- **Single Imputation:** Filling in each missing value with a single plausible value (e.g., a regression prediction) to create one complete dataset.\n- **Multiple Imputation (MI):** Creating *m* complete datasets by filling in missing values with *m* different plausible draws from a predictive distribution, analyzing each dataset, and then combining the results.\n\nThe primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nListwise deletion is a common but problematic method for handling missing data. Under which specific condition will it produce biased parameter estimates, and why is it always considered statistically inefficient?", "Options": {"A": "It produces bias by imputing incorrect values for missing data, and it is inefficient because it can only be used for linear models.", "B": "It produces bias when the data are Missing at Random (MAR) but not MCAR, and it is always inefficient because it discards information from partially complete cases.", "C": "It is always unbiased as long as the missingness is ignorable (MAR), but it is inefficient because it requires complex calculations.", "D": "It produces bias when the data are MCAR, and it is inefficient only when the data are MAR."}, "Answer": "B", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the two primary statistical failures of listwise deletion: the condition for bias and the reason for inefficiency. The distractors are designed to capture common errors: B reverses the conditions (Conceptual Opposite), C makes an incorrect claim about bias under MAR (Almost Right), and D confuses deletion with imputation (Conceptual Conflation).", "qid": "2", "question": "### Background\n\nCommon methods for handling missing data include:\n- **Listwise Deletion:** Discarding any observation with one or more missing values.\n- **Single Imputation:** Filling in each missing value with a single plausible value (e.g., a regression prediction) to create one complete dataset.\n- **Multiple Imputation (MI):** Creating *m* complete datasets by filling in missing values with *m* different plausible draws from a predictive distribution, analyzing each dataset, and then combining the results.\n\nThe primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nListwise deletion is a common but problematic method for handling missing data. Under which specific condition will it produce biased parameter estimates, and why is it always considered statistically inefficient?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 19, "Question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of testing the Melitz model's predictions, focusing on why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nThe theoretical change in productivity for a sector `s` in country 1 under the Melitz model is:\n\n```latex\nd\\ln\\Phi_{1s}=\\xi_{1s}d\\ln\\tau_{21s}-\\xi_{2s}d\\ln\\tau_{12s}-\\xi_{3s}d\\ln w_{1}\n```\n(Eq. 1)\n\nwhere `\\xi_{1s}` is the response to import costs `\\tau_{21s}`. Trefler's empirical DiD specification for Canadian productivity is:\n\n```latex\n\\Delta\\ln\\Phi_{s,t}^{C A}=\\beta^{C A}\\Delta\\tau_{s,t}^{C A}+\\beta^{U S}\\Delta\\tau_{s,t}^{U S}+\\gamma_{t}+\\text{Controls}_{st}+\\varepsilon_{st}\n```\n(Eq. 2)\n\nwhere `\\Delta\\tau_{s,t}^{C A}` represents the change in Canadian import tariffs. Trefler finds a statistically significant `\\hat{\\beta}^{CA} < 0`.\n\n### Question\n\nBased on the structurally symmetric Melitz model where the theoretical productivity response to import costs `\\xi_{1}` is positive, what does the model predict for the coefficient `\\beta^{CA}` in Trefler's regression, and why does his actual finding present a puzzle?", "Options": {"A": "The model predicts `\\beta^{CA} > 0`, but this is because higher tariffs (`\\Delta\\tau > 0`) protect domestic firms, raising their productivity. Trefler's finding of `\\hat{\\beta}^{CA} < 0` is a puzzle because it suggests protection hurts productivity.", "B": "The model predicts `\\beta^{CA}` will be biased due to the wage term `d\\ln w_{1}`. The puzzle is that Trefler's time fixed effects did not adequately control for this general equilibrium shock.", "C": "The model predicts `\\beta^{CA} > 0` because productivity and import costs move in the same direction. Trefler's finding of `\\hat{\\beta}^{CA} < 0` is a puzzle because it implies productivity rose most where import tariffs were cut the most, the opposite of the prediction.", "D": "The model predicts `\\beta^{CA} < 0` because lower tariffs (`\\Delta\\tau < 0`) increase competition and productivity. Trefler's finding confirms this prediction, so there is no puzzle."}, "Answer": "C", "pi_justification": "This item assesses the student's ability to connect a theoretical parameter to its empirical counterpart in a regression context, which was the core of the original QA's first question. It uses a Reverse-Reasoning strategy. Distractor B represents the common but incorrect intuition that the paper refutes. Distractor C conflates the central puzzle with one of its potential explanations (omitted variables). Distractor D correctly identifies the predicted sign of the coefficient but provides an incorrect explanation for the underlying theoretical mechanism.", "qid": "19", "question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of testing the Melitz model's predictions, focusing on why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nThe theoretical change in productivity for a sector `s` in country 1 under the Melitz model is:\n\n```latex\nd\\ln\\Phi_{1s}=\\xi_{1s}d\\ln\\tau_{21s}-\\xi_{2s}d\\ln\\tau_{12s}-\\xi_{3s}d\\ln w_{1}\n```\n(Eq. 1)\n\nwhere `\\xi_{1s}` is the response to import costs `\\tau_{21s}`. Trefler's empirical DiD specification for Canadian productivity is:\n\n```latex\n\\Delta\\ln\\Phi_{s,t}^{C A}=\\beta^{C A}\\Delta\\tau_{s,t}^{C A}+\\beta^{U S}\\Delta\\tau_{s,t}^{U S}+\\gamma_{t}+\\text{Controls}_{st}+\\varepsilon_{st}\n```\n(Eq. 2)\n\nwhere `\\Delta\\tau_{s,t}^{C A}` represents the change in Canadian import tariffs. Trefler finds a statistically significant `\\hat{\\beta}^{CA} < 0`.\n\n### Question\n\nBased on the structurally symmetric Melitz model where the theoretical productivity response to import costs `\\xi_{1}` is positive, what does the model predict for the coefficient `\\beta^{CA}` in Trefler's regression, and why does his actual finding present a puzzle?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 16, "Question": "### Background\n\nAn analysis of Indian manufacturing firms produced estimates for an extended Cobb-Douglas production function. The goal is to understand the productivity effects of a firm's own R&D (`LNRND`) and industry-wide R&D spillovers (`LNSRDEXP`).\n\n### Data / Model Specification\n\nThe core log-linear production function is:\n```latex\n\\hat{y}_{i j t}=\\hat{a}_{i j t}+\\alpha\\hat{s}_{j t}+\\beta\\hat{r}_{i j t}+\\gamma\\hat{k}_{i j t}+\\delta\\hat{l}_{i j t}+\\hat{e}_{i j t}\n```\nwhere `y` is value-added, `s` is spillover R&D, `r` is own R&D, `k` is physical capital, and `l` is labor, all in logs.\n\nTwo models were estimated for the \"Overall Industry\": a standard OLS model (Table 1) and a model correcting for first-order serial correlation using a Cochrane-Orcutt transformation (Table 2).\n\n**Table 1: OLS Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.016 (3.80) |\n| LNSRDEXP ($\\alpha$) | 0.093 (8.01) |\n| DW | 0.794 |\n*t-statistics in parentheses.*\n\n**Table 2: Cochrane-Orcutt Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.009 (1.49) |\n| LNSRDEXP ($\\alpha$) | 0.055 (2.58) |\n| DW | 2.170 |\n*t-statistics in parentheses.*\n\n### Question\n\nComparing the results for own R&D (`LNRND`) in Table 1 and Table 2, what is the most accurate econometric explanation for the change in the coefficient's statistical significance?", "Options": {"A": "The change in significance is due to correcting for unobserved firm-specific heterogeneity, which was confounding the OLS results.", "B": "The OLS model in Table 1 was correctly specified, and the Cochrane-Orcutt procedure in Table 2 incorrectly reduced the coefficient's significance.", "C": "The OLS model in Table 1 suffered from negative serial correlation (DW < 2), which biased the t-statistic for `LNRND` downward. The correction fixed this.", "D": "The OLS model in Table 1 suffered from positive serial correlation, which biased the coefficient estimate for `LNRND` upward. The correction in Table 2 removed this bias, revealing a smaller, insignificant effect."}, "Answer": "D", "pi_justification": "This item replaces part of a larger QA problem. It uses a Reverse-Reasoning strategy to assess a core concept: diagnosing and understanding the consequences of serial correlation. The correct answer (C) requires interpreting the Durbin-Watson statistic (0.794 indicates positive serial correlation) and knowing that this biases coefficients and standard errors, leading to spurious significance. Distractor A is a conceptual opposite. Distractor B correctly identifies the DW value but incorrectly states the direction of bias. Distractor D correctly identifies a potential problem (heterogeneity) but attributes the change to the wrong correction, testing if the student can distinguish between serial correlation and heterogeneity corrections.", "qid": "16", "question": "### Background\n\nAn analysis of Indian manufacturing firms produced estimates for an extended Cobb-Douglas production function. The goal is to understand the productivity effects of a firm's own R&D (`LNRND`) and industry-wide R&D spillovers (`LNSRDEXP`).\n\n### Data / Model Specification\n\nThe core log-linear production function is:\n```latex\n\\hat{y}_{i j t}=\\hat{a}_{i j t}+\\alpha\\hat{s}_{j t}+\\beta\\hat{r}_{i j t}+\\gamma\\hat{k}_{i j t}+\\delta\\hat{l}_{i j t}+\\hat{e}_{i j t}\n```\nwhere `y` is value-added, `s` is spillover R&D, `r` is own R&D, `k` is physical capital, and `l` is labor, all in logs.\n\nTwo models were estimated for the \"Overall Industry\": a standard OLS model (Table 1) and a model correcting for first-order serial correlation using a Cochrane-Orcutt transformation (Table 2).\n\n**Table 1: OLS Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.016 (3.80) |\n| LNSRDEXP ($\\alpha$) | 0.093 (8.01) |\n| DW | 0.794 |\n*t-statistics in parentheses.*\n\n**Table 2: Cochrane-Orcutt Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.009 (1.49) |\n| LNSRDEXP ($\\alpha$) | 0.055 (2.58) |\n| DW | 2.170 |\n*t-statistics in parentheses.*\n\n### Question\n\nComparing the results for own R&D (`LNRND`) in Table 1 and Table 2, what is the most accurate econometric explanation for the change in the coefficient's statistical significance?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 4, "Question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant. According to the paper's theory, the data are marginally uninformative about `\\nu = F(\\lambda|\\psi)`, where `F` is the CDF corresponding to the prior in Eq. (2).\n\n### Question\n\nWhich of the following quantities is a one-to-one function of `\\nu`, implying that all identifying information for it comes entirely from the prior specification?", "Options": {"A": "The instrument's noncentrality parameter, `\\pi_2^2 x'x / \\omega_{22}`.", "B": "The structural parameter of interest, `\\lambda`.", "C": "The reduced form coefficient on the instrument, `\\pi_2`.", "D": "The structural coefficient on the instrument, `\\pi_1 - \\lambda\\pi_2`."}, "Answer": "D", "pi_justification": "Assessment Target: Tests the ability to identify the specific parametric function for which the data are uninformative in the simultaneous equations model. Strategy: Reverse-Reasoning. The user must derive the transformation from `\\nu` to the options. Distractor Logic: A) is a directly identified parameter (Conceptual Opposite). C) is the ultimate parameter of interest, but not the direct transformation of `\\nu` (Almost Right). D) is a component of the prior variance, not the uninformative quantity itself (True but Irrelevant).", "qid": "4", "question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant. According to the paper's theory, the data are marginally uninformative about `\\nu = F(\\lambda|\\psi)`, where `F` is the CDF corresponding to the prior in Eq. (2).\n\n### Question\n\nWhich of the following quantities is a one-to-one function of `\\nu`, implying that all identifying information for it comes entirely from the prior specification?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 18, "Question": "### Background\n\nThe final preferred model for the **Light industry** was a Fixed Effect specification. In this industry, private sector R&D accounts for 80% of total industrial R&D expenditure.\n\n### Data / Model Specification\n\n**Table 1: Final Selected Model Estimates for the Light Industry**\n| Variables | Light Industry (Fixed Effect) |\n|:---|---:|\n| LNRND ($\\beta$) | 0.012 (1.16) |\n| LNSRDEXP ($\\alpha$) | 0.357 (5.34) |\n*t-statistics in parentheses.*\n\n### Question\n\nSuppose a policy induces a 10% increase in the R&D expenditures of *all private firms* in the Light industry. Based on the statistically significant effects in the model, what is the estimated total percentage increase in a representative firm's output?", "Options": {"A": "3.57%", "B": "2.86%", "C": "0.12%", "D": "2.98%"}, "Answer": "B", "pi_justification": "This item replaces the most complex part of a larger QA problem, using a Computational Judgment strategy. It tests the ability to perform a multi-step policy counterfactual calculation. The correct answer (B) requires: (1) calculating the increase in the spillover stock (10% * 0.80 = 8%), (2) applying the spillover elasticity (0.357 * 8% = 2.856%), and (3) correctly ignoring the own-R&D effect because its coefficient is statistically insignificant (t=1.16). Distractor A is a Step-Omission Error (only calculates the private return and ignores significance). Distractor C is a Conceptual Error (adds the insignificant private return to the social return). Distractor D is a Calculation Error (forgets to weight the spillover increase by the 80% private share).", "qid": "18", "question": "### Background\n\nThe final preferred model for the **Light industry** was a Fixed Effect specification. In this industry, private sector R&D accounts for 80% of total industrial R&D expenditure.\n\n### Data / Model Specification\n\n**Table 1: Final Selected Model Estimates for the Light Industry**\n| Variables | Light Industry (Fixed Effect) |\n|:---|---:|\n| LNRND ($\\beta$) | 0.012 (1.16) |\n| LNSRDEXP ($\\alpha$) | 0.357 (5.34) |\n*t-statistics in parentheses.*\n\n### Question\n\nSuppose a policy induces a 10% increase in the R&D expenditures of *all private firms* in the Light industry. Based on the statistically significant effects in the model, what is the estimated total percentage increase in a representative firm's output?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 13, "Question": "### Background\n\nIn the model, the identity of the decisive median voter can change randomly from period to period due to shocks (`ε`) affecting voter participation or eligibility. The median voter's preference, `α^m`, is defined implicitly as the value that splits the voting population in half:\n\n```latex\n\\int_{0}^{\\alpha^{m}(\\varepsilon)} \\gamma(\\alpha, \\varepsilon) d\\alpha - \\frac{1}{2} = 0 \\quad \\text{(Eq. 1)}\n```\n\nwhere `γ(α, ε)` is the probability density function of preferences. The sensitivity of the median voter to a shock is given by:\n\n```latex\n\\frac{d\\alpha^m}{d\\varepsilon} = - \\frac{\\int_{0}^{\\alpha^{m}} \\gamma_{\\varepsilon}(\\alpha, \\varepsilon) d\\alpha}{\\gamma(\\alpha^m, \\varepsilon)} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAccording to the model, a political system is considered more unstable if the identity of the median voter (`α^m`) is highly sensitive to random shocks (`ε`). Given the formula for this sensitivity (Eq. 2), which characteristic of the voter preference distribution `γ(α)` would lead to the greatest political instability?", "Options": {"A": "A low density of voters at the median, `γ(α^m, ε)`.", "B": "A distribution where the total number of voters is very large.", "C": "A high density of voters at the median, `γ(α^m, ε)`.", "D": "A distribution that is unimodal and symmetric around `α = 0.5`."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to assess a key testable implication of the model: the link between preference distribution and political instability. It replaces a QA sub-question because it targets a specific, sharp insight perfectly suited for a choice format. The distractors are designed based on a clear error matrix: Conceptual Opposite (A: high density implies stability), True but Irrelevant/Distracting (C: a unimodal distribution implies high median density and thus stability), and Irrelevant (D).", "qid": "13", "question": "### Background\n\nIn the model, the identity of the decisive median voter can change randomly from period to period due to shocks (`ε`) affecting voter participation or eligibility. The median voter's preference, `α^m`, is defined implicitly as the value that splits the voting population in half:\n\n```latex\n\\int_{0}^{\\alpha^{m}(\\varepsilon)} \\gamma(\\alpha, \\varepsilon) d\\alpha - \\frac{1}{2} = 0 \\quad \\text{(Eq. 1)}\n```\n\nwhere `γ(α, ε)` is the probability density function of preferences. The sensitivity of the median voter to a shock is given by:\n\n```latex\n\\frac{d\\alpha^m}{d\\varepsilon} = - \\frac{\\int_{0}^{\\alpha^{m}} \\gamma_{\\varepsilon}(\\alpha, \\varepsilon) d\\alpha}{\\gamma(\\alpha^m, \\varepsilon)} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAccording to the model, a political system is considered more unstable if the identity of the median voter (`α^m`) is highly sensitive to random shocks (`ε`). Given the formula for this sensitivity (Eq. 2), which characteristic of the voter preference distribution `γ(α)` would lead to the greatest political instability?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 15, "Question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n4.  **Non-negativity:** `f_i(c, E) ≥ 0` for all `i`.\n\n### Question\n\nWhich set of axioms, when imposed on the General Proportional Rule (GPR), is sufficient to characterize the class of Proportional Rules, where the rule takes the form `f_i(c, E) = \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E` with `\\sum_{k \\in K} W_k(\\bar{c}, E) = 1`?", "Options": {"A": "Efficiency and Non-negativity.", "B": "Uniform Treatment of Uniforms and Efficiency.", "C": "No Award for Null and Efficiency.", "D": "No Award for Null and Non-negativity."}, "Answer": "C", "pi_justification": "This item assesses the axiomatic derivation of the Proportional Rule from the GPR, a task in the original QA problem. It uses a Reverse-Reasoning strategy, asking which set of axioms produces a known result. The correct option identifies the precise pair of axioms that force the two key constraints (`A_i=0` from NAN, and `∑W_k=1` from Efficiency). Distractors are other plausible combinations of axioms from the paper that lead to different or incomplete characterizations.", "qid": "15", "question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n4.  **Non-negativity:** `f_i(c, E) ≥ 0` for all `i`.\n\n### Question\n\nWhich set of axioms, when imposed on the General Proportional Rule (GPR), is sufficient to characterize the class of Proportional Rules, where the rule takes the form `f_i(c, E) = \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E` with `\\sum_{k \\in K} W_k(\\bar{c}, E) = 1`?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 17, "Question": "### Background\n\nTo account for unobserved firm-specific heterogeneity (e.g., managerial skill), Fixed Effect (FE) and Random Effect (RE) models were estimated for a panel of Indian manufacturing firms. A Hausman test was conducted to choose between the two specifications.\n\n### Data / Model Specification\n\nThe table below presents FE and RE estimates for the \"Overall Industry\" after correcting for serial correlation. The Hausman test yielded a statistic of 53.19.\n\n**Table 1: Fixed vs. Random Effect Models**\n| Variables | Fixed Effect | Random Effect |\n|:---|---:|---:|\n| LNRND ($\\beta$) | 0.007 (1.54) | 0.010 (2.07) |\n| LNSRDEXP ($\\alpha$) | 0.101 (3.34) | 0.122 (5.64) |\n*t-statistics in parentheses.*\n\n### Question\n\nGiven the Hausman test statistic of 53.19 (which is highly significant), which statement provides the correct conclusion and justification?", "Options": {"A": "The Fixed Effect model is preferred because the test rejects the null hypothesis that unobserved effects are uncorrelated with the regressors, implying the Random Effect model would be biased.", "B": "The Fixed Effect model is preferred because the coefficients are smaller, indicating they are more conservative and reliable.", "C": "The Random Effect model is preferred because the test rejects the null hypothesis that the Fixed Effect model is consistent.", "D": "The Random Effect model is preferred because it is more efficient and the test fails to reject its validity."}, "Answer": "A", "pi_justification": "This item replaces part of a larger QA problem and assesses the student's ability to apply a standard specification test. The strategy is Conceptual Judgment. The correct answer (B) requires knowing the null hypothesis of the Hausman test (no correlation between unobserved effects and regressors) and the implication of its rejection (RE is inconsistent, so FE is preferred). Distractor A correctly states that RE is more efficient but ignores the test result, making it 'True but Irrelevant'. Distractor C misstates the null hypothesis and conclusion. Distractor D offers a folk-theory justification ('smaller is better') that is not the basis for the statistical decision.", "qid": "17", "question": "### Background\n\nTo account for unobserved firm-specific heterogeneity (e.g., managerial skill), Fixed Effect (FE) and Random Effect (RE) models were estimated for a panel of Indian manufacturing firms. A Hausman test was conducted to choose between the two specifications.\n\n### Data / Model Specification\n\nThe table below presents FE and RE estimates for the \"Overall Industry\" after correcting for serial correlation. The Hausman test yielded a statistic of 53.19.\n\n**Table 1: Fixed vs. Random Effect Models**\n| Variables | Fixed Effect | Random Effect |\n|:---|---:|---:|\n| LNRND ($\\beta$) | 0.007 (1.54) | 0.010 (2.07) |\n| LNSRDEXP ($\\alpha$) | 0.101 (3.34) | 0.122 (5.64) |\n*t-statistics in parentheses.*\n\n### Question\n\nGiven the Hausman test statistic of 53.19 (which is highly significant), which statement provides the correct conclusion and justification?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 10, "Question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, what specific linear combination of the regression coefficients are the data marginally uninformative about?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` has rank `K-1` because `X_2 = X_1d`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe identified parameter is `\\psi = \\beta_1 + d\\beta_2 = [I_{K-1}, d] \\beta`. The data are marginally uninformative for any parameter that is a priori independent of `\\psi`. The specific uninformative quantity is `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c = [d', -1]'`. Independence between `\\psi` and `\\alpha` is established by showing their prior covariance is zero.\n\n### Question\n\nWhich calculation correctly demonstrates the a priori independence of `\\psi` and `\\alpha`?", "Options": {"A": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot \\underline{Q}^{-1} \\cdot (c'\\underline{Q}^{-1})' = [I_{K-1}, d] \\cdot \\underline{Q}^{-1} \\cdot \\underline{Q}^{-1}c \\neq 0`", "B": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot (c'\\underline{Q}^{-1})' = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot \\underline{Q}^{-1}c = [I_{K-1}, d]c = d-d=0`", "C": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot (\\underline{Q}^{-1}c)' = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot c'\\underline{Q}^{-1} \\neq 0`", "D": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot c = d-d=0`, because the `\\underline{Q}` matrices are irrelevant."}, "Answer": "B", "pi_justification": "Assessment Target: Tests the core mathematical derivation for the uninformative quantity in the multicollinearity case. Strategy: Computational Judgment. The user must identify the correct algebraic steps. Distractor Logic: A) makes a mistake with the transpose rule for matrices. C) uses `Q⁻¹` instead of `Q` in the covariance formula (Formula Misuse). D) arrives at the right answer but omits the crucial `Q Q⁻¹ = I` cancellation, which is the heart of the proof (Step-Omission Error).", "qid": "10", "question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, what specific linear combination of the regression coefficients are the data marginally uninformative about?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` has rank `K-1` because `X_2 = X_1d`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe identified parameter is `\\psi = \\beta_1 + d\\beta_2 = [I_{K-1}, d] \\beta`. The data are marginally uninformative for any parameter that is a priori independent of `\\psi`. The specific uninformative quantity is `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c = [d', -1]'`. Independence between `\\psi` and `\\alpha` is established by showing their prior covariance is zero.\n\n### Question\n\nWhich calculation correctly demonstrates the a priori independence of `\\psi` and `\\alpha`?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 7, "Question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`.\n\n### Question\n\nAs censoring disappears (the probability of observation `\\psi_1 \\to 1`), the model becomes point-identified. In this limit, the posterior probability that `\\lambda` falls within the estimated set `\\Lambda(\\hat{\\psi})` approaches zero. Why does a 95% Bayesian Highest Posterior Density (HPD) interval not suffer from this issue?", "Options": {"A": "Because the Bayesian HPD interval always converges to the full parameter space, `[0,1]`, as more data is collected.", "B": "Because the Bayesian HPD interval's width is fixed by the prior and does not shrink as the data become more informative.", "C": "Because the HPD interval is constructed to contain 95% of the posterior mass by definition, while `\\Lambda(\\hat{\\psi})` is an estimate of a set whose width shrinks to zero.", "D": "Because the Bayesian analysis uses a prior that prevents the probability `\\psi_1` from ever reaching 1."}, "Answer": "C", "pi_justification": "Assessment Target: Tests understanding of the fundamental difference between a Bayesian credible interval and a frequentist set estimate in the limit of point-identification. Strategy: Reverse-Reasoning. The user must explain why one method avoids a paradox. Distractor Logic: B) is a Conceptual Opposite; HPD intervals shrink with more information. C) is also a Conceptual Opposite; they converge to a point, not the whole space. D) describes a possible prior choice but is not the fundamental reason the HPD interval behaves correctly.", "qid": "7", "question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`.\n\n### Question\n\nAs censoring disappears (the probability of observation `\\psi_1 \\to 1`), the model becomes point-identified. In this limit, the posterior probability that `\\lambda` falls within the estimated set `\\Lambda(\\hat{\\psi})` approaches zero. Why does a 95% Bayesian Highest Posterior Density (HPD) interval not suffer from this issue?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 3, "Question": "### Background\n\nIn Multiple Imputation (MI), results from *m* analyses of imputed datasets are combined. The total variance ($T$) of the final point estimate is calculated to account for two sources of uncertainty:\n\n1.  **Within-Imputation Variance ($\\overline{U}$):** The average of the estimated variances from each of the *m* analyses. This captures standard **sampling variance**.\n2.  **Between-Imputation Variance ($B$):** The variance of the *m* point estimates across the datasets. This captures **imputation uncertainty**.\n\nThe total variance is given by:\n```latex\nT = \\overline{U} + \\left(1 + \\frac{1}{m}\\right)B \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nSingle imputation creates a complete dataset but yields invalid confidence intervals because it ignores imputation uncertainty. How does Multiple Imputation (MI), using the variance formula provided in Eq. (1), solve this specific problem?", "Options": {"A": "By using the within-imputation variance ($\\overline{U}$) to measure the uncertainty from missing data, while the between-imputation variance ($B$) captures sampling error.", "B": "By creating multiple datasets, which increases the overall sample size and statistical power compared to listwise deletion.", "C": "By using the between-imputation variance ($B$) to explicitly quantify the uncertainty introduced by the missing data, and adding it to the sampling variance ($\\overline{U}$) to calculate the total variance ($T$).", "D": "By averaging the point estimates across multiple datasets, which produces a more accurate point estimate than any single imputation."}, "Answer": "C", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the core inferential advantage of Multiple Imputation over Single Imputation. It uses a Reverse-Reasoning strategy, asking how MI's formal machinery solves a known problem. The distractors are designed based on common errors: B reverses the roles of the variance components (Formula Misuse), C states a true but tangential benefit of MI (Almost Right), and D confuses the benefits of MI over listwise deletion with its specific advantage over single imputation.", "qid": "3", "question": "### Background\n\nIn Multiple Imputation (MI), results from *m* analyses of imputed datasets are combined. The total variance ($T$) of the final point estimate is calculated to account for two sources of uncertainty:\n\n1.  **Within-Imputation Variance ($\\overline{U}$):** The average of the estimated variances from each of the *m* analyses. This captures standard **sampling variance**.\n2.  **Between-Imputation Variance ($B$):** The variance of the *m* point estimates across the datasets. This captures **imputation uncertainty**.\n\nThe total variance is given by:\n```latex\nT = \\overline{U} + \\left(1 + \\frac{1}{m}\\right)B \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nSingle imputation creates a complete dataset but yields invalid confidence intervals because it ignores imputation uncertainty. How does Multiple Imputation (MI), using the variance formula provided in Eq. (1), solve this specific problem?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 11, "Question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, how can a researcher's beliefs about what the data cannot inform be used to specify a prior distribution?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` and `X_2 = X_1`, so `\\beta` is a 2x1 vector. The identified parameter is `\\psi = \\beta_1 + \\beta_2`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe general theory states that the data are marginally uninformative about `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c` describes the collinearity. In this case, `d=1`, so `c = [1, -1]'`.\n\n### Question\n\nA researcher believes, based on economic theory, that this particular experiment will be completely uninformative about the *difference* in coefficients, `\\beta_1 - \\beta_2`. To align their prior `\\beta ~ N(\\underline{b}, \\underline{Q})` with this belief, what restriction must be placed on the prior covariance matrix `\\underline{Q} = \\begin{pmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix}`?", "Options": {"A": "The prior correlation between `\\beta_1` and `\\beta_2` must be negative one (`\\rho=-1`).", "B": "The prior mean of `\\beta_1` must equal the prior mean of `\\beta_2`.", "C": "The prior variances of `\\beta_1` and `\\beta_2` must be equal (`\\sigma_1^2 = \\sigma_2^2`).", "D": "The prior correlation between `\\beta_1` and `\\beta_2` must be zero (`\\rho=0`)."}, "Answer": "C", "pi_justification": "Assessment Target: Tests the application of the general theory to a specific prior elicitation problem. Strategy: Scenario Application. The user must translate a high-level belief into a concrete mathematical restriction. Distractor Logic: A) and C) represent common but incorrect intuitions about the required prior structure. The belief `\\alpha \\propto \\beta_1 - \\beta_2` requires `c'Q⁻¹ \\propto [1, -1]`, which leads to `q¹¹=q²²` in the precision matrix, implying `σ₁²=σ₂²` in the covariance matrix. A common error is to assume `ρ=0` is the required condition.", "qid": "11", "question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, how can a researcher's beliefs about what the data cannot inform be used to specify a prior distribution?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` and `X_2 = X_1`, so `\\beta` is a 2x1 vector. The identified parameter is `\\psi = \\beta_1 + \\beta_2`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe general theory states that the data are marginally uninformative about `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c` describes the collinearity. In this case, `d=1`, so `c = [1, -1]'`.\n\n### Question\n\nA researcher believes, based on economic theory, that this particular experiment will be completely uninformative about the *difference* in coefficients, `\\beta_1 - \\beta_2`. To align their prior `\\beta ~ N(\\underline{b}, \\underline{Q})` with this belief, what restriction must be placed on the prior covariance matrix `\\underline{Q} = \\begin{pmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix}`?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 6, "Question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`. This posterior is constructed by averaging a conditional prior `f(\\lambda|\\psi)` over the posterior of the identified parameters, `f(\\psi|y)`.\n\n### Question\n\nWhat is a key advantage of the Bayesian posterior `f(\\lambda|y)` that is absent from the simple reporting of the estimated set `\\Lambda(\\hat{\\psi})`?", "Options": {"A": "The Bayesian posterior is always uniform over the bounds `\\Lambda(\\hat{\\psi})`, providing a clear non-informative benchmark.", "B": "The Bayesian posterior coherently integrates the statistical uncertainty about the identified parameters `\\psi` and provides a probabilistic weighting of values for `\\lambda`.", "C": "The Bayesian posterior's support is always narrower than the bounds `\\Lambda(\\hat{\\psi})`, providing more precise inference.", "D": "The Bayesian posterior provides a single point estimate for `\\lambda`, completely resolving the identification problem."}, "Answer": "B", "pi_justification": "Assessment Target: Tests the central conceptual difference between a Bayesian posterior and a frequentist estimated set in a partially identified model. Strategy: Conceptual Judgment. Distractor Logic: A) is a Conceptual Opposite; the Bayesian approach does not resolve the identification problem. B) is a common misconception; the posterior is generally not uniform. D) is incorrect; the marginal posterior support is typically the entire parameter space (e.g., [0,1]), which is wider than the conditional bounds.", "qid": "6", "question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`. This posterior is constructed by averaging a conditional prior `f(\\lambda|\\psi)` over the posterior of the identified parameters, `f(\\psi|y)`.\n\n### Question\n\nWhat is a key advantage of the Bayesian posterior `f(\\lambda|y)` that is absent from the simple reporting of the estimated set `\\Lambda(\\hat{\\psi})`?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 14, "Question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`. For GPRs, this is equivalent to `A_1(\\bar{c}, E) = ... = A_{|N|}(\\bar{c}, E)`. Let this common value be `A(\\bar{c}, E)`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n\n### Question\n\nA Generalized Proportional Rule (GPR) is refined by imposing two axioms: Efficiency (`∑_{i∈N} f_i(c, E) = E`) and Uniform Treatment of Uniforms (UTU), which implies the non-proportional component `A_i` is the same for all entities (`A_i(\\bar{c}, E) = A(\\bar{c}, E)`). Which of the following formulas correctly represents the resulting division rule `f_i(c, E)`?", "Options": {"A": "```latex\nf_i(c, E) = \\frac{E}{|N|} + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} (W_k(\\bar{c}, E) - 1) E\n```", "B": "```latex\nf_i(c, E) = \\frac{E}{|N|}(1 - \\sum_{k \\in K} W_k(\\bar{c}, E)) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E\n```", "C": "```latex\nf_i(c, E) = (E - \\sum_{k \\in K} W_k(\\bar{c}, E) E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E\n```", "D": "```latex\nf_i(c, E) = \\frac{E}{|N|}(1 - \\sum_{k \\in K} W_k(\\bar{c}, E)) + \\sum_{k \\in K} c_{ik} W_k(\\bar{c}, E) E\n```"}, "Answer": "B", "pi_justification": "This item assesses the ability to perform the multi-step algebraic derivation from Corollary 2, a core task in the original QA problem. It uses a Computational Judgment strategy. The correct option is the unique functional form that results from correctly applying the Efficiency and UTU axioms to the GPR. Distractors are derived from plausible algebraic errors in the derivation, such as mishandling the summation over `|N|` entities (B), incorrectly solving the efficiency constraint (C), or omitting terms from the proportional component (D).", "qid": "14", "question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`. For GPRs, this is equivalent to `A_1(\\bar{c}, E) = ... = A_{|N|}(\\bar{c}, E)`. Let this common value be `A(\\bar{c}, E)`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n\n### Question\n\nA Generalized Proportional Rule (GPR) is refined by imposing two axioms: Efficiency (`∑_{i∈N} f_i(c, E) = E`) and Uniform Treatment of Uniforms (UTU), which implies the non-proportional component `A_i` is the same for all entities (`A_i(\\bar{c}, E) = A(\\bar{c}, E)`). Which of the following formulas correctly represents the resulting division rule `f_i(c, E)`?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 1, "Question": "### Background\n\nA researcher is faced with a dataset where some observations are incomplete. The validity of different statistical methods depends on the assumed mechanism generating the missing data. The primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nUnder which of the following scenarios does the Missing at Random (MAR) assumption hold, but the more restrictive Missing Completely at Random (MCAR) assumption is violated?", "Options": {"A": "In a survey of income and education, individuals with lower reported education levels are more likely to refuse to answer the income question.", "B": "A researcher accidentally spills coffee on a random subset of survey forms, making the income data on them illegible.", "C": "In a survey of income, the distribution of reported income is highly skewed to the right.", "D": "In a survey of income, individuals with extremely high incomes are the most likely to refuse to report their income."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the crucial distinction between MCAR and MAR assumptions. It uses a Scenario Application strategy, requiring the test-taker to apply abstract definitions to concrete examples. The distractors are designed to probe for common confusions: B represents a pure MCAR case, C represents the more complex MNAR case (a key conceptual neighbor), and D is a true but irrelevant statement about data distribution.", "qid": "1", "question": "### Background\n\nA researcher is faced with a dataset where some observations are incomplete. The validity of different statistical methods depends on the assumed mechanism generating the missing data. The primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nUnder which of the following scenarios does the Missing at Random (MAR) assumption hold, but the more restrictive Missing Completely at Random (MCAR) assumption is violated?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 5, "Question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant.\n\n### Question\n\nThe prior variance in Eq. (2) is inversely proportional to the instrument's noncentrality parameter, `\\pi_2^2 x'x / \\omega_{22}`. What is the primary justification for this choice, and what does it imply if a researcher observes that the instrument `x_t` is very weak (i.e., `\\pi_2` is close to zero)?", "Options": {"A": "Justification: Prior uncertainty about the exclusion restriction should decrease as the instrument becomes stronger. Implication: A weak instrument implies the prior on `\\lambda` should be very diffuse (high variance).", "B": "Justification: Prior uncertainty about the exclusion restriction should increase as the instrument becomes stronger. Implication: A weak instrument implies the prior on `\\lambda` should be very concentrated (low variance).", "C": "Justification: Computational convenience in the Gibbs sampler. Implication: A weak instrument implies the posterior for `\\lambda` will be improper.", "D": "Justification: To ensure the posterior distribution is proper. Implication: A weak instrument implies the prior on `\\lambda` must be very concentrated (low variance)."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the deep logic behind the prior specification, linking instrument strength to prior uncertainty. Strategy: Scenario Application. The user must apply the logic to the scenario of a weak instrument. Distractor Logic: B) has an incorrect justification and a conceptually opposite implication. C) has a conceptually opposite justification and implication. D) offers a plausible but incorrect justification (computational convenience) and an incorrect implication.", "qid": "5", "question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant.\n\n### Question\n\nThe prior variance in Eq. (2) is inversely proportional to the instrument's noncentrality parameter, `\\pi_2^2 x'x / \\omega_{22}`. What is the primary justification for this choice, and what does it imply if a researcher observes that the instrument `x_t` is very weak (i.e., `\\pi_2` is close to zero)?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 12, "Question": "### Background\n\nIn the model, a group of heterogeneous individuals makes decisions via majority rule. The preferences of the `i`-th member of the group over two public goods, `g` and `f`, are given by:\n\n```latex\nW^{i} \\equiv E\\left\\{ \\sum_{t=1}^{2} \\left[ \\alpha^{i}u(g_{t}) + (1-\\alpha^{i})u(f_{t}) \\right] \\right\\} \\quad \\text{(Eq. 1)}\n```\n\nwhere `α^i` is the individual's preference parameter. This preference structure is crucial for the model's tractability.\n\n### Question\n\nIn this model, what is the primary role of the parameter `α^i` and which resulting property of the preference structure is crucial for applying the median voter theorem?\n", "Options": {"A": "It acts as a utility weight defining heterogeneous tastes, and the resulting convexity of the choice set allows for an interior solution.", "B": "It determines an individual's rate of time preference, and the resulting stationarity of preferences ensures that the median voter is stable over time.", "C": "It represents an individual's risk aversion, and the resulting concavity of preferences allows for a unique optimum.", "D": "It acts as a utility weight defining heterogeneous tastes, and the resulting single-crossing property of indifference curves ensures preferences are single-peaked."}, "Answer": "D", "pi_justification": "This item assesses the understanding of the model's microfoundations, specifically how individual preferences are defined and aggregated. It replaces a portion of a QA problem because the concepts are distinct and allow for high-fidelity distractors. The distractors test for common confusions between preference parameters (α^i), utility function properties (concavity), and the conditions for political aggregation (single-crossing).", "qid": "12", "question": "### Background\n\nIn the model, a group of heterogeneous individuals makes decisions via majority rule. The preferences of the `i`-th member of the group over two public goods, `g` and `f`, are given by:\n\n```latex\nW^{i} \\equiv E\\left\\{ \\sum_{t=1}^{2} \\left[ \\alpha^{i}u(g_{t}) + (1-\\alpha^{i})u(f_{t}) \\right] \\right\\} \\quad \\text{(Eq. 1)}\n```\n\nwhere `α^i` is the individual's preference parameter. This preference structure is crucial for the model's tractability.\n\n### Question\n\nIn this model, what is the primary role of the parameter `α^i` and which resulting property of the preference structure is crucial for applying the median voter theorem?\n", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 22, "Question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe optimal weight `α₁` on the macro forecast `η̂_{T+1}^{(1)}` is chosen to minimize the mean squared error of the final, updated micro forecast. \n\nIn the paper's 'base scenario', the micro model is perfectly specified and macro data are exact aggregates of micro data. In this idealized case, the optimal weight `α₁` on the macro forecast is zero. What is the direct implication of `α₁ = 0`?", "Options": {"A": "The macro forecast is biased, while the micro forecast is unbiased.", "B": "The micro forecast is a sufficient statistic, containing all relevant information.", "C": "The micro and macro forecasts are perfectly correlated, making one redundant.", "D": "The variance of the macro forecast is infinitely large compared to the micro forecast."}, "Answer": "B", "pi_justification": "Assessment Target: Assesses understanding of the key theoretical conclusion from the idealized base case. Strategy: Reverse-Reasoning (given an outcome, `α₁=0`, infer the underlying condition). Distractor Logic: (A) Plausible but not the direct implication; both are unbiased in the base case. (C) Is Almost Right, but sufficiency is the more precise and fundamental concept. (D) Is a condition that would also lead to `α₁` approaching zero, but it is not the reason in this specific scenario (Conceptual Opposite).", "qid": "22", "question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe optimal weight `α₁` on the macro forecast `η̂_{T+1}^{(1)}` is chosen to minimize the mean squared error of the final, updated micro forecast. \n\nIn the paper's 'base scenario', the micro model is perfectly specified and macro data are exact aggregates of micro data. In this idealized case, the optimal weight `α₁` on the macro forecast is zero. What is the direct implication of `α₁ = 0`?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 25, "Question": "### Background\n\n**Research Question.** This problem investigates how the Rational Expectations Hypothesis (REH) imposes testable cross-equation restrictions within a linear simultaneous equations model.\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\pi_1, \\pi_{1,0}^*$: Scalar reduced-form coefficients.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to lagged exogenous variables.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process. Under REH, the expectation $y_{1,t}^*$ is a linear function of the information set, which in this case simplifies to $y_{1,t}^* = \\psi_0 z_{t-1}$.\n\n---\n\n### Question\n\nWhat is the correct expression for the coefficient $\\psi_0$ that is consistent with the Rational Expectations Hypothesis?", "Options": {"A": "$\\psi_0 = \\frac{\\pi_1 A_1}{1 - \\pi_{1,0}^*}$", "B": "$\\psi_0 = \\pi_1 A_1$", "C": "$\\psi_0 = \\frac{\\pi_1 A_1}{1 + \\pi_{1,0}^*}$", "D": "$\\psi_0 = \\frac{\\pi_1}{1 - \\pi_{1,0}^*}$"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item captures the core of the original QA's Q3(a). Assessment Target: Test the ability to apply the general theory of REH restrictions to a specific, simplified model. Strategy: Computational Judgment. Distractor Logic: (B) Sign Error in the denominator, a common algebraic slip. (C) Step-Omission Error, ignoring the feedback term from the expectation itself ($y_{1,t}^* \\pi_{1,0}^*$). (D) Formula Misuse, omitting the persistence parameter $A_1$ from the exogenous process forecast.", "qid": "25", "question": "### Background\n\n**Research Question.** This problem investigates how the Rational Expectations Hypothesis (REH) imposes testable cross-equation restrictions within a linear simultaneous equations model.\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\pi_1, \\pi_{1,0}^*$: Scalar reduced-form coefficients.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to lagged exogenous variables.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process. Under REH, the expectation $y_{1,t}^*$ is a linear function of the information set, which in this case simplifies to $y_{1,t}^* = \\psi_0 z_{t-1}$.\n\n---\n\n### Question\n\nWhat is the correct expression for the coefficient $\\psi_0$ that is consistent with the Rational Expectations Hypothesis?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 27, "Question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe contestant's preferences are represented by the CARA utility function:\n\n```latex\nU(W) = -e^{-\\alpha W}\n\\quad \\text{(Eq. 1)}\n```\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round, where `0.5 * stake \\le b \\le stake`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings (Distribution A) to the simulated distribution from a risk-neutral strategy (Distribution B).\n\n**Table 1. Data for Method 2 (Distributional Comparison)**\n\n| Distribution | Mean Winnings (\\$) | Std. Dev. of Winnings (\\$) |\n| :--- | :--- | :--- |\n| A: Actual Sample | `\\mu_A = 4,677` | `\\sigma_A = 4,258` |\n| B: Risk-Neutral Sim. | `\\mu_B = 6,987` | `\\sigma_B = 10,843` |\n\n### Question\n\nUsing the certainty equivalent approximation `CE ≈ μ - (α/2)σ²` and the data in Table 1, calculate the coefficient of absolute risk aversion (`α`) that would make a contestant indifferent between the actual sample distribution of winnings (Distribution A) and the simulated risk-neutral distribution (Distribution B).", "Options": {"A": "0.701", "B": "0.000310", "C": "0.000047", "D": "0.000023"}, "Answer": "C", "pi_justification": "This item assesses the ability to apply a specific formula for risk aversion using data from a table. It uses a Computational Judgment strategy. The distractors are designed to catch common errors: B is a Step-Omission Error (omitting the '2' in the numerator of the rearranged formula); C is a Conceptual Error (confusing the result of Method 2 with the reported result of Method 1); D is a Formula Misuse error (using standard deviation instead of variance in the denominator).", "qid": "27", "question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe contestant's preferences are represented by the CARA utility function:\n\n```latex\nU(W) = -e^{-\\alpha W}\n\\quad \\text{(Eq. 1)}\n```\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round, where `0.5 * stake \\le b \\le stake`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings (Distribution A) to the simulated distribution from a risk-neutral strategy (Distribution B).\n\n**Table 1. Data for Method 2 (Distributional Comparison)**\n\n| Distribution | Mean Winnings (\\$) | Std. Dev. of Winnings (\\$) |\n| :--- | :--- | :--- |\n| A: Actual Sample | `\\mu_A = 4,677` | `\\sigma_A = 4,258` |\n| B: Risk-Neutral Sim. | `\\mu_B = 6,987` | `\\sigma_B = 10,843` |\n\n### Question\n\nUsing the certainty equivalent approximation `CE ≈ μ - (α/2)σ²` and the data in Table 1, calculate the coefficient of absolute risk aversion (`α`) that would make a contestant indifferent between the actual sample distribution of winnings (Distribution A) and the simulated risk-neutral distribution (Distribution B).", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 24, "Question": "### Background\n\n**Research Question.** This problem explores an alternative, one-step GLS approach to combining micro and macro forecasts and its connection to Bayesian updating.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}` by combining two sources of information: a direct forecast `ŷ_{T+1}^{(2)}` and a forecast of a linear combination of `y_{T+1}`, `η̂_{T+1}^{(1)}`.\n\n### Data / Model Specification\n\nThe problem can be framed in Bayesian terms as follows:\n*   **Prior:** Our initial belief about `y_{T+1}` comes from the micro model: `y_{T+1} ∼ N_p(ŷ_{T+1}^{(2)}, Σ)`.\n*   **Likelihood:** The macro forecast is viewed as data providing a signal about `y_{T+1}`: `η̂_{T+1}^{(1)} | y_{T+1} ∼ N(a'y_{T+1}, σ₁²)`.\n\nThe posterior distribution is then derived by combining the prior and the likelihood using Bayes' rule.\n\nWhen framing the forecast combination problem in these Bayesian terms, how are the initial micro and macro forecasts used?", "Options": {"A": "Both forecasts are combined to form a joint likelihood function, with a flat, uninformative prior.", "B": "The micro forecast is the parameter to be estimated, and the macro forecast is the posterior mean.", "C": "The macro forecast serves as the prior belief, which is updated using the micro forecast as the data.", "D": "The micro forecast serves as the prior belief, which is updated using the macro forecast as the data."}, "Answer": "D", "pi_justification": "Assessment Target: Tests for a sharp understanding of how the Bayesian paradigm is applied in this context. Strategy: Scenario Application (applying Bayesian terminology). Distractor Logic: (A) is the classic Conceptual Opposite error of reversing the roles of prior and data/likelihood. (C) describes an alternative, valid Bayesian approach, but it is not the one specified in the problem stem (True but Irrelevant). (D) demonstrates a confusion of basic Bayesian terms.", "qid": "24", "question": "### Background\n\n**Research Question.** This problem explores an alternative, one-step GLS approach to combining micro and macro forecasts and its connection to Bayesian updating.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}` by combining two sources of information: a direct forecast `ŷ_{T+1}^{(2)}` and a forecast of a linear combination of `y_{T+1}`, `η̂_{T+1}^{(1)}`.\n\n### Data / Model Specification\n\nThe problem can be framed in Bayesian terms as follows:\n*   **Prior:** Our initial belief about `y_{T+1}` comes from the micro model: `y_{T+1} ∼ N_p(ŷ_{T+1}^{(2)}, Σ)`.\n*   **Likelihood:** The macro forecast is viewed as data providing a signal about `y_{T+1}`: `η̂_{T+1}^{(1)} | y_{T+1} ∼ N(a'y_{T+1}, σ₁²)`.\n\nThe posterior distribution is then derived by combining the prior and the likelihood using Bayes' rule.\n\nWhen framing the forecast combination problem in these Bayesian terms, how are the initial micro and macro forecasts used?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 21, "Question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe paper proposes a simpler weight, `α₂`, which is derived under the assumption that the covariance between the micro and macro forecasts is zero. The formula is:\n```latex\nα₂ = \\frac{σ₂²}{(z_T'γ - a'X_Tβ)² + σ₁² + σ₂²}\n```\nwhere `σ₁²` and `σ₂²` are the variances of the macro and aggregated micro forecasts, respectively.\n\nIn the formula for the simplified weight `α₂`, what is the correct interpretation of the 'inconsistency' term `(z_T'γ - a'X_Tβ)²`?", "Options": {"A": "It quantifies the systematic difference between the macro model's structure and the aggregated micro model's structure.", "B": "It reflects the degree of correlation between the micro and macro forecast errors.", "C": "It captures the portion of the micro forecast variance that is explained by the macro model.", "D": "It measures the estimation uncertainty (variance) of the macro forecast."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the core economic intuition behind the forecast combination formula, specifically the role of model inconsistency. Strategy: Scenario Application (interpreting a term within a formula). Distractor Logic: (A) Confuses the inconsistency term with the variance term `σ₁²`. (C) Is a Conceptual Opposite, as this formula is derived assuming zero correlation. (D) Is a plausible-sounding but incorrect statistical interpretation (Formula Misuse).", "qid": "21", "question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe paper proposes a simpler weight, `α₂`, which is derived under the assumption that the covariance between the micro and macro forecasts is zero. The formula is:\n```latex\nα₂ = \\frac{σ₂²}{(z_T'γ - a'X_Tβ)² + σ₁² + σ₂²}\n```\nwhere `σ₁²` and `σ₂²` are the variances of the macro and aggregated micro forecasts, respectively.\n\nIn the formula for the simplified weight `α₂`, what is the correct interpretation of the 'inconsistency' term `(z_T'γ - a'X_Tβ)²`?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 30, "Question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Equilibrium Conditions:** For a given safe rate `R_0`, a marginal type `p* = 1 - sqrt(R_0/γ)` separates non-monitoring (`p ≤ p*`) from monitoring (`p > p*`) banks. For `p > p*`, banks choose an optimal monitoring intensity `m_p* > 0` determined by the first-order condition:\n  ```latex\nc''(m_p) - \\frac{R_0}{(1-p+m_p)^2} = 0 \\quad \\text{(Eq. 1)}\n  ```\n\n### Question\n\nUnder the quadratic cost assumption, what is the equilibrium probability of failure (`p - m_p*`) for a monitoring bank lending to an entrepreneur of type `p > p*`?", "Options": {"A": "`p`", "B": "`p*`", "C": "`0`", "D": "`p - p*`"}, "Answer": "B", "pi_justification": "Assesses understanding of the model's key result for the intensive margin of risk. Strategy: Computational Judgment. Distractors represent common conceptual errors: B (confusing failure probability with the entrepreneur's baseline type), C (assuming monitoring eliminates all risk), and D (providing the formula for monitoring intensity `m_p*` instead of the failure probability `p - m_p*`).", "qid": "30", "question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Equilibrium Conditions:** For a given safe rate `R_0`, a marginal type `p* = 1 - sqrt(R_0/γ)` separates non-monitoring (`p ≤ p*`) from monitoring (`p > p*`) banks. For `p > p*`, banks choose an optimal monitoring intensity `m_p* > 0` determined by the first-order condition:\n  ```latex\nc''(m_p) - \\frac{R_0}{(1-p+m_p)^2} = 0 \\quad \\text{(Eq. 1)}\n  ```\n\n### Question\n\nUnder the quadratic cost assumption, what is the equilibrium probability of failure (`p - m_p*`) for a monitoring bank lending to an entrepreneur of type `p > p*`?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 23, "Question": "### Background\n\n**Research Question.** This problem explores alternative approaches to combining micro and macro forecasts.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}`. The paper discusses two main methods:\n1.  A **two-step approach**: First, an optimal combined macro forecast `η̂_{T+1}` is created. Second, the micro forecast is updated to be consistent with this macro total.\n2.  A **one-step GLS approach**: This method frames the problem as a single regression model and finds the `y_{T+1}` that simultaneously best fits both the initial micro and macro forecasts.\n\n### Data / Model Specification\n\nThe one-step GLS approach is formulated as the regression problem:\n```latex\n\\begin{pmatrix} η̂_{T+1}^{(1)} \\\\ ŷ_{T+1}^{(2)} \\end{pmatrix} = \\begin{pmatrix} a' \\\\ I_p \\end{pmatrix} y_{T+1} + \\begin{pmatrix} ε₁ \\\\ ε₂ \\end{pmatrix}\n```\n\nWhat is the key conceptual difference between the one-step GLS approach and the two-step approach?", "Options": {"A": "The GLS approach assumes forecast errors are normally distributed, while the two-step approach does not.", "B": "The GLS approach is only optimal if the forecasts are uncorrelated, while the two-step approach is always optimal.", "C": "The GLS approach simultaneously balances deviations from both forecasts, while the two-step approach sequentially imposes the combined macro forecast as a hard constraint.", "D": "The GLS approach is Bayesian, while the two-step approach is frequentist."}, "Answer": "C", "pi_justification": "Assessment Target: Tests the ability to distinguish between the two main theoretical approaches in the paper. Strategy: Conceptual Judgment. Distractor Logic: The distractors target common but incorrect ways of differentiating statistical models. (A) is an incorrect classification. (B) is an incorrect statement about assumptions, as normality is useful for both. (D) makes an incorrect claim about the optimality conditions of the two methods.", "qid": "23", "question": "### Background\n\n**Research Question.** This problem explores alternative approaches to combining micro and macro forecasts.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}`. The paper discusses two main methods:\n1.  A **two-step approach**: First, an optimal combined macro forecast `η̂_{T+1}` is created. Second, the micro forecast is updated to be consistent with this macro total.\n2.  A **one-step GLS approach**: This method frames the problem as a single regression model and finds the `y_{T+1}` that simultaneously best fits both the initial micro and macro forecasts.\n\n### Data / Model Specification\n\nThe one-step GLS approach is formulated as the regression problem:\n```latex\n\\begin{pmatrix} η̂_{T+1}^{(1)} \\\\ ŷ_{T+1}^{(2)} \\end{pmatrix} = \\begin{pmatrix} a' \\\\ I_p \\end{pmatrix} y_{T+1} + \\begin{pmatrix} ε₁ \\\\ ε₂ \\end{pmatrix}\n```\n\nWhat is the key conceptual difference between the one-step GLS approach and the two-step approach?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 29, "Question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Banking Sector:** The loan market is contestable, forcing banks to set the lowest feasible lending rate `R_p`. A bank will choose to initiate monitoring (`m > 0`) only if the marginal benefit of doing so exceeds the marginal cost. The marginal type `p*` is the entrepreneur type for whom a bank is indifferent between `m=0` and initiating monitoring.\n\n### Question\n\nGiven the quadratic cost function, what is the expression for the marginal entrepreneur type `p*` that separates the non-monitoring sector (`p ≤ p*`) from the monitoring sector (`p > p*`) in terms of the safe rate `R_0` and the cost parameter `γ`?", "Options": {"A": "`p* = 1 - sqrt(R_0/γ)`", "B": "`p* = 1 + sqrt(R_0/γ)`", "C": "`p* = 1 - sqrt(γ/R_0)`", "D": "`p* = 1 - (R_0/γ)`"}, "Answer": "A", "pi_justification": "Assesses the student's ability to derive the key threshold for the extensive margin of risk. Strategy: Computational Judgment. Distractors are based on common algebraic errors: B (sign error on the square root), C (omitting the square root), and D (inverting the fraction within the root).", "qid": "29", "question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Banking Sector:** The loan market is contestable, forcing banks to set the lowest feasible lending rate `R_p`. A bank will choose to initiate monitoring (`m > 0`) only if the marginal benefit of doing so exceeds the marginal cost. The marginal type `p*` is the entrepreneur type for whom a bank is indifferent between `m=0` and initiating monitoring.\n\n### Question\n\nGiven the quadratic cost function, what is the expression for the marginal entrepreneur type `p*` that separates the non-monitoring sector (`p ≤ p*`) from the monitoring sector (`p > p*`) in terms of the safe rate `R_0` and the cost parameter `γ`?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 31, "Question": "### Background\n\nAn agent evaluates a bet on the outcome of a two-stage draw. The first draw is from an unambiguous source, while the second is from an ambiguous source. The agent observes the second draw and then evaluates a bet on the first.\n\n*   State space `S = \\{rr, rg, gr, gg\\}`.\n*   `R_1 = \\{rr, rg\\}`: Event that first draw is red.\n*   `R_2 = \\{rr, gr\\}`: Event that second draw is red.\n*   `X = 1_{R_1}`: The random variable being evaluated (a bet on the first draw being red).\n*   `\\mathcal{P} = \\{R_2, G_2\\}`: The information partition, where `G_2` is the event the second draw is green.\n*   The agent's beliefs are captured by a totally monotone capacity `π` with the following non-zero Möbius transform values: `μ_π(rr) = μ_π(gg) = 1/6`, `μ_π(rg) = μ_π(gr) = 1/12`, `μ_π(R_1) = μ_π(G_1) = 1/4`.\n\n---\n\n### Data / Model Specification\n\nThe paper's proposed **proxy update** rule first forms a proxy capacity `π^\\mathcal{P}` and then updates it via `π(A|B) = π^\\mathcal{P}(A ∩ B) / π^\\mathcal{P}(B)`. The Möbius transform of the proxy is given by:\n\n```latex\n\\mu_{\\pi}^{\\mathcal{P}}(A) = \\sum_{B \\in \\mathcal{P}} \\sum_{\\{D : D \\cap B = A\\}} \\frac{|A|}{|D|} \\cdot \\mu_{\\pi}(D) \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nUsing the proxy updating rule, what is the conditional evaluation of the bet `X` after observing a red ball from the second draw, `E(X|R_2)`?", "Options": {"A": "1/3", "B": "1/2", "C": "7/12", "D": "5/12"}, "Answer": "C", "pi_justification": "Assessment Target: This question tests the student's ability to correctly apply the formula for the proxy update rule, the paper's core technical contribution. Strategy: Computational Judgment. The student must perform a multi-step calculation to arrive at a single numerical answer. Distractor Logic: (A) Correct answer. (B) `E(X|G_2)`, the value conditional on the other signal, testing if the student correctly identifies the conditioning event. (C) The result from the alternative prior-by-prior updating rule, testing for conceptual confusion between the two models. (D) The ex-ante value `E(X)`, testing if the student incorrectly assumes the information is uninformative.", "qid": "31", "question": "### Background\n\nAn agent evaluates a bet on the outcome of a two-stage draw. The first draw is from an unambiguous source, while the second is from an ambiguous source. The agent observes the second draw and then evaluates a bet on the first.\n\n*   State space `S = \\{rr, rg, gr, gg\\}`.\n*   `R_1 = \\{rr, rg\\}`: Event that first draw is red.\n*   `R_2 = \\{rr, gr\\}`: Event that second draw is red.\n*   `X = 1_{R_1}`: The random variable being evaluated (a bet on the first draw being red).\n*   `\\mathcal{P} = \\{R_2, G_2\\}`: The information partition, where `G_2` is the event the second draw is green.\n*   The agent's beliefs are captured by a totally monotone capacity `π` with the following non-zero Möbius transform values: `μ_π(rr) = μ_π(gg) = 1/6`, `μ_π(rg) = μ_π(gr) = 1/12`, `μ_π(R_1) = μ_π(G_1) = 1/4`.\n\n---\n\n### Data / Model Specification\n\nThe paper's proposed **proxy update** rule first forms a proxy capacity `π^\\mathcal{P}` and then updates it via `π(A|B) = π^\\mathcal{P}(A ∩ B) / π^\\mathcal{P}(B)`. The Möbius transform of the proxy is given by:\n\n```latex\n\\mu_{\\pi}^{\\mathcal{P}}(A) = \\sum_{B \\in \\mathcal{P}} \\sum_{\\{D : D \\cap B = A\\}} \\frac{|A|}{|D|} \\cdot \\mu_{\\pi}(D) \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nUsing the proxy updating rule, what is the conditional evaluation of the bet `X` after observing a red ball from the second draw, `E(X|R_2)`?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 33, "Question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nBased on the model and results in Table 1, which statement best describes the moderating effect of R&D intensity on the relationship between corruption and a firm's decision to invest?", "Options": {"A": "R&D-intensive firms are less deterred by corruption, likely due to stronger bargaining power.", "B": "R&D-intensive firms are more deterred by corruption because their complex operations are more exposed to bureaucratic hurdles.", "C": "Corruption uniformly deters all firms, and R&D intensity has a separate, independent negative effect on investment.", "D": "The positive interaction effect shows that R&D-intensive firms are more adept at using bribes to their advantage."}, "Answer": "A", "pi_justification": "This item assesses the conceptual understanding of an interaction effect, a key nuance in the paper's findings. It uses a Reverse-Reasoning strategy, asking for the best explanation of an observed statistical pattern. Distractors are designed to be plausible but incorrect: (B) is a plausible alternative mechanism (Conceptual Opposite), (C) ignores the interaction term entirely (Step-Omission Error), and (D) misinterprets the 'bargaining power' mechanism as active participation in corruption rather than resistance to it.", "qid": "33", "question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nBased on the model and results in Table 1, which statement best describes the moderating effect of R&D intensity on the relationship between corruption and a firm's decision to invest?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 36, "Question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to the `Price+IHD` group, which provided real-time energy usage information. However, only 72% of assigned households ultimately complied and had a functioning device installed. The Intent-to-Treat (ITT) effect—the average effect on all *assigned* households—was estimated to be a 13.7% reduction in electricity usage (-0.137 in log terms).\n\nThe Treatment-on-the-Treated (ToT) effect measures the causal effect only on the sub-population of compliers. It can be calculated by scaling the ITT effect by the compliance rate.\n\n### Data / Model Specification\n\n- **ITT Estimate for `Price+IHD`:** -0.137\n- **Compliance Rate for `Price+IHD`:** 0.72\n\n---\n\n### Question\n\nUsing the provided data, what is the implied Treatment-on-the-Treated (ToT) estimate, and what is the economic intuition for its relationship to the ITT estimate?", "Options": {"A": "The ToT estimate is -0.099 (-0.137 * 0.72). Its magnitude is smaller than the ITT because it applies to a smaller, more selective group of households.", "B": "The ToT estimate is -0.137, the same as the ITT. The estimate is valid for compliers because the initial randomization ensures the groups are comparable.", "C": "The ToT estimate is -0.190 (-0.137 / 0.72). Its magnitude is larger than the ITT because the compliers were likely more motivated to save energy from the start.", "D": "The ToT estimate is -0.190 (-0.137 / 0.72). Its magnitude is larger than the ITT because the ITT effect is diluted by including non-compliers who had a zero treatment effect."}, "Answer": "D", "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.0). It uses the 'Computational Judgment' strategy to test the mathematical and intuitive relationship between ITT and ToT. The correct answer (B) provides the correct calculation (scaling up the ITT by the compliance rate) and the correct intuition (ITT is a diluted average). Distractors are based on predictable errors: (A) uses the wrong formula (multiplication instead of division) and faulty intuition; (C) fails to make any adjustment; (D) arrives at the correct number but provides subtly incorrect intuition, attributing the difference to selection bias rather than the mechanical process of the Wald estimator.", "qid": "36", "question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to the `Price+IHD` group, which provided real-time energy usage information. However, only 72% of assigned households ultimately complied and had a functioning device installed. The Intent-to-Treat (ITT) effect—the average effect on all *assigned* households—was estimated to be a 13.7% reduction in electricity usage (-0.137 in log terms).\n\nThe Treatment-on-the-Treated (ToT) effect measures the causal effect only on the sub-population of compliers. It can be calculated by scaling the ITT effect by the compliance rate.\n\n### Data / Model Specification\n\n- **ITT Estimate for `Price+IHD`:** -0.137\n- **Compliance Rate for `Price+IHD`:** 0.72\n\n---\n\n### Question\n\nUsing the provided data, what is the implied Treatment-on-the-Treated (ToT) estimate, and what is the economic intuition for its relationship to the ITT estimate?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 40, "Question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). An 'assignment' is a matrix, $D = (\\delta_{ij})$, where $\\delta_{ij}$ is the proportion of meetings between type-i workers and type-j jobs that result in a match. A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. An assignment is an equilibrium if the $\\delta_{ij}$ values are consistent with these private decisions.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nThe model considers three potential equilibrium assignments, with the 'Exact' assignment defined by $\\delta_{11}=\\delta_{22}=\\delta_{33}=1$ and all other $\\delta_{ij}=0$. Equilibrium reservation values for the 'Exact' and 'Mixed (2, 3)' assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Mixed (2, 3) | 0.323           | 0.870           | 2.313           |\n| Exact        | 0.301           | 0.904           | 2.410           |\n\n### Question\n\nIn the 'Exact' equilibrium, the model specifies that matches between type-2 workers and type-3 jobs are always rejected (i.e., $\\delta_{23}=0$). Is this rejection privately rational for the agents, and why?", "Options": {"A": "Yes, it is rational because the 'Exact' assignment by definition requires that only diagonal matches ($\\delta_{ii}=1$) are formed.", "B": "No, it is irrational because the match output ($a_{23}=3.2$) is greater than the sum of reservation values calculated using the 'Mixed (2, 3)' equilibrium data ($w_{02}+r_{03}=3.183$).", "C": "No, it is irrational because the match output ($a_{23}=3.2$) is greater than the sum of the agents' reservation values ($w_{02}+r_{02}=1.808$).", "D": "Yes, it is rational because the match output ($a_{23}=3.2$) is less than the sum of the agents' reservation values in the 'Exact' equilibrium ($w_{02}+r_{03}=3.314$)."}, "Answer": "D", "pi_justification": "This item assesses the ability to apply the model's core equilibrium condition. It uses a Computational Judgment strategy. Distractors are designed based on common errors: B (Lookup Error: using data from the wrong equilibrium), C (Calculation Error: using the wrong reservation value, r_02 instead of r_03), and D (Conceptual Error: confusing the definition of an assignment with the equilibrium condition).", "qid": "40", "question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). An 'assignment' is a matrix, $D = (\\delta_{ij})$, where $\\delta_{ij}$ is the proportion of meetings between type-i workers and type-j jobs that result in a match. A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. An assignment is an equilibrium if the $\\delta_{ij}$ values are consistent with these private decisions.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nThe model considers three potential equilibrium assignments, with the 'Exact' assignment defined by $\\delta_{11}=\\delta_{22}=\\delta_{33}=1$ and all other $\\delta_{ij}=0$. Equilibrium reservation values for the 'Exact' and 'Mixed (2, 3)' assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Mixed (2, 3) | 0.323           | 0.870           | 2.313           |\n| Exact        | 0.301           | 0.904           | 2.410           |\n\n### Question\n\nIn the 'Exact' equilibrium, the model specifies that matches between type-2 workers and type-3 jobs are always rejected (i.e., $\\delta_{23}=0$). Is this rejection privately rational for the agents, and why?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 26, "Question": "### Background\n\n**Research Question.** This problem investigates the economic intuition behind the cross-equation restrictions imposed by the Rational Expectations Hypothesis (REH).\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to the lagged exogenous variable $z_{t-1}$, such that $y_{1,t}^* = \\psi_0 z_{t-1}$.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error and $\\pi_1 > 0$. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process and $0 < A_1 < 1$.\n\n---\n\n### Question\n\nHow does the coefficient $\\psi_0$ change as the persistence of the exogenous process, $A_1$, increases, and what is the economic intuition for this change?", "Options": {"A": "$\\psi_0$ decreases, because higher persistence in the exogenous process makes the endogenous feedback loop relatively less important.", "B": "$\\psi_0$ increases, because a more persistent process means $z_{t-1}$ is a stronger predictor of the future value $z_t$, so rational agents weigh it more heavily.", "C": "$\\psi_0$ increases, because higher persistence implies greater future volatility, causing agents to react more strongly to any new information.", "D": "$\\psi_0$ is unaffected, because $A_1$ is a parameter of the exogenous process and does not enter the structural equation for $y_{1,t}$."}, "Answer": "B", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item captures the core of the original QA's Q3(b) and Q3(c). Assessment Target: Test the economic intuition behind the model's comparative statics. Strategy: Scenario Application. Distractor Logic: (A) is the correct answer, linking persistence to predictive power. (B) is a Conceptual Opposite, suggesting the wrong direction of change with a plausible-sounding but incorrect reason. (C) represents a common error of failing to see cross-equation linkages (True but Irrelevant logic). (D) is Almost Right in direction but provides an incorrect economic mechanism (confusing persistence with volatility).", "qid": "26", "question": "### Background\n\n**Research Question.** This problem investigates the economic intuition behind the cross-equation restrictions imposed by the Rational Expectations Hypothesis (REH).\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to the lagged exogenous variable $z_{t-1}$, such that $y_{1,t}^* = \\psi_0 z_{t-1}$.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error and $\\pi_1 > 0$. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process and $0 < A_1 < 1$.\n\n---\n\n### Question\n\nHow does the coefficient $\\psi_0$ change as the persistence of the exogenous process, $A_1$, increases, and what is the economic intuition for this change?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 37, "Question": "### Background\n\nResearchers conducted a randomized controlled trial (RCT) assigning households to `Control`, `Price-Only`, and `Price+IHD` groups. A check of pre-treatment characteristics revealed a statistically significant difference in average home size between the `Price-Only` and `Control` groups. Furthermore, the study experienced selective attrition: households on a time-of-use (TOU) rate plan were significantly more likely to drop out of the `Price+IHD` group than other households in that group.\n\n---\n\n### Question\n\nGiven the baseline imbalance and selective attrition, which statement correctly assesses the threats to the study's internal validity and the appropriate analytical approach?", "Options": {"A": "The baseline imbalance is a minor concern that can be addressed by including home size as a control variable, and the selective attrition is addressed by using Intent-to-Treat (ITT) analysis, which preserves the benefits of the original randomization.", "B": "The baseline imbalance proves the randomization failed, and the selective attrition introduces bias; therefore, no causal claims can be made from the experiment.", "C": "The selective attrition is the primary threat, which can be solved by comparing outcomes only among the households who completed the study (compliers).", "D": "The Intent-to-Treat (ITT) principle solves the problem of the baseline imbalance, but the selective attrition requires using a Treatment-on-the-Treated (ToT) analysis to obtain an unbiased estimate."}, "Answer": "A", "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.5). It assesses the student's ability to correctly diagnose and propose remedies for two common threats to validity in an RCT: baseline imbalance and selective attrition. The strategy is 'Conceptual Judgment'. The correct answer (C) identifies the standard best practices. Distractors are based on classic misconceptions: (A) is overly pessimistic and dismisses valid remedies; (B) proposes an analysis (naive comparison of compliers) that is biased by selective attrition; (D) incorrectly maps the solutions to the problems (ITT addresses attrition's effect on the 'assignment' estimate, not baseline imbalance).", "qid": "37", "question": "### Background\n\nResearchers conducted a randomized controlled trial (RCT) assigning households to `Control`, `Price-Only`, and `Price+IHD` groups. A check of pre-treatment characteristics revealed a statistically significant difference in average home size between the `Price-Only` and `Control` groups. Furthermore, the study experienced selective attrition: households on a time-of-use (TOU) rate plan were significantly more likely to drop out of the `Price+IHD` group than other households in that group.\n\n---\n\n### Question\n\nGiven the baseline imbalance and selective attrition, which statement correctly assesses the threats to the study's internal validity and the appropriate analytical approach?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 32, "Question": "### Background\n\nConsider an agent evaluating a prospect where information arrives sequentially. The agent's ex-ante evaluation is `E(X)`. After observing an event `B` from a partition `\\mathcal{P}`, the agent forms a conditional evaluation `E(X|B)`. For the specific example in the paper involving two draws, the ex-ante value of a bet `X` is `E(X) = 1/2`. When updating beliefs using the **prior-by-prior** rule, the conditional evaluations are `E^m(X|R_2) = 1/3` and `E^m(X|G_2) = 1/6`. Both values are strictly less than the ex-ante value, a phenomenon known as \"all news is bad news.\"\n\n---\n\n### Question\n\nWhat is the core reason that the prior-by-prior updating rule is susceptible to the \"all news is bad news\" phenomenon, while the paper's proposed proxy update rule is not?", "Options": {"A": "The proxy update rule always satisfies the law of iterated expectations, whereas the prior-by-prior rule does not.", "B": "The proxy update rule transforms the set of priors into a single Bayesian prior before updating, eliminating all ambiguity.", "C": "Prior-by-prior updating is only defined for totally monotone capacities, while proxy updating works for any capacity.", "D": "Prior-by-prior updating allows the agent to select a different worst-case prior for each possible information event, while proxy updating commits the agent to a single proxy belief structure before conditioning."}, "Answer": "D", "pi_justification": "Assessment Target: This question assesses the student's understanding of the fundamental behavioral difference between the proxy update rule and its main alternative, prior-by-prior updating. Strategy: Conceptual distinction. The question asks for the 'why' behind a key difference. Distractor Logic: (A) Correct answer, capturing the re-optimization vs. commitment distinction. (B) Conceptual Opposite: Both rules can violate the law of iterated expectations; this is a central point of the paper. (C) Factual Error: Both rules are typically discussed in the context of totally monotone (or at least supermodular) capacities. (D) Almost Right: The proxy update makes the *partition elements* unambiguous, but it does not necessarily eliminate all ambiguity from the conditional beliefs.", "qid": "32", "question": "### Background\n\nConsider an agent evaluating a prospect where information arrives sequentially. The agent's ex-ante evaluation is `E(X)`. After observing an event `B` from a partition `\\mathcal{P}`, the agent forms a conditional evaluation `E(X|B)`. For the specific example in the paper involving two draws, the ex-ante value of a bet `X` is `E(X) = 1/2`. When updating beliefs using the **prior-by-prior** rule, the conditional evaluations are `E^m(X|R_2) = 1/3` and `E^m(X|G_2) = 1/6`. Both values are strictly less than the ex-ante value, a phenomenon known as \"all news is bad news.\"\n\n---\n\n### Question\n\nWhat is the core reason that the prior-by-prior updating rule is susceptible to the \"all news is bad news\" phenomenon, while the paper's proposed proxy update rule is not?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 28, "Question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round. This method yields an average `\\hat{\\alpha} \\approx 0.000310`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings to the simulated distribution from a risk-neutral strategy. This method yields a much smaller `\\hat{\\alpha}`.\n\n### Question\n\nAccording to the paper's analysis, which of the following best explains why Method 1 (final-round bets) yields a much higher estimate of risk aversion (`\\alpha`) than Method 2 (distributional comparison)?", "Options": {"A": "Contestants are more risk-loving in the final round due to the 'house money' effect, which should lead to a *lower* `\\alpha` from Method 1, making the results contradictory.", "B": "The risk-neutral strategy in Method 2 is an unrealistic benchmark because it fails to account for contestants' ability to count cards.", "C": "The final-round bets in Method 1 are heavily censored (many contestants bet the minimum or maximum), which biases the `\\alpha` estimate upwards.", "D": "Method 1 measures behavior only in high-stake final rounds, while Method 2 averages behavior across all rounds where risk-taking is not constant but depends heavily on the current stake, a violation of the CARA model's core assumption."}, "Answer": "D", "pi_justification": "This item assesses the student's ability to synthesize findings and understand the paper's core critique of standard utility theory. It uses a Reverse-Reasoning strategy, asking for the cause of an observed empirical puzzle. Distractor A is 'True but Irrelevant,' as censoring is a technical issue but not the core behavioral explanation for the large discrepancy. Distractor B is a 'Conceptual Opposite,' misstating the implication of the house money effect in this context. Distractor D is 'Almost Right,' mentioning a real detail from the paper (card counting) but misattributing it as the primary cause for the discrepancy in `α` estimates.", "qid": "28", "question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round. This method yields an average `\\hat{\\alpha} \\approx 0.000310`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings to the simulated distribution from a risk-neutral strategy. This method yields a much smaller `\\hat{\\alpha}`.\n\n### Question\n\nAccording to the paper's analysis, which of the following best explains why Method 1 (final-round bets) yields a much higher estimate of risk aversion (`\\alpha`) than Method 2 (distributional comparison)?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 41, "Question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. The economy can be stuck in an inefficient equilibrium.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nEquilibrium reservation values for two potential assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Exact        | 0.301           | 0.904           | 2.410           |\n| Mixed (1, 2) | 0.293           | 0.870           | 2.581           |\n\n### Question\n\nSuppose the economy is in the 'Exact' equilibrium, where matches between type-1 workers and type-2 jobs are rejected. To shift the economy to the more efficient 'Mixed (1, 2)' equilibrium, a planner must incentivize this match. What is the minimum per-match subsidy required to make the (worker 1, job 2) match privately rational, based on the 'Exact' equilibrium reservation values?", "Options": {"A": "0.025", "B": "-0.025", "C": "0.114", "D": "0.017"}, "Answer": "A", "pi_justification": "This item tests the ability to perform a policy-relevant calculation based on the model. It uses a Computational Judgment strategy. Distractors are designed based on plausible errors: B (Sign Error: reporting the private deficit instead of the subsidy needed to overcome it), C (Lookup Error: using reservation values from the target 'Mixed (1, 2)' equilibrium instead of the initial 'Exact' equilibrium), and D (Misapplication: calculating the subsidy for a different, incorrect match, such as (2,3)).", "qid": "41", "question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. The economy can be stuck in an inefficient equilibrium.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nEquilibrium reservation values for two potential assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Exact        | 0.301           | 0.904           | 2.410           |\n| Mixed (1, 2) | 0.293           | 0.870           | 2.581           |\n\n### Question\n\nSuppose the economy is in the 'Exact' equilibrium, where matches between type-1 workers and type-2 jobs are rejected. To shift the economy to the more efficient 'Mixed (1, 2)' equilibrium, a planner must incentivize this match. What is the minimum per-match subsidy required to make the (worker 1, job 2) match privately rational, based on the 'Exact' equilibrium reservation values?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 39, "Question": "### Background\n\n**Research Question.** This problem investigates the dynamic adjustment of the interest rate to a monetary policy shock, testing the paper's proposition of short-run \"over-adjustment\" within the specific Miller-Orr model of money demand.\n\n**Setting / Institutional Environment.** We consider the Miller-Orr \"pure diffusion\" case for a representative individual's money holdings, where cash flows follow a random walk. The monetary authority contracts the money supply by a fraction `\\gamma` through open-market sales of bonds.\n\n### Data / Model Specification\n\nFor the Miller-Orr model with a lower barrier of 0, an upper barrier `h`, and a return point `z`:\n1.  **Optimal Policy:** The optimal return point is `z=h/3`.\n2.  **Average Holdings:** The average money holding is `M = 4h/9`.\n3.  **Over-adjustment Condition:** Over-adjustment of the interest rate occurs if the new long-run average money holding, `M' = 4h'/9`, is less than the amount of money remaining after the short-run adjustment, `(1-\\gamma)M`.\n4.  **Derived Result:** The paper shows that for the Miller-Orr case, the condition for over-adjustment reduces to the following polynomial inequality in `y = h'/h`:\n    ```latex\n    \\frac{1}{2}y^{3}-\\frac{1}{2}y^{2}-\\frac{1}{18}y+\\frac{1}{18}<0\n    ```\n    The roots of this polynomial are at `y=1` and `y=\\pm 1/3`.\n\n### Question\n\nGiven the model and the derived inequality, under what conditions will the interest rate over-adjust in the short run in response to a monetary contraction of size `\\gamma`?", "Options": {"A": "Over-adjustment occurs for monetary contractions `\\gamma` that are greater than 2/3.", "B": "Over-adjustment never occurs; the interest rate adjusts smoothly to its new long-run equilibrium.", "C": "Over-adjustment occurs for monetary contractions `\\gamma` that are less than 2/3.", "D": "Over-adjustment occurs for monetary contractions `\\gamma` that are less than 1/3."}, "Answer": "C", "pi_justification": "This item assesses the ability to solve a mathematical condition and translate the result into an economic conclusion. It uses a Computational Judgment strategy. The correct answer requires solving a polynomial inequality for `y=h'/h` to find `y > 1/3`, and then correctly converting this into a condition on the monetary contraction `\\gamma` using `y = M'/M = (1-\\gamma_{LR})/(1)`. Distractors are based on common errors: (B) is a sign error in solving the inequality; (C) is a unit error, confusing the bound on `y` with the bound on `\\gamma`; (D) represents a plausible but incorrect conceptual belief about model dynamics.", "qid": "39", "question": "### Background\n\n**Research Question.** This problem investigates the dynamic adjustment of the interest rate to a monetary policy shock, testing the paper's proposition of short-run \"over-adjustment\" within the specific Miller-Orr model of money demand.\n\n**Setting / Institutional Environment.** We consider the Miller-Orr \"pure diffusion\" case for a representative individual's money holdings, where cash flows follow a random walk. The monetary authority contracts the money supply by a fraction `\\gamma` through open-market sales of bonds.\n\n### Data / Model Specification\n\nFor the Miller-Orr model with a lower barrier of 0, an upper barrier `h`, and a return point `z`:\n1.  **Optimal Policy:** The optimal return point is `z=h/3`.\n2.  **Average Holdings:** The average money holding is `M = 4h/9`.\n3.  **Over-adjustment Condition:** Over-adjustment of the interest rate occurs if the new long-run average money holding, `M' = 4h'/9`, is less than the amount of money remaining after the short-run adjustment, `(1-\\gamma)M`.\n4.  **Derived Result:** The paper shows that for the Miller-Orr case, the condition for over-adjustment reduces to the following polynomial inequality in `y = h'/h`:\n    ```latex\n    \\frac{1}{2}y^{3}-\\frac{1}{2}y^{2}-\\frac{1}{18}y+\\frac{1}{18}<0\n    ```\n    The roots of this polynomial are at `y=1` and `y=\\pm 1/3`.\n\n### Question\n\nGiven the model and the derived inequality, under what conditions will the interest rate over-adjust in the short run in response to a monetary contraction of size `\\gamma`?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 45, "Question": "## Background\n\n**Research Question.** This problem investigates the long-term financial impact of providing social information to new donors. The analysis is a one-year follow-up of the *new donors* from an initial field experiment, tracking their donation behavior to assess causal effects on long-term outcomes.\n\n**Variables & Parameters.**\n- `Renewal rate`: The proportion of new donors from a given experimental group who made another donation one year later.\n- `Mean contribution ($)`: The average donation amount in year t+1, *conditional on renewing*.\n- `Expected revenue ($)`: The unconditional expected donation amount from a new donor in year t+1, calculated as `Renewal rate` × `Mean contribution`.\n\n---\n\n## Data / Model Specification\n\nFrom a separate analysis on the initial impact, the estimated immediate (year t) revenue increase from the `$300` signal for *new donors* was **$9.00** compared to the control group.\n\nThe long-term outcomes for new donors one year later (year t+1) are summarized in Table 1.\n\n**Table 1: New Donors One Year Later**\n\n| Treatment | Renewal rate (%) | Mean contribution ($) | Expected revenue ($) |\n|:---|:---:|:---:|:---:|\n| Control | 12 | 86.11 | 10.62 |\n| $300 | 32 | 93.97 | 29.95 |\n\n---\n\n## Question\n\nA station manager is considering a new policy: all *new* callers will receive the `$300` social information treatment. Based on the data provided, what is the total expected two-year revenue increase *per new donor* under this policy, compared to the no-information (control) policy?", "Options": {"A": "$28.33", "B": "$19.33", "C": "$38.95", "D": "$9.00"}, "Answer": "A", "pi_justification": "This item assesses the ability to synthesize information from multiple sources (text and a table) to perform a multi-step calculation. It uses a 'Computational Judgment' strategy. The correct answer requires summing the immediate effect ($9.00) and the long-term effect ($29.95 - $10.62 = $19.33). Distractors are designed based on common errors: (A) is a 'Step-Omission Error' that only calculates the long-term effect; (C) is a 'Step-Omission Error' that only considers the immediate effect; (D) is a 'Formula Misuse' error where the student adds the total year 2 revenue to the year 1 gain instead of the *increase* in year 2 revenue.", "qid": "45", "question": "## Background\n\n**Research Question.** This problem investigates the long-term financial impact of providing social information to new donors. The analysis is a one-year follow-up of the *new donors* from an initial field experiment, tracking their donation behavior to assess causal effects on long-term outcomes.\n\n**Variables & Parameters.**\n- `Renewal rate`: The proportion of new donors from a given experimental group who made another donation one year later.\n- `Mean contribution ($)`: The average donation amount in year t+1, *conditional on renewing*.\n- `Expected revenue ($)`: The unconditional expected donation amount from a new donor in year t+1, calculated as `Renewal rate` × `Mean contribution`.\n\n---\n\n## Data / Model Specification\n\nFrom a separate analysis on the initial impact, the estimated immediate (year t) revenue increase from the `$300` signal for *new donors* was **$9.00** compared to the control group.\n\nThe long-term outcomes for new donors one year later (year t+1) are summarized in Table 1.\n\n**Table 1: New Donors One Year Later**\n\n| Treatment | Renewal rate (%) | Mean contribution ($) | Expected revenue ($) |\n|:---|:---:|:---:|:---:|\n| Control | 12 | 86.11 | 10.62 |\n| $300 | 32 | 93.97 | 29.95 |\n\n---\n\n## Question\n\nA station manager is considering a new policy: all *new* callers will receive the `$300` social information treatment. Based on the data provided, what is the total expected two-year revenue increase *per new donor* under this policy, compared to the no-information (control) policy?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 42, "Question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers three potential equilibrium assignments: 'Mixed (2, 3)', 'Exact', and 'Mixed (1, 2)'. The 'Exact' assignment only allows matches between agents of the same type (e.g., worker 1 with job 1). The 'Mixed (1, 2)' assignment allows matches between types 1 and 2, in addition to same-type matches. Key outcomes are shown in the tables below.\n\n**Table 1: Unemployment Rates**\n| Assignment   | $u_1=v_1$ | $u_2=v_2$ | $u_3=v_3$ |\n| :----------- | :-------- | :-------- | :-------- |\n| Exact        | 0.375     | 0.375     | 0.375     |\n| Mixed (1, 2) | 0.250     | 0.250     | 0.333     |\n\n**Table 2: Aggregate Output**\n| Assignment   | Output, $\\pi$ |\n| :----------- | :------------- |\n| Exact        | 7.50           |\n| Mixed (1, 2) | 7.72           |\n\n### Question\n\nThe 'Mixed (1, 2)' equilibrium yields a higher total output ($\\pi=7.72$) than the 'Exact' equilibrium ($\\pi=7.50$). According to the model's logic, what is the primary reason for this efficiency gain?", "Options": {"A": "The 'Mixed (1, 2)' equilibrium is more efficient because it forces type-3 workers to accept lower reservation wages, making them form matches more often.", "B": "By allowing moderately productive matches between type-1 and type-2 agents, the search pool changes, which allows the highly productive type-3 agents to find each other and match more quickly, increasing the frequency of high-output ($a_{33}=8.0$) matches.", "C": "The output from new matches between type-1 and type-2 agents ($a_{12}=1.18$) is substantially greater than the output from the type-1 matches they replace ($a_{11}=1.0$), and this direct gain accounts for most of the difference.", "D": "The 'Mixed (1, 2)' equilibrium is more efficient simply because it has lower unemployment rates for a majority of worker types compared to the 'Exact' assignment."}, "Answer": "B", "pi_justification": "This item assesses the student's ability to synthesize information from multiple tables to understand the model's main economic insight. It uses a Reverse-Reasoning strategy. Distractors are designed to test for common misinterpretations: B (True but Incomplete: focusing on a minor effect while ignoring the larger externality on type-3 agents), C (Factually Incorrect: contradicting data from the paper showing reservation wages for type 3 actually increase), and D (Oversimplification: ignoring the crucial compositional effects of matches, which are central to the paper's argument).", "qid": "42", "question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers three potential equilibrium assignments: 'Mixed (2, 3)', 'Exact', and 'Mixed (1, 2)'. The 'Exact' assignment only allows matches between agents of the same type (e.g., worker 1 with job 1). The 'Mixed (1, 2)' assignment allows matches between types 1 and 2, in addition to same-type matches. Key outcomes are shown in the tables below.\n\n**Table 1: Unemployment Rates**\n| Assignment   | $u_1=v_1$ | $u_2=v_2$ | $u_3=v_3$ |\n| :----------- | :-------- | :-------- | :-------- |\n| Exact        | 0.375     | 0.375     | 0.375     |\n| Mixed (1, 2) | 0.250     | 0.250     | 0.333     |\n\n**Table 2: Aggregate Output**\n| Assignment   | Output, $\\pi$ |\n| :----------- | :------------- |\n| Exact        | 7.50           |\n| Mixed (1, 2) | 7.72           |\n\n### Question\n\nThe 'Mixed (1, 2)' equilibrium yields a higher total output ($\\pi=7.72$) than the 'Exact' equilibrium ($\\pi=7.50$). According to the model's logic, what is the primary reason for this efficiency gain?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 34, "Question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nUsing the coefficient estimates in Table 1, at what threshold level of R&D intensity (R&D as a share of sales) does the marginal effect of corruption on the firm's investment propensity become zero?", "Options": {"A": "17.6%", "B": "12.3%", "C": "9.2%", "D": "The marginal effect is always negative, so no such threshold exists."}, "Answer": "B", "pi_justification": "This item tests the ability to translate an econometric model with an interaction term into a precise quantitative prediction. It uses the Computational Judgment strategy. The correct answer requires setting the marginal effect equation ($\\beta_{Corruption} + \\beta_{Interaction} \\times R\\&D = 0$) to zero and solving. Distractors are based on common errors: (B) uses the wrong coefficient in the calculation (Formula Misuse), (A) is a plausible number derived from a different part of the paper (a subtle Formula Misuse), and (D) represents a conceptual failure to understand how a positive interaction term can offset a negative main effect.", "qid": "34", "question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nUsing the coefficient estimates in Table 1, at what threshold level of R&D intensity (R&D as a share of sales) does the marginal effect of corruption on the firm's investment propensity become zero?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 38, "Question": "### Background\n\nIn an experiment on electricity demand, researchers exposed households to two types of high-price events:\n\n- **Day-Ahead (DA) Events:** Households received 24-hour advance notification of a moderate price increase (e.g., from ~$0.20/kWh to $0.70/kWh).\n- **30-Minute (TM) Events:** Households received 30-minute advance notification of a much larger price increase (e.g., from ~$0.20/kWh to $1.45/kWh).\n\n---\n\n### Question\n\nWhat is the primary behavioral trade-off that this experimental design choice—including both DA and TM events—allows the researchers to investigate?", "Options": {"A": "The difference between short-run and long-run price elasticity of demand.", "B": "The relative importance of the price signal's magnitude versus the household's ability to plan and overcome adjustment costs.", "C": "Whether households respond more to price increases in the afternoon versus the evening.", "D": "Whether households who own their homes are more price-sensitive than households who rent."}, "Answer": "B", "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.5). It assesses the ability to interpret the logic behind a specific experimental design choice. The strategy is 'Conceptual Interpretation'. The correct answer (C) accurately captures the trade-off between the size of the incentive (price) and the constraints on action (time to plan). Distractors represent plausible but incorrect interpretations: (A) confuses within-day variation with the long-run/short-run distinction; (B) focuses on a secondary feature (timing) rather than the core design contrast; (D) describes a potential analysis of heterogeneity, not the primary purpose of the DA vs. TM variation.", "qid": "38", "question": "### Background\n\nIn an experiment on electricity demand, researchers exposed households to two types of high-price events:\n\n- **Day-Ahead (DA) Events:** Households received 24-hour advance notification of a moderate price increase (e.g., from ~$0.20/kWh to $0.70/kWh).\n- **30-Minute (TM) Events:** Households received 30-minute advance notification of a much larger price increase (e.g., from ~$0.20/kWh to $1.45/kWh).\n\n---\n\n### Question\n\nWhat is the primary behavioral trade-off that this experimental design choice—including both DA and TM events—allows the researchers to investigate?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 47, "Question": "## Background\n\n**Research Question.** This problem aims to estimate the causal effect of providing social information on donation amounts and to understand the properties of the estimators used.\n\n**Setting / Institutional Environment.** The analysis is based on a randomized controlled trial at a public radio station. Callers were randomly assigned to a control group or treatment groups. The study uses regression analysis to control for other observable characteristics.\n\n---\n\n## Data / Model Specification\n\nThe relationship between social information and pledge amounts is estimated using Ordinary Least Squares (OLS) and Robust Regression. Robust regression is a method designed to be less sensitive to outliers than OLS.\n\n**Table 1: The Social Information Effect (standard errors in parentheses)**\n\n| | All data (OLS) | All data (Robust) |\n|:---|:---:|:---:|\n| **$300** | 39.599** (13.609) | 10.710* (4.730) |\n*Controls for other treatments and covariates included. **p < 0.01, *p < 0.05*\n\n---\n\n## Question\n\nThe OLS estimate for the `$300` treatment effect (39.599) is nearly four times larger than the robust regression estimate (10.710). Which of the following statements provides the best explanation for this discrepancy?", "Options": {"A": "The control variables included in the model must be more influential in the OLS specification than in the robust regression, thus accounting for the difference.", "B": "The robust regression is likely biased towards zero, while the OLS estimate is unbiased and therefore more reliable.", "C": "The OLS estimator for a treatment effect is equivalent to the difference in group means, which is highly sensitive to a few large outlier donations in the treatment group that the robust regression method down-weights.", "D": "The discrepancy is caused by heteroskedasticity in the error term, which biases OLS coefficients but is corrected by robust regression."}, "Answer": "C", "pi_justification": "This item assesses the understanding of how different estimators behave in the presence of outliers. It uses a 'Reverse-Reasoning' strategy, asking for the cause of an observed empirical pattern. The correct answer (A) correctly identifies that OLS is a difference-in-means and is sensitive to outliers, which robust regression is designed to mitigate. Distractors target common econometric confusions: (B) is a 'Conceptual Opposite', incorrectly claiming OLS is more reliable here; (C) confuses robust *regression* (which handles outliers) with robust *standard errors* (which handle heteroskedasticity); (D) offers an incorrect explanation, as the difference is driven by the estimator's sensitivity to the dependent variable's distribution, not the controls.", "qid": "47", "question": "## Background\n\n**Research Question.** This problem aims to estimate the causal effect of providing social information on donation amounts and to understand the properties of the estimators used.\n\n**Setting / Institutional Environment.** The analysis is based on a randomized controlled trial at a public radio station. Callers were randomly assigned to a control group or treatment groups. The study uses regression analysis to control for other observable characteristics.\n\n---\n\n## Data / Model Specification\n\nThe relationship between social information and pledge amounts is estimated using Ordinary Least Squares (OLS) and Robust Regression. Robust regression is a method designed to be less sensitive to outliers than OLS.\n\n**Table 1: The Social Information Effect (standard errors in parentheses)**\n\n| | All data (OLS) | All data (Robust) |\n|:---|:---:|:---:|\n| **$300** | 39.599** (13.609) | 10.710* (4.730) |\n*Controls for other treatments and covariates included. **p < 0.01, *p < 0.05*\n\n---\n\n## Question\n\nThe OLS estimate for the `$300` treatment effect (39.599) is nearly four times larger than the robust regression estimate (10.710). Which of the following statements provides the best explanation for this discrepancy?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 43, "Question": "### Background\n\nThe paper models a household's income process to construct a measure of idiosyncratic risk. The model for log income, `ln Y_it`, is:\n\n```latex\n\\ln Y_{i t}=\\alpha+X_{i t}^{\\prime}\\beta+\\epsilon_{i t} \\quad \\text{(Eq. (1))}\n```\n\nThe error term `ε_it` is decomposed into three components:\n\n```latex\n\\epsilon_{i t}=\\eta_{i}+\\nu_{i t} \\quad \\text{where} \\quad \\nu_{i t}=\\rho\\nu_{i t-1}+\\omega_{i t} \\quad \\text{(Eq. (2))}\n```\n\n- `η_i` is a time-invariant household-specific effect.\n- `ν_it` is a serially correlated transitory shock.\n- `ω_it` is an i.i.d. innovation error.\n\nThe paper uses the household-specific variance of the estimated innovation, `Var(ω̂_it)`, as its primary measure of income risk.\n\n### Question\n\nWhy is the variance of the innovation term `ω_it` the most appropriate measure of income risk for testing the theory of precautionary savings, as opposed to the variance of the other error components?", "Options": {"A": "Because the serially correlated shock `ν_it` captures persistent income shocks, which are the primary concern for long-term savings.", "B": "Because the time-invariant effect `η_i` represents the largest source of income variation between households.", "C": "Because `ω_it` is the only component that captures the true, unpredictable shocks to income that households cannot anticipate.", "D": "Because the total residual `ε_it` captures all sources of unobserved income variation, making its variance the most comprehensive measure of risk."}, "Answer": "C", "pi_justification": "This item assesses understanding of the paper's core variable construction. It uses a conceptual strategy to test whether students can distinguish between predictable and unpredictable components of an income process. The correct answer (A) identifies the i.i.d. innovation as the source of true uncertainty. Distractors are designed based on common misconceptions: (B) confuses risk with permanent heterogeneity (Conceptual Opposite); (C) fails to distinguish between predictable and unpredictable variation (Formula Misuse); (D) correctly identifies `ν_it` as persistent but incorrectly identifies it as the target for precautionary motives, as part of it is predictable (Almost Right).", "qid": "43", "question": "### Background\n\nThe paper models a household's income process to construct a measure of idiosyncratic risk. The model for log income, `ln Y_it`, is:\n\n```latex\n\\ln Y_{i t}=\\alpha+X_{i t}^{\\prime}\\beta+\\epsilon_{i t} \\quad \\text{(Eq. (1))}\n```\n\nThe error term `ε_it` is decomposed into three components:\n\n```latex\n\\epsilon_{i t}=\\eta_{i}+\\nu_{i t} \\quad \\text{where} \\quad \\nu_{i t}=\\rho\\nu_{i t-1}+\\omega_{i t} \\quad \\text{(Eq. (2))}\n```\n\n- `η_i` is a time-invariant household-specific effect.\n- `ν_it` is a serially correlated transitory shock.\n- `ω_it` is an i.i.d. innovation error.\n\nThe paper uses the household-specific variance of the estimated innovation, `Var(ω̂_it)`, as its primary measure of income risk.\n\n### Question\n\nWhy is the variance of the innovation term `ω_it` the most appropriate measure of income risk for testing the theory of precautionary savings, as opposed to the variance of the other error components?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 50, "Question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium'.\n\n**Variables & Parameters.**\n\n*   `T'_{k*}`: The marginal transfer price for retailer `k` at equilibrium.\n*   `(p^B, e^B)`: The prices and efforts that arise in standard Bertrand competition.\n*   `D_e`: The 2x2 matrix of demand derivatives with respect to effort, where element `(i, k)` is `∂D_k/∂e_i`.\n\n### Data / Model Specification\n\nIn any contract equilibrium, the following system of equations must hold for `i=1, 2`:\n\n```latex\n\\sum_{k=1}^{2}(T_{k}^{*\\prime}-c)\\frac{\\partial D_{k}}{\\partial e_{i}}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\n**Assumption 1:** A uniform increase in efforts `e_1` and `e_2` causes demand `D_i` to rise. This implies that the matrix `D_e` is invertible.\n\n### Question\n\nGiven the necessary equilibrium condition shown in Eq. (1) and Assumption 1, what is the equilibrium marginal transfer price `T'_{k*}` for each retailer `k`, and what is the resulting market outcome?", "Options": {"A": "The marginal transfer price equals the fully integrated price (`T'_{k*} = p_k^I`), which successfully implements the fully integrated outcome.", "B": "The marginal transfer price for each retailer equals the manufacturer's marginal cost (`T'_{k*} = c`), leading to the standard Bertrand outcome (`p^B, e^B`).", "C": "The marginal transfer price is set below marginal cost (`T'_{k*} < c`) to subsidize effort, leading to higher-than-Bertrand effort levels.", "D": "The marginal transfer price is set above marginal cost (`T'_{k*} > c`) to extract rent, leading to prices above the Bertrand level."}, "Answer": "B", "pi_justification": "This item assesses the main mathematical result of Proposition 1, which was derived in the second and third parts of the original QA problem. It uses a Computational Judgment strategy. The correct answer (A) states the paper's surprising result: secret contracting with retail effort forces the manufacturer to price at marginal cost, leading to the competitive Bertrand outcome. Distractor (B) is a classic misconception, representing the result from models *without* retail effort (O'Brien and Shaffer). Distractor (C) represents a common but incorrect intuition about monopoly pricing. Distractor (D) describes a strategy (subsidizing effort) that might be optimal with public contracts but is not sustainable with secret contracts.", "qid": "50", "question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium'.\n\n**Variables & Parameters.**\n\n*   `T'_{k*}`: The marginal transfer price for retailer `k` at equilibrium.\n*   `(p^B, e^B)`: The prices and efforts that arise in standard Bertrand competition.\n*   `D_e`: The 2x2 matrix of demand derivatives with respect to effort, where element `(i, k)` is `∂D_k/∂e_i`.\n\n### Data / Model Specification\n\nIn any contract equilibrium, the following system of equations must hold for `i=1, 2`:\n\n```latex\n\\sum_{k=1}^{2}(T_{k}^{*\\prime}-c)\\frac{\\partial D_{k}}{\\partial e_{i}}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\n**Assumption 1:** A uniform increase in efforts `e_1` and `e_2` causes demand `D_i` to rise. This implies that the matrix `D_e` is invertible.\n\n### Question\n\nGiven the necessary equilibrium condition shown in Eq. (1) and Assumption 1, what is the equilibrium marginal transfer price `T'_{k*}` for each retailer `k`, and what is the resulting market outcome?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 44, "Question": "### Background\n\nThe paper uses a two-stage estimation procedure. In the first stage, an income process is estimated to generate measures of permanent income and income risk. In the second stage, these generated variables are used to predict household portfolio choice (e.g., the share of wealth held in liquid assets).\n\nTo identify the causal effect of risk, the model relies on an exclusion restriction: occupation dummies are included as predictors in the first-stage income regression but are excluded from the second-stage portfolio choice regression.\n\n### Question\n\nWhich of the following scenarios, if true, would most directly violate the key identifying assumption of this research design?", "Options": {"A": "The measure of household income contains random measurement error that is uncorrelated with occupation.", "B": "Occupation is a very strong predictor of a household's permanent income and income stability.", "C": "Households with higher education levels tend to have both higher income and a lower preference for liquid assets.", "D": "Farmers have a systematic need to hold large grain stocks as a productive input (e.g., for seed), independent of their income risk."}, "Answer": "D", "pi_justification": "This item assesses the ability to critique the paper's identification strategy. It uses a Scenario Application strategy. The correct answer (B) describes a direct channel through which occupation affects the portfolio outcome variable (liquid wealth share) for reasons other than income, thus violating the exclusion restriction. Distractors are designed to be plausible but incorrect: (A) describes a condition required for the instrument to be strong, not a violation (True but Irrelevant); (C) describes a standard confounding variable that is handled by including education as a control in the second stage (True but Irrelevant); (D) describes a measurement error problem that does not violate this specific assumption (Conceptual Opposite).", "qid": "44", "question": "### Background\n\nThe paper uses a two-stage estimation procedure. In the first stage, an income process is estimated to generate measures of permanent income and income risk. In the second stage, these generated variables are used to predict household portfolio choice (e.g., the share of wealth held in liquid assets).\n\nTo identify the causal effect of risk, the model relies on an exclusion restriction: occupation dummies are included as predictors in the first-stage income regression but are excluded from the second-stage portfolio choice regression.\n\n### Question\n\nWhich of the following scenarios, if true, would most directly violate the key identifying assumption of this research design?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 48, "Question": "## Background\n\n**Research Question.** This problem examines the causal validity of a treatment effect estimate from a field experiment.\n\n**Setting / Institutional Environment.** A study estimates the causal effect of social information on donation amounts using a linear model:\n\n```latex\n\\text{PledgeAmount}_i = \\alpha + \\beta_{300} T_{i,300} + X_i'\\gamma + \\varepsilon_i\n\\quad \\text{(Eq. (1))}\n```\n\nwhere $T_{i,300}$ is an indicator for assignment to the `$300` treatment condition.\n\n---\n\n## Scenario\n\nSuppose the randomization protocol failed. Specifically, assume that experimenters, contrary to instructions, were more likely to assign callers who sounded enthusiastic and wealthy (an unobserved characteristic, `HighPropensity`) to the `$300` treatment group. By definition, these callers are predisposed to making larger donations regardless of treatment.\n\n---\n\n## Question\n\nUnder this scenario of failed randomization, how is the OLS estimate of the treatment effect, $\\hat{\\beta}_{300}$, biased?", "Options": {"A": "It will be biased downwards, understating the true causal effect.", "B": "It will be unbiased, because OLS is the best linear unbiased estimator.", "C": "It will be biased, but the direction is ambiguous without knowing the sign of the error term.", "D": "It will be biased upwards, overstating the true causal effect."}, "Answer": "D", "pi_justification": "This item assesses the ability to apply the omitted variable bias (OVB) framework. It uses a 'Scenario Application' strategy. The correct answer (A) requires identifying the two conditions for OVB: the omitted variable (`HighPropensity`) is correlated with the outcome (positively, by definition) and correlated with the treatment variable (positively, by the scenario). The product of these two positive effects creates a positive bias. Distractors target common errors: (B) is the 'Conceptual Opposite'; (C) misapplies the Gauss-Markov theorem, which requires the assumption of zero conditional mean (violated here); (D) incorrectly claims ambiguity when the direction is clearly identified by the scenario's premises.", "qid": "48", "question": "## Background\n\n**Research Question.** This problem examines the causal validity of a treatment effect estimate from a field experiment.\n\n**Setting / Institutional Environment.** A study estimates the causal effect of social information on donation amounts using a linear model:\n\n```latex\n\\text{PledgeAmount}_i = \\alpha + \\beta_{300} T_{i,300} + X_i'\\gamma + \\varepsilon_i\n\\quad \\text{(Eq. (1))}\n```\n\nwhere $T_{i,300}$ is an indicator for assignment to the `$300` treatment condition.\n\n---\n\n## Scenario\n\nSuppose the randomization protocol failed. Specifically, assume that experimenters, contrary to instructions, were more likely to assign callers who sounded enthusiastic and wealthy (an unobserved characteristic, `HighPropensity`) to the `$300` treatment group. By definition, these callers are predisposed to making larger donations regardless of treatment.\n\n---\n\n## Question\n\nUnder this scenario of failed randomization, how is the OLS estimate of the treatment effect, $\\hat{\\beta}_{300}$, biased?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 51, "Question": "### Background\n\nIn modeling family labor supply, a key challenge is that many women do not participate in the labor market, creating a sample of working women that is not random. The paper estimates its model on a sample of households where the wife works positive hours.\n\n### The Question\n\nAn analyst estimates a family labor supply model using data only from households where the wife works positive hours. Standard regression methods (e.g., Seemingly Unrelated Regressions) applied to this selected sample yield biased and inconsistent estimates for the male labor supply equation. What is the fundamental econometric reason for this bias?", "Options": {"A": "The wife's participation decision is an omitted variable in the husband's labor supply equation.", "B": "The smaller sample size leads to less precise estimates and a loss of statistical power.", "C": "Unobserved factors affecting the wife's participation are correlated with unobserved determinants of the husband's labor supply, inducing a correlation between the model's regressors and its error term.", "D": "The sample of working wives is not representative of the general population of all wives, violating the assumption of random sampling."}, "Answer": "C", "pi_justification": "This question assesses the core concept of sample selection bias. It uses a Reverse-Reasoning strategy, asking for the fundamental cause of a known problem. Distractor A is 'Almost Right' but lacks the crucial link to the error term. Distractor B confuses inconsistency with inefficiency. Distractor C misidentifies the problem as omitted variable bias.", "qid": "51", "question": "### Background\n\nIn modeling family labor supply, a key challenge is that many women do not participate in the labor market, creating a sample of working women that is not random. The paper estimates its model on a sample of households where the wife works positive hours.\n\n### The Question\n\nAn analyst estimates a family labor supply model using data only from households where the wife works positive hours. Standard regression methods (e.g., Seemingly Unrelated Regressions) applied to this selected sample yield biased and inconsistent estimates for the male labor supply equation. What is the fundamental econometric reason for this bias?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 55, "Question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\n### Question\n\nConsider two states facing identical, very strong rebel groups. State A has a **credibility crisis** (very low `h`, high `c`); its promises are worthless, but it is efficient at implementing policies. State B has an **implementation crisis** (high `h`, very low `c`); its promises are credible, but its bureaucracy is ineffective. According to the model, which state is more likely to experience a civil war, and why?\n", "Options": {"A": "State B, because its high commitment capacity (`h`) would require the government to make costly promises it would rather avoid by fighting.", "B": "State B, because its low implementation capacity (`c`) makes the peaceful consolidation process too slow and unreliable, forcing the government to fight.", "C": "State A, because its low commitment capacity (`h`) creates a severe 'liquidity problem,' making it unable to make a credible offer large enough to buy out a strong rebel group.", "D": "State A, because its high implementation capacity (`c`) makes the government overconfident in its ability to win a war."}, "Answer": "C", "pi_justification": "This item assesses the ability to distinguish between two forms of state capacity and apply them to a comparative scenario. It uses a Scenario Application strategy. The correct option (C) requires identifying that commitment capacity (`h`) is the binding constraint when dealing with strong rebels who require large payoffs. Distractors are designed to test common points of confusion: (A) over-weighting the importance of implementation speed, (B) introducing an outside behavioral logic, and (D) misstating the government's preferences.", "qid": "55", "question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\n### Question\n\nConsider two states facing identical, very strong rebel groups. State A has a **credibility crisis** (very low `h`, high `c`); its promises are worthless, but it is efficient at implementing policies. State B has an **implementation crisis** (high `h`, very low `c`); its promises are credible, but its bureaucracy is ineffective. According to the model, which state is more likely to experience a civil war, and why?\n", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 56, "Question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\nIn the model, large 'contingent spoils' (`γ`) that are only realized after peace is achieved create a high 'cost of delay' for any peaceful settlement, making war more likely.\n\n### Question\n\nAn international actor wants to prevent a civil war in a country where the primary driver of conflict is the existence of massive, easily accessible oil revenues that will become available only once the state is stable (i.e., `γ` is extremely large). Which of the following interventions is likely to be most effective at preventing conflict, according to the model's logic?\n", "Options": {"A": "A 'Good Governance' program to increase implementation capacity (`c`), because it directly speeds up the multi-period buyout process, reducing the delay in realizing the spoils.", "B": "A 'Good Governance' program to increase implementation capacity (`c`), because with large spoils at stake, the government needs certainty that its consolidation efforts will succeed.", "C": "A 'Rule of Law' program to increase commitment capacity (`h`), because it makes the government appear more trustworthy to international partners.", "D": "A 'Rule of Law' program to increase commitment capacity (`h`), because it can solve the government's 'liquidity problem,' potentially allowing a one-period buyout that eliminates the costly delay."}, "Answer": "D", "pi_justification": "This item tests deep application of the model's logic to a policy problem. It uses a Reverse-Reasoning strategy: given that the 'cost of delay' is the key problem, the student must identify the most effective solution. The correct option (D) recognizes that increasing commitment capacity (`h`) offers a more fundamental solution by enabling a single-period settlement, which is superior to marginally speeding up a multi-period one (the effect of `c`). Distractors represent plausible but less effective solutions (A, C) or reasoning external to the model (B).", "qid": "56", "question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\nIn the model, large 'contingent spoils' (`γ`) that are only realized after peace is achieved create a high 'cost of delay' for any peaceful settlement, making war more likely.\n\n### Question\n\nAn international actor wants to prevent a civil war in a country where the primary driver of conflict is the existence of massive, easily accessible oil revenues that will become available only once the state is stable (i.e., `γ` is extremely large). Which of the following interventions is likely to be most effective at preventing conflict, according to the model's logic?\n", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 57, "Question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nA temporary, unexpected fiscal expansion occurs in the home country today (`dG_0 > 0`). This policy is transmitted internationally, causing a fall in foreign private consumption. What is the primary economic mechanism driving this negative transmission?", "Options": {"A": "The expansion creates an excess demand for current goods, raising the world interest rate and reducing the present value of wealth in both countries.", "B": "The expansion creates an excess supply of current goods, lowering the world interest rate and crowding in foreign investment.", "C": "The expansion worsens the foreign country's terms of trade, reducing the value of its exports.", "D": "The expansion is funded by taxes on the home country, which directly reduces foreign income through trade linkages."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses understanding of the transmission mechanism for a current transitory fiscal shock using a Reverse-Reasoning strategy. The correct answer (A) identifies the key role of the interest rate in clearing the goods market. Distractor (B) presents the opposite mechanism. Distractor (C) incorrectly applies a concept from the two-good model, testing for model confusion. Distractor (D) offers a vague, non-model-based explanation.", "qid": "57", "question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nA temporary, unexpected fiscal expansion occurs in the home country today (`dG_0 > 0`). This policy is transmitted internationally, causing a fall in foreign private consumption. What is the primary economic mechanism driving this negative transmission?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 49, "Question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium', where no `M-R_i` pair has an incentive to secretly deviate from the equilibrium contracts.\n\n**Variables & Parameters.**\n\n*   `T'_{i*}`: The marginal transfer price for retailer `i` at equilibrium.\n*   `(p^I, e^I)`: The prices and efforts that maximize total industry profit (the 'fully integrated' outcome).\n*   `C'_i(e_i)`: The marginal cost of effort for retailer `i`.\n*   `D_i(p, e)`: Demand for retailer `i`'s product.\n\n### Data / Model Specification\n\nIn the competition stage, each retailer `i` chooses its effort `e_i` to maximize its own profit. The first-order condition for this choice is:\n\n```latex\n\\big(p_{i}^{*}-T_{i}^{*\\prime}\\big)\\frac{\\partial D_{i}}{\\partial e_{i}}-C_{i}^{\\prime}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\nFor any positive effort `e_i > 0`, the marginal cost of effort `C'_i` is strictly positive.\n\n### Question\n\nIn this model, why is it impossible for the manufacturer to use secret bilateral contracts to achieve the fully integrated profit outcome `(p^I, e^I)`?", "Options": {"A": "To incentivize positive sales effort, a retailer must earn a positive margin (`p_i - T'_{i*} > 0`), but to eliminate the opportunism problem, the manufacturer must set the margin to zero.", "B": "The manufacturer cannot commit to charging a high enough marginal transfer price `T'_{i*}` to achieve the integrated outcome because retailers would refuse such an unprofitable contract.", "C": "The non-contractibility of effort means the manufacturer cannot directly specify `e_i` in the contract, making it impossible to enforce the integrated effort level.", "D": "Retailers have an incentive to exert too much effort to steal market share, which the manufacturer cannot prevent with bilateral contracts."}, "Answer": "A", "pi_justification": "This item assesses the core economic conflict that prevents the manufacturer from achieving the first-best outcome, which was the subject of the first part of the original QA problem. It uses a Reverse-Reasoning strategy. The correct answer (A) identifies the fundamental tension between providing margins to incentivize effort and eliminating margins to prevent opportunism. Distractor (B) is a true precondition but not the direct cause of the conflict. Distractor (C) posits the opposite problem (too much effort). Distractor (D) misidentifies the reason for failure, focusing on retailer participation rather than the manufacturer's own conflicting incentives.", "qid": "49", "question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium', where no `M-R_i` pair has an incentive to secretly deviate from the equilibrium contracts.\n\n**Variables & Parameters.**\n\n*   `T'_{i*}`: The marginal transfer price for retailer `i` at equilibrium.\n*   `(p^I, e^I)`: The prices and efforts that maximize total industry profit (the 'fully integrated' outcome).\n*   `C'_i(e_i)`: The marginal cost of effort for retailer `i`.\n*   `D_i(p, e)`: Demand for retailer `i`'s product.\n\n### Data / Model Specification\n\nIn the competition stage, each retailer `i` chooses its effort `e_i` to maximize its own profit. The first-order condition for this choice is:\n\n```latex\n\\big(p_{i}^{*}-T_{i}^{*\\prime}\\big)\\frac{\\partial D_{i}}{\\partial e_{i}}-C_{i}^{\\prime}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\nFor any positive effort `e_i > 0`, the marginal cost of effort `C'_i` is strictly positive.\n\n### Question\n\nIn this model, why is it impossible for the manufacturer to use secret bilateral contracts to achieve the fully integrated profit outcome `(p^I, e^I)`?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 53, "Question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. The rebel faction's decision to accept a peaceful proposal is governed by its Peaceful Participation Constraint (PPC):\n\n```latex\nz_{k}+\\beta V_{R}(s_{k+1}) \\geq f_{R}+\\beta d_{k}p_{k}(1+\\gamma)V +\\beta(1-d_{k})[(1-\\varepsilon)V_{R}(s_{k})+\\varepsilon V_{R}(s_{k+1})] \\quad \\text{(Eq. (1))}\n```\n\nwhere `V_R(s)` is the rebel's continuation payoff from state `s`, and `ε` is the probability G's consolidation succeeds despite inconclusive fighting. The government (G) has **coercive power** when `d_k < 1` and `ε > 0`, which allows it to relax the PPC by choosing a lower `V_R(s_{k+1})`.\n\n### Question\n\nAssuming the government has coercive power and seeks a peaceful settlement, what is its optimal strategy to maximize its own payoff?\n", "Options": {"A": "Offer the largest possible current transfer (`z_k=1`) to induce the rebels to accept the weakest possible future position (the lowest `V_R(s_{k+1})`).", "B": "Propose a future state `s_{k+1}` where fighting is most decisive (`d_{k+1}=1`) to create efficiency gains for both parties.", "C": "Propose the strongest possible future for the rebels (highest `V_R(s_{k+1})`) to minimize the required current transfer `z_k`.", "D": "Offer the smallest possible current transfer (`z_k`) that satisfies the rebels' participation constraint, regardless of the future state `s_{k+1}`."}, "Answer": "A", "pi_justification": "This item assesses the core mechanism of 'coercive power' during peaceful negotiations. It uses a Computational Judgment strategy, requiring students to interpret the Peaceful Participation Constraint to deduce the optimal government strategy. The correct option (A) identifies the trade-off between current transfers and future rebel weakness. Distractors are designed based on common errors: (B) a myopic focus on minimizing the current transfer, (C) misinterpreting the government's objective function, and (D) confusing the peaceful buyout logic with the 'truce' logic from another part of the model.", "qid": "53", "question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. The rebel faction's decision to accept a peaceful proposal is governed by its Peaceful Participation Constraint (PPC):\n\n```latex\nz_{k}+\\beta V_{R}(s_{k+1}) \\geq f_{R}+\\beta d_{k}p_{k}(1+\\gamma)V +\\beta(1-d_{k})[(1-\\varepsilon)V_{R}(s_{k})+\\varepsilon V_{R}(s_{k+1})] \\quad \\text{(Eq. (1))}\n```\n\nwhere `V_R(s)` is the rebel's continuation payoff from state `s`, and `ε` is the probability G's consolidation succeeds despite inconclusive fighting. The government (G) has **coercive power** when `d_k < 1` and `ε > 0`, which allows it to relax the PPC by choosing a lower `V_R(s_{k+1})`.\n\n### Question\n\nAssuming the government has coercive power and seeks a peaceful settlement, what is its optimal strategy to maximize its own payoff?\n", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 58, "Question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nThe home country's government announces today that it will undertake a temporary fiscal expansion in a future period `s > 0` (`dG_s > 0`). What is the immediate effect on the *current* consumption of the foreign private sector (`c_0^*`), and why?", "Options": {"A": "`c_0^*` decreases, because the announcement raises the interest rates linking the present to the future, lowering the present value of foreign wealth.", "B": "`c_0^*` increases, because the announcement lowers the interest rates linking the present to the future, raising the present value of the foreign country's income stream.", "C": "`c_0^*` increases, because foreign consumers save more today in anticipation of higher future taxes in their own country.", "D": "`c_0^*` is unchanged, because the fiscal expansion has not yet occurred and therefore has no effect on current variables."}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item tests the core concept of how anticipated future policies affect current outcomes, a key contribution of the paper. It uses a Scenario Application strategy. The correct answer (A) captures the wealth revaluation mechanism. Distractor (B) reverses the logic. Distractor (C) represents a naive view that ignores rational expectations, a key assumption of the model. Distractor (D) provides a plausible-sounding but incorrect savings story.", "qid": "58", "question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nThe home country's government announces today that it will undertake a temporary fiscal expansion in a future period `s > 0` (`dG_s > 0`). What is the immediate effect on the *current* consumption of the foreign private sector (`c_0^*`), and why?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 54, "Question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. If the rebels fight and the fighting is inconclusive (which happens with probability `1-d_k`), the government's consolidation effort to move to state `s_{k+1}` still succeeds with probability `ε`.\n\nThe government (G) has **coercive power** when `d_k < 1` and `ε > 0`.\n\n### Question\n\nSuppose the government makes a proposal `(z_k, s_{k+1})` that it knows the rebels will reject, triggering a fight. How should the government choose the `s_{k+1}` component of this rejected proposal to maximize its own expected payoff from the conflict?\n", "Options": {"A": "Choose `s_{k+1} = E` (the monopoly state) to signal its resolve and deter future opposition.", "B": "Choose `s_{k+1} = E` (the monopoly state) to maximize its own continuation value `V_G(s_{k+1})` in the event that fighting is inconclusive but its consolidation efforts succeed.", "C": "Choose `s_{k+1} = s_k` (no change), because the proposal is strategically irrelevant once the rebels have decided to fight.", "D": "Choose the `s_{k+1}` that would have required the largest peaceful transfer `z_k`, to show the rebels the value of the deal they are rejecting."}, "Answer": "B", "pi_justification": "This item tests a critical pivot in the model's logic: how the government's objective changes when moving from a peace negotiation to a violent conflict. It uses a Scenario Application strategy. The correct option (B) requires understanding that the government's choice of `s_{k+1}` now aims to maximize its own potential payoff, not influence the rebels' choice. Distractors are based on plausible but incorrect reasoning: (A) a behavioral explanation instead of the model's rationalist one, (C) misapplication of the peace-time formula, and (D) incorrectly assuming the proposal becomes completely irrelevant.", "qid": "54", "question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. If the rebels fight and the fighting is inconclusive (which happens with probability `1-d_k`), the government's consolidation effort to move to state `s_{k+1}` still succeeds with probability `ε`.\n\nThe government (G) has **coercive power** when `d_k < 1` and `ε > 0`.\n\n### Question\n\nSuppose the government makes a proposal `(z_k, s_{k+1})` that it knows the rebels will reject, triggering a fight. How should the government choose the `s_{k+1}` component of this rejected proposal to maximize its own expected payoff from the conflict?\n", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 61, "Question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nIn the numerical example, the vertical merger causes two key changes shown in Table 1: the integrated firm's punishment payoff increases from 0 to 10, and an unintegrated firm's cheating payoff decreases from 100 to 80. Which statement correctly identifies the economic effects driving these changes?", "Options": {"A": "The increase to 10 is the punishment effect; the decrease to 80 is the outlets effect.", "B": "The increase to 10 is the punishment effect; the decrease to 80 is the reaction effect.", "C": "Both changes are caused by the outlets effect, as the integrated firm is shielded from competition in all phases of the game.", "D": "The increase to 10 is the outlets effect; the decrease to 80 is the punishment effect."}, "Answer": "A", "pi_justification": "This item tests the core conceptual distinction between the 'outlets effect' and the 'punishment effect'. It uses a Reverse-Reasoning strategy, asking the user to map outcomes to causes. Distractors include a direct conceptual swap (A), conflating the two effects (B), and introducing a true but irrelevant concept from a different model (C).", "qid": "61", "question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nIn the numerical example, the vertical merger causes two key changes shown in Table 1: the integrated firm's punishment payoff increases from 0 to 10, and an unintegrated firm's cheating payoff decreases from 100 to 80. Which statement correctly identifies the economic effects driving these changes?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 35, "Question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to one of three groups: `Control`, `Price-Only`, and `Price+IHD`. The `Price+IHD` group was intended to receive an in-home display (IHD) providing real-time usage information. However, due to scheduling and other issues, there was significant non-compliance. While compliance was high in other groups, only 72% of households assigned to the `Price+IHD` group completed the study and had a functioning IHD installed.\n\nResearchers estimated the Intent-to-Treat (ITT) effect, which measures the causal effect of being *assigned* to a treatment group, regardless of compliance.\n\n### Data / Model Specification\n\nThe table below shows the ITT estimates from a difference-in-differences model with household and hour-by-day fixed effects. The dependent variable is the natural log of electricity usage, `ln(kWh)`.\n\n**Table 1: ITT Treatment Effects on ln(kWh) for All Events**\n\n| | Coefficient (Std. Err.) |\n| :--- | :--- |\n| Price-only | -0.038 (0.036) |\n| Price + IHD | -0.137*** (0.046) |\n\n*Notes: *** p<0.01.*\n\n---\n\n### Question\n\nGiven the results in Table 1 and the context of 72% compliance in the `Price+IHD` group, which statement most accurately interprets the Intent-to-Treat (ITT) estimate and its policy relevance?", "Options": {"A": "The ITT estimate of -0.137 is biased due to the 28% non-compliance rate and therefore does not represent a valid causal effect of the program.", "B": "The ITT estimate of -0.137 shows the causal effect of the IHD on the 72% of households who complied, indicating that the device itself caused a 13.7% usage reduction for its users.", "C": "The ITT estimate of -0.137 represents the average effect of the program on all households *assigned* to the `Price+IHD` group, making it the relevant parameter for a policymaker considering a large-scale rollout where similar non-compliance is expected.", "D": "The ITT estimate of -0.137 shows that being assigned to the `Price+IHD` group caused a 13.7 percentage point decrease in electricity usage, answering the question of the program's effect under perfect compliance."}, "Answer": "C", "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.0). It assesses the core concept of interpreting an Intent-to-Treat (ITT) estimate in the presence of non-compliance. The strategy is conceptual interpretation. The correct answer (C) accurately defines ITT and its policy relevance. Distractors are designed based on common misconceptions: (A) incorrectly describes the Treatment-on-the-Treated (ToT) effect; (B) incorrectly claims ITT is biased by non-compliance (it is not biased as an estimate of the effect of *assignment*); (D) uses incorrect units ('percentage point') for a log-level model and incorrectly describes the policy question.", "qid": "35", "question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to one of three groups: `Control`, `Price-Only`, and `Price+IHD`. The `Price+IHD` group was intended to receive an in-home display (IHD) providing real-time usage information. However, due to scheduling and other issues, there was significant non-compliance. While compliance was high in other groups, only 72% of households assigned to the `Price+IHD` group completed the study and had a functioning IHD installed.\n\nResearchers estimated the Intent-to-Treat (ITT) effect, which measures the causal effect of being *assigned* to a treatment group, regardless of compliance.\n\n### Data / Model Specification\n\nThe table below shows the ITT estimates from a difference-in-differences model with household and hour-by-day fixed effects. The dependent variable is the natural log of electricity usage, `ln(kWh)`.\n\n**Table 1: ITT Treatment Effects on ln(kWh) for All Events**\n\n| | Coefficient (Std. Err.) |\n| :--- | :--- |\n| Price-only | -0.038 (0.036) |\n| Price + IHD | -0.137*** (0.046) |\n\n*Notes: *** p<0.01.*\n\n---\n\n### Question\n\nGiven the results in Table 1 and the context of 72% compliance in the `Price+IHD` group, which statement most accurately interprets the Intent-to-Treat (ITT) estimate and its policy relevance?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 59, "Question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nAssume the foreign country is more patient than the home country (`δ* > δ`). The home government announces a future temporary fiscal *cut* (`dG_s < 0`). This announcement has a primary effect of raising home wealth and lowering foreign wealth. What is the secondary effect of this wealth redistribution on the interest rate `r_{t-1}` for a period `t` far from `s`?", "Options": {"A": "The interest rate `r_{t-1}` will decrease, because wealth is transferred from a high-saving country to a low-saving country, creating an incipient surplus of world savings.", "B": "The interest rate `r_{t-1}` will not change, as the shock is specific to period `s` and does not affect other periods.", "C": "The interest rate `r_{t-1}` will increase, because wealth is transferred from a high-saving country to a low-saving country, creating an incipient shortage of world savings.", "D": "The interest rate `r_{t-1}` will decrease, because the fiscal cut signals future abundance, which lowers the return to saving."}, "Answer": "C", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the subtle secondary effect of a future policy announcement on the term structure of interest rates, using a Scenario Application strategy. The correct answer (A) requires a two-step reasoning process: identify the direction of wealth transfer and then determine its impact on world savings given the saving propensities. Distractor (B) offers a plausible but incorrect narrative. Distractor (C) makes a sign error in the second step of the reasoning. Distractor (D) fails to recognize the full general equilibrium effects in an integrated capital market.", "qid": "59", "question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nAssume the foreign country is more patient than the home country (`δ* > δ`). The home government announces a future temporary fiscal *cut* (`dG_s < 0`). This announcement has a primary effect of raising home wealth and lowering foreign wealth. What is the secondary effect of this wealth redistribution on the interest rate `r_{t-1}` for a period `t` far from `s`?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 52, "Question": "### Background\n\nTo address sample selection bias from female non-participation, the authors use a Full Information Maximum Likelihood (FIML) estimator. The likelihood function for the truncated sample of households with working wives is:\n```latex\nL = \\prod_{h=1}^{H} \\frac{\\phi(u_{fh}, u_{mh})}{\\operatorname{Pr}(l_{fh} < T_f)}\n```\nwhere `φ(u_fh, u_mh)` is the joint probability density of the errors, assumed to be bivariate normal. The denominator is the probability of being selected into the sample.\n\n### The Question\n\nA critic argues that the unobserved preference for female non-market time might be skewed, violating the FIML estimator's joint normality assumption. If the critic is correct, what is the primary consequence of this violation, and what is the most appropriate econometric response to assess the model's robustness?", "Options": {"A": "Consequence: Inconsistent estimates. Response: Find a valid instrumental variable for the wife's participation decision.", "B": "Consequence: Inefficient but still consistent estimates. Response: Use robust standard errors.", "C": "Consequence: Biased estimates for the female equation only. Response: Use a different parametric distribution like Logit instead of Probit for the selection stage.", "D": "Consequence: Inconsistent estimates for all model parameters. Response: Re-estimate using a semi-parametric control function method that does not rely on a specific distributional assumption."}, "Answer": "D", "pi_justification": "This question tests advanced understanding of the limitations of FIML selection models. It uses a Scenario Application strategy. The correct option correctly identifies both the consequence (inconsistency, not inefficiency) and the state-of-the-art solution (semi-parametric methods). Distractor A confuses inconsistency with inefficiency. Distractor B confuses selection with standard endogeneity. Distractor C incorrectly localizes the problem and proposes an insufficient fix.", "qid": "52", "question": "### Background\n\nTo address sample selection bias from female non-participation, the authors use a Full Information Maximum Likelihood (FIML) estimator. The likelihood function for the truncated sample of households with working wives is:\n```latex\nL = \\prod_{h=1}^{H} \\frac{\\phi(u_{fh}, u_{mh})}{\\operatorname{Pr}(l_{fh} < T_f)}\n```\nwhere `φ(u_fh, u_mh)` is the joint probability density of the errors, assumed to be bivariate normal. The denominator is the probability of being selected into the sample.\n\n### The Question\n\nA critic argues that the unobserved preference for female non-market time might be skewed, violating the FIML estimator's joint normality assumption. If the critic is correct, what is the primary consequence of this violation, and what is the most appropriate econometric response to assess the model's robustness?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 62, "Question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\nThe incentive to cheat is defined as the ratio of the one-time gain from cheating to the per-period profit foregone by triggering punishment:\n\n```latex\n\\text{Incentive to Cheat} = \\frac{\\pi_{\\text{cheat}} - \\pi_{\\text{collude}}}{\\pi_{\\text{collude}} - \\pi_{\\text{punish}}} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n\n### The Question\n\nSuppose a policy intervention intensifies downstream competition, causing the noncooperative profit of each downstream firm (`π^NC`) to fall from 10 to 5. This reduces the 'Payoff if punished' for the integrated firm to 5. Assume the integrated firm's collusive payoff remains 30. What is the new 'Incentive to Cheat' for the integrated firm, and what is the implication for antitrust?", "Options": {"A": "14.0; the merger becomes a greater antitrust concern.", "B": "2.8; the merger becomes a greater antitrust concern.", "C": "3.5; there is no change in antitrust concern.", "D": "2.8; the merger becomes less of an antitrust concern."}, "Answer": "B", "pi_justification": "This item assesses the ability to apply the model to a novel scenario, combining calculation with judgment. It uses a Scenario Application strategy. Distractors are based on a correct calculation but incorrect interpretation (B), a failure to update a parameter (C), and a fundamental misuse of the formula (D).", "qid": "62", "question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\nThe incentive to cheat is defined as the ratio of the one-time gain from cheating to the per-period profit foregone by triggering punishment:\n\n```latex\n\\text{Incentive to Cheat} = \\frac{\\pi_{\\text{cheat}} - \\pi_{\\text{collude}}}{\\pi_{\\text{collude}} - \\pi_{\\text{punish}}} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n\n### The Question\n\nSuppose a policy intervention intensifies downstream competition, causing the noncooperative profit of each downstream firm (`π^NC`) to fall from 10 to 5. This reduces the 'Payoff if punished' for the integrated firm to 5. Assume the integrated firm's collusive payoff remains 30. What is the new 'Incentive to Cheat' for the integrated firm, and what is the implication for antitrust?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 63, "Question": "### Background\nIn a generalized Cournot oligopoly model, the dynamic evolution of the output vector `x_t` is governed by a discrete-time system `x_t = B_n x_{t-1} + \\text{constant}`. The stability of this system depends on the eigenvalues of the transition matrix `B_n`.\n\n### Data / Model Specification\nAssume the following facts derived from the model:\n1.  The eigenvalues of `B_n` are related to the eigenvalues of another matrix `K A_n` (denoted `\\lambda_i`) by the transformation: `eig(B_n)_i = 1 + \\lambda_i`.\n2.  All eigenvalues `\\lambda_i` of `K A_n` are real and negative.\n3.  For the special case where all firms have the same adjustment speed, `K_i = K`, the most negative eigenvalue of `K A_n` is given by:\n    ```latex\n    \\lambda_n = -K \\left( \\frac{(n+1)b+d}{2b+d} \\right)\n    ```\n    where `n` is the number of firms, `b > 0` is related to the demand slope, and `d` is the slope of the firms' marginal cost curves.\n\n### Question\nA discrete linear dynamic system is stable if and only if all eigenvalues of its transition matrix have an absolute value less than 1. Given the information above, what is the necessary and sufficient condition for stability in this model?", "Options": {"A": "```latex\nK > 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```", "B": "```latex\nK < \\frac{2b+d}{(n+1)b+d}\n```", "C": "```latex\nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```", "D": "```latex\nK < 2 \\left( \\frac{(n+1)b+d}{2b+d} \\right)\n```"}, "Answer": "C", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item uses the 'Computational Judgment' strategy. It tests the ability to combine the abstract stability rule (`|1+\\lambda| < 1`) with the specific properties of `\\lambda` (real, negative) to derive the crucial intermediate condition `\\lambda > -2`, and then correctly perform the algebraic manipulation on the formula for `\\lambda_n` to arrive at the final stability condition. Distractors are based on common, high-frequency errors: (B) is a sign error from failing to flip the inequality; (C) is a formula misuse error from incorrectly isolating K; (D) is a step-omission error from forgetting the factor of 2.", "qid": "63", "question": "### Background\nIn a generalized Cournot oligopoly model, the dynamic evolution of the output vector `x_t` is governed by a discrete-time system `x_t = B_n x_{t-1} + \\text{constant}`. The stability of this system depends on the eigenvalues of the transition matrix `B_n`.\n\n### Data / Model Specification\nAssume the following facts derived from the model:\n1.  The eigenvalues of `B_n` are related to the eigenvalues of another matrix `K A_n` (denoted `\\lambda_i`) by the transformation: `eig(B_n)_i = 1 + \\lambda_i`.\n2.  All eigenvalues `\\lambda_i` of `K A_n` are real and negative.\n3.  For the special case where all firms have the same adjustment speed, `K_i = K`, the most negative eigenvalue of `K A_n` is given by:\n    ```latex\n    \\lambda_n = -K \\left( \\frac{(n+1)b+d}{2b+d} \\right)\n    ```\n    where `n` is the number of firms, `b > 0` is related to the demand slope, and `d` is the slope of the firms' marginal cost curves.\n\n### Question\nA discrete linear dynamic system is stable if and only if all eigenvalues of its transition matrix have an absolute value less than 1. Given the information above, what is the necessary and sufficient condition for stability in this model?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 60, "Question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nAfter a single vertical merger, what is the maximum one-period 'Payoff if cheating' for one of the remaining unintegrated upstream firms, and what is the economic reason for this value?", "Options": {"A": "70, because the integrated firm's collusive share (30) is subtracted from the total monopoly profit.", "B": "100, because a deviating firm can still undercut its rivals' offers to capture the entire market.", "C": "90, because the integrated firm's downstream profit in the punishment phase (10) is subtracted from the total monopoly profit.", "D": "80, because the deviating firm cannot profitably sell to the integrated downstream firm, which represents one-fifth of the market."}, "Answer": "D", "pi_justification": "This item assesses the user's ability to apply the 'outlets effect' in a concrete calculation. It uses a Computational Judgment strategy. Distractors are designed based on common errors: ignoring the effect entirely (A), confusing punishment profit with deviation profit reduction (B), and using an irrelevant collusive value (D).", "qid": "60", "question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nAfter a single vertical merger, what is the maximum one-period 'Payoff if cheating' for one of the remaining unintegrated upstream firms, and what is the economic reason for this value?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 46, "Question": "## Background\n\n**Research Question.** This problem investigates the validity of the estimated long-term impact of social information on donor renewal.\n\n**Setting / Institutional Environment.** The analysis is a one-year follow-up of the *new donors* from an initial field experiment. The study tracks their donation behavior one year after the initial intervention. However, not all original participants could be tracked or responded in the follow-up year.\n\n---\n\n## Scenario\n\nSuppose the follow-up study suffers from non-random attrition. Specifically, assume that new donors who had a very positive experience with the radio station (an unobserved characteristic) are both more likely to be found and successfully contacted in the follow-up sample *and* more likely to renew their donation. Furthermore, assume that receiving the `$300` treatment itself enhances this positive experience, making these donors even easier to track and include in the follow-up analysis.\n\n---\n\n## Question\n\nGiven this scenario, how would the estimated treatment effect of the `$300` signal on the probability of renewal likely be biased?", "Options": {"A": "Unbiased, because the initial treatment assignment was random.", "B": "Biased upwards (overestimated).", "C": "The direction of the bias cannot be determined from the information given.", "D": "Biased downwards (underestimated)."}, "Answer": "B", "pi_justification": "This item assesses the student's ability to apply the concept of sample selection bias. It uses a 'Scenario Application' strategy. The correct answer requires recognizing that the treatment is correlated with selection into the sample, and selection is correlated with the outcome, leading to an upward bias. Distractors target common misconceptions: (B) is the 'Conceptual Opposite'; (C) is 'True but Irrelevant', as it correctly states a fact about the initial experiment but ignores that the bias arises from non-random *attrition* in the follow-up stage; (D) is incorrect because the premises provided are sufficient to sign the bias.", "qid": "46", "question": "## Background\n\n**Research Question.** This problem investigates the validity of the estimated long-term impact of social information on donor renewal.\n\n**Setting / Institutional Environment.** The analysis is a one-year follow-up of the *new donors* from an initial field experiment. The study tracks their donation behavior one year after the initial intervention. However, not all original participants could be tracked or responded in the follow-up year.\n\n---\n\n## Scenario\n\nSuppose the follow-up study suffers from non-random attrition. Specifically, assume that new donors who had a very positive experience with the radio station (an unobserved characteristic) are both more likely to be found and successfully contacted in the follow-up sample *and* more likely to renew their donation. Furthermore, assume that receiving the `$300` treatment itself enhances this positive experience, making these donors even easier to track and include in the follow-up analysis.\n\n---\n\n## Question\n\nGiven this scenario, how would the estimated treatment effect of the `$300` signal on the probability of renewal likely be biased?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 71, "Question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nA researcher uses the restricted test `Ȳ⁽²⁾` based on a strong prior that `θ₁=θ₂`. However, the true data generating process is `(θ₁, θ₂) = (-0.1, 0.0)`. According to Table 1, what is the approximate probability that the researcher commits a Type II error (fails to reject the false null `H₀: θ₁=θ₂=0`) at the 5% significance level?", "Options": {"A": "51.3%", "B": "39.8%", "C": "60.2%", "D": "91.8%"}, "Answer": "C", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to perform a precise quantitative calculation and interpretation from a data table. It uses a Computational Judgment strategy. The correct answer is 1 - 0.398 = 0.602. Distractor B is a classic Step-Omission Error, reporting the power instead of the Type II error probability (1 - power). Distractor C is a Formula Misuse error, correctly calculating the Type II error but for the wrong test (the unrestricted one: 1 - 0.487). Distractor D results from a Lookup Error, using data from the wrong row (`(0.0, -0.1)`) to calculate the Type II error (1 - 0.082).", "qid": "71", "question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nA researcher uses the restricted test `Ȳ⁽²⁾` based on a strong prior that `θ₁=θ₂`. However, the true data generating process is `(θ₁, θ₂) = (-0.1, 0.0)`. According to Table 1, what is the approximate probability that the researcher commits a Type II error (fails to reject the false null `H₀: θ₁=θ₂=0`) at the 5% significance level?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 73, "Question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n*   `a_{ij}`: The amount of factor `j` required to produce one unit of commodity `i`, chosen to minimize cost.\n*   `A`: The `n x n` matrix of input coefficients `a_{ij}`.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production:\n\n```latex\nq_{i} = \\sum_{j=1}^{n} a_{ij} p_{j}\n```\n\nA standard result from production theory (derived from the envelope theorem) states that at the cost-minimizing input choice:\n\n```latex\n\\sum_{j=1}^{n} p_{j} \\frac{\\partial a_{ij}}{\\partial p_{k}} = 0\n```\n\n---\n\n### The Questions\n\nGiven the cost function in Eq. (1) and the envelope theorem result in Eq. (2), what is the Jacobian matrix `J` of the mapping from factor prices `p` to commodity prices `q`, where the elements of the Jacobian are `J_{ik} = ∂q_i/∂p_k`?", "Options": {"A": "The matrix of input coefficients, `A`", "B": "The matrix `A + M`, where `M` is a matrix with elements `M_{ik} = Σ_j p_j (∂a_{ij}/∂p_k)`", "C": "The zero matrix, `0`", "D": "The transpose of the input matrix, `A^T`"}, "Answer": "A", "pi_justification": "This question tests the ability to perform a key mathematical derivation central to the paper's local analysis. It uses a Computational Judgment strategy. The correct answer (A) results from correctly applying the product rule and the provided envelope theorem identity in Eq. (2). Distractor (B) represents a common index-confusion error. Distractor (D) represents a 'Step-Omission Error,' where the student applies the product rule but fails to use the identity in Eq. (2) to cancel the second term. Distractor (C) represents a misunderstanding of the envelope theorem's application.", "qid": "73", "question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n*   `a_{ij}`: The amount of factor `j` required to produce one unit of commodity `i`, chosen to minimize cost.\n*   `A`: The `n x n` matrix of input coefficients `a_{ij}`.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production:\n\n```latex\nq_{i} = \\sum_{j=1}^{n} a_{ij} p_{j}\n```\n\nA standard result from production theory (derived from the envelope theorem) states that at the cost-minimizing input choice:\n\n```latex\n\\sum_{j=1}^{n} p_{j} \\frac{\\partial a_{ij}}{\\partial p_{k}} = 0\n```\n\n---\n\n### The Questions\n\nGiven the cost function in Eq. (1) and the envelope theorem result in Eq. (2), what is the Jacobian matrix `J` of the mapping from factor prices `p` to commodity prices `q`, where the elements of the Jacobian are `J_{ik} = ∂q_i/∂p_k`?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 68, "Question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nWhat is the primary identification problem that the repeat-sales methodology, which transforms Eq. (1) into Eq. (2), is designed to solve?", "Options": {"A": "It eliminates the influence of citywide economic shocks `δ_t` on individual house prices.", "B": "It corrects for measurement error in the reported sales price `p_{it}`.", "C": "It accounts for heteroskedasticity where price variance increases with the time between sales.", "D": "It controls for selection bias caused by changes in the mix of properties sold over time."}, "Answer": "D", "pi_justification": "This item assesses the student's understanding of the fundamental purpose of the repeat-sales (fixed effects) methodology, which is central to the paper. It uses a conceptual strategy. The correct answer (A) identifies selection bias as the key problem. Distractors are designed based on common confusions: (B) is a different econometric problem (True but Irrelevant), (C) refers to a secondary weighting step, not the core differencing transformation (Conceptual: Almost Right), and (D) is the conceptual opposite of the method's goal.", "qid": "68", "question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nWhat is the primary identification problem that the repeat-sales methodology, which transforms Eq. (1) into Eq. (2), is designed to solve?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 66, "Question": "### Background\n\nTo identify the causal impact of black in-migration on white population change, a study uses an instrumental variable (IV). The instrument interacts historical, pre-1900 settlement patterns of blacks from specific southern states with decade-by-decade out-migration shocks from those same states.\n\n### Question\n\nThe validity of this IV strategy rests on the exclusion restriction. Which of the following scenarios would most clearly violate this assumption?", "Options": {"A": "The overall economic boom of the 1920s increases population growth in all northern cities, an effect that is absorbed by the model's city-level fixed effects.", "B": "A severe boll weevil infestation in Georgia simultaneously pushes black agricultural workers to migrate north and disrupts the cotton supply to textile mills in northern neighborhoods where Georgian migrants historically settled, causing an independent economic decline in those specific neighborhoods.", "C": "Social networks from the 19th century weaken over time, so that by 1920, settlement patterns of new migrants from a given southern state are no longer correlated with the settlement patterns of migrants from that same state before 1900.", "D": "Newly arriving black migrants tend to settle in neighborhoods that are already experiencing population growth due to factors like new housing construction, which is the primary source of endogeneity the IV is designed to correct."}, "Answer": "B", "pi_justification": "Assessment Target: Understanding the exclusion restriction in a specific research context. Strategy: Scenario Application. Distractor Logic: The correct answer (A) describes a classic violation: the instrument (driven by the Georgia shock) has a direct effect on the outcome (white population change) through a channel other than the endogenous variable (a local economic shock). Distractor (B) describes a violation of the *relevance* assumption, not the exclusion restriction. Distractor (C) describes a scenario that is correctly handled by the model's fixed effects. Distractor (D) describes the endogeneity problem itself, not a failure of the proposed solution.", "qid": "66", "question": "### Background\n\nTo identify the causal impact of black in-migration on white population change, a study uses an instrumental variable (IV). The instrument interacts historical, pre-1900 settlement patterns of blacks from specific southern states with decade-by-decade out-migration shocks from those same states.\n\n### Question\n\nThe validity of this IV strategy rests on the exclusion restriction. Which of the following scenarios would most clearly violate this assumption?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 69, "Question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nSuppose that during a housing boom, homeowners frequently undertake unobserved major renovations (e.g., new kitchens) between the sale at time `t'` and the sale at time `t`. How would this behavior affect the price appreciation (`δ_t - δ_{t'}`) estimated using the repeat-sales model (Eq. 2)?", "Options": {"A": "The model would overstate true price appreciation because it would incorrectly attribute price gains from quality improvements to market-wide price growth.", "B": "The effect is ambiguous because some renovations add more value than others, making the direction of the bias impossible to determine without more data.", "C": "The model would understate true price appreciation because the cost of renovations is not factored into the sale price.", "D": "The estimate would be unbiased because the unobserved renovations are random and would be absorbed by the error term `ε_{it} - ε_{it'}` which has an expected value of zero."}, "Answer": "A", "pi_justification": "This item tests the student's ability to analyze a key threat to the validity of the repeat-sales identification strategy. It uses a Scenario Application strategy. The correct answer (A) correctly identifies the direction of the bias and the reason. Distractors are based on plausible errors: (B) introduces irrelevant cost reasoning (Formula Misuse), (C) incorrectly assumes the violation of the assumption away (Conceptual Opposite), and (D) offers a weak 'ambiguous' conclusion when a clear directional prediction can be made (Conceptual: Almost Right).", "qid": "69", "question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nSuppose that during a housing boom, homeowners frequently undertake unobserved major renovations (e.g., new kitchens) between the sale at time `t'` and the sale at time `t`. How would this behavior affect the price appreciation (`δ_t - δ_{t'}`) estimated using the repeat-sales model (Eq. 2)?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 64, "Question": "### Background\nThe stability of a generalized Cournot oligopoly model is determined by the condition:\n\n```latex\nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```\n\nwhere `K` is the adjustment speed, `d` is the slope of marginal cost, and `n` is the number of firms.\n\n### Question\nThe original Theocharis model is a special case of this generalized model, assuming constant marginal costs and instantaneous adjustment. What does the stability condition simplify to under the Theocharis assumptions (`d=0` and `K=1`)?", "Options": {"A": "The number of firms must be less than or equal to 3 (`n ≤ 3`).", "B": "The number of firms must be greater than 3 (`n > 3`).", "C": "The number of firms must be less than 2 (`n < 2`).", "D": "The number of firms must be less than 3 (`n < 3`)."}, "Answer": "D", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item uses the 'Computational Judgment' strategy. It tests the ability to apply the general formula to a specific, important limiting case discussed in the paper. The calculation is straightforward but requires careful algebraic simplification. The correct answer (A) recovers the famous Theocharis result. Distractors represent common errors: (B) is the condition for instability (a conceptual opposite); (C) incorrectly includes the boundary case of `n=3`, which is oscillatory, not stable; (D) would result from an algebraic slip.", "qid": "64", "question": "### Background\nThe stability of a generalized Cournot oligopoly model is determined by the condition:\n\n```latex\nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```\n\nwhere `K` is the adjustment speed, `d` is the slope of marginal cost, and `n` is the number of firms.\n\n### Question\nThe original Theocharis model is a special case of this generalized model, assuming constant marginal costs and instantaneous adjustment. What does the stability condition simplify to under the Theocharis assumptions (`d=0` and `K=1`)?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 67, "Question": "### Background\n\nAn instrumental variable (IV) strategy is used to estimate the causal effect of black in-migration (`\\Delta B`) on white population change (`\\Delta W`). A potential threat to this strategy is that the instrument is mechanically correlated with the number of black residents in a neighborhood in 1900. If neighborhoods with larger initial black populations had different pre-existing trends in white population change, this could bias the results. To test this, the model is re-estimated including the number of blacks in 1900 as a control variable.\n\n### Data / Model Specification\n\n**Table 1: IV Robustness Check for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) Baseline IV | (2) IV with Control |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -3.389*** | -2.910*** |\n| | (0.386) | (0.644) |\n| Number of blacks in 1900 | | -0.343 |\n| | | (0.358) |\n\n*Notes: Spatial standard errors in parentheses. Both regressions include city fixed effects.*\n\n### Question\n\nWhat does the comparison of the coefficient on `\\Delta B` between column (1) and column (2) imply about the validity of the IV strategy?", "Options": {"A": "The coefficient on `\\Delta B` decreases, indicating that the baseline IV estimate was biased and that the true causal effect is much smaller.", "B": "The standard error on `\\Delta B` increases substantially, which indicates that the instrument becomes weak once the control variable is included.", "C": "The coefficient on `\\Delta B` remains large, significant, and relatively stable, suggesting the main finding of substantial white flight is not primarily driven by confounding factors associated with the initial size of the black population.", "D": "The coefficient on `Number of blacks in 1900` is not statistically significant, proving that there was no correlation between the instrument and initial neighborhood characteristics."}, "Answer": "C", "pi_justification": "Assessment Target: Interpreting the result of a key robustness check for an IV model. Strategy: Reverse-Reasoning. Distractor Logic: The correct answer (A) accurately interprets the stability of the coefficient as a sign of robustness against this specific threat. Distractor (B) is an over-interpretation of the modest change; the effect is still very large and economically similar. Distractor (C) confuses an increase in standard error (expected with more controls and a smaller sample) with a weak instrument problem (which is assessed with the first-stage F-statistic). Distractor (D) misinterprets the purpose of the check; the goal is to see the effect on the main coefficient, not to test the significance of the control itself.", "qid": "67", "question": "### Background\n\nAn instrumental variable (IV) strategy is used to estimate the causal effect of black in-migration (`\\Delta B`) on white population change (`\\Delta W`). A potential threat to this strategy is that the instrument is mechanically correlated with the number of black residents in a neighborhood in 1900. If neighborhoods with larger initial black populations had different pre-existing trends in white population change, this could bias the results. To test this, the model is re-estimated including the number of blacks in 1900 as a control variable.\n\n### Data / Model Specification\n\n**Table 1: IV Robustness Check for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) Baseline IV | (2) IV with Control |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -3.389*** | -2.910*** |\n| | (0.386) | (0.644) |\n| Number of blacks in 1900 | | -0.343 |\n| | | (0.358) |\n\n*Notes: Spatial standard errors in parentheses. Both regressions include city fixed effects.*\n\n### Question\n\nWhat does the comparison of the coefficient on `\\Delta B` between column (1) and column (2) imply about the validity of the IV strategy?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 76, "Question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`. The distribution of tenant abilities is given by `g(e_i)`.\n\n### Question\n\nThe paper concludes that the welfare ranking of the two regimes is ambiguous and depends on the distribution of tenant abilities, `g(e_i)`. What is the fundamental reason for this ambiguity?", "Options": {"A": "Landlords face risk in the no-linking regime but not in the linking regime, and the model assumes they are risk-averse.", "B": "High-ability tenants earn informational rents in the linking regime, which are competed away in the no-linking regime.", "C": "The linking regime is always socially efficient, while the no-linking regime is always inefficient.", "D": "The linking regime's profit is determined by the marginal tenant's productivity, while the no-linking regime's expected profit depends on the average productivity of all tenants."}, "Answer": "D", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item targets the paper's core policy conclusion. It uses a Conceptual Judgment strategy to identify the key trade-off. The correct answer (A) pinpoints the 'marginal vs. average' mechanism. Distractors are designed to be plausible but incorrect: (B) is an overstatement (linking isn't always efficient) and doesn't explain the profit ambiguity; (C) is a Conceptual Error (model assumes risk neutrality); (D) is True but Irrelevant, as it describes tenant welfare, not the ambiguity in landlord profits which drives the overall conclusion.", "qid": "76", "question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`. The distribution of tenant abilities is given by `g(e_i)`.\n\n### Question\n\nThe paper concludes that the welfare ranking of the two regimes is ambiguous and depends on the distribution of tenant abilities, `g(e_i)`. What is the fundamental reason for this ambiguity?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 70, "Question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nBased on the results in Table 1, which statement best characterizes the trade-off between the restricted test (`Ȳ⁽²⁾`) and the unrestricted test (`Υ_W⁽²⁾`)?", "Options": {"A": "The unrestricted test is always more powerful than the restricted test because it does not impose false constraints.", "B": "The restricted test is more powerful when its assumption (`θ₁=θ₂`) holds true, but can be severely less powerful when the assumption is false.", "C": "The restricted test is more powerful when `θ₁=θ₂` and has only slightly less power when the assumption is false.", "D": "Since both tests have the correct empirical size, the choice between them is arbitrary and does not significantly affect inference."}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to interpret Monte Carlo evidence to understand the fundamental power-efficiency trade-off in hypothesis testing. It uses a Scenario Application strategy. Distractor B is a Conceptual Opposite, ignoring the efficiency gain of a correct restriction. Distractor C is an 'Almost Right' statement that understates the severe power loss documented in the table (e.g., for `θ₁=0.3, θ₂=-0.3`). Distractor D is 'True but Irrelevant,' as it correctly notes the size but incorrectly dismisses the large power differences.", "qid": "70", "question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nBased on the results in Table 1, which statement best characterizes the trade-off between the restricted test (`Ȳ⁽²⁾`) and the unrestricted test (`Υ_W⁽²⁾`)?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 72, "Question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `f_i(p)`: The cost function for commodity `i`, returning the minimum cost to produce one unit given factor price vector `p`.\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production, defining a mapping `f` from factor prices `p` to commodity prices `q`.\n\n---\n\n### The Questions\n\nIn the context of the Factor Price Equalization theorem, why is the existence of a unique global inverse for the cost function mapping `p` to `q` (i.e., `p = f^{-1}(q)`) a critical condition?", "Options": {"A": "It ensures that multiple factor price vectors can correspond to a single commodity price vector, allowing countries to specialize.", "B": "It proves that free trade in goods will lead to the equalization of commodity prices across countries.", "C": "It is a direct mathematical consequence of the concavity of the cost function, which is a fundamental property of cost minimization.", "D": "It implies that for any given vector of commodity prices `q` established by trade, there is only one possible corresponding vector of factor prices `p`."}, "Answer": "D", "pi_justification": "This question assesses the core economic logic of the Factor Price Equalization theorem. It uses a conceptual reasoning strategy. The correct option (A) states the central implication. Distractor (B) is the conceptual opposite. Distractor (C) is 'True but Irrelevant,' as concavity does not guarantee a global inverse. Distractor (D) confuses the theorem's premise (equal commodity prices) with its conclusion.", "qid": "72", "question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `f_i(p)`: The cost function for commodity `i`, returning the minimum cost to produce one unit given factor price vector `p`.\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production, defining a mapping `f` from factor prices `p` to commodity prices `q`.\n\n---\n\n### The Questions\n\nIn the context of the Factor Price Equalization theorem, why is the existence of a unique global inverse for the cost function mapping `p` to `q` (i.e., `p = f^{-1}(q)`) a critical condition?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 65, "Question": "### Background\n\nAn analysis of \"white flight\" in early 20th-century northern U.S. cities estimates the effect of black in-migration on white population change. The core empirical challenge is that black arrivals do not choose neighborhoods randomly, creating endogeneity. The paper estimates both an OLS model and an instrumental variables (IV) model to address this.\n\n### Data / Model Specification\n\nThe model estimated is:\n```latex\n\\Delta W_{ij}^{t1-t0} = \\alpha + \\beta \\Delta B_{ij}^{t1-t0} + \\eta_{j} + \\epsilon_{ij} \\quad \\text{(Eq. 1)}\n```\nwhere `\\Delta W` is the change in white population, `\\Delta B` is the change in black population, and `\\eta_j` is a city fixed effect. A negative `\\beta` indicates white flight.\n\n**Table 1: OLS and IV Results for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) OLS | (2) IV |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -1.492*** | -3.389*** |\n| | (0.075) | (0.246) |\n\n*Notes: Standard errors in parentheses. *** p<0.01.*\n\n### Question\n\nBased on the results in Table 1, what is the direction of the endogeneity bias in the OLS model, and what is the most likely economic explanation for this bias as discussed in the paper?", "Options": {"A": "The OLS estimate is biased upwards (closer to zero), likely because both black and white households were attracted to the same neighborhoods with improving economic conditions or amenities.", "B": "The OLS estimate is biased downwards (more negative), likely because both black and white households were attracted to the same neighborhoods with improving economic conditions or amenities.", "C": "The OLS estimate is biased upwards (closer to zero), likely because black households were moving into neighborhoods that whites were already abandoning for reasons unrelated to race.", "D": "The OLS estimate is biased downwards (more negative), likely because black households were moving into neighborhoods that whites were already abandoning for reasons unrelated to race."}, "Answer": "A", "pi_justification": "Assessment Target: Diagnosing the direction and cause of OLS bias by comparing OLS and IV estimates. Strategy: Reverse-Reasoning. Distractor Logic: The correct answer (A) correctly identifies the positive bias (upward, toward zero) and the corresponding economic story (shared amenities). Distractor (B) gets both the direction of bias and the economic story wrong (this story would cause positive bias). Distractors (C) and (D) correctly identify one component (bias direction or story) but incorrectly pair it with the other, testing for partial understanding.", "qid": "65", "question": "### Background\n\nAn analysis of \"white flight\" in early 20th-century northern U.S. cities estimates the effect of black in-migration on white population change. The core empirical challenge is that black arrivals do not choose neighborhoods randomly, creating endogeneity. The paper estimates both an OLS model and an instrumental variables (IV) model to address this.\n\n### Data / Model Specification\n\nThe model estimated is:\n```latex\n\\Delta W_{ij}^{t1-t0} = \\alpha + \\beta \\Delta B_{ij}^{t1-t0} + \\eta_{j} + \\epsilon_{ij} \\quad \\text{(Eq. 1)}\n```\nwhere `\\Delta W` is the change in white population, `\\Delta B` is the change in black population, and `\\eta_j` is a city fixed effect. A negative `\\beta` indicates white flight.\n\n**Table 1: OLS and IV Results for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) OLS | (2) IV |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -1.492*** | -3.389*** |\n| | (0.075) | (0.246) |\n\n*Notes: Standard errors in parentheses. *** p<0.01.*\n\n### Question\n\nBased on the results in Table 1, what is the direction of the endogeneity bias in the OLS model, and what is the most likely economic explanation for this bias as discussed in the paper?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 75, "Question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nIn the no-linking regime, a landlord cannot screen tenants and hires randomly from the pool of applicants. If a landlord happens to draw a tenant of ability `e_i` (where `i \\geq n`), what is the correct expression for the landlord's ex-post profit, `\\pi_i^{no-link}`?", "Options": {"A": "`(1-\\alpha)F(e_i, \\hat{b}_n) + (r_i - r_0)a\\hat{b}_n`", "B": "`(1-\\alpha)F(e_i, b_i^*)`", "C": "`(1-\\alpha)F(e_i, \\hat{b}_n)`", "D": "`F(e_i, \\hat{b}_n) - (1+r_0)a\\hat{b}_n - w`"}, "Answer": "C", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to correctly apply the profit definition in a new institutional context. It uses a Computational Judgment strategy. The correct answer requires recognizing that the landlord's profit is their output share, evaluated at the mandated capital level, with no credit component. Distractors represent common errors: (B) Formula Misuse (using the wrong capital level), (D) Conceptual Error (incorrectly including a credit-linking term that is banned), and (C) Formula Misuse (using the total surplus expression instead of the landlord's profit).", "qid": "75", "question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nIn the no-linking regime, a landlord cannot screen tenants and hires randomly from the pool of applicants. If a landlord happens to draw a tenant of ability `e_i` (where `i \\geq n`), what is the correct expression for the landlord's ex-post profit, `\\pi_i^{no-link}`?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 79, "Question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified by an overidentification test, the Falsification Adaptive Set (FAS) provides a range of parameter estimates consistent with minimally relaxed versions of the model. This approach avoids the arbitrary selection of sensitivity parameters by letting the data determine the necessary degree of relaxation of the instrument exclusion restrictions.\n\n### Data / Model Specification\n\nConsider a linear IV model with one endogenous variable (`K=1`) and `L` instruments. The Falsification Adaptive Set (FAS) for the causal effect `β` is given by the interval:\n\n```latex\n\\mathrm{FAS} = \\left[ \\min_{l: \\pi_l \\neq 0} b_l, \\max_{l: \\pi_l \\neq 0} b_l \\right] \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l = ψ_l / π_l` is the population 2SLS coefficient on the endogenous variable `X` using `Z_l` as the excluded instrument and the remaining instruments `Z_{-l}` as controls. `π_l` is the corresponding first-stage coefficient.\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan (`Z_1`), Railroads (`Z_2`), and Exploration (`Z_3`). The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| **Panel B. Controlling for other instruments** | | |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n*Note: Standard practice is to discard results from instruments with a first-stage F-statistic below 10, as they are considered weak and unreliable.*\n\n### Question\n\nBased on the information provided in Table 1 and the definition in Eq. (1), what is the estimated Falsification Adaptive Set (FAS) for the effect of log highway km?", "Options": {"A": "[0.18, 3.65]", "B": "[0.24, 3.47]", "C": "[0.18, 0.42]", "D": "[0.42, 3.65]"}, "Answer": "C", "pi_justification": "This item assesses the ability to apply a theoretical formula to empirical results, a core skill. It uses a Computational Judgment strategy. The correct answer requires identifying the relevant instruments by screening out the weak one (F-stat < 10) and then taking the min and max of the remaining estimates. Distractor A represents a Step-Omission Error, where the user fails to screen for the weak instrument. Distractor C is a Formula Misuse error, based on incorrect calculations with the endpoints. Distractor D represents both a screening failure and a misidentification of the minimum value.", "qid": "79", "question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified by an overidentification test, the Falsification Adaptive Set (FAS) provides a range of parameter estimates consistent with minimally relaxed versions of the model. This approach avoids the arbitrary selection of sensitivity parameters by letting the data determine the necessary degree of relaxation of the instrument exclusion restrictions.\n\n### Data / Model Specification\n\nConsider a linear IV model with one endogenous variable (`K=1`) and `L` instruments. The Falsification Adaptive Set (FAS) for the causal effect `β` is given by the interval:\n\n```latex\n\\mathrm{FAS} = \\left[ \\min_{l: \\pi_l \\neq 0} b_l, \\max_{l: \\pi_l \\neq 0} b_l \\right] \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l = ψ_l / π_l` is the population 2SLS coefficient on the endogenous variable `X` using `Z_l` as the excluded instrument and the remaining instruments `Z_{-l}` as controls. `π_l` is the corresponding first-stage coefficient.\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan (`Z_1`), Railroads (`Z_2`), and Exploration (`Z_3`). The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| **Panel B. Controlling for other instruments** | | |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n*Note: Standard practice is to discard results from instruments with a first-stage F-statistic below 10, as they are considered weak and unreliable.*\n\n### Question\n\nBased on the information provided in Table 1 and the definition in Eq. (1), what is the estimated Falsification Adaptive Set (FAS) for the effect of log highway km?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 74, "Question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nThe paper argues that the no-linking regime is always inefficient. Under which specific condition would the capital requirement for the marginal tenant, `\\hat{b}_n`, coincidentally be efficient (i.e., `\\hat{b}_n = b_n^*`)?", "Options": {"A": "`w = 0`", "B": "`r_n = r_0`", "C": "`z^* = 0` (where `z^*` is the landlord's profit in the linking regime)", "D": "`r_n > r_0`"}, "Answer": "B", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the deep understanding of what drives inefficiency in the no-linking regime. It uses a Reverse-Reasoning strategy, asking for the specific condition that would eliminate the inefficiency. The correct answer, `r_n = r_0`, requires connecting the IR constraints from both regimes. Distractors are based on common confusions: (B) is a plausible but incorrect condition, (C) confuses the outside option with efficiency conditions, and (D) confuses landlord profit with social surplus maximization.", "qid": "74", "question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nThe paper argues that the no-linking regime is always inefficient. Under which specific condition would the capital requirement for the marginal tenant, `\\hat{b}_n`, coincidentally be efficient (i.e., `\\hat{b}_n = b_n^*`)?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 78, "Question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms generally fail, and what specific design features are required to solve this problem?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy. Simple linear mechanisms (like the Shareholding and Helpman-Razin mechanisms) fail because they cannot consistently align stockholders' incentives. A more complex, non-linear mechanism is proposed to solve this problem.\n\n---\n\n### Data / Model Specification\n\nThe **Shared Cost Mechanism** is designed to overcome the failures of simpler linear mechanisms. Its key features are:\n-   Message: Each stockholder `i` sends a message `m_i = (x_i, a_i)`, where `x_i` is a vector of proposed input increments and `a_i` is a vector of proposed cost share transfers.\n-   Allocation Rule: `x_j = \\sum_i x_{ij}`\n-   Cost-Sharing Rule:\n    ```latex\n    C_{ij}(m) = (1 - \\sum_{k \\ne i} a_{kj}) x_j + (1 - \\sum_k a_{kj})^2 \\quad \\text{(Eq. (1))}\n    ```\n\n---\n\nHow does the design of the Shared Cost Mechanism, particularly its cost-sharing rule (Eq. (1)), successfully guarantee the existence of an equilibrium where simpler linear mechanisms fail?", "Options": {"A": "It fixes each stockholder's cost share to be equal to their initial endowed revenue share (`\\bar{\\theta}_{ij}`), which provides a stable and predictable financing plan.", "B": "It endogenizes cost shares via the `a_{ij}` message and uses the quadratic term to create an incentive for these shares to sum to one, providing the flexibility needed to align heterogeneous preferences.", "C": "The quadratic term ensures that the total costs collected from all stockholders exactly equal the total investment (`\\sum_i C_{ij}(m) = x_j`), preventing any budget shortfalls for the firm.", "D": "It allows stockholders to directly communicate their desired input levels (`x_{ij}`), ensuring the final investment `x_j` reflects the average preference of all stockholders."}, "Answer": "B", "pi_justification": "This item assesses understanding of the paper's central innovation: the design of the Shared Cost Mechanism. It uses a Reverse-Reasoning strategy, asking for the specific design features that ensure equilibrium existence. The distractors are designed to isolate the critical components: (A) focuses on a necessary but insufficient part of the message, (B) proposes a solution the paper explicitly rejects, and (D) offers a correct but less precise explanation of the quadratic term's function, making the correct answer superior.", "qid": "78", "question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms generally fail, and what specific design features are required to solve this problem?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy. Simple linear mechanisms (like the Shareholding and Helpman-Razin mechanisms) fail because they cannot consistently align stockholders' incentives. A more complex, non-linear mechanism is proposed to solve this problem.\n\n---\n\n### Data / Model Specification\n\nThe **Shared Cost Mechanism** is designed to overcome the failures of simpler linear mechanisms. Its key features are:\n-   Message: Each stockholder `i` sends a message `m_i = (x_i, a_i)`, where `x_i` is a vector of proposed input increments and `a_i` is a vector of proposed cost share transfers.\n-   Allocation Rule: `x_j = \\sum_i x_{ij}`\n-   Cost-Sharing Rule:\n    ```latex\n    C_{ij}(m) = (1 - \\sum_{k \\ne i} a_{kj}) x_j + (1 - \\sum_k a_{kj})^2 \\quad \\text{(Eq. (1))}\n    ```\n\n---\n\nHow does the design of the Shared Cost Mechanism, particularly its cost-sharing rule (Eq. (1)), successfully guarantee the existence of an equilibrium where simpler linear mechanisms fail?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 82, "Question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nBased on the results in Table 1, which statement most accurately summarizes the causal effect of an electoral loss on a novice candidate's propensity to run for office again?", "Options": {"A": "An electoral loss significantly reduces a male candidate's probability of running again, but it has no statistically significant effect on a female candidate's probability of running again.", "B": "An electoral loss reduces the probability of running again for both men and women, but the deterrence effect is over 50% larger for women than for men.", "C": "An electoral loss reduces a female candidate's probability of running again by approximately 10.1 percentage points.", "D": "Female candidates are, on average, 10.1 percentage points less likely to run for office again compared to male candidates, regardless of the election outcome."}, "Answer": "B", "pi_justification": "Value: Assesses the student's ability to interpret the main finding from an interaction model, requiring them to synthesize multiple coefficients. Strategy: Computational Judgment. The student must understand that the effect for men is `β` (-16.0pp), the effect for women is `β+γ` (-16.0 + -10.1 = -26.1pp), and then compare them. Distractor Logic: (B) is a Step-Omission Error, reporting only the interaction term `γ`. (C) is a Conceptual Error, misinterpreting the meaning of the interaction term. (D) is a Conceptual Error, confusing an interaction effect with a main effect.", "qid": "82", "question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nBased on the results in Table 1, which statement most accurately summarizes the causal effect of an electoral loss on a novice candidate's propensity to run for office again?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 81, "Question": "### Background\n\nIn an instrumental variable (IV) model with multiple instruments, some instruments may be more problematic than others. A key diagnostic is the conditional first-stage F-statistic, which measures the strength of an instrument after controlling for the other instruments. An F-statistic below 10 typically indicates a 'weak instrument,' which can lead to unreliable and biased 2SLS estimates.\n\n### Data / Model Specification\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan, Railroads, and Exploration. The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n### Question\n\nBased on the evidence in Table 1, which instrument is most problematic when used in a model that controls for the other instruments, and what is the primary statistical reason?", "Options": {"A": "'Railroads', because its conditional first-stage F-statistic is extremely low, indicating it is a weak instrument.", "B": "'Plan', because its 2SLS estimate (0.18) is the smallest, suggesting it may be biased downwards.", "C": "'Exploration', because its F-statistic (27.0) is significantly lower than that of 'Plan' (54.8).", "D": "'Railroads', because its 2SLS estimate (3.65) is an extreme outlier compared to the others."}, "Answer": "A", "pi_justification": "This item assesses the ability to interpret standard econometric diagnostics to evaluate instrument validity, a critical skill for applying the paper's methods. It uses a Reverse-Reasoning strategy (inferring the problematic instrument from its statistical properties). The correct answer identifies the instrument with the critically low F-statistic. Distractor B identifies the correct instrument but for a secondary reason (the outlier estimate is a *symptom* of the weak first stage, not the root cause), making it an 'Almost Right' conceptual error. Distractor C is 'True but Irrelevant' because an F-stat of 27 is still very strong. Distractor A offers an economically irrelevant reason.", "qid": "81", "question": "### Background\n\nIn an instrumental variable (IV) model with multiple instruments, some instruments may be more problematic than others. A key diagnostic is the conditional first-stage F-statistic, which measures the strength of an instrument after controlling for the other instruments. An F-statistic below 10 typically indicates a 'weak instrument,' which can lead to unreliable and biased 2SLS estimates.\n\n### Data / Model Specification\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan, Railroads, and Exploration. The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n### Question\n\nBased on the evidence in Table 1, which instrument is most problematic when used in a model that controls for the other instruments, and what is the primary statistical reason?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 83, "Question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nThe paper argues that the gender gap in persistence is driven by the behavior of candidates who barely lost, not by those who barely won. Which coefficient in Table 1 provides the most direct evidence for the \"not by those who barely won\" part of this claim?", "Options": {"A": "The coefficient on `Female x Lost` (γ = -0.101), because it shows that losing has a different effect on women.", "B": "The standard error on `Female` (0.036), because it is large relative to the coefficient, indicating high uncertainty.", "C": "The coefficient on `Female` (δ = 0.007), because its small size and statistical insignificance imply no meaningful difference in persistence between male and female winners.", "D": "The coefficient on `Lost` (β = -0.160), because it shows the large negative effect of losing for the baseline group."}, "Answer": "C", "pi_justification": "Value: Assesses a nuanced point of econometric interpretation: identifying which coefficient in an interaction model represents the baseline difference between groups (i.e., among the control group, `Lost=0`). Strategy: Reverse-Reasoning. The student must infer which piece of evidence supports a specific part of the paper's argument. Distractor Logic: (A) is Almost Right, as it explains the 'driven by losers' part, but not the 'not by winners' part. (B) is True but Irrelevant to the gender comparison among winners. (D) points to the statistical tool (the SE) rather than the economic parameter (the coefficient) that carries the interpretation.", "qid": "83", "question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nThe paper argues that the gender gap in persistence is driven by the behavior of candidates who barely lost, not by those who barely won. Which coefficient in Table 1 provides the most direct evidence for the \"not by those who barely won\" part of this claim?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 80, "Question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified, the Falsification Adaptive Set (FAS) provides a range of plausible parameter estimates. For any potential value `b` within the FAS, the minimal required violation of the exclusion restriction for instrument `l` (its direct effect `|γ_l|`) is given by the vector `δ(b)` on the Falsification Frontier.\n\n### Data / Model Specification\n\nThe required violation for instrument `l` to be consistent with a true causal effect `β_true = b` is:\n\n```latex\n|\\gamma_l| = |\\psi_l - b\\pi_l| = |\\pi_l| \\cdot |b_l - b| \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l` is the 2SLS estimate using instrument `l` and `π_l` is its first-stage coefficient. The first-stage F-statistic is proportional to `π_l^2`, so the relative strength of two instruments can be compared using the square root of the ratio of their F-statistics: `|π_1|/|π_2| ≈ sqrt(F_1/F_2)`.\n\nAn empirical study provides the following results for two strong instruments, 'Plan' and 'Exploration':\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (6) Exploration |\n| :--- | :---: | :---: |\n| log highway km (`b_l`) | 0.18 | 0.42 |\n| First-stage F stat. | 54.8 | 27.0 |\n\n### Question\n\nSuppose the true causal effect is `β_true = 0.25`. To make the model consistent with this true value, which instrument requires a larger violation of its exclusion restriction (`|γ_l|`)?", "Options": {"A": "'Plan', because it has a stronger first stage (higher F-statistic).", "B": "'Plan', because its point estimate (0.18) is closer to the true value (0.25).", "C": "'Exploration', because its point estimate (0.42) is further from the true value (0.25).", "D": "'Exploration', because the required violation `|π_l| ⋅ |b_l - 0.25|` is larger for it after accounting for first-stage strengths."}, "Answer": "D", "pi_justification": "This item tests deep comprehension by requiring a multi-step calculation that synthesizes point estimates, a theoretical formula, and test statistics. It uses a Computational Judgment strategy. The correct answer requires calculating the weighted distance `|π_l| ⋅ |b_l - b|` for both instruments. Distractor D represents a Step-Omission Error, as it correctly identifies the instrument with the larger unweighted distance `|b_l - b|` but fails to account for the first-stage strength `|π_l|`. Distractor B is True but Irrelevant, misapplying the meaning of the F-statistic in this context. Distractor A uses the same flawed logic as D but for the wrong instrument.", "qid": "80", "question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified, the Falsification Adaptive Set (FAS) provides a range of plausible parameter estimates. For any potential value `b` within the FAS, the minimal required violation of the exclusion restriction for instrument `l` (its direct effect `|γ_l|`) is given by the vector `δ(b)` on the Falsification Frontier.\n\n### Data / Model Specification\n\nThe required violation for instrument `l` to be consistent with a true causal effect `β_true = b` is:\n\n```latex\n|\\gamma_l| = |\\psi_l - b\\pi_l| = |\\pi_l| \\cdot |b_l - b| \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l` is the 2SLS estimate using instrument `l` and `π_l` is its first-stage coefficient. The first-stage F-statistic is proportional to `π_l^2`, so the relative strength of two instruments can be compared using the square root of the ratio of their F-statistics: `|π_1|/|π_2| ≈ sqrt(F_1/F_2)`.\n\nAn empirical study provides the following results for two strong instruments, 'Plan' and 'Exploration':\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (6) Exploration |\n| :--- | :---: | :---: |\n| log highway km (`b_l`) | 0.18 | 0.42 |\n| First-stage F stat. | 54.8 | 27.0 |\n\n### Question\n\nSuppose the true causal effect is `β_true = 0.25`. To make the model consistent with this true value, which instrument requires a larger violation of its exclusion restriction (`|γ_l|`)?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 84, "Question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates. A central hypothesis is that the gender gap in persistence is concentrated in electoral environments with lower female representation.\n\n### Data / Model Specification\n\nThe analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nThe paper estimates this model on different subsamples of political offices. Table 1 shows the baseline effect of losing for men. Table 2 shows the *additional* effect for women (`γ`) in two different office types.\n\n**Table 1. Baseline Effect of Loss for Male Candidates**\n\n| Variable | Coefficient | Standard Error |\n|----------|-------------|----------------|\n| `Lost`   | -0.160***   | (0.027)        |\n\n*Source: Adapted from Table 2 of the source paper.*\n\n**Table 2. Heterogeneity by Office Type (Differential Effect for Women)**\n\n| Sample / Office Type         | `Female x Lost` Coefficient (γ) | Standard Error |\n|------------------------------|---------------------------------|----------------|\n| City Council (Low Rep.)      | -0.165**                        | (0.065)        |\n| School Board (High Rep.)     | -0.057                          | (0.051)        |\n\n*Source: Adapted from Table 4 of the source paper. ** p<0.05.*\n\n### Question\n\nBased on these results, in which electoral environment is the gender gap in persistence most pronounced, and what is the approximate total deterrence effect of a loss for a woman in that environment?", "Options": {"A": "Most pronounced in School Board races; the total effect for a woman is a reduction of about 5.7 percentage points.", "B": "Most pronounced in School Board races; the total effect for a woman is a reduction of about 21.7 percentage points.", "C": "Most pronounced in City Council races; the total effect for a woman is a reduction of about 16.5 percentage points.", "D": "Most pronounced in City Council races; the total effect for a woman is a reduction of about 32.5 percentage points."}, "Answer": "D", "pi_justification": "Value: Assesses the ability to test a hypothesis using subgroup analysis and to correctly calculate the total effect for a specific group in an interaction model. Strategy: Scenario Application. The student must apply the general model to two specific scenarios (office types). Distractor Logic: (A) is a Step-Omission Error, reporting only the interaction term `γ` for City Council races. (B) correctly calculates the effect for School Board races but incorrectly identifies it as the more pronounced environment. (C) is a Step-Omission Error for School Board races, reporting only the interaction term `γ`.", "qid": "84", "question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates. A central hypothesis is that the gender gap in persistence is concentrated in electoral environments with lower female representation.\n\n### Data / Model Specification\n\nThe analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nThe paper estimates this model on different subsamples of political offices. Table 1 shows the baseline effect of losing for men. Table 2 shows the *additional* effect for women (`γ`) in two different office types.\n\n**Table 1. Baseline Effect of Loss for Male Candidates**\n\n| Variable | Coefficient | Standard Error |\n|----------|-------------|----------------|\n| `Lost`   | -0.160***   | (0.027)        |\n\n*Source: Adapted from Table 2 of the source paper.*\n\n**Table 2. Heterogeneity by Office Type (Differential Effect for Women)**\n\n| Sample / Office Type         | `Female x Lost` Coefficient (γ) | Standard Error |\n|------------------------------|---------------------------------|----------------|\n| City Council (Low Rep.)      | -0.165**                        | (0.065)        |\n| School Board (High Rep.)     | -0.057                          | (0.051)        |\n\n*Source: Adapted from Table 4 of the source paper. ** p<0.05.*\n\n### Question\n\nBased on these results, in which electoral environment is the gender gap in persistence most pronounced, and what is the approximate total deterrence effect of a loss for a woman in that environment?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 88, "Question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nModel (5) is selected as the best specification due to its superior out-of-sample performance (lowest RMSE). When Model (7) adds CO2 concentrations (`S_{t-1}`) to Model (5), its performance worsens, and the coefficient on `S_{t-1}` becomes statistically insignificant. What is the most likely econometric explanation for this phenomenon?", "Options": {"A": "Omitted variable bias from excluding an important climate forcing agent in Model (7).", "B": "Reverse causality, where higher temperatures are driving CO2 concentrations.", "C": "The ARMA(1,1) error structure is misspecified for Model (7).", "D": "High multicollinearity between cumulative emissions (`C_{t-1}`) and CO2 concentrations (`S_{t-1}`)."}, "Answer": "D", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to diagnose a classic econometric problem from regression output, a critical skill for empirical researchers. Strategy: Reverse-Reasoning. The distractors represent common but incorrect diagnoses: (B) is a Conceptual Opposite, as the problem stems from adding, not omitting, a variable; (C) is True but Irrelevant, as misspecification is less likely to cause these specific symptoms than multicollinearity; (D) describes a different form of endogeneity not directly supported by the evidence in the table.", "qid": "88", "question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nModel (5) is selected as the best specification due to its superior out-of-sample performance (lowest RMSE). When Model (7) adds CO2 concentrations (`S_{t-1}`) to Model (5), its performance worsens, and the coefficient on `S_{t-1}` becomes statistically insignificant. What is the most likely econometric explanation for this phenomenon?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 77, "Question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms for determining firm investment and its financing generally fail to guarantee an equilibrium?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy where stockholders must collectively decide on firms' production plans. This analysis compares two distinct linear mechanisms that fail.\n\n---\n\n### Data / Model Specification\n\nThe decision problem of a stockholder is to choose a consumption plan, a portfolio `\\theta_i`, and a message `m_i` to maximize utility. The firm's input `x_j` and the stockholder's cost share `C_{ij}` are determined by the specific mechanism's rules.\n\n1.  **The Shareholding Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = \\theta_{ij} x_j`\n\n2.  **The Helpman-Razin Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = (1/I) x_j`\n\nBoth mechanisms are shown in the paper to fail to guarantee an equilibrium in general.\n\n---\n\nWhat are the distinct fundamental reasons for the failure of the Shareholding Mechanism and the Helpman-Razin Mechanism, respectively?", "Options": {"A": "The Shareholding Mechanism fails because it forces portfolio shares (`\\theta_{ij}`) to serve a dual role of risk-sharing and cost-sharing, preventing unanimity. The Helpman-Razin Mechanism fails because its exogenously fixed cost shares are too rigid to accommodate heterogeneous stockholder preferences.", "B": "Both mechanisms fail because they do not satisfy the Ekern-Wilson spanning condition, which is a necessary condition for equilibrium existence in any stock market model.", "C": "The Shareholding Mechanism fails because its cost shares are too rigid to accommodate heterogeneous preferences. The Helpman-Razin Mechanism fails because it forces portfolio shares (`\\theta_{ij}`) to serve a dual role of risk-sharing and cost-sharing.", "D": "Both mechanisms fail for the same reason: they force portfolio shares (`\\theta_{ij}`) to serve a dual role of risk-sharing and cost-sharing, which prevents stockholders from reaching a unanimous decision on the firm's investment level."}, "Answer": "A", "pi_justification": "This item assesses the core diagnostic understanding of why simple, linear mechanisms fail in this setting. It uses a Reverse-Reasoning strategy, asking the user to identify the distinct causes for two related failures. The distractors are designed to test for common conceptual confusions: (B) conflating the two distinct failures, (C) reversing the causes and effects, and (D) attributing the failure to a related but different concept (spanning) that the paper shows is not the core issue.", "qid": "77", "question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms for determining firm investment and its financing generally fail to guarantee an equilibrium?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy where stockholders must collectively decide on firms' production plans. This analysis compares two distinct linear mechanisms that fail.\n\n---\n\n### Data / Model Specification\n\nThe decision problem of a stockholder is to choose a consumption plan, a portfolio `\\theta_i`, and a message `m_i` to maximize utility. The firm's input `x_j` and the stockholder's cost share `C_{ij}` are determined by the specific mechanism's rules.\n\n1.  **The Shareholding Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = \\theta_{ij} x_j`\n\n2.  **The Helpman-Razin Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = (1/I) x_j`\n\nBoth mechanisms are shown in the paper to fail to guarantee an equilibrium in general.\n\n---\n\nWhat are the distinct fundamental reasons for the failure of the Shareholding Mechanism and the Helpman-Razin Mechanism, respectively?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 86, "Question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe long-run dynamic adjustment of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) is governed by a process that converges to a long-run equilibrium. In this equilibrium, the marginal revenue product of each factor equals its cost:\n1.  `p_{i}^{v} (\\delta F_{i} / \\delta K) = \\pi_{i}r` (for capital)\n2.  `p_{i}^{v} (\\delta F_{i} / \\delta E) = \\bar{w_{i}}` (for labor)\n\nAssume the production function `F_i` is Cobb-Douglas:\n\n```latex\nF_i(\\bar{K}_i, \\bar{E}_i) = Z \\bar{K}_i^\\beta \\bar{E}_i^{1-\\beta}\n```\n\n## Question\n\nGiven the long-run equilibrium conditions and the specified Cobb-Douglas production function, how does a permanent increase in the price of capital goods, `\\pi_i`, affect the sector's long-run equilibrium capital-labor ratio, `\\bar{K}_i^* / \\bar{E}_i^*`?", "Options": {"A": "It increases the capital-labor ratio.", "B": "It decreases the capital-labor ratio.", "C": "The effect is ambiguous and depends on whether `\\beta` is greater or less than 0.5.", "D": "It has no effect on the capital-labor ratio."}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item uses the 'Computational Judgment' strategy to replace the complex derivation in question 3 of the original QA problem. It tests the core economic insight: the law of demand for factors of production. To answer correctly, the user must mentally (or on paper) derive the optimal capital-labor ratio and determine the sign of its derivative with respect to the price of capital. The distractors represent classic errors: (B) is the conceptual opposite (violating the law of demand), (C) incorrectly assumes a fixed-proportions technology (zero elasticity of substitution), and (D) introduces a plausible but incorrect complexity.", "qid": "86", "question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe long-run dynamic adjustment of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) is governed by a process that converges to a long-run equilibrium. In this equilibrium, the marginal revenue product of each factor equals its cost:\n1.  `p_{i}^{v} (\\delta F_{i} / \\delta K) = \\pi_{i}r` (for capital)\n2.  `p_{i}^{v} (\\delta F_{i} / \\delta E) = \\bar{w_{i}}` (for labor)\n\nAssume the production function `F_i` is Cobb-Douglas:\n\n```latex\nF_i(\\bar{K}_i, \\bar{E}_i) = Z \\bar{K}_i^\\beta \\bar{E}_i^{1-\\beta}\n```\n\n## Question\n\nGiven the long-run equilibrium conditions and the specified Cobb-Douglas production function, how does a permanent increase in the price of capital goods, `\\pi_i`, affect the sector's long-run equilibrium capital-labor ratio, `\\bar{K}_i^* / \\bar{E}_i^*`?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 87, "Question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nBased on the preferred Model (5) in Table 1, what is the estimated long-run steady-state temperature increase resulting from a sustained increase of 1000 GtC in cumulative CO2 emissions?", "Options": {"A": "0.48 °C", "B": "0.36 °C", "C": "1.55 °C", "D": "1.16 °C"}, "Answer": "C", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem's core components are highly suitable for conversion. This item tests the ability to calculate a long-run multiplier from a dynamic model, a key skill in interpreting econometric results. Strategy: Computational Judgment. The distractors are designed based on common errors: (A) is the static effect (`β_2`), ignoring the dynamic multiplier `1/(1-β_1)`; (B) is a formula misuse (`β_2/β_1`); (C) is a data misread error, correctly applying the formula but using the `β_1` coefficient from the wrong model (Model 6).", "qid": "87", "question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nBased on the preferred Model (5) in Table 1, what is the estimated long-run steady-state temperature increase resulting from a sustained increase of 1000 GtC in cumulative CO2 emissions?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 89, "Question": "### Background\n\n**Research Question:** This problem examines the mathematical construction of uncorrelated time-series scenarios using Chebyshev polynomials, which serve as efficient inputs for emulating a complex climate model.\n\n**Setting / Institutional Environment:** The methodology generates a set of basis vectors for CO2 emissions from a family of orthogonal polynomials. These polynomials are defined over a canonical interval `x ∈ [-1, 1]` and must be transformed to be used for a specific time horizon, e.g., years `t ∈ [2005, 2100]`.\n\n### Data / Model Specification\n\nChebyshev polynomials are defined by the recurrence relation:\n\n```latex\nP_{0}(x)=1, \\quad P_{1}(x)=x, \\quad P_{k}(x)=2x P_{k-1}(x)-P_{k-2}(x) \\quad \\text{for } k=2,3,\\ldots \\quad \\text{(Eq. 1)}\n```\n\nThe domain `x ∈ [-1, 1]` is mapped to a general time interval `t ∈ [a, b]` using the transformation:\n\n```latex\nt = \\left(\\frac{b-a}{2}\\right)x + \\frac{a+b}{2} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAn emissions scenario `E_2(t)` is constructed based on the Chebyshev polynomial of degree 2, `P_2(x)`. The domain `x ∈ [-1, 1]` is mapped to the time horizon `t ∈ [2005, 2100]`. The range of the polynomial `[-1, 1]` is then linearly scaled to an emissions range of `[-5, 29.6708]` GtC/yr. What is the approximate emissions level for the year `t=2052.5`, which corresponds to the midpoint of the time horizon?", "Options": {"A": "-5.0 GtC/yr", "B": "12.34 GtC/yr", "C": "29.67 GtC/yr", "D": "-1.0 GtC/yr"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 10.0). The original QA is purely technical and computational, making it ideal for conversion. This item tests the multi-step process of applying the mathematical tools described. Strategy: Computational Judgment. The correct answer requires deriving `P_2(x)=2x^2-1`, finding that `t=2052.5` maps to `x=0`, evaluating `P_2(0)=-1`, and mapping this to the minimum of the emissions range. Distractors are based on predictable errors: (B) is a Step-Omission Error, forgetting the final range scaling; (C) is a Conceptual Error, taking the midpoint of the emissions range; (D) is another Conceptual Error, evaluating at an endpoint (`x=1`) instead of the midpoint.", "qid": "89", "question": "### Background\n\n**Research Question:** This problem examines the mathematical construction of uncorrelated time-series scenarios using Chebyshev polynomials, which serve as efficient inputs for emulating a complex climate model.\n\n**Setting / Institutional Environment:** The methodology generates a set of basis vectors for CO2 emissions from a family of orthogonal polynomials. These polynomials are defined over a canonical interval `x ∈ [-1, 1]` and must be transformed to be used for a specific time horizon, e.g., years `t ∈ [2005, 2100]`.\n\n### Data / Model Specification\n\nChebyshev polynomials are defined by the recurrence relation:\n\n```latex\nP_{0}(x)=1, \\quad P_{1}(x)=x, \\quad P_{k}(x)=2x P_{k-1}(x)-P_{k-2}(x) \\quad \\text{for } k=2,3,\\ldots \\quad \\text{(Eq. 1)}\n```\n\nThe domain `x ∈ [-1, 1]` is mapped to a general time interval `t ∈ [a, b]` using the transformation:\n\n```latex\nt = \\left(\\frac{b-a}{2}\\right)x + \\frac{a+b}{2} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAn emissions scenario `E_2(t)` is constructed based on the Chebyshev polynomial of degree 2, `P_2(x)`. The domain `x ∈ [-1, 1]` is mapped to the time horizon `t ∈ [2005, 2100]`. The range of the polynomial `[-1, 1]` is then linearly scaled to an emissions range of `[-5, 29.6708]` GtC/yr. What is the approximate emissions level for the year `t=2052.5`, which corresponds to the midpoint of the time horizon?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 92, "Question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive Total Least Squares (TLS) estimator selects the rank of the estimated matrix `\\hat{M}` by keeping only the singular components of `X` that have a high estimated signal-to-noise ratio.\n\n---\n\n### Data / Model Specification\n\nThe adaptive TLS estimator selects its rank `\\hat{k}` according to the rule:\n```latex\n\\hat{k} = \\#\\{j : \\hat{\\pi}_j > 1/2\\}, \\quad \\text{where} \\quad \\hat{\\pi}_j = 1 - p\\hat{\\sigma}^2/\\hat{l}_j^2\n```\nThe variance `\\sigma^2` is estimated using the Smallest Singular Value (SSV) estimator, `\\hat{\\sigma}^2 = \\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p`.\n\n---\n\n### Question\n\nSuppose the condition for the SSV estimator to be consistent fails, such that the true signal `l_q^2/p` converges to a positive constant `c > 0` as `p \\to \\infty`. How does this affect the rank `\\hat{k}` selected by the adaptive TLS procedure in large samples?", "Options": {"A": "The effect on rank is ambiguous, as the upward bias in `\\hat{\\sigma}^2` is offset by the upward bias in the other estimated singular values `\\hat{l}_j^2`.", "B": "The rank selection will be asymptotically unbiased because the bias in `\\hat{\\sigma}^2` is a lower-order effect that does not affect the thresholding rule.", "C": "The selected rank `\\hat{k}` will be systematically too high because the procedure mistakes signal for noise.", "D": "The selected rank `\\hat{k}` will be systematically too low because the overestimated noise level leads to overly conservative thresholding."}, "Answer": "D", "pi_justification": "This item assesses the ability to trace the consequences of a parameter identification failure through a multi-step reasoning chain. It uses the Computational Judgment strategy. The distractors map to predictable error paths: (A) is the Conceptual Opposite, reversing the direction of the final effect; (C) represents a Step-Omission Error, failing to propagate the bias through the formula; (D) is a plausible but incorrect hedge that fails to recognize the dominant, systematic effect of the biased variance estimate.", "qid": "92", "question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive Total Least Squares (TLS) estimator selects the rank of the estimated matrix `\\hat{M}` by keeping only the singular components of `X` that have a high estimated signal-to-noise ratio.\n\n---\n\n### Data / Model Specification\n\nThe adaptive TLS estimator selects its rank `\\hat{k}` according to the rule:\n```latex\n\\hat{k} = \\#\\{j : \\hat{\\pi}_j > 1/2\\}, \\quad \\text{where} \\quad \\hat{\\pi}_j = 1 - p\\hat{\\sigma}^2/\\hat{l}_j^2\n```\nThe variance `\\sigma^2` is estimated using the Smallest Singular Value (SSV) estimator, `\\hat{\\sigma}^2 = \\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p`.\n\n---\n\n### Question\n\nSuppose the condition for the SSV estimator to be consistent fails, such that the true signal `l_q^2/p` converges to a positive constant `c > 0` as `p \\to \\infty`. How does this affect the rank `\\hat{k}` selected by the adaptive TLS procedure in large samples?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 91, "Question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive estimation procedures in the paper rely on a consistent estimator for the error variance `\\sigma^2`.\n\n---\n\n### Data / Model Specification\n\nWhen replication is unavailable, the paper proposes the Smallest Singular Value (SSV) estimator for the variance:\n```latex\n\\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p \n```\nwhere `\\hat{l}_q` is the smallest singular value of the data matrix `X`. This estimator is consistent for `\\sigma^2 + p^{-1}l_q^2`, where `l_q` is the smallest singular value of the true matrix `M`.\n\n---\n\n### Question\n\nUnder what condition on the true mean matrix `M` is the Smallest Singular Value (SSV) estimator a consistent estimator for the true error variance `\\sigma^2` as `p \\to \\infty`?", "Options": {"A": "The true matrix `M` must have reduced rank (`l_q = 0`).", "B": "The true matrix `M` must be full rank (`l_q > 0`).", "C": "The smallest singular value of `M`, `l_q`, must satisfy `l_q^2/p \\to 0`.", "D": "The largest singular value of `M`, `l_1`, must satisfy `l_1^2/p \\to 0`."}, "Answer": "C", "pi_justification": "This item assesses the key identification condition for the proposed variance estimator. It uses a Scenario Application strategy. The distractors are designed based on common errors: (A) is the conceptual opposite, the condition under which bias is likely; (B) involves formula misuse by confusing the largest and smallest singular values; (D) is an 'Almost Right' error, as it is a sufficient but not necessary condition, making the correct answer more precise.", "qid": "91", "question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive estimation procedures in the paper rely on a consistent estimator for the error variance `\\sigma^2`.\n\n---\n\n### Data / Model Specification\n\nWhen replication is unavailable, the paper proposes the Smallest Singular Value (SSV) estimator for the variance:\n```latex\n\\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p \n```\nwhere `\\hat{l}_q` is the smallest singular value of the data matrix `X`. This estimator is consistent for `\\sigma^2 + p^{-1}l_q^2`, where `l_q` is the smallest singular value of the true matrix `M`.\n\n---\n\n### Question\n\nUnder what condition on the true mean matrix `M` is the Smallest Singular Value (SSV) estimator a consistent estimator for the true error variance `\\sigma^2` as `p \\to \\infty`?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 94, "Question": "## Background\n\n**Research Question.** This problem examines the foundational conditions for the existence of a quasi-Lindahl equilibrium, focusing on the critical role of assumptions about consumer preferences.\n\n**Setting / Institutional Environment.** The economy consists of a continuum of consumers and a finite number of producers, with both private and public goods. The existence proof for equilibrium relies on a fixed-point theorem, which requires that the consumer's quasi-demand correspondence is non-empty and convex-valued.\n\n**Variables & Parameters.**\n\n*   $x(t) = (x_\\pi(t), x_g) \\in R^{n+l}$: A consumption bundle for consumer $t$.\n*   $p = (p_\\pi, p_g) \\in S$: A price system for private and public goods.\n*   $\\delta(t) \\in R_+^l$: A vector of personalized Lindahl shares for consumer $t$.\n*   $\\gtrsim_t$: The preference relation for consumer $t$.\n*   $x >_t y$: Strict preference, defined as $x \\gtrsim_t y$ and not $y \\gtrsim_t x$.\n*   $D^k(p, \\delta, t)$: The quasi-demand set for consumer $t$, containing all bundles in their (compactified) choice set that are not strictly preferred by any other affordable bundle.\n\n---\n\n## Data / Model Specification\n\nThe existence proof requires showing that the individual quasi-demand set, $D^k(p, \\delta, t)$, is convex. The proof proceeds by contradiction:\n\n1.  Take any two bundles $z^1, z^2$ from the quasi-demand set $D^k$.\n2.  Form their convex combination $\\bar{z} = \\lambda z^1 + (1-\\lambda)z^2$.\n3.  Assume $\\bar{z}$ is *not* in $D^k$. This means there exists an affordable bundle $y$ such that $y >_t \\bar{z}$.\n4.  Since $z^1 \\in D^k$, it cannot be that $y >_t z^1$. From this, infer that $z^1 \\gtrsim_t y$. (Step A)\n5.  Similarly, since $z^2 \\in D^k$, it cannot be that $y >_t z^2$. From this, infer that $z^2 \\gtrsim_t y$. (Step B)\n6.  Using the results from Steps A and B, infer that $\\bar{z} \\gtrsim_t y$. (Step C)\n7.  This leads to a contradiction, since $\\bar{z} \\gtrsim_t y$ is incompatible with the assumption $y >_t \\bar{z}$.\n\nKey preference axioms are:\n*   **Completeness:** For any two bundles $x,y$, either $x \\gtrsim_t y$ or $y \\gtrsim_t x$.\n*   **Convexity:** If $x \\gtrsim_t y$, then $\\lambda x + (1-\\lambda)y \\gtrsim_t y$ for all $\\lambda \\in [0,1]$.\n\n---\n\n## Question\n\nWhich step in the proof outline above relies critically on the **completeness** of preferences?", "Options": {"A": "Step 2: Establishing that the convex combination $\\bar{z}$ is in the consumer's budget set.", "B": "Step A: Inferring $z^1 \\gtrsim_t y$ from the fact that it is not the case that $y >_t z^1$.", "C": "Step 7: Reaching a contradiction between $\\bar{z} \\gtrsim_t y$ and $y >_t \\bar{z}$.", "D": "Step C: Inferring $\\bar{z} \\gtrsim_t y$ from $z^1 \\gtrsim_t y$ and $z^2 \\gtrsim_t y$."}, "Answer": "B", "pi_justification": "This question assesses the student's understanding of the precise logical role of the completeness axiom in microeconomic proofs. It uses a Reverse-Reasoning strategy, asking the student to identify the specific step in a proof that relies on the axiom. Distractor (B) targets the common confusion between the roles of completeness and convexity. Distractors (C) and (D) test whether the student can distinguish between properties derived from preferences versus those from the budget set or logical definitions.", "qid": "94", "question": "## Background\n\n**Research Question.** This problem examines the foundational conditions for the existence of a quasi-Lindahl equilibrium, focusing on the critical role of assumptions about consumer preferences.\n\n**Setting / Institutional Environment.** The economy consists of a continuum of consumers and a finite number of producers, with both private and public goods. The existence proof for equilibrium relies on a fixed-point theorem, which requires that the consumer's quasi-demand correspondence is non-empty and convex-valued.\n\n**Variables & Parameters.**\n\n*   $x(t) = (x_\\pi(t), x_g) \\in R^{n+l}$: A consumption bundle for consumer $t$.\n*   $p = (p_\\pi, p_g) \\in S$: A price system for private and public goods.\n*   $\\delta(t) \\in R_+^l$: A vector of personalized Lindahl shares for consumer $t$.\n*   $\\gtrsim_t$: The preference relation for consumer $t$.\n*   $x >_t y$: Strict preference, defined as $x \\gtrsim_t y$ and not $y \\gtrsim_t x$.\n*   $D^k(p, \\delta, t)$: The quasi-demand set for consumer $t$, containing all bundles in their (compactified) choice set that are not strictly preferred by any other affordable bundle.\n\n---\n\n## Data / Model Specification\n\nThe existence proof requires showing that the individual quasi-demand set, $D^k(p, \\delta, t)$, is convex. The proof proceeds by contradiction:\n\n1.  Take any two bundles $z^1, z^2$ from the quasi-demand set $D^k$.\n2.  Form their convex combination $\\bar{z} = \\lambda z^1 + (1-\\lambda)z^2$.\n3.  Assume $\\bar{z}$ is *not* in $D^k$. This means there exists an affordable bundle $y$ such that $y >_t \\bar{z}$.\n4.  Since $z^1 \\in D^k$, it cannot be that $y >_t z^1$. From this, infer that $z^1 \\gtrsim_t y$. (Step A)\n5.  Similarly, since $z^2 \\in D^k$, it cannot be that $y >_t z^2$. From this, infer that $z^2 \\gtrsim_t y$. (Step B)\n6.  Using the results from Steps A and B, infer that $\\bar{z} \\gtrsim_t y$. (Step C)\n7.  This leads to a contradiction, since $\\bar{z} \\gtrsim_t y$ is incompatible with the assumption $y >_t \\bar{z}$.\n\nKey preference axioms are:\n*   **Completeness:** For any two bundles $x,y$, either $x \\gtrsim_t y$ or $y \\gtrsim_t x$.\n*   **Convexity:** If $x \\gtrsim_t y$, then $\\lambda x + (1-\\lambda)y \\gtrsim_t y$ for all $\\lambda \\in [0,1]$.\n\n---\n\n## Question\n\nWhich step in the proof outline above relies critically on the **completeness** of preferences?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 99, "Question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n---\n\n### Data / Model Specification\n\nThe Empirical Likelihood (EL) test statistic for the null hypothesis `H₀: E[g(X, θ)] = 0` for some `θ` is denoted `T_n`. A small value of `T_n` indicates that the model's restrictions are compatible with the observed data.\n\n---\n\n### Question\n\nAn econometrician performs the EL overidentification test and obtains a small value for the test statistic `T_n`, leading to a failure to reject the null hypothesis `H₀`. What is the correct interpretation of this result and the primary inferential risk associated with it?", "Options": {"A": "The small `T_n` value implies the instruments are weak, and the primary risk is that the standard `χ²(r-p)` distribution is a poor approximation to the test statistic's true distribution.", "B": "The result indicates a severe model misspecification, and the primary risk is a Type I error (falsely rejecting a valid model).", "C": "The result suggests the model's restrictions are compatible with the data, but the primary risk is a Type II error, as the test may have low power to detect meaningful model misspecifications.", "D": "The result proves the model is correctly specified, and the main remaining risk is that the parameter estimates `θ` are biased in finite samples."}, "Answer": "C", "pi_justification": "This question uses a Reverse-Reasoning strategy to assess the user's understanding of test interpretation and inferential risks. Given a specific outcome (failure to reject), the user must identify the correct conclusion and its main caveat. The correct answer (A) correctly identifies the interpretation (compatibility) and the risk (Type II error/low power). Distractors are based on high-fidelity errors: B is the conceptual opposite, misinterpreting the result and confusing Type I/II errors. C overstates the conclusion ('proves') and introduces a different, though related, issue (estimator bias). D confuses the concept of model misspecification (tested by `T_n`) with weak identification and size distortion, which are distinct econometric problems.", "qid": "99", "question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n---\n\n### Data / Model Specification\n\nThe Empirical Likelihood (EL) test statistic for the null hypothesis `H₀: E[g(X, θ)] = 0` for some `θ` is denoted `T_n`. A small value of `T_n` indicates that the model's restrictions are compatible with the observed data.\n\n---\n\n### Question\n\nAn econometrician performs the EL overidentification test and obtains a small value for the test statistic `T_n`, leading to a failure to reject the null hypothesis `H₀`. What is the correct interpretation of this result and the primary inferential risk associated with it?", "model_evaluation_result": {"model_raw_response": "\"C\"", "model_answer": "C", "error": null}}
{"ID": 97, "Question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`). Production in each industry is subject to industry-level external economies of scale. With labor's wage as the numeraire, the average cost of production for good `j` is `AC_j = y_j^{-1/2}`. The relative supply price is therefore:\n```latex\n\\frac{AC_2}{AC_1} = \\left(\\frac{y_1}{y_2}\\right)^{1/2} \\quad \\text{(Eq. 1)}\n```\nIn autarky, two identical countries are at a symmetric equilibrium `C_0` where `y_1 = y_2`. At this point, the relative supply price is 1, matching the world demand price ratio. \n\n### Question\n\nAccording to a Marshallian stability analysis, how should the symmetric equilibrium point `C_0` be characterized when the countries open to trade?", "Options": {"A": "Unstable, because a small increase in the output of one good lowers its average cost, creating an incentive for further expansion until specialization is complete.", "B": "Unstable, because the marginal cost of production is always lower than the world price, leading to infinite production.", "C": "Neutrally stable, because at the world price ratio of 1, there is no incentive to move away from the symmetric production point.", "D": "Stable, because a small increase in the output of one good raises its average cost, creating an incentive to return to the symmetric equilibrium."}, "Answer": "A", "pi_justification": "This item tests the user's ability to apply the concept of Marshallian stability to the specific model from the paper's Appendix. It uses a Reverse-Reasoning strategy, asking for the characterization of an equilibrium point. The distractors are based on predictable errors: A applies the incorrect stability logic from a standard CRS model, C correctly identifies the equilibrium condition but fails to perform the stability test, and D confuses the relevant cost concepts.", "qid": "97", "question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`). Production in each industry is subject to industry-level external economies of scale. With labor's wage as the numeraire, the average cost of production for good `j` is `AC_j = y_j^{-1/2}`. The relative supply price is therefore:\n```latex\n\\frac{AC_2}{AC_1} = \\left(\\frac{y_1}{y_2}\\right)^{1/2} \\quad \\text{(Eq. 1)}\n```\nIn autarky, two identical countries are at a symmetric equilibrium `C_0` where `y_1 = y_2`. At this point, the relative supply price is 1, matching the world demand price ratio. \n\n### Question\n\nAccording to a Marshallian stability analysis, how should the symmetric equilibrium point `C_0` be characterized when the countries open to trade?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 96, "Question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`), with a total endowment normalized to 1. Production in each industry is subject to industry-level external economies of scale, given by the technology:\n```latex\ny_j = g(y_j) L_j \\quad \\text{where} \\quad g(x) = x^{1/2}\n```\nThis technology results in a production possibilities curve (PPC) described by the equation:\n```latex\ny_1^{1/2} + y_2^{1/2} = 1 \\quad \\text{(Eq. 1)}\n```\nThis PPC is non-convex (bowed-in towards the origin). In autarky, two identical countries are at a symmetric equilibrium `C_0` with a price ratio of 1. Under free trade, it is possible for one country to specialize in good 1 (producing at `P'=(1,0)`) and the other in good 2 (producing at `P''=(0,1)`), while the world price ratio remains 1.\n\n### Question\n\nGiven this setup, what is the fundamental reason that both identical countries can achieve a higher level of welfare through trade, despite no change in the price ratio?", "Options": {"A": "Gains are driven by underlying differences in factor endowments between the two countries, which manifest as a comparative advantage.", "B": "The production technology implies a convex production set (bowed-out PPC), where specialization according to comparative advantage is always optimal.", "C": "Gains arise because specialization allows each country to move down its marginal cost curve, leading to a lower world price for both goods.", "D": "Specialization allows countries to produce at the endpoints of the non-convex PPC, enabling them to trade along a 'chord' that lies outside their original individual production sets."}, "Answer": "D", "pi_justification": "This item assesses the understanding of the central, counter-intuitive result from the paper's Appendix. It uses a Scenario Application strategy, asking the user to identify the correct economic mechanism for gains from trade in a non-convex economy. The distractors are designed to target common misconceptions: B suggests a price-based mechanism that isn't present, C incorrectly applies standard comparative advantage theory, and D fundamentally misinterprets the geometry of the production set, which is the key to the entire problem.", "qid": "96", "question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`), with a total endowment normalized to 1. Production in each industry is subject to industry-level external economies of scale, given by the technology:\n```latex\ny_j = g(y_j) L_j \\quad \\text{where} \\quad g(x) = x^{1/2}\n```\nThis technology results in a production possibilities curve (PPC) described by the equation:\n```latex\ny_1^{1/2} + y_2^{1/2} = 1 \\quad \\text{(Eq. 1)}\n```\nThis PPC is non-convex (bowed-in towards the origin). In autarky, two identical countries are at a symmetric equilibrium `C_0` with a price ratio of 1. Under free trade, it is possible for one country to specialize in good 1 (producing at `P'=(1,0)`) and the other in good 2 (producing at `P''=(0,1)`), while the world price ratio remains 1.\n\n### Question\n\nGiven this setup, what is the fundamental reason that both identical countries can achieve a higher level of welfare through trade, despite no change in the price ratio?", "model_evaluation_result": {"model_raw_response": "\"D\"", "model_answer": "D", "error": null}}
{"ID": 100, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `p(q_n)`: Price of hospital care for non-covered patients.\n- `AC(q_n + q_m; α)`: Average cost of care, which is a function of total patients and an inefficiency parameter `α`.\n- `α`: An X-inefficiency shift parameter, where a higher `α` increases average cost (`∂AC/∂α > 0`).\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe hospital's profit function is given by:\n```latex\n\\pi = p(q_n) \\cdot q_n + k \\cdot q_m \\cdot AC(q_n+q_m; \\alpha) - (q_n+q_m) \\cdot AC(q_n+q_m; \\alpha) \\quad \\text{(Eq. (1))}\n```\n\nUnder the profit-maximization model, a hospital has a direct financial incentive to increase its own inefficiency (i.e., increase `α` to raise `AC`) only if which of the following conditions is met?", "Options": {"A": "The number of cost-reimbursed patients, `q_m`, exceeds the number of non-reimbursed patients, `q_n`.", "B": "The 'plus' portion of revenue from reimbursed patients, `(k-1) * q_m * AC`, exceeds the total cost of non-reimbursed patients, `q_n * AC`.", "C": "The reimbursement factor, `k`, is greater than 1.", "D": "The share of cost-reimbursed patients, `q_m / (q_n + q_m)`, exceeds the reciprocal of the reimbursement factor, `1/k`."}, "Answer": "D", "pi_justification": "This item assesses the student's ability to derive or recognize the key theoretical condition from the paper's profit-maximization model. It uses a Reverse-Reasoning strategy. The correct option (A) is the derived result. Distractors are designed based on common conceptual errors: (B) compares the wrong components of the profit function, (C) represents an intuitive but incorrect simplification, and (D) states a necessary but insufficient condition.", "qid": "100", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `p(q_n)`: Price of hospital care for non-covered patients.\n- `AC(q_n + q_m; α)`: Average cost of care, which is a function of total patients and an inefficiency parameter `α`.\n- `α`: An X-inefficiency shift parameter, where a higher `α` increases average cost (`∂AC/∂α > 0`).\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe hospital's profit function is given by:\n```latex\n\\pi = p(q_n) \\cdot q_n + k \\cdot q_m \\cdot AC(q_n+q_m; \\alpha) - (q_n+q_m) \\cdot AC(q_n+q_m; \\alpha) \\quad \\text{(Eq. (1))}\n```\n\nUnder the profit-maximization model, a hospital has a direct financial incentive to increase its own inefficiency (i.e., increase `α` to raise `AC`) only if which of the following conditions is met?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
{"ID": 85, "Question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe evolution of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) from one period to the next is governed by the following equations, which combine an accelerator mechanism with a neoclassical profitability term:\n\n```latex\n\\frac{d\\bar{K_{i}}}{\\bar{K_{i}}} = A_{i}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\pi_{i}r} \\frac{\\delta F_{i}}{\\delta K}\\right]^{\\sigma_{i}} - 1 \\quad \\text{(Eq. (1))}\n```\n\n```latex\n\\frac{d\\bar{E_{i}}}{\\bar{E_{i}}} = A_{i}^{\\prime}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\bar{w_{i}}} \\frac{\\delta F_{i}}{\\delta E}\\right]^{\\sigma_{i}^{\\prime}} - 1 \\quad \\text{(Eq. (2))}\n```\nwhere `g_i` is expected demand growth, `\\alpha_i` is the activity level, `p_i^v` is the value-added price, `\\pi_i r` is the rental cost of capital, `\\bar{w}_i` is the wage cost, and `F_i` is the production function.\n\n## Question\n\nIn the model, a long-run equilibrium is a steady state where capital and labor stocks are constant (`d\\bar{K}_i / \\bar{K}_i = 0`, `d\\bar{E}_i / \\bar{E}_i = 0`), the economy operates at full capacity (`\\alpha_i = 1`), and expected demand growth is zero (`g_i = 0`). Based on the dynamic adjustment equations (Eq. (1) and Eq. (2)), which pair of conditions correctly describes this long-run equilibrium?", "Options": {"A": "The marginal revenue product of capital equals its rental cost (`p_i^v (\\delta F_i / \\delta K) = \\pi_i r`), and the marginal revenue product of labor equals its wage cost (`p_i^v (\\delta F_i / \\delta E) = \\bar{w}_i`).", "B": "The marginal revenue product of capital equals the wage (`p_i^v (\\delta F_i / \\delta K) = \\bar{w}_i`), and the marginal revenue product of labor equals the rental cost of capital (`p_i^v (\\delta F_i / \\delta E) = \\pi_i r`).", "C": "The average revenue product of capital equals its rental cost (`p_i^v (F_i / \\bar{K}_i) = \\pi_i r`), and the average revenue product of labor equals its wage (`p_i^v (F_i / \\bar{E}_i) = \\bar{w}_i`).", "D": "The marginal product of capital is zero (`\\delta F_i / \\delta K = 0`), and the marginal product of labor is zero (`\\delta F_i / \\delta E = 0`)."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item replaces the derivation task in question 2 of the original QA problem with a sharp conceptual check on its outcome. It assesses whether the user can correctly identify the standard neoclassical profit-maximization conditions that emerge as the long-run steady state of the model's dynamic equations. Distractors are designed based on common errors in microeconomic theory: (B) incorrectly sets marginal products to zero, (C) swaps the prices of capital and labor, and (D) confuses marginal products with average products.", "qid": "85", "question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe evolution of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) from one period to the next is governed by the following equations, which combine an accelerator mechanism with a neoclassical profitability term:\n\n```latex\n\\frac{d\\bar{K_{i}}}{\\bar{K_{i}}} = A_{i}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\pi_{i}r} \\frac{\\delta F_{i}}{\\delta K}\\right]^{\\sigma_{i}} - 1 \\quad \\text{(Eq. (1))}\n```\n\n```latex\n\\frac{d\\bar{E_{i}}}{\\bar{E_{i}}} = A_{i}^{\\prime}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\bar{w_{i}}} \\frac{\\delta F_{i}}{\\delta E}\\right]^{\\sigma_{i}^{\\prime}} - 1 \\quad \\text{(Eq. (2))}\n```\nwhere `g_i` is expected demand growth, `\\alpha_i` is the activity level, `p_i^v` is the value-added price, `\\pi_i r` is the rental cost of capital, `\\bar{w}_i` is the wage cost, and `F_i` is the production function.\n\n## Question\n\nIn the model, a long-run equilibrium is a steady state where capital and labor stocks are constant (`d\\bar{K}_i / \\bar{K}_i = 0`, `d\\bar{E}_i / \\bar{E}_i = 0`), the economy operates at full capacity (`\\alpha_i = 1`), and expected demand growth is zero (`g_i = 0`). Based on the dynamic adjustment equations (Eq. (1) and Eq. (2)), which pair of conditions correctly describes this long-run equilibrium?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 93, "Question": "### Background\n\nThe paper develops 'adaptive' estimators (e.g., `\\hat{M}_{TS}`) that are computable from data. It compares their performance to 'oracle' estimators (e.g., `\\widetilde{M}_{TS}`), which are defined as the best possible estimators within a certain class but are infeasible because they depend on the unknown true parameters of the model.\n\n---\n\n### Data / Model Specification\n\nA key theoretical result (Theorem 4.2) establishes that, under certain assumptions, the risk of the adaptive Total Shrinkage estimator converges to the risk of the oracle Total Shrinkage estimator as the number of rows `p` tends to infinity:\n```latex\n\\lim_{p \\to \\infty} |R(\\hat{M}_{TS}, M, \\sigma^2) - R(\\widetilde{M}_{TS}, M, \\sigma^2)| = 0\n```\n\n---\n\n### Question\n\nWhat is the primary theoretical significance of this finding that the risk of the adaptive estimator converges to the risk of the oracle estimator?", "Options": {"A": "It validates that the data-driven procedure of minimizing estimated risk successfully approximates the theoretically best possible performance in large samples.", "B": "It shows that the adaptive estimator is optimal for small sample sizes, where estimation is most difficult.", "C": "It proves that the adaptive estimator becomes unbiased as the sample size grows.", "D": "It demonstrates that the singular value decomposition provides a computationally stable method for estimation."}, "Answer": "A", "pi_justification": "This item assesses the core conceptual takeaway of the paper's asymptotic theory. The distractors are designed to probe for common misunderstandings: (A) is an 'Almost Right' error that misidentifies the goal (risk reduction, not unbiasedness); (B) is a 'Conceptual Opposite' regarding the asymptotic nature of the result; (C) is 'True but Irrelevant', confusing a computational property with the theoretical significance of risk convergence.", "qid": "93", "question": "### Background\n\nThe paper develops 'adaptive' estimators (e.g., `\\hat{M}_{TS}`) that are computable from data. It compares their performance to 'oracle' estimators (e.g., `\\widetilde{M}_{TS}`), which are defined as the best possible estimators within a certain class but are infeasible because they depend on the unknown true parameters of the model.\n\n---\n\n### Data / Model Specification\n\nA key theoretical result (Theorem 4.2) establishes that, under certain assumptions, the risk of the adaptive Total Shrinkage estimator converges to the risk of the oracle Total Shrinkage estimator as the number of rows `p` tends to infinity:\n```latex\n\\lim_{p \\to \\infty} |R(\\hat{M}_{TS}, M, \\sigma^2) - R(\\widetilde{M}_{TS}, M, \\sigma^2)| = 0\n```\n\n---\n\n### Question\n\nWhat is the primary theoretical significance of this finding that the risk of the adaptive estimator converges to the risk of the oracle estimator?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 101, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `AC`: Average cost of care.\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe paper shows that under both profit-maximization and output-maximization models, a hospital has an incentive to inflate its costs if and only if the following condition holds:\n```latex\n\\frac{q_m}{q_n + q_m} > \\frac{1}{k}\n```\n\nA hospital operates in an environment described by these models. If its cost-reimbursement plan includes a 5% 'plus' factor (i.e., `k=1.05`), what is the minimum percentage of the hospital's patients that must be covered by this plan for the hospital to find it advantageous to increase costs?", "Options": {"A": "95.2%", "B": "80.0%", "C": "5.0%", "D": "99.5%"}, "Answer": "A", "pi_justification": "This item tests the student's ability to apply the derived theoretical condition to a concrete numerical scenario. It uses a Computational Judgment strategy. The correct option (C) requires correct substitution and calculation. Distractors are based on predictable errors: (A) misuses the 'plus' factor directly as the threshold (Formula Misuse), (B) is a salient but irrelevant number from the text (True but Irrelevant), and (D) represents a plausible calculation slip (Numerical Error).", "qid": "101", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `AC`: Average cost of care.\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe paper shows that under both profit-maximization and output-maximization models, a hospital has an incentive to inflate its costs if and only if the following condition holds:\n```latex\n\\frac{q_m}{q_n + q_m} > \\frac{1}{k}\n```\n\nA hospital operates in an environment described by these models. If its cost-reimbursement plan includes a 5% 'plus' factor (i.e., `k=1.05`), what is the minimum percentage of the hospital's patients that must be covered by this plan for the hospital to find it advantageous to increase costs?", "model_evaluation_result": {"model_raw_response": "\"A\"", "model_answer": "A", "error": null}}
{"ID": 98, "Question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n**Variables & Parameters.**\n- `g(X, θ)`: A vector of `r` moment functions.\n- `θ`: A `p`-dimensional vector of parameters.\n\n---\n\n### Data / Model Specification\n\nThe validity of the overidentifying restrictions is evaluated by testing the null hypothesis `H₀` against the alternative `H₁`:\n\n```latex\nH_{0}: \\mathbb{E}[g(X, \\theta)] = 0 \\quad \\text{for some } \\theta \\in \\Theta \n```\n```latex\nH_{1}: \\mathbb{E}[g(X, \\theta)] \\neq 0 \\quad \\text{for any } \\theta \\in \\Theta\n```\n\nThe test statistic for `H₀` has a limiting distribution `T_n \\xrightarrow{d} \\chi^2(r-p)`.\n\n---\n\n### Question\n\nWhich of the following statements provides the most accurate description of the test for overidentifying restrictions?", "Options": {"A": "It tests the null hypothesis that the individual parameters in `θ` are statistically significant, with `r` degrees of freedom.", "B": "It tests the null hypothesis that the economic model is correctly specified, and its degrees of freedom are `r-p` because `p` degrees of freedom are used to estimate the model parameters.", "C": "It is primarily used for just-identified models (`r=p`) to confirm that the model perfectly fits the sample data.", "D": "It determines the optimal number of moment conditions `r` to use for a given number of parameters `p`, with `r-p` degrees of freedom."}, "Answer": "B", "pi_justification": "This question assesses fundamental knowledge about overidentification tests. It uses a direct query strategy to test three core concepts: the meaning of the null hypothesis (model specification), the condition for the test's relevance (`r>p`), and the logic for the degrees of freedom (`r-p`). Distractors are designed based on common conceptual errors: B confuses model specification with parameter significance and uses the wrong degrees of freedom; C confuses over- with just-identification; D confuses the purpose of the test with model selection.", "qid": "98", "question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n**Variables & Parameters.**\n- `g(X, θ)`: A vector of `r` moment functions.\n- `θ`: A `p`-dimensional vector of parameters.\n\n---\n\n### Data / Model Specification\n\nThe validity of the overidentifying restrictions is evaluated by testing the null hypothesis `H₀` against the alternative `H₁`:\n\n```latex\nH_{0}: \\mathbb{E}[g(X, \\theta)] = 0 \\quad \\text{for some } \\theta \\in \\Theta \n```\n```latex\nH_{1}: \\mathbb{E}[g(X, \\theta)] \\neq 0 \\quad \\text{for any } \\theta \\in \\Theta\n```\n\nThe test statistic for `H₀` has a limiting distribution `T_n \\xrightarrow{d} \\chi^2(r-p)`.\n\n---\n\n### Question\n\nWhich of the following statements provides the most accurate description of the test for overidentifying restrictions?", "model_evaluation_result": {"model_raw_response": "\"B\"", "model_answer": "B", "error": null}}
