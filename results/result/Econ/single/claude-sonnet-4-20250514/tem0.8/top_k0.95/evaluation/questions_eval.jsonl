{"ID": 38, "Question": "### Background\n\nIn an experiment on electricity demand, researchers exposed households to two types of high-price events:\n\n- **Day-Ahead (DA) Events:** Households received 24-hour advance notification of a moderate price increase (e.g., from ~$0.20/kWh to $0.70/kWh).\n- **30-Minute (TM) Events:** Households received 30-minute advance notification of a much larger price increase (e.g., from ~$0.20/kWh to $1.45/kWh).\n\n---\n\n### Question\n\nWhat is the primary behavioral trade-off that this experimental design choice—including both DA and TM events—allows the researchers to investigate?", "Options": {"A": "The difference between short-run and long-run price elasticity of demand.", "B": "Whether households respond more to price increases in the afternoon versus the evening.", "C": "The relative importance of the price signal's magnitude versus the household's ability to plan and overcome adjustment costs.", "D": "Whether households who own their homes are more price-sensitive than households who rent."}, "Answer": "C", "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.5). It assesses the ability to interpret the logic behind a specific experimental design choice. The strategy is 'Conceptual Interpretation'. The correct answer (C) accurately captures the trade-off between the size of the incentive (price) and the constraints on action (time to plan). Distractors represent plausible but incorrect interpretations: (A) confuses within-day variation with the long-run/short-run distinction; (B) focuses on a secondary feature (timing) rather than the core design contrast; (D) describes a potential analysis of heterogeneity, not the primary purpose of the DA vs. TM variation.", "qid": "38", "question": "### Background\n\nIn an experiment on electricity demand, researchers exposed households to two types of high-price events:\n\n- **Day-Ahead (DA) Events:** Households received 24-hour advance notification of a moderate price increase (e.g., from ~$0.20/kWh to $0.70/kWh).\n- **30-Minute (TM) Events:** Households received 30-minute advance notification of a much larger price increase (e.g., from ~$0.20/kWh to $1.45/kWh).\n\n---\n\n### Question\n\nWhat is the primary behavioral trade-off that this experimental design choice—including both DA and TM events—allows the researchers to investigate?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 3, "Question": "### Background\n\nIn Multiple Imputation (MI), results from *m* analyses of imputed datasets are combined. The total variance ($T$) of the final point estimate is calculated to account for two sources of uncertainty:\n\n1.  **Within-Imputation Variance ($\\overline{U}$):** The average of the estimated variances from each of the *m* analyses. This captures standard **sampling variance**.\n2.  **Between-Imputation Variance ($B$):** The variance of the *m* point estimates across the datasets. This captures **imputation uncertainty**.\n\nThe total variance is given by:\n```latex\nT = \\overline{U} + \\left(1 + \\frac{1}{m}\\right)B \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nSingle imputation creates a complete dataset but yields invalid confidence intervals because it ignores imputation uncertainty. How does Multiple Imputation (MI), using the variance formula provided in Eq. (1), solve this specific problem?", "Options": {"A": "By using the between-imputation variance ($B$) to explicitly quantify the uncertainty introduced by the missing data, and adding it to the sampling variance ($\\overline{U}$) to calculate the total variance ($T$).", "B": "By using the within-imputation variance ($\\overline{U}$) to measure the uncertainty from missing data, while the between-imputation variance ($B$) captures sampling error.", "C": "By averaging the point estimates across multiple datasets, which produces a more accurate point estimate than any single imputation.", "D": "By creating multiple datasets, which increases the overall sample size and statistical power compared to listwise deletion."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the core inferential advantage of Multiple Imputation over Single Imputation. It uses a Reverse-Reasoning strategy, asking how MI's formal machinery solves a known problem. The distractors are designed based on common errors: B reverses the roles of the variance components (Formula Misuse), C states a true but tangential benefit of MI (Almost Right), and D confuses the benefits of MI over listwise deletion with its specific advantage over single imputation.", "qid": "3", "question": "### Background\n\nIn Multiple Imputation (MI), results from *m* analyses of imputed datasets are combined. The total variance ($T$) of the final point estimate is calculated to account for two sources of uncertainty:\n\n1.  **Within-Imputation Variance ($\\overline{U}$):** The average of the estimated variances from each of the *m* analyses. This captures standard **sampling variance**.\n2.  **Between-Imputation Variance ($B$):** The variance of the *m* point estimates across the datasets. This captures **imputation uncertainty**.\n\nThe total variance is given by:\n```latex\nT = \\overline{U} + \\left(1 + \\frac{1}{m}\\right)B \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nSingle imputation creates a complete dataset but yields invalid confidence intervals because it ignores imputation uncertainty. How does Multiple Imputation (MI), using the variance formula provided in Eq. (1), solve this specific problem?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 36, "Question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to the `Price+IHD` group, which provided real-time energy usage information. However, only 72% of assigned households ultimately complied and had a functioning device installed. The Intent-to-Treat (ITT) effect—the average effect on all *assigned* households—was estimated to be a 13.7% reduction in electricity usage (-0.137 in log terms).\n\nThe Treatment-on-the-Treated (ToT) effect measures the causal effect only on the sub-population of compliers. It can be calculated by scaling the ITT effect by the compliance rate.\n\n### Data / Model Specification\n\n- **ITT Estimate for `Price+IHD`:** -0.137\n- **Compliance Rate for `Price+IHD`:** 0.72\n\n---\n\n### Question\n\nUsing the provided data, what is the implied Treatment-on-the-Treated (ToT) estimate, and what is the economic intuition for its relationship to the ITT estimate?", "Options": {"A": "The ToT estimate is -0.099 (-0.137 * 0.72). Its magnitude is smaller than the ITT because it applies to a smaller, more selective group of households.", "B": "The ToT estimate is -0.190 (-0.137 / 0.72). Its magnitude is larger than the ITT because the ITT effect is diluted by including non-compliers who had a zero treatment effect.", "C": "The ToT estimate is -0.137, the same as the ITT. The estimate is valid for compliers because the initial randomization ensures the groups are comparable.", "D": "The ToT estimate is -0.190 (-0.137 / 0.72). Its magnitude is larger than the ITT because the compliers were likely more motivated to save energy from the start."}, "Answer": "B", "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.0). It uses the 'Computational Judgment' strategy to test the mathematical and intuitive relationship between ITT and ToT. The correct answer (B) provides the correct calculation (scaling up the ITT by the compliance rate) and the correct intuition (ITT is a diluted average). Distractors are based on predictable errors: (A) uses the wrong formula (multiplication instead of division) and faulty intuition; (C) fails to make any adjustment; (D) arrives at the correct number but provides subtly incorrect intuition, attributing the difference to selection bias rather than the mechanical process of the Wald estimator.", "qid": "36", "question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to the `Price+IHD` group, which provided real-time energy usage information. However, only 72% of assigned households ultimately complied and had a functioning device installed. The Intent-to-Treat (ITT) effect—the average effect on all *assigned* households—was estimated to be a 13.7% reduction in electricity usage (-0.137 in log terms).\n\nThe Treatment-on-the-Treated (ToT) effect measures the causal effect only on the sub-population of compliers. It can be calculated by scaling the ITT effect by the compliance rate.\n\n### Data / Model Specification\n\n- **ITT Estimate for `Price+IHD`:** -0.137\n- **Compliance Rate for `Price+IHD`:** 0.72\n\n---\n\n### Question\n\nUsing the provided data, what is the implied Treatment-on-the-Treated (ToT) estimate, and what is the economic intuition for its relationship to the ITT estimate?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 2, "Question": "### Background\n\nCommon methods for handling missing data include:\n- **Listwise Deletion:** Discarding any observation with one or more missing values.\n- **Single Imputation:** Filling in each missing value with a single plausible value (e.g., a regression prediction) to create one complete dataset.\n- **Multiple Imputation (MI):** Creating *m* complete datasets by filling in missing values with *m* different plausible draws from a predictive distribution, analyzing each dataset, and then combining the results.\n\nThe primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nListwise deletion is a common but problematic method for handling missing data. Under which specific condition will it produce biased parameter estimates, and why is it always considered statistically inefficient?", "Options": {"A": "It produces bias when the data are Missing at Random (MAR) but not MCAR, and it is always inefficient because it discards information from partially complete cases.", "B": "It produces bias when the data are MCAR, and it is inefficient only when the data are MAR.", "C": "It is always unbiased as long as the missingness is ignorable (MAR), but it is inefficient because it requires complex calculations.", "D": "It produces bias by imputing incorrect values for missing data, and it is inefficient because it can only be used for linear models."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the two primary statistical failures of listwise deletion: the condition for bias and the reason for inefficiency. The distractors are designed to capture common errors: B reverses the conditions (Conceptual Opposite), C makes an incorrect claim about bias under MAR (Almost Right), and D confuses deletion with imputation (Conceptual Conflation).", "qid": "2", "question": "### Background\n\nCommon methods for handling missing data include:\n- **Listwise Deletion:** Discarding any observation with one or more missing values.\n- **Single Imputation:** Filling in each missing value with a single plausible value (e.g., a regression prediction) to create one complete dataset.\n- **Multiple Imputation (MI):** Creating *m* complete datasets by filling in missing values with *m* different plausible draws from a predictive distribution, analyzing each dataset, and then combining the results.\n\nThe primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nListwise deletion is a common but problematic method for handling missing data. Under which specific condition will it produce biased parameter estimates, and why is it always considered statistically inefficient?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 32, "Question": "### Background\n\nConsider an agent evaluating a prospect where information arrives sequentially. The agent's ex-ante evaluation is `E(X)`. After observing an event `B` from a partition `\\mathcal{P}`, the agent forms a conditional evaluation `E(X|B)`. For the specific example in the paper involving two draws, the ex-ante value of a bet `X` is `E(X) = 1/2`. When updating beliefs using the **prior-by-prior** rule, the conditional evaluations are `E^m(X|R_2) = 1/3` and `E^m(X|G_2) = 1/6`. Both values are strictly less than the ex-ante value, a phenomenon known as \"all news is bad news.\"\n\n---\n\n### Question\n\nWhat is the core reason that the prior-by-prior updating rule is susceptible to the \"all news is bad news\" phenomenon, while the paper's proposed proxy update rule is not?", "Options": {"A": "Prior-by-prior updating allows the agent to select a different worst-case prior for each possible information event, while proxy updating commits the agent to a single proxy belief structure before conditioning.", "B": "The proxy update rule always satisfies the law of iterated expectations, whereas the prior-by-prior rule does not.", "C": "Prior-by-prior updating is only defined for totally monotone capacities, while proxy updating works for any capacity.", "D": "The proxy update rule transforms the set of priors into a single Bayesian prior before updating, eliminating all ambiguity."}, "Answer": "A", "pi_justification": "Assessment Target: This question assesses the student's understanding of the fundamental behavioral difference between the proxy update rule and its main alternative, prior-by-prior updating. Strategy: Conceptual distinction. The question asks for the 'why' behind a key difference. Distractor Logic: (A) Correct answer, capturing the re-optimization vs. commitment distinction. (B) Conceptual Opposite: Both rules can violate the law of iterated expectations; this is a central point of the paper. (C) Factual Error: Both rules are typically discussed in the context of totally monotone (or at least supermodular) capacities. (D) Almost Right: The proxy update makes the *partition elements* unambiguous, but it does not necessarily eliminate all ambiguity from the conditional beliefs.", "qid": "32", "question": "### Background\n\nConsider an agent evaluating a prospect where information arrives sequentially. The agent's ex-ante evaluation is `E(X)`. After observing an event `B` from a partition `\\mathcal{P}`, the agent forms a conditional evaluation `E(X|B)`. For the specific example in the paper involving two draws, the ex-ante value of a bet `X` is `E(X) = 1/2`. When updating beliefs using the **prior-by-prior** rule, the conditional evaluations are `E^m(X|R_2) = 1/3` and `E^m(X|G_2) = 1/6`. Both values are strictly less than the ex-ante value, a phenomenon known as \"all news is bad news.\"\n\n---\n\n### Question\n\nWhat is the core reason that the prior-by-prior updating rule is susceptible to the \"all news is bad news\" phenomenon, while the paper's proposed proxy update rule is not?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 78, "Question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms generally fail, and what specific design features are required to solve this problem?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy. Simple linear mechanisms (like the Shareholding and Helpman-Razin mechanisms) fail because they cannot consistently align stockholders' incentives. A more complex, non-linear mechanism is proposed to solve this problem.\n\n---\n\n### Data / Model Specification\n\nThe **Shared Cost Mechanism** is designed to overcome the failures of simpler linear mechanisms. Its key features are:\n-   Message: Each stockholder `i` sends a message `m_i = (x_i, a_i)`, where `x_i` is a vector of proposed input increments and `a_i` is a vector of proposed cost share transfers.\n-   Allocation Rule: `x_j = \\sum_i x_{ij}`\n-   Cost-Sharing Rule:\n    ```latex\n    C_{ij}(m) = (1 - \\sum_{k \\ne i} a_{kj}) x_j + (1 - \\sum_k a_{kj})^2 \\quad \\text{(Eq. (1))}\n    ```\n\n---\n\nHow does the design of the Shared Cost Mechanism, particularly its cost-sharing rule (Eq. (1)), successfully guarantee the existence of an equilibrium where simpler linear mechanisms fail?", "Options": {"A": "It allows stockholders to directly communicate their desired input levels (`x_{ij}`), ensuring the final investment `x_j` reflects the average preference of all stockholders.", "B": "It fixes each stockholder's cost share to be equal to their initial endowed revenue share (`\\bar{\\theta}_{ij}`), which provides a stable and predictable financing plan.", "C": "It endogenizes cost shares via the `a_{ij}` message and uses the quadratic term to create an incentive for these shares to sum to one, providing the flexibility needed to align heterogeneous preferences.", "D": "The quadratic term ensures that the total costs collected from all stockholders exactly equal the total investment (`\\sum_i C_{ij}(m) = x_j`), preventing any budget shortfalls for the firm."}, "Answer": "C", "pi_justification": "This item assesses understanding of the paper's central innovation: the design of the Shared Cost Mechanism. It uses a Reverse-Reasoning strategy, asking for the specific design features that ensure equilibrium existence. The distractors are designed to isolate the critical components: (A) focuses on a necessary but insufficient part of the message, (B) proposes a solution the paper explicitly rejects, and (D) offers a correct but less precise explanation of the quadratic term's function, making the correct answer superior.", "qid": "78", "question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms generally fail, and what specific design features are required to solve this problem?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy. Simple linear mechanisms (like the Shareholding and Helpman-Razin mechanisms) fail because they cannot consistently align stockholders' incentives. A more complex, non-linear mechanism is proposed to solve this problem.\n\n---\n\n### Data / Model Specification\n\nThe **Shared Cost Mechanism** is designed to overcome the failures of simpler linear mechanisms. Its key features are:\n-   Message: Each stockholder `i` sends a message `m_i = (x_i, a_i)`, where `x_i` is a vector of proposed input increments and `a_i` is a vector of proposed cost share transfers.\n-   Allocation Rule: `x_j = \\sum_i x_{ij}`\n-   Cost-Sharing Rule:\n    ```latex\n    C_{ij}(m) = (1 - \\sum_{k \\ne i} a_{kj}) x_j + (1 - \\sum_k a_{kj})^2 \\quad \\text{(Eq. (1))}\n    ```\n\n---\n\nHow does the design of the Shared Cost Mechanism, particularly its cost-sharing rule (Eq. (1)), successfully guarantee the existence of an equilibrium where simpler linear mechanisms fail?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 43, "Question": "### Background\n\nThe paper models a household's income process to construct a measure of idiosyncratic risk. The model for log income, `ln Y_it`, is:\n\n```latex\n\\ln Y_{i t}=\\alpha+X_{i t}^{\\prime}\\beta+\\epsilon_{i t} \\quad \\text{(Eq. (1))}\n```\n\nThe error term `ε_it` is decomposed into three components:\n\n```latex\n\\epsilon_{i t}=\\eta_{i}+\\nu_{i t} \\quad \\text{where} \\quad \\nu_{i t}=\\rho\\nu_{i t-1}+\\omega_{i t} \\quad \\text{(Eq. (2))}\n```\n\n- `η_i` is a time-invariant household-specific effect.\n- `ν_it` is a serially correlated transitory shock.\n- `ω_it` is an i.i.d. innovation error.\n\nThe paper uses the household-specific variance of the estimated innovation, `Var(ω̂_it)`, as its primary measure of income risk.\n\n### Question\n\nWhy is the variance of the innovation term `ω_it` the most appropriate measure of income risk for testing the theory of precautionary savings, as opposed to the variance of the other error components?", "Options": {"A": "Because `ω_it` is the only component that captures the true, unpredictable shocks to income that households cannot anticipate.", "B": "Because the time-invariant effect `η_i` represents the largest source of income variation between households.", "C": "Because the total residual `ε_it` captures all sources of unobserved income variation, making its variance the most comprehensive measure of risk.", "D": "Because the serially correlated shock `ν_it` captures persistent income shocks, which are the primary concern for long-term savings."}, "Answer": "A", "pi_justification": "This item assesses understanding of the paper's core variable construction. It uses a conceptual strategy to test whether students can distinguish between predictable and unpredictable components of an income process. The correct answer (A) identifies the i.i.d. innovation as the source of true uncertainty. Distractors are designed based on common misconceptions: (B) confuses risk with permanent heterogeneity (Conceptual Opposite); (C) fails to distinguish between predictable and unpredictable variation (Formula Misuse); (D) correctly identifies `ν_it` as persistent but incorrectly identifies it as the target for precautionary motives, as part of it is predictable (Almost Right).", "qid": "43", "question": "### Background\n\nThe paper models a household's income process to construct a measure of idiosyncratic risk. The model for log income, `ln Y_it`, is:\n\n```latex\n\\ln Y_{i t}=\\alpha+X_{i t}^{\\prime}\\beta+\\epsilon_{i t} \\quad \\text{(Eq. (1))}\n```\n\nThe error term `ε_it` is decomposed into three components:\n\n```latex\n\\epsilon_{i t}=\\eta_{i}+\\nu_{i t} \\quad \\text{where} \\quad \\nu_{i t}=\\rho\\nu_{i t-1}+\\omega_{i t} \\quad \\text{(Eq. (2))}\n```\n\n- `η_i` is a time-invariant household-specific effect.\n- `ν_it` is a serially correlated transitory shock.\n- `ω_it` is an i.i.d. innovation error.\n\nThe paper uses the household-specific variance of the estimated innovation, `Var(ω̂_it)`, as its primary measure of income risk.\n\n### Question\n\nWhy is the variance of the innovation term `ω_it` the most appropriate measure of income risk for testing the theory of precautionary savings, as opposed to the variance of the other error components?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 56, "Question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\nIn the model, large 'contingent spoils' (`γ`) that are only realized after peace is achieved create a high 'cost of delay' for any peaceful settlement, making war more likely.\n\n### Question\n\nAn international actor wants to prevent a civil war in a country where the primary driver of conflict is the existence of massive, easily accessible oil revenues that will become available only once the state is stable (i.e., `γ` is extremely large). Which of the following interventions is likely to be most effective at preventing conflict, according to the model's logic?\n", "Options": {"A": "A 'Good Governance' program to increase implementation capacity (`c`), because it directly speeds up the multi-period buyout process, reducing the delay in realizing the spoils.", "B": "A 'Rule of Law' program to increase commitment capacity (`h`), because it makes the government appear more trustworthy to international partners.", "C": "A 'Good Governance' program to increase implementation capacity (`c`), because with large spoils at stake, the government needs certainty that its consolidation efforts will succeed.", "D": "A 'Rule of Law' program to increase commitment capacity (`h`), because it can solve the government's 'liquidity problem,' potentially allowing a one-period buyout that eliminates the costly delay."}, "Answer": "D", "pi_justification": "This item tests deep application of the model's logic to a policy problem. It uses a Reverse-Reasoning strategy: given that the 'cost of delay' is the key problem, the student must identify the most effective solution. The correct option (D) recognizes that increasing commitment capacity (`h`) offers a more fundamental solution by enabling a single-period settlement, which is superior to marginally speeding up a multi-period one (the effect of `c`). Distractors represent plausible but less effective solutions (A, C) or reasoning external to the model (B).", "qid": "56", "question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\nIn the model, large 'contingent spoils' (`γ`) that are only realized after peace is achieved create a high 'cost of delay' for any peaceful settlement, making war more likely.\n\n### Question\n\nAn international actor wants to prevent a civil war in a country where the primary driver of conflict is the existence of massive, easily accessible oil revenues that will become available only once the state is stable (i.e., `γ` is extremely large). Which of the following interventions is likely to be most effective at preventing conflict, according to the model's logic?\n", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 44, "Question": "### Background\n\nThe paper uses a two-stage estimation procedure. In the first stage, an income process is estimated to generate measures of permanent income and income risk. In the second stage, these generated variables are used to predict household portfolio choice (e.g., the share of wealth held in liquid assets).\n\nTo identify the causal effect of risk, the model relies on an exclusion restriction: occupation dummies are included as predictors in the first-stage income regression but are excluded from the second-stage portfolio choice regression.\n\n### Question\n\nWhich of the following scenarios, if true, would most directly violate the key identifying assumption of this research design?", "Options": {"A": "Occupation is a very strong predictor of a household's permanent income and income stability.", "B": "Farmers have a systematic need to hold large grain stocks as a productive input (e.g., for seed), independent of their income risk.", "C": "Households with higher education levels tend to have both higher income and a lower preference for liquid assets.", "D": "The measure of household income contains random measurement error that is uncorrelated with occupation."}, "Answer": "B", "pi_justification": "This item assesses the ability to critique the paper's identification strategy. It uses a Scenario Application strategy. The correct answer (B) describes a direct channel through which occupation affects the portfolio outcome variable (liquid wealth share) for reasons other than income, thus violating the exclusion restriction. Distractors are designed to be plausible but incorrect: (A) describes a condition required for the instrument to be strong, not a violation (True but Irrelevant); (C) describes a standard confounding variable that is handled by including education as a control in the second stage (True but Irrelevant); (D) describes a measurement error problem that does not violate this specific assumption (Conceptual Opposite).", "qid": "44", "question": "### Background\n\nThe paper uses a two-stage estimation procedure. In the first stage, an income process is estimated to generate measures of permanent income and income risk. In the second stage, these generated variables are used to predict household portfolio choice (e.g., the share of wealth held in liquid assets).\n\nTo identify the causal effect of risk, the model relies on an exclusion restriction: occupation dummies are included as predictors in the first-stage income regression but are excluded from the second-stage portfolio choice regression.\n\n### Question\n\nWhich of the following scenarios, if true, would most directly violate the key identifying assumption of this research design?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 55, "Question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\n### Question\n\nConsider two states facing identical, very strong rebel groups. State A has a **credibility crisis** (very low `h`, high `c`); its promises are worthless, but it is efficient at implementing policies. State B has an **implementation crisis** (high `h`, very low `c`); its promises are credible, but its bureaucracy is ineffective. According to the model, which state is more likely to experience a civil war, and why?\n", "Options": {"A": "State B, because its low implementation capacity (`c`) makes the peaceful consolidation process too slow and unreliable, forcing the government to fight.", "B": "State A, because its high implementation capacity (`c`) makes the government overconfident in its ability to win a war.", "C": "State A, because its low commitment capacity (`h`) creates a severe 'liquidity problem,' making it unable to make a credible offer large enough to buy out a strong rebel group.", "D": "State B, because its high commitment capacity (`h`) would require the government to make costly promises it would rather avoid by fighting."}, "Answer": "C", "pi_justification": "This item assesses the ability to distinguish between two forms of state capacity and apply them to a comparative scenario. It uses a Scenario Application strategy. The correct option (C) requires identifying that commitment capacity (`h`) is the binding constraint when dealing with strong rebels who require large payoffs. Distractors are designed to test common points of confusion: (A) over-weighting the importance of implementation speed, (B) introducing an outside behavioral logic, and (D) misstating the government's preferences.", "qid": "55", "question": "### Background\n\nThis problem examines how two distinct dimensions of state capacity—the ability to make credible commitments and the ability to effectively implement policies—affect the likelihood of civil conflict. The baseline model assumes a weak state that cannot commit to future transfers and has perfect policy implementation.\n\n### Data / Model Specification\n\nTwo extensions to the model consider different types of state capacity:\n\n1.  **Commitment Capacity (`h`):** The government can make credible promises about sharing future spoils. A higher `h ∈ [0,1]` means these promises are more likely to be honored, allowing the government to offer large future transfers to buy out rebels, thus addressing its 'liquidity problem'.\n\n2.  **Implementation Capacity (`c`):** The government's non-violent efforts to consolidate power (e.g., weakening the opposition) succeed with probability `c ∈ [0,1]`. A higher `c` increases the speed and reliability of a peaceful buyout process.\n\n### Question\n\nConsider two states facing identical, very strong rebel groups. State A has a **credibility crisis** (very low `h`, high `c`); its promises are worthless, but it is efficient at implementing policies. State B has an **implementation crisis** (high `h`, very low `c`); its promises are credible, but its bureaucracy is ineffective. According to the model, which state is more likely to experience a civil war, and why?\n", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 81, "Question": "### Background\n\nIn an instrumental variable (IV) model with multiple instruments, some instruments may be more problematic than others. A key diagnostic is the conditional first-stage F-statistic, which measures the strength of an instrument after controlling for the other instruments. An F-statistic below 10 typically indicates a 'weak instrument,' which can lead to unreliable and biased 2SLS estimates.\n\n### Data / Model Specification\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan, Railroads, and Exploration. The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n### Question\n\nBased on the evidence in Table 1, which instrument is most problematic when used in a model that controls for the other instruments, and what is the primary statistical reason?", "Options": {"A": "'Plan', because its 2SLS estimate (0.18) is the smallest, suggesting it may be biased downwards.", "B": "'Railroads', because its 2SLS estimate (3.65) is an extreme outlier compared to the others.", "C": "'Exploration', because its F-statistic (27.0) is significantly lower than that of 'Plan' (54.8).", "D": "'Railroads', because its conditional first-stage F-statistic is extremely low, indicating it is a weak instrument."}, "Answer": "D", "pi_justification": "This item assesses the ability to interpret standard econometric diagnostics to evaluate instrument validity, a critical skill for applying the paper's methods. It uses a Reverse-Reasoning strategy (inferring the problematic instrument from its statistical properties). The correct answer identifies the instrument with the critically low F-statistic. Distractor B identifies the correct instrument but for a secondary reason (the outlier estimate is a *symptom* of the weak first stage, not the root cause), making it an 'Almost Right' conceptual error. Distractor C is 'True but Irrelevant' because an F-stat of 27 is still very strong. Distractor A offers an economically irrelevant reason.", "qid": "81", "question": "### Background\n\nIn an instrumental variable (IV) model with multiple instruments, some instruments may be more problematic than others. A key diagnostic is the conditional first-stage F-statistic, which measures the strength of an instrument after controlling for the other instruments. An F-statistic below 10 typically indicates a 'weak instrument,' which can lead to unreliable and biased 2SLS estimates.\n\n### Data / Model Specification\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan, Railroads, and Exploration. The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n### Question\n\nBased on the evidence in Table 1, which instrument is most problematic when used in a model that controls for the other instruments, and what is the primary statistical reason?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 47, "Question": "## Background\n\n**Research Question.** This problem aims to estimate the causal effect of providing social information on donation amounts and to understand the properties of the estimators used.\n\n**Setting / Institutional Environment.** The analysis is based on a randomized controlled trial at a public radio station. Callers were randomly assigned to a control group or treatment groups. The study uses regression analysis to control for other observable characteristics.\n\n---\n\n## Data / Model Specification\n\nThe relationship between social information and pledge amounts is estimated using Ordinary Least Squares (OLS) and Robust Regression. Robust regression is a method designed to be less sensitive to outliers than OLS.\n\n**Table 1: The Social Information Effect (standard errors in parentheses)**\n\n| | All data (OLS) | All data (Robust) |\n|:---|:---:|:---:|\n| **$300** | 39.599** (13.609) | 10.710* (4.730) |\n*Controls for other treatments and covariates included. **p < 0.01, *p < 0.05*\n\n---\n\n## Question\n\nThe OLS estimate for the `$300` treatment effect (39.599) is nearly four times larger than the robust regression estimate (10.710). Which of the following statements provides the best explanation for this discrepancy?", "Options": {"A": "The OLS estimator for a treatment effect is equivalent to the difference in group means, which is highly sensitive to a few large outlier donations in the treatment group that the robust regression method down-weights.", "B": "The robust regression is likely biased towards zero, while the OLS estimate is unbiased and therefore more reliable.", "C": "The discrepancy is caused by heteroskedasticity in the error term, which biases OLS coefficients but is corrected by robust regression.", "D": "The control variables included in the model must be more influential in the OLS specification than in the robust regression, thus accounting for the difference."}, "Answer": "A", "pi_justification": "This item assesses the understanding of how different estimators behave in the presence of outliers. It uses a 'Reverse-Reasoning' strategy, asking for the cause of an observed empirical pattern. The correct answer (A) correctly identifies that OLS is a difference-in-means and is sensitive to outliers, which robust regression is designed to mitigate. Distractors target common econometric confusions: (B) is a 'Conceptual Opposite', incorrectly claiming OLS is more reliable here; (C) confuses robust *regression* (which handles outliers) with robust *standard errors* (which handle heteroskedasticity); (D) offers an incorrect explanation, as the difference is driven by the estimator's sensitivity to the dependent variable's distribution, not the controls.", "qid": "47", "question": "## Background\n\n**Research Question.** This problem aims to estimate the causal effect of providing social information on donation amounts and to understand the properties of the estimators used.\n\n**Setting / Institutional Environment.** The analysis is based on a randomized controlled trial at a public radio station. Callers were randomly assigned to a control group or treatment groups. The study uses regression analysis to control for other observable characteristics.\n\n---\n\n## Data / Model Specification\n\nThe relationship between social information and pledge amounts is estimated using Ordinary Least Squares (OLS) and Robust Regression. Robust regression is a method designed to be less sensitive to outliers than OLS.\n\n**Table 1: The Social Information Effect (standard errors in parentheses)**\n\n| | All data (OLS) | All data (Robust) |\n|:---|:---:|:---:|\n| **$300** | 39.599** (13.609) | 10.710* (4.730) |\n*Controls for other treatments and covariates included. **p < 0.01, *p < 0.05*\n\n---\n\n## Question\n\nThe OLS estimate for the `$300` treatment effect (39.599) is nearly four times larger than the robust regression estimate (10.710). Which of the following statements provides the best explanation for this discrepancy?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 51, "Question": "### Background\n\nIn modeling family labor supply, a key challenge is that many women do not participate in the labor market, creating a sample of working women that is not random. The paper estimates its model on a sample of households where the wife works positive hours.\n\n### The Question\n\nAn analyst estimates a family labor supply model using data only from households where the wife works positive hours. Standard regression methods (e.g., Seemingly Unrelated Regressions) applied to this selected sample yield biased and inconsistent estimates for the male labor supply equation. What is the fundamental econometric reason for this bias?", "Options": {"A": "The sample of working wives is not representative of the general population of all wives, violating the assumption of random sampling.", "B": "The smaller sample size leads to less precise estimates and a loss of statistical power.", "C": "The wife's participation decision is an omitted variable in the husband's labor supply equation.", "D": "Unobserved factors affecting the wife's participation are correlated with unobserved determinants of the husband's labor supply, inducing a correlation between the model's regressors and its error term."}, "Answer": "D", "pi_justification": "This question assesses the core concept of sample selection bias. It uses a Reverse-Reasoning strategy, asking for the fundamental cause of a known problem. Distractor A is 'Almost Right' but lacks the crucial link to the error term. Distractor B confuses inconsistency with inefficiency. Distractor C misidentifies the problem as omitted variable bias.", "qid": "51", "question": "### Background\n\nIn modeling family labor supply, a key challenge is that many women do not participate in the labor market, creating a sample of working women that is not random. The paper estimates its model on a sample of households where the wife works positive hours.\n\n### The Question\n\nAn analyst estimates a family labor supply model using data only from households where the wife works positive hours. Standard regression methods (e.g., Seemingly Unrelated Regressions) applied to this selected sample yield biased and inconsistent estimates for the male labor supply equation. What is the fundamental econometric reason for this bias?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 66, "Question": "### Background\n\nTo identify the causal impact of black in-migration on white population change, a study uses an instrumental variable (IV). The instrument interacts historical, pre-1900 settlement patterns of blacks from specific southern states with decade-by-decade out-migration shocks from those same states.\n\n### Question\n\nThe validity of this IV strategy rests on the exclusion restriction. Which of the following scenarios would most clearly violate this assumption?", "Options": {"A": "A severe boll weevil infestation in Georgia simultaneously pushes black agricultural workers to migrate north and disrupts the cotton supply to textile mills in northern neighborhoods where Georgian migrants historically settled, causing an independent economic decline in those specific neighborhoods.", "B": "Social networks from the 19th century weaken over time, so that by 1920, settlement patterns of new migrants from a given southern state are no longer correlated with the settlement patterns of migrants from that same state before 1900.", "C": "The overall economic boom of the 1920s increases population growth in all northern cities, an effect that is absorbed by the model's city-level fixed effects.", "D": "Newly arriving black migrants tend to settle in neighborhoods that are already experiencing population growth due to factors like new housing construction, which is the primary source of endogeneity the IV is designed to correct."}, "Answer": "A", "pi_justification": "Assessment Target: Understanding the exclusion restriction in a specific research context. Strategy: Scenario Application. Distractor Logic: The correct answer (A) describes a classic violation: the instrument (driven by the Georgia shock) has a direct effect on the outcome (white population change) through a channel other than the endogenous variable (a local economic shock). Distractor (B) describes a violation of the *relevance* assumption, not the exclusion restriction. Distractor (C) describes a scenario that is correctly handled by the model's fixed effects. Distractor (D) describes the endogeneity problem itself, not a failure of the proposed solution.", "qid": "66", "question": "### Background\n\nTo identify the causal impact of black in-migration on white population change, a study uses an instrumental variable (IV). The instrument interacts historical, pre-1900 settlement patterns of blacks from specific southern states with decade-by-decade out-migration shocks from those same states.\n\n### Question\n\nThe validity of this IV strategy rests on the exclusion restriction. Which of the following scenarios would most clearly violate this assumption?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 7, "Question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`.\n\n### Question\n\nAs censoring disappears (the probability of observation `\\psi_1 \\to 1`), the model becomes point-identified. In this limit, the posterior probability that `\\lambda` falls within the estimated set `\\Lambda(\\hat{\\psi})` approaches zero. Why does a 95% Bayesian Highest Posterior Density (HPD) interval not suffer from this issue?", "Options": {"A": "Because the HPD interval is constructed to contain 95% of the posterior mass by definition, while `\\Lambda(\\hat{\\psi})` is an estimate of a set whose width shrinks to zero.", "B": "Because the Bayesian HPD interval's width is fixed by the prior and does not shrink as the data become more informative.", "C": "Because the Bayesian HPD interval always converges to the full parameter space, `[0,1]`, as more data is collected.", "D": "Because the Bayesian analysis uses a prior that prevents the probability `\\psi_1` from ever reaching 1."}, "Answer": "A", "pi_justification": "Assessment Target: Tests understanding of the fundamental difference between a Bayesian credible interval and a frequentist set estimate in the limit of point-identification. Strategy: Reverse-Reasoning. The user must explain why one method avoids a paradox. Distractor Logic: B) is a Conceptual Opposite; HPD intervals shrink with more information. C) is also a Conceptual Opposite; they converge to a point, not the whole space. D) describes a possible prior choice but is not the fundamental reason the HPD interval behaves correctly.", "qid": "7", "question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`.\n\n### Question\n\nAs censoring disappears (the probability of observation `\\psi_1 \\to 1`), the model becomes point-identified. In this limit, the posterior probability that `\\lambda` falls within the estimated set `\\Lambda(\\hat{\\psi})` approaches zero. Why does a 95% Bayesian Highest Posterior Density (HPD) interval not suffer from this issue?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 24, "Question": "### Background\n\n**Research Question.** This problem explores an alternative, one-step GLS approach to combining micro and macro forecasts and its connection to Bayesian updating.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}` by combining two sources of information: a direct forecast `ŷ_{T+1}^{(2)}` and a forecast of a linear combination of `y_{T+1}`, `η̂_{T+1}^{(1)}`.\n\n### Data / Model Specification\n\nThe problem can be framed in Bayesian terms as follows:\n*   **Prior:** Our initial belief about `y_{T+1}` comes from the micro model: `y_{T+1} ∼ N_p(ŷ_{T+1}^{(2)}, Σ)`.\n*   **Likelihood:** The macro forecast is viewed as data providing a signal about `y_{T+1}`: `η̂_{T+1}^{(1)} | y_{T+1} ∼ N(a'y_{T+1}, σ₁²)`.\n\nThe posterior distribution is then derived by combining the prior and the likelihood using Bayes' rule.\n\nWhen framing the forecast combination problem in these Bayesian terms, how are the initial micro and macro forecasts used?", "Options": {"A": "The macro forecast serves as the prior belief, which is updated using the micro forecast as the data.", "B": "The micro forecast serves as the prior belief, which is updated using the macro forecast as the data.", "C": "Both forecasts are combined to form a joint likelihood function, with a flat, uninformative prior.", "D": "The micro forecast is the parameter to be estimated, and the macro forecast is the posterior mean."}, "Answer": "B", "pi_justification": "Assessment Target: Tests for a sharp understanding of how the Bayesian paradigm is applied in this context. Strategy: Scenario Application (applying Bayesian terminology). Distractor Logic: (A) is the classic Conceptual Opposite error of reversing the roles of prior and data/likelihood. (C) describes an alternative, valid Bayesian approach, but it is not the one specified in the problem stem (True but Irrelevant). (D) demonstrates a confusion of basic Bayesian terms.", "qid": "24", "question": "### Background\n\n**Research Question.** This problem explores an alternative, one-step GLS approach to combining micro and macro forecasts and its connection to Bayesian updating.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}` by combining two sources of information: a direct forecast `ŷ_{T+1}^{(2)}` and a forecast of a linear combination of `y_{T+1}`, `η̂_{T+1}^{(1)}`.\n\n### Data / Model Specification\n\nThe problem can be framed in Bayesian terms as follows:\n*   **Prior:** Our initial belief about `y_{T+1}` comes from the micro model: `y_{T+1} ∼ N_p(ŷ_{T+1}^{(2)}, Σ)`.\n*   **Likelihood:** The macro forecast is viewed as data providing a signal about `y_{T+1}`: `η̂_{T+1}^{(1)} | y_{T+1} ∼ N(a'y_{T+1}, σ₁²)`.\n\nThe posterior distribution is then derived by combining the prior and the likelihood using Bayes' rule.\n\nWhen framing the forecast combination problem in these Bayesian terms, how are the initial micro and macro forecasts used?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 99, "Question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n---\n\n### Data / Model Specification\n\nThe Empirical Likelihood (EL) test statistic for the null hypothesis `H₀: E[g(X, θ)] = 0` for some `θ` is denoted `T_n`. A small value of `T_n` indicates that the model's restrictions are compatible with the observed data.\n\n---\n\n### Question\n\nAn econometrician performs the EL overidentification test and obtains a small value for the test statistic `T_n`, leading to a failure to reject the null hypothesis `H₀`. What is the correct interpretation of this result and the primary inferential risk associated with it?", "Options": {"A": "The result suggests the model's restrictions are compatible with the data, but the primary risk is a Type II error, as the test may have low power to detect meaningful model misspecifications.", "B": "The result indicates a severe model misspecification, and the primary risk is a Type I error (falsely rejecting a valid model).", "C": "The result proves the model is correctly specified, and the main remaining risk is that the parameter estimates `θ` are biased in finite samples.", "D": "The small `T_n` value implies the instruments are weak, and the primary risk is that the standard `χ²(r-p)` distribution is a poor approximation to the test statistic's true distribution."}, "Answer": "A", "pi_justification": "This question uses a Reverse-Reasoning strategy to assess the user's understanding of test interpretation and inferential risks. Given a specific outcome (failure to reject), the user must identify the correct conclusion and its main caveat. The correct answer (A) correctly identifies the interpretation (compatibility) and the risk (Type II error/low power). Distractors are based on high-fidelity errors: B is the conceptual opposite, misinterpreting the result and confusing Type I/II errors. C overstates the conclusion ('proves') and introduces a different, though related, issue (estimator bias). D confuses the concept of model misspecification (tested by `T_n`) with weak identification and size distortion, which are distinct econometric problems.", "qid": "99", "question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n---\n\n### Data / Model Specification\n\nThe Empirical Likelihood (EL) test statistic for the null hypothesis `H₀: E[g(X, θ)] = 0` for some `θ` is denoted `T_n`. A small value of `T_n` indicates that the model's restrictions are compatible with the observed data.\n\n---\n\n### Question\n\nAn econometrician performs the EL overidentification test and obtains a small value for the test statistic `T_n`, leading to a failure to reject the null hypothesis `H₀`. What is the correct interpretation of this result and the primary inferential risk associated with it?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 93, "Question": "### Background\n\nThe paper develops 'adaptive' estimators (e.g., `\\hat{M}_{TS}`) that are computable from data. It compares their performance to 'oracle' estimators (e.g., `\\widetilde{M}_{TS}`), which are defined as the best possible estimators within a certain class but are infeasible because they depend on the unknown true parameters of the model.\n\n---\n\n### Data / Model Specification\n\nA key theoretical result (Theorem 4.2) establishes that, under certain assumptions, the risk of the adaptive Total Shrinkage estimator converges to the risk of the oracle Total Shrinkage estimator as the number of rows `p` tends to infinity:\n```latex\n\\lim_{p \\to \\infty} |R(\\hat{M}_{TS}, M, \\sigma^2) - R(\\widetilde{M}_{TS}, M, \\sigma^2)| = 0\n```\n\n---\n\n### Question\n\nWhat is the primary theoretical significance of this finding that the risk of the adaptive estimator converges to the risk of the oracle estimator?", "Options": {"A": "It proves that the adaptive estimator becomes unbiased as the sample size grows.", "B": "It shows that the adaptive estimator is optimal for small sample sizes, where estimation is most difficult.", "C": "It demonstrates that the singular value decomposition provides a computationally stable method for estimation.", "D": "It validates that the data-driven procedure of minimizing estimated risk successfully approximates the theoretically best possible performance in large samples."}, "Answer": "D", "pi_justification": "This item assesses the core conceptual takeaway of the paper's asymptotic theory. The distractors are designed to probe for common misunderstandings: (A) is an 'Almost Right' error that misidentifies the goal (risk reduction, not unbiasedness); (B) is a 'Conceptual Opposite' regarding the asymptotic nature of the result; (C) is 'True but Irrelevant', confusing a computational property with the theoretical significance of risk convergence.", "qid": "93", "question": "### Background\n\nThe paper develops 'adaptive' estimators (e.g., `\\hat{M}_{TS}`) that are computable from data. It compares their performance to 'oracle' estimators (e.g., `\\widetilde{M}_{TS}`), which are defined as the best possible estimators within a certain class but are infeasible because they depend on the unknown true parameters of the model.\n\n---\n\n### Data / Model Specification\n\nA key theoretical result (Theorem 4.2) establishes that, under certain assumptions, the risk of the adaptive Total Shrinkage estimator converges to the risk of the oracle Total Shrinkage estimator as the number of rows `p` tends to infinity:\n```latex\n\\lim_{p \\to \\infty} |R(\\hat{M}_{TS}, M, \\sigma^2) - R(\\widetilde{M}_{TS}, M, \\sigma^2)| = 0\n```\n\n---\n\n### Question\n\nWhat is the primary theoretical significance of this finding that the risk of the adaptive estimator converges to the risk of the oracle estimator?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 83, "Question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nThe paper argues that the gender gap in persistence is driven by the behavior of candidates who barely lost, not by those who barely won. Which coefficient in Table 1 provides the most direct evidence for the \"not by those who barely won\" part of this claim?", "Options": {"A": "The coefficient on `Female x Lost` (γ = -0.101), because it shows that losing has a different effect on women.", "B": "The coefficient on `Lost` (β = -0.160), because it shows the large negative effect of losing for the baseline group.", "C": "The coefficient on `Female` (δ = 0.007), because its small size and statistical insignificance imply no meaningful difference in persistence between male and female winners.", "D": "The standard error on `Female` (0.036), because it is large relative to the coefficient, indicating high uncertainty."}, "Answer": "C", "pi_justification": "Value: Assesses a nuanced point of econometric interpretation: identifying which coefficient in an interaction model represents the baseline difference between groups (i.e., among the control group, `Lost=0`). Strategy: Reverse-Reasoning. The student must infer which piece of evidence supports a specific part of the paper's argument. Distractor Logic: (A) is Almost Right, as it explains the 'driven by losers' part, but not the 'not by winners' part. (B) is True but Irrelevant to the gender comparison among winners. (D) points to the statistical tool (the SE) rather than the economic parameter (the coefficient) that carries the interpretation.", "qid": "83", "question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nThe paper argues that the gender gap in persistence is driven by the behavior of candidates who barely lost, not by those who barely won. Which coefficient in Table 1 provides the most direct evidence for the \"not by those who barely won\" part of this claim?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 6, "Question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`. This posterior is constructed by averaging a conditional prior `f(\\lambda|\\psi)` over the posterior of the identified parameters, `f(\\psi|y)`.\n\n### Question\n\nWhat is a key advantage of the Bayesian posterior `f(\\lambda|y)` that is absent from the simple reporting of the estimated set `\\Lambda(\\hat{\\psi})`?", "Options": {"A": "The Bayesian posterior provides a single point estimate for `\\lambda`, completely resolving the identification problem.", "B": "The Bayesian posterior is always uniform over the bounds `\\Lambda(\\hat{\\psi})`, providing a clear non-informative benchmark.", "C": "The Bayesian posterior coherently integrates the statistical uncertainty about the identified parameters `\\psi` and provides a probabilistic weighting of values for `\\lambda`.", "D": "The Bayesian posterior's support is always narrower than the bounds `\\Lambda(\\hat{\\psi})`, providing more precise inference."}, "Answer": "C", "pi_justification": "Assessment Target: Tests the central conceptual difference between a Bayesian posterior and a frequentist estimated set in a partially identified model. Strategy: Conceptual Judgment. Distractor Logic: A) is a Conceptual Opposite; the Bayesian approach does not resolve the identification problem. B) is a common misconception; the posterior is generally not uniform. D) is incorrect; the marginal posterior support is typically the entire parameter space (e.g., [0,1]), which is wider than the conditional bounds.", "qid": "6", "question": "### Background\n\n**Research Question.** In a censored sampling model where a parameter of interest is not point-identified, how does the Bayesian approach to inference differ from a frequentist bounds-based approach?\n\n**Setting.** We observe a binary variable `Z_t` indicating whether a continuous outcome `y_t` is observed. `y_t` is only observed when `Z_t=1`. The parameter `\\mu`, governing the distribution of `y_t` when `Z_t=0`, is not identified. The non-identified parameter of interest is `\\lambda = P(y_t \\ge 0)`.\n\n### Data / Model Specification\n\nThe parameter `\\lambda` is constrained to lie within the bounds `\\Lambda(\\psi) = [\\psi_1\\Phi(\\psi_2), \\psi_1\\Phi(\\psi_2) + 1 - \\psi_1]`, where `\\psi=(\\psi_1, \\psi_2)` are identified parameters. A frequentist approach reports the estimated set `\\Lambda(\\hat{\\psi})`, where `\\hat{\\psi}` is the Maximum Likelihood Estimate. A Bayesian analysis specifies a prior and derives the marginal posterior distribution for `\\lambda`, `f(\\lambda|y)`. This posterior is constructed by averaging a conditional prior `f(\\lambda|\\psi)` over the posterior of the identified parameters, `f(\\psi|y)`.\n\n### Question\n\nWhat is a key advantage of the Bayesian posterior `f(\\lambda|y)` that is absent from the simple reporting of the estimated set `\\Lambda(\\hat{\\psi})`?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 52, "Question": "### Background\n\nTo address sample selection bias from female non-participation, the authors use a Full Information Maximum Likelihood (FIML) estimator. The likelihood function for the truncated sample of households with working wives is:\n```latex\nL = \\prod_{h=1}^{H} \\frac{\\phi(u_{fh}, u_{mh})}{\\operatorname{Pr}(l_{fh} < T_f)}\n```\nwhere `φ(u_fh, u_mh)` is the joint probability density of the errors, assumed to be bivariate normal. The denominator is the probability of being selected into the sample.\n\n### The Question\n\nA critic argues that the unobserved preference for female non-market time might be skewed, violating the FIML estimator's joint normality assumption. If the critic is correct, what is the primary consequence of this violation, and what is the most appropriate econometric response to assess the model's robustness?", "Options": {"A": "Consequence: Inefficient but still consistent estimates. Response: Use robust standard errors.", "B": "Consequence: Inconsistent estimates. Response: Find a valid instrumental variable for the wife's participation decision.", "C": "Consequence: Biased estimates for the female equation only. Response: Use a different parametric distribution like Logit instead of Probit for the selection stage.", "D": "Consequence: Inconsistent estimates for all model parameters. Response: Re-estimate using a semi-parametric control function method that does not rely on a specific distributional assumption."}, "Answer": "D", "pi_justification": "This question tests advanced understanding of the limitations of FIML selection models. It uses a Scenario Application strategy. The correct option correctly identifies both the consequence (inconsistency, not inefficiency) and the state-of-the-art solution (semi-parametric methods). Distractor A confuses inconsistency with inefficiency. Distractor B confuses selection with standard endogeneity. Distractor C incorrectly localizes the problem and proposes an insufficient fix.", "qid": "52", "question": "### Background\n\nTo address sample selection bias from female non-participation, the authors use a Full Information Maximum Likelihood (FIML) estimator. The likelihood function for the truncated sample of households with working wives is:\n```latex\nL = \\prod_{h=1}^{H} \\frac{\\phi(u_{fh}, u_{mh})}{\\operatorname{Pr}(l_{fh} < T_f)}\n```\nwhere `φ(u_fh, u_mh)` is the joint probability density of the errors, assumed to be bivariate normal. The denominator is the probability of being selected into the sample.\n\n### The Question\n\nA critic argues that the unobserved preference for female non-market time might be skewed, violating the FIML estimator's joint normality assumption. If the critic is correct, what is the primary consequence of this violation, and what is the most appropriate econometric response to assess the model's robustness?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 96, "Question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`), with a total endowment normalized to 1. Production in each industry is subject to industry-level external economies of scale, given by the technology:\n```latex\ny_j = g(y_j) L_j \\quad \\text{where} \\quad g(x) = x^{1/2}\n```\nThis technology results in a production possibilities curve (PPC) described by the equation:\n```latex\ny_1^{1/2} + y_2^{1/2} = 1 \\quad \\text{(Eq. 1)}\n```\nThis PPC is non-convex (bowed-in towards the origin). In autarky, two identical countries are at a symmetric equilibrium `C_0` with a price ratio of 1. Under free trade, it is possible for one country to specialize in good 1 (producing at `P'=(1,0)`) and the other in good 2 (producing at `P''=(0,1)`), while the world price ratio remains 1.\n\n### Question\n\nGiven this setup, what is the fundamental reason that both identical countries can achieve a higher level of welfare through trade, despite no change in the price ratio?", "Options": {"A": "Specialization allows countries to produce at the endpoints of the non-convex PPC, enabling them to trade along a 'chord' that lies outside their original individual production sets.", "B": "Gains arise because specialization allows each country to move down its marginal cost curve, leading to a lower world price for both goods.", "C": "Gains are driven by underlying differences in factor endowments between the two countries, which manifest as a comparative advantage.", "D": "The production technology implies a convex production set (bowed-out PPC), where specialization according to comparative advantage is always optimal."}, "Answer": "A", "pi_justification": "This item assesses the understanding of the central, counter-intuitive result from the paper's Appendix. It uses a Scenario Application strategy, asking the user to identify the correct economic mechanism for gains from trade in a non-convex economy. The distractors are designed to target common misconceptions: B suggests a price-based mechanism that isn't present, C incorrectly applies standard comparative advantage theory, and D fundamentally misinterprets the geometry of the production set, which is the key to the entire problem.", "qid": "96", "question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`), with a total endowment normalized to 1. Production in each industry is subject to industry-level external economies of scale, given by the technology:\n```latex\ny_j = g(y_j) L_j \\quad \\text{where} \\quad g(x) = x^{1/2}\n```\nThis technology results in a production possibilities curve (PPC) described by the equation:\n```latex\ny_1^{1/2} + y_2^{1/2} = 1 \\quad \\text{(Eq. 1)}\n```\nThis PPC is non-convex (bowed-in towards the origin). In autarky, two identical countries are at a symmetric equilibrium `C_0` with a price ratio of 1. Under free trade, it is possible for one country to specialize in good 1 (producing at `P'=(1,0)`) and the other in good 2 (producing at `P''=(0,1)`), while the world price ratio remains 1.\n\n### Question\n\nGiven this setup, what is the fundamental reason that both identical countries can achieve a higher level of welfare through trade, despite no change in the price ratio?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 28, "Question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round. This method yields an average `\\hat{\\alpha} \\approx 0.000310`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings to the simulated distribution from a risk-neutral strategy. This method yields a much smaller `\\hat{\\alpha}`.\n\n### Question\n\nAccording to the paper's analysis, which of the following best explains why Method 1 (final-round bets) yields a much higher estimate of risk aversion (`\\alpha`) than Method 2 (distributional comparison)?", "Options": {"A": "The final-round bets in Method 1 are heavily censored (many contestants bet the minimum or maximum), which biases the `\\alpha` estimate upwards.", "B": "Contestants are more risk-loving in the final round due to the 'house money' effect, which should lead to a *lower* `\\alpha` from Method 1, making the results contradictory.", "C": "Method 1 measures behavior only in high-stake final rounds, while Method 2 averages behavior across all rounds where risk-taking is not constant but depends heavily on the current stake, a violation of the CARA model's core assumption.", "D": "The risk-neutral strategy in Method 2 is an unrealistic benchmark because it fails to account for contestants' ability to count cards."}, "Answer": "C", "pi_justification": "This item assesses the student's ability to synthesize findings and understand the paper's core critique of standard utility theory. It uses a Reverse-Reasoning strategy, asking for the cause of an observed empirical puzzle. Distractor A is 'True but Irrelevant,' as censoring is a technical issue but not the core behavioral explanation for the large discrepancy. Distractor B is a 'Conceptual Opposite,' misstating the implication of the house money effect in this context. Distractor D is 'Almost Right,' mentioning a real detail from the paper (card counting) but misattributing it as the primary cause for the discrepancy in `α` estimates.", "qid": "28", "question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round. This method yields an average `\\hat{\\alpha} \\approx 0.000310`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings to the simulated distribution from a risk-neutral strategy. This method yields a much smaller `\\hat{\\alpha}`.\n\n### Question\n\nAccording to the paper's analysis, which of the following best explains why Method 1 (final-round bets) yields a much higher estimate of risk aversion (`\\alpha`) than Method 2 (distributional comparison)?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 37, "Question": "### Background\n\nResearchers conducted a randomized controlled trial (RCT) assigning households to `Control`, `Price-Only`, and `Price+IHD` groups. A check of pre-treatment characteristics revealed a statistically significant difference in average home size between the `Price-Only` and `Control` groups. Furthermore, the study experienced selective attrition: households on a time-of-use (TOU) rate plan were significantly more likely to drop out of the `Price+IHD` group than other households in that group.\n\n---\n\n### Question\n\nGiven the baseline imbalance and selective attrition, which statement correctly assesses the threats to the study's internal validity and the appropriate analytical approach?", "Options": {"A": "The baseline imbalance proves the randomization failed, and the selective attrition introduces bias; therefore, no causal claims can be made from the experiment.", "B": "The selective attrition is the primary threat, which can be solved by comparing outcomes only among the households who completed the study (compliers).", "C": "The baseline imbalance is a minor concern that can be addressed by including home size as a control variable, and the selective attrition is addressed by using Intent-to-Treat (ITT) analysis, which preserves the benefits of the original randomization.", "D": "The Intent-to-Treat (ITT) principle solves the problem of the baseline imbalance, but the selective attrition requires using a Treatment-on-the-Treated (ToT) analysis to obtain an unbiased estimate."}, "Answer": "C", "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.5). It assesses the student's ability to correctly diagnose and propose remedies for two common threats to validity in an RCT: baseline imbalance and selective attrition. The strategy is 'Conceptual Judgment'. The correct answer (C) identifies the standard best practices. Distractors are based on classic misconceptions: (A) is overly pessimistic and dismisses valid remedies; (B) proposes an analysis (naive comparison of compliers) that is biased by selective attrition; (D) incorrectly maps the solutions to the problems (ITT addresses attrition's effect on the 'assignment' estimate, not baseline imbalance).", "qid": "37", "question": "### Background\n\nResearchers conducted a randomized controlled trial (RCT) assigning households to `Control`, `Price-Only`, and `Price+IHD` groups. A check of pre-treatment characteristics revealed a statistically significant difference in average home size between the `Price-Only` and `Control` groups. Furthermore, the study experienced selective attrition: households on a time-of-use (TOU) rate plan were significantly more likely to drop out of the `Price+IHD` group than other households in that group.\n\n---\n\n### Question\n\nGiven the baseline imbalance and selective attrition, which statement correctly assesses the threats to the study's internal validity and the appropriate analytical approach?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 17, "Question": "### Background\n\nTo account for unobserved firm-specific heterogeneity (e.g., managerial skill), Fixed Effect (FE) and Random Effect (RE) models were estimated for a panel of Indian manufacturing firms. A Hausman test was conducted to choose between the two specifications.\n\n### Data / Model Specification\n\nThe table below presents FE and RE estimates for the \"Overall Industry\" after correcting for serial correlation. The Hausman test yielded a statistic of 53.19.\n\n**Table 1: Fixed vs. Random Effect Models**\n| Variables | Fixed Effect | Random Effect |\n|:---|---:|---:|\n| LNRND ($\\beta$) | 0.007 (1.54) | 0.010 (2.07) |\n| LNSRDEXP ($\\alpha$) | 0.101 (3.34) | 0.122 (5.64) |\n*t-statistics in parentheses.*\n\n### Question\n\nGiven the Hausman test statistic of 53.19 (which is highly significant), which statement provides the correct conclusion and justification?", "Options": {"A": "The Random Effect model is preferred because it is more efficient and the test fails to reject its validity.", "B": "The Fixed Effect model is preferred because the test rejects the null hypothesis that unobserved effects are uncorrelated with the regressors, implying the Random Effect model would be biased.", "C": "The Random Effect model is preferred because the test rejects the null hypothesis that the Fixed Effect model is consistent.", "D": "The Fixed Effect model is preferred because the coefficients are smaller, indicating they are more conservative and reliable."}, "Answer": "B", "pi_justification": "This item replaces part of a larger QA problem and assesses the student's ability to apply a standard specification test. The strategy is Conceptual Judgment. The correct answer (B) requires knowing the null hypothesis of the Hausman test (no correlation between unobserved effects and regressors) and the implication of its rejection (RE is inconsistent, so FE is preferred). Distractor A correctly states that RE is more efficient but ignores the test result, making it 'True but Irrelevant'. Distractor C misstates the null hypothesis and conclusion. Distractor D offers a folk-theory justification ('smaller is better') that is not the basis for the statistical decision.", "qid": "17", "question": "### Background\n\nTo account for unobserved firm-specific heterogeneity (e.g., managerial skill), Fixed Effect (FE) and Random Effect (RE) models were estimated for a panel of Indian manufacturing firms. A Hausman test was conducted to choose between the two specifications.\n\n### Data / Model Specification\n\nThe table below presents FE and RE estimates for the \"Overall Industry\" after correcting for serial correlation. The Hausman test yielded a statistic of 53.19.\n\n**Table 1: Fixed vs. Random Effect Models**\n| Variables | Fixed Effect | Random Effect |\n|:---|---:|---:|\n| LNRND ($\\beta$) | 0.007 (1.54) | 0.010 (2.07) |\n| LNSRDEXP ($\\alpha$) | 0.101 (3.34) | 0.122 (5.64) |\n*t-statistics in parentheses.*\n\n### Question\n\nGiven the Hausman test statistic of 53.19 (which is highly significant), which statement provides the correct conclusion and justification?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 35, "Question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to one of three groups: `Control`, `Price-Only`, and `Price+IHD`. The `Price+IHD` group was intended to receive an in-home display (IHD) providing real-time usage information. However, due to scheduling and other issues, there was significant non-compliance. While compliance was high in other groups, only 72% of households assigned to the `Price+IHD` group completed the study and had a functioning IHD installed.\n\nResearchers estimated the Intent-to-Treat (ITT) effect, which measures the causal effect of being *assigned* to a treatment group, regardless of compliance.\n\n### Data / Model Specification\n\nThe table below shows the ITT estimates from a difference-in-differences model with household and hour-by-day fixed effects. The dependent variable is the natural log of electricity usage, `ln(kWh)`.\n\n**Table 1: ITT Treatment Effects on ln(kWh) for All Events**\n\n| | Coefficient (Std. Err.) |\n| :--- | :--- |\n| Price-only | -0.038 (0.036) |\n| Price + IHD | -0.137*** (0.046) |\n\n*Notes: *** p<0.01.*\n\n---\n\n### Question\n\nGiven the results in Table 1 and the context of 72% compliance in the `Price+IHD` group, which statement most accurately interprets the Intent-to-Treat (ITT) estimate and its policy relevance?", "Options": {"A": "The ITT estimate of -0.137 shows the causal effect of the IHD on the 72% of households who complied, indicating that the device itself caused a 13.7% usage reduction for its users.", "B": "The ITT estimate of -0.137 is biased due to the 28% non-compliance rate and therefore does not represent a valid causal effect of the program.", "C": "The ITT estimate of -0.137 represents the average effect of the program on all households *assigned* to the `Price+IHD` group, making it the relevant parameter for a policymaker considering a large-scale rollout where similar non-compliance is expected.", "D": "The ITT estimate of -0.137 shows that being assigned to the `Price+IHD` group caused a 13.7 percentage point decrease in electricity usage, answering the question of the program's effect under perfect compliance."}, "Answer": "C", "pi_justification": "This question replaces part of a QA problem (Suitability Score: 9.0). It assesses the core concept of interpreting an Intent-to-Treat (ITT) estimate in the presence of non-compliance. The strategy is conceptual interpretation. The correct answer (C) accurately defines ITT and its policy relevance. Distractors are designed based on common misconceptions: (A) incorrectly describes the Treatment-on-the-Treated (ToT) effect; (B) incorrectly claims ITT is biased by non-compliance (it is not biased as an estimate of the effect of *assignment*); (D) uses incorrect units ('percentage point') for a log-level model and incorrectly describes the policy question.", "qid": "35", "question": "### Background\n\nIn a randomized controlled trial (RCT), households were assigned to one of three groups: `Control`, `Price-Only`, and `Price+IHD`. The `Price+IHD` group was intended to receive an in-home display (IHD) providing real-time usage information. However, due to scheduling and other issues, there was significant non-compliance. While compliance was high in other groups, only 72% of households assigned to the `Price+IHD` group completed the study and had a functioning IHD installed.\n\nResearchers estimated the Intent-to-Treat (ITT) effect, which measures the causal effect of being *assigned* to a treatment group, regardless of compliance.\n\n### Data / Model Specification\n\nThe table below shows the ITT estimates from a difference-in-differences model with household and hour-by-day fixed effects. The dependent variable is the natural log of electricity usage, `ln(kWh)`.\n\n**Table 1: ITT Treatment Effects on ln(kWh) for All Events**\n\n| | Coefficient (Std. Err.) |\n| :--- | :--- |\n| Price-only | -0.038 (0.036) |\n| Price + IHD | -0.137*** (0.046) |\n\n*Notes: *** p<0.01.*\n\n---\n\n### Question\n\nGiven the results in Table 1 and the context of 72% compliance in the `Price+IHD` group, which statement most accurately interprets the Intent-to-Treat (ITT) estimate and its policy relevance?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 72, "Question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `f_i(p)`: The cost function for commodity `i`, returning the minimum cost to produce one unit given factor price vector `p`.\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production, defining a mapping `f` from factor prices `p` to commodity prices `q`.\n\n---\n\n### The Questions\n\nIn the context of the Factor Price Equalization theorem, why is the existence of a unique global inverse for the cost function mapping `p` to `q` (i.e., `p = f^{-1}(q)`) a critical condition?", "Options": {"A": "It implies that for any given vector of commodity prices `q` established by trade, there is only one possible corresponding vector of factor prices `p`.", "B": "It ensures that multiple factor price vectors can correspond to a single commodity price vector, allowing countries to specialize.", "C": "It is a direct mathematical consequence of the concavity of the cost function, which is a fundamental property of cost minimization.", "D": "It proves that free trade in goods will lead to the equalization of commodity prices across countries."}, "Answer": "A", "pi_justification": "This question assesses the core economic logic of the Factor Price Equalization theorem. It uses a conceptual reasoning strategy. The correct option (A) states the central implication. Distractor (B) is the conceptual opposite. Distractor (C) is 'True but Irrelevant,' as concavity does not guarantee a global inverse. Distractor (D) confuses the theorem's premise (equal commodity prices) with its conclusion.", "qid": "72", "question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `f_i(p)`: The cost function for commodity `i`, returning the minimum cost to produce one unit given factor price vector `p`.\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production, defining a mapping `f` from factor prices `p` to commodity prices `q`.\n\n---\n\n### The Questions\n\nIn the context of the Factor Price Equalization theorem, why is the existence of a unique global inverse for the cost function mapping `p` to `q` (i.e., `p = f^{-1}(q)`) a critical condition?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 23, "Question": "### Background\n\n**Research Question.** This problem explores alternative approaches to combining micro and macro forecasts.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}`. The paper discusses two main methods:\n1.  A **two-step approach**: First, an optimal combined macro forecast `η̂_{T+1}` is created. Second, the micro forecast is updated to be consistent with this macro total.\n2.  A **one-step GLS approach**: This method frames the problem as a single regression model and finds the `y_{T+1}` that simultaneously best fits both the initial micro and macro forecasts.\n\n### Data / Model Specification\n\nThe one-step GLS approach is formulated as the regression problem:\n```latex\n\\begin{pmatrix} η̂_{T+1}^{(1)} \\\\ ŷ_{T+1}^{(2)} \\end{pmatrix} = \\begin{pmatrix} a' \\\\ I_p \\end{pmatrix} y_{T+1} + \\begin{pmatrix} ε₁ \\\\ ε₂ \\end{pmatrix}\n```\n\nWhat is the key conceptual difference between the one-step GLS approach and the two-step approach?", "Options": {"A": "The GLS approach is Bayesian, while the two-step approach is frequentist.", "B": "The GLS approach assumes forecast errors are normally distributed, while the two-step approach does not.", "C": "The GLS approach simultaneously balances deviations from both forecasts, while the two-step approach sequentially imposes the combined macro forecast as a hard constraint.", "D": "The GLS approach is only optimal if the forecasts are uncorrelated, while the two-step approach is always optimal."}, "Answer": "C", "pi_justification": "Assessment Target: Tests the ability to distinguish between the two main theoretical approaches in the paper. Strategy: Conceptual Judgment. Distractor Logic: The distractors target common but incorrect ways of differentiating statistical models. (A) is an incorrect classification. (B) is an incorrect statement about assumptions, as normality is useful for both. (D) makes an incorrect claim about the optimality conditions of the two methods.", "qid": "23", "question": "### Background\n\n**Research Question.** This problem explores alternative approaches to combining micro and macro forecasts.\n\n**Setting / Institutional Environment.** A forecaster seeks an optimal estimate for the true micro vector `y_{T+1}`. The paper discusses two main methods:\n1.  A **two-step approach**: First, an optimal combined macro forecast `η̂_{T+1}` is created. Second, the micro forecast is updated to be consistent with this macro total.\n2.  A **one-step GLS approach**: This method frames the problem as a single regression model and finds the `y_{T+1}` that simultaneously best fits both the initial micro and macro forecasts.\n\n### Data / Model Specification\n\nThe one-step GLS approach is formulated as the regression problem:\n```latex\n\\begin{pmatrix} η̂_{T+1}^{(1)} \\\\ ŷ_{T+1}^{(2)} \\end{pmatrix} = \\begin{pmatrix} a' \\\\ I_p \\end{pmatrix} y_{T+1} + \\begin{pmatrix} ε₁ \\\\ ε₂ \\end{pmatrix}\n```\n\nWhat is the key conceptual difference between the one-step GLS approach and the two-step approach?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 98, "Question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n**Variables & Parameters.**\n- `g(X, θ)`: A vector of `r` moment functions.\n- `θ`: A `p`-dimensional vector of parameters.\n\n---\n\n### Data / Model Specification\n\nThe validity of the overidentifying restrictions is evaluated by testing the null hypothesis `H₀` against the alternative `H₁`:\n\n```latex\nH_{0}: \\mathbb{E}[g(X, \\theta)] = 0 \\quad \\text{for some } \\theta \\in \\Theta \n```\n```latex\nH_{1}: \\mathbb{E}[g(X, \\theta)] \\neq 0 \\quad \\text{for any } \\theta \\in \\Theta\n```\n\nThe test statistic for `H₀` has a limiting distribution `T_n \\xrightarrow{d} \\chi^2(r-p)`.\n\n---\n\n### Question\n\nWhich of the following statements provides the most accurate description of the test for overidentifying restrictions?", "Options": {"A": "It tests the null hypothesis that the economic model is correctly specified, and its degrees of freedom are `r-p` because `p` degrees of freedom are used to estimate the model parameters.", "B": "It tests the null hypothesis that the individual parameters in `θ` are statistically significant, with `r` degrees of freedom.", "C": "It is primarily used for just-identified models (`r=p`) to confirm that the model perfectly fits the sample data.", "D": "It determines the optimal number of moment conditions `r` to use for a given number of parameters `p`, with `r-p` degrees of freedom."}, "Answer": "A", "pi_justification": "This question assesses fundamental knowledge about overidentification tests. It uses a direct query strategy to test three core concepts: the meaning of the null hypothesis (model specification), the condition for the test's relevance (`r>p`), and the logic for the degrees of freedom (`r-p`). Distractors are designed based on common conceptual errors: B confuses model specification with parameter significance and uses the wrong degrees of freedom; C confuses over- with just-identification; D confuses the purpose of the test with model selection.", "qid": "98", "question": "### Background\n\n**Setting / Institutional Environment.** Consider a general econometric setting where an economic model is specified through a set of `r` population moment conditions `E[g(X, θ)] = 0` involving `p` unknown parameters, with `r > p` (overidentification).\n\n**Variables & Parameters.**\n- `g(X, θ)`: A vector of `r` moment functions.\n- `θ`: A `p`-dimensional vector of parameters.\n\n---\n\n### Data / Model Specification\n\nThe validity of the overidentifying restrictions is evaluated by testing the null hypothesis `H₀` against the alternative `H₁`:\n\n```latex\nH_{0}: \\mathbb{E}[g(X, \\theta)] = 0 \\quad \\text{for some } \\theta \\in \\Theta \n```\n```latex\nH_{1}: \\mathbb{E}[g(X, \\theta)] \\neq 0 \\quad \\text{for any } \\theta \\in \\Theta\n```\n\nThe test statistic for `H₀` has a limiting distribution `T_n \\xrightarrow{d} \\chi^2(r-p)`.\n\n---\n\n### Question\n\nWhich of the following statements provides the most accurate description of the test for overidentifying restrictions?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 34, "Question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nUsing the coefficient estimates in Table 1, at what threshold level of R&D intensity (R&D as a share of sales) does the marginal effect of corruption on the firm's investment propensity become zero?", "Options": {"A": "9.2%", "B": "17.6%", "C": "12.3%", "D": "The marginal effect is always negative, so no such threshold exists."}, "Answer": "C", "pi_justification": "This item tests the ability to translate an econometric model with an interaction term into a precise quantitative prediction. It uses the Computational Judgment strategy. The correct answer requires setting the marginal effect equation ($\\beta_{Corruption} + \\beta_{Interaction} \\times R\\&D = 0$) to zero and solving. Distractors are based on common errors: (B) uses the wrong coefficient in the calculation (Formula Misuse), (A) is a plausible number derived from a different part of the paper (a subtle Formula Misuse), and (D) represents a conceptual failure to understand how a positive interaction term can offset a negative main effect.", "qid": "34", "question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nUsing the coefficient estimates in Table 1, at what threshold level of R&D intensity (R&D as a share of sales) does the marginal effect of corruption on the firm's investment propensity become zero?", "model_evaluation_result": {"model_raw_response": "I need to find the threshold level of R&D intensity where the marginal effect of corruption on investment propensity becomes zero.\n\nThe marginal effect of corruption on the probability of investment is:\n∂Pr(DFDI_{ij}=1)/∂Corruption_j = φ(·) × [β₁ + β₃ × R&D_i]\n\nwhere φ(·) is the standard normal PDF (always positive).\n\nFor the marginal effect to be zero, the term in brackets must equal zero:\nβ₁ + β₃ × R&D_i = 0\n\nSolving for R&D_i:\nR&D_i = -β₁/β₃\n\nUsing the coefficient estimates from Table 1:\n- β₁ = -1.015\n- β₃ = 8.230\n\nR&D_i = -(-1.015)/8.230 = 1.015/8.230 = 0.123\n\nConverting to percentage: 0.123 × 100% = 12.3%\n\nC", "model_answer": "D", "error": null}}
{"ID": 103, "Question": "### Background\n\nThe paper's full dynamic model specifies the money market equilibrium as follows:\n\n```latex\nM(t) = hP(t)y(t) + L[r(t), P(t-1)]\n```\n\nIn this formulation, current speculative balances depend on the current interest rate `r(t)` but on the *lagged* price level `P(t-1)`.\n\n### Question\n\nWhat is the most plausible behavioral justification offered in the paper for using the lagged price level `P(t-1)` instead of the current price level `P(t)` to determine speculative balances?\n", "Options": {"A": "Wealth-holders react more slowly and with more hesitation to general price level changes than to more immediate bond market signals.", "B": "The general price level `P(t)` is a statistical construct that is not observable by agents until after period `t` has concluded.", "C": "The lag reflects delays in the production process, as investment decisions must be made in the prior period.", "D": "The model assumes that a rising price level in the previous period mechanically causes a rising price level in the current period."}, "Answer": "A", "pi_justification": "This item assesses understanding of the behavioral reasoning behind a key modeling choice. It uses the 'Reverse-Reasoning' strategy. The correct answer (A) is the paper's explicit justification. Distractor B is a plausible 'True but Irrelevant' statement about data reporting. Distractor C is a 'Conceptual Opposite' error, confusing a monetary expectation lag with a real-side production lag. Distractor D introduces an unsupported assumption about price dynamics.", "qid": "103", "question": "### Background\n\nThe paper's full dynamic model specifies the money market equilibrium as follows:\n\n```latex\nM(t) = hP(t)y(t) + L[r(t), P(t-1)]\n```\n\nIn this formulation, current speculative balances depend on the current interest rate `r(t)` but on the *lagged* price level `P(t-1)`.\n\n### Question\n\nWhat is the most plausible behavioral justification offered in the paper for using the lagged price level `P(t-1)` instead of the current price level `P(t)` to determine speculative balances?\n", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 45, "Question": "## Background\n\n**Research Question.** This problem investigates the long-term financial impact of providing social information to new donors. The analysis is a one-year follow-up of the *new donors* from an initial field experiment, tracking their donation behavior to assess causal effects on long-term outcomes.\n\n**Variables & Parameters.**\n- `Renewal rate`: The proportion of new donors from a given experimental group who made another donation one year later.\n- `Mean contribution ($)`: The average donation amount in year t+1, *conditional on renewing*.\n- `Expected revenue ($)`: The unconditional expected donation amount from a new donor in year t+1, calculated as `Renewal rate` × `Mean contribution`.\n\n---\n\n## Data / Model Specification\n\nFrom a separate analysis on the initial impact, the estimated immediate (year t) revenue increase from the `$300` signal for *new donors* was **$9.00** compared to the control group.\n\nThe long-term outcomes for new donors one year later (year t+1) are summarized in Table 1.\n\n**Table 1: New Donors One Year Later**\n\n| Treatment | Renewal rate (%) | Mean contribution ($) | Expected revenue ($) |\n|:---|:---:|:---:|:---:|\n| Control | 12 | 86.11 | 10.62 |\n| $300 | 32 | 93.97 | 29.95 |\n\n---\n\n## Question\n\nA station manager is considering a new policy: all *new* callers will receive the `$300` social information treatment. Based on the data provided, what is the total expected two-year revenue increase *per new donor* under this policy, compared to the no-information (control) policy?", "Options": {"A": "$19.33", "B": "$28.33", "C": "$9.00", "D": "$38.95"}, "Answer": "B", "pi_justification": "This item assesses the ability to synthesize information from multiple sources (text and a table) to perform a multi-step calculation. It uses a 'Computational Judgment' strategy. The correct answer requires summing the immediate effect ($9.00) and the long-term effect ($29.95 - $10.62 = $19.33). Distractors are designed based on common errors: (A) is a 'Step-Omission Error' that only calculates the long-term effect; (C) is a 'Step-Omission Error' that only considers the immediate effect; (D) is a 'Formula Misuse' error where the student adds the total year 2 revenue to the year 1 gain instead of the *increase* in year 2 revenue.", "qid": "45", "question": "## Background\n\n**Research Question.** This problem investigates the long-term financial impact of providing social information to new donors. The analysis is a one-year follow-up of the *new donors* from an initial field experiment, tracking their donation behavior to assess causal effects on long-term outcomes.\n\n**Variables & Parameters.**\n- `Renewal rate`: The proportion of new donors from a given experimental group who made another donation one year later.\n- `Mean contribution ($)`: The average donation amount in year t+1, *conditional on renewing*.\n- `Expected revenue ($)`: The unconditional expected donation amount from a new donor in year t+1, calculated as `Renewal rate` × `Mean contribution`.\n\n---\n\n## Data / Model Specification\n\nFrom a separate analysis on the initial impact, the estimated immediate (year t) revenue increase from the `$300` signal for *new donors* was **$9.00** compared to the control group.\n\nThe long-term outcomes for new donors one year later (year t+1) are summarized in Table 1.\n\n**Table 1: New Donors One Year Later**\n\n| Treatment | Renewal rate (%) | Mean contribution ($) | Expected revenue ($) |\n|:---|:---:|:---:|:---:|\n| Control | 12 | 86.11 | 10.62 |\n| $300 | 32 | 93.97 | 29.95 |\n\n---\n\n## Question\n\nA station manager is considering a new policy: all *new* callers will receive the `$300` social information treatment. Based on the data provided, what is the total expected two-year revenue increase *per new donor* under this policy, compared to the no-information (control) policy?", "model_evaluation_result": {"model_raw_response": "I need to calculate the total expected two-year revenue increase per new donor from implementing the $300 social information treatment compared to the control.\n\nGiven information:\n- Year t (immediate) revenue increase from $300 treatment: $9.00\n- Year t+1 expected revenue:\n  - Control: $10.62\n  - $300 treatment: $29.95\n\nTo find the total two-year revenue increase per new donor:\n\nYear t revenue increase = $9.00\n\nYear t+1 revenue increase = Expected revenue ($300) - Expected revenue (Control)\n= $29.95 - $10.62 = $19.33\n\nTotal two-year revenue increase = Year t increase + Year t+1 increase\n= $9.00 + $19.33 = $28.33\n\nB", "model_answer": "B", "error": null}}
{"ID": 110, "Question": "### Background\n\nTwo proposed solutions to the 'persistence problem' in monetary models are (1) assuming firms face convex demand curves and (2) assuming firms use specific factors of production. In both cases, the models can generate macroeconomic persistence, but only by assuming extreme parameter values.\n\n---\n\n### Data / Model Specification\n\nFor example, the convex demand model requires a parameterization implying that a 2.3% price increase causes demand to drop to nearly zero. The specific factors model requires a parameterization implying that a 0.1% price difference causes a 665-fold difference in sales.\n\n---\n\n### Question\n\nGiven these findings, what is the paper's central critique of these proposed solutions?", "Options": {"A": "The models are microeconomically plausible but fail to generate sufficient macroeconomic persistence.", "B": "The models achieve the desired macroeconomic outcome but rely on empirically implausible microeconomic foundations.", "C": "The models are too computationally complex to be useful for policy analysis, regardless of their plausibility.", "D": "The models show that persistence is possible, but only in economies with extremely high demand elasticity."}, "Answer": "B", "pi_justification": "This item assesses the ability to synthesize the results of quantitative critiques into a general conclusion about modeling strategy. It uses a conceptual judgment strategy. The correct answer (B) accurately summarizes the paper's argument about the trade-off between macro fit and micro plausibility. Distractor A is the conceptual opposite. Distractor C is a 'true but irrelevant' style critique, as the paper's focus is on plausibility, not complexity. Distractor D is a correct statement about one model but fails to capture the general, synthesized critique of both.", "qid": "110", "question": "### Background\n\nTwo proposed solutions to the 'persistence problem' in monetary models are (1) assuming firms face convex demand curves and (2) assuming firms use specific factors of production. In both cases, the models can generate macroeconomic persistence, but only by assuming extreme parameter values.\n\n---\n\n### Data / Model Specification\n\nFor example, the convex demand model requires a parameterization implying that a 2.3% price increase causes demand to drop to nearly zero. The specific factors model requires a parameterization implying that a 0.1% price difference causes a 665-fold difference in sales.\n\n---\n\n### Question\n\nGiven these findings, what is the paper's central critique of these proposed solutions?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 113, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical foundations of the Analytic Policy Function Iteration (APFI) framework, which is designed to solve a general class of linear rational expectations models with endogenous information frictions.\n\n**Setting / Institutional Environment.** The APFI method is an iterative algorithm that finds an equilibrium policy function. Its validity and computational feasibility rest on three key mathematical theorems that justify its core operational steps.\n\n---\n\n### Data / Model Specification\n\nThe APFI framework is designed to solve models cast in the canonical form:\n```latex\n\\sum_{k=0}^{l}A_{k}y_{t-k}+\\sum_{k=0}^{h}B_{k}\\mathbb{E}_{t}y_{t+k}=\\mathbf{0}\n```\nThe **Baseline APFI Algorithm** involves three main steps in each iteration:\n1.  **Projection:** Approximate the current guess of the policy function using a finite-order VARMA process (i.e., a rational function).\n2.  **Evaluation:** Use the VARMA approximation to compute conditional expectations.\n3.  **Updating & Recursion:** Solve for an updated policy function and repeat until the function converges on a numerical grid.\n\nThis algorithm is justified by three foundational theorems from the paper:\n-   **Theorem 3.1 (Denseness of Rational Functions):** Any covariance-stationary equilibrium process, which resides in the Hardy space `H²`, can be approximated arbitrarily well by a VARMA process (a rational function).\n-   **Theorem 3.2 (Analytic Continuation & Convergence):** A solution found on the real unit interval `(-1, 1)` uniquely extends to the entire complex unit disk. Furthermore, pointwise convergence of a sequence of rational functions on this real interval implies convergence in the `H²` norm.\n-   **Theorem 3.3 (Annihilation):** The crucial \"annihilation\" step in computing expectations can be performed efficiently and accurately using the Inverse Discrete Fourier Transform (IDFT).\n\n---\n\n### The Questions\n\nAccording to the paper, what is the primary theoretical role of **Theorem 3.1 (Denseness of Rational Functions)** for the **Projection** step of the APFI algorithm?", "Options": {"A": "It proves that the true equilibrium solution is always a finite-order VARMA process.", "B": "It guarantees that the basis of VARMA processes is rich enough to approximate any possible stationary equilibrium solution arbitrarily well.", "C": "It ensures that the algorithm's iterative mapping is a contraction, guaranteeing a unique solution.", "D": "It provides the formula for calculating the VARMA coefficients from the policy function values on the grid."}, "Answer": "B", "pi_justification": "This item assesses the understanding of Theorem 3.1's role in justifying the APFI algorithm's projection step (Conceptual Judgment). The correct option (B) correctly identifies that denseness ensures the approximation basis is sufficient. Distractors target common conceptual errors: confusing approximation with identity (A, Almost Right), misattributing the theorem's role to ensuring uniqueness (C, Conceptual Error), or confusing the justification with a procedural detail (D, True but Irrelevant).", "qid": "113", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical foundations of the Analytic Policy Function Iteration (APFI) framework, which is designed to solve a general class of linear rational expectations models with endogenous information frictions.\n\n**Setting / Institutional Environment.** The APFI method is an iterative algorithm that finds an equilibrium policy function. Its validity and computational feasibility rest on three key mathematical theorems that justify its core operational steps.\n\n---\n\n### Data / Model Specification\n\nThe APFI framework is designed to solve models cast in the canonical form:\n```latex\n\\sum_{k=0}^{l}A_{k}y_{t-k}+\\sum_{k=0}^{h}B_{k}\\mathbb{E}_{t}y_{t+k}=\\mathbf{0}\n```\nThe **Baseline APFI Algorithm** involves three main steps in each iteration:\n1.  **Projection:** Approximate the current guess of the policy function using a finite-order VARMA process (i.e., a rational function).\n2.  **Evaluation:** Use the VARMA approximation to compute conditional expectations.\n3.  **Updating & Recursion:** Solve for an updated policy function and repeat until the function converges on a numerical grid.\n\nThis algorithm is justified by three foundational theorems from the paper:\n-   **Theorem 3.1 (Denseness of Rational Functions):** Any covariance-stationary equilibrium process, which resides in the Hardy space `H²`, can be approximated arbitrarily well by a VARMA process (a rational function).\n-   **Theorem 3.2 (Analytic Continuation & Convergence):** A solution found on the real unit interval `(-1, 1)` uniquely extends to the entire complex unit disk. Furthermore, pointwise convergence of a sequence of rational functions on this real interval implies convergence in the `H²` norm.\n-   **Theorem 3.3 (Annihilation):** The crucial \"annihilation\" step in computing expectations can be performed efficiently and accurately using the Inverse Discrete Fourier Transform (IDFT).\n\n---\n\n### The Questions\n\nAccording to the paper, what is the primary theoretical role of **Theorem 3.1 (Denseness of Rational Functions)** for the **Projection** step of the APFI algorithm?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 118, "Question": "### Background\n\n**Research Question.** This problem uses the paper's leading example to provide the geometric intuition for why financial market incompleteness leads to a continuum of real equilibrium allocations, contrasting it with the determinate outcome in a complete markets setting.\n\n**Setting / Institutional Environment.** Consider a simple exchange economy with 2 households, 1 physical good, and 3 spot markets (e.g., today, tomorrow-state-A, tomorrow-state-B). Allocations are described in terms of excess demands `z_h` in a 3-dimensional space.\n\n---\n\n### Data / Model Specification\n\nIn a **complete markets (Walrasian)** setting, a household's choices are constrained by a single lifetime budget constraint, which defines a 2-dimensional **budget plane** in the 3D space of excess demands:\n\n```latex\nz_h^0 + p^1 z_h^1 + p^2 z_h^2 = 0 \\quad \\text{(Eq. (1))}\n```\n\nIn the **incomplete markets** model with one financial instrument, the household faces two reduced-form budget constraints, which together define a 1-dimensional **budget line**.\n\nEquilibrium allocations occur at intersections of the two households' 2-dimensional offer surfaces that can be supported by a common budget set.\n\n---\n\n### Question\n\nWhat is the core geometric reason that the incomplete market structure generally leads to a continuum of equilibrium allocations, while the complete market structure leads to isolated equilibrium allocations?", "Options": {"A": "The two households' offer surfaces, which are 2-dimensional, generically intersect in a 1-dimensional curve. A 1D budget line can be adjusted to support any point on this curve.", "B": "Incomplete markets cause the offer surfaces to become 1-dimensional lines, which intersect at more points than 2-dimensional planes do.", "C": "Equilibrium requires the budget set to be tangent to both offer surfaces. A 1D budget line can be tangent to a curve of points, whereas a 2D plane can only be tangent at isolated points.", "D": "The budget line in the incomplete market case must pass through the origin, while the budget plane does not, allowing for more intersections."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the central geometric intuition of the paper's leading example. Strategy: Conceptual Interpretation. Distractor Logic: The correct answer (A) accurately describes the geometric mechanism. Distractor B incorrectly claims the offer surfaces change shape (Conceptual Error). Distractor C misstates the equilibrium condition; tangency is the condition for the plane, not the line (Incorrect Premise). Distractor D makes a false claim about the geometry; both budget sets pass through the origin in excess demand space (Factual Error).", "qid": "118", "question": "### Background\n\n**Research Question.** This problem uses the paper's leading example to provide the geometric intuition for why financial market incompleteness leads to a continuum of real equilibrium allocations, contrasting it with the determinate outcome in a complete markets setting.\n\n**Setting / Institutional Environment.** Consider a simple exchange economy with 2 households, 1 physical good, and 3 spot markets (e.g., today, tomorrow-state-A, tomorrow-state-B). Allocations are described in terms of excess demands `z_h` in a 3-dimensional space.\n\n---\n\n### Data / Model Specification\n\nIn a **complete markets (Walrasian)** setting, a household's choices are constrained by a single lifetime budget constraint, which defines a 2-dimensional **budget plane** in the 3D space of excess demands:\n\n```latex\nz_h^0 + p^1 z_h^1 + p^2 z_h^2 = 0 \\quad \\text{(Eq. (1))}\n```\n\nIn the **incomplete markets** model with one financial instrument, the household faces two reduced-form budget constraints, which together define a 1-dimensional **budget line**.\n\nEquilibrium allocations occur at intersections of the two households' 2-dimensional offer surfaces that can be supported by a common budget set.\n\n---\n\n### Question\n\nWhat is the core geometric reason that the incomplete market structure generally leads to a continuum of equilibrium allocations, while the complete market structure leads to isolated equilibrium allocations?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 123, "Question": "### Background\n\n**Research Question.** How does the structure of industry-wide adjustment costs for creating new production units determine the cyclical response of an industry's creation and destruction margins?\n\n**Setting.** In a vintage model of creative destruction, an industry can respond to demand fluctuations by adjusting the rate of new unit creation or the rate of old unit destruction. The paper's central theoretical argument is that the cost structure of creation governs which margin adjusts. A key concept is the \"insulation effect,\" where adjustments on the creation margin can shield existing units from demand shocks.\n\n### Data / Model Specification\n\nThe cost of creating a new production unit, `c`, is an increasing function of the aggregate creation rate, `f(0,t)`:\n```latex\nc = c(f(0,t)), \\quad \\text{with } c(\\cdot) > 0 \\text{ and } c'(\\cdot) \\ge 0\n```\n\n### Question\n\nUnder what condition does the \"insulation effect\" become incomplete, leading the destruction margin (`ā(t)`) to become cyclically responsive, and what is the economic rationale?", "Options": {"A": "`c'(f(0,t)) > 0`, because convex costs create an incentive to smooth the creation process, preventing it from fully absorbing demand shocks.", "B": "`c'(f(0,t)) = 0`, because constant costs allow the creation margin to perfectly track and absorb demand shocks.", "C": "`c'(f(0,t)) > 0`, because firms want to create as many units as possible during booms, which amplifies the business cycle.", "D": "`c'(f(0,t)) < 0`, because economies of scale in creation make it optimal to concentrate creation in specific periods."}, "Answer": "A", "pi_justification": "This item assesses the core theoretical mechanism of the paper: the link between convex creation costs and incomplete insulation. It uses a Reverse-Reasoning strategy, asking for the condition that produces a given outcome. Distractor B correctly describes the condition for the opposite case (full insulation). Distractor C uses the correct condition but provides faulty reasoning. Distractor D introduces an irrelevant and incorrect concept (decreasing marginal costs).", "qid": "123", "question": "### Background\n\n**Research Question.** How does the structure of industry-wide adjustment costs for creating new production units determine the cyclical response of an industry's creation and destruction margins?\n\n**Setting.** In a vintage model of creative destruction, an industry can respond to demand fluctuations by adjusting the rate of new unit creation or the rate of old unit destruction. The paper's central theoretical argument is that the cost structure of creation governs which margin adjusts. A key concept is the \"insulation effect,\" where adjustments on the creation margin can shield existing units from demand shocks.\n\n### Data / Model Specification\n\nThe cost of creating a new production unit, `c`, is an increasing function of the aggregate creation rate, `f(0,t)`:\n```latex\nc = c(f(0,t)), \\quad \\text{with } c(\\cdot) > 0 \\text{ and } c'(\\cdot) \\ge 0\n```\n\n### Question\n\nUnder what condition does the \"insulation effect\" become incomplete, leading the destruction margin (`ā(t)`) to become cyclically responsive, and what is the economic rationale?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 64, "Question": "### Background\nThe stability of a generalized Cournot oligopoly model is determined by the condition:\n\n```latex\nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```\n\nwhere `K` is the adjustment speed, `d` is the slope of marginal cost, and `n` is the number of firms.\n\n### Question\nThe original Theocharis model is a special case of this generalized model, assuming constant marginal costs and instantaneous adjustment. What does the stability condition simplify to under the Theocharis assumptions (`d=0` and `K=1`)?", "Options": {"A": "The number of firms must be less than 3 (`n < 3`).", "B": "The number of firms must be greater than 3 (`n > 3`).", "C": "The number of firms must be less than or equal to 3 (`n ≤ 3`).", "D": "The number of firms must be less than 2 (`n < 2`)."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item uses the 'Computational Judgment' strategy. It tests the ability to apply the general formula to a specific, important limiting case discussed in the paper. The calculation is straightforward but requires careful algebraic simplification. The correct answer (A) recovers the famous Theocharis result. Distractors represent common errors: (B) is the condition for instability (a conceptual opposite); (C) incorrectly includes the boundary case of `n=3`, which is oscillatory, not stable; (D) would result from an algebraic slip.", "qid": "64", "question": "### Background\nThe stability of a generalized Cournot oligopoly model is determined by the condition:\n\n```latex\nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```\n\nwhere `K` is the adjustment speed, `d` is the slope of marginal cost, and `n` is the number of firms.\n\n### Question\nThe original Theocharis model is a special case of this generalized model, assuming constant marginal costs and instantaneous adjustment. What does the stability condition simplify to under the Theocharis assumptions (`d=0` and `K=1`)?", "model_evaluation_result": {"model_raw_response": "Looking at the given stability condition:\n\n```latex\nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```\n\nUnder the Theocharis assumptions:\n- `d = 0` (constant marginal costs)\n- `K = 1` (instantaneous adjustment)\n\nSubstituting these values:\n\n```latex\n1 < 2 \\left( \\frac{2b+0}{(n+1)b+0} \\right)\n```\n\nThis simplifies to:\n```latex\n1 < 2 \\left( \\frac{2b}{(n+1)b} \\right)\n```\n\nThe `b` terms cancel out:\n```latex\n1 < 2 \\left( \\frac{2}{n+1} \\right)\n```\n\n```latex\n1 < \\frac{4}{n+1}\n```\n\nMultiplying both sides by `(n+1)`:\n```latex\nn+1 < 4\n```\n\nTherefore:\n```latex\nn < 3\n```\n\nA", "model_answer": "A", "error": null}}
{"ID": 114, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical foundations of the Analytic Policy Function Iteration (APFI) framework, which is designed to solve a general class of linear rational expectations models with endogenous information frictions.\n\n**Setting / Institutional Environment.** The APFI method is an iterative algorithm that finds an equilibrium policy function. Its validity and computational feasibility rest on three key mathematical theorems that justify its core operational steps.\n\n---\n\n### Data / Model Specification\n\nThe APFI framework is designed to solve models cast in the canonical form:\n```latex\n\\sum_{k=0}^{l}A_{k}y_{t-k}+\\sum_{k=0}^{h}B_{k}\\mathbb{E}_{t}y_{t+k}=\\mathbf{0}\n```\nThe **Baseline APFI Algorithm** involves three main steps in each iteration:\n1.  **Projection:** Approximate the current guess of the policy function using a finite-order VARMA process (i.e., a rational function).\n2.  **Evaluation:** Use the VARMA approximation to compute conditional expectations.\n3.  **Updating & Recursion:** Solve for an updated policy function and repeat until the function converges on a numerical grid.\n\nThis algorithm is justified by three foundational theorems from the paper:\n-   **Theorem 3.1 (Denseness of Rational Functions):** Any covariance-stationary equilibrium process, which resides in the Hardy space `H²`, can be approximated arbitrarily well by a VARMA process (a rational function).\n-   **Theorem 3.2 (Analytic Continuation & Convergence):** A solution found on the real unit interval `(-1, 1)` uniquely extends to the entire complex unit disk. Furthermore, pointwise convergence of a sequence of rational functions on this real interval implies convergence in the `H²` norm.\n-   **Theorem 3.3 (Annihilation):** The crucial \"annihilation\" step in computing expectations can be performed efficiently and accurately using the Inverse Discrete Fourier Transform (IDFT).\n\n---\n\n### The Questions\n\nThe paper argues that the standard analytical method for computing expectations (using the residue theorem) is infeasible for models with endogenous information, making the IDFT approach of **Theorem 3.3** necessary. What is the fundamental reason for this infeasibility?", "Options": {"A": "The residue theorem is numerically unstable and less accurate than the IDFT.", "B": "The residue theorem requires knowledge of the function's poles (singularities), but their locations are endogenous to the equilibrium and thus unknown beforehand.", "C": "The residue theorem cannot be applied to non-rational functions, which are common in these models.", "D": "The residue theorem is computationally much slower than the IDFT, making it impractical for large models."}, "Answer": "B", "pi_justification": "This item uses a Reverse-Reasoning strategy to probe the core challenge that Theorem 3.3 overcomes. It asks for the specific reason why the standard analytical method fails in the context of endogenous information. The correct answer (B) pinpoints the circularity of endogenous pole locations. Distractors offer plausible but incorrect reasons: focusing on accuracy/stability instead of feasibility (A), a related but secondary issue (C), and focusing on speed instead of feasibility (D).", "qid": "114", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical foundations of the Analytic Policy Function Iteration (APFI) framework, which is designed to solve a general class of linear rational expectations models with endogenous information frictions.\n\n**Setting / Institutional Environment.** The APFI method is an iterative algorithm that finds an equilibrium policy function. Its validity and computational feasibility rest on three key mathematical theorems that justify its core operational steps.\n\n---\n\n### Data / Model Specification\n\nThe APFI framework is designed to solve models cast in the canonical form:\n```latex\n\\sum_{k=0}^{l}A_{k}y_{t-k}+\\sum_{k=0}^{h}B_{k}\\mathbb{E}_{t}y_{t+k}=\\mathbf{0}\n```\nThe **Baseline APFI Algorithm** involves three main steps in each iteration:\n1.  **Projection:** Approximate the current guess of the policy function using a finite-order VARMA process (i.e., a rational function).\n2.  **Evaluation:** Use the VARMA approximation to compute conditional expectations.\n3.  **Updating & Recursion:** Solve for an updated policy function and repeat until the function converges on a numerical grid.\n\nThis algorithm is justified by three foundational theorems from the paper:\n-   **Theorem 3.1 (Denseness of Rational Functions):** Any covariance-stationary equilibrium process, which resides in the Hardy space `H²`, can be approximated arbitrarily well by a VARMA process (a rational function).\n-   **Theorem 3.2 (Analytic Continuation & Convergence):** A solution found on the real unit interval `(-1, 1)` uniquely extends to the entire complex unit disk. Furthermore, pointwise convergence of a sequence of rational functions on this real interval implies convergence in the `H²` norm.\n-   **Theorem 3.3 (Annihilation):** The crucial \"annihilation\" step in computing expectations can be performed efficiently and accurately using the Inverse Discrete Fourier Transform (IDFT).\n\n---\n\n### The Questions\n\nThe paper argues that the standard analytical method for computing expectations (using the residue theorem) is infeasible for models with endogenous information, making the IDFT approach of **Theorem 3.3** necessary. What is the fundamental reason for this infeasibility?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 117, "Question": "### Background\n\n**Research Question.** This problem explores the maximal degree of real indeterminacy in an incomplete markets model when the financial structure itself (i.e., bond returns) is allowed to vary, rather than being treated as a fixed parameter.\n\n**Setting / Institutional Environment.** The model is extended to consider the set of all possible no-arbitrage financial structures. The total indeterminacy when the structure itself is variable can be decomposed into distinct sources.\n\n**Variables & Parameters.**\n- `N+1`: The number of spot markets.\n- `M`: The number of distinct financial instruments.\n- `n = N - M`: The degree of market incompleteness or deficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper provides an intuitive decomposition for the maximal degree of indeterminacy (`nM`):\n\n```latex\nnM = n + (M-1) + (n-1)(M-1) \\quad \\text{(Eq. (1))}\n```\n\nEach term on the right-hand side of Eq. (1) captures a distinct source of flexibility in the economic environment.\n\n---\n\n### Question\n\nWhat is the correct economic interpretation of the `(M-1)` term in the decomposition formula?", "Options": {"A": "The baseline indeterminacy arising from the deficiency of financial instruments relative to the number of states.", "B": "The additional indeterminacy from allowing the M bond prices to vary, holding relative yields fixed.", "C": "The additional indeterminacy from allowing the bond yields themselves to vary, which changes the structure of the portfolio wealth subspace.", "D": "The indeterminacy that arises from having a sufficient number of households with diverse preferences."}, "Answer": "B", "pi_justification": "Assessment Target: Tests the conceptual understanding of the different sources of indeterminacy. Strategy: Conceptual Interpretation. Distractor Logic: The correct answer (B) correctly identifies the `(M-1)` term with variable prices. Distractor A is the interpretation of the `n` term (Conceptual Opposite). Distractor C is the interpretation of the `(n-1)(M-1)` term (Conceptual Opposite). Distractor D describes Assumption A5 (`m>n`), which is a condition for indeterminacy to exist but is not part of this specific decomposition (True but Irrelevant).", "qid": "117", "question": "### Background\n\n**Research Question.** This problem explores the maximal degree of real indeterminacy in an incomplete markets model when the financial structure itself (i.e., bond returns) is allowed to vary, rather than being treated as a fixed parameter.\n\n**Setting / Institutional Environment.** The model is extended to consider the set of all possible no-arbitrage financial structures. The total indeterminacy when the structure itself is variable can be decomposed into distinct sources.\n\n**Variables & Parameters.**\n- `N+1`: The number of spot markets.\n- `M`: The number of distinct financial instruments.\n- `n = N - M`: The degree of market incompleteness or deficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper provides an intuitive decomposition for the maximal degree of indeterminacy (`nM`):\n\n```latex\nnM = n + (M-1) + (n-1)(M-1) \\quad \\text{(Eq. (1))}\n```\n\nEach term on the right-hand side of Eq. (1) captures a distinct source of flexibility in the economic environment.\n\n---\n\n### Question\n\nWhat is the correct economic interpretation of the `(M-1)` term in the decomposition formula?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 65, "Question": "### Background\n\nAn analysis of \"white flight\" in early 20th-century northern U.S. cities estimates the effect of black in-migration on white population change. The core empirical challenge is that black arrivals do not choose neighborhoods randomly, creating endogeneity. The paper estimates both an OLS model and an instrumental variables (IV) model to address this.\n\n### Data / Model Specification\n\nThe model estimated is:\n```latex\n\\Delta W_{ij}^{t1-t0} = \\alpha + \\beta \\Delta B_{ij}^{t1-t0} + \\eta_{j} + \\epsilon_{ij} \\quad \\text{(Eq. 1)}\n```\nwhere `\\Delta W` is the change in white population, `\\Delta B` is the change in black population, and `\\eta_j` is a city fixed effect. A negative `\\beta` indicates white flight.\n\n**Table 1: OLS and IV Results for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) OLS | (2) IV |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -1.492*** | -3.389*** |\n| | (0.075) | (0.246) |\n\n*Notes: Standard errors in parentheses. *** p<0.01.*\n\n### Question\n\nBased on the results in Table 1, what is the direction of the endogeneity bias in the OLS model, and what is the most likely economic explanation for this bias as discussed in the paper?", "Options": {"A": "The OLS estimate is biased upwards (closer to zero), likely because both black and white households were attracted to the same neighborhoods with improving economic conditions or amenities.", "B": "The OLS estimate is biased downwards (more negative), likely because black households were moving into neighborhoods that whites were already abandoning for reasons unrelated to race.", "C": "The OLS estimate is biased upwards (closer to zero), likely because black households were moving into neighborhoods that whites were already abandoning for reasons unrelated to race.", "D": "The OLS estimate is biased downwards (more negative), likely because both black and white households were attracted to the same neighborhoods with improving economic conditions or amenities."}, "Answer": "A", "pi_justification": "Assessment Target: Diagnosing the direction and cause of OLS bias by comparing OLS and IV estimates. Strategy: Reverse-Reasoning. Distractor Logic: The correct answer (A) correctly identifies the positive bias (upward, toward zero) and the corresponding economic story (shared amenities). Distractor (B) gets both the direction of bias and the economic story wrong (this story would cause positive bias). Distractors (C) and (D) correctly identify one component (bias direction or story) but incorrectly pair it with the other, testing for partial understanding.", "qid": "65", "question": "### Background\n\nAn analysis of \"white flight\" in early 20th-century northern U.S. cities estimates the effect of black in-migration on white population change. The core empirical challenge is that black arrivals do not choose neighborhoods randomly, creating endogeneity. The paper estimates both an OLS model and an instrumental variables (IV) model to address this.\n\n### Data / Model Specification\n\nThe model estimated is:\n```latex\n\\Delta W_{ij}^{t1-t0} = \\alpha + \\beta \\Delta B_{ij}^{t1-t0} + \\eta_{j} + \\epsilon_{ij} \\quad \\text{(Eq. 1)}\n```\nwhere `\\Delta W` is the change in white population, `\\Delta B` is the change in black population, and `\\eta_j` is a city fixed effect. A negative `\\beta` indicates white flight.\n\n**Table 1: OLS and IV Results for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) OLS | (2) IV |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -1.492*** | -3.389*** |\n| | (0.075) | (0.246) |\n\n*Notes: Standard errors in parentheses. *** p<0.01.*\n\n### Question\n\nBased on the results in Table 1, what is the direction of the endogeneity bias in the OLS model, and what is the most likely economic explanation for this bias as discussed in the paper?", "model_evaluation_result": {"model_raw_response": "Looking at the results in Table 1:\n- OLS estimate: -1.492\n- IV estimate: -3.389\n\nThe IV estimate is more negative than the OLS estimate, which means the OLS estimate is biased upwards (closer to zero) relative to the true causal effect.\n\nIn the context of white flight studies, this upward bias typically occurs because black households often moved into neighborhoods that white households were already leaving for reasons unrelated to racial composition - such as neighborhood decline, economic deterioration, or other factors making these areas less desirable. This creates a spurious positive correlation that attenuates the true negative effect of black in-migration on white population change.\n\nWhen using instrumental variables to isolate the exogenous variation in black migration, we remove this confounding factor and recover the larger (more negative) true causal effect of white flight.\n\nC", "model_answer": "C", "error": null}}
{"ID": 124, "Question": "### Background\n\n**Research Question.** How does the structure of industry-wide adjustment costs for creating new production units determine the cyclical response of an industry's creation and destruction margins?\n\n**Setting.** In a vintage model of creative destruction, an industry can respond to demand fluctuations by adjusting the rate of new unit creation or the rate of old unit destruction. The paper's central theoretical argument is that the cost structure of creation governs which margin adjusts.\n\n### Data / Model Specification\n\nThe cost of creating a new production unit, `c`, is an increasing function of the aggregate creation rate, `f(0,t)`. Using a first-order Taylor approximation, the change in the creation rate, `Δf`, in response to a shock to the present value of a new firm, `ΔV`, can be expressed as:\n```latex\n\\Delta f \\approx \\frac{\\Delta V}{c'(f_0)}\n```\nwhere `c'(f₀)` is the slope of the creation cost function at the initial creation rate `f₀`.\n\n### Question\n\nGiven the derived relationship in Eq. (1), what is the correct interpretation of the role of the adjustment cost parameter `c'(f₀)`?", "Options": {"A": "A larger `c'(f₀)` dampens the response of creation to a given shock, reflecting a stronger incentive to smooth the creation process.", "B": "A larger `c'(f₀)` amplifies the response of creation, as higher marginal costs force a more dramatic adjustment to meet demand.", "C": "`c'(f₀)` determines the steady-state level of creation but does not affect its cyclical response to shocks.", "D": "A larger `c'(f₀)` makes the value of new firms (`ΔV`) more sensitive to demand shocks."}, "Answer": "A", "pi_justification": "This item tests the student's ability to interpret a key mathematical result from the model. It is a Computational Judgment question that requires translating a formula into an economic concept. Distractor B is the conceptual opposite. Distractor C is an 'Almost Right' error, confusing level effects with cyclical effects. Distractor D misinterprets the causal chain in the formula.", "qid": "124", "question": "### Background\n\n**Research Question.** How does the structure of industry-wide adjustment costs for creating new production units determine the cyclical response of an industry's creation and destruction margins?\n\n**Setting.** In a vintage model of creative destruction, an industry can respond to demand fluctuations by adjusting the rate of new unit creation or the rate of old unit destruction. The paper's central theoretical argument is that the cost structure of creation governs which margin adjusts.\n\n### Data / Model Specification\n\nThe cost of creating a new production unit, `c`, is an increasing function of the aggregate creation rate, `f(0,t)`. Using a first-order Taylor approximation, the change in the creation rate, `Δf`, in response to a shock to the present value of a new firm, `ΔV`, can be expressed as:\n```latex\n\\Delta f \\approx \\frac{\\Delta V}{c'(f_0)}\n```\nwhere `c'(f₀)` is the slope of the creation cost function at the initial creation rate `f₀`.\n\n### Question\n\nGiven the derived relationship in Eq. (1), what is the correct interpretation of the role of the adjustment cost parameter `c'(f₀)`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 128, "Question": "### Background\n\n**Research Question.** This problem evaluates the cost-effectiveness of health insurance subsidies.\n\n**Setting / Institutional Environment.** A key concept in policy evaluation is the \"inframarginal transfer,\" which is the portion of a subsidy paid to individuals who would have purchased the good (e.g., health insurance) even without the subsidy. A policy is more cost-effective at changing behavior (e.g., covering the uninsured) if a smaller fraction of its total cost consists of inframarginal transfers.\n\n### Question\n\nA policymaker proposes extending the same health insurance tax subsidy to two different populations: Group A, with a pre-policy private insurance coverage rate of 30%, and Group B, with a pre-policy rate of 80%. Assuming both groups have identical price sensitivity, for which group would the subsidy be more cost-effective at reducing the number of uninsured, and why?", "Options": {"A": "Group B, because the higher initial coverage rate indicates a stronger underlying demand for insurance, making subsidies more effective.", "B": "Group B, because with more people already insured, the market is more stable and administrative costs are lower.", "C": "Group A, because its members are more likely to have lower incomes and therefore be more responsive to the subsidy.", "D": "Group A, because a smaller share of the total subsidy cost would go to people who would have purchased insurance anyway."}, "Answer": "D", "pi_justification": "This item assesses the understanding of 'inframarginal transfers' and their role in policy cost-effectiveness, a core concept from the source QA. It uses a Scenario Application strategy. The correct answer (D) correctly identifies that a lower initial coverage rate means a smaller share of the subsidy is 'wasted' on inframarginal transfers. Distractors A and B represent a classic conceptual error, incorrectly associating higher pre-existing uptake with higher policy effectiveness. Distractor C provides a plausible but irrelevant reason, as the prompt states price sensitivity is identical.", "qid": "128", "question": "### Background\n\n**Research Question.** This problem evaluates the cost-effectiveness of health insurance subsidies.\n\n**Setting / Institutional Environment.** A key concept in policy evaluation is the \"inframarginal transfer,\" which is the portion of a subsidy paid to individuals who would have purchased the good (e.g., health insurance) even without the subsidy. A policy is more cost-effective at changing behavior (e.g., covering the uninsured) if a smaller fraction of its total cost consists of inframarginal transfers.\n\n### Question\n\nA policymaker proposes extending the same health insurance tax subsidy to two different populations: Group A, with a pre-policy private insurance coverage rate of 30%, and Group B, with a pre-policy rate of 80%. Assuming both groups have identical price sensitivity, for which group would the subsidy be more cost-effective at reducing the number of uninsured, and why?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 121, "Question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal can design a mechanism to extract all economic surplus from privately informed agents.\n\n**Setting:** The analysis centers on the properties of the common prior, $p$, over the space of all type profiles.\n\n### Data / Model Specification\n\n**Key Definitions:**\n- **Beliefs-Determine-Preferences (BDP) Prior:** A prior where, with probability 1, a bidder's beliefs about others uniquely determine their own valuation.\n- **Non-BDP (NBDP) Prior:** A prior that is not BDP.\n- **Convex Combination of Priors:** A mixture $p = \\alpha p' + (1-\\alpha)p''$ for $\\alpha \\in (0,1)$. A key result is that a non-degenerate convex combination of a BDP prior and an NBDP prior is always an NBDP prior.\n\n### The Question\n\nSuppose a researcher has two common priors: $p'$, which is a BDP prior, and $p''$, which is a non-BDP prior. If the true prior is a non-degenerate convex combination of the two, $p = 0.5 p' + 0.5 p''$, what is the property of the resulting prior $p$?", "Options": {"A": "$p$ is a non-BDP prior because the type profiles that violate BDP in $p''$ still exist with positive probability in the mixture $p$.", "B": "$p$ is a BDP prior because the BDP property of $p'$ 'dominates' the mixture.", "C": "The property of $p$ is ambiguous and depends on the specific structures of $p'$ and $p''$.", "D": "$p$ is 'partially BDP', meaning FSE is possible for half of the types."}, "Answer": "A", "pi_justification": "This item uses a Scenario Application strategy to test the student's understanding of how the BDP property behaves under mixtures, a key result from Proposition 1. The correct answer requires understanding *why* the mixture is non-BDP. Distractor B is the conceptual opposite, C suggests ambiguity where there is none, and D invents a plausible-sounding but incorrect concept ('partially BDP').", "qid": "121", "question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal can design a mechanism to extract all economic surplus from privately informed agents.\n\n**Setting:** The analysis centers on the properties of the common prior, $p$, over the space of all type profiles.\n\n### Data / Model Specification\n\n**Key Definitions:**\n- **Beliefs-Determine-Preferences (BDP) Prior:** A prior where, with probability 1, a bidder's beliefs about others uniquely determine their own valuation.\n- **Non-BDP (NBDP) Prior:** A prior that is not BDP.\n- **Convex Combination of Priors:** A mixture $p = \\alpha p' + (1-\\alpha)p''$ for $\\alpha \\in (0,1)$. A key result is that a non-degenerate convex combination of a BDP prior and an NBDP prior is always an NBDP prior.\n\n### The Question\n\nSuppose a researcher has two common priors: $p'$, which is a BDP prior, and $p''$, which is a non-BDP prior. If the true prior is a non-degenerate convex combination of the two, $p = 0.5 p' + 0.5 p''$, what is the property of the resulting prior $p$?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 71, "Question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nA researcher uses the restricted test `Ȳ⁽²⁾` based on a strong prior that `θ₁=θ₂`. However, the true data generating process is `(θ₁, θ₂) = (-0.1, 0.0)`. According to Table 1, what is the approximate probability that the researcher commits a Type II error (fails to reject the false null `H₀: θ₁=θ₂=0`) at the 5% significance level?", "Options": {"A": "60.2%", "B": "39.8%", "C": "51.3%", "D": "91.8%"}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to perform a precise quantitative calculation and interpretation from a data table. It uses a Computational Judgment strategy. The correct answer is 1 - 0.398 = 0.602. Distractor B is a classic Step-Omission Error, reporting the power instead of the Type II error probability (1 - power). Distractor C is a Formula Misuse error, correctly calculating the Type II error but for the wrong test (the unrestricted one: 1 - 0.487). Distractor D results from a Lookup Error, using data from the wrong row (`(0.0, -0.1)`) to calculate the Type II error (1 - 0.082).", "qid": "71", "question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nA researcher uses the restricted test `Ȳ⁽²⁾` based on a strong prior that `θ₁=θ₂`. However, the true data generating process is `(θ₁, θ₂) = (-0.1, 0.0)`. According to Table 1, what is the approximate probability that the researcher commits a Type II error (fails to reject the false null `H₀: θ₁=θ₂=0`) at the 5% significance level?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to identify what constitutes a Type II error for the restricted test when the true parameters are (θ₁, θ₂) = (-0.1, 0.0).\n\nThe restricted test Ȳ⁽²⁾ tests the null hypothesis H₀: θ₁ = θ₂ = 0.\n\nWhen the true parameters are (θ₁, θ₂) = (-0.1, 0.0), the null hypothesis is false because:\n- θ₁ = -0.1 ≠ 0\n- The joint condition θ₁ = θ₂ = 0 is violated\n\nA Type II error occurs when we fail to reject a false null hypothesis.\n\nFrom Table 1, for the case (θ₁, θ₂) = (-0.1, 0.0), the restricted test Ȳ⁽²⁾ has a rejection frequency of 0.398.\n\nThis means:\n- The test rejects the false null 39.8% of the time (this is the power)\n- The test fails to reject the false null 100% - 39.8% = 60.2% of the time\n\nTherefore, the probability of committing a Type II error is 60.2%.\n\nA", "model_answer": "A", "error": null}}
{"ID": 76, "Question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`. The distribution of tenant abilities is given by `g(e_i)`.\n\n### Question\n\nThe paper concludes that the welfare ranking of the two regimes is ambiguous and depends on the distribution of tenant abilities, `g(e_i)`. What is the fundamental reason for this ambiguity?", "Options": {"A": "The linking regime's profit is determined by the marginal tenant's productivity, while the no-linking regime's expected profit depends on the average productivity of all tenants.", "B": "The linking regime is always socially efficient, while the no-linking regime is always inefficient.", "C": "Landlords face risk in the no-linking regime but not in the linking regime, and the model assumes they are risk-averse.", "D": "High-ability tenants earn informational rents in the linking regime, which are competed away in the no-linking regime."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item targets the paper's core policy conclusion. It uses a Conceptual Judgment strategy to identify the key trade-off. The correct answer (A) pinpoints the 'marginal vs. average' mechanism. Distractors are designed to be plausible but incorrect: (B) is an overstatement (linking isn't always efficient) and doesn't explain the profit ambiguity; (C) is a Conceptual Error (model assumes risk neutrality); (D) is True but Irrelevant, as it describes tenant welfare, not the ambiguity in landlord profits which drives the overall conclusion.", "qid": "76", "question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`. The distribution of tenant abilities is given by `g(e_i)`.\n\n### Question\n\nThe paper concludes that the welfare ranking of the two regimes is ambiguous and depends on the distribution of tenant abilities, `g(e_i)`. What is the fundamental reason for this ambiguity?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 62, "Question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\nThe incentive to cheat is defined as the ratio of the one-time gain from cheating to the per-period profit foregone by triggering punishment:\n\n```latex\n\\text{Incentive to Cheat} = \\frac{\\pi_{\\text{cheat}} - \\pi_{\\text{collude}}}{\\pi_{\\text{collude}} - \\pi_{\\text{punish}}} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n\n### The Question\n\nSuppose a policy intervention intensifies downstream competition, causing the noncooperative profit of each downstream firm (`π^NC`) to fall from 10 to 5. This reduces the 'Payoff if punished' for the integrated firm to 5. Assume the integrated firm's collusive payoff remains 30. What is the new 'Incentive to Cheat' for the integrated firm, and what is the implication for antitrust?", "Options": {"A": "2.8; the merger becomes a greater antitrust concern.", "B": "2.8; the merger becomes less of an antitrust concern.", "C": "3.5; there is no change in antitrust concern.", "D": "14.0; the merger becomes a greater antitrust concern."}, "Answer": "A", "pi_justification": "This item assesses the ability to apply the model to a novel scenario, combining calculation with judgment. It uses a Scenario Application strategy. Distractors are based on a correct calculation but incorrect interpretation (B), a failure to update a parameter (C), and a fundamental misuse of the formula (D).", "qid": "62", "question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\nThe incentive to cheat is defined as the ratio of the one-time gain from cheating to the per-period profit foregone by triggering punishment:\n\n```latex\n\\text{Incentive to Cheat} = \\frac{\\pi_{\\text{cheat}} - \\pi_{\\text{collude}}}{\\pi_{\\text{collude}} - \\pi_{\\text{punish}}} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n\n### The Question\n\nSuppose a policy intervention intensifies downstream competition, causing the noncooperative profit of each downstream firm (`π^NC`) to fall from 10 to 5. This reduces the 'Payoff if punished' for the integrated firm to 5. Assume the integrated firm's collusive payoff remains 30. What is the new 'Incentive to Cheat' for the integrated firm, and what is the implication for antitrust?", "model_evaluation_result": {"model_raw_response": "I need to calculate the new \"Incentive to Cheat\" using the given formula and data.\n\nGiven information:\n- Formula: Incentive to Cheat = (π_cheat - π_collude) / (π_collude - π_punish)\n- From Table 1: π_collude = 30, π_cheat = 100\n- After policy intervention: π_punish falls from 10 to 5\n- π_collude remains 30\n\nCalculating the new Incentive to Cheat:\n- π_cheat - π_collude = 100 - 30 = 70\n- π_collude - π_punish = 30 - 5 = 25\n- Incentive to Cheat = 70/25 = 2.8\n\nNow I need to determine the antitrust implication. \n\nBefore the intervention:\n- Incentive to Cheat = (100 - 30)/(30 - 10) = 70/20 = 3.5\n\nAfter the intervention:\n- Incentive to Cheat = 2.8\n\nSince the incentive to cheat decreased from 3.5 to 2.8, collusion becomes more stable (easier to sustain). When collusion is more stable, the merger becomes less of an antitrust concern because the integrated firm is less likely to deviate from collusive behavior.\n\nB", "model_answer": "B", "error": null}}
{"ID": 41, "Question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. The economy can be stuck in an inefficient equilibrium.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nEquilibrium reservation values for two potential assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Exact        | 0.301           | 0.904           | 2.410           |\n| Mixed (1, 2) | 0.293           | 0.870           | 2.581           |\n\n### Question\n\nSuppose the economy is in the 'Exact' equilibrium, where matches between type-1 workers and type-2 jobs are rejected. To shift the economy to the more efficient 'Mixed (1, 2)' equilibrium, a planner must incentivize this match. What is the minimum per-match subsidy required to make the (worker 1, job 2) match privately rational, based on the 'Exact' equilibrium reservation values?", "Options": {"A": "0.025", "B": "-0.025", "C": "0.017", "D": "0.114"}, "Answer": "A", "pi_justification": "This item tests the ability to perform a policy-relevant calculation based on the model. It uses a Computational Judgment strategy. Distractors are designed based on plausible errors: B (Sign Error: reporting the private deficit instead of the subsidy needed to overcome it), C (Lookup Error: using reservation values from the target 'Mixed (1, 2)' equilibrium instead of the initial 'Exact' equilibrium), and D (Misapplication: calculating the subsidy for a different, incorrect match, such as (2,3)).", "qid": "41", "question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. The economy can be stuck in an inefficient equilibrium.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nEquilibrium reservation values for two potential assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Exact        | 0.301           | 0.904           | 2.410           |\n| Mixed (1, 2) | 0.293           | 0.870           | 2.581           |\n\n### Question\n\nSuppose the economy is in the 'Exact' equilibrium, where matches between type-1 workers and type-2 jobs are rejected. To shift the economy to the more efficient 'Mixed (1, 2)' equilibrium, a planner must incentivize this match. What is the minimum per-match subsidy required to make the (worker 1, job 2) match privately rational, based on the 'Exact' equilibrium reservation values?", "model_evaluation_result": {"model_raw_response": "I need to determine the minimum per-match subsidy required to make the (worker 1, job 2) match privately rational in the 'Exact' equilibrium.\n\nFrom the problem:\n- In the 'Exact' equilibrium, matches between type-1 workers and type-2 jobs are rejected\n- This means the match (1,2) is not privately optimal under current reservation values\n- I need to find the minimum subsidy to make this match privately rational\n\nFor a match to be privately rational, the output must be greater than or equal to the sum of reservation wage and profit:\n$a_{ij} \\geq w_{0i} + r_{0j}$\n\nFrom the productivity matrix A:\n$a_{12} = 1.18$\n\nFrom Table 1, under the 'Exact' equilibrium:\n- $w_{01} = 0.301$\n- $r_{02} = 0.904$\n\nSo the sum of reservation values is:\n$w_{01} + r_{02} = 0.301 + 0.904 = 1.205$\n\nCurrently, the match (1,2) is not privately rational because:\n$a_{12} = 1.18 < 1.205 = w_{01} + r_{02}$\n\nThe deficit is:\n$1.205 - 1.18 = 0.025$\n\nTo make this match privately rational, we need a subsidy of at least 0.025 to cover this deficit.\n\nTherefore, the minimum per-match subsidy required is 0.025.\n\nA", "model_answer": "A", "error": null}}
{"ID": 104, "Question": "### Background\n\nEarly real business cycle models relied on Richard Rogerson's “employment lotteries” aggregation theory to generate a high aggregate labor supply elasticity from a model with indivisible labor. This framework was used by Edward Prescott to argue that differences in labor income tax rates could explain the large employment gap between the U.S. and Europe.\n\n### Data / Model Specification\n\nIn a static economy with a representative family, indivisible labor, and a flat labor tax `τ`, the elasticity of the equilibrium aggregate labor supply `N` with respect to the net-of-tax rate `(1-τ)` is given by:\n\n```latex\n\\eta_{N, 1-\\tau} = \\frac{\\gamma}{(1-\\tau)^2+\\gamma}\n\\quad \\text{(Eq. (1))}\n```\n\nA common calibration, consistent with balanced growth, sets `γ=1`.\n\n### Question\n\nUsing the model specified in Eq. (1) with `γ=1`, what is the approximate aggregate labor supply elasticity for a European-style economy with a labor income tax rate (τ) of 0.60?", "Options": {"A": "0.86", "B": "0.74", "C": "1.00", "D": "0.16"}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to apply a core formula from the paper to a specific case. It uses a Computational Judgment strategy. The distractors are designed based on common errors: (B) is the elasticity for the US tax rate (`τ=0.4`), a plausible mix-up; (C) is a salient value (`unity`) mentioned in the text for a limiting case, representing a conceptual error; (D) is an intermediate value from the calculation (`(1-τ)^2`), representing a step-omission error.", "qid": "104", "question": "### Background\n\nEarly real business cycle models relied on Richard Rogerson's “employment lotteries” aggregation theory to generate a high aggregate labor supply elasticity from a model with indivisible labor. This framework was used by Edward Prescott to argue that differences in labor income tax rates could explain the large employment gap between the U.S. and Europe.\n\n### Data / Model Specification\n\nIn a static economy with a representative family, indivisible labor, and a flat labor tax `τ`, the elasticity of the equilibrium aggregate labor supply `N` with respect to the net-of-tax rate `(1-τ)` is given by:\n\n```latex\n\\eta_{N, 1-\\tau} = \\frac{\\gamma}{(1-\\tau)^2+\\gamma}\n\\quad \\text{(Eq. (1))}\n```\n\nA common calibration, consistent with balanced growth, sets `γ=1`.\n\n### Question\n\nUsing the model specified in Eq. (1) with `γ=1`, what is the approximate aggregate labor supply elasticity for a European-style economy with a labor income tax rate (τ) of 0.60?", "model_evaluation_result": {"model_raw_response": "I need to calculate the aggregate labor supply elasticity using the given formula with γ = 1 and τ = 0.60.\n\nGiven:\n- Formula: η_{N, 1-τ} = γ/[(1-τ)² + γ]\n- γ = 1\n- τ = 0.60\n\nFirst, I'll calculate (1-τ):\n1-τ = 1 - 0.60 = 0.40\n\nNext, I'll calculate (1-τ)²:\n(1-τ)² = (0.40)² = 0.16\n\nNow I can substitute into the formula:\nη_{N, 1-τ} = 1/(0.16 + 1) = 1/1.16 ≈ 0.862\n\nRounding to two decimal places, this gives approximately 0.86.\n\nD", "model_answer": "D", "error": null}}
{"ID": 1, "Question": "### Background\n\nA researcher is faced with a dataset where some observations are incomplete. The validity of different statistical methods depends on the assumed mechanism generating the missing data. The primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nUnder which of the following scenarios does the Missing at Random (MAR) assumption hold, but the more restrictive Missing Completely at Random (MCAR) assumption is violated?", "Options": {"A": "In a survey of income and education, individuals with lower reported education levels are more likely to refuse to answer the income question.", "B": "A researcher accidentally spills coffee on a random subset of survey forms, making the income data on them illegible.", "C": "In a survey of income, individuals with extremely high incomes are the most likely to refuse to report their income.", "D": "In a survey of income, the distribution of reported income is highly skewed to the right."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the crucial distinction between MCAR and MAR assumptions. It uses a Scenario Application strategy, requiring the test-taker to apply abstract definitions to concrete examples. The distractors are designed to probe for common confusions: B represents a pure MCAR case, C represents the more complex MNAR case (a key conceptual neighbor), and D is a true but irrelevant statement about data distribution.", "qid": "1", "question": "### Background\n\nA researcher is faced with a dataset where some observations are incomplete. The validity of different statistical methods depends on the assumed mechanism generating the missing data. The primary assumptions are:\n- **MCAR (Missing Completely at Random):** The probability of a value being missing is independent of any data, observed or unobserved.\n- **MAR (Missing at Random):** The probability of a value being missing depends only on observed data.\n\n---\n\n### Question\n\nUnder which of the following scenarios does the Missing at Random (MAR) assumption hold, but the more restrictive Missing Completely at Random (MCAR) assumption is violated?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify a scenario where MAR holds but MCAR is violated.\n\nLet me analyze each option:\n\nA) Individuals with lower reported education levels are more likely to refuse to answer the income question.\n- The missingness of income depends on observed education level\n- This satisfies MAR (missingness depends only on observed data)\n- This violates MCAR (missingness is not independent of all data - it depends on education)\n- MAR holds, MCAR violated ✓\n\nB) Coffee spilled on random subset of forms making income data illegible.\n- This is completely random - no relationship to any variables\n- This satisfies both MCAR and MAR\n- Does not meet the criteria ✗\n\nC) Individuals with extremely high incomes are most likely to refuse to report income.\n- Missingness depends on the unobserved income value itself\n- This violates MAR (depends on unobserved data)\n- This also violates MCAR\n- Neither assumption holds ✗\n\nD) Distribution of reported income is highly skewed.\n- This describes the distribution of observed data, not the missing data mechanism\n- No information about missingness pattern\n- Not relevant to missing data assumptions ✗\n\nA", "model_answer": "A", "error": null}}
{"ID": 91, "Question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive estimation procedures in the paper rely on a consistent estimator for the error variance `\\sigma^2`.\n\n---\n\n### Data / Model Specification\n\nWhen replication is unavailable, the paper proposes the Smallest Singular Value (SSV) estimator for the variance:\n```latex\n\\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p \n```\nwhere `\\hat{l}_q` is the smallest singular value of the data matrix `X`. This estimator is consistent for `\\sigma^2 + p^{-1}l_q^2`, where `l_q` is the smallest singular value of the true matrix `M`.\n\n---\n\n### Question\n\nUnder what condition on the true mean matrix `M` is the Smallest Singular Value (SSV) estimator a consistent estimator for the true error variance `\\sigma^2` as `p \\to \\infty`?", "Options": {"A": "The true matrix `M` must be full rank (`l_q > 0`).", "B": "The largest singular value of `M`, `l_1`, must satisfy `l_1^2/p \\to 0`.", "C": "The smallest singular value of `M`, `l_q`, must satisfy `l_q^2/p \\to 0`.", "D": "The true matrix `M` must have reduced rank (`l_q = 0`)."}, "Answer": "C", "pi_justification": "This item assesses the key identification condition for the proposed variance estimator. It uses a Scenario Application strategy. The distractors are designed based on common errors: (A) is the conceptual opposite, the condition under which bias is likely; (B) involves formula misuse by confusing the largest and smallest singular values; (D) is an 'Almost Right' error, as it is a sufficient but not necessary condition, making the correct answer more precise.", "qid": "91", "question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive estimation procedures in the paper rely on a consistent estimator for the error variance `\\sigma^2`.\n\n---\n\n### Data / Model Specification\n\nWhen replication is unavailable, the paper proposes the Smallest Singular Value (SSV) estimator for the variance:\n```latex\n\\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p \n```\nwhere `\\hat{l}_q` is the smallest singular value of the data matrix `X`. This estimator is consistent for `\\sigma^2 + p^{-1}l_q^2`, where `l_q` is the smallest singular value of the true matrix `M`.\n\n---\n\n### Question\n\nUnder what condition on the true mean matrix `M` is the Smallest Singular Value (SSV) estimator a consistent estimator for the true error variance `\\sigma^2` as `p \\to \\infty`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine when the SSV estimator is consistent for σ².\n\nGiven information:\n- The SSV estimator is: σ̂²_SSV = l̂²_q / p\n- This estimator is consistent for σ² + p⁻¹l²_q\n- We want it to be consistent for σ²\n\nFor the SSV estimator to be consistent for σ², we need:\nσ̂²_SSV → σ² as p → ∞\n\nSince σ̂²_SSV is consistent for σ² + p⁻¹l²_q, we need:\nσ² + p⁻¹l²_q → σ² as p → ∞\n\nThis requires:\np⁻¹l²_q → 0 as p → ∞\n\nWhich is equivalent to:\nl²_q/p → 0 as p → ∞\n\nThis condition ensures that the bias term p⁻¹l²_q vanishes asymptotically, making the estimator consistent for the true error variance σ².\n\nLooking at the options:\n- Option A requires M to be full rank, but this doesn't address the convergence condition\n- Option B involves the largest singular value l₁, which is not relevant here\n- Option C states exactly what we derived: l²_q/p → 0\n- Option D requires M to have reduced rank (l_q = 0), which is a special case but not necessary\n\nC", "model_answer": "A", "error": null}}
{"ID": 57, "Question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nA temporary, unexpected fiscal expansion occurs in the home country today (`dG_0 > 0`). This policy is transmitted internationally, causing a fall in foreign private consumption. What is the primary economic mechanism driving this negative transmission?", "Options": {"A": "The expansion creates an excess demand for current goods, raising the world interest rate and reducing the present value of wealth in both countries.", "B": "The expansion creates an excess supply of current goods, lowering the world interest rate and crowding in foreign investment.", "C": "The expansion worsens the foreign country's terms of trade, reducing the value of its exports.", "D": "The expansion is funded by taxes on the home country, which directly reduces foreign income through trade linkages."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses understanding of the transmission mechanism for a current transitory fiscal shock using a Reverse-Reasoning strategy. The correct answer (A) identifies the key role of the interest rate in clearing the goods market. Distractor (B) presents the opposite mechanism. Distractor (C) incorrectly applies a concept from the two-good model, testing for model confusion. Distractor (D) offers a vague, non-model-based explanation.", "qid": "57", "question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nA temporary, unexpected fiscal expansion occurs in the home country today (`dG_0 > 0`). This policy is transmitted internationally, causing a fall in foreign private consumption. What is the primary economic mechanism driving this negative transmission?", "model_evaluation_result": {"model_raw_response": "Looking at this two-country model with integrated capital markets, I need to trace through the transmission mechanism of a temporary fiscal expansion.\n\nWhen the home country implements a temporary fiscal expansion (dG₀ > 0):\n\n1. **Initial Impact**: Government spending increases current demand for goods in the home country.\n\n2. **Market Clearing**: In a one-good world with integrated capital markets, this creates excess demand in the global goods market at the current interest rate.\n\n3. **Interest Rate Adjustment**: To restore equilibrium, the world interest rate must rise. This higher interest rate reduces investment and consumption demand globally until the market clears.\n\n4. **Wealth Effects**: The higher world interest rate reduces the present value of future income streams in both countries. Since consumption is proportional to wealth, this reduces private consumption in both the home and foreign countries.\n\n5. **International Transmission**: The foreign country experiences a fall in private consumption not because of any direct fiscal impact (the policy is only in the home country), but because the higher world interest rate reduces foreign private wealth.\n\nThis mechanism explains why foreign consumption falls: the temporary fiscal expansion raises the world interest rate, which reduces the present value of wealth globally.\n\nOption A correctly identifies this transmission mechanism - the expansion creates excess demand for current goods, raising the world interest rate and reducing the present value of wealth in both countries.\n\nA", "model_answer": "A", "error": null}}
{"ID": 85, "Question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe evolution of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) from one period to the next is governed by the following equations, which combine an accelerator mechanism with a neoclassical profitability term:\n\n```latex\n\\frac{d\\bar{K_{i}}}{\\bar{K_{i}}} = A_{i}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\pi_{i}r} \\frac{\\delta F_{i}}{\\delta K}\\right]^{\\sigma_{i}} - 1 \\quad \\text{(Eq. (1))}\n```\n\n```latex\n\\frac{d\\bar{E_{i}}}{\\bar{E_{i}}} = A_{i}^{\\prime}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\bar{w_{i}}} \\frac{\\delta F_{i}}{\\delta E}\\right]^{\\sigma_{i}^{\\prime}} - 1 \\quad \\text{(Eq. (2))}\n```\nwhere `g_i` is expected demand growth, `\\alpha_i` is the activity level, `p_i^v` is the value-added price, `\\pi_i r` is the rental cost of capital, `\\bar{w}_i` is the wage cost, and `F_i` is the production function.\n\n## Question\n\nIn the model, a long-run equilibrium is a steady state where capital and labor stocks are constant (`d\\bar{K}_i / \\bar{K}_i = 0`, `d\\bar{E}_i / \\bar{E}_i = 0`), the economy operates at full capacity (`\\alpha_i = 1`), and expected demand growth is zero (`g_i = 0`). Based on the dynamic adjustment equations (Eq. (1) and Eq. (2)), which pair of conditions correctly describes this long-run equilibrium?", "Options": {"A": "The marginal revenue product of capital equals its rental cost (`p_i^v (\\delta F_i / \\delta K) = \\pi_i r`), and the marginal revenue product of labor equals its wage cost (`p_i^v (\\delta F_i / \\delta E) = \\bar{w}_i`).", "B": "The marginal product of capital is zero (`\\delta F_i / \\delta K = 0`), and the marginal product of labor is zero (`\\delta F_i / \\delta E = 0`).", "C": "The marginal revenue product of capital equals the wage (`p_i^v (\\delta F_i / \\delta K) = \\bar{w}_i`), and the marginal revenue product of labor equals the rental cost of capital (`p_i^v (\\delta F_i / \\delta E) = \\pi_i r`).", "D": "The average revenue product of capital equals its rental cost (`p_i^v (F_i / \\bar{K}_i) = \\pi_i r`), and the average revenue product of labor equals its wage (`p_i^v (F_i / \\bar{E}_i) = \\bar{w}_i`)."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item replaces the derivation task in question 2 of the original QA problem with a sharp conceptual check on its outcome. It assesses whether the user can correctly identify the standard neoclassical profit-maximization conditions that emerge as the long-run steady state of the model's dynamic equations. Distractors are designed based on common errors in microeconomic theory: (B) incorrectly sets marginal products to zero, (C) swaps the prices of capital and labor, and (D) confuses marginal products with average products.", "qid": "85", "question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe evolution of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) from one period to the next is governed by the following equations, which combine an accelerator mechanism with a neoclassical profitability term:\n\n```latex\n\\frac{d\\bar{K_{i}}}{\\bar{K_{i}}} = A_{i}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\pi_{i}r} \\frac{\\delta F_{i}}{\\delta K}\\right]^{\\sigma_{i}} - 1 \\quad \\text{(Eq. (1))}\n```\n\n```latex\n\\frac{d\\bar{E_{i}}}{\\bar{E_{i}}} = A_{i}^{\\prime}[g_{i}-(1-\\alpha_{i})] + \\left[\\frac{p_{i}^{v}}{\\bar{w_{i}}} \\frac{\\delta F_{i}}{\\delta E}\\right]^{\\sigma_{i}^{\\prime}} - 1 \\quad \\text{(Eq. (2))}\n```\nwhere `g_i` is expected demand growth, `\\alpha_i` is the activity level, `p_i^v` is the value-added price, `\\pi_i r` is the rental cost of capital, `\\bar{w}_i` is the wage cost, and `F_i` is the production function.\n\n## Question\n\nIn the model, a long-run equilibrium is a steady state where capital and labor stocks are constant (`d\\bar{K}_i / \\bar{K}_i = 0`, `d\\bar{E}_i / \\bar{E}_i = 0`), the economy operates at full capacity (`\\alpha_i = 1`), and expected demand growth is zero (`g_i = 0`). Based on the dynamic adjustment equations (Eq. (1) and Eq. (2)), which pair of conditions correctly describes this long-run equilibrium?", "model_evaluation_result": {"model_raw_response": "I need to analyze the long-run equilibrium conditions based on the given dynamic adjustment equations.\n\nGiven conditions for long-run equilibrium:\n- Capital and labor stocks are constant: $\\frac{d\\bar{K}_i}{\\bar{K}_i} = 0$ and $\\frac{d\\bar{E}_i}{\\bar{E}_i} = 0$\n- Full capacity operation: $\\alpha_i = 1$\n- Zero expected demand growth: $g_i = 0$\n\nFrom Equation (1), setting $\\frac{d\\bar{K}_i}{\\bar{K}_i} = 0$:\n$$0 = A_i[g_i - (1-\\alpha_i)] + \\left[\\frac{p_i^v}{\\pi_i r} \\frac{\\delta F_i}{\\delta K}\\right]^{\\sigma_i} - 1$$\n\nSubstituting $g_i = 0$ and $\\alpha_i = 1$:\n$$0 = A_i[0 - (1-1)] + \\left[\\frac{p_i^v}{\\pi_i r} \\frac{\\delta F_i}{\\delta K}\\right]^{\\sigma_i} - 1$$\n$$0 = 0 + \\left[\\frac{p_i^v}{\\pi_i r} \\frac{\\delta F_i}{\\delta K}\\right]^{\\sigma_i} - 1$$\n$$\\left[\\frac{p_i^v}{\\pi_i r} \\frac{\\delta F_i}{\\delta K}\\right]^{\\sigma_i} = 1$$\n\nTaking the $\\sigma_i$-th root:\n$$\\frac{p_i^v}{\\pi_i r} \\frac{\\delta F_i}{\\delta K} = 1$$\n\nTherefore: $p_i^v \\frac{\\delta F_i}{\\delta K} = \\pi_i r$\n\nFrom Equation (2), setting $\\frac{d\\bar{E}_i}{\\bar{E}_i} = 0$:\n$$0 = A", "model_answer": "A", "error": null}}
{"ID": 21, "Question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe paper proposes a simpler weight, `α₂`, which is derived under the assumption that the covariance between the micro and macro forecasts is zero. The formula is:\n```latex\nα₂ = \\frac{σ₂²}{(z_T'γ - a'X_Tβ)² + σ₁² + σ₂²}\n```\nwhere `σ₁²` and `σ₂²` are the variances of the macro and aggregated micro forecasts, respectively.\n\nIn the formula for the simplified weight `α₂`, what is the correct interpretation of the 'inconsistency' term `(z_T'γ - a'X_Tβ)²`?", "Options": {"A": "It measures the estimation uncertainty (variance) of the macro forecast.", "B": "It quantifies the systematic difference between the macro model's structure and the aggregated micro model's structure.", "C": "It reflects the degree of correlation between the micro and macro forecast errors.", "D": "It captures the portion of the micro forecast variance that is explained by the macro model."}, "Answer": "B", "pi_justification": "Assessment Target: Tests the core economic intuition behind the forecast combination formula, specifically the role of model inconsistency. Strategy: Scenario Application (interpreting a term within a formula). Distractor Logic: (A) Confuses the inconsistency term with the variance term `σ₁²`. (C) Is a Conceptual Opposite, as this formula is derived assuming zero correlation. (D) Is a plausible-sounding but incorrect statistical interpretation (Formula Misuse).", "qid": "21", "question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe paper proposes a simpler weight, `α₂`, which is derived under the assumption that the covariance between the micro and macro forecasts is zero. The formula is:\n```latex\nα₂ = \\frac{σ₂²}{(z_T'γ - a'X_Tβ)² + σ₁² + σ₂²}\n```\nwhere `σ₁²` and `σ₂²` are the variances of the macro and aggregated micro forecasts, respectively.\n\nIn the formula for the simplified weight `α₂`, what is the correct interpretation of the 'inconsistency' term `(z_T'γ - a'X_Tβ)²`?", "model_evaluation_result": {"model_raw_response": "Looking at the formula for α₂:\n\n```\nα₂ = σ₂²/[(z_T'γ - a'X_Tβ)² + σ₁² + σ₂²]\n```\n\nThe term `(z_T'γ - a'X_Tβ)²` represents:\n- `z_T'γ`: the macro model's forecast structure\n- `a'X_Tβ`: the aggregated micro model's forecast structure  \n- The difference between these two structures, squared\n\nThis term captures the systematic discrepancy between what the macro model predicts and what the aggregated micro model predicts. It's not about random variance or correlation, but about the fundamental structural differences in how the two modeling approaches generate their forecasts.\n\nWhen the macro and micro models are perfectly consistent (i.e., when aggregated, they would produce identical forecasts), this term equals zero. When there are systematic differences in their modeling approaches, this term becomes positive, representing the \"inconsistency\" between the two forecasting frameworks.\n\nB", "model_answer": "B", "error": null}}
{"ID": 73, "Question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n*   `a_{ij}`: The amount of factor `j` required to produce one unit of commodity `i`, chosen to minimize cost.\n*   `A`: The `n x n` matrix of input coefficients `a_{ij}`.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production:\n\n```latex\nq_{i} = \\sum_{j=1}^{n} a_{ij} p_{j}\n```\n\nA standard result from production theory (derived from the envelope theorem) states that at the cost-minimizing input choice:\n\n```latex\n\\sum_{j=1}^{n} p_{j} \\frac{\\partial a_{ij}}{\\partial p_{k}} = 0\n```\n\n---\n\n### The Questions\n\nGiven the cost function in Eq. (1) and the envelope theorem result in Eq. (2), what is the Jacobian matrix `J` of the mapping from factor prices `p` to commodity prices `q`, where the elements of the Jacobian are `J_{ik} = ∂q_i/∂p_k`?", "Options": {"A": "The matrix of input coefficients, `A`", "B": "The transpose of the input matrix, `A^T`", "C": "The zero matrix, `0`", "D": "The matrix `A + M`, where `M` is a matrix with elements `M_{ik} = Σ_j p_j (∂a_{ij}/∂p_k)`"}, "Answer": "A", "pi_justification": "This question tests the ability to perform a key mathematical derivation central to the paper's local analysis. It uses a Computational Judgment strategy. The correct answer (A) results from correctly applying the product rule and the provided envelope theorem identity in Eq. (2). Distractor (B) represents a common index-confusion error. Distractor (D) represents a 'Step-Omission Error,' where the student applies the product rule but fails to use the identity in Eq. (2) to cancel the second term. Distractor (C) represents a misunderstanding of the envelope theorem's application.", "qid": "73", "question": "### Background\n\n**Research Question.** This problem investigates the foundational economic properties and mathematical conditions that underpin the Factor Price Equalization (FPE) theorem. It explores how the assumption of cost-minimization shapes the cost function and how this relates to the local and global invertibility of the mapping from factor prices to commodity prices.\n\n**Setting / Institutional Environment.** We consider a model with `n` commodities and `n` factors of production. Commodity prices are determined by competitive firms choosing production techniques from a compact set of options to minimize costs. International trade ensures that the vector of commodity prices, `q`, is the same across trading countries with identical technologies.\n\n**Variables & Parameters.**\n*   `q`: An `n x 1` vector of commodity prices.\n*   `p`: An `n x 1` vector of factor prices.\n*   `a_{ij}`: The amount of factor `j` required to produce one unit of commodity `i`, chosen to minimize cost.\n*   `A`: The `n x n` matrix of input coefficients `a_{ij}`.\n\n---\n\n### Data / Model Specification\n\nThe price of each commodity is determined by its minimum cost of production:\n\n```latex\nq_{i} = \\sum_{j=1}^{n} a_{ij} p_{j}\n```\n\nA standard result from production theory (derived from the envelope theorem) states that at the cost-minimizing input choice:\n\n```latex\n\\sum_{j=1}^{n} p_{j} \\frac{\\partial a_{ij}}{\\partial p_{k}} = 0\n```\n\n---\n\n### The Questions\n\nGiven the cost function in Eq. (1) and the envelope theorem result in Eq. (2), what is the Jacobian matrix `J` of the mapping from factor prices `p` to commodity prices `q`, where the elements of the Jacobian are `J_{ik} = ∂q_i/∂p_k`?", "model_evaluation_result": {"model_raw_response": "I need to find the Jacobian matrix J where J_{ik} = ∂q_i/∂p_k.\n\nStarting with the cost function:\nq_i = Σ_{j=1}^n a_{ij} p_j\n\nTaking the partial derivative with respect to p_k:\n∂q_i/∂p_k = ∂/∂p_k [Σ_{j=1}^n a_{ij} p_j]\n\nUsing the product rule:\n∂q_i/∂p_k = Σ_{j=1}^n [∂a_{ij}/∂p_k · p_j + a_{ij} · ∂p_j/∂p_k]\n\nSince ∂p_j/∂p_k = 1 if j=k and 0 otherwise:\n∂q_i/∂p_k = Σ_{j=1}^n p_j ∂a_{ij}/∂p_k + a_{ik}\n\nNow I need to use the envelope theorem result from Eq. (2):\nΣ_{j=1}^n p_j ∂a_{ij}/∂p_k = 0\n\nThis means the first term in my expression equals zero, so:\n∂q_i/∂p_k = 0 + a_{ik} = a_{ik}\n\nTherefore, the Jacobian matrix J has elements J_{ik} = a_{ik}, which means J = A.\n\nThe answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 63, "Question": "### Background\nIn a generalized Cournot oligopoly model, the dynamic evolution of the output vector `x_t` is governed by a discrete-time system `x_t = B_n x_{t-1} + \\text{constant}`. The stability of this system depends on the eigenvalues of the transition matrix `B_n`.\n\n### Data / Model Specification\nAssume the following facts derived from the model:\n1.  The eigenvalues of `B_n` are related to the eigenvalues of another matrix `K A_n` (denoted `\\lambda_i`) by the transformation: `eig(B_n)_i = 1 + \\lambda_i`.\n2.  All eigenvalues `\\lambda_i` of `K A_n` are real and negative.\n3.  For the special case where all firms have the same adjustment speed, `K_i = K`, the most negative eigenvalue of `K A_n` is given by:\n    ```latex\n    \\lambda_n = -K \\left( \\frac{(n+1)b+d}{2b+d} \\right)\n    ```\n    where `n` is the number of firms, `b > 0` is related to the demand slope, and `d` is the slope of the firms' marginal cost curves.\n\n### Question\nA discrete linear dynamic system is stable if and only if all eigenvalues of its transition matrix have an absolute value less than 1. Given the information above, what is the necessary and sufficient condition for stability in this model?", "Options": {"A": "```latex\nK < 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```", "B": "```latex\nK > 2 \\left( \\frac{2b+d}{(n+1)b+d} \\right)\n```", "C": "```latex\nK < 2 \\left( \\frac{(n+1)b+d}{2b+d} \\right)\n```", "D": "```latex\nK < \\frac{2b+d}{(n+1)b+d}\n```"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item uses the 'Computational Judgment' strategy. It tests the ability to combine the abstract stability rule (`|1+\\lambda| < 1`) with the specific properties of `\\lambda` (real, negative) to derive the crucial intermediate condition `\\lambda > -2`, and then correctly perform the algebraic manipulation on the formula for `\\lambda_n` to arrive at the final stability condition. Distractors are based on common, high-frequency errors: (B) is a sign error from failing to flip the inequality; (C) is a formula misuse error from incorrectly isolating K; (D) is a step-omission error from forgetting the factor of 2.", "qid": "63", "question": "### Background\nIn a generalized Cournot oligopoly model, the dynamic evolution of the output vector `x_t` is governed by a discrete-time system `x_t = B_n x_{t-1} + \\text{constant}`. The stability of this system depends on the eigenvalues of the transition matrix `B_n`.\n\n### Data / Model Specification\nAssume the following facts derived from the model:\n1.  The eigenvalues of `B_n` are related to the eigenvalues of another matrix `K A_n` (denoted `\\lambda_i`) by the transformation: `eig(B_n)_i = 1 + \\lambda_i`.\n2.  All eigenvalues `\\lambda_i` of `K A_n` are real and negative.\n3.  For the special case where all firms have the same adjustment speed, `K_i = K`, the most negative eigenvalue of `K A_n` is given by:\n    ```latex\n    \\lambda_n = -K \\left( \\frac{(n+1)b+d}{2b+d} \\right)\n    ```\n    where `n` is the number of firms, `b > 0` is related to the demand slope, and `d` is the slope of the firms' marginal cost curves.\n\n### Question\nA discrete linear dynamic system is stable if and only if all eigenvalues of its transition matrix have an absolute value less than 1. Given the information above, what is the necessary and sufficient condition for stability in this model?", "model_evaluation_result": {"model_raw_response": "I need to find the condition for stability of the discrete-time system.\n\nGiven information:\n- Stability requires all eigenvalues of B_n to have absolute value < 1\n- eig(B_n)_i = 1 + λ_i where λ_i are eigenvalues of KA_n\n- All λ_i are real and negative\n- The most negative eigenvalue is λ_n = -K((n+1)b+d)/(2b+d)\n\nFor stability, I need |eig(B_n)_i| < 1 for all i.\n\nSince eig(B_n)_i = 1 + λ_i and all λ_i are negative, we have:\neig(B_n)_i = 1 + λ_i < 1 (since λ_i < 0)\n\nThe critical condition comes from ensuring eig(B_n)_i > -1:\n1 + λ_i > -1\nλ_i > -2\n\nSince λ_n is the most negative eigenvalue, if λ_n > -2, then all other eigenvalues will also satisfy this condition.\n\nSo I need:\nλ_n > -2\n\nSubstituting λ_n = -K((n+1)b+d)/(2b+d):\n\n-K((n+1)b+d)/(2b+d) > -2\n\nMultiplying both sides by -1 (and flipping the inequality):\nK((n+1)b+d)/(2b+d) < 2\n\nTherefore:\nK < 2(2b+d)/((n+1)b+d)\n\nThe answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 116, "Question": "### Background\n\n**Research Question.** This problem explores the maximal degree of real indeterminacy in an incomplete markets model when the financial structure itself (i.e., bond returns) is allowed to vary, rather than being treated as a fixed parameter.\n\n**Setting / Institutional Environment.** The model is extended to consider the set of all possible no-arbitrage financial structures, which can be represented by different bond return matrices. The analysis compares the indeterminacy arising from a fixed financial structure to the total indeterminacy when the structure itself is variable.\n\n**Variables & Parameters.**\n- `N+1`: The number of spot markets.\n- `M`: The number of distinct financial instruments.\n- `n = N - M`: The degree of market incompleteness or deficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper summarizes the different levels of indeterminacy based on which components of the financial structure are treated as variable.\n\n**Table 1: Degrees of Real Indeterminacy**\n\n| | Minimal (Fixed Returns) | Intermediate (Variable Prices) | Maximal (Variable Returns) |\n| :--- | :--- | :--- | :--- |\n| **Degree of Indeterminacy** | `n` | `N - 1` | `nM` |\n\nConsider an economy with `N+1=10` spot markets (so `N=9`) and `M=3` distinct financial assets.\n\n---\n\n### Question\n\nFor the economy described, what is the degree of real indeterminacy for the **intermediate** case where bond prices are variable but yields are fixed?", "Options": {"A": "6", "B": "8", "C": "9", "D": "18"}, "Answer": "B", "pi_justification": "Assessment Target: Tests the ability to correctly identify and apply a formula from a table to a specific numerical case. Strategy: Computational Judgment. Distractor Logic: The correct answer is `N - 1 = 9 - 1 = 8`. Distractor A (`6`) is the result of misapplying the 'Minimal' case formula (`n = N - M = 9 - 3 = 6`). Distractor D (`18`) is the result of misapplying the 'Maximal' case formula (`nM = 6 * 3 = 18`). Distractor C (`9`) results from a step-omission error, using `N` instead of `N-1`.", "qid": "116", "question": "### Background\n\n**Research Question.** This problem explores the maximal degree of real indeterminacy in an incomplete markets model when the financial structure itself (i.e., bond returns) is allowed to vary, rather than being treated as a fixed parameter.\n\n**Setting / Institutional Environment.** The model is extended to consider the set of all possible no-arbitrage financial structures, which can be represented by different bond return matrices. The analysis compares the indeterminacy arising from a fixed financial structure to the total indeterminacy when the structure itself is variable.\n\n**Variables & Parameters.**\n- `N+1`: The number of spot markets.\n- `M`: The number of distinct financial instruments.\n- `n = N - M`: The degree of market incompleteness or deficiency.\n\n---\n\n### Data / Model Specification\n\nThe paper summarizes the different levels of indeterminacy based on which components of the financial structure are treated as variable.\n\n**Table 1: Degrees of Real Indeterminacy**\n\n| | Minimal (Fixed Returns) | Intermediate (Variable Prices) | Maximal (Variable Returns) |\n| :--- | :--- | :--- | :--- |\n| **Degree of Indeterminacy** | `n` | `N - 1` | `nM` |\n\nConsider an economy with `N+1=10` spot markets (so `N=9`) and `M=3` distinct financial assets.\n\n---\n\n### Question\n\nFor the economy described, what is the degree of real indeterminacy for the **intermediate** case where bond prices are variable but yields are fixed?", "model_evaluation_result": {"model_raw_response": "Looking at the given information:\n\n- N+1 = 10 spot markets, so N = 9\n- M = 3 distinct financial assets\n- We need the intermediate case degree of indeterminacy\n\nFrom Table 1, the intermediate case (Variable Prices) has degree of indeterminacy = N - 1\n\nTherefore: N - 1 = 9 - 1 = 8\n\nB", "model_answer": "B", "error": null}}
{"ID": 90, "Question": "### Background\n\n**Research Question:** This problem examines the mathematical construction of uncorrelated time-series scenarios using Chebyshev polynomials, which serve as efficient inputs for emulating a complex climate model.\n\n**Setting / Institutional Environment:** The methodology generates a set of basis vectors for CO2 emissions from a family of orthogonal polynomials. These polynomials are defined over a canonical interval `x ∈ [-1, 1]` and must be transformed to be used for a specific time horizon, e.g., years `t ∈ [2005, 2100]`.\n\n### Data / Model Specification\n\nChebyshev polynomials are defined by the recurrence relation:\n\n```latex\nP_{0}(x)=1, \\quad P_{1}(x)=x, \\quad P_{k}(x)=2x P_{k-1}(x)-P_{k-2}(x) \\quad \\text{for } k=2,3,\\ldots \\quad \\text{(Eq. 1)}\n```\n\nThe domain `x ∈ [-1, 1]` is mapped to a general time interval `t ∈ [a, b]` using the transformation:\n\n```latex\nt = \\left(\\frac{b-a}{2}\\right)x + \\frac{a+b}{2} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nConsider an emissions scenario based on the Chebyshev polynomial `P_2(x) = 2x^2 - 1`. Initially, it is defined over the time horizon `[2005, 2100]`. If the horizon is extended to `[2005, 2150]`, how does the `x` value in the canonical domain `[-1, 1]` corresponding to the year `t=2100` change?", "Options": {"A": "It remains unchanged at 1.0.", "B": "It decreases from 1.0 to approximately 0.31.", "C": "It increases from 1.0 to approximately 1.52.", "D": "It is undefined because 2100 is no longer an endpoint."}, "Answer": "B", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 10.0). This item tests the understanding of how the domain transformation works and its sensitivity to the time horizon, a key technical detail. Strategy: Computational Judgment. The correct answer requires calculating the `x` value for `t=2100` under both the old and new transformations. Distractors are designed to catch common misunderstandings: (A) is a Conceptual Error, failing to recognize the transformation depends on the interval endpoints; (C) represents a plausible calculation error; (D) is an 'Almost Right' distractor that correctly identifies that 2100 is no longer an endpoint but incorrectly concludes the value is undefined.", "qid": "90", "question": "### Background\n\n**Research Question:** This problem examines the mathematical construction of uncorrelated time-series scenarios using Chebyshev polynomials, which serve as efficient inputs for emulating a complex climate model.\n\n**Setting / Institutional Environment:** The methodology generates a set of basis vectors for CO2 emissions from a family of orthogonal polynomials. These polynomials are defined over a canonical interval `x ∈ [-1, 1]` and must be transformed to be used for a specific time horizon, e.g., years `t ∈ [2005, 2100]`.\n\n### Data / Model Specification\n\nChebyshev polynomials are defined by the recurrence relation:\n\n```latex\nP_{0}(x)=1, \\quad P_{1}(x)=x, \\quad P_{k}(x)=2x P_{k-1}(x)-P_{k-2}(x) \\quad \\text{for } k=2,3,\\ldots \\quad \\text{(Eq. 1)}\n```\n\nThe domain `x ∈ [-1, 1]` is mapped to a general time interval `t ∈ [a, b]` using the transformation:\n\n```latex\nt = \\left(\\frac{b-a}{2}\\right)x + \\frac{a+b}{2} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nConsider an emissions scenario based on the Chebyshev polynomial `P_2(x) = 2x^2 - 1`. Initially, it is defined over the time horizon `[2005, 2100]`. If the horizon is extended to `[2005, 2150]`, how does the `x` value in the canonical domain `[-1, 1]` corresponding to the year `t=2100` change?", "model_evaluation_result": {"model_raw_response": "I need to analyze how the transformation from the canonical domain [-1, 1] to different time intervals affects the x value corresponding to t = 2100.\n\nGiven transformation equation:\nt = ((b-a)/2)x + (a+b)/2\n\nLet me solve for x in terms of t:\nx = (2t - (a+b))/(b-a)\n\n**Case 1: Original horizon [2005, 2100]**\n- a = 2005, b = 2100\n- For t = 2100:\nx = (2(2100) - (2005 + 2100))/(2100 - 2005)\nx = (4200 - 4105)/95\nx = 95/95 = 1.0\n\n**Case 2: Extended horizon [2005, 2150]**\n- a = 2005, b = 2150\n- For t = 2100:\nx = (2(2100) - (2005 + 2150))/(2150 - 2005)\nx = (4200 - 4155)/145\nx = 45/145\nx ≈ 0.31\n\nSo when the time horizon is extended from [2005, 2100] to [2005, 2150], the x value corresponding to t = 2100 changes from 1.0 to approximately 0.31.\n\nThis makes sense because in the original interval, 2100 was the right endpoint (x = 1), but in the extended interval, 2100 is no longer an endpoint and maps to an interior point of the canonical domain.\n\nB", "model_answer": "B", "error": null}}
{"ID": 140, "Question": "### Background\n\n**Research Question.** This problem examines how different types of information technology can have opposing effects on the boundaries of the firm, depending on which organizational trade-off they influence. It focuses on the shipper's decision to use a private fleet versus a for-hire carrier.\n\n**Setting / Institutional Environment.** A shipper's choice between a private fleet (integration) and a for-hire carrier (disintegration) involves two key trade-offs:\n1.  **Service vs. Cost:** Private fleets are generally better at providing customized, high-service hauls (e.g., complex cargo handling), but motivating drivers for these tasks is difficult and costly.\n2.  **Utilization Efficiency:** For-hire carriers are specialists in logistics and can achieve higher truck utilization by finding backhauls across a large network of customers, a key comparative advantage.\n\nWe consider a specific type of on-board computer:\n-   **`OBC_M` (Monitoring):** Provides verifiable data on driver actions, making it easier to contract for high-service tasks.\n\n### The Question\n\nAccording to the \"contractual externality\" argument presented in the paper, why does the adoption of a driver-monitoring OBC (`OBC_M`) lead to more shipper ownership of trucks (i.e., more private fleets)?", "Options": {"A": "`OBC_M` makes it cheaper to motivate drivers for high-service tasks, increasing the optimal service level demanded by shippers and thereby favoring private fleets which specialize in high service.", "B": "`OBC_M` enhances the logistical efficiency of private fleets, allowing them to better compete with the utilization rates of for-hire carriers.", "C": "`OBC_M` reduces the need for relationship-specific investments by drivers, making arm's-length contracts with for-hire carriers more efficient.", "D": "`OBC_M` allows shippers to directly monitor for-hire carrier drivers, reducing agency costs in outsourced relationships without changing ownership."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the core 'contractual externality' argument. It uses a Scenario Application strategy to test the student's grasp of the multi-step causal chain. The correct answer (A) captures the full logic. Distractors are designed to pinpoint specific misunderstandings: (B) confuses the mechanism with the one for logistics tech, (C) applies G&H logic in the opposite direction, and (D) identifies a plausible but incorrect causal path that is not the contractual externality discussed.", "qid": "140", "question": "### Background\n\n**Research Question.** This problem examines how different types of information technology can have opposing effects on the boundaries of the firm, depending on which organizational trade-off they influence. It focuses on the shipper's decision to use a private fleet versus a for-hire carrier.\n\n**Setting / Institutional Environment.** A shipper's choice between a private fleet (integration) and a for-hire carrier (disintegration) involves two key trade-offs:\n1.  **Service vs. Cost:** Private fleets are generally better at providing customized, high-service hauls (e.g., complex cargo handling), but motivating drivers for these tasks is difficult and costly.\n2.  **Utilization Efficiency:** For-hire carriers are specialists in logistics and can achieve higher truck utilization by finding backhauls across a large network of customers, a key comparative advantage.\n\nWe consider a specific type of on-board computer:\n-   **`OBC_M` (Monitoring):** Provides verifiable data on driver actions, making it easier to contract for high-service tasks.\n\n### The Question\n\nAccording to the \"contractual externality\" argument presented in the paper, why does the adoption of a driver-monitoring OBC (`OBC_M`) lead to more shipper ownership of trucks (i.e., more private fleets)?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 109, "Question": "### Background\n\nAnother proposed solution to the persistence problem is to assume that each firm's production requires a **specific factor** (e.g., land) in addition to labor and capital. This leads to decreasing returns to scale at the firm level, which can, under certain conditions, make prices less sensitive to aggregate shocks.\n\n---\n\n### Data / Model Specification\n\nIn this model, the relative output of two firms `i` and `j` is given by:\n```latex\ny_i/y_j = (P_j/P_i)^\\varepsilon\n```\n(Eq. 1)\nwhere `\\varepsilon` is the elasticity of demand for the firms' products.\n\n---\n\n### Question\n\nThe paper calculates that to achieve an empirically realistic contract multiplier of 20, a demand elasticity of `\\varepsilon = 6500` is required. Using the relative output formula in Eq. (1), what is the implied ratio of firm `i`'s output to firm `j`'s output (`y_i/y_j`) if firm `j`'s price is just 0.1% higher than firm `i`'s (i.e., `P_j/P_i = 1.001`)?", "Options": {"A": "The ratio is approximately 7.5.", "B": "The ratio is approximately 665.", "C": "The ratio is approximately 6,507.", "D": "The ratio is approximately 1.0."}, "Answer": "B", "pi_justification": "This item assesses the ability to perform a quantitative critique of a theoretical model using a Computational Judgment strategy. The correct answer requires calculating an exponential. Distractor A (7.5) results from a common linear approximation error (`1+nx`). Distractor C (6,507) results from incorrectly multiplying instead of exponentiating, a formula misuse error. Distractor D (1.0) represents a failure to understand the model, assuming no effect.", "qid": "109", "question": "### Background\n\nAnother proposed solution to the persistence problem is to assume that each firm's production requires a **specific factor** (e.g., land) in addition to labor and capital. This leads to decreasing returns to scale at the firm level, which can, under certain conditions, make prices less sensitive to aggregate shocks.\n\n---\n\n### Data / Model Specification\n\nIn this model, the relative output of two firms `i` and `j` is given by:\n```latex\ny_i/y_j = (P_j/P_i)^\\varepsilon\n```\n(Eq. 1)\nwhere `\\varepsilon` is the elasticity of demand for the firms' products.\n\n---\n\n### Question\n\nThe paper calculates that to achieve an empirically realistic contract multiplier of 20, a demand elasticity of `\\varepsilon = 6500` is required. Using the relative output formula in Eq. (1), what is the implied ratio of firm `i`'s output to firm `j`'s output (`y_i/y_j`) if firm `j`'s price is just 0.1% higher than firm `i`'s (i.e., `P_j/P_i = 1.001`)?", "model_evaluation_result": {"model_raw_response": "I need to calculate the ratio y_i/y_j using the given formula and values.\n\nGiven:\n- Formula: y_i/y_j = (P_j/P_i)^ε\n- ε = 6500\n- P_j/P_i = 1.001\n\nSubstituting into the formula:\ny_i/y_j = (1.001)^6500\n\nTo calculate (1.001)^6500:\nUsing the approximation for small x: (1 + x)^n ≈ e^(nx) when x is small\nHere, x = 0.001 and n = 6500\n\nSo: (1.001)^6500 ≈ e^(0.001 × 6500) = e^6.5\n\ne^6.5 ≈ 665\n\nTherefore, y_i/y_j ≈ 665\n\nB", "model_answer": "B", "error": null}}
{"ID": 13, "Question": "### Background\n\nIn the model, the identity of the decisive median voter can change randomly from period to period due to shocks (`ε`) affecting voter participation or eligibility. The median voter's preference, `α^m`, is defined implicitly as the value that splits the voting population in half:\n\n```latex\n\\int_{0}^{\\alpha^{m}(\\varepsilon)} \\gamma(\\alpha, \\varepsilon) d\\alpha - \\frac{1}{2} = 0 \\quad \\text{(Eq. 1)}\n```\n\nwhere `γ(α, ε)` is the probability density function of preferences. The sensitivity of the median voter to a shock is given by:\n\n```latex\n\\frac{d\\alpha^m}{d\\varepsilon} = - \\frac{\\int_{0}^{\\alpha^{m}} \\gamma_{\\varepsilon}(\\alpha, \\varepsilon) d\\alpha}{\\gamma(\\alpha^m, \\varepsilon)} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAccording to the model, a political system is considered more unstable if the identity of the median voter (`α^m`) is highly sensitive to random shocks (`ε`). Given the formula for this sensitivity (Eq. 2), which characteristic of the voter preference distribution `γ(α)` would lead to the greatest political instability?", "Options": {"A": "A high density of voters at the median, `γ(α^m, ε)`.", "B": "A low density of voters at the median, `γ(α^m, ε)`.", "C": "A distribution that is unimodal and symmetric around `α = 0.5`.", "D": "A distribution where the total number of voters is very large."}, "Answer": "B", "pi_justification": "This item uses a Reverse-Reasoning strategy to assess a key testable implication of the model: the link between preference distribution and political instability. It replaces a QA sub-question because it targets a specific, sharp insight perfectly suited for a choice format. The distractors are designed based on a clear error matrix: Conceptual Opposite (A: high density implies stability), True but Irrelevant/Distracting (C: a unimodal distribution implies high median density and thus stability), and Irrelevant (D).", "qid": "13", "question": "### Background\n\nIn the model, the identity of the decisive median voter can change randomly from period to period due to shocks (`ε`) affecting voter participation or eligibility. The median voter's preference, `α^m`, is defined implicitly as the value that splits the voting population in half:\n\n```latex\n\\int_{0}^{\\alpha^{m}(\\varepsilon)} \\gamma(\\alpha, \\varepsilon) d\\alpha - \\frac{1}{2} = 0 \\quad \\text{(Eq. 1)}\n```\n\nwhere `γ(α, ε)` is the probability density function of preferences. The sensitivity of the median voter to a shock is given by:\n\n```latex\n\\frac{d\\alpha^m}{d\\varepsilon} = - \\frac{\\int_{0}^{\\alpha^{m}} \\gamma_{\\varepsilon}(\\alpha, \\varepsilon) d\\alpha}{\\gamma(\\alpha^m, \\varepsilon)} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAccording to the model, a political system is considered more unstable if the identity of the median voter (`α^m`) is highly sensitive to random shocks (`ε`). Given the formula for this sensitivity (Eq. 2), which characteristic of the voter preference distribution `γ(α)` would lead to the greatest political instability?", "model_evaluation_result": {"model_raw_response": "Looking at equation (2), the sensitivity of the median voter to shocks is:\n\n$$\\frac{d\\alpha^m}{d\\varepsilon} = - \\frac{\\int_{0}^{\\alpha^{m}} \\gamma_{\\varepsilon}(\\alpha, \\varepsilon) d\\alpha}{\\gamma(\\alpha^m, \\varepsilon)}$$\n\nThe magnitude of this sensitivity (which determines political instability) depends on the absolute value of this expression. The denominator is $\\gamma(\\alpha^m, \\varepsilon)$, which represents the density of voters at the median.\n\nWhen $\\gamma(\\alpha^m, \\varepsilon)$ is **small** (low density at the median), the denominator becomes small, making the entire fraction larger in absolute value. This means the median voter's identity becomes highly sensitive to shocks.\n\nIntuitively, if there are few voters near the median position, small changes in voter participation or eligibility can easily shift which voter becomes the new median. The median voter's identity becomes volatile.\n\nConversely, when $\\gamma(\\alpha^m, \\varepsilon)$ is **large** (high density at the median), there are many voters clustered around the median position, making the system more stable since it takes larger shocks to meaningfully change the median voter's identity.\n\nTherefore, a low density of voters at the median leads to the greatest political instability.\n\nB", "model_answer": "B", "error": null}}
{"ID": 8, "Question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), the implied prior correlation between `\\psi` and `\\lambda` is:\n```latex\nCorr(\\psi, \\lambda) = \\frac{1}{\\sqrt{1 + \\tau^2/\\omega^2}} \\quad \\text{(Eq. 1)}\n```\nMarginal learning about the non-identified parameter `\\lambda` is possible if and only if `\\psi` and `\\lambda` are a priori dependent (i.e., their correlation is non-zero).\n\n### Question\n\nWhat is the effect of using an improper prior for `\\beta` (i.e., letting `\\tau^2 \\to \\infty`), and what is the direct consequence for learning about `\\lambda`?", "Options": {"A": "The prior correlation `Corr(\\psi, \\lambda)` goes to 1, which maximizes marginal learning about `\\lambda`.", "B": "The prior correlation `Corr(\\psi, \\lambda)` goes to 0, which has no effect on learning about `\\lambda` as that is determined by the likelihood.", "C": "The prior correlation `Corr(\\psi, \\lambda)` becomes undefined, which makes the joint posterior for `(\\psi, \\lambda)` improper.", "D": "The prior correlation `Corr(\\psi, \\lambda)` goes to 0, which makes `\\psi` and `\\lambda` a priori independent and prevents any marginal learning about `\\lambda`."}, "Answer": "D", "pi_justification": "Assessment Target: Tests the ability to calculate the effect of an improper prior on the key prior dependence and understand its consequence for learning. Strategy: Computational Judgment. The user must evaluate a limit and connect it to the theory. Distractor Logic: A) is the Conceptual Opposite, describing the case where `\\omega^2 \\to \\infty`. B) correctly evaluates the limit but draws an incorrect conclusion about its consequence. C) describes a plausible but incorrect outcome.", "qid": "8", "question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), the implied prior correlation between `\\psi` and `\\lambda` is:\n```latex\nCorr(\\psi, \\lambda) = \\frac{1}{\\sqrt{1 + \\tau^2/\\omega^2}} \\quad \\text{(Eq. 1)}\n```\nMarginal learning about the non-identified parameter `\\lambda` is possible if and only if `\\psi` and `\\lambda` are a priori dependent (i.e., their correlation is non-zero).\n\n### Question\n\nWhat is the effect of using an improper prior for `\\beta` (i.e., letting `\\tau^2 \\to \\infty`), and what is the direct consequence for learning about `\\lambda`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze what happens to the prior correlation when τ² → ∞ (improper prior for β).\n\nFrom Equation 1:\nCorr(ψ, λ) = 1/√(1 + τ²/ω²)\n\nAs τ² → ∞:\n- The ratio τ²/ω² → ∞ \n- Therefore 1 + τ²/ω² → ∞\n- So 1/√(1 + τ²/ω²) → 0\n\nThis means Corr(ψ, λ) → 0.\n\nThe background explicitly states: \"Marginal learning about the non-identified parameter λ is possible if and only if ψ and λ are a priori dependent (i.e., their correlation is non-zero).\"\n\nSince the correlation goes to 0, ψ and λ become a priori independent, which prevents any marginal learning about the non-identified parameter λ.\n\nLet me check the options:\n- A) Incorrect - correlation goes to 0, not 1\n- B) Incorrect - while correlation goes to 0, it claims no effect on learning, but the background clearly states learning requires non-zero correlation\n- C) Incorrect - the correlation is well-defined (it equals 0), and the posterior can still be proper\n- D) Correct - correlation goes to 0, making ψ and λ a priori independent, preventing marginal learning about λ\n\nD", "model_answer": "A", "error": null}}
{"ID": 14, "Question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`. For GPRs, this is equivalent to `A_1(\\bar{c}, E) = ... = A_{|N|}(\\bar{c}, E)`. Let this common value be `A(\\bar{c}, E)`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n\n### Question\n\nA Generalized Proportional Rule (GPR) is refined by imposing two axioms: Efficiency (`∑_{i∈N} f_i(c, E) = E`) and Uniform Treatment of Uniforms (UTU), which implies the non-proportional component `A_i` is the same for all entities (`A_i(\\bar{c}, E) = A(\\bar{c}, E)`). Which of the following formulas correctly represents the resulting division rule `f_i(c, E)`?", "Options": {"A": "```latex\nf_i(c, E) = \\frac{E}{|N|}(1 - \\sum_{k \\in K} W_k(\\bar{c}, E)) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E\n```", "B": "```latex\nf_i(c, E) = (E - \\sum_{k \\in K} W_k(\\bar{c}, E) E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E\n```", "C": "```latex\nf_i(c, E) = \\frac{E}{|N|} + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} (W_k(\\bar{c}, E) - 1) E\n```", "D": "```latex\nf_i(c, E) = \\frac{E}{|N|}(1 - \\sum_{k \\in K} W_k(\\bar{c}, E)) + \\sum_{k \\in K} c_{ik} W_k(\\bar{c}, E) E\n```"}, "Answer": "A", "pi_justification": "This item assesses the ability to perform the multi-step algebraic derivation from Corollary 2, a core task in the original QA problem. It uses a Computational Judgment strategy. The correct option is the unique functional form that results from correctly applying the Efficiency and UTU axioms to the GPR. Distractors are derived from plausible algebraic errors in the derivation, such as mishandling the summation over `|N|` entities (B), incorrectly solving the efficiency constraint (C), or omitting terms from the proportional component (D).", "qid": "14", "question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`. For GPRs, this is equivalent to `A_1(\\bar{c}, E) = ... = A_{|N|}(\\bar{c}, E)`. Let this common value be `A(\\bar{c}, E)`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n\n### Question\n\nA Generalized Proportional Rule (GPR) is refined by imposing two axioms: Efficiency (`∑_{i∈N} f_i(c, E) = E`) and Uniform Treatment of Uniforms (UTU), which implies the non-proportional component `A_i` is the same for all entities (`A_i(\\bar{c}, E) = A(\\bar{c}, E)`). Which of the following formulas correctly represents the resulting division rule `f_i(c, E)`?", "model_evaluation_result": {"model_raw_response": "I need to analyze how the Efficiency and UTU axioms constrain the Generalized Proportional Rule.\n\nStarting with the GPR:\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E\n```\n\nFrom UTU, we have:\n- All entities get the same non-proportional component: $A_i(\\bar{c}, E) = A(\\bar{c}, E)$ for all $i$\n\nFrom Efficiency, we need:\n```latex\n\\sum_{i \\in N} f_i(c, E) = E\n```\n\nSubstituting the GPR with UTU constraint:\n```latex\n\\sum_{i \\in N} [A(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E] = E\n```\n\nThis gives us:\n```latex\n|N| \\cdot A(\\bar{c}, E) + \\sum_{i \\in N} \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E = E\n```\n\nSince $\\sum_{i \\in N} c_{ik} = \\bar{c}_k$ (by definition of $\\bar{c}_k$), we have:\n```latex\n\\sum_{i \\in N} \\frac{c_{ik}}{\\bar{c}_k} = \\frac{\\sum_{i \\in N} c_{ik}}{\\bar{c}_k} = \\frac{\\bar{c}_k}{\\bar{c}_k} = 1\n```\n\nTherefore:\n```latex\n|N| \\cdot A(\\bar{c}, E) + \\sum_{k \\in K} W_k(\\bar{c}, E) E = E\n```\n\nSolving for $A(\\bar{c}, E)$:\n```latex\nA(\\bar{c}, E) = \\frac{E - \\sum", "model_answer": "A", "error": null}}
{"ID": 82, "Question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nBased on the results in Table 1, which statement most accurately summarizes the causal effect of an electoral loss on a novice candidate's propensity to run for office again?", "Options": {"A": "An electoral loss reduces the probability of running again for both men and women, but the deterrence effect is over 50% larger for women than for men.", "B": "An electoral loss reduces a female candidate's probability of running again by approximately 10.1 percentage points.", "C": "An electoral loss significantly reduces a male candidate's probability of running again, but it has no statistically significant effect on a female candidate's probability of running again.", "D": "Female candidates are, on average, 10.1 percentage points less likely to run for office again compared to male candidates, regardless of the election outcome."}, "Answer": "A", "pi_justification": "Value: Assesses the student's ability to interpret the main finding from an interaction model, requiring them to synthesize multiple coefficients. Strategy: Computational Judgment. The student must understand that the effect for men is `β` (-16.0pp), the effect for women is `β+γ` (-16.0 + -10.1 = -26.1pp), and then compare them. Distractor Logic: (B) is a Step-Omission Error, reporting only the interaction term `γ`. (C) is a Conceptual Error, misinterpreting the meaning of the interaction term. (D) is a Conceptual Error, confusing an interaction effect with a main effect.", "qid": "82", "question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates in California local elections. The analysis uses a close-election Regression Discontinuity (RD) design.\n\n### Data / Model Specification\n\nThe outcome variable is an indicator for whether a candidate runs again within four years. The analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nWhere:\n*   `Lost_it`: 1 if the candidate lost, 0 if they won.\n*   `Female_i`: 1 if the candidate is female, 0 if male.\n*   `β`: The effect of losing for male candidates.\n*   `γ`: The *additional* effect of losing for female candidates.\n*   `δ`: The baseline difference in persistence between female and male *winners*.\n\nResults from the primary specification are presented in Table 1.\n\n**Table 1. Effect of Electoral Loss on Running Again (Interaction Model)**\n\n| Variable         | Coefficient | Standard Error |\n|------------------|-------------|----------------|\n| `Female x Lost`  | -0.101**    | (0.041)        |\n| `Lost`           | -0.160***   | (0.027)        |\n| `Female`         | 0.007       | (0.036)        |\n\n*Source: Adapted from Tables 2 & 3 of the source paper. Results from local linear regression with optimal bandwidth. *** p<0.01, ** p<0.05. The `Lost` coefficient is from the male-only regression for clarity.*\n\n### Question\n\nBased on the results in Table 1, which statement most accurately summarizes the causal effect of an electoral loss on a novice candidate's propensity to run for office again?", "model_evaluation_result": {"model_raw_response": "Looking at the regression model and results:\n\nThe model is: Y = α + β(Lost) + γ(Female × Lost) + δ(Female) + ... + ε\n\nWhere:\n- β = -0.160 (effect of losing for males)\n- γ = -0.101 (additional effect of losing for females)\n- δ = 0.007 (baseline difference between female and male winners)\n\nThis means:\n- For males who lose: effect = β = -0.160\n- For females who lose: effect = β + γ = -0.160 + (-0.101) = -0.261\n\nThe deterrence effect for women (-0.261) compared to men (-0.160):\n-0.261 / -0.160 = 1.63, meaning the effect is about 63% larger for women.\n\nSince 63% > 50%, option A correctly states that the deterrence effect is over 50% larger for women than for men.\n\nLet me verify the other options:\n- B is incorrect: -0.101 is just the additional effect, not the total effect for women\n- C is incorrect: Both effects are statistically significant\n- D is incorrect: 0.007 is the baseline difference for winners, not the interaction effect\n\nA", "model_answer": "A", "error": null}}
{"ID": 27, "Question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe contestant's preferences are represented by the CARA utility function:\n\n```latex\nU(W) = -e^{-\\alpha W}\n\\quad \\text{(Eq. 1)}\n```\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round, where `0.5 * stake \\le b \\le stake`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings (Distribution A) to the simulated distribution from a risk-neutral strategy (Distribution B).\n\n**Table 1. Data for Method 2 (Distributional Comparison)**\n\n| Distribution | Mean Winnings (\\$) | Std. Dev. of Winnings (\\$) |\n| :--- | :--- | :--- |\n| A: Actual Sample | `\\mu_A = 4,677` | `\\sigma_A = 4,258` |\n| B: Risk-Neutral Sim. | `\\mu_B = 6,987` | `\\sigma_B = 10,843` |\n\n### Question\n\nUsing the certainty equivalent approximation `CE ≈ μ - (α/2)σ²` and the data in Table 1, calculate the coefficient of absolute risk aversion (`α`) that would make a contestant indifferent between the actual sample distribution of winnings (Distribution A) and the simulated risk-neutral distribution (Distribution B).", "Options": {"A": "0.000047", "B": "0.000023", "C": "0.000310", "D": "0.701"}, "Answer": "A", "pi_justification": "This item assesses the ability to apply a specific formula for risk aversion using data from a table. It uses a Computational Judgment strategy. The distractors are designed to catch common errors: B is a Step-Omission Error (omitting the '2' in the numerator of the rearranged formula); C is a Conceptual Error (confusing the result of Method 2 with the reported result of Method 1); D is a Formula Misuse error (using standard deviation instead of variance in the denominator).", "qid": "27", "question": "### Background\n\n**Research Question.** This problem examines how the coefficient of absolute risk aversion (`\\alpha`) is estimated from contestant behavior. The analysis assumes contestants have a Constant Absolute Risk Aversion (CARA) utility function.\n\n**Setting.** A contestant with initial wealth `W` faces a gamble with even money payouts. They choose a bet `b`. With probability `p` they win, for final wealth `W+b`; with probability `q=1-p` they lose, for final wealth `W-b`.\n\n### Data / Model Specification\n\nThe contestant's preferences are represented by the CARA utility function:\n\n```latex\nU(W) = -e^{-\\alpha W}\n\\quad \\text{(Eq. 1)}\n```\n\nThe paper uses two methods to estimate a lower bound on `\\alpha`:\n1.  **Method 1 (Final-Round Bets):** Based on bets in the final round, where `0.5 * stake \\le b \\le stake`.\n2.  **Method 2 (Distributional Comparison):** Compares the actual distribution of winnings (Distribution A) to the simulated distribution from a risk-neutral strategy (Distribution B).\n\n**Table 1. Data for Method 2 (Distributional Comparison)**\n\n| Distribution | Mean Winnings (\\$) | Std. Dev. of Winnings (\\$) |\n| :--- | :--- | :--- |\n| A: Actual Sample | `\\mu_A = 4,677` | `\\sigma_A = 4,258` |\n| B: Risk-Neutral Sim. | `\\mu_B = 6,987` | `\\sigma_B = 10,843` |\n\n### Question\n\nUsing the certainty equivalent approximation `CE ≈ μ - (α/2)σ²` and the data in Table 1, calculate the coefficient of absolute risk aversion (`α`) that would make a contestant indifferent between the actual sample distribution of winnings (Distribution A) and the simulated risk-neutral distribution (Distribution B).", "model_evaluation_result": {"model_raw_response": "I need to find the coefficient of absolute risk aversion (α) that makes a contestant indifferent between Distribution A and Distribution B using the certainty equivalent approximation.\n\nGiven information:\n- Distribution A (Actual): μ_A = 4,677, σ_A = 4,258\n- Distribution B (Risk-Neutral): μ_B = 6,987, σ_B = 10,843\n- Certainty equivalent approximation: CE ≈ μ - (α/2)σ²\n\nFor indifference between the two distributions, their certainty equivalents must be equal:\nCE_A = CE_B\n\nSetting up the equation:\nμ_A - (α/2)σ_A² = μ_B - (α/2)σ_B²\n\nSubstituting the values:\n4,677 - (α/2)(4,258)² = 6,987 - (α/2)(10,843)²\n\nCalculating the squared terms:\n- σ_A² = (4,258)² = 18,130,564\n- σ_B² = (10,843)² = 117,570,649\n\nSubstituting:\n4,677 - (α/2)(18,130,564) = 6,987 - (α/2)(117,570,649)\n\nRearranging to solve for α:\n4,677 - 6,987 = (α/2)(18,130,564) - (α/2)(117,570,649)\n-2,310 = (α/2)(18,130,564 - 117,570,649)\n-2,310 = (α/2)(-99,440,085)\n-2,310 = -α(49,720,042.5)\n\nSolving for α:\nα = 2,310 / 49,720,042.5\nα = 0.000046459...\n\nRounding to the closest option:\nα ≈ 0.000047\n\nA", "model_answer": "A", "error": null}}
{"ID": 33, "Question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nBased on the model and results in Table 1, which statement best describes the moderating effect of R&D intensity on the relationship between corruption and a firm's decision to invest?", "Options": {"A": "R&D-intensive firms are less deterred by corruption, likely due to stronger bargaining power.", "B": "R&D-intensive firms are more deterred by corruption because their complex operations are more exposed to bureaucratic hurdles.", "C": "Corruption uniformly deters all firms, and R&D intensity has a separate, independent negative effect on investment.", "D": "The positive interaction effect shows that R&D-intensive firms are more adept at using bribes to their advantage."}, "Answer": "A", "pi_justification": "This item assesses the conceptual understanding of an interaction effect, a key nuance in the paper's findings. It uses a Reverse-Reasoning strategy, asking for the best explanation of an observed statistical pattern. Distractors are designed to be plausible but incorrect: (B) is a plausible alternative mechanism (Conceptual Opposite), (C) ignores the interaction term entirely (Step-Omission Error), and (D) misinterprets the 'bargaining power' mechanism as active participation in corruption rather than resistance to it.", "qid": "33", "question": "### Background\n\nThe paper establishes that, on average, corruption in a host country deters foreign direct investment (FDI) from Swedish multinational enterprises (MNEs). This problem explores whether this effect is uniform across all firms or if it is moderated by firm-specific characteristics, such as R&D intensity. The hypothesis is that firms with greater bargaining power may be less affected by corruption.\n\n### Data / Model Specification\n\nThe probability of firm $i$ investing in country $j$ ($DFDI_{ij}=1$) is estimated using a probit model that includes an interaction term between host-country corruption and the firm's R&D intensity.\n\n```latex\nPr(DFDI_{ij}=1) = \\Phi(\\alpha_{0} + \\beta_{1}Corruption_{j} + \\beta_{2}R\\&D_{i} + \\beta_{3}(Corruption_{j} \\times R\\&D_{i}) + ...)\n\\quad \\quad \\text{(Eq. (1))}\n```\n\nwhere $R\\&D_i$ is the firm's R&D expenditure as a share of its total sales, and $\\Phi$ is the standard normal CDF. The coefficients reported below are from the selection stage of a Heckman model.\n\n**Table 1: Selected Probit Coefficients for the Investment Decision**\n\n| Variable                  | Coefficient Estimate |\n| ------------------------- | -------------------- |\n| `Corruption`              | -1.015***            |\n| `R&D`                     | -5.771***            |\n| `Corruption` x `R&D`      | 8.230***             |\n\n_Note: *** p<0.01. Standard errors not shown._\n\n### Question\n\nBased on the model and results in Table 1, which statement best describes the moderating effect of R&D intensity on the relationship between corruption and a firm's decision to invest?", "model_evaluation_result": {"model_raw_response": "Looking at the probit model results, I need to analyze how the interaction term affects the relationship between corruption and investment decisions.\n\nThe key coefficients are:\n- Corruption: -1.015*** (negative main effect)\n- R&D: -5.771*** (negative main effect)  \n- Corruption × R&D: 8.230*** (positive interaction effect)\n\nTo understand the moderating effect, I need to consider how R&D intensity changes the marginal effect of corruption on investment probability.\n\nThe marginal effect of corruption on investment probability is:\n∂Pr(DFDI=1)/∂Corruption = β₁ + β₃ × R&D = -1.015 + 8.230 × R&D\n\nFor firms with low R&D intensity (R&D ≈ 0):\nMarginal effect ≈ -1.015 (strongly negative - corruption strongly deters investment)\n\nFor firms with higher R&D intensity:\nAs R&D increases, the marginal effect becomes less negative due to the positive interaction term (8.230 × R&D). At sufficiently high R&D levels, the marginal effect could even become positive.\n\nThis means that R&D-intensive firms are less deterred by corruption than firms with low R&D intensity. The positive interaction coefficient (8.230) indicates that higher R&D intensity moderates (reduces) the negative effect of corruption on investment decisions.\n\nThis pattern is consistent with the hypothesis that R&D-intensive firms have greater bargaining power, making them less sensitive to corruption in host countries.\n\nA", "model_answer": "D", "error": null}}
{"ID": 136, "Question": "## Background\n\n**Research Question.** This problem explores the fundamental definition of a two-sided market, focusing on the distinction between the price level and price structure.\n\n**Setting.** A platform charges per-interaction fees `a^B` and `a^S`. The total price level is `a = a^B + a^S`. The market is **one-sided** if transaction volume `V` depends only on `a`, and **two-sided** if `V` also depends on the price structure (the specific values of `a^B` and `a^S`).\n\n---\n\n## Data / Model Specification\n\nConsider a market where end-users (buyer and seller) bargain over the transaction price under asymmetric information. This bargaining is inefficient (i.e., the Coase theorem fails), leading to a suboptimal volume of trade. The platform decides to change its price structure by increasing the seller's fee to `a^S + Δa` and decreasing the buyer's fee to `a^B - Δa`, keeping the total fee `a` constant.\n\n---\n\n## Question\n\nWhat is the immediate impact of this price structure change on the market?", "Options": {"A": "The market remains one-sided because sellers and buyers adjust their price offers and acceptance thresholds by `Δa`, neutralizing the change and leaving transaction volume unaffected.", "B": "The market becomes two-sided because the bargaining inefficiency means that any change in price structure will directly impact the volume of transactions.", "C": "The volume of transactions increases significantly because the lower fee for buyers (`a^B - Δa`) stimulates demand more than the higher seller fee suppresses supply.", "D": "The market remains one-sided because the platform's rules force sellers to absorb the entire fee increase `Δa` without changing their price offers to buyers."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the key insight that Coase theorem failure is necessary but not sufficient for two-sidedness. It uses a Scenario Application strategy. The correct answer identifies the isomorphic nature of the bargaining game, which is the specific mechanism for neutrality. Distractor B represents the classic misconception of equating any inefficiency with two-sidedness. Distractor C makes an unsubstantiated claim about relative elasticities. Distractor D misidentifies the mechanism of neutrality; it's not absorption but perfect pass-through.", "qid": "136", "question": "## Background\n\n**Research Question.** This problem explores the fundamental definition of a two-sided market, focusing on the distinction between the price level and price structure.\n\n**Setting.** A platform charges per-interaction fees `a^B` and `a^S`. The total price level is `a = a^B + a^S`. The market is **one-sided** if transaction volume `V` depends only on `a`, and **two-sided** if `V` also depends on the price structure (the specific values of `a^B` and `a^S`).\n\n---\n\n## Data / Model Specification\n\nConsider a market where end-users (buyer and seller) bargain over the transaction price under asymmetric information. This bargaining is inefficient (i.e., the Coase theorem fails), leading to a suboptimal volume of trade. The platform decides to change its price structure by increasing the seller's fee to `a^S + Δa` and decreasing the buyer's fee to `a^B - Δa`, keeping the total fee `a` constant.\n\n---\n\n## Question\n\nWhat is the immediate impact of this price structure change on the market?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 16, "Question": "### Background\n\nAn analysis of Indian manufacturing firms produced estimates for an extended Cobb-Douglas production function. The goal is to understand the productivity effects of a firm's own R&D (`LNRND`) and industry-wide R&D spillovers (`LNSRDEXP`).\n\n### Data / Model Specification\n\nThe core log-linear production function is:\n```latex\n\\hat{y}_{i j t}=\\hat{a}_{i j t}+\\alpha\\hat{s}_{j t}+\\beta\\hat{r}_{i j t}+\\gamma\\hat{k}_{i j t}+\\delta\\hat{l}_{i j t}+\\hat{e}_{i j t}\n```\nwhere `y` is value-added, `s` is spillover R&D, `r` is own R&D, `k` is physical capital, and `l` is labor, all in logs.\n\nTwo models were estimated for the \"Overall Industry\": a standard OLS model (Table 1) and a model correcting for first-order serial correlation using a Cochrane-Orcutt transformation (Table 2).\n\n**Table 1: OLS Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.016 (3.80) |\n| LNSRDEXP ($\\alpha$) | 0.093 (8.01) |\n| DW | 0.794 |\n*t-statistics in parentheses.*\n\n**Table 2: Cochrane-Orcutt Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.009 (1.49) |\n| LNSRDEXP ($\\alpha$) | 0.055 (2.58) |\n| DW | 2.170 |\n*t-statistics in parentheses.*\n\n### Question\n\nComparing the results for own R&D (`LNRND`) in Table 1 and Table 2, what is the most accurate econometric explanation for the change in the coefficient's statistical significance?", "Options": {"A": "The OLS model in Table 1 was correctly specified, and the Cochrane-Orcutt procedure in Table 2 incorrectly reduced the coefficient's significance.", "B": "The OLS model in Table 1 suffered from negative serial correlation (DW < 2), which biased the t-statistic for `LNRND` downward. The correction fixed this.", "C": "The OLS model in Table 1 suffered from positive serial correlation, which biased the coefficient estimate for `LNRND` upward. The correction in Table 2 removed this bias, revealing a smaller, insignificant effect.", "D": "The change in significance is due to correcting for unobserved firm-specific heterogeneity, which was confounding the OLS results."}, "Answer": "C", "pi_justification": "This item replaces part of a larger QA problem. It uses a Reverse-Reasoning strategy to assess a core concept: diagnosing and understanding the consequences of serial correlation. The correct answer (C) requires interpreting the Durbin-Watson statistic (0.794 indicates positive serial correlation) and knowing that this biases coefficients and standard errors, leading to spurious significance. Distractor A is a conceptual opposite. Distractor B correctly identifies the DW value but incorrectly states the direction of bias. Distractor D correctly identifies a potential problem (heterogeneity) but attributes the change to the wrong correction, testing if the student can distinguish between serial correlation and heterogeneity corrections.", "qid": "16", "question": "### Background\n\nAn analysis of Indian manufacturing firms produced estimates for an extended Cobb-Douglas production function. The goal is to understand the productivity effects of a firm's own R&D (`LNRND`) and industry-wide R&D spillovers (`LNSRDEXP`).\n\n### Data / Model Specification\n\nThe core log-linear production function is:\n```latex\n\\hat{y}_{i j t}=\\hat{a}_{i j t}+\\alpha\\hat{s}_{j t}+\\beta\\hat{r}_{i j t}+\\gamma\\hat{k}_{i j t}+\\delta\\hat{l}_{i j t}+\\hat{e}_{i j t}\n```\nwhere `y` is value-added, `s` is spillover R&D, `r` is own R&D, `k` is physical capital, and `l` is labor, all in logs.\n\nTwo models were estimated for the \"Overall Industry\": a standard OLS model (Table 1) and a model correcting for first-order serial correlation using a Cochrane-Orcutt transformation (Table 2).\n\n**Table 1: OLS Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.016 (3.80) |\n| LNSRDEXP ($\\alpha$) | 0.093 (8.01) |\n| DW | 0.794 |\n*t-statistics in parentheses.*\n\n**Table 2: Cochrane-Orcutt Estimates**\n| Variables | Overall industry |\n|:---|---:|\n| LNRND ($\\beta$) | 0.009 (1.49) |\n| LNSRDEXP ($\\alpha$) | 0.055 (2.58) |\n| DW | 2.170 |\n*t-statistics in parentheses.*\n\n### Question\n\nComparing the results for own R&D (`LNRND`) in Table 1 and Table 2, what is the most accurate econometric explanation for the change in the coefficient's statistical significance?", "model_evaluation_result": {"model_raw_response": "Looking at the key information:\n\n**Table 1 (OLS):**\n- LNRND coefficient: 0.016 with t-statistic of 3.80 (significant)\n- DW = 0.794 (well below 2, indicating positive serial correlation)\n\n**Table 2 (Cochrane-Orcutt):**\n- LNRND coefficient: 0.009 with t-statistic of 1.49 (not significant)\n- DW = 2.170 (close to 2, indicating serial correlation has been corrected)\n\nThe Durbin-Watson statistic of 0.794 in Table 1 clearly indicates positive serial correlation (DW << 2). When positive serial correlation is present in OLS:\n\n1. The coefficient estimates themselves remain unbiased\n2. However, the standard errors are biased downward\n3. This leads to inflated t-statistics and false significance\n\nThe Cochrane-Orcutt procedure corrects for this serial correlation. After correction:\n- The coefficient estimate changes from 0.016 to 0.009 (but this is due to the transformation, not bias correction)\n- More importantly, the t-statistic drops dramatically from 3.80 to 1.49\n- The DW statistic improves to 2.170, confirming the serial correlation has been addressed\n\nThe loss of statistical significance is primarily due to the correction of biased (too small) standard errors in the OLS model, which had made the coefficient appear more significant than it actually was.\n\nD", "model_answer": "D", "error": null}}
{"ID": 107, "Question": "### Background\n\n**Research Question.** This question assesses understanding of a key methodological assumption—intertemporal independence—and the consequences of its violation.\n\n**Setting / Institutional Environment.** The paper's main parametric analysis relies on a time-separable utility model, which assumes that choices between two periods (e.g., today vs. 4 weeks) are not affected by consumption in a third period (e.g., 8 weeks). The authors test this assumption by comparing choices in a standard two-period task from Part I with choices in a specially designed three-period task from Part II where consumption in the third period is non-zero. The empirical test supported the assumption of intertemporal independence.\n\n### The Question\n\nConsider a hypothetical scenario where the test for intertemporal independence had failed. Specifically, assume that for the Present Group (allocating between today and 4 weeks), consumption is intertemporally *complementary*, meaning the marginal utility of consumption at 4 weeks *increases* with consumption at 8 weeks. \n\nIn this scenario, how would the allocation to the 4-week option in the Part II task (where 8-week consumption was positive) have compared to the allocation in the equivalent Part I task (where 8-week consumption was zero)?", "Options": {"A": "The allocation to the 4-week option would be systematically higher in Part II.", "B": "The allocation to the 4-week option would be systematically lower in Part II.", "C": "The allocation would be unaffected because the third-period reward is exogenous.", "D": "The allocation would be higher for some interest rates and lower for others, with no systematic bias."}, "Answer": "A", "pi_justification": "Assessment Target: Tests deep reasoning about the implications of non-separable preferences. This item replaces the most difficult part of a complex QA problem. Strategy: Reverse-Reasoning. Distractor Logic: (A) Correct: complementarity with positive 8-week consumption raises the marginal utility of 4-week consumption, shifting allocation towards it. (B) Conceptual Opposite: this would be the result of intertemporal substitutability, not complementarity. (C) Formula Misuse: incorrectly applies the logic of separability to a non-separable case. (D) True but Irrelevant: a common hedge that avoids the core logic; the direction of the bias is systematic, not ambiguous.", "qid": "107", "question": "### Background\n\n**Research Question.** This question assesses understanding of a key methodological assumption—intertemporal independence—and the consequences of its violation.\n\n**Setting / Institutional Environment.** The paper's main parametric analysis relies on a time-separable utility model, which assumes that choices between two periods (e.g., today vs. 4 weeks) are not affected by consumption in a third period (e.g., 8 weeks). The authors test this assumption by comparing choices in a standard two-period task from Part I with choices in a specially designed three-period task from Part II where consumption in the third period is non-zero. The empirical test supported the assumption of intertemporal independence.\n\n### The Question\n\nConsider a hypothetical scenario where the test for intertemporal independence had failed. Specifically, assume that for the Present Group (allocating between today and 4 weeks), consumption is intertemporally *complementary*, meaning the marginal utility of consumption at 4 weeks *increases* with consumption at 8 weeks. \n\nIn this scenario, how would the allocation to the 4-week option in the Part II task (where 8-week consumption was positive) have compared to the allocation in the equivalent Part I task (where 8-week consumption was zero)?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand the economic implications of intertemporal complementarity.\n\nGiven scenario:\n- Consumption is intertemporally complementary\n- Marginal utility of consumption at 4 weeks increases with consumption at 8 weeks\n- Part I: 8-week consumption = 0\n- Part II: 8-week consumption > 0\n\nWith intertemporal complementarity, when 8-week consumption is positive (Part II), the marginal utility of consuming at 4 weeks becomes higher compared to when 8-week consumption is zero (Part I).\n\nSince the marginal utility of 4-week consumption is higher in Part II, individuals would find it more attractive to allocate money to the 4-week option in Part II compared to Part I.\n\nThis means people would systematically choose to allocate more to the 4-week option when there's positive consumption at 8 weeks (Part II) than when there's no consumption at 8 weeks (Part I).\n\nA", "model_answer": "A", "error": null}}
{"ID": 80, "Question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified, the Falsification Adaptive Set (FAS) provides a range of plausible parameter estimates. For any potential value `b` within the FAS, the minimal required violation of the exclusion restriction for instrument `l` (its direct effect `|γ_l|`) is given by the vector `δ(b)` on the Falsification Frontier.\n\n### Data / Model Specification\n\nThe required violation for instrument `l` to be consistent with a true causal effect `β_true = b` is:\n\n```latex\n|\\gamma_l| = |\\psi_l - b\\pi_l| = |\\pi_l| \\cdot |b_l - b| \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l` is the 2SLS estimate using instrument `l` and `π_l` is its first-stage coefficient. The first-stage F-statistic is proportional to `π_l^2`, so the relative strength of two instruments can be compared using the square root of the ratio of their F-statistics: `|π_1|/|π_2| ≈ sqrt(F_1/F_2)`.\n\nAn empirical study provides the following results for two strong instruments, 'Plan' and 'Exploration':\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (6) Exploration |\n| :--- | :---: | :---: |\n| log highway km (`b_l`) | 0.18 | 0.42 |\n| First-stage F stat. | 54.8 | 27.0 |\n\n### Question\n\nSuppose the true causal effect is `β_true = 0.25`. To make the model consistent with this true value, which instrument requires a larger violation of its exclusion restriction (`|γ_l|`)?", "Options": {"A": "'Plan', because its point estimate (0.18) is closer to the true value (0.25).", "B": "'Plan', because it has a stronger first stage (higher F-statistic).", "C": "'Exploration', because the required violation `|π_l| ⋅ |b_l - 0.25|` is larger for it after accounting for first-stage strengths.", "D": "'Exploration', because its point estimate (0.42) is further from the true value (0.25)."}, "Answer": "C", "pi_justification": "This item tests deep comprehension by requiring a multi-step calculation that synthesizes point estimates, a theoretical formula, and test statistics. It uses a Computational Judgment strategy. The correct answer requires calculating the weighted distance `|π_l| ⋅ |b_l - b|` for both instruments. Distractor D represents a Step-Omission Error, as it correctly identifies the instrument with the larger unweighted distance `|b_l - b|` but fails to account for the first-stage strength `|π_l|`. Distractor B is True but Irrelevant, misapplying the meaning of the F-statistic in this context. Distractor A uses the same flawed logic as D but for the wrong instrument.", "qid": "80", "question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified, the Falsification Adaptive Set (FAS) provides a range of plausible parameter estimates. For any potential value `b` within the FAS, the minimal required violation of the exclusion restriction for instrument `l` (its direct effect `|γ_l|`) is given by the vector `δ(b)` on the Falsification Frontier.\n\n### Data / Model Specification\n\nThe required violation for instrument `l` to be consistent with a true causal effect `β_true = b` is:\n\n```latex\n|\\gamma_l| = |\\psi_l - b\\pi_l| = |\\pi_l| \\cdot |b_l - b| \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l` is the 2SLS estimate using instrument `l` and `π_l` is its first-stage coefficient. The first-stage F-statistic is proportional to `π_l^2`, so the relative strength of two instruments can be compared using the square root of the ratio of their F-statistics: `|π_1|/|π_2| ≈ sqrt(F_1/F_2)`.\n\nAn empirical study provides the following results for two strong instruments, 'Plan' and 'Exploration':\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (6) Exploration |\n| :--- | :---: | :---: |\n| log highway km (`b_l`) | 0.18 | 0.42 |\n| First-stage F stat. | 54.8 | 27.0 |\n\n### Question\n\nSuppose the true causal effect is `β_true = 0.25`. To make the model consistent with this true value, which instrument requires a larger violation of its exclusion restriction (`|γ_l|`)?", "model_evaluation_result": {"model_raw_response": "I need to calculate the required violation |γ_l| for each instrument when β_true = 0.25 using Equation 1.\n\nFrom Equation 1: |γ_l| = |π_l| · |b_l - b|\n\nGiven data:\n- Plan: b_l = 0.18, F-stat = 54.8\n- Exploration: b_l = 0.42, F-stat = 27.0\n- β_true = 0.25\n\nFirst, I need to find the relative strengths |π_1|/|π_2| using the F-statistics:\n|π_Plan|/|π_Exploration| = √(F_Plan/F_Exploration) = √(54.8/27.0) = √2.03 ≈ 1.42\n\nThis means |π_Plan| ≈ 1.42 × |π_Exploration|\n\nNow I'll calculate the required violations:\n\nFor Plan:\n|γ_Plan| = |π_Plan| · |0.18 - 0.25| = |π_Plan| · 0.07\n\nFor Exploration:\n|γ_Exploration| = |π_Exploration| · |0.42 - 0.25| = |π_Exploration| · 0.17\n\nTo compare these, I can express both in terms of |π_Exploration|:\n|γ_Plan| = 1.42 × |π_Exploration| × 0.07 = 0.099 × |π_Exploration|\n|γ_Exploration| = |π_Exploration| × 0.17 = 0.17 × |π_Exploration|\n\nSince 0.17 > 0.099, Exploration requires a larger violation of its exclusion restriction.\n\nThis makes intuitive sense: even though Plan has a stronger first stage, Exploration's point estimate (0.42) is much further from the true value (0.25) than Plan's estimate (0.18), and this distance effect dominates.\n\nC", "model_answer": "C", "error": null}}
{"ID": 84, "Question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates. A central hypothesis is that the gender gap in persistence is concentrated in electoral environments with lower female representation.\n\n### Data / Model Specification\n\nThe analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nThe paper estimates this model on different subsamples of political offices. Table 1 shows the baseline effect of losing for men. Table 2 shows the *additional* effect for women (`γ`) in two different office types.\n\n**Table 1. Baseline Effect of Loss for Male Candidates**\n\n| Variable | Coefficient | Standard Error |\n|----------|-------------|----------------|\n| `Lost`   | -0.160***   | (0.027)        |\n\n*Source: Adapted from Table 2 of the source paper.*\n\n**Table 2. Heterogeneity by Office Type (Differential Effect for Women)**\n\n| Sample / Office Type         | `Female x Lost` Coefficient (γ) | Standard Error |\n|------------------------------|---------------------------------|----------------|\n| City Council (Low Rep.)      | -0.165**                        | (0.065)        |\n| School Board (High Rep.)     | -0.057                          | (0.051)        |\n\n*Source: Adapted from Table 4 of the source paper. ** p<0.05.*\n\n### Question\n\nBased on these results, in which electoral environment is the gender gap in persistence most pronounced, and what is the approximate total deterrence effect of a loss for a woman in that environment?", "Options": {"A": "Most pronounced in City Council races; the total effect for a woman is a reduction of about 16.5 percentage points.", "B": "Most pronounced in School Board races; the total effect for a woman is a reduction of about 21.7 percentage points.", "C": "Most pronounced in School Board races; the total effect for a woman is a reduction of about 5.7 percentage points.", "D": "Most pronounced in City Council races; the total effect for a woman is a reduction of about 32.5 percentage points."}, "Answer": "D", "pi_justification": "Value: Assesses the ability to test a hypothesis using subgroup analysis and to correctly calculate the total effect for a specific group in an interaction model. Strategy: Scenario Application. The student must apply the general model to two specific scenarios (office types). Distractor Logic: (A) is a Step-Omission Error, reporting only the interaction term `γ` for City Council races. (B) correctly calculates the effect for School Board races but incorrectly identifies it as the more pronounced environment. (C) is a Step-Omission Error for School Board races, reporting only the interaction term `γ`.", "qid": "84", "question": "### Background\n\nThis paper investigates whether an initial electoral loss has a differential causal effect on the subsequent political participation of male and female novice candidates. A central hypothesis is that the gender gap in persistence is concentrated in electoral environments with lower female representation.\n\n### Data / Model Specification\n\nThe analysis estimates the effect of losing using a model with an interaction term for female candidates:\n\n```latex\nY_{i,t+4} = \\alpha + \\beta \\mathrm{Lost}_{it} + \\gamma(\\mathrm{Female}_{i} \\times \\mathrm{Lost}_{it}) + \\delta \\mathrm{Female}_{i} + ... + \\varepsilon_{it}\n```\n\nThe paper estimates this model on different subsamples of political offices. Table 1 shows the baseline effect of losing for men. Table 2 shows the *additional* effect for women (`γ`) in two different office types.\n\n**Table 1. Baseline Effect of Loss for Male Candidates**\n\n| Variable | Coefficient | Standard Error |\n|----------|-------------|----------------|\n| `Lost`   | -0.160***   | (0.027)        |\n\n*Source: Adapted from Table 2 of the source paper.*\n\n**Table 2. Heterogeneity by Office Type (Differential Effect for Women)**\n\n| Sample / Office Type         | `Female x Lost` Coefficient (γ) | Standard Error |\n|------------------------------|---------------------------------|----------------|\n| City Council (Low Rep.)      | -0.165**                        | (0.065)        |\n| School Board (High Rep.)     | -0.057                          | (0.051)        |\n\n*Source: Adapted from Table 4 of the source paper. ** p<0.05.*\n\n### Question\n\nBased on these results, in which electoral environment is the gender gap in persistence most pronounced, and what is the approximate total deterrence effect of a loss for a woman in that environment?", "model_evaluation_result": {"model_raw_response": "Looking at this step by step:\n\n1. **Understanding the model**: The total effect of losing for women = β (baseline effect for men) + γ (additional effect for women)\n\n2. **From the tables**:\n   - Baseline effect for men (β) = -0.160\n   - Additional effect for women in City Council (γ) = -0.165**\n   - Additional effect for women in School Board (γ) = -0.057 (not significant)\n\n3. **Calculating total effects for women**:\n   - City Council: -0.160 + (-0.165) = -0.325 = 32.5 percentage points reduction\n   - School Board: -0.160 + (-0.057) = -0.217 = 21.7 percentage points reduction\n\n4. **Gender gap comparison**: \n   - City Council: The additional deterrent effect for women (-0.165) is statistically significant and larger in magnitude\n   - School Board: The additional deterrent effect for women (-0.057) is not statistically significant\n\n5. **Conclusion**: The gender gap is most pronounced in City Council races (low female representation environment) where women face an additional significant deterrent effect, resulting in a total deterrence effect of about 32.5 percentage points.\n\nD", "model_answer": "D", "error": null}}
{"ID": 5, "Question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant.\n\n### Question\n\nThe prior variance in Eq. (2) is inversely proportional to the instrument's noncentrality parameter, `\\pi_2^2 x'x / \\omega_{22}`. What is the primary justification for this choice, and what does it imply if a researcher observes that the instrument `x_t` is very weak (i.e., `\\pi_2` is close to zero)?", "Options": {"A": "Justification: Prior uncertainty about the exclusion restriction should decrease as the instrument becomes stronger. Implication: A weak instrument implies the prior on `\\lambda` should be very diffuse (high variance).", "B": "Justification: To ensure the posterior distribution is proper. Implication: A weak instrument implies the prior on `\\lambda` must be very concentrated (low variance).", "C": "Justification: Prior uncertainty about the exclusion restriction should increase as the instrument becomes stronger. Implication: A weak instrument implies the prior on `\\lambda` should be very concentrated (low variance).", "D": "Justification: Computational convenience in the Gibbs sampler. Implication: A weak instrument implies the posterior for `\\lambda` will be improper."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the deep logic behind the prior specification, linking instrument strength to prior uncertainty. Strategy: Scenario Application. The user must apply the logic to the scenario of a weak instrument. Distractor Logic: B) has an incorrect justification and a conceptually opposite implication. C) has a conceptually opposite justification and implication. D) offers a plausible but incorrect justification (computational convenience) and an incorrect implication.", "qid": "5", "question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant.\n\n### Question\n\nThe prior variance in Eq. (2) is inversely proportional to the instrument's noncentrality parameter, `\\pi_2^2 x'x / \\omega_{22}`. What is the primary justification for this choice, and what does it imply if a researcher observes that the instrument `x_t` is very weak (i.e., `\\pi_2` is close to zero)?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand the relationship between instrument strength and prior variance in the Bayesian identification approach.\n\nFrom Equation (2), the prior variance is:\n$$\\frac{c\\omega_{22}}{\\pi_2^2 x'x}$$\n\nThis variance is inversely proportional to the noncentrality parameter $\\pi_2^2 x'x / \\omega_{22}$.\n\nLet me analyze what happens with instrument strength:\n\n**Strong instrument** (large $|\\pi_2|$):\n- The noncentrality parameter $\\pi_2^2 x'x / \\omega_{22}$ is large\n- Prior variance $\\frac{c\\omega_{22}}{\\pi_2^2 x'x}$ is small (concentrated prior)\n- The data is informative about $\\lambda$, so we need less prior information\n\n**Weak instrument** (small $|\\pi_2|$):\n- The noncentrality parameter $\\pi_2^2 x'x / \\omega_{22}$ is small  \n- Prior variance $\\frac{c\\omega_{22}}{\\pi_2^2 x'x}$ is large (diffuse prior)\n- The data is less informative about $\\lambda$, so we rely more on the prior\n\nThe economic intuition is that when the instrument is strong, the data provides good identification, so we can use a more concentrated prior. When the instrument is weak, the data provides poor identification, so we need a more diffuse prior to reflect our greater uncertainty.\n\nLooking at the options:\n\nA) States that prior uncertainty should decrease as instrument becomes stronger (correct), and weak instrument implies diffuse prior (correct).\n\nB) Incorrect - weak instrument doesn't imply concentrated prior.\n\nC) Incorrect - prior uncertainty should decrease, not increase, as instrument becomes stronger.\n\nD) Incorrect - this isn't about computational convenience, and weak instrument doesn't make posterior improper.\n\nA", "model_answer": "A", "error": null}}
{"ID": 46, "Question": "## Background\n\n**Research Question.** This problem investigates the validity of the estimated long-term impact of social information on donor renewal.\n\n**Setting / Institutional Environment.** The analysis is a one-year follow-up of the *new donors* from an initial field experiment. The study tracks their donation behavior one year after the initial intervention. However, not all original participants could be tracked or responded in the follow-up year.\n\n---\n\n## Scenario\n\nSuppose the follow-up study suffers from non-random attrition. Specifically, assume that new donors who had a very positive experience with the radio station (an unobserved characteristic) are both more likely to be found and successfully contacted in the follow-up sample *and* more likely to renew their donation. Furthermore, assume that receiving the `$300` treatment itself enhances this positive experience, making these donors even easier to track and include in the follow-up analysis.\n\n---\n\n## Question\n\nGiven this scenario, how would the estimated treatment effect of the `$300` signal on the probability of renewal likely be biased?", "Options": {"A": "Biased upwards (overestimated).", "B": "Biased downwards (underestimated).", "C": "Unbiased, because the initial treatment assignment was random.", "D": "The direction of the bias cannot be determined from the information given."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to apply the concept of sample selection bias. It uses a 'Scenario Application' strategy. The correct answer requires recognizing that the treatment is correlated with selection into the sample, and selection is correlated with the outcome, leading to an upward bias. Distractors target common misconceptions: (B) is the 'Conceptual Opposite'; (C) is 'True but Irrelevant', as it correctly states a fact about the initial experiment but ignores that the bias arises from non-random *attrition* in the follow-up stage; (D) is incorrect because the premises provided are sufficient to sign the bias.", "qid": "46", "question": "## Background\n\n**Research Question.** This problem investigates the validity of the estimated long-term impact of social information on donor renewal.\n\n**Setting / Institutional Environment.** The analysis is a one-year follow-up of the *new donors* from an initial field experiment. The study tracks their donation behavior one year after the initial intervention. However, not all original participants could be tracked or responded in the follow-up year.\n\n---\n\n## Scenario\n\nSuppose the follow-up study suffers from non-random attrition. Specifically, assume that new donors who had a very positive experience with the radio station (an unobserved characteristic) are both more likely to be found and successfully contacted in the follow-up sample *and* more likely to renew their donation. Furthermore, assume that receiving the `$300` treatment itself enhances this positive experience, making these donors even easier to track and include in the follow-up analysis.\n\n---\n\n## Question\n\nGiven this scenario, how would the estimated treatment effect of the `$300` signal on the probability of renewal likely be biased?", "model_evaluation_result": {"model_raw_response": "Looking at this scenario, I need to analyze how non-random attrition affects the estimated treatment effect.\n\nKey facts:\n- Donors with very positive experiences are more likely to be tracked AND more likely to renew\n- The $300 treatment enhances positive experiences, making treated donors easier to track\n- This creates differential attrition between treatment and control groups\n\nLet me trace through the bias mechanism:\n\n1. **Selection into follow-up sample**: Treated donors are more likely to be included in the follow-up because the treatment enhanced their positive experience, making them easier to track.\n\n2. **Correlation with outcome**: Those same positive experiences that make donors easier to track also make them more likely to renew donations.\n\n3. **Differential composition**: The follow-up sample will contain a higher proportion of \"renewal-prone\" donors in the treatment group compared to the control group.\n\n4. **Bias direction**: When comparing renewal rates between treatment and control in the follow-up sample, we're comparing:\n   - Treatment group: enriched with high-renewal-probability donors\n   - Control group: less enriched (closer to random sample)\n\nThis makes the treatment group appear more effective at promoting renewals than it actually is, because part of the observed difference comes from sample composition rather than the true causal effect.\n\nThe estimated treatment effect will be **biased upwards (overestimated)**.\n\nA", "model_answer": "A", "error": null}}
{"ID": 54, "Question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. If the rebels fight and the fighting is inconclusive (which happens with probability `1-d_k`), the government's consolidation effort to move to state `s_{k+1}` still succeeds with probability `ε`.\n\nThe government (G) has **coercive power** when `d_k < 1` and `ε > 0`.\n\n### Question\n\nSuppose the government makes a proposal `(z_k, s_{k+1})` that it knows the rebels will reject, triggering a fight. How should the government choose the `s_{k+1}` component of this rejected proposal to maximize its own expected payoff from the conflict?\n", "Options": {"A": "Choose `s_{k+1} = E` (the monopoly state) to signal its resolve and deter future opposition.", "B": "Choose `s_{k+1} = E` (the monopoly state) to maximize its own continuation value `V_G(s_{k+1})` in the event that fighting is inconclusive but its consolidation efforts succeed.", "C": "Choose the `s_{k+1}` that would have required the largest peaceful transfer `z_k`, to show the rebels the value of the deal they are rejecting.", "D": "Choose `s_{k+1} = s_k` (no change), because the proposal is strategically irrelevant once the rebels have decided to fight."}, "Answer": "B", "pi_justification": "This item tests a critical pivot in the model's logic: how the government's objective changes when moving from a peace negotiation to a violent conflict. It uses a Scenario Application strategy. The correct option (B) requires understanding that the government's choice of `s_{k+1}` now aims to maximize its own potential payoff, not influence the rebels' choice. Distractors are based on plausible but incorrect reasoning: (A) a behavioral explanation instead of the model's rationalist one, (C) misapplication of the peace-time formula, and (D) incorrectly assuming the proposal becomes completely irrelevant.", "qid": "54", "question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. If the rebels fight and the fighting is inconclusive (which happens with probability `1-d_k`), the government's consolidation effort to move to state `s_{k+1}` still succeeds with probability `ε`.\n\nThe government (G) has **coercive power** when `d_k < 1` and `ε > 0`.\n\n### Question\n\nSuppose the government makes a proposal `(z_k, s_{k+1})` that it knows the rebels will reject, triggering a fight. How should the government choose the `s_{k+1}` component of this rejected proposal to maximize its own expected payoff from the conflict?\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand what happens when the government makes a proposal it knows will be rejected.\n\nWhen rebels reject the proposal, fighting occurs. The key insight is in the problem setup: \"If the rebels fight and the fighting is inconclusive (which happens with probability `1-d_k`), the government's consolidation effort to move to state `s_{k+1}` still succeeds with probability `ε`.\"\n\nThis means that even during conflict, the government can still attempt to consolidate power and move to its proposed future state `s_{k+1}` with probability `ε` (when fighting is inconclusive).\n\nSince the government has coercive power (`d_k < 1` and `ε > 0`), it has a positive probability of successfully implementing its chosen `s_{k+1}` even through conflict.\n\nGiven this mechanism, the government should choose the `s_{k+1}` that maximizes its expected payoff in the event that consolidation succeeds. Since `E` represents the monopoly state (where the government has maximum power), choosing `s_{k+1} = E` would maximize the government's continuation value `V_G(s_{k+1})`.\n\nThe other options are less compelling:\n- Option A focuses on signaling, but the question asks about maximizing expected payoff from conflict\n- Option C is about showing value to rebels, but they've already decided to fight\n- Option D incorrectly assumes the proposal is irrelevant, when the consolidation mechanism makes it strategically important\n\nB", "model_answer": "A", "error": null}}
{"ID": 48, "Question": "## Background\n\n**Research Question.** This problem examines the causal validity of a treatment effect estimate from a field experiment.\n\n**Setting / Institutional Environment.** A study estimates the causal effect of social information on donation amounts using a linear model:\n\n```latex\n\\text{PledgeAmount}_i = \\alpha + \\beta_{300} T_{i,300} + X_i'\\gamma + \\varepsilon_i\n\\quad \\text{(Eq. (1))}\n```\n\nwhere $T_{i,300}$ is an indicator for assignment to the `$300` treatment condition.\n\n---\n\n## Scenario\n\nSuppose the randomization protocol failed. Specifically, assume that experimenters, contrary to instructions, were more likely to assign callers who sounded enthusiastic and wealthy (an unobserved characteristic, `HighPropensity`) to the `$300` treatment group. By definition, these callers are predisposed to making larger donations regardless of treatment.\n\n---\n\n## Question\n\nUnder this scenario of failed randomization, how is the OLS estimate of the treatment effect, $\\hat{\\beta}_{300}$, biased?", "Options": {"A": "It will be biased upwards, overstating the true causal effect.", "B": "It will be biased downwards, understating the true causal effect.", "C": "It will be unbiased, because OLS is the best linear unbiased estimator.", "D": "It will be biased, but the direction is ambiguous without knowing the sign of the error term."}, "Answer": "A", "pi_justification": "This item assesses the ability to apply the omitted variable bias (OVB) framework. It uses a 'Scenario Application' strategy. The correct answer (A) requires identifying the two conditions for OVB: the omitted variable (`HighPropensity`) is correlated with the outcome (positively, by definition) and correlated with the treatment variable (positively, by the scenario). The product of these two positive effects creates a positive bias. Distractors target common errors: (B) is the 'Conceptual Opposite'; (C) misapplies the Gauss-Markov theorem, which requires the assumption of zero conditional mean (violated here); (D) incorrectly claims ambiguity when the direction is clearly identified by the scenario's premises.", "qid": "48", "question": "## Background\n\n**Research Question.** This problem examines the causal validity of a treatment effect estimate from a field experiment.\n\n**Setting / Institutional Environment.** A study estimates the causal effect of social information on donation amounts using a linear model:\n\n```latex\n\\text{PledgeAmount}_i = \\alpha + \\beta_{300} T_{i,300} + X_i'\\gamma + \\varepsilon_i\n\\quad \\text{(Eq. (1))}\n```\n\nwhere $T_{i,300}$ is an indicator for assignment to the `$300` treatment condition.\n\n---\n\n## Scenario\n\nSuppose the randomization protocol failed. Specifically, assume that experimenters, contrary to instructions, were more likely to assign callers who sounded enthusiastic and wealthy (an unobserved characteristic, `HighPropensity`) to the `$300` treatment group. By definition, these callers are predisposed to making larger donations regardless of treatment.\n\n---\n\n## Question\n\nUnder this scenario of failed randomization, how is the OLS estimate of the treatment effect, $\\hat{\\beta}_{300}$, biased?", "model_evaluation_result": {"model_raw_response": "Looking at this scenario, I need to analyze how the failed randomization affects the OLS estimate.\n\nThe key issue is that experimenters were more likely to assign \"HighPropensity\" individuals (those who sound enthusiastic and wealthy) to the $300 treatment group. These individuals are predisposed to making larger donations regardless of treatment.\n\nThis creates a positive correlation between:\n- Treatment assignment (T_{i,300})  \n- Unobserved characteristics that increase donation amounts (HighPropensity)\n\nIn the regression model:\nPledgeAmount_i = α + β_{300} T_{i,300} + X_i'γ + ε_i\n\nThe error term ε_i now contains the effect of HighPropensity, which is positively correlated with T_{i,300}. This violates the key OLS assumption that E[ε_i | T_{i,300}] = 0.\n\nSince:\n- HighPropensity increases donation amounts (positive effect on outcome)\n- HighPropensity is more likely in the treatment group (positive correlation with treatment)\n\nThe OLS estimator will attribute some of the effect of HighPropensity to the treatment itself, making the treatment appear more effective than it actually is.\n\nTherefore, β̂_{300} will be biased upwards, overstating the true causal effect of the $300 treatment.\n\nA", "model_answer": "A", "error": null}}
{"ID": 77, "Question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms for determining firm investment and its financing generally fail to guarantee an equilibrium?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy where stockholders must collectively decide on firms' production plans. This analysis compares two distinct linear mechanisms that fail.\n\n---\n\n### Data / Model Specification\n\nThe decision problem of a stockholder is to choose a consumption plan, a portfolio `\\theta_i`, and a message `m_i` to maximize utility. The firm's input `x_j` and the stockholder's cost share `C_{ij}` are determined by the specific mechanism's rules.\n\n1.  **The Shareholding Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = \\theta_{ij} x_j`\n\n2.  **The Helpman-Razin Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = (1/I) x_j`\n\nBoth mechanisms are shown in the paper to fail to guarantee an equilibrium in general.\n\n---\n\nWhat are the distinct fundamental reasons for the failure of the Shareholding Mechanism and the Helpman-Razin Mechanism, respectively?", "Options": {"A": "The Shareholding Mechanism fails because it forces portfolio shares (`\\theta_{ij}`) to serve a dual role of risk-sharing and cost-sharing, preventing unanimity. The Helpman-Razin Mechanism fails because its exogenously fixed cost shares are too rigid to accommodate heterogeneous stockholder preferences.", "B": "Both mechanisms fail for the same reason: they force portfolio shares (`\\theta_{ij}`) to serve a dual role of risk-sharing and cost-sharing, which prevents stockholders from reaching a unanimous decision on the firm's investment level.", "C": "The Shareholding Mechanism fails because its cost shares are too rigid to accommodate heterogeneous preferences. The Helpman-Razin Mechanism fails because it forces portfolio shares (`\\theta_{ij}`) to serve a dual role of risk-sharing and cost-sharing.", "D": "Both mechanisms fail because they do not satisfy the Ekern-Wilson spanning condition, which is a necessary condition for equilibrium existence in any stock market model."}, "Answer": "A", "pi_justification": "This item assesses the core diagnostic understanding of why simple, linear mechanisms fail in this setting. It uses a Reverse-Reasoning strategy, asking the user to identify the distinct causes for two related failures. The distractors are designed to test for common conceptual confusions: (B) conflating the two distinct failures, (C) reversing the causes and effects, and (D) attributing the failure to a related but different concept (spanning) that the paper shows is not the core issue.", "qid": "77", "question": "### Background\n\n**Research Question.** In a stock ownership economy, why do simple, linear collective decision mechanisms for determining firm investment and its financing generally fail to guarantee an equilibrium?\n\n**Setting / Institutional Environment.** The setting is a two-period stock ownership economy where stockholders must collectively decide on firms' production plans. This analysis compares two distinct linear mechanisms that fail.\n\n---\n\n### Data / Model Specification\n\nThe decision problem of a stockholder is to choose a consumption plan, a portfolio `\\theta_i`, and a message `m_i` to maximize utility. The firm's input `x_j` and the stockholder's cost share `C_{ij}` are determined by the specific mechanism's rules.\n\n1.  **The Shareholding Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = \\theta_{ij} x_j`\n\n2.  **The Helpman-Razin Mechanism:**\n    -   Allocation Rule: `x_j = \\sum_i m_{ij}`\n    -   Cost-Sharing Rule: `C_{ij}(m) = (1/I) x_j`\n\nBoth mechanisms are shown in the paper to fail to guarantee an equilibrium in general.\n\n---\n\nWhat are the distinct fundamental reasons for the failure of the Shareholding Mechanism and the Helpman-Razin Mechanism, respectively?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify the distinct fundamental reasons why each mechanism fails to guarantee equilibrium.\n\nLet me analyze each mechanism:\n\n**The Shareholding Mechanism:**\n- Allocation Rule: x_j = Σᵢ m_ij\n- Cost-Sharing Rule: C_ij(m) = θ_ij x_j\n\nHere, the cost share that stockholder i pays for firm j's input is directly tied to their portfolio share θ_ij. This creates a fundamental problem: the portfolio shares θ_ij must simultaneously serve two distinct economic functions:\n1. Risk-sharing (determining how much of firm j's random returns stockholder i receives)\n2. Cost-sharing (determining how much of firm j's input costs stockholder i pays)\n\nThis dual role creates conflicts because optimal risk-sharing generally requires different portfolio allocations than optimal cost-sharing, preventing stockholders from reaching unanimous decisions.\n\n**The Helpman-Razin Mechanism:**\n- Allocation Rule: x_j = Σᵢ m_ij  \n- Cost-Sharing Rule: C_ij(m) = (1/I) x_j\n\nHere, costs are shared equally among all I stockholders, regardless of their portfolio shares or preferences. This exogenously fixed equal cost-sharing rule is too rigid - it cannot accommodate the heterogeneous preferences and circumstances of different stockholders, leading to disagreement about optimal investment levels.\n\nThe key insight is that these are distinct failure modes:\n- Shareholding Mechanism: Portfolio shares forced into dual role\n- Helpman-Razin Mechanism: Exogenously fixed cost shares too rigid\n\nA", "model_answer": "A", "error": null}}
{"ID": 89, "Question": "### Background\n\n**Research Question:** This problem examines the mathematical construction of uncorrelated time-series scenarios using Chebyshev polynomials, which serve as efficient inputs for emulating a complex climate model.\n\n**Setting / Institutional Environment:** The methodology generates a set of basis vectors for CO2 emissions from a family of orthogonal polynomials. These polynomials are defined over a canonical interval `x ∈ [-1, 1]` and must be transformed to be used for a specific time horizon, e.g., years `t ∈ [2005, 2100]`.\n\n### Data / Model Specification\n\nChebyshev polynomials are defined by the recurrence relation:\n\n```latex\nP_{0}(x)=1, \\quad P_{1}(x)=x, \\quad P_{k}(x)=2x P_{k-1}(x)-P_{k-2}(x) \\quad \\text{for } k=2,3,\\ldots \\quad \\text{(Eq. 1)}\n```\n\nThe domain `x ∈ [-1, 1]` is mapped to a general time interval `t ∈ [a, b]` using the transformation:\n\n```latex\nt = \\left(\\frac{b-a}{2}\\right)x + \\frac{a+b}{2} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAn emissions scenario `E_2(t)` is constructed based on the Chebyshev polynomial of degree 2, `P_2(x)`. The domain `x ∈ [-1, 1]` is mapped to the time horizon `t ∈ [2005, 2100]`. The range of the polynomial `[-1, 1]` is then linearly scaled to an emissions range of `[-5, 29.6708]` GtC/yr. What is the approximate emissions level for the year `t=2052.5`, which corresponds to the midpoint of the time horizon?", "Options": {"A": "-5.0 GtC/yr", "B": "-1.0 GtC/yr", "C": "12.34 GtC/yr", "D": "29.67 GtC/yr"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 10.0). The original QA is purely technical and computational, making it ideal for conversion. This item tests the multi-step process of applying the mathematical tools described. Strategy: Computational Judgment. The correct answer requires deriving `P_2(x)=2x^2-1`, finding that `t=2052.5` maps to `x=0`, evaluating `P_2(0)=-1`, and mapping this to the minimum of the emissions range. Distractors are based on predictable errors: (B) is a Step-Omission Error, forgetting the final range scaling; (C) is a Conceptual Error, taking the midpoint of the emissions range; (D) is another Conceptual Error, evaluating at an endpoint (`x=1`) instead of the midpoint.", "qid": "89", "question": "### Background\n\n**Research Question:** This problem examines the mathematical construction of uncorrelated time-series scenarios using Chebyshev polynomials, which serve as efficient inputs for emulating a complex climate model.\n\n**Setting / Institutional Environment:** The methodology generates a set of basis vectors for CO2 emissions from a family of orthogonal polynomials. These polynomials are defined over a canonical interval `x ∈ [-1, 1]` and must be transformed to be used for a specific time horizon, e.g., years `t ∈ [2005, 2100]`.\n\n### Data / Model Specification\n\nChebyshev polynomials are defined by the recurrence relation:\n\n```latex\nP_{0}(x)=1, \\quad P_{1}(x)=x, \\quad P_{k}(x)=2x P_{k-1}(x)-P_{k-2}(x) \\quad \\text{for } k=2,3,\\ldots \\quad \\text{(Eq. 1)}\n```\n\nThe domain `x ∈ [-1, 1]` is mapped to a general time interval `t ∈ [a, b]` using the transformation:\n\n```latex\nt = \\left(\\frac{b-a}{2}\\right)x + \\frac{a+b}{2} \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAn emissions scenario `E_2(t)` is constructed based on the Chebyshev polynomial of degree 2, `P_2(x)`. The domain `x ∈ [-1, 1]` is mapped to the time horizon `t ∈ [2005, 2100]`. The range of the polynomial `[-1, 1]` is then linearly scaled to an emissions range of `[-5, 29.6708]` GtC/yr. What is the approximate emissions level for the year `t=2052.5`, which corresponds to the midpoint of the time horizon?", "model_evaluation_result": {"model_raw_response": "I need to solve this step by step.\n\nFirst, let me find the Chebyshev polynomial of degree 2, P₂(x).\n\nUsing the recurrence relation:\n- P₀(x) = 1\n- P₁(x) = x\n- P₂(x) = 2x·P₁(x) - P₀(x) = 2x·x - 1 = 2x² - 1\n\nNext, I need to find the x-value corresponding to t = 2052.5.\n\nUsing the transformation equation: t = ((b-a)/2)x + (a+b)/2\nWhere a = 2005, b = 2100, and t = 2052.5\n\n2052.5 = ((2100-2005)/2)x + (2005+2100)/2\n2052.5 = (95/2)x + 2052.5\n2052.5 = 47.5x + 2052.5\n0 = 47.5x\nx = 0\n\nSo at the midpoint of the time horizon (t = 2052.5), x = 0.\n\nNow I can evaluate P₂(0):\nP₂(0) = 2(0)² - 1 = -1\n\nThe polynomial value is -1, which needs to be scaled from the range [-1, 1] to the emissions range [-5, 29.6708] GtC/yr.\n\nUsing linear scaling:\n- The range of P₂(x) is [-1, 1] (span = 2)\n- The emissions range is [-5, 29.6708] (span = 34.6708)\n\nThe scaling transformation is:\nE = (P₂(x) - (-1))/(1 - (-1)) × (29.6708 - (-5)) + (-5)\nE = (P₂(x) + 1)/2 × 34.6708 - 5\n\nFor P₂(0) = -1:\nE = (-1 + 1)/2 × 34.6708 - 5\nE = 0/2 × 34.", "model_answer": "I need to solve this step by step.\n\nFirst, let me find the Chebyshev polynomial of degree 2, P₂(x).\n\nUsing the recurrence relation:\n- P₀(x) = 1\n- P₁(x) = x\n- P₂(x) = 2x·P₁(x) - P₀(x) = 2x·x - 1 = 2x² - 1\n\nNext, I need to find the x-value corresponding to t = 2052.5.\n\nUsing the transformation equation: t = ((b-a)/2)x + (a+b)/2\nWhere a = 2005, b = 2100, and t = 2052.5\n\n2052.5 = ((2100-2005)/2)x + (2005+2100)/2\n2052.5 = (95/2)x + 2052.5\n2052.5 = 47.5x + 2052.5\n0 = 47.5x\nx = 0\n\nSo at the midpoint of the time horizon (t = 2052.5), x = 0.\n\nNow I can evaluate P₂(0):\nP₂(0) = 2(0)² - 1 = -1\n\nThe polynomial value is -1, which needs to be scaled from the range [-1, 1] to the emissions range [-5, 29.6708] GtC/yr.\n\nUsing linear scaling:\n- The range of P₂(x) is [-1, 1] (span = 2)\n- The emissions range is [-5, 29.6708] (span = 34.6708)\n\nThe scaling transformation is:\nE = (P₂(x) - (-1))/(1 - (-1)) × (29.6708 - (-5)) + (-5)\nE = (P₂(x) + 1)/2 × 34.6708 - 5\n\nFor P₂(0) = -1:\nE = (-1 + 1)/2 × 34.6708 - 5\nE = 0/2 × 34.", "error": "Warning: Could not extract single choice letter from: I need to solve this step by step.\n\nFirst, let me ..."}}
{"ID": 40, "Question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). An 'assignment' is a matrix, $D = (\\delta_{ij})$, where $\\delta_{ij}$ is the proportion of meetings between type-i workers and type-j jobs that result in a match. A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. An assignment is an equilibrium if the $\\delta_{ij}$ values are consistent with these private decisions.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nThe model considers three potential equilibrium assignments, with the 'Exact' assignment defined by $\\delta_{11}=\\delta_{22}=\\delta_{33}=1$ and all other $\\delta_{ij}=0$. Equilibrium reservation values for the 'Exact' and 'Mixed (2, 3)' assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Mixed (2, 3) | 0.323           | 0.870           | 2.313           |\n| Exact        | 0.301           | 0.904           | 2.410           |\n\n### Question\n\nIn the 'Exact' equilibrium, the model specifies that matches between type-2 workers and type-3 jobs are always rejected (i.e., $\\delta_{23}=0$). Is this rejection privately rational for the agents, and why?", "Options": {"A": "Yes, it is rational because the match output ($a_{23}=3.2$) is less than the sum of the agents' reservation values in the 'Exact' equilibrium ($w_{02}+r_{03}=3.314$).", "B": "No, it is irrational because the match output ($a_{23}=3.2$) is greater than the sum of reservation values calculated using the 'Mixed (2, 3)' equilibrium data ($w_{02}+r_{03}=3.183$).", "C": "No, it is irrational because the match output ($a_{23}=3.2$) is greater than the sum of the agents' reservation values ($w_{02}+r_{02}=1.808$).", "D": "Yes, it is rational because the 'Exact' assignment by definition requires that only diagonal matches ($\\delta_{ii}=1$) are formed."}, "Answer": "A", "pi_justification": "This item assesses the ability to apply the model's core equilibrium condition. It uses a Computational Judgment strategy. Distractors are designed based on common errors: B (Lookup Error: using data from the wrong equilibrium), C (Calculation Error: using the wrong reservation value, r_02 instead of r_03), and D (Conceptual Error: confusing the definition of an assignment with the equilibrium condition).", "qid": "40", "question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers a labor market with three types of workers (i=1,2,3) and three types of jobs (j=1,2,3). An 'assignment' is a matrix, $D = (\\delta_{ij})$, where $\\delta_{ij}$ is the proportion of meetings between type-i workers and type-j jobs that result in a match. A match is privately optimal if the output $a_{ij}$ is greater than or equal to the sum of the reservation wage and profit, $w_{0i} + r_{0j}$. An assignment is an equilibrium if the $\\delta_{ij}$ values are consistent with these private decisions.\n\nThe productivity of each match type is given by the matrix $A$:\n\n```latex\nA={\\left[\\begin{array}{ccc} 1.0 & 1.18 & 1.0 \\\\ 1.18 & 3.0 & 3.2 \\\\ 1.0 & 3.2 & 8.0 \\end{array}\\right]}\n```\n\nThe model considers three potential equilibrium assignments, with the 'Exact' assignment defined by $\\delta_{11}=\\delta_{22}=\\delta_{33}=1$ and all other $\\delta_{ij}=0$. Equilibrium reservation values for the 'Exact' and 'Mixed (2, 3)' assignments are presented in Table 1.\n\n**Table 1: Reservation Values**\n| Assignment   | $w_{01}=r_{01}$ | $w_{02}=r_{02}$ | $w_{03}=r_{03}$ |\n| :----------- | :-------------- | :-------------- | :-------------- |\n| Mixed (2, 3) | 0.323           | 0.870           | 2.313           |\n| Exact        | 0.301           | 0.904           | 2.410           |\n\n### Question\n\nIn the 'Exact' equilibrium, the model specifies that matches between type-2 workers and type-3 jobs are always rejected (i.e., $\\delta_{23}=0$). Is this rejection privately rational for the agents, and why?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to determine whether the rejection of matches between type-2 workers and type-3 jobs (δ₂₃ = 0) is privately rational in the 'Exact' equilibrium.\n\nFrom the problem setup:\n- A match is privately optimal if the output a_ij ≥ w₀ᵢ + r₀ⱼ\n- If output < reservation sum, the match should be rejected (rational)\n- If output ≥ reservation sum, the match should be accepted\n\nGiven data:\n- Match output: a₂₃ = 3.2\n- In 'Exact' equilibrium: w₀₂ = r₀₂ = 0.904 and w₀₃ = r₀₃ = 2.410\n\nLet me calculate the sum of reservation values for a type-2 worker and type-3 job in the 'Exact' equilibrium:\nw₀₂ + r₀₃ = 0.904 + 2.410 = 3.314\n\nComparing:\n- Match output: a₂₃ = 3.2\n- Sum of reservations: w₀₂ + r₀₃ = 3.314\n\nSince 3.2 < 3.314, the match output is less than the sum of reservation values, making rejection privately rational for both agents.\n\nLooking at the options:\n\nA) States rejection is rational because a₂₃ = 3.2 < w₀₂ + r₀₃ = 3.314 ✓\n\nB) Uses Mixed equilibrium values, which is not relevant for analyzing the Exact equilibrium\n\nC) Incorrectly uses w₀₂ + r₀₂ instead of w₀₂ + r₀₃\n\nD) Gives a definitional reason rather than analyzing private rationality\n\nA", "model_answer": "A", "error": null}}
{"ID": 39, "Question": "### Background\n\n**Research Question.** This problem investigates the dynamic adjustment of the interest rate to a monetary policy shock, testing the paper's proposition of short-run \"over-adjustment\" within the specific Miller-Orr model of money demand.\n\n**Setting / Institutional Environment.** We consider the Miller-Orr \"pure diffusion\" case for a representative individual's money holdings, where cash flows follow a random walk. The monetary authority contracts the money supply by a fraction `\\gamma` through open-market sales of bonds.\n\n### Data / Model Specification\n\nFor the Miller-Orr model with a lower barrier of 0, an upper barrier `h`, and a return point `z`:\n1.  **Optimal Policy:** The optimal return point is `z=h/3`.\n2.  **Average Holdings:** The average money holding is `M = 4h/9`.\n3.  **Over-adjustment Condition:** Over-adjustment of the interest rate occurs if the new long-run average money holding, `M' = 4h'/9`, is less than the amount of money remaining after the short-run adjustment, `(1-\\gamma)M`.\n4.  **Derived Result:** The paper shows that for the Miller-Orr case, the condition for over-adjustment reduces to the following polynomial inequality in `y = h'/h`:\n    ```latex\n    \\frac{1}{2}y^{3}-\\frac{1}{2}y^{2}-\\frac{1}{18}y+\\frac{1}{18}<0\n    ```\n    The roots of this polynomial are at `y=1` and `y=\\pm 1/3`.\n\n### Question\n\nGiven the model and the derived inequality, under what conditions will the interest rate over-adjust in the short run in response to a monetary contraction of size `\\gamma`?", "Options": {"A": "Over-adjustment occurs for monetary contractions `\\gamma` that are less than 2/3.", "B": "Over-adjustment occurs for monetary contractions `\\gamma` that are greater than 2/3.", "C": "Over-adjustment occurs for monetary contractions `\\gamma` that are less than 1/3.", "D": "Over-adjustment never occurs; the interest rate adjusts smoothly to its new long-run equilibrium."}, "Answer": "A", "pi_justification": "This item assesses the ability to solve a mathematical condition and translate the result into an economic conclusion. It uses a Computational Judgment strategy. The correct answer requires solving a polynomial inequality for `y=h'/h` to find `y > 1/3`, and then correctly converting this into a condition on the monetary contraction `\\gamma` using `y = M'/M = (1-\\gamma_{LR})/(1)`. Distractors are based on common errors: (B) is a sign error in solving the inequality; (C) is a unit error, confusing the bound on `y` with the bound on `\\gamma`; (D) represents a plausible but incorrect conceptual belief about model dynamics.", "qid": "39", "question": "### Background\n\n**Research Question.** This problem investigates the dynamic adjustment of the interest rate to a monetary policy shock, testing the paper's proposition of short-run \"over-adjustment\" within the specific Miller-Orr model of money demand.\n\n**Setting / Institutional Environment.** We consider the Miller-Orr \"pure diffusion\" case for a representative individual's money holdings, where cash flows follow a random walk. The monetary authority contracts the money supply by a fraction `\\gamma` through open-market sales of bonds.\n\n### Data / Model Specification\n\nFor the Miller-Orr model with a lower barrier of 0, an upper barrier `h`, and a return point `z`:\n1.  **Optimal Policy:** The optimal return point is `z=h/3`.\n2.  **Average Holdings:** The average money holding is `M = 4h/9`.\n3.  **Over-adjustment Condition:** Over-adjustment of the interest rate occurs if the new long-run average money holding, `M' = 4h'/9`, is less than the amount of money remaining after the short-run adjustment, `(1-\\gamma)M`.\n4.  **Derived Result:** The paper shows that for the Miller-Orr case, the condition for over-adjustment reduces to the following polynomial inequality in `y = h'/h`:\n    ```latex\n    \\frac{1}{2}y^{3}-\\frac{1}{2}y^{2}-\\frac{1}{18}y+\\frac{1}{18}<0\n    ```\n    The roots of this polynomial are at `y=1` and `y=\\pm 1/3`.\n\n### Question\n\nGiven the model and the derived inequality, under what conditions will the interest rate over-adjust in the short run in response to a monetary contraction of size `\\gamma`?", "model_evaluation_result": {"model_raw_response": "I need to analyze the Miller-Orr model and the given polynomial inequality to determine when over-adjustment occurs.\n\nGiven information:\n- The polynomial inequality for over-adjustment is: (1/2)y³ - (1/2)y² - (1/18)y + 1/18 < 0\n- The roots are at y = 1 and y = ±1/3\n- y = h'/h (ratio of new to old upper barrier)\n- Over-adjustment occurs when M' < (1-γ)M, where M' = 4h'/9 and M = 4h/9\n\nLet me analyze the polynomial inequality:\n- The polynomial has roots at y = -1/3, y = 1/3, and y = 1\n- Since we're dealing with a cubic with positive leading coefficient (1/2), the polynomial is negative between consecutive roots where the number of roots to the left is odd\n\nFor y > 0 (which makes economic sense), the relevant roots are y = 1/3 and y = 1.\n\nThe polynomial is negative for y ∈ (1/3, 1).\n\nNow I need to connect this to the monetary contraction γ:\n- From the over-adjustment condition: M' < (1-γ)M\n- Substituting: (4h'/9) < (1-γ)(4h/9)\n- Simplifying: h'/h < (1-γ)\n- Therefore: y < (1-γ)\n- This means: γ < (1-y)\n\nSince over-adjustment occurs when y ∈ (1/3, 1), we need:\n- When y = 1/3: γ < (1-1/3) = 2/3\n- When y = 1: γ < (1-1) = 0\n\nSince y must be in (1/3, 1) for over-adjustment, and γ < (1-y), we get:\n- For y approaching 1/3 from above: γ approaches 2/3 from below\n- For y approaching 1 from below: γ approaches 0 from above\n\nTherefore, over-adjustment occurs when γ < 2", "model_answer": "I need to analyze the Miller-Orr model and the given polynomial inequality to determine when over-adjustment occurs.\n\nGiven information:\n- The polynomial inequality for over-adjustment is: (1/2)y³ - (1/2)y² - (1/18)y + 1/18 < 0\n- The roots are at y = 1 and y = ±1/3\n- y = h'/h (ratio of new to old upper barrier)\n- Over-adjustment occurs when M' < (1-γ)M, where M' = 4h'/9 and M = 4h/9\n\nLet me analyze the polynomial inequality:\n- The polynomial has roots at y = -1/3, y = 1/3, and y = 1\n- Since we're dealing with a cubic with positive leading coefficient (1/2), the polynomial is negative between consecutive roots where the number of roots to the left is odd\n\nFor y > 0 (which makes economic sense), the relevant roots are y = 1/3 and y = 1.\n\nThe polynomial is negative for y ∈ (1/3, 1).\n\nNow I need to connect this to the monetary contraction γ:\n- From the over-adjustment condition: M' < (1-γ)M\n- Substituting: (4h'/9) < (1-γ)(4h/9)\n- Simplifying: h'/h < (1-γ)\n- Therefore: y < (1-γ)\n- This means: γ < (1-y)\n\nSince over-adjustment occurs when y ∈ (1/3, 1), we need:\n- When y = 1/3: γ < (1-1/3) = 2/3\n- When y = 1: γ < (1-1) = 0\n\nSince y must be in (1/3, 1) for over-adjustment, and γ < (1-y), we get:\n- For y approaching 1/3 from above: γ approaches 2/3 from below\n- For y approaching 1 from below: γ approaches 0 from above\n\nTherefore, over-adjustment occurs when γ < 2", "error": "Warning: Could not extract single choice letter from: I need to analyze the Miller-Orr model and the giv..."}}
{"ID": 95, "Question": "## Background\n\n**Research Question.** This problem examines the foundational conditions for the existence of a quasi-Lindahl equilibrium, focusing on the critical role of assumptions about consumer preferences.\n\n**Setting / Institutional Environment.** The economy consists of a continuum of consumers and a finite number of producers, with both private and public goods. The existence proof for equilibrium relies on a fixed-point theorem, which requires that the consumer's quasi-demand correspondence is non-empty and convex-valued.\n\n**Variables & Parameters.**\n\n*   $x(t) = (x_\\pi(t), x_g) \\in R^{n+l}$: A consumption bundle for consumer $t$.\n*   $p = (p_\\pi, p_g) \\in S$: A price system for private and public goods.\n*   $\\delta(t) \\in R_+^l$: A vector of personalized Lindahl shares for consumer $t$.\n*   $\\gtrsim_t$: The preference relation for consumer $t$.\n*   $x >_t y$: Strict preference, defined as $x \\gtrsim_t y$ and not $y \\gtrsim_t x$.\n*   $D^k(p, \\delta, t)$: The quasi-demand set for consumer $t$, containing all bundles in their (compactified) choice set that are not strictly preferred by any other affordable bundle.\n\n---\n\n## Data / Model Specification\n\nThe existence proof relies on key assumptions about preferences, including completeness and convexity. The Fan-Glicksberg fixed-point theorem, used to prove existence, requires the quasi-demand correspondence to be convex-valued. This, in turn, requires that each individual's quasi-demand set, $D^k(p, \\delta, t)$, is a convex set.\n\nConsider a scenario where a consumer's preferences are **not complete**. Suppose $z^1$ and $z^2$ are two distinct bundles in the consumer's budget set, and both are in the quasi-demand set $D^k$. Now, consider their convex combination $\\bar{z} = 0.5z^1 + 0.5z^2$. Because preferences are incomplete, there exists another affordable bundle $y$ which is incomparable to both $z^1$ and $z^2$, but is strictly preferred to their combination, i.e., $y >_t \\bar{z}$.\n\n---\n\n## Question\n\nGiven this scenario, what is the direct mathematical consequence for the quasi-demand set $D^k$ and the overall proof strategy?", "Options": {"A": "The set $D^k$ is not convex, which invalidates a necessary condition for the Fan-Glicksberg fixed-point theorem.", "B": "The set $D^k$ must be empty, because no bundle can be optimal if preferences are incomplete.", "C": "The set $D^k$ is not closed, violating the continuity assumptions of the model.", "D": "The consumer's budget set must be non-linear, which is a contradiction."}, "Answer": "A", "pi_justification": "This question tests the consequence of violating the completeness axiom for the structure of the optimal choice set. It uses a Scenario Application strategy. The correct answer identifies non-convexity as the key failure. Distractor (B) confuses ill-behaved with non-existent. Distractor (C) confuses convexity with the topological property of being closed. Distractor (D) incorrectly attributes the problem to the budget constraint rather than the preference structure.", "qid": "95", "question": "## Background\n\n**Research Question.** This problem examines the foundational conditions for the existence of a quasi-Lindahl equilibrium, focusing on the critical role of assumptions about consumer preferences.\n\n**Setting / Institutional Environment.** The economy consists of a continuum of consumers and a finite number of producers, with both private and public goods. The existence proof for equilibrium relies on a fixed-point theorem, which requires that the consumer's quasi-demand correspondence is non-empty and convex-valued.\n\n**Variables & Parameters.**\n\n*   $x(t) = (x_\\pi(t), x_g) \\in R^{n+l}$: A consumption bundle for consumer $t$.\n*   $p = (p_\\pi, p_g) \\in S$: A price system for private and public goods.\n*   $\\delta(t) \\in R_+^l$: A vector of personalized Lindahl shares for consumer $t$.\n*   $\\gtrsim_t$: The preference relation for consumer $t$.\n*   $x >_t y$: Strict preference, defined as $x \\gtrsim_t y$ and not $y \\gtrsim_t x$.\n*   $D^k(p, \\delta, t)$: The quasi-demand set for consumer $t$, containing all bundles in their (compactified) choice set that are not strictly preferred by any other affordable bundle.\n\n---\n\n## Data / Model Specification\n\nThe existence proof relies on key assumptions about preferences, including completeness and convexity. The Fan-Glicksberg fixed-point theorem, used to prove existence, requires the quasi-demand correspondence to be convex-valued. This, in turn, requires that each individual's quasi-demand set, $D^k(p, \\delta, t)$, is a convex set.\n\nConsider a scenario where a consumer's preferences are **not complete**. Suppose $z^1$ and $z^2$ are two distinct bundles in the consumer's budget set, and both are in the quasi-demand set $D^k$. Now, consider their convex combination $\\bar{z} = 0.5z^1 + 0.5z^2$. Because preferences are incomplete, there exists another affordable bundle $y$ which is incomparable to both $z^1$ and $z^2$, but is strictly preferred to their combination, i.e., $y >_t \\bar{z}$.\n\n---\n\n## Question\n\nGiven this scenario, what is the direct mathematical consequence for the quasi-demand set $D^k$ and the overall proof strategy?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze what happens when preferences are incomplete and how this affects the convexity of the quasi-demand set D^k.\n\nThe key scenario described is:\n- Two bundles z¹ and z² are both in the quasi-demand set D^k\n- Their convex combination z̄ = 0.5z¹ + 0.5z² exists\n- Due to incomplete preferences, there exists an affordable bundle y such that y >_t z̄\n\nLet me trace through the logic:\n\n1) If z¹ and z² are in D^k, by definition of the quasi-demand set, there is no affordable bundle that strictly dominates either of them.\n\n2) For D^k to be convex, any convex combination of elements in D^k must also be in D^k. So z̄ should be in D^k.\n\n3) But the scenario tells us there exists an affordable bundle y such that y >_t z̄.\n\n4) If y >_t z̄ and y is affordable, then z̄ cannot be in the quasi-demand set D^k, because the quasi-demand set contains only bundles that are not strictly dominated by any other affordable bundle.\n\n5) This creates a contradiction: z̄ should be in D^k (for convexity) but cannot be in D^k (due to being dominated by y).\n\n6) Therefore, D^k is not convex.\n\nThe Fan-Glicksberg fixed-point theorem requires correspondences to be convex-valued. If the quasi-demand correspondence is not convex-valued (because D^k is not convex), then this violates a necessary condition for applying the theorem.\n\nThe answer is A.\n\nA", "model_answer": "D", "error": null}}
{"ID": 50, "Question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium'.\n\n**Variables & Parameters.**\n\n*   `T'_{k*}`: The marginal transfer price for retailer `k` at equilibrium.\n*   `(p^B, e^B)`: The prices and efforts that arise in standard Bertrand competition.\n*   `D_e`: The 2x2 matrix of demand derivatives with respect to effort, where element `(i, k)` is `∂D_k/∂e_i`.\n\n### Data / Model Specification\n\nIn any contract equilibrium, the following system of equations must hold for `i=1, 2`:\n\n```latex\n\\sum_{k=1}^{2}(T_{k}^{*\\prime}-c)\\frac{\\partial D_{k}}{\\partial e_{i}}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\n**Assumption 1:** A uniform increase in efforts `e_1` and `e_2` causes demand `D_i` to rise. This implies that the matrix `D_e` is invertible.\n\n### Question\n\nGiven the necessary equilibrium condition shown in Eq. (1) and Assumption 1, what is the equilibrium marginal transfer price `T'_{k*}` for each retailer `k`, and what is the resulting market outcome?", "Options": {"A": "The marginal transfer price for each retailer equals the manufacturer's marginal cost (`T'_{k*} = c`), leading to the standard Bertrand outcome (`p^B, e^B`).", "B": "The marginal transfer price equals the fully integrated price (`T'_{k*} = p_k^I`), which successfully implements the fully integrated outcome.", "C": "The marginal transfer price is set above marginal cost (`T'_{k*} > c`) to extract rent, leading to prices above the Bertrand level.", "D": "The marginal transfer price is set below marginal cost (`T'_{k*} < c`) to subsidize effort, leading to higher-than-Bertrand effort levels."}, "Answer": "A", "pi_justification": "This item assesses the main mathematical result of Proposition 1, which was derived in the second and third parts of the original QA problem. It uses a Computational Judgment strategy. The correct answer (A) states the paper's surprising result: secret contracting with retail effort forces the manufacturer to price at marginal cost, leading to the competitive Bertrand outcome. Distractor (B) is a classic misconception, representing the result from models *without* retail effort (O'Brien and Shaffer). Distractor (C) represents a common but incorrect intuition about monopoly pricing. Distractor (D) describes a strategy (subsidizing effort) that might be optimal with public contracts but is not sustainable with secret contracts.", "qid": "50", "question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium'.\n\n**Variables & Parameters.**\n\n*   `T'_{k*}`: The marginal transfer price for retailer `k` at equilibrium.\n*   `(p^B, e^B)`: The prices and efforts that arise in standard Bertrand competition.\n*   `D_e`: The 2x2 matrix of demand derivatives with respect to effort, where element `(i, k)` is `∂D_k/∂e_i`.\n\n### Data / Model Specification\n\nIn any contract equilibrium, the following system of equations must hold for `i=1, 2`:\n\n```latex\n\\sum_{k=1}^{2}(T_{k}^{*\\prime}-c)\\frac{\\partial D_{k}}{\\partial e_{i}}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\n**Assumption 1:** A uniform increase in efforts `e_1` and `e_2` causes demand `D_i` to rise. This implies that the matrix `D_e` is invertible.\n\n### Question\n\nGiven the necessary equilibrium condition shown in Eq. (1) and Assumption 1, what is the equilibrium marginal transfer price `T'_{k*}` for each retailer `k`, and what is the resulting market outcome?", "model_evaluation_result": {"model_raw_response": "Looking at this vertical contracting problem, I need to analyze the equilibrium condition in Eq. (1) and determine what it implies for the marginal transfer prices.\n\nThe key equilibrium condition is:\n$$\\sum_{k=1}^{2}(T_{k}^{*\\prime}-c)\\frac{\\partial D_{k}}{\\partial e_{i}}=0 \\quad \\text{for } i=1,2$$\n\nThis can be written in matrix form as:\n$$D_e^T (T'^* - c \\mathbf{1}) = \\mathbf{0}$$\n\nwhere $T'^* = (T_1'^*, T_2'^*)^T$ and $\\mathbf{1} = (1,1)^T$.\n\nSince $D_e$ is invertible by Assumption 1, $D_e^T$ is also invertible. For the equation $D_e^T (T'^* - c \\mathbf{1}) = \\mathbf{0}$ to hold, we must have:\n\n$$T'^* - c \\mathbf{1} = \\mathbf{0}$$\n\nTherefore: $T_k'^* = c$ for both retailers.\n\nThis means the marginal transfer price equals the manufacturer's marginal cost for each retailer.\n\nWhen retailers face marginal transfer prices equal to the manufacturer's marginal cost, they internalize the true social cost of production. In the absence of other distortions, this leads to the standard Bertrand competition outcome where retailers compete as if they were integrated firms with marginal cost $c$.\n\nThe answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 141, "Question": "### Background\n\n**Research Question.** This problem examines how different types of information technology can have opposing effects on the boundaries of the firm, depending on which organizational trade-off they influence. It focuses on the shipper's decision to use a private fleet versus a for-hire carrier.\n\n**Setting / Institutional Environment.** A shipper's choice between a private fleet (integration) and a for-hire carrier (disintegration) involves two key trade-offs:\n1.  **Service vs. Cost:** Private fleets are generally better at providing customized, high-service hauls.\n2.  **Utilization Efficiency:** For-hire carriers are specialists in logistics and can achieve higher truck utilization by finding backhauls across a large network of customers, a key comparative advantage.\n\nWe consider a specific type of on-board computer:\n-   **`OBC_L` (Logistics):** Provides GPS tracking and communication, improving a dispatcher's ability to find and schedule backhauls.\n\n### The Question\n\nIn contrast to monitoring technology, a logistics-enhancing OBC (`OBC_L`) improves a dispatcher's ability to find and schedule backhauls. Based on the 'comparative advantage' mechanism, what is the predicted effect of `OBC_L` adoption on firm boundaries?", "Options": {"A": "It leads to less shipper ownership because it amplifies the core advantage of for-hire carriers in network logistics and utilization efficiency.", "B": "It leads to more shipper ownership because it allows private fleets to close the utilization gap with for-hire carriers.", "C": "It leads to more shipper ownership because better logistics require higher driver service levels, which are best provided by private fleets.", "D": "It has no effect on ownership, as logistics are independent of the contractual problems between shippers and carriers."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the 'comparative advantage' argument using a qualitative Computational Judgment strategy. It tests a core, sharp prediction from the paper. The correct answer (A) identifies the right effect and mechanism. Distractors test for common errors: (B) reverses the logic of who benefits more from the technology, (C) incorrectly links the logistics mechanism to the service-level argument, and (D) denies the premise of the question.", "qid": "141", "question": "### Background\n\n**Research Question.** This problem examines how different types of information technology can have opposing effects on the boundaries of the firm, depending on which organizational trade-off they influence. It focuses on the shipper's decision to use a private fleet versus a for-hire carrier.\n\n**Setting / Institutional Environment.** A shipper's choice between a private fleet (integration) and a for-hire carrier (disintegration) involves two key trade-offs:\n1.  **Service vs. Cost:** Private fleets are generally better at providing customized, high-service hauls.\n2.  **Utilization Efficiency:** For-hire carriers are specialists in logistics and can achieve higher truck utilization by finding backhauls across a large network of customers, a key comparative advantage.\n\nWe consider a specific type of on-board computer:\n-   **`OBC_L` (Logistics):** Provides GPS tracking and communication, improving a dispatcher's ability to find and schedule backhauls.\n\n### The Question\n\nIn contrast to monitoring technology, a logistics-enhancing OBC (`OBC_L`) improves a dispatcher's ability to find and schedule backhauls. Based on the 'comparative advantage' mechanism, what is the predicted effect of `OBC_L` adoption on firm boundaries?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 143, "Question": "### Background\n\n**Research Question.** This problem examines the critical assumptions required to identify the causal return to education using family background variables as instruments, particularly highlighting the different identification challenges posed by the Family and Individual models.\n\n**Setting.** The identification strategy relies on using a vector of family background variables, `Z_if`, as instruments for education. The validity of these instruments depends on their relationship with unobserved parental endowments (`ε_f`) and child-specific idiosyncratic endowments (`ξ_if`).\n\n### Data / Model Specification\n\nThe paper models a child's total earnings endowment (`ε_if`) and the family background variables (`Z_if`) as follows:\n```latex\n\\ln(\\varepsilon_{if}) = \\rho \\ln(\\varepsilon_f) + \\xi_{if} \\quad \\text{(Eq. 1)}\n```\n```latex\n\\mathbf{Z}_{if} = \\Pi \\ln(\\varepsilon_f) + \\xi_f + \\kappa_1 \\xi_{if} + \\kappa_2 \\xi_{kf} \\quad \\text{(Eq. 2)}\n```\nwhere `ρ` is the intergenerational correlation of endowments, `Π` links `Z` to parental endowment, and `κ_1` and `κ_2` link `Z` to the idiosyncratic endowments of child `i` and their sibling `k`.\n\nAn instrument is valid if it is uncorrelated with the error term of the education choice equation. \n*   In the **Family Model**, this error term `e_if1` depends on the *difference* in idiosyncratic endowments: `e_{if1} = δ(ξ_{if} - ξ_{kf}) + h_f + ...`\n*   In the **Individual Model**, this error term `e_if2` depends on the child's *total* endowment: `e_{if2} ∝ α(ρ \\ln(ε_f) + ξ_{if}) + ...`\n\n### Question\n\nWhich statement best explains why family background variables (`Z_if`) face different identification challenges as instruments for education in the Individual Model versus the Family Model, assuming intergenerational correlation in endowments (`ρ > 0`) and `Π ≠ 0`?", "Options": {"A": "In the Individual Model, `Z_if` is correlated with the error term via the shared parental endowment `ε_f`, making it invalid. In the Family Model, the error term is driven by the *difference* in sibling endowments, so the shared parental endowment `ε_f` is differenced out, preserving instrument validity under certain assumptions.", "B": "In the Individual Model, instruments are valid because individuals' choices are independent of family background. In the Family Model, `Z_if` is correlated with parental preferences for equity, making it an invalid instrument.", "C": "In the Individual Model, instrument validity requires assuming no intergenerational correlation in endowments (`ρ = 0`). In the Family Model, instrument validity requires assuming parents do not compensate for endowment differences (`δ = 0`).", "D": "In both models, `Z_if` is an invalid instrument because it is correlated with unobserved family resources (`h_f`). The only way to achieve identification is with a sibling-difference estimator."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the core identification logic of the paper. It uses a Reverse-Reasoning strategy, asking the student to identify the correct explanation for a key theoretical result. The correct answer (A) accurately summarizes the central argument. Distractor (B) is a conceptual opposite. Distractor (C) presents plausible but incorrect assumptions for each model. Distractor (D) offers a true statement about a potential problem (`h_f`) but incorrectly presents it as the primary distinction and the only solution, which is a 'True but Irrelevant/Incomplete' error.", "qid": "143", "question": "### Background\n\n**Research Question.** This problem examines the critical assumptions required to identify the causal return to education using family background variables as instruments, particularly highlighting the different identification challenges posed by the Family and Individual models.\n\n**Setting.** The identification strategy relies on using a vector of family background variables, `Z_if`, as instruments for education. The validity of these instruments depends on their relationship with unobserved parental endowments (`ε_f`) and child-specific idiosyncratic endowments (`ξ_if`).\n\n### Data / Model Specification\n\nThe paper models a child's total earnings endowment (`ε_if`) and the family background variables (`Z_if`) as follows:\n```latex\n\\ln(\\varepsilon_{if}) = \\rho \\ln(\\varepsilon_f) + \\xi_{if} \\quad \\text{(Eq. 1)}\n```\n```latex\n\\mathbf{Z}_{if} = \\Pi \\ln(\\varepsilon_f) + \\xi_f + \\kappa_1 \\xi_{if} + \\kappa_2 \\xi_{kf} \\quad \\text{(Eq. 2)}\n```\nwhere `ρ` is the intergenerational correlation of endowments, `Π` links `Z` to parental endowment, and `κ_1` and `κ_2` link `Z` to the idiosyncratic endowments of child `i` and their sibling `k`.\n\nAn instrument is valid if it is uncorrelated with the error term of the education choice equation. \n*   In the **Family Model**, this error term `e_if1` depends on the *difference* in idiosyncratic endowments: `e_{if1} = δ(ξ_{if} - ξ_{kf}) + h_f + ...`\n*   In the **Individual Model**, this error term `e_if2` depends on the child's *total* endowment: `e_{if2} ∝ α(ρ \\ln(ε_f) + ξ_{if}) + ...`\n\n### Question\n\nWhich statement best explains why family background variables (`Z_if`) face different identification challenges as instruments for education in the Individual Model versus the Family Model, assuming intergenerational correlation in endowments (`ρ > 0`) and `Π ≠ 0`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 148, "Question": "### Background\n\nThe paper's main identification result (Theorem 1) is derived for the case of continuously observed durations `t` and two discrete covariate values `x₀, x₁`. However, the authors argue the result can be adapted to a different data structure common in applied work.\n\n### Data / Model Specification\n\nConsider a scenario with **discretely observed durations**, where the CDF `F(t|x)` is known only for two time points, `t ∈ {t₀, t₁}`, but for a **continuous range of covariates** `x`.\n\nThe paper proposes analyzing this by \"swapping the roles\" of time and covariates: the continuous function `φ(x)` takes the role of the integrated baseline hazard `Λ(t)`, and the fixed values `Λ(t₀)` and `Λ(t₁)` take the roles of the discrete covariate effects `φ(x₀)` and `φ(x₁)`. Theorem 1 is then applied to this re-framed problem.\n\n### Question\n\nUnder this \"role-swapping\" scenario, what is the precise identification result for the original structural functions `φ(·)` and `Λ(·)`?", "Options": {"A": "The entire functions `φ(·)` and `Λ(·)` are both identified up to a common power transformation.", "B": "The entire function `φ(·)` is identified up to a power transformation, but the function `Λ(·)` is only identified at the points `t₀` and `t₁`.", "C": "The entire function `Λ(·)` is identified up to a power transformation, but the function `φ(·)` is only identified for two specific values of `x`.", "D": "Neither function is identified nonparametrically; only the product `φ(x)Λ(t)` is identified at the observed points."}, "Answer": "B", "pi_justification": "This item tests the application of the paper's core theorem to a novel data scenario, a key extension discussed in Section 4. It uses the Scenario Application strategy. The distractors are built around a matrix of common comprehension errors: overstating the power of the result (A), completely inverting the logic of what is identified (C), and understating the result (D).", "qid": "148", "question": "### Background\n\nThe paper's main identification result (Theorem 1) is derived for the case of continuously observed durations `t` and two discrete covariate values `x₀, x₁`. However, the authors argue the result can be adapted to a different data structure common in applied work.\n\n### Data / Model Specification\n\nConsider a scenario with **discretely observed durations**, where the CDF `F(t|x)` is known only for two time points, `t ∈ {t₀, t₁}`, but for a **continuous range of covariates** `x`.\n\nThe paper proposes analyzing this by \"swapping the roles\" of time and covariates: the continuous function `φ(x)` takes the role of the integrated baseline hazard `Λ(t)`, and the fixed values `Λ(t₀)` and `Λ(t₁)` take the roles of the discrete covariate effects `φ(x₀)` and `φ(x₁)`. Theorem 1 is then applied to this re-framed problem.\n\n### Question\n\nUnder this \"role-swapping\" scenario, what is the precise identification result for the original structural functions `φ(·)` and `Λ(·)`?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 61, "Question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nIn the numerical example, the vertical merger causes two key changes shown in Table 1: the integrated firm's punishment payoff increases from 0 to 10, and an unintegrated firm's cheating payoff decreases from 100 to 80. Which statement correctly identifies the economic effects driving these changes?", "Options": {"A": "The increase to 10 is the outlets effect; the decrease to 80 is the punishment effect.", "B": "Both changes are caused by the outlets effect, as the integrated firm is shielded from competition in all phases of the game.", "C": "The increase to 10 is the punishment effect; the decrease to 80 is the reaction effect.", "D": "The increase to 10 is the punishment effect; the decrease to 80 is the outlets effect."}, "Answer": "D", "pi_justification": "This item tests the core conceptual distinction between the 'outlets effect' and the 'punishment effect'. It uses a Reverse-Reasoning strategy, asking the user to map outcomes to causes. Distractors include a direct conceptual swap (A), conflating the two effects (B), and introducing a true but irrelevant concept from a different model (C).", "qid": "61", "question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nIn the numerical example, the vertical merger causes two key changes shown in Table 1: the integrated firm's punishment payoff increases from 0 to 10, and an unintegrated firm's cheating payoff decreases from 100 to 80. Which statement correctly identifies the economic effects driving these changes?", "model_evaluation_result": {"model_raw_response": "I need to analyze the economic effects of vertical merger on upstream collusion stability.\n\nLet me examine the changes in Table 1:\n\n1. **Integrated firm's punishment payoff increases from 0 to 10**\n   - In non-integration: unintegrated firms earn 0 when punished\n   - In single integration: integrated firm earns 10 when punished\n   - This increase occurs because the integrated firm now has downstream operations that continue to generate profits even when upstream collusion breaks down\n\n2. **Unintegrated firm's cheating payoff decreases from 100 to 80**\n   - In non-integration: cheating firm gets 100\n   - In single integration: unintegrated firm gets 80 when cheating\n   - This decrease occurs because there's now one fewer downstream customer available (the integrated firm doesn't buy from competitors)\n\nNow I need to match these to the correct economic effects:\n\n- **Outlets effect**: When a firm integrates vertically, unintegrated competitors lose access to that firm's downstream outlets, reducing their potential profits\n- **Punishment effect**: The integrated firm's downstream operations provide a \"cushion\" during punishment phases, as downstream profits continue even when upstream collusion fails\n\nLooking at the changes:\n1. The increase from 0 to 10 (integrated firm's punishment payoff) = **Punishment effect** (downstream operations provide profits during punishment)\n2. The decrease from 100 to 80 (unintegrated firm's cheating payoff) = **Outlets effect** (fewer downstream outlets available to sell to)\n\nD", "model_answer": "D", "error": null}}
{"ID": 9, "Question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), marginal learning about the non-identified parameter `\\lambda` is possible because `\\psi` and `\\lambda` are a priori dependent. We consider two scenarios with improper priors.\n\n### Question\n\nA researcher argues for using an improper prior on `\\beta` (`\\tau^2 \\to \\infty`) to be \"non-informative.\" Which statement best critiques this reasoning and correctly contrasts it with using an improper prior on `\\lambda` (`\\omega^2 \\to \\infty`) instead?", "Options": {"A": "The prior is not non-informative; it dogmatically asserts `\\psi` and `\\lambda` are independent, preventing learning about `\\lambda`. In contrast, an improper prior on `\\lambda` asserts perfect dependence, maximizing learning about `\\lambda`.", "B": "The prior is indeed non-informative, allowing the data to speak freely about both `\\beta` and `\\lambda`. An improper prior on `\\lambda` would have the same effect.", "C": "The prior is not non-informative because it prevents learning about `\\lambda`. An improper prior on `\\lambda` would also prevent learning about `\\lambda`, but for different reasons.", "D": "The prior is not non-informative because it makes the posterior for `\\beta` improper. An improper prior on `\\lambda` is truly non-informative and allows learning about both parameters."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the subtle, paradoxical nature of \"non-informative\" priors in this context. Strategy: Scenario Application. The user must critique one scenario and contrast it with another. Distractor Logic: B) is incorrect on all points. C) has a partially correct critique but an incorrect contrast. D) offers an incorrect critique and an incorrect contrast. The correct answer requires understanding the precise and opposite effects of the two different improper prior specifications.", "qid": "9", "question": "### Background\n\n**Research Question.** How does the use of improper \"non-informative\" priors on some parameters affect the potential for Bayesian learning about other, non-identified parameters in the same model?\n\n**Setting.** A single observation `y` is drawn from `N(\\beta+\\lambda, \\sigma^2)`. The identified parameter is `\\psi = \\beta+\\lambda`, and `\\lambda` is the non-identified parameter. The priors on `\\beta` and `\\lambda` are independent: `\\beta \\sim N(0, \\tau^2)` and `\\lambda \\sim N(0, \\omega^2)`.\n\n### Data / Model Specification\n\nWith proper priors (`\\tau^2, \\omega^2` finite), marginal learning about the non-identified parameter `\\lambda` is possible because `\\psi` and `\\lambda` are a priori dependent. We consider two scenarios with improper priors.\n\n### Question\n\nA researcher argues for using an improper prior on `\\beta` (`\\tau^2 \\to \\infty`) to be \"non-informative.\" Which statement best critiques this reasoning and correctly contrasts it with using an improper prior on `\\lambda` (`\\omega^2 \\to \\infty`) instead?", "model_evaluation_result": {"model_raw_response": "Looking at this Bayesian identification problem, I need to analyze what happens when we use improper priors on different parameters.\n\n**Key Setup:**\n- Observable: y ~ N(β+λ, σ²)\n- Identified parameter: ψ = β+λ  \n- Non-identified parameter: λ\n- With proper priors: β ~ N(0, τ²), λ ~ N(0, ω²)\n\n**Critical Insight:** With proper priors, we can learn about the non-identified parameter λ because ψ and λ are a priori dependent through their relationship with β.\n\n**Analyzing the improper prior on β (τ² → ∞):**\n\nWhen τ² → ∞, the prior on β becomes flat/improper. This creates a crucial problem:\n\n- The posterior variance of λ approaches ω² (the prior variance)\n- This means the data provides no information about λ\n- Effectively, ψ and λ become asymptotically independent in the posterior\n- This is the opposite of \"non-informative\" - it's a strong assumption that prevents learning\n\n**Contrasting with improper prior on λ (ω² → ∞):**\n\nWhen ω² → ∞:\n- The prior on λ becomes flat/improper\n- But the relationship between ψ and λ through β is preserved\n- The posterior can still update beliefs about λ through the data\n- This maintains the dependence structure needed for learning\n\n**Evaluating the options:**\n\nOption A correctly identifies that:\n1. The improper prior on β is NOT non-informative\n2. It dogmatically asserts independence between ψ and λ\n3. This prevents learning about λ\n4. An improper prior on λ maintains dependence, allowing learning\n\nA", "model_answer": "A", "error": null}}
{"ID": 92, "Question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive Total Least Squares (TLS) estimator selects the rank of the estimated matrix `\\hat{M}` by keeping only the singular components of `X` that have a high estimated signal-to-noise ratio.\n\n---\n\n### Data / Model Specification\n\nThe adaptive TLS estimator selects its rank `\\hat{k}` according to the rule:\n```latex\n\\hat{k} = \\#\\{j : \\hat{\\pi}_j > 1/2\\}, \\quad \\text{where} \\quad \\hat{\\pi}_j = 1 - p\\hat{\\sigma}^2/\\hat{l}_j^2\n```\nThe variance `\\sigma^2` is estimated using the Smallest Singular Value (SSV) estimator, `\\hat{\\sigma}^2 = \\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p`.\n\n---\n\n### Question\n\nSuppose the condition for the SSV estimator to be consistent fails, such that the true signal `l_q^2/p` converges to a positive constant `c > 0` as `p \\to \\infty`. How does this affect the rank `\\hat{k}` selected by the adaptive TLS procedure in large samples?", "Options": {"A": "The selected rank `\\hat{k}` will be systematically too high because the procedure mistakes signal for noise.", "B": "The selected rank `\\hat{k}` will be systematically too low because the overestimated noise level leads to overly conservative thresholding.", "C": "The rank selection will be asymptotically unbiased because the bias in `\\hat{\\sigma}^2` is a lower-order effect that does not affect the thresholding rule.", "D": "The effect on rank is ambiguous, as the upward bias in `\\hat{\\sigma}^2` is offset by the upward bias in the other estimated singular values `\\hat{l}_j^2`."}, "Answer": "B", "pi_justification": "This item assesses the ability to trace the consequences of a parameter identification failure through a multi-step reasoning chain. It uses the Computational Judgment strategy. The distractors map to predictable error paths: (A) is the Conceptual Opposite, reversing the direction of the final effect; (C) represents a Step-Omission Error, failing to propagate the bias through the formula; (D) is a plausible but incorrect hedge that fails to recognize the dominant, systematic effect of the biased variance estimate.", "qid": "92", "question": "### Background\n\nThe model is `X = M + E`, where `X` is a `p x q` data matrix, `M` is the true mean matrix, and `E` is a random error matrix with i.i.d. rows having covariance `\\sigma^2 I_q`. The adaptive Total Least Squares (TLS) estimator selects the rank of the estimated matrix `\\hat{M}` by keeping only the singular components of `X` that have a high estimated signal-to-noise ratio.\n\n---\n\n### Data / Model Specification\n\nThe adaptive TLS estimator selects its rank `\\hat{k}` according to the rule:\n```latex\n\\hat{k} = \\#\\{j : \\hat{\\pi}_j > 1/2\\}, \\quad \\text{where} \\quad \\hat{\\pi}_j = 1 - p\\hat{\\sigma}^2/\\hat{l}_j^2\n```\nThe variance `\\sigma^2` is estimated using the Smallest Singular Value (SSV) estimator, `\\hat{\\sigma}^2 = \\hat{\\sigma}_{SSV}^2 = \\hat{l}_q^2 / p`.\n\n---\n\n### Question\n\nSuppose the condition for the SSV estimator to be consistent fails, such that the true signal `l_q^2/p` converges to a positive constant `c > 0` as `p \\to \\infty`. How does this affect the rank `\\hat{k}` selected by the adaptive TLS procedure in large samples?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze what happens when the SSV estimator fails to be consistent.\n\nWhen the SSV estimator is consistent, we have $\\hat{\\sigma}^2 = \\hat{l}_q^2/p \\to \\sigma^2$ as $p \\to \\infty$.\n\nHowever, when the consistency condition fails, we're told that $l_q^2/p \\to c > 0$ where $c > 0$ is a positive constant. This means the smallest singular value $l_q^2$ contains signal, not just noise.\n\nSince $\\hat{\\sigma}^2 = \\hat{l}_q^2/p$ and $\\hat{l}_q^2 \\to l_q^2$, we have:\n$$\\hat{\\sigma}^2 \\to c > 0$$\n\nBut the true noise variance is $\\sigma^2$. Since $l_q^2/p \\to c > 0$, this means $l_q^2$ contains both signal and noise, so $c > \\sigma^2$. Therefore:\n$$\\hat{\\sigma}^2 \\to c > \\sigma^2$$\n\nThe SSV estimator overestimates the true noise variance.\n\nNow let's see how this affects the rank selection rule:\n$$\\hat{\\pi}_j = 1 - \\frac{p\\hat{\\sigma}^2}{\\hat{l}_j^2}$$\n\nA component is selected if $\\hat{\\pi}_j > 1/2$, which means:\n$$1 - \\frac{p\\hat{\\sigma}^2}{\\hat{l}_j^2} > \\frac{1}{2}$$\n$$\\frac{p\\hat{\\sigma}^2}{\\hat{l}_j^2} < \\frac{1}{2}$$\n$$\\hat{l}_j^2 > 2p\\hat{\\sigma}^2$$\n\nSince $\\hat{\\sigma}^2$ is overestimated (converges to $c > \\sigma^2$ instead of $\\sigma^2$), the threshold $2p\\hat{\\sigma}^2$ becomes too high. This means:\n\n- Components that", "model_answer": "A", "error": null}}
{"ID": 150, "Question": "## Background\n\n**Research Question.** This problem evaluates the experimental design used to test theoretical predictions about charitable competition, focusing on the role of control treatments for ensuring internal validity.\n\n**Setting.** A series of controlled laboratory experiments where subjects make real donation decisions. The design varies the relationship between the charitable causes (e.g., complements).\n\n---\n\n## Data / Model Specification\n\nTwo experiments focused on complementary causes:\n- **`Comp`**: Subjects donated to toothpaste and toothbrushes. Any unpaired items (e.g., a tube of toothpaste without a matching toothbrush) were forfeited and not donated.\n- **`Comp-W`**: This experiment was identical to `Comp`, except that unpaired items were still donated individually to a homeless person.\n\n---\n\n## Question\n\nWhat is the most important reason for conducting the `Comp-W` experiment as a control for the `Comp` experiment?", "Options": {"A": "To test if subjects understood that toothpaste and toothbrushes are complements.", "B": "To ensure that the results in `Comp` were driven by the complementary nature of the goods, not by an aversion to the \"waste\" of forfeited donations.", "C": "To measure the baseline level of giving to complementary charities in a more realistic setting.", "D": "To see if the \"business stealing\" effect re-emerges when the complementarity is weakened."}, "Answer": "B", "pi_justification": "This item assesses the understanding of internal validity and confounders in experimental design, using a Scenario Application strategy. The correct answer (B) identifies the specific confound—aversion to waste—that the `Comp-W` design is meant to rule out. Distractor A is an 'Almost Right' error; while related, the key issue is the *reason* for the behavior, not just understanding. Distractor C is 'True but Irrelevant'; while `Comp-W` might be more realistic, its primary purpose as a control is to validate the `Comp` results. Distractor D is also plausible but secondary; the main goal is to check the mechanism in `Comp`, not just to see if stealing appears.", "qid": "150", "question": "## Background\n\n**Research Question.** This problem evaluates the experimental design used to test theoretical predictions about charitable competition, focusing on the role of control treatments for ensuring internal validity.\n\n**Setting.** A series of controlled laboratory experiments where subjects make real donation decisions. The design varies the relationship between the charitable causes (e.g., complements).\n\n---\n\n## Data / Model Specification\n\nTwo experiments focused on complementary causes:\n- **`Comp`**: Subjects donated to toothpaste and toothbrushes. Any unpaired items (e.g., a tube of toothpaste without a matching toothbrush) were forfeited and not donated.\n- **`Comp-W`**: This experiment was identical to `Comp`, except that unpaired items were still donated individually to a homeless person.\n\n---\n\n## Question\n\nWhat is the most important reason for conducting the `Comp-W` experiment as a control for the `Comp` experiment?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 88, "Question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nModel (5) is selected as the best specification due to its superior out-of-sample performance (lowest RMSE). When Model (7) adds CO2 concentrations (`S_{t-1}`) to Model (5), its performance worsens, and the coefficient on `S_{t-1}` becomes statistically insignificant. What is the most likely econometric explanation for this phenomenon?", "Options": {"A": "High multicollinearity between cumulative emissions (`C_{t-1}`) and CO2 concentrations (`S_{t-1}`).", "B": "Omitted variable bias from excluding an important climate forcing agent in Model (7).", "C": "The ARMA(1,1) error structure is misspecified for Model (7).", "D": "Reverse causality, where higher temperatures are driving CO2 concentrations."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to diagnose a classic econometric problem from regression output, a critical skill for empirical researchers. Strategy: Reverse-Reasoning. The distractors represent common but incorrect diagnoses: (B) is a Conceptual Opposite, as the problem stems from adding, not omitting, a variable; (C) is True but Irrelevant, as misspecification is less likely to cause these specific symptoms than multicollinearity; (D) describes a different form of endogeneity not directly supported by the evidence in the table.", "qid": "88", "question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nModel (5) is selected as the best specification due to its superior out-of-sample performance (lowest RMSE). When Model (7) adds CO2 concentrations (`S_{t-1}`) to Model (5), its performance worsens, and the coefficient on `S_{t-1}` becomes statistically insignificant. What is the most likely econometric explanation for this phenomenon?", "model_evaluation_result": {"model_raw_response": "Looking at this econometric problem, I need to analyze what happens when Model (7) adds CO2 concentrations (S_{t-1}) to Model (5).\n\nKey observations:\n- Model (5) has the best performance (lowest RMSE of 0.0338)\n- When S_{t-1} is added to create Model (7), performance worsens (RMSE increases to 0.0411)\n- In Model (7), the coefficient on S_{t-1} becomes statistically insignificant (0.0286 with SE 0.0399, giving a t-stat of about 0.72)\n- The coefficient on C_{t-1} remains significant in Model (7)\n\nThis pattern is classic multicollinearity. CO2 concentrations and cumulative CO2 emissions are highly correlated because:\n- Cumulative emissions directly drive atmospheric CO2 concentrations\n- Both variables capture essentially the same underlying information about CO2's climate forcing effect\n\nWhen two highly correlated variables are included together:\n- Standard errors increase (making coefficients less significant)\n- Model performance can worsen despite adding a variable\n- Individual coefficients become unreliable even though the overall model fit may seem reasonable\n\nThe other options don't fit:\n- B) Omitted variable bias would improve performance when adding the missing variable\n- C) ARMA misspecification would affect all models similarly\n- D) Reverse causality doesn't explain the insignificance pattern observed\n\nA", "model_answer": "B", "error": null}}
{"ID": 87, "Question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nBased on the preferred Model (5) in Table 1, what is the estimated long-run steady-state temperature increase resulting from a sustained increase of 1000 GtC in cumulative CO2 emissions?", "Options": {"A": "0.36 °C", "B": "0.48 °C", "C": "1.16 °C", "D": "1.55 °C"}, "Answer": "D", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem's core components are highly suitable for conversion. This item tests the ability to calculate a long-run multiplier from a dynamic model, a key skill in interpreting econometric results. Strategy: Computational Judgment. The distractors are designed based on common errors: (A) is the static effect (`β_2`), ignoring the dynamic multiplier `1/(1-β_1)`; (B) is a formula misuse (`β_2/β_1`); (C) is a data misread error, correctly applying the formula but using the `β_1` coefficient from the wrong model (Model 6).", "qid": "87", "question": "### Background\n\n**Research Question:** This problem concerns the empirical model selection for a climate emulator, focusing on the choice of covariates that best predict global temperature anomalies and the interpretation of the final model's dynamic properties.\n\n**Setting / Institutional Environment:** Three competing dynamic models are estimated using simulated data generated from four orthogonal emissions scenarios run through the MAGICC climate model. Their predictive performance is evaluated based on their ability to forecast temperature anomalies for an out-of-sample set of four standard, correlated emissions scenarios (the RCPs). The primary goal is to select the most accurate and parsimonious model and understand its implications.\n\n### Data / Model Specification\n\nThe dependent variable in all models is the temperature anomaly `T_t` (°C) relative to preindustrial levels. The potential covariates are lagged temperature `T_{t-1}`, lagged cumulative CO2 emissions since 1765, `C_{t-1}` (measured in thousands of Gigatons of Carbon, `GtC * 10^3`), and lagged CO2 concentrations, `S_{t-1}` (measured in `ppm * 10^2`). All models include an ARMA(1,1) error term.\n\nThe three competing model specifications are:\n- **Model (5):** `T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t`\n- **Model (6):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + ε_t`\n- **Model (7):** `T_t = β_0 + β_1 T_{t-1} + β_2 S_{t-1} + β_3 C_{t-1} + ε_t`\n\nEstimation and out-of-sample performance results are presented in Table 1.\n\n**Table 1: Approximation results for different model specifications**\n*The last two columns report the average errors of prediction for the out-of-sample testing set of RCP scenarios. Standard errors are in parentheses.*\n\n| Model | `β_0` | `β_1` | `β_2` | `β_3` | RMSE (°C) | MAPE (%) |\n|:---|:---|:---|:---|:---|:---|:---|\n| (5) | 0.2500 (0.0344) | 0.7650 (0.0227) | 0.3632 (on `C_{t-1}`) (0.0345) | | 0.0338 | 1.62% |\n| (6) | 0.1188 (0.0401) | 0.6874 (0.0193) | 0.1503 (on `S_{t-1}`) (0.0089) | | 0.0426 | 1.75% |\n| (7) | 0.1230 (0.0396) | 0.6820 (0.0209) | 0.0286 (on `S_{t-1}`) (0.0399) | 0.1445 (on `C_{t-1}`) (0.0121) | 0.0411 | 1.68% |\n\n### Question\n\nBased on the preferred Model (5) in Table 1, what is the estimated long-run steady-state temperature increase resulting from a sustained increase of 1000 GtC in cumulative CO2 emissions?", "model_evaluation_result": {"model_raw_response": "I need to find the long-run steady-state temperature increase from a sustained increase of 1000 GtC in cumulative CO2 emissions using Model (5).\n\nModel (5) is: T_t = β_0 + β_1 T_{t-1} + β_2 C_{t-1} + ε_t\n\nFrom Table 1, the coefficients for Model (5) are:\n- β_0 = 0.2500\n- β_1 = 0.7650\n- β_2 = 0.3632\n\nIn the long-run steady state, T_t = T_{t-1} = T*, and C_{t-1} = C* (the new steady-state level of cumulative emissions).\n\nSo the steady-state equation becomes:\nT* = β_0 + β_1 T* + β_2 C*\n\nSolving for T*:\nT* - β_1 T* = β_0 + β_2 C*\nT*(1 - β_1) = β_0 + β_2 C*\nT* = (β_0 + β_2 C*)/(1 - β_1)\n\nFor a sustained increase of 1000 GtC, we need to find the change in steady-state temperature:\nΔT* = β_2 ΔC*/(1 - β_1)\n\nWhere:\n- ΔC* = 1000 GtC = 1 (since C is measured in thousands of GtC, i.e., GtC × 10^3)\n- β_2 = 0.3632\n- β_1 = 0.7650\n\nTherefore:\nΔT* = 0.3632 × 1/(1 - 0.7650)\nΔT* = 0.3632/0.2350\nΔT* = 1.546 °C\n\nRounding to two decimal places: 1.55 °C\n\nD", "model_answer": "C", "error": null}}
{"ID": 49, "Question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium', where no `M-R_i` pair has an incentive to secretly deviate from the equilibrium contracts.\n\n**Variables & Parameters.**\n\n*   `T'_{i*}`: The marginal transfer price for retailer `i` at equilibrium.\n*   `(p^I, e^I)`: The prices and efforts that maximize total industry profit (the 'fully integrated' outcome).\n*   `C'_i(e_i)`: The marginal cost of effort for retailer `i`.\n*   `D_i(p, e)`: Demand for retailer `i`'s product.\n\n### Data / Model Specification\n\nIn the competition stage, each retailer `i` chooses its effort `e_i` to maximize its own profit. The first-order condition for this choice is:\n\n```latex\n\\big(p_{i}^{*}-T_{i}^{*\\prime}\\big)\\frac{\\partial D_{i}}{\\partial e_{i}}-C_{i}^{\\prime}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\nFor any positive effort `e_i > 0`, the marginal cost of effort `C'_i` is strictly positive.\n\n### Question\n\nIn this model, why is it impossible for the manufacturer to use secret bilateral contracts to achieve the fully integrated profit outcome `(p^I, e^I)`?", "Options": {"A": "To incentivize positive sales effort, a retailer must earn a positive margin (`p_i - T'_{i*} > 0`), but to eliminate the opportunism problem, the manufacturer must set the margin to zero.", "B": "The non-contractibility of effort means the manufacturer cannot directly specify `e_i` in the contract, making it impossible to enforce the integrated effort level.", "C": "Retailers have an incentive to exert too much effort to steal market share, which the manufacturer cannot prevent with bilateral contracts.", "D": "The manufacturer cannot commit to charging a high enough marginal transfer price `T'_{i*}` to achieve the integrated outcome because retailers would refuse such an unprofitable contract."}, "Answer": "A", "pi_justification": "This item assesses the core economic conflict that prevents the manufacturer from achieving the first-best outcome, which was the subject of the first part of the original QA problem. It uses a Reverse-Reasoning strategy. The correct answer (A) identifies the fundamental tension between providing margins to incentivize effort and eliminating margins to prevent opportunism. Distractor (B) is a true precondition but not the direct cause of the conflict. Distractor (C) posits the opposite problem (too much effort). Distractor (D) misidentifies the reason for failure, focusing on retailer participation rather than the manufacturer's own conflicting incentives.", "qid": "49", "question": "### Background\n\n**Research Question.** This problem investigates the equilibrium outcome of a vertical relationship where a manufacturer uses secret bilateral contracts and retailers compete in both price and non-contractible, demand-enhancing sales effort.\n\n**Setting / Institutional Environment.** A manufacturer (M) with constant marginal cost `c` supplies two differentiated retailers (`R_1`, `R_2`) via secret, nonlinear contracts `T_i`. Retailers compete by choosing price `p_i` and non-contractible sales effort `e_i`. The equilibrium concept is 'contract equilibrium', where no `M-R_i` pair has an incentive to secretly deviate from the equilibrium contracts.\n\n**Variables & Parameters.**\n\n*   `T'_{i*}`: The marginal transfer price for retailer `i` at equilibrium.\n*   `(p^I, e^I)`: The prices and efforts that maximize total industry profit (the 'fully integrated' outcome).\n*   `C'_i(e_i)`: The marginal cost of effort for retailer `i`.\n*   `D_i(p, e)`: Demand for retailer `i`'s product.\n\n### Data / Model Specification\n\nIn the competition stage, each retailer `i` chooses its effort `e_i` to maximize its own profit. The first-order condition for this choice is:\n\n```latex\n\\big(p_{i}^{*}-T_{i}^{*\\prime}\\big)\\frac{\\partial D_{i}}{\\partial e_{i}}-C_{i}^{\\prime}=0\n\\quad\\quad\text{(Eq. (1))}\n```\n\nFor any positive effort `e_i > 0`, the marginal cost of effort `C'_i` is strictly positive.\n\n### Question\n\nIn this model, why is it impossible for the manufacturer to use secret bilateral contracts to achieve the fully integrated profit outcome `(p^I, e^I)`?", "model_evaluation_result": {"model_raw_response": "Looking at this vertical contracting problem, I need to identify the fundamental tension that prevents achieving the fully integrated outcome.\n\nFrom equation (1), we see that retailer i chooses effort to satisfy:\n(p_i* - T'_i*) × (∂D_i/∂e_i) - C'_i = 0\n\nThis can be rearranged as:\n(p_i* - T'_i*) = C'_i / (∂D_i/∂e_i)\n\nFor the retailer to choose positive effort (e_i > 0), we need C'_i > 0, which means the retailer must earn a positive margin: p_i* - T'_i* > 0.\n\nHowever, in vertical relationships, there's a well-known \"opportunism problem\" or \"double marginalization problem.\" When retailers have positive margins, they don't fully internalize the manufacturer's costs and benefits. To eliminate this opportunism and achieve perfect coordination, the manufacturer would ideally want to set the marginal transfer price equal to the retail price (T'_i* = p_i*), giving the retailer zero margin.\n\nBut here's the contradiction: \n- To get positive effort, retailers need positive margins (p_i* - T'_i* > 0)\n- To eliminate opportunism, the manufacturer needs zero margins (p_i* - T'_i* = 0)\n\nThese two requirements are mutually incompatible.\n\nOption A correctly identifies this fundamental trade-off between providing effort incentives and eliminating opportunism.\n\nA", "model_answer": "C", "error": null}}
{"ID": 60, "Question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nAfter a single vertical merger, what is the maximum one-period 'Payoff if cheating' for one of the remaining unintegrated upstream firms, and what is the economic reason for this value?", "Options": {"A": "100, because a deviating firm can still undercut its rivals' offers to capture the entire market.", "B": "90, because the integrated firm's downstream profit in the punishment phase (10) is subtracted from the total monopoly profit.", "C": "80, because the deviating firm cannot profitably sell to the integrated downstream firm, which represents one-fifth of the market.", "D": "70, because the integrated firm's collusive share (30) is subtracted from the total monopoly profit."}, "Answer": "C", "pi_justification": "This item assesses the user's ability to apply the 'outlets effect' in a concrete calculation. It uses a Computational Judgment strategy. Distractors are designed based on common errors: ignoring the effect entirely (A), confusing punishment profit with deviation profit reduction (B), and using an irrelevant collusive value (D).", "qid": "60", "question": "### Background\n\n**Research Question.** This problem explores how a single vertical merger affects the stability of upstream collusion, using a numerical example to illustrate the core trade-offs.\n\n**Setting / Institutional Environment.** The setting is an industry with five upstream firms and five downstream firms. The total industry profit under a vertically integrated monopoly is 100. Collusion is sustained via trigger strategies, where any deviation leads to a permanent reversion to a noncooperative equilibrium. In this noncooperative equilibrium, unintegrated upstream firms earn zero profit, while each of the five downstream firms earns a profit of 10.\n\n### Data / Model Specification\n\n**Table 1: Payoffs in Numerical Example**\n\n| Market structure    | Firm type    | Payoff if colluding | Payoff if cheating | Payoff if punished |\n|---------------------|--------------|---------------------|--------------------|--------------------|\n| Non-integration     | Unintegrated | 20                  | 100                | 0                  |\n| Single integration  | Integrated   | 30                  | 100                | 10                 |\n|                     | Unintegrated | 17.5                | 80                 | 0                  |\n\n### The Question\n\nAfter a single vertical merger, what is the maximum one-period 'Payoff if cheating' for one of the remaining unintegrated upstream firms, and what is the economic reason for this value?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine the maximum one-period payoff for a cheating unintegrated upstream firm after a single vertical merger.\n\nFrom Table 1, I can see that after single integration, an unintegrated firm's \"Payoff if cheating\" is 80.\n\nLet me analyze why this value makes economic sense:\n\nBefore integration: There were 5 upstream firms and 5 downstream firms, all separate entities. A cheating upstream firm could potentially capture the entire market by undercutting rivals, earning the full monopoly profit of 100.\n\nAfter single integration: One upstream firm merges with one downstream firm, creating an integrated entity. Now there are 4 unintegrated upstream firms and 4 unintegrated downstream firms, plus 1 integrated firm.\n\nThe key insight is that when an unintegrated upstream firm deviates/cheats, it cannot profitably sell to the integrated downstream firm. The integrated firm has its own upstream division and has no incentive to buy from an external upstream firm when it cheats (breaks collusion).\n\nSince the integrated firm represents 1/5 of the downstream market, the cheating unintegrated upstream firm can only access 4/5 of the market. Therefore, its maximum payoff is 4/5 × 100 = 80.\n\nThis matches the value given in Table 1 and aligns perfectly with option C's explanation.\n\nC", "model_answer": "A", "error": null}}
{"ID": 153, "Question": "### Background\n\nThe study approximates total manufacturing pollution using data organized by \"activity\" (from the National Emissions Inventory, NEI) rather than by more precise industry codes. To test the validity of this choice, the author compares the approximation to the \"true\" pollution calculated from industry codes for years where both are available.\n\n### Data / Model Specification\n\n**Table 1: Ratio of True Manufacturing Pollution to Approximated Pollution**\n\n| Pollutant | 1990 | 1996 | 1997 | 1998 | 1999 | 2000 | 2001 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| All four | 0.77 | 0.73 | 0.74 | 0.74 | 0.73 | 0.72 | 0.73 |\n\n### Question\n\nTable 1 examines the ratio of true manufacturing pollution to the study's approximation. Based on the trend for 'All four' pollutants, what is the most valid conclusion regarding the study's main finding that the 'technique effect' was the primary driver of the cleanup?", "Options": {"A": "The ratio's stability supports the approximation's validity, and its slight downward trend suggests the study's estimate of the technique effect's importance is likely conservative (an understatement).", "B": "The ratio's slight downward trend suggests that the study's approximation is increasingly inaccurate, likely leading to an overstatement of the technique effect's importance.", "C": "The ratio's fluctuation between 0.72 and 0.77 demonstrates significant instability, rendering the approximation unreliable for drawing conclusions.", "D": "The consistent downward trend from 1996 to 2001 indicates that the approximation method was becoming progressively worse and should not have been used."}, "Answer": "A", "pi_justification": "This item assesses the ability to critique a data construction choice by interpreting a validation table. It uses a Reverse-Reasoning strategy. Distractors target common misinterpretations: B incorrectly identifies the direction of the potential bias, C mischaracterizes 'stability', and D overstates the magnitude and implication of the slight trend.", "qid": "153", "question": "### Background\n\nThe study approximates total manufacturing pollution using data organized by \"activity\" (from the National Emissions Inventory, NEI) rather than by more precise industry codes. To test the validity of this choice, the author compares the approximation to the \"true\" pollution calculated from industry codes for years where both are available.\n\n### Data / Model Specification\n\n**Table 1: Ratio of True Manufacturing Pollution to Approximated Pollution**\n\n| Pollutant | 1990 | 1996 | 1997 | 1998 | 1999 | 2000 | 2001 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| All four | 0.77 | 0.73 | 0.74 | 0.74 | 0.73 | 0.72 | 0.73 |\n\n### Question\n\nTable 1 examines the ratio of true manufacturing pollution to the study's approximation. Based on the trend for 'All four' pollutants, what is the most valid conclusion regarding the study's main finding that the 'technique effect' was the primary driver of the cleanup?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 100, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `p(q_n)`: Price of hospital care for non-covered patients.\n- `AC(q_n + q_m; α)`: Average cost of care, which is a function of total patients and an inefficiency parameter `α`.\n- `α`: An X-inefficiency shift parameter, where a higher `α` increases average cost (`∂AC/∂α > 0`).\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe hospital's profit function is given by:\n```latex\n\\pi = p(q_n) \\cdot q_n + k \\cdot q_m \\cdot AC(q_n+q_m; \\alpha) - (q_n+q_m) \\cdot AC(q_n+q_m; \\alpha) \\quad \\text{(Eq. (1))}\n```\n\nUnder the profit-maximization model, a hospital has a direct financial incentive to increase its own inefficiency (i.e., increase `α` to raise `AC`) only if which of the following conditions is met?", "Options": {"A": "The share of cost-reimbursed patients, `q_m / (q_n + q_m)`, exceeds the reciprocal of the reimbursement factor, `1/k`.", "B": "The 'plus' portion of revenue from reimbursed patients, `(k-1) * q_m * AC`, exceeds the total cost of non-reimbursed patients, `q_n * AC`.", "C": "The number of cost-reimbursed patients, `q_m`, exceeds the number of non-reimbursed patients, `q_n`.", "D": "The reimbursement factor, `k`, is greater than 1."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to derive or recognize the key theoretical condition from the paper's profit-maximization model. It uses a Reverse-Reasoning strategy. The correct option (A) is the derived result. Distractors are designed based on common conceptual errors: (B) compares the wrong components of the profit function, (C) represents an intuitive but incorrect simplification, and (D) states a necessary but insufficient condition.", "qid": "100", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `p(q_n)`: Price of hospital care for non-covered patients.\n- `AC(q_n + q_m; α)`: Average cost of care, which is a function of total patients and an inefficiency parameter `α`.\n- `α`: An X-inefficiency shift parameter, where a higher `α` increases average cost (`∂AC/∂α > 0`).\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe hospital's profit function is given by:\n```latex\n\\pi = p(q_n) \\cdot q_n + k \\cdot q_m \\cdot AC(q_n+q_m; \\alpha) - (q_n+q_m) \\cdot AC(q_n+q_m; \\alpha) \\quad \\text{(Eq. (1))}\n```\n\nUnder the profit-maximization model, a hospital has a direct financial incentive to increase its own inefficiency (i.e., increase `α` to raise `AC`) only if which of the following conditions is met?", "model_evaluation_result": {"model_raw_response": "I need to analyze when a hospital has an incentive to increase inefficiency (α) to raise average costs.\n\nLet me examine the profit function:\nπ = p(qₙ) · qₙ + k · qₘ · AC(qₙ+qₘ; α) - (qₙ+qₘ) · AC(qₙ+qₘ; α)\n\nFor the hospital to have an incentive to increase α (and thus AC), the derivative of profit with respect to α must be positive:\n\n∂π/∂α > 0\n\nTaking the derivative:\n∂π/∂α = k · qₘ · (∂AC/∂α) - (qₙ+qₘ) · (∂AC/∂α)\n\nSince ∂AC/∂α > 0 (given in the problem), I can factor this out:\n∂π/∂α = (∂AC/∂α) · [k · qₘ - (qₙ+qₘ)]\n∂π/∂α = (∂AC/∂α) · [k · qₘ - qₙ - qₘ]\n∂π/∂α = (∂AC/∂α) · [qₘ(k-1) - qₙ]\n\nFor the hospital to have an incentive to increase inefficiency:\n∂π/∂α > 0\n\nSince ∂AC/∂α > 0, this requires:\nqₘ(k-1) - qₙ > 0\nqₘ(k-1) > qₙ\nqₘ/qₙ > 1/(k-1)\n\nRearranging:\nqₘ/(qₙ+qₘ) > 1/[k-1 + qₙ/qₘ]\n\nBut let me approach this more directly. The condition is:\nqₘ(k-1) > qₙ\n\nDividing both sides by (qₙ+qₘ):\nqₘ(k", "model_answer": "I need to analyze when a hospital has an incentive to increase inefficiency (α) to raise average costs.\n\nLet me examine the profit function:\nπ = p(qₙ) · qₙ + k · qₘ · AC(qₙ+qₘ; α) - (qₙ+qₘ) · AC(qₙ+qₘ; α)\n\nFor the hospital to have an incentive to increase α (and thus AC), the derivative of profit with respect to α must be positive:\n\n∂π/∂α > 0\n\nTaking the derivative:\n∂π/∂α = k · qₘ · (∂AC/∂α) - (qₙ+qₘ) · (∂AC/∂α)\n\nSince ∂AC/∂α > 0 (given in the problem), I can factor this out:\n∂π/∂α = (∂AC/∂α) · [k · qₘ - (qₙ+qₘ)]\n∂π/∂α = (∂AC/∂α) · [k · qₘ - qₙ - qₘ]\n∂π/∂α = (∂AC/∂α) · [qₘ(k-1) - qₙ]\n\nFor the hospital to have an incentive to increase inefficiency:\n∂π/∂α > 0\n\nSince ∂AC/∂α > 0, this requires:\nqₘ(k-1) - qₙ > 0\nqₘ(k-1) > qₙ\nqₘ/qₙ > 1/(k-1)\n\nRearranging:\nqₘ/(qₙ+qₘ) > 1/[k-1 + qₙ/qₘ]\n\nBut let me approach this more directly. The condition is:\nqₘ(k-1) > qₙ\n\nDividing both sides by (qₙ+qₘ):\nqₘ(k", "error": "Warning: Could not extract single choice letter from: I need to analyze when a hospital has an incentive..."}}
{"ID": 25, "Question": "### Background\n\n**Research Question.** This problem investigates how the Rational Expectations Hypothesis (REH) imposes testable cross-equation restrictions within a linear simultaneous equations model.\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\pi_1, \\pi_{1,0}^*$: Scalar reduced-form coefficients.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to lagged exogenous variables.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process. Under REH, the expectation $y_{1,t}^*$ is a linear function of the information set, which in this case simplifies to $y_{1,t}^* = \\psi_0 z_{t-1}$.\n\n---\n\n### Question\n\nWhat is the correct expression for the coefficient $\\psi_0$ that is consistent with the Rational Expectations Hypothesis?", "Options": {"A": "$\\psi_0 = \\frac{\\pi_1 A_1}{1 - \\pi_{1,0}^*}$", "B": "$\\psi_0 = \\frac{\\pi_1 A_1}{1 + \\pi_{1,0}^*}$", "C": "$\\psi_0 = \\pi_1 A_1$", "D": "$\\psi_0 = \\frac{\\pi_1}{1 - \\pi_{1,0}^*}$"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item captures the core of the original QA's Q3(a). Assessment Target: Test the ability to apply the general theory of REH restrictions to a specific, simplified model. Strategy: Computational Judgment. Distractor Logic: (B) Sign Error in the denominator, a common algebraic slip. (C) Step-Omission Error, ignoring the feedback term from the expectation itself ($y_{1,t}^* \\pi_{1,0}^*$). (D) Formula Misuse, omitting the persistence parameter $A_1$ from the exogenous process forecast.", "qid": "25", "question": "### Background\n\n**Research Question.** This problem investigates how the Rational Expectations Hypothesis (REH) imposes testable cross-equation restrictions within a linear simultaneous equations model.\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\pi_1, \\pi_{1,0}^*$: Scalar reduced-form coefficients.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to lagged exogenous variables.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process. Under REH, the expectation $y_{1,t}^*$ is a linear function of the information set, which in this case simplifies to $y_{1,t}^* = \\psi_0 z_{t-1}$.\n\n---\n\n### Question\n\nWhat is the correct expression for the coefficient $\\psi_0$ that is consistent with the Rational Expectations Hypothesis?", "model_evaluation_result": {"model_raw_response": "I need to find the correct expression for $\\psi_0$ under the Rational Expectations Hypothesis.\n\nGiven information:\n- Reduced form: $y_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t}$\n- Exogenous process: $z_{t} = A_{1} z_{t-1} + \\varepsilon_{t}$\n- Under REH: $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}] = \\psi_0 z_{t-1}$\n\nTo find $\\psi_0$, I need to use the consistency condition that the rational expectation must equal the actual expected value of the endogenous variable.\n\nTaking expectations of the reduced form equation conditional on $\\Omega_{t-1}$:\n\n$E[y_{1,t} | \\Omega_{t-1}] = E[y_{1,t}^* \\pi_{1,0}^* | \\Omega_{t-1}] + E[z_{t} \\pi_1 | \\Omega_{t-1}] + E[v_{1,t} | \\Omega_{t-1}]$\n\nSince $y_{1,t}^*$ is known at $t-1$, $E[v_{1,t} | \\Omega_{t-1}] = 0$, and $E[z_{t} | \\Omega_{t-1}] = A_1 z_{t-1}$:\n\n$y_{1,t}^* = y_{1,t}^* \\pi_{1,0}^* + \\pi_1 A_1 z_{t-1}$\n\nSubstituting $y_{1,t}^* = \\psi_0 z_{t-1}$:\n\n$\\psi_0 z_{t-1} = \\psi_0 z_{t-1} \\pi_{1,0}^* + \\pi_1 A_1 z_{t-1}$\n\nDiv", "model_answer": "I need to find the correct expression for $\\psi_0$ under the Rational Expectations Hypothesis.\n\nGiven information:\n- Reduced form: $y_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t}$\n- Exogenous process: $z_{t} = A_{1} z_{t-1} + \\varepsilon_{t}$\n- Under REH: $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}] = \\psi_0 z_{t-1}$\n\nTo find $\\psi_0$, I need to use the consistency condition that the rational expectation must equal the actual expected value of the endogenous variable.\n\nTaking expectations of the reduced form equation conditional on $\\Omega_{t-1}$:\n\n$E[y_{1,t} | \\Omega_{t-1}] = E[y_{1,t}^* \\pi_{1,0}^* | \\Omega_{t-1}] + E[z_{t} \\pi_1 | \\Omega_{t-1}] + E[v_{1,t} | \\Omega_{t-1}]$\n\nSince $y_{1,t}^*$ is known at $t-1$, $E[v_{1,t} | \\Omega_{t-1}] = 0$, and $E[z_{t} | \\Omega_{t-1}] = A_1 z_{t-1}$:\n\n$y_{1,t}^* = y_{1,t}^* \\pi_{1,0}^* + \\pi_1 A_1 z_{t-1}$\n\nSubstituting $y_{1,t}^* = \\psi_0 z_{t-1}$:\n\n$\\psi_0 z_{t-1} = \\psi_0 z_{t-1} \\pi_{1,0}^* + \\pi_1 A_1 z_{t-1}$\n\nDiv", "error": "Warning: Could not extract single choice letter from: I need to find the correct expression for $\\psi_0$..."}}
{"ID": 20, "Question": "### Background\n\n**Research Question.** This problem investigates how violating the \"structural symmetry\" assumption in the Melitz model can lead to omitted variable bias, potentially explaining the puzzle of why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nIf coefficients are heterogeneous across sectors, the theoretical difference-in-differences (DiD) comparison for a unilateral import liberalization between a more-liberalized sector A and a less-liberalized sector B is:\n\n```latex\nd\\ln\\Phi_{1A}-d\\ln\\Phi_{1B}=\\xi_{1B}(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})+(\\xi_{1A}-\\xi_{1B})d\\ln\\tau_{21A}-(\\xi_{3A}-\\xi_{3B})d\\ln w_{1}\n```\n(Eq. 1)\n\n### Question\n\nAn econometrician tries to estimate the effect of import tariff cuts on productivity using a simple DiD regression of `(d\\ln\\Phi_{1A}-d\\ln\\Phi_{1B})` on `(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})`. Assume the direct tariff effect is homogeneous (`\\xi_{1A} = \\xi_{1B} = \\xi_1 > 0`), but sensitivity to wages is not. Consider a scenario where:\n1. Unilateral liberalization causes the general equilibrium wage to fall (`d\\ln w_1 < 0`).\n2. The largest tariff cuts (`d\\ln\\tau_{21A}` is most negative) were given to the most labor-intensive sectors, which are also most sensitive to wage changes (`\\xi_{3A} > \\xi_{3B}`).\n\nUnder this scenario, what is the nature of the omitted variable bias, and what is its implication for the estimated coefficient `\\hat{\\beta}`?", "Options": {"A": "The bias is negative. This could cause the estimated `\\hat{\\beta}` to be negative even if the true causal effect `\\xi_1` is positive, potentially resolving the puzzle.", "B": "The bias is positive. This would make the estimated `\\hat{\\beta}` even more positive than the true `\\xi_1`, deepening the puzzle.", "C": "The bias is negative, but this would only make the estimated `\\hat{\\beta}` smaller than the true `\\xi_1` while remaining positive, thus failing to resolve the puzzle.", "D": "There is no bias because the DiD specification perfectly controls for the wage shock `d\\ln w_1`."}, "Answer": "A", "pi_justification": "This item uses a Scenario Application strategy to test the ability to perform a multi-step omitted variable bias analysis, which was the core of the original QA's third question. The distractors are designed based on predictable errors: B results from a sign error in calculating the covariance term of the bias formula; C shows a misunderstanding of how a large bias can flip the sign of an estimate; D incorrectly applies the DiD logic when its key identifying assumption is explicitly violated.", "qid": "20", "question": "### Background\n\n**Research Question.** This problem investigates how violating the \"structural symmetry\" assumption in the Melitz model can lead to omitted variable bias, potentially explaining the puzzle of why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nIf coefficients are heterogeneous across sectors, the theoretical difference-in-differences (DiD) comparison for a unilateral import liberalization between a more-liberalized sector A and a less-liberalized sector B is:\n\n```latex\nd\\ln\\Phi_{1A}-d\\ln\\Phi_{1B}=\\xi_{1B}(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})+(\\xi_{1A}-\\xi_{1B})d\\ln\\tau_{21A}-(\\xi_{3A}-\\xi_{3B})d\\ln w_{1}\n```\n(Eq. 1)\n\n### Question\n\nAn econometrician tries to estimate the effect of import tariff cuts on productivity using a simple DiD regression of `(d\\ln\\Phi_{1A}-d\\ln\\Phi_{1B})` on `(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})`. Assume the direct tariff effect is homogeneous (`\\xi_{1A} = \\xi_{1B} = \\xi_1 > 0`), but sensitivity to wages is not. Consider a scenario where:\n1. Unilateral liberalization causes the general equilibrium wage to fall (`d\\ln w_1 < 0`).\n2. The largest tariff cuts (`d\\ln\\tau_{21A}` is most negative) were given to the most labor-intensive sectors, which are also most sensitive to wage changes (`\\xi_{3A} > \\xi_{3B}`).\n\nUnder this scenario, what is the nature of the omitted variable bias, and what is its implication for the estimated coefficient `\\hat{\\beta}`?", "model_evaluation_result": {"model_raw_response": "I need to analyze the omitted variable bias in this difference-in-differences regression.\n\nGiven equation:\n```\nd\\ln\\Phi_{1A}-d\\ln\\Phi_{1B}=\\xi_{1B}(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})+(\\xi_{1A}-\\xi_{1B})d\\ln\\tau_{21A}-(\\xi_{3A}-\\xi_{3B})d\\ln w_{1}\n```\n\nThe econometrician regresses `(d\\ln\\Phi_{1A}-d\\ln\\Phi_{1B})` on `(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})`, but omits the wage term.\n\nGiven assumptions:\n- Direct tariff effect is homogeneous: `ξ₁ₐ = ξ₁ᵦ = ξ₁ > 0`\n- This means `(ξ₁ₐ - ξ₁ᵦ) = 0`, so the second term drops out\n- The equation simplifies to: `d\\ln\\Phi_{1A}-d\\ln\\Phi_{1B} = ξ₁(d\\ln\\tau_{21A}-d\\ln\\tau_{21B}) - (ξ₃ₐ-ξ₃ᵦ)d\\ln w₁`\n\nThe omitted variable is the wage term: `-(ξ₃ₐ-ξ₃ᵦ)d\\ln w₁`\n\nFor omitted variable bias, I need to consider the correlation between the omitted variable and the included regressor.\n\nGiven conditions:\n1. `d\\ln w₁ < 0` (wages fall)\n2. Largest tariff cuts (most negative `d\\ln\\tau_{21A}`) went to most labor-intensive sectors\n3. Most labor-intensive sectors are most sensitive to wages: `ξ₃ₐ > ξ₃ᵦ`\n\nSo `(ξ₃ₐ-ξ", "model_answer": "I need to analyze the omitted variable bias in this difference-in-differences regression.\n\nGiven equation:\n```\nd\\ln\\Phi_{1A}-d\\ln\\Phi_{1B}=\\xi_{1B}(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})+(\\xi_{1A}-\\xi_{1B})d\\ln\\tau_{21A}-(\\xi_{3A}-\\xi_{3B})d\\ln w_{1}\n```\n\nThe econometrician regresses `(d\\ln\\Phi_{1A}-d\\ln\\Phi_{1B})` on `(d\\ln\\tau_{21A}-d\\ln\\tau_{21B})`, but omits the wage term.\n\nGiven assumptions:\n- Direct tariff effect is homogeneous: `ξ₁ₐ = ξ₁ᵦ = ξ₁ > 0`\n- This means `(ξ₁ₐ - ξ₁ᵦ) = 0`, so the second term drops out\n- The equation simplifies to: `d\\ln\\Phi_{1A}-d\\ln\\Phi_{1B} = ξ₁(d\\ln\\tau_{21A}-d\\ln\\tau_{21B}) - (ξ₃ₐ-ξ₃ᵦ)d\\ln w₁`\n\nThe omitted variable is the wage term: `-(ξ₃ₐ-ξ₃ᵦ)d\\ln w₁`\n\nFor omitted variable bias, I need to consider the correlation between the omitted variable and the included regressor.\n\nGiven conditions:\n1. `d\\ln w₁ < 0` (wages fall)\n2. Largest tariff cuts (most negative `d\\ln\\tau_{21A}`) went to most labor-intensive sectors\n3. Most labor-intensive sectors are most sensitive to wages: `ξ₃ₐ > ξ₃ᵦ`\n\nSo `(ξ₃ₐ-ξ", "error": "Warning: Could not extract single choice letter from: I need to analyze the omitted variable bias in thi..."}}
{"ID": 70, "Question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nBased on the results in Table 1, which statement best characterizes the trade-off between the restricted test (`Ȳ⁽²⁾`) and the unrestricted test (`Υ_W⁽²⁾`)?", "Options": {"A": "The restricted test is more powerful when its assumption (`θ₁=θ₂`) holds true, but can be severely less powerful when the assumption is false.", "B": "The unrestricted test is always more powerful than the restricted test because it does not impose false constraints.", "C": "The restricted test is more powerful when `θ₁=θ₂` and has only slightly less power when the assumption is false.", "D": "Since both tests have the correct empirical size, the choice between them is arbitrary and does not significantly affect inference."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to interpret Monte Carlo evidence to understand the fundamental power-efficiency trade-off in hypothesis testing. It uses a Scenario Application strategy. Distractor B is a Conceptual Opposite, ignoring the efficiency gain of a correct restriction. Distractor C is an 'Almost Right' statement that understates the severe power loss documented in the table (e.g., for `θ₁=0.3, θ₂=-0.3`). Distractor D is 'True but Irrelevant,' as it correctly notes the size but incorrectly dismisses the large power differences.", "qid": "70", "question": "### Background\n\n**Research Question.** This problem explores the power-efficiency trade-off between a restricted and an unrestricted joint test for fractional integration in a multi-factor model, a key consideration for applied researchers.\n\n**Setting.** A Monte Carlo experiment uses a two-factor GARMA model to generate data: `(1-2cos(0.15)L+L²)¹⁺ᶿ¹(1-2cos(π/2)L+L²)¹⁺ᶿ² xₜ = εₜ`, with `T=100`. The null hypothesis is `H₀: θ₁=θ₂=0`. Two tests are compared:\n1.  **Unrestricted Test (`Υ_W⁽²⁾`)**: Tests `H₀: φ₁=0, φ₂=0`, distributed as `χ²(2)`.\n2.  **Restricted Test (`Ȳ⁽²⁾`)**: Imposes the restriction `θ₁=θ₂` and tests a single parameter, distributed as `χ²(1)`.\n\n**Variables & Parameters.**\n- `θ₁, θ₂`: Deviations of the true integration orders from the null value of 1 at frequencies `γ₁=0.15` and `γ₂=π/2`.\n- Rejection Frequency: Empirical power at the 5% significance level.\n\n### Data / Model Specification\n\n**Table 1: Empirical Rejection Frequencies, T=100**\n\n| `θ₁` | `θ₂` | Restricted Test `Ȳ⁽²⁾` | Unrestricted Test `Υ_W⁽²⁾` |\n|:---|:---|---:|---:|\n| 0.0 | 0.0 | **0.047** | **0.049** |\n| -0.1 | -0.1 | 0.611 | 0.502 |\n| -0.2 | -0.2 | 0.992 | 0.978 |\n| -0.1 | 0.0 | 0.398 | 0.487 |\n| 0.0 | -0.1 | 0.082 | 0.161 |\n| 0.3 | -0.3 | 0.247 | 0.999 |\n\n*Source: Table 2 from the paper. Empirical size is in bold.*\n\n### Question\n\nBased on the results in Table 1, which statement best characterizes the trade-off between the restricted test (`Ȳ⁽²⁾`) and the unrestricted test (`Υ_W⁽²⁾`)?", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to analyze when each test performs better and identify the key trade-off.\n\nFirst, let me examine cases where the restriction θ₁=θ₂ holds true:\n- When θ₁=θ₂=0: Both tests have correct size (0.047 vs 0.049)\n- When θ₁=θ₂=-0.1: Restricted test has power 0.611 vs unrestricted 0.502\n- When θ₁=θ₂=-0.2: Restricted test has power 0.992 vs unrestricted 0.978\n\nSo when the restriction holds, the restricted test is more powerful.\n\nNow let me examine cases where the restriction θ₁=θ₂ is violated:\n- When θ₁=-0.1, θ₂=0: Restricted test has power 0.398 vs unrestricted 0.487\n- When θ₁=0, θ₂=-0.1: Restricted test has power 0.082 vs unrestricted 0.161\n- When θ₁=0.3, θ₂=-0.3: This is the most dramatic case - restricted test has power 0.247 vs unrestricted 0.999\n\nThe last case is particularly striking - when θ₁ and θ₂ have opposite signs of equal magnitude, the restricted test has very low power (0.247) while the unrestricted test has nearly perfect power (0.999). This represents a severe loss of power for the restricted test.\n\nThis pattern shows the classic power-efficiency trade-off: the restricted test gains efficiency (higher power) when its assumption is correct, but can suffer severe power loss when the assumption is violated.\n\nA", "model_answer": "A", "error": null}}
{"ID": 108, "Question": "### Background\n\nOne proposed solution to the persistence problem is to assume firms face a **convex demand curve**, where demand becomes more elastic as the firm raises its price. This can make firms' optimal prices less sensitive to changes in marginal cost.\n\n---\n\n### Data / Model Specification\n\nThe sensitivity of a firm's price to its marginal cost is captured by a pass-through parameter `\\varphi`:\n```latex\n\\varphi=\\frac{1-(1/\\varepsilon)}{2-(\\chi/\\varepsilon)}\n```\n(Eq. 1)\nwhere `\\varepsilon` is the steady-state demand elasticity and `\\chi` is a parameter governing demand curvature (more negative `\\chi` means more convexity). A second-order Taylor expansion of the demand function `d(P)` around the steady-state relative price of 1 is:\n```latex\nd(P) \\approx 1 - \\varepsilon(P-1) + \\frac{\\varepsilon \\chi}{2}(P-1)^2\n```\n(Eq. 2)\n\n---\n\n### Question\n\nThe paper calculates that to achieve even a modest contract multiplier, a demand curvature of `\\chi = -289` is required. Using the demand approximation in Eq. (2) with a steady-state demand elasticity `\\varepsilon = 10`, what is the approximate percentage drop in a firm's demand if it raises its relative price by 2.3% (i.e., `P=1.023`)?", "Options": {"A": "Demand falls by 23%.", "B": "Demand falls by 33%.", "C": "Demand falls by 99%.", "D": "Demand falls by 76%."}, "Answer": "C", "pi_justification": "This item assesses the ability to perform a quantitative critique of a theoretical model. It uses a Computational Judgment strategy. The correct answer requires applying a second-order Taylor expansion correctly. Distractor A (23%) results from ignoring the second-order term, a step-omission error. Distractor B (33%) results from a formula misuse error (failing to square the deviation in the second term). Distractor D (76%) results from ignoring the first-order term.", "qid": "108", "question": "### Background\n\nOne proposed solution to the persistence problem is to assume firms face a **convex demand curve**, where demand becomes more elastic as the firm raises its price. This can make firms' optimal prices less sensitive to changes in marginal cost.\n\n---\n\n### Data / Model Specification\n\nThe sensitivity of a firm's price to its marginal cost is captured by a pass-through parameter `\\varphi`:\n```latex\n\\varphi=\\frac{1-(1/\\varepsilon)}{2-(\\chi/\\varepsilon)}\n```\n(Eq. 1)\nwhere `\\varepsilon` is the steady-state demand elasticity and `\\chi` is a parameter governing demand curvature (more negative `\\chi` means more convexity). A second-order Taylor expansion of the demand function `d(P)` around the steady-state relative price of 1 is:\n```latex\nd(P) \\approx 1 - \\varepsilon(P-1) + \\frac{\\varepsilon \\chi}{2}(P-1)^2\n```\n(Eq. 2)\n\n---\n\n### Question\n\nThe paper calculates that to achieve even a modest contract multiplier, a demand curvature of `\\chi = -289` is required. Using the demand approximation in Eq. (2) with a steady-state demand elasticity `\\varepsilon = 10`, what is the approximate percentage drop in a firm's demand if it raises its relative price by 2.3% (i.e., `P=1.023`)?", "model_evaluation_result": {"model_raw_response": "I need to calculate the percentage drop in demand when a firm raises its relative price by 2.3% using the given demand approximation.\n\nGiven:\n- χ = -289\n- ε = 10\n- P = 1.023 (2.3% increase from steady state of 1)\n- Demand approximation: d(P) ≈ 1 - ε(P-1) + (εχ/2)(P-1)²\n\nLet me substitute the values:\n\nFirst, calculate (P-1):\nP - 1 = 1.023 - 1 = 0.023\n\nNow substitute into the demand function:\nd(P) ≈ 1 - 10(0.023) + (10 × (-289)/2)(0.023)²\n\nCalculate each term:\n- First term: 1\n- Second term: -10 × 0.023 = -0.23\n- Third term: (10 × (-289)/2) × (0.023)² = (-1445) × 0.000529 = -0.764\n\nTherefore:\nd(P) ≈ 1 - 0.23 - 0.764 = 0.006\n\nThis means demand falls to about 0.6% of its original level, which represents a drop of approximately 99.4%.\n\nRounding to the nearest option, this is a 99% drop in demand.\n\nC", "model_answer": "C", "error": null}}
{"ID": 30, "Question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Equilibrium Conditions:** For a given safe rate `R_0`, a marginal type `p* = 1 - sqrt(R_0/γ)` separates non-monitoring (`p ≤ p*`) from monitoring (`p > p*`) banks. For `p > p*`, banks choose an optimal monitoring intensity `m_p* > 0` determined by the first-order condition:\n  ```latex\nc''(m_p) - \\frac{R_0}{(1-p+m_p)^2} = 0 \\quad \\text{(Eq. 1)}\n  ```\n\n### Question\n\nUnder the quadratic cost assumption, what is the equilibrium probability of failure (`p - m_p*`) for a monitoring bank lending to an entrepreneur of type `p > p*`?", "Options": {"A": "`p*`", "B": "`p`", "C": "`0`", "D": "`p - p*`"}, "Answer": "A", "pi_justification": "Assesses understanding of the model's key result for the intensive margin of risk. Strategy: Computational Judgment. Distractors represent common conceptual errors: B (confusing failure probability with the entrepreneur's baseline type), C (assuming monitoring eliminates all risk), and D (providing the formula for monitoring intensity `m_p*` instead of the failure probability `p - m_p*`).", "qid": "30", "question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Equilibrium Conditions:** For a given safe rate `R_0`, a marginal type `p* = 1 - sqrt(R_0/γ)` separates non-monitoring (`p ≤ p*`) from monitoring (`p > p*`) banks. For `p > p*`, banks choose an optimal monitoring intensity `m_p* > 0` determined by the first-order condition:\n  ```latex\nc''(m_p) - \\frac{R_0}{(1-p+m_p)^2} = 0 \\quad \\text{(Eq. 1)}\n  ```\n\n### Question\n\nUnder the quadratic cost assumption, what is the equilibrium probability of failure (`p - m_p*`) for a monitoring bank lending to an entrepreneur of type `p > p*`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to find the equilibrium probability of failure for a monitoring bank lending to an entrepreneur of type p > p*.\n\nGiven information:\n- Entrepreneur type p has baseline failure probability p\n- Bank monitoring m reduces failure probability to p - m (success probability becomes 1 - p + m)\n- Monitoring cost: c(m) = (γ/2)m²\n- Marginal type: p* = 1 - √(R₀/γ)\n- For p > p*, banks choose optimal monitoring m_p* from the FOC: c'(m_p*) = R₀/(1-p+m_p*)²\n\nLet me solve for the optimal monitoring level.\n\nThe first-order condition is:\nc'(m_p*) = R₀/(1-p+m_p*)²\n\nSince c(m) = (γ/2)m², we have c'(m) = γm.\n\nSo: γm_p* = R₀/(1-p+m_p*)²\n\nRearranging: γm_p*(1-p+m_p*)² = R₀\n\nThis is a complex equation, but I can use the definition of p* to simplify.\n\nFrom p* = 1 - √(R₀/γ), we get:\n√(R₀/γ) = 1 - p*\nR₀/γ = (1 - p*)²\nR₀ = γ(1 - p*)²\n\nSubstituting back into the FOC:\nγm_p*(1-p+m_p*)² = γ(1 - p*)²\nm_p*(1-p+m_p*)² = (1 - p*)²\n\nTaking the square root:\n√(m_p*) · (1-p+m_p*) = 1 - p*\n\nThis suggests that the equilibrium failure probability p - m_p* equals p*.\n\nTo verify: if p - m_p* = p*, then the success probability is 1 - p + m_p* = 1 - p*, which matches the marginal type's success probability. This makes economic sense as", "model_answer": "Looking at this problem, I need to find the equilibrium probability of failure for a monitoring bank lending to an entrepreneur of type p > p*.\n\nGiven information:\n- Entrepreneur type p has baseline failure probability p\n- Bank monitoring m reduces failure probability to p - m (success probability becomes 1 - p + m)\n- Monitoring cost: c(m) = (γ/2)m²\n- Marginal type: p* = 1 - √(R₀/γ)\n- For p > p*, banks choose optimal monitoring m_p* from the FOC: c'(m_p*) = R₀/(1-p+m_p*)²\n\nLet me solve for the optimal monitoring level.\n\nThe first-order condition is:\nc'(m_p*) = R₀/(1-p+m_p*)²\n\nSince c(m) = (γ/2)m², we have c'(m) = γm.\n\nSo: γm_p* = R₀/(1-p+m_p*)²\n\nRearranging: γm_p*(1-p+m_p*)² = R₀\n\nThis is a complex equation, but I can use the definition of p* to simplify.\n\nFrom p* = 1 - √(R₀/γ), we get:\n√(R₀/γ) = 1 - p*\nR₀/γ = (1 - p*)²\nR₀ = γ(1 - p*)²\n\nSubstituting back into the FOC:\nγm_p*(1-p+m_p*)² = γ(1 - p*)²\nm_p*(1-p+m_p*)² = (1 - p*)²\n\nTaking the square root:\n√(m_p*) · (1-p+m_p*) = 1 - p*\n\nThis suggests that the equilibrium failure probability p - m_p* equals p*.\n\nTo verify: if p - m_p* = p*, then the success probability is 1 - p + m_p* = 1 - p*, which matches the marginal type's success probability. This makes economic sense as", "error": "Warning: Could not extract single choice letter from: Looking at this problem, I need to find the equili..."}}
{"ID": 68, "Question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nWhat is the primary identification problem that the repeat-sales methodology, which transforms Eq. (1) into Eq. (2), is designed to solve?", "Options": {"A": "It controls for selection bias caused by changes in the mix of properties sold over time.", "B": "It corrects for measurement error in the reported sales price `p_{it}`.", "C": "It accounts for heteroskedasticity where price variance increases with the time between sales.", "D": "It eliminates the influence of citywide economic shocks `δ_t` on individual house prices."}, "Answer": "A", "pi_justification": "This item assesses the student's understanding of the fundamental purpose of the repeat-sales (fixed effects) methodology, which is central to the paper. It uses a conceptual strategy. The correct answer (A) identifies selection bias as the key problem. Distractors are designed based on common confusions: (B) is a different econometric problem (True but Irrelevant), (C) refers to a secondary weighting step, not the core differencing transformation (Conceptual: Almost Right), and (D) is the conceptual opposite of the method's goal.", "qid": "68", "question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nWhat is the primary identification problem that the repeat-sales methodology, which transforms Eq. (1) into Eq. (2), is designed to solve?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand what identification problem the repeat-sales methodology solves by transforming from Equation (1) to Equation (2).\n\nIn Equation (1): p_{it} = ν_i + δ_t + ε_{it}\n\nThe key issue is that we cannot directly estimate the time effects δ_t because the house-specific fixed effects ν_i are unobserved. If we tried to estimate δ_t from a cross-section of different houses sold in month t, our estimates would be confounded by the varying quality/characteristics of different houses (captured by ν_i).\n\nThe repeat-sales methodology solves this by taking differences for the same house sold at two different times:\n\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'}\n\nThis transformation eliminates the house-specific fixed effect ν_i, allowing clean identification of the time effects δ_t.\n\nLet me evaluate each option:\n\nA) This describes the core problem - without controlling for house-specific characteristics, changes in which properties sell over time would bias estimates of aggregate price movements. The repeat-sales method solves this by using the same properties.\n\nB) The methodology doesn't specifically address measurement error in prices.\n\nC) While heteroskedasticity might be a concern, it's not the primary identification problem the method was designed to solve.\n\nD) The method actually aims to isolate and measure the citywide shocks δ_t, not eliminate them.\n\nThe primary identification problem is that without controlling for the unobserved heterogeneity across properties (ν_i), estimates of aggregate price trends would be biased by changes in the composition of properties sold over time.\n\nA", "model_answer": "A", "error": null}}
{"ID": 69, "Question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nSuppose that during a housing boom, homeowners frequently undertake unobserved major renovations (e.g., new kitchens) between the sale at time `t'` and the sale at time `t`. How would this behavior affect the price appreciation (`δ_t - δ_{t'}`) estimated using the repeat-sales model (Eq. 2)?", "Options": {"A": "The model would overstate true price appreciation because it would incorrectly attribute price gains from quality improvements to market-wide price growth.", "B": "The model would understate true price appreciation because the cost of renovations is not factored into the sale price.", "C": "The estimate would be unbiased because the unobserved renovations are random and would be absorbed by the error term `ε_{it} - ε_{it'}` which has an expected value of zero.", "D": "The effect is ambiguous because some renovations add more value than others, making the direction of the bias impossible to determine without more data."}, "Answer": "A", "pi_justification": "This item tests the student's ability to analyze a key threat to the validity of the repeat-sales identification strategy. It uses a Scenario Application strategy. The correct answer (A) correctly identifies the direction of the bias and the reason. Distractors are based on plausible errors: (B) introduces irrelevant cost reasoning (Formula Misuse), (C) incorrectly assumes the violation of the assumption away (Conceptual Opposite), and (D) offers a weak 'ambiguous' conclusion when a clear directional prediction can be made (Conceptual: Almost Right).", "qid": "69", "question": "### Background\n\n**Research Question.** This problem examines the econometric methodology for constructing a constant-quality house price index that controls for time-invariant heterogeneity across properties, as pioneered by Case and Shiller.\n\n**Setting / Institutional Environment.** The analysis is based on the repeat-sales methodology. This approach uses data on properties that have sold at least twice to isolate aggregate price movements from changes in the specific mix of homes sold over time, which would otherwise bias a simple average of prices.\n\n**Variables & Parameters.**\n- `p_{it}`: Log sales price of house `i` sold in month `t`.\n- `ν_i`: A time-invariant, house-specific fixed effect, capturing all unique characteristics of house `i` (e.g., location, size, quality).\n- `δ_t`: A month fixed effect representing the citywide log price level in month `t`. The sequence of estimated `δ_t` forms the house price index.\n- `ε_{it}`: An idiosyncratic error term for house `i` in month `t`.\n- `t'`: The month of the *previous* sale of house `i`.\n\n---\n\n### Data / Model Specification\n\nThe price of house `i` sold at time `t` is modeled as:\n```latex\np_{it} = ν_i + δ_t + ε_{it} \\quad \\text{(Eq. 1)}\n```\nTo estimate the time effects, `δ_t`, the model is transformed by observing the same house `i` sold at two different times, `t` and `t'`:\n```latex\np_{it} - p_{it'} = δ_t - δ_{t'} + ε_{it} - ε_{it'} \\quad \\text{(Eq. 2)}\n```\n\n---\n\nSuppose that during a housing boom, homeowners frequently undertake unobserved major renovations (e.g., new kitchens) between the sale at time `t'` and the sale at time `t`. How would this behavior affect the price appreciation (`δ_t - δ_{t'}`) estimated using the repeat-sales model (Eq. 2)?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze how unobserved renovations between sales would affect the repeat-sales model's estimate of price appreciation.\n\nThe key insight is understanding what the repeat-sales model is trying to measure versus what actually happens when renovations occur.\n\nThe model assumes that the difference in prices for the same house sold at two different times reflects only:\n1. Market-wide price changes (δₜ - δₜ')\n2. Random error (εᵢₜ - εᵢₜ')\n\nHowever, if major renovations occur between sales, the house at time t is fundamentally different (higher quality) than the same house at time t'. The renovation effectively changes the house-specific component νᵢ.\n\nWhen the model observes:\npᵢₜ - pᵢₜ' = (νᵢ + renovation value) + δₜ + εᵢₜ - (νᵢ + δₜ' + εᵢₜ')\n\nThis simplifies to:\npᵢₜ - pᵢₜ' = renovation value + δₜ - δₜ' + εᵢₜ - εᵢₜ'\n\nThe model interprets this entire price difference as δₜ - δₜ' (plus error), but part of it is actually due to quality improvements from renovations. This means the estimated market-wide price appreciation δₜ - δₜ' will be inflated by the value added from renovations.\n\nSince this happens systematically during housing booms (not randomly), the bias will be consistent and upward.\n\nA", "model_answer": "A", "error": null}}
{"ID": 97, "Question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`). Production in each industry is subject to industry-level external economies of scale. With labor's wage as the numeraire, the average cost of production for good `j` is `AC_j = y_j^{-1/2}`. The relative supply price is therefore:\n```latex\n\\frac{AC_2}{AC_1} = \\left(\\frac{y_1}{y_2}\\right)^{1/2} \\quad \\text{(Eq. 1)}\n```\nIn autarky, two identical countries are at a symmetric equilibrium `C_0` where `y_1 = y_2`. At this point, the relative supply price is 1, matching the world demand price ratio. \n\n### Question\n\nAccording to a Marshallian stability analysis, how should the symmetric equilibrium point `C_0` be characterized when the countries open to trade?", "Options": {"A": "Stable, because a small increase in the output of one good raises its average cost, creating an incentive to return to the symmetric equilibrium.", "B": "Unstable, because a small increase in the output of one good lowers its average cost, creating an incentive for further expansion until specialization is complete.", "C": "Neutrally stable, because at the world price ratio of 1, there is no incentive to move away from the symmetric production point.", "D": "Unstable, because the marginal cost of production is always lower than the world price, leading to infinite production."}, "Answer": "B", "pi_justification": "This item tests the user's ability to apply the concept of Marshallian stability to the specific model from the paper's Appendix. It uses a Reverse-Reasoning strategy, asking for the characterization of an equilibrium point. The distractors are based on predictable errors: A applies the incorrect stability logic from a standard CRS model, C correctly identifies the equilibrium condition but fails to perform the stability test, and D confuses the relevant cost concepts.", "qid": "97", "question": "### Background\n\nThis problem uses a specific example from the paper's Appendix to demonstrate a key, counter-intuitive result: in the presence of increasing returns to scale, two identical countries can gain from trade even if autarkic and free-trade commodity prices are identical.\n\n### Data / Model Specification\n\nConsider an economy with two industries (j=1, 2) and one factor of production, labor (`L`). Production in each industry is subject to industry-level external economies of scale. With labor's wage as the numeraire, the average cost of production for good `j` is `AC_j = y_j^{-1/2}`. The relative supply price is therefore:\n```latex\n\\frac{AC_2}{AC_1} = \\left(\\frac{y_1}{y_2}\\right)^{1/2} \\quad \\text{(Eq. 1)}\n```\nIn autarky, two identical countries are at a symmetric equilibrium `C_0` where `y_1 = y_2`. At this point, the relative supply price is 1, matching the world demand price ratio. \n\n### Question\n\nAccording to a Marshallian stability analysis, how should the symmetric equilibrium point `C_0` be characterized when the countries open to trade?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the stability of the symmetric equilibrium C₀ using Marshallian stability analysis.\n\nGiven information:\n- Average cost: AC_j = y_j^(-1/2)\n- Relative supply price: AC₂/AC₁ = (y₁/y₂)^(1/2)\n- At symmetric equilibrium C₀: y₁ = y₂, so relative price = 1\n- World demand price ratio = 1\n\nFor Marshallian stability analysis, I need to examine what happens when there's a small perturbation from the symmetric equilibrium.\n\nLet's say there's a small increase in y₁ (output of good 1):\n\n1) If y₁ increases slightly while y₂ stays constant:\n   - The ratio y₁/y₂ increases\n   - Therefore (y₁/y₂)^(1/2) increases\n   - This means AC₂/AC₁ increases, so the relative price of good 2 rises\n\n2) Since AC_j = y_j^(-1/2), when y₁ increases:\n   - AC₁ = y₁^(-1/2) decreases (average cost of good 1 falls)\n   - Good 1 becomes relatively cheaper to produce\n\n3) With increasing returns to scale (decreasing average costs), this creates a positive feedback:\n   - Lower average cost for good 1 makes it more profitable\n   - This incentivizes further expansion of good 1 production\n   - Which further reduces AC₁, creating more incentive to expand\n   - This process continues until complete specialization\n\nThis describes an unstable equilibrium - any small deviation from the symmetric point creates forces that push further away from equilibrium rather than back toward it.\n\nThe answer is B) Unstable, because a small increase in the output of one good lowers its average cost, creating an incentive for further expansion until specialization is complete.\n\nB", "model_answer": "B", "error": null}}
{"ID": 157, "Question": "### Background\n\n**Setting.** The paper's empirical analysis begins by estimating a regression of individual bids on various bond and auction characteristics to control for observable heterogeneity before estimating the structural auction model. A selection of results from this regression (Specification I) is shown in Table 1.\n\n### Data / Model Specification\n\n**Table 1: Selected Coefficients from First-Stage Bid Regression**\n| Variable                  | Coefficient | S.E.   |\n| :------------------------ | :---------- | :----- |\n| *(Low market uncertainty)*|             |        |\n| High market uncertainty   | -0.1775**   | .0173  |\n| *(AAA credit rating)*     |             |        |\n| A credit rating           | 6.7172**    | .5895  |\n| AA credit rating          | 5.3292**    | .5544  |\n\n*Note: `**p<.01`; reference case in parentheses.*\n\n### Question\n\nIn the bid regression results shown in Table 1, the coefficient on 'High market uncertainty' is negative and statistically significant. According to auction theory, what is the most likely explanation for this finding?", "Options": {"A": "Bidders become more risk-averse in uncertain markets and demand a higher premium.", "B": "The winner's curse is exacerbated, leading bidders to shade their bids more aggressively to avoid overpaying.", "C": "Higher uncertainty signals lower secondary market demand for the bond, reducing its fundamental value.", "D": "The affiliation between bidders' signals weakens, making it harder to infer the bond's value from competitors' behavior."}, "Answer": "B", "pi_justification": "This item assesses understanding of the winner's curse, a core concept in auction theory. It uses a Reverse-Reasoning strategy, asking for the theoretical cause of an empirical finding. Distractor A (Risk Aversion) is a plausible but distinct concept. Distractor C (Reduced Demand) offers a macro-level explanation for a micro-strategic phenomenon. Distractor D (Affiliation) mentions a related concept but misidentifies its role, making it an 'Almost Right' option.", "qid": "157", "question": "### Background\n\n**Setting.** The paper's empirical analysis begins by estimating a regression of individual bids on various bond and auction characteristics to control for observable heterogeneity before estimating the structural auction model. A selection of results from this regression (Specification I) is shown in Table 1.\n\n### Data / Model Specification\n\n**Table 1: Selected Coefficients from First-Stage Bid Regression**\n| Variable                  | Coefficient | S.E.   |\n| :------------------------ | :---------- | :----- |\n| *(Low market uncertainty)*|             |        |\n| High market uncertainty   | -0.1775**   | .0173  |\n| *(AAA credit rating)*     |             |        |\n| A credit rating           | 6.7172**    | .5895  |\n| AA credit rating          | 5.3292**    | .5544  |\n\n*Note: `**p<.01`; reference case in parentheses.*\n\n### Question\n\nIn the bid regression results shown in Table 1, the coefficient on 'High market uncertainty' is negative and statistically significant. According to auction theory, what is the most likely explanation for this finding?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 154, "Question": "### Background\n\nA study decomposes pollution changes into scale, composition, and technique effects. The main analysis covers 1987-2001. A robustness check extends the analysis to 1972-2001, but because industry-specific pollution intensity data (`z_i`) is not available for 1972, the calculation must use the 1987 intensity data for the entire period.\n\n### Data / Model Specification\n\n**Table 1: Fraction of Cleanup Due to Technique: Alternative Estimates**\n\n| Pollutant | Baseline (1987-2001) (a) | Deflating using PPI (b) | 1972-2001 (using 1987 intensities) (c) |\n| :--- | :--- | :--- | :--- |\n| All four | 0.78 | 0.69 | 0.62 |\n\n### Question\n\nThe analysis in column (c) of Table 1 extends the time period to 1972-2001 but must use 1987 pollution intensity data. For this calculation to be an unbiased estimate of the technique effect's share, which critical assumption about technological change from 1972 to 1987 must hold?", "Options": {"A": "Technological changes must have altered every industry's emissions intensity by an equal percentage.", "B": "Each industry's emissions intensity (the `z_i` values) must have remained constant over that period.", "C": "The composition of US manufacturing (the `θ_i` shares) must have remained stable between 1972 and 1987.", "D": "Only the most pollution-intensive industries experienced technological improvements in abatement."}, "Answer": "A", "pi_justification": "This item assesses deep understanding of the identification strategy and its underlying assumptions, a key skill in empirical analysis. It uses a Scenario Application strategy. The distractors represent classic conceptual errors: B confuses the actual assumption with a much stronger (and obviously false) one, C confuses a condition on technology with a condition on composition, and D represents a specific scenario that would explicitly violate the required assumption.", "qid": "154", "question": "### Background\n\nA study decomposes pollution changes into scale, composition, and technique effects. The main analysis covers 1987-2001. A robustness check extends the analysis to 1972-2001, but because industry-specific pollution intensity data (`z_i`) is not available for 1972, the calculation must use the 1987 intensity data for the entire period.\n\n### Data / Model Specification\n\n**Table 1: Fraction of Cleanup Due to Technique: Alternative Estimates**\n\n| Pollutant | Baseline (1987-2001) (a) | Deflating using PPI (b) | 1972-2001 (using 1987 intensities) (c) |\n| :--- | :--- | :--- | :--- |\n| All four | 0.78 | 0.69 | 0.62 |\n\n### Question\n\nThe analysis in column (c) of Table 1 extends the time period to 1972-2001 but must use 1987 pollution intensity data. For this calculation to be an unbiased estimate of the technique effect's share, which critical assumption about technological change from 1972 to 1987 must hold?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 4, "Question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant. According to the paper's theory, the data are marginally uninformative about `\\nu = F(\\lambda|\\psi)`, where `F` is the CDF corresponding to the prior in Eq. (2).\n\n### Question\n\nWhich of the following quantities is a one-to-one function of `\\nu`, implying that all identifying information for it comes entirely from the prior specification?", "Options": {"A": "The reduced form coefficient on the instrument, `\\pi_2`.", "B": "The structural coefficient on the instrument, `\\pi_1 - \\lambda\\pi_2`.", "C": "The structural parameter of interest, `\\lambda`.", "D": "The instrument's noncentrality parameter, `\\pi_2^2 x'x / \\omega_{22}`."}, "Answer": "B", "pi_justification": "Assessment Target: Tests the ability to identify the specific parametric function for which the data are uninformative in the simultaneous equations model. Strategy: Reverse-Reasoning. The user must derive the transformation from `\\nu` to the options. Distractor Logic: A) is a directly identified parameter (Conceptual Opposite). C) is the ultimate parameter of interest, but not the direct transformation of `\\nu` (Almost Right). D) is a component of the prior variance, not the uninformative quantity itself (True but Irrelevant).", "qid": "4", "question": "### Background\n\n**Research Question.** In a simultaneous equations model that is not identified via a traditional exclusion restriction, how can Bayesian priors be used to achieve a form of \"stochastic identification\"?\n\n**Setting.** Consider a two-equation system where `y_{t1}` and `y_{t2}` are endogenous variables and `x_t` is an exogenous instrument. The model is specified via its reduced form.\n\n**Variables and Parameters.**\n- `y_{t1}, y_{t2}`: Endogenous variables at time `t`.\n- `x_t`: Exogenous instrument at time `t`.\n- `\\pi_1, \\pi_2`: Reduced form coefficients.\n- `\\Omega`: `2`x`2` covariance matrix of reduced form errors, with elements `\\omega_{ij}`.\n- `\\lambda`: The structural parameter of interest, representing the effect of `y_{t2}` on `y_{t1}`.\n- `\\psi = [\\pi_1, \\pi_2, \\omega_{11}, \\omega_{12}, \\omega_{22}]'`: The vector of identified reduced form parameters.\n\n### Data / Model Specification\n\nThe reduced form of the system is:\n```latex\ny_{t1} = \\pi_1 x_t + u_{t1}\ny_{t2} = \\pi_2 x_t + u_{t2}\n```\nThe corresponding structural equation for `y_{t1}` is:\n```latex\ny_{t1} = \\lambda y_{t2} + (\\pi_1 - \\lambda\\pi_2)x_t + (u_{t1} - \\lambda u_{t2}) \\quad \\text{(Eq. 1)}\n```\nThe parameter `\\lambda` is identified if `\\pi_1 - \\lambda\\pi_2 = 0`. A Bayesian approach can impose this restriction stochastically via a conditional prior for `\\lambda` given `\\psi`:\n```latex\nf(\\lambda|\\psi) = \\phi\\left(\\lambda \\bigg| \\frac{\\pi_1}{\\pi_2}, \\frac{c\\omega_{22}}{\\pi_2^2 x'x}\\right) \\quad \\text{(Eq. 2)}\n```\nwhere `\\phi(\\cdot|\\mu,\\sigma^2)` is a normal PDF and `c` is a known positive constant. According to the paper's theory, the data are marginally uninformative about `\\nu = F(\\lambda|\\psi)`, where `F` is the CDF corresponding to the prior in Eq. (2).\n\n### Question\n\nWhich of the following quantities is a one-to-one function of `\\nu`, implying that all identifying information for it comes entirely from the prior specification?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to identify which quantity is a one-to-one function of ν = F(λ|ψ), where F is the CDF of the conditional prior for λ given ψ.\n\nFrom equation (2), the prior is:\nf(λ|ψ) = φ(λ | π₁/π₂, cω₂₂/(π₂²x'x))\n\nThis means λ follows a normal distribution conditional on ψ, with mean π₁/π₂ and variance cω₂₂/(π₂²x'x).\n\nSince ν = F(λ|ψ) is the CDF of this distribution, and the CDF of a continuous distribution is strictly monotonic, there is a one-to-one correspondence between λ and ν. Given any value of ψ, knowing ν uniquely determines λ through the inverse CDF: λ = F⁻¹(ν|ψ).\n\nLet me check each option:\n\nA) π₂ is part of the reduced form parameters ψ, which are identified from the data. It's not a function of ν.\n\nB) π₁ - λπ₂: While this involves λ (which is a function of ν), it also involves π₁ and π₂ from ψ. This is not purely a function of ν alone.\n\nC) λ: This is exactly what ν represents through the CDF transformation. There's a one-to-one relationship between λ and ν given ψ.\n\nD) π₂²x'x/ω₂₂: This involves only elements of ψ and the data matrix x'x, not ν.\n\nThe key insight is that according to the theory mentioned, \"the data are marginally uninformative about ν = F(λ|ψ).\" This means all identifying information about ν (and hence about λ) comes from the prior specification, not from the data.\n\nC", "model_answer": "A", "error": null}}
{"ID": 10, "Question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, what specific linear combination of the regression coefficients are the data marginally uninformative about?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` has rank `K-1` because `X_2 = X_1d`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe identified parameter is `\\psi = \\beta_1 + d\\beta_2 = [I_{K-1}, d] \\beta`. The data are marginally uninformative for any parameter that is a priori independent of `\\psi`. The specific uninformative quantity is `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c = [d', -1]'`. Independence between `\\psi` and `\\alpha` is established by showing their prior covariance is zero.\n\n### Question\n\nWhich calculation correctly demonstrates the a priori independence of `\\psi` and `\\alpha`?", "Options": {"A": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot (\\underline{Q}^{-1}c)' = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot c'\\underline{Q}^{-1} \\neq 0`", "B": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot (c'\\underline{Q}^{-1})' = [I_{K-1}, d] \\cdot \\underline{Q} \\cdot \\underline{Q}^{-1}c = [I_{K-1}, d]c = d-d=0`", "C": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot \\underline{Q}^{-1} \\cdot (c'\\underline{Q}^{-1})' = [I_{K-1}, d] \\cdot \\underline{Q}^{-1} \\cdot \\underline{Q}^{-1}c \\neq 0`", "D": "`Cov(\\psi, \\alpha) = [I_{K-1}, d] \\cdot c = d-d=0`, because the `\\underline{Q}` matrices are irrelevant."}, "Answer": "B", "pi_justification": "Assessment Target: Tests the core mathematical derivation for the uninformative quantity in the multicollinearity case. Strategy: Computational Judgment. The user must identify the correct algebraic steps. Distractor Logic: A) makes a mistake with the transpose rule for matrices. C) uses `Q⁻¹` instead of `Q` in the covariance formula (Formula Misuse). D) arrives at the right answer but omits the crucial `Q Q⁻¹ = I` cancellation, which is the heart of the proof (Step-Omission Error).", "qid": "10", "question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, what specific linear combination of the regression coefficients are the data marginally uninformative about?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` has rank `K-1` because `X_2 = X_1d`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe identified parameter is `\\psi = \\beta_1 + d\\beta_2 = [I_{K-1}, d] \\beta`. The data are marginally uninformative for any parameter that is a priori independent of `\\psi`. The specific uninformative quantity is `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c = [d', -1]'`. Independence between `\\psi` and `\\alpha` is established by showing their prior covariance is zero.\n\n### Question\n\nWhich calculation correctly demonstrates the a priori independence of `\\psi` and `\\alpha`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to find the correct calculation for the prior covariance between ψ and α.\n\nGiven:\n- ψ = [I_{K-1}, d]β  \n- α = c'Q̄^{-1}β where c = [d', -1]'\n- β ~ N(b̄, Q̄)\n\nFor two linear combinations of a multivariate normal random vector, the covariance is:\nCov(Aβ, Bβ) = A·Q̄·B'\n\nSo: Cov(ψ, α) = [I_{K-1}, d]·Q̄·(c'Q̄^{-1})'\n\nLet me work through the algebra:\n- (c'Q̄^{-1})' = (Q̄^{-1})' c = Q̄^{-1} c (since Q̄^{-1} is symmetric)\n\nTherefore:\nCov(ψ, α) = [I_{K-1}, d]·Q̄·Q̄^{-1}c = [I_{K-1}, d]c\n\nNow I need to compute [I_{K-1}, d]c where c = [d', -1]':\n\n[I_{K-1}, d]c = I_{K-1}·d + d·(-1) = d - d = 0\n\nLet me check each option:\n\nA) Has the transpose in the wrong place: (Q̄^{-1}c)' ≠ Q̄^{-1}c, and claims ≠ 0\nB) Correctly shows: Q̄·Q̄^{-1}c = c, then [I_{K-1}, d]c = d - d = 0 ✓\nC) Uses Q̄^{-1} instead of Q̄ in the covariance formula, which is incorrect\nD) Ignores the Q̄ matrices entirely, which is wrong since they're part of the covariance structure\n\nOption B follows the correct mathematical steps and arrives at the right conclusion that the covariance is zero.\n\nB", "model_answer": "A", "error": null}}
{"ID": 111, "Question": "## Background\n\nThis problem explores the geometric conditions that determine whether a fully revealing equilibrium (FRE) can exist in a constrained policy space, and how to constructively test for this existence. We consider a cheap talk game where the directions of the two senders' biases are deterministic, given by linearly independent vectors $b_1, b_2 \\in \\mathbb{R}^p$. The policy space $Y \\subseteq \\mathbb{R}^p$ is a closed, convex set.\n\n---\n\n## Data / Model Specification\n\nA key result (Proposition 6 in the paper) provides a constructive test for the existence of a FRE. It states that a FRE exists if and only if for any reports $\\theta', \\theta'' \\in Y$, the following set has a non-empty intersection with the policy space $Y$:\n\n```latex\nM_{b_{1},b_{2}}(\\theta^{\\prime},\\theta^{\\prime\\prime})=\\left\\{x\\in\\mathbb{R}^{p}\\middle|\\begin{array}{l}{b_{1}\\cdot x=\\operatorname*{min}\\{b_{1}\\cdot\\theta^{\\prime},b_{1}\\cdot\\theta^{\\prime\\prime}\\}}\\\\{b_{2}\\cdot x=\\operatorname*{min}\\{b_{2}\\cdot\\theta^{\\prime},b_{2}\\cdot\\theta^{\\prime\\prime}\\}}\\end{array}\\right\\} \\quad \\text{(Eq. 1)}\n```\n\nA receiver strategy that always picks a point in this intersection is called the \"min rule.\"\n\n---\n\n## Question\n\nConsider a scenario in $\\mathbb{R}^2$ with bias vectors $b_1=(1,0)$ and $b_2=(0,1)$. The senders issue conflicting reports $\\theta'=(5,2)$ and $\\theta''=(3,6)$. The feasible policy space is a circle defined by $Y = \\{y \\in \\mathbb{R}^2 | (y_1-1)^2 + (y_2-1)^2 \\le 4 \\}$. Based on the constructive test in Eq. (1), does a fully revealing equilibrium (FRE) exist in this environment?", "Options": {"A": "No, a FRE does not exist because the min-rule punishment point (3,2) is not a feasible policy.", "B": "Yes, a FRE exists because the min-rule punishment point (3,2) is a feasible policy.", "C": "Yes, a FRE exists because the origin (0,0) can serve as a punishment and is a feasible policy.", "D": "The existence of a FRE cannot be determined without knowing the magnitudes of the senders' biases."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the application of a core constructive theorem (the 'min rule'). It uses the 'Computational Judgment' strategy, requiring a multi-step process: 1) calculate the specific punishment point using the min rule formula, and 2) test if that point lies within the constrained policy space to determine if a FRE exists. The distractors are designed to diagnose specific errors: (B) represents an error in the final feasibility check (a calculation slip on the inequality); (C) represents a conceptual error, substituting a generic 'safe' point for the required min-rule point; (D) targets a fundamental misunderstanding of the model's core assumption that punishments must work for arbitrarily large biases, making the analysis independent of specific magnitudes.", "qid": "111", "question": "## Background\n\nThis problem explores the geometric conditions that determine whether a fully revealing equilibrium (FRE) can exist in a constrained policy space, and how to constructively test for this existence. We consider a cheap talk game where the directions of the two senders' biases are deterministic, given by linearly independent vectors $b_1, b_2 \\in \\mathbb{R}^p$. The policy space $Y \\subseteq \\mathbb{R}^p$ is a closed, convex set.\n\n---\n\n## Data / Model Specification\n\nA key result (Proposition 6 in the paper) provides a constructive test for the existence of a FRE. It states that a FRE exists if and only if for any reports $\\theta', \\theta'' \\in Y$, the following set has a non-empty intersection with the policy space $Y$:\n\n```latex\nM_{b_{1},b_{2}}(\\theta^{\\prime},\\theta^{\\prime\\prime})=\\left\\{x\\in\\mathbb{R}^{p}\\middle|\\begin{array}{l}{b_{1}\\cdot x=\\operatorname*{min}\\{b_{1}\\cdot\\theta^{\\prime},b_{1}\\cdot\\theta^{\\prime\\prime}\\}}\\\\{b_{2}\\cdot x=\\operatorname*{min}\\{b_{2}\\cdot\\theta^{\\prime},b_{2}\\cdot\\theta^{\\prime\\prime}\\}}\\end{array}\\right\\} \\quad \\text{(Eq. 1)}\n```\n\nA receiver strategy that always picks a point in this intersection is called the \"min rule.\"\n\n---\n\n## Question\n\nConsider a scenario in $\\mathbb{R}^2$ with bias vectors $b_1=(1,0)$ and $b_2=(0,1)$. The senders issue conflicting reports $\\theta'=(5,2)$ and $\\theta''=(3,6)$. The feasible policy space is a circle defined by $Y = \\{y \\in \\mathbb{R}^2 | (y_1-1)^2 + (y_2-1)^2 \\le 4 \\}$. Based on the constructive test in Eq. (1), does a fully revealing equilibrium (FRE) exist in this environment?", "model_evaluation_result": {"model_raw_response": "I need to analyze whether a fully revealing equilibrium (FRE) exists using the constructive test given in Equation 1.\n\nGiven information:\n- Bias vectors: $b_1 = (1,0)$ and $b_2 = (0,1)$\n- Reports: $\\theta' = (5,2)$ and $\\theta'' = (3,6)$\n- Policy space: $Y = \\{y \\in \\mathbb{R}^2 | (y_1-1)^2 + (y_2-1)^2 \\leq 4\\}$ (circle centered at (1,1) with radius 2)\n\nFirst, I'll find the set $M_{b_1,b_2}(\\theta', \\theta'')$ using Equation 1:\n\nFor $b_1 = (1,0)$:\n- $b_1 \\cdot \\theta' = (1,0) \\cdot (5,2) = 5$\n- $b_1 \\cdot \\theta'' = (1,0) \\cdot (3,6) = 3$\n- $\\min\\{b_1 \\cdot \\theta', b_1 \\cdot \\theta''\\} = \\min\\{5,3\\} = 3$\n\nFor $b_2 = (0,1)$:\n- $b_2 \\cdot \\theta' = (0,1) \\cdot (5,2) = 2$\n- $b_2 \\cdot \\theta'' = (0,1) \\cdot (3,6) = 6$\n- $\\min\\{b_2 \\cdot \\theta', b_2 \\cdot \\theta''\\} = \\min\\{2,6\\} = 2$\n\nTherefore, the set $M_{b_1,b_2}(\\theta', \\theta'')$ consists of points $x = (x_1, x_2)$ such that:\n- $b_1 \\cdot x = 3$, which means $(1,0) \\cdot (x_1, x_2) = x_1 ", "model_answer": "I need to analyze whether a fully revealing equilibrium (FRE) exists using the constructive test given in Equation 1.\n\nGiven information:\n- Bias vectors: $b_1 = (1,0)$ and $b_2 = (0,1)$\n- Reports: $\\theta' = (5,2)$ and $\\theta'' = (3,6)$\n- Policy space: $Y = \\{y \\in \\mathbb{R}^2 | (y_1-1)^2 + (y_2-1)^2 \\leq 4\\}$ (circle centered at (1,1) with radius 2)\n\nFirst, I'll find the set $M_{b_1,b_2}(\\theta', \\theta'')$ using Equation 1:\n\nFor $b_1 = (1,0)$:\n- $b_1 \\cdot \\theta' = (1,0) \\cdot (5,2) = 5$\n- $b_1 \\cdot \\theta'' = (1,0) \\cdot (3,6) = 3$\n- $\\min\\{b_1 \\cdot \\theta', b_1 \\cdot \\theta''\\} = \\min\\{5,3\\} = 3$\n\nFor $b_2 = (0,1)$:\n- $b_2 \\cdot \\theta' = (0,1) \\cdot (5,2) = 2$\n- $b_2 \\cdot \\theta'' = (0,1) \\cdot (3,6) = 6$\n- $\\min\\{b_2 \\cdot \\theta', b_2 \\cdot \\theta''\\} = \\min\\{2,6\\} = 2$\n\nTherefore, the set $M_{b_1,b_2}(\\theta', \\theta'')$ consists of points $x = (x_1, x_2)$ such that:\n- $b_1 \\cdot x = 3$, which means $(1,0) \\cdot (x_1, x_2) = x_1", "error": "Warning: Could not extract single choice letter from: I need to analyze whether a fully revealing equili..."}}
{"ID": 15, "Question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n4.  **Non-negativity:** `f_i(c, E) ≥ 0` for all `i`.\n\n### Question\n\nWhich set of axioms, when imposed on the General Proportional Rule (GPR), is sufficient to characterize the class of Proportional Rules, where the rule takes the form `f_i(c, E) = \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E` with `\\sum_{k \\in K} W_k(\\bar{c}, E) = 1`?", "Options": {"A": "No Award for Null and Efficiency.", "B": "Uniform Treatment of Uniforms and Efficiency.", "C": "No Award for Null and Non-negativity.", "D": "Efficiency and Non-negativity."}, "Answer": "A", "pi_justification": "This item assesses the axiomatic derivation of the Proportional Rule from the GPR, a task in the original QA problem. It uses a Reverse-Reasoning strategy, asking which set of axioms produces a known result. The correct option identifies the precise pair of axioms that force the two key constraints (`A_i=0` from NAN, and `∑W_k=1` from Efficiency). Distractors are other plausible combinations of axioms from the paper that lead to different or incomplete characterizations.", "qid": "15", "question": "### Background\n\nWe analyze a general division problem on a rich domain with `|N| ≥ 3` entities. The analysis starts from the 'Generalized Proportional Rule' (GPR), which the paper establishes as the general form for any reallocation-proof and bounded rule.\n\n### Data / Model Specification\n\n**Generalized Proportional Rule (GPR):**\n```latex\nf_i(c, E) = A_i(\\bar{c}, E) + \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E \n\n```\n**Axioms:**\n1.  **Efficiency:** `∑_{i∈N} f_i(c, E) = E`.\n2.  **Uniform Treatment of Uniforms (UTU):** If `c_1 = ... = c_|N|`, then `f_1 = ... = f_|N|`.\n3.  **No Award for Null (NAN):** If `c_i = 0`, then `f_i(c, E) = 0`.\n4.  **Non-negativity:** `f_i(c, E) ≥ 0` for all `i`.\n\n### Question\n\nWhich set of axioms, when imposed on the General Proportional Rule (GPR), is sufficient to characterize the class of Proportional Rules, where the rule takes the form `f_i(c, E) = \\sum_{k \\in K} \\frac{c_{ik}}{\\bar{c}_k} W_k(\\bar{c}, E) E` with `\\sum_{k \\in K} W_k(\\bar{c}, E) = 1`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine which axioms are sufficient to transform the General Proportional Rule (GPR) into the class of Proportional Rules.\n\nThe GPR is given as:\n```\nf_i(c, E) = A_i(c̄, E) + ∑_{k∈K} (c_ik/c̄_k) W_k(c̄, E) E\n```\n\nThe target Proportional Rules have the form:\n```\nf_i(c, E) = ∑_{k∈K} (c_ik/c̄_k) W_k(c̄, E) E\n```\n\nThe key difference is that Proportional Rules have no additive term A_i(c̄, E) - they consist only of the weighted proportional allocation.\n\nLet me analyze what each axiom does to the GPR:\n\n**No Award for Null (NAN):** If c_i = 0, then f_i(c, E) = 0.\nWhen c_i = 0, we have c_ik = 0 for all k, so the proportional terms become zero. This gives us:\nf_i(c, E) = A_i(c̄, E)\n\nFor NAN to hold, we need A_i(c̄, E) = 0 whenever agent i has zero claims. This is a strong constraint on the additive terms.\n\n**Uniform Treatment of Uniforms (UTU):** When all agents have identical claim vectors, they receive equal allocations.\n\nWhen c_1 = ... = c_|N|, all agents contribute equally to each c̄_k, so the proportional terms are identical across agents. For equal treatment, the additive terms A_i(c̄, E) must also be equal across agents in this case.\n\n**Efficiency:** ∑_{i∈N} f_i(c, E) = E\nThis constrains ∑_i A_i(c̄, E) + ∑_i ∑_k (c_ik/c̄_k) W_k(c̄, E) E = E\nSince ", "model_answer": "Looking at this problem, I need to determine which axioms are sufficient to transform the General Proportional Rule (GPR) into the class of Proportional Rules.\n\nThe GPR is given as:\n```\nf_i(c, E) = A_i(c̄, E) + ∑_{k∈K} (c_ik/c̄_k) W_k(c̄, E) E\n```\n\nThe target Proportional Rules have the form:\n```\nf_i(c, E) = ∑_{k∈K} (c_ik/c̄_k) W_k(c̄, E) E\n```\n\nThe key difference is that Proportional Rules have no additive term A_i(c̄, E) - they consist only of the weighted proportional allocation.\n\nLet me analyze what each axiom does to the GPR:\n\n**No Award for Null (NAN):** If c_i = 0, then f_i(c, E) = 0.\nWhen c_i = 0, we have c_ik = 0 for all k, so the proportional terms become zero. This gives us:\nf_i(c, E) = A_i(c̄, E)\n\nFor NAN to hold, we need A_i(c̄, E) = 0 whenever agent i has zero claims. This is a strong constraint on the additive terms.\n\n**Uniform Treatment of Uniforms (UTU):** When all agents have identical claim vectors, they receive equal allocations.\n\nWhen c_1 = ... = c_|N|, all agents contribute equally to each c̄_k, so the proportional terms are identical across agents. For equal treatment, the additive terms A_i(c̄, E) must also be equal across agents in this case.\n\n**Efficiency:** ∑_{i∈N} f_i(c, E) = E\nThis constrains ∑_i A_i(c̄, E) + ∑_i ∑_k (c_ik/c̄_k) W_k(c̄, E) E = E\nSince", "error": "Warning: Could not extract single choice letter from: Looking at this problem, I need to determine which..."}}
{"ID": 12, "Question": "### Background\n\nIn the model, a group of heterogeneous individuals makes decisions via majority rule. The preferences of the `i`-th member of the group over two public goods, `g` and `f`, are given by:\n\n```latex\nW^{i} \\equiv E\\left\\{ \\sum_{t=1}^{2} \\left[ \\alpha^{i}u(g_{t}) + (1-\\alpha^{i})u(f_{t}) \\right] \\right\\} \\quad \\text{(Eq. 1)}\n```\n\nwhere `α^i` is the individual's preference parameter. This preference structure is crucial for the model's tractability.\n\n### Question\n\nIn this model, what is the primary role of the parameter `α^i` and which resulting property of the preference structure is crucial for applying the median voter theorem?\n", "Options": {"A": "It represents an individual's risk aversion, and the resulting concavity of preferences allows for a unique optimum.", "B": "It acts as a utility weight defining heterogeneous tastes, and the resulting single-crossing property of indifference curves ensures preferences are single-peaked.", "C": "It determines an individual's rate of time preference, and the resulting stationarity of preferences ensures that the median voter is stable over time.", "D": "It acts as a utility weight defining heterogeneous tastes, and the resulting convexity of the choice set allows for an interior solution."}, "Answer": "B", "pi_justification": "This item assesses the understanding of the model's microfoundations, specifically how individual preferences are defined and aggregated. It replaces a portion of a QA problem because the concepts are distinct and allow for high-fidelity distractors. The distractors test for common confusions between preference parameters (α^i), utility function properties (concavity), and the conditions for political aggregation (single-crossing).", "qid": "12", "question": "### Background\n\nIn the model, a group of heterogeneous individuals makes decisions via majority rule. The preferences of the `i`-th member of the group over two public goods, `g` and `f`, are given by:\n\n```latex\nW^{i} \\equiv E\\left\\{ \\sum_{t=1}^{2} \\left[ \\alpha^{i}u(g_{t}) + (1-\\alpha^{i})u(f_{t}) \\right] \\right\\} \\quad \\text{(Eq. 1)}\n```\n\nwhere `α^i` is the individual's preference parameter. This preference structure is crucial for the model's tractability.\n\n### Question\n\nIn this model, what is the primary role of the parameter `α^i` and which resulting property of the preference structure is crucial for applying the median voter theorem?\n", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to identify what α^i represents and what property makes the median voter theorem applicable.\n\nFrom the utility function, α^i clearly acts as a weight between the two public goods g and f. When α^i is higher, individual i places more weight on good g relative to good f, and vice versa. This represents heterogeneous preferences across individuals.\n\nThe key insight is about what property is needed for the median voter theorem to work. The median voter theorem requires that preferences be single-peaked along the policy dimension. This happens when indifference curves have the single-crossing property - meaning that if one individual prefers policy A to policy B, and another individual is indifferent between them, then the first individual will prefer A to any policy that the second individual prefers less than B.\n\nIn models with utility weights like α^i, this single-crossing property naturally emerges. Individuals with different α^i values will have indifference curves that cross at most once when plotted in the (g,f) space, leading to single-peaked preferences over any linear combination of the goods (which is typically how voting occurs in these models).\n\nThe other options don't fit:\n- A) α^i isn't about risk aversion, and concavity alone doesn't ensure median voter theorem applicability\n- C) α^i isn't about time preference (both goods appear in the same time periods)\n- D) Convexity of choice set is about feasibility, not about preference structure needed for median voter theorem\n\nB", "model_answer": "A", "error": null}}
{"ID": 160, "Question": "### Background\n\n**Setting.** A key assumption for the canonical Milgrom-Weber revenue ranking theorem (`R_E ≥ R_S ≥ R_F`) is bidder symmetry—the idea that all bidders are ex-ante identical. The paper finds empirical evidence that this assumption is violated in the municipal bond auction data.\n\n### Question\n\nThe paper provides evidence that the bidder symmetry assumption, which is crucial for the main theoretical results, is violated in the data (e.g., in-state bidders are more likely to win). How does the author empirically assess the potential impact of this asymmetry on the main findings?", "Options": {"A": "By adding bidder-specific fixed effects to the main bid regression to control for time-invariant heterogeneity.", "B": "By using the bidder's location (in-state vs. out-of-state) as an instrumental variable for their information quality.", "C": "By re-estimating the entire model on a restricted subsample of auctions that excludes likely sources of asymmetry.", "D": "By developing and estimating a more complex, asymmetric model of strategic bidding."}, "Answer": "C", "pi_justification": "This item tests the student's understanding of the paper's research design and how it addresses threats to validity. It assesses knowledge of a specific methodological choice. The distractors are all plausible but incorrect empirical strategies. Distractor A (Fixed Effects) and B (Instrumental Variables) are common econometric techniques but were not used for this purpose. Distractor D describes a much more involved solution that the author did not pursue, making the actual choice seem pragmatic by comparison.", "qid": "160", "question": "### Background\n\n**Setting.** A key assumption for the canonical Milgrom-Weber revenue ranking theorem (`R_E ≥ R_S ≥ R_F`) is bidder symmetry—the idea that all bidders are ex-ante identical. The paper finds empirical evidence that this assumption is violated in the municipal bond auction data.\n\n### Question\n\nThe paper provides evidence that the bidder symmetry assumption, which is crucial for the main theoretical results, is violated in the data (e.g., in-state bidders are more likely to win). How does the author empirically assess the potential impact of this asymmetry on the main findings?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 26, "Question": "### Background\n\n**Research Question.** This problem investigates the economic intuition behind the cross-equation restrictions imposed by the Rational Expectations Hypothesis (REH).\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to the lagged exogenous variable $z_{t-1}$, such that $y_{1,t}^* = \\psi_0 z_{t-1}$.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error and $\\pi_1 > 0$. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process and $0 < A_1 < 1$.\n\n---\n\n### Question\n\nHow does the coefficient $\\psi_0$ change as the persistence of the exogenous process, $A_1$, increases, and what is the economic intuition for this change?", "Options": {"A": "$\\psi_0$ increases, because a more persistent process means $z_{t-1}$ is a stronger predictor of the future value $z_t$, so rational agents weigh it more heavily.", "B": "$\\psi_0$ decreases, because higher persistence in the exogenous process makes the endogenous feedback loop relatively less important.", "C": "$\\psi_0$ is unaffected, because $A_1$ is a parameter of the exogenous process and does not enter the structural equation for $y_{1,t}$.", "D": "$\\psi_0$ increases, because higher persistence implies greater future volatility, causing agents to react more strongly to any new information."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item captures the core of the original QA's Q3(b) and Q3(c). Assessment Target: Test the economic intuition behind the model's comparative statics. Strategy: Scenario Application. Distractor Logic: (A) is the correct answer, linking persistence to predictive power. (B) is a Conceptual Opposite, suggesting the wrong direction of change with a plausible-sounding but incorrect reason. (C) represents a common error of failing to see cross-equation linkages (True but Irrelevant logic). (D) is Almost Right in direction but provides an incorrect economic mechanism (confusing persistence with volatility).", "qid": "26", "question": "### Background\n\n**Research Question.** This problem investigates the economic intuition behind the cross-equation restrictions imposed by the Rational Expectations Hypothesis (REH).\n\n**Setting / Institutional Environment.** We consider a time-series structural model where rational expectations of endogenous variables appear as predetermined variables. The exogenous variables are assumed to follow a known, finite-order vector autoregressive (VAR) process.\n\n**Variables & Parameters.**\n- $y_t$: Endogenous variable(s).\n- $z_t$: Exogenous variable(s).\n- $\\Omega_{t-1}$: Information set available at the end of period $t-1$.\n- $y_{1,t}^* = E[y_{1,t} | \\Omega_{t-1}]$: Rational expectation of $y_{1,t}$.\n- $\\psi_0$: Scalar coefficient linking the expectation $y_{1,t}^*$ to the lagged exogenous variable $z_{t-1}$, such that $y_{1,t}^* = \\psi_0 z_{t-1}$.\n- $A_1$: Scalar coefficient of the AR(1) process for $z_t$.\n\n---\n\n### Data / Model Specification\n\nThe reduced form of the structural model for the first endogenous variable is given by:\n```latex\ny_{1,t} = y_{1,t}^* \\pi_{1,0}^* + z_{t} \\pi_1 + v_{1,t} \n```\nwhere $v_{1,t}$ is a serially uncorrelated reduced-form error and $\\pi_1 > 0$. The single exogenous variable follows an AR(1) process:\n```latex\nz_{t} = A_{1} z_{t-1} + \\varepsilon_{t}\n```\nwhere $\\varepsilon_t$ is a white noise process and $0 < A_1 < 1$.\n\n---\n\n### Question\n\nHow does the coefficient $\\psi_0$ change as the persistence of the exogenous process, $A_1$, increases, and what is the economic intuition for this change?", "model_evaluation_result": {"model_raw_response": "Looking at this rational expectations model, I need to determine how the coefficient ψ₀ (which links the rational expectation y*₁,t to the lagged exogenous variable z_{t-1}) changes with the persistence parameter A₁.\n\nLet me work through the economic logic:\n\nIn a rational expectations framework, agents form expectations y*₁,t = E[y₁,t | Ω_{t-1}] based on available information. Since y*₁,t = ψ₀z_{t-1}, the coefficient ψ₀ represents how much weight rational agents place on the lagged exogenous variable when forming expectations about the current endogenous variable.\n\nThe key insight is that z_{t-1} is valuable for predicting y₁,t only insofar as it helps predict the current exogenous variable z_t, which directly affects y₁,t through the coefficient π₁.\n\nFrom the AR(1) process z_t = A₁z_{t-1} + ε_t, we know that E[z_t | z_{t-1}] = A₁z_{t-1}.\n\nWhen A₁ is higher (more persistent):\n- z_{t-1} becomes a stronger predictor of z_t\n- The expected value of z_t given past information depends more heavily on z_{t-1}\n- Rational agents should therefore place more weight on z_{t-1} when forming expectations\n- This means ψ₀ should increase with A₁\n\nWhen A₁ is lower (less persistent):\n- z_{t-1} is a weaker predictor of z_t (more of z_t is just random noise ε_t)\n- Rational agents should place less weight on z_{t-1}\n- This means ψ₀ should be smaller\n\nThe economic intuition is that rational agents optimally use available information, and when the exogenous process is more persistent, lagged values become more informative about current conditions.\n\nA", "model_answer": "A", "error": null}}
{"ID": 94, "Question": "## Background\n\n**Research Question.** This problem examines the foundational conditions for the existence of a quasi-Lindahl equilibrium, focusing on the critical role of assumptions about consumer preferences.\n\n**Setting / Institutional Environment.** The economy consists of a continuum of consumers and a finite number of producers, with both private and public goods. The existence proof for equilibrium relies on a fixed-point theorem, which requires that the consumer's quasi-demand correspondence is non-empty and convex-valued.\n\n**Variables & Parameters.**\n\n*   $x(t) = (x_\\pi(t), x_g) \\in R^{n+l}$: A consumption bundle for consumer $t$.\n*   $p = (p_\\pi, p_g) \\in S$: A price system for private and public goods.\n*   $\\delta(t) \\in R_+^l$: A vector of personalized Lindahl shares for consumer $t$.\n*   $\\gtrsim_t$: The preference relation for consumer $t$.\n*   $x >_t y$: Strict preference, defined as $x \\gtrsim_t y$ and not $y \\gtrsim_t x$.\n*   $D^k(p, \\delta, t)$: The quasi-demand set for consumer $t$, containing all bundles in their (compactified) choice set that are not strictly preferred by any other affordable bundle.\n\n---\n\n## Data / Model Specification\n\nThe existence proof requires showing that the individual quasi-demand set, $D^k(p, \\delta, t)$, is convex. The proof proceeds by contradiction:\n\n1.  Take any two bundles $z^1, z^2$ from the quasi-demand set $D^k$.\n2.  Form their convex combination $\\bar{z} = \\lambda z^1 + (1-\\lambda)z^2$.\n3.  Assume $\\bar{z}$ is *not* in $D^k$. This means there exists an affordable bundle $y$ such that $y >_t \\bar{z}$.\n4.  Since $z^1 \\in D^k$, it cannot be that $y >_t z^1$. From this, infer that $z^1 \\gtrsim_t y$. (Step A)\n5.  Similarly, since $z^2 \\in D^k$, it cannot be that $y >_t z^2$. From this, infer that $z^2 \\gtrsim_t y$. (Step B)\n6.  Using the results from Steps A and B, infer that $\\bar{z} \\gtrsim_t y$. (Step C)\n7.  This leads to a contradiction, since $\\bar{z} \\gtrsim_t y$ is incompatible with the assumption $y >_t \\bar{z}$.\n\nKey preference axioms are:\n*   **Completeness:** For any two bundles $x,y$, either $x \\gtrsim_t y$ or $y \\gtrsim_t x$.\n*   **Convexity:** If $x \\gtrsim_t y$, then $\\lambda x + (1-\\lambda)y \\gtrsim_t y$ for all $\\lambda \\in [0,1]$.\n\n---\n\n## Question\n\nWhich step in the proof outline above relies critically on the **completeness** of preferences?", "Options": {"A": "Step A: Inferring $z^1 \\gtrsim_t y$ from the fact that it is not the case that $y >_t z^1$.", "B": "Step C: Inferring $\\bar{z} \\gtrsim_t y$ from $z^1 \\gtrsim_t y$ and $z^2 \\gtrsim_t y$.", "C": "Step 2: Establishing that the convex combination $\\bar{z}$ is in the consumer's budget set.", "D": "Step 7: Reaching a contradiction between $\\bar{z} \\gtrsim_t y$ and $y >_t \\bar{z}$."}, "Answer": "A", "pi_justification": "This question assesses the student's understanding of the precise logical role of the completeness axiom in microeconomic proofs. It uses a Reverse-Reasoning strategy, asking the student to identify the specific step in a proof that relies on the axiom. Distractor (B) targets the common confusion between the roles of completeness and convexity. Distractors (C) and (D) test whether the student can distinguish between properties derived from preferences versus those from the budget set or logical definitions.", "qid": "94", "question": "## Background\n\n**Research Question.** This problem examines the foundational conditions for the existence of a quasi-Lindahl equilibrium, focusing on the critical role of assumptions about consumer preferences.\n\n**Setting / Institutional Environment.** The economy consists of a continuum of consumers and a finite number of producers, with both private and public goods. The existence proof for equilibrium relies on a fixed-point theorem, which requires that the consumer's quasi-demand correspondence is non-empty and convex-valued.\n\n**Variables & Parameters.**\n\n*   $x(t) = (x_\\pi(t), x_g) \\in R^{n+l}$: A consumption bundle for consumer $t$.\n*   $p = (p_\\pi, p_g) \\in S$: A price system for private and public goods.\n*   $\\delta(t) \\in R_+^l$: A vector of personalized Lindahl shares for consumer $t$.\n*   $\\gtrsim_t$: The preference relation for consumer $t$.\n*   $x >_t y$: Strict preference, defined as $x \\gtrsim_t y$ and not $y \\gtrsim_t x$.\n*   $D^k(p, \\delta, t)$: The quasi-demand set for consumer $t$, containing all bundles in their (compactified) choice set that are not strictly preferred by any other affordable bundle.\n\n---\n\n## Data / Model Specification\n\nThe existence proof requires showing that the individual quasi-demand set, $D^k(p, \\delta, t)$, is convex. The proof proceeds by contradiction:\n\n1.  Take any two bundles $z^1, z^2$ from the quasi-demand set $D^k$.\n2.  Form their convex combination $\\bar{z} = \\lambda z^1 + (1-\\lambda)z^2$.\n3.  Assume $\\bar{z}$ is *not* in $D^k$. This means there exists an affordable bundle $y$ such that $y >_t \\bar{z}$.\n4.  Since $z^1 \\in D^k$, it cannot be that $y >_t z^1$. From this, infer that $z^1 \\gtrsim_t y$. (Step A)\n5.  Similarly, since $z^2 \\in D^k$, it cannot be that $y >_t z^2$. From this, infer that $z^2 \\gtrsim_t y$. (Step B)\n6.  Using the results from Steps A and B, infer that $\\bar{z} \\gtrsim_t y$. (Step C)\n7.  This leads to a contradiction, since $\\bar{z} \\gtrsim_t y$ is incompatible with the assumption $y >_t \\bar{z}$.\n\nKey preference axioms are:\n*   **Completeness:** For any two bundles $x,y$, either $x \\gtrsim_t y$ or $y \\gtrsim_t x$.\n*   **Convexity:** If $x \\gtrsim_t y$, then $\\lambda x + (1-\\lambda)y \\gtrsim_t y$ for all $\\lambda \\in [0,1]$.\n\n---\n\n## Question\n\nWhich step in the proof outline above relies critically on the **completeness** of preferences?", "model_evaluation_result": {"model_raw_response": "Looking at this proof step by step, I need to identify where completeness of preferences is critically used.\n\nLet me analyze each step:\n\n**Step A**: We know that $z^1 \\in D^k$, which means there's no affordable bundle that is strictly preferred to $z^1$. Since $y$ is affordable and we assumed $y >_t \\bar{z}$, if we also had $y >_t z^1$, this would contradict $z^1 \\in D^k$. So we can't have $y >_t z^1$.\n\nThe key question is: how do we get from \"not $(y >_t z^1)$\" to \"$z^1 \\gtrsim_t y$\"?\n\nBy definition, $y >_t z^1$ means \"$y \\gtrsim_t z^1$ and not $(z^1 \\gtrsim_t y)$\".\n\nSo \"not $(y >_t z^1)$\" means \"not $[y \\gtrsim_t z^1$ and not $(z^1 \\gtrsim_t y)]$\".\n\nUsing De Morgan's law: \"not $(y \\gtrsim_t z^1)$ or $(z^1 \\gtrsim_t y)$\".\n\nNow, **completeness** tells us that either $y \\gtrsim_t z^1$ or $z^1 \\gtrsim_t y$ (or both).\n\nIf \"not $(y \\gtrsim_t z^1)$ or $(z^1 \\gtrsim_t y)$\" is true, and we know from completeness that at least one of $y \\gtrsim_t z^1$ or $z^1 \\gtrsim_t y$ must hold, then we can conclude that $z^1 \\gtrsim_t y$ must be true.\n\nWithout completeness, we couldn't make this inference - we would only know that it's not the case that $y$ is strictly preferred to $z^1$, but we couldn't conclude that $z^1$ is weakly preferred to $y$.\n\n**Step B** uses the same logic.\n\n**Step C** uses convex", "model_answer": "A", "error": null}}
{"ID": 31, "Question": "### Background\n\nAn agent evaluates a bet on the outcome of a two-stage draw. The first draw is from an unambiguous source, while the second is from an ambiguous source. The agent observes the second draw and then evaluates a bet on the first.\n\n*   State space `S = \\{rr, rg, gr, gg\\}`.\n*   `R_1 = \\{rr, rg\\}`: Event that first draw is red.\n*   `R_2 = \\{rr, gr\\}`: Event that second draw is red.\n*   `X = 1_{R_1}`: The random variable being evaluated (a bet on the first draw being red).\n*   `\\mathcal{P} = \\{R_2, G_2\\}`: The information partition, where `G_2` is the event the second draw is green.\n*   The agent's beliefs are captured by a totally monotone capacity `π` with the following non-zero Möbius transform values: `μ_π(rr) = μ_π(gg) = 1/6`, `μ_π(rg) = μ_π(gr) = 1/12`, `μ_π(R_1) = μ_π(G_1) = 1/4`.\n\n---\n\n### Data / Model Specification\n\nThe paper's proposed **proxy update** rule first forms a proxy capacity `π^\\mathcal{P}` and then updates it via `π(A|B) = π^\\mathcal{P}(A ∩ B) / π^\\mathcal{P}(B)`. The Möbius transform of the proxy is given by:\n\n```latex\n\\mu_{\\pi}^{\\mathcal{P}}(A) = \\sum_{B \\in \\mathcal{P}} \\sum_{\\{D : D \\cap B = A\\}} \\frac{|A|}{|D|} \\cdot \\mu_{\\pi}(D) \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nUsing the proxy updating rule, what is the conditional evaluation of the bet `X` after observing a red ball from the second draw, `E(X|R_2)`?", "Options": {"A": "7/12", "B": "5/12", "C": "1/3", "D": "1/2"}, "Answer": "A", "pi_justification": "Assessment Target: This question tests the student's ability to correctly apply the formula for the proxy update rule, the paper's core technical contribution. Strategy: Computational Judgment. The student must perform a multi-step calculation to arrive at a single numerical answer. Distractor Logic: (A) Correct answer. (B) `E(X|G_2)`, the value conditional on the other signal, testing if the student correctly identifies the conditioning event. (C) The result from the alternative prior-by-prior updating rule, testing for conceptual confusion between the two models. (D) The ex-ante value `E(X)`, testing if the student incorrectly assumes the information is uninformative.", "qid": "31", "question": "### Background\n\nAn agent evaluates a bet on the outcome of a two-stage draw. The first draw is from an unambiguous source, while the second is from an ambiguous source. The agent observes the second draw and then evaluates a bet on the first.\n\n*   State space `S = \\{rr, rg, gr, gg\\}`.\n*   `R_1 = \\{rr, rg\\}`: Event that first draw is red.\n*   `R_2 = \\{rr, gr\\}`: Event that second draw is red.\n*   `X = 1_{R_1}`: The random variable being evaluated (a bet on the first draw being red).\n*   `\\mathcal{P} = \\{R_2, G_2\\}`: The information partition, where `G_2` is the event the second draw is green.\n*   The agent's beliefs are captured by a totally monotone capacity `π` with the following non-zero Möbius transform values: `μ_π(rr) = μ_π(gg) = 1/6`, `μ_π(rg) = μ_π(gr) = 1/12`, `μ_π(R_1) = μ_π(G_1) = 1/4`.\n\n---\n\n### Data / Model Specification\n\nThe paper's proposed **proxy update** rule first forms a proxy capacity `π^\\mathcal{P}` and then updates it via `π(A|B) = π^\\mathcal{P}(A ∩ B) / π^\\mathcal{P}(B)`. The Möbius transform of the proxy is given by:\n\n```latex\n\\mu_{\\pi}^{\\mathcal{P}}(A) = \\sum_{B \\in \\mathcal{P}} \\sum_{\\{D : D \\cap B = A\\}} \\frac{|A|}{|D|} \\cdot \\mu_{\\pi}(D) \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nUsing the proxy updating rule, what is the conditional evaluation of the bet `X` after observing a red ball from the second draw, `E(X|R_2)`?", "model_evaluation_result": {"model_raw_response": "I need to calculate the conditional evaluation E(X|R₂) using the proxy updating rule.\n\nFirst, let me identify the key components:\n- State space S = {rr, rg, gr, gg}\n- R₁ = {rr, rg} (first draw is red)\n- R₂ = {rr, gr} (second draw is red)\n- G₂ = {rg, gg} (second draw is green)\n- Information partition P = {R₂, G₂}\n- X = 1_{R₁} (bet on first draw being red)\n\nGiven Möbius transform values:\n- μ_π(rr) = μ_π(gg) = 1/6\n- μ_π(rg) = μ_π(gr) = 1/12\n- μ_π(R₁) = μ_π(G₁) = 1/4\n\nStep 1: Calculate the proxy capacity π^P using Equation (1).\n\nFor the proxy Möbius transform μ_π^P(A), I need to consider all sets D such that D ∩ B = A for each B ∈ P.\n\nFor A = {rr}:\n- When B = R₂: Need D such that D ∩ R₂ = {rr}\n  - D could be {rr} with |A|/|D| = 1/1 = 1\n  - D could be {rr, rg} with |A|/|D| = 1/2\n- When B = G₂: Need D such that D ∩ G₂ = {rr} (impossible since rr ∉ G₂)\n\nSo μ_π^P({rr}) = 1 · μ_π({rr}) + 1/2 · μ_π({rr,rg}) = 1 · 1/6 + 1/2 · 1/4 = 1/6 + 1/8 = 7/24\n\nFor A = {gr}:\n- When B = R", "model_answer": "A", "error": null}}
{"ID": 115, "Question": "### Background\n\n**Research Question.** This problem analyzes the paper's core empirical findings on the interdependent choices of labor supply and housing demand, based on the estimated elasticities for different household types.\n\n**Setting / Institutional Environment.** A joint labor-housing demand system is estimated for several distinct demographic groups using data from the 1976 Panel Study of Income Dynamics. The outputs are wage and income elasticities of labor supply (hours worked) and housing demand. The analysis focuses on interpreting these elasticities to understand household decision-making and the crucial linkages between work and housing choices.\n\n**Variables & Parameters.**\n- `j_h`, `j_s`: Annual hours worked by the head (`h`) and spouse (`s`).\n- `w_h`, `w_s`: Hourly wage for the head and spouse.\n- `ε_{j_i, w_k}`: Elasticity of labor supply of person `i` with respect to the wage of person `k`.\n- Unit of observation: Household.\n\n---\n\n### Data / Model Specification\n\nSelected estimated elasticities for “Two-Earner Families with Children” are presented in the table below.\n\n**Table 1: Elasticities of Hours Employed**\n\n| Group | Head's Own-Wage (`ε_{j_h, w_h}`) | Spouse's Own-Wage (`ε_{j_s, w_s}`) | Head's Cross-Wage (`ε_{j_h, w_s}`) | Spouse's Cross-Wage (`ε_{j_s, w_h}`) |\n| :--- | :---: | :---: | :---: | :---: |\n| Two-Earner Families with Children | -0.002 | -0.086 | -0.004 | -0.194 |\n\n*Note: In this study, the 'head' is the husband and the 'spouse' is the wife.*\n\n---\n\n### Question\n\nA policymaker wants to mitigate a decline in labor force participation among “Two-Earner Families with Children.” They consider two policies: (i) a 10% wage subsidy for the head of household, or (ii) a 10% wage subsidy for the spouse. Which policy results in a larger (i.e., less negative) net change in total household work hours? (Assume for simplicity that the head and spouse work the same number of hours before the subsidy.)", "Options": {"A": "The 10% wage subsidy for the spouse.", "B": "The 10% wage subsidy for the head.", "C": "Both policies result in an identical net change in total household work hours.", "D": "Neither policy can mitigate the decline, as both result in a net increase in total household work hours."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.25). The original QA problem's components are highly suitable for conversion. Conceptual Clarity = 9/10 as the answers are standard, structured interpretations or calculations. Discriminability = 9.5/10 due to classic, high-frequency misconceptions that create high-fidelity distractors.\n\nThis item uses the 'Computational Judgment' strategy to assess the most complex part of the original QA problem. It requires students to synthesize four different elasticities to calculate the net effect of two competing policies, testing the paper's core theme of joint decision-making. The distractors are designed based on specific error paths: (B) ignoring the large cross-wage effect of the head's wage on the spouse's hours; (C) a calculation error; (D) a sign error on the final net effect.", "qid": "115", "question": "### Background\n\n**Research Question.** This problem analyzes the paper's core empirical findings on the interdependent choices of labor supply and housing demand, based on the estimated elasticities for different household types.\n\n**Setting / Institutional Environment.** A joint labor-housing demand system is estimated for several distinct demographic groups using data from the 1976 Panel Study of Income Dynamics. The outputs are wage and income elasticities of labor supply (hours worked) and housing demand. The analysis focuses on interpreting these elasticities to understand household decision-making and the crucial linkages between work and housing choices.\n\n**Variables & Parameters.**\n- `j_h`, `j_s`: Annual hours worked by the head (`h`) and spouse (`s`).\n- `w_h`, `w_s`: Hourly wage for the head and spouse.\n- `ε_{j_i, w_k}`: Elasticity of labor supply of person `i` with respect to the wage of person `k`.\n- Unit of observation: Household.\n\n---\n\n### Data / Model Specification\n\nSelected estimated elasticities for “Two-Earner Families with Children” are presented in the table below.\n\n**Table 1: Elasticities of Hours Employed**\n\n| Group | Head's Own-Wage (`ε_{j_h, w_h}`) | Spouse's Own-Wage (`ε_{j_s, w_s}`) | Head's Cross-Wage (`ε_{j_h, w_s}`) | Spouse's Cross-Wage (`ε_{j_s, w_h}`) |\n| :--- | :---: | :---: | :---: | :---: |\n| Two-Earner Families with Children | -0.002 | -0.086 | -0.004 | -0.194 |\n\n*Note: In this study, the 'head' is the husband and the 'spouse' is the wife.*\n\n---\n\n### Question\n\nA policymaker wants to mitigate a decline in labor force participation among “Two-Earner Families with Children.” They consider two policies: (i) a 10% wage subsidy for the head of household, or (ii) a 10% wage subsidy for the spouse. Which policy results in a larger (i.e., less negative) net change in total household work hours? (Assume for simplicity that the head and spouse work the same number of hours before the subsidy.)", "model_evaluation_result": {"model_raw_response": "I need to calculate the net change in total household work hours for each policy using the given elasticities.\n\nGiven elasticities for \"Two-Earner Families with Children\":\n- Head's own-wage elasticity: ε_{j_h, w_h} = -0.002\n- Spouse's own-wage elasticity: ε_{j_s, w_s} = -0.086\n- Head's cross-wage elasticity: ε_{j_h, w_s} = -0.004\n- Spouse's cross-wage elasticity: ε_{j_s, w_h} = -0.194\n\nBoth policies involve a 10% wage increase (subsidy).\n\n**Policy (i): 10% wage subsidy for the head**\n- Effect on head's hours: ε_{j_h, w_h} × 10% = -0.002 × 0.10 = -0.0002 (0.02% decrease)\n- Effect on spouse's hours: ε_{j_s, w_h} × 10% = -0.194 × 0.10 = -0.0194 (1.94% decrease)\n- Total effect: -0.0002 + (-0.0194) = -0.0196 (1.96% decrease in total hours)\n\n**Policy (ii): 10% wage subsidy for the spouse**\n- Effect on spouse's hours: ε_{j_s, w_s} × 10% = -0.086 × 0.10 = -0.0086 (0.86% decrease)\n- Effect on head's hours: ε_{j_h, w_s} × 10% = -0.004 × 0.10 = -0.0004 (0.04% decrease)\n- Total effect: -0.0086 + (-0.0004) = -0.009 (0.9% decrease in total hours)\n\nComparing the results:\n- Policy (i): -1.96% change in total hours\n- Policy (ii): -0.9% change in total hours\n\nSince -0.9% is less negative than -1.96%, the spouse subsidy", "model_answer": "I need to calculate the net change in total household work hours for each policy using the given elasticities.\n\nGiven elasticities for \"Two-Earner Families with Children\":\n- Head's own-wage elasticity: ε_{j_h, w_h} = -0.002\n- Spouse's own-wage elasticity: ε_{j_s, w_s} = -0.086\n- Head's cross-wage elasticity: ε_{j_h, w_s} = -0.004\n- Spouse's cross-wage elasticity: ε_{j_s, w_h} = -0.194\n\nBoth policies involve a 10% wage increase (subsidy).\n\n**Policy (i): 10% wage subsidy for the head**\n- Effect on head's hours: ε_{j_h, w_h} × 10% = -0.002 × 0.10 = -0.0002 (0.02% decrease)\n- Effect on spouse's hours: ε_{j_s, w_h} × 10% = -0.194 × 0.10 = -0.0194 (1.94% decrease)\n- Total effect: -0.0002 + (-0.0194) = -0.0196 (1.96% decrease in total hours)\n\n**Policy (ii): 10% wage subsidy for the spouse**\n- Effect on spouse's hours: ε_{j_s, w_s} × 10% = -0.086 × 0.10 = -0.0086 (0.86% decrease)\n- Effect on head's hours: ε_{j_h, w_s} × 10% = -0.004 × 0.10 = -0.0004 (0.04% decrease)\n- Total effect: -0.0086 + (-0.0004) = -0.009 (0.9% decrease in total hours)\n\nComparing the results:\n- Policy (i): -1.96% change in total hours\n- Policy (ii): -0.9% change in total hours\n\nSince -0.9% is less negative than -1.96%, the spouse subsidy", "error": "Warning: Could not extract single choice letter from: I need to calculate the net change in total househ..."}}
{"ID": 106, "Question": "### Background\n\n**Research Question.** This question examines the paper's non-parametric evidence for the magnitude effect, which is the finding that allocation choices change as the total budget is scaled up.\n\n**Setting / Institutional Environment.** In Part I of an experiment, subjects make seven allocation choices for each of five different budget levels (€20, €40, €60, €80, €160). To test for a magnitude effect without assuming a specific utility function, the paper compares the vector of choices across pairs of budget levels using Hotelling’s T-squared tests. The null hypothesis is that the mean vectors of choices are equal.\n\n### Data / Model Specification\n\n**Table 1: Multivariate Mean Difference Tests (F-statistics) Between Total Budgets**\n\n| Comparison | €40 | €60 | €80 | €160 |\n| :--- | :--- | :--- | :--- | :--- |\n| **€20** | 3.318*** | 5.116*** | 6.367*** | 6.048*** |\n| **€40** | | 1.548 | 3.934*** | 3.943*** |\n| **€60** | | | 1.576 | 3.033*** |\n| **€80** | | | | 1.710 |\n\n*Notes: *** indicates p < 0.01.*\n\nBased on the pattern of significance in Table 1, what can be concluded about the nature of the magnitude effect?", "Options": {"A": "The effect is strongest at low stake levels and diminishes as stakes increase.", "B": "The effect is weak at low stake levels but becomes stronger as stakes increase.", "C": "The effect is statistically significant and has a constant strength across all budget increases.", "D": "The effect is not statistically significant between any adjacent budget levels."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the ability to interpret a table of statistical test results and identify a non-linear pattern. This item replaces the first part of a more complex QA problem. Strategy: Computational Judgment. Distractor Logic: (A) Correct interpretation of the 'concave' pattern (large initial jump, smaller subsequent changes). (B) Conceptual Opposite: claims a 'convex' pattern. (C) Step-Omission Error: correctly notes significance but misses the changing strength of the effect. (D) Factual Error: misreads the table, as several adjacent comparisons are indeed significant (e.g., €20 vs €40).", "qid": "106", "question": "### Background\n\n**Research Question.** This question examines the paper's non-parametric evidence for the magnitude effect, which is the finding that allocation choices change as the total budget is scaled up.\n\n**Setting / Institutional Environment.** In Part I of an experiment, subjects make seven allocation choices for each of five different budget levels (€20, €40, €60, €80, €160). To test for a magnitude effect without assuming a specific utility function, the paper compares the vector of choices across pairs of budget levels using Hotelling’s T-squared tests. The null hypothesis is that the mean vectors of choices are equal.\n\n### Data / Model Specification\n\n**Table 1: Multivariate Mean Difference Tests (F-statistics) Between Total Budgets**\n\n| Comparison | €40 | €60 | €80 | €160 |\n| :--- | :--- | :--- | :--- | :--- |\n| **€20** | 3.318*** | 5.116*** | 6.367*** | 6.048*** |\n| **€40** | | 1.548 | 3.934*** | 3.943*** |\n| **€60** | | | 1.576 | 3.033*** |\n| **€80** | | | | 1.710 |\n\n*Notes: *** indicates p < 0.01.*\n\nBased on the pattern of significance in Table 1, what can be concluded about the nature of the magnitude effect?", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to analyze the pattern of F-statistics and their significance levels to understand how the magnitude effect changes across different budget levels.\n\nKey observations from the table:\n\n1. **€20 vs all other budgets**: All comparisons are highly significant (p < 0.01), with F-statistics ranging from 3.318 to 6.367.\n\n2. **€40 vs higher budgets**: \n   - €40 vs €60: Not significant (F = 1.548)\n   - €40 vs €80: Significant (F = 3.934***)\n   - €40 vs €160: Significant (F = 3.943***)\n\n3. **€60 vs higher budgets**:\n   - €60 vs €80: Not significant (F = 1.576)\n   - €60 vs €160: Significant (F = 3.033***)\n\n4. **€80 vs €160**: Not significant (F = 1.710)\n\nThe pattern shows that:\n- The €20 budget produces significantly different choices compared to ALL other budget levels\n- Adjacent budget comparisons at higher levels (€40 vs €60, €60 vs €80, €80 vs €160) are generally NOT significant\n- Larger jumps at higher budget levels (€40 vs €160, €60 vs €160) remain significant but with smaller F-statistics than the €20 comparisons\n\nThis indicates that the magnitude effect is strongest when moving away from the lowest budget level (€20), and the effect diminishes as we move to higher stake levels where adjacent comparisons become non-significant.\n\nA", "model_answer": "A", "error": null}}
{"ID": 171, "Question": "### Background\n\n**Research Question.** This problem investigates the formal proof of identification for the distribution of preference heterogeneity, `F_0`.\n\n**Setting / Institutional Environment.** The proof of Theorem 1 uses the Cramér-Wold device. It involves an orthonormal transformation `θ_i = Aβ_i`, where the first row of matrix `A` is `c'`. This construction, combined with the key identification assumption, implies `Pr{θ_{i1} > 0} = 1`.\n\n### Data / Model Specification\n\nThe model is `y_i = 1\\{x_i'\\beta_i \\ge 0\\}`. The proof of identification establishes that the distribution of the ratio `θ_{i2}/θ_{i1}` is identified, where `θ_{i2}` is the `(K-1)x1` vector of the remaining elements of `θ_i`.\n\n### Question\n\nIn the formal proof of identification, what is the most critical role of the condition `Pr{θ_{i1} > 0} = 1`?", "Options": {"A": "It ensures that the denominator `θ_{i1}` is not zero, so the ratio `θ_{i2}/θ_{i1}` is well-defined.", "B": "It allows the distributions of `θ_{i1}` and `θ_{i2}` to be identified separately before combining them.", "C": "It guarantees that the transformation matrix `A` is non-singular.", "D": "It ensures a unique one-to-one mapping from the identified distribution of the ratio `θ_{i2}/θ_{i1}` back to the distribution of the vector `θ_i`."}, "Answer": "D", "pi_justification": "This item tests a deeper understanding of the formal proof mechanics. It uses a Scenario Application strategy, focusing on a specific step in the proof. The primary distractor (A) is an 'Almost Right' option that states a necessary but insufficient consequence of the assumption; the key insight is not just that the ratio is well-defined, but that the known sign of the denominator allows for a unique mapping back to the original vector, which is what option (D) correctly identifies. This discriminates between superficial and deep comprehension.", "qid": "171", "question": "### Background\n\n**Research Question.** This problem investigates the formal proof of identification for the distribution of preference heterogeneity, `F_0`.\n\n**Setting / Institutional Environment.** The proof of Theorem 1 uses the Cramér-Wold device. It involves an orthonormal transformation `θ_i = Aβ_i`, where the first row of matrix `A` is `c'`. This construction, combined with the key identification assumption, implies `Pr{θ_{i1} > 0} = 1`.\n\n### Data / Model Specification\n\nThe model is `y_i = 1\\{x_i'\\beta_i \\ge 0\\}`. The proof of identification establishes that the distribution of the ratio `θ_{i2}/θ_{i1}` is identified, where `θ_{i2}` is the `(K-1)x1` vector of the remaining elements of `θ_i`.\n\n### Question\n\nIn the formal proof of identification, what is the most critical role of the condition `Pr{θ_{i1} > 0} = 1`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 170, "Question": "### Background\n\n**Research Question.** This problem investigates the conditions required for the nonparametric identification of the distribution of preference heterogeneity, `F_0`, in a random coefficients binary choice model.\n\n**Setting / Institutional Environment.** Without sufficient restrictions, the distribution `F_0` is not identified, meaning multiple different distributions of `β_i` can produce the same observable choice probabilities `p(x, F) = Pr(y=1|x)`. For example, a uniform distribution on the unit circle and a distribution uniform only on the first and third quadrants can both yield `p(x,F) = 1/2` for all `x`.\n\n### Data / Model Specification\n\nTo solve the non-identification problem, Theorem 1 introduces a key assumption:\n\n*   **Assumption (iii):** There exists a vector `c` on the unit hypersphere `B` such that `Pr{c'β_i > 0} = 1`.\n\nThis assumption implies that the entire support of the distribution `F_0` is contained in an open half-hypersphere.\n\n### Question\n\nWhat is the primary mechanism by which Assumption (iii) restores identification of the distribution `F_0`?", "Options": {"A": "It ensures the log-likelihood function is strictly concave, which guarantees a unique maximum for the estimator.", "B": "It prevents the construction of an alternative, non-negative density function that produces the same choice probabilities.", "C": "It requires the regressors `x_i` to have a continuous distribution with support over the entire space.", "D": "It normalizes the scale of the `β_i` coefficients, which is necessary for any binary choice model."}, "Answer": "B", "pi_justification": "This item assesses the core intuition behind the paper's main identification strategy. It uses a Reverse-Reasoning approach, asking for the mechanism behind a stated result. The distractors are designed to probe for common confusions by mixing up the identification assumption with other distinct concepts from the paper: optimization properties (A: concavity), regressor assumptions (C), and scale normalization (D).", "qid": "170", "question": "### Background\n\n**Research Question.** This problem investigates the conditions required for the nonparametric identification of the distribution of preference heterogeneity, `F_0`, in a random coefficients binary choice model.\n\n**Setting / Institutional Environment.** Without sufficient restrictions, the distribution `F_0` is not identified, meaning multiple different distributions of `β_i` can produce the same observable choice probabilities `p(x, F) = Pr(y=1|x)`. For example, a uniform distribution on the unit circle and a distribution uniform only on the first and third quadrants can both yield `p(x,F) = 1/2` for all `x`.\n\n### Data / Model Specification\n\nTo solve the non-identification problem, Theorem 1 introduces a key assumption:\n\n*   **Assumption (iii):** There exists a vector `c` on the unit hypersphere `B` such that `Pr{c'β_i > 0} = 1`.\n\nThis assumption implies that the entire support of the distribution `F_0` is contained in an open half-hypersphere.\n\n### Question\n\nWhat is the primary mechanism by which Assumption (iii) restores identification of the distribution `F_0`?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 178, "Question": "### Background\n\n**Research Question.** This problem assesses the quantitative importance of structured management practices in explaining firm-level productivity dispersion, relative to other well-established drivers like R&D, ICT, and human capital.\n\n**Setting / Institutional Environment.** The analysis is conducted at the firm level, using data on U.S. manufacturing firms. Plant-level data are aggregated to the firm level.\n\n**Variables & Parameters.**\n- **Dependent Variable**: Firm-level `log(labor productivity)`.\n- **Key Independent Variables**:\n    - `Management score`: Firm-level average management score.\n    - `R&D`: Log of (1 + R&D intensity).\n    - `ICT/worker`: ICT investment per worker.\n    - `Skills`: Share of employees with a college degree.\n\n---\n\n### Data / Model Specification\n\nThe table below presents results from OLS regressions of firm-level log labor productivity on each of the four key drivers.\n\n**Table 1: Drivers of Productivity Variation**\n| | (1) | (2) | (3) | (4) | (5) |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| **Management score** | 0.864 | | | | 0.612 |\n| | (0.043) | | | | (0.043) |\n| **R&D** | | 0.133 | | | 0.095 |\n| | | (0.010) | | | (0.010) |\n| **ICT/worker** | | | 0.062 | | 0.047 |\n| | | | (0.006) | | (0.006) |\n| **Skills** | | | | 0.800 | 0.208 |\n| | | | | (0.064) | (0.060) |\n\n*Notes: Columns (1)-(4) represent separate regressions. Column (5) includes all four variables. The dependent variable is firm-level log(labor productivity).*\n\n---\n\n### Question\n\nThe coefficient on 'Skills' decreases from 0.800 in the simple regression (column 4) to 0.208 in the multiple regression (column 5). What is the most likely econometric explanation for this change?", "Options": {"A": "The simple regression coefficient was upwardly biased because 'Skills' is positively correlated with other productivity-enhancing factors like R&D and technology, which are omitted from column (4).", "B": "The simple regression coefficient was downwardly biased, and including other variables in column (5) corrected this bias.", "C": "The 'Skills' variable is measured with significant error, and including more variables in the regression exacerbates this measurement error, causing attenuation bias.", "D": "High multicollinearity between 'Skills' and the other variables in column (5) makes the simple regression coefficient unreliable, while the multiple regression coefficient is the true causal effect."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to test the user's understanding of omitted variable bias (OVB), a core econometric concept. The correct answer (A) provides the standard OVB explanation for the observed coefficient change. The distractors are designed based on common conceptual errors: (B) states the wrong direction of bias; (C) confuses OVB with measurement error; (D) misattributes the change to multicollinearity, which affects standard errors but doesn't systematically explain the change in the coefficient's magnitude in this way.", "qid": "178", "question": "### Background\n\n**Research Question.** This problem assesses the quantitative importance of structured management practices in explaining firm-level productivity dispersion, relative to other well-established drivers like R&D, ICT, and human capital.\n\n**Setting / Institutional Environment.** The analysis is conducted at the firm level, using data on U.S. manufacturing firms. Plant-level data are aggregated to the firm level.\n\n**Variables & Parameters.**\n- **Dependent Variable**: Firm-level `log(labor productivity)`.\n- **Key Independent Variables**:\n    - `Management score`: Firm-level average management score.\n    - `R&D`: Log of (1 + R&D intensity).\n    - `ICT/worker`: ICT investment per worker.\n    - `Skills`: Share of employees with a college degree.\n\n---\n\n### Data / Model Specification\n\nThe table below presents results from OLS regressions of firm-level log labor productivity on each of the four key drivers.\n\n**Table 1: Drivers of Productivity Variation**\n| | (1) | (2) | (3) | (4) | (5) |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| **Management score** | 0.864 | | | | 0.612 |\n| | (0.043) | | | | (0.043) |\n| **R&D** | | 0.133 | | | 0.095 |\n| | | (0.010) | | | (0.010) |\n| **ICT/worker** | | | 0.062 | | 0.047 |\n| | | | (0.006) | | (0.006) |\n| **Skills** | | | | 0.800 | 0.208 |\n| | | | | (0.064) | (0.060) |\n\n*Notes: Columns (1)-(4) represent separate regressions. Column (5) includes all four variables. The dependent variable is firm-level log(labor productivity).*\n\n---\n\n### Question\n\nThe coefficient on 'Skills' decreases from 0.800 in the simple regression (column 4) to 0.208 in the multiple regression (column 5). What is the most likely econometric explanation for this change?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 19, "Question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of testing the Melitz model's predictions, focusing on why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nThe theoretical change in productivity for a sector `s` in country 1 under the Melitz model is:\n\n```latex\nd\\ln\\Phi_{1s}=\\xi_{1s}d\\ln\\tau_{21s}-\\xi_{2s}d\\ln\\tau_{12s}-\\xi_{3s}d\\ln w_{1}\n```\n(Eq. 1)\n\nwhere `\\xi_{1s}` is the response to import costs `\\tau_{21s}`. Trefler's empirical DiD specification for Canadian productivity is:\n\n```latex\n\\Delta\\ln\\Phi_{s,t}^{C A}=\\beta^{C A}\\Delta\\tau_{s,t}^{C A}+\\beta^{U S}\\Delta\\tau_{s,t}^{U S}+\\gamma_{t}+\\text{Controls}_{st}+\\varepsilon_{st}\n```\n(Eq. 2)\n\nwhere `\\Delta\\tau_{s,t}^{C A}` represents the change in Canadian import tariffs. Trefler finds a statistically significant `\\hat{\\beta}^{CA} < 0`.\n\n### Question\n\nBased on the structurally symmetric Melitz model where the theoretical productivity response to import costs `\\xi_{1}` is positive, what does the model predict for the coefficient `\\beta^{CA}` in Trefler's regression, and why does his actual finding present a puzzle?", "Options": {"A": "The model predicts `\\beta^{CA} > 0` because productivity and import costs move in the same direction. Trefler's finding of `\\hat{\\beta}^{CA} < 0` is a puzzle because it implies productivity rose most where import tariffs were cut the most, the opposite of the prediction.", "B": "The model predicts `\\beta^{CA} < 0` because lower tariffs (`\\Delta\\tau < 0`) increase competition and productivity. Trefler's finding confirms this prediction, so there is no puzzle.", "C": "The model predicts `\\beta^{CA}` will be biased due to the wage term `d\\ln w_{1}`. The puzzle is that Trefler's time fixed effects did not adequately control for this general equilibrium shock.", "D": "The model predicts `\\beta^{CA} > 0`, but this is because higher tariffs (`\\Delta\\tau > 0`) protect domestic firms, raising their productivity. Trefler's finding of `\\hat{\\beta}^{CA} < 0` is a puzzle because it suggests protection hurts productivity."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to connect a theoretical parameter to its empirical counterpart in a regression context, which was the core of the original QA's first question. It uses a Reverse-Reasoning strategy. Distractor B represents the common but incorrect intuition that the paper refutes. Distractor C conflates the central puzzle with one of its potential explanations (omitted variables). Distractor D correctly identifies the predicted sign of the coefficient but provides an incorrect explanation for the underlying theoretical mechanism.", "qid": "19", "question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of testing the Melitz model's predictions, focusing on why empirical findings contradict the model's predictions.\n\n### Data / Model Specification\n\nThe theoretical change in productivity for a sector `s` in country 1 under the Melitz model is:\n\n```latex\nd\\ln\\Phi_{1s}=\\xi_{1s}d\\ln\\tau_{21s}-\\xi_{2s}d\\ln\\tau_{12s}-\\xi_{3s}d\\ln w_{1}\n```\n(Eq. 1)\n\nwhere `\\xi_{1s}` is the response to import costs `\\tau_{21s}`. Trefler's empirical DiD specification for Canadian productivity is:\n\n```latex\n\\Delta\\ln\\Phi_{s,t}^{C A}=\\beta^{C A}\\Delta\\tau_{s,t}^{C A}+\\beta^{U S}\\Delta\\tau_{s,t}^{U S}+\\gamma_{t}+\\text{Controls}_{st}+\\varepsilon_{st}\n```\n(Eq. 2)\n\nwhere `\\Delta\\tau_{s,t}^{C A}` represents the change in Canadian import tariffs. Trefler finds a statistically significant `\\hat{\\beta}^{CA} < 0`.\n\n### Question\n\nBased on the structurally symmetric Melitz model where the theoretical productivity response to import costs `\\xi_{1}` is positive, what does the model predict for the coefficient `\\beta^{CA}` in Trefler's regression, and why does his actual finding present a puzzle?", "model_evaluation_result": {"model_raw_response": "Looking at this step by step:\n\n1) From Equation 1, the theoretical Melitz model shows:\n   `d ln Φ₁ₛ = ξ₁ₛ d ln τ₂₁ₛ - ξ₂ₛ d ln τ₁₂ₛ - ξ₃ₛ d ln w₁`\n\n2) The problem states that ξ₁ₛ (response to import costs) is positive in the symmetric Melitz model.\n\n3) In Trefler's specification (Equation 2), `Δτₛ,ₜᶜᴬ` represents changes in Canadian import tariffs, which corresponds to the `d ln τ₂₁ₛ` term (tariffs on imports into Canada).\n\n4) Since ξ₁ₛ > 0, the model predicts that increases in import tariffs should increase productivity, meaning βᶜᴬ > 0.\n\n5) However, Trefler finds β̂ᶜᴬ < 0, which means productivity increased most where tariffs were cut the most (since a negative coefficient on tariff changes implies an inverse relationship).\n\n6) This creates a puzzle because it's the opposite of what the Melitz model predicts - the model says higher import costs should raise productivity, but the data shows productivity rose where import tariffs fell.\n\nThe answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 122, "Question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal can design a mechanism to extract all economic surplus from privately informed agents.\n\n**Setting:** The analysis centers on the properties of the common prior, $p$, over the space of all type profiles. A key condition for Full Surplus Extraction (FSE) is that the prior must have the Beliefs-Determine-Preferences (BDP) property.\n\n### Data / Model Specification\n\n**Key Definitions:**\n- **BDP Prior:** A prior where a bidder's beliefs uniquely determine their valuation.\n- **Non-BDP Prior:** A prior where there exist high-valuation types who share identical beliefs with corresponding low-valuation types. Under a non-BDP prior, a high-valuation type can mimic a low-valuation type to earn an 'information rent'.\n- **FSE Mechanism:** A mechanism designed for a BDP prior, where the payment extracted from each type is equal to their valuation.\n\n### The Question\n\nA principal mistakenly believes the prior is BDP and designs a mechanism to achieve FSE from bidder $i$. The true prior is non-BDP, containing a set of high-valuation types $A_i$ (with probability $p(A_i)=0.2$) who can mimic a corresponding set of low-valuation types $A_i'$. For any such mimicking pair, the high valuation is $\\hat{v}_H=100$ and the low valuation is $\\hat{v}_L=60$. For all other types (with probability 0.8), their average valuation is 80, and the FSE mechanism works correctly. What is the principal's actual expected revenue per bidder?", "Options": {"A": "76", "B": "84", "C": "64", "D": "80"}, "Answer": "A", "pi_justification": "This item uses a Computational Judgment strategy to assess the practical application of the revenue shortfall concept. It requires students to calculate the actual revenue in a specific scenario where high-valuation types strategically misreport. Distractor B represents the ideal but unachievable revenue (Step-Omission Error), C represents a severe miscalculation by ignoring revenue from mimicking types (Formula Misuse), and D represents a plausible but incorrect averaging heuristic (Formula Misuse).", "qid": "122", "question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal can design a mechanism to extract all economic surplus from privately informed agents.\n\n**Setting:** The analysis centers on the properties of the common prior, $p$, over the space of all type profiles. A key condition for Full Surplus Extraction (FSE) is that the prior must have the Beliefs-Determine-Preferences (BDP) property.\n\n### Data / Model Specification\n\n**Key Definitions:**\n- **BDP Prior:** A prior where a bidder's beliefs uniquely determine their valuation.\n- **Non-BDP Prior:** A prior where there exist high-valuation types who share identical beliefs with corresponding low-valuation types. Under a non-BDP prior, a high-valuation type can mimic a low-valuation type to earn an 'information rent'.\n- **FSE Mechanism:** A mechanism designed for a BDP prior, where the payment extracted from each type is equal to their valuation.\n\n### The Question\n\nA principal mistakenly believes the prior is BDP and designs a mechanism to achieve FSE from bidder $i$. The true prior is non-BDP, containing a set of high-valuation types $A_i$ (with probability $p(A_i)=0.2$) who can mimic a corresponding set of low-valuation types $A_i'$. For any such mimicking pair, the high valuation is $\\hat{v}_H=100$ and the low valuation is $\\hat{v}_L=60$. For all other types (with probability 0.8), their average valuation is 80, and the FSE mechanism works correctly. What is the principal's actual expected revenue per bidder?", "model_evaluation_result": {"model_raw_response": "I need to analyze this mechanism design problem step by step.\n\nGiven information:\n- Principal mistakenly believes prior is BDP and designs FSE mechanism\n- True prior is non-BDP\n- High-valuation types in set A_i can mimic low-valuation types in A_i'\n- P(A_i) = 0.2 (probability of high types who can mimic)\n- For mimicking pairs: v_H = 100, v_L = 60\n- For other types (probability 0.8): average valuation = 80\n- FSE mechanism works correctly for non-mimicking types\n\nUnder the FSE mechanism designed for BDP prior:\n- Each type is supposed to pay their valuation\n- But high types in A_i can mimic low types to pay less\n\nLet me calculate the expected revenue:\n\nFor the 20% of high types who can mimic (A_i):\n- They have valuation 100 but can mimic low types with valuation 60\n- They will choose to mimic and pay only 60 instead of 100\n- Revenue from these types: 60\n\nFor the remaining 80% of types:\n- FSE mechanism works correctly\n- They pay their average valuation of 80\n- Revenue from these types: 80\n\nExpected revenue per bidder:\n= 0.2 × 60 + 0.8 × 80\n= 12 + 64\n= 76\n\nA", "model_answer": "A", "error": null}}
{"ID": 11, "Question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, how can a researcher's beliefs about what the data cannot inform be used to specify a prior distribution?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` and `X_2 = X_1`, so `\\beta` is a 2x1 vector. The identified parameter is `\\psi = \\beta_1 + \\beta_2`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe general theory states that the data are marginally uninformative about `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c` describes the collinearity. In this case, `d=1`, so `c = [1, -1]'`.\n\n### Question\n\nA researcher believes, based on economic theory, that this particular experiment will be completely uninformative about the *difference* in coefficients, `\\beta_1 - \\beta_2`. To align their prior `\\beta ~ N(\\underline{b}, \\underline{Q})` with this belief, what restriction must be placed on the prior covariance matrix `\\underline{Q} = \\begin{pmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix}`?", "Options": {"A": "The prior correlation between `\\beta_1` and `\\beta_2` must be zero (`\\rho=0`).", "B": "The prior variances of `\\beta_1` and `\\beta_2` must be equal (`\\sigma_1^2 = \\sigma_2^2`).", "C": "The prior correlation between `\\beta_1` and `\\beta_2` must be negative one (`\\rho=-1`).", "D": "The prior mean of `\\beta_1` must equal the prior mean of `\\beta_2`."}, "Answer": "B", "pi_justification": "Assessment Target: Tests the application of the general theory to a specific prior elicitation problem. Strategy: Scenario Application. The user must translate a high-level belief into a concrete mathematical restriction. Distractor Logic: A) and C) represent common but incorrect intuitions about the required prior structure. The belief `\\alpha \\propto \\beta_1 - \\beta_2` requires `c'Q⁻¹ \\propto [1, -1]`, which leads to `q¹¹=q²²` in the precision matrix, implying `σ₁²=σ₂²` in the covariance matrix. A common error is to assume `ρ=0` is the required condition.", "qid": "11", "question": "### Background\n\n**Research Question.** In a linear regression model with perfect multicollinearity, how can a researcher's beliefs about what the data cannot inform be used to specify a prior distribution?\n\n**Setting.** Consider the model `y = X\\beta + \\varepsilon` where `X = [X_1, X_2]` and `X_2 = X_1`, so `\\beta` is a 2x1 vector. The identified parameter is `\\psi = \\beta_1 + \\beta_2`. The prior for `\\beta` is `N(\\underline{b}, \\underline{Q})`.\n\n### Data / Model Specification\n\nThe general theory states that the data are marginally uninformative about `\\alpha = c' \\underline{Q}^{-1} \\beta`, where `c` describes the collinearity. In this case, `d=1`, so `c = [1, -1]'`.\n\n### Question\n\nA researcher believes, based on economic theory, that this particular experiment will be completely uninformative about the *difference* in coefficients, `\\beta_1 - \\beta_2`. To align their prior `\\beta ~ N(\\underline{b}, \\underline{Q})` with this belief, what restriction must be placed on the prior covariance matrix `\\underline{Q} = \\begin{pmatrix} \\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2 \\end{pmatrix}`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand what it means for the data to be \"uninformative about the difference in coefficients, β₁ - β₂\" and how this relates to the prior specification.\n\nThe key insight is in the statement: \"the data are marginally uninformative about α = c'Q̄⁻¹β, where c describes the collinearity.\" Here, c = [1, -1]' and the researcher believes the experiment will be uninformative about β₁ - β₂.\n\nFor the data to be uninformative about β₁ - β₂, this difference must correspond to the unidentified direction in parameter space. Given the perfect multicollinearity where X₂ = X₁, the identified parameter is ψ = β₁ + β₂, while β₁ - β₂ represents the unidentified direction.\n\nThe condition α = c'Q̄⁻¹β relates to this unidentified direction. For the researcher's belief to be reflected in the prior, the prior must assign infinite variance to the unidentified direction β₁ - β₂.\n\nLet me examine what happens with each option:\n\nFor option C (ρ = -1), the covariance matrix becomes:\nQ̄ = [σ₁²    -σ₁σ₂]\n    [-σ₁σ₂   σ₂²]\n\nWhen ρ = -1, this matrix becomes singular (determinant = 0), meaning it has infinite variance in some direction. Specifically, when ρ = -1, the prior assigns infinite uncertainty to linear combinations of β₁ and β₂ that involve their difference.\n\nThis mathematical property aligns with the economic interpretation: if the researcher believes the data cannot inform about β₁ - β₂, then the prior should reflect complete uncertainty (infinite variance) about this difference.\n\nThe other options don't create this necessary singularity:\n- Option A (ρ = 0): Creates independence but finite variances\n- Option B (σ₁² = σ₂²): Equalizes variances but doesn't create singularity\n- Option D: Concerns prior means, not the covariance", "model_answer": "C", "error": null}}
{"ID": 189, "Question": "### Background\n\nAfter establishing that large horizontal RJV networks are associated with declining market shares, the paper seeks to translate this finding into a practical policy tool by estimating a specific network size threshold above which collusion appears to dominate. The key finding is an estimated critical threshold of $K^*=0.18$.\n\n### Data / Model Specification\n\n**Table 1: Application of Critical Network Size ($K^*=0.18$) to Selected Industries**\n| SIC4 | Industry Description | Year | % Firms in Network > K* | # Firms in Network > K* | # Firms in Industry |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 3571 | Electronic Computers | 1991 | 0.4706 | 16 | 34 |\n\n### Question\n\nBased on the finding that horizontal networks larger than 18% of the industry ($K^*=0.18$) are associated with collusive behavior, the paper presents the data in Table 1. What is the most accurate policy interpretation of the finding for the Electronic Computers industry in 1991?", "Options": {"A": "The average horizontal RJV network in the industry connected 47.06% of the firms, indicating widespread collusion.", "B": "A significant portion of firms (16 of 34) participated in networks large enough to be flagged for potential antitrust scrutiny.", "C": "The 16 firms in the largest network colluded to reduce their collective market share by 18%.", "D": "The industry as a whole was found to be colluding, as 16 of the 34 firms were part of a single large RJV."}, "Answer": "B", "pi_justification": "This item assesses the ability to translate a statistical rule into a practical policy application. Distractors are designed based on common misinterpretations of the data in the table, such as confusing the meaning of the '% Firms in Network > K*' column with an average network size, or conflating the threshold value with an effect size. Strategy: Scenario Application.", "qid": "189", "question": "### Background\n\nAfter establishing that large horizontal RJV networks are associated with declining market shares, the paper seeks to translate this finding into a practical policy tool by estimating a specific network size threshold above which collusion appears to dominate. The key finding is an estimated critical threshold of $K^*=0.18$.\n\n### Data / Model Specification\n\n**Table 1: Application of Critical Network Size ($K^*=0.18$) to Selected Industries**\n| SIC4 | Industry Description | Year | % Firms in Network > K* | # Firms in Network > K* | # Firms in Industry |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 3571 | Electronic Computers | 1991 | 0.4706 | 16 | 34 |\n\n### Question\n\nBased on the finding that horizontal networks larger than 18% of the industry ($K^*=0.18$) are associated with collusive behavior, the paper presents the data in Table 1. What is the most accurate policy interpretation of the finding for the Electronic Computers industry in 1991?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 74, "Question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nThe paper argues that the no-linking regime is always inefficient. Under which specific condition would the capital requirement for the marginal tenant, `\\hat{b}_n`, coincidentally be efficient (i.e., `\\hat{b}_n = b_n^*`)?", "Options": {"A": "`r_n = r_0`", "B": "`r_n > r_0`", "C": "`w = 0`", "D": "`z^* = 0` (where `z^*` is the landlord's profit in the linking regime)"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the deep understanding of what drives inefficiency in the no-linking regime. It uses a Reverse-Reasoning strategy, asking for the specific condition that would eliminate the inefficiency. The correct answer, `r_n = r_0`, requires connecting the IR constraints from both regimes. Distractors are based on common confusions: (B) is a plausible but incorrect condition, (C) confuses the outside option with efficiency conditions, and (D) confuses landlord profit with social surplus maximization.", "qid": "74", "question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nThe paper argues that the no-linking regime is always inefficient. Under which specific condition would the capital requirement for the marginal tenant, `\\hat{b}_n`, coincidentally be efficient (i.e., `\\hat{b}_n = b_n^*`)?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine when the capital requirement in the no-linking regime would coincidentally be efficient.\n\nLet me analyze the key equations:\n\n**No-linking regime:** The minimum capital requirement $\\hat{b}_n$ makes the marginal tenant indifferent:\n$$\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w$$\n\n**Efficiency condition:** The first-best capital level $b_n^*$ maximizes total surplus:\n$$\\frac{\\partial}{\\partial b}[F(e_n, b) - (1+r_0)ab] = 0$$\n\nThis gives us: $\\alpha F_b(e_n, b_n^*) = (1+r_0)a$\n\n**Linking regime:** In equilibrium, the marginal tenant gets zero profit:\n$$\\alpha F(e_n, b_n) - (1+r_n)a b_n = w$$\n\nFor the landlord's optimization in the linking regime:\n$$\\frac{\\partial}{\\partial b}[\\alpha F(e_n, b) - (1+r_n)ab] = 0$$\n\nThis gives us: $\\alpha F_b(e_n, b_n) = (1+r_n)a$\n\nNow, for $\\hat{b}_n = b_n^*$ to hold, I need the first-order conditions to be the same.\n\nFrom the efficiency condition: $\\alpha F_b(e_n, b_n^*) = (1+r_0)a$\nFrom the linking regime: $\\alpha F_b(e_n, b_n) = (1+r_n)a$\n\nIf $\\hat{b}_n = b_n^* = b_n$, then both conditions must hold simultaneously, which requires:\n$$(1+r_0)a = (1+r_n)a$$\n\nThis simplifies to: $r_0 = r_n$\n\nThis makes economic sense: when the market interest rate equals the rate charged to the marginal tenant in the linking regime, the screening advantage disappears, and the no-linking", "model_answer": "Looking at this problem, I need to determine when the capital requirement in the no-linking regime would coincidentally be efficient.\n\nLet me analyze the key equations:\n\n**No-linking regime:** The minimum capital requirement $\\hat{b}_n$ makes the marginal tenant indifferent:\n$$\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w$$\n\n**Efficiency condition:** The first-best capital level $b_n^*$ maximizes total surplus:\n$$\\frac{\\partial}{\\partial b}[F(e_n, b) - (1+r_0)ab] = 0$$\n\nThis gives us: $\\alpha F_b(e_n, b_n^*) = (1+r_0)a$\n\n**Linking regime:** In equilibrium, the marginal tenant gets zero profit:\n$$\\alpha F(e_n, b_n) - (1+r_n)a b_n = w$$\n\nFor the landlord's optimization in the linking regime:\n$$\\frac{\\partial}{\\partial b}[\\alpha F(e_n, b) - (1+r_n)ab] = 0$$\n\nThis gives us: $\\alpha F_b(e_n, b_n) = (1+r_n)a$\n\nNow, for $\\hat{b}_n = b_n^*$ to hold, I need the first-order conditions to be the same.\n\nFrom the efficiency condition: $\\alpha F_b(e_n, b_n^*) = (1+r_0)a$\nFrom the linking regime: $\\alpha F_b(e_n, b_n) = (1+r_n)a$\n\nIf $\\hat{b}_n = b_n^* = b_n$, then both conditions must hold simultaneously, which requires:\n$$(1+r_0)a = (1+r_n)a$$\n\nThis simplifies to: $r_0 = r_n$\n\nThis makes economic sense: when the market interest rate equals the rate charged to the marginal tenant in the linking regime, the screening advantage disappears, and the no-linking", "error": "Warning: Could not extract single choice letter from: Looking at this problem, I need to determine when ..."}}
{"ID": 22, "Question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe optimal weight `α₁` on the macro forecast `η̂_{T+1}^{(1)}` is chosen to minimize the mean squared error of the final, updated micro forecast. \n\nIn the paper's 'base scenario', the micro model is perfectly specified and macro data are exact aggregates of micro data. In this idealized case, the optimal weight `α₁` on the macro forecast is zero. What is the direct implication of `α₁ = 0`?", "Options": {"A": "The macro forecast is biased, while the micro forecast is unbiased.", "B": "The micro forecast is a sufficient statistic, containing all relevant information.", "C": "The micro and macro forecasts are perfectly correlated, making one redundant.", "D": "The variance of the macro forecast is infinitely large compared to the micro forecast."}, "Answer": "B", "pi_justification": "Assessment Target: Assesses understanding of the key theoretical conclusion from the idealized base case. Strategy: Reverse-Reasoning (given an outcome, `α₁=0`, infer the underlying condition). Distractor Logic: (A) Plausible but not the direct implication; both are unbiased in the base case. (C) Is Almost Right, but sufficiency is the more precise and fundamental concept. (D) Is a condition that would also lead to `α₁` approaching zero, but it is not the reason in this specific scenario (Conceptual Opposite).", "qid": "22", "question": "### Background\n\n**Research Question.** This problem focuses on the core theoretical contribution of the paper: deriving the optimal rule for combining micro and macro forecasts to produce an improved, consistent set of forecasts.\n\n**Setting / Institutional Environment.** An economy is modeled at two levels. A 'micro' office produces a forecast for a vector of `p` disaggregated variables, `ŷ_{T+1}^{(2)}`. A separate 'macro' office produces a forecast for a single aggregated variable, `η̂_{T+1}^{(1)}`. The two levels are linked by a known aggregation vector, `a`, but the models are estimated independently. The goal is to combine these forecasts.\n\n### Data / Model Specification\n\nThe optimal weight `α₁` on the macro forecast `η̂_{T+1}^{(1)}` is chosen to minimize the mean squared error of the final, updated micro forecast. \n\nIn the paper's 'base scenario', the micro model is perfectly specified and macro data are exact aggregates of micro data. In this idealized case, the optimal weight `α₁` on the macro forecast is zero. What is the direct implication of `α₁ = 0`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand what it means when the optimal weight α₁ on the macro forecast is zero in the base scenario.\n\nGiven information:\n- The micro model is perfectly specified\n- Macro data are exact aggregates of micro data\n- The optimal weight α₁ on the macro forecast is zero\n- The weight is chosen to minimize mean squared error\n\nWhen α₁ = 0, this means the optimal combination puts zero weight on the macro forecast and full weight on the micro forecast. This occurs when adding the macro forecast provides no additional information that would improve the forecast accuracy.\n\nLet me evaluate each option:\n\nA) This suggests bias differences, but the problem states both models are working with exact aggregates and the micro model is perfectly specified. Bias isn't the key issue here.\n\nB) If the micro forecast contains all relevant information for optimal forecasting, then adding the macro forecast cannot improve the mean squared error. This would make α₁ = 0 optimal, as the micro forecast is sufficient.\n\nC) Perfect correlation alone doesn't necessarily make α₁ = 0 optimal. Even with perfect correlation, if one forecast had lower variance, it could still receive positive weight.\n\nD) The problem doesn't indicate infinite variance in the macro forecast. Moreover, if macro variance were infinite, this would be about relative precision rather than sufficiency.\n\nThe key insight is that in the base scenario with perfect micro model specification and exact aggregation, the micro forecast already incorporates all the information optimally. The macro forecast, being an aggregate, cannot provide additional independent information that would improve the forecast.\n\nB", "model_answer": "A", "error": null}}
{"ID": 179, "Question": "### Background\n\n**Research Question.** This problem examines whether structured management practices predict long-term establishment performance, specifically survival, and whether this relationship holds even within multi-plant firms.\n\n**Setting / Institutional Environment.** The analysis uses a 2010 cross-section of U.S. manufacturing plants and tracks their survival status through 2015.\n\n**Variables & Parameters.**\n- **Dependent Variable**: `Exit`, an indicator variable equal to 1 if the establishment ceased operations between 2010 and 2015.\n- **Key Independent Variable**: `Management`, the structured management score in 2010 (scaled 0-1).\n\n---\n\n### Data / Model Specification\n\nSelected results from a linear probability model of plant exit are presented below.\n\n**Table 1: Management and Probability of Exit, 2010-2015**\n| | (1) | (2) |\n| :--- | :---: | :---: |\n| **Management (2010)** | -0.180 | -0.286 |\n| | (0.014) | (0.033) |\n| **Firm Fixed Effects** | No | Yes |\n\n*Notes: The dependent variable is a dummy for exit by 2015. The sample in column (2) is restricted to establishments in multi-plant firms.* \n\n---\n\n### Question\n\nIn Table 1, the coefficient on 'Management' is -0.180 in column (1) but becomes more negative (-0.286) in column (2) when firm fixed effects are included. What is the most plausible interpretation of this finding?", "Options": {"A": "Within a multi-plant firm, a poorly-managed plant is disproportionately more likely to be closed compared to its better-managed siblings, suggesting a process of internal creative destruction.", "B": "Firms with better average management practices are more likely to close plants, which is why the coefficient becomes more negative.", "C": "Measurement error in the management score is likely biasing the firm fixed-effects estimate in column (2) upwards, away from zero.", "D": "The relationship between management and exit is weaker in multi-plant firms than in single-plant firms."}, "Answer": "A", "pi_justification": "This item tests the ability to interpret a key finding from a fixed-effects model and understand the economic story it implies. The correct answer (A) accurately describes the within-firm comparison and the paper's 'internal creative destruction' argument. Distractors target common misinterpretations: (B) misinterprets the nature of a within-firm estimate; (C) incorrectly describes the direction of bias from measurement error in an FE model (it causes attenuation towards zero); (D) is a conceptual opposite of what the larger coefficient implies.", "qid": "179", "question": "### Background\n\n**Research Question.** This problem examines whether structured management practices predict long-term establishment performance, specifically survival, and whether this relationship holds even within multi-plant firms.\n\n**Setting / Institutional Environment.** The analysis uses a 2010 cross-section of U.S. manufacturing plants and tracks their survival status through 2015.\n\n**Variables & Parameters.**\n- **Dependent Variable**: `Exit`, an indicator variable equal to 1 if the establishment ceased operations between 2010 and 2015.\n- **Key Independent Variable**: `Management`, the structured management score in 2010 (scaled 0-1).\n\n---\n\n### Data / Model Specification\n\nSelected results from a linear probability model of plant exit are presented below.\n\n**Table 1: Management and Probability of Exit, 2010-2015**\n| | (1) | (2) |\n| :--- | :---: | :---: |\n| **Management (2010)** | -0.180 | -0.286 |\n| | (0.014) | (0.033) |\n| **Firm Fixed Effects** | No | Yes |\n\n*Notes: The dependent variable is a dummy for exit by 2015. The sample in column (2) is restricted to establishments in multi-plant firms.* \n\n---\n\n### Question\n\nIn Table 1, the coefficient on 'Management' is -0.180 in column (1) but becomes more negative (-0.286) in column (2) when firm fixed effects are included. What is the most plausible interpretation of this finding?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 42, "Question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers three potential equilibrium assignments: 'Mixed (2, 3)', 'Exact', and 'Mixed (1, 2)'. The 'Exact' assignment only allows matches between agents of the same type (e.g., worker 1 with job 1). The 'Mixed (1, 2)' assignment allows matches between types 1 and 2, in addition to same-type matches. Key outcomes are shown in the tables below.\n\n**Table 1: Unemployment Rates**\n| Assignment   | $u_1=v_1$ | $u_2=v_2$ | $u_3=v_3$ |\n| :----------- | :-------- | :-------- | :-------- |\n| Exact        | 0.375     | 0.375     | 0.375     |\n| Mixed (1, 2) | 0.250     | 0.250     | 0.333     |\n\n**Table 2: Aggregate Output**\n| Assignment   | Output, $\\pi$ |\n| :----------- | :------------- |\n| Exact        | 7.50           |\n| Mixed (1, 2) | 7.72           |\n\n### Question\n\nThe 'Mixed (1, 2)' equilibrium yields a higher total output ($\\pi=7.72$) than the 'Exact' equilibrium ($\\pi=7.50$). According to the model's logic, what is the primary reason for this efficiency gain?", "Options": {"A": "By allowing moderately productive matches between type-1 and type-2 agents, the search pool changes, which allows the highly productive type-3 agents to find each other and match more quickly, increasing the frequency of high-output ($a_{33}=8.0$) matches.", "B": "The output from new matches between type-1 and type-2 agents ($a_{12}=1.18$) is substantially greater than the output from the type-1 matches they replace ($a_{11}=1.0$), and this direct gain accounts for most of the difference.", "C": "The 'Mixed (1, 2)' equilibrium is more efficient because it forces type-3 workers to accept lower reservation wages, making them form matches more often.", "D": "The 'Mixed (1, 2)' equilibrium is more efficient simply because it has lower unemployment rates for a majority of worker types compared to the 'Exact' assignment."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to synthesize information from multiple tables to understand the model's main economic insight. It uses a Reverse-Reasoning strategy. Distractors are designed to test for common misinterpretations: B (True but Incomplete: focusing on a minor effect while ignoring the larger externality on type-3 agents), C (Factually Incorrect: contradicting data from the paper showing reservation wages for type 3 actually increase), and D (Oversimplification: ignoring the crucial compositional effects of matches, which are central to the paper's argument).", "qid": "42", "question": "### Background\n\nThis question examines a search model with heterogeneous workers and jobs, where multiple stable matching equilibria can exist. The model demonstrates that decentralized, self-interested decisions by workers and firms do not necessarily lead to the socially optimal (output-maximizing) assignment, revealing the presence of search externalities.\n\n### Data / Model Specification\n\nThe model considers three potential equilibrium assignments: 'Mixed (2, 3)', 'Exact', and 'Mixed (1, 2)'. The 'Exact' assignment only allows matches between agents of the same type (e.g., worker 1 with job 1). The 'Mixed (1, 2)' assignment allows matches between types 1 and 2, in addition to same-type matches. Key outcomes are shown in the tables below.\n\n**Table 1: Unemployment Rates**\n| Assignment   | $u_1=v_1$ | $u_2=v_2$ | $u_3=v_3$ |\n| :----------- | :-------- | :-------- | :-------- |\n| Exact        | 0.375     | 0.375     | 0.375     |\n| Mixed (1, 2) | 0.250     | 0.250     | 0.333     |\n\n**Table 2: Aggregate Output**\n| Assignment   | Output, $\\pi$ |\n| :----------- | :------------- |\n| Exact        | 7.50           |\n| Mixed (1, 2) | 7.72           |\n\n### Question\n\nThe 'Mixed (1, 2)' equilibrium yields a higher total output ($\\pi=7.72$) than the 'Exact' equilibrium ($\\pi=7.50$). According to the model's logic, what is the primary reason for this efficiency gain?", "model_evaluation_result": {"model_raw_response": "Looking at this search model problem, I need to analyze why the 'Mixed (1, 2)' equilibrium achieves higher output than the 'Exact' equilibrium.\n\nLet me examine the key information:\n- 'Mixed (1, 2)' output: 7.72\n- 'Exact' output: 7.50\n- The difference is 0.22\n\nFrom the unemployment rates table, I can see that in the 'Mixed (1, 2)' equilibrium:\n- Types 1 and 2 have lower unemployment (0.250 vs 0.375)\n- Type 3 has slightly higher unemployment (0.333 vs 0.375)\n\nNow let me evaluate each option:\n\nOption A suggests that allowing type-1 and type-2 matches changes the search pool dynamics, enabling type-3 agents to find each other more quickly and form more high-output matches (a₃₃=8.0). This describes an indirect network effect where cross-type matching in one segment improves matching efficiency in another segment.\n\nOption B focuses on the direct output gain from new type-1/type-2 matches (a₁₂=1.18) compared to type-1 matches they replace (a₁₁=1.0). However, this direct gain of 0.18 per match seems insufficient to explain the total output increase of 0.22.\n\nOption C suggests type-3 workers accept lower wages, but this doesn't align with the search model framework where efficiency comes from better matching patterns, not wage adjustments.\n\nOption D merely restates that unemployment is lower for most types, but doesn't explain the mechanism behind the efficiency gain.\n\nThe key insight is that in search models with multiple agent types, allowing cross-type matching can create positive externalities. When types 1 and 2 can match with each other, it reduces congestion in the overall matching process, making it easier for the remaining agents (including type-3) to find their optimal matches. This indirect effect through improved search dynamics is the primary driver of efficiency gains in such models.\n\nA", "model_answer": "A", "error": null}}
{"ID": 86, "Question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe long-run dynamic adjustment of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) is governed by a process that converges to a long-run equilibrium. In this equilibrium, the marginal revenue product of each factor equals its cost:\n1.  `p_{i}^{v} (\\delta F_{i} / \\delta K) = \\pi_{i}r` (for capital)\n2.  `p_{i}^{v} (\\delta F_{i} / \\delta E) = \\bar{w_{i}}` (for labor)\n\nAssume the production function `F_i` is Cobb-Douglas:\n\n```latex\nF_i(\\bar{K}_i, \\bar{E}_i) = Z \\bar{K}_i^\\beta \\bar{E}_i^{1-\\beta}\n```\n\n## Question\n\nGiven the long-run equilibrium conditions and the specified Cobb-Douglas production function, how does a permanent increase in the price of capital goods, `\\pi_i`, affect the sector's long-run equilibrium capital-labor ratio, `\\bar{K}_i^* / \\bar{E}_i^*`?", "Options": {"A": "It decreases the capital-labor ratio.", "B": "It increases the capital-labor ratio.", "C": "It has no effect on the capital-labor ratio.", "D": "The effect is ambiguous and depends on whether `\\beta` is greater or less than 0.5."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item uses the 'Computational Judgment' strategy to replace the complex derivation in question 3 of the original QA problem. It tests the core economic insight: the law of demand for factors of production. To answer correctly, the user must mentally (or on paper) derive the optimal capital-labor ratio and determine the sign of its derivative with respect to the price of capital. The distractors represent classic errors: (B) is the conceptual opposite (violating the law of demand), (C) incorrectly assumes a fixed-proportions technology (zero elasticity of substitution), and (D) introduces a plausible but incorrect complexity.", "qid": "86", "question": "## Background\n\nThis problem examines the micro-foundational model of firm behavior, from short-run production decisions under rigidities to the long-run dynamic adjustment of capital and labor, which forms the core engine of the paper's CGE model.\n\n## Data / Model Specification\n\nThe long-run dynamic adjustment of a firm's capital stock (`\\bar{K}_i`) and labor capacity (`\\bar{E}_i`) is governed by a process that converges to a long-run equilibrium. In this equilibrium, the marginal revenue product of each factor equals its cost:\n1.  `p_{i}^{v} (\\delta F_{i} / \\delta K) = \\pi_{i}r` (for capital)\n2.  `p_{i}^{v} (\\delta F_{i} / \\delta E) = \\bar{w_{i}}` (for labor)\n\nAssume the production function `F_i` is Cobb-Douglas:\n\n```latex\nF_i(\\bar{K}_i, \\bar{E}_i) = Z \\bar{K}_i^\\beta \\bar{E}_i^{1-\\beta}\n```\n\n## Question\n\nGiven the long-run equilibrium conditions and the specified Cobb-Douglas production function, how does a permanent increase in the price of capital goods, `\\pi_i`, affect the sector's long-run equilibrium capital-labor ratio, `\\bar{K}_i^* / \\bar{E}_i^*`?", "model_evaluation_result": {"model_raw_response": "I need to analyze how a permanent increase in the price of capital goods affects the long-run equilibrium capital-labor ratio in a Cobb-Douglas production function.\n\nGiven information:\n- Production function: F_i(K̄_i, Ē_i) = Z K̄_i^β Ē_i^(1-β)\n- Long-run equilibrium conditions:\n  1. p_i^v (∂F_i/∂K) = π_i r (for capital)\n  2. p_i^v (∂F_i/∂E) = w̄_i (for labor)\n\nFirst, let me find the marginal products:\n- ∂F_i/∂K = β Z K̄_i^(β-1) Ē_i^(1-β)\n- ∂F_i/∂E = (1-β) Z K̄_i^β Ē_i^(-β)\n\nSubstituting into the equilibrium conditions:\n1. p_i^v · β Z K̄_i^(β-1) Ē_i^(1-β) = π_i r\n2. p_i^v · (1-β) Z K̄_i^β Ē_i^(-β) = w̄_i\n\nTo find the capital-labor ratio, I'll divide equation (1) by equation (2):\n\n[p_i^v · β Z K̄_i^(β-1) Ē_i^(1-β)] / [p_i^v · (1-β) Z K̄_i^β Ē_i^(-β)] = (π_i r) / w̄_i\n\nSimplifying the left side:\n[β K̄_i^(β-1) Ē_i^(1-β)] / [(1-β) K̄_i^β Ē_i^(-β)] = (π_i r) / w̄_i\n\n[β / (1-β)] · [K̄_i^(β-1) / K̄_i^β] ·", "model_answer": "I need to analyze how a permanent increase in the price of capital goods affects the long-run equilibrium capital-labor ratio in a Cobb-Douglas production function.\n\nGiven information:\n- Production function: F_i(K̄_i, Ē_i) = Z K̄_i^β Ē_i^(1-β)\n- Long-run equilibrium conditions:\n  1. p_i^v (∂F_i/∂K) = π_i r (for capital)\n  2. p_i^v (∂F_i/∂E) = w̄_i (for labor)\n\nFirst, let me find the marginal products:\n- ∂F_i/∂K = β Z K̄_i^(β-1) Ē_i^(1-β)\n- ∂F_i/∂E = (1-β) Z K̄_i^β Ē_i^(-β)\n\nSubstituting into the equilibrium conditions:\n1. p_i^v · β Z K̄_i^(β-1) Ē_i^(1-β) = π_i r\n2. p_i^v · (1-β) Z K̄_i^β Ē_i^(-β) = w̄_i\n\nTo find the capital-labor ratio, I'll divide equation (1) by equation (2):\n\n[p_i^v · β Z K̄_i^(β-1) Ē_i^(1-β)] / [p_i^v · (1-β) Z K̄_i^β Ē_i^(-β)] = (π_i r) / w̄_i\n\nSimplifying the left side:\n[β K̄_i^(β-1) Ē_i^(1-β)] / [(1-β) K̄_i^β Ē_i^(-β)] = (π_i r) / w̄_i\n\n[β / (1-β)] · [K̄_i^(β-1) / K̄_i^β] ·", "error": "Warning: Could not extract single choice letter from: I need to analyze how a permanent increase in the ..."}}
{"ID": 79, "Question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified by an overidentification test, the Falsification Adaptive Set (FAS) provides a range of parameter estimates consistent with minimally relaxed versions of the model. This approach avoids the arbitrary selection of sensitivity parameters by letting the data determine the necessary degree of relaxation of the instrument exclusion restrictions.\n\n### Data / Model Specification\n\nConsider a linear IV model with one endogenous variable (`K=1`) and `L` instruments. The Falsification Adaptive Set (FAS) for the causal effect `β` is given by the interval:\n\n```latex\n\\mathrm{FAS} = \\left[ \\min_{l: \\pi_l \\neq 0} b_l, \\max_{l: \\pi_l \\neq 0} b_l \\right] \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l = ψ_l / π_l` is the population 2SLS coefficient on the endogenous variable `X` using `Z_l` as the excluded instrument and the remaining instruments `Z_{-l}` as controls. `π_l` is the corresponding first-stage coefficient.\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan (`Z_1`), Railroads (`Z_2`), and Exploration (`Z_3`). The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| **Panel B. Controlling for other instruments** | | |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n*Note: Standard practice is to discard results from instruments with a first-stage F-statistic below 10, as they are considered weak and unreliable.*\n\n### Question\n\nBased on the information provided in Table 1 and the definition in Eq. (1), what is the estimated Falsification Adaptive Set (FAS) for the effect of log highway km?", "Options": {"A": "[0.18, 3.65]", "B": "[0.18, 0.42]", "C": "[0.24, 3.47]", "D": "[0.42, 3.65]"}, "Answer": "B", "pi_justification": "This item assesses the ability to apply a theoretical formula to empirical results, a core skill. It uses a Computational Judgment strategy. The correct answer requires identifying the relevant instruments by screening out the weak one (F-stat < 10) and then taking the min and max of the remaining estimates. Distractor A represents a Step-Omission Error, where the user fails to screen for the weak instrument. Distractor C is a Formula Misuse error, based on incorrect calculations with the endpoints. Distractor D represents both a screening failure and a misidentification of the minimum value.", "qid": "79", "question": "### Background\n\nWhen a linear instrumental variable (IV) model with multiple instruments is falsified by an overidentification test, the Falsification Adaptive Set (FAS) provides a range of parameter estimates consistent with minimally relaxed versions of the model. This approach avoids the arbitrary selection of sensitivity parameters by letting the data determine the necessary degree of relaxation of the instrument exclusion restrictions.\n\n### Data / Model Specification\n\nConsider a linear IV model with one endogenous variable (`K=1`) and `L` instruments. The Falsification Adaptive Set (FAS) for the causal effect `β` is given by the interval:\n\n```latex\n\\mathrm{FAS} = \\left[ \\min_{l: \\pi_l \\neq 0} b_l, \\max_{l: \\pi_l \\neq 0} b_l \\right] \\quad \\text{(Eq. 1)}\n```\n\nwhere `b_l = ψ_l / π_l` is the population 2SLS coefficient on the endogenous variable `X` using `Z_l` as the excluded instrument and the remaining instruments `Z_{-l}` as controls. `π_l` is the corresponding first-stage coefficient.\n\nAn empirical study on the effect of highways on trade uses three instruments: Plan (`Z_1`), Railroads (`Z_2`), and Exploration (`Z_3`). The table below shows 2SLS estimates (`b_l`) where each instrument is used one at a time, while controlling for the other two.\n\n**Table 1: 2SLS Estimates with Single Instruments and Controls**\n\n| | (4) Plan | (5) Railroads | (6) Exploration |\n| :--- | :---: | :---: | :---: |\n| **Panel B. Controlling for other instruments** | | |\n| log highway km (`b_l`) | 0.18 (0.21) | 3.65 (4.16) | 0.42 (0.52) |\n| First-stage F stat. | 54.8 | 1.27 | 27.0 |\n\n*Note: Standard practice is to discard results from instruments with a first-stage F-statistic below 10, as they are considered weak and unreliable.*\n\n### Question\n\nBased on the information provided in Table 1 and the definition in Eq. (1), what is the estimated Falsification Adaptive Set (FAS) for the effect of log highway km?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine the FAS interval using Equation 1:\n\nFAS = [min_{l: π_l ≠ 0} b_l, max_{l: π_l ≠ 0} b_l]\n\nFrom Table 1, I have three instruments with their 2SLS estimates (b_l) and first-stage F statistics:\n\n1. Plan (Z_1): b_1 = 0.18, F-stat = 54.8\n2. Railroads (Z_2): b_2 = 3.65, F-stat = 1.27  \n3. Exploration (Z_3): b_3 = 0.42, F-stat = 27.0\n\nThe key question is whether to include all instruments or exclude weak ones. The note states that \"standard practice is to discard results from instruments with a first-stage F-statistic below 10, as they are considered weak and unreliable.\"\n\nThe Railroads instrument has F-stat = 1.27 < 10, making it weak by this standard.\n\nHowever, the FAS methodology is specifically designed for situations where instruments may be invalid (hence the overidentification test failure mentioned in the background). The condition π_l ≠ 0 in the FAS formula refers to whether the instrument has any predictive power for the endogenous variable, not whether it meets conventional strength thresholds.\n\nSince even weak instruments can provide information about the range of possible estimates under different degrees of assumption violation, and the FAS is meant to be comprehensive in capturing this range, I should include all instruments that have non-zero first-stage coefficients.\n\nAll three instruments have positive F-statistics, indicating π_l ≠ 0 for all of them.\n\nTherefore:\n- Minimum b_l = min(0.18, 3.65, 0.42) = 0.18\n- Maximum b_l = max(0.18, 3.65, 0.42) = 3.65\n\nFAS = [0.18, 3.65]\n\nA", "model_answer": "A", "error": null}}
{"ID": 133, "Question": "### Background\n\nThe study estimates a structural model of a Bayesian game where a provider's capacity choice $K_i$ is a function of observable market characteristics $X$, competitors' expected capacity $E[K_{-i}|X]$, and unobservable private information $\\varepsilon_i$. After estimation, the model's predictions are compared to the data to validate its performance.\n\n### Data / Model Specification\n\n**Table 1: Model Fit Comparison for DaVita**\n\n| Statistic | Observed | GMM Prediction |\n| :--- | :--- | :--- |\n| Mean Capacity | 4.40 | 4.23 |\n| SD Capacity | 9.23 | 4.46 |\n| Mean Entry Prob. | 0.23 | 0.24 |\n\n### The Question\n\nTable 1 shows that the standard deviation of DaVita's observed capacity (9.23) is much larger than the standard deviation of the model's predicted *expected* capacity (4.46). What is the best interpretation of this discrepancy?", "Options": {"A": "The model's predictions for *expected* capacity are based only on observable characteristics (X), while observed capacity choices are also driven by large, unobservable private information shocks (ε_i), which increase the total variance.", "B": "The large observed variance is primarily due to measurement error in the capacity data, which the model correctly smooths out.", "C": "The discrepancy indicates that the model overfits the mean of the data at the expense of fitting the variance.", "D": "The GMM estimator is known to be biased in finite samples, which likely causes it to underestimate the true variance of the underlying process."}, "Answer": "A", "pi_justification": "This item uses the Reverse-Reasoning strategy. Given a key empirical finding from the model fit table, the student must infer the underlying structural cause. It assesses a deep understanding of how incomplete information models partition variance between observable and unobservable components. Distractors offer plausible but incorrect statistical explanations (measurement error, overfitting, finite sample bias) to discriminate between students who understand the model's structure and those relying on general statistical heuristics.", "qid": "133", "question": "### Background\n\nThe study estimates a structural model of a Bayesian game where a provider's capacity choice $K_i$ is a function of observable market characteristics $X$, competitors' expected capacity $E[K_{-i}|X]$, and unobservable private information $\\varepsilon_i$. After estimation, the model's predictions are compared to the data to validate its performance.\n\n### Data / Model Specification\n\n**Table 1: Model Fit Comparison for DaVita**\n\n| Statistic | Observed | GMM Prediction |\n| :--- | :--- | :--- |\n| Mean Capacity | 4.40 | 4.23 |\n| SD Capacity | 9.23 | 4.46 |\n| Mean Entry Prob. | 0.23 | 0.24 |\n\n### The Question\n\nTable 1 shows that the standard deviation of DaVita's observed capacity (9.23) is much larger than the standard deviation of the model's predicted *expected* capacity (4.46). What is the best interpretation of this discrepancy?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 119, "Question": "### Background\n\n**Research Question.** This problem uses the paper's leading example to test the robustness of the indeterminacy result when financial asset yields are not fixed but depend on spot prices.\n\n**Setting / Institutional Environment.** Consider a simple exchange economy with 2 households, 1 good, and 3 spot markets. In the baseline model with exogenous yields, the budget set is a line. An alternative financial instrument is an installment contract with price-dependent yields.\n\n---\n\n### Data / Model Specification\n\nThe budget constraints for the installment contract model are:\n\n```latex\nz_h^0 + \\left[\\frac{\\pi}{p^2-q}\\right]p^2 z_h^2 = 0 \\quad \\text{(Eq. (1))}\n```\n\n```latex\np^1 z_h^1 - \\left[\\frac{p^1-q}{p^2-q}\\right]p^2 z_h^2 = 0 \\quad \\text{(Eq. (2))}\n```\n\nwhere `z_h` is the excess demand vector, `p` is the price vector, and `π` and `q` are parameters of the contract.\n\n---\n\n### Question\n\nIn this alternative model with price-dependent yields, what is the nature of the household's budget set for a *given* price vector `p`, and what does this imply about the indeterminacy of equilibrium?", "Options": {"A": "The budget set is a 2-dimensional plane, which restores the generic determinacy of a complete market.", "B": "The budget set is a 1-dimensional line, implying that real indeterminacy is robust to this form of endogenous yields.", "C": "The budget set is a non-linear surface because of the price terms in the coefficients, which restores generic determinacy.", "D": "The budget set is a 1-dimensional line, but this specific structure of price dependence eliminates the indeterminacy."}, "Answer": "B", "pi_justification": "Assessment Target: Tests the robustness of the core geometric insight by applying it to a new scenario. Strategy: Scenario Application. Distractor Logic: The correct answer (B) recognizes that for a fixed `p`, the equations are linear and define a line. Distractor C represents the most plausible misconception: confusing the fact that the *family* of budget lines changes with `p` with the idea that the budget set *itself* is non-linear for a given `p`. Distractor A incorrectly equates price-dependence with market completeness. Distractor D correctly identifies the geometry but draws the wrong conclusion about its implications.", "qid": "119", "question": "### Background\n\n**Research Question.** This problem uses the paper's leading example to test the robustness of the indeterminacy result when financial asset yields are not fixed but depend on spot prices.\n\n**Setting / Institutional Environment.** Consider a simple exchange economy with 2 households, 1 good, and 3 spot markets. In the baseline model with exogenous yields, the budget set is a line. An alternative financial instrument is an installment contract with price-dependent yields.\n\n---\n\n### Data / Model Specification\n\nThe budget constraints for the installment contract model are:\n\n```latex\nz_h^0 + \\left[\\frac{\\pi}{p^2-q}\\right]p^2 z_h^2 = 0 \\quad \\text{(Eq. (1))}\n```\n\n```latex\np^1 z_h^1 - \\left[\\frac{p^1-q}{p^2-q}\\right]p^2 z_h^2 = 0 \\quad \\text{(Eq. (2))}\n```\n\nwhere `z_h` is the excess demand vector, `p` is the price vector, and `π` and `q` are parameters of the contract.\n\n---\n\n### Question\n\nIn this alternative model with price-dependent yields, what is the nature of the household's budget set for a *given* price vector `p`, and what does this imply about the indeterminacy of equilibrium?", "model_evaluation_result": {"model_raw_response": "Looking at the budget constraints given:\n\n```latex\nz_h^0 + \\left[\\frac{\\pi}{p^2-q}\\right]p^2 z_h^2 = 0 \\quad \\text{(Eq. (1))}\n```\n\n```latex\np^1 z_h^1 - \\left[\\frac{p^1-q}{p^2-q}\\right]p^2 z_h^2 = 0 \\quad \\text{(Eq. (2))}\n```\n\nFor a **given** price vector `p`, all the terms in brackets become constants. This means:\n- The coefficient `[π/(p²-q)]p²` in equation (1) is a constant\n- The coefficient `[(p¹-q)/(p²-q)]p²` in equation (2) is a constant\n\nWith these constants, we have two linear equations in three variables (z_h^0, z_h^1, z_h^2). This system defines a 1-dimensional line in the 3-dimensional excess demand space.\n\nThe key insight is that even though the yields are now price-dependent (endogenous), for any given price vector, the budget set remains a line rather than expanding to a higher-dimensional space. This means the household still faces the same dimensional constraint as in the baseline model with exogenous yields.\n\nSince the budget set maintains its 1-dimensional nature, the indeterminacy result from the baseline model with fixed yields carries over to this model with price-dependent yields. The endogeneity of yields doesn't eliminate the fundamental source of indeterminacy.\n\nB", "model_answer": "B", "error": null}}
{"ID": 190, "Question": "### Background\n\nAfter establishing that large horizontal RJV networks are associated with declining market shares, the paper conducts several robustness checks to defend the validity of its measures.\n\n### Data / Model Specification\n\n**Table 1: Correlation of Market Share Measures with 'Gold Standard' Data**\n| | $MS_{FCC}$ | $MS_{Gartner}$ |\n| :--- | :--- | :--- |\n| $MS_{COMPUSTAT}$ | 0.9546*** | 0.8957*** |\n*Notes: `***` denotes significance at the 1% level.*\n\n**Table 2: System GMM Results using Market-Share-Weighted Network ($RN\\_MS$)**\n| Cumulative Effect on Market Share (`MS`) | Coefficient |\n| :--- | :--- |\n| Horizontal Network (MS based), small | 0.0037 |\n| Horizontal Network (MS based), medium | -0.0167* |\n| Horizontal Network (MS based), large | -0.0166** |\n*Notes: **p<0.05, *p<0.10.*\n\n### Question\n\nA critic argues the paper's main conclusion is unreliable because (i) the market share variable is poorly measured and (ii) the network variable wrongly treats all competitor links as equal. Based on the evidence in Table 1 and Table 2, which statement provides the strongest rebuttal to this critique?", "Options": {"A": "The critique is valid because Table 2 shows different coefficient estimates than the main analysis, indicating the results are not robust.", "B": "The critique is refuted because Table 1 shows a high correlation with better data sources, and Table 2 shows that a more sophisticated, market-share-weighted network measure yields the same qualitative conclusion.", "C": "The critique is only partially refuted; while Table 1 addresses the market share issue, Table 2's positive coefficient for small networks contradicts the paper's collusion theory.", "D": "The critique is valid because correlation in Table 1 does not imply causation, and the System GMM model used for Table 2 cannot handle measurement error."}, "Answer": "B", "pi_justification": "This item tests the ability to understand and synthesize the logic of two distinct robustness checks. The distractors are designed to mimic sophisticated but flawed critiques, testing for a deep understanding of what makes an empirical result credible (e.g., confusing qualitative robustness with identical coefficients, or misinterpreting the underlying theory). Strategy: Computational Judgment.", "qid": "190", "question": "### Background\n\nAfter establishing that large horizontal RJV networks are associated with declining market shares, the paper conducts several robustness checks to defend the validity of its measures.\n\n### Data / Model Specification\n\n**Table 1: Correlation of Market Share Measures with 'Gold Standard' Data**\n| | $MS_{FCC}$ | $MS_{Gartner}$ |\n| :--- | :--- | :--- |\n| $MS_{COMPUSTAT}$ | 0.9546*** | 0.8957*** |\n*Notes: `***` denotes significance at the 1% level.*\n\n**Table 2: System GMM Results using Market-Share-Weighted Network ($RN\\_MS$)**\n| Cumulative Effect on Market Share (`MS`) | Coefficient |\n| :--- | :--- |\n| Horizontal Network (MS based), small | 0.0037 |\n| Horizontal Network (MS based), medium | -0.0167* |\n| Horizontal Network (MS based), large | -0.0166** |\n*Notes: **p<0.05, *p<0.10.*\n\n### Question\n\nA critic argues the paper's main conclusion is unreliable because (i) the market share variable is poorly measured and (ii) the network variable wrongly treats all competitor links as equal. Based on the evidence in Table 1 and Table 2, which statement provides the strongest rebuttal to this critique?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 112, "Question": "### Background\n\nThis problem investigates the paper's central claim: the robustness of the standard F-test to non-normal errors is critically determined by the characteristics of the regressor design matrix, particularly the presence of high-leverage observations. The analysis hinges on an analytical expression for the deviation of the true significance level (`α*`) from its nominal level (`α`), and on diagnostic measures derived from the regressor matrix.\n\n### Data / Model Specification\n\nThe paper derives the actual significance level `α*` as a function of the nominal level `α`, the error distribution's cumulants (`λᵢ`), and the regressor design matrix (`X`). The relationship is given by:\n\n```latex\n\\alpha^{*} = \\alpha + b(W_{0};\\nu_{2},\\nu_{1})d(W_{0};\\lambda,X)\n```\n\nThe term `d(...)` is an infinite series in the error cumulants (e.g., `λ₃` for skewness, `λ₄` for kurtosis) whose coefficients depend on the regressor matrix. The paper shows that the sensitivity to kurtosis (`λ₄`) is heavily influenced by a diagnostic function of the regressors:\n\n*   **`HX(4)`**: Measures sensitivity to kurtosis (`λ₄`). It is defined as `HX(4) = \\sum_{i=1}^{n} h_{ii}^2`, where `hᵢᵢ` are the diagonal elements of the projection matrix `H` associated with the `q` regressors being tested. A large `HX(4)` value relative to its theoretical range indicates the presence of high-leverage points, which makes the F-test more sensitive to error kurtosis.\n\nThese functions have theoretical bounds, which are provided in **Table 1**. The paper analyzes several datasets, including two contrasting cases whose diagnostic values are provided in **Table 2** for the test of all slope coefficients being zero (`q=k`).\n\n**Table 1: Theoretical Bounds for HX-functions**\n\n| HX-function | Lower bound | Upper bound |\n| :--- | :--- | :--- |\n| `HX(4)` | `q²/n` | `q(1 - 1/n)` |\n\n**Table 2: Computed HX-function Values for Two Datasets (for q=k)**\n\n| Dataset | n | k | q | `HX(4)` |\n| :--- | :-: | :-: | :-: | :--- |\n| TEXTILE | 17 | 2 | 2 | 0.3 |\n| ANOVA1 | 15 | 5 | 5 | 4.4 |\n\n### Question\n\nBased on the theory and data provided, for which dataset is the F-test's validity more likely to be compromised by non-normal error kurtosis (`λ₄`), and why? Your reasoning should be based on the position of the observed `HX(4)` value within its theoretical range.", "Options": {"A": "ANOVA1, because its observed `HX(4)` of 4.4 is very close to its theoretical upper bound of approximately 4.67, indicating a design with severe leverage points.", "B": "TEXTILE, because its observed `HX(4)` of 0.3 is very close to its theoretical lower bound of approximately 0.24, and a design close to the lower bound is most sensitive to non-normality.", "C": "ANOVA1, because its sample size (n=15) is smaller than TEXTILE's (n=17), and smaller samples are always less robust.", "D": "TEXTILE, because its `HX(4)` value (0.3) is much smaller than ANOVA1's (4.4), and a smaller diagnostic value indicates greater sensitivity."}, "Answer": "A", "pi_justification": "This item assesses the user's ability to apply the paper's diagnostic tools. It uses a Computational Judgment strategy, requiring the user to (1) calculate the theoretical bounds for the `HX(4)` diagnostic for two datasets, (2) compare the observed values to these bounds, and (3) draw the correct conclusion about which design is more sensitive to non-normal errors. The distractors are designed to catch common errors: B confuses the meaning of the lower bound (Conceptual Opposite), C focuses on a true but irrelevant fact about sample size to misdirect from the core diagnostic (True but Irrelevant), and D makes a naive comparison of absolute values without normalization (Formula Misuse).", "qid": "112", "question": "### Background\n\nThis problem investigates the paper's central claim: the robustness of the standard F-test to non-normal errors is critically determined by the characteristics of the regressor design matrix, particularly the presence of high-leverage observations. The analysis hinges on an analytical expression for the deviation of the true significance level (`α*`) from its nominal level (`α`), and on diagnostic measures derived from the regressor matrix.\n\n### Data / Model Specification\n\nThe paper derives the actual significance level `α*` as a function of the nominal level `α`, the error distribution's cumulants (`λᵢ`), and the regressor design matrix (`X`). The relationship is given by:\n\n```latex\n\\alpha^{*} = \\alpha + b(W_{0};\\nu_{2},\\nu_{1})d(W_{0};\\lambda,X)\n```\n\nThe term `d(...)` is an infinite series in the error cumulants (e.g., `λ₃` for skewness, `λ₄` for kurtosis) whose coefficients depend on the regressor matrix. The paper shows that the sensitivity to kurtosis (`λ₄`) is heavily influenced by a diagnostic function of the regressors:\n\n*   **`HX(4)`**: Measures sensitivity to kurtosis (`λ₄`). It is defined as `HX(4) = \\sum_{i=1}^{n} h_{ii}^2`, where `hᵢᵢ` are the diagonal elements of the projection matrix `H` associated with the `q` regressors being tested. A large `HX(4)` value relative to its theoretical range indicates the presence of high-leverage points, which makes the F-test more sensitive to error kurtosis.\n\nThese functions have theoretical bounds, which are provided in **Table 1**. The paper analyzes several datasets, including two contrasting cases whose diagnostic values are provided in **Table 2** for the test of all slope coefficients being zero (`q=k`).\n\n**Table 1: Theoretical Bounds for HX-functions**\n\n| HX-function | Lower bound | Upper bound |\n| :--- | :--- | :--- |\n| `HX(4)` | `q²/n` | `q(1 - 1/n)` |\n\n**Table 2: Computed HX-function Values for Two Datasets (for q=k)**\n\n| Dataset | n | k | q | `HX(4)` |\n| :--- | :-: | :-: | :-: | :--- |\n| TEXTILE | 17 | 2 | 2 | 0.3 |\n| ANOVA1 | 15 | 5 | 5 | 4.4 |\n\n### Question\n\nBased on the theory and data provided, for which dataset is the F-test's validity more likely to be compromised by non-normal error kurtosis (`λ₄`), and why? Your reasoning should be based on the position of the observed `HX(4)` value within its theoretical range.", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine which dataset has an F-test more sensitive to non-normal error kurtosis based on the HX(4) diagnostic measure.\n\nLet me calculate the theoretical bounds for each dataset:\n\nFor TEXTILE (n=17, q=2):\n- Lower bound: q²/n = 4/17 ≈ 0.24\n- Upper bound: q(1-1/n) = 2(16/17) ≈ 1.88\n- Observed HX(4) = 0.3\n\nFor ANOVA1 (n=15, q=5):\n- Lower bound: q²/n = 25/15 ≈ 1.67\n- Upper bound: q(1-1/n) = 5(14/15) ≈ 4.67\n- Observed HX(4) = 4.4\n\nNow I need to assess where each observed value falls within its theoretical range:\n\nTEXTILE: 0.3 is close to the lower bound of 0.24 but still well within the normal range.\n\nANOVA1: 4.4 is very close to the upper bound of 4.67. This indicates that the design matrix has severe leverage points - observations with disproportionate influence on the regression.\n\nThe key insight from the background is that HX(4) measures sensitivity to kurtosis, and high values (approaching the upper bound) indicate the presence of high-leverage points that make the F-test more sensitive to departures from normality in the error distribution.\n\nWhen HX(4) approaches its upper bound, it signals a design matrix configuration where certain observations have excessive leverage, making the test statistic more susceptible to the fourth moment (kurtosis) of the error distribution.\n\nA", "model_answer": "A", "error": null}}
{"ID": 129, "Question": "### Background\n\n**Research Question.** This problem examines the foundational elements of the paper's model: how worker heterogeneity is defined and how optimal contracts are structured in the benchmark private sector.\n\n**Setting / Institutional Environment.** The model features a competitive private sector where workers are paid their marginal product. Workers are heterogeneous along two unobservable dimensions: their intrinsic motivation for public service (`\\gamma_i`) and their disutility from effort (`\\theta_i`).\n\n---\n\n### Data / Model Specification\n\nThere are three worker types: `regular` (r), `dedicated` (m), and `lazy` (l). Their utility is defined by a laziness parameter `\\theta_i` and a motivation parameter `\\gamma_i`, with key assumptions: `0 < \\theta_r = \\theta_m < \\theta_l` and `\\gamma_m > \\gamma_r = \\gamma_l = 0`.\n\nIn the competitive private sector, production is `q(e)=e` and the wage is `w=pe`. A worker's utility is `U_i = pe - \\theta_i C(e)`, where `C'(e)>0` and `C''(e)>0`.\n\n---\n\n### The Question\n\nIn the competitive private sector, a worker chooses effort `e` to maximize their utility. Given the model's assumptions about the laziness parameter `\\theta_i`, which of the following correctly describes the optimal effort levels (`e_i^*`) for the three worker types?", "Options": {"A": "`e_r^* = e_m^* > e_l^*`", "B": "`e_l^* > e_r^* = e_m^*`", "C": "`e_m^* > e_r^* > e_l^*`", "D": "`e_r^* > e_m^* = e_l^*`"}, "Answer": "A", "pi_justification": "This item assesses the foundational understanding of how worker heterogeneity translates into behavior in the benchmark private sector setting. It uses a Computational Judgment strategy. The correct answer requires deriving the FOC `p = \\theta_i C'(e)` and understanding that since `C'' > 0`, a higher `\\theta_i` implies a lower `e_i^*`. Distractor B represents a sign error in this logic. Distractor C incorrectly incorporates public service motivation into the private sector calculation, a common conceptual slip. Distractor D confuses the definitions of the worker types.", "qid": "129", "question": "### Background\n\n**Research Question.** This problem examines the foundational elements of the paper's model: how worker heterogeneity is defined and how optimal contracts are structured in the benchmark private sector.\n\n**Setting / Institutional Environment.** The model features a competitive private sector where workers are paid their marginal product. Workers are heterogeneous along two unobservable dimensions: their intrinsic motivation for public service (`\\gamma_i`) and their disutility from effort (`\\theta_i`).\n\n---\n\n### Data / Model Specification\n\nThere are three worker types: `regular` (r), `dedicated` (m), and `lazy` (l). Their utility is defined by a laziness parameter `\\theta_i` and a motivation parameter `\\gamma_i`, with key assumptions: `0 < \\theta_r = \\theta_m < \\theta_l` and `\\gamma_m > \\gamma_r = \\gamma_l = 0`.\n\nIn the competitive private sector, production is `q(e)=e` and the wage is `w=pe`. A worker's utility is `U_i = pe - \\theta_i C(e)`, where `C'(e)>0` and `C''(e)>0`.\n\n---\n\n### The Question\n\nIn the competitive private sector, a worker chooses effort `e` to maximize their utility. Given the model's assumptions about the laziness parameter `\\theta_i`, which of the following correctly describes the optimal effort levels (`e_i^*`) for the three worker types?", "model_evaluation_result": {"model_raw_response": "I need to analyze the optimal effort choice for each worker type in the competitive private sector.\n\nGiven information:\n- Production function: q(e) = e\n- Wage: w = pe\n- Utility function: U_i = pe - θ_i C(e)\n- Worker types with laziness parameters: 0 < θ_r = θ_m < θ_l\n- Motivation parameters: γ_m > γ_r = γ_l = 0 (but motivation doesn't appear in the private sector utility function)\n- C'(e) > 0 and C''(e) > 0\n\nTo find optimal effort, I maximize utility with respect to effort:\n\nMax U_i = pe - θ_i C(e)\n\nFirst-order condition:\n∂U_i/∂e = p - θ_i C'(e) = 0\n\nThis gives us:\np = θ_i C'(e)\n\nOr: C'(e) = p/θ_i\n\nSince C''(e) > 0, C'(e) is increasing in e. This means higher values of C'(e) correspond to higher effort levels.\n\nFrom the first-order condition:\n- For regular workers: C'(e_r*) = p/θ_r\n- For dedicated workers: C'(e_m*) = p/θ_m  \n- For lazy workers: C'(e_l*) = p/θ_l\n\nSince θ_r = θ_m < θ_l, we have:\n- p/θ_r = p/θ_m > p/θ_l\n- Therefore: C'(e_r*) = C'(e_m*) > C'(e_l*)\n\nSince C'(e) is increasing in e, this implies:\ne_r* = e_m* > e_l*\n\nThe intuition is that workers with higher laziness parameters (θ_l) find effort more costly, so they choose lower effort levels. Regular and dedicated workers have the same laziness parameter in the private sector, so they choose the same effort level.\n\nA", "model_answer": "C", "error": null}}
{"ID": 125, "Question": "### Background\n\n**Research Question.** How does the structure of industry-wide adjustment costs for creating new production units determine the cyclical response of an industry's creation and destruction margins?\n\n**Setting.** In a vintage model of creative destruction with convex creation costs (`c' > 0`), the industry has an incentive to smooth creation over the business cycle. This leads to an incomplete \"insulation effect,\" meaning that both creation and destruction margins respond to demand shocks.\n\n### Data / Model Specification\n\nA government introduces a counter-cyclical subsidy, `s(t)`, for each new unit created (`s(t) > 0` in recessions, `s(t) = 0` in expansions). The private cost of creation becomes `c(f(0,t)) - s(t)`.\n\n### Question\n\nIn the model with convex creation costs (`c' > 0`), what is the most likely combined effect of this subsidy policy during a recession?", "Options": {"A": "It will make job creation less cyclical and job destruction less cyclical, thereby dampening the \"cleansing effect.\"", "B": "It will make job creation less cyclical but amplify job destruction, enhancing the \"cleansing effect.\"", "C": "It will have no effect on job destruction, as the subsidy only targets new firms and does not affect existing ones.", "D": "It will make both job creation and job destruction more cyclical, as the subsidy encourages over-investment."}, "Answer": "A", "pi_justification": "This item uses the Scenario Application strategy, asking students to apply the model's logic to a novel policy intervention. The correct answer requires tracing the policy's effect through both the creation and destruction margins via the general equilibrium price mechanism. Distractor C represents a common partial equilibrium error (ignoring the price effect). Distractor B correctly identifies the direct effect on creation but gets the indirect effect on destruction wrong. Distractor D gets the direction of both effects wrong.", "qid": "125", "question": "### Background\n\n**Research Question.** How does the structure of industry-wide adjustment costs for creating new production units determine the cyclical response of an industry's creation and destruction margins?\n\n**Setting.** In a vintage model of creative destruction with convex creation costs (`c' > 0`), the industry has an incentive to smooth creation over the business cycle. This leads to an incomplete \"insulation effect,\" meaning that both creation and destruction margins respond to demand shocks.\n\n### Data / Model Specification\n\nA government introduces a counter-cyclical subsidy, `s(t)`, for each new unit created (`s(t) > 0` in recessions, `s(t) = 0` in expansions). The private cost of creation becomes `c(f(0,t)) - s(t)`.\n\n### Question\n\nIn the model with convex creation costs (`c' > 0`), what is the most likely combined effect of this subsidy policy during a recession?", "model_evaluation_result": {"model_raw_response": "Looking at this vintage model of creative destruction with convex creation costs, I need to analyze how a counter-cyclical creation subsidy affects both creation and destruction margins during recessions.\n\nKey elements:\n- Convex creation costs (c' > 0) create incentives to smooth creation over time\n- Counter-cyclical subsidy s(t) > 0 in recessions, s(t) = 0 in expansions\n- The subsidy reduces private creation costs to c(f(0,t)) - s(t)\n\nDuring a recession with the subsidy:\n\n**Effect on job creation:**\n- The subsidy directly reduces creation costs during recessions\n- This counteracts the natural decline in creation that would occur due to lower demand\n- Result: Creation becomes less cyclical (less responsive to demand fluctuations)\n\n**Effect on job destruction:**\n- With subsidized creation making it easier to build new productive capacity during recessions, there's less pressure to preserve existing marginal units\n- The subsidy indirectly encourages more destruction of inefficient units during downturns\n- This amplifies the natural \"cleansing effect\" where recessions clear out less productive firms/units\n\n**The \"cleansing effect\":**\n- This refers to recessions' role in eliminating inefficient production units\n- By making creation cheaper during recessions, the subsidy enhances this cleansing process\n- Firms are more willing to destroy inefficient units knowing replacement is subsidized\n\nThe answer is B.\n\nB", "model_answer": "B", "error": null}}
{"ID": 58, "Question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nThe home country's government announces today that it will undertake a temporary fiscal expansion in a future period `s > 0` (`dG_s > 0`). What is the immediate effect on the *current* consumption of the foreign private sector (`c_0^*`), and why?", "Options": {"A": "`c_0^*` increases, because the announcement lowers the interest rates linking the present to the future, raising the present value of the foreign country's income stream.", "B": "`c_0^*` decreases, because the announcement raises the interest rates linking the present to the future, lowering the present value of foreign wealth.", "C": "`c_0^*` is unchanged, because the fiscal expansion has not yet occurred and therefore has no effect on current variables.", "D": "`c_0^*` increases, because foreign consumers save more today in anticipation of higher future taxes in their own country."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item tests the core concept of how anticipated future policies affect current outcomes, a key contribution of the paper. It uses a Scenario Application strategy. The correct answer (A) captures the wealth revaluation mechanism. Distractor (B) reverses the logic. Distractor (C) represents a naive view that ignores rational expectations, a key assumption of the model. Distractor (D) provides a plausible-sounding but incorrect savings story.", "qid": "58", "question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nThe home country's government announces today that it will undertake a temporary fiscal expansion in a future period `s > 0` (`dG_s > 0`). What is the immediate effect on the *current* consumption of the foreign private sector (`c_0^*`), and why?", "model_evaluation_result": {"model_raw_response": "I need to analyze how a future fiscal expansion announcement in the home country affects current foreign consumption through wealth effects and interest rate changes.\n\nLet me work through the transmission mechanism:\n\n1. **Announcement Effect**: When the home country announces a future fiscal expansion (dG_s > 0 at time s > 0), this creates immediate expectations about future economic conditions.\n\n2. **Interest Rate Effect**: The anticipated future fiscal expansion will increase future demand for goods and capital. This raises expected future interest rates, which through arbitrage conditions, affects current interest rates - specifically, current interest rates must rise to maintain equilibrium across time periods.\n\n3. **Wealth Effect on Foreign Country**: Higher current interest rates reduce the present value of future income streams. For the foreign country, this means:\n   - Foreign wealth = Present value of (foreign output - foreign taxes)\n   - With higher discount rates (interest rates), this present value falls\n   - Lower foreign wealth → lower foreign consumption (since consumption is proportional to wealth)\n\n4. **Why Other Options Are Wrong**:\n   - Option A is incorrect: Interest rates rise, not fall\n   - Option C is incorrect: Announcements of future policy have immediate effects through expectations and wealth channels\n   - Option D is incorrect: This describes a saving response that doesn't align with the wealth-based consumption model described\n\nThe key insight is that even though the fiscal expansion is in the future and in another country, the announcement immediately affects interest rates, which changes the present value of wealth, affecting current consumption decisions.\n\nB", "model_answer": "A", "error": null}}
{"ID": 126, "Question": "### Background\n\n**Research Question.** How can the net change in an industry's employment be decomposed into its constituent gross flows, and how do these flows behave during a cyclical downturn?\n\n**Setting.** In a vintage capital model, the total number of production units (or jobs), `N(t)`, evolves through the continuous creation of new units and the destruction of existing units.\n\n### Data / Model Specification\n\nThe endogenous component of job destruction (i.e., scrapping of unprofitable units) is given by the term `f(ā(t),t)[1-ȧ(t)]`, where `ā(t)` is the age of the oldest unit and `ȧ(t)` is its time derivative.\n\n### Question\n\nAt the onset of a sharp, unanticipated recession, the \"cleansing effect\" manifests as a spike in job destruction. Which component of the endogenous destruction rate is primarily responsible for this initial spike?", "Options": {"A": "The term `-f(ā(t),t)ȧ(t)`, because the obsolescence age `ā(t)` falls rapidly, making `ȧ(t)` large and negative.", "B": "The term `f(ā(t),t)`, because the number of firms at the obsolescence margin suddenly increases.", "C": "The exogenous destruction rate `δN(t)`, because failures increase during recessions.", "D": "Both components, `f(ā(t),t)` and `-f(ā(t),t)ȧ(t)`, contribute equally to the spike."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to assess a deep dynamic insight of the model. It asks the student to identify the specific mathematical term responsible for the 'cleansing effect' phenomenon. Distractor B points to the wrong term, reflecting a misunderstanding of stocks vs. flows. Distractor C introduces an out-of-model concept and points to the wrong part of the overall destruction equation. Distractor D reflects an inability to distinguish the primary driver of the dynamic adjustment.", "qid": "126", "question": "### Background\n\n**Research Question.** How can the net change in an industry's employment be decomposed into its constituent gross flows, and how do these flows behave during a cyclical downturn?\n\n**Setting.** In a vintage capital model, the total number of production units (or jobs), `N(t)`, evolves through the continuous creation of new units and the destruction of existing units.\n\n### Data / Model Specification\n\nThe endogenous component of job destruction (i.e., scrapping of unprofitable units) is given by the term `f(ā(t),t)[1-ȧ(t)]`, where `ā(t)` is the age of the oldest unit and `ȧ(t)` is its time derivative.\n\n### Question\n\nAt the onset of a sharp, unanticipated recession, the \"cleansing effect\" manifests as a spike in job destruction. Which component of the endogenous destruction rate is primarily responsible for this initial spike?", "model_evaluation_result": {"model_raw_response": "Looking at this vintage capital model question, I need to analyze what happens to the endogenous job destruction term `f(ā(t),t)[1-ȧ(t)]` during a sharp recession.\n\nLet me break down the components:\n- `f(ā(t),t)` represents the density of firms at the obsolescence age `ā(t)`\n- `ȧ(t)` is the time derivative of the obsolescence age\n- The full expression can be rewritten as: `f(ā(t),t) - f(ā(t),t)ȧ(t)`\n\nDuring a sharp, unanticipated recession:\n\n1. The obsolescence age `ā(t)` falls rapidly because units that were previously profitable become unprofitable due to worsened economic conditions\n2. This means `ȧ(t)` becomes large and negative (the cutoff age is decreasing quickly)\n3. Therefore, `-f(ā(t),t)ȧ(t)` becomes large and positive, since we have a negative times a negative\n\nThe \"cleansing effect\" refers to the accelerated destruction of less productive units during recessions. The initial spike occurs because the margin for what constitutes an \"obsolete\" unit shifts dramatically - units that were viable before the recession suddenly become unviable.\n\nWhile `f(ā(t),t)` (the density at the margin) may also change, the primary driver of the initial spike is the rapid movement of the obsolescence threshold itself, captured by the large negative value of `ȧ(t)`.\n\nA", "model_answer": "A", "error": null}}
{"ID": 213, "Question": "### Background\n\n**Research Question.** This study investigates the behavioral responses of students to receiving a free home laptop, specifically examining how the intervention alters their patterns of computer access and use both at home and in other locations.\n\n**Setting / Institutional Environment.** In a randomized experiment in Lima, Peru, students within designated 'treatment schools' participated in lotteries for XO laptops intended for home use. The analysis compares outcomes for lottery winners (treatment) and non-winners (control) within the same schools.\n\n### Data / Model Specification\n\nThe causal effects are estimated using the following regression model, which includes class fixed effects (`\\mu_j`) and baseline controls (`\\mathbf{X_{ijk}}`):\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\delta Winner_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nTable 1 presents the estimates for the treatment effect `\\delta` on outcomes related to computer access and use.\n\n**Table 1: Effects on Computer Access and Use**\n\n| | Winners | Non-winners | Adjusted Difference (ITT) | Observations |\n| :--- | :---: | :---: | :---: | :---: |\n| **Access** | | | |\n| Computer or laptop at home | 0.89 | 0.50 | 0.40*** (0.03) | 2,851 |\n| **Use by place (last week)** | | | |\n| Home | 0.82 | 0.50 | 0.33*** (0.02) | 2,641 |\n| Internet cafe | 0.43 | 0.54 | -0.11*** (0.02) | 2,631 |\n| Friend's house | 0.20 | 0.23 | -0.02 (0.02) | 2,561 |\n| **Use by place (minutes yesterday)** | | | |\n| Home | 61.85 | 44.99 | 18.20*** (4.93) | 2,851 |\n| Internet cafe | 30.75 | 34.25 | -3.23 (3.67) | 2,851 |\n| Friend's house | 11.30 | 15.44 | -3.78* (2.06) | 2,851 |\n\n*Notes: The 'Adjusted Difference' column presents the estimated `δ`. Standard errors, clustered at the school level, are in parentheses. ***p<0.01, *p<0.10.*\n\nThe estimated effect of winning a laptop on having a 'Computer or laptop at home' is 0.40, not 1.0, because 50% of non-winners already had a computer. Given this and the evidence of substitution in Table 1, why might a voucher program for internet cafes be more cost-effective at increasing students' *total* hours of computer use than the laptop giveaway?", "Options": {"A": "The laptop program provides an inframarginal, in-kind transfer to many students who already have computers, while a voucher directly subsidizes the marginal cost of usage for all students.", "B": "Students prefer using computers in internet cafes over using them at home.", "C": "The XO laptops provided in the program were of lower quality than the computers available in internet cafes.", "D": "The laptop program failed to increase device-specific OLPC skills, making it ineffective."}, "Answer": "A", "pi_justification": "This item assesses high-level application of microeconomic principles (inframarginal transfers, marginal cost) to evaluate a policy alternative, synthesizing multiple pieces of evidence from the table. It uses a Scenario Application strategy. Distractors include an unsupported claim (B), a true but irrelevant fact (C), and a factually incorrect statement about the study's other findings (D).", "qid": "213", "question": "### Background\n\n**Research Question.** This study investigates the behavioral responses of students to receiving a free home laptop, specifically examining how the intervention alters their patterns of computer access and use both at home and in other locations.\n\n**Setting / Institutional Environment.** In a randomized experiment in Lima, Peru, students within designated 'treatment schools' participated in lotteries for XO laptops intended for home use. The analysis compares outcomes for lottery winners (treatment) and non-winners (control) within the same schools.\n\n### Data / Model Specification\n\nThe causal effects are estimated using the following regression model, which includes class fixed effects (`\\mu_j`) and baseline controls (`\\mathbf{X_{ijk}}`):\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\delta Winner_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nTable 1 presents the estimates for the treatment effect `\\delta` on outcomes related to computer access and use.\n\n**Table 1: Effects on Computer Access and Use**\n\n| | Winners | Non-winners | Adjusted Difference (ITT) | Observations |\n| :--- | :---: | :---: | :---: | :---: |\n| **Access** | | | |\n| Computer or laptop at home | 0.89 | 0.50 | 0.40*** (0.03) | 2,851 |\n| **Use by place (last week)** | | | |\n| Home | 0.82 | 0.50 | 0.33*** (0.02) | 2,641 |\n| Internet cafe | 0.43 | 0.54 | -0.11*** (0.02) | 2,631 |\n| Friend's house | 0.20 | 0.23 | -0.02 (0.02) | 2,561 |\n| **Use by place (minutes yesterday)** | | | |\n| Home | 61.85 | 44.99 | 18.20*** (4.93) | 2,851 |\n| Internet cafe | 30.75 | 34.25 | -3.23 (3.67) | 2,851 |\n| Friend's house | 11.30 | 15.44 | -3.78* (2.06) | 2,851 |\n\n*Notes: The 'Adjusted Difference' column presents the estimated `δ`. Standard errors, clustered at the school level, are in parentheses. ***p<0.01, *p<0.10.*\n\nThe estimated effect of winning a laptop on having a 'Computer or laptop at home' is 0.40, not 1.0, because 50% of non-winners already had a computer. Given this and the evidence of substitution in Table 1, why might a voucher program for internet cafes be more cost-effective at increasing students' *total* hours of computer use than the laptop giveaway?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 131, "Question": "### Background\n\n**Research Question.** This problem examines the foundational elements of the paper's model, specifically how a cost-minimizing public agency designs an optimal contract for its most motivated workers.\n\n**Setting / Institutional Environment.** A public agency hires only `dedicated` workers (`m`) to produce a quantity `Q`. Dedicated workers are defined by having public service motivation (`\\gamma_m > 0`) and standard disutility of effort (`\\theta_m = \\theta_r`). The agency minimizes total costs.\n\n---\n\n### Data / Model Specification\n\nThe utility of a dedicated worker in the public sector is `U_m = w + \\gamma_m V(e) - \\theta_m C(e)`, where `V(e)` is a concave function (`V' > 0, V'' < 0`). The worker's optimal effort in the private sector is `e_m^*`.\n\n---\n\n### The Question\n\nWhen the public agency hires only `dedicated` workers (`m`), what is the optimal effort level `e_m` it requires, and what is the economic intuition for this choice?", "Options": {"A": "`e_m < e_m^*`, because the agency maximizes total extracted rent from motivation by hiring more workers at a lower effort level where the marginal motivational rent is higher.", "B": "`e_m = e_m^*`, because the agency replicates the efficient private sector contract to minimize costs.", "C": "`e_m > e_m^*`, because the agency takes advantage of the dedicated workers' motivation to demand extra effort for the same pay.", "D": "`e_m < e_m^*`, because requiring less effort is a way to reward dedicated workers for their public service motivation."}, "Answer": "A", "pi_justification": "This item assesses understanding of one of the paper's key preliminary results: the structure of the optimal contract for the most motivated workers. It uses a Scenario Application strategy. The correct answer identifies both the correct outcome (`e_m < e_m^*`) and the correct economic reason related to maximizing total rent from a concave motivation function. Distractor B incorrectly generalizes the result from other worker types. Distractor C represents a common but incorrect intuition about exploitation. Distractor D is an 'Almost Right' option that gets the direction correct but provides a behavioral explanation instead of the correct optimization-based one.", "qid": "131", "question": "### Background\n\n**Research Question.** This problem examines the foundational elements of the paper's model, specifically how a cost-minimizing public agency designs an optimal contract for its most motivated workers.\n\n**Setting / Institutional Environment.** A public agency hires only `dedicated` workers (`m`) to produce a quantity `Q`. Dedicated workers are defined by having public service motivation (`\\gamma_m > 0`) and standard disutility of effort (`\\theta_m = \\theta_r`). The agency minimizes total costs.\n\n---\n\n### Data / Model Specification\n\nThe utility of a dedicated worker in the public sector is `U_m = w + \\gamma_m V(e) - \\theta_m C(e)`, where `V(e)` is a concave function (`V' > 0, V'' < 0`). The worker's optimal effort in the private sector is `e_m^*`.\n\n---\n\n### The Question\n\nWhen the public agency hires only `dedicated` workers (`m`), what is the optimal effort level `e_m` it requires, and what is the economic intuition for this choice?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze how a cost-minimizing public agency would set the optimal effort level for dedicated workers with public service motivation.\n\nKey elements:\n- Dedicated workers have utility: U_m = w + γ_m V(e) - θ_m C(e)\n- They have public service motivation (γ_m > 0) \n- V(e) is concave (V' > 0, V'' < 0)\n- e_m* is their optimal effort in the private sector\n- The agency wants to minimize total costs\n\nThe crucial insight is understanding how public service motivation affects the optimal contract design.\n\nIn the private sector, dedicated workers choose e_m* to maximize their own utility. In the public sector, the agency can leverage their public service motivation (γ_m > 0) to extract additional effort.\n\nSince dedicated workers derive utility from the public value V(e) of their effort, the agency can require higher effort levels than the private sector optimum while still satisfying the workers' participation constraint. The workers are willing to exert this extra effort because they value the public benefit it creates.\n\nFrom a cost-minimization perspective, the agency optimally exploits this motivation by demanding effort levels above the private sector benchmark. This allows the agency to get more output per dollar spent on wages.\n\nThe economic intuition is that public service motivation creates a \"wedge\" that allows the agency to extract additional effort without proportionally increasing compensation, since workers derive direct utility from contributing to public value.\n\nC", "model_answer": "C", "error": null}}
{"ID": 216, "Question": "### Background\n\n**Research Question.** A key concern with the study's main within-school randomization is that the treatment effect estimates could be biased by spillovers, where non-winning students are affected by their peers who received laptops. This analysis explores two distinct strategies to identify and quantify such spillovers.\n\n### Data / Model Specification\n\nTwo strategies were used to test for spillovers:\n1.  **Friends Strategy:** Compared non-winners with winner friends to non-winners without winner friends. This analysis found no significant spillovers on academic or cognitive skills.\n2.  **DiD Strategy:** Compared non-winners in treatment schools to students in control schools over time. The results are in Table 1.\n\n**Table 1: DiD Spillover Effects on Non-winners in Treatment Schools**\n\n| Outcome | DiD Estimate (`η`) |\n| :--- | :---: |\n| Objective OLPC test | 0.24** (0.11) |\n| Math | -0.16 (0.17) |\n| Cognitive skills (Raven's) | 0.09 (0.06) |\n\n*Notes: **p<0.05.*\n\nA critic claims the study's main null effects on academic skills are biased towards zero due to positive spillovers contaminating the control group. Based on the provided analyses, what is the strongest rebuttal to this claim?", "Options": {"A": "The DiD analysis, the most comprehensive spillover test, found no statistically significant spillovers on cognitive or academic skills, even while it successfully detected spillovers on device-specific skills.", "B": "The 'Friends' analysis proved that spillovers only occur between close friends, so the broader control group was unaffected.", "C": "Both spillover analyses found no evidence of any spillovers whatsoever, for any outcome.", "D": "The main analysis already controlled for spillovers by including class fixed effects."}, "Answer": "A", "pi_justification": "This item assesses the ability to synthesize evidence from multiple, complex analyses to construct a coherent scientific argument. It uses a Scenario Application strategy (rebutting a critique). Distractors include an incomplete argument (B), a factually incorrect summary of the results (C), and a fundamental econometric misconception about the role of fixed effects (D).", "qid": "216", "question": "### Background\n\n**Research Question.** A key concern with the study's main within-school randomization is that the treatment effect estimates could be biased by spillovers, where non-winning students are affected by their peers who received laptops. This analysis explores two distinct strategies to identify and quantify such spillovers.\n\n### Data / Model Specification\n\nTwo strategies were used to test for spillovers:\n1.  **Friends Strategy:** Compared non-winners with winner friends to non-winners without winner friends. This analysis found no significant spillovers on academic or cognitive skills.\n2.  **DiD Strategy:** Compared non-winners in treatment schools to students in control schools over time. The results are in Table 1.\n\n**Table 1: DiD Spillover Effects on Non-winners in Treatment Schools**\n\n| Outcome | DiD Estimate (`η`) |\n| :--- | :---: |\n| Objective OLPC test | 0.24** (0.11) |\n| Math | -0.16 (0.17) |\n| Cognitive skills (Raven's) | 0.09 (0.06) |\n\n*Notes: **p<0.05.*\n\nA critic claims the study's main null effects on academic skills are biased towards zero due to positive spillovers contaminating the control group. Based on the provided analyses, what is the strongest rebuttal to this claim?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 101, "Question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `AC`: Average cost of care.\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe paper shows that under both profit-maximization and output-maximization models, a hospital has an incentive to inflate its costs if and only if the following condition holds:\n```latex\n\\frac{q_m}{q_n + q_m} > \\frac{1}{k}\n```\n\nA hospital operates in an environment described by these models. If its cost-reimbursement plan includes a 5% 'plus' factor (i.e., `k=1.05`), what is the minimum percentage of the hospital's patients that must be covered by this plan for the hospital to find it advantageous to increase costs?", "Options": {"A": "5.0%", "B": "80.0%", "C": "95.2%", "D": "99.5%"}, "Answer": "C", "pi_justification": "This item tests the student's ability to apply the derived theoretical condition to a concrete numerical scenario. It uses a Computational Judgment strategy. The correct option (C) requires correct substitution and calculation. Distractors are based on predictable errors: (A) misuses the 'plus' factor directly as the threshold (Formula Misuse), (B) is a salient but irrelevant number from the text (True but Irrelevant), and (D) represents a plausible calculation slip (Numerical Error).", "qid": "101", "question": "### Background\n\n**Research Question.** This problem investigates the theoretical conditions under which a hospital has an economic incentive to inflate its own costs when a portion of its patients are covered by cost-plus reimbursement plans.\n\n### Data / Model Specification\n\nLet the following variables and parameters define the hospital's environment:\n- `q_n`: Number of patients not covered by a cost-reimbursement scheme.\n- `q_m`: Number of patients covered by a cost-reimbursement scheme.\n- `AC`: Average cost of care.\n- `k`: The cost-reimbursement factor, representing the proportion of average cost reimbursed (e.g., `k=1.05` for a 5% 'plus' factor).\n\nThe paper shows that under both profit-maximization and output-maximization models, a hospital has an incentive to inflate its costs if and only if the following condition holds:\n```latex\n\\frac{q_m}{q_n + q_m} > \\frac{1}{k}\n```\n\nA hospital operates in an environment described by these models. If its cost-reimbursement plan includes a 5% 'plus' factor (i.e., `k=1.05`), what is the minimum percentage of the hospital's patients that must be covered by this plan for the hospital to find it advantageous to increase costs?", "model_evaluation_result": {"model_raw_response": "I need to find the minimum percentage of patients covered by the cost-reimbursement plan for the hospital to have an incentive to inflate costs.\n\nGiven information:\n- Cost-reimbursement factor: k = 1.05 (5% plus factor)\n- Condition for cost inflation incentive: q_m/(q_n + q_m) > 1/k\n\nFirst, let me calculate 1/k:\n1/k = 1/1.05 = 0.9524 (approximately)\n\nThe condition becomes:\nq_m/(q_n + q_m) > 0.9524\n\nSince q_m/(q_n + q_m) represents the fraction of patients covered by the cost-reimbursement plan, I need this fraction to exceed 0.9524.\n\nConverting to percentage:\n0.9524 × 100% = 95.24%\n\nSince the hospital needs the fraction to be strictly greater than 1/k, the minimum percentage must be slightly above 95.24%.\n\nLooking at the options:\n- A) 5.0% - Too low\n- B) 80.0% - Too low  \n- C) 95.2% - This is less than 95.24%, so insufficient\n- D) 99.5% - This exceeds 95.24%, so this would satisfy the condition\n\nThe answer is D) 99.5%, as this is the only option that exceeds the threshold of approximately 95.24%.\n\nD", "model_answer": "A", "error": null}}
{"ID": 18, "Question": "### Background\n\nThe final preferred model for the **Light industry** was a Fixed Effect specification. In this industry, private sector R&D accounts for 80% of total industrial R&D expenditure.\n\n### Data / Model Specification\n\n**Table 1: Final Selected Model Estimates for the Light Industry**\n| Variables | Light Industry (Fixed Effect) |\n|:---|---:|\n| LNRND ($\\beta$) | 0.012 (1.16) |\n| LNSRDEXP ($\\alpha$) | 0.357 (5.34) |\n*t-statistics in parentheses.*\n\n### Question\n\nSuppose a policy induces a 10% increase in the R&D expenditures of *all private firms* in the Light industry. Based on the statistically significant effects in the model, what is the estimated total percentage increase in a representative firm's output?", "Options": {"A": "0.12%", "B": "2.86%", "C": "2.98%", "D": "3.57%"}, "Answer": "B", "pi_justification": "This item replaces the most complex part of a larger QA problem, using a Computational Judgment strategy. It tests the ability to perform a multi-step policy counterfactual calculation. The correct answer (B) requires: (1) calculating the increase in the spillover stock (10% * 0.80 = 8%), (2) applying the spillover elasticity (0.357 * 8% = 2.856%), and (3) correctly ignoring the own-R&D effect because its coefficient is statistically insignificant (t=1.16). Distractor A is a Step-Omission Error (only calculates the private return and ignores significance). Distractor C is a Conceptual Error (adds the insignificant private return to the social return). Distractor D is a Calculation Error (forgets to weight the spillover increase by the 80% private share).", "qid": "18", "question": "### Background\n\nThe final preferred model for the **Light industry** was a Fixed Effect specification. In this industry, private sector R&D accounts for 80% of total industrial R&D expenditure.\n\n### Data / Model Specification\n\n**Table 1: Final Selected Model Estimates for the Light Industry**\n| Variables | Light Industry (Fixed Effect) |\n|:---|---:|\n| LNRND ($\\beta$) | 0.012 (1.16) |\n| LNSRDEXP ($\\alpha$) | 0.357 (5.34) |\n*t-statistics in parentheses.*\n\n### Question\n\nSuppose a policy induces a 10% increase in the R&D expenditures of *all private firms* in the Light industry. Based on the statistically significant effects in the model, what is the estimated total percentage increase in a representative firm's output?", "model_evaluation_result": {"model_raw_response": "I need to analyze the fixed effects model results for the Light industry to determine the impact of a 10% increase in private R&D expenditure on firm output.\n\nGiven information:\n- Private sector R&D accounts for 80% of total industrial R&D expenditure\n- LNRND coefficient (β) = 0.012 (t-stat = 1.16) - not statistically significant\n- LNSRDEXP coefficient (α) = 0.357 (t-stat = 5.34) - statistically significant\n\nThe question asks for the effect based on \"statistically significant effects in the model.\" The t-statistic of 1.16 for LNRND is not statistically significant (typically need |t| > 1.96 for 5% significance), while the t-statistic of 5.34 for LNSRDEXP is highly significant.\n\nSince we should only consider statistically significant effects, I need to focus on LNSRDEXP.\n\nThe policy increases R&D expenditures of all private firms by 10%. Since private sector R&D accounts for 80% of total industrial R&D expenditure, this represents an increase in total industrial R&D.\n\nHowever, the question asks about the effect on a representative firm's output, and we only have the statistically significant coefficient for LNSRDEXP (0.357).\n\nA 10% increase in the relevant R&D variable would lead to:\n0.357 × ln(1.10) = 0.357 × 0.0953 = 0.034 = 3.4%\n\nWait, let me reconsider. The coefficient 0.357 represents the elasticity, so a 10% increase would yield:\n0.357 × 10% = 3.57%\n\nD", "model_answer": "D", "error": null}}
{"ID": 139, "Question": "### Background\n\nConsider a game-theoretic model of drilling timing where two firms with adjacent leases decide each period whether to drill. The payoff from drilling now is `$V_D = p-c$`. The expected payoff from waiting one period is `$V_W = q \\cdot [\\delta p(1-c)] + (1-q) \\cdot [\\delta(p-c)]$`, where `$q$` is the rival's probability of drilling and `$\\delta$` is the discount factor.\n\n### Question\n\nIn the symmetric mixed-strategy equilibrium, each firm must be indifferent between drilling and waiting, so `$V_D = V_W$`. By setting the two payoff expressions equal and solving for `$q$`, what is the equilibrium probability that a firm drills in any given period (prior to the deadline)?", "Options": {"A": "```latex\nq = \\frac{(1-\\delta)(p-c)}{\\delta c(1-p)}\n```", "B": "```latex\nq = \\frac{(1+\\delta)(p-c)}{\\delta c(1-p)}\n```", "C": "```latex\nq = \\frac{\\delta c(1-p)}{(1-\\delta)(p-c)}\n```", "D": "```latex\nq = \\frac{(p-c)}{\\delta c(1-p)}\n```"}, "Answer": "A", "pi_justification": "This item assesses the ability to execute the mathematical derivation of a core theoretical result from the paper. It uses a 'Computational Judgment' strategy. Option A is the correct derivation. Option B introduces a sign error in the discount term `(1+δ)`, a common algebraic slip. Option C inverts the numerator and denominator, a mistake that can happen when isolating `q`. Option D omits the `(1-δ)` term, representing a step-omission error in the algebra.", "qid": "139", "question": "### Background\n\nConsider a game-theoretic model of drilling timing where two firms with adjacent leases decide each period whether to drill. The payoff from drilling now is `$V_D = p-c$`. The expected payoff from waiting one period is `$V_W = q \\cdot [\\delta p(1-c)] + (1-q) \\cdot [\\delta(p-c)]$`, where `$q$` is the rival's probability of drilling and `$\\delta$` is the discount factor.\n\n### Question\n\nIn the symmetric mixed-strategy equilibrium, each firm must be indifferent between drilling and waiting, so `$V_D = V_W$`. By setting the two payoff expressions equal and solving for `$q$`, what is the equilibrium probability that a firm drills in any given period (prior to the deadline)?", "model_evaluation_result": {"model_raw_response": "I need to solve for the equilibrium probability q by setting VD = VW.\n\nGiven:\n- VD = p - c\n- VW = q · [δp(1-c)] + (1-q) · [δ(p-c)]\n\nSetting VD = VW:\np - c = q · [δp(1-c)] + (1-q) · [δ(p-c)]\n\nLet me expand the right side:\np - c = q · δp(1-c) + (1-q) · δ(p-c)\np - c = qδp(1-c) + δ(p-c) - qδ(p-c)\np - c = qδp(1-c) + δ(p-c) - qδ(p-c)\np - c = δ(p-c) + qδp(1-c) - qδ(p-c)\np - c = δ(p-c) + qδ[p(1-c) - (p-c)]\np - c = δ(p-c) + qδ[p - pc - p + c]\np - c = δ(p-c) + qδ[c - pc]\np - c = δ(p-c) + qδc(1-p)\n\nNow solving for q:\np - c - δ(p-c) = qδc(1-p)\n(p-c)(1-δ) = qδc(1-p)\nq = (p-c)(1-δ) / [δc(1-p)]\nq = (1-δ)(p-c) / [δc(1-p)]\n\nThis matches option A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 214, "Question": "### Background\n\n**Research Question.** A key concern with the study's main within-school randomization is that the treatment effect estimates could be biased by spillovers. This analysis explores strategies to identify and quantify such spillovers.\n\n### Data / Model Specification\n\n**Strategy 1: Spillovers across Friendship Networks**\nThis strategy focuses on the sample of non-winners and compares those with friends who won a laptop to those without. The model is:\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\gamma Friend_{ijk} + \\lambda N_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nwhere `Friend` is an indicator for having a winner friend and `N` is the student's total number of friends, measured at baseline. The key assumption is that, conditional on `N`, `Friend` is as good as random.\n\nIn the 'Spillovers across Friendship Networks' analysis (Eq. 1), why is it crucial to control for `N_ijk`, the total number of friends a student has?", "Options": {"A": "To prevent omitted variable bias, as students with more friends are mechanically more likely to have a winner friend and may also have systematically different outcomes.", "B": "To ensure the sample is representative of the entire school population.", "C": "To measure the direct effect of popularity, which is the main outcome of interest.", "D": "Because the number of friends is likely affected by the treatment, making it a bad control."}, "Answer": "A", "pi_justification": "This item assesses understanding of a specific econometric concept: the role of a control variable in preventing omitted variable bias in a network context. It uses a Reverse-Reasoning strategy (why is this control needed?). Distractors are based on confusing internal with external validity (B), confusing a control with an outcome (C), and misapplying the 'bad control' concept (D).", "qid": "214", "question": "### Background\n\n**Research Question.** A key concern with the study's main within-school randomization is that the treatment effect estimates could be biased by spillovers. This analysis explores strategies to identify and quantify such spillovers.\n\n### Data / Model Specification\n\n**Strategy 1: Spillovers across Friendship Networks**\nThis strategy focuses on the sample of non-winners and compares those with friends who won a laptop to those without. The model is:\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\gamma Friend_{ijk} + \\lambda N_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nwhere `Friend` is an indicator for having a winner friend and `N` is the student's total number of friends, measured at baseline. The key assumption is that, conditional on `N`, `Friend` is as good as random.\n\nIn the 'Spillovers across Friendship Networks' analysis (Eq. 1), why is it crucial to control for `N_ijk`, the total number of friends a student has?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 215, "Question": "### Background\n\n**Research Question.** A key concern with the study's main within-school randomization is that the treatment effect estimates could be biased by spillovers. This analysis explores strategies to identify and quantify such spillovers.\n\n### Data / Model Specification\n\n**Strategy: Spillovers across Classmates (Difference-in-Differences)**\nThis strategy compares all non-winners in treatment schools to all students in control schools using baseline and follow-up data. This strategy is used because treatment and control schools were not balanced at baseline, as shown in Table 1.\n\n**Table 1: Balance for 'DiD' Strategy (All Students at Baseline)**\n\n| | Treatment schools | Control schools | Adjusted Difference |\n| :--- | :---: | :---: | :---: |\n| Raven's progressive matrices | -0.06 | 0.14 | -0.22*** (0.04) |\n\n*Notes: ***p<0.01.*\n\nBased on the evidence in Table 1, why was the Difference-in-Differences (DiD) strategy necessary for estimating spillovers on classmates, and what is its key identifying assumption?", "Options": {"A": "Because treatment and control schools were not balanced at baseline; the DiD assumption is that their outcomes would have followed parallel trends in the absence of the intervention.", "B": "Because attrition was high in the control schools; the DiD assumption is that attrition was random.", "C": "Because treatment and control schools were balanced at baseline; the DiD assumption is that this balance would persist over time.", "D": "Because the spillover effect was expected to be negative; the DiD assumption is that the treatment effect is constant across all students."}, "Answer": "A", "pi_justification": "This item tests the ability to justify the choice of an identification strategy (DiD) based on a balance table and to correctly state its core assumption. It uses a Reverse-Reasoning strategy. Distractors are based on incorrect reasons for using DiD (B), factually incorrect premises from the table (C), and irrelevant logic paired with an incorrect assumption (D).", "qid": "215", "question": "### Background\n\n**Research Question.** A key concern with the study's main within-school randomization is that the treatment effect estimates could be biased by spillovers. This analysis explores strategies to identify and quantify such spillovers.\n\n### Data / Model Specification\n\n**Strategy: Spillovers across Classmates (Difference-in-Differences)**\nThis strategy compares all non-winners in treatment schools to all students in control schools using baseline and follow-up data. This strategy is used because treatment and control schools were not balanced at baseline, as shown in Table 1.\n\n**Table 1: Balance for 'DiD' Strategy (All Students at Baseline)**\n\n| | Treatment schools | Control schools | Adjusted Difference |\n| :--- | :---: | :---: | :---: |\n| Raven's progressive matrices | -0.06 | 0.14 | -0.22*** (0.04) |\n\n*Notes: ***p<0.01.*\n\nBased on the evidence in Table 1, why was the Difference-in-Differences (DiD) strategy necessary for estimating spillovers on classmates, and what is its key identifying assumption?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 29, "Question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Banking Sector:** The loan market is contestable, forcing banks to set the lowest feasible lending rate `R_p`. A bank will choose to initiate monitoring (`m > 0`) only if the marginal benefit of doing so exceeds the marginal cost. The marginal type `p*` is the entrepreneur type for whom a bank is indifferent between `m=0` and initiating monitoring.\n\n### Question\n\nGiven the quadratic cost function, what is the expression for the marginal entrepreneur type `p*` that separates the non-monitoring sector (`p ≤ p*`) from the monitoring sector (`p > p*`) in terms of the safe rate `R_0` and the cost parameter `γ`?", "Options": {"A": "`p* = 1 - sqrt(R_0/γ)`", "B": "`p* = 1 + sqrt(R_0/γ)`", "C": "`p* = 1 - (R_0/γ)`", "D": "`p* = 1 - sqrt(γ/R_0)`"}, "Answer": "A", "pi_justification": "Assesses the student's ability to derive the key threshold for the extensive margin of risk. Strategy: Computational Judgment. Distractors are based on common algebraic errors: B (sign error on the square root), C (omitting the square root), and D (inverting the fraction within the root).", "qid": "29", "question": "### Background\n\nThis problem analyzes the core mechanism of the paper's static general equilibrium model. The economy consists of a continuum of penniless entrepreneurs, each characterized by an observable risk type `p ∈ [0, 1]`, a competitive banking sector, and investors who provide a fixed aggregate supply of savings, `w`.\n\n### Data / Model Specification\n\n- **Entrepreneurs and Projects:** An entrepreneur of type `p` has a project that requires one unit of investment and fails with baseline probability `p`. Bank monitoring, `m ∈ [0, p]`, can increase the success probability to `1 - p + m`.\n- **Monitoring Cost:** The cost of monitoring is quadratic: `c(m) = (γ/2)m²`, where `γ > 0`.\n- **Banking Sector:** The loan market is contestable, forcing banks to set the lowest feasible lending rate `R_p`. A bank will choose to initiate monitoring (`m > 0`) only if the marginal benefit of doing so exceeds the marginal cost. The marginal type `p*` is the entrepreneur type for whom a bank is indifferent between `m=0` and initiating monitoring.\n\n### Question\n\nGiven the quadratic cost function, what is the expression for the marginal entrepreneur type `p*` that separates the non-monitoring sector (`p ≤ p*`) from the monitoring sector (`p > p*`) in terms of the safe rate `R_0` and the cost parameter `γ`?", "model_evaluation_result": {"model_raw_response": "I need to find the marginal entrepreneur type p* that separates the non-monitoring and monitoring sectors.\n\nLet me analyze the bank's decision problem for monitoring.\n\nFor an entrepreneur of type p, the bank chooses monitoring level m to maximize expected profit. The success probability becomes (1 - p + m) with monitoring cost c(m) = (γ/2)m².\n\nThe bank's expected profit from lending to type p is:\nπ = (1 - p + m)R_p - R_0 - (γ/2)m²\n\nwhere R_0 is the safe rate (cost of funds) and R_p is the lending rate.\n\nTaking the first-order condition with respect to m:\n∂π/∂m = R_p - γm = 0\n\nThis gives the optimal monitoring level: m* = R_p/γ\n\nHowever, monitoring is only feasible if m* ≤ p (can't monitor more than the baseline failure probability).\n\nAt the marginal type p*, the bank is indifferent between monitoring (m > 0) and not monitoring (m = 0).\n\nFor the marginal type, the optimal monitoring level equals the constraint: m* = p*\n\nSo: p* = R_p*/γ\n\nIn competitive equilibrium, banks earn zero profit, so:\n(1 - p* + m*)R_p* = R_0 + (γ/2)(m*)²\n\nSubstituting m* = p* and R_p* = γp*:\n(1 - p* + p*)γp* = R_0 + (γ/2)(p*)²\nγp* = R_0 + (γ/2)(p*)²\nγp* - (γ/2)(p*)² = R_0\nγp*(1 - p*/2) = R_0\n\nRearranging:\nγp* - (γ/2)(p*)² = R_0\n(γ/2)(p*)² - γp* + R_0 = 0\n\nUsing the quadratic formula:\np* = [γ ± √(γ² - 4(γ/2)R_0)] / γ\np*", "model_answer": "I need to find the marginal entrepreneur type p* that separates the non-monitoring and monitoring sectors.\n\nLet me analyze the bank's decision problem for monitoring.\n\nFor an entrepreneur of type p, the bank chooses monitoring level m to maximize expected profit. The success probability becomes (1 - p + m) with monitoring cost c(m) = (γ/2)m².\n\nThe bank's expected profit from lending to type p is:\nπ = (1 - p + m)R_p - R_0 - (γ/2)m²\n\nwhere R_0 is the safe rate (cost of funds) and R_p is the lending rate.\n\nTaking the first-order condition with respect to m:\n∂π/∂m = R_p - γm = 0\n\nThis gives the optimal monitoring level: m* = R_p/γ\n\nHowever, monitoring is only feasible if m* ≤ p (can't monitor more than the baseline failure probability).\n\nAt the marginal type p*, the bank is indifferent between monitoring (m > 0) and not monitoring (m = 0).\n\nFor the marginal type, the optimal monitoring level equals the constraint: m* = p*\n\nSo: p* = R_p*/γ\n\nIn competitive equilibrium, banks earn zero profit, so:\n(1 - p* + m*)R_p* = R_0 + (γ/2)(m*)²\n\nSubstituting m* = p* and R_p* = γp*:\n(1 - p* + p*)γp* = R_0 + (γ/2)(p*)²\nγp* = R_0 + (γ/2)(p*)²\nγp* - (γ/2)(p*)² = R_0\nγp*(1 - p*/2) = R_0\n\nRearranging:\nγp* - (γ/2)(p*)² = R_0\n(γ/2)(p*)² - γp* + R_0 = 0\n\nUsing the quadratic formula:\np* = [γ ± √(γ² - 4(γ/2)R_0)] / γ\np*", "error": "Warning: Could not extract single choice letter from: I need to find the marginal entrepreneur type p* t..."}}
{"ID": 151, "Question": "## Background\n\n**Research Question.** This problem evaluates how the experimental design distinguishes between competing behavioral models of giving (pure altruism vs. warm-glow).\n\n**Setting.** The `Subs` experiment involved a single donor (N=1) for a cause, while the `Subs-M` experiment involved two donors (N=2) for the same cause.\n\n---\n\n## Data / Model Specification\n\nThe pure altruism model assumes an agent's utility depends on the total amount donated to a cause, `G`, regardless of who contributes. This implies that the equilibrium total donation `G*` is determined by the marginal costs and benefits of giving and is independent of the number of donors `N`.\n\n---\n\n## Question\n\nUnder the pure altruism model, what is the specific, testable prediction when comparing the average *individual* donation in the single-donor `Subs` experiment to the average *individual* donation in the two-donor `Subs-M` experiment?", "Options": {"A": "The average individual donation will be half as large in `Subs-M` as in `Subs`.", "B": "The average individual donation will be the same in both experiments.", "C": "The total donation (summed across donors) will be twice as large in `Subs-M` as in `Subs`.", "D": "The average individual donation will be negative in `Subs-M` due to free-riding."}, "Answer": "A", "pi_justification": "This item assesses the ability to derive a sharp prediction from a theoretical model, a form of Computational Judgment. The correct answer (A) correctly applies the logic of the pure altruism model: if total giving `G*` is constant, and `N` goes from 1 to 2, each individual's contribution must be `G*/2`. Distractor B is the prediction of a pure warm-glow model (Conceptual Opposite). Distractor C confuses individual and total giving; the model predicts total giving will be the same, not double. Distractor D is an illogical exaggeration of the free-riding effect.", "qid": "151", "question": "## Background\n\n**Research Question.** This problem evaluates how the experimental design distinguishes between competing behavioral models of giving (pure altruism vs. warm-glow).\n\n**Setting.** The `Subs` experiment involved a single donor (N=1) for a cause, while the `Subs-M` experiment involved two donors (N=2) for the same cause.\n\n---\n\n## Data / Model Specification\n\nThe pure altruism model assumes an agent's utility depends on the total amount donated to a cause, `G`, regardless of who contributes. This implies that the equilibrium total donation `G*` is determined by the marginal costs and benefits of giving and is independent of the number of donors `N`.\n\n---\n\n## Question\n\nUnder the pure altruism model, what is the specific, testable prediction when comparing the average *individual* donation in the single-donor `Subs` experiment to the average *individual* donation in the two-donor `Subs-M` experiment?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand what the pure altruism model predicts about individual donations when the number of donors changes.\n\nKey facts from the problem:\n- Pure altruism model: utility depends on total amount donated (G), regardless of who contributes\n- The equilibrium total donation G* is independent of the number of donors N\n- Subs experiment: N=1 (single donor)\n- Subs-M experiment: N=2 (two donors)\n\nUnder pure altruism, if the total donation G* is the same regardless of whether there's 1 or 2 donors, then:\n\n- In Subs (N=1): Individual donation = G*\n- In Subs-M (N=2): Individual donation = G*/2 (since two people split the same total)\n\nThis means the average individual donation in Subs-M will be exactly half the individual donation in Subs.\n\nLooking at the options:\nA) States individual donation will be half as large in Subs-M - this matches the prediction\nB) States individual donation will be the same - this would contradict the model\nC) States total donation will be twice as large in Subs-M - this contradicts the model (total should be the same)\nD) States individual donation will be negative - this doesn't follow from the model\n\nA", "model_answer": "A", "error": null}}
{"ID": 142, "Question": "### Background\n\n**Research Question.** You are empirically testing the opposing predictions about how two types of on-board computers (OBCs) affect a shipper's decision to own its own trucks.\n\n### Data / Model Specification\n\nYou estimate the following linear probability model for shipper ownership:\n`ShipperOwn_i = β_0 + β_M OBC_{Mi} + β_L OBC_{Li} + X_i'δ + ε_i`\n\nWhere:\n- `OBC_M` is a monitoring technology.\n- `OBC_L` is a logistics technology.\n\nYour theory predicts:\n- `β_M > 0` (monitoring tech increases shipper ownership).\n- `β_L < 0` (logistics tech decreases shipper ownership).\n\nHowever, you face two empirical challenges:\n1.  Many OBCs bundle both features, so `Corr(OBC_M, OBC_L) > 0`.\n2.  Adoption of any advanced technology is correlated with unobserved firm sophistication, which itself favors private fleets. So, `Corr(OBC_L, Sophistication) > 0` and the effect of `Sophistication` on `ShipperOwn` is positive.\n\n### The Question\n\nGiven this scenario, what is the most likely direction of the bias on the OLS estimate of `β_L`, the coefficient for the logistics OBC, when `Sophistication` is an omitted variable?", "Options": {"A": "Positive bias, making the estimate appear less negative, or even positive.", "B": "Negative bias, making the estimate appear more negative than it truly is.", "C": "The bias is ambiguous because the positive bias from omitted sophistication and negative bias from confounding with `OBC_M` may offset.", "D": "There is no bias on `β_L` because `OBC_M` is included in the regression, which controls for its effect."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item uses a Computational Judgment strategy to test a difficult econometric concept (decomposing bias), capturing the essence of the original QA's 'Mathematical Apex'. The correct answer (A) correctly identifies the positive bias from two separate sources. Distractors are based on common errors: (B) a sign error on the total bias, (C) a sign error on one component of the bias (confounding with `OBC_M` also creates positive bias), and (D) a fundamental misunderstanding of omitted variable bias.", "qid": "142", "question": "### Background\n\n**Research Question.** You are empirically testing the opposing predictions about how two types of on-board computers (OBCs) affect a shipper's decision to own its own trucks.\n\n### Data / Model Specification\n\nYou estimate the following linear probability model for shipper ownership:\n`ShipperOwn_i = β_0 + β_M OBC_{Mi} + β_L OBC_{Li} + X_i'δ + ε_i`\n\nWhere:\n- `OBC_M` is a monitoring technology.\n- `OBC_L` is a logistics technology.\n\nYour theory predicts:\n- `β_M > 0` (monitoring tech increases shipper ownership).\n- `β_L < 0` (logistics tech decreases shipper ownership).\n\nHowever, you face two empirical challenges:\n1.  Many OBCs bundle both features, so `Corr(OBC_M, OBC_L) > 0`.\n2.  Adoption of any advanced technology is correlated with unobserved firm sophistication, which itself favors private fleets. So, `Corr(OBC_L, Sophistication) > 0` and the effect of `Sophistication` on `ShipperOwn` is positive.\n\n### The Question\n\nGiven this scenario, what is the most likely direction of the bias on the OLS estimate of `β_L`, the coefficient for the logistics OBC, when `Sophistication` is an omitted variable?", "model_evaluation_result": {"model_raw_response": "I need to analyze the bias on the OLS estimate of β_L when Sophistication is omitted.\n\nGiven information:\n- Theory predicts β_L < 0 (logistics tech decreases shipper ownership)\n- Corr(OBC_L, Sophistication) > 0 (logistics OBC adoption correlates with sophistication)\n- Sophistication has a positive effect on ShipperOwn (sophisticated firms favor private fleets)\n- There's also correlation between OBC_M and OBC_L, but OBC_M is included in the regression\n\nFor omitted variable bias, I need to consider:\n1. The correlation between the omitted variable (Sophistication) and the included variable (OBC_L)\n2. The effect of the omitted variable (Sophistication) on the dependent variable (ShipperOwn)\n\nThe bias formula for an omitted variable is:\nBias = (effect of omitted variable on Y) × (correlation between omitted variable and included variable)\n\nHere:\n- Effect of Sophistication on ShipperOwn = positive\n- Corr(OBC_L, Sophistication) = positive\n\nTherefore: Bias = positive × positive = positive bias\n\nThis means the OLS estimate of β_L will be biased upward (more positive) than the true value.\n\nSince the true β_L is negative (theory predicts β_L < 0), a positive bias will make the estimate appear less negative than it truly is, or potentially even positive.\n\nA", "model_answer": "A", "error": null}}
{"ID": 165, "Question": "### Background\n\n**Research Question.** This problem explores the derivation and interpretation of the core structural model of the coffee market, focusing on its long-run equilibrium.\n\n**Setting / Institutional Environment.** The model represents the international coffee market with three core components: market clearing for green coffee beans, a biological growth process for coffee trees with a one-year depreciation cycle, and roaster demand.\n\n**Variables & Parameters.**\n- `p^W`: Steady-state international wholesale price of coffee.\n- `Y_t`: Quantity of green coffee beans supplied at time `t`.\n- `Q_t`: Quantity demanded by roasters at time `t`.\n- `\\Delta I_t`: Change in inventory holdings at time `t`.\n- `N_t`: Number of coffee trees at time `t`.\n- `b`: Steady-state average yield per tree.\n- `\\varphi`: Steady-state number of new trees entering production.\n- `\\beta`: Annual survival rate of tree stocks (`0 < \\beta < 1`).\n- `a`: Steady-state demand intercept.\n- `c`: Slope of the inverse demand curve (positive constant).\n- Unit of observation: Time `t` is measured in months.\n\n---\n\n### Data / Model Specification\n\nThe structural model is defined by the following equations:\n\n```latex\nY_{t}=Q_{t}+\\Delta I_{t} \\quad \\text{(Eq. (1))}\n```\n\n```latex\nY_{t}=b_{t}N_{t} \\quad \\text{(Eq. (2))}\n```\n\n```latex\nN_{t}=\\beta N_{t-12}+\\varphi_{t} \\quad \\text{(Eq. (3))}\n```\n\n```latex\np_{t}^{\\mathrm{W}}=a_{t}–c Q_{t} \\quad \\text{(Eq. (4))}\n```\n\n---\n\n### Question\n\nBy imposing steady-state conditions on the model (i.e., `p_t^W = p^W`, `a_t = a`, `b_t = b`, `\\varphi_t = \\varphi`, `N_t = N`, and `\\Delta I_t = 0`), what is the correct expression for the steady-state wholesale price `p^W`?", "Options": {"A": "```latex\np^W = a - \\frac{c b \\varphi}{1 - \\beta}\n```", "B": "```latex\np^W = a - c b \\varphi (1 - \\beta)\n```", "C": "```latex\np^W = a - c b \\varphi\n```", "D": "```latex\np^W = a + \\frac{c b \\varphi}{1 - \\beta}\n```"}, "Answer": "A", "pi_justification": "Assessment Target: Tests the ability to solve the structural model for its long-run equilibrium, a foundational skill in dynamic modeling. Strategy: Computational Judgment. The user must derive the steady-state tree stock `N = \\varphi / (1 - \\beta)` and substitute it into the steady-state demand curve `p^W = a - c b N`. Distractor Logic: (B) Formula Misuse: incorrectly multiplies by `(1 - \\beta)` instead of dividing. (C) Step-Omission Error: omits the `1 / (1 - \\beta)` term, effectively assuming a 100% depreciation rate. (D) Sign Error: incorrectly signs the supply term, violating basic supply/demand logic.", "qid": "165", "question": "### Background\n\n**Research Question.** This problem explores the derivation and interpretation of the core structural model of the coffee market, focusing on its long-run equilibrium.\n\n**Setting / Institutional Environment.** The model represents the international coffee market with three core components: market clearing for green coffee beans, a biological growth process for coffee trees with a one-year depreciation cycle, and roaster demand.\n\n**Variables & Parameters.**\n- `p^W`: Steady-state international wholesale price of coffee.\n- `Y_t`: Quantity of green coffee beans supplied at time `t`.\n- `Q_t`: Quantity demanded by roasters at time `t`.\n- `\\Delta I_t`: Change in inventory holdings at time `t`.\n- `N_t`: Number of coffee trees at time `t`.\n- `b`: Steady-state average yield per tree.\n- `\\varphi`: Steady-state number of new trees entering production.\n- `\\beta`: Annual survival rate of tree stocks (`0 < \\beta < 1`).\n- `a`: Steady-state demand intercept.\n- `c`: Slope of the inverse demand curve (positive constant).\n- Unit of observation: Time `t` is measured in months.\n\n---\n\n### Data / Model Specification\n\nThe structural model is defined by the following equations:\n\n```latex\nY_{t}=Q_{t}+\\Delta I_{t} \\quad \\text{(Eq. (1))}\n```\n\n```latex\nY_{t}=b_{t}N_{t} \\quad \\text{(Eq. (2))}\n```\n\n```latex\nN_{t}=\\beta N_{t-12}+\\varphi_{t} \\quad \\text{(Eq. (3))}\n```\n\n```latex\np_{t}^{\\mathrm{W}}=a_{t}–c Q_{t} \\quad \\text{(Eq. (4))}\n```\n\n---\n\n### Question\n\nBy imposing steady-state conditions on the model (i.e., `p_t^W = p^W`, `a_t = a`, `b_t = b`, `\\varphi_t = \\varphi`, `N_t = N`, and `\\Delta I_t = 0`), what is the correct expression for the steady-state wholesale price `p^W`?", "model_evaluation_result": {"model_raw_response": "I need to derive the steady-state wholesale price by working through the system of equations under steady-state conditions.\n\nGiven steady-state conditions:\n- $p_t^W = p^W$\n- $a_t = a$\n- $b_t = b$ \n- $\\varphi_t = \\varphi$\n- $N_t = N$\n- $\\Delta I_t = 0$\n\nStarting with the equations:\n\nFrom Eq. (1) with $\\Delta I_t = 0$:\n$Y = Q$\n\nFrom Eq. (2):\n$Y = bN$\n\nTherefore: $Q = bN$\n\nFrom Eq. (3) in steady state:\n$N = \\beta N + \\varphi$\n\nSolving for $N$:\n$N - \\beta N = \\varphi$\n$N(1 - \\beta) = \\varphi$\n$N = \\frac{\\varphi}{1 - \\beta}$\n\nNow substituting back:\n$Q = bN = b \\cdot \\frac{\\varphi}{1 - \\beta} = \\frac{b\\varphi}{1 - \\beta}$\n\nFrom Eq. (4):\n$p^W = a - cQ$\n\nSubstituting the expression for $Q$:\n$p^W = a - c \\cdot \\frac{b\\varphi}{1 - \\beta} = a - \\frac{cb\\varphi}{1 - \\beta}$\n\nA", "model_answer": "A", "error": null}}
{"ID": 177, "Question": "### Background\n\n**Research Question.** This problem assesses the quantitative importance of structured management practices in explaining firm-level productivity dispersion, relative to other well-established drivers like R&D, ICT, and human capital.\n\n**Setting / Institutional Environment.** The analysis is conducted at the firm level, using data on U.S. manufacturing firms. Plant-level data are aggregated to the firm level.\n\n**Variables & Parameters.**\n- **Dependent Variable**: Firm-level `log(labor productivity)`.\n- **Key Independent Variables**:\n    - `Management score`: Firm-level average management score.\n    - `R&D`: Log of (1 + R&D intensity).\n    - `ICT/worker`: ICT investment per worker.\n    - `Skills`: Share of employees with a college degree.\n\n---\n\n### Data / Model Specification\n\nThe table below presents results from OLS regressions of firm-level log labor productivity on each of the four key drivers.\n\n**Table 1: Drivers of Productivity Variation**\n| | (1) | (2) | (3) | (4) |\n| :--- | :---: | :---: | :---: | :---: |\n| **Management score** | 0.864 | | | |\n| | (0.043) | | | |\n| **R&D** | | 0.133 | | |\n| | | (0.010) | | |\n| **ICT/worker** | | | 0.062 | |\n| | | | (0.006) | |\n| **Skills** | | | | 0.800 |\n| | | | | (0.064) |\n| **Share of 90-10 explained** | 0.216 | 0.216 | 0.120 | 0.159 |\n\n*Notes: Columns (1)-(4) represent separate regressions. The dependent variable is firm-level log(labor productivity). 'Share of 90-10 explained' is the fraction of the 90th-10th percentile productivity gap accounted for by the 90th-10th percentile gap in the respective driver.*\n\n---\n\n### Question\n\nBased on the results in Table 1, which statement most accurately describes the quantitative importance of management practices in explaining firm-level productivity dispersion?", "Options": {"A": "The 90-10 spread in management practices accounts for a share of the productivity spread that is comparable to R&D and larger than the shares accounted for by ICT or skills.", "B": "Management practices account for a significantly smaller share of the productivity spread than all other measured factors, including ICT and skills.", "C": "Management practices are the single most important driver of productivity, accounting for a larger share of the spread than R&D, ICT, and skills combined.", "D": "The regression coefficient for Skills (0.800) is the largest of all individual factors, implying that skills are the most powerful driver of productivity."}, "Answer": "A", "pi_justification": "This item assesses the ability to interpret the paper's key quantitative findings on the magnitude of management's impact. It uses a Computational Judgment/Interpretation strategy. The correct answer (A) accurately reflects the comparison of the 'Share of 90-10 explained' values. Distractors are designed to trap common errors: (B) is a conceptual opposite; (C) is an overstatement (Almost Right); (D) targets the classic confusion between a large regression coefficient and a variable's overall explanatory power (True but Irrelevant/Misleading).", "qid": "177", "question": "### Background\n\n**Research Question.** This problem assesses the quantitative importance of structured management practices in explaining firm-level productivity dispersion, relative to other well-established drivers like R&D, ICT, and human capital.\n\n**Setting / Institutional Environment.** The analysis is conducted at the firm level, using data on U.S. manufacturing firms. Plant-level data are aggregated to the firm level.\n\n**Variables & Parameters.**\n- **Dependent Variable**: Firm-level `log(labor productivity)`.\n- **Key Independent Variables**:\n    - `Management score`: Firm-level average management score.\n    - `R&D`: Log of (1 + R&D intensity).\n    - `ICT/worker`: ICT investment per worker.\n    - `Skills`: Share of employees with a college degree.\n\n---\n\n### Data / Model Specification\n\nThe table below presents results from OLS regressions of firm-level log labor productivity on each of the four key drivers.\n\n**Table 1: Drivers of Productivity Variation**\n| | (1) | (2) | (3) | (4) |\n| :--- | :---: | :---: | :---: | :---: |\n| **Management score** | 0.864 | | | |\n| | (0.043) | | | |\n| **R&D** | | 0.133 | | |\n| | | (0.010) | | |\n| **ICT/worker** | | | 0.062 | |\n| | | | (0.006) | |\n| **Skills** | | | | 0.800 |\n| | | | | (0.064) |\n| **Share of 90-10 explained** | 0.216 | 0.216 | 0.120 | 0.159 |\n\n*Notes: Columns (1)-(4) represent separate regressions. The dependent variable is firm-level log(labor productivity). 'Share of 90-10 explained' is the fraction of the 90th-10th percentile productivity gap accounted for by the 90th-10th percentile gap in the respective driver.*\n\n---\n\n### Question\n\nBased on the results in Table 1, which statement most accurately describes the quantitative importance of management practices in explaining firm-level productivity dispersion?", "model_evaluation_result": {"model_raw_response": "Looking at the \"Share of 90-10 explained\" row in Table 1:\n\n- Management score: 0.216 (21.6%)\n- R&D: 0.216 (21.6%) \n- ICT/worker: 0.120 (12.0%)\n- Skills: 0.159 (15.9%)\n\nThis metric shows the fraction of the 90th-10th percentile productivity gap that each factor explains.\n\nManagement practices explain 21.6% of the productivity spread, which is:\n- Equal to R&D (21.6%)\n- Greater than ICT (12.0%)\n- Greater than Skills (15.9%)\n\nThis matches option A's description that management practices account for a share \"comparable to R&D and larger than the shares accounted for by ICT or skills.\"\n\nOption B is incorrect - management explains more than ICT and skills.\nOption C is incorrect - management is not larger than all others combined.\nOption D is incorrect - while Skills has the largest coefficient (0.800), the question asks about quantitative importance in explaining dispersion, which is measured by the \"Share of 90-10 explained\" metric, not the coefficient size.\n\nA", "model_answer": "D", "error": null}}
{"ID": 231, "Question": "### Background\n\n**Research Question.** This problem analyzes the econometric model of a single mother's discrete choice among employment and childcare alternatives.\n\n**Setting and Sample.** A single mother makes a static choice from a set of seven mutually exclusive alternatives (e.g., 'No-work', 'Part-time work with paid care', 'Full-time work with unpaid care', etc.).\n\n### The Question\n\nThe paper estimates this choice model using a multinomial logit (MNL) framework, which relies on the Independence of Irrelevant Alternatives (IIA) assumption. Why is this assumption likely violated in this context, and what is the appropriate remedy?", "Options": {"A": "IIA is violated because some alternatives (e.g., 'Part-time, Paid Care' and 'Part-time, Unpaid Care') are closer substitutes than others. A Nested Logit model, which groups similar alternatives into 'nests' to allow for correlation in unobserved utility, would be an appropriate remedy.", "B": "IIA is violated because wages are endogenous and correlated with the error term. The appropriate remedy is to use instrumental variables for wages.", "C": "IIA is violated because the error terms are heteroskedastic across individuals. The appropriate remedy is to use a Probit model instead of a Logit model.", "D": "IIA is not violated because the seven choices are defined to be mutually exclusive. The standard MNL model is therefore appropriate."}, "Answer": "A", "pi_justification": "This item assesses a core concept in discrete choice econometrics: the IIA assumption. It requires the student to identify the specific reason for the violation (substitution patterns) and match it to the correct, more flexible model (Nested Logit). The distractors are based on common confusions: B (True but Irrelevant) conflates the IIA problem with endogeneity, C (Conceptual Opposite) confuses IIA with heteroskedasticity and proposes an irrelevant fix, and D denies the problem exists.", "qid": "231", "question": "### Background\n\n**Research Question.** This problem analyzes the econometric model of a single mother's discrete choice among employment and childcare alternatives.\n\n**Setting and Sample.** A single mother makes a static choice from a set of seven mutually exclusive alternatives (e.g., 'No-work', 'Part-time work with paid care', 'Full-time work with unpaid care', etc.).\n\n### The Question\n\nThe paper estimates this choice model using a multinomial logit (MNL) framework, which relies on the Independence of Irrelevant Alternatives (IIA) assumption. Why is this assumption likely violated in this context, and what is the appropriate remedy?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 138, "Question": "### Background\n\nAfter winning a wildcat lease, a firm has five years (20 quarters) to begin exploration. This creates an information externality, as firms have an incentive to wait and let a rival drill an adjacent tract first to learn about the shared prospect. The paper contrasts two models of drilling behavior:\n- **Optimal Coordinated Plan:** Firms cooperate to drill sequentially, maximizing joint profits and minimizing delay.\n- **War of Attrition:** Firms strategically delay drilling, hoping to free-ride on a rival's information. This non-cooperative game predicts a U-shaped pattern for the drilling hazard rate over the lease term.\n\n### Data / Model Specification\n\n**Table 1. Wildcat Tract Drilling Characteristics by Year of Initial Drilling, 1954-1979**\n\n| Year After Acquisition | 1 | 2 | 3 | 4 | 5 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Risk Set (Number)** | 2255 | 1522 | 1159 | 971 | 816 |\n| **Tracts Drilled (Number)** | 733 | 363 | 188 | 155 | 214 |\n\n*Source: Adapted from Table IV in the original paper.*\n\n### Question\n\nBased on the data in Table 1, what is the approximate empirical drilling hazard rate for Year 5, and which model of drilling behavior does the overall five-year pattern support?", "Options": {"A": "The hazard rate is approximately 26.2%, and the U-shaped pattern supports the \"War of Attrition\" model.", "B": "The hazard rate is approximately 26.2%, and the U-shaped pattern supports the \"Optimal Coordinated Plan\".", "C": "The hazard rate is approximately 9.5% (214/2255), and the declining pattern supports the \"War of Attrition\" model.", "D": "The hazard rate is approximately 32.5%, and the initially high rate supports the \"Optimal Coordinated Plan\"."}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a calculation from a table and match the resulting empirical pattern to the correct game-theoretic model. It uses a 'Computational Judgment' strategy. Option A is correct. Option B has the correct calculation but incorrect theory, testing conceptual understanding. Option C uses the wrong denominator for the hazard rate (total tracts instead of the risk set), a common calculation error. Option D correctly identifies the Year 1 hazard rate but misinterprets the overall pattern and misapplies the theory.", "qid": "138", "question": "### Background\n\nAfter winning a wildcat lease, a firm has five years (20 quarters) to begin exploration. This creates an information externality, as firms have an incentive to wait and let a rival drill an adjacent tract first to learn about the shared prospect. The paper contrasts two models of drilling behavior:\n- **Optimal Coordinated Plan:** Firms cooperate to drill sequentially, maximizing joint profits and minimizing delay.\n- **War of Attrition:** Firms strategically delay drilling, hoping to free-ride on a rival's information. This non-cooperative game predicts a U-shaped pattern for the drilling hazard rate over the lease term.\n\n### Data / Model Specification\n\n**Table 1. Wildcat Tract Drilling Characteristics by Year of Initial Drilling, 1954-1979**\n\n| Year After Acquisition | 1 | 2 | 3 | 4 | 5 |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Risk Set (Number)** | 2255 | 1522 | 1159 | 971 | 816 |\n| **Tracts Drilled (Number)** | 733 | 363 | 188 | 155 | 214 |\n\n*Source: Adapted from Table IV in the original paper.*\n\n### Question\n\nBased on the data in Table 1, what is the approximate empirical drilling hazard rate for Year 5, and which model of drilling behavior does the overall five-year pattern support?", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to:\n\n1. Calculate the hazard rate for Year 5\n2. Identify the overall pattern across all 5 years\n3. Determine which model this pattern supports\n\n**Calculating Year 5 hazard rate:**\nHazard rate = (Tracts drilled in Year 5) / (Risk set at start of Year 5)\n= 214 / 816 = 0.262 = 26.2%\n\n**Calculating hazard rates for all years:**\n- Year 1: 733/2255 = 32.5%\n- Year 2: 363/1522 = 23.8%\n- Year 3: 188/1159 = 16.2%\n- Year 4: 155/971 = 16.0%\n- Year 5: 214/816 = 26.2%\n\n**Identifying the pattern:**\nThe hazard rates start high (32.5%), decline through years 2-4 (23.8% → 16.2% → 16.0%), then increase significantly in year 5 (26.2%). This creates a U-shaped pattern.\n\n**Matching to theoretical models:**\n- **War of Attrition model:** Predicts a U-shaped hazard rate pattern due to strategic delay followed by urgency as the lease expiration approaches\n- **Optimal Coordinated Plan:** Would predict a more systematic, coordinated drilling schedule without the strategic delay and end-of-period rush\n\nThe empirical U-shaped pattern matches the War of Attrition model's prediction of strategic delay followed by increased drilling urgency near lease expiration.\n\nA", "model_answer": "A", "error": null}}
{"ID": 145, "Question": "### Background\n\n**Research Question.** This problem investigates the drivers of the observed decline in 'world efficiency', a measure of how close the world's average productivity is to the technological frontier. The central question is whether this decline is due to falling productivity within countries or a reallocation of economic activity across countries.\n\n**Setting / Institutional Environment.** The study analyzes data for 38 economies from 1995 to 2011. A key empirical finding is that world efficiency in the market sector fell from 51% to 46%. This occurred as the share of world inputs used by the frontier country (the US) declined, while the shares of large, less-efficient emerging economies (China, India) increased.\n\n---\n\n### Data / Model Specification\n\nWorld efficiency is defined as the ratio of world average productivity to the productivity frontier:\n```latex\nE_t = \\frac{T_t}{T_{t,max}} = \\frac{\\sum_{k=1}^K \\omega_{kt} T_{kt}}{T_{t,max}} \\quad \\text{(Eq. (1))}\n```\nwhere `T_{kt}` is the TFP level for country `k` at time `t`, `\\omega_{kt}` is its share of world inputs, and `T_{t,max}` is the productivity frontier.\n\nTo isolate the cause of the decline in `E_t`, the authors compute a counterfactual efficiency for 2011 that uses 2011 productivity levels but holds input shares constant at their 1995 levels:\n```latex\nE^*_{2011} = \\frac{\\sum_{k=1}^K \\omega_{k,1995} T_{k,2011}}{T_{2011,max}} \\quad \\text{(Eq. (2))}\n```\n**Observed Values:**\n- `E_{1995}` = 0.510\n- `E_{2011}` = 0.460\n- `E^*_{2011}` = 0.505\n\n---\n\n### The Question\n\nGiven the observed values, what is the primary conclusion from the counterfactual analysis regarding the 5 percentage point decline in world efficiency (`E_t`) from 1995 to 2011?", "Options": {"A": "The decline was almost entirely driven by a compositional shift of world inputs towards countries with lower productivity levels.", "B": "The decline was almost entirely driven by falling productivity levels within most of the 38 countries in the sample.", "C": "The decline was driven roughly equally by both compositional shifts and within-country productivity declines.", "D": "The decline was an artifact of the 2008 financial crisis, which disproportionately affected high-productivity countries."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the ability to interpret a counterfactual experiment and draw a causal inference. Strategy: Computational Judgment. The student must compare the actual change (`0.510 -> 0.460`) with the counterfactual change (`0.510 -> 0.505`) to deduce the primary driver. Distractor Logic: B is the conceptual opposite (blaming within-country effects). C is a plausible but incorrect conclusion if one doesn't compute the magnitudes. D is a true but irrelevant fact that doesn't follow from the provided counterfactual evidence.", "qid": "145", "question": "### Background\n\n**Research Question.** This problem investigates the drivers of the observed decline in 'world efficiency', a measure of how close the world's average productivity is to the technological frontier. The central question is whether this decline is due to falling productivity within countries or a reallocation of economic activity across countries.\n\n**Setting / Institutional Environment.** The study analyzes data for 38 economies from 1995 to 2011. A key empirical finding is that world efficiency in the market sector fell from 51% to 46%. This occurred as the share of world inputs used by the frontier country (the US) declined, while the shares of large, less-efficient emerging economies (China, India) increased.\n\n---\n\n### Data / Model Specification\n\nWorld efficiency is defined as the ratio of world average productivity to the productivity frontier:\n```latex\nE_t = \\frac{T_t}{T_{t,max}} = \\frac{\\sum_{k=1}^K \\omega_{kt} T_{kt}}{T_{t,max}} \\quad \\text{(Eq. (1))}\n```\nwhere `T_{kt}` is the TFP level for country `k` at time `t`, `\\omega_{kt}` is its share of world inputs, and `T_{t,max}` is the productivity frontier.\n\nTo isolate the cause of the decline in `E_t`, the authors compute a counterfactual efficiency for 2011 that uses 2011 productivity levels but holds input shares constant at their 1995 levels:\n```latex\nE^*_{2011} = \\frac{\\sum_{k=1}^K \\omega_{k,1995} T_{k,2011}}{T_{2011,max}} \\quad \\text{(Eq. (2))}\n```\n**Observed Values:**\n- `E_{1995}` = 0.510\n- `E_{2011}` = 0.460\n- `E^*_{2011}` = 0.505\n\n---\n\n### The Question\n\nGiven the observed values, what is the primary conclusion from the counterfactual analysis regarding the 5 percentage point decline in world efficiency (`E_t`) from 1995 to 2011?", "model_evaluation_result": {"model_raw_response": "Looking at the counterfactual analysis:\n\n- Actual decline in world efficiency: E₁₉₉₅ - E₂₀₁₁ = 0.510 - 0.460 = 0.050 (5 percentage points)\n\n- Counterfactual E*₂₀₁₁ = 0.505 uses 2011 productivity levels but holds input shares at 1995 levels\n\nIf productivity changes within countries were the primary driver, we would expect E*₂₀₁₁ to be close to E₂₀₁₁ = 0.460, since it uses 2011 productivity levels.\n\nHowever, E*₂₀₁₁ = 0.505 is very close to E₁₉₉₅ = 0.510, differing by only 0.005.\n\nThis means that if input shares had remained at 1995 levels, world efficiency would have barely declined despite using 2011 productivity levels. The fact that actual E₂₀₁₁ = 0.460 is much lower than E*₂₀₁₁ = 0.505 indicates that the change in input shares (compositional shift toward less efficient countries like China and India) was the primary cause of the decline.\n\nThe compositional effect accounts for approximately 0.505 - 0.460 = 0.045 out of the total 0.050 decline, which is 90% of the total decline.\n\nA", "model_answer": "A", "error": null}}
{"ID": 53, "Question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. The rebel faction's decision to accept a peaceful proposal is governed by its Peaceful Participation Constraint (PPC):\n\n```latex\nz_{k}+\\beta V_{R}(s_{k+1}) \\geq f_{R}+\\beta d_{k}p_{k}(1+\\gamma)V +\\beta(1-d_{k})[(1-\\varepsilon)V_{R}(s_{k})+\\varepsilon V_{R}(s_{k+1})] \\quad \\text{(Eq. (1))}\n```\n\nwhere `V_R(s)` is the rebel's continuation payoff from state `s`, and `ε` is the probability G's consolidation succeeds despite inconclusive fighting. The government (G) has **coercive power** when `d_k < 1` and `ε > 0`, which allows it to relax the PPC by choosing a lower `V_R(s_{k+1})`.\n\n### Question\n\nAssuming the government has coercive power and seeks a peaceful settlement, what is its optimal strategy to maximize its own payoff?\n", "Options": {"A": "Offer the largest possible current transfer (`z_k=1`) to induce the rebels to accept the weakest possible future position (the lowest `V_R(s_{k+1})`).", "B": "Offer the smallest possible current transfer (`z_k`) that satisfies the rebels' participation constraint, regardless of the future state `s_{k+1}`.", "C": "Propose the strongest possible future for the rebels (highest `V_R(s_{k+1})`) to minimize the required current transfer `z_k`.", "D": "Propose a future state `s_{k+1}` where fighting is most decisive (`d_{k+1}=1`) to create efficiency gains for both parties."}, "Answer": "A", "pi_justification": "This item assesses the core mechanism of 'coercive power' during peaceful negotiations. It uses a Computational Judgment strategy, requiring students to interpret the Peaceful Participation Constraint to deduce the optimal government strategy. The correct option (A) identifies the trade-off between current transfers and future rebel weakness. Distractors are designed based on common errors: (B) a myopic focus on minimizing the current transfer, (C) misinterpreting the government's objective function, and (D) confusing the peaceful buyout logic with the 'truce' logic from another part of the model.", "qid": "53", "question": "### Background\n\nThis problem investigates the fundamental mechanism of 'coercive power' that motivates a government to consolidate its position against an armed opposition. The setting is an infinite-horizon game where the government (G) makes a take-it-or-leave-it proposal to the rebels (R). R can accept the proposal or reject it by fighting.\n\n### Data / Model Specification\n\nThe state of the power distribution in period `k` is `s_k = (d_k, p_k)`. The government's proposal is `(z_k, s_{k+1})`, a transfer today and a proposed power distribution for tomorrow. The rebel faction's decision to accept a peaceful proposal is governed by its Peaceful Participation Constraint (PPC):\n\n```latex\nz_{k}+\\beta V_{R}(s_{k+1}) \\geq f_{R}+\\beta d_{k}p_{k}(1+\\gamma)V +\\beta(1-d_{k})[(1-\\varepsilon)V_{R}(s_{k})+\\varepsilon V_{R}(s_{k+1})] \\quad \\text{(Eq. (1))}\n```\n\nwhere `V_R(s)` is the rebel's continuation payoff from state `s`, and `ε` is the probability G's consolidation succeeds despite inconclusive fighting. The government (G) has **coercive power** when `d_k < 1` and `ε > 0`, which allows it to relax the PPC by choosing a lower `V_R(s_{k+1})`.\n\n### Question\n\nAssuming the government has coercive power and seeks a peaceful settlement, what is its optimal strategy to maximize its own payoff?\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand how the government can use its coercive power to maximize its payoff while ensuring peace.\n\nFrom the Peaceful Participation Constraint (PPC):\n```\nz_k + βV_R(s_{k+1}) ≥ f_R + βd_k p_k(1+γ)V + β(1-d_k)[(1-ε)V_R(s_k) + εV_R(s_{k+1})]\n```\n\nThe government has coercive power when d_k < 1 and ε > 0. This means that even if fighting is inconclusive (probability 1-d_k), there's still a probability ε that the government can consolidate its position and impose its preferred future state s_{k+1}.\n\nTo maximize its payoff, the government wants to:\n1. Minimize the transfer z_k (since this comes out of its own resources)\n2. Choose s_{k+1} to maximize its own continuation value\n\nThe key insight is that coercive power allows the government to threaten a worse future for the rebels if they fight. When ε > 0, even inconclusive fighting gives the government a chance to impose its preferred (rebel-unfavorable) outcome.\n\nLooking at the constraint, I can solve for the minimum z_k:\n```\nz_k = f_R + βd_k p_k(1+γ)V + β(1-d_k)[(1-ε)V_R(s_k) + εV_R(s_{k+1})] - βV_R(s_{k+1})\n```\n\nSimplifying:\n```\nz_k = f_R + βd_k p_k(1+γ)V + β(1-d_k)(1-ε)V_R(s_k) + β(1-d_k)εV_R(s_{k+1}) - βV_R(s_{k+1})\nz_k = f_R + βd_k p_k(1+γ)V + β(1-d_k)(1-ε)V_R(s_k)", "model_answer": "Looking at this problem, I need to understand how the government can use its coercive power to maximize its payoff while ensuring peace.\n\nFrom the Peaceful Participation Constraint (PPC):\n```\nz_k + βV_R(s_{k+1}) ≥ f_R + βd_k p_k(1+γ)V + β(1-d_k)[(1-ε)V_R(s_k) + εV_R(s_{k+1})]\n```\n\nThe government has coercive power when d_k < 1 and ε > 0. This means that even if fighting is inconclusive (probability 1-d_k), there's still a probability ε that the government can consolidate its position and impose its preferred future state s_{k+1}.\n\nTo maximize its payoff, the government wants to:\n1. Minimize the transfer z_k (since this comes out of its own resources)\n2. Choose s_{k+1} to maximize its own continuation value\n\nThe key insight is that coercive power allows the government to threaten a worse future for the rebels if they fight. When ε > 0, even inconclusive fighting gives the government a chance to impose its preferred (rebel-unfavorable) outcome.\n\nLooking at the constraint, I can solve for the minimum z_k:\n```\nz_k = f_R + βd_k p_k(1+γ)V + β(1-d_k)[(1-ε)V_R(s_k) + εV_R(s_{k+1})] - βV_R(s_{k+1})\n```\n\nSimplifying:\n```\nz_k = f_R + βd_k p_k(1+γ)V + β(1-d_k)(1-ε)V_R(s_k) + β(1-d_k)εV_R(s_{k+1}) - βV_R(s_{k+1})\nz_k = f_R + βd_k p_k(1+γ)V + β(1-d_k)(1-ε)V_R(s_k)", "error": "Warning: Could not extract single choice letter from: Looking at this problem, I need to understand how ..."}}
{"ID": 239, "Question": "### Background\n\n**Research Question.** This problem develops the theoretical and empirical framework used to identify the determinants of firm sales growth in transition economies. The core challenge is to distinguish the impact of changes in productive inputs from the influence of historical legacies of the central planning era.\n\n### Data / Model Specification\n\nA static, levels-based model with a time-invariant, firm-specific effect \\(f_i\\) can be written as:\n\n```latex\n\\ln(S_{it}) = \\alpha \\ln(L_{it}) + \\beta \\ln(K_{it}) + f_i + u_{it} \\quad \\text{(Eq. (1))}\n```\n\nThe paper instead uses a first-differenced model:\n\n```latex\n\\Delta\\ln S_{i} = \\alpha\\Delta\\ln L_{i} + \\beta\\Delta\\ln K_{i} + \\varepsilon_{i} \\quad \\text{(Eq. (2))}\n```\n\nwhere \\(\\Delta\\) denotes the change from one period to the next. This dynamic specification is chosen because static models are likely biased by \"historical or institutional factors\" that are firm-specific and time-invariant.\n\n### Question\n\nThe paper prefers the first-differenced specification in Eq. (2) over a static levels-based model (Eq. (1)). What is the primary econometric justification for this choice?", "Options": {"A": "To eliminate bias from unobserved, time-invariant firm characteristics, such as historical political connections or legacy infrastructure.", "B": "To account for time-varying macroeconomic shocks, such as the 1998 Russian financial crisis, that affect all firms.", "C": "To solve the problem of measurement error in the capital stock variable, which is a known issue in transition economies.", "D": "To allow for the direct estimation of the magnitude of each firm's unique historical advantages."}, "Answer": "A", "pi_justification": "This item assesses the student's understanding of the core methodological choice in the paper: using first-differencing to control for fixed effects. It directly replaces the conceptual questions Q2 and Q3 from the source QA. The distractors are designed based on common econometric misconceptions: B confuses firm fixed effects with time fixed effects; C confuses the solution for omitted variables with the problem of measurement error; D represents a conceptual opposite of what first-differencing achieves.", "qid": "239", "question": "### Background\n\n**Research Question.** This problem develops the theoretical and empirical framework used to identify the determinants of firm sales growth in transition economies. The core challenge is to distinguish the impact of changes in productive inputs from the influence of historical legacies of the central planning era.\n\n### Data / Model Specification\n\nA static, levels-based model with a time-invariant, firm-specific effect \\(f_i\\) can be written as:\n\n```latex\n\\ln(S_{it}) = \\alpha \\ln(L_{it}) + \\beta \\ln(K_{it}) + f_i + u_{it} \\quad \\text{(Eq. (1))}\n```\n\nThe paper instead uses a first-differenced model:\n\n```latex\n\\Delta\\ln S_{i} = \\alpha\\Delta\\ln L_{i} + \\beta\\Delta\\ln K_{i} + \\varepsilon_{i} \\quad \\text{(Eq. (2))}\n```\n\nwhere \\(\\Delta\\) denotes the change from one period to the next. This dynamic specification is chosen because static models are likely biased by \"historical or institutional factors\" that are firm-specific and time-invariant.\n\n### Question\n\nThe paper prefers the first-differenced specification in Eq. (2) over a static levels-based model (Eq. (1)). What is the primary econometric justification for this choice?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 212, "Question": "### Background\n\n**Research Question.** This study investigates the behavioral responses of students to receiving a free home laptop, specifically examining how the intervention alters their patterns of computer access and use both at home and in other locations.\n\n**Setting / Institutional Environment.** In a randomized experiment in Lima, Peru, students within designated 'treatment schools' participated in lotteries for XO laptops intended for home use. The analysis compares outcomes for lottery winners (treatment) and non-winners (control) within the same schools.\n\n### Data / Model Specification\n\nThe causal effects are estimated using the following regression model, which includes class fixed effects (`\\mu_j`) and baseline controls (`\\mathbf{X_{ijk}}`):\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\delta Winner_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nTable 1 presents the estimates for the treatment effect `\\delta` on outcomes related to computer access and use.\n\n**Table 1: Effects on Computer Access and Use**\n\n| | Winners | Non-winners | Adjusted Difference (ITT) | Observations |\n| :--- | :---: | :---: | :---: | :---: |\n| **Access** | | | |\n| Computer or laptop at home | 0.89 | 0.50 | 0.40*** (0.03) | 2,851 |\n| **Use by place (last week)** | | | |\n| Home | 0.82 | 0.50 | 0.33*** (0.02) | 2,641 |\n| Internet cafe | 0.43 | 0.54 | -0.11*** (0.02) | 2,631 |\n| Friend's house | 0.20 | 0.23 | -0.02 (0.02) | 2,561 |\n| **Use by place (minutes yesterday)** | | | |\n| Home | 61.85 | 44.99 | 18.20*** (4.93) | 2,851 |\n| Internet cafe | 30.75 | 34.25 | -3.23 (3.67) | 2,851 |\n| Friend's house | 11.30 | 15.44 | -3.78* (2.06) | 2,851 |\n\n*Notes: The 'Adjusted Difference' column presents the estimated `δ`. Standard errors, clustered at the school level, are in parentheses. ***p<0.01, *p<0.10.*\n\nUsing the point estimates for 'minutes yesterday' in Table 1, what is the net change in a student's total daily computer usage (across Home, Internet cafe, and Friend's house) from winning the lottery?", "Options": {"A": "An increase of 11.19 minutes.", "B": "An increase of 18.20 minutes.", "C": "An increase of 25.21 minutes.", "D": "A decrease of 7.01 minutes."}, "Answer": "A", "pi_justification": "This item tests the ability to perform a multi-step calculation from a table and arrive at a precise numerical answer. It uses a Computational Judgment strategy. Distractors are based on predictable calculation errors: ignoring parts of the formula (B), making a sign error (C), and using the wrong components of the formula (D).", "qid": "212", "question": "### Background\n\n**Research Question.** This study investigates the behavioral responses of students to receiving a free home laptop, specifically examining how the intervention alters their patterns of computer access and use both at home and in other locations.\n\n**Setting / Institutional Environment.** In a randomized experiment in Lima, Peru, students within designated 'treatment schools' participated in lotteries for XO laptops intended for home use. The analysis compares outcomes for lottery winners (treatment) and non-winners (control) within the same schools.\n\n### Data / Model Specification\n\nThe causal effects are estimated using the following regression model, which includes class fixed effects (`\\mu_j`) and baseline controls (`\\mathbf{X_{ijk}}`):\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\delta Winner_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nTable 1 presents the estimates for the treatment effect `\\delta` on outcomes related to computer access and use.\n\n**Table 1: Effects on Computer Access and Use**\n\n| | Winners | Non-winners | Adjusted Difference (ITT) | Observations |\n| :--- | :---: | :---: | :---: | :---: |\n| **Access** | | | |\n| Computer or laptop at home | 0.89 | 0.50 | 0.40*** (0.03) | 2,851 |\n| **Use by place (last week)** | | | |\n| Home | 0.82 | 0.50 | 0.33*** (0.02) | 2,641 |\n| Internet cafe | 0.43 | 0.54 | -0.11*** (0.02) | 2,631 |\n| Friend's house | 0.20 | 0.23 | -0.02 (0.02) | 2,561 |\n| **Use by place (minutes yesterday)** | | | |\n| Home | 61.85 | 44.99 | 18.20*** (4.93) | 2,851 |\n| Internet cafe | 30.75 | 34.25 | -3.23 (3.67) | 2,851 |\n| Friend's house | 11.30 | 15.44 | -3.78* (2.06) | 2,851 |\n\n*Notes: The 'Adjusted Difference' column presents the estimated `δ`. Standard errors, clustered at the school level, are in parentheses. ***p<0.01, *p<0.10.*\n\nUsing the point estimates for 'minutes yesterday' in Table 1, what is the net change in a student's total daily computer usage (across Home, Internet cafe, and Friend's house) from winning the lottery?", "model_evaluation_result": {"model_raw_response": "I need to calculate the net change in total daily computer usage by summing the adjusted differences for all three locations.\n\nFrom Table 1, the adjusted differences (treatment effects) for \"minutes yesterday\" are:\n- Home: +18.20 minutes\n- Internet cafe: -3.23 minutes  \n- Friend's house: -3.78 minutes\n\nNet change = 18.20 + (-3.23) + (-3.78)\nNet change = 18.20 - 3.23 - 3.78\nNet change = 18.20 - 7.01\nNet change = 11.19 minutes\n\nThe net effect shows an increase of 11.19 minutes in total daily computer usage.\n\nA", "model_answer": "A", "error": null}}
{"ID": 181, "Question": "### Background\n\n**Research Question.** This analysis seeks to quantify the empirical impact of banking crises on exports, focusing on how the effect varies based on an industry's dependence on external finance (`RZ`).\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences approach where the treatment is a banking crisis and the treatment intensity is an industry's intrinsic reliance on external finance (`RZ`).\n\n### Data / Model Specification\n\nThe dependent variable is log(1 + Exports). An industry at the 75th percentile of external finance dependence has `RZ` = 0.45, while an industry at the 25th percentile has `RZ` = 0.09.\n\n**Table 1: Baseline Regression Result**\n\n| Variable | Coefficient | Std. Error |\n|:---|:---:|:---:|\n| `RZ*Crisis` | -0.141*** | [0.041] |\n\n*Notes: `Crisis` is a dummy for a banking crisis. *** p<0.01.*\n\nBased on the result in Table 1, what is the estimated differential impact of a banking crisis on log exports for an industry at the 75th percentile of external finance dependence (`RZ` = 0.45) compared to one at the 25th percentile (`RZ` = 0.09)?", "Options": {"A": "Exports are approximately 5.1 percentage points lower for the high-dependence industry.", "B": "Exports are approximately 14.1 percentage points lower for the high-dependence industry.", "C": "Exports are approximately 5.1 percentage points higher for the high-dependence industry.", "D": "Exports are approximately 6.3 percentage points lower for the high-dependence industry."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item tests the ability to correctly interpret and apply a regression coefficient from an interaction term to calculate a marginal effect. It uses a Computational Judgment strategy. Distractors are designed to catch common errors: (B) misinterpreting the coefficient as the final effect (Formula Misuse), (C) a sign error, and (D) failing to calculate the difference between the two groups (Step-Omission Error).", "qid": "181", "question": "### Background\n\n**Research Question.** This analysis seeks to quantify the empirical impact of banking crises on exports, focusing on how the effect varies based on an industry's dependence on external finance (`RZ`).\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences approach where the treatment is a banking crisis and the treatment intensity is an industry's intrinsic reliance on external finance (`RZ`).\n\n### Data / Model Specification\n\nThe dependent variable is log(1 + Exports). An industry at the 75th percentile of external finance dependence has `RZ` = 0.45, while an industry at the 25th percentile has `RZ` = 0.09.\n\n**Table 1: Baseline Regression Result**\n\n| Variable | Coefficient | Std. Error |\n|:---|:---:|:---:|\n| `RZ*Crisis` | -0.141*** | [0.041] |\n\n*Notes: `Crisis` is a dummy for a banking crisis. *** p<0.01.*\n\nBased on the result in Table 1, what is the estimated differential impact of a banking crisis on log exports for an industry at the 75th percentile of external finance dependence (`RZ` = 0.45) compared to one at the 25th percentile (`RZ` = 0.09)?", "model_evaluation_result": {"model_raw_response": "I need to calculate the differential impact of a banking crisis on log exports between industries at the 75th percentile (RZ = 0.45) and 25th percentile (RZ = 0.09) of external finance dependence.\n\nFrom the regression result, the coefficient on RZ*Crisis is -0.141.\n\nThe differential impact is calculated as:\nCoefficient × (RZ_75th - RZ_25th)\n\n= -0.141 × (0.45 - 0.09)\n= -0.141 × 0.36\n= -0.05076\n\nThis means exports are approximately 5.1 percentage points lower for the high-dependence industry (75th percentile) compared to the low-dependence industry (25th percentile) during a banking crisis.\n\nSince the dependent variable is log(1 + Exports), a coefficient of -0.05076 can be interpreted as approximately a 5.1 percentage point decrease.\n\nA", "model_answer": "A", "error": null}}
{"ID": 147, "Question": "### Background\n\nThe paper's main result (Theorem 1) shows that the GAFT model is identified up to a power transformation `ρ` if the model's primitives exhibit regular variation at their tails. The power `ρ` depends on the tail exponents of the primitives in any two observationally equivalent models, denoted `(Λ, φ, G)` and `(Λ̃, φ̃, G̃)`.\n\nSpecifically, if the densities `g` and `g̃` vary regularly at 0 with exponents `τ̲` and `τ̅` respectively, then `ρ = (τ̅+1)/(τ̲+1)`.\n\n### Data / Model Specification\n\nThe Mixed Proportional Hazards (MPH) model is a special case of GAFT. Prior literature established identification for the MPH model under two different assumptions:\n\n1.  **Elbers & Ridder:** `lim_{s→0} g(s)` is a positive, finite constant.\n2.  **Heckman & Singer:** `g(s)` varies regularly at 0 with a known exponent `τ ∈ (-1, 0)`.\n\n### Question\n\nWithin the framework of Theorem 1, why do both of these seemingly different assumptions lead to point identification (i.e., the ambiguity parameter `ρ` must equal 1)?", "Options": {"A": "Both assumptions imply that `g(s)` and any observationally equivalent `g̃(s)` must have the same, known exponent of regular variation at zero, forcing the ratio `(τ̅+1)/(τ̲+1)` to be 1.", "B": "Both assumptions require the baseline hazard `λ(t)` to be constant for small `t`, which directly implies `ρ=1`.", "C": "Both assumptions are equivalent to requiring the unobserved heterogeneity distribution `H(υ)` to be a single point mass, which eliminates all ambiguity.", "D": "Both assumptions ensure that the transformation `K(s)` must be the identity function `K(s)=s`, which is a result of `ρ=1`, not the cause."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to synthesize information by unifying two different historical identification assumptions under the paper's general framework (Theorem 1). It uses a Reverse-Reasoning strategy, asking for the underlying reason for a shared outcome (`ρ=1`). Distractors are designed to test for confusion between model primitives (`g` vs. `λ`) (B), over-simplification of the assumptions (C), and confusing a result with a cause (D).", "qid": "147", "question": "### Background\n\nThe paper's main result (Theorem 1) shows that the GAFT model is identified up to a power transformation `ρ` if the model's primitives exhibit regular variation at their tails. The power `ρ` depends on the tail exponents of the primitives in any two observationally equivalent models, denoted `(Λ, φ, G)` and `(Λ̃, φ̃, G̃)`.\n\nSpecifically, if the densities `g` and `g̃` vary regularly at 0 with exponents `τ̲` and `τ̅` respectively, then `ρ = (τ̅+1)/(τ̲+1)`.\n\n### Data / Model Specification\n\nThe Mixed Proportional Hazards (MPH) model is a special case of GAFT. Prior literature established identification for the MPH model under two different assumptions:\n\n1.  **Elbers & Ridder:** `lim_{s→0} g(s)` is a positive, finite constant.\n2.  **Heckman & Singer:** `g(s)` varies regularly at 0 with a known exponent `τ ∈ (-1, 0)`.\n\n### Question\n\nWithin the framework of Theorem 1, why do both of these seemingly different assumptions lead to point identification (i.e., the ambiguity parameter `ρ` must equal 1)?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand why both the Elbers & Ridder and Heckman & Singer assumptions lead to point identification in the GAFT framework.\n\nLet me analyze each assumption in terms of regular variation:\n\n**Elbers & Ridder assumption**: `lim_{s→0} g(s)` is a positive, finite constant.\n- This means g(s) approaches a constant as s→0\n- In terms of regular variation, this corresponds to g(s) varying regularly at 0 with exponent τ = 0\n- So g(s) ~ constant × s^0 = constant as s→0\n\n**Heckman & Singer assumption**: g(s) varies regularly at 0 with a known exponent τ ∈ (-1, 0).\n- This directly specifies the exponent of regular variation\n\nNow, for the identification result from Theorem 1:\n- If two observationally equivalent models have densities g and g̃ with exponents τ̲ and τ̅ respectively\n- Then ρ = (τ̅+1)/(τ̲+1)\n- For point identification, we need ρ = 1, which requires τ̅ = τ̲\n\nUnder both assumptions:\n- The exponent of regular variation is either known to be 0 (Elbers & Ridder) or known to be some specific value in (-1,0) (Heckman & Singer)\n- Since the exponent is known/specified, any observationally equivalent model must have the same exponent\n- Therefore τ̅ = τ̲, which makes ρ = (τ̅+1)/(τ̲+1) = 1\n\nThis matches option A exactly - both assumptions imply that g(s) and any observationally equivalent g̃(s) must have the same, known exponent of regular variation at zero.\n\nA", "model_answer": "A", "error": null}}
{"ID": 244, "Question": "### Background\n\n**Research Question:** This problem investigates the robustness of the Generalized Stochastic Simulation Algorithm (GSSA) by examining its performance under high multicollinearity.\n\n**Setting / Institutional Environment:** The GSSA approximates a policy function by regressing a target variable on a matrix of basis functions, `X`. Using ordinary polynomials (e.g., `k²`, `k⁴`) often leads to an ill-conditioned `X` matrix, causing standard Ordinary Least Squares (OLS) to fail. The paper tests an alternative set of basis functions, Hermite polynomials, to address this.\n\n---\n\n### Data / Model Specification\n\nThe following table summarizes the stability of the OLS method when using different polynomial bases.\n\n**Table 1: Stability with Different Polynomial Bases (OLS Method)**\n\n| Polynomial Degree | Ordinary Polynomials `E_mean` | Hermite Polynomials `E_mean` |\n| :--- | :---: | :---: |\n| 2nd | -5.46 | -5.46 |\n| 3rd | — | -6.84 |\n| 4th | — | -7.97 |\n| 5th | — | -9.09 |\n*Note: '—' indicates the method failed to converge.* \n\n---\n\n### Question\n\nAccording to the paper and the results in Table 1, what is the primary mechanism through which using Hermite polynomials as basis functions enhances the numerical stability of the regression step, allowing OLS to succeed where it previously failed?\n", "Options": {"A": "Hermite polynomials are constructed to be orthogonal, which reduces the multicollinearity between the columns of the regressor matrix `X`.", "B": "Hermite polynomials are computationally faster to evaluate than ordinary polynomials, which reduces round-off errors.", "C": "Hermite polynomials are linear functions, which simplifies the regression problem and avoids the need to invert a matrix.", "D": "Hermite polynomials are only defined for normally distributed data, which matches the shock process and improves the regression fit."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). The original QA problem is a strong candidate for conversion. Its questions probe specific mechanisms and interpretations with highly convergent answers, and the potential for creating plausible distractors based on conceptual confusion is high.\n\nThis item assesses the core concept from Part 1 of the original QA. It tests the student's understanding of why one set of basis functions is more stable than another. Distractor B is a 'True but Irrelevant' type of error (or potentially false, but plausible). Distractor C is a Conceptual Opposite, as Hermite polynomials of degree > 1 are not linear. Distractor D is a conceptual error that misrepresents the properties of Hermite polynomials and their application.", "qid": "244", "question": "### Background\n\n**Research Question:** This problem investigates the robustness of the Generalized Stochastic Simulation Algorithm (GSSA) by examining its performance under high multicollinearity.\n\n**Setting / Institutional Environment:** The GSSA approximates a policy function by regressing a target variable on a matrix of basis functions, `X`. Using ordinary polynomials (e.g., `k²`, `k⁴`) often leads to an ill-conditioned `X` matrix, causing standard Ordinary Least Squares (OLS) to fail. The paper tests an alternative set of basis functions, Hermite polynomials, to address this.\n\n---\n\n### Data / Model Specification\n\nThe following table summarizes the stability of the OLS method when using different polynomial bases.\n\n**Table 1: Stability with Different Polynomial Bases (OLS Method)**\n\n| Polynomial Degree | Ordinary Polynomials `E_mean` | Hermite Polynomials `E_mean` |\n| :--- | :---: | :---: |\n| 2nd | -5.46 | -5.46 |\n| 3rd | — | -6.84 |\n| 4th | — | -7.97 |\n| 5th | — | -9.09 |\n*Note: '—' indicates the method failed to converge.* \n\n---\n\n### Question\n\nAccording to the paper and the results in Table 1, what is the primary mechanism through which using Hermite polynomials as basis functions enhances the numerical stability of the regression step, allowing OLS to succeed where it previously failed?\n", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 197, "Question": "### Background\n\n**Research Question.** This problem investigates the empirical relationship between parents' marriage type (homogamous vs. interfaith) and the effectiveness of religious transmission to their children, shedding light on marriage as a mechanism for purposeful socialization.\n\n**Setting.** The analysis uses U.S. survey data on the religious affiliation of parents and their children. The data allows for a direct comparison of socialization outcomes between families where parents share the same religion and families where they do not.\n\n**Variables and Parameters.**\n- **Marriage Type**: A categorical variable indicating the religious combination of parents. We focus on PP (Protestant-Protestant), CC (Catholic-Catholic), JJ (Jewish-Jewish), and PC (Protestant-Catholic).\n- **Child's Religion**: A categorical variable for the child's religious identity (Protestant, Catholic, Jewish, or Other).\n- **Socialization Probability**: The conditional probability that a child from a given marriage type adopts a specific religion, as reported in the table.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Socialization Probabilities for Selected Marriage Types**\n\n| Marriage Type | Child is Protestant | Child is Catholic | Child is Jew | Child is Other |\n| :--- | :---: | :---: | :---: | :---: |\n| PP (Prot.-Prot.) | **0.9179** | 0.0284 | 0.0000 | 0.0537 |\n| CC (Cath.-Cath.) | 0.0850 | **0.8571** | 0.0034 | 0.0544 |\n| JJ (Jew-Jew) | 0.0370 | 0.0000 | **0.9259** | 0.0370 |\n| PC (Prot.-Cath.) | 0.5116 | 0.3140 | 0.0000 | 0.1744 |\n\n*Source: Table 2 in the original paper. Each cell reports the sample probability that a child from the row's marriage type is a member of the column's religious group.*\n\n---\n\n### Question\n\nSuppose a new government policy promoting social mixing is implemented, which causes 100 Protestant families who would have formed PP marriages and 100 Catholic families who would have formed CC marriages to instead form 200 new PC marriages. Based on the probabilities in Table 1, what is the approximate net change in the total number of Catholic children produced by this group of 200 families as a result of the policy?", "Options": {"A": "A decrease of 26", "B": "An increase of 26", "C": "A decrease of 89", "D": "An increase of 2"}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a complex, policy-relevant counterfactual analysis using probabilistic data. It uses the Computational Judgment strategy. The distractors are designed to catch common errors: (B) is a sign error, confusing a decrease with an increase; (C) is a step-omission error where the student reports the baseline number of Catholic children instead of the net change; (D) confuses the result for Catholics with the result for Protestants, testing careful reading.", "qid": "197", "question": "### Background\n\n**Research Question.** This problem investigates the empirical relationship between parents' marriage type (homogamous vs. interfaith) and the effectiveness of religious transmission to their children, shedding light on marriage as a mechanism for purposeful socialization.\n\n**Setting.** The analysis uses U.S. survey data on the religious affiliation of parents and their children. The data allows for a direct comparison of socialization outcomes between families where parents share the same religion and families where they do not.\n\n**Variables and Parameters.**\n- **Marriage Type**: A categorical variable indicating the religious combination of parents. We focus on PP (Protestant-Protestant), CC (Catholic-Catholic), JJ (Jewish-Jewish), and PC (Protestant-Catholic).\n- **Child's Religion**: A categorical variable for the child's religious identity (Protestant, Catholic, Jewish, or Other).\n- **Socialization Probability**: The conditional probability that a child from a given marriage type adopts a specific religion, as reported in the table.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Socialization Probabilities for Selected Marriage Types**\n\n| Marriage Type | Child is Protestant | Child is Catholic | Child is Jew | Child is Other |\n| :--- | :---: | :---: | :---: | :---: |\n| PP (Prot.-Prot.) | **0.9179** | 0.0284 | 0.0000 | 0.0537 |\n| CC (Cath.-Cath.) | 0.0850 | **0.8571** | 0.0034 | 0.0544 |\n| JJ (Jew-Jew) | 0.0370 | 0.0000 | **0.9259** | 0.0370 |\n| PC (Prot.-Cath.) | 0.5116 | 0.3140 | 0.0000 | 0.1744 |\n\n*Source: Table 2 in the original paper. Each cell reports the sample probability that a child from the row's marriage type is a member of the column's religious group.*\n\n---\n\n### Question\n\nSuppose a new government policy promoting social mixing is implemented, which causes 100 Protestant families who would have formed PP marriages and 100 Catholic families who would have formed CC marriages to instead form 200 new PC marriages. Based on the probabilities in Table 1, what is the approximate net change in the total number of Catholic children produced by this group of 200 families as a result of the policy?", "model_evaluation_result": {"model_raw_response": "I need to calculate the change in the number of Catholic children when 100 PP marriages and 100 CC marriages are replaced by 200 PC marriages.\n\n**Before the policy:**\n- 100 PP marriages: Each has probability 0.0284 of producing a Catholic child\n  - Expected Catholic children = 100 × 0.0284 = 2.84\n\n- 100 CC marriages: Each has probability 0.8571 of producing a Catholic child\n  - Expected Catholic children = 100 × 0.8571 = 85.71\n\n- Total Catholic children before = 2.84 + 85.71 = 88.55\n\n**After the policy:**\n- 200 PC marriages: Each has probability 0.3140 of producing a Catholic child\n  - Expected Catholic children = 200 × 0.3140 = 62.80\n\n**Net change:**\nNet change = 62.80 - 88.55 = -25.75\n\nThis represents a decrease of approximately 26 Catholic children.\n\nA", "model_answer": "A", "error": null}}
{"ID": 161, "Question": "### Background\n\n**Research Question.** This problem asks you to quantify the differential impact of the COVID-19 shock on the planned quantities and prices of credit-constrained versus unconstrained firms, based on the paper's core empirical model.\n\n**Setting / Institutional Environment.** The analysis uses survey data on expectations from Italian firms, collected immediately before (`t-1`) and during (`t`) the first COVID-19 lockdown. The identification strategy treats the pandemic as an exogenous shock that causes firms to revise their plans.\n\n**Variables & Parameters.**\n*   `\\mathbb{F}_{i,t}(y)`: Firm `i`'s post-outbreak expectation for the 12-month growth rate of outcome `y`.\n*   `CC_{i,t-1}`: An indicator for whether firm `i` was credit constrained pre-outbreak (1=yes).\n*   Outcomes `y`: `Sal^g1Y` (sales growth), `Emp^g1Y` (employment growth), `P^g` (price growth). All are in percentage points.\n\n---\n\n### Data / Model Specification\n\nThe baseline model is:\n```latex\n\\mathbb{F}_{i,t}(y) = \\alpha + \\beta CC_{i,t-1} + \\delta \\mathbb{F}_{i,t-1}(y) + \\gamma^{\\top}x_{i,t-1} + \\lambda_{s} + \\lambda_{p} + \\varepsilon_{i,t}\n```\nwhere `x` is a vector of controls.\n\nSelected results from the paper's baseline estimation are presented below.\n\n**Table 1: The Effect of Credit Constraints on Firms' Expectations**\n| | (1) `\\mathbb{F}_{i,t}(Sal^g1Y)` | (2) `\\mathbb{F}_{i,t}(Emp^g1Y)` | (3) `\\mathbb{F}_{i,t}(P^g)` |\n|:---|:---:|:---:|:---:|\n| **Credit constrained** (`CC`) | -10.62*** | -10.25*** | 3.754*** |\n| | [2.111] | [2.515] | [0.949] |\n| **Essential** | 11.22*** | 4.577** | -2.842** |\n| | [1.218] | [2.190] | [1.286] |\n\n*Notes: Dependent variables are post-COVID expectations for 12-month growth rates. Standard errors are in brackets. *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nBased on the results in Table 1, which of the following statements correctly interprets the effect of being credit constrained?", "Options": {"A": "A credit-constrained firm plans to increase its prices by 3.75 percentage points more over the next 12 months than a comparable unconstrained firm.", "B": "A credit-constrained firm plans to decrease its prices by 3.75 percentage points more over the next 12 months than a comparable unconstrained firm.", "C": "A credit-constrained firm plans to decrease its prices by 2.84 percentage points more over the next 12 months than a comparable unconstrained firm.", "D": "A credit-constrained firm expects its sales to grow by 3.75 percentage points more over the next 12 months than a comparable unconstrained firm."}, "Answer": "A", "pi_justification": "This item assesses the foundational skill of interpreting a regression coefficient from a table. It replaces the repetitive interpretation part of the original QA problem. The distractors are designed to catch common errors: (B) is a sign error, (C) confuses the coefficient of interest with another one (`Essential`), and (D) correctly identifies the coefficient's value but misattributes it to the wrong outcome variable (sales instead of prices).", "qid": "161", "question": "### Background\n\n**Research Question.** This problem asks you to quantify the differential impact of the COVID-19 shock on the planned quantities and prices of credit-constrained versus unconstrained firms, based on the paper's core empirical model.\n\n**Setting / Institutional Environment.** The analysis uses survey data on expectations from Italian firms, collected immediately before (`t-1`) and during (`t`) the first COVID-19 lockdown. The identification strategy treats the pandemic as an exogenous shock that causes firms to revise their plans.\n\n**Variables & Parameters.**\n*   `\\mathbb{F}_{i,t}(y)`: Firm `i`'s post-outbreak expectation for the 12-month growth rate of outcome `y`.\n*   `CC_{i,t-1}`: An indicator for whether firm `i` was credit constrained pre-outbreak (1=yes).\n*   Outcomes `y`: `Sal^g1Y` (sales growth), `Emp^g1Y` (employment growth), `P^g` (price growth). All are in percentage points.\n\n---\n\n### Data / Model Specification\n\nThe baseline model is:\n```latex\n\\mathbb{F}_{i,t}(y) = \\alpha + \\beta CC_{i,t-1} + \\delta \\mathbb{F}_{i,t-1}(y) + \\gamma^{\\top}x_{i,t-1} + \\lambda_{s} + \\lambda_{p} + \\varepsilon_{i,t}\n```\nwhere `x` is a vector of controls.\n\nSelected results from the paper's baseline estimation are presented below.\n\n**Table 1: The Effect of Credit Constraints on Firms' Expectations**\n| | (1) `\\mathbb{F}_{i,t}(Sal^g1Y)` | (2) `\\mathbb{F}_{i,t}(Emp^g1Y)` | (3) `\\mathbb{F}_{i,t}(P^g)` |\n|:---|:---:|:---:|:---:|\n| **Credit constrained** (`CC`) | -10.62*** | -10.25*** | 3.754*** |\n| | [2.111] | [2.515] | [0.949] |\n| **Essential** | 11.22*** | 4.577** | -2.842** |\n| | [1.218] | [2.190] | [1.286] |\n\n*Notes: Dependent variables are post-COVID expectations for 12-month growth rates. Standard errors are in brackets. *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nBased on the results in Table 1, which of the following statements correctly interprets the effect of being credit constrained?", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to interpret the coefficient for \"Credit constrained\" (CC) in column (3) for price growth expectations.\n\nThe coefficient is 3.754*** for the price growth equation, which means:\n\n- Credit-constrained firms have post-COVID price growth expectations that are 3.754 percentage points **higher** than unconstrained firms\n- This is a positive coefficient, indicating credit-constrained firms plan to **increase** their prices more (or decrease them less) compared to unconstrained firms\n- The interpretation is relative: credit-constrained firms plan to increase prices by 3.75 percentage points **more** than comparable unconstrained firms\n\nLooking at the options:\n\nA) Correctly states that credit-constrained firms plan to increase prices by 3.75 percentage points more than unconstrained firms ✓\n\nB) Incorrectly states \"decrease\" when the positive coefficient indicates an increase\n\nC) Uses the wrong coefficient (2.84 is from the \"Essential\" variable, not credit constraints)\n\nD) Confuses price effects with sales effects and uses the wrong coefficient\n\nA", "model_answer": "A", "error": null}}
{"ID": 233, "Question": "### Background\n\n**Research Question.** This problem investigates the macroeconomic management of a large foreign aid inflow, focusing on the external sector's adjustment, a phenomenon known as \"Dutch Disease.\"\n\n**Setting.** Producers in the model allocate output between exports and domestic sales based on relative prices. An aid inflow can alter these prices and be managed by central bank policy.\n\n**Variables & Parameters.**\n- `X`: Quantity of goods allocated to exports.\n- `DOM`: Quantity of goods allocated to domestic sales.\n- `PX/PD`: Relative price of exports to domestic goods.\n- `ΔNFA`: Change in net foreign assets (reserve accumulation).\n\n---\n\n### Data / Model Specification\n\n1.  **Trade Allocation:** Producers allocate total output between exports (`X`) and domestic sales (`DOM`) based on relative prices, according to a Constant Elasticity of Transformation (CET) function.\n    ```latex\n    X/\\mathrm{DOM} = \\{ (\\mathrm{PX/PD}) \\cdot C \\}^{\\sigma\\mathrm{DE}}\n    ```\n    where `C` is a constant.\n\n2.  **Reserve Management:** The central bank accumulates reserves (`ΔNFA`) as a policy response, which affects the balance of payments and the exchange rate.\n\n**Table 1: Simulation Impact of a 5pp Aid Increase (2007)**\n*(Absolute deviations from baseline)*\n\n| Variable | 2007 Impact |\n| :--- | :--- |\n| Real Exchange Rate (% change) | -0.51 |\n| Change in Foreign Assets (% of GDP) | +1.57 |\n\n---\n\n### Question\n\nFollowing a 5 percentage point increase in non-food aid, the simulation shows an immediate real exchange rate appreciation (Table 1). According to the model's logic, which statement best explains this 'Dutch Disease' effect and one of the key macroeconomic policy factors that mitigates it?", "Options": {"A": "The appreciation is caused by the large currency inflow, which harms export competitiveness; however, the central bank's accumulation of foreign reserves dampens this effect.", "B": "The appreciation is caused by falling domestic prices, which helps export competitiveness; the effect is amplified by the central bank selling foreign reserves.", "C": "The appreciation is caused by the currency inflow, and it is primarily mitigated by the long-run increase in productivity from aid-funded public investment.", "D": "The appreciation makes imports cheaper, and it is mitigated by the government using the aid to reduce its fiscal deficit."}, "Answer": "A", "pi_justification": "This item assesses understanding of the 'Dutch Disease' phenomenon and the macroeconomic policy tools used to manage it. It uses a Scenario Application strategy, asking the student to apply theory to simulation results. Distractors are designed based on common confusions: B reverses the logic of both the cause and the mitigation, C presents a correct but long-term structural mitigation instead of the immediate policy response shown in the data, and D confuses correlation (fiscal balance improves) with the specific causal mechanism for exchange rate management (reserve policy).", "qid": "233", "question": "### Background\n\n**Research Question.** This problem investigates the macroeconomic management of a large foreign aid inflow, focusing on the external sector's adjustment, a phenomenon known as \"Dutch Disease.\"\n\n**Setting.** Producers in the model allocate output between exports and domestic sales based on relative prices. An aid inflow can alter these prices and be managed by central bank policy.\n\n**Variables & Parameters.**\n- `X`: Quantity of goods allocated to exports.\n- `DOM`: Quantity of goods allocated to domestic sales.\n- `PX/PD`: Relative price of exports to domestic goods.\n- `ΔNFA`: Change in net foreign assets (reserve accumulation).\n\n---\n\n### Data / Model Specification\n\n1.  **Trade Allocation:** Producers allocate total output between exports (`X`) and domestic sales (`DOM`) based on relative prices, according to a Constant Elasticity of Transformation (CET) function.\n    ```latex\n    X/\\mathrm{DOM} = \\{ (\\mathrm{PX/PD}) \\cdot C \\}^{\\sigma\\mathrm{DE}}\n    ```\n    where `C` is a constant.\n\n2.  **Reserve Management:** The central bank accumulates reserves (`ΔNFA`) as a policy response, which affects the balance of payments and the exchange rate.\n\n**Table 1: Simulation Impact of a 5pp Aid Increase (2007)**\n*(Absolute deviations from baseline)*\n\n| Variable | 2007 Impact |\n| :--- | :--- |\n| Real Exchange Rate (% change) | -0.51 |\n| Change in Foreign Assets (% of GDP) | +1.57 |\n\n---\n\n### Question\n\nFollowing a 5 percentage point increase in non-food aid, the simulation shows an immediate real exchange rate appreciation (Table 1). According to the model's logic, which statement best explains this 'Dutch Disease' effect and one of the key macroeconomic policy factors that mitigates it?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 182, "Question": "### Background\n\n**Research Question.** This analysis seeks to quantify the empirical impact of banking crises on exports, testing if the effect is amplified in 'deep' crises (those with above-median GDP loss).\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences approach where the treatment is a banking crisis and the treatment intensity is an industry's intrinsic reliance on external finance (`RZ`). The model includes an interaction term to capture the additional effect of a 'deep' crisis.\n\n### Data / Model Specification\n\nThe dependent variable is log(1 + Exports). The highly dependent Drugs industry has `RZ` = 1.46, while the cash-rich Tobacco industry has `RZ` = -0.45.\n\n**Table 1: Heterogeneity by Crisis Depth**\n\n| Variable | Coefficient | Std. Error |\n|:---|:---:|:---:|\n| `RZ*Crisis` | -0.116*** | [0.0428] |\n| `RZ*Crisis*Deep` | -0.326** | [0.133] |\n\n*Notes: `Crisis` is a dummy for a banking crisis. `Deep` is a dummy for crises with above-median output loss. ** p<0.05, *** p<0.01.*\n\nUsing the results from Table 1, calculate the predicted differential change in log exports between the highly dependent Drugs industry (`RZ` = 1.46) and the cash-rich Tobacco industry (`RZ` = -0.45) if the country experiences a 'deep' crisis.", "Options": {"A": "The Drugs industry's exports are predicted to fall by 84.4 percentage points relative to the Tobacco industry.", "B": "The Drugs industry's exports are predicted to fall by 22.1 percentage points relative to the Tobacco industry.", "C": "The Drugs industry's exports are predicted to fall by 64.5 percentage points relative to the Tobacco industry.", "D": "The Drugs industry's exports are predicted to fall by 44.6 percentage points relative to the Tobacco industry."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses a student's ability to perform a complex counterfactual calculation involving a three-way interaction term. It uses a Computational Judgment strategy. The distractors are based on a matrix of likely errors: (B) ignoring the main interaction term for 'deep' crises (Step-Omission Error), (C) failing to calculate the full differential effect for both industries (Step-Omission Error), and (D) making a sign error in the final comparison (Sign Error).", "qid": "182", "question": "### Background\n\n**Research Question.** This analysis seeks to quantify the empirical impact of banking crises on exports, testing if the effect is amplified in 'deep' crises (those with above-median GDP loss).\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences approach where the treatment is a banking crisis and the treatment intensity is an industry's intrinsic reliance on external finance (`RZ`). The model includes an interaction term to capture the additional effect of a 'deep' crisis.\n\n### Data / Model Specification\n\nThe dependent variable is log(1 + Exports). The highly dependent Drugs industry has `RZ` = 1.46, while the cash-rich Tobacco industry has `RZ` = -0.45.\n\n**Table 1: Heterogeneity by Crisis Depth**\n\n| Variable | Coefficient | Std. Error |\n|:---|:---:|:---:|\n| `RZ*Crisis` | -0.116*** | [0.0428] |\n| `RZ*Crisis*Deep` | -0.326** | [0.133] |\n\n*Notes: `Crisis` is a dummy for a banking crisis. `Deep` is a dummy for crises with above-median output loss. ** p<0.05, *** p<0.01.*\n\nUsing the results from Table 1, calculate the predicted differential change in log exports between the highly dependent Drugs industry (`RZ` = 1.46) and the cash-rich Tobacco industry (`RZ` = -0.45) if the country experiences a 'deep' crisis.", "model_evaluation_result": {"model_raw_response": "I need to calculate the differential change in log exports between the Drugs industry (RZ = 1.46) and Tobacco industry (RZ = -0.45) during a deep crisis.\n\nFor a deep crisis, both Crisis = 1 and Deep = 1, so the total effect is:\nEffect = β₁(RZ × Crisis) + β₂(RZ × Crisis × Deep)\nEffect = β₁(RZ × 1) + β₂(RZ × 1 × 1)\nEffect = RZ × (β₁ + β₂)\n\nFrom the table:\n- β₁ = -0.116\n- β₂ = -0.326\n\nSo the total coefficient is: -0.116 + (-0.326) = -0.442\n\nFor Drugs industry (RZ = 1.46):\nEffect_Drugs = 1.46 × (-0.442) = -0.645\n\nFor Tobacco industry (RZ = -0.45):\nEffect_Tobacco = -0.45 × (-0.442) = 0.199\n\nThe differential effect is:\nDifferential = Effect_Drugs - Effect_Tobacco\nDifferential = -0.645 - 0.199 = -0.844\n\nThis means the Drugs industry's exports fall by 84.4 percentage points relative to the Tobacco industry.\n\nA", "model_answer": "A", "error": null}}
{"ID": 246, "Question": "### Background\n\n**Research Question:** This problem investigates the robustness of the Generalized Stochastic Simulation Algorithm (GSSA) to non-Gaussian shocks by testing it on a model featuring 'rare disasters' (large, low-probability negative shocks).\n\n**Setting / Institutional Environment:** The paper adapts its integration scheme to explicitly account for the disaster and no-disaster states and their respective probabilities. The goal is to see if high accuracy can be maintained.\n\n---\n\n### Data / Model Specification\n\nThe table below shows the best accuracy achieved in the rare disaster model. For comparison, the best accuracy achieved in the standard model (without rare disasters) using an equivalent setup was `E_mean` = -8.91.\n\n**Table 1: Performance in Rare Disasters Model (5th Degree Poly.)**\n\n| Method | `E_mean` (log10) | `E_max` (log10) |\n| :--- | :---: | :---: |\n| RLS-TSVD | -8.66 | -6.54 |\n\n---\n\n### Question\n\nComparing the best accuracy in the rare disaster model (`E_mean` = -8.66) with the benchmark from the standard model (`E_mean` = -8.91), what can be concluded about GSSA's robustness?\n", "Options": {"A": "GSSA is highly robust; while there is a small, measurable loss of accuracy, it still achieves exceptionally high accuracy (mean errors of order 10⁻⁹).", "B": "GSSA is not robust to rare disasters, as the mean error increases by several orders of magnitude.", "C": "GSSA's accuracy is unaffected by rare disasters, as the log10 errors are nearly identical.", "D": "GSSA's mean accuracy (`E_mean`) is robust, but its worst-case accuracy (`E_max`) deteriorates significantly, making it unsuitable for this type of model."}, "Answer": "A", "pi_justification": "This item assesses the interpretation from Part 3 of the original QA. It requires the student to compare two numbers and draw a nuanced conclusion. Distractor B is a Conceptual Opposite, vastly overstating the loss of accuracy. Distractor C is an 'Almost Right' option that fails to acknowledge the small difference. Distractor D draws an overly strong negative conclusion from the `E_max` value without sufficient context for comparison, while the paper's main conclusion focuses on the successful achievement of high accuracy overall.", "qid": "246", "question": "### Background\n\n**Research Question:** This problem investigates the robustness of the Generalized Stochastic Simulation Algorithm (GSSA) to non-Gaussian shocks by testing it on a model featuring 'rare disasters' (large, low-probability negative shocks).\n\n**Setting / Institutional Environment:** The paper adapts its integration scheme to explicitly account for the disaster and no-disaster states and their respective probabilities. The goal is to see if high accuracy can be maintained.\n\n---\n\n### Data / Model Specification\n\nThe table below shows the best accuracy achieved in the rare disaster model. For comparison, the best accuracy achieved in the standard model (without rare disasters) using an equivalent setup was `E_mean` = -8.91.\n\n**Table 1: Performance in Rare Disasters Model (5th Degree Poly.)**\n\n| Method | `E_mean` (log10) | `E_max` (log10) |\n| :--- | :---: | :---: |\n| RLS-TSVD | -8.66 | -6.54 |\n\n---\n\n### Question\n\nComparing the best accuracy in the rare disaster model (`E_mean` = -8.66) with the benchmark from the standard model (`E_mean` = -8.91), what can be concluded about GSSA's robustness?\n", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 137, "Question": "## Background\n\n**Research Question.** This problem explores the fundamental definition of a two-sided market and the conditions that create non-neutrality in the price structure.\n\n**Setting.** A platform charges per-interaction fees `a^B` and `a^S`. The total price level is `a = a^B + a^S`. The market is **one-sided** if transaction volume `V` depends only on `a`, and **two-sided** if `V` also depends on the price structure.\n\n---\n\n## Data / Model Specification\n\nConsider a market where a seller with private cost information makes a price offer `P` to a buyer. In the absence of constraints, a change in platform fees from `(a^S, a^B)` to `(a^S + Δa, a^B - Δa)` is neutral, as the seller simply adjusts their offer `P` upwards by `Δa`. Now, suppose the platform imposes a price cap `P_max` on the transaction price between the seller and buyer. The seller's optimal unconstrained offer before the fee change is `P*`.\n\n---\n\n## Question\n\nUnder which of the following conditions will the price cap `P_max` cause the platform's change in fee structure to be non-neutral, thus making the market two-sided?", "Options": {"A": "The cap prevents the seller from fully passing through the fee increase, which occurs if `P* + Δa > P_max`.", "B": "The cap is binding on the seller's original optimal price, i.e., `P* > P_max`.", "C": "The cap forces the seller to absorb all costs, which occurs if `P_max < a^S + Δa`.", "D": "The cap is non-binding, allowing the seller to freely adjust their price, i.e., `P* + Δa < P_max`."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to identify a condition that breaks price neutrality. It uses a Scenario Application strategy. The correct answer precisely identifies the binding constraint that prevents the isomorphic adjustment of prices. Distractor B identifies a condition where the cap is already binding, but this doesn't explain why the *change* in fees is non-neutral. Distractor C introduces an irrelevant comparison. Distractor D describes the exact condition under which neutrality *holds*, making it a conceptual opposite.", "qid": "137", "question": "## Background\n\n**Research Question.** This problem explores the fundamental definition of a two-sided market and the conditions that create non-neutrality in the price structure.\n\n**Setting.** A platform charges per-interaction fees `a^B` and `a^S`. The total price level is `a = a^B + a^S`. The market is **one-sided** if transaction volume `V` depends only on `a`, and **two-sided** if `V` also depends on the price structure.\n\n---\n\n## Data / Model Specification\n\nConsider a market where a seller with private cost information makes a price offer `P` to a buyer. In the absence of constraints, a change in platform fees from `(a^S, a^B)` to `(a^S + Δa, a^B - Δa)` is neutral, as the seller simply adjusts their offer `P` upwards by `Δa`. Now, suppose the platform imposes a price cap `P_max` on the transaction price between the seller and buyer. The seller's optimal unconstrained offer before the fee change is `P*`.\n\n---\n\n## Question\n\nUnder which of the following conditions will the price cap `P_max` cause the platform's change in fee structure to be non-neutral, thus making the market two-sided?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand when a price cap makes the fee structure change non-neutral.\n\nIn the absence of constraints, when fees change from (a^S, a^B) to (a^S + Δa, a^B - Δa), the seller can simply adjust their price P upward by Δa to maintain the same net payoff. This makes the change neutral since transaction volume V depends only on the total fee level a = a^B + a^S.\n\nHowever, with a price cap P_max, the seller may not be able to make this full adjustment.\n\nLet me analyze each option:\n\nA) \"The cap prevents the seller from fully passing through the fee increase, which occurs if P* + Δa > P_max.\"\n\nThis is the key condition. If the seller's optimal response would be to set price P* + Δa (to fully pass through the increased fee), but the cap prevents this because P* + Δa > P_max, then the seller cannot fully adjust. This creates non-neutrality because the seller must absorb some of the fee increase, making the fee structure matter beyond just the total level.\n\nB) \"The cap is binding on the seller's original optimal price, i.e., P* > P_max.\"\n\nThis describes when the cap was already binding before the fee change, but doesn't necessarily create non-neutrality from the fee structure change itself.\n\nC) \"The cap forces the seller to absorb all costs, which occurs if P_max < a^S + Δa.\"\n\nThis condition is about the relationship between the cap and fees, but doesn't directly address when the fee structure change becomes non-neutral.\n\nD) \"The cap is non-binding, allowing the seller to freely adjust their price, i.e., P* + Δa < P_max.\"\n\nIf the cap doesn't bind after the fee change, the seller can fully adjust their price, maintaining neutrality.\n\nThe correct answer is A, as it identifies the precise condition where the price cap prevents the seller from fully passing through the fee increase, creating non-neutrality in the fee structure.\n\nA", "model_answer": "B", "error": null}}
{"ID": 75, "Question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nIn the no-linking regime, a landlord cannot screen tenants and hires randomly from the pool of applicants. If a landlord happens to draw a tenant of ability `e_i` (where `i \\geq n`), what is the correct expression for the landlord's ex-post profit, `\\pi_i^{no-link}`?", "Options": {"A": "`(1-\\alpha)F(e_i, \\hat{b}_n)`", "B": "`(1-\\alpha)F(e_i, b_i^*)`", "C": "`F(e_i, \\hat{b}_n) - (1+r_0)a\\hat{b}_n - w`", "D": "`(1-\\alpha)F(e_i, \\hat{b}_n) + (r_i - r_0)a\\hat{b}_n`"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to correctly apply the profit definition in a new institutional context. It uses a Computational Judgment strategy. The correct answer requires recognizing that the landlord's profit is their output share, evaluated at the mandated capital level, with no credit component. Distractors represent common errors: (B) Formula Misuse (using the wrong capital level), (D) Conceptual Error (incorrectly including a credit-linking term that is banned), and (C) Formula Misuse (using the total surplus expression instead of the landlord's profit).", "qid": "75", "question": "### Background\n\nThis question analyzes the paper's central policy conclusion by comparing the welfare properties of the baseline regime (which allows interlinked contracts) with a counterfactual regime where such interlinking is prohibited.\n\n### Data / Model Specification\n\nWe compare two policy regimes:\n1.  **Linking Regime:** The baseline model where landlords offer interlinked contracts `s_i = (b_i, r_i)` and can perfectly screen tenants. The equilibrium interest rate for the marginal tenant `e_n` is `r_n`.\n2.  **No-Linking Regime:** A counterfactual policy bans tied credit. Landlords can only set a single minimum capital requirement, `\\hat{b}`, for all tenants. Tenants borrow from the market at rate `r_0`. Landlords cannot distinguish between applicants who meet the requirement and must hire a tenant at random from this pool.\n\nThe minimum capital requirement in the no-linking regime, `\\hat{b}_n`, is set to make the marginal tenant `e_n` just indifferent to participating:\n```latex\n\\alpha F(e_n, \\hat{b}_n) - (1+r_0)a \\hat{b}_n = w \\quad \\text{(Eq. 1)}\n```\nwhere `w` is the tenant's outside wage. The first-best (efficient) capital level for this tenant, `b_n^*`, is the one that maximizes total surplus `F(e_n, b) - (1+r_0)ab`.\n\n### Question\n\nIn the no-linking regime, a landlord cannot screen tenants and hires randomly from the pool of applicants. If a landlord happens to draw a tenant of ability `e_i` (where `i \\geq n`), what is the correct expression for the landlord's ex-post profit, `\\pi_i^{no-link}`?", "model_evaluation_result": {"model_raw_response": "I need to analyze the landlord's profit in the no-linking regime when they randomly draw a tenant of ability e_i.\n\nIn the no-linking regime:\n- Landlords set a single minimum capital requirement ĥb_n for all tenants\n- Tenants borrow at market rate r_0 \n- Landlords cannot screen and must hire randomly\n- The tenant gets share α of output, landlord gets share (1-α)\n\nWhen a landlord draws tenant e_i with capital ĥb_n:\n- Total output is F(e_i, ĥb_n)\n- Landlord's share of output is (1-α)F(e_i, ĥb_n)\n- The tenant handles their own financing at rate r_0, so the landlord doesn't bear credit costs\n- The landlord doesn't pay wages since this is a sharecropping arrangement where the tenant gets share α\n\nThe landlord's ex-post profit is simply their share of the total output produced by tenant e_i using the required capital level ĥb_n.\n\nLooking at the options:\nA) (1-α)F(e_i, ĥb_n) - This represents the landlord's share of output\nB) (1-α)F(e_i, b_i*) - This uses the first-best capital level, not the actual requirement\nC) F(e_i, ĥb_n) - (1+r_0)aĥb_n - w - This looks like total surplus minus costs and wages\nD) (1-α)F(e_i, ĥb_n) + (r_i - r_0)aĥb_n - This includes an interest rate differential that doesn't apply in no-linking\n\nThe correct answer is A, as it represents the landlord's share (1-α) of the total output F(e_i, ĥb_n) produced by the randomly drawn tenant.\n\nA", "model_answer": "A", "error": null}}
{"ID": 196, "Question": "### Background\n\nThis paper develops a theoretical model to understand the meaning of residual correlations around Engel curves. The analysis is grounded in the classical theory of consumer demand but makes specific assumptions to achieve analytical tractability when using cross-sectional budget data.\n\n### Data / Model Specification\n\nThe consumer's problem is to maximize a utility function `u(x)` subject to a budget constraint `p'x = M`. The model makes two key assumptions:\n\n**Assumption I:** The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`.\nThe vector `a` represents consumer-specific linear preference terms, while the matrix `B` of quadratic terms is assumed to be identical for all consumers.\n\n**Assumption II:** Prices `p` are also identical for all consumers in the budget survey.\n\nThe first-order conditions for utility maximization can be written as a linear system:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -a \\\\ -M \\end{pmatrix} \\quad \\text{(Eq. (1))}\n```\nThe inverse of the bordered matrix on the left defines the substitution matrix `C` and the income-effect vector `c`:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix}^{-1} = \\begin{pmatrix} C & c \\\\ c' & c_0 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n```\nThis leads to a demand system `x = -Ca - cM`. The Engel curve for good `i` is found by separating the mean preference `α` from the idiosyncratic preference `(a - α)`. The disturbance term `ν` for the vector of goods is `ν = -C(a - α)`.\n\n### Question\n\nGiven the matrix system for utility maximization in Eq. (1) and the definition of the inverse in Eq. (2), what is the correct expression for the vector of demand functions, `x`?", "Options": {"A": "`x = -Ca - cM`", "B": "`x = Ca + cM`", "C": "`x = -Ba + pM`", "D": "`x = -Ca - cλ`"}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests the student's ability to perform the basic matrix algebra that underpins the entire model. It is a direct Computational Judgment question. The correct answer (A) is the result of the derivation. The distractors are based on common algebraic errors: (B) is a Sign Error, (C) is a Formula Misuse error (using original matrices instead of the inverse), and (D) is a Variable Error (confusing income `M` with the Lagrange multiplier `λ`).", "qid": "196", "question": "### Background\n\nThis paper develops a theoretical model to understand the meaning of residual correlations around Engel curves. The analysis is grounded in the classical theory of consumer demand but makes specific assumptions to achieve analytical tractability when using cross-sectional budget data.\n\n### Data / Model Specification\n\nThe consumer's problem is to maximize a utility function `u(x)` subject to a budget constraint `p'x = M`. The model makes two key assumptions:\n\n**Assumption I:** The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`.\nThe vector `a` represents consumer-specific linear preference terms, while the matrix `B` of quadratic terms is assumed to be identical for all consumers.\n\n**Assumption II:** Prices `p` are also identical for all consumers in the budget survey.\n\nThe first-order conditions for utility maximization can be written as a linear system:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -a \\\\ -M \\end{pmatrix} \\quad \\text{(Eq. (1))}\n```\nThe inverse of the bordered matrix on the left defines the substitution matrix `C` and the income-effect vector `c`:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix}^{-1} = \\begin{pmatrix} C & c \\\\ c' & c_0 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n```\nThis leads to a demand system `x = -Ca - cM`. The Engel curve for good `i` is found by separating the mean preference `α` from the idiosyncratic preference `(a - α)`. The disturbance term `ν` for the vector of goods is `ν = -C(a - α)`.\n\n### Question\n\nGiven the matrix system for utility maximization in Eq. (1) and the definition of the inverse in Eq. (2), what is the correct expression for the vector of demand functions, `x`?", "model_evaluation_result": {"model_raw_response": "Looking at the matrix system in Eq. (1):\n\n```\n[B  -p] [x]   [-a]\n[-p' 0] [λ] = [-M]\n```\n\nAnd the inverse matrix definition in Eq. (2):\n\n```\n[B  -p]^(-1)   [C  c]\n[-p' 0]      = [c' c₀]\n```\n\nTo find the demand functions x, I need to solve the system by multiplying both sides of Eq. (1) by the inverse matrix:\n\n```\n[x]   [C  c] [-a]\n[λ] = [c' c₀][-M]\n```\n\nPerforming the matrix multiplication for the first row (which gives us x):\n\nx = C(-a) + c(-M) = -Ca - cM\n\nThis matches exactly what the background text states: \"This leads to a demand system x = -Ca - cM.\"\n\nThe answer is A.", "model_answer": "B", "error": null}}
{"ID": 180, "Question": "### Background\n\n**Research Question.** This problem examines whether structured management practices predict long-term establishment performance, specifically survival.\n\n**Setting / Institutional Environment.** The paper notes that the relationship between management and survival is stronger for young plants. Suppose the coefficient from a linear probability model of exit on management is -0.360 for young plants (≤ 5 years old) and -0.180 for mature plants (> 20 years old). A policymaker considers an intervention that improves the management score by 0.1 units for all plants that adopt it.\n\n---\n\n### Question\n\nIf the intervention is adopted by 1,000 young plants and 1,000 mature plants, what is the expected outcome?", "Options": {"A": "The intervention is expected to prevent approximately 36 closures among the young plants and 18 closures among the mature plants, suggesting it is more effective when targeted at younger firms.", "B": "The intervention is expected to prevent approximately 360 closures among young plants and 180 closures among mature plants.", "C": "The intervention is expected to prevent approximately 54 closures in total, with the impact being equally distributed between young and mature firms.", "D": "The intervention is more effective for mature plants, preventing 36 closures among them compared to 18 for young plants."}, "Answer": "A", "pi_justification": "This is a Scenario Application question that tests the ability to apply regression coefficients to a concrete policy counterfactual. It requires both accurate calculation and correct interpretation to draw a valid policy conclusion. The correct answer (A) performs the calculations correctly (1000 * 0.1 * 0.360 = 36; 1000 * 0.1 * 0.180 = 18) and draws the right inference. Distractors are based on common errors: (B) is a decimal place error; (C) fails to recognize the differential impact; (D) reverses the roles of the coefficients, leading to a flawed conclusion.", "qid": "180", "question": "### Background\n\n**Research Question.** This problem examines whether structured management practices predict long-term establishment performance, specifically survival.\n\n**Setting / Institutional Environment.** The paper notes that the relationship between management and survival is stronger for young plants. Suppose the coefficient from a linear probability model of exit on management is -0.360 for young plants (≤ 5 years old) and -0.180 for mature plants (> 20 years old). A policymaker considers an intervention that improves the management score by 0.1 units for all plants that adopt it.\n\n---\n\n### Question\n\nIf the intervention is adopted by 1,000 young plants and 1,000 mature plants, what is the expected outcome?", "model_evaluation_result": {"model_raw_response": "I need to calculate the expected impact of the management intervention on plant closures.\n\nGiven information:\n- Coefficient for young plants (≤ 5 years): -0.360\n- Coefficient for mature plants (> 20 years): -0.180\n- Management score improvement: 0.1 units\n- Number of plants in each group: 1,000\n\nThe coefficient represents the change in probability of exit for a one-unit change in management score. Since we're looking at a 0.1 unit improvement:\n\nFor young plants:\n- Change in exit probability = -0.360 × 0.1 = -0.036\n- This means a 3.6 percentage point reduction in exit probability\n- For 1,000 young plants: 1,000 × 0.036 = 36 fewer closures\n\nFor mature plants:\n- Change in exit probability = -0.180 × 0.1 = -0.018\n- This means a 1.8 percentage point reduction in exit probability\n- For 1,000 mature plants: 1,000 × 0.018 = 18 fewer closures\n\nThe intervention prevents 36 closures among young plants and 18 among mature plants, showing it's more effective for younger firms (36 > 18).\n\nA", "model_answer": "A", "error": null}}
{"ID": 167, "Question": "### Background\n\n**Research Question.** This problem deconstructs the econometric methodology used to classify firms as 'strategic' or 'confused' based on their VAT reporting behavior. The core challenge is to attribute responsibility for a reporting discrepancy observed between a buyer and a seller to each firm's intrinsic reporting tendency.\n\n**Setting and Sample.** The analysis uses monthly, transaction-level VAT data for firm-pairs in Uganda. The methodology, inspired by Abowd et al. (1999), is applied to the largest \"connected set\" of trading firms, where firms are linked directly or indirectly through their trading partners.\n\n---\n\n### Data / Model Specification\n\nThe methodology proceeds in two main steps:\n\n1.  **Define and Decompose Discrepancy:** The discrepancy between a buyer `f` and seller `f'` in month `t` is defined as the difference between the VAT reported by the buyer and the seller:\n    ```latex\n    d_{f f't} \\equiv Y_{f f't}^{B} - Y_{f f't}^{S} \\quad \\text{(Eq. (1))}\n    ```\n    A positive `d` implies a seller shortfall (liability-reducing), while a negative `d` implies a buyer shortfall (liability-increasing). This discrepancy is then decomposed using a two-way fixed-effects model:\n    ```latex\n    d_{f f't} = \\delta_{c} + \\delta_{f}^{b} + \\delta_{f'}^{s} + \\delta_{t} + r_{f f't} \\quad \\text{(Eq. (2))}\n    ```\n    where `\\delta_f^b` is a buyer fixed effect for firm `f`, `\\delta_{f'}^s` is a seller fixed effect for firm `f'`, and `\\delta_t` is a month fixed effect.\n\n2.  **Construct Firm-Level Classification Statistic:** The estimated fixed effects for a given firm `f` are aggregated into a single statistic, `Q_f`, which summarizes its overall misreporting behavior:\n    ```latex\n    Q_{f} \\equiv w_{s} \\cdot \\hat{\\delta}_{f}^{s} + w_{b} \\cdot \\hat{\\delta}_{f}^{b} \\quad \\text{(Eq. (3))}\n    ```\n    where `w_s` and `w_b` are the number of observations for firm `f` as a seller and buyer, respectively. A firm is classified as 'advantageous' if `Q_f > 0` and 'disadvantageous' if `Q_f < 0`.\n\n---\n\nBased on the model specification, what is the correct interpretation of an estimated buyer fixed effect `\\hat{\\delta}_f^b` that is significantly positive for a firm `f`?", "Options": {"A": "The firm systematically over-reports its purchases, which reduces its net VAT liability.", "B": "The firm systematically under-reports its purchases, which increases its net VAT liability.", "C": "The firm systematically under-reports its sales, which reduces its net VAT liability.", "D": "The firm is classified as a 'disadvantageous' misreporter because it contributes positively to discrepancies."}, "Answer": "A", "pi_justification": "This item assesses the core interpretation of the model's key parameters. It uses a Computational Judgment strategy. The correct answer requires understanding how the discrepancy is defined and how input VAT credits affect liability. Distractors are designed based on common errors: (B) is a sign error/conceptual opposite; (C) confuses the buyer and seller fixed effects; (D) incorrectly links a single positive fixed effect to the final 'disadvantageous' classification, which depends on the weighted sum `Q_f`.", "qid": "167", "question": "### Background\n\n**Research Question.** This problem deconstructs the econometric methodology used to classify firms as 'strategic' or 'confused' based on their VAT reporting behavior. The core challenge is to attribute responsibility for a reporting discrepancy observed between a buyer and a seller to each firm's intrinsic reporting tendency.\n\n**Setting and Sample.** The analysis uses monthly, transaction-level VAT data for firm-pairs in Uganda. The methodology, inspired by Abowd et al. (1999), is applied to the largest \"connected set\" of trading firms, where firms are linked directly or indirectly through their trading partners.\n\n---\n\n### Data / Model Specification\n\nThe methodology proceeds in two main steps:\n\n1.  **Define and Decompose Discrepancy:** The discrepancy between a buyer `f` and seller `f'` in month `t` is defined as the difference between the VAT reported by the buyer and the seller:\n    ```latex\n    d_{f f't} \\equiv Y_{f f't}^{B} - Y_{f f't}^{S} \\quad \\text{(Eq. (1))}\n    ```\n    A positive `d` implies a seller shortfall (liability-reducing), while a negative `d` implies a buyer shortfall (liability-increasing). This discrepancy is then decomposed using a two-way fixed-effects model:\n    ```latex\n    d_{f f't} = \\delta_{c} + \\delta_{f}^{b} + \\delta_{f'}^{s} + \\delta_{t} + r_{f f't} \\quad \\text{(Eq. (2))}\n    ```\n    where `\\delta_f^b` is a buyer fixed effect for firm `f`, `\\delta_{f'}^s` is a seller fixed effect for firm `f'`, and `\\delta_t` is a month fixed effect.\n\n2.  **Construct Firm-Level Classification Statistic:** The estimated fixed effects for a given firm `f` are aggregated into a single statistic, `Q_f`, which summarizes its overall misreporting behavior:\n    ```latex\n    Q_{f} \\equiv w_{s} \\cdot \\hat{\\delta}_{f}^{s} + w_{b} \\cdot \\hat{\\delta}_{f}^{b} \\quad \\text{(Eq. (3))}\n    ```\n    where `w_s` and `w_b` are the number of observations for firm `f` as a seller and buyer, respectively. A firm is classified as 'advantageous' if `Q_f > 0` and 'disadvantageous' if `Q_f < 0`.\n\n---\n\nBased on the model specification, what is the correct interpretation of an estimated buyer fixed effect `\\hat{\\delta}_f^b` that is significantly positive for a firm `f`?", "model_evaluation_result": {"model_raw_response": "Looking at this step by step:\n\n1. The discrepancy is defined as: d_{ff't} = Y_{ff't}^B - Y_{ff't}^S (buyer report minus seller report)\n\n2. The decomposition is: d_{ff't} = δ_c + δ_f^b + δ_{f'}^s + δ_t + r_{ff't}\n\n3. A positive buyer fixed effect δ_f^b means that when firm f acts as a buyer, it systematically contributes positively to discrepancies.\n\n4. Since d = (buyer report) - (seller report), a positive contribution from the buyer side means the buyer is reporting higher values than what sellers report for the same transactions.\n\n5. When a buyer over-reports its purchases, this increases the VAT input credits it can claim, which reduces its net VAT liability (since net VAT = output VAT - input VAT credits).\n\n6. This behavior is \"advantageous\" to the firm because it reduces tax liability, but \"disadvantageous\" to the tax authority because it reduces tax revenue.\n\nThe answer is A.\n\nA", "model_answer": "B", "error": null}}
{"ID": 252, "Question": "### Background\n\n**Research Question.** This problem investigates the causal factors behind lower cooperation rates in repeated games with stochastic horizons.\n\n**Setting / Institutional Environment.** In the experiment, subjects were organized into sessions of 24 participants. Within each session, subjects interacted in pairs for a series of 23 supergames. The analysis of cooperation rates pools data from all subjects across all supergames and sessions.\n\n### Question\n\nThe regression analysis of cooperation rates clusters standard errors at the session level. What is the primary statistical justification for this procedure in this experimental context?", "Options": {"A": "To correct for the fact that observations from subjects within the same session are not independent due to shared experiences.", "B": "To correct for heteroskedasticity, where the variance of cooperation rates might differ between treatment groups.", "C": "To mitigate bias in the estimated treatment effects caused by unobserved session-level factors like the time of day.", "D": "To account for the fact that subjects' behavior changes systematically over the 23 supergames due to learning."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item captures the methodological component of the original QA problem. It assesses knowledge of a core econometric concept (clustered standard errors) in an applied context. The correct answer identifies intra-class correlation as the key issue. Distractor B ('True but Irrelevant') points to heteroskedasticity, which is addressed by robust standard errors, not necessarily clustering. Distractor C ('Conceptual Opposite') confuses the purpose of clustering (correcting standard errors) with the problem of omitted variable bias (which affects coefficient estimates). Distractor D describes a dynamic that is modeled with covariates (like supergame number), not corrected by clustering.", "qid": "252", "question": "### Background\n\n**Research Question.** This problem investigates the causal factors behind lower cooperation rates in repeated games with stochastic horizons.\n\n**Setting / Institutional Environment.** In the experiment, subjects were organized into sessions of 24 participants. Within each session, subjects interacted in pairs for a series of 23 supergames. The analysis of cooperation rates pools data from all subjects across all supergames and sessions.\n\n### Question\n\nThe regression analysis of cooperation rates clusters standard errors at the session level. What is the primary statistical justification for this procedure in this experimental context?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 249, "Question": "### Background\n\nA central claim of the paper is that extensive scale (economic integration across regions, `N_c`) is fundamentally different from intensive scale (population growth within a region, `n`). This distinction arises from their asymmetric effects on the productivity of learning.\n\n### Data / Model Specification\n\nThe technology for accumulating human capital (`\\dot{h}`) is given by:\n```latex\n\\dot{h} = \\left(\\frac{M}{e_M n}\\right)^\\gamma h^{1-\\gamma} e_L \\quad \\text{(Eq. 1)}\n```\nwhere `M` is the degree of specialization. Specialization, in turn, depends on the total scale of the modern economy:\n```latex\nM = b_1 e_M h n N_c \\quad \\text{(Eq. 2)}\n```\nSubstituting Eq. (2) into Eq. (1) shows that the productivity of learning is positively affected by extensive scale (`N_c`) but is independent of intensive scale (`n`).\n\n### Question\n\nWhat is the core economic intuition for why intensive scale (`n`) has a neutral effect on learning productivity, while extensive scale (`N_c`) has a positive effect?", "Options": {"A": "An increase in `n` creates both a positive specialization effect and an offsetting negative congestion effect, while an increase in `N_c` increases specialization without adding to local congestion.", "B": "An increase in `N_c` raises wages more than an increase in `n` does, making the opportunity cost of not learning higher.", "C": "An increase in `n` only creates negative urban congestion effects, while an increase in `N_c` only creates positive specialization effects.", "D": "Economic integration (`N_c`) is assumed to be an exogenous policy variable, while population (`n`) is an endogenous outcome, which explains their different long-run impacts."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem targets sharp, specific concepts that are highly convertible (Conceptual Clarity = 9/10). The distinction between level and growth effects, and the mechanisms behind them, are classic sources of confusion, making it ideal for creating high-fidelity distractors (Discriminability = 10/10). This question targets the central mechanism of the paper's growth theory. It uses a Reverse-Reasoning strategy, asking for the intuition behind a given result. Distractors are designed to be plausible but incorrect explanations, including one that is Almost Right (B), one that is a Conceptual Opposite (C), and one that is True but Irrelevant (D).", "qid": "249", "question": "### Background\n\nA central claim of the paper is that extensive scale (economic integration across regions, `N_c`) is fundamentally different from intensive scale (population growth within a region, `n`). This distinction arises from their asymmetric effects on the productivity of learning.\n\n### Data / Model Specification\n\nThe technology for accumulating human capital (`\\dot{h}`) is given by:\n```latex\n\\dot{h} = \\left(\\frac{M}{e_M n}\\right)^\\gamma h^{1-\\gamma} e_L \\quad \\text{(Eq. 1)}\n```\nwhere `M` is the degree of specialization. Specialization, in turn, depends on the total scale of the modern economy:\n```latex\nM = b_1 e_M h n N_c \\quad \\text{(Eq. 2)}\n```\nSubstituting Eq. (2) into Eq. (1) shows that the productivity of learning is positively affected by extensive scale (`N_c`) but is independent of intensive scale (`n`).\n\n### Question\n\nWhat is the core economic intuition for why intensive scale (`n`) has a neutral effect on learning productivity, while extensive scale (`N_c`) has a positive effect?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 173, "Question": "### Background\n\n**Research Question.** This problem explores the economic interpretation of the core incentive compatibility (IC) constraint that governs whether cooperation can be sustained in a repeated game with imperfect monitoring.\n\n**Setting / Institutional Environment.** A long-run Player 1 (LR) interacts with a sequence of short-run Player 2s. Incentives are provided by a punishment mechanism where, upon observing certain signals, the LR player's continuation value reverts from the best equilibrium payoff `v*` to their minmax payoff, `u`.\n\n### Data / Model Specification\n\nThe incentive compatibility constraint can be rearranged to show that the immediate gain from deviation, `g`, must be less than or equal to the discounted expected future loss from punishment:\n\n```latex\ng \\leq \\frac{\\delta(q-p)}{1-\\delta(1-p)}(\\bar{u}-{u}) \\quad \\text{(Eq. (1))}\n```\n\nHere, `δ` is the discount factor, `p` is the on-path punishment probability, `q` is the off-path punishment probability, and `(ū-u)` is the per-period loss in payoff during punishment.\n\n### Question\n\nIn the expression for the discounted expected future loss in Eq. (1), which component represents the 'information content' of the signal, i.e., the marginal increase in the probability of punishment from deviating?", "Options": {"A": "`(\\bar{u}-{u})`", "B": "`\\delta / (1-\\delta(1-p))`", "C": "`(q-p)`", "D": "`g`"}, "Answer": "C", "pi_justification": "This item tests the economic interpretation of the paper's central incentive constraint. It uses a Conceptual Judgment strategy, asking for the identification of a specific economic concept within a complex formula. The distractors are other components of the same formula, each with a distinct economic meaning (capital loss, effective discount factor), making them highly plausible but incorrect (Almost Right). This forces a precise mapping between mathematical terms and economic concepts.", "qid": "173", "question": "### Background\n\n**Research Question.** This problem explores the economic interpretation of the core incentive compatibility (IC) constraint that governs whether cooperation can be sustained in a repeated game with imperfect monitoring.\n\n**Setting / Institutional Environment.** A long-run Player 1 (LR) interacts with a sequence of short-run Player 2s. Incentives are provided by a punishment mechanism where, upon observing certain signals, the LR player's continuation value reverts from the best equilibrium payoff `v*` to their minmax payoff, `u`.\n\n### Data / Model Specification\n\nThe incentive compatibility constraint can be rearranged to show that the immediate gain from deviation, `g`, must be less than or equal to the discounted expected future loss from punishment:\n\n```latex\ng \\leq \\frac{\\delta(q-p)}{1-\\delta(1-p)}(\\bar{u}-{u}) \\quad \\text{(Eq. (1))}\n```\n\nHere, `δ` is the discount factor, `p` is the on-path punishment probability, `q` is the off-path punishment probability, and `(ū-u)` is the per-period loss in payoff during punishment.\n\n### Question\n\nIn the expression for the discounted expected future loss in Eq. (1), which component represents the 'information content' of the signal, i.e., the marginal increase in the probability of punishment from deviating?", "model_evaluation_result": {"model_raw_response": "Looking at the incentive compatibility constraint in Eq. (1):\n\n```\ng ≤ δ(q-p)/(1-δ(1-p))(ū-u)\n```\n\nI need to identify which component represents the \"information content\" of the signal - specifically, the marginal increase in the probability of punishment from deviating.\n\nLet me analyze each component:\n\n- `g`: This is the immediate gain from deviation, not related to signal information content.\n\n- `(ū-u)`: This represents the per-period loss in payoff during punishment - the magnitude of the punishment, not the probability change.\n\n- `δ/(1-δ(1-p))`: This is a discounting factor that accounts for the present value of future punishments, but doesn't capture the probability change from deviating.\n\n- `(q-p)`: This represents the difference between:\n  - `q`: the probability of punishment when the player deviates (off-path)\n  - `p`: the probability of punishment when the player cooperates (on-path)\n\nThe term `(q-p)` directly captures how much more likely punishment becomes when a player deviates compared to when they cooperate. This is precisely the \"marginal increase in the probability of punishment from deviating\" - which is the definition of the information content of the signal in this monitoring context.\n\nC", "model_answer": "C", "error": null}}
{"ID": 152, "Question": "### Background\n\nAn analysis decomposes the change in pollution from US manufacturing between 1987 and 2001 into three effects: `scale` (overall economic growth), `composition` (the mix of industries), and `technique` (changes in production and abatement technology). The `technique` effect is calculated as a residual after accounting for the other two.\n\n### Data / Model Specification\n\n**Table 1: Scale, Composition, and Technique Effects for SO2, 1987-2001**\n\n*All figures are percentage changes relative to 1987 levels.*\n\n| Effect | Value |\n| :--- | :--- |\n| Scale effect (a) | 0.24 |\n| Scale, composition, and technique (observed total) (b) | -0.27 |\n| Scale and composition (predicted) (c) | 0.12 |\n| Fraction of cleanup due to technique [(c) - (b)]/[(a) - (b)] (d) | 0.77 |\n\n### Question\n\nBased on the data for SO2 in Table 1, which of the following statements is the most accurate calculation and interpretation?", "Options": {"A": "The composition effect alone reduced emissions by 12 percentage points, and the technique effect alone reduced them by 39 percentage points, with technique accounting for 77% of the total cleanup gap.", "B": "The composition effect reduced emissions by 12 percentage points, but the technique effect only reduced them by 15 percentage points because the observed total change was -0.27 and the predicted change from scale and composition was 0.12.", "C": "The composition effect increased emissions by 12 percentage points relative to the 1987 baseline, implying a shift towards dirtier industries.", "D": "The technique effect was a 39 percentage point reduction, but this represents only a minority of the total cleanup because the scale effect (24 percentage points) was so large."}, "Answer": "A", "pi_justification": "This item assesses the ability to perform multi-step calculations from a table and interpret the results in context. It uses a Computational Judgment strategy. Distractors are designed based on common arithmetic errors (sign error in B), misreading the table (confusing a combined effect with a marginal effect in C), and faulty reasoning about the final fraction's meaning (D).", "qid": "152", "question": "### Background\n\nAn analysis decomposes the change in pollution from US manufacturing between 1987 and 2001 into three effects: `scale` (overall economic growth), `composition` (the mix of industries), and `technique` (changes in production and abatement technology). The `technique` effect is calculated as a residual after accounting for the other two.\n\n### Data / Model Specification\n\n**Table 1: Scale, Composition, and Technique Effects for SO2, 1987-2001**\n\n*All figures are percentage changes relative to 1987 levels.*\n\n| Effect | Value |\n| :--- | :--- |\n| Scale effect (a) | 0.24 |\n| Scale, composition, and technique (observed total) (b) | -0.27 |\n| Scale and composition (predicted) (c) | 0.12 |\n| Fraction of cleanup due to technique [(c) - (b)]/[(a) - (b)] (d) | 0.77 |\n\n### Question\n\nBased on the data for SO2 in Table 1, which of the following statements is the most accurate calculation and interpretation?", "model_evaluation_result": {"model_raw_response": "Looking at this step by step:\n\nFirst, let me identify what each effect represents:\n- Scale effect (a) = 0.24 (24 percentage point increase due to economic growth)\n- Observed total (b) = -0.27 (27 percentage point decrease overall)\n- Scale + composition (c) = 0.12 (12 percentage point increase from both scale and composition)\n\nNow I can calculate the individual effects:\n\n**Composition effect alone:**\nScale + composition = 0.12\nScale alone = 0.24\nTherefore: Composition effect = 0.12 - 0.24 = -0.12 (12 percentage point reduction)\n\n**Technique effect:**\nTechnique effect = Observed total - (Scale + composition)\nTechnique effect = -0.27 - 0.12 = -0.39 (39 percentage point reduction)\n\n**Verification of the fraction:**\nTotal cleanup gap = Scale effect - Observed total = 0.24 - (-0.27) = 0.51\nTechnique contribution to cleanup = (Scale + composition) - Observed total = 0.12 - (-0.27) = 0.39\nFraction due to technique = 0.39/0.51 = 0.77 ✓\n\nNow checking each option:\n\nA) States composition effect reduced emissions by 12 pp, technique reduced by 39 pp, and technique accounts for 77% of cleanup gap. This matches my calculations exactly.\n\nB) Incorrectly calculates technique effect as 15 pp instead of 39 pp.\n\nC) Says composition increased emissions by 12 pp, but I calculated it decreased by 12 pp.\n\nD) Correctly identifies technique as 39 pp reduction but incorrectly claims it's a minority of cleanup.\n\nA", "model_answer": "A", "error": null}}
{"ID": 221, "Question": "### Background\n\n**Research Question.** This question tests the ability to apply a specific formula from a stochastic frontier analysis (SFA) model to determine the nature of product synergies in banking.\n\n**Setting / Institutional Environment.** The analysis uses a translog cost function to model the efficient cost frontier for U.S. banks. The parameters of this function reveal characteristics of the underlying production technology, such as economies of diversification (also known as scope economies).\n\n### Data / Model Specification\n\nThe model uses a translog functional form for the cost frontier, `ln c(y,w,z)`. Economies of diversification are measured by cost complementarities. A negative value of the cross-partial derivative `∂²c / ∂yᵢ∂yⱼ` indicates cost complementarity (economies of diversification). For the translog model, this is approximated at the sample mean by:\n\n```latex\n\\frac{\\partial^2 c}{\\partial y_i \\partial y_j} \\approx \\alpha_{i} \\alpha_{j} + \\alpha_{i j} \\quad \\text{(Eq. 1)}\n```\n\n**Table 1: Selected Parameter Estimates**\n\n| Parameter | Description | Estimate |\n| :--- | :--- | :--- |\n| `\\alpha_1` | Coeff on ln(demand deposits, `y₁`) | 0.316 |\n| `\\alpha_2` | Coeff on ln(time deposits, `y₂`) | 0.577 |\n| `\\alpha_{12}` | Coeff on ln(demand deposits) * ln(time deposits) | -0.047 |\n\n### Question\n\nUsing the provided model specification and parameter estimates, what is the nature of cost complementarity between demand deposits (`y₁`) and time deposits (`y₂`)?\n", "Options": {"A": "Diseconomies of diversification exist, because the complementarity measure is approximately +0.135.", "B": "Economies of diversification exist, because the complementarity measure is approximately +0.135.", "C": "Economies of diversification exist, because the complementarity measure is approximately -0.047.", "D": "Diseconomies of diversification exist, because the complementarity measure is approximately +0.846."}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a specific calculation based on a model's functional form and interpret the result in economic terms. It uses a Computational Judgment strategy. Distractors are designed to trap common errors: (B) Conceptual Opposite: correctly calculating the value but misinterpreting the sign's implication for economies vs. diseconomies; (C) Step-Omission Error: using only the interaction term `α₁₂` instead of the full formula; (D) Formula Misuse: adding `α₁` and `α₂` instead of multiplying them.", "qid": "221", "question": "### Background\n\n**Research Question.** This question tests the ability to apply a specific formula from a stochastic frontier analysis (SFA) model to determine the nature of product synergies in banking.\n\n**Setting / Institutional Environment.** The analysis uses a translog cost function to model the efficient cost frontier for U.S. banks. The parameters of this function reveal characteristics of the underlying production technology, such as economies of diversification (also known as scope economies).\n\n### Data / Model Specification\n\nThe model uses a translog functional form for the cost frontier, `ln c(y,w,z)`. Economies of diversification are measured by cost complementarities. A negative value of the cross-partial derivative `∂²c / ∂yᵢ∂yⱼ` indicates cost complementarity (economies of diversification). For the translog model, this is approximated at the sample mean by:\n\n```latex\n\\frac{\\partial^2 c}{\\partial y_i \\partial y_j} \\approx \\alpha_{i} \\alpha_{j} + \\alpha_{i j} \\quad \\text{(Eq. 1)}\n```\n\n**Table 1: Selected Parameter Estimates**\n\n| Parameter | Description | Estimate |\n| :--- | :--- | :--- |\n| `\\alpha_1` | Coeff on ln(demand deposits, `y₁`) | 0.316 |\n| `\\alpha_2` | Coeff on ln(time deposits, `y₂`) | 0.577 |\n| `\\alpha_{12}` | Coeff on ln(demand deposits) * ln(time deposits) | -0.047 |\n\n### Question\n\nUsing the provided model specification and parameter estimates, what is the nature of cost complementarity between demand deposits (`y₁`) and time deposits (`y₂`)?\n", "model_evaluation_result": {"model_raw_response": "I need to calculate the cross-partial derivative using the given formula and parameter estimates.\n\nGiven:\n- α₁ = 0.316\n- α₂ = 0.577  \n- α₁₂ = -0.047\n\nUsing Equation 1:\n∂²c / ∂y₁∂y₂ ≈ α₁ × α₂ + α₁₂\n\nCalculating:\n∂²c / ∂y₁∂y₂ ≈ (0.316) × (0.577) + (-0.047)\n∂²c / ∂y₁∂y₂ ≈ 0.182 + (-0.047)\n∂²c / ∂y₁∂y₂ ≈ 0.135\n\nSince the cross-partial derivative is positive (+0.135), this indicates diseconomies of diversification. A negative value would indicate economies of diversification (cost complementarity).\n\nA", "model_answer": "A", "error": null}}
{"ID": 127, "Question": "### Background\n\n**Research Question.** This problem evaluates the full policy impact of the post-1996 health insurance subsidies for the self-employed, considering not only the effect on private coverage but also interactions with public insurance.\n\n**Setting / Institutional Environment.** After estimating the price elasticity of demand for private insurance, the paper analyzes \"reverse crowd-out,\" where private subsidies induce people to leave public insurance. The net effect of the subsidy on the total number of uninsured depends on the relative size of the increase in private coverage versus the decrease in public coverage.\n\n### Data / Model Specification\n\nKey results from the paper's regression analysis are provided below.\n\n**Table 1: Estimated Marginal Effects for Children (Sample-Weighted TP Model)**\n| Dependent Variable | Marginal Effect of a 0.01 increase in TP |\n|:---|:---:|\n| Private Coverage | -0.561** |\n| Public Coverage | 0.620*** |\n\n*Note: TP is the Simple Tax Price of private insurance.*\n\n### Question\n\nGiven the estimated marginal effects for children in Table 1, what is the net change in the probability of a child being *uninsured* resulting from a 0.01 **decrease** in the Simple Tax Price (TP), and what does this imply about the subsidy's primary effect?", "Options": {"A": "A decrease of 1.181 percentage points, implying the subsidy is highly effective at reducing uninsurance.", "B": "A decrease of 0.059 percentage points, implying a small but positive effect on reducing uninsurance.", "C": "An increase of 0.059 percentage points, implying the subsidy's main effect is substitution from public to private coverage, with a negligible net impact on uninsurance.", "D": "An increase of 1.181 percentage points, implying the subsidy is counterproductive and significantly increases uninsurance."}, "Answer": "C", "pi_justification": "This item assesses the ability to synthesize multiple regression coefficients to calculate a net policy effect, a key concept from the source QA. It uses a Computational Judgment strategy. The correct answer requires calculating `ΔUninsured = -ΔPrivate - ΔPublic` and correctly handling the signs for a price *decrease*. Distractor A results from summing the absolute values of the coefficients. Distractor B results from a sign error in the calculation (`0.620 - 0.561`). Distractor D uses the wrong calculation (summing absolute values) and the wrong sign.", "qid": "127", "question": "### Background\n\n**Research Question.** This problem evaluates the full policy impact of the post-1996 health insurance subsidies for the self-employed, considering not only the effect on private coverage but also interactions with public insurance.\n\n**Setting / Institutional Environment.** After estimating the price elasticity of demand for private insurance, the paper analyzes \"reverse crowd-out,\" where private subsidies induce people to leave public insurance. The net effect of the subsidy on the total number of uninsured depends on the relative size of the increase in private coverage versus the decrease in public coverage.\n\n### Data / Model Specification\n\nKey results from the paper's regression analysis are provided below.\n\n**Table 1: Estimated Marginal Effects for Children (Sample-Weighted TP Model)**\n| Dependent Variable | Marginal Effect of a 0.01 increase in TP |\n|:---|:---:|\n| Private Coverage | -0.561** |\n| Public Coverage | 0.620*** |\n\n*Note: TP is the Simple Tax Price of private insurance.*\n\n### Question\n\nGiven the estimated marginal effects for children in Table 1, what is the net change in the probability of a child being *uninsured* resulting from a 0.01 **decrease** in the Simple Tax Price (TP), and what does this imply about the subsidy's primary effect?", "model_evaluation_result": {"model_raw_response": "I need to analyze the net effect on uninsurance from a 0.01 decrease in the Simple Tax Price (TP).\n\nGiven information:\n- Private Coverage marginal effect: -0.561** (for 0.01 increase in TP)\n- Public Coverage marginal effect: 0.620*** (for 0.01 increase in TP)\n\nSince we're looking at a 0.01 **decrease** in TP, I need to reverse the signs:\n- Effect on Private Coverage: +0.561 (increase)\n- Effect on Public Coverage: -0.620 (decrease)\n\nThe change in uninsurance equals the negative of the sum of changes in private and public coverage:\n\nChange in Uninsured = -(Change in Private + Change in Public)\nChange in Uninsured = -(0.561 + (-0.620))\nChange in Uninsured = -(0.561 - 0.620)\nChange in Uninsured = -(-0.059)\nChange in Uninsured = +0.059\n\nThis means uninsurance increases by 0.059 percentage points.\n\nThe interpretation: The subsidy decreases the tax price, which increases private coverage by 0.561 percentage points but decreases public coverage by 0.620 percentage points. The net effect is that more people leave public insurance than gain private insurance, resulting in a small increase in uninsurance of 0.059 percentage points.\n\nThis suggests the subsidy's main effect is substitution from public to private coverage rather than reducing overall uninsurance.\n\nC", "model_answer": "C", "error": null}}
{"ID": 187, "Question": "### Background\n\n**Research Question.** This problem investigates the role of household worklessness as a primary driver of poverty, focusing on how this relationship is exacerbated for children compared to the general population.\n\n**Setting / Institutional Environment.** The analysis uses data for Britain in 2000/1, defining poverty based on net household income after housing costs. It compares the situation of all individuals with the specific situation of children, categorized by their household's employment status.\n\n**Variables & Parameters.**\n- **Employment Circumstance:** Categories describing the work intensity within a household (e.g., 'Workless', 'Couple, one FT').\n- **Population Share:** The percentage of a given population (all individuals or children) that resides in a household of a specific employment type.\n- **Poverty Rate:** The percentage of individuals or children within a specific group whose household income falls below the poverty line.\n- **Unit of observation:** Individual or Child, based on their household's employment status.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Individual Poverty by Employment Circumstance, 2000/1**\n\n|                     | % individuals in each type | % of each type in poverty | % contribution to overall poverty |\n| :------------------ | :------------------------- | :------------------------ | :------------------------------- |\n| Workless            | 17.0                       | 64.4                      | 51.4                             |\n| ...                 | ...                        | ...                       | ...                              |\n| **Total**           | **100**                    | **21.3**                  | **100**                          |\n\n**Table 2: Child Poverty by Employment Circumstance, 2000/1**\n\n|                             | % children in each type | % children in poverty | % contribution to overall child poverty |\n| :-------------------------- | :---------------------- | :-------------------- | :-------------------------------------- |\n| Workless                    | 20.7                    | 77.4                  | 52.8                                    |\n| ...                         | ...                     | ...                   | ...                                     |\n| **Total**                   | **100**                 | **30.3**              | **100**                                 |\n\n*Note: Ellipses (...) denote omitted rows for brevity.*\n\n---\n\n### The Question\n\nSuppose a policy reduces the poverty rate *within* the 'Workless' household category by exactly 10 percentage points for all groups. Based on the data in Table 2, what would be the new overall poverty rate for children?", "Options": {"A": "28.23%", "B": "20.30%", "C": "28.60%", "D": "28.70%"}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a weighted-average calculation to evaluate a policy counterfactual, a key skill in quantitative analysis. It uses a 'Computational Judgment' strategy. The distractors are based on a matrix of high-frequency calculation errors: omitting the population weight (B), using the wrong population weight from the other table (C), and confusing percentage points with a percentage change (D).", "qid": "187", "question": "### Background\n\n**Research Question.** This problem investigates the role of household worklessness as a primary driver of poverty, focusing on how this relationship is exacerbated for children compared to the general population.\n\n**Setting / Institutional Environment.** The analysis uses data for Britain in 2000/1, defining poverty based on net household income after housing costs. It compares the situation of all individuals with the specific situation of children, categorized by their household's employment status.\n\n**Variables & Parameters.**\n- **Employment Circumstance:** Categories describing the work intensity within a household (e.g., 'Workless', 'Couple, one FT').\n- **Population Share:** The percentage of a given population (all individuals or children) that resides in a household of a specific employment type.\n- **Poverty Rate:** The percentage of individuals or children within a specific group whose household income falls below the poverty line.\n- **Unit of observation:** Individual or Child, based on their household's employment status.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Individual Poverty by Employment Circumstance, 2000/1**\n\n|                     | % individuals in each type | % of each type in poverty | % contribution to overall poverty |\n| :------------------ | :------------------------- | :------------------------ | :------------------------------- |\n| Workless            | 17.0                       | 64.4                      | 51.4                             |\n| ...                 | ...                        | ...                       | ...                              |\n| **Total**           | **100**                    | **21.3**                  | **100**                          |\n\n**Table 2: Child Poverty by Employment Circumstance, 2000/1**\n\n|                             | % children in each type | % children in poverty | % contribution to overall child poverty |\n| :-------------------------- | :---------------------- | :-------------------- | :-------------------------------------- |\n| Workless                    | 20.7                    | 77.4                  | 52.8                                    |\n| ...                         | ...                     | ...                   | ...                                     |\n| **Total**                   | **100**                 | **30.3**              | **100**                                 |\n\n*Note: Ellipses (...) denote omitted rows for brevity.*\n\n---\n\n### The Question\n\nSuppose a policy reduces the poverty rate *within* the 'Workless' household category by exactly 10 percentage points for all groups. Based on the data in Table 2, what would be the new overall poverty rate for children?", "model_evaluation_result": {"model_raw_response": "I need to calculate the new overall poverty rate for children after a 10 percentage point reduction in poverty rate within the 'Workless' household category.\n\nFrom Table 2, I have:\n- % children in Workless households: 20.7%\n- Current poverty rate for children in Workless households: 77.4%\n- Current overall child poverty rate: 30.3%\n\nFirst, let me understand what the 10 percentage point reduction means:\n- New poverty rate for children in Workless households = 77.4% - 10 = 67.4%\n\nTo calculate the new overall poverty rate, I need to determine how this change affects the total.\n\nThe contribution of Workless households to overall child poverty is currently 52.8% of all child poverty. This means:\n- Current contribution in absolute terms = 20.7% × 77.4% = 16.0218%\n\nAfter the policy change:\n- New contribution from Workless households = 20.7% × 67.4% = 13.9518%\n\nThe reduction in overall poverty rate from this change:\n- Reduction = 16.0218% - 13.9518% = 2.07 percentage points\n\nTherefore, the new overall child poverty rate would be:\n- New rate = 30.3% - 2.07% = 28.23%\n\nA", "model_answer": "A", "error": null}}
{"ID": 175, "Question": "## Background\n\n**Research Question.** This problem critically examines the widespread practice of first-differencing time series data to induce stationarity, highlighting the dangers of \"over-differencing.\"\n\n**Setting.** An analyst is modeling a relationship between two time series variables, \\(y_t\\) and \\(x_t\\). The true underlying relationship is a simple static model in levels, but the analyst, fearing a spurious regression, mechanically first-differences the data before estimation.\n\n---\n\n## Data / Model Specification\n\nThe true data generating process is the static levels model:\n```latex\ny_{t}=\\gamma_{0}x_{t}+\\epsilon_{t} \\quad \\text{(Eq. (1))}\n```\nwhere \\(\\epsilon_t\\) is a white noise error term with variance \\(\\sigma^2\\).\n\nThe analyst estimates the first-difference model:\n```latex\n\\Delta y_{t}=\\gamma_{0}\\Delta x_{t}+\\omega_{t} \\quad \\text{(Eq. (2))}\n```\n\n---\n\n## Question\n\nGiven that the true model is Eq. (1), what are the statistical properties of the error term, \\(\\omega_t\\), in the analyst's estimated model, Eq. (2)?", "Options": {"A": "It will be a first-order moving average process with an autocorrelation coefficient of -0.5.", "B": "It will be a first-order autoregressive process with a root near 1.", "C": "It will remain white noise, as the original error term was white noise.", "D": "It will be a first-order moving average process with an autocorrelation coefficient of +0.5."}, "Answer": "A", "pi_justification": "This item assesses a critical concept: the statistical consequences of 'over-differencing.' It uses a Scenario Application strategy. The distractors are designed to capture common errors: (B) confuses the problem with the solution, (C) ignores the transformation's effect on the error term, and (D) represents a common sign error in the calculation.", "qid": "175", "question": "## Background\n\n**Research Question.** This problem critically examines the widespread practice of first-differencing time series data to induce stationarity, highlighting the dangers of \"over-differencing.\"\n\n**Setting.** An analyst is modeling a relationship between two time series variables, \\(y_t\\) and \\(x_t\\). The true underlying relationship is a simple static model in levels, but the analyst, fearing a spurious regression, mechanically first-differences the data before estimation.\n\n---\n\n## Data / Model Specification\n\nThe true data generating process is the static levels model:\n```latex\ny_{t}=\\gamma_{0}x_{t}+\\epsilon_{t} \\quad \\text{(Eq. (1))}\n```\nwhere \\(\\epsilon_t\\) is a white noise error term with variance \\(\\sigma^2\\).\n\nThe analyst estimates the first-difference model:\n```latex\n\\Delta y_{t}=\\gamma_{0}\\Delta x_{t}+\\omega_{t} \\quad \\text{(Eq. (2))}\n```\n\n---\n\n## Question\n\nGiven that the true model is Eq. (1), what are the statistical properties of the error term, \\(\\omega_t\\), in the analyst's estimated model, Eq. (2)?", "model_evaluation_result": {"model_raw_response": "I need to analyze what happens to the error term when the analyst incorrectly first-differences a levels model.\n\nGiven:\n- True model: $y_t = \\gamma_0 x_t + \\epsilon_t$ where $\\epsilon_t$ is white noise\n- Analyst estimates: $\\Delta y_t = \\gamma_0 \\Delta x_t + \\omega_t$\n\nLet me derive what $\\omega_t$ actually is.\n\nFrom the true model:\n$y_t = \\gamma_0 x_t + \\epsilon_t$\n$y_{t-1} = \\gamma_0 x_{t-1} + \\epsilon_{t-1}$\n\nTaking first differences:\n$\\Delta y_t = y_t - y_{t-1} = \\gamma_0(x_t - x_{t-1}) + (\\epsilon_t - \\epsilon_{t-1})$\n$\\Delta y_t = \\gamma_0 \\Delta x_t + (\\epsilon_t - \\epsilon_{t-1})$\n\nComparing with the analyst's model $\\Delta y_t = \\gamma_0 \\Delta x_t + \\omega_t$:\n\n$\\omega_t = \\epsilon_t - \\epsilon_{t-1}$\n\nNow I need to determine the properties of $\\omega_t = \\epsilon_t - \\epsilon_{t-1}$ where $\\epsilon_t$ is white noise.\n\nSince $\\epsilon_t$ is white noise with variance $\\sigma^2$:\n- $E[\\epsilon_t] = 0$\n- $Var[\\epsilon_t] = \\sigma^2$\n- $Cov[\\epsilon_t, \\epsilon_s] = 0$ for $t \\neq s$\n\nFor $\\omega_t = \\epsilon_t - \\epsilon_{t-1}$:\n\n1) $E[\\omega_t] = E[\\epsilon_t - \\epsilon_{t-1}] = 0$\n\n2) $Var[\\omega_t] = Var[\\epsilon_t - \\epsilon_{t-1}] = Var[\\epsilon_t] + Var[\\epsilon_{t-1}] = ", "model_answer": "I need to analyze what happens to the error term when the analyst incorrectly first-differences a levels model.\n\nGiven:\n- True model: $y_t = \\gamma_0 x_t + \\epsilon_t$ where $\\epsilon_t$ is white noise\n- Analyst estimates: $\\Delta y_t = \\gamma_0 \\Delta x_t + \\omega_t$\n\nLet me derive what $\\omega_t$ actually is.\n\nFrom the true model:\n$y_t = \\gamma_0 x_t + \\epsilon_t$\n$y_{t-1} = \\gamma_0 x_{t-1} + \\epsilon_{t-1}$\n\nTaking first differences:\n$\\Delta y_t = y_t - y_{t-1} = \\gamma_0(x_t - x_{t-1}) + (\\epsilon_t - \\epsilon_{t-1})$\n$\\Delta y_t = \\gamma_0 \\Delta x_t + (\\epsilon_t - \\epsilon_{t-1})$\n\nComparing with the analyst's model $\\Delta y_t = \\gamma_0 \\Delta x_t + \\omega_t$:\n\n$\\omega_t = \\epsilon_t - \\epsilon_{t-1}$\n\nNow I need to determine the properties of $\\omega_t = \\epsilon_t - \\epsilon_{t-1}$ where $\\epsilon_t$ is white noise.\n\nSince $\\epsilon_t$ is white noise with variance $\\sigma^2$:\n- $E[\\epsilon_t] = 0$\n- $Var[\\epsilon_t] = \\sigma^2$\n- $Cov[\\epsilon_t, \\epsilon_s] = 0$ for $t \\neq s$\n\nFor $\\omega_t = \\epsilon_t - \\epsilon_{t-1}$:\n\n1) $E[\\omega_t] = E[\\epsilon_t - \\epsilon_{t-1}] = 0$\n\n2) $Var[\\omega_t] = Var[\\epsilon_t - \\epsilon_{t-1}] = Var[\\epsilon_t] + Var[\\epsilon_{t-1}] =", "error": "Warning: Could not extract single choice letter from: I need to analyze what happens to the error term w..."}}
{"ID": 174, "Question": "### Background\n\n**Research Question.** This problem tests a key algebraic step in deriving the incentive compatibility (IC) constraint from the underlying value equations in a repeated game with imperfect monitoring.\n\n**Setting / Institutional Environment.** A long-run Player 1 (LR) interacts with a sequence of short-run Player 2s. Incentives are provided by a punishment mechanism where the LR player's continuation value `w(z)` can be either the best equilibrium payoff `v*` (reward) or the minmax payoff `u` (punishment).\n\n### Data / Model Specification\n\nThe on-path payoff `v*` must satisfy the promise-keeping constraint:\n\n```latex\nv^* = (1-\\delta)\\bar{u} + \\delta E[w(z)|+1] \\quad \\text{(Eq. (1))}\n```\n\nLet `p` be the probability of punishment after the cooperative action `+1`. The expected continuation payoff is therefore `E[w(z)|+1] = (1-p)v* + pu`.\n\n### Question\n\nA key step in deriving the main incentive constraint is to solve the promise-keeping equation (Eq. (1)) for the 'capital loss' from punishment, `(v* - u)`. What is the correct expression for `(v* - u)`?", "Options": {"A": "`(1-\\delta)(\\bar{u}-{u}) / (1 - \\delta + \\delta p)`", "B": "`\\delta(q-p)(\\bar{u}-{u}) / (1-\\delta)`", "C": "`(1-\\delta)(\\bar{u}-{u}) / (1 - \\delta)`", "D": "`(1-\\delta)g / (\\delta(q-p))`"}, "Answer": "A", "pi_justification": "This item assesses a critical step in the derivation of the main incentive constraint, testing algebraic manipulation without requiring the full open-ended derivation. It uses a Computational Judgment strategy. The distractors are designed based on common algebraic errors (Step-Omission Error in C) or by using terms from other related formulas to create confusion (Formula Misuse in B and D), ensuring that only a correct derivation yields the right answer.", "qid": "174", "question": "### Background\n\n**Research Question.** This problem tests a key algebraic step in deriving the incentive compatibility (IC) constraint from the underlying value equations in a repeated game with imperfect monitoring.\n\n**Setting / Institutional Environment.** A long-run Player 1 (LR) interacts with a sequence of short-run Player 2s. Incentives are provided by a punishment mechanism where the LR player's continuation value `w(z)` can be either the best equilibrium payoff `v*` (reward) or the minmax payoff `u` (punishment).\n\n### Data / Model Specification\n\nThe on-path payoff `v*` must satisfy the promise-keeping constraint:\n\n```latex\nv^* = (1-\\delta)\\bar{u} + \\delta E[w(z)|+1] \\quad \\text{(Eq. (1))}\n```\n\nLet `p` be the probability of punishment after the cooperative action `+1`. The expected continuation payoff is therefore `E[w(z)|+1] = (1-p)v* + pu`.\n\n### Question\n\nA key step in deriving the main incentive constraint is to solve the promise-keeping equation (Eq. (1)) for the 'capital loss' from punishment, `(v* - u)`. What is the correct expression for `(v* - u)`?", "model_evaluation_result": {"model_raw_response": "I need to solve the promise-keeping constraint for (v* - u).\n\nGiven:\n- v* = (1-δ)ū + δE[w(z)|+1]\n- E[w(z)|+1] = (1-p)v* + pu\n- p is the probability of punishment after cooperative action +1\n\nLet me substitute the expected continuation payoff into the promise-keeping constraint:\n\nv* = (1-δ)ū + δ[(1-p)v* + pu]\n\nExpanding:\nv* = (1-δ)ū + δ(1-p)v* + δpu\n\nRearranging to collect v* terms:\nv* - δ(1-p)v* = (1-δ)ū + δpu\n\nFactoring out v*:\nv*[1 - δ(1-p)] = (1-δ)ū + δpu\n\nSimplifying the coefficient of v*:\nv*[1 - δ + δp] = (1-δ)ū + δpu\n\nSolving for v*:\nv* = [(1-δ)ū + δpu] / [1 - δ + δp]\n\nNow I need to find (v* - u):\nv* - u = [(1-δ)ū + δpu] / [1 - δ + δp] - u\n\nTo subtract u, I need a common denominator:\nv* - u = [(1-δ)ū + δpu - u(1 - δ + δp)] / [1 - δ + δp]\n\nExpanding the numerator:\nv* - u = [(1-δ)ū + δpu - u + δu - δpu] / [1 - δ + δp]\n\nSimplifying (δpu cancels):\nv* - u = [(1-δ)ū - u + δu] / [1 - δ + δp]\n\nFactoring:\nv* - u = [(1-δ)ū - u(1-δ)] / [1 - δ + δp]\nv* - u", "model_answer": "I need to solve the promise-keeping constraint for (v* - u).\n\nGiven:\n- v* = (1-δ)ū + δE[w(z)|+1]\n- E[w(z)|+1] = (1-p)v* + pu\n- p is the probability of punishment after cooperative action +1\n\nLet me substitute the expected continuation payoff into the promise-keeping constraint:\n\nv* = (1-δ)ū + δ[(1-p)v* + pu]\n\nExpanding:\nv* = (1-δ)ū + δ(1-p)v* + δpu\n\nRearranging to collect v* terms:\nv* - δ(1-p)v* = (1-δ)ū + δpu\n\nFactoring out v*:\nv*[1 - δ(1-p)] = (1-δ)ū + δpu\n\nSimplifying the coefficient of v*:\nv*[1 - δ + δp] = (1-δ)ū + δpu\n\nSolving for v*:\nv* = [(1-δ)ū + δpu] / [1 - δ + δp]\n\nNow I need to find (v* - u):\nv* - u = [(1-δ)ū + δpu] / [1 - δ + δp] - u\n\nTo subtract u, I need a common denominator:\nv* - u = [(1-δ)ū + δpu - u(1 - δ + δp)] / [1 - δ + δp]\n\nExpanding the numerator:\nv* - u = [(1-δ)ū + δpu - u + δu - δpu] / [1 - δ + δp]\n\nSimplifying (δpu cancels):\nv* - u = [(1-δ)ū - u + δu] / [1 - δ + δp]\n\nFactoring:\nv* - u = [(1-δ)ū - u(1-δ)] / [1 - δ + δp]\nv* - u", "error": "Warning: Could not extract single choice letter from: I need to solve the promise-keeping constraint for..."}}
{"ID": 219, "Question": "### Background\n\n**Research Question.** This problem assesses the empirical evidence for the hypothesis that the quality of a country's judiciary mediates the relationship between economic unpredictability and firm growth, using a fixed-effects OLS model.\n\n**Setting / Institutional Environment.** The analysis uses a country fixed-effects model on a cross-section of firms from the World Business Environment Survey (WBES, 1999-2000). This approach controls for all time-invariant country-level characteristics.\n\n**Variables & Parameters.**\n*   `FirmGrowth`: The percentage change in a firm's sales over the preceding 3 years.\n*   `EconomicUnpred`: A firm's perception of economic unpredictability, on a scale from 1 (completely predictable) to 6 (completely unpredictable).\n*   `Judiciary`: A firm's perception of how problematic the judiciary is for its business, on a scale from 1 (no obstacle) to 4 (major obstacle).\n\n---\n\n### Data / Model Specification\n\nThe estimated country fixed-effects model is:\n\n```latex\nFirmGrowth_{ij} = \\alpha_i + ... + \\gamma_8 Judiciary_{j} + \\gamma_9 EconomicUnpred_{ij} + \\gamma_{10} (EconomicUnpred_{ij} \\times Judiciary_j) + u_{ij} \n```\n\nwhere `i` is the country index and `j` is the firm index. `α_i` represents the country fixed effects.\n\n**Table 1. Volatility and Firm Growth (Selected Results from Table 4, Column 2)**\n\n| Variable                      | Coefficient (Std. Err.) |\n| :---------------------------- | :---------------------- |\n| `Judiciary`                   | 0.029 (0.023)           |\n| `EconomicUnpred`              | 0.014 (0.012)           |\n| `EconomicUnpred` x `Judiciary`| -0.012*** (0.004)       |\n| *Overall effect: EconUnpred*  | *-0.019** (0.006)*      |\n| Observations                  | 4325                    |\n\n*Notes: The dependent variable is `FirmGrowth`. Model includes country fixed effects and other firm-level controls. *** p<0.01, ** p<0.05.*\n\n---\n\nUsing the results in Table 1, what is the estimated marginal effect of a one-unit increase in `EconomicUnpred` on `FirmGrowth` for a firm that reports the judiciary is a 'major obstacle' (`Judiciary` = 4)?", "Options": {"A": "A 3.4 percentage point decrease in the 3-year sales growth rate.", "B": "A 3.4 percentage point increase in the 3-year sales growth rate.", "C": "A 1.2 percentage point decrease in the 3-year sales growth rate.", "D": "A 1.9 percentage point decrease in the 3-year sales growth rate."}, "Answer": "A", "pi_justification": "This 'Computational Judgment' item assesses the core skill of calculating and interpreting a marginal effect from an interaction model, which was the focus of the original QA's first two parts. The distractors are designed based on high-frequency errors: B (sign error in calculation), C (omitting the main effect from the marginal effect formula), and D (misinterpreting a different reported statistic from the table as the relevant one).", "qid": "219", "question": "### Background\n\n**Research Question.** This problem assesses the empirical evidence for the hypothesis that the quality of a country's judiciary mediates the relationship between economic unpredictability and firm growth, using a fixed-effects OLS model.\n\n**Setting / Institutional Environment.** The analysis uses a country fixed-effects model on a cross-section of firms from the World Business Environment Survey (WBES, 1999-2000). This approach controls for all time-invariant country-level characteristics.\n\n**Variables & Parameters.**\n*   `FirmGrowth`: The percentage change in a firm's sales over the preceding 3 years.\n*   `EconomicUnpred`: A firm's perception of economic unpredictability, on a scale from 1 (completely predictable) to 6 (completely unpredictable).\n*   `Judiciary`: A firm's perception of how problematic the judiciary is for its business, on a scale from 1 (no obstacle) to 4 (major obstacle).\n\n---\n\n### Data / Model Specification\n\nThe estimated country fixed-effects model is:\n\n```latex\nFirmGrowth_{ij} = \\alpha_i + ... + \\gamma_8 Judiciary_{j} + \\gamma_9 EconomicUnpred_{ij} + \\gamma_{10} (EconomicUnpred_{ij} \\times Judiciary_j) + u_{ij} \n```\n\nwhere `i` is the country index and `j` is the firm index. `α_i` represents the country fixed effects.\n\n**Table 1. Volatility and Firm Growth (Selected Results from Table 4, Column 2)**\n\n| Variable                      | Coefficient (Std. Err.) |\n| :---------------------------- | :---------------------- |\n| `Judiciary`                   | 0.029 (0.023)           |\n| `EconomicUnpred`              | 0.014 (0.012)           |\n| `EconomicUnpred` x `Judiciary`| -0.012*** (0.004)       |\n| *Overall effect: EconUnpred*  | *-0.019** (0.006)*      |\n| Observations                  | 4325                    |\n\n*Notes: The dependent variable is `FirmGrowth`. Model includes country fixed effects and other firm-level controls. *** p<0.01, ** p<0.05.*\n\n---\n\nUsing the results in Table 1, what is the estimated marginal effect of a one-unit increase in `EconomicUnpred` on `FirmGrowth` for a firm that reports the judiciary is a 'major obstacle' (`Judiciary` = 4)?", "model_evaluation_result": {"model_raw_response": "I need to calculate the marginal effect of a one-unit increase in `EconomicUnpred` on `FirmGrowth` when `Judiciary` = 4.\n\nFrom the regression model:\n```\nFirmGrowth = α + ... + γ₈ Judiciary + γ₉ EconomicUnpred + γ₁₀ (EconomicUnpred × Judiciary) + u\n```\n\nThe marginal effect of `EconomicUnpred` on `FirmGrowth` is:\n```\n∂FirmGrowth/∂EconomicUnpred = γ₉ + γ₁₀ × Judiciary\n```\n\nFrom Table 1:\n- γ₉ (coefficient on `EconomicUnpred`) = 0.014\n- γ₁₀ (coefficient on `EconomicUnpred × Judiciary`) = -0.012\n- `Judiciary` = 4 (major obstacle)\n\nCalculating the marginal effect:\n```\nMarginal effect = 0.014 + (-0.012) × 4\n                = 0.014 - 0.048\n                = -0.034\n```\n\nThis means a one-unit increase in `EconomicUnpred` leads to a 3.4 percentage point decrease in the 3-year sales growth rate when the judiciary is perceived as a major obstacle.\n\nA", "model_answer": "A", "error": null}}
{"ID": 183, "Question": "### Background\n\nThis problem analyzes the baseline monetary model without credit, which serves as a benchmark for the paper's main analysis. In this simplified economy, agents are either producers or money holders. They meet randomly, and a trade occurs when a money holder meets a producer of their desired consumption good.\n\n### Data / Model Specification\n\nThe economy is characterized by:\n*   `V_m`, `V_p`: Stationary value functions for a money holder and a producer.\n*   `r`: Rate of time preference.\n*   `β`: Poisson arrival rate of trading partners.\n*   `M`: Proportion of money holders in the population, `M` ∈ (0, 1).\n*   `z`: Probability of a single coincidence of wants.\n*   `u(q)`: A strictly increasing and strictly concave utility function with `u(0)=0`, `u'(0)=∞`, and `u'(∞)=0`.\n\nThe equilibrium is described by the following Bellman equations and bargaining outcome:\n\n**Bellman Equations:**\n```latex\nrV_m = \\beta(1-M)z(V_p - V_m + u(q_m)) \\quad \\quad \\text{(Eq. 1)}\n```\n```latex\nrV_p = \\beta M z(V_m - V_p - q_m) \\quad \\quad \\text{(Eq. 2)}\n```\n**Bargaining Outcome (Take-it-or-leave-it offer by money holder):**\n```latex\nq_m = V_m - V_p \\quad \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nBased on the model specification, what is the stationary equilibrium value of being a producer, `V_p`?", "Options": {"A": "`V_p = 0`", "B": "`V_p = r / (βMz)`", "C": "`V_p = -q_m`", "D": "`V_p` is positive but depends on the utility function `u(q)`."}, "Answer": "A", "pi_justification": "This item assesses the foundational result of the no-credit model using a Computational Judgment strategy. The correct answer requires substituting the bargaining outcome (Eq. 3) into the producer's Bellman equation (Eq. 2), a key deductive step. Distractors represent common conceptual errors: B misapplies the Bellman equation, C confuses value with surplus, and D reflects an incorrect intuition about producer welfare in this specific bargaining environment.", "qid": "183", "question": "### Background\n\nThis problem analyzes the baseline monetary model without credit, which serves as a benchmark for the paper's main analysis. In this simplified economy, agents are either producers or money holders. They meet randomly, and a trade occurs when a money holder meets a producer of their desired consumption good.\n\n### Data / Model Specification\n\nThe economy is characterized by:\n*   `V_m`, `V_p`: Stationary value functions for a money holder and a producer.\n*   `r`: Rate of time preference.\n*   `β`: Poisson arrival rate of trading partners.\n*   `M`: Proportion of money holders in the population, `M` ∈ (0, 1).\n*   `z`: Probability of a single coincidence of wants.\n*   `u(q)`: A strictly increasing and strictly concave utility function with `u(0)=0`, `u'(0)=∞`, and `u'(∞)=0`.\n\nThe equilibrium is described by the following Bellman equations and bargaining outcome:\n\n**Bellman Equations:**\n```latex\nrV_m = \\beta(1-M)z(V_p - V_m + u(q_m)) \\quad \\quad \\text{(Eq. 1)}\n```\n```latex\nrV_p = \\beta M z(V_m - V_p - q_m) \\quad \\quad \\text{(Eq. 2)}\n```\n**Bargaining Outcome (Take-it-or-leave-it offer by money holder):**\n```latex\nq_m = V_m - V_p \\quad \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nBased on the model specification, what is the stationary equilibrium value of being a producer, `V_p`?", "model_evaluation_result": {"model_raw_response": "I need to solve for the stationary equilibrium value of being a producer, V_p, using the given Bellman equations and bargaining outcome.\n\nGiven equations:\n- (Eq. 1): rV_m = β(1-M)z(V_p - V_m + u(q_m))\n- (Eq. 2): rV_p = βMz(V_m - V_p - q_m)\n- (Eq. 3): q_m = V_m - V_p\n\nLet me substitute equation (3) into equations (1) and (2):\n\nFrom (Eq. 3): V_m = V_p + q_m\n\nSubstituting into (Eq. 1):\nr(V_p + q_m) = β(1-M)z(V_p - (V_p + q_m) + u(q_m))\nr(V_p + q_m) = β(1-M)z(-q_m + u(q_m))\n\nSubstituting into (Eq. 2):\nrV_p = βMz((V_p + q_m) - V_p - q_m)\nrV_p = βMz(0)\nrV_p = 0\n\nTherefore: V_p = 0\n\nLet me verify this makes sense:\n- If V_p = 0, then from (Eq. 3): q_m = V_m\n- From (Eq. 2): r(0) = βMz(V_m - 0 - q_m) = βMz(q_m - q_m) = 0 ✓\n- This is consistent\n\nThe economic intuition is that in this take-it-or-leave-it bargaining setup where the money holder makes the offer, the producer receives no surplus - all the gains from trade go to the money holder.\n\nA", "model_answer": "A", "error": null}}
{"ID": 159, "Question": "### Background\n\n**Setting.** The paper's analysis is motivated by the Milgrom-Weber revenue ranking theorem, which states that under conditions like bidder symmetry and affiliated values, expected revenues are ranked `R_E ≥ R_S ≥ R_F` (English ≥ Second-Price ≥ First-Price).\n\n### Question\n\nThe Milgrom-Weber theorem predicts that second-price auctions yield at least as much revenue as first-price auctions (`R_S ≥ R_F`). However, this can be reversed in the presence of a single, known 'strong' bidder competing against several 'weak' bidders. In such an asymmetric setting, why might a first-price auction yield higher revenue (`R_F > R_S`)?", "Options": {"A": "In a second-price auction, the strong bidder wins by paying the low bid of a weak competitor; in a first-price auction, strategic uncertainty forces the strong bidder to place a higher winning bid.", "B": "In a first-price auction, weak bidders collude to drive up the price paid by the strong bidder.", "C": "In a second-price auction, the strong bidder must bid their true value, which is high; in a first-price auction, they can shade their bid significantly lower.", "D": "Weak bidders are more likely to drop out of a first-price auction, reducing competition and lowering revenue compared to a second-price auction."}, "Answer": "A", "pi_justification": "This item assesses a deep theoretical concept from auction theory that is relevant to the paper's context. It uses a Scenario Application strategy. The correct answer (A) accurately describes the strategic logic. Distractor B introduces collusion, which is outside the non-cooperative model being discussed. Distractor C (Conceptual Opposite) incorrectly describes bidding strategies in both formats. Distractor D makes an incorrect claim about entry and revenue effects.", "qid": "159", "question": "### Background\n\n**Setting.** The paper's analysis is motivated by the Milgrom-Weber revenue ranking theorem, which states that under conditions like bidder symmetry and affiliated values, expected revenues are ranked `R_E ≥ R_S ≥ R_F` (English ≥ Second-Price ≥ First-Price).\n\n### Question\n\nThe Milgrom-Weber theorem predicts that second-price auctions yield at least as much revenue as first-price auctions (`R_S ≥ R_F`). However, this can be reversed in the presence of a single, known 'strong' bidder competing against several 'weak' bidders. In such an asymmetric setting, why might a first-price auction yield higher revenue (`R_F > R_S`)?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze why first-price auctions might yield higher revenue than second-price auctions when there's bidder asymmetry (one strong bidder vs. multiple weak bidders).\n\nLet me work through the logic:\n\nIn a **second-price auction** with one strong and multiple weak bidders:\n- The strong bidder wins by paying the second-highest bid\n- Since weak bidders have lower values, their bids will be relatively low\n- The strong bidder pays only the highest weak bidder's bid, which is low\n- Revenue = second-highest bid (from a weak bidder) = low\n\nIn a **first-price auction** with the same setup:\n- The strong bidder knows they're likely to win but faces strategic uncertainty about exactly how much to bid\n- They can't just bid slightly above the weak bidders' values because they don't know those values precisely\n- To ensure winning, they must bid higher than they would pay in a second-price auction\n- This strategic uncertainty and the need to secure victory leads to higher bids\n- Revenue = winning bid from strong bidder = higher than in second-price case\n\nOption A captures this logic perfectly: \"In a second-price auction, the strong bidder wins by paying the low bid of a weak competitor; in a first-price auction, strategic uncertainty forces the strong bidder to place a higher winning bid.\"\n\nOptions B, C, and D either describe implausible scenarios (collusion) or get the mechanisms wrong.\n\nA", "model_answer": "A", "error": null}}
{"ID": 266, "Question": "### Background\n\nThe paper's benchmark model uses a Cobb-Douglas production function for investment goods, which implies a unit elasticity of substitution between domestic and imported inputs. The robustness of this assumption is tested via sensitivity analysis.\n\n### Data / Model Specification\n\n**Table 1: Sensitivity Analysis - Change in RPI, `q` (%)**\n\n| | Data | Model (CES Elasticity of Substitution) |\n|---|---|---| \n| | | 0.80 | 1.33 | Cobb-Douglas (1.0) |\n| Before reform | 44 | 37 | 13 | 23 |\n| During reform | -26 | -46 | -19 | -31 |\n\n### Question\n\nThe sensitivity analysis in Table 1 explores how the elasticity of substitution between domestic and imported inputs affects the model's results. What is the key conclusion from this analysis?", "Options": {"A": "A higher elasticity of substitution (e.g., 1.33) amplifies the RPI response, making trade distortions more impactful.", "B": "A lower elasticity of substitution (e.g., 0.80) amplifies the RPI response, suggesting that strong complementarity between inputs makes trade distortions more impactful.", "C": "The elasticity of substitution has a negligible effect on the model's predictions for the pre-reform period but a large effect during the reform period.", "D": "The Cobb-Douglas specification (elasticity = 1.0) matches the data most closely, confirming it as the most plausible assumption."}, "Answer": "B", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to interpret sensitivity analysis and understand the underlying economic intuition. This is a conceptual judgment item. Distractors represent common conceptual errors: A reverses the relationship, C misreads the table's symmetry, and D makes a false claim about which model fits best (the lower elasticity model is closer to the pre-reform data).", "qid": "266", "question": "### Background\n\nThe paper's benchmark model uses a Cobb-Douglas production function for investment goods, which implies a unit elasticity of substitution between domestic and imported inputs. The robustness of this assumption is tested via sensitivity analysis.\n\n### Data / Model Specification\n\n**Table 1: Sensitivity Analysis - Change in RPI, `q` (%)**\n\n| | Data | Model (CES Elasticity of Substitution) |\n|---|---|---| \n| | | 0.80 | 1.33 | Cobb-Douglas (1.0) |\n| Before reform | 44 | 37 | 13 | 23 |\n| During reform | -26 | -46 | -19 | -31 |\n\n### Question\n\nThe sensitivity analysis in Table 1 explores how the elasticity of substitution between domestic and imported inputs affects the model's results. What is the key conclusion from this analysis?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 166, "Question": "### Background\n\n**Research Question.** This problem explores the interpretation of the core structural model of the coffee market, focusing on its long-run equilibrium.\n\n**Setting / Institutional Environment.** The model represents the international coffee market with three core components: market clearing for green coffee beans, a biological growth process for coffee trees with a one-year depreciation cycle, and roaster demand.\n\n**Variables & Parameters.**\n- `p^W`: Steady-state international wholesale price of coffee.\n- `Y_t`: Quantity of green coffee beans supplied at time `t`.\n- `Q_t`: Quantity demanded by roasters at time `t`.\n- `\\Delta I_t`: Change in inventory holdings at time `t`.\n- `N_t`: Number of coffee trees at time `t`.\n- `b`: Steady-state average yield per tree.\n- `\\varphi`: Steady-state number of new trees entering production.\n- `\\beta`: Annual survival rate of tree stocks (`0 < \\beta < 1`).\n- `a`: Steady-state demand intercept.\n- `c`: Slope of the inverse demand curve (positive constant).\n- Unit of observation: Time `t` is measured in months.\n\n---\n\n### Data / Model Specification\n\nThe structural model is defined by the following equations:\n\n```latex\nY_{t}=Q_{t}+\\Delta I_{t} \\quad \\text{(Eq. (1))}\n```\n\n```latex\nY_{t}=b_{t}N_{t} \\quad \\text{(Eq. (2))}\n```\n\n```latex\nN_{t}=\\beta N_{t-12}+\\varphi_{t} \\quad \\text{(Eq. (3))}\n```\n\n```latex\np_{t}^{\\mathrm{W}}=a_{t}–c Q_{t} \\quad \\text{(Eq. (4))}\n```\n\n---\n\n### Question\n\nAccording to the model's steady-state equilibrium, how does an increase in the tree survival rate (`\\beta`) affect the long-run equilibrium price `p^W`, holding all other parameters constant?", "Options": {"A": "It decreases the price by increasing the long-run stock of productive trees.", "B": "It increases the price because a higher survival rate implies healthier, more valuable trees.", "C": "It has no effect on the price, as it only alters the depreciation rate, not new plantings.", "D": "The effect is ambiguous because it depends on the level of new plantings `\\varphi`."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the ability to perform and interpret comparative statics on the model's long-run equilibrium. Strategy: Computational Judgment. The user must understand that a higher `\\beta` increases the steady-state tree stock `N = \\varphi / (1 - \\beta)`, which in turn increases supply and lowers the price. Distractor Logic: (B) Conceptual Opposite: confuses the health of an individual tree with the aggregate market price effect of having more trees. (C) True but Irrelevant / Step-Omission Error: correctly notes that `\\beta` is not `\\varphi`, but fails to see the link between `\\beta` and the total stock `N`. (D) Conceptual Error: incorrectly suggests ambiguity where the model provides a clear, negative relationship.", "qid": "166", "question": "### Background\n\n**Research Question.** This problem explores the interpretation of the core structural model of the coffee market, focusing on its long-run equilibrium.\n\n**Setting / Institutional Environment.** The model represents the international coffee market with three core components: market clearing for green coffee beans, a biological growth process for coffee trees with a one-year depreciation cycle, and roaster demand.\n\n**Variables & Parameters.**\n- `p^W`: Steady-state international wholesale price of coffee.\n- `Y_t`: Quantity of green coffee beans supplied at time `t`.\n- `Q_t`: Quantity demanded by roasters at time `t`.\n- `\\Delta I_t`: Change in inventory holdings at time `t`.\n- `N_t`: Number of coffee trees at time `t`.\n- `b`: Steady-state average yield per tree.\n- `\\varphi`: Steady-state number of new trees entering production.\n- `\\beta`: Annual survival rate of tree stocks (`0 < \\beta < 1`).\n- `a`: Steady-state demand intercept.\n- `c`: Slope of the inverse demand curve (positive constant).\n- Unit of observation: Time `t` is measured in months.\n\n---\n\n### Data / Model Specification\n\nThe structural model is defined by the following equations:\n\n```latex\nY_{t}=Q_{t}+\\Delta I_{t} \\quad \\text{(Eq. (1))}\n```\n\n```latex\nY_{t}=b_{t}N_{t} \\quad \\text{(Eq. (2))}\n```\n\n```latex\nN_{t}=\\beta N_{t-12}+\\varphi_{t} \\quad \\text{(Eq. (3))}\n```\n\n```latex\np_{t}^{\\mathrm{W}}=a_{t}–c Q_{t} \\quad \\text{(Eq. (4))}\n```\n\n---\n\n### Question\n\nAccording to the model's steady-state equilibrium, how does an increase in the tree survival rate (`\\beta`) affect the long-run equilibrium price `p^W`, holding all other parameters constant?", "model_evaluation_result": {"model_raw_response": "Looking at this structural model, I need to analyze how the tree survival rate β affects the long-run equilibrium price p^W.\n\nLet me work through the steady-state equilibrium:\n\nIn steady state, all variables are constant over time, so:\n- N_t = N_{t-12} = N* (steady-state tree stock)\n- Y_t = Y* (steady-state supply)\n- Q_t = Q* (steady-state demand)\n- ΔI_t = 0 (no inventory changes)\n\nFrom equation (3) in steady state:\nN* = βN* + φ\nN*(1 - β) = φ\nN* = φ/(1 - β)\n\nFrom equation (2):\nY* = bN* = b × φ/(1 - β)\n\nFrom equation (1) with ΔI_t = 0:\nY* = Q*\n\nSo: Q* = b × φ/(1 - β)\n\nFrom equation (4):\np^W = a - cQ* = a - c × b × φ/(1 - β)\n\nNow I can see how β affects p^W:\n\n∂p^W/∂β = ∂/∂β[a - cbφ/(1 - β)]\n= -cbφ × ∂/∂β[1/(1 - β)]\n= -cbφ × [1/(1 - β)²]\n= -cbφ/(1 - β)² < 0\n\nSince c, b, φ are all positive parameters, an increase in β decreases p^W.\n\nThe economic intuition: A higher survival rate β means more trees survive each year, leading to a larger steady-state tree stock N*. This increases coffee supply Y*, which increases quantity demanded Q* (through market clearing), and with a downward-sloping demand curve (negative slope -c), this results in a lower equilibrium price.\n\nA", "model_answer": "A", "error": null}}
{"ID": 184, "Question": "### Background\n\nThis problem analyzes the baseline monetary model without credit, which serves as a benchmark for the paper's main analysis. In this simplified economy, agents are either producers or money holders. They meet randomly, and a trade occurs when a money holder meets a producer of their desired consumption good.\n\n### Data / Model Specification\n\nThe economy is characterized by:\n*   `V_m`, `V_p`: Stationary value functions for a money holder and a producer.\n*   `r`: Rate of time preference.\n*   `β`: Poisson arrival rate of trading partners.\n*   `M`: Proportion of money holders in the population, `M` ∈ (0, 1).\n*   `z`: Probability of a single coincidence of wants.\n*   `u(q)`: A strictly increasing and strictly concave utility function with `u(0)=0`, `u'(0)=∞`, and `u'(∞)=0`.\n\nThe equilibrium is described by the following Bellman equations and bargaining outcome:\n\n**Bellman Equations:**\n```latex\nrV_m = \\beta(1-M)z(V_p - V_m + u(q_m)) \\quad \\quad \\text{(Eq. 1)}\n```\n```latex\nrV_p = \\beta M z(V_m - V_p - q_m) \\quad \\quad \\text{(Eq. 2)}\n```\n**Bargaining Outcome (Take-it-or-leave-it offer by money holder):**\n```latex\nq_m = V_m - V_p \\quad \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nAfter solving for `V_p` and `V_m` in terms of `q_m`, which equation correctly represents the stationary equilibrium condition that determines the purchasing power of money, `q_m`?", "Options": {"A": "```latex\nu(q_m) - \\left[1 + \\frac{r}{\\beta z(1-M)}\\right]q_m = 0\n```", "B": "```latex\nu(q_m) - \\left[1 + \\frac{r}{\\beta z M}\\right]q_m = 0\n```", "C": "```latex\nu(q_m) - q_m = 0\n```", "D": "```latex\nu(q_m) - \\left[1 - \\frac{r}{\\beta z(1-M)}\\right]q_m = 0\n```"}, "Answer": "A", "pi_justification": "This item tests the ability to derive the central equilibrium equation of the no-credit model, a key application of the Computational Judgment strategy. The correct answer requires substituting `V_p=0` and `V_m=q_m` into the money holder's Bellman equation and rearranging. The distractors are designed based on high-frequency algebraic and conceptual errors: B (Formula Misuse) confuses the proportion of producers with money holders, C (Step-Omission Error) omits the crucial discounting term, and D (Sign Error) introduces a sign error.", "qid": "184", "question": "### Background\n\nThis problem analyzes the baseline monetary model without credit, which serves as a benchmark for the paper's main analysis. In this simplified economy, agents are either producers or money holders. They meet randomly, and a trade occurs when a money holder meets a producer of their desired consumption good.\n\n### Data / Model Specification\n\nThe economy is characterized by:\n*   `V_m`, `V_p`: Stationary value functions for a money holder and a producer.\n*   `r`: Rate of time preference.\n*   `β`: Poisson arrival rate of trading partners.\n*   `M`: Proportion of money holders in the population, `M` ∈ (0, 1).\n*   `z`: Probability of a single coincidence of wants.\n*   `u(q)`: A strictly increasing and strictly concave utility function with `u(0)=0`, `u'(0)=∞`, and `u'(∞)=0`.\n\nThe equilibrium is described by the following Bellman equations and bargaining outcome:\n\n**Bellman Equations:**\n```latex\nrV_m = \\beta(1-M)z(V_p - V_m + u(q_m)) \\quad \\quad \\text{(Eq. 1)}\n```\n```latex\nrV_p = \\beta M z(V_m - V_p - q_m) \\quad \\quad \\text{(Eq. 2)}\n```\n**Bargaining Outcome (Take-it-or-leave-it offer by money holder):**\n```latex\nq_m = V_m - V_p \\quad \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nAfter solving for `V_p` and `V_m` in terms of `q_m`, which equation correctly represents the stationary equilibrium condition that determines the purchasing power of money, `q_m`?", "model_evaluation_result": {"model_raw_response": "I need to solve for the stationary equilibrium condition by working through the Bellman equations and bargaining outcome.\n\nGiven:\n- Equation 1: $rV_m = \\beta(1-M)z(V_p - V_m + u(q_m))$\n- Equation 2: $rV_p = \\beta M z(V_m - V_p - q_m)$\n- Equation 3: $q_m = V_m - V_p$\n\nFirst, I'll solve for $V_m$ and $V_p$ in terms of $q_m$.\n\nFrom Equation 3: $V_p = V_m - q_m$\n\nSubstituting into Equation 1:\n$rV_m = \\beta(1-M)z((V_m - q_m) - V_m + u(q_m))$\n$rV_m = \\beta(1-M)z(-q_m + u(q_m))$\n$rV_m = \\beta(1-M)z(u(q_m) - q_m)$\n\nTherefore: $V_m = \\frac{\\beta(1-M)z(u(q_m) - q_m)}{r}$\n\nFrom Equation 3: $V_p = V_m - q_m = \\frac{\\beta(1-M)z(u(q_m) - q_m)}{r} - q_m$\n\nNow I'll verify this is consistent with Equation 2:\n$rV_p = \\beta M z(V_m - V_p - q_m)$\n\nSince $V_m - V_p = q_m$ (from Equation 3):\n$rV_p = \\beta M z(q_m - q_m) = 0$\n\nThis means $V_p = 0$ in equilibrium.\n\nIf $V_p = 0$, then from Equation 3: $V_m = q_m$\n\nSubstituting back into the expression for $V_m$:\n$q_m = \\frac{\\beta(1-M)z(u(q_", "model_answer": "I need to solve for the stationary equilibrium condition by working through the Bellman equations and bargaining outcome.\n\nGiven:\n- Equation 1: $rV_m = \\beta(1-M)z(V_p - V_m + u(q_m))$\n- Equation 2: $rV_p = \\beta M z(V_m - V_p - q_m)$\n- Equation 3: $q_m = V_m - V_p$\n\nFirst, I'll solve for $V_m$ and $V_p$ in terms of $q_m$.\n\nFrom Equation 3: $V_p = V_m - q_m$\n\nSubstituting into Equation 1:\n$rV_m = \\beta(1-M)z((V_m - q_m) - V_m + u(q_m))$\n$rV_m = \\beta(1-M)z(-q_m + u(q_m))$\n$rV_m = \\beta(1-M)z(u(q_m) - q_m)$\n\nTherefore: $V_m = \\frac{\\beta(1-M)z(u(q_m) - q_m)}{r}$\n\nFrom Equation 3: $V_p = V_m - q_m = \\frac{\\beta(1-M)z(u(q_m) - q_m)}{r} - q_m$\n\nNow I'll verify this is consistent with Equation 2:\n$rV_p = \\beta M z(V_m - V_p - q_m)$\n\nSince $V_m - V_p = q_m$ (from Equation 3):\n$rV_p = \\beta M z(q_m - q_m) = 0$\n\nThis means $V_p = 0$ in equilibrium.\n\nIf $V_p = 0$, then from Equation 3: $V_m = q_m$\n\nSubstituting back into the expression for $V_m$:\n$q_m = \\frac{\\beta(1-M)z(u(q_", "error": "Warning: Could not extract single choice letter from: I need to solve for the stationary equilibrium con..."}}
{"ID": 186, "Question": "### Background\n\n**Research Question.** This problem investigates the role of household worklessness as a primary driver of poverty, focusing on how this relationship is exacerbated for children compared to the general population.\n\n**Setting / Institutional Environment.** The analysis uses data for Britain in 2000/1, defining poverty based on net household income after housing costs. It compares the situation of all individuals with the specific situation of children, categorized by their household's employment status.\n\n**Variables & Parameters.**\n- **Employment Circumstance:** Categories describing the work intensity within a household (e.g., 'Workless', 'Couple, one FT').\n- **Population Share:** The percentage of a given population (all individuals or children) that resides in a household of a specific employment type.\n- **Poverty Rate:** The percentage of individuals or children within a specific group whose household income falls below the poverty line.\n- **Unit of observation:** Individual or Child, based on their household's employment status.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Individual Poverty by Employment Circumstance, 2000/1**\n\n|                     | % individuals in each type | % of each type in poverty | % contribution to overall poverty |\n| :------------------ | :------------------------- | :------------------------ | :------------------------------- |\n| Workless            | 17.0                       | 64.4                      | 51.4                             |\n| ...                 | ...                        | ...                       | ...                              |\n| **Total**           | **100**                    | **21.3**                  | **100**                          |\n\n**Table 2: Child Poverty by Employment Circumstance, 2000/1**\n\n|                             | % children in each type | % children in poverty | % contribution to overall child poverty |\n| :-------------------------- | :---------------------- | :-------------------- | :-------------------------------------- |\n| Workless                    | 20.7                    | 77.4                  | 52.8                                    |\n| ...                         | ...                     | ...                   | ...                                     |\n| **Total**                   | **100**                 | **30.3**              | **100**                                 |\n\n*Note: Ellipses (...) denote omitted rows for brevity.*\n\n---\n\n### The Question\n\nBased on the data in Table 1 and Table 2, which statement most accurately compares the poverty situation of children in workless households to that of all individuals in workless households?", "Options": {"A": "Children in workless households face a higher absolute poverty rate and a higher relative poverty risk than individuals in workless households.", "B": "Children in workless households face a higher absolute poverty rate, but their poverty risk relative to the average child is slightly lower than the equivalent relative risk for all individuals.", "C": "Children are less likely to live in workless households than the general population, but when they do, their poverty rate is higher.", "D": "The contribution of workless households to overall poverty is smaller for children than it is for the general population."}, "Answer": "B", "pi_justification": "This item assesses the ability to synthesize data from two tables to make a nuanced comparison, requiring both direct data lookup and a 'Computational Judgment' (calculating relative risk). Distractors are designed to catch common misinterpretations: focusing on only one metric (A), misreading population shares (C), or misreading contribution shares (D).", "qid": "186", "question": "### Background\n\n**Research Question.** This problem investigates the role of household worklessness as a primary driver of poverty, focusing on how this relationship is exacerbated for children compared to the general population.\n\n**Setting / Institutional Environment.** The analysis uses data for Britain in 2000/1, defining poverty based on net household income after housing costs. It compares the situation of all individuals with the specific situation of children, categorized by their household's employment status.\n\n**Variables & Parameters.**\n- **Employment Circumstance:** Categories describing the work intensity within a household (e.g., 'Workless', 'Couple, one FT').\n- **Population Share:** The percentage of a given population (all individuals or children) that resides in a household of a specific employment type.\n- **Poverty Rate:** The percentage of individuals or children within a specific group whose household income falls below the poverty line.\n- **Unit of observation:** Individual or Child, based on their household's employment status.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Individual Poverty by Employment Circumstance, 2000/1**\n\n|                     | % individuals in each type | % of each type in poverty | % contribution to overall poverty |\n| :------------------ | :------------------------- | :------------------------ | :------------------------------- |\n| Workless            | 17.0                       | 64.4                      | 51.4                             |\n| ...                 | ...                        | ...                       | ...                              |\n| **Total**           | **100**                    | **21.3**                  | **100**                          |\n\n**Table 2: Child Poverty by Employment Circumstance, 2000/1**\n\n|                             | % children in each type | % children in poverty | % contribution to overall child poverty |\n| :-------------------------- | :---------------------- | :-------------------- | :-------------------------------------- |\n| Workless                    | 20.7                    | 77.4                  | 52.8                                    |\n| ...                         | ...                     | ...                   | ...                                     |\n| **Total**                   | **100**                 | **30.3**              | **100**                                 |\n\n*Note: Ellipses (...) denote omitted rows for brevity.*\n\n---\n\n### The Question\n\nBased on the data in Table 1 and Table 2, which statement most accurately compares the poverty situation of children in workless households to that of all individuals in workless households?", "model_evaluation_result": {"model_raw_response": "Looking at the data from Tables 1 and 2, I need to compare the poverty situation of children in workless households to that of all individuals in workless households.\n\nLet me analyze each component:\n\n**Absolute poverty rates:**\n- All individuals in workless households: 64.4% in poverty\n- Children in workless households: 77.4% in poverty\n- Children face a higher absolute poverty rate ✓\n\n**Relative poverty risk (compared to their respective population averages):**\n- All individuals in workless households: 64.4% vs 21.3% overall = 3.02 times higher\n- Children in workless households: 77.4% vs 30.3% overall = 2.55 times higher\n- The relative risk for children is actually lower (2.55 vs 3.02) ✓\n\n**Population share:**\n- All individuals in workless households: 17.0%\n- Children in workless households: 20.7%\n- Children are more likely to live in workless households, not less ✗\n\n**Contribution to overall poverty:**\n- Workless households contribute 51.4% to overall poverty for all individuals\n- Workless households contribute 52.8% to overall child poverty\n- The contribution is slightly higher for children, not smaller ✗\n\nOption A is incorrect because while the absolute poverty rate is higher, the relative risk is lower.\n\nOption B correctly identifies both: higher absolute poverty rate (77.4% vs 64.4%) AND lower relative risk (2.55 times vs 3.02 times the respective averages).\n\nOption C is incorrect because children are more likely (20.7% vs 17.0%) to live in workless households.\n\nOption D is incorrect because the contribution is higher for children (52.8% vs 51.4%).\n\nB", "model_answer": "A", "error": null}}
{"ID": 202, "Question": "### Background\n\n**Research Question.** This problem assesses the causal effect of insurers' fee screens on physicians' list prices and evaluates how sensitive these estimates are to theoretical and empirical model specification.\n\n**Setting / Institutional Environment.** The study estimates the elasticity of physician office prices with respect to insurer fee screens. It compares estimates from a prior, misspecified model with a new, theoretically-grounded model. The new model makes two key corrections: (1) it uses the correct theoretical weights for insurer fees (unweighted for Medicaid, weighted by share of list-price patients for others) and (2) it uses a stricter, theory-consistent definition of Medicaid \"participation.\"\n\n### Data / Model Specification\n\n**Table 1: Office Fee Elasticities from Alternative Model Specifications**\n\n| Model Specification | Participant Definition | Medicaid | Medicare | Private |\n| :--- | :--- | :--- | :--- | :--- |\n| Sloan (original study) | \"some or all\" | 0.18 | 0.03 | 0.03 |\n| Eq. 2.1 (Sloan spec, new data) | \"some or all\" | 0.16 | 0.30 | 0.61 |\n| **Eq. 2.3 (Corrected Model)** | | | | |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Participants* | \"all\" | 0.40 | 0.28 | 0.53 |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Nonparticipants* | \"all\" | 0.17 | 0.25 | 0.52 |\n\n### Question\n\nBased on the corrected elasticities for a *participant* physician in Table 1, what is the predicted total percentage increase in their office list price from a uniform 10% increase in both Medicare and Private fee screens?", "Options": {"A": "2.8%", "B": "5.3%", "C": "7.7%", "D": "8.1%"}, "Answer": "D", "pi_justification": "This item assesses the ability to correctly select and apply elasticity estimates from a table to a quantitative scenario. It uses a Computational Judgment strategy. Distractors are designed based on predictable errors: using only one of the two relevant elasticities (Step-Omission Error) or using the correct formula but with data from the wrong row of the table (Lookup Error).", "qid": "202", "question": "### Background\n\n**Research Question.** This problem assesses the causal effect of insurers' fee screens on physicians' list prices and evaluates how sensitive these estimates are to theoretical and empirical model specification.\n\n**Setting / Institutional Environment.** The study estimates the elasticity of physician office prices with respect to insurer fee screens. It compares estimates from a prior, misspecified model with a new, theoretically-grounded model. The new model makes two key corrections: (1) it uses the correct theoretical weights for insurer fees (unweighted for Medicaid, weighted by share of list-price patients for others) and (2) it uses a stricter, theory-consistent definition of Medicaid \"participation.\"\n\n### Data / Model Specification\n\n**Table 1: Office Fee Elasticities from Alternative Model Specifications**\n\n| Model Specification | Participant Definition | Medicaid | Medicare | Private |\n| :--- | :--- | :--- | :--- | :--- |\n| Sloan (original study) | \"some or all\" | 0.18 | 0.03 | 0.03 |\n| Eq. 2.1 (Sloan spec, new data) | \"some or all\" | 0.16 | 0.30 | 0.61 |\n| **Eq. 2.3 (Corrected Model)** | | | | |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Participants* | \"all\" | 0.40 | 0.28 | 0.53 |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Nonparticipants* | \"all\" | 0.17 | 0.25 | 0.52 |\n\n### Question\n\nBased on the corrected elasticities for a *participant* physician in Table 1, what is the predicted total percentage increase in their office list price from a uniform 10% increase in both Medicare and Private fee screens?", "model_evaluation_result": {"model_raw_response": "I need to calculate the total percentage increase in office list price for a participant physician when both Medicare and Private fee screens increase by 10%.\n\nFrom Table 1, for the corrected model (Eq. 2.3) with \"all\" participant definition:\n- Medicare elasticity = 0.28\n- Private elasticity = 0.53\n\nWhen fee screens increase by 10%, the percentage change in office list price is calculated as:\nPercentage change = (Elasticity × Percentage change in fee screen)\n\nFor Medicare: 0.28 × 10% = 2.8%\nFor Private: 0.53 × 10% = 5.3%\n\nSince both Medicare and Private fee screens are increasing simultaneously by 10%, the total effect is the sum of the individual effects:\n\nTotal percentage increase = 2.8% + 5.3% = 8.1%\n\nD", "model_answer": "D", "error": null}}
{"ID": 264, "Question": "### Background\n\nThe paper's primary empirical objective is to quantitatively account for the dramatic rise and fall of India's relative price of investment (RPI) between 1981 and 2006. The core hypothesis is that this pattern was driven by changes in trade policy affecting imported capital goods, which are a key input into domestic investment. The analysis first establishes that the RPI movement is concentrated in tradable equipment, not non-tradable structures.\n\n### Data / Model Specification\n\n**Table 1: Price Changes by Investment Component (%)**\n\n| Component | Change 1981-1991 | Change 1991-2006 |\n|---|---|---|\n| Relative Price of Machines | +70% | -30% |\n| Relative Price of Structures | +130% | +13% |\n\n### Question\n\nBased on the data in Table 1, which statement provides the strongest support for the paper's hypothesis that trade policies for *imported* capital goods were the primary driver of India's RPI movements?", "Options": {"A": "The parallel movement of machine and structure prices suggests an economy-wide factor, like monetary policy, was the main cause.", "B": "The continuous rise in the price of structures indicates that distortions in the non-tradable construction sector were the key driver for the aggregate RPI.", "C": "The fact that the price of tradable machines follows the aggregate RPI's rise-and-fall pattern, while the price of non-tradable structures does not, isolates the causal mechanism to trade-related factors.", "D": "The fall in machine prices after 1991 was smaller than the rise before 1991, implying that policy reforms were only partially effective."}, "Answer": "C", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to interpret stylized facts to support a specific causal mechanism. This is a Reverse-Reasoning item. Distractors are designed to test for common errors: B and D are true but irrelevant statements that don't address the core mechanism, while A represents a conceptual opposite by incorrectly interpreting the divergent trends as parallel.", "qid": "264", "question": "### Background\n\nThe paper's primary empirical objective is to quantitatively account for the dramatic rise and fall of India's relative price of investment (RPI) between 1981 and 2006. The core hypothesis is that this pattern was driven by changes in trade policy affecting imported capital goods, which are a key input into domestic investment. The analysis first establishes that the RPI movement is concentrated in tradable equipment, not non-tradable structures.\n\n### Data / Model Specification\n\n**Table 1: Price Changes by Investment Component (%)**\n\n| Component | Change 1981-1991 | Change 1991-2006 |\n|---|---|---|\n| Relative Price of Machines | +70% | -30% |\n| Relative Price of Structures | +130% | +13% |\n\n### Question\n\nBased on the data in Table 1, which statement provides the strongest support for the paper's hypothesis that trade policies for *imported* capital goods were the primary driver of India's RPI movements?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 156, "Question": "### Background\n\n**Research Question.** This problem investigates the core inferential framework of the paper for testing a shape restriction `θ₀ ∈ Λ`, where `Λ` is a convex cone. It focuses on the paper's proposed solution for constructing a non-conservative test.\n\n**Setting / Institutional Environment.** The analysis is set in an abstract Hilbert space `H` of functions with norm `||·||_H`.\n\n---\n\n### Data / Model Specification\n\nLet `θ₀` be the true function and `θ̂ₙ` be its estimator. The test statistic is `Tₙ = rₙ||θ̂ₙ - Π_Λ(θ̂ₙ)||_H`. Under the null `H₀: θ₀ ∈ Λ`, the following inequality holds for `0 ≤ κₙ ≤ rₙ`:\n```latex\nT_n \\leq \\left\\| r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + \\kappa_{n}\\theta_{0} - \\Pi_{\\Lambda}\\big(r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + \\kappa_{n}\\theta_{0}\\big) \\right\\|_{\\mathbf{H}} \\quad \\text{(Eq. (1))}\n```\nThe paper's method involves bootstrapping the right-hand side of Eq. (1) to get a critical value.\n\n---\n\n### The Question\n\nThe paper's method uses the inequality in Eq. (1) and a tuning parameter `κₙ` that is chosen to satisfy `κₙ → ∞`. What is the primary statistical benefit of this approach compared to the \"least favorable\" test, which corresponds to setting `κₙ = 0`?", "Options": {"A": "It guarantees that the test's empirical size is exactly equal to the nominal level `α` for all null hypotheses, not just the least favorable ones.", "B": "It is required for the validity of the bootstrap procedure, which would be invalid if `κₙ` were set to 0.", "C": "It makes the test asymptotically non-conservative by using a tighter upper bound for the test statistic, leading to improved power when `θ₀` is in the interior of the null set `Λ`.", "D": "It simplifies computation by removing the need to calculate the projection `Π_Λ(·)` in the bootstrap replications."}, "Answer": "C", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item uses a Computational Judgment strategy to assess understanding of the paper's main contribution over prior methods (achieving non-conservativeness). The correct answer correctly links `κₙ → ∞` to improved power. Distractor (A) makes a claim (similarity) that the paper explicitly states does not hold, (B) confuses the condition for power with the condition for validity, and (D) is factually incorrect.", "qid": "156", "question": "### Background\n\n**Research Question.** This problem investigates the core inferential framework of the paper for testing a shape restriction `θ₀ ∈ Λ`, where `Λ` is a convex cone. It focuses on the paper's proposed solution for constructing a non-conservative test.\n\n**Setting / Institutional Environment.** The analysis is set in an abstract Hilbert space `H` of functions with norm `||·||_H`.\n\n---\n\n### Data / Model Specification\n\nLet `θ₀` be the true function and `θ̂ₙ` be its estimator. The test statistic is `Tₙ = rₙ||θ̂ₙ - Π_Λ(θ̂ₙ)||_H`. Under the null `H₀: θ₀ ∈ Λ`, the following inequality holds for `0 ≤ κₙ ≤ rₙ`:\n```latex\nT_n \\leq \\left\\| r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + \\kappa_{n}\\theta_{0} - \\Pi_{\\Lambda}\\big(r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + \\kappa_{n}\\theta_{0}\\big) \\right\\|_{\\mathbf{H}} \\quad \\text{(Eq. (1))}\n```\nThe paper's method involves bootstrapping the right-hand side of Eq. (1) to get a critical value.\n\n---\n\n### The Question\n\nThe paper's method uses the inequality in Eq. (1) and a tuning parameter `κₙ` that is chosen to satisfy `κₙ → ∞`. What is the primary statistical benefit of this approach compared to the \"least favorable\" test, which corresponds to setting `κₙ = 0`?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand the statistical benefit of choosing κₙ → ∞ versus setting κₙ = 0 (the \"least favorable\" test).\n\nThe key insight is in the inequality structure. When θ₀ is in the interior of the constraint set Λ (i.e., not on the boundary), the \"least favorable\" approach with κₙ = 0 tends to be overly conservative because it assumes the worst-case scenario where θ₀ is on the boundary of Λ.\n\nBy choosing κₙ → ∞, the method effectively:\n1. Uses a tighter upper bound for the test statistic when θ₀ is in the interior of Λ\n2. This tighter bound leads to smaller critical values\n3. Smaller critical values result in higher power (less conservative test)\n4. The test becomes \"asymptotically non-conservative\" in the sense that it achieves the correct size asymptotically rather than being overly conservative\n\nLet me evaluate each option:\n\nA) This claims exact size control for all null hypotheses, which is too strong a claim and not the main benefit.\n\nB) This suggests bootstrap validity requires κₙ > 0, but bootstrap can work with κₙ = 0, it's just less efficient.\n\nC) This correctly identifies that the method provides a tighter upper bound leading to improved power when θ₀ is in the interior of Λ, making the test asymptotically non-conservative.\n\nD) This is incorrect - the projection Πₛ(·) is still needed in bootstrap replications.\n\nC", "model_answer": "A", "error": null}}
{"ID": 207, "Question": "### Background\n\n**Research Question.** This problem evaluates the economic and policy implications of correcting a model's specification by analyzing the key parameters of the wage-price feedback loop in the author's final, dynamically stable model.\n\n**Setting and Sample.** The analysis compares parameter estimates from Hines' original, dynamically unstable model with those from the author's re-specified, dynamically stable model, estimated using Two-Stage Least Squares (2SLS) on UK data from 1921-61. The core of the wage-price spiral is the feedback between wage changes and price changes.\n\n**Variables and Parameters.**\n- `ΔW_t`: Percentage change in hourly wage rates.\n- `ΔP'_t`: Percentage change in retail prices (re-specified).\n- `a₃`: The coefficient on price changes in the wage equation, representing the price-to-wage pass-through.\n- `b₁`: The coefficient on wage changes in the price equation, representing the wage-to-price pass-through.\n\n---\n\n### Data / Model Specification\n\nHines' original (unstable) model produced an estimate of the price-to-wage pass-through (`a₃`) of approximately 0.6.\n\nThe author's final preferred specification drops an insignificant productivity variable and is estimated using 2SLS. The key behavioral equations and their estimated coefficients are presented in Table 1.\n\n**Table 1: Final 2SLS Model Estimates**\n\n| Equation | Dependent Var. | Intercept | `ΔT_t` | `T_t` | `ΔP'_{t-1/2}` | `ΔW_t` | `ΔM_t` |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| (Eq. 1) Wage Eq. | `ΔW_t` | -4.68 | 2.21 | 0.215 | 0.294 | - | - |\n| | | | (0.29) | (0.057) | (0.123) | | |\n| (Eq. 2) Price Eq. | `ΔP'_t` | -0.146 | - | - | - | 0.702 | 0.0732 |\n| | | | | | | (0.069) | (0.0358) |\n\n*Note: Asymptotic standard errors are in parentheses.*\n\nIn a simplified dynamic system, the total cumulative long-run impact of a one-time price shock is given by the multiplier `1 / (1 - k)`, where `k = a₃ * b₁` is the round-trip feedback multiplier. This is valid only if the system is stable (`|k| < 1`).\n\n---\n\n### Question\n\nUsing the data provided, calculate the long-run inflation multiplier implied by the author's final model and compare it to the multiplier implied by using Hines' inconsistent estimate of `a₃`. Which of the following statements correctly quantifies this comparison and its policy implication?", "Options": {"A": "The author's model implies a long-run multiplier of approximately 1.26, whereas Hines' estimate implies a multiplier of 1.73. This suggests that relying on the misspecified model would cause policymakers to overstate the persistence of inflation following a shock.", "B": "The author's model implies a long-run multiplier of approximately 1.26, whereas Hines' estimate implies a multiplier of 1.73. This suggests that relying on the misspecified model would cause policymakers to understate the persistence of inflation following a shock.", "C": "The author's model implies a round-trip feedback effect of 0.21, whereas Hines' estimate implies a feedback effect of 0.42. This suggests that relying on the misspecified model would cause policymakers to understate the persistence of inflation following a shock.", "D": "The author's model implies a long-run multiplier of approximately 4.85, whereas Hines' estimate implies a multiplier of 2.37. This suggests that relying on the misspecified model would cause policymakers to overstate the persistence of inflation following a shock."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the pinnacle of the original QA problem: quantifying the policy error from using a misspecified model. It uses a Computational Judgment strategy, requiring a multi-step calculation and a final comparative interpretation. Distractor B correctly calculates the multipliers but reverses the policy implication. Distractor C correctly identifies the policy implication but reports the one-period feedback `k` instead of the long-run multiplier `1/(1-k)`, a classic Step-Omission Error. Distractor D uses an incorrect formula, `1/k`, for the multiplier.", "qid": "207", "question": "### Background\n\n**Research Question.** This problem evaluates the economic and policy implications of correcting a model's specification by analyzing the key parameters of the wage-price feedback loop in the author's final, dynamically stable model.\n\n**Setting and Sample.** The analysis compares parameter estimates from Hines' original, dynamically unstable model with those from the author's re-specified, dynamically stable model, estimated using Two-Stage Least Squares (2SLS) on UK data from 1921-61. The core of the wage-price spiral is the feedback between wage changes and price changes.\n\n**Variables and Parameters.**\n- `ΔW_t`: Percentage change in hourly wage rates.\n- `ΔP'_t`: Percentage change in retail prices (re-specified).\n- `a₃`: The coefficient on price changes in the wage equation, representing the price-to-wage pass-through.\n- `b₁`: The coefficient on wage changes in the price equation, representing the wage-to-price pass-through.\n\n---\n\n### Data / Model Specification\n\nHines' original (unstable) model produced an estimate of the price-to-wage pass-through (`a₃`) of approximately 0.6.\n\nThe author's final preferred specification drops an insignificant productivity variable and is estimated using 2SLS. The key behavioral equations and their estimated coefficients are presented in Table 1.\n\n**Table 1: Final 2SLS Model Estimates**\n\n| Equation | Dependent Var. | Intercept | `ΔT_t` | `T_t` | `ΔP'_{t-1/2}` | `ΔW_t` | `ΔM_t` |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| (Eq. 1) Wage Eq. | `ΔW_t` | -4.68 | 2.21 | 0.215 | 0.294 | - | - |\n| | | | (0.29) | (0.057) | (0.123) | | |\n| (Eq. 2) Price Eq. | `ΔP'_t` | -0.146 | - | - | - | 0.702 | 0.0732 |\n| | | | | | | (0.069) | (0.0358) |\n\n*Note: Asymptotic standard errors are in parentheses.*\n\nIn a simplified dynamic system, the total cumulative long-run impact of a one-time price shock is given by the multiplier `1 / (1 - k)`, where `k = a₃ * b₁` is the round-trip feedback multiplier. This is valid only if the system is stable (`|k| < 1`).\n\n---\n\n### Question\n\nUsing the data provided, calculate the long-run inflation multiplier implied by the author's final model and compare it to the multiplier implied by using Hines' inconsistent estimate of `a₃`. Which of the following statements correctly quantifies this comparison and its policy implication?", "model_evaluation_result": {"model_raw_response": "I need to calculate the long-run inflation multiplier for both models and compare them.\n\nFrom the table, I can identify the key coefficients:\n- Author's model: a₃ = 0.294 (price-to-wage pass-through from Eq. 1)\n- Author's model: b₁ = 0.702 (wage-to-price pass-through from Eq. 2)\n- Hines' original model: a₃ = 0.6 (given in the problem)\n- Hines' model uses the same b₁ = 0.702 (since only a₃ changed between models)\n\nThe long-run multiplier formula is: 1 / (1 - k), where k = a₃ × b₁\n\nFor the author's model:\nk = 0.294 × 0.702 = 0.206\nLong-run multiplier = 1 / (1 - 0.206) = 1 / 0.794 = 1.26\n\nFor Hines' model:\nk = 0.6 × 0.702 = 0.421\nLong-run multiplier = 1 / (1 - 0.421) = 1 / 0.579 = 1.73\n\nComparing the results:\n- Author's model: multiplier ≈ 1.26\n- Hines' model: multiplier ≈ 1.73\n\nSince Hines' model produces a higher multiplier (1.73 vs 1.26), it would suggest greater persistence of inflation shocks. Therefore, relying on the misspecified model would cause policymakers to overstate the persistence of inflation following a shock.\n\nA", "model_answer": "A", "error": null}}
{"ID": 205, "Question": "### Background\n\n**Research Question.** What are the market characteristics and strategic considerations for authors and publishers in different academic fields?\n\n**Setting / Institutional Environment.** The study surveys authors from Economics departments and Business schools, revealing significant differences in their publication patterns and market outcomes. Economics faculty tend to write more specialized, technical books for a smaller academic audience, while Business school faculty write more textbooks aimed at larger student and professional markets. These differences are reflected in the sales distributions for books from each field.\n\n**Variables & Parameters.**\n*   `Book Type`: Classification of publications (e.g., `Textbook`, `Technical or specialized book`).\n*   `Sales Volume`: The total number of copies sold, grouped into categories.\n*   `Publisher's Profit`: The net revenue a publisher earns from a book.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Publication and Sales Profiles by Academic Field**\n\n| Metric                               | Economics Departments | Business Schools |\n|--------------------------------------|-----------------------|------------------|\n| **Book Type Distribution**           |                       |                  |\n| % Textbooks*                         | 43.3%                 | 60.8%            |\n| % Technical or specialized books     | 35.3%                 | 22.8%            |\n| **Sales Volume Distribution**        |                       |                  |\n| % of books selling 1-2,000 copies    | 46%                   | 25%              |\n| % of books selling 2,001-5,000 copies| 22%                   | 24%              |\n| % of books selling 5,001-20,000 copies| 20%                   | 29%              |\n| % of books selling over 20,000 copies| 8%                    | 16%              |\n\n*Textbooks are defined as Freshman, Advanced undergraduate, and Graduate texts combined.*\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following best characterizes the difference between the 'publication models' of Economics departments and Business schools?", "Options": {"A": "Economics faculty focus on high-volume textbooks, while Business school faculty focus on niche technical books.", "B": "Business school books have double the probability of selling over 20,000 copies compared to Economics books.", "C": "Both fields produce a similar percentage of textbooks, but Business school books sell better due to more effective marketing.", "D": "Economics faculty focus more on specialized, lower-volume technical books for a niche audience, while Business school faculty focus more on higher-volume textbooks for a broader market."}, "Answer": "D", "pi_justification": "This item assesses the ability to interpret descriptive statistics and synthesize them into a coherent qualitative description. The correct option accurately summarizes the relationship between book type focus and sales outcomes for the two fields. Distractors include a conceptual opposite (A), a true but incomplete statement (B), and a factually incorrect and speculative statement (C).", "qid": "205", "question": "### Background\n\n**Research Question.** What are the market characteristics and strategic considerations for authors and publishers in different academic fields?\n\n**Setting / Institutional Environment.** The study surveys authors from Economics departments and Business schools, revealing significant differences in their publication patterns and market outcomes. Economics faculty tend to write more specialized, technical books for a smaller academic audience, while Business school faculty write more textbooks aimed at larger student and professional markets. These differences are reflected in the sales distributions for books from each field.\n\n**Variables & Parameters.**\n*   `Book Type`: Classification of publications (e.g., `Textbook`, `Technical or specialized book`).\n*   `Sales Volume`: The total number of copies sold, grouped into categories.\n*   `Publisher's Profit`: The net revenue a publisher earns from a book.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Publication and Sales Profiles by Academic Field**\n\n| Metric                               | Economics Departments | Business Schools |\n|--------------------------------------|-----------------------|------------------|\n| **Book Type Distribution**           |                       |                  |\n| % Textbooks*                         | 43.3%                 | 60.8%            |\n| % Technical or specialized books     | 35.3%                 | 22.8%            |\n| **Sales Volume Distribution**        |                       |                  |\n| % of books selling 1-2,000 copies    | 46%                   | 25%              |\n| % of books selling 2,001-5,000 copies| 22%                   | 24%              |\n| % of books selling 5,001-20,000 copies| 20%                   | 29%              |\n| % of books selling over 20,000 copies| 8%                    | 16%              |\n\n*Textbooks are defined as Freshman, Advanced undergraduate, and Graduate texts combined.*\n\n---\n\n### Question\n\nBased on the data in **Table 1**, which of the following best characterizes the difference between the 'publication models' of Economics departments and Business schools?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 191, "Question": "### Background\n\n**Research Question.** This problem develops the paper's theoretical framework to understand how leader identity affects group coordination in a weakest-link game by introducing heterogeneity in player types.\n\n**Setting / Institutional Environment.** The model considers a four-player coordination game where players choose effort `x` on a continuum `[0, W]`. A fraction of the population are 'behavioral types' who follow leaders of their own religion but not others. 'Rational types' maximize expected payoffs, accounting for the possible presence of behavioral types. The analysis assumes the probability of having more than one behavioral player in a group is negligible.\n\n### Data / Model Specification\n\n1.  **No-Leader Equilibrium:** In the absence of a leader, rational players assume others' efforts are drawn from `U[0, W]`. The optimal strategy is to match the expected minimum of the other three players' efforts, leading to a baseline effort `x*`.\n\n2.  **Leader Equilibrium:** With a leader, the optimal effort choices for a rational player under a Hindu leader (`x_H`) and a Muslim leader (`x_M`) are given by:\n\n    ```latex\n    x_{H} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( (1-p)a_{H}\\frac{W}{3} + p a_{M}x^{**} \\right) \\quad \\text{(Eq. 1)}\n    ```\n\n    ```latex\n    x_{M} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( p a_{M}\\frac{W}{3} + (1-p)a_{H}x^{**} \\right) \\quad \\text{(Eq. 2)}\n    ```\n\n    where `p` is the population share of Muslims, `a_H` and `a_M` are the fractions of behavioral types for each religion, `\\tilde{a} = p a_M + (1-p) a_H` is the overall fraction of behavioral types, `x**` is a low effort level (`x** < x*`), and `W` is the maximum effort.\n\n### The Question\n\nIn the model of a rational player's effort choice under a Hindu leader (Eq. 1), what is the economic interpretation of the term `W/3`?", "Options": {"A": "The optimal effort for a rational player who anticipates that the other two players in the group are also rational.", "B": "The risk-dominant equilibrium effort when all four players in the group are rational.", "C": "The high effort level proposed by the Hindu leader, which a behavioral Hindu player would follow.", "D": "The expected effort of a single player chosen at random from the population."}, "Answer": "A", "pi_justification": "Assesses understanding of the model's core logic by requiring interpretation of a key term. Strategy: Computational Judgment. Distractors are based on common formula misapplications (`W/4`, `W/2`) and a subtle conceptual confusion between a behavioral player's action and a rational player's strategic response.", "qid": "191", "question": "### Background\n\n**Research Question.** This problem develops the paper's theoretical framework to understand how leader identity affects group coordination in a weakest-link game by introducing heterogeneity in player types.\n\n**Setting / Institutional Environment.** The model considers a four-player coordination game where players choose effort `x` on a continuum `[0, W]`. A fraction of the population are 'behavioral types' who follow leaders of their own religion but not others. 'Rational types' maximize expected payoffs, accounting for the possible presence of behavioral types. The analysis assumes the probability of having more than one behavioral player in a group is negligible.\n\n### Data / Model Specification\n\n1.  **No-Leader Equilibrium:** In the absence of a leader, rational players assume others' efforts are drawn from `U[0, W]`. The optimal strategy is to match the expected minimum of the other three players' efforts, leading to a baseline effort `x*`.\n\n2.  **Leader Equilibrium:** With a leader, the optimal effort choices for a rational player under a Hindu leader (`x_H`) and a Muslim leader (`x_M`) are given by:\n\n    ```latex\n    x_{H} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( (1-p)a_{H}\\frac{W}{3} + p a_{M}x^{**} \\right) \\quad \\text{(Eq. 1)}\n    ```\n\n    ```latex\n    x_{M} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( p a_{M}\\frac{W}{3} + (1-p)a_{H}x^{**} \\right) \\quad \\text{(Eq. 2)}\n    ```\n\n    where `p` is the population share of Muslims, `a_H` and `a_M` are the fractions of behavioral types for each religion, `\\tilde{a} = p a_M + (1-p) a_H` is the overall fraction of behavioral types, `x**` is a low effort level (`x** < x*`), and `W` is the maximum effort.\n\n### The Question\n\nIn the model of a rational player's effort choice under a Hindu leader (Eq. 1), what is the economic interpretation of the term `W/3`?", "model_evaluation_result": {"model_raw_response": "Looking at Equation 1, I need to understand what `W/3` represents in the context of the model.\n\nThe equation shows:\n```\nx_H = (1-ã)³x* + 3(1-ã)² [(1-p)a_H(W/3) + p a_M x**]\n```\n\nLet me break down the components:\n- The term `(1-p)a_H(W/3)` appears in the context of a rational player's decision under a Hindu leader\n- `(1-p)` is the fraction of Hindus in the population\n- `a_H` is the fraction of behavioral types among Hindus\n- So `(1-p)a_H` represents the probability of encountering a behavioral Hindu player\n\nThe key insight is that `W/3` appears specifically in the term related to behavioral Hindu players under a Hindu leader. According to the model setup, behavioral types follow leaders of their own religion.\n\nSince `W` is the maximum effort level, and behavioral players follow their religious leader's proposal, `W/3` represents the high effort level that a Hindu leader proposes, which behavioral Hindu players would then follow.\n\nThis interpretation is consistent with:\n1. The model's assumption that behavioral types follow leaders of their own religion\n2. The context where this term appears (specifically for behavioral Hindus under a Hindu leader)\n3. The fact that it's a fraction of the maximum possible effort `W`\n\nC", "model_answer": "C", "error": null}}
{"ID": 277, "Question": "### Background\n\nA benevolent social planner chooses a threshold `t*` to allocate consumers between two network goods, `a` and `b`, to maximize ex-ante expected aggregate welfare. The allocation must balance the gains from network effects and vertical quality against the losses from overriding consumers' idiosyncratic tastes. The parameter `β` represents the precision of the idiosyncratic taste distribution; a higher `β` implies consumers are more homogeneous in their tastes.\n\n### Question\nThe paper finds that the socially optimal threshold `t*` is decreasing in `β`. A higher `β` implies consumers are more homogeneous. What is the primary economic reason for this result?", "Options": {"A": "As consumers become more homogeneous, the social cost of overriding their individual tastes decreases, making it more attractive for the planner to create a larger, more asymmetric network to maximize network effects.", "B": "As consumers become more homogeneous, their private signals become more correlated, which makes the network externality less important for social welfare.", "C": "As consumers become more homogeneous, the vertical quality difference between the products becomes less significant, leading the planner to favor a more balanced market.", "D": "As consumers become more homogeneous, firms are forced to compete more aggressively on price, which naturally leads to a more asymmetric market outcome."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the economic reasoning behind a key comparative statics result. Strategy: Reverse-Reasoning. Distractor Logic: (B) is a conceptual opposite, as network effects become more central to the planner's decision. (C) incorrectly links taste homogeneity (`β`) to vertical quality. (D) imports concepts of strategic pricing from the sponsored-network case, which are irrelevant to the social planner's problem.", "qid": "277", "question": "### Background\n\nA benevolent social planner chooses a threshold `t*` to allocate consumers between two network goods, `a` and `b`, to maximize ex-ante expected aggregate welfare. The allocation must balance the gains from network effects and vertical quality against the losses from overriding consumers' idiosyncratic tastes. The parameter `β` represents the precision of the idiosyncratic taste distribution; a higher `β` implies consumers are more homogeneous in their tastes.\n\n### Question\nThe paper finds that the socially optimal threshold `t*` is decreasing in `β`. A higher `β` implies consumers are more homogeneous. What is the primary economic reason for this result?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 155, "Question": "### Background\n\n**Research Question.** This problem investigates the core inferential framework of the paper for testing a shape restriction `θ₀ ∈ Λ`, where `Λ` is a convex cone. It focuses on the challenge posed by a non-standard limiting distribution.\n\n**Setting / Institutional Environment.** The analysis is set in an abstract Hilbert space `H` of functions with norm `||·||_H`.\n\n---\n\n### Data / Model Specification\n\nLet `θ₀` be the true function of interest and `θ̂ₙ` be an unconstrained estimator. The null hypothesis is `H₀: θ₀ ∈ Λ`. The test statistic is `Tₙ = rₙ||θ̂ₙ - Π_Λ(θ̂ₙ)||_H`, where `Π_Λ(·)` is the projection operator onto `Λ` and `rₙ → ∞` is a rate of convergence.\n\nUnder the null hypothesis `θ₀ ∈ Λ`, the test statistic can be expressed as:\n```latex\nT_n = \\left\\| r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + r_{n}\\theta_{0} - \\Pi_{\\Lambda}\\big(r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + r_{n}\\theta_{0}\\big) \\right\\|_{\\mathbf{H}} \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### The Question\n\nThe test statistic `Tₙ` in Eq. (1) has a limiting distribution that depends on the unknown true function `θ₀`, specifically through the term `rₙθ₀`. Why does this term invalidate a standard plug-in bootstrap procedure where `rₙθ₀` would be replaced by its estimate `rₙθ̂ₙ`?\n", "Options": {"A": "The projection operator `Π_Λ(·)` is non-linear, and standard bootstrap methods are only valid for linear estimators.", "B": "The estimation error `rₙ(θ̂ₙ - θ₀)` is of order `O_p(1)` and does not vanish, so replacing `rₙθ₀` with `rₙθ̂ₙ` introduces a non-negligible error into the bootstrap distribution.", "C": "The rate of convergence `rₙ` is unknown in practice and must be estimated, which introduces too much uncertainty for the bootstrap to be valid.", "D": "The term `rₙθ₀` can be consistently estimated by `rₙθ̂ₙ`, but the resulting bootstrap is computationally infeasible in a Hilbert space."}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item uses a Reverse-Reasoning strategy to assess understanding of the fundamental statistical challenge that motivates the paper's entire methodology. The correct answer pinpoints the non-vanishing estimation error of the drift term. Distractor (A) is a true but secondary issue, (C) is factually incorrect for this context, and (D) represents the primary conceptual misconception.", "qid": "155", "question": "### Background\n\n**Research Question.** This problem investigates the core inferential framework of the paper for testing a shape restriction `θ₀ ∈ Λ`, where `Λ` is a convex cone. It focuses on the challenge posed by a non-standard limiting distribution.\n\n**Setting / Institutional Environment.** The analysis is set in an abstract Hilbert space `H` of functions with norm `||·||_H`.\n\n---\n\n### Data / Model Specification\n\nLet `θ₀` be the true function of interest and `θ̂ₙ` be an unconstrained estimator. The null hypothesis is `H₀: θ₀ ∈ Λ`. The test statistic is `Tₙ = rₙ||θ̂ₙ - Π_Λ(θ̂ₙ)||_H`, where `Π_Λ(·)` is the projection operator onto `Λ` and `rₙ → ∞` is a rate of convergence.\n\nUnder the null hypothesis `θ₀ ∈ Λ`, the test statistic can be expressed as:\n```latex\nT_n = \\left\\| r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + r_{n}\\theta_{0} - \\Pi_{\\Lambda}\\big(r_{n}(\\hat{\\theta}_{n}-\\theta_{0}) + r_{n}\\theta_{0}\\big) \\right\\|_{\\mathbf{H}} \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### The Question\n\nThe test statistic `Tₙ` in Eq. (1) has a limiting distribution that depends on the unknown true function `θ₀`, specifically through the term `rₙθ₀`. Why does this term invalidate a standard plug-in bootstrap procedure where `rₙθ₀` would be replaced by its estimate `rₙθ̂ₙ`?\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand why replacing `rₙθ₀` with `rₙθ̂ₙ` in the bootstrap procedure would be problematic.\n\nThe key insight is in the structure of the test statistic under the null hypothesis:\n\n`Tₙ = ||rₙ(θ̂ₙ - θ₀) + rₙθ₀ - Π_Λ(rₙ(θ̂ₙ - θ₀) + rₙθ₀)||_H`\n\nThe limiting distribution of this test statistic depends on:\n1. The term `rₙ(θ̂ₙ - θ₀)`, which converges in distribution to some limit (typically `O_p(1)`)\n2. The term `rₙθ₀`, which represents the scaled true parameter\n3. The projection operator `Π_Λ(·)` applied to their sum\n\nIn a standard plug-in bootstrap, we would replace `rₙθ₀` with `rₙθ̂ₙ`. However, this replacement introduces an error of magnitude:\n\n`rₙθ̂ₙ - rₙθ₀ = rₙ(θ̂ₙ - θ₀)`\n\nSince `rₙ(θ̂ₙ - θ₀) = O_p(1)` (this is the estimation error scaled by the rate of convergence), this replacement error is not negligible - it's of the same order as the main stochastic component of the test statistic.\n\nThis means that the bootstrap distribution based on replacing `rₙθ₀` with `rₙθ̂ₙ` would differ from the true limiting distribution by a non-vanishing amount, making the bootstrap invalid for inference.\n\nThe answer is B.\n\nB", "model_answer": "B", "error": null}}
{"ID": 168, "Question": "### Background\n\n**Research Question.** This problem deconstructs the econometric methodology used to classify firms as 'strategic' or 'confused' based on their VAT reporting behavior. The core challenge is to attribute responsibility for a reporting discrepancy observed between a buyer and a seller to each firm's intrinsic reporting tendency.\n\n**Setting and Sample.** The analysis uses monthly, transaction-level VAT data for firm-pairs in Uganda. The methodology, inspired by Abowd et al. (1999), is applied to the largest \"connected set\" of trading firms, where firms are linked directly or indirectly through their trading partners.\n\n---\n\n### Data / Model Specification\n\nThe methodology proceeds in two main steps:\n\n1.  **Define and Decompose Discrepancy:** The discrepancy between a buyer `f` and seller `f'` in month `t` is defined as the difference between the VAT reported by the buyer and the seller:\n    ```latex\n    d_{f f't} \\equiv Y_{f f't}^{B} - Y_{f f't}^{S} \\quad \\text{(Eq. (1))}\n    ```\n    A positive `d` implies a seller shortfall (liability-reducing), while a negative `d` implies a buyer shortfall (liability-increasing). This discrepancy is then decomposed using a two-way fixed-effects model:\n    ```latex\n    d_{f f't} = \\delta_{c} + \\delta_{f}^{b} + \\delta_{f'}^{s} + \\delta_{t} + r_{f f't} \\quad \\text{(Eq. (2))}\n    ```\n    where `\\delta_f^b` is a buyer fixed effect for firm `f`, `\\delta_{f'}^s` is a seller fixed effect for firm `f'`, and `\\delta_t` is a month fixed effect.\n\n2.  **Construct Firm-Level Classification Statistic:** The estimated fixed effects for a given firm `f` are aggregated into a single statistic, `Q_f`, which summarizes its overall misreporting behavior:\n    ```latex\n    Q_{f} \\equiv w_{s} \\cdot \\hat{\\delta}_{f}^{s} + w_{b} \\cdot \\hat{\\delta}_{f}^{b} \\quad \\text{(Eq. (3))}\n    ```\n    where `w_s` and `w_b` are the number of observations for firm `f` as a seller and buyer, respectively. A firm is classified as 'advantageous' if `Q_f > 0` and 'disadvantageous' if `Q_f < 0`.\n\n---\n\nThe paper notes that the individual fixed effects `\\delta_f^b` and `\\delta_{f'}^s` can only be estimated for firms within a 'connected set.' How does observing a firm `f` buying from two different sellers, `f'` and `f''`, resolve the identification problem that exists if `f` only bought from `f'`?", "Options": {"A": "It allows the model to estimate the relative reporting tendency of seller `f'` compared to seller `f''`.", "B": "It doubles the number of observations for firm `f`, which solves the problem by increasing statistical power.", "C": "It proves that the error term `r_{ff't}` is not systematically correlated with the fixed effects.", "D": "It ensures that the buyer fixed effect `\\delta_f^b` is not perfectly collinear with the month fixed effect `\\delta_t`."}, "Answer": "A", "pi_justification": "This item assesses understanding of the identification strategy for two-way fixed effects models in a network context, using a Reverse-Reasoning approach. The correct answer identifies the key insight that identification comes from comparing a firm's transactions with multiple partners. Distractors target common statistical misunderstandings: (B) confuses identification with precision/power; (C) brings in an unrelated assumption (exogeneity); (D) misidentifies the source of the collinearity.", "qid": "168", "question": "### Background\n\n**Research Question.** This problem deconstructs the econometric methodology used to classify firms as 'strategic' or 'confused' based on their VAT reporting behavior. The core challenge is to attribute responsibility for a reporting discrepancy observed between a buyer and a seller to each firm's intrinsic reporting tendency.\n\n**Setting and Sample.** The analysis uses monthly, transaction-level VAT data for firm-pairs in Uganda. The methodology, inspired by Abowd et al. (1999), is applied to the largest \"connected set\" of trading firms, where firms are linked directly or indirectly through their trading partners.\n\n---\n\n### Data / Model Specification\n\nThe methodology proceeds in two main steps:\n\n1.  **Define and Decompose Discrepancy:** The discrepancy between a buyer `f` and seller `f'` in month `t` is defined as the difference between the VAT reported by the buyer and the seller:\n    ```latex\n    d_{f f't} \\equiv Y_{f f't}^{B} - Y_{f f't}^{S} \\quad \\text{(Eq. (1))}\n    ```\n    A positive `d` implies a seller shortfall (liability-reducing), while a negative `d` implies a buyer shortfall (liability-increasing). This discrepancy is then decomposed using a two-way fixed-effects model:\n    ```latex\n    d_{f f't} = \\delta_{c} + \\delta_{f}^{b} + \\delta_{f'}^{s} + \\delta_{t} + r_{f f't} \\quad \\text{(Eq. (2))}\n    ```\n    where `\\delta_f^b` is a buyer fixed effect for firm `f`, `\\delta_{f'}^s` is a seller fixed effect for firm `f'`, and `\\delta_t` is a month fixed effect.\n\n2.  **Construct Firm-Level Classification Statistic:** The estimated fixed effects for a given firm `f` are aggregated into a single statistic, `Q_f`, which summarizes its overall misreporting behavior:\n    ```latex\n    Q_{f} \\equiv w_{s} \\cdot \\hat{\\delta}_{f}^{s} + w_{b} \\cdot \\hat{\\delta}_{f}^{b} \\quad \\text{(Eq. (3))}\n    ```\n    where `w_s` and `w_b` are the number of observations for firm `f` as a seller and buyer, respectively. A firm is classified as 'advantageous' if `Q_f > 0` and 'disadvantageous' if `Q_f < 0`.\n\n---\n\nThe paper notes that the individual fixed effects `\\delta_f^b` and `\\delta_{f'}^s` can only be estimated for firms within a 'connected set.' How does observing a firm `f` buying from two different sellers, `f'` and `f''`, resolve the identification problem that exists if `f` only bought from `f'`?", "model_evaluation_result": {"model_raw_response": "Looking at this identification problem, I need to understand what happens when we move from one trading relationship to multiple trading relationships.\n\nIn the two-way fixed effects model:\n```\nd_{ff't} = δ_c + δ_f^b + δ_{f'}^s + δ_t + r_{ff't}\n```\n\nIf firm `f` only bought from seller `f'`, we would have perfect collinearity between the buyer fixed effect `δ_f^b` and seller fixed effect `δ_{f'}^s`. We couldn't separately identify how much of any observed discrepancy is due to the buyer's reporting tendency versus the seller's reporting tendency.\n\nHowever, when firm `f` buys from two different sellers `f'` and `f''`, we get:\n- Transaction with `f'`: `d_{ff't} = δ_c + δ_f^b + δ_{f'}^s + δ_t + r_{ff't}`\n- Transaction with `f''`: `d_{ff''t} = δ_c + δ_f^b + δ_{f''}^s + δ_t + r_{ff''t}`\n\nNow we can compare these relationships. The difference in discrepancies between these two transactions isolates the difference between the two sellers' reporting tendencies:\n`d_{ff't} - d_{ff''t} = δ_{f'}^s - δ_{f''}^s + (r_{ff't} - r_{ff''t})`\n\nThis allows us to estimate the relative reporting tendency of seller `f'` compared to seller `f''`, which is exactly what option A describes.\n\nThe other options don't address the core identification issue:\n- B) More observations help with precision but don't solve the identification problem\n- C) This is about error correlation, not the identification issue\n- D) Month fixed effects aren't the source of the collinearity problem\n\nA", "model_answer": "A", "error": null}}
{"ID": 172, "Question": "### Background\n\n**Research Question.** This problem explores the fundamental strategic trade-off in a repeated game with imperfect monitoring.\n\n**Setting / Institutional Environment.** A long-run Player 1 (LR) interacts with a sequence of short-run Player 2s. In each period, Player 2 decides whether to enter. If they enter, Player 1 chooses to cooperate (`+1`) or deviate (`-1`).\n\n### Data / Model Specification\n\nThe stage-game payoffs are given in Table 1.\n\n**Table 1: Stage-Game Payoffs (Player 1, Player 2)**\n\n| Player 1 | Player 2: Out | Player 2: In |\n|:---|:---:|:---:|\n| **+1** | `u`, 0 | `ū`, 1 |\n| **-1** | `u`, 0 | `ū + g`, -1 |\n\nwhere `ū > u` and `g > 0`.\n\n### Question\n\nIn the one-shot stage game described in Table 1, what is the unique Nash Equilibrium outcome and the key strategic reason for it?", "Options": {"A": "(Player 1: +1, Player 2: In); this is the Pareto-optimal outcome.", "B": "(Player 1: -1, Player 2: Out); Player 1's dominant strategy is -1.", "C": "(Player 1: -1, Player 2: Out); Player 2 anticipates that if they play In, Player 1 has an incentive to play -1.", "D": "(Player 1: +1, Player 2: Out); Player 2's dominant strategy is Out."}, "Answer": "C", "pi_justification": "This item assesses the foundational understanding of the stage game's strategic tension. It uses a Reverse-Reasoning strategy, asking the student to identify the equilibrium by reasoning backward from Player 1's incentives. Distractors are designed to catch common errors in game theory analysis: confusing Pareto optimality with equilibrium (A), misidentifying dominant strategies (B, D), and failing to use backward induction logic.", "qid": "172", "question": "### Background\n\n**Research Question.** This problem explores the fundamental strategic trade-off in a repeated game with imperfect monitoring.\n\n**Setting / Institutional Environment.** A long-run Player 1 (LR) interacts with a sequence of short-run Player 2s. In each period, Player 2 decides whether to enter. If they enter, Player 1 chooses to cooperate (`+1`) or deviate (`-1`).\n\n### Data / Model Specification\n\nThe stage-game payoffs are given in Table 1.\n\n**Table 1: Stage-Game Payoffs (Player 1, Player 2)**\n\n| Player 1 | Player 2: Out | Player 2: In |\n|:---|:---:|:---:|\n| **+1** | `u`, 0 | `ū`, 1 |\n| **-1** | `u`, 0 | `ū + g`, -1 |\n\nwhere `ū > u` and `g > 0`.\n\n### Question\n\nIn the one-shot stage game described in Table 1, what is the unique Nash Equilibrium outcome and the key strategic reason for it?", "model_evaluation_result": {"model_raw_response": "I need to analyze this one-shot stage game to find the unique Nash Equilibrium.\n\nLet me examine the payoff matrix:\n\n**Table 1: Stage-Game Payoffs (Player 1, Player 2)**\n| Player 1 | Player 2: Out | Player 2: In |\n|:---|:---:|:---:|\n| **+1** | `u`, 0 | `ū`, 1 |\n| **-1** | `u`, 0 | `ū + g`, -1 |\n\nGiven: `ū > u` and `g > 0`\n\nLet me analyze each player's optimal strategies:\n\n**Player 1's perspective:**\n- If Player 2 plays Out: Player 1 gets `u` regardless of choosing +1 or -1\n- If Player 2 plays In: Player 1 gets `ū` with +1, or `ū + g` with -1\n\nSince `g > 0`, we have `ū + g > ū`. Therefore, if Player 2 enters, Player 1 prefers to play -1.\n\n**Player 2's perspective:**\n- If Player 2 plays Out: Player 2 gets 0 regardless of Player 1's action\n- If Player 2 plays In and Player 1 plays +1: Player 2 gets 1\n- If Player 2 plays In and Player 1 plays -1: Player 2 gets -1\n\nNow, Player 2 must anticipate Player 1's rational response. If Player 2 enters, Player 1 will rationally choose -1 (since `ū + g > ū`), giving Player 2 a payoff of -1.\n\nComparing Player 2's options:\n- Out: payoff = 0\n- In: payoff = -1 (since Player 1 will choose -1)\n\nSince 0 > -1, Player 2's optimal strategy is Out.\n\nGiven that Player 2 will play Out, Player 1 is indifferent between +1 and -1 (both give payoff `u`). However, the equilibrium outcome is determined by Player 2's rational decision to stay Out.", "model_answer": "I need to analyze this one-shot stage game to find the unique Nash Equilibrium.\n\nLet me examine the payoff matrix:\n\n**Table 1: Stage-Game Payoffs (Player 1, Player 2)**\n| Player 1 | Player 2: Out | Player 2: In |\n|:---|:---:|:---:|\n| **+1** | `u`, 0 | `ū`, 1 |\n| **-1** | `u`, 0 | `ū + g`, -1 |\n\nGiven: `ū > u` and `g > 0`\n\nLet me analyze each player's optimal strategies:\n\n**Player 1's perspective:**\n- If Player 2 plays Out: Player 1 gets `u` regardless of choosing +1 or -1\n- If Player 2 plays In: Player 1 gets `ū` with +1, or `ū + g` with -1\n\nSince `g > 0`, we have `ū + g > ū`. Therefore, if Player 2 enters, Player 1 prefers to play -1.\n\n**Player 2's perspective:**\n- If Player 2 plays Out: Player 2 gets 0 regardless of Player 1's action\n- If Player 2 plays In and Player 1 plays +1: Player 2 gets 1\n- If Player 2 plays In and Player 1 plays -1: Player 2 gets -1\n\nNow, Player 2 must anticipate Player 1's rational response. If Player 2 enters, Player 1 will rationally choose -1 (since `ū + g > ū`), giving Player 2 a payoff of -1.\n\nComparing Player 2's options:\n- Out: payoff = 0\n- In: payoff = -1 (since Player 1 will choose -1)\n\nSince 0 > -1, Player 2's optimal strategy is Out.\n\nGiven that Player 2 will play Out, Player 1 is indifferent between +1 and -1 (both give payoff `u`). However, the equilibrium outcome is determined by Player 2's rational decision to stay Out.", "error": "Warning: Could not extract single choice letter from: I need to analyze this one-shot stage game to find..."}}
{"ID": 188, "Question": "### Background\n\nAfter establishing that large horizontal RJV networks are associated with declining market shares, the paper seeks to translate this finding into a practical policy tool by estimating a specific network size threshold above which collusion appears to dominate.\n\n### Data / Model Specification\n\nTo estimate a continuous threshold, the authors specify a quadratic relationship between the horizontal network measure ($RN_{imt}$) and market share ($MS_{imt}$):\n\n```latex\nM S_{i m t}=\\alpha_{0}+\\alpha_{1}M S_{i m t-1}+ B_1 RN_{imt} + B_2 RN_{imt}^2 + ... + \\varepsilon_{i m t} \\quad \\text{(Eq. (1))}\n```\n\nwhere $RN_{imt}$ is the fraction of competitors firm $i$ is linked to, and $B_1$ and $B_2$ are the cumulative coefficients.\n\n### Question\n\nThe paper's results indicate that the effect of network size on market share is positive for small networks but becomes negative for networks larger than a critical threshold of $K^*=0.18$. What does this finding imply about the estimated cumulative coefficients $B_1$ (on $RN_{imt}$) and $B_2$ (on $RN_{imt}^2$)?", "Options": {"A": "$B_1$ must be positive and $B_2$ must be negative.", "B": "$B_1$ must be negative and $B_2$ must be positive.", "C": "Both $B_1$ and $B_2$ must be negative.", "D": "Both $B_1$ and $B_2$ must be positive."}, "Answer": "A", "pi_justification": "This item tests the understanding of how a quadratic functional form translates into a non-linear economic relationship (an inverted U-shape). The distractors represent common mathematical errors in interpreting polynomial models, such as reversing the signs for a U-shaped curve. Strategy: Reverse-Reasoning.", "qid": "188", "question": "### Background\n\nAfter establishing that large horizontal RJV networks are associated with declining market shares, the paper seeks to translate this finding into a practical policy tool by estimating a specific network size threshold above which collusion appears to dominate.\n\n### Data / Model Specification\n\nTo estimate a continuous threshold, the authors specify a quadratic relationship between the horizontal network measure ($RN_{imt}$) and market share ($MS_{imt}$):\n\n```latex\nM S_{i m t}=\\alpha_{0}+\\alpha_{1}M S_{i m t-1}+ B_1 RN_{imt} + B_2 RN_{imt}^2 + ... + \\varepsilon_{i m t} \\quad \\text{(Eq. (1))}\n```\n\nwhere $RN_{imt}$ is the fraction of competitors firm $i$ is linked to, and $B_1$ and $B_2$ are the cumulative coefficients.\n\n### Question\n\nThe paper's results indicate that the effect of network size on market share is positive for small networks but becomes negative for networks larger than a critical threshold of $K^*=0.18$. What does this finding imply about the estimated cumulative coefficients $B_1$ (on $RN_{imt}$) and $B_2$ (on $RN_{imt}^2$)?", "model_evaluation_result": {"model_raw_response": "Looking at this quadratic relationship, I need to determine what the coefficients must be given the described pattern.\n\nThe relationship is: $MS_{imt} = \\alpha_0 + \\alpha_1 MS_{imt-1} + B_1 RN_{imt} + B_2 RN_{imt}^2 + ... + \\varepsilon_{imt}$\n\nThe key information is:\n- Effect is positive for small networks (network size increases market share)\n- Effect becomes negative for networks larger than threshold $K^* = 0.18$\n- This describes an inverted U-shape relationship\n\nFor a quadratic function $f(x) = B_1 x + B_2 x^2$:\n\nThe derivative is: $f'(x) = B_1 + 2B_2 x$\n\nGiven the pattern:\n- At small values of $RN_{imt}$ (near 0), the effect should be positive\n- At the threshold $K^* = 0.18$, the effect changes from positive to negative\n- For values above 0.18, the effect should be negative\n\nThis means:\n- The derivative starts positive when $RN_{imt}$ is small\n- The derivative becomes zero at $K^* = 0.18$ \n- The derivative becomes negative for $RN_{imt} > 0.18$\n\nFor this to occur:\n- $B_1$ must be positive (so the derivative starts positive when $RN_{imt} = 0$)\n- $B_2$ must be negative (so the derivative decreases as $RN_{imt}$ increases, eventually becoming negative)\n\nThis creates the inverted U-shape where the relationship is initially positive but turns negative after the threshold.\n\nA", "model_answer": "A", "error": null}}
{"ID": 144, "Question": "### Background\n\n**Research Question.** This problem examines the critical assumptions required to identify the causal return to education using family background variables as instruments, particularly highlighting the different identification challenges posed by the Family and Individual models.\n\n**Setting.** The identification strategy relies on using a vector of family background variables, `Z_if`, as instruments for education. The validity of these instruments depends on their relationship with unobserved parental endowments (`ε_f`) and child-specific idiosyncratic endowments (`ξ_if`).\n\n### Data / Model Specification\n\nThe paper models a child's total earnings endowment (`ε_if`) and the family background variables (`Z_if`) as follows:\n```latex\n\\ln(\\varepsilon_{if}) = \\rho \\ln(\\varepsilon_f) + \\xi_{if} \\quad \\text{(Eq. 1)}\n```\n```latex\n\\mathbf{Z}_{if} = \\Pi \\ln(\\varepsilon_f) + \\xi_f + \\kappa_1 \\xi_{if} + \\kappa_2 \\xi_{kf} \\quad \\text{(Eq. 2)}\n```\nwhere `ρ` is the intergenerational correlation of endowments, `Π` links `Z` to parental endowment, and `κ_1` and `κ_2` link `Z` to the idiosyncratic endowments of child `i` and their sibling `k`.\n\nThe paper argues that a sibling-difference estimator can identify the return to education in both models under the single key assumption of \"no Z-response to children's idiosyncratic endowments,\" which means `κ_1 = κ_2`.\n\n### Question\n\nConsider the following potential family background variables (`Z_if`). Which pair correctly identifies one variable for which the `κ_1 = κ_2` assumption is highly plausible, and one for which it is likely violated?", "Options": {"A": "Plausible: Mother's education. Violated: Whether the child ever lived in a single-parent family.", "B": "Plausible: Whether the child ever lived in a single-parent family. Violated: Mother's education.", "C": "Plausible: Parents' income when the child was 16. Violated: Mother's age at the child's birth.", "D": "Plausible: Number of siblings. Violated: Parents' house value when the child was 16."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to apply a key abstract identification assumption to concrete, real-world variables. It uses a Scenario Application strategy. The correct answer (A) provides the clearest contrast: mother's education is pre-determined and cannot respond to a child's specific endowment, while family structure can change in response to a child's characteristics (e.g., a difficult child straining a marriage). Distractor (B) is a direct reversal of the correct logic. Distractors (C) and (D) present less clear-cut cases or pairs where both variables are likely to satisfy (or violate) the assumption, making them weaker choices.", "qid": "144", "question": "### Background\n\n**Research Question.** This problem examines the critical assumptions required to identify the causal return to education using family background variables as instruments, particularly highlighting the different identification challenges posed by the Family and Individual models.\n\n**Setting.** The identification strategy relies on using a vector of family background variables, `Z_if`, as instruments for education. The validity of these instruments depends on their relationship with unobserved parental endowments (`ε_f`) and child-specific idiosyncratic endowments (`ξ_if`).\n\n### Data / Model Specification\n\nThe paper models a child's total earnings endowment (`ε_if`) and the family background variables (`Z_if`) as follows:\n```latex\n\\ln(\\varepsilon_{if}) = \\rho \\ln(\\varepsilon_f) + \\xi_{if} \\quad \\text{(Eq. 1)}\n```\n```latex\n\\mathbf{Z}_{if} = \\Pi \\ln(\\varepsilon_f) + \\xi_f + \\kappa_1 \\xi_{if} + \\kappa_2 \\xi_{kf} \\quad \\text{(Eq. 2)}\n```\nwhere `ρ` is the intergenerational correlation of endowments, `Π` links `Z` to parental endowment, and `κ_1` and `κ_2` link `Z` to the idiosyncratic endowments of child `i` and their sibling `k`.\n\nThe paper argues that a sibling-difference estimator can identify the return to education in both models under the single key assumption of \"no Z-response to children's idiosyncratic endowments,\" which means `κ_1 = κ_2`.\n\n### Question\n\nConsider the following potential family background variables (`Z_if`). Which pair correctly identifies one variable for which the `κ_1 = κ_2` assumption is highly plausible, and one for which it is likely violated?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand what the assumption κ₁ = κ₂ means and when it would be plausible versus violated.\n\nThe assumption κ₁ = κ₂ means that family background variables (Z_if) respond equally to both siblings' idiosyncratic endowments. In other words, the family background measure is not differentially influenced by one child's specific characteristics compared to their sibling's.\n\nLet me analyze each variable:\n\n**Mother's education**: This is fixed before children are born and cannot change in response to either child's idiosyncratic endowments. κ₁ = κ₂ = 0, so the assumption holds.\n\n**Whether the child ever lived in a single-parent family**: This could be influenced by a specific child's behavior or needs (e.g., if one child has behavioral problems that strain the marriage). This would violate κ₁ = κ₂ since it could respond differently to different children's idiosyncratic characteristics.\n\n**Parents' income when child was 16**: This is measured at a specific point in time for each child and could be influenced by that particular child's needs, achievements, or problems at that age. This would likely violate κ₁ = κ₂.\n\n**Number of siblings**: This is a family-level characteristic that doesn't vary between siblings within the same family, so κ₁ = κ₂ would hold.\n\n**Mother's age at child's birth**: This is fixed for each child at birth and cannot respond to the child's later idiosyncratic endowments, so κ₁ = κ₂ = 0.\n\n**Parents' house value when child was 16**: Similar to income, this could be influenced by the specific child's circumstances at that time, violating κ₁ = κ₂.\n\nLooking at the options:\n\nA) Plausible: Mother's education (✓ - fixed before birth)\n   Violated: Whether child lived in single-parent family (✓ - could respond to specific child's characteristics)\n\nThis pairing makes the most sense theoretically.\n\nA", "model_answer": "A", "error": null}}
{"ID": 204, "Question": "### Background\n\n**Research Question.** What are the market characteristics and strategic considerations for authors and publishers in different academic fields?\n\n**Setting / Institutional Environment.** The study surveys authors from Economics departments and Business schools, revealing significant differences in their publication patterns and market outcomes. Economics faculty tend to write more specialized, technical books for a smaller academic audience, while Business school faculty write more textbooks aimed at larger student and professional markets. These differences are reflected in the sales distributions for books from each field.\n\n**Variables & Parameters.**\n*   `Book Type`: Classification of publications (e.g., `Textbook`, `Technical or specialized book`).\n*   `Sales Volume`: The total number of copies sold, grouped into categories.\n*   `Publisher's Profit`: The net revenue a publisher earns from a book.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Publication and Sales Profiles by Academic Field**\n\n| Metric                               | Economics Departments | Business Schools |\n|--------------------------------------|-----------------------|------------------|\n| **Book Type Distribution**           |                       |                  |\n| % Textbooks*                         | 43.3%                 | 60.8%            |\n| % Technical or specialized books     | 35.3%                 | 22.8%            |\n| **Sales Volume Distribution**        |                       |                  |\n| % of books selling 1-2,000 copies    | 46%                   | 25%              |\n| % of books selling 2,001-5,000 copies| 22%                   | 24%              |\n| % of books selling 5,001-20,000 copies| 20%                   | 29%              |\n| % of books selling over 20,000 copies| 8%                    | 16%              |\n\n*Textbooks are defined as Freshman, Advanced undergraduate, and Graduate texts combined.*\n\nTo calculate expected sales, use the midpoint of each sales range from **Table 1** (use 1,500 for 1-2k; 3,500 for 2k-5k; 12,500 for 5k-20k; and 30,000 for >20k).\n\n---\n\n### Question\n\nA risk-averse publisher must choose between a typical Economics department project and a typical Business School project. The publisher defines 'commercial failure' as selling 5,000 or fewer copies. Based on the data in **Table 1**, which of the following statements is the most accurate strategic conclusion?", "Options": {"A": "The Economics project is superior for a risk-averse publisher because its lower sales potential implies a smaller potential loss.", "B": "The Business School project is superior because it has higher expected sales, despite having a higher risk of commercial failure.", "C": "The Business School project is superior because it has both a higher expected sales volume and a lower probability of commercial failure.", "D": "The Economics project is superior because it has both a higher expected sales volume and a lower probability of commercial failure."}, "Answer": "C", "pi_justification": "This item assesses the ability to perform expected value calculations and risk analysis using distributional data. It uses a Computational Judgment strategy. The correct option requires the student to calculate both expected sales and the probability of failure, and then recognize that one option dominates the other on both metrics. Distractors are designed based on common errors: misinterpreting risk aversion (A), partial calculation (B), and significant calculation errors leading to the opposite conclusion (D).", "qid": "204", "question": "### Background\n\n**Research Question.** What are the market characteristics and strategic considerations for authors and publishers in different academic fields?\n\n**Setting / Institutional Environment.** The study surveys authors from Economics departments and Business schools, revealing significant differences in their publication patterns and market outcomes. Economics faculty tend to write more specialized, technical books for a smaller academic audience, while Business school faculty write more textbooks aimed at larger student and professional markets. These differences are reflected in the sales distributions for books from each field.\n\n**Variables & Parameters.**\n*   `Book Type`: Classification of publications (e.g., `Textbook`, `Technical or specialized book`).\n*   `Sales Volume`: The total number of copies sold, grouped into categories.\n*   `Publisher's Profit`: The net revenue a publisher earns from a book.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Publication and Sales Profiles by Academic Field**\n\n| Metric                               | Economics Departments | Business Schools |\n|--------------------------------------|-----------------------|------------------|\n| **Book Type Distribution**           |                       |                  |\n| % Textbooks*                         | 43.3%                 | 60.8%            |\n| % Technical or specialized books     | 35.3%                 | 22.8%            |\n| **Sales Volume Distribution**        |                       |                  |\n| % of books selling 1-2,000 copies    | 46%                   | 25%              |\n| % of books selling 2,001-5,000 copies| 22%                   | 24%              |\n| % of books selling 5,001-20,000 copies| 20%                   | 29%              |\n| % of books selling over 20,000 copies| 8%                    | 16%              |\n\n*Textbooks are defined as Freshman, Advanced undergraduate, and Graduate texts combined.*\n\nTo calculate expected sales, use the midpoint of each sales range from **Table 1** (use 1,500 for 1-2k; 3,500 for 2k-5k; 12,500 for 5k-20k; and 30,000 for >20k).\n\n---\n\n### Question\n\nA risk-averse publisher must choose between a typical Economics department project and a typical Business School project. The publisher defines 'commercial failure' as selling 5,000 or fewer copies. Based on the data in **Table 1**, which of the following statements is the most accurate strategic conclusion?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to calculate two key metrics for each field:\n1. Expected sales volume\n2. Probability of commercial failure (≤5,000 copies)\n\n**Expected Sales Calculation:**\n\nFor Economics Departments:\n- 46% × 1,500 + 22% × 3,500 + 20% × 12,500 + 8% × 30,000\n- = 690 + 770 + 2,500 + 2,400 = 6,360 copies\n\nFor Business Schools:\n- 25% × 1,500 + 24% × 3,500 + 29% × 12,500 + 16% × 30,000\n- = 375 + 840 + 3,625 + 4,800 = 9,640 copies\n\n**Probability of Commercial Failure (≤5,000 copies):**\n\nCommercial failure includes books selling 1-2,000 copies AND 2,001-5,000 copies.\n\nEconomics Departments: 46% + 22% = 68%\nBusiness Schools: 25% + 24% = 49%\n\n**Summary:**\n- Business Schools have higher expected sales (9,640 vs 6,360)\n- Business Schools have lower probability of commercial failure (49% vs 68%)\n\nFor a risk-averse publisher, the Business School project is superior on both dimensions: it offers higher expected returns AND lower risk of failure.\n\nC", "model_answer": "C", "error": null}}
{"ID": 169, "Question": "### Background\n\n**Research Question.** This problem deconstructs the econometric methodology used to classify firms as 'strategic' or 'confused' based on their VAT reporting behavior. The core challenge is to attribute responsibility for a reporting discrepancy observed between a buyer and a seller to each firm's intrinsic reporting tendency.\n\n**Setting and Sample.** The analysis uses monthly, transaction-level VAT data for firm-pairs in Uganda. The methodology, inspired by Abowd et al. (1999), is applied to the largest \"connected set\" of trading firms, where firms are linked directly or indirectly through their trading partners.\n\n---\n\n### Data / Model Specification\n\nThe methodology proceeds in two main steps:\n\n1.  **Define and Decompose Discrepancy:** The discrepancy between a buyer `f` and seller `f'` in month `t` is defined as the difference between the VAT reported by the buyer and the seller:\n    ```latex\n    d_{f f't} \\equiv Y_{f f't}^{B} - Y_{f f't}^{S} \\quad \\text{(Eq. (1))}\n    ```\n    A positive `d` implies a seller shortfall (liability-reducing), while a negative `d` implies a buyer shortfall (liability-increasing). This discrepancy is then decomposed using a two-way fixed-effects model:\n    ```latex\n    d_{f f't} = \\delta_{c} + \\delta_{f}^{b} + \\delta_{f'}^{s} + \\delta_{t} + r_{f f't} \\quad \\text{(Eq. (2))}\n    ```\n    where `\\delta_f^b` is a buyer fixed effect for firm `f`, `\\delta_{f'}^s` is a seller fixed effect for firm `f'`, and `\\delta_t` is a month fixed effect.\n\n2.  **Construct Firm-Level Classification Statistic:** The estimated fixed effects for a given firm `f` are aggregated into a single statistic, `Q_f`, which summarizes its overall misreporting behavior:\n    ```latex\n    Q_{f} \\equiv w_{s} \\cdot \\hat{\\delta}_{f}^{s} + w_{b} \\cdot \\hat{\\delta}_{f}^{b} \\quad \\text{(Eq. (3))}\n    ```\n    where `w_s` and `w_b` are the number of observations for firm `f` as a seller and buyer, respectively. A firm is classified as 'advantageous' if `Q_f > 0` and 'disadvantageous' if `Q_f < 0`.\n\n---\n\nThe model's validity rests on the assumption that unobserved factors in a transaction are uncorrelated with the firm fixed effects. Suppose this assumption is violated because of 'assortative matching,' where advantageous misreporters (who tend to have positive `\\delta^b` and `\\delta^s`) systematically prefer to trade with each other. How would this likely bias the results?", "Options": {"A": "It would exaggerate the estimated dispersion of firm types, making advantageous firms appear more strategic than they truly are.", "B": "It would attenuate the estimated dispersion of firm types, biasing all fixed effects toward zero.", "C": "It would bias the estimates for advantageous firms but not for disadvantageous firms.", "D": "It would have no biasing effect on the fixed effects but would increase their standard errors."}, "Answer": "A", "pi_justification": "This item assesses a sophisticated critique of the model's identifying assumption, using a Scenario Application strategy. The correct answer requires reasoning through the consequences of omitted variable bias in this specific context. Distractors represent plausible but incorrect conclusions: (B) is the opposite effect (attenuation bias); (C) incorrectly assumes the bias is isolated; (D) confuses the effect of sorting bias with the effect of non-constant error variance.", "qid": "169", "question": "### Background\n\n**Research Question.** This problem deconstructs the econometric methodology used to classify firms as 'strategic' or 'confused' based on their VAT reporting behavior. The core challenge is to attribute responsibility for a reporting discrepancy observed between a buyer and a seller to each firm's intrinsic reporting tendency.\n\n**Setting and Sample.** The analysis uses monthly, transaction-level VAT data for firm-pairs in Uganda. The methodology, inspired by Abowd et al. (1999), is applied to the largest \"connected set\" of trading firms, where firms are linked directly or indirectly through their trading partners.\n\n---\n\n### Data / Model Specification\n\nThe methodology proceeds in two main steps:\n\n1.  **Define and Decompose Discrepancy:** The discrepancy between a buyer `f` and seller `f'` in month `t` is defined as the difference between the VAT reported by the buyer and the seller:\n    ```latex\n    d_{f f't} \\equiv Y_{f f't}^{B} - Y_{f f't}^{S} \\quad \\text{(Eq. (1))}\n    ```\n    A positive `d` implies a seller shortfall (liability-reducing), while a negative `d` implies a buyer shortfall (liability-increasing). This discrepancy is then decomposed using a two-way fixed-effects model:\n    ```latex\n    d_{f f't} = \\delta_{c} + \\delta_{f}^{b} + \\delta_{f'}^{s} + \\delta_{t} + r_{f f't} \\quad \\text{(Eq. (2))}\n    ```\n    where `\\delta_f^b` is a buyer fixed effect for firm `f`, `\\delta_{f'}^s` is a seller fixed effect for firm `f'`, and `\\delta_t` is a month fixed effect.\n\n2.  **Construct Firm-Level Classification Statistic:** The estimated fixed effects for a given firm `f` are aggregated into a single statistic, `Q_f`, which summarizes its overall misreporting behavior:\n    ```latex\n    Q_{f} \\equiv w_{s} \\cdot \\hat{\\delta}_{f}^{s} + w_{b} \\cdot \\hat{\\delta}_{f}^{b} \\quad \\text{(Eq. (3))}\n    ```\n    where `w_s` and `w_b` are the number of observations for firm `f` as a seller and buyer, respectively. A firm is classified as 'advantageous' if `Q_f > 0` and 'disadvantageous' if `Q_f < 0`.\n\n---\n\nThe model's validity rests on the assumption that unobserved factors in a transaction are uncorrelated with the firm fixed effects. Suppose this assumption is violated because of 'assortative matching,' where advantageous misreporters (who tend to have positive `\\delta^b` and `\\delta^s`) systematically prefer to trade with each other. How would this likely bias the results?", "model_evaluation_result": {"model_raw_response": "Looking at this econometric problem, I need to analyze how assortative matching would bias the two-way fixed effects estimation.\n\nThe key insight is understanding what happens when advantageous misreporters (positive δ^b and δ^s) systematically trade with each other.\n\nIn the model:\nd_{ff't} = δ_c + δ_f^b + δ_{f'}^s + δ_t + r_{ff't}\n\nThe identifying assumption is that the residual r_{ff't} is uncorrelated with the fixed effects. However, with assortative matching:\n\n- When two advantageous firms trade (both have positive δ^b and δ^s), their natural tendency would be for the buyer to over-report and the seller to under-report, creating a large positive discrepancy\n- But the model tries to explain this large discrepancy using only the sum δ_f^b + δ_{f'}^s\n- Since these firms are systematically trading together, the model cannot fully separate their individual effects from their joint matching behavior\n\nThis creates a classic attenuation bias problem. The fixed effects estimator will underestimate the true magnitude of individual firm effects because it cannot distinguish between:\n1. A firm's intrinsic misreporting tendency\n2. The additional discrepancy created by strategic matching with similar firms\n\nThe systematic correlation between trading partners means the model attributes some of each firm's true effect to the other firm, pulling both estimates toward zero.\n\nThis affects the entire distribution of firm types - both advantageous and disadvantageous firms - because the fixed effects are estimated jointly and the normalization constraints spread the bias across all firms.\n\nB", "model_answer": "A", "error": null}}
{"ID": 279, "Question": "### Background\n\n**Research Question.** This problem investigates the extent to which short-run estimates of the Elasticity of Taxable Income (ETI) are biased by temporary intertemporal income shifting around a tax reform.\n\n**Setting / Institutional Environment.** The ETI is a key parameter measuring the responsiveness of reported income to changes in the net-of-tax rate. It is often estimated using tax reforms as natural experiments. This analysis uses a difference-in-differences (DiD) regression on monthly Danish panel data from 2009-2010 to estimate the ETI from a reform that took effect on Jan 1, 2010. The core strategy is to re-estimate the ETI on samples that systematically exclude the months immediately surrounding the reform to isolate the contribution of income shifting.\n\n**Variables & Parameters.**\n- `ETI`: The Elasticity of Taxable Income.\n- `w_{y,m,i}`: Normalized percentage change in monthly wage.\n- `d_y,i^2010`: Year dummy (1 if 2010).\n- `d_i^T`: Treatment group dummy.\n- `τ_{y,i}`: Marginal tax rate.\n\n---\n\n### Data / Model Specification\n\nThe DiD regression model used to estimate the ETI is:\n```latex\nw_{y,m,i} = \\beta_{0} + \\beta_{1}d_{y,i}^{2010} + \\beta_{2}d_{i}^{T} + \\beta_{3} \\frac{1-\\tau_{y,i}}{1-\\tau_{2009,i}} + \\varepsilon_{y,m,i} \n\\quad \\text{(Eq. (1))}\n```\nThe coefficient `β₃` is the ETI estimate. It can be shown that `β₃` is the ratio of the DiD change in income to the DiD change in the net-of-tax rate.\n\n**Table 1: Importance of Shifting for ETI Estimates**\n\n| Income Group | All months (1) | Excl. D09 & J10 (2) | Excl. N09, D09, & J10 (3) |\n| :--- | :--- | :--- | :--- |\n| Full sample | 0.10 (0.08, 0.11) | 0.03 (0.01, 0.04) | 0.01 (-0.00, 0.03) |\n| P99 < income | 0.26 (0.21, 0.31) | -0.00 (-0.06, 0.05) | -0.06 (-0.11, -0.01) |\n\n*Note: The table reports ETI estimates and 95% confidence intervals. The estimation sample in column (1) includes all months from Jan 2009 to Dec 2010. Columns (2) and (3) exclude the indicated months.*\n\n---\n\n### The Question\n\nA policymaker uses a short-run ETI estimate of 0.25, derived from a similar pre-announced tax reform, to calculate the deadweight loss of a new tax. Based on the findings in Table 1, what is the most significant risk of using this ETI estimate?", "Options": {"A": "The estimate is likely severely upwardly biased because it primarily captures temporary tax avoidance (income shifting) rather than the real, long-run behavioral changes relevant for deadweight loss calculations.", "B": "The estimate is likely an underestimate of the true long-run ETI, as people take time to adjust their behavior fully.", "C": "The findings imply that the true long-run ETI is zero, meaning the tax has no deadweight loss.", "D": "The estimate is only valid for the top 1% of earners and cannot be applied to the general population."}, "Answer": "A", "pi_justification": "This item tests the user's understanding of the paper's main policy and methodological implication using a Scenario Application strategy. It requires applying the specific finding about shifting bias to a general policy context. The correct option (A) identifies the upward bias from confounding shifting with real responses. Distractors are based on high-fidelity errors: Conceptual Opposite (B: claiming downward bias), a Classic Misconception (C: conflating a zero short-run response with a zero long-run ETI), and a Misinterpretation (D) that focuses on a secondary issue (heterogeneity) over the primary one (identification bias).", "qid": "279", "question": "### Background\n\n**Research Question.** This problem investigates the extent to which short-run estimates of the Elasticity of Taxable Income (ETI) are biased by temporary intertemporal income shifting around a tax reform.\n\n**Setting / Institutional Environment.** The ETI is a key parameter measuring the responsiveness of reported income to changes in the net-of-tax rate. It is often estimated using tax reforms as natural experiments. This analysis uses a difference-in-differences (DiD) regression on monthly Danish panel data from 2009-2010 to estimate the ETI from a reform that took effect on Jan 1, 2010. The core strategy is to re-estimate the ETI on samples that systematically exclude the months immediately surrounding the reform to isolate the contribution of income shifting.\n\n**Variables & Parameters.**\n- `ETI`: The Elasticity of Taxable Income.\n- `w_{y,m,i}`: Normalized percentage change in monthly wage.\n- `d_y,i^2010`: Year dummy (1 if 2010).\n- `d_i^T`: Treatment group dummy.\n- `τ_{y,i}`: Marginal tax rate.\n\n---\n\n### Data / Model Specification\n\nThe DiD regression model used to estimate the ETI is:\n```latex\nw_{y,m,i} = \\beta_{0} + \\beta_{1}d_{y,i}^{2010} + \\beta_{2}d_{i}^{T} + \\beta_{3} \\frac{1-\\tau_{y,i}}{1-\\tau_{2009,i}} + \\varepsilon_{y,m,i} \n\\quad \\text{(Eq. (1))}\n```\nThe coefficient `β₃` is the ETI estimate. It can be shown that `β₃` is the ratio of the DiD change in income to the DiD change in the net-of-tax rate.\n\n**Table 1: Importance of Shifting for ETI Estimates**\n\n| Income Group | All months (1) | Excl. D09 & J10 (2) | Excl. N09, D09, & J10 (3) |\n| :--- | :--- | :--- | :--- |\n| Full sample | 0.10 (0.08, 0.11) | 0.03 (0.01, 0.04) | 0.01 (-0.00, 0.03) |\n| P99 < income | 0.26 (0.21, 0.31) | -0.00 (-0.06, 0.05) | -0.06 (-0.11, -0.01) |\n\n*Note: The table reports ETI estimates and 95% confidence intervals. The estimation sample in column (1) includes all months from Jan 2009 to Dec 2010. Columns (2) and (3) exclude the indicated months.*\n\n---\n\n### The Question\n\nA policymaker uses a short-run ETI estimate of 0.25, derived from a similar pre-announced tax reform, to calculate the deadweight loss of a new tax. Based on the findings in Table 1, what is the most significant risk of using this ETI estimate?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 285, "Question": "### Background\n\nTheorem 1 of the paper states that the equilibrium payoff vector `d_N` from the sequential demand game is a core allocation. The proof relies on a chain of established theoretical results connecting the economic assumptions of the model to properties of the induced game.\n\n### Question\n\nWhich of the following correctly describes the logical argument that guarantees `d_N` is in the core?", "Options": {"A": "The convex production function `f` ensures the associated NTU game is *ordinal convex* (Lemma 4.1), and for such games, the vector `d_N` derived from backward induction is guaranteed to be in the core (Lemma 4.2).", "B": "The existence of a core (Scarf's result) implies that the backward induction process must select a point from it, which happens to be `d_N`.", "C": "The vector `d_N` is in the core because it is the unique subgame perfect equilibrium, and all subgame perfect equilibria in this type of game are, by definition, core allocations.", "D": "The associated NTU game is ordinal convex, which implies the production function `f` must be convex, and this convexity ensures that `d_N` is a stable outcome."}, "Answer": "A", "pi_justification": "This item tests the student's understanding of the theoretical underpinnings of Theorem 1. The task is to identify the correct logical chain of the proof. The correct option (A) correctly links the convex function to ordinal convexity (Moulin's result) and ordinal convexity to the core property of the backward induction vector (Ichiishi's result). Distractors represent common errors in theoretical reasoning: B uses a related but less precise argument, C makes a false general claim, and D reverses the causal logic between the economic assumption and the game-theoretic property.", "qid": "285", "question": "### Background\n\nTheorem 1 of the paper states that the equilibrium payoff vector `d_N` from the sequential demand game is a core allocation. The proof relies on a chain of established theoretical results connecting the economic assumptions of the model to properties of the induced game.\n\n### Question\n\nWhich of the following correctly describes the logical argument that guarantees `d_N` is in the core?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 199, "Question": "### Background\n\n**Research Question.** This problem examines the core theoretical mechanism of a macro model with a fragile banking system. It focuses on how banks, in maximizing returns for their financiers, endogenously determine their leverage and risk.\n\n**Setting.** The model features a relationship-lending bank that finances projects with stochastic returns. The bank is funded by deposits and capital. The demandable nature of deposits exposes the bank to runs. A bank manager chooses the capital structure (leverage) to maximize the combined expected payoff to depositors and capitalists.\n\n### Data / Model Specification\n\nThe return of a bank's project is `R_t^A + x_t`, where `x_t` is a random shock uniformly distributed on `[-h, h]`. The bank manager chooses the optimal deposit-to-asset ratio `d_t`:\n```latex\nd_{t}=\\frac{1}{R_{t}}\\frac{R_{t}^{A}+h}{2-\\lambda+c(1+\\lambda)} \\quad \\text{(Eq. 1)}\n```\nwhere `R_t` is the deposit rate, `R_t^A` is the expected project return, `λ` is the bank's special liquidation skill, and `c` is a resource loss in a run.\n\nBank risk is measured by the probability of a run, `φ_t`:\n```latex\n\\phi_{t} = \\frac{1}{2}\\left(1 - \\frac{R_{t}^{A}-R_{t}d_{t}}{h}\\right) \\quad \\text{(Eq. 2)}\n```\n### Question\n\nIn this model, banks endogenously adjust their leverage (`d_t`) in response to changes in the economic environment. What is the net effect of an increase in underlying project riskiness (`h`) on the equilibrium probability of a bank run (`φ_t`)?", "Options": {"A": "It increases, because banks endogenously increase their leverage in response to higher risk, and this effect reinforces the direct impact of higher risk.", "B": "It decreases, because for a fixed level of leverage, a wider distribution of project outcomes makes the specific run-triggering event less likely.", "C": "It has no net effect, because banks perfectly adjust their leverage to keep the probability of a run constant.", "D": "It decreases, because banks prudently reduce their leverage in response to a riskier environment, more than offsetting the direct impact of higher risk."}, "Answer": "A", "pi_justification": "This item tests the central theoretical result of the banking model using a Computational Judgment strategy. The correct answer requires the student to understand that the bank's endogenous leverage choice (risk-taking) dominates, leading to a positive relationship between project risk and bank risk. Distractors are designed based on common errors: B isolates a partial effect (ignoring endogeneity), D assumes the wrong behavioral response (risk aversion instead of risk-taking), and C posits a plausible but incorrect cancellation of effects.", "qid": "199", "question": "### Background\n\n**Research Question.** This problem examines the core theoretical mechanism of a macro model with a fragile banking system. It focuses on how banks, in maximizing returns for their financiers, endogenously determine their leverage and risk.\n\n**Setting.** The model features a relationship-lending bank that finances projects with stochastic returns. The bank is funded by deposits and capital. The demandable nature of deposits exposes the bank to runs. A bank manager chooses the capital structure (leverage) to maximize the combined expected payoff to depositors and capitalists.\n\n### Data / Model Specification\n\nThe return of a bank's project is `R_t^A + x_t`, where `x_t` is a random shock uniformly distributed on `[-h, h]`. The bank manager chooses the optimal deposit-to-asset ratio `d_t`:\n```latex\nd_{t}=\\frac{1}{R_{t}}\\frac{R_{t}^{A}+h}{2-\\lambda+c(1+\\lambda)} \\quad \\text{(Eq. 1)}\n```\nwhere `R_t` is the deposit rate, `R_t^A` is the expected project return, `λ` is the bank's special liquidation skill, and `c` is a resource loss in a run.\n\nBank risk is measured by the probability of a run, `φ_t`:\n```latex\n\\phi_{t} = \\frac{1}{2}\\left(1 - \\frac{R_{t}^{A}-R_{t}d_{t}}{h}\\right) \\quad \\text{(Eq. 2)}\n```\n### Question\n\nIn this model, banks endogenously adjust their leverage (`d_t`) in response to changes in the economic environment. What is the net effect of an increase in underlying project riskiness (`h`) on the equilibrium probability of a bank run (`φ_t`)?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze how an increase in project riskiness (h) affects the equilibrium probability of a bank run (φ_t) through both direct and indirect effects.\n\nFrom Equation 2, the probability of a run is:\nφ_t = (1/2)(1 - (R_t^A - R_t*d_t)/h)\n\nLet me examine how h affects φ_t:\n\n**Direct effect**: Higher h appears in the denominator, which would tend to decrease φ_t for fixed leverage.\n\n**Indirect effect**: From Equation 1, the optimal leverage d_t is:\nd_t = (1/R_t) * (R_t^A + h)/(2 - λ + c(1 + λ))\n\nThis shows that d_t increases with h - banks choose higher leverage when project riskiness increases.\n\n**Net effect analysis**: \nWhen h increases:\n1. Banks endogenously increase leverage (d_t increases)\n2. Higher leverage means R_t*d_t increases\n3. This makes (R_t^A - R_t*d_t) more negative\n4. Therefore (R_t^A - R_t*d_t)/h becomes more negative\n5. This makes φ_t = (1/2)(1 - (R_t^A - R_t*d_t)/h) larger\n\nThe key insight is that banks respond to higher underlying risk by taking on more leverage, which increases their fragility. This behavioral response amplifies rather than mitigates the impact of increased project riskiness.\n\nThe answer describes a situation where banks endogenously increase leverage in response to higher risk, and this behavioral response reinforces (rather than offsets) the direct impact of higher risk on run probability.\n\nA", "model_answer": "A", "error": null}}
{"ID": 208, "Question": "### Background\n\n**Research Question.** This problem examines the structure of the author's proposed alternative wage-price model.\n\n**Setting and Sample.** After demonstrating the instability of the Hines model, the author proposes a re-specified system of equations for the UK economy (1921-61).\n\n---\n\n### Data / Model Specification\n\nThe author's re-specified model takes the form:\n```latex\nΔT_t = c_0 + c_1 T_{t-1} + c_2 ΔP'_{t-1/2} + c_3 D_{t-1/2} + V_{3t} \\quad \\text{(Eq. 1)}\n```\n```latex\nΔW_t = a_0 + a_1 ΔT_t + a_2 T_t + a_3 ΔP'_{t-1/2} + V_{1t} \\quad \\text{(Eq. 2)}\n```\n```latex\nΔP'_t = b_0 + b_1 ΔW_t + b_2 ΔM_t + b_3 ΔX_t + V_{2t} \\quad \\text{(Eq. 3)}\n```\nThis system is closed with the identity `T_t = T_{t-1} + ΔT_t`.\n\n---\n\n### Question\n\nBased on the system of equations (Eq. 1 - Eq. 3), which of the following best describes the recursive, \"stepwise chain of causation\" for the endogenous variables within a single time period `t`?", "Options": {"A": "Predetermined variables first determine `ΔT_t`, which in turn determines `T_t`. Then `ΔW_t` is determined, and finally `ΔP'_t` is determined.", "B": "Predetermined variables first determine `ΔP'_t`, which then determines `ΔW_t`. Then `ΔT_t` and `T_t` are determined simultaneously.", "C": "The variables `ΔT_t`, `ΔW_t`, and `ΔP'_t` are all determined simultaneously by the predetermined variables in the system.", "D": "Predetermined variables first determine `ΔT_t`, which in turn determines `T_t`. Then `ΔP'_t` is determined, and finally `ΔW_t` is determined."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.3). This item assesses the core concept from part (a) of the original QA: understanding the model's recursive structure. The correct answer requires tracing the causal flow through the equations. The distractors are designed based on plausible misreadings of the system: Distractor B reverses the wage/price determination, Distractor C incorrectly assumes simultaneity (the property the recursive structure is designed to avoid), and Distractor D swaps the final two steps in the causal chain.", "qid": "208", "question": "### Background\n\n**Research Question.** This problem examines the structure of the author's proposed alternative wage-price model.\n\n**Setting and Sample.** After demonstrating the instability of the Hines model, the author proposes a re-specified system of equations for the UK economy (1921-61).\n\n---\n\n### Data / Model Specification\n\nThe author's re-specified model takes the form:\n```latex\nΔT_t = c_0 + c_1 T_{t-1} + c_2 ΔP'_{t-1/2} + c_3 D_{t-1/2} + V_{3t} \\quad \\text{(Eq. 1)}\n```\n```latex\nΔW_t = a_0 + a_1 ΔT_t + a_2 T_t + a_3 ΔP'_{t-1/2} + V_{1t} \\quad \\text{(Eq. 2)}\n```\n```latex\nΔP'_t = b_0 + b_1 ΔW_t + b_2 ΔM_t + b_3 ΔX_t + V_{2t} \\quad \\text{(Eq. 3)}\n```\nThis system is closed with the identity `T_t = T_{t-1} + ΔT_t`.\n\n---\n\n### Question\n\nBased on the system of equations (Eq. 1 - Eq. 3), which of the following best describes the recursive, \"stepwise chain of causation\" for the endogenous variables within a single time period `t`?", "model_evaluation_result": {"model_raw_response": "Looking at the system of equations, I need to trace the causal chain within period t by examining which variables appear as explanatory variables in each equation.\n\nLet me analyze the recursive structure:\n\n**Equation 1**: ΔT_t = c_0 + c_1 T_{t-1} + c_2 ΔP'_{t-1/2} + c_3 D_{t-1/2} + V_{3t}\n- ΔT_t depends only on predetermined variables (lagged values)\n- So ΔT_t is determined first\n\n**Identity**: T_t = T_{t-1} + ΔT_t\n- Once ΔT_t is determined, T_t is immediately determined\n\n**Equation 2**: ΔW_t = a_0 + a_1 ΔT_t + a_2 T_t + a_3 ΔP'_{t-1/2} + V_{1t}\n- ΔW_t depends on ΔT_t and T_t (both now determined) plus predetermined variables\n- So ΔW_t is determined second\n\n**Equation 3**: ΔP'_t = b_0 + b_1 ΔW_t + b_2 ΔM_t + b_3 ΔX_t + V_{2t}\n- ΔP'_t depends on ΔW_t (now determined) plus exogenous variables ΔM_t and ΔX_t\n- So ΔP'_t is determined last\n\nThe recursive chain is:\n1. Predetermined variables → ΔT_t\n2. ΔT_t → T_t (via identity)\n3. ΔT_t, T_t, and predetermined variables → ΔW_t\n4. ΔW_t and exogenous variables → ΔP'_t\n\nA", "model_answer": "A", "error": null}}
{"ID": 185, "Question": "### Background\n\nThis problem analyzes the baseline monetary model without credit, which serves as a benchmark for the paper's main analysis. In this simplified economy, agents are either producers or money holders. They meet randomly, and a trade occurs when a money holder meets a producer of their desired consumption good.\n\n### Data / Model Specification\n\nThe economy is characterized by:\n*   `V_m`, `V_p`: Stationary value functions for a money holder and a producer.\n*   `r`: Rate of time preference.\n*   `β`: Poisson arrival rate of trading partners.\n*   `M`: Proportion of money holders in the population, `M` ∈ (0, 1).\n*   `z`: Probability of a single coincidence of wants.\n*   `u(q)`: A strictly increasing and strictly concave utility function with `u(0)=0`, `u'(0)=∞`, and `u'(∞)=0`.\n\nThe equilibrium is described by the following Bellman equations and bargaining outcome:\n\n**Bellman Equations:**\n```latex\nrV_m = \\beta(1-M)z(V_p - V_m + u(q_m)) \\quad \\quad \\text{(Eq. 1)}\n```\n```latex\nrV_p = \\beta M z(V_m - V_p - q_m) \\quad \\quad \\text{(Eq. 2)}\n```\n**Bargaining Outcome (Take-it-or-leave-it offer by money holder):**\n```latex\nq_m = V_m - V_p \\quad \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nA key condition for a credit market to exist is that a debtor has an incentive to repay the IOU, which is formally `V_p - V_d - q_m ≥ 0`. In the context of this no-credit equilibrium (where `V_d = 0`), why is this condition violated?", "Options": {"A": "Because `V_p = 0`, the condition simplifies to `-q_m ≥ 0`, which is false since `q_m > 0`.", "B": "Because `V_p` is positive, but smaller than `q_m`.", "C": "Because money holders would refuse to trade with debtors, making `q_m` effectively infinite for them.", "D": "Because the utility gain `u(q_m)` is not part of the debtor's repayment surplus."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to test the internal consistency of the no-credit equilibrium. It requires the student to apply the equilibrium result (`V_p=0`) to an external condition (the incentive for credit) to see why it fails. The correct answer follows this direct logic. Distractor B is based on an incorrect premise. Distractors C and D introduce plausible but irrelevant or incorrect mechanisms, testing the student's ability to focus on the specific mathematical condition provided.", "qid": "185", "question": "### Background\n\nThis problem analyzes the baseline monetary model without credit, which serves as a benchmark for the paper's main analysis. In this simplified economy, agents are either producers or money holders. They meet randomly, and a trade occurs when a money holder meets a producer of their desired consumption good.\n\n### Data / Model Specification\n\nThe economy is characterized by:\n*   `V_m`, `V_p`: Stationary value functions for a money holder and a producer.\n*   `r`: Rate of time preference.\n*   `β`: Poisson arrival rate of trading partners.\n*   `M`: Proportion of money holders in the population, `M` ∈ (0, 1).\n*   `z`: Probability of a single coincidence of wants.\n*   `u(q)`: A strictly increasing and strictly concave utility function with `u(0)=0`, `u'(0)=∞`, and `u'(∞)=0`.\n\nThe equilibrium is described by the following Bellman equations and bargaining outcome:\n\n**Bellman Equations:**\n```latex\nrV_m = \\beta(1-M)z(V_p - V_m + u(q_m)) \\quad \\quad \\text{(Eq. 1)}\n```\n```latex\nrV_p = \\beta M z(V_m - V_p - q_m) \\quad \\quad \\text{(Eq. 2)}\n```\n**Bargaining Outcome (Take-it-or-leave-it offer by money holder):**\n```latex\nq_m = V_m - V_p \\quad \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nA key condition for a credit market to exist is that a debtor has an incentive to repay the IOU, which is formally `V_p - V_d - q_m ≥ 0`. In the context of this no-credit equilibrium (where `V_d = 0`), why is this condition violated?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze why the condition `V_p - V_d - q_m ≥ 0` is violated in the no-credit equilibrium.\n\nGiven information:\n- In the no-credit equilibrium, `V_d = 0` (no debtors exist)\n- The condition becomes: `V_p - 0 - q_m ≥ 0`, or `V_p ≥ q_m`\n- From the bargaining outcome (Eq. 3): `q_m = V_m - V_p`\n\nLet me substitute the bargaining outcome into the condition:\n`V_p ≥ q_m`\n`V_p ≥ V_m - V_p`\n`2V_p ≥ V_m`\n\nNow I need to determine the relationship between `V_p` and `V_m` using the Bellman equations.\n\nFrom equations (1) and (2):\n`rV_m = β(1-M)z(V_p - V_m + u(q_m))`\n`rV_p = βMz(V_m - V_p - q_m)`\n\nSince `q_m = V_m - V_p` and `u(q_m) > 0` (because `q_m > 0` and utility is positive), the first equation shows that `V_m` receives a positive flow payoff from the possibility of trading.\n\nThe second equation shows that `V_p` receives a negative flow payoff from `q_m` (the quantity given up) but a positive flow payoff from becoming a money holder.\n\nLooking at the structure, since money holders can consume when they find a producer, while producers must give up goods to become money holders, we expect `V_m > V_p` in equilibrium.\n\nThis means `2V_p < V_m`, so `V_p < q_m` (since `q_m = V_m - V_p`).\n\nTherefore, `V_p - q_m < 0`, violating the condition.\n\nThe answer is B) Because `V_p` is positive, but smaller than `q_m`.", "model_answer": "B", "error": null}}
{"ID": 287, "Question": "This problem deconstructs the paper's core identification strategy: a panel Instrumental Variables (IV) approach that is equivalent to a Difference-in-Differences (DD) estimator. The analysis compares two groups of hospitals over two years (1988 and 1989). The 'treatment group' consists of hospitals that opened a new catheterization lab in 1989 ('Open cath lab'). The 'control group' consists of hospitals that already had a catheterization lab in both years ('With cath lab'). The adoption of the technology serves as the instrument for patient-level treatment.\n\n**Table 1: Trends in Catheterization and Mortality Rates (in percent)**\n\n| Hospital Group | Year | 90-day Catheterization Rate | 30-day Mortality Rate |\n| :--- | :---: | :---: | :---: |\n| With cath lab (Control) | 1988 | 33.0 | 22.8 |\n| | 1989 | 34.5 | 22.2 |\n| Open cath lab (Treated) | 1988 | 20.0 | 23.3 |\n| | 1989 | 28.3 | 23.1 |\n\nThe validity of the Difference-in-Differences estimate relies on the parallel trends assumption. Which of the following is the correct statement of this assumption for the mortality outcome in this context?", "Options": {"A": "In the absence of the new cath labs, the mortality trend from 1988 to 1989 at the 'Open cath lab' hospitals would have been the same as the trend at the 'With cath lab' hospitals.", "B": "The mortality rates at 'Open cath lab' hospitals and 'With cath lab' hospitals should be the same before the new labs are opened in 1988.", "C": "The mortality rates at both types of hospitals should follow the same trend after 1989.", "D": "The number of patients treated at both types of hospitals should follow the same trend between 1988 and 1989."}, "Answer": "A", "pi_justification": "This item assesses the conceptual understanding of the key identifying assumption in a DD framework. The correct answer (A) correctly states the counterfactual nature of the assumption. Distractor B makes the classic error of confusing parallel trends with equal levels (Conceptual Opposite). Distractor C misstates the assumption by applying it to the post-treatment period instead of the counterfactual pre-to-post trend. Distractor D is True but Irrelevant, as it refers to patient volume, not the outcome variable.", "qid": "287", "question": "This problem deconstructs the paper's core identification strategy: a panel Instrumental Variables (IV) approach that is equivalent to a Difference-in-Differences (DD) estimator. The analysis compares two groups of hospitals over two years (1988 and 1989). The 'treatment group' consists of hospitals that opened a new catheterization lab in 1989 ('Open cath lab'). The 'control group' consists of hospitals that already had a catheterization lab in both years ('With cath lab'). The adoption of the technology serves as the instrument for patient-level treatment.\n\n**Table 1: Trends in Catheterization and Mortality Rates (in percent)**\n\n| Hospital Group | Year | 90-day Catheterization Rate | 30-day Mortality Rate |\n| :--- | :---: | :---: | :---: |\n| With cath lab (Control) | 1988 | 33.0 | 22.8 |\n| | 1989 | 34.5 | 22.2 |\n| Open cath lab (Treated) | 1988 | 20.0 | 23.3 |\n| | 1989 | 28.3 | 23.1 |\n\nThe validity of the Difference-in-Differences estimate relies on the parallel trends assumption. Which of the following is the correct statement of this assumption for the mortality outcome in this context?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 209, "Question": "## Background\n\n**Research Question.** This problem examines a method to distinguish the effect of adverse selection from a potential confounding \"scale effect\" in the pricing of thoroughbred yearlings.\n\n**Setting / Institutional Environment.** A potential critique of the main finding is that the `Racing Intensity` variable may be correlated with the overall scale of a seller's operation. If larger breeding operations have better reputations and command higher prices, the estimated coefficient on `Racing Intensity` could be biased. This analysis re-parameterizes the model to test this possibility directly.\n\n**Variables & Parameters.**\n- `ln(Price)`: Natural logarithm of the hammer price.\n- `R`: `Racing Starts`, the number of races the breeder started a horse in 1993.\n- `B`: `Breeding Starts + 1`, where `Breeding Starts` is the number of races started by horses bred by the seller.\n- `Race Zero`, `Breed Zero`: Indicator variables for sellers with zero racing or breeding starts, respectively.\n\n---\n\n## Data / Model Specification\n\nThe analysis compares a restricted model with an unrestricted version derived from an algebraic identity.\n\n**Restricted Model:**\n```latex\nln(Price_i) = \\alpha_0 + \\alpha_1 ln(R_i/B_i + 1) + \\gamma'X_i + \\epsilon_i\n```\n(Eq. 1)\n\n**Algebraic Identity:**\n```latex\nln(R/B + 1) = ln(R+B) - ln(B)\n```\n(Eq. 2)\n\n**Unrestricted Model:**\n```latex\nln(Price_i) = \\theta_0 + \\alpha_1^* ln(R_i+B_i) - \\beta^{**} ln(B_i) + \\gamma'X_i + \\nu_i\n```\n(Eq. 3)\n\n**Table 1: Selected Regression Coefficients for ln(Price)**\n| Specification | (3) Restricted | (4) Restricted | (5) Unrestricted | (6) Unrestricted |\n| :--- | :---: | :---: | :---: | :---: |\n| `Race/Breed Zero` Dummies | No | Yes | No | Yes |\n| `ln(R/B + 1)` | -0.1026 | -0.1404 | | |\n| `ln(R+B)` | | | -0.0638 | -0.1465 |\n| `-ln(B)` | | | -0.1200 | -0.1340 |\n| F-test (`\\alpha_1^* = \\beta^{**}`)| | | F = 9.58 | F = 0.068 |\n| Critical F (5%) | | | ~3.88 | ~3.88 |\n\n---\n\n## The Question\n\nBased on the models and results in Table 1, what is the correct linear restriction to test for the absence of scale effects, and what is the conclusion from the F-test comparing specifications (3) and (5)?", "Options": {"A": "Restriction: `α₁* = β**`. Conclusion: Reject the null; the restricted model (3) is misspecified because significant scale effects are present.", "B": "Restriction: `α₁* = -β**`. Conclusion: Reject the null; the restricted model (3) is misspecified because significant scale effects are present.", "C": "Restriction: `α₁* = β**`. Conclusion: Fail to reject the null; the restricted model (3) is valid and there are no scale effects.", "D": "Restriction: `α₁* = β**`. Conclusion: Reject the null; this directly confirms the presence of adverse selection in model (3)."}, "Answer": "A", "pi_justification": "This item assesses the ability to identify the correct parameter restriction for a hypothesis test and interpret the result of an F-test from a regression table. It uses a Computational Judgment strategy. Distractors are designed based on common errors: sign error in the restriction (B), misinterpretation of statistical significance by incorrectly comparing the F-statistic to the critical value (C), and conflating the test's purpose (testing for scale effects) with the paper's main finding (D).", "qid": "209", "question": "## Background\n\n**Research Question.** This problem examines a method to distinguish the effect of adverse selection from a potential confounding \"scale effect\" in the pricing of thoroughbred yearlings.\n\n**Setting / Institutional Environment.** A potential critique of the main finding is that the `Racing Intensity` variable may be correlated with the overall scale of a seller's operation. If larger breeding operations have better reputations and command higher prices, the estimated coefficient on `Racing Intensity` could be biased. This analysis re-parameterizes the model to test this possibility directly.\n\n**Variables & Parameters.**\n- `ln(Price)`: Natural logarithm of the hammer price.\n- `R`: `Racing Starts`, the number of races the breeder started a horse in 1993.\n- `B`: `Breeding Starts + 1`, where `Breeding Starts` is the number of races started by horses bred by the seller.\n- `Race Zero`, `Breed Zero`: Indicator variables for sellers with zero racing or breeding starts, respectively.\n\n---\n\n## Data / Model Specification\n\nThe analysis compares a restricted model with an unrestricted version derived from an algebraic identity.\n\n**Restricted Model:**\n```latex\nln(Price_i) = \\alpha_0 + \\alpha_1 ln(R_i/B_i + 1) + \\gamma'X_i + \\epsilon_i\n```\n(Eq. 1)\n\n**Algebraic Identity:**\n```latex\nln(R/B + 1) = ln(R+B) - ln(B)\n```\n(Eq. 2)\n\n**Unrestricted Model:**\n```latex\nln(Price_i) = \\theta_0 + \\alpha_1^* ln(R_i+B_i) - \\beta^{**} ln(B_i) + \\gamma'X_i + \\nu_i\n```\n(Eq. 3)\n\n**Table 1: Selected Regression Coefficients for ln(Price)**\n| Specification | (3) Restricted | (4) Restricted | (5) Unrestricted | (6) Unrestricted |\n| :--- | :---: | :---: | :---: | :---: |\n| `Race/Breed Zero` Dummies | No | Yes | No | Yes |\n| `ln(R/B + 1)` | -0.1026 | -0.1404 | | |\n| `ln(R+B)` | | | -0.0638 | -0.1465 |\n| `-ln(B)` | | | -0.1200 | -0.1340 |\n| F-test (`\\alpha_1^* = \\beta^{**}`)| | | F = 9.58 | F = 0.068 |\n| Critical F (5%) | | | ~3.88 | ~3.88 |\n\n---\n\n## The Question\n\nBased on the models and results in Table 1, what is the correct linear restriction to test for the absence of scale effects, and what is the conclusion from the F-test comparing specifications (3) and (5)?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand the relationship between the restricted and unrestricted models and what restriction would test for scale effects.\n\nFrom the algebraic identity in Eq. 2:\nln(R/B + 1) = ln(R+B) - ln(B)\n\nIf I substitute this into the restricted model (Eq. 1):\nln(Price_i) = α₀ + α₁[ln(R+B) - ln(B)] + γ'X_i + ε_i\n= α₀ + α₁ln(R+B) - α₁ln(B) + γ'X_i + ε_i\n\nComparing this to the unrestricted model (Eq. 3):\nln(Price_i) = θ₀ + α₁*ln(R_i+B_i) - β**ln(B_i) + γ'X_i + ν_i\n\nFor these to be equivalent (i.e., for the restricted model to be valid), we need:\n- α₁* = α₁ (coefficient on ln(R+B))\n- β** = α₁ (coefficient on ln(B))\n\nTherefore, the restriction is α₁* = β**.\n\nLooking at Table 1, comparing specifications (3) and (5):\n- ln(R+B) coefficient: α₁* = -0.0638\n- -ln(B) coefficient: -β** = -0.1200, so β** = 0.1200\n- F-test: F = 9.58\n- Critical F (5%): ~3.88\n\nSince F = 9.58 > 3.88, we reject the null hypothesis that α₁* = β**.\n\nThis rejection means the restricted model is misspecified - there are significant scale effects present that the restricted model fails to capture.\n\nA", "model_answer": "B", "error": null}}
{"ID": 162, "Question": "### Background\n\n**Research Question.** This problem asks you to quantify the differential impact of the COVID-19 shock on the planned quantities and prices of credit-constrained versus unconstrained firms, based on the paper's core empirical model.\n\n**Setting / Institutional Environment.** The analysis uses survey data on expectations from Italian firms, collected immediately before (`t-1`) and during (`t`) the first COVID-19 lockdown. The identification strategy treats the pandemic as an exogenous shock that causes firms to revise their plans.\n\n**Variables & Parameters.**\n*   `\\mathbb{F}_{i,t}(y)`: Firm `i`'s post-outbreak expectation for the 12-month growth rate of outcome `y`.\n*   `CC_{i,t-1}`: An indicator for whether firm `i` was credit constrained pre-outbreak (1=yes).\n*   `Essential_i`: An indicator for whether firm `i` was classified as essential and allowed to operate during lockdown (1=yes).\n*   Outcomes `y`: `Sal^g1Y` (sales growth), `Emp^g1Y` (employment growth), `P^g` (price growth). All are in percentage points.\n\n---\n\n### Data / Model Specification\n\nThe baseline model is:\n```latex\n\\mathbb{F}_{i,t}(y) = \\alpha + \\beta CC_{i,t-1} + \\delta \\mathbb{F}_{i,t-1}(y) + \\gamma^{\\top}x_{i,t-1} + \\lambda_{s} + \\lambda_{p} + \\varepsilon_{i,t}\n```\nwhere `x` is a vector of controls including `Essential`.\n\nSelected results from the paper's baseline estimation are presented below.\n\n**Table 1: The Effect of Credit Constraints on Firms' Expectations**\n| | (1) `\\mathbb{F}_{i,t}(Sal^g1Y)` | (2) `\\mathbb{F}_{i,t}(Emp^g1Y)` | (3) `\\mathbb{F}_{i,t}(P^g)` |\n|:---|:---:|:---:|:---:|\n| **Credit constrained** (`CC`) | -10.62*** | -10.25*** | 3.754*** |\n| | [2.111] | [2.515] | [0.949] |\n| **Essential** | 11.22*** | 4.577** | -2.842** |\n| | [1.218] | [2.190] | [1.286] |\n\n*Notes: Dependent variables are post-COVID expectations for 12-month growth rates. Standard errors are in brackets. *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nConsider a firm that is initially non-essential (`Essential=0`) and credit-constrained (`CC=1`). Using the coefficients from Table 1, which of two mutually exclusive policy scenarios is predicted to have a larger positive impact on its 12-month sales growth expectation (`\\mathbb{F}_{i,t}(Sal^g1Y)`)?", "Options": {"A": "Reclassifying the firm as 'essential' is more effective, improving expected sales growth by 11.22 pp, compared to 10.62 pp from resolving its credit constraint.", "B": "Resolving the firm's credit constraint is more effective, improving expected sales growth by 10.62 pp, compared to a negative impact from being reclassified as 'essential'.", "C": "Reclassifying the firm as 'essential' is more effective, improving expected sales growth by 11.22 pp, compared to 10.25 pp from resolving its credit constraint.", "D": "Both policies have a negative impact on the firm's expected sales growth."}, "Answer": "A", "pi_justification": "This item uses the Computational Judgment strategy to replace the calculation part of the original QA problem. It requires students to correctly identify the relevant coefficients, calculate the impact of two separate counterfactual changes, and compare the results. The distractors are designed based on predictable errors: (B) makes a comparison error and a sign error for the 'essential' effect, (C) uses the wrong coefficient (for employment) in the comparison, and (D) makes a sign error for both effects.", "qid": "162", "question": "### Background\n\n**Research Question.** This problem asks you to quantify the differential impact of the COVID-19 shock on the planned quantities and prices of credit-constrained versus unconstrained firms, based on the paper's core empirical model.\n\n**Setting / Institutional Environment.** The analysis uses survey data on expectations from Italian firms, collected immediately before (`t-1`) and during (`t`) the first COVID-19 lockdown. The identification strategy treats the pandemic as an exogenous shock that causes firms to revise their plans.\n\n**Variables & Parameters.**\n*   `\\mathbb{F}_{i,t}(y)`: Firm `i`'s post-outbreak expectation for the 12-month growth rate of outcome `y`.\n*   `CC_{i,t-1}`: An indicator for whether firm `i` was credit constrained pre-outbreak (1=yes).\n*   `Essential_i`: An indicator for whether firm `i` was classified as essential and allowed to operate during lockdown (1=yes).\n*   Outcomes `y`: `Sal^g1Y` (sales growth), `Emp^g1Y` (employment growth), `P^g` (price growth). All are in percentage points.\n\n---\n\n### Data / Model Specification\n\nThe baseline model is:\n```latex\n\\mathbb{F}_{i,t}(y) = \\alpha + \\beta CC_{i,t-1} + \\delta \\mathbb{F}_{i,t-1}(y) + \\gamma^{\\top}x_{i,t-1} + \\lambda_{s} + \\lambda_{p} + \\varepsilon_{i,t}\n```\nwhere `x` is a vector of controls including `Essential`.\n\nSelected results from the paper's baseline estimation are presented below.\n\n**Table 1: The Effect of Credit Constraints on Firms' Expectations**\n| | (1) `\\mathbb{F}_{i,t}(Sal^g1Y)` | (2) `\\mathbb{F}_{i,t}(Emp^g1Y)` | (3) `\\mathbb{F}_{i,t}(P^g)` |\n|:---|:---:|:---:|:---:|\n| **Credit constrained** (`CC`) | -10.62*** | -10.25*** | 3.754*** |\n| | [2.111] | [2.515] | [0.949] |\n| **Essential** | 11.22*** | 4.577** | -2.842** |\n| | [1.218] | [2.190] | [1.286] |\n\n*Notes: Dependent variables are post-COVID expectations for 12-month growth rates. Standard errors are in brackets. *** p<0.01, ** p<0.05.*\n\n---\n\n### Question\n\nConsider a firm that is initially non-essential (`Essential=0`) and credit-constrained (`CC=1`). Using the coefficients from Table 1, which of two mutually exclusive policy scenarios is predicted to have a larger positive impact on its 12-month sales growth expectation (`\\mathbb{F}_{i,t}(Sal^g1Y)`)?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the impact of two policy scenarios on a firm that is initially non-essential (Essential=0) and credit-constrained (CC=1).\n\nFrom Table 1, column (1) for sales growth expectations:\n- Credit constrained coefficient: -10.62***\n- Essential coefficient: 11.22***\n\nThe baseline firm has:\n- CC = 1 (credit constrained)\n- Essential = 0 (non-essential)\n\n**Scenario 1: Reclassifying as essential**\n- Change Essential from 0 to 1\n- Impact: +11.22 percentage points\n\n**Scenario 2: Resolving credit constraint**\n- Change CC from 1 to 0\n- Since the coefficient is -10.62 for being credit constrained, removing the constraint (going from CC=1 to CC=0) eliminates this negative effect\n- Impact: -(-10.62) = +10.62 percentage points\n\nComparing the impacts:\n- Reclassifying as essential: +11.22 pp\n- Resolving credit constraint: +10.62 pp\n\nSince 11.22 > 10.62, reclassifying as essential has a larger positive impact.\n\nLooking at the options, option A correctly states that reclassifying as essential is more effective (11.22 pp vs 10.62 pp from resolving the credit constraint).\n\nA", "model_answer": "A", "error": null}}
{"ID": 248, "Question": "### Background\n\nThe paper's model of long-run development culminates in a balanced growth path (BGP). A key part of the paper's methodology is calibrating the model's structural parameters to match historical data and stylized facts. This problem tests the ability to work backwards from a calibrated outcome to find an underlying parameter value.\n\n### Data / Model Specification\n\nOn the BGP, per capita income `y_e` at the entry point is given by:\n```latex\ny_e = \\frac{A_P}{u_b} \\quad \\text{(Eq. 1)}\n```\nwhere `A_P` is the productivity parameter of the primitive technology, and `u_b` is a composite term related to the effort in the modern sector, `e_{Mb}`:\n```latex\nu_b = \\frac{1}{e_{Mb}} \\quad \\text{(Eq. 2)}\n```\nThe model is calibrated such that income at the entry to balanced growth is normalized to `y_e = 1000`, and the implied effort in the modern sector is `e_{Mb} = 0.54`.\n\n### Question\n\nGiven the model's calibration, what is the implied value of the primitive technology's productivity parameter, `A_P`?", "Options": {"A": "1852", "B": "0.0019", "C": "5000", "D": "540"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 10.0). The original QA problem is purely computational with a single correct numerical answer, making it perfectly suited for conversion (Conceptual Clarity = 10/10). The multi-step calculation provides numerous opportunities for predictable errors, allowing for the creation of high-fidelity distractors (Discriminability = 10/10). This item assesses the ability to work backwards from a calibrated outcome to an underlying structural parameter. It uses the Reverse-Reasoning strategy. Distractors are designed based on plausible errors, such as inverting the core relationship (B), using the wrong effort share in the calculation (C), or misapplying the formula (D).", "qid": "248", "question": "### Background\n\nThe paper's model of long-run development culminates in a balanced growth path (BGP). A key part of the paper's methodology is calibrating the model's structural parameters to match historical data and stylized facts. This problem tests the ability to work backwards from a calibrated outcome to find an underlying parameter value.\n\n### Data / Model Specification\n\nOn the BGP, per capita income `y_e` at the entry point is given by:\n```latex\ny_e = \\frac{A_P}{u_b} \\quad \\text{(Eq. 1)}\n```\nwhere `A_P` is the productivity parameter of the primitive technology, and `u_b` is a composite term related to the effort in the modern sector, `e_{Mb}`:\n```latex\nu_b = \\frac{1}{e_{Mb}} \\quad \\text{(Eq. 2)}\n```\nThe model is calibrated such that income at the entry to balanced growth is normalized to `y_e = 1000`, and the implied effort in the modern sector is `e_{Mb} = 0.54`.\n\n### Question\n\nGiven the model's calibration, what is the implied value of the primitive technology's productivity parameter, `A_P`?", "model_evaluation_result": {"model_raw_response": "I need to find the value of A_P given the calibrated values and the model equations.\n\nGiven information:\n- y_e = 1000 (normalized income at entry to balanced growth)\n- e_{Mb} = 0.54 (effort in the modern sector)\n- Equation 1: y_e = A_P / u_b\n- Equation 2: u_b = 1 / e_{Mb}\n\nFirst, I'll calculate u_b using Equation 2:\nu_b = 1 / e_{Mb} = 1 / 0.54 = 1.8519 (approximately)\n\nNow I can use Equation 1 to solve for A_P:\ny_e = A_P / u_b\n1000 = A_P / 1.8519\nA_P = 1000 × 1.8519 = 1851.9\n\nRounding to the nearest whole number, A_P ≈ 1852.\n\nD", "model_answer": "D", "error": null}}
{"ID": 260, "Question": "### Background\n\n**Research Question.** This problem analyzes the main empirical findings of India's Operation Blackboard (OB) program, focusing on the impacts on primary school completion.\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences (DiD) approach to estimate the causal effect of the OB teacher provision program. The program's intensity varied by state.\n\n### Data / Model Specification\n\nThe effect on primary school completion is estimated using an individual-level DiD model:\n\n```latex\ny_{ijk} = \\alpha + \\beta (\\mathrm{Intensity}_{j} \\times \\mathrm{Post-OB}_{k}) + \\text{Controls} + \\varepsilon_{ijk}\n```\n\nwhere `y_ijk` is an indicator for primary school completion. The most robust specifications include state-specific time trends.\n\n**Table 1: Effect on Primary School Completion for Girls (Robust Specification)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :---: |\n| `Intensity_j × Post-OB_k` | 0.0091* (0.0044) |\n\n*Source: Adapted from paper's Table 5, Column (5). Specification includes state program intensity × year of birth trend control. * denotes significance at 5%.*\n\n### Question\n\nThe paper reports the average program intensity across India was 1.6 OB teachers per 1000 children. Using the most robust estimate for girls from Table 1, what was the Average Treatment Effect on the Treated (ATT) for girls' primary school completion in a state with average program intensity?", "Options": {"A": "The program increased girls' primary school completion by approximately 1.46 percentage points.", "B": "The program increased girls' primary school completion by approximately 0.91 percentage points.", "C": "The program decreased girls' primary school completion by approximately 1.46 percentage points.", "D": "The program increased girls' primary school completion by approximately 14.6 percentage points."}, "Answer": "A", "pi_justification": "This item assesses the ability to calculate an Average Treatment Effect on the Treated (ATT) from a regression coefficient and a given treatment intensity. It uses a Computational Judgment strategy. The correct answer (A) is the product of the coefficient (0.0091) and the intensity (1.6). Distractor (B) represents a unit error where the student incorrectly uses the coefficient as the final answer without scaling by intensity. Distractor (C) is a sign error. Distractor (D) is a calculation error (off by a factor of 10).", "qid": "260", "question": "### Background\n\n**Research Question.** This problem analyzes the main empirical findings of India's Operation Blackboard (OB) program, focusing on the impacts on primary school completion.\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences (DiD) approach to estimate the causal effect of the OB teacher provision program. The program's intensity varied by state.\n\n### Data / Model Specification\n\nThe effect on primary school completion is estimated using an individual-level DiD model:\n\n```latex\ny_{ijk} = \\alpha + \\beta (\\mathrm{Intensity}_{j} \\times \\mathrm{Post-OB}_{k}) + \\text{Controls} + \\varepsilon_{ijk}\n```\n\nwhere `y_ijk` is an indicator for primary school completion. The most robust specifications include state-specific time trends.\n\n**Table 1: Effect on Primary School Completion for Girls (Robust Specification)**\n\n| Variable | Coefficient (Std. Err.) |\n| :--- | :---: |\n| `Intensity_j × Post-OB_k` | 0.0091* (0.0044) |\n\n*Source: Adapted from paper's Table 5, Column (5). Specification includes state program intensity × year of birth trend control. * denotes significance at 5%.*\n\n### Question\n\nThe paper reports the average program intensity across India was 1.6 OB teachers per 1000 children. Using the most robust estimate for girls from Table 1, what was the Average Treatment Effect on the Treated (ATT) for girls' primary school completion in a state with average program intensity?", "model_evaluation_result": {"model_raw_response": "To find the Average Treatment Effect on the Treated (ATT), I need to multiply the coefficient by the average program intensity.\n\nGiven information:\n- Coefficient from Table 1: 0.0091\n- Average program intensity across India: 1.6 OB teachers per 1000 children\n\nATT = Coefficient × Average Program Intensity\nATT = 0.0091 × 1.6 = 0.01456\n\nConverting to percentage points: 0.01456 × 100 = 1.456 percentage points\n\nThis rounds to approximately 1.46 percentage points.\n\nSince the coefficient is positive (0.0091), this represents an increase in girls' primary school completion.\n\nA", "model_answer": "A", "error": null}}
{"ID": 227, "Question": "### Background\n\nThis problem examines the core behavioral foundations of a dynamic commodity market model, focusing on the optimal decisions of forward-looking private arbitrageurs. Their actions, driven by rational expectations of future prices, determine the intertemporal allocation of the commodity.\n\nIn an annual market model, competitive, expected profit-maximizing arbitrageurs decide how much to store (`x_t`) for the next period based on the current price `p_t` and their rational expectation of the price at time `t+1`, `p_{t+1}^{e}`.\n\n### Data / Model Specification\n\nThe arbitrageur's decision is governed by the intertemporal arbitrage conditions given in Eq. (1):\n\n```latex\nx_{t}\\geq0, \\quad p_{t}\\geq\\delta p_{t+1}^{e}-\\kappa, \\quad p_{t}>\\delta p_{t+1}^{e}-\\kappa\\Rightarrow x_{t}=0.\n```\n\nKey model parameters are:\n- Annual discount factor: `δ = 0.916`\n- Unit storage cost: `κ = $0.36` per bushel\n\nAt time `t`, the current market price is `p_t = $5.50`. Agents rationally expect the price next year to be `p_{t+1}^{e} = $6.50`. Given the model's parameters, what is the optimal action for private arbitrageurs?", "Options": {"A": "Store positive quantities of the commodity (`x_t > 0`), as the current price is below the expected net return from storage.", "B": "Do not store any of the commodity (`x_t = 0`), as the current price is above the expected net return from storage.", "C": "Do not store any of the commodity (`x_t = 0`), because the undiscounted expected profit (`p_{t+1}^{e} - κ`) is the relevant metric.", "D": "Do not store any of the commodity (`x_t = 0`), because the storage cost `κ` should be added to, not subtracted from, the discounted expected price."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the user's ability to apply the core no-arbitrage condition. It uses a Computational Judgment strategy. The correct answer requires calculating the expected net return from storage (`δ * p_{t+1}^{e} - κ = $5.594`) and comparing it to the current price (`$5.50`). Distractors are based on plausible errors: (B) misinterprets the result of the comparison; (C) is a Step-Omission Error (ignores discounting); (D) is a Sign Error in the formula.", "qid": "227", "question": "### Background\n\nThis problem examines the core behavioral foundations of a dynamic commodity market model, focusing on the optimal decisions of forward-looking private arbitrageurs. Their actions, driven by rational expectations of future prices, determine the intertemporal allocation of the commodity.\n\nIn an annual market model, competitive, expected profit-maximizing arbitrageurs decide how much to store (`x_t`) for the next period based on the current price `p_t` and their rational expectation of the price at time `t+1`, `p_{t+1}^{e}`.\n\n### Data / Model Specification\n\nThe arbitrageur's decision is governed by the intertemporal arbitrage conditions given in Eq. (1):\n\n```latex\nx_{t}\\geq0, \\quad p_{t}\\geq\\delta p_{t+1}^{e}-\\kappa, \\quad p_{t}>\\delta p_{t+1}^{e}-\\kappa\\Rightarrow x_{t}=0.\n```\n\nKey model parameters are:\n- Annual discount factor: `δ = 0.916`\n- Unit storage cost: `κ = $0.36` per bushel\n\nAt time `t`, the current market price is `p_t = $5.50`. Agents rationally expect the price next year to be `p_{t+1}^{e} = $6.50`. Given the model's parameters, what is the optimal action for private arbitrageurs?", "model_evaluation_result": {"model_raw_response": "I need to analyze the arbitrage condition to determine the optimal storage decision.\n\nGiven information:\n- δ = 0.916 (discount factor)\n- κ = $0.36 (storage cost per bushel)\n- p_t = $5.50 (current price)\n- p_{t+1}^e = $6.50 (expected future price)\n\nThe arbitrage condition states:\n- x_t ≥ 0\n- p_t ≥ δp_{t+1}^e - κ\n- If p_t > δp_{t+1}^e - κ, then x_t = 0\n\nLet me calculate δp_{t+1}^e - κ:\nδp_{t+1}^e - κ = 0.916 × $6.50 - $0.36\n= $5.954 - $0.36\n= $5.594\n\nNow I compare the current price with this threshold:\np_t = $5.50\nδp_{t+1}^e - κ = $5.594\n\nSince p_t = $5.50 < $5.594 = δp_{t+1}^e - κ, the condition p_t ≥ δp_{t+1}^e - κ is NOT satisfied.\n\nThis means the current price is below the expected net return from storage (discounted future price minus storage cost). When the current price is low relative to the expected discounted future price net of storage costs, it's profitable to store the commodity.\n\nTherefore, arbitrageurs should store positive quantities (x_t > 0).\n\nA", "model_answer": "A", "error": null}}
{"ID": 193, "Question": "### Background\n\n**Research Question.** This problem develops the paper's theoretical framework to understand how leader identity affects group coordination in a weakest-link game by introducing heterogeneity in player types.\n\n**Setting / Institutional Environment.** The model considers a four-player coordination game where players choose effort `x` on a continuum `[0, W]`. A fraction of the population are 'behavioral types' who follow leaders of their own religion but not others. 'Rational types' maximize expected payoffs, accounting for the possible presence of behavioral types. The analysis assumes the probability of having more than one behavioral player in a group is negligible.\n\n### Data / Model Specification\n\n1.  **No-Leader Equilibrium:** In the absence of a leader, rational players assume others' efforts are drawn from `U[0, W]`. The optimal strategy is to match the expected minimum of the other three players' efforts, leading to a baseline effort `x*`.\n\n2.  **Leader Equilibrium:** With a leader, the optimal effort choices for a rational player under a Hindu leader (`x_H`) and a Muslim leader (`x_M`) are given by:\n\n    ```latex\n    x_{H} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( (1-p)a_{H}\\frac{W}{3} + p a_{M}x^{**} \\right) \\quad \\text{(Eq. 1)}\n    ```\n\n    ```latex\n    x_{M} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( p a_{M}\\frac{W}{3} + (1-p)a_{H}x^{**} \\right) \\quad \\text{(Eq. 2)}\n    ```\n\n    where `p` is the population share of Muslims, `a_H` and `a_M` are the fractions of behavioral types for each religion, `\\tilde{a} = p a_M + (1-p) a_H` is the overall fraction of behavioral types, `x**` is a low effort level (`x** < x*`), and `W` is the maximum effort.\n\n### The Question\n\nLet `D = x_M - x_H` be the effort gap between Muslim-led and Hindu-led groups. Given that Muslims are a population minority (`p < 0.5`), how does the sensitivity of this gap `D` to a change in the fraction of behavioral Hindus (`a_H`) compare to its sensitivity to an equivalent change in the fraction of behavioral Muslims (`a_M`)?", "Options": {"A": "The gap `D` is more sensitive to a change in `a_H` than to a change in `a_M`.", "B": "The gap `D` is more sensitive to a change in `a_M` than to a change in `a_H`.", "C": "The gap `D` is equally sensitive to changes in `a_H` and `a_M`.", "D": "The sensitivity cannot be determined without knowing the absolute values of `a_H` and `a_M`."}, "Answer": "A", "pi_justification": "Assesses a deeper understanding of the model's mechanics via a comparative static question. Strategy: Computational Judgment. Distractors are designed to catch common errors: the Conceptual Opposite, ignoring the population weights which determine the sensitivity (Step-Omission), and a True but Irrelevant statement that distracts from the core comparison.", "qid": "193", "question": "### Background\n\n**Research Question.** This problem develops the paper's theoretical framework to understand how leader identity affects group coordination in a weakest-link game by introducing heterogeneity in player types.\n\n**Setting / Institutional Environment.** The model considers a four-player coordination game where players choose effort `x` on a continuum `[0, W]`. A fraction of the population are 'behavioral types' who follow leaders of their own religion but not others. 'Rational types' maximize expected payoffs, accounting for the possible presence of behavioral types. The analysis assumes the probability of having more than one behavioral player in a group is negligible.\n\n### Data / Model Specification\n\n1.  **No-Leader Equilibrium:** In the absence of a leader, rational players assume others' efforts are drawn from `U[0, W]`. The optimal strategy is to match the expected minimum of the other three players' efforts, leading to a baseline effort `x*`.\n\n2.  **Leader Equilibrium:** With a leader, the optimal effort choices for a rational player under a Hindu leader (`x_H`) and a Muslim leader (`x_M`) are given by:\n\n    ```latex\n    x_{H} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( (1-p)a_{H}\\frac{W}{3} + p a_{M}x^{**} \\right) \\quad \\text{(Eq. 1)}\n    ```\n\n    ```latex\n    x_{M} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( p a_{M}\\frac{W}{3} + (1-p)a_{H}x^{**} \\right) \\quad \\text{(Eq. 2)}\n    ```\n\n    where `p` is the population share of Muslims, `a_H` and `a_M` are the fractions of behavioral types for each religion, `\\tilde{a} = p a_M + (1-p) a_H` is the overall fraction of behavioral types, `x**` is a low effort level (`x** < x*`), and `W` is the maximum effort.\n\n### The Question\n\nLet `D = x_M - x_H` be the effort gap between Muslim-led and Hindu-led groups. Given that Muslims are a population minority (`p < 0.5`), how does the sensitivity of this gap `D` to a change in the fraction of behavioral Hindus (`a_H`) compare to its sensitivity to an equivalent change in the fraction of behavioral Muslims (`a_M`)?", "model_evaluation_result": {"model_raw_response": "I need to analyze how the effort gap D = x_M - x_H responds to changes in a_H and a_M.\n\nFirst, let me calculate D by subtracting equation (1) from equation (2):\n\nD = x_M - x_H = 3(1-ã)² × [p·a_M·(W/3) + (1-p)·a_H·x** - (1-p)·a_H·(W/3) - p·a_M·x**]\n\nSimplifying:\nD = 3(1-ã)² × [p·a_M·(W/3 - x**) - (1-p)·a_H·(W/3 - x**)]\nD = 3(1-ã)² × (W/3 - x**) × [p·a_M - (1-p)·a_H]\n\nNow I need to find the partial derivatives:\n\n∂D/∂a_H:\nSince ã = p·a_M + (1-p)·a_H, we have ∂ã/∂a_H = (1-p)\n\n∂D/∂a_H = 3(W/3 - x**) × [∂/∂a_H((1-ã)² × [p·a_M - (1-p)·a_H])]\n\nUsing the product rule:\n∂D/∂a_H = 3(W/3 - x**) × [-2(1-ã)(1-p)[p·a_M - (1-p)·a_H] + (1-ã)²(-(1-p))]\n∂D/∂a_H = 3(W/3 - x**)(1-ã)(1-p) × [-2[p·a_M - (1-p)·a_H] - (1-ã)]\n\n∂D/∂a_M:\nSince ∂ã/∂a_M = p\n\n∂D/∂a_M = 3(W/3 - x**)", "model_answer": "D", "error": null}}
{"ID": 291, "Question": "### Background\n\nIndividuals are assumed to model inflation as a first-order autoregressive, AR(1), process. They recursively estimate the parameters of this model based on the inflation rates they have experienced during their lifetimes. The rate at which they update their beliefs is governed by an age-dependent gain function.\n\n### Data / Model Specification\n\nConsider two learning models:\n1.  **Age-Dependent Decreasing Gain (the paper's model):** The gain `\\gamma_{t,s}` for a person of age `t-s` is `\\theta / (t-s)`, so it declines towards zero as the person ages.\n2.  **Constant Gain:** The gain `\\gamma` is a small positive constant for all individuals at all times.\n\n### Question\n\nWhich of these models implies \"perpetual learning\" and would be better suited for an agent to adapt to a sudden, permanent structural break in the inflation process that occurs late in their life?", "Options": {"A": "The constant-gain model, because it continues to place significant weight on new data indefinitely.", "B": "The age-dependent model, because its parameter estimates eventually converge to a stable value.", "C": "The age-dependent model, because it places more weight on recent data when an individual is young.", "D": "The constant-gain model, because it weights all past data equally."}, "Answer": "A", "pi_justification": "This item assesses understanding of the dynamic properties and long-run implications of different learning rules. It uses a Scenario Application strategy (a structural break late in life). Distractors are designed to pinpoint specific confusions: B incorrectly links convergence to adaptability; C is a true but irrelevant statement about the age-dependent model; D mischaracterizes constant-gain learning, which involves exponential decay, not equal weighting.", "qid": "291", "question": "### Background\n\nIndividuals are assumed to model inflation as a first-order autoregressive, AR(1), process. They recursively estimate the parameters of this model based on the inflation rates they have experienced during their lifetimes. The rate at which they update their beliefs is governed by an age-dependent gain function.\n\n### Data / Model Specification\n\nConsider two learning models:\n1.  **Age-Dependent Decreasing Gain (the paper's model):** The gain `\\gamma_{t,s}` for a person of age `t-s` is `\\theta / (t-s)`, so it declines towards zero as the person ages.\n2.  **Constant Gain:** The gain `\\gamma` is a small positive constant for all individuals at all times.\n\n### Question\n\nWhich of these models implies \"perpetual learning\" and would be better suited for an agent to adapt to a sudden, permanent structural break in the inflation process that occurs late in their life?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 296, "Question": "### Background\n\nThe paper's framework for analyzing price competition can be applied to the classic Hotelling model. In this model, two firms (1 and 2) are located at `x₁` and `x₂` on a line segment `[0, 1]` (with `x₁ < x₂`), and a continuum of consumers is distributed uniformly along this line.\n\n---\n\n### Data / Model Specification\n\nA consumer at location `a` has a preference parameter `θ_a` defined by their relative transportation costs. Assume transportation costs are **linear** in distance `d`: `t(d) = kd` for some `k > 0`. The preference parameter is therefore:\n\n```latex\nθ_a = t(|x₁ - a|) - t(|x₂ - a|) = k|x₁ - a| - k|x₂ - a|\n```\n\nIt is a known result that with linear transportation costs, a pure-strategy price equilibrium may not exist.\n\n---\n\nWithin the context of this paper's theoretical framework, what is the fundamental reason for the non-existence of a pure-strategy equilibrium in the Hotelling model with linear costs?", "Options": {"A": "The linear cost function generates mass points in the consumer preference distribution `F(·)`, which violates the model's core assumption of a continuous distribution.", "B": "The firms' locations `x₁` and `x₂` are too close to each other, which makes the products overly substitutable and leads to a price war.", "C": "The uniform distribution of consumers along the line `[0, 1]` is too dispersed, preventing firms from establishing a stable market share.", "D": "The transportation cost `k` is too low, meaning consumers do not have strong enough preferences for either firm to sustain prices above marginal cost."}, "Answer": "A", "pi_justification": "This question assesses the ability to apply the paper's general theory to a specific, canonical model (Hotelling) and identify the precise point of failure. It uses a Scenario Application strategy. The correct answer (A) identifies the violation of Assumption 1. Distractors are designed to capture common but incorrect explanations: (B) describes the outcome of the instability (the price war) rather than its structural cause within this framework (True but Irrelevant); (C) suggests the opposite of the real problem, which is concentration of preferences, not dispersion (Conceptual Opposite); (D) focuses on the level of a parameter (`k`) rather than the functional form of costs, which is the critical issue.", "qid": "296", "question": "### Background\n\nThe paper's framework for analyzing price competition can be applied to the classic Hotelling model. In this model, two firms (1 and 2) are located at `x₁` and `x₂` on a line segment `[0, 1]` (with `x₁ < x₂`), and a continuum of consumers is distributed uniformly along this line.\n\n---\n\n### Data / Model Specification\n\nA consumer at location `a` has a preference parameter `θ_a` defined by their relative transportation costs. Assume transportation costs are **linear** in distance `d`: `t(d) = kd` for some `k > 0`. The preference parameter is therefore:\n\n```latex\nθ_a = t(|x₁ - a|) - t(|x₂ - a|) = k|x₁ - a| - k|x₂ - a|\n```\n\nIt is a known result that with linear transportation costs, a pure-strategy price equilibrium may not exist.\n\n---\n\nWithin the context of this paper's theoretical framework, what is the fundamental reason for the non-existence of a pure-strategy equilibrium in the Hotelling model with linear costs?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 259, "Question": "### Background\n\n**Research Question.** This problem analyzes the severe finite-sample distortions of estimators and test statistics in Markov-switching models, critiquing the reliability of inference based on standard asymptotic theory and evaluating proposed corrections.\n\n**Setting / Institutional Environment.** A Monte Carlo simulation is conducted to evaluate the properties of Maximum Likelihood Estimation (MLE) for a simple symmetric two-state switching model. One thousand samples of size `T=50` are generated from a known data generating process. The true parameters are `μ₁=2`, `μ₂=-2`, `p₁₁=p₂₂=0.8`, and `σ₁²=σ₂²=1`.\n\n**Variables & Parameters.**\n\n*   `λ`: A generic parameter of the model (e.g., `σ₁²`).\n*   `T`: Sample size, `T=50`.\n*   `m`: Number of estimated parameters in the model.\n*   `l`: Degrees of freedom of an asymptotic `χ²` distribution.\n*   `Avg. Actual S.E.`: The true sampling standard deviation of an estimator, calculated from the Monte Carlo replications.\n*   `Avg. Estimated S.E.`: The average of the standard errors computed for each sample using the outer-product-of-gradient (OPG) estimator.\n\n---\n\n### Data / Model Specification\n\nThe following table consolidates key results from the paper's Monte Carlo study for a sample size of `T=50`.\n\n**Table 1. Monte Carlo Results for MLE and Specification Tests (T=50)**\n\n| Parameter / Test | Statistic | Value |\n| :--- | :--- | :--- |\n| **Parameter Estimation** | | |\n| `σ₁²` | Avg. Actual S.E. | 0.40 |\n| `σ₁²` | Avg. Estimated S.E. | 0.26 |\n| **t-statistic Distribution** | | |\n| t-stat for `σ₁²` | 5th percentile | -2.90 |\n| t-stat for `σ₁²` | 95th percentile | 1.00 |\n| `N(0,1)` distribution | 5th percentile | -1.64 |\n| `N(0,1)` distribution | 95th percentile | 1.64 |\n| **Specification Test Size** | | |\n| NTW Test for ARCH | Actual Rejection Rate (α=5%) | 17% |\n\nThe paper proposes a small-sample correction for a Newey-Tauchen-White (NTW) test statistic, which is asymptotically `χ²(l)`. The corrected statistic is calculated as:\n\n```latex\n\\text{Corrected Statistic} = (\\text{Raw NTW Statistic}) \\times \\frac{T-m}{T \\cdot l} \\quad \\text{(Eq. 1)}\n```\n\nThis corrected statistic should be compared to an `F(l, T-m)` distribution.\n\n---\n\n### Question\n\nA researcher conducts a Newey-Tauchen-White test for ARCH effects, which has `l=4` degrees of freedom. The model was estimated with `T=50` observations and `m=6` parameters. The raw test statistic is 13.5. Using the paper's recommended small-sample correction procedure and an `F(4, 44)` critical value of 2.58, what is the final statistical conclusion?", "Options": {"A": "The corrected F-statistic is 2.97; since this exceeds the critical value of 2.58, the null hypothesis of no ARCH effects is rejected.", "B": "The corrected F-statistic is 2.97; since this is less than the asymptotic `χ²(4)` critical value of 9.49, the null hypothesis is not rejected.", "C": "The corrected F-statistic is 11.88; since this exceeds the critical value of 2.58, the null hypothesis of no ARCH effects is rejected.", "D": "The corrected F-statistic is 0.0675; since this is less than the critical value of 2.58, the null hypothesis of no ARCH effects is not rejected."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item uses the Computational Judgment strategy. It assesses the ability to correctly apply the multi-step correction formula and draw the appropriate conclusion by comparing the result to the correct critical value. Distractor B correctly calculates the statistic but compares it to the wrong distribution, a key conceptual error the paper warns against. Distractors C and D represent plausible calculation errors in applying the formula.", "qid": "259", "question": "### Background\n\n**Research Question.** This problem analyzes the severe finite-sample distortions of estimators and test statistics in Markov-switching models, critiquing the reliability of inference based on standard asymptotic theory and evaluating proposed corrections.\n\n**Setting / Institutional Environment.** A Monte Carlo simulation is conducted to evaluate the properties of Maximum Likelihood Estimation (MLE) for a simple symmetric two-state switching model. One thousand samples of size `T=50` are generated from a known data generating process. The true parameters are `μ₁=2`, `μ₂=-2`, `p₁₁=p₂₂=0.8`, and `σ₁²=σ₂²=1`.\n\n**Variables & Parameters.**\n\n*   `λ`: A generic parameter of the model (e.g., `σ₁²`).\n*   `T`: Sample size, `T=50`.\n*   `m`: Number of estimated parameters in the model.\n*   `l`: Degrees of freedom of an asymptotic `χ²` distribution.\n*   `Avg. Actual S.E.`: The true sampling standard deviation of an estimator, calculated from the Monte Carlo replications.\n*   `Avg. Estimated S.E.`: The average of the standard errors computed for each sample using the outer-product-of-gradient (OPG) estimator.\n\n---\n\n### Data / Model Specification\n\nThe following table consolidates key results from the paper's Monte Carlo study for a sample size of `T=50`.\n\n**Table 1. Monte Carlo Results for MLE and Specification Tests (T=50)**\n\n| Parameter / Test | Statistic | Value |\n| :--- | :--- | :--- |\n| **Parameter Estimation** | | |\n| `σ₁²` | Avg. Actual S.E. | 0.40 |\n| `σ₁²` | Avg. Estimated S.E. | 0.26 |\n| **t-statistic Distribution** | | |\n| t-stat for `σ₁²` | 5th percentile | -2.90 |\n| t-stat for `σ₁²` | 95th percentile | 1.00 |\n| `N(0,1)` distribution | 5th percentile | -1.64 |\n| `N(0,1)` distribution | 95th percentile | 1.64 |\n| **Specification Test Size** | | |\n| NTW Test for ARCH | Actual Rejection Rate (α=5%) | 17% |\n\nThe paper proposes a small-sample correction for a Newey-Tauchen-White (NTW) test statistic, which is asymptotically `χ²(l)`. The corrected statistic is calculated as:\n\n```latex\n\\text{Corrected Statistic} = (\\text{Raw NTW Statistic}) \\times \\frac{T-m}{T \\cdot l} \\quad \\text{(Eq. 1)}\n```\n\nThis corrected statistic should be compared to an `F(l, T-m)` distribution.\n\n---\n\n### Question\n\nA researcher conducts a Newey-Tauchen-White test for ARCH effects, which has `l=4` degrees of freedom. The model was estimated with `T=50` observations and `m=6` parameters. The raw test statistic is 13.5. Using the paper's recommended small-sample correction procedure and an `F(4, 44)` critical value of 2.58, what is the final statistical conclusion?", "model_evaluation_result": {"model_raw_response": "I need to apply the small-sample correction formula to the raw NTW test statistic and compare it to the F-distribution critical value.\n\nGiven information:\n- Raw NTW statistic = 13.5\n- l = 4 (degrees of freedom)\n- T = 50 (sample size)\n- m = 6 (number of parameters)\n- F(4, 44) critical value = 2.58\n\nUsing the correction formula from Equation 1:\nCorrected Statistic = (Raw NTW Statistic) × (T-m)/(T × l)\n\nSubstituting the values:\nCorrected Statistic = 13.5 × (50-6)/(50 × 4)\nCorrected Statistic = 13.5 × 44/200\nCorrected Statistic = 13.5 × 0.22\nCorrected Statistic = 2.97\n\nSince the corrected F-statistic (2.97) exceeds the F(4, 44) critical value of 2.58, we reject the null hypothesis of no ARCH effects.\n\nA", "model_answer": "A", "error": null}}
{"ID": 297, "Question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to distinguish between two competing behavioral explanations for an observed choice anomaly.\n\n**Setting.** In a primary experiment, subjects chose between three products (High, Medium, Low benefit) in a two-stage process: first choose a product, then choose one of its associated costs. The key finding was that when costs were presented in a complex arithmetic format (`COMPLEX COST` treatment), subjects made systematically biased mistakes, choosing the suboptimal High-benefit option far more often than the equally suboptimal Low-benefit option. This rejected the null hypothesis of symmetric, random mistakes.\n\nTwo potential explanations for this bias are considered:\n1.  **Salience:** Complexity in the cost dimension makes the simple, easy-to-evaluate benefit dimension more salient. Decision-makers overweight this salient dimension, leading them to be attracted to the High-benefit option.\n2.  **Narrow Bracketing:** Decision-makers simplify the two-stage problem by ignoring the future cost-selection stage. They focus only on the first stage and choose the option with the highest immediate benefit (the High option), without fully integrating the downstream consequences of costs.\n\n### The Question\n\nWhich of the following statements best captures the fundamental difference between the Salience and Narrow Bracketing explanations for the observed bias in the `COMPLEX COST` treatment?", "Options": {"A": "Salience posits that decision-makers are drawn to the simple benefit attribute due to the complexity of the cost attribute, while Narrow Bracketing posits they simplify the choice by ignoring the future cost-selection stage entirely.", "B": "Salience posits that subjects ignore the cost dimension completely, while Narrow Bracketing posits that they miscalculate the complex costs.", "C": "Salience is about simplifying a multi-stage choice into a single stage, while Narrow Bracketing is about attentional focus being drawn to the most prominent attribute on the screen.", "D": "Salience suggests that complexity increases random errors, while Narrow Bracketing suggests complexity leads to a systematic preference for the highest benefit."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question assesses the foundational understanding of the two competing behavioral theories discussed in the paper. It uses a 'Conceptual' distractor design. The correct answer (A) accurately distinguishes the attentional focus of Salience from the sequential simplification of Narrow Bracketing. Distractors include a subtle mischaracterization (B, Almost Right), a direct swap of the concepts (C, Conceptual Opposite), and an inaccurate description of the Salience theory's prediction (D).", "qid": "297", "question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to distinguish between two competing behavioral explanations for an observed choice anomaly.\n\n**Setting.** In a primary experiment, subjects chose between three products (High, Medium, Low benefit) in a two-stage process: first choose a product, then choose one of its associated costs. The key finding was that when costs were presented in a complex arithmetic format (`COMPLEX COST` treatment), subjects made systematically biased mistakes, choosing the suboptimal High-benefit option far more often than the equally suboptimal Low-benefit option. This rejected the null hypothesis of symmetric, random mistakes.\n\nTwo potential explanations for this bias are considered:\n1.  **Salience:** Complexity in the cost dimension makes the simple, easy-to-evaluate benefit dimension more salient. Decision-makers overweight this salient dimension, leading them to be attracted to the High-benefit option.\n2.  **Narrow Bracketing:** Decision-makers simplify the two-stage problem by ignoring the future cost-selection stage. They focus only on the first stage and choose the option with the highest immediate benefit (the High option), without fully integrating the downstream consequences of costs.\n\n### The Question\n\nWhich of the following statements best captures the fundamental difference between the Salience and Narrow Bracketing explanations for the observed bias in the `COMPLEX COST` treatment?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 211, "Question": "### Background\n\n**Research Question.** This study investigates the behavioral responses of students to receiving a free home laptop, specifically examining how the intervention alters their patterns of computer access and use both at home and in other locations.\n\n**Setting / Institutional Environment.** In a randomized experiment in Lima, Peru, students within designated 'treatment schools' participated in lotteries for XO laptops intended for home use. The analysis compares outcomes for lottery winners (treatment) and non-winners (control) within the same schools.\n\n### Data / Model Specification\n\nThe causal effects are estimated using the following regression model, which includes class fixed effects (`\\mu_j`) and baseline controls (`\\mathbf{X_{ijk}}`):\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\delta Winner_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nTable 1 presents the estimates for the treatment effect `\\delta` on outcomes related to computer access and use.\n\n**Table 1: Effects on Computer Access and Use**\n\n| | Winners | Non-winners | Adjusted Difference (ITT) | Observations |\n| :--- | :---: | :---: | :---: | :---: |\n| **Access** | | | |\n| Computer or laptop at home | 0.89 | 0.50 | 0.40*** (0.03) | 2,851 |\n| **Use by place (last week)** | | | |\n| Home | 0.82 | 0.50 | 0.33*** (0.02) | 2,641 |\n| Internet cafe | 0.43 | 0.54 | -0.11*** (0.02) | 2,631 |\n| Friend's house | 0.20 | 0.23 | -0.02 (0.02) | 2,561 |\n| **Use by place (minutes yesterday)** | | | |\n| Home | 61.85 | 44.99 | 18.20*** (4.93) | 2,851 |\n| Internet cafe | 30.75 | 34.25 | -3.23 (3.67) | 2,851 |\n| Friend's house | 11.30 | 15.44 | -3.78* (2.06) | 2,851 |\n\n*Notes: The 'Adjusted Difference' column presents the estimated `δ`. Standard errors, clustered at the school level, are in parentheses. ***p<0.01, *p<0.10.*\n\nBased on the results in Table 1, which statement best describes the behavioral response of students who won a laptop?", "Options": {"A": "They substituted computer use at home for use in internet cafes, indicating the two are substitute goods.", "B": "They used the home laptop in addition to their previous computer activities, indicating the goods are complements.", "C": "The program increased computer use at home but had no significant effect on computer use elsewhere.", "D": "The total time spent using computers across all locations significantly decreased."}, "Answer": "A", "pi_justification": "This item assesses the ability to interpret regression coefficients from a table to identify a key behavioral pattern (substitution). It uses a Scenario Application strategy. Distractors are designed based on common conceptual errors: confusing substitutes with complements (B), ignoring significant coefficients (C), and miscalculating the net effect (D).", "qid": "211", "question": "### Background\n\n**Research Question.** This study investigates the behavioral responses of students to receiving a free home laptop, specifically examining how the intervention alters their patterns of computer access and use both at home and in other locations.\n\n**Setting / Institutional Environment.** In a randomized experiment in Lima, Peru, students within designated 'treatment schools' participated in lotteries for XO laptops intended for home use. The analysis compares outcomes for lottery winners (treatment) and non-winners (control) within the same schools.\n\n### Data / Model Specification\n\nThe causal effects are estimated using the following regression model, which includes class fixed effects (`\\mu_j`) and baseline controls (`\\mathbf{X_{ijk}}`):\n\n```latex\nY_{ijk} = \\beta'\\mathbf{X_{ijk}} + \\delta Winner_{ijk} + \\mu_{j} + \\varepsilon_{ijk} \\quad \\text{(Eq. 1)}\n```\n\nTable 1 presents the estimates for the treatment effect `\\delta` on outcomes related to computer access and use.\n\n**Table 1: Effects on Computer Access and Use**\n\n| | Winners | Non-winners | Adjusted Difference (ITT) | Observations |\n| :--- | :---: | :---: | :---: | :---: |\n| **Access** | | | |\n| Computer or laptop at home | 0.89 | 0.50 | 0.40*** (0.03) | 2,851 |\n| **Use by place (last week)** | | | |\n| Home | 0.82 | 0.50 | 0.33*** (0.02) | 2,641 |\n| Internet cafe | 0.43 | 0.54 | -0.11*** (0.02) | 2,631 |\n| Friend's house | 0.20 | 0.23 | -0.02 (0.02) | 2,561 |\n| **Use by place (minutes yesterday)** | | | |\n| Home | 61.85 | 44.99 | 18.20*** (4.93) | 2,851 |\n| Internet cafe | 30.75 | 34.25 | -3.23 (3.67) | 2,851 |\n| Friend's house | 11.30 | 15.44 | -3.78* (2.06) | 2,851 |\n\n*Notes: The 'Adjusted Difference' column presents the estimated `δ`. Standard errors, clustered at the school level, are in parentheses. ***p<0.01, *p<0.10.*\n\nBased on the results in Table 1, which statement best describes the behavioral response of students who won a laptop?", "model_evaluation_result": {"model_raw_response": "Looking at the results in Table 1, I need to analyze the behavioral response patterns of lottery winners compared to non-winners.\n\nKey findings from the data:\n\n**Home computer use increased significantly:**\n- Winners used computers at home 33 percentage points more frequently than non-winners\n- Winners spent 18.20 more minutes per day using computers at home\n\n**Internet cafe use decreased significantly:**\n- Winners were 11 percentage points less likely to use internet cafes\n- This represents a substantial decrease from the non-winner baseline of 54% usage\n\n**Friend's house use:**\n- No significant difference in frequency (last week)\n- Small but significant decrease in minutes per day (-3.78 minutes)\n\n**Pattern Analysis:**\nThe data shows a clear substitution pattern: as students gained access to computers at home through winning laptops, they reduced their computer use at internet cafes. The decrease in internet cafe usage (-11 percentage points) partially offsets the increase in home usage (+33 percentage points), indicating that students are replacing some of their internet cafe computer time with home computer time.\n\nThis is the classic economic behavior of substitute goods - when the price/accessibility of one good improves (free home laptop), consumption shifts away from the substitute good (paid internet cafe access).\n\nA", "model_answer": "A", "error": null}}
{"ID": 194, "Question": "### Background\n\nThis paper develops a theoretical model to understand the meaning of residual correlations around Engel curves. The analysis is grounded in the classical theory of consumer demand but makes specific assumptions to achieve analytical tractability when using cross-sectional budget data.\n\n### Data / Model Specification\n\nThe consumer's problem is to maximize a utility function `u(x)` subject to a budget constraint `p'x = M`. The model makes two key assumptions:\n\n**Assumption I:** The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`.\nThe vector `a` represents consumer-specific linear preference terms, while the matrix `B` of quadratic terms is assumed to be identical for all consumers.\n\n**Assumption II:** Prices `p` are also identical for all consumers in the budget survey.\n\nThe first-order conditions for utility maximization can be written as a linear system:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -a \\\\ -M \\end{pmatrix} \\quad \\text{(Eq. (1))}\n```\nThe inverse of the bordered matrix on the left defines the substitution matrix `C` and the income-effect vector `c`:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix}^{-1} = \\begin{pmatrix} C & c \\\\ c' & c_0 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n```\nThis leads to a demand system `x = -Ca - cM`. The Engel curve for good `i` is found by separating the mean preference `α` from the idiosyncratic preference `(a - α)`. The disturbance term `ν` for the vector of goods is `ν = -C(a - α)`.\n\n### Question\n\nIn the specified model, the joint assumption of a quadratic utility function (Assumption I) and a common quadratic coefficient matrix `B` (part of Assumption II) is crucial. What is the primary mathematical consequence of this setup that makes the paper's analysis of a single residual correlation across the whole sample theoretically valid?", "Options": {"A": "It ensures the substitution matrix `C` is constant for all families, regardless of their income level.", "B": "It guarantees that all goods are substitutes (`c_ij > 0`), simplifying the interpretation.", "C": "It forces the linear preference coefficients `a` to be the same for all families, removing all residual variation.", "D": "It ensures that the utility function is globally concave, satisfying the second-order conditions for a maximum."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of a critical foundational assumption. It uses a Reverse-Reasoning strategy. The correct answer (A) identifies the key implication for the model's validity. Distractor (B) is a false statement about the model's implications. Distractor (C) contradicts a core premise of the model (heterogeneity in `a`). Distractor (D) is a 'True but Irrelevant' statement, a sophisticated error where a true property of the model is cited, but it doesn't answer the specific question asked.", "qid": "194", "question": "### Background\n\nThis paper develops a theoretical model to understand the meaning of residual correlations around Engel curves. The analysis is grounded in the classical theory of consumer demand but makes specific assumptions to achieve analytical tractability when using cross-sectional budget data.\n\n### Data / Model Specification\n\nThe consumer's problem is to maximize a utility function `u(x)` subject to a budget constraint `p'x = M`. The model makes two key assumptions:\n\n**Assumption I:** The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`.\nThe vector `a` represents consumer-specific linear preference terms, while the matrix `B` of quadratic terms is assumed to be identical for all consumers.\n\n**Assumption II:** Prices `p` are also identical for all consumers in the budget survey.\n\nThe first-order conditions for utility maximization can be written as a linear system:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -a \\\\ -M \\end{pmatrix} \\quad \\text{(Eq. (1))}\n```\nThe inverse of the bordered matrix on the left defines the substitution matrix `C` and the income-effect vector `c`:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix}^{-1} = \\begin{pmatrix} C & c \\\\ c' & c_0 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n```\nThis leads to a demand system `x = -Ca - cM`. The Engel curve for good `i` is found by separating the mean preference `α` from the idiosyncratic preference `(a - α)`. The disturbance term `ν` for the vector of goods is `ν = -C(a - α)`.\n\n### Question\n\nIn the specified model, the joint assumption of a quadratic utility function (Assumption I) and a common quadratic coefficient matrix `B` (part of Assumption II) is crucial. What is the primary mathematical consequence of this setup that makes the paper's analysis of a single residual correlation across the whole sample theoretically valid?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand what makes the analysis of residual correlations theoretically valid across the entire sample.\n\nLet me trace through the key mathematical relationships:\n\n1. The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`\n2. The quadratic coefficient matrix `B` is identical for all consumers\n3. Prices `p` are identical for all consumers\n4. The first-order conditions lead to the inverse matrix relationship in Eq. (2)\n\nThe crucial insight is in how the substitution matrix `C` and income-effect vector `c` are defined. From Eq. (2), we see that:\n\n```\n[B  -p]^(-1) = [C  c]\n[-p'  0]       [c' c₀]\n```\n\nSince both `B` (the quadratic coefficient matrix) and `p` (the price vector) are assumed to be identical across all consumers, the bordered matrix on the left side is identical for all consumers. This means its inverse is also identical for all consumers.\n\nTherefore, the substitution matrix `C` and the income-effect vector `c` are the same for every family in the sample, regardless of their income level or individual preference parameters `a`.\n\nThis constancy of `C` across all families is what allows the paper to analyze residual correlations meaningfully across the entire sample - the structural relationships between goods (captured by `C`) don't vary with income or individual preferences.\n\nLooking at the other options:\n- B) is about the sign of coefficients, not the fundamental mathematical structure\n- C) is incorrect - the `a` coefficients vary across families (that's the source of heterogeneity)\n- D) is about optimization conditions, not the correlation analysis\n\nA", "model_answer": "B", "error": null}}
{"ID": 217, "Question": "### Background\n\nThis question analyzes the second-period pricing game in a duopoly model with switching costs. The setting is a standard spatial differentiation model (a Hotelling line of length *t*) where two firms, A and B, are located at the endpoints. In the second period, firms have established first-period market shares, *σ*ᴬ and *σ*ᴮ, and they set prices *p*₂ᴬ and *p*₂ᴮ simultaneously. Firms have a constant marginal cost *c*. Consumers incur a switching cost *s* if they change suppliers. The consumer base is heterogeneous: a fraction *ν* are new, a fraction *μ* have tastes that are now independent of their first-period location, and the remainder have unchanged tastes.\n\n### Data / Model Specification\n\nFirm A's second-period profit is *π*₂ᴬ = (*p*₂ᴬ - *c*)*q*₂ᴬ. Its total second-period sales, *q*₂ᴬ, are the sum of sales to the different consumer groups, which can be expressed as:\n```latex\nq_{2}^{A}(p_{2}^{A},p_{2}^{B}) = \\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right] \\quad \\text{(Eq. 1)}\n```\nAssume that the parameters are such that an interior solution exists and *μ*+*ν* > 0.\n\n### Question\n\nIn the second-period asymmetric equilibrium, what is the relationship between a firm's first-period market share (*σ*ᴬ) and its second-period price (*p*₂ᴬ), and what is the economic intuition behind it?", "Options": {"A": "The firm with the larger market share charges a higher price, as it has a greater incentive to exploit its large base of locked-in customers.", "B": "The firm with the larger market share charges a lower price to aggressively defend its position against its smaller rival.", "C": "Both firms charge the same price regardless of market share, because second-period competition eliminates any first-period advantage.", "D": "The firm with the larger market share charges a higher price, but this effect diminishes as the switching cost (*s*) increases."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses understanding of the core trade-off in the second-period asymmetric equilibrium, combining the result of a derivation with its economic interpretation. It uses a Computational Judgment strategy. Distractors include a conceptual opposite (B), an incorrect generalization of the symmetric case (C), and an 'almost right' statement with an incorrect relationship to a key parameter (D).", "qid": "217", "question": "### Background\n\nThis question analyzes the second-period pricing game in a duopoly model with switching costs. The setting is a standard spatial differentiation model (a Hotelling line of length *t*) where two firms, A and B, are located at the endpoints. In the second period, firms have established first-period market shares, *σ*ᴬ and *σ*ᴮ, and they set prices *p*₂ᴬ and *p*₂ᴮ simultaneously. Firms have a constant marginal cost *c*. Consumers incur a switching cost *s* if they change suppliers. The consumer base is heterogeneous: a fraction *ν* are new, a fraction *μ* have tastes that are now independent of their first-period location, and the remainder have unchanged tastes.\n\n### Data / Model Specification\n\nFirm A's second-period profit is *π*₂ᴬ = (*p*₂ᴬ - *c*)*q*₂ᴬ. Its total second-period sales, *q*₂ᴬ, are the sum of sales to the different consumer groups, which can be expressed as:\n```latex\nq_{2}^{A}(p_{2}^{A},p_{2}^{B}) = \\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right] \\quad \\text{(Eq. 1)}\n```\nAssume that the parameters are such that an interior solution exists and *μ*+*ν* > 0.\n\n### Question\n\nIn the second-period asymmetric equilibrium, what is the relationship between a firm's first-period market share (*σ*ᴬ) and its second-period price (*p*₂ᴬ), and what is the economic intuition behind it?", "model_evaluation_result": {"model_raw_response": "I need to analyze the second-period pricing game in this duopoly model with switching costs.\n\nGiven information:\n- Firm A's demand function: $q_{2}^{A}(p_{2}^{A},p_{2}^{B}) = \\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right]$\n- Firm A's profit: $\\pi_{2}^{A} = (p_{2}^{A} - c)q_{2}^{A}$\n- Consumer groups: fraction $\\nu$ are new, fraction $\\mu$ have independent tastes, remainder have unchanged tastes\n- Switching cost $s$ applies when consumers change suppliers\n\nTo find the equilibrium, I need to derive the first-order conditions. Taking the derivative of firm A's profit with respect to $p_{2}^{A}$:\n\n$\\frac{\\partial \\pi_{2}^{A}}{\\partial p_{2}^{A}} = q_{2}^{A} + (p_{2}^{A} - c)\\frac{\\partial q_{2}^{A}}{\\partial p_{2}^{A}} = 0$\n\nFrom the demand function:\n$\\frac{\\partial q_{2}^{A}}{\\partial p_{2}^{A}} = -\\frac{1}{2}(\\mu+\\nu)$\n\nThis gives us:\n$q_{2}^{A} - (p_{2}^{A} - c)\\frac{1}{2}(\\mu+\\nu) = 0$\n\nSubstituting the demand function and solving for $p_{2}^{A}$:\n$\\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right] = (p_{2}^{A", "model_answer": "A", "error": null}}
{"ID": 200, "Question": "### Background\n\n**Research Question.** This problem examines the core theoretical mechanism of a macro model with a fragile banking system. It focuses on how banks, in maximizing returns for their financiers, endogenously determine their leverage, risk, and capital accumulation.\n\n**Setting.** The model features a relationship-lending bank that finances projects with stochastic returns. The bank is funded by deposits and capital. The demandable nature of deposits exposes the bank to runs. A bank manager chooses the capital structure (leverage) to maximize the combined expected payoff to depositors and capitalists.\n\n### Data / Model Specification\n\nBank capital `BK_t` accumulates from the reinvested earnings of bank capitalists according to the following law of motion:\n```latex\nBK_{t}=\\frac{\\theta}{\\pi_{t}}[BK_{t-1}+\\varrho_{t}^{BK}Q_{t}K_{t}] \\quad \\text{(Eq. 1)}\n```\nwhere `θ` is the bank survival rate, `π_t` is inflation, `ϱ_t^{BK}` is the expected unitary payoff to capitalists (their profit rate), `Q_t` is the price of capital assets, and `K_t` is the stock of capital.\n\n### Question\n\nAccording to the model's 'bank balance sheet channel', a positive shock to the economy can strengthen bank capital, which in turn fuels a credit expansion that amplifies the initial shock. This amplification is most directly initiated by an increase in which variable from the capital accumulation equation (Eq. 1)?", "Options": {"A": "`Q_t` (Asset Prices)", "B": "`ϱ_t^{BK}` (Unitary Payoff to Capitalists)", "C": "`BK_{t-1}` (Lagged Bank Capital)", "D": "`θ` (Bank Survival Rate)"}, "Answer": "A", "pi_justification": "This item assesses understanding of the bank balance sheet channel using a Reverse-Reasoning strategy. Given the outcome (credit expansion), the student must identify the primary trigger within the model's structure. The correct answer is `Q_t`, the asset price, which directly increases the valuation of the bank's asset base (`Q_t K_t`). Distractors represent plausible but incorrect alternatives: `ϱ_t^{BK}` is the profit rate not the valuation shock, `BK_{t-1}` is a state variable not the shock, and `θ` is an irrelevant structural parameter.", "qid": "200", "question": "### Background\n\n**Research Question.** This problem examines the core theoretical mechanism of a macro model with a fragile banking system. It focuses on how banks, in maximizing returns for their financiers, endogenously determine their leverage, risk, and capital accumulation.\n\n**Setting.** The model features a relationship-lending bank that finances projects with stochastic returns. The bank is funded by deposits and capital. The demandable nature of deposits exposes the bank to runs. A bank manager chooses the capital structure (leverage) to maximize the combined expected payoff to depositors and capitalists.\n\n### Data / Model Specification\n\nBank capital `BK_t` accumulates from the reinvested earnings of bank capitalists according to the following law of motion:\n```latex\nBK_{t}=\\frac{\\theta}{\\pi_{t}}[BK_{t-1}+\\varrho_{t}^{BK}Q_{t}K_{t}] \\quad \\text{(Eq. 1)}\n```\nwhere `θ` is the bank survival rate, `π_t` is inflation, `ϱ_t^{BK}` is the expected unitary payoff to capitalists (their profit rate), `Q_t` is the price of capital assets, and `K_t` is the stock of capital.\n\n### Question\n\nAccording to the model's 'bank balance sheet channel', a positive shock to the economy can strengthen bank capital, which in turn fuels a credit expansion that amplifies the initial shock. This amplification is most directly initiated by an increase in which variable from the capital accumulation equation (Eq. 1)?", "model_evaluation_result": {"model_raw_response": "Looking at the bank capital accumulation equation and the mechanism described:\n\nThe \"bank balance sheet channel\" describes how a positive economic shock strengthens bank capital, leading to credit expansion that amplifies the initial shock.\n\nFrom equation (1): `BK_t = (θ/π_t)[BK_{t-1} + ϱ_t^{BK}Q_t K_t]`\n\nFor this amplification mechanism to work, the positive shock must increase a variable that:\n1. Directly increases current bank capital (BK_t)\n2. Can respond immediately to economic conditions\n3. Creates the foundation for credit expansion\n\nAnalyzing each option:\n\nA) `Q_t` (Asset Prices) - Asset prices typically rise with positive economic shocks and directly increase the value term `ϱ_t^{BK}Q_t K_t` in the equation, immediately strengthening bank capital.\n\nB) `ϱ_t^{BK}` (Unitary Payoff to Capitalists) - While this could increase with better economic conditions, it's more of an outcome of banking decisions rather than the initial trigger.\n\nC) `BK_{t-1}` (Lagged Bank Capital) - This is predetermined from the previous period and cannot respond to current shocks.\n\nD) `θ` (Bank Survival Rate) - This is typically a structural parameter that doesn't fluctuate with short-term economic conditions.\n\nThe most direct channel for a positive economic shock to immediately strengthen bank capital is through rising asset prices (`Q_t`), which increases the value of the bank's capital base and initiates the amplification process.\n\nA", "model_answer": "A", "error": null}}
{"ID": 262, "Question": "### Background\n\n**Research Question.** This problem examines the practical consequences of using the correct asymptotic distribution for forecast encompassing tests when applied to nested linear models.\n\n**Setting / Institutional Environment.** We compare a restricted forecasting model (Model 1) to an unrestricted one (Model 2) that includes `k_2` additional parameters. The total sample is split into an in-sample period of size `R` and an out-of-sample period of size `P`. The limiting ratio is `π = lim P/R`.\n\n---\n\n### Data / Model Specification\n\nThe analysis shows that when `0 < π < ∞`, the limiting distribution of the ENC-T test is non-standard and depends on `π` and `k_2`. When `π = 0`, the limiting distribution is standard normal N(0,1).\n\n**Table 1: Selected 90th Percentiles (10% Critical Values) of the Asymptotic Distribution for ENC-T**\n| k_2 | π=0.2 | π=1.0 |\n|:---:|:-----:|:-----:|\n| 1 | 1.002 | 0.955 |\n| 2 | 1.101 | 1.066 |\n\n---\n\n### Question\n\nA researcher has a sample with `R=100` in-sample observations and `P=100` out-of-sample predictions, so their sample ratio is `π̂ = 1.0`. Their unrestricted model includes `k_2=1` extra predictor. They calculate an ENC-T statistic of 1.15. What is the correct conclusion for a one-sided test at the 10% significance level?", "Options": {"A": "Reject the null hypothesis, as the statistic (1.15) exceeds the correct 10% critical value from Table 1 (0.955).", "B": "Fail to reject the null hypothesis, as the statistic (1.15) is less than the standard normal 10% critical value (1.282).", "C": "Fail to reject the null hypothesis, as the statistic (1.15) is less than the critical value for `k_2=2` and `π=0.2` (1.101).", "D": "Reject the null hypothesis, as the statistic (1.15) exceeds the critical value for `k_2=2` and `π=1.0` (1.066)."}, "Answer": "A", "pi_justification": "This item assesses the ability to apply the paper's central theoretical finding to a practical inference problem. It uses a 'Computational Judgment' strategy. The correct answer (A) requires correctly identifying `k_2=1` and `π=1.0` in the table, retrieving the correct critical value (0.955), and applying the correct decision rule (Reject if statistic > CV). Distractor (B) represents the most common and critical error: incorrectly using the standard normal critical value, leading to the opposite conclusion. Distractor (C) represents a table misreading error (wrong row/column) combined with an incorrect decision. Distractor (D) represents a different table misreading error (`k_2=2` instead of `k_2=1`) which leads to the right conclusion for the wrong reason.", "qid": "262", "question": "### Background\n\n**Research Question.** This problem examines the practical consequences of using the correct asymptotic distribution for forecast encompassing tests when applied to nested linear models.\n\n**Setting / Institutional Environment.** We compare a restricted forecasting model (Model 1) to an unrestricted one (Model 2) that includes `k_2` additional parameters. The total sample is split into an in-sample period of size `R` and an out-of-sample period of size `P`. The limiting ratio is `π = lim P/R`.\n\n---\n\n### Data / Model Specification\n\nThe analysis shows that when `0 < π < ∞`, the limiting distribution of the ENC-T test is non-standard and depends on `π` and `k_2`. When `π = 0`, the limiting distribution is standard normal N(0,1).\n\n**Table 1: Selected 90th Percentiles (10% Critical Values) of the Asymptotic Distribution for ENC-T**\n| k_2 | π=0.2 | π=1.0 |\n|:---:|:-----:|:-----:|\n| 1 | 1.002 | 0.955 |\n| 2 | 1.101 | 1.066 |\n\n---\n\n### Question\n\nA researcher has a sample with `R=100` in-sample observations and `P=100` out-of-sample predictions, so their sample ratio is `π̂ = 1.0`. Their unrestricted model includes `k_2=1` extra predictor. They calculate an ENC-T statistic of 1.15. What is the correct conclusion for a one-sided test at the 10% significance level?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to identify the correct critical value and compare it to the test statistic of 1.15.\n\nGiven information:\n- R = 100, P = 100, so π̂ = P/R = 1.0\n- k₂ = 1 (one extra predictor in unrestricted model)\n- ENC-T statistic = 1.15\n- One-sided test at 10% significance level\n\nFrom Table 1, I need to find the 90th percentile (10% critical value) for:\n- k₂ = 1\n- π = 1.0\n\nLooking at the table:\nFor k₂ = 1 and π = 1.0, the critical value is 0.955.\n\nSince this is a one-sided test at the 10% level, I reject the null hypothesis if the test statistic exceeds the critical value.\n\nComparison: 1.15 > 0.955\n\nTherefore, I reject the null hypothesis because the test statistic (1.15) exceeds the correct critical value (0.955).\n\nA", "model_answer": "A", "error": null}}
{"ID": 241, "Question": "### Background\n\n**Research Question:** This problem investigates the practical performance of the Generalized Stochastic Simulation Algorithm (GSSA), focusing on the critical role of the integration method for accuracy and efficiency.\n\n**Setting / Institutional Environment:** The analysis uses numerical experiments from a standard representative-agent model to compare different integration methods.\n\n**Variables & Parameters:**\n- `MC(J)`: Monte Carlo integration with `J` random draws.\n- `Q(J)`: Gauss-Hermite quadrature with `J` deterministic nodes.\n- `T`: The simulation length.\n- `E_mean`: Mean absolute unit-free Euler error (in log10 units).\n- `CPU`: Computation time in seconds.\n\n---\n\n### Data / Model Specification\n\nThe following table provides results from the paper's numerical experiments for a 5th degree polynomial approximation.\n\n**Table 1: Accuracy of Integration Methods (Representative-Agent Model)**\n\n| Integration Method | T | `E_mean` (log10) | CPU (sec) |\n| :--- | :---: | :---: | :---: |\n| MC(1) | 10,000 | -4.23 | 80 |\n| MC(2000) | 10,000 | -6.07 | 4551 |\n| Q(1) | 100 | -6.22 | 7 |\n\n---\n\n### Question\n\nBased on the data in Table 1, which statement most accurately compares the efficiency of the most computationally intensive Monte Carlo method shown (MC(2000)) with the simplest deterministic method shown (Q(1))?\n", "Options": {"A": "The Q(1) method is both more accurate and over 600 times faster, indicating that sampling error, not computational time, is the primary bottleneck for accuracy in Monte Carlo methods.", "B": "The MC(2000) method is significantly more accurate because it uses more integration nodes, although it is much slower than the Q(1) method.", "C": "Both methods achieve similar levels of accuracy, but the Q(1) method is faster, making it only marginally more efficient.", "D": "The Q(1) method is faster but less accurate than the MC(2000) method, presenting a classic speed-accuracy trade-off."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem is highly suitable for conversion. Its components rely on table lookups, a precise calculation, and a structured recommendation, all of which can be tested with high fidelity. Misconception potential is high (e.g., misinterpreting log errors, calculation slips, suboptimal strategic choices), allowing for strong distractors.\n\nThis item assesses the core conclusion from Part 1 of the original QA. It uses a Computational Judgment strategy, requiring the student to compare values from the table and draw a conclusion about relative efficiency. Distractor B is a Conceptual Opposite, incorrectly claiming MC(2000) is more accurate by misinterpreting the log10 errors (-6.07 > -6.22). Distractor D also misinterprets accuracy. Distractor C is an 'Almost Right' option that correctly identifies Q(1) as faster but understates the massive difference in efficiency.", "qid": "241", "question": "### Background\n\n**Research Question:** This problem investigates the practical performance of the Generalized Stochastic Simulation Algorithm (GSSA), focusing on the critical role of the integration method for accuracy and efficiency.\n\n**Setting / Institutional Environment:** The analysis uses numerical experiments from a standard representative-agent model to compare different integration methods.\n\n**Variables & Parameters:**\n- `MC(J)`: Monte Carlo integration with `J` random draws.\n- `Q(J)`: Gauss-Hermite quadrature with `J` deterministic nodes.\n- `T`: The simulation length.\n- `E_mean`: Mean absolute unit-free Euler error (in log10 units).\n- `CPU`: Computation time in seconds.\n\n---\n\n### Data / Model Specification\n\nThe following table provides results from the paper's numerical experiments for a 5th degree polynomial approximation.\n\n**Table 1: Accuracy of Integration Methods (Representative-Agent Model)**\n\n| Integration Method | T | `E_mean` (log10) | CPU (sec) |\n| :--- | :---: | :---: | :---: |\n| MC(1) | 10,000 | -4.23 | 80 |\n| MC(2000) | 10,000 | -6.07 | 4551 |\n| Q(1) | 100 | -6.22 | 7 |\n\n---\n\n### Question\n\nBased on the data in Table 1, which statement most accurately compares the efficiency of the most computationally intensive Monte Carlo method shown (MC(2000)) with the simplest deterministic method shown (Q(1))?\n", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1:\n\n- MC(2000): E_mean = -6.07, CPU = 4551 seconds\n- Q(1): E_mean = -6.22, CPU = 7 seconds\n\nLet me analyze the comparison:\n\n**Accuracy:** Q(1) has E_mean = -6.22 vs MC(2000) has E_mean = -6.07. Since these are log10 units of error, a more negative number indicates lower error (better accuracy). Therefore, Q(1) is slightly more accurate.\n\n**Speed:** Q(1) takes 7 seconds vs MC(2000) takes 4551 seconds. The ratio is 4551/7 = 650, so Q(1) is about 650 times faster (over 600 times faster).\n\n**Interpretation:** The fact that a simple deterministic method (Q(1)) with just 1 node outperforms a Monte Carlo method with 2000 draws suggests that the limitation in Monte Carlo accuracy comes from sampling error rather than computational constraints. Even with 2000 draws, MC still has sampling variability, while the deterministic quadrature method eliminates this source of error.\n\nThe answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 232, "Question": "### Background\n\n**Research Question.** This problem investigates the macroeconomic management of a large foreign aid inflow, focusing on the government's fiscal response (tax effort).\n\n**Setting.** The model incorporates a behavioral equation for the government's tax effort. A key concern in the literature is that aid may create a disincentive for governments to collect domestic taxes.\n\n**Variables & Parameters.**\n- `TXR`: Effective tax rate on domestic sales.\n- `AID/NGDP`: Total foreign aid as a share of GDP.\n- `a4`: Coefficient on the log aid-to-GDP ratio in the `TXR` equation.\n\n---\n\n### Data / Model Specification\n\n**Fiscal Response:** The effective tax rate `TXR` is modeled as a function of past behavior and the current aid-to-GDP ratio.\n```latex\n\\log(\\mathrm{TXR})_t = a_1 + a_2 \\cdot \\log(\\mathrm{TXR})_{t-1} + a_3 \\cdot \\log(\\mathrm{GTOT}/\\mathrm{NGDP})_{t-1} + a_4 \\cdot \\log(\\mathrm{AID}/\\mathrm{NGDP})_t + \\nu_t \n```\n\n**Table 1: OLS Estimation Results for Domestic Tax Rate**\n| Coefficient | Estimated Value | Two-standard-error confidence interval |\n| :--- | :--- | :--- |\n| `a4` on `log(AID/NGDP)` | -0.122 | [-0.336, 0.093] |\n\n---\n\n### Question\n\nA common concern is that foreign aid might undermine a government's incentive to collect taxes. Based on the provided OLS estimation results for the effective tax rate (`TXR`) in Table 1, what can be concluded about this 'fiscal response' in Ethiopia?", "Options": {"A": "The estimated effect of aid on the tax rate is statistically insignificant, so there is no robust evidence that aid harms tax collection.", "B": "The estimated effect of aid on the tax rate is highly significant and confirms that aid severely undermines tax collection.", "C": "The negative point estimate (-0.122) proves that aid harms tax collection, even though the effect is not statistically significant.", "D": "The effect is statistically significant because the confidence interval is not symmetric around the point estimate."}, "Answer": "A", "pi_justification": "This item assesses the ability to correctly interpret statistical significance from regression output, a foundational skill in econometrics. It uses a Computational Judgment strategy. The distractors are designed to trap common errors: C incorrectly prioritizes the point estimate over significance, B misreads significance entirely, and D represents a misunderstanding of how confidence intervals are used to test hypotheses.", "qid": "232", "question": "### Background\n\n**Research Question.** This problem investigates the macroeconomic management of a large foreign aid inflow, focusing on the government's fiscal response (tax effort).\n\n**Setting.** The model incorporates a behavioral equation for the government's tax effort. A key concern in the literature is that aid may create a disincentive for governments to collect domestic taxes.\n\n**Variables & Parameters.**\n- `TXR`: Effective tax rate on domestic sales.\n- `AID/NGDP`: Total foreign aid as a share of GDP.\n- `a4`: Coefficient on the log aid-to-GDP ratio in the `TXR` equation.\n\n---\n\n### Data / Model Specification\n\n**Fiscal Response:** The effective tax rate `TXR` is modeled as a function of past behavior and the current aid-to-GDP ratio.\n```latex\n\\log(\\mathrm{TXR})_t = a_1 + a_2 \\cdot \\log(\\mathrm{TXR})_{t-1} + a_3 \\cdot \\log(\\mathrm{GTOT}/\\mathrm{NGDP})_{t-1} + a_4 \\cdot \\log(\\mathrm{AID}/\\mathrm{NGDP})_t + \\nu_t \n```\n\n**Table 1: OLS Estimation Results for Domestic Tax Rate**\n| Coefficient | Estimated Value | Two-standard-error confidence interval |\n| :--- | :--- | :--- |\n| `a4` on `log(AID/NGDP)` | -0.122 | [-0.336, 0.093] |\n\n---\n\n### Question\n\nA common concern is that foreign aid might undermine a government's incentive to collect taxes. Based on the provided OLS estimation results for the effective tax rate (`TXR`) in Table 1, what can be concluded about this 'fiscal response' in Ethiopia?", "model_evaluation_result": {"model_raw_response": "Looking at the OLS estimation results in Table 1:\n\n- The coefficient `a4` on `log(AID/NGDP)` is estimated at -0.122\n- The two-standard-error confidence interval is [-0.336, 0.093]\n\nTo assess statistical significance, I need to check if the confidence interval includes zero. The interval [-0.336, 0.093] does include zero, which means the estimated effect is not statistically significant at conventional levels.\n\nWhile the point estimate is negative (-0.122), suggesting that aid might reduce tax collection, this effect cannot be distinguished from zero with statistical confidence. Therefore, we cannot conclude that there is robust evidence of aid harming tax collection.\n\nOption A correctly identifies that the effect is statistically insignificant and appropriately concludes there is no robust evidence of harm to tax collection.\n\nOption B is incorrect because the effect is not statistically significant.\n\nOption C is incorrect because a non-significant result does not \"prove\" anything about the true effect.\n\nOption D is incorrect because asymmetry of a confidence interval does not determine statistical significance - what matters is whether the interval includes zero.\n\nA", "model_answer": "A", "error": null}}
{"ID": 298, "Question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to distinguish between two competing behavioral explanations for an observed choice anomaly.\n\n**Setting.** In a primary experiment, subjects chose between three products (High, Medium, Low benefit) in a two-stage process: first choose a product, then choose one of its associated costs. The key finding was that when costs were presented in a complex arithmetic format (`COMPLEX COST` treatment), subjects made systematically biased mistakes, choosing the suboptimal High-benefit option far more often than the equally suboptimal Low-benefit option. This rejected the null hypothesis of symmetric, random mistakes.\n\nTwo potential explanations for this bias are considered:\n1.  **Salience:** Complexity in the cost dimension makes the simple, easy-to-evaluate benefit dimension more salient. Decision-makers overweight this salient dimension, leading them to be attracted to the High-benefit option.\n2.  **Narrow Bracketing:** Decision-makers simplify the two-stage problem by ignoring the future cost-selection stage. They focus only on the first stage and choose the option with the highest immediate benefit (the High option), without fully integrating the downstream consequences of costs.\n\nTo distinguish between these two explanations, a follow-up treatment was conducted where subjects made a single choice from a menu of nine options (e.g., 'Loan A with Repayment X', 'Loan A with Repayment Y', etc.), effectively choosing the product and its cost at the same time.\n\n### The Question\n\nWhat is the primary causal logic for using a 'Simultaneous Choice' follow-up experiment to distinguish between the Salience and Narrow Bracketing theories?", "Options": {"A": "The simultaneous design makes the Narrow Bracketing strategy of ignoring the future cost stage difficult or impossible, thus generating a different choice prediction than the Salience theory.", "B": "The simultaneous design increases the overall cognitive load, which should reduce the bias if it is caused by Salience.", "C": "The simultaneous design gives subjects more total time to decide, allowing them to overcome the complexity and make more optimal choices under either theory.", "D": "The simultaneous design forces subjects to compare nine distinct options, which tests for aversion to large choice sets, a separate behavioral bias."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question assesses the ability to understand the logic of experimental design for causal identification. It uses a 'Reverse-Reasoning' strategy, asking for the rationale behind the design. The correct answer (A) identifies the core manipulation: disabling the Narrow Bracketing heuristic. Distractors are based on flawed causal interpretations (B) or true but irrelevant side effects of the design change (C, D).", "qid": "298", "question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to distinguish between two competing behavioral explanations for an observed choice anomaly.\n\n**Setting.** In a primary experiment, subjects chose between three products (High, Medium, Low benefit) in a two-stage process: first choose a product, then choose one of its associated costs. The key finding was that when costs were presented in a complex arithmetic format (`COMPLEX COST` treatment), subjects made systematically biased mistakes, choosing the suboptimal High-benefit option far more often than the equally suboptimal Low-benefit option. This rejected the null hypothesis of symmetric, random mistakes.\n\nTwo potential explanations for this bias are considered:\n1.  **Salience:** Complexity in the cost dimension makes the simple, easy-to-evaluate benefit dimension more salient. Decision-makers overweight this salient dimension, leading them to be attracted to the High-benefit option.\n2.  **Narrow Bracketing:** Decision-makers simplify the two-stage problem by ignoring the future cost-selection stage. They focus only on the first stage and choose the option with the highest immediate benefit (the High option), without fully integrating the downstream consequences of costs.\n\nTo distinguish between these two explanations, a follow-up treatment was conducted where subjects made a single choice from a menu of nine options (e.g., 'Loan A with Repayment X', 'Loan A with Repayment Y', etc.), effectively choosing the product and its cost at the same time.\n\n### The Question\n\nWhat is the primary causal logic for using a 'Simultaneous Choice' follow-up experiment to distinguish between the Salience and Narrow Bracketing theories?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 223, "Question": "### Background\n\nAn economy features risk-averse workers and risk-neutral firms. Labor services are traded in a **spot market** where wages and prices are fully flexible. The economy is subject to real technology shocks (`s_t`) and nominal money supply shocks (`x_t`).\n\n### Data / Model Specification\n\nThe equilibrium price level and nominal wage in the spot market are given by:\n\n```latex\np_t(s,x) = \\frac{m_{t-1}x_t}{F g(s_t)} \\quad \\text{(Eq. 1)}\n```\n\n```latex\nw_t(s,x) = \\frac{m_{t-1}x_t \\phi(s_t)}{F} \\quad \\text{(Eq. 2)}\n```\n\nwhere `g(s) = f(\\gamma, s)` and `\\phi(s) = f_l(\\gamma, s) / f(\\gamma, s)`. A worker from generation `t` earns `w_t` in their youth and consumes `c^s = w_t x_{t+1} / p_{t+1}` in their old age.\n\n### Question\n\nGiven the spot market equilibrium, which of the following correctly states the real consumption `c^s` for a worker of generation `t` and accurately describes its properties?", "Options": {"A": "`c^s = \\phi(s_t) g(s_{t+1})`. Consumption is independent of nominal shocks (`x_t`, `x_{t+1}`), so money is neutral.", "B": "`c^s = k \\bar{\\phi} g(s_{t+1}) / x_t`. Consumption depends on the nominal shock `x_t`, so money is not neutral.", "C": "`c^s = \\phi(s_t) g(s_{t+1}) / x_t`. Consumption depends on the nominal shock `x_t` due to price level effects.", "D": "`c^s = \\bar{\\phi} g(s_{t+1})`. The worker is fully insured against the youth-period technology shock `s_t`."}, "Answer": "A", "pi_justification": "This item replaces part of a QA problem (Suitability Score: 9.5). It uses the Computational Judgment strategy, requiring the student to perform the derivation for spot market consumption and identify its core properties. Distractors are designed to catch common errors: (B) confusing the spot market with the contract market outcome, (C) an algebraic error where a term fails to cancel, and (D) misattributing the insurance properties of a contract market to the spot market.", "qid": "223", "question": "### Background\n\nAn economy features risk-averse workers and risk-neutral firms. Labor services are traded in a **spot market** where wages and prices are fully flexible. The economy is subject to real technology shocks (`s_t`) and nominal money supply shocks (`x_t`).\n\n### Data / Model Specification\n\nThe equilibrium price level and nominal wage in the spot market are given by:\n\n```latex\np_t(s,x) = \\frac{m_{t-1}x_t}{F g(s_t)} \\quad \\text{(Eq. 1)}\n```\n\n```latex\nw_t(s,x) = \\frac{m_{t-1}x_t \\phi(s_t)}{F} \\quad \\text{(Eq. 2)}\n```\n\nwhere `g(s) = f(\\gamma, s)` and `\\phi(s) = f_l(\\gamma, s) / f(\\gamma, s)`. A worker from generation `t` earns `w_t` in their youth and consumes `c^s = w_t x_{t+1} / p_{t+1}` in their old age.\n\n### Question\n\nGiven the spot market equilibrium, which of the following correctly states the real consumption `c^s` for a worker of generation `t` and accurately describes its properties?", "model_evaluation_result": {"model_raw_response": "I need to derive the real consumption for a worker from generation t and analyze its properties.\n\nGiven information:\n- Worker earns wage w_t in youth\n- Consumes c^s = w_t x_{t+1} / p_{t+1} in old age\n- Price level: p_t(s,x) = (m_{t-1}x_t)/(F g(s_t))\n- Nominal wage: w_t(s,x) = (m_{t-1}x_t φ(s_t))/F\n\nLet me substitute the expressions:\n\nFor a generation t worker:\n- Earns w_t = (m_{t-1}x_t φ(s_t))/F in youth\n- Consumes c^s = w_t x_{t+1} / p_{t+1} in old age\n\nThe price level in period t+1 is:\np_{t+1} = (m_t x_{t+1})/(F g(s_{t+1}))\n\nSince m_t = m_{t-1}x_t (money supply evolves), we have:\np_{t+1} = (m_{t-1}x_t x_{t+1})/(F g(s_{t+1}))\n\nNow substituting into the consumption formula:\nc^s = w_t x_{t+1} / p_{t+1}\nc^s = [(m_{t-1}x_t φ(s_t))/F] × x_{t+1} / [(m_{t-1}x_t x_{t+1})/(F g(s_{t+1}))]\nc^s = [(m_{t-1}x_t φ(s_t))/F] × x_{t+1} × [F g(s_{t+1})/(m_{t-1}x_t x_{t+1})]\nc^s = φ(s_t) g(s_{t+1})\n\nThe nominal shocks x_t and x_{t+1} cancel out completely, leaving only the real technology shocks s_t and s_{t+1}.\n\nThis means:\n1", "model_answer": "I need to derive the real consumption for a worker from generation t and analyze its properties.\n\nGiven information:\n- Worker earns wage w_t in youth\n- Consumes c^s = w_t x_{t+1} / p_{t+1} in old age\n- Price level: p_t(s,x) = (m_{t-1}x_t)/(F g(s_t))\n- Nominal wage: w_t(s,x) = (m_{t-1}x_t φ(s_t))/F\n\nLet me substitute the expressions:\n\nFor a generation t worker:\n- Earns w_t = (m_{t-1}x_t φ(s_t))/F in youth\n- Consumes c^s = w_t x_{t+1} / p_{t+1} in old age\n\nThe price level in period t+1 is:\np_{t+1} = (m_t x_{t+1})/(F g(s_{t+1}))\n\nSince m_t = m_{t-1}x_t (money supply evolves), we have:\np_{t+1} = (m_{t-1}x_t x_{t+1})/(F g(s_{t+1}))\n\nNow substituting into the consumption formula:\nc^s = w_t x_{t+1} / p_{t+1}\nc^s = [(m_{t-1}x_t φ(s_t))/F] × x_{t+1} / [(m_{t-1}x_t x_{t+1})/(F g(s_{t+1}))]\nc^s = [(m_{t-1}x_t φ(s_t))/F] × x_{t+1} × [F g(s_{t+1})/(m_{t-1}x_t x_{t+1})]\nc^s = φ(s_t) g(s_{t+1})\n\nThe nominal shocks x_t and x_{t+1} cancel out completely, leaving only the real technology shocks s_t and s_{t+1}.\n\nThis means:\n1", "error": "Warning: Could not extract single choice letter from: I need to derive the real consumption for a worker..."}}
{"ID": 206, "Question": "### Background\n\n**Research Question.** This problem investigates the emergence of market failures within a mature, largely private financial system and the consequent role of government intervention.\n\n**Setting.** The context is the British capital market from the late 19th to mid-20th century. This market is described as having a \"statist start\" (Bank of England founded to fund government debt) but subsequently following an \"orthodox path\" of private institutional development, integrating local markets into a national one centered in London.\n\n**Key Concepts.**\n\n*   **Orthodox Path:** A historical mode of financial development characterized by the spontaneous evolution of institutions to contain lender exuberance and integrate markets, driven by private actors.\n*   **Market Gaps:** The text identifies two persistent failures in the British market:\n    1.  **The Macmillan Gap:** A market failure in equity finance for medium-sized enterprises. Firms needing less than £100,000 could raise capital locally, while those needing more than £1,500,000 could issue shares in London. The gap existed for capital sums between these thresholds.\n    2.  **The Temporal Gap:** A lack of financing for medium-term debt (5 to 10 years), which was needed for equipment finance. The market was well-developed for very short-term trade finance (90-180 days) and long-term borrowing (10-20 years), but not for the intermediate maturities.\n*   **Coase Theorem (in this context):** The proposition that if transaction costs are low, the specific form of institutions does not matter for achieving efficient outcomes, as private arrangements will emerge to fill any needs.\n*   **Corrective Government Institutions:** Specialized agencies created to fill identified market gaps, including:\n    *   **Industrial and Commercial Finance Corporation (ICFC):** Established after 1945 \"to fill the Macmillan gap.\"\n    *   **Export Credit Guarantees Department (ECGD):** Established in the 1930s \"to assist in financing the export of capital equipment.\"\n\n---\n\n### Data / Model Specification\n\nThis is a conceptual problem based on historical analysis. The key data points are the financial thresholds defining the Macmillan Gap.\n\n---\n\n### The Question\n\nConsider a hypothetical medium-sized British industrial firm in 1950 seeking a £500,000 loan with a 7-year maturity to finance new factory equipment for *domestic* production. This request falls into both the Macmillan Gap (by size) and the temporal gap (by maturity). Based on the mandates described in the text, which institution was the appropriate source of finance for this firm?", "Options": {"A": "The Industrial and Commercial Finance Corporation (ICFC).", "B": "The Export Credit Guarantees Department (ECGD).", "C": "Both the ICFC and the ECGD would be equally appropriate.", "D": "Neither, because the firm requires debt financing, whereas the Macmillan Gap was primarily about equity capital."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to apply specific factual knowledge about policy institutions to a novel scenario. It uses a Scenario Application strategy. The distractors are designed to test for common comprehension errors: B is an 'Almost Right' option that ignores a key qualifier ('export'); C represents a failure to differentiate; D represents a plausible but incorrect inference based on a detail from the text.", "qid": "206", "question": "### Background\n\n**Research Question.** This problem investigates the emergence of market failures within a mature, largely private financial system and the consequent role of government intervention.\n\n**Setting.** The context is the British capital market from the late 19th to mid-20th century. This market is described as having a \"statist start\" (Bank of England founded to fund government debt) but subsequently following an \"orthodox path\" of private institutional development, integrating local markets into a national one centered in London.\n\n**Key Concepts.**\n\n*   **Orthodox Path:** A historical mode of financial development characterized by the spontaneous evolution of institutions to contain lender exuberance and integrate markets, driven by private actors.\n*   **Market Gaps:** The text identifies two persistent failures in the British market:\n    1.  **The Macmillan Gap:** A market failure in equity finance for medium-sized enterprises. Firms needing less than £100,000 could raise capital locally, while those needing more than £1,500,000 could issue shares in London. The gap existed for capital sums between these thresholds.\n    2.  **The Temporal Gap:** A lack of financing for medium-term debt (5 to 10 years), which was needed for equipment finance. The market was well-developed for very short-term trade finance (90-180 days) and long-term borrowing (10-20 years), but not for the intermediate maturities.\n*   **Coase Theorem (in this context):** The proposition that if transaction costs are low, the specific form of institutions does not matter for achieving efficient outcomes, as private arrangements will emerge to fill any needs.\n*   **Corrective Government Institutions:** Specialized agencies created to fill identified market gaps, including:\n    *   **Industrial and Commercial Finance Corporation (ICFC):** Established after 1945 \"to fill the Macmillan gap.\"\n    *   **Export Credit Guarantees Department (ECGD):** Established in the 1930s \"to assist in financing the export of capital equipment.\"\n\n---\n\n### Data / Model Specification\n\nThis is a conceptual problem based on historical analysis. The key data points are the financial thresholds defining the Macmillan Gap.\n\n---\n\n### The Question\n\nConsider a hypothetical medium-sized British industrial firm in 1950 seeking a £500,000 loan with a 7-year maturity to finance new factory equipment for *domestic* production. This request falls into both the Macmillan Gap (by size) and the temporal gap (by maturity). Based on the mandates described in the text, which institution was the appropriate source of finance for this firm?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze which institution would be appropriate for a £500,000, 7-year loan for domestic factory equipment.\n\nLet me examine each institution's mandate:\n\n**ICFC (Industrial and Commercial Finance Corporation):**\n- Established \"to fill the Macmillan gap\"\n- The Macmillan gap was specifically described as \"a market failure in equity finance for medium-sized enterprises\"\n- The gap existed for capital sums between £100,000 and £1,500,000\n- The firm's £500,000 need falls squarely within this range\n\n**ECGD (Export Credit Guarantees Department):**\n- Established \"to assist in financing the export of capital equipment\"\n- This is specifically for export financing\n- The firm's equipment is for domestic production, not export\n\n**Key considerations:**\n- The loan is for £500,000 (within the Macmillan gap range)\n- It's for domestic production (not export)\n- The question asks about debt financing, but the Macmillan gap was described as primarily about equity finance\n\nHowever, the ICFC was created specifically to address the financing needs of medium-sized enterprises that fell into the Macmillan gap, regardless of whether it was technically equity or debt. The institution would have been the natural government response to fill this market failure.\n\nThe ECGD is clearly inappropriate since this is domestic, not export, financing.\n\nD", "model_answer": "D", "error": null}}
{"ID": 300, "Question": "### Background\n\n**Research Question.** This problem investigates the micro-level drivers of the change in the aggregate imported input share following a large devaluation, focusing on the puzzle of why import-intensive firms expand.\n\n**Setting / Institutional Environment.** The analysis uses firm-level data from Mexico (post-1995 devaluation) to decompose the change in the aggregate imported input share into components attributable to within-firm changes versus between-firm reallocation of market share.\n\n### Data / Model Specification\n\nThe growth in the aggregate imported input share (\\(\\Delta S_I\\)) is decomposed into several components. The two key components for continuing firms are:\n-   **Within:** Captures changes in firms' own import shares, holding their initial market shares constant. It reflects firm-level substitution.\n-   **Between:** Captures the reallocation of market share across firms, holding their initial import shares constant.\n\n**Table 1: Decomposition for Mexico (change from 1994 to 1997, in percentage points)**\n| Year | Within | Between | Covariance | Net entry | All |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| 1997 | -1.54 | 12.34 | 2.61 | 6.95 | 20.36 |\n\nBased on the decomposition in Table 1, what do the 'Within' and 'Between' components reveal about firm behavior, and why is the 'Between' component's value considered a puzzle for standard trade models?", "Options": {"A": "The negative 'Within' component shows individual firms substituted away from imports, while the large positive 'Between' component shows market share shifted to import-intensive firms, which is puzzling as standard models predict they should contract.", "B": "The positive 'Within' component shows firms imported more, while the negative 'Between' component shows market share shifted away from import-intensive firms, matching standard models.", "C": "The negative 'Within' component is the main puzzle, as it implies firms' production technology has an elasticity of substitution greater than one.", "D": "The positive 'Between' component is expected, but its magnitude (12.34) is puzzlingly small compared to what standard models predict."}, "Answer": "A", "pi_justification": "This item assesses the ability to interpret a decomposition table and contrast the empirical finding with a theoretical benchmark. It uses a Computational Judgment strategy. The distractors are designed based on common errors: B misreads both signs from the table, C misidentifies the source of the puzzle, and D incorrectly states the prediction of standard models.", "qid": "300", "question": "### Background\n\n**Research Question.** This problem investigates the micro-level drivers of the change in the aggregate imported input share following a large devaluation, focusing on the puzzle of why import-intensive firms expand.\n\n**Setting / Institutional Environment.** The analysis uses firm-level data from Mexico (post-1995 devaluation) to decompose the change in the aggregate imported input share into components attributable to within-firm changes versus between-firm reallocation of market share.\n\n### Data / Model Specification\n\nThe growth in the aggregate imported input share (\\(\\Delta S_I\\)) is decomposed into several components. The two key components for continuing firms are:\n-   **Within:** Captures changes in firms' own import shares, holding their initial market shares constant. It reflects firm-level substitution.\n-   **Between:** Captures the reallocation of market share across firms, holding their initial import shares constant.\n\n**Table 1: Decomposition for Mexico (change from 1994 to 1997, in percentage points)**\n| Year | Within | Between | Covariance | Net entry | All |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| 1997 | -1.54 | 12.34 | 2.61 | 6.95 | 20.36 |\n\nBased on the decomposition in Table 1, what do the 'Within' and 'Between' components reveal about firm behavior, and why is the 'Between' component's value considered a puzzle for standard trade models?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 220, "Question": "### Background\n\n**Research Question.** A key concern in this analysis is that the measure of `EconomicUnpred` could be endogenous due to reverse causality.\n\n---\n\n### Data / Model Specification\n\nConsider a simplified model without the interaction term:\n\n```latex\nFirmGrowth = \\beta_0 + \\beta_1 EconomicUnpred + ... + \\epsilon\n```\n\n---\n\nSuppose that managers of firms with intrinsically low growth prospects (e.g., due to unobserved poor management, leading to a negative error term `ε`) become pessimistic and are thus more likely to report high levels of `EconomicUnpred`. What is the direction of the resulting bias on the OLS estimator `β̂₁`?", "Options": {"A": "The OLS estimate `β̂₁` will be biased to be more negative than the true causal effect `β₁`.", "B": "The OLS estimate `β̂₁` will be biased to be less negative (closer to zero) than the true causal effect `β₁`.", "C": "The OLS estimate `β̂₁` will be biased towards zero, regardless of the true effect.", "D": "The OLS estimate `β̂₁` will be biased, but the direction of the bias cannot be determined."}, "Answer": "A", "pi_justification": "This 'Reverse-Reasoning' item tests the ability to deduce the statistical consequences of endogeneity, the focus of the original QA's third part. Given a specific economic story for reverse causality, the student must identify the direction of the resulting OLS bias. Distractors target common confusions: B (incorrectly identifying a positive bias), C (a common but imprecise heuristic), and D (failing to see that the direction is in fact determined by the premise).", "qid": "220", "question": "### Background\n\n**Research Question.** A key concern in this analysis is that the measure of `EconomicUnpred` could be endogenous due to reverse causality.\n\n---\n\n### Data / Model Specification\n\nConsider a simplified model without the interaction term:\n\n```latex\nFirmGrowth = \\beta_0 + \\beta_1 EconomicUnpred + ... + \\epsilon\n```\n\n---\n\nSuppose that managers of firms with intrinsically low growth prospects (e.g., due to unobserved poor management, leading to a negative error term `ε`) become pessimistic and are thus more likely to report high levels of `EconomicUnpred`. What is the direction of the resulting bias on the OLS estimator `β̂₁`?", "model_evaluation_result": {"model_raw_response": "Looking at this endogeneity problem step by step:\n\n**The Setup:**\n- Model: FirmGrowth = β₀ + β₁EconomicUnpred + ... + ε\n- Problem: Firms with low growth prospects (negative ε) are more likely to report high EconomicUnpred\n- This creates correlation between EconomicUnpred and the error term ε\n\n**The Bias Analysis:**\nThe bias in OLS occurs when E[EconomicUnpred × ε] ≠ 0.\n\nGiven the scenario:\n- Low growth firms have negative ε (poor unobserved fundamentals)\n- These same firms report higher EconomicUnpred (due to pessimism)\n- Therefore: EconomicUnpred and ε are negatively correlated\n- So E[EconomicUnpred × ε] < 0\n\n**The Direction of Bias:**\nThe OLS estimator has bias = E[EconomicUnpred × ε] / Var(EconomicUnpred)\n\nSince E[EconomicUnpred × ε] < 0, the bias is negative.\n\nThis means: β̂₁ = β₁ + (negative bias) = β₁ - |bias|\n\nTherefore, the OLS estimate β̂₁ will be more negative than the true causal effect β₁.\n\n**A**", "model_answer": "A", "error": null}}
{"ID": 299, "Question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to distinguish between two competing behavioral explanations for an observed choice anomaly.\n\n**Setting.** In a primary experiment, subjects chose between three products (High, Medium, Low benefit) in a two-stage process: first choose a product, then choose one of its associated costs. The key finding was that when costs were presented in a complex arithmetic format (`COMPLEX COST` treatment), subjects made systematically biased mistakes, choosing the suboptimal High-benefit option far more often than the equally suboptimal Low-benefit option. This rejected the null hypothesis of symmetric, random mistakes.\n\nTwo potential explanations for this bias are considered:\n1.  **Salience:** Complexity in the cost dimension makes the simple, easy-to-evaluate benefit dimension more salient. Decision-makers overweight this salient dimension, leading them to be attracted to the High-benefit option.\n2.  **Narrow Bracketing:** Decision-makers simplify the two-stage problem by ignoring the future cost-selection stage. They focus only on the first stage and choose the option with the highest immediate benefit (the High option), without fully integrating the downstream consequences of costs.\n\nTo distinguish between these two explanations, a follow-up treatment was conducted where subjects made a single choice from a menu of nine options, effectively choosing the product and its cost at the same time. The result of this follow-up treatment was that choice patterns in the `COMPLEX COST` condition did not significantly change; the bias towards the High-benefit option persisted.\n\n### The Question\n\nGiven this finding, what is the most accurate conclusion?", "Options": {"A": "The result supports the Salience explanation because the bias persisted even when the Narrow Bracketing heuristic was made difficult to apply.", "B": "The result supports the Narrow Bracketing explanation because the bias was shown to be robust to changes in the choice architecture.", "C": "The result is inconclusive because it is consistent with the predictions of both the Salience and Narrow Bracketing theories.", "D": "The result suggests a third, unstated theory is responsible, as neither Salience nor Narrow Bracketing can fully account for the persistence of the bias."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question tests the final step of scientific inference: connecting experimental evidence to theory. It uses a 'Computational Judgment' strategy (in a logical sense). The correct answer (A) draws the appropriate conclusion. Distractors represent common errors in interpretation: concluding support for the falsified theory (B, Conceptual Opposite), claiming inconclusiveness when the evidence is decisive (C), or unnecessarily invoking an unstated alternative (D).", "qid": "299", "question": "### Background\n\n**Research Question.** This problem examines the identification strategy used to distinguish between two competing behavioral explanations for an observed choice anomaly.\n\n**Setting.** In a primary experiment, subjects chose between three products (High, Medium, Low benefit) in a two-stage process: first choose a product, then choose one of its associated costs. The key finding was that when costs were presented in a complex arithmetic format (`COMPLEX COST` treatment), subjects made systematically biased mistakes, choosing the suboptimal High-benefit option far more often than the equally suboptimal Low-benefit option. This rejected the null hypothesis of symmetric, random mistakes.\n\nTwo potential explanations for this bias are considered:\n1.  **Salience:** Complexity in the cost dimension makes the simple, easy-to-evaluate benefit dimension more salient. Decision-makers overweight this salient dimension, leading them to be attracted to the High-benefit option.\n2.  **Narrow Bracketing:** Decision-makers simplify the two-stage problem by ignoring the future cost-selection stage. They focus only on the first stage and choose the option with the highest immediate benefit (the High option), without fully integrating the downstream consequences of costs.\n\nTo distinguish between these two explanations, a follow-up treatment was conducted where subjects made a single choice from a menu of nine options, effectively choosing the product and its cost at the same time. The result of this follow-up treatment was that choice patterns in the `COMPLEX COST` condition did not significantly change; the bias towards the High-benefit option persisted.\n\n### The Question\n\nGiven this finding, what is the most accurate conclusion?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 210, "Question": "## Background\n\n**Research Question.** This problem examines a method to distinguish the effect of adverse selection from a potential confounding \"scale effect\" in the pricing of thoroughbred yearlings.\n\n**Setting / Institutional Environment.** A potential critique of the main finding is that the `Racing Intensity` variable may be correlated with the overall scale of a seller's operation. If larger breeding operations have better reputations and command higher prices, the estimated coefficient on `Racing Intensity` could be biased. This analysis re-parameterizes the model to test this possibility directly.\n\n**Variables & Parameters.**\n- `ln(Price)`: Natural logarithm of the hammer price.\n- `R`: `Racing Starts`, the number of races the breeder started a horse in 1993.\n- `B`: `Breeding Starts + 1`, where `Breeding Starts` is the number of races started by horses bred by the seller.\n- `Race Zero`, `Breed Zero`: Indicator variables for sellers with zero racing or breeding starts, respectively.\n\n---\n\n## Data / Model Specification\n\nThe analysis compares a restricted model with an unrestricted version derived from an algebraic identity.\n\n**Restricted Model:**\n```latex\nln(Price_i) = \\alpha_0 + \\alpha_1 ln(R_i/B_i + 1) + \\gamma'X_i + \\epsilon_i\n```\n(Eq. 1)\n\n**Algebraic Identity:**\n```latex\nln(R/B + 1) = ln(R+B) - ln(B)\n```\n(Eq. 2)\n\n**Unrestricted Model:**\n```latex\nln(Price_i) = \\theta_0 + \\alpha_1^* ln(R_i+B_i) - \\beta^{**} ln(B_i) + \\gamma'X_i + \\nu_i\n```\n(Eq. 3)\n\n**Table 1: Selected Regression Coefficients for ln(Price)**\n| Specification | (3) Restricted | (4) Restricted | (5) Unrestricted | (6) Unrestricted |\n| :--- | :---: | :---: | :---: | :---: |\n| `Race/Breed Zero` Dummies | No | Yes | No | Yes |\n| `ln(R/B + 1)` | -0.1026 | -0.1404 | | |\n| `ln(R+B)` | | | -0.0638 | -0.1465 |\n| `-ln(B)` | | | -0.1200 | -0.1340 |\n| F-test (`\\alpha_1^* = \\beta^{**}`)| | | F = 9.58 | F = 0.068 |\n| Critical F (5%) | | | ~3.88 | ~3.88 |\n\n---\n\n## The Question\n\nThe F-statistic for the scale effects test drops from 9.58 (in the model without zero-start dummies) to 0.068 (in the model with zero-start dummies). What is the most accurate interpretation of this change and its implication for the paper's adverse selection hypothesis?", "Options": {"A": "The initial scale effect was an artifact driven by sellers with zero racing or breeding starts; after controlling for them, the `Racing Intensity` variable is a valid measure, and the evidence for adverse selection holds.", "B": "The inclusion of the dummy variables invalidates the F-test, making the results unreliable; the presence of adverse selection is now uncertain.", "C": "The scale effect is real, but the dummy variables absorb it, showing that adverse selection only exists for sellers with non-zero starts.", "D": "The dramatic drop in the F-statistic indicates that the `Race Zero` and `Breed Zero` dummy variables are highly statistically significant."}, "Answer": "A", "pi_justification": "This item tests the ability to synthesize results across different model specifications to understand a robustness check. It uses a Reverse-Reasoning strategy (inferring the cause of a statistical change). Distractors are designed to be plausible but incorrect interpretations: suggesting the control invalidates the test (B, Conceptual Opposite), a subtle misreading of the result (C, Almost Right), and stating a true but secondary point that misses the main conclusion (D, True but Irrelevant).", "qid": "210", "question": "## Background\n\n**Research Question.** This problem examines a method to distinguish the effect of adverse selection from a potential confounding \"scale effect\" in the pricing of thoroughbred yearlings.\n\n**Setting / Institutional Environment.** A potential critique of the main finding is that the `Racing Intensity` variable may be correlated with the overall scale of a seller's operation. If larger breeding operations have better reputations and command higher prices, the estimated coefficient on `Racing Intensity` could be biased. This analysis re-parameterizes the model to test this possibility directly.\n\n**Variables & Parameters.**\n- `ln(Price)`: Natural logarithm of the hammer price.\n- `R`: `Racing Starts`, the number of races the breeder started a horse in 1993.\n- `B`: `Breeding Starts + 1`, where `Breeding Starts` is the number of races started by horses bred by the seller.\n- `Race Zero`, `Breed Zero`: Indicator variables for sellers with zero racing or breeding starts, respectively.\n\n---\n\n## Data / Model Specification\n\nThe analysis compares a restricted model with an unrestricted version derived from an algebraic identity.\n\n**Restricted Model:**\n```latex\nln(Price_i) = \\alpha_0 + \\alpha_1 ln(R_i/B_i + 1) + \\gamma'X_i + \\epsilon_i\n```\n(Eq. 1)\n\n**Algebraic Identity:**\n```latex\nln(R/B + 1) = ln(R+B) - ln(B)\n```\n(Eq. 2)\n\n**Unrestricted Model:**\n```latex\nln(Price_i) = \\theta_0 + \\alpha_1^* ln(R_i+B_i) - \\beta^{**} ln(B_i) + \\gamma'X_i + \\nu_i\n```\n(Eq. 3)\n\n**Table 1: Selected Regression Coefficients for ln(Price)**\n| Specification | (3) Restricted | (4) Restricted | (5) Unrestricted | (6) Unrestricted |\n| :--- | :---: | :---: | :---: | :---: |\n| `Race/Breed Zero` Dummies | No | Yes | No | Yes |\n| `ln(R/B + 1)` | -0.1026 | -0.1404 | | |\n| `ln(R+B)` | | | -0.0638 | -0.1465 |\n| `-ln(B)` | | | -0.1200 | -0.1340 |\n| F-test (`\\alpha_1^* = \\beta^{**}`)| | | F = 9.58 | F = 0.068 |\n| Critical F (5%) | | | ~3.88 | ~3.88 |\n\n---\n\n## The Question\n\nThe F-statistic for the scale effects test drops from 9.58 (in the model without zero-start dummies) to 0.068 (in the model with zero-start dummies). What is the most accurate interpretation of this change and its implication for the paper's adverse selection hypothesis?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand what the F-test is measuring and what the dramatic change in the F-statistic tells us.\n\nThe F-test is testing whether α₁* = β** in the unrestricted model. If this restriction holds, then the unrestricted model collapses to the restricted model, meaning the scale effect (represented by the separate coefficients on ln(R+B) and -ln(B)) is not present.\n\nKey observations:\n- Without zero-start dummies: F = 9.58 > 3.88 (critical value) → Reject the null hypothesis that α₁* = β**\n- With zero-start dummies: F = 0.068 < 3.88 (critical value) → Fail to reject the null hypothesis that α₁* = β**\n\nThe dramatic drop from 9.58 to 0.068 indicates that:\n\n1. Initially, there appeared to be a significant scale effect (the coefficients were significantly different)\n2. After including the zero-start dummies, this scale effect disappears (the coefficients are now statistically equal)\n\nThis suggests that the apparent scale effect in the first specification was being driven by the unusual behavior of sellers with zero racing or breeding starts. These sellers likely have very different pricing patterns that were confounding the scale effect test.\n\nOnce we control for these zero-start sellers with dummy variables, the scale effect disappears, meaning the Racing Intensity variable (R/B ratio) is now a clean measure of adverse selection without confounding scale effects.\n\nThis supports the paper's adverse selection hypothesis because it shows that the Racing Intensity measure is valid after properly controlling for the problematic zero-start observations.\n\nA", "model_answer": "B", "error": null}}
{"ID": 253, "Question": "### Background\n\n**Research Question.** This problem examines the core mathematical structure of the Integer-Valued Autoregressive (INAR) model for count data, focusing on its decomposition and conditional properties.\n\n**Setting / Institutional Environment.** The model is a first-order integer-valued autoregressive process, INAR(1), for a time series of counts `X_t`. The core mechanism assumes that the count in period `t` is the sum of a 'carry-over' part, representing survivals from the `X_{t-1}` counts, and an 'innovation' part, representing new events in period `t`.\n\n**Variables & Parameters.**\n- `X_t`: A discrete random variable representing the count of events at time `t`.\n- `α_t`: The carry-over or 'thinning' probability, `α_t ∈ [0, 1]`.\n- `∘`: The binomial thinning operator, where `α ∘ X` means each of the `X` items survives independently with probability `α`.\n- `C_{t-1}`: The carry-over component from `X_{t-1}`.\n- `I_t`: The innovation component at time `t`, assumed to be independent of past counts.\n- `λ_t^I`: The mean of the innovation process `I_t`.\n\n---\n\n### Data / Model Specification\n\nThe INAR(1) process decomposes the count `X_t` as the sum of two independent components:\n```latex\nX_t = C_{t-1} + I_t\n```\nConditional on `X_{t-1} = x_{t-1}`, `C_{t-1}` follows a Binomial distribution, `B(x_{t-1}, α_t)`. The innovation term `I_t` is an independent Poisson draw with mean `λ_t^I`.\n\n---\n\nUsing the decomposition `X_t = C_{t-1} + I_t` and the properties of conditional expectation, what is the correct expression for `E(X_t | X_{t-1} = x_{t-1})`?", "Options": {"A": "```latex\n\\alpha_t x_{t-1} + \\lambda_t^I\n```", "B": "```latex\n\\alpha_t + \\lambda_t^I\n```", "C": "```latex\nx_{t-1} + \\lambda_t^I\n```", "D": "```latex\n\\alpha_t (x_{t-1} + \\lambda_t^I)\n```"}, "Answer": "A", "pi_justification": "Assesses the ability to derive the conditional mean of the INAR(1) process, a fundamental property. This is a Computational Judgment item. Distractors are based on common algebraic and conceptual errors, such as misapplying the law of iterated expectations (B), incorrectly stating the mean of a binomial distribution (C), or incorrectly applying the linearity of expectation (D).", "qid": "253", "question": "### Background\n\n**Research Question.** This problem examines the core mathematical structure of the Integer-Valued Autoregressive (INAR) model for count data, focusing on its decomposition and conditional properties.\n\n**Setting / Institutional Environment.** The model is a first-order integer-valued autoregressive process, INAR(1), for a time series of counts `X_t`. The core mechanism assumes that the count in period `t` is the sum of a 'carry-over' part, representing survivals from the `X_{t-1}` counts, and an 'innovation' part, representing new events in period `t`.\n\n**Variables & Parameters.**\n- `X_t`: A discrete random variable representing the count of events at time `t`.\n- `α_t`: The carry-over or 'thinning' probability, `α_t ∈ [0, 1]`.\n- `∘`: The binomial thinning operator, where `α ∘ X` means each of the `X` items survives independently with probability `α`.\n- `C_{t-1}`: The carry-over component from `X_{t-1}`.\n- `I_t`: The innovation component at time `t`, assumed to be independent of past counts.\n- `λ_t^I`: The mean of the innovation process `I_t`.\n\n---\n\n### Data / Model Specification\n\nThe INAR(1) process decomposes the count `X_t` as the sum of two independent components:\n```latex\nX_t = C_{t-1} + I_t\n```\nConditional on `X_{t-1} = x_{t-1}`, `C_{t-1}` follows a Binomial distribution, `B(x_{t-1}, α_t)`. The innovation term `I_t` is an independent Poisson draw with mean `λ_t^I`.\n\n---\n\nUsing the decomposition `X_t = C_{t-1} + I_t` and the properties of conditional expectation, what is the correct expression for `E(X_t | X_{t-1} = x_{t-1})`?", "model_evaluation_result": {"model_raw_response": "I need to find the conditional expectation E(X_t | X_{t-1} = x_{t-1}) using the given decomposition.\n\nGiven:\n- X_t = C_{t-1} + I_t\n- C_{t-1} | X_{t-1} = x_{t-1} ~ B(x_{t-1}, α_t)\n- I_t ~ Poisson(λ_t^I)\n- C_{t-1} and I_t are independent\n\nUsing the linearity of conditional expectation:\nE(X_t | X_{t-1} = x_{t-1}) = E(C_{t-1} + I_t | X_{t-1} = x_{t-1})\n                            = E(C_{t-1} | X_{t-1} = x_{t-1}) + E(I_t | X_{t-1} = x_{t-1})\n\nSince I_t is independent of X_{t-1}:\nE(I_t | X_{t-1} = x_{t-1}) = E(I_t) = λ_t^I\n\nFor the carry-over component, given X_{t-1} = x_{t-1}, C_{t-1} follows a binomial distribution B(x_{t-1}, α_t).\n\nThe expected value of a binomial distribution B(n, p) is np.\n\nTherefore:\nE(C_{t-1} | X_{t-1} = x_{t-1}) = x_{t-1} · α_t\n\nCombining these results:\nE(X_t | X_{t-1} = x_{t-1}) = α_t x_{t-1} + λ_t^I\n\nA", "model_answer": "B", "error": null}}
{"ID": 242, "Question": "### Background\n\n**Research Question:** This problem investigates the practical performance of the Generalized Stochastic Simulation Algorithm (GSSA), focusing on the convergence properties of the Monte Carlo integration method.\n\n**Setting / Institutional Environment:** The analysis uses numerical experiments from a standard representative-agent model.\n\n**Variables & Parameters:**\n- `MC(J)`: Monte Carlo integration with `J` random draws.\n- `T`: The simulation length.\n- `E_mean`: Mean absolute unit-free Euler error (in log10 units).\n\n---\n\n### Data / Model Specification\n\nThe paper claims that the error of the MC(1) integration method scales with `1/√T`. The following table provides results from numerical experiments for a 5th degree polynomial approximation.\n\n**Table 1: Performance of MC(1) Integration**\n\n| Integration Method | T | `E_mean` (log10) |\n| :--- | :---: | :---: |\n| MC(1) | 10,000 | -4.23 |\n| MC(1) | 100,000 | -4.71 |\n\n---\n\n### Question\n\nUsing the data in Table 1, by what factor does the absolute error of the MC(1) method decrease when the simulation length `T` is increased from 10,000 to 100,000? How does this compare to the theoretical `1/√T` convergence rate?\n", "Options": {"A": "The error decreases by a factor of approximately 3.0, which is consistent with the theoretical prediction of √10 ≈ 3.16.", "B": "The error decreases by a factor of approximately 10.0, which is consistent with a theoretical 1/T convergence rate.", "C": "The error decreases by a factor of approximately 1.1, which is much slower than the theoretical prediction.", "D": "The error decreases by a factor of approximately 0.48, indicating that the error actually increased."}, "Answer": "A", "pi_justification": "This item assesses the quantitative verification from Part 2 of the original QA. It uses a Computational Judgment strategy. The calculation requires converting log10 errors to absolute errors and finding their ratio. Distractors are designed based on common numerical errors. Distractor B represents a `T` vs `√T` confusion. Distractor C represents a simple ratio of the log values (`4.71/4.23`). Distractor D represents the difference of the log values (`4.71-4.23`), which is a Step-Omission Error (forgetting to take the antilog).", "qid": "242", "question": "### Background\n\n**Research Question:** This problem investigates the practical performance of the Generalized Stochastic Simulation Algorithm (GSSA), focusing on the convergence properties of the Monte Carlo integration method.\n\n**Setting / Institutional Environment:** The analysis uses numerical experiments from a standard representative-agent model.\n\n**Variables & Parameters:**\n- `MC(J)`: Monte Carlo integration with `J` random draws.\n- `T`: The simulation length.\n- `E_mean`: Mean absolute unit-free Euler error (in log10 units).\n\n---\n\n### Data / Model Specification\n\nThe paper claims that the error of the MC(1) integration method scales with `1/√T`. The following table provides results from numerical experiments for a 5th degree polynomial approximation.\n\n**Table 1: Performance of MC(1) Integration**\n\n| Integration Method | T | `E_mean` (log10) |\n| :--- | :---: | :---: |\n| MC(1) | 10,000 | -4.23 |\n| MC(1) | 100,000 | -4.71 |\n\n---\n\n### Question\n\nUsing the data in Table 1, by what factor does the absolute error of the MC(1) method decrease when the simulation length `T` is increased from 10,000 to 100,000? How does this compare to the theoretical `1/√T` convergence rate?\n", "model_evaluation_result": {"model_raw_response": "I need to analyze the convergence rate of the MC(1) integration method using the given data.\n\nGiven data:\n- At T = 10,000: E_mean = -4.23 (log10)\n- At T = 100,000: E_mean = -4.71 (log10)\n\nFirst, I'll convert the log10 errors to actual error values:\n- At T = 10,000: Error = 10^(-4.23) ≈ 5.89 × 10^(-5)\n- At T = 100,000: Error = 10^(-4.71) ≈ 1.95 × 10^(-5)\n\nThe factor by which the error decreases:\nFactor = (Error at T=10,000) / (Error at T=100,000)\nFactor = (5.89 × 10^(-5)) / (1.95 × 10^(-5)) ≈ 3.02\n\nNow I'll check the theoretical prediction:\n- T increases from 10,000 to 100,000, which is a factor of 10\n- Under 1/√T convergence: Error should decrease by √10 ≈ 3.16\n\nComparing:\n- Observed decrease factor: ≈ 3.0\n- Theoretical prediction: √10 ≈ 3.16\n\nThe observed factor of approximately 3.0 is very close to the theoretical prediction of √10 ≈ 3.16, which confirms the 1/√T convergence rate.\n\nA", "model_answer": "A", "error": null}}
{"ID": 203, "Question": "### Background\n\n**Research Question.** This problem assesses the causal effect of insurers' fee screens on physicians' list prices and evaluates how sensitive these estimates are to theoretical and empirical model specification.\n\n**Setting / Institutional Environment.** The study estimates the elasticity of physician office prices with respect to insurer fee screens. It compares estimates from a prior, misspecified model with a new, theoretically-grounded model. The new model makes two key corrections: (1) it uses the correct theoretical weights for insurer fees (unweighted for Medicaid, weighted by share of list-price patients for others) and (2) it uses a stricter, theory-consistent definition of Medicaid \"participation.\"\n\n### Data / Model Specification\n\n**Table 1: Office Fee Elasticities from Alternative Model Specifications**\n\n| Model Specification | Participant Definition | Medicaid | Medicare | Private |\n| :--- | :--- | :--- | :--- | :--- |\n| Sloan (original study) | \"some or all\" | 0.18 | 0.03 | 0.03 |\n| Eq. 2.1 (Sloan spec, new data) | \"some or all\" | 0.16 | 0.30 | 0.61 |\n| **Eq. 2.3 (Corrected Model)** | | | | |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Participants* | \"all\" | 0.40 | 0.28 | 0.53 |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Nonparticipants* | \"all\" | 0.17 | 0.25 | 0.52 |\n\n### Question\n\nA regulator proposes a 5% cut to all insurer fee screens (Medicaid, Medicare, and Private). Assume this policy causes a physician who was previously a 'participant' to switch their behavior and become a 'nonparticipant'. Based on the elasticities in Table 1, what is the predicted total percentage change in this physician's office list price, and what is the most likely direct consequence for patient care?", "Options": {"A": "The price will fall by 4.7%, and the most likely consequence is reduced access to care for Medicaid patients.", "B": "The price will fall by 6.05%, and the most likely consequence is reduced access to care for Medicaid patients.", "C": "The price will fall by 4.7%, but there will be no significant impact on patient access as the price drop is small.", "D": "The price will rise by 4.7%, as physicians try to recover lost income from lower fees."}, "Answer": "A", "pi_justification": "This high-difficulty item tests the ability to apply elasticity estimates in a dynamic counterfactual scenario that involves a change in agent type. It uses a Scenario Application strategy. The correct answer requires both a precise calculation and a correct inference about the policy's main unintended consequence. Distractors are based on using the wrong set of elasticities for the calculation (a common conceptual error), getting the calculation right but drawing the wrong conclusion about the consequences, or getting the direction of the price change wrong.", "qid": "203", "question": "### Background\n\n**Research Question.** This problem assesses the causal effect of insurers' fee screens on physicians' list prices and evaluates how sensitive these estimates are to theoretical and empirical model specification.\n\n**Setting / Institutional Environment.** The study estimates the elasticity of physician office prices with respect to insurer fee screens. It compares estimates from a prior, misspecified model with a new, theoretically-grounded model. The new model makes two key corrections: (1) it uses the correct theoretical weights for insurer fees (unweighted for Medicaid, weighted by share of list-price patients for others) and (2) it uses a stricter, theory-consistent definition of Medicaid \"participation.\"\n\n### Data / Model Specification\n\n**Table 1: Office Fee Elasticities from Alternative Model Specifications**\n\n| Model Specification | Participant Definition | Medicaid | Medicare | Private |\n| :--- | :--- | :--- | :--- | :--- |\n| Sloan (original study) | \"some or all\" | 0.18 | 0.03 | 0.03 |\n| Eq. 2.1 (Sloan spec, new data) | \"some or all\" | 0.16 | 0.30 | 0.61 |\n| **Eq. 2.3 (Corrected Model)** | | | | |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Participants* | \"all\" | 0.40 | 0.28 | 0.53 |\n| &nbsp;&nbsp;&nbsp;&nbsp;*Nonparticipants* | \"all\" | 0.17 | 0.25 | 0.52 |\n\n### Question\n\nA regulator proposes a 5% cut to all insurer fee screens (Medicaid, Medicare, and Private). Assume this policy causes a physician who was previously a 'participant' to switch their behavior and become a 'nonparticipant'. Based on the elasticities in Table 1, what is the predicted total percentage change in this physician's office list price, and what is the most likely direct consequence for patient care?", "model_evaluation_result": {"model_raw_response": "I need to analyze the effect of a 5% cut to all insurer fee screens on a physician's office list price, considering that the physician switches from \"participant\" to \"nonparticipant\" status.\n\nFrom Table 1, the corrected model (Eq. 2.3) shows elasticities for:\n\n**Participants:**\n- Medicaid: 0.40\n- Medicare: 0.28  \n- Private: 0.53\n\n**Nonparticipants:**\n- Medicaid: 0.17\n- Medicare: 0.25\n- Private: 0.52\n\nThe physician starts as a participant but switches to nonparticipant due to the 5% fee cut.\n\nFor a 5% decrease in fees (-5%), the percentage change in price equals:\nElasticity × (-5%)\n\n**As a participant (initial elasticities):**\n- Medicaid effect: 0.40 × (-5%) = -2.0%\n- Medicare effect: 0.28 × (-5%) = -1.4%\n- Private effect: 0.53 × (-5%) = -2.65%\n- Total initial effect: -2.0% + (-1.4%) + (-2.65%) = -6.05%\n\n**As a nonparticipant (after switching):**\n- Medicaid effect: 0.17 × (-5%) = -0.85%\n- Medicare effect: 0.25 × (-5%) = -1.25%\n- Private effect: 0.52 × (-5%) = -2.6%\n- Total nonparticipant effect: -0.85% + (-1.25%) + (-2.6%) = -4.7%\n\nSince the physician switches from participant to nonparticipant, the relevant elasticities are the nonparticipant ones, giving a total price decrease of 4.7%.\n\nThe most likely consequence is reduced access for Medicaid patients, as physicians who become nonparticipants typically reduce their acceptance of Medicaid patients due to lower reimbursement rates.\n\nB", "model_answer": "B", "error": null}}
{"ID": 228, "Question": "### Background\n\n**Research Question.** This problem concerns the identification of the causal effects of childcare prices and wages on a single mother's employment and childcare choices, addressing endogeneity and unobserved heterogeneity.\n\n**Setting and Sample.** The analysis uses a discrete choice model where the indirect utility of each choice is a function of potentially endogenous prices and wages.\n\n### Data / Model Specification\n\nThe model consists of a main utility equation and auxiliary equations for price and wages.\n\n**1. Indirect Utility Function:** The linear approximation of the indirect utility for alternative `i` is:\n```latex\nV_{i} = X\\beta_{i} + \\alpha_{P i}P_{s}^{*} + \\alpha_{PT i}W_{PT} + \\alpha_{FT i}W_{FT} + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n```\nwhere `P_s*` is the effective price of childcare, `W_PT` and `W_FT` are part-time and full-time wages, and `\\varepsilon_i` is an error term.\n\n**2. Identification of Price and Wages:** To address the endogeneity of price and wages, the model uses state of residence indicators (`D_s`) as instruments in reduced-form equations:\n```latex\nP_{s} = D_{s}\\delta_{s} + X\\delta_{P} + \\xi_{P} \\quad \\text{(Eq. 2)}\n```\n\n### The Question\n\nBased on the model specification and economic theory, which of the following statements is correct?", "Options": {"A": "For an alternative involving full-time work and paid care, `\\alpha_{Pi}` is expected to be negative, and a valid instrument `D_s` must be uncorrelated with the utility error term `\\varepsilon_i`.", "B": "For an alternative involving full-time work and paid care, `\\alpha_{Pi}` is expected to be negative, and a valid instrument `D_s` must be correlated with the childcare price `P_s` but not with the wage `W_{FT}`.", "C": "For an alternative involving full-time work and paid care, `\\alpha_{Pi}` is expected to be positive, and a valid instrument `D_s` must be uncorrelated with the utility error term `\\varepsilon_i`.", "D": "For an alternative involving full-time work and paid care, `\\alpha_{Pi}` is expected to be positive, and a valid instrument `D_s` must be correlated with the childcare price `P_s`."}, "Answer": "A", "pi_justification": "This item assesses the student's understanding of the model's theoretical predictions (sign of coefficients) and the fundamental assumptions of the identification strategy (exclusion restriction). The distractors are designed to isolate misunderstandings of theory versus econometrics. Distractor B (Conceptual Opposite) confuses the exclusion restriction with the relevance condition and adds an unnecessary constraint. Distractor C (Sign Error) reverses the expected sign of the price coefficient. Distractor D combines both errors.", "qid": "228", "question": "### Background\n\n**Research Question.** This problem concerns the identification of the causal effects of childcare prices and wages on a single mother's employment and childcare choices, addressing endogeneity and unobserved heterogeneity.\n\n**Setting and Sample.** The analysis uses a discrete choice model where the indirect utility of each choice is a function of potentially endogenous prices and wages.\n\n### Data / Model Specification\n\nThe model consists of a main utility equation and auxiliary equations for price and wages.\n\n**1. Indirect Utility Function:** The linear approximation of the indirect utility for alternative `i` is:\n```latex\nV_{i} = X\\beta_{i} + \\alpha_{P i}P_{s}^{*} + \\alpha_{PT i}W_{PT} + \\alpha_{FT i}W_{FT} + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n```\nwhere `P_s*` is the effective price of childcare, `W_PT` and `W_FT` are part-time and full-time wages, and `\\varepsilon_i` is an error term.\n\n**2. Identification of Price and Wages:** To address the endogeneity of price and wages, the model uses state of residence indicators (`D_s`) as instruments in reduced-form equations:\n```latex\nP_{s} = D_{s}\\delta_{s} + X\\delta_{P} + \\xi_{P} \\quad \\text{(Eq. 2)}\n```\n\n### The Question\n\nBased on the model specification and economic theory, which of the following statements is correct?", "model_evaluation_result": {"model_raw_response": "I need to analyze this discrete choice model for single mothers' employment and childcare decisions, focusing on the expected sign of the price coefficient and instrument validity conditions.\n\nLet me break this down:\n\n1) **Expected sign of α_Pi for full-time work with paid care:**\n- P_s* represents the effective price of childcare\n- For an alternative involving paid childcare, higher childcare prices should decrease the utility of that choice\n- Therefore, α_Pi should be negative (higher prices reduce utility)\n\n2) **Instrument validity conditions:**\n- For D_s to be a valid instrument, it must satisfy two conditions:\n  - Relevance: D_s must be correlated with the endogenous variable P_s (which it instruments for)\n  - Exogeneity: D_s must be uncorrelated with the error term ε_i in the utility equation\n\nLooking at the options:\n\nA) Says α_Pi should be negative (correct) and D_s must be uncorrelated with ε_i (correct - this is the exogeneity condition)\n\nB) Says α_Pi should be negative (correct) but incorrectly states D_s must not be correlated with W_FT. This is not a standard instrument validity condition.\n\nC) Says α_Pi should be positive (incorrect - higher childcare prices should reduce utility)\n\nD) Says α_Pi should be positive (incorrect) and only mentions correlation with P_s (incomplete - missing the exogeneity condition)\n\nOption A correctly identifies both the expected negative sign of the price coefficient and the key exogeneity condition for instrument validity.\n\nA", "model_answer": "A", "error": null}}
{"ID": 224, "Question": "### Background\n\nAn economy features risk-averse workers and risk-neutral firms. Labor services are traded via **fixed nominal wage contracts**, where the wage is set before the period's shocks are realized. The economy is subject to real technology shocks (`s_t`) and nominal money supply shocks (`x_t`).\n\n### Data / Model Specification\n\nThe pre-determined equilibrium nominal wage is:\n\n```latex\n\\bar{w}_t = \\frac{m_{t-1}k\\bar{\\phi}}{F} \\quad \\text{(Eq. 1)}\n```\n\nwhere `k \\equiv 1/E(1/x)` and `\\bar{\\phi} \\equiv E_s[\\phi(s)]`. The price function is the same as in the spot market: `p_t(s,x) = m_{t-1}x_t / (F g(s_t))`.\n\nA worker from generation `t` earns `\\bar{w}_t` in their youth and consumes `c^{\\text{RW}} = \\bar{w}_t x_{t+1} / p_{t+1}` in their old age.\n\n### Question\n\nGiven the fixed nominal wage contract equilibrium, which of the following correctly states the real consumption `c^{\\text{RW}}` for a worker of generation `t` and accurately describes its properties?", "Options": {"A": "`c^{\\text{RW}} = \\phi(s_t) g(s_{t+1})`. Consumption is independent of nominal shocks, so money is neutral.", "B": "`c^{\\text{RW}} = k \\bar{\\phi} g(s_{t+1}) / x_t`. Consumption is independent of the youth-period technology shock `s_t` but depends on the nominal shock `x_t`, so money is not neutral.", "C": "`c^{\\text{RW}} = k \\bar{\\phi} g(s_{t+1})`. Consumption is fully insured against both youth-period technology shocks and nominal shocks.", "D": "`c^{\\text{RW}} = k \\phi(s_t) g(s_{t+1}) / x_t`. The contract wage depends on the realized technology shock `s_t`."}, "Answer": "B", "pi_justification": "This item replaces part of a QA problem (Suitability Score: 9.5). It uses the Computational Judgment strategy, requiring the student to derive consumption under a fixed nominal wage and identify its key properties regarding risk-sharing and monetary non-neutrality. Distractors are designed to catch common errors: (A) confusing the contract market with the spot market outcome, (C) an incomplete derivation that ignores the effect of `x_t` on the price level, and (D) a misunderstanding of the nature of a fixed-wage contract (using `\\phi(s_t)` instead of its expectation `\\bar{\\phi}`).", "qid": "224", "question": "### Background\n\nAn economy features risk-averse workers and risk-neutral firms. Labor services are traded via **fixed nominal wage contracts**, where the wage is set before the period's shocks are realized. The economy is subject to real technology shocks (`s_t`) and nominal money supply shocks (`x_t`).\n\n### Data / Model Specification\n\nThe pre-determined equilibrium nominal wage is:\n\n```latex\n\\bar{w}_t = \\frac{m_{t-1}k\\bar{\\phi}}{F} \\quad \\text{(Eq. 1)}\n```\n\nwhere `k \\equiv 1/E(1/x)` and `\\bar{\\phi} \\equiv E_s[\\phi(s)]`. The price function is the same as in the spot market: `p_t(s,x) = m_{t-1}x_t / (F g(s_t))`.\n\nA worker from generation `t` earns `\\bar{w}_t` in their youth and consumes `c^{\\text{RW}} = \\bar{w}_t x_{t+1} / p_{t+1}` in their old age.\n\n### Question\n\nGiven the fixed nominal wage contract equilibrium, which of the following correctly states the real consumption `c^{\\text{RW}}` for a worker of generation `t` and accurately describes its properties?", "model_evaluation_result": {"model_raw_response": "I need to derive the real consumption for a worker of generation t under the fixed nominal wage contract.\n\nGiven information:\n- Pre-determined nominal wage: $\\bar{w}_t = \\frac{m_{t-1}k\\bar{\\phi}}{F}$\n- Price function: $p_t(s,x) = \\frac{m_{t-1}x_t}{Fg(s_t)}$\n- Worker consumption: $c^{RW} = \\frac{\\bar{w}_t x_{t+1}}{p_{t+1}}$\n\nLet me substitute the expressions:\n\n$c^{RW} = \\frac{\\bar{w}_t x_{t+1}}{p_{t+1}}$\n\n$= \\frac{\\frac{m_{t-1}k\\bar{\\phi}}{F} \\cdot x_{t+1}}{\\frac{m_t x_{t+1}}{Fg(s_{t+1})}}$\n\n$= \\frac{m_{t-1}k\\bar{\\phi} \\cdot x_{t+1} \\cdot Fg(s_{t+1})}{F \\cdot m_t x_{t+1}}$\n\n$= \\frac{m_{t-1}k\\bar{\\phi}g(s_{t+1})}{m_t}$\n\nSince $m_t = m_{t-1}$ (money supply in period t), this simplifies to:\n\n$c^{RW} = k\\bar{\\phi}g(s_{t+1})$\n\nNow let me analyze the properties:\n\n1. The consumption does NOT depend on the youth-period technology shock $s_t$ (it depends on $s_{t+1}$, the old-age period shock)\n2. The consumption does NOT depend on any nominal shocks ($x_t$ or $x_{t+1}$ cancel out)\n3. The consumption depends on $k = 1/E(1/x)$ and $\\bar{\\phi} = E_s[\\phi(s)]$, which are expectations\n4. The worker is fully insured against both youth-period technology shocks and nominal shocks\n\nThe result", "model_answer": "I need to derive the real consumption for a worker of generation t under the fixed nominal wage contract.\n\nGiven information:\n- Pre-determined nominal wage: $\\bar{w}_t = \\frac{m_{t-1}k\\bar{\\phi}}{F}$\n- Price function: $p_t(s,x) = \\frac{m_{t-1}x_t}{Fg(s_t)}$\n- Worker consumption: $c^{RW} = \\frac{\\bar{w}_t x_{t+1}}{p_{t+1}}$\n\nLet me substitute the expressions:\n\n$c^{RW} = \\frac{\\bar{w}_t x_{t+1}}{p_{t+1}}$\n\n$= \\frac{\\frac{m_{t-1}k\\bar{\\phi}}{F} \\cdot x_{t+1}}{\\frac{m_t x_{t+1}}{Fg(s_{t+1})}}$\n\n$= \\frac{m_{t-1}k\\bar{\\phi} \\cdot x_{t+1} \\cdot Fg(s_{t+1})}{F \\cdot m_t x_{t+1}}$\n\n$= \\frac{m_{t-1}k\\bar{\\phi}g(s_{t+1})}{m_t}$\n\nSince $m_t = m_{t-1}$ (money supply in period t), this simplifies to:\n\n$c^{RW} = k\\bar{\\phi}g(s_{t+1})$\n\nNow let me analyze the properties:\n\n1. The consumption does NOT depend on the youth-period technology shock $s_t$ (it depends on $s_{t+1}$, the old-age period shock)\n2. The consumption does NOT depend on any nominal shocks ($x_t$ or $x_{t+1}$ cancel out)\n3. The consumption depends on $k = 1/E(1/x)$ and $\\bar{\\phi} = E_s[\\phi(s)]$, which are expectations\n4. The worker is fully insured against both youth-period technology shocks and nominal shocks\n\nThe result", "error": "Warning: Could not extract single choice letter from: I need to derive the real consumption for a worker..."}}
{"ID": 303, "Question": "### Background\n\n**Research Question.** This problem addresses the core challenge of estimating demand systems derived from *nonlinear* parallel preference structures, where consumer utility `u` is unobservable and the standard method of deriving Marshallian demands via an invertible cost function fails.\n\n**Setting / Institutional Environment.** We consider a general parallel preference structure where the income-consumption curves may be nonlinear. The key issue is that the cost function `C(u,p)` cannot be easily inverted to find an indirect utility function `V(p,y)`, precluding the use of Roy's identity. An alternative estimation strategy is therefore required.\n\n**Variables & Parameters.**\n- `x_i`: Hicksian demand for good `i` (`i = 1, ..., N`).\n- `p`: An `N x 1` vector of prices.\n- `u`: The unobservable scalar utility index.\n- `f_i(·)`: A utility-dependent function, potentially nonlinear.\n- `ψ^i(p)`: A price-dependent function, `∂A(p)/∂p_i`.\n- `x_1`: The observed consumption of the numeraire good (`i=1`).\n\n---\n\n### Data / Model Specification\n\nThe theoretical model is the system of `N` Hicksian demand functions for a general parallel preference structure:\n```latex\nx_i = f_i(u) + ψ^i(p) \n\n\\quad\\text{for } i = 1, ..., N \\quad\\text{(Eq. 1)}\n```\nBy using the numeraire good `x_1` to substitute for `u`, one can derive the estimable 'generalized Ashenfelter-Heckman' (GAH) system:\n```latex\nx_i = g_i(x_1 - ψ^1(p)) + ψ^i(p) \n\n\\quad\\text{for } i = 2, ..., N \\quad\\text{(Eq. 2)}\n```\nwhere `g_i(·) = f_i(f_1^{-1}(·))`.\n\n---\n\n### Question\n\nIn the derived GAH system (Eq. 2), why would estimating the relationship using Ordinary Least Squares (OLS) lead to biased and inconsistent parameter estimates?", "Options": {"A": "The regressor `x_1` is endogenous because it is chosen simultaneously with `x_i`, making it correlated with unobserved factors in the error term.", "B": "The regressor `x_1` is likely to be highly correlated with the price vector `p`, causing multicollinearity.", "C": "The function `g_i(·)` is nonlinear, and OLS is only suitable for linear models.", "D": "The prices `p` are measured with error, which biases all coefficients."}, "Answer": "A", "pi_justification": "This item tests the core econometric insight from the original question's apex part. It uses a Reverse-Reasoning strategy. Distractors target common statistical misconceptions: confusing multicollinearity with endogeneity (B), misunderstanding OLS capabilities (C), and focusing on a different, less central problem (D).", "qid": "303", "question": "### Background\n\n**Research Question.** This problem addresses the core challenge of estimating demand systems derived from *nonlinear* parallel preference structures, where consumer utility `u` is unobservable and the standard method of deriving Marshallian demands via an invertible cost function fails.\n\n**Setting / Institutional Environment.** We consider a general parallel preference structure where the income-consumption curves may be nonlinear. The key issue is that the cost function `C(u,p)` cannot be easily inverted to find an indirect utility function `V(p,y)`, precluding the use of Roy's identity. An alternative estimation strategy is therefore required.\n\n**Variables & Parameters.**\n- `x_i`: Hicksian demand for good `i` (`i = 1, ..., N`).\n- `p`: An `N x 1` vector of prices.\n- `u`: The unobservable scalar utility index.\n- `f_i(·)`: A utility-dependent function, potentially nonlinear.\n- `ψ^i(p)`: A price-dependent function, `∂A(p)/∂p_i`.\n- `x_1`: The observed consumption of the numeraire good (`i=1`).\n\n---\n\n### Data / Model Specification\n\nThe theoretical model is the system of `N` Hicksian demand functions for a general parallel preference structure:\n```latex\nx_i = f_i(u) + ψ^i(p) \n\n\\quad\\text{for } i = 1, ..., N \\quad\\text{(Eq. 1)}\n```\nBy using the numeraire good `x_1` to substitute for `u`, one can derive the estimable 'generalized Ashenfelter-Heckman' (GAH) system:\n```latex\nx_i = g_i(x_1 - ψ^1(p)) + ψ^i(p) \n\n\\quad\\text{for } i = 2, ..., N \\quad\\text{(Eq. 2)}\n```\nwhere `g_i(·) = f_i(f_1^{-1}(·))`.\n\n---\n\n### Question\n\nIn the derived GAH system (Eq. 2), why would estimating the relationship using Ordinary Least Squares (OLS) lead to biased and inconsistent parameter estimates?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 120, "Question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal (e.g., a seller) can design a mechanism to extract all economic surplus from privately informed agents (e.g., bidders), a scenario known as Full Surplus Extraction (FSE).\n\n**Setting:** A seller designs an incentive compatible (IC) and individually rational (IR) mechanism to sell a single object. An agent's private information is captured by their 'type' $\\theta_i$, which consists of a valuation for the object $\\hat{v}_i(\\theta_i)$ and a belief about others' types $\\hat{b}_i(\\theta_i)$. The analysis centers on the properties of the common prior, $p$, over the space of all type profiles.\n\n### Data / Model Specification\n\n**Key Definitions:**\n- **Beliefs-Determine-Preferences (BDP):** A prior $p$ is BDP for bidder $i$ if, with $p$-probability 1, any two of bidder $i$'s types with different valuations must also have different beliefs.\n- **Non-BDP Prior:** A prior $p$ is non-BDP for bidder $i$ if there exist disjoint sets of types, $A_i$ and $A_i'$, each with positive $p$-probability, such that for any type $\\theta_i \\in A_i$, there exists a type $\\theta_i' \\in A_i'$ with the same beliefs, $\\hat{b}_i(\\theta_i) = \\hat{b}_i(\\theta_i')$, but a strictly lower valuation, $\\hat{v}_i(\\theta_i) > \\hat{v}_i(\\theta_i')$.\n\n### The Question\n\nConsider a prior that is non-BDP for bidder $i$. This means there's a positive probability of finding a high-valuation type $\\theta_i$ and a low-valuation type $\\theta_i'$ who share identical beliefs but have different valuations ($\\hat{v}_i(\\theta_i) > \\hat{v}_i(\\theta_i')$). Why does this structure make Full Surplus Extraction impossible?", "Options": {"A": "The high-valuation type $\\theta_i$ can mimic the low-valuation type $\\theta_i'$, pay the lower amount intended for $\\theta_i'$, and retain the difference in valuations, $\\hat{v}_i(\\theta_i) - \\hat{v}_i(\\theta_i')$, as a positive information rent.", "B": "The seller cannot distinguish between the two types, so they must offer both the same contract, which is inefficient.", "C": "The identical beliefs cause the incentive compatibility constraints for both types to fail, making any mechanism design impossible.", "D": "The low-valuation type $\\theta_i'$ can mimic the high-valuation type $\\theta_i$ to try and win the object more often, which breaks the mechanism."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to target the core logical step in the proof that FSE requires BDP. It assesses whether the student understands the mechanism of strategic mimicry that generates information rents. Distractor B is an 'Almost Right' common intuition, C misidentifies which constraint fails, and D reverses the logic of the mimicry, making it a classic 'Conceptual Opposite' error.", "qid": "120", "question": "### Background\n\n**Research Question:** This problem investigates the fundamental informational conditions under which a principal (e.g., a seller) can design a mechanism to extract all economic surplus from privately informed agents (e.g., bidders), a scenario known as Full Surplus Extraction (FSE).\n\n**Setting:** A seller designs an incentive compatible (IC) and individually rational (IR) mechanism to sell a single object. An agent's private information is captured by their 'type' $\\theta_i$, which consists of a valuation for the object $\\hat{v}_i(\\theta_i)$ and a belief about others' types $\\hat{b}_i(\\theta_i)$. The analysis centers on the properties of the common prior, $p$, over the space of all type profiles.\n\n### Data / Model Specification\n\n**Key Definitions:**\n- **Beliefs-Determine-Preferences (BDP):** A prior $p$ is BDP for bidder $i$ if, with $p$-probability 1, any two of bidder $i$'s types with different valuations must also have different beliefs.\n- **Non-BDP Prior:** A prior $p$ is non-BDP for bidder $i$ if there exist disjoint sets of types, $A_i$ and $A_i'$, each with positive $p$-probability, such that for any type $\\theta_i \\in A_i$, there exists a type $\\theta_i' \\in A_i'$ with the same beliefs, $\\hat{b}_i(\\theta_i) = \\hat{b}_i(\\theta_i')$, but a strictly lower valuation, $\\hat{v}_i(\\theta_i) > \\hat{v}_i(\\theta_i')$.\n\n### The Question\n\nConsider a prior that is non-BDP for bidder $i$. This means there's a positive probability of finding a high-valuation type $\\theta_i$ and a low-valuation type $\\theta_i'$ who share identical beliefs but have different valuations ($\\hat{v}_i(\\theta_i) > \\hat{v}_i(\\theta_i')$). Why does this structure make Full Surplus Extraction impossible?", "model_evaluation_result": {"model_raw_response": "Looking at this mechanism design problem, I need to understand why non-BDP priors prevent Full Surplus Extraction (FSE).\n\nThe key insight is in the definition of non-BDP: there exist types θᵢ and θᵢ' where:\n- They have identical beliefs: b̂ᵢ(θᵢ) = b̂ᵢ(θᵢ')  \n- But different valuations: v̂ᵢ(θᵢ) > v̂ᵢ(θᵢ')\n\nIn mechanism design, FSE means extracting all surplus, leaving agents with zero information rents. This requires the mechanism to perfectly discriminate between types and charge each exactly their valuation.\n\nHowever, when two types have identical beliefs, they form identical expectations about:\n- Other agents' types and behaviors\n- The mechanism's outcomes\n- Their own optimal strategies\n\nSince mechanism design relies on agents' beliefs to create incentive compatibility, types with identical beliefs will evaluate any given mechanism option identically. This means:\n\n1. The high-valuation type θᵢ will see the contract intended for the low-valuation type θᵢ' as equally attractive (from a beliefs perspective)\n2. Since θᵢ has higher valuation than θᵢ', they can choose θᵢ''s contract and still get positive utility\n3. Specifically, θᵢ can \"mimic\" θᵢ', pay the lower amount designed for θᵢ', and retain the valuation difference v̂ᵢ(θᵢ) - v̂ᵢ(θᵢ') as information rent\n\nThis mimicking opportunity prevents the seller from extracting full surplus from the high-valuation type.\n\nA", "model_answer": "A", "error": null}}
{"ID": 230, "Question": "### Background\n\n**Research Question.** This problem analyzes the theoretical model of a single mother's discrete choice among employment and childcare alternatives.\n\n**Setting and Sample.** A single mother makes a static choice from a set of mutually exclusive alternatives. The choice is modeled within a Random Utility Maximization (RUM) framework, where utility depends on the chosen alternative's attributes.\n\n### Data / Model Specification\n\nThe choice set and corresponding budget constraints are given in Table 1.\n\n**Table 1: List of Alternatives and Budget Constraints**\n\n| Alternative | Employment Status | Childcare Payment | Subsidy Status | Budget Constraint |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | No-work | — | — | `C = N` |\n| 2 | Part-time | Yes | Yes | `C + (P_s - S)H_p = W_{PT}H_p + N` |\n| 4 | Part-time | Yes | No | `C + P_sH_p = W_{PT}H_p + N` |\n\n*Where C=Consumption, N=Non-wage income, P_s=Price of care, S=Subsidy, H_p=Hours of paid care, W_PT=Part-time wage.*\n\nThe indirect utility for alternative `i` is `V_i = v_i + \\varepsilon_i`, where `v_i` is the observable component and `\\varepsilon_i` is the unobserved component. The mother chooses alternative `i` if `V_i > V_j` for all other alternatives `j`.\n\n### The Question\n\nConsidering the budget constraints in Table 1, how does a childcare subsidy `S` affect a mother's choice, and what is the fundamental condition for choosing alternative `i` over `j`?", "Options": {"A": "The subsidy `S` reduces the effective price of childcare, and alternative `i` is chosen if the difference in unobserved utility (`\\varepsilon_j - \\varepsilon_i`) is less than the difference in observed utility (`v_i - v_j`).", "B": "The subsidy `S` reduces the effective price of childcare, and alternative `i` is chosen if the difference in unobserved utility (`\\varepsilon_i - \\varepsilon_j`) is less than the difference in observed utility (`v_i - v_j`).", "C": "The subsidy `S` acts as a lump-sum transfer increasing non-wage income `N`, and alternative `i` is chosen if the difference in unobserved utility (`\\varepsilon_j - \\varepsilon_i`) is less than the difference in observed utility (`v_i - v_j`).", "D": "The subsidy `S` acts as a lump-sum transfer increasing non-wage income `N`, and alternative `i` is chosen if the observed utility `v_i` is greater than `v_j`."}, "Answer": "A", "pi_justification": "This item tests the student's ability to interpret the model's budget constraints and perform a basic derivation within the Random Utility framework. The distractors are designed to separate errors in economic interpretation (C) from errors in algebraic manipulation (B).", "qid": "230", "question": "### Background\n\n**Research Question.** This problem analyzes the theoretical model of a single mother's discrete choice among employment and childcare alternatives.\n\n**Setting and Sample.** A single mother makes a static choice from a set of mutually exclusive alternatives. The choice is modeled within a Random Utility Maximization (RUM) framework, where utility depends on the chosen alternative's attributes.\n\n### Data / Model Specification\n\nThe choice set and corresponding budget constraints are given in Table 1.\n\n**Table 1: List of Alternatives and Budget Constraints**\n\n| Alternative | Employment Status | Childcare Payment | Subsidy Status | Budget Constraint |\n| :--- | :--- | :--- | :--- | :--- |\n| 1 | No-work | — | — | `C = N` |\n| 2 | Part-time | Yes | Yes | `C + (P_s - S)H_p = W_{PT}H_p + N` |\n| 4 | Part-time | Yes | No | `C + P_sH_p = W_{PT}H_p + N` |\n\n*Where C=Consumption, N=Non-wage income, P_s=Price of care, S=Subsidy, H_p=Hours of paid care, W_PT=Part-time wage.*\n\nThe indirect utility for alternative `i` is `V_i = v_i + \\varepsilon_i`, where `v_i` is the observable component and `\\varepsilon_i` is the unobserved component. The mother chooses alternative `i` if `V_i > V_j` for all other alternatives `j`.\n\n### The Question\n\nConsidering the budget constraints in Table 1, how does a childcare subsidy `S` affect a mother's choice, and what is the fundamental condition for choosing alternative `i` over `j`?", "model_evaluation_result": {"model_raw_response": "Looking at this discrete choice problem, I need to analyze two key aspects: how the subsidy S affects the budget constraint and the condition for choosing alternative i over j.\n\nFirst, examining the budget constraints in Table 1:\n- Alternative 2 (Part-time with subsidy): C + (Ps - S)Hp = WPTHp + N\n- Alternative 4 (Part-time without subsidy): C + PsHp = WPTHp + N\n\nThe subsidy S directly reduces the effective price of childcare from Ps to (Ps - S). This is a price reduction, not a lump-sum transfer to non-wage income N.\n\nSecond, for the choice condition in the Random Utility Maximization framework:\n- Alternative i is chosen if Vi > Vj for all other alternatives j\n- Since Vi = vi + εi and Vj = vj + εj\n- The condition becomes: vi + εi > vj + εj\n- Rearranging: εj - εi < vi - vj\n\nThis means alternative i is chosen when the difference in unobserved utility components (εj - εi) is less than the difference in observed utility components (vi - vj).\n\nEvaluating the options:\n- Options C and D incorrectly describe the subsidy as a lump-sum transfer affecting N\n- Option B has the wrong sign for the unobserved utility difference (εi - εj instead of εj - εi)\n- Option A correctly identifies both the price reduction effect and the proper choice condition\n\nA", "model_answer": "C", "error": null}}
{"ID": 164, "Question": "### Background\n\nExpected Uncertain Utility (EUU) theory is built on a set of axioms governing preferences over acts. A key distinction is made between **ideal events**, which have quantifiable probabilities, and **diffuse events**, which represent purely unquantifiable uncertainty.\n\n### Data / Model Specification\n\nAn agent is observed to have the following preference pattern:\n-   Let `D₁` and `D₂` be two disjoint, non-null diffuse events.\n-   **Bet A:** Pays $100 if an outcome in `D₁` occurs, $0 otherwise.\n-   **Bet B:** Pays $100 if an outcome in `D₁ ∪ D₂` occurs, $0 otherwise.\n-   The agent is indifferent between Bet A and Bet B (`Bet A ~ Bet B`).\n\nThis indifference violates strict state-wise dominance, as Bet B offers the same prizes as Bet A on `D₁` and a better prize on `D₂`.\n\n### Question\n\nThis specific pattern of indifference is a direct consequence of which core axiom of EUU theory?", "Options": {"A": "Axiom 2 (Dominance): If an act `f` pays strictly more than an act `g` in every state, then `f` is strictly preferred to `g`.", "B": "Axiom 3 (Symmetry for diffuse events): For any prizes `x,y`, ideal event `E`, and diffuse events `D, D'`, the agent is indifferent between a bet on `E∩D` and a bet on `E∩D'`.", "C": "Axiom 4 (Comparative Probability): The ranking of bets on ideal events is independent of the prizes used, establishing a unique probability measure `μ`.", "D": "Axiom 6 (Continuity): If a sequence of acts `f_n` converges to `f`, and `g ≿ f_n ≿ h` for all `n`, then `g ≿ f ≿ h`."}, "Answer": "B", "pi_justification": "This question assesses the understanding of the axiomatic foundations of EUU. Strategy: Reverse-Reasoning. It asks to identify the axiom that explains a specific, counter-intuitive behavioral pattern. The correct answer is Axiom 3, which forces all diffuse (purely unquantifiable) events to be treated symmetrically, leading to the observed indifference. Distractor A is the principle that is being violated. Distractors C and D are other important axioms in the system, but they are not the direct cause of this specific indifference pattern over diffuse sets.", "qid": "164", "question": "### Background\n\nExpected Uncertain Utility (EUU) theory is built on a set of axioms governing preferences over acts. A key distinction is made between **ideal events**, which have quantifiable probabilities, and **diffuse events**, which represent purely unquantifiable uncertainty.\n\n### Data / Model Specification\n\nAn agent is observed to have the following preference pattern:\n-   Let `D₁` and `D₂` be two disjoint, non-null diffuse events.\n-   **Bet A:** Pays $100 if an outcome in `D₁` occurs, $0 otherwise.\n-   **Bet B:** Pays $100 if an outcome in `D₁ ∪ D₂` occurs, $0 otherwise.\n-   The agent is indifferent between Bet A and Bet B (`Bet A ~ Bet B`).\n\nThis indifference violates strict state-wise dominance, as Bet B offers the same prizes as Bet A on `D₁` and a better prize on `D₂`.\n\n### Question\n\nThis specific pattern of indifference is a direct consequence of which core axiom of EUU theory?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to identify which axiom of EUU theory directly leads to the observed indifference between Bet A and Bet B.\n\nLet me analyze what's happening:\n- Bet A pays $100 on D₁, $0 otherwise\n- Bet B pays $100 on D₁ ∪ D₂, $0 otherwise\n- The agent is indifferent: Bet A ~ Bet B\n\nThis violates standard dominance because Bet B clearly dominates Bet A (same payoff on D₁, better payoff on D₂).\n\nNow let me examine each axiom:\n\n**Option A (Dominance)**: This axiom would actually be violated by the observed behavior, not explain it. If this axiom held strictly, Bet B should be strictly preferred to Bet A.\n\n**Option B (Symmetry for diffuse events)**: This axiom states that for any prizes x,y, ideal event E, and diffuse events D, D', the agent is indifferent between bets on E∩D and E∩D'. This is about symmetry between different diffuse events when intersected with ideal events.\n\n**Option C (Comparative Probability)**: This deals with ranking bets on ideal events and establishing probability measures, not directly relevant to diffuse events.\n\n**Option D (Continuity)**: This is a technical condition about convergence of acts, not directly related to the specific indifference pattern.\n\nThe key insight is that in EUU theory, diffuse events represent pure unquantifiable uncertainty. The symmetry axiom (Option B) captures the idea that all diffuse events are treated equivalently - the agent cannot distinguish between different sources of unquantifiable uncertainty.\n\nThe indifference between Bet A and Bet B reflects this symmetry: since D₁ and D₂ are both diffuse events, the agent treats the uncertainty about D₂ as essentially equivalent to having no additional information. This makes the agent indifferent between getting $100 on just D₁ versus getting $100 on D₁ ∪ D₂.\n\nB", "model_answer": "A", "error": null}}
{"ID": 195, "Question": "### Background\n\nThis paper develops a theoretical model to understand the meaning of residual correlations around Engel curves. The analysis is grounded in the classical theory of consumer demand but makes specific assumptions to achieve analytical tractability when using cross-sectional budget data.\n\n### Data / Model Specification\n\nThe consumer's problem is to maximize a utility function `u(x)` subject to a budget constraint `p'x = M`. The model makes two key assumptions:\n\n**Assumption I:** The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`.\nThe vector `a` represents consumer-specific linear preference terms, while the matrix `B` of quadratic terms is assumed to be identical for all consumers.\n\n**Assumption II:** Prices `p` are also identical for all consumers in the budget survey.\n\nThe first-order conditions for utility maximization can be written as a linear system:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -a \\\\ -M \\end{pmatrix} \\quad \\text{(Eq. (1))}\n```\nThe inverse of the bordered matrix on the left defines the substitution matrix `C` and the income-effect vector `c`:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix}^{-1} = \\begin{pmatrix} C & c \\\\ c' & c_0 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n```\nThis leads to a demand system `x = -Ca - cM`. The Engel curve for good `i` is found by separating the mean preference `α` from the idiosyncratic preference `(a - α)`. The disturbance term `ν` for the vector of goods is `ν = -C(a - α)`.\n\n### Question\n\nThe disturbance vector `ν = -C(a - α)` represents a family's deviation from the average consumption pattern due to its unique tastes. Based on the properties of the model's matrices, what is the total expenditure on this disturbance bundle, `p'ν`, and what is the correct economic intuition?", "Options": {"A": "`p'ν = 0`, because taste-driven deviations from the mean consumption bundle must be budget-neutral.", "B": "`p'ν > 0`, because taste deviations are correlated with income, leading to higher overall spending.", "C": "`p'ν < 0`, because the substitution matrix `C` is negative semi-definite, constraining the value of the bundle.", "D": "The value of `p'ν` is indeterminate without knowing the specific taste profile `(a - α)`."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests a key derived property of the model's disturbance term and its economic meaning. The correct answer (A) pairs the correct mathematical result with the correct intuition. Distractor (B) incorrectly links this property to the income-correlation model discussed elsewhere in the paper. Distractor (C) misapplies the concept of a negative semi-definite matrix. Distractor (D) fails to recognize that the result is a general identity of the model, not a case-specific outcome.", "qid": "195", "question": "### Background\n\nThis paper develops a theoretical model to understand the meaning of residual correlations around Engel curves. The analysis is grounded in the classical theory of consumer demand but makes specific assumptions to achieve analytical tractability when using cross-sectional budget data.\n\n### Data / Model Specification\n\nThe consumer's problem is to maximize a utility function `u(x)` subject to a budget constraint `p'x = M`. The model makes two key assumptions:\n\n**Assumption I:** The utility function is quadratic: `u(x) = a'x + (1/2)x'Bx`.\nThe vector `a` represents consumer-specific linear preference terms, while the matrix `B` of quadratic terms is assumed to be identical for all consumers.\n\n**Assumption II:** Prices `p` are also identical for all consumers in the budget survey.\n\nThe first-order conditions for utility maximization can be written as a linear system:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\lambda \\end{pmatrix} = \\begin{pmatrix} -a \\\\ -M \\end{pmatrix} \\quad \\text{(Eq. (1))}\n```\nThe inverse of the bordered matrix on the left defines the substitution matrix `C` and the income-effect vector `c`:\n```latex\n\\begin{pmatrix} B & -p \\\\ -p' & 0 \\end{pmatrix}^{-1} = \\begin{pmatrix} C & c \\\\ c' & c_0 \\end{pmatrix} \\quad \\text{(Eq. (2))}\n```\nThis leads to a demand system `x = -Ca - cM`. The Engel curve for good `i` is found by separating the mean preference `α` from the idiosyncratic preference `(a - α)`. The disturbance term `ν` for the vector of goods is `ν = -C(a - α)`.\n\n### Question\n\nThe disturbance vector `ν = -C(a - α)` represents a family's deviation from the average consumption pattern due to its unique tastes. Based on the properties of the model's matrices, what is the total expenditure on this disturbance bundle, `p'ν`, and what is the correct economic intuition?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine the value of p'ν where ν = -C(a - α).\n\nLet me work through this step by step:\n\nFrom the given information:\n- ν = -C(a - α) represents taste-driven deviations from average consumption\n- C is the substitution matrix from the bordered matrix inverse\n- The bordered matrix system comes from the first-order conditions of utility maximization\n\nThe key insight comes from the structure of the bordered matrix inverse:\n```\n[B  -p]^(-1) = [C  c ]\n[-p' 0]        [c' c₀]\n```\n\nFrom the properties of this bordered matrix inverse, we know that:\nCc + pc₀ = 0\n\nThis means: Cc = -pc₀\n\nNow, let's calculate p'ν:\np'ν = p'(-C(a - α))\n    = -p'C(a - α)\n\nSince C is symmetric (as it's derived from the inverse of a symmetric bordered matrix), we have:\np'C = (Cp)'\n\nFrom the matrix relationship Cc = -pc₀, we can derive that:\np'C = -c₀p'\n\nBut more directly, from the fundamental property of the bordered matrix system, we know that:\np'C = 0\n\nThis is because the substitution matrix C has the property that price-weighted sums of substitution effects equal zero (this follows from the budget constraint and the structure of the utility maximization problem).\n\nTherefore:\np'ν = -p'C(a - α) = 0\n\nThe economic intuition is that taste-driven deviations from the mean consumption pattern must be budget-neutral - families may consume more of some goods and less of others due to their preferences, but the total expenditure value of these deviations must sum to zero.\n\nA", "model_answer": "C", "error": null}}
{"ID": 250, "Question": "### Background\n\nThe paper's central thesis is that extensive scale (economic integration, `N_c`) is the key driver of long-run growth, while intensive scale (population, `n`) is not, even though both affect the level of income.\n\n### Data / Model Specification\n\n- Per capita income in the modern sector, `y_M`, depends on the product of `n` and `N_c`:\n  ```latex\n  y_M = A_M (n N_c)^{1-\\alpha} (e_M h)^{2-\\alpha}\n  ```\n- The productivity of learning, `δ`, which drives the long-run growth rate of human capital and income, depends only on `N_c`:\n  ```latex\n  \\delta = (b_1 N_c)^\\gamma\n  ```\n\n### Question\n\nA policymaker wants to boost long-run economic performance and is considering two mutually exclusive options, each of which will double the total modern-sector population (`n N_c`):\n\n(a) A pro-natalist policy that doubles population per city (`n`).\n(b) A trade liberalization policy that doubles the number of integrated cities (`N_c`).\n\nWhich statement accurately compares the long-run consequences of these two policies?", "Options": {"A": "Both policies produce the same one-time increase in the level of per capita income, but only the trade liberalization policy increases the long-run growth rate.", "B": "Both policies produce the same one-time increase in the income level and the same increase in the long-run growth rate.", "C": "The trade liberalization policy leads to a larger one-time increase in the income level and a higher long-run growth rate.", "D": "The pro-natalist policy is superior because it increases the domestic market size, leading to a higher long-run growth rate than the trade policy."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem targets sharp, specific concepts that are highly convertible (Conceptual Clarity = 9/10). The distinction between level and growth effects, and the mechanisms behind them, are classic sources of confusion, making it ideal for creating high-fidelity distractors (Discriminability = 10/10). This apex question assesses the synthesis of the paper's static and dynamic results, forcing a distinction between level and growth effects. It uses a Scenario Application strategy. The distractors are built around the classic and predictable misconception of conflating level and growth effects (B), making it a powerful diagnostic tool.", "qid": "250", "question": "### Background\n\nThe paper's central thesis is that extensive scale (economic integration, `N_c`) is the key driver of long-run growth, while intensive scale (population, `n`) is not, even though both affect the level of income.\n\n### Data / Model Specification\n\n- Per capita income in the modern sector, `y_M`, depends on the product of `n` and `N_c`:\n  ```latex\n  y_M = A_M (n N_c)^{1-\\alpha} (e_M h)^{2-\\alpha}\n  ```\n- The productivity of learning, `δ`, which drives the long-run growth rate of human capital and income, depends only on `N_c`:\n  ```latex\n  \\delta = (b_1 N_c)^\\gamma\n  ```\n\n### Question\n\nA policymaker wants to boost long-run economic performance and is considering two mutually exclusive options, each of which will double the total modern-sector population (`n N_c`):\n\n(a) A pro-natalist policy that doubles population per city (`n`).\n(b) A trade liberalization policy that doubles the number of integrated cities (`N_c`).\n\nWhich statement accurately compares the long-run consequences of these two policies?", "model_evaluation_result": {"model_raw_response": "Looking at this systematically:\n\n**Level Effects (one-time income increase):**\nBoth policies double the total modern-sector population (n × N_c), so from the income equation:\ny_M = A_M (n N_c)^(1-α) (e_M h)^(2-α)\n\nBoth policies will produce the same proportional increase in the level of per capita income since (n N_c) appears with the same exponent (1-α) in both cases.\n\n**Growth Effects (long-run growth rate):**\nThe key difference lies in the productivity of learning equation:\nδ = (b_1 N_c)^γ\n\n- Policy (a): Doubles n, keeps N_c constant → δ remains unchanged → no change in long-run growth rate\n- Policy (b): Doubles N_c, keeps n constant → δ increases by factor 2^γ → higher long-run growth rate\n\nThe central thesis explicitly states that \"extensive scale (economic integration, N_c) is the key driver of long-run growth, while intensive scale (population, n) is not.\"\n\nTherefore:\n- Both policies have identical level effects\n- Only the trade liberalization policy (doubling N_c) affects the long-run growth rate\n- The pro-natalist policy (doubling n) has no impact on long-run growth\n\nA", "model_answer": "A", "error": null}}
{"ID": 306, "Question": "### Background\n\nThe linear parallel structure is a special case of the Gorman polar form. The paper notes this is the most general model that \"yields a globally consistent aggregate preference function that is independent of the distribution of income.\"\n\n---\n\n### Question\n\nWhat is the primary practical advantage of this aggregation property for empirical demand analysis?", "Options": {"A": "It allows researchers to use aggregate market-level data (e.g., total income) to estimate demand systems as if they were generated by a single representative agent.", "B": "It ensures that the estimated demand elasticities will be constant across all income levels.", "C": "It simplifies estimation by guaranteeing that all demand functions are linear in prices and income.", "D": "It proves that individual consumer preferences are identical, which is a necessary assumption for market analysis."}, "Answer": "A", "pi_justification": "This item assesses the synthesis part of the original QA. It tests a deep conceptual point about the bridge between micro theory and macro data. Distractors are designed to be plausible but incorrect interpretations of the aggregation property, targeting common oversimplifications or false conclusions.", "qid": "306", "question": "### Background\n\nThe linear parallel structure is a special case of the Gorman polar form. The paper notes this is the most general model that \"yields a globally consistent aggregate preference function that is independent of the distribution of income.\"\n\n---\n\n### Question\n\nWhat is the primary practical advantage of this aggregation property for empirical demand analysis?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 225, "Question": "### Background\n\nIn an economy with real (technology, `s`) and nominal (money, `x`) shocks, risk-averse workers can trade labor with risk-neutral firms in one of two market structures:\n1.  **Spot Market:** Provides consumption `c^s = \\phi(s_t) g(s_{t+1})`, which is exposed to real shocks but insured against nominal shocks.\n2.  **Fixed Nominal Wage Contract:** Provides consumption `c^{\\text{RW}} = k \\bar{\\phi} g(s_{t+1}) / x_t`, which is insured against youth-period real shocks (`s_t`) but exposed to nominal shocks (`x_t`).\n\n### Question\n\nConsider the choice between these two market structures from the perspective of a risk-averse worker. If the central bank's policy leads to a mean-preserving increase in the variance of the nominal shock `x_t`, what is the most likely consequence for the optimal market structure?", "Options": {"A": "The fixed nominal wage contract becomes relatively more attractive because it provides better insurance against real shocks.", "B": "The spot market becomes relatively more attractive because it perfectly insures the worker against the now more volatile nominal shocks.", "C": "The relative attractiveness of the two structures is unchanged because the mean of the nominal shock is constant.", "D": "Both market structures become less attractive, but the fixed nominal wage contract is affected less because firms are risk-neutral."}, "Answer": "B", "pi_justification": "This item replaces part of a QA problem (Suitability Score: 9.5). It uses the Scenario Application strategy to test the student's understanding of the core trade-off between the two market structures. The question assesses whether the student can correctly infer how the desirability of each insurance mechanism changes when the variance of a specific shock increases. Distractors represent classic misconceptions: (A) Conceptual Opposite, (C) ignoring the role of variance for risk-averse agents, and (D) flawed reasoning about who bears the risk.", "qid": "225", "question": "### Background\n\nIn an economy with real (technology, `s`) and nominal (money, `x`) shocks, risk-averse workers can trade labor with risk-neutral firms in one of two market structures:\n1.  **Spot Market:** Provides consumption `c^s = \\phi(s_t) g(s_{t+1})`, which is exposed to real shocks but insured against nominal shocks.\n2.  **Fixed Nominal Wage Contract:** Provides consumption `c^{\\text{RW}} = k \\bar{\\phi} g(s_{t+1}) / x_t`, which is insured against youth-period real shocks (`s_t`) but exposed to nominal shocks (`x_t`).\n\n### Question\n\nConsider the choice between these two market structures from the perspective of a risk-averse worker. If the central bank's policy leads to a mean-preserving increase in the variance of the nominal shock `x_t`, what is the most likely consequence for the optimal market structure?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze how increased nominal shock variance affects each market structure's attractiveness to a risk-averse worker.\n\nLet me examine the consumption under each structure:\n\n**Spot Market:** `c^s = φ(s_t) g(s_{t+1})`\n- Exposed to real shocks (s_t and s_{t+1})\n- Completely insulated from nominal shocks (x_t doesn't appear)\n\n**Fixed Nominal Wage Contract:** `c^RW = k φ̄ g(s_{t+1}) / x_t`\n- Insured against youth-period real shocks (s_t) since φ̄ is constant\n- Still exposed to old-age real shocks (s_{t+1})\n- Directly exposed to nominal shocks through the 1/x_t term\n\nNow, when the central bank increases the variance of x_t while keeping its mean constant:\n\n- The spot market is unaffected since it has no exposure to nominal shocks\n- The fixed nominal wage contract becomes riskier because consumption now has higher variance due to the 1/x_t term (higher variance in x_t increases variance in 1/x_t)\n\nFor a risk-averse worker, increased consumption volatility reduces utility. Since only the fixed wage contract becomes more volatile while the spot market remains unchanged, the spot market becomes relatively more attractive.\n\nThe key insight is that the spot market provides perfect insurance against nominal shocks (complete insulation), while the fixed wage contract exposes the worker to these shocks.\n\nB", "model_answer": "B", "error": null}}
{"ID": 254, "Question": "### Background\n\n**Research Question.** This problem examines the core mathematical structure of the Integer-Valued Autoregressive (INAR) model for count data, focusing on its limiting behavior.\n\n**Setting / Institutional Environment.** The model is a first-order integer-valued autoregressive process, INAR(1), for a time series of counts `X_t`. The core mechanism assumes that the count in period `t` is the sum of a 'carry-over' part from `X_{t-1}` and an 'innovation' part `I_t`.\n\n---\n\n### Data / Model Specification\n\nThe INAR(1) process `X_t = α_t ∘ X_{t-1} + I_t` has a conditional Probability Generating Function (PGF) given `X_{t-1}=x_{t-1}` of:\n```latex\nG_{X_t|x_{t-1}}(s) = (1 - \\alpha_t + \\alpha_t s)^{x_{t-1}} \\exp(\\lambda_t^I(s-1))\n```\nwhere `λ_t^I = λ_t - α_t λ_{t-1}`.\n\n---\n\nConsider the conditional distribution of `X_t` given `x_{t-1}`. Analyze its limiting behavior in two scenarios: (i) as the carry-over probability `α_t → 0` (no state dependence), and (ii) as `α_t → 1` (perfect state dependence). Which of the following correctly identifies the limiting distributions?", "Options": {"A": "As `α_t → 0`, the distribution approaches Poisson; as `α_t → 1`, it approaches a shifted Poisson.", "B": "As `α_t → 0`, the distribution approaches Binomial; as `α_t → 1`, it approaches a shifted Poisson.", "C": "As `α_t → 0`, the distribution approaches Poisson; as `α_t → 1`, it also approaches a Poisson.", "D": "As `α_t → 0`, the distribution approaches a shifted Poisson; as `α_t → 1`, it approaches Poisson."}, "Answer": "A", "pi_justification": "This high-difficulty item assesses the user's ability to perform an asymptotic analysis of the model's core generating function, a key test of deep model comprehension. It uses a Computational Judgment strategy. Distractors are designed to catch common errors in limit calculations (C), misidentification of probability generating functions (B), or simply swapping the two cases (D, Conceptual Opposite).", "qid": "254", "question": "### Background\n\n**Research Question.** This problem examines the core mathematical structure of the Integer-Valued Autoregressive (INAR) model for count data, focusing on its limiting behavior.\n\n**Setting / Institutional Environment.** The model is a first-order integer-valued autoregressive process, INAR(1), for a time series of counts `X_t`. The core mechanism assumes that the count in period `t` is the sum of a 'carry-over' part from `X_{t-1}` and an 'innovation' part `I_t`.\n\n---\n\n### Data / Model Specification\n\nThe INAR(1) process `X_t = α_t ∘ X_{t-1} + I_t` has a conditional Probability Generating Function (PGF) given `X_{t-1}=x_{t-1}` of:\n```latex\nG_{X_t|x_{t-1}}(s) = (1 - \\alpha_t + \\alpha_t s)^{x_{t-1}} \\exp(\\lambda_t^I(s-1))\n```\nwhere `λ_t^I = λ_t - α_t λ_{t-1}`.\n\n---\n\nConsider the conditional distribution of `X_t` given `x_{t-1}`. Analyze its limiting behavior in two scenarios: (i) as the carry-over probability `α_t → 0` (no state dependence), and (ii) as `α_t → 1` (perfect state dependence). Which of the following correctly identifies the limiting distributions?", "model_evaluation_result": {"model_raw_response": "I need to analyze the limiting behavior of the INAR(1) process by examining the conditional PGF as α_t approaches 0 and 1.\n\nGiven PGF: G_{X_t|x_{t-1}}(s) = (1 - α_t + α_t s)^{x_{t-1}} exp(λ_t^I(s-1))\n\nwhere λ_t^I = λ_t - α_t λ_{t-1}\n\n**Case (i): As α_t → 0**\n\nWhen α_t → 0:\n- The first term: (1 - α_t + α_t s)^{x_{t-1}} → (1)^{x_{t-1}} = 1\n- λ_t^I = λ_t - α_t λ_{t-1} → λ_t - 0 = λ_t\n- The second term: exp(λ_t^I(s-1)) → exp(λ_t(s-1))\n\nTherefore: G_{X_t|x_{t-1}}(s) → 1 · exp(λ_t(s-1)) = exp(λ_t(s-1))\n\nThis is the PGF of a Poisson distribution with parameter λ_t.\n\n**Case (ii): As α_t → 1**\n\nWhen α_t → 1:\n- The first term: (1 - α_t + α_t s)^{x_{t-1}} → (0 + s)^{x_{t-1}} = s^{x_{t-1}}\n- λ_t^I = λ_t - α_t λ_{t-1} → λ_t - λ_{t-1}\n- The second term: exp(λ_t^I(s-1)) → exp((λ_t - λ_{t-1})(s-1))\n\nTherefore: G_{X_t|x_{t-1}}(s) → s^{x_{t-1}} · exp((λ_t - λ_{t-1})(s-", "model_answer": "I need to analyze the limiting behavior of the INAR(1) process by examining the conditional PGF as α_t approaches 0 and 1.\n\nGiven PGF: G_{X_t|x_{t-1}}(s) = (1 - α_t + α_t s)^{x_{t-1}} exp(λ_t^I(s-1))\n\nwhere λ_t^I = λ_t - α_t λ_{t-1}\n\n**Case (i): As α_t → 0**\n\nWhen α_t → 0:\n- The first term: (1 - α_t + α_t s)^{x_{t-1}} → (1)^{x_{t-1}} = 1\n- λ_t^I = λ_t - α_t λ_{t-1} → λ_t - 0 = λ_t\n- The second term: exp(λ_t^I(s-1)) → exp(λ_t(s-1))\n\nTherefore: G_{X_t|x_{t-1}}(s) → 1 · exp(λ_t(s-1)) = exp(λ_t(s-1))\n\nThis is the PGF of a Poisson distribution with parameter λ_t.\n\n**Case (ii): As α_t → 1**\n\nWhen α_t → 1:\n- The first term: (1 - α_t + α_t s)^{x_{t-1}} → (0 + s)^{x_{t-1}} = s^{x_{t-1}}\n- λ_t^I = λ_t - α_t λ_{t-1} → λ_t - λ_{t-1}\n- The second term: exp(λ_t^I(s-1)) → exp((λ_t - λ_{t-1})(s-1))\n\nTherefore: G_{X_t|x_{t-1}}(s) → s^{x_{t-1}} · exp((λ_t - λ_{t-1})(s-", "error": "Warning: Could not extract single choice letter from: I need to analyze the limiting behavior of the INA..."}}
{"ID": 226, "Question": "### Background\n\nThis problem examines the core behavioral foundations of a dynamic commodity market model, focusing on the optimal decisions of the two key types of forward-looking private agents: producers and arbitrageurs. Their actions, driven by rational expectations of future prices, jointly determine the intertemporal allocation of the commodity.\n\nIn an annual market model, competitive, expected profit-maximizing arbitrageurs decide how much to store (`x_t`) for the next period, while rational producers decide how much acreage (`a_t`) to plant for the next period's harvest. Both decisions are made at time `t` based on the same rationally-formed expectation of the price at time `t+1`, `p_{t+1}^{e}`.\n\n### Data / Model Specification\n\nThe arbitrageur's decision is governed by the intertemporal arbitrage conditions given in Eq. (1):\n\n```latex\nx_{t}\\geq0, \\quad p_{t}\\geq\\delta p_{t+1}^{e}-\\kappa, \\quad p_{t}>\\delta p_{t+1}^{e}-\\kappa\\Rightarrow x_{t}=0.\n```\n\nThe producer's decision is captured by the acreage supply function in Eq. (2), which has a positive elasticity with respect to the expected price:\n\n```latex\na_{t}=\\alpha{\\left(p_{t+1}^{e}\\right)}\n```\n\nSuppose there is a permanent, unexpected decrease in the unit storage cost, `κ`. Based on the model's general equilibrium logic, what is the most likely impact on the decisions made at time `t`?", "Options": {"A": "Private storage (`x_t`) increases, while acreage planted (`a_t`) remains unchanged.", "B": "Both private storage (`x_t`) and acreage planted (`a_t`) increase.", "C": "Private storage (`x_t`) decreases, while acreage planted (`a_t`) increases.", "D": "Acreage planted (`a_t`) decreases, while private storage (`x_t`) increases."}, "Answer": "D", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the user's ability to trace a parameter shock through a rational expectations general equilibrium model. It uses a Scenario Application strategy. The distractors are designed to diagnose specific reasoning errors: (A) represents a partial equilibrium error, ignoring the feedback effect on expected price; (B) correctly identifies the direction of the storage change but gets the direction of the price expectation change wrong; (C) gets both effects backward.", "qid": "226", "question": "### Background\n\nThis problem examines the core behavioral foundations of a dynamic commodity market model, focusing on the optimal decisions of the two key types of forward-looking private agents: producers and arbitrageurs. Their actions, driven by rational expectations of future prices, jointly determine the intertemporal allocation of the commodity.\n\nIn an annual market model, competitive, expected profit-maximizing arbitrageurs decide how much to store (`x_t`) for the next period, while rational producers decide how much acreage (`a_t`) to plant for the next period's harvest. Both decisions are made at time `t` based on the same rationally-formed expectation of the price at time `t+1`, `p_{t+1}^{e}`.\n\n### Data / Model Specification\n\nThe arbitrageur's decision is governed by the intertemporal arbitrage conditions given in Eq. (1):\n\n```latex\nx_{t}\\geq0, \\quad p_{t}\\geq\\delta p_{t+1}^{e}-\\kappa, \\quad p_{t}>\\delta p_{t+1}^{e}-\\kappa\\Rightarrow x_{t}=0.\n```\n\nThe producer's decision is captured by the acreage supply function in Eq. (2), which has a positive elasticity with respect to the expected price:\n\n```latex\na_{t}=\\alpha{\\left(p_{t+1}^{e}\\right)}\n```\n\nSuppose there is a permanent, unexpected decrease in the unit storage cost, `κ`. Based on the model's general equilibrium logic, what is the most likely impact on the decisions made at time `t`?", "model_evaluation_result": {"model_raw_response": "Looking at this dynamic commodity market model, I need to analyze how a permanent decrease in storage cost κ affects the decisions of arbitrageurs and producers.\n\nFrom the arbitrage conditions in Eq. (1):\n- Storage occurs when: p_t ≥ δp_{t+1}^e - κ\n- No storage when: p_t > δp_{t+1}^e - κ ⟹ x_t = 0\n\nWhen κ decreases (storage becomes cheaper):\n- The threshold δp_{t+1}^e - κ increases\n- This makes the storage condition p_t ≥ δp_{t+1}^e - κ easier to satisfy\n- Therefore, private storage x_t is more likely to increase\n\nFor the producer's decision from Eq. (2):\n- a_t = α(p_{t+1}^e)\n- The acreage decision depends on the expected future price p_{t+1}^e\n\nIn general equilibrium, when storage becomes cheaper:\n- More storage today means less supply available for current consumption\n- This tends to increase current prices p_t\n- With rational expectations, agents anticipate that cheaper storage will lead to more intertemporal arbitrage\n- This typically leads to expectations of higher future prices p_{t+1}^e\n- Higher expected future prices increase current acreage planting a_t\n\nThe general equilibrium effect of cheaper storage is that it:\n1. Increases the incentive to store (direct effect on x_t)\n2. Leads to higher expected future prices (affecting both storage and planting decisions)\n3. Increases acreage planting through higher price expectations\n\nB", "model_answer": "B", "error": null}}
{"ID": 281, "Question": "### Background\n\n**Research Question.** This problem analyzes the economic incentives for intertemporal income shifting created by the 2010 Danish tax reform.\n\n**Setting / Institutional Environment.** A tax reform passed in May 2009 became effective on January 1, 2010. The reform significantly altered the marginal tax rates on labor income, particularly for high-income earners. This created a window of opportunity for individuals, with their employer's cooperation, to legally postpone wage payments from late 2009 to early 2010 to reduce their total tax liability.\n\n**Variables & Parameters.**\n- `τ`: Marginal tax rate (dimensionless).\n- Net-of-tax rate: Defined as `1 - τ` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Marginal Tax Rates on Labor Income in Denmark**\n\n| Labor Income Component | Tax Base | 2009 Rate (%) | 2009 Rate (%) | 2010 Rate (%) | 2010 Rate (%) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| *Income Threshold (1,000 DKK)* | | *< 377* | *> 377* | *< 424* | *> 424* |\n| Labor market contributions (LMC) | LI | 8 | 8 | 8 | 8 |\n| Regional tax (RT) | LI × (1-LMC) | 32.8 | 32.8 | 32.8 | 32.8 |\n| Bottom tax bracket (BT) | LI × (1-LMC) | 5.04 | 5.04 | 3.67 | 3.67 |\n| Middle tax bracket (MT) | LI × (1-LMC) | 0 | 6 | 0 | 0 |\n| Top tax bracket (TT) | LI × (1-LMC) | 0 | 15 | 0 | 15 |\n| Church tax (CT) | LI × (1-LMC) | 0.7 | 0.7 | 0.7 | 0.7 |\n| **Marginal tax rate on labor income** | | **43.5** | **62.8** | **42.2** | **56.0** |\n\n*Note: The marginal tax rate equals `LMC + (1 - LMC) × (RT + BT + MT + TT + CT)`.*\n\n---\n\n### The Question\n\nA high-income individual was due to receive a 100,000 DKK bonus. By shifting the payment from December 2009 to January 2010, what was the monetary tax saving?", "Options": {"A": "6,800 DKK", "B": "37,200 DKK", "C": "44,000 DKK", "D": "1,300 DKK"}, "Answer": "A", "pi_justification": "This item tests the ability to apply the tax schedule to a concrete scenario to quantify the monetary gains from tax avoidance, using a Scenario Application strategy. The user must calculate the tax liability in both years and find the difference. The correct answer is A (100,000 * (0.628 - 0.560)). Distractors are based on plausible errors: reporting the after-tax income in 2009 (B) or 2010 (C) instead of the difference (Step-Omission Error), or applying the tax rates for the incorrect income group (D) (Formula Misuse).", "qid": "281", "question": "### Background\n\n**Research Question.** This problem analyzes the economic incentives for intertemporal income shifting created by the 2010 Danish tax reform.\n\n**Setting / Institutional Environment.** A tax reform passed in May 2009 became effective on January 1, 2010. The reform significantly altered the marginal tax rates on labor income, particularly for high-income earners. This created a window of opportunity for individuals, with their employer's cooperation, to legally postpone wage payments from late 2009 to early 2010 to reduce their total tax liability.\n\n**Variables & Parameters.**\n- `τ`: Marginal tax rate (dimensionless).\n- Net-of-tax rate: Defined as `1 - τ` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Marginal Tax Rates on Labor Income in Denmark**\n\n| Labor Income Component | Tax Base | 2009 Rate (%) | 2009 Rate (%) | 2010 Rate (%) | 2010 Rate (%) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| *Income Threshold (1,000 DKK)* | | *< 377* | *> 377* | *< 424* | *> 424* |\n| Labor market contributions (LMC) | LI | 8 | 8 | 8 | 8 |\n| Regional tax (RT) | LI × (1-LMC) | 32.8 | 32.8 | 32.8 | 32.8 |\n| Bottom tax bracket (BT) | LI × (1-LMC) | 5.04 | 5.04 | 3.67 | 3.67 |\n| Middle tax bracket (MT) | LI × (1-LMC) | 0 | 6 | 0 | 0 |\n| Top tax bracket (TT) | LI × (1-LMC) | 0 | 15 | 0 | 15 |\n| Church tax (CT) | LI × (1-LMC) | 0.7 | 0.7 | 0.7 | 0.7 |\n| **Marginal tax rate on labor income** | | **43.5** | **62.8** | **42.2** | **56.0** |\n\n*Note: The marginal tax rate equals `LMC + (1 - LMC) × (RT + BT + MT + TT + CT)`.*\n\n---\n\n### The Question\n\nA high-income individual was due to receive a 100,000 DKK bonus. By shifting the payment from December 2009 to January 2010, what was the monetary tax saving?", "model_evaluation_result": {"model_raw_response": "I need to calculate the tax saving from shifting a 100,000 DKK bonus payment from December 2009 to January 2010 for a high-income individual.\n\nFrom the table, I can see the marginal tax rates for high-income earners (above the income thresholds):\n- 2009: 62.8% (for income > 377,000 DKK)\n- 2010: 56.0% (for income > 424,000 DKK)\n\nSince this is a high-income individual receiving a 100,000 DKK bonus, they would be in the higher tax bracket in both years.\n\nTax calculation:\n- Tax if paid in December 2009: 100,000 × 0.628 = 62,800 DKK\n- Tax if paid in January 2010: 100,000 × 0.560 = 56,000 DKK\n\nTax saving = 62,800 - 56,000 = 6,800 DKK\n\nA", "model_answer": "A", "error": null}}
{"ID": 280, "Question": "### Background\n\n**Research Question.** This problem analyzes the economic incentives for intertemporal income shifting created by the 2010 Danish tax reform.\n\n**Setting / Institutional Environment.** A tax reform passed in May 2009 became effective on January 1, 2010. The reform significantly altered the marginal tax rates on labor income, particularly for high-income earners. This created a window of opportunity for individuals, with their employer's cooperation, to legally postpone wage payments from late 2009 to early 2010 to reduce their total tax liability.\n\n**Variables & Parameters.**\n- `τ`: Marginal tax rate (dimensionless).\n- Net-of-tax rate: Defined as `1 - τ` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Marginal Tax Rates on Labor Income in Denmark**\n\n| Labor Income Component | Tax Base | 2009 Rate (%) | 2009 Rate (%) | 2010 Rate (%) | 2010 Rate (%) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| *Income Threshold (1,000 DKK)* | | *< 377* | *> 377* | *< 424* | *> 424* |\n| Labor market contributions (LMC) | LI | 8 | 8 | 8 | 8 |\n| Regional tax (RT) | LI × (1-LMC) | 32.8 | 32.8 | 32.8 | 32.8 |\n| Bottom tax bracket (BT) | LI × (1-LMC) | 5.04 | 5.04 | 3.67 | 3.67 |\n| Middle tax bracket (MT) | LI × (1-LMC) | 0 | 6 | 0 | 0 |\n| Top tax bracket (TT) | LI × (1-LMC) | 0 | 15 | 0 | 15 |\n| Church tax (CT) | LI × (1-LMC) | 0.7 | 0.7 | 0.7 | 0.7 |\n| **Marginal tax rate on labor income** | | **43.5** | **62.8** | **42.2** | **56.0** |\n\n*Note: The marginal tax rate equals `LMC + (1 - LMC) × (RT + BT + MT + TT + CT)`.*\n\n---\n\n### The Question\n\nUsing the data in Table 1, what was the percentage change in the net-of-tax rate (`1 - τ`) from 2009 to 2010 for a high-income earner (income > 377,000 DKK)?", "Options": {"A": "18.3%", "B": "-10.8%", "C": "-6.8%", "D": "2.3%"}, "Answer": "A", "pi_justification": "This item assesses the ability to calculate the key economic incentive driving the paper's analysis, using a Computational Judgment strategy. The user must extract the correct marginal tax rates from Table 1 and apply the formula for the percentage change in the net-of-tax rate. The correct answer is A. Distractors are designed based on classic, high-frequency errors: B (calculating the percentage change of the gross tax rate), C (confusing percentage points with percent change), and D (using data for the wrong comparison group).", "qid": "280", "question": "### Background\n\n**Research Question.** This problem analyzes the economic incentives for intertemporal income shifting created by the 2010 Danish tax reform.\n\n**Setting / Institutional Environment.** A tax reform passed in May 2009 became effective on January 1, 2010. The reform significantly altered the marginal tax rates on labor income, particularly for high-income earners. This created a window of opportunity for individuals, with their employer's cooperation, to legally postpone wage payments from late 2009 to early 2010 to reduce their total tax liability.\n\n**Variables & Parameters.**\n- `τ`: Marginal tax rate (dimensionless).\n- Net-of-tax rate: Defined as `1 - τ` (dimensionless).\n\n---\n\n### Data / Model Specification\n\n**Table 1: Marginal Tax Rates on Labor Income in Denmark**\n\n| Labor Income Component | Tax Base | 2009 Rate (%) | 2009 Rate (%) | 2010 Rate (%) | 2010 Rate (%) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| *Income Threshold (1,000 DKK)* | | *< 377* | *> 377* | *< 424* | *> 424* |\n| Labor market contributions (LMC) | LI | 8 | 8 | 8 | 8 |\n| Regional tax (RT) | LI × (1-LMC) | 32.8 | 32.8 | 32.8 | 32.8 |\n| Bottom tax bracket (BT) | LI × (1-LMC) | 5.04 | 5.04 | 3.67 | 3.67 |\n| Middle tax bracket (MT) | LI × (1-LMC) | 0 | 6 | 0 | 0 |\n| Top tax bracket (TT) | LI × (1-LMC) | 0 | 15 | 0 | 15 |\n| Church tax (CT) | LI × (1-LMC) | 0.7 | 0.7 | 0.7 | 0.7 |\n| **Marginal tax rate on labor income** | | **43.5** | **62.8** | **42.2** | **56.0** |\n\n*Note: The marginal tax rate equals `LMC + (1 - LMC) × (RT + BT + MT + TT + CT)`.*\n\n---\n\n### The Question\n\nUsing the data in Table 1, what was the percentage change in the net-of-tax rate (`1 - τ`) from 2009 to 2010 for a high-income earner (income > 377,000 DKK)?", "model_evaluation_result": {"model_raw_response": "I need to calculate the percentage change in the net-of-tax rate for high-income earners from 2009 to 2010.\n\nFrom Table 1, for high-income earners (income > 377,000 DKK in 2009 and > 424,000 DKK in 2010):\n\n**2009:**\n- Marginal tax rate (τ) = 62.8%\n- Net-of-tax rate = 1 - τ = 1 - 0.628 = 0.372\n\n**2010:**\n- Marginal tax rate (τ) = 56.0%\n- Net-of-tax rate = 1 - τ = 1 - 0.560 = 0.440\n\n**Percentage change calculation:**\nPercentage change = [(New value - Old value) / Old value] × 100%\nPercentage change = [(0.440 - 0.372) / 0.372] × 100%\nPercentage change = [0.068 / 0.372] × 100%\nPercentage change = 0.1828 × 100% = 18.28%\n\nRounding to one decimal place: 18.3%\n\nA", "model_answer": "A", "error": null}}
{"ID": 229, "Question": "### Background\n\n**Research Question.** This problem concerns the identification of the causal effects of childcare prices and wages on a single mother's employment and childcare choices, addressing endogeneity and unobserved heterogeneity.\n\n**Setting and Sample.** The analysis uses a discrete choice model where the indirect utility of each choice is a function of potentially endogenous prices and wages.\n\n### Data / Model Specification\n\nThe model consists of a main utility equation and auxiliary equations for price and wages.\n\n**1. Indirect Utility Function:** The linear approximation of the indirect utility for alternative `i` is:\n```latex\nV_{i} = X\\beta_{i} + \\alpha_{P i}P_{s}^{*} + \\alpha_{PT i}W_{PT} + \\alpha_{FT i}W_{FT} + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n```\n**2. Identification of Price:** To address the endogeneity of price, the model uses state of residence indicators (`D_s`) as instruments.\n\n### The Question\n\nThe key identifying assumption is the exclusion restriction, `Cov(D_s, \\varepsilon_i | X) = 0`. Suppose this is violated because states (`D_s`) with a high cost of living (and thus high childcare prices, `P_s`) also have more generous welfare benefits, which directly increases the utility of the 'No-work' alternative, thereby lowering the relative utility of working alternatives. What is the resulting direction of bias on the estimated childcare price coefficient, `\\hat{\\alpha}_{Pi}`?", "Options": {"A": "Biased away from zero (more negative), because the instrument is negatively correlated with the structural error term.", "B": "Biased towards zero, because the instrument is negatively correlated with the structural error term.", "C": "Biased away from zero (more negative), because the instrument is positively correlated with the structural error term.", "D": "There is no bias, as this violation affects the utility of a different alternative ('No-work')."}, "Answer": "A", "pi_justification": "This is a Reverse-Reasoning item that tests a deep understanding of instrumental variable bias. The student must correctly deduce the sign of the correlation between the instrument and the structural error (`Cov(Z,u) < 0`) and combine it with the sign of the first-stage correlation (`Cov(Z,X) > 0`) to determine the direction of bias. Distractors are designed based on a matrix of common errors: B confuses the direction of bias, C misidentifies the sign of the exclusion violation, and D reflects a fundamental misunderstanding of how exclusion violations cause bias in a relative choice model.", "qid": "229", "question": "### Background\n\n**Research Question.** This problem concerns the identification of the causal effects of childcare prices and wages on a single mother's employment and childcare choices, addressing endogeneity and unobserved heterogeneity.\n\n**Setting and Sample.** The analysis uses a discrete choice model where the indirect utility of each choice is a function of potentially endogenous prices and wages.\n\n### Data / Model Specification\n\nThe model consists of a main utility equation and auxiliary equations for price and wages.\n\n**1. Indirect Utility Function:** The linear approximation of the indirect utility for alternative `i` is:\n```latex\nV_{i} = X\\beta_{i} + \\alpha_{P i}P_{s}^{*} + \\alpha_{PT i}W_{PT} + \\alpha_{FT i}W_{FT} + \\varepsilon_{i} \\quad \\text{(Eq. 1)}\n```\n**2. Identification of Price:** To address the endogeneity of price, the model uses state of residence indicators (`D_s`) as instruments.\n\n### The Question\n\nThe key identifying assumption is the exclusion restriction, `Cov(D_s, \\varepsilon_i | X) = 0`. Suppose this is violated because states (`D_s`) with a high cost of living (and thus high childcare prices, `P_s`) also have more generous welfare benefits, which directly increases the utility of the 'No-work' alternative, thereby lowering the relative utility of working alternatives. What is the resulting direction of bias on the estimated childcare price coefficient, `\\hat{\\alpha}_{Pi}`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the direction of bias when the exclusion restriction is violated.\n\nGiven information:\n- The instrument D_s (state indicators) is used to identify the effect of childcare prices P_s\n- Violation: States with high childcare prices also have generous welfare benefits\n- This increases utility of 'No-work' alternative, lowering relative utility of working alternatives\n\nLet me trace through the bias mechanism:\n\n1) High-cost states have both:\n   - High childcare prices P_s\n   - Generous welfare benefits → higher utility for 'No-work'\n\n2) Higher utility for 'No-work' means lower relative utility for working alternatives (where childcare is needed)\n\n3) This creates a negative correlation between D_s and the error term ε_i for working alternatives:\n   - When D_s indicates high-cost state → ε_i is more negative (lower utility for working)\n   - Cov(D_s, ε_i) < 0\n\n4) In IV estimation, the bias direction is determined by:\n   Bias ∝ Cov(instrument, error) / Cov(instrument, endogenous variable)\n\n5) Here:\n   - Cov(D_s, ε_i) < 0 (negative, as established above)\n   - Cov(D_s, P_s) > 0 (positive, since high-cost states have high prices)\n   - Therefore: Bias ∝ (negative)/(positive) = negative\n\n6) Since the true α_Pi is expected to be negative (higher childcare prices reduce utility of working), a negative bias makes the estimate more negative (biased away from zero).\n\nThe answer describes both the correlation direction and the resulting bias correctly.\n\nB", "model_answer": "B", "error": null}}
{"ID": 314, "Question": "### Background\n\n**Research Question:** How can the topological structure of the Nash equilibrium correspondence be exploited to create a computationally feasible algorithm for finding equilibria in finite games?\n\n**Setting:** The core challenge in computing equilibria is the dimensional mismatch between the vast space of games (`Γ`) and the smaller space of mixed strategies (`Σ`). The Kohlberg-Mertens structure theorem provides a homeomorphism `H` that connects the equilibrium graph `E` to the game space `Γ`, enabling a more efficient computational approach by working in a space of dimension `m`, the total number of pure strategies.\n\n**Variables and Parameters.**\n- `(G̃, g)`: A parameterized decomposition of a game `G`.\n- `σ`: A mixed-strategy profile.\n- `v_s(σ)`: The expected payoff for pure strategy `s` given profile `σ`.\n- `z`: An `m`-dimensional vector in the image space of the homeomorphism `H`.\n- `H`: The homeomorphism from the equilibrium graph `E` to the space of games `Γ`.\n- `r(z)`: A retraction map from `ℝ^m` to the strategy space `Σ`.\n- `h(z)`: A map from `ℝ^m` to the `g`-parameter space.\n\n### Data / Model Specification\n\nThe analysis relies on the following key relationships:\n\n1.  A game `G` is decomposed into `(G̃, g)`. The expected payoff for player `n`'s pure strategy `s ∈ S_n` is:\n    ```latex\n    v_s(\\sigma) = g_s + \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n    Let `G̃(σ)_s` denote the summation term.\n\n2.  The Kohlberg-Mertens homeomorphism `H` maps a point on the equilibrium graph `(G̃, g, σ)` to the space of games. For a fixed `G̃`, it is defined as `H(G̃, g, σ) = (G̃, z)`, where:\n    ```latex\n    z_s = \\sigma_s + v_s(\\sigma) \n    ```\n\n3.  The retraction map `r: ℝ^m → Σ` maps a vector `z` to a mixed-strategy profile `σ = r(z)`. The components `σ_s` are defined by `σ_s = (z_s - v_n(z))^+`, where `v_n(z)` is the unique real number for player `n` that solves:\n    ```latex\n    \\sum_{s \\in S_n} (z_s - v_n(z))^+ = 1\n    ```\n    where `(x)⁺ ≡ max{0, x}`.\n\n4.  The inverse map `H⁻¹(G̃, z)` yields the point `(G̃, g, σ)` on the equilibrium graph where `σ = r(z)` and `g_s` is given by:\n    ```latex\n    g_s = z_s - \\sigma_s - \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n\n### Question\n\nThe structure theorem and the resulting formulation of the equilibrium-finding problem in terms of the map `h(z)` represent a significant conceptual breakthrough. What is the primary advantage of this approach for creating a practical algorithm, particularly in addressing the 'curse of dimensionality'?", "Options": {"A": "It guarantees that the equilibrium path is a straight line, which can be followed with a simple linear solver.", "B": "It works directly in the space of mixed strategies `Σ`, avoiding the need for the auxiliary space `ℝ^m`.", "C": "It ensures that every game has a unique equilibrium, simplifying the search to finding that single point.", "D": "It transforms the problem from a search in the high-dimensional space of all games (`Γ`) to a root-finding problem in `ℝ^m`, a space whose dimension (`m`) scales with the number of pure strategies, not the number of payoff entries."}, "Answer": "D", "pi_justification": "This item assesses the high-level understanding of the paper's core conceptual contribution. It uses a Scenario Application strategy. The distractors are designed as 'Conceptual Opposites' that make claims contrary to the method's actual properties, thereby testing for fundamental misunderstandings of the algorithm's structure and purpose.", "qid": "314", "question": "### Background\n\n**Research Question:** How can the topological structure of the Nash equilibrium correspondence be exploited to create a computationally feasible algorithm for finding equilibria in finite games?\n\n**Setting:** The core challenge in computing equilibria is the dimensional mismatch between the vast space of games (`Γ`) and the smaller space of mixed strategies (`Σ`). The Kohlberg-Mertens structure theorem provides a homeomorphism `H` that connects the equilibrium graph `E` to the game space `Γ`, enabling a more efficient computational approach by working in a space of dimension `m`, the total number of pure strategies.\n\n**Variables and Parameters.**\n- `(G̃, g)`: A parameterized decomposition of a game `G`.\n- `σ`: A mixed-strategy profile.\n- `v_s(σ)`: The expected payoff for pure strategy `s` given profile `σ`.\n- `z`: An `m`-dimensional vector in the image space of the homeomorphism `H`.\n- `H`: The homeomorphism from the equilibrium graph `E` to the space of games `Γ`.\n- `r(z)`: A retraction map from `ℝ^m` to the strategy space `Σ`.\n- `h(z)`: A map from `ℝ^m` to the `g`-parameter space.\n\n### Data / Model Specification\n\nThe analysis relies on the following key relationships:\n\n1.  A game `G` is decomposed into `(G̃, g)`. The expected payoff for player `n`'s pure strategy `s ∈ S_n` is:\n    ```latex\n    v_s(\\sigma) = g_s + \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n    Let `G̃(σ)_s` denote the summation term.\n\n2.  The Kohlberg-Mertens homeomorphism `H` maps a point on the equilibrium graph `(G̃, g, σ)` to the space of games. For a fixed `G̃`, it is defined as `H(G̃, g, σ) = (G̃, z)`, where:\n    ```latex\n    z_s = \\sigma_s + v_s(\\sigma) \n    ```\n\n3.  The retraction map `r: ℝ^m → Σ` maps a vector `z` to a mixed-strategy profile `σ = r(z)`. The components `σ_s` are defined by `σ_s = (z_s - v_n(z))^+`, where `v_n(z)` is the unique real number for player `n` that solves:\n    ```latex\n    \\sum_{s \\in S_n} (z_s - v_n(z))^+ = 1\n    ```\n    where `(x)⁺ ≡ max{0, x}`.\n\n4.  The inverse map `H⁻¹(G̃, z)` yields the point `(G̃, g, σ)` on the equilibrium graph where `σ = r(z)` and `g_s` is given by:\n    ```latex\n    g_s = z_s - \\sigma_s - \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n\n### Question\n\nThe structure theorem and the resulting formulation of the equilibrium-finding problem in terms of the map `h(z)` represent a significant conceptual breakthrough. What is the primary advantage of this approach for creating a practical algorithm, particularly in addressing the 'curse of dimensionality'?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 235, "Question": "### Background\n\n**Research Question.** This problem requires the derivation of the log-likelihood function for a restricted system of equations (Model 4), a crucial step for maximum likelihood estimation and hypothesis testing.\n\n**Setting / Institutional Environment.** The setting is a two-equation system where parameters are restricted based on economic theory. The log-likelihood function involves a trace term based on the error covariance matrix `Σ` and the data matrix `Z`.\n\n### Data / Model Specification\n\nThe system's errors can be written as a `T x 2` matrix `V = ZA`, where `Z` is the matrix of all data and `A = [a₁, a₂]` is a matrix of parameters. The error covariance matrix for Model 4 is:\n\n```latex\n\\Sigma = \n\\begin{pmatrix}\n\\sigma^2 & \\gamma\\sigma^2 \\\\\n\\gamma\\sigma^2 & \\sigma^2\n\\end{pmatrix}\n\\quad \\text{(Eq. (1))}\n```\n\nThe log-likelihood function for Model 4 involves the term `trace(Σ⁻¹A'Z'ZA)`. Let `S = Z'Z`.\n\nAfter substituting the correct `Σ⁻¹` for the matrix in Eq. (1) and simplifying, what is this trace term equal to?", "Options": {"A": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} (a_1'Sa_1 - 2\\gamma a_1'Sa_2 + a_2'Sa_2)\n```", "B": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} (a_1'Sa_1 + 2\\gamma a_1'Sa_2 + a_2'Sa_2)\n```", "C": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} (a_1'Sa_1 - \\gamma a_1'Sa_2 + a_2'Sa_2)\n```", "D": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} ((a_1'Sa_1)(a_2'Sa_2) - (a_1'Sa_2)^2)\n```"}, "Answer": "A", "pi_justification": "This item tests the core mathematical derivation needed to construct the Model 4 likelihood function, using a Computational Judgment strategy. The distractors are based on plausible algebraic errors: B (Sign Error on the cross-term), C (Step-Omission Error, forgetting the cross-term is counted twice in the trace), and D (Conceptual Error, confusing the trace operation with a determinant calculation).", "qid": "235", "question": "### Background\n\n**Research Question.** This problem requires the derivation of the log-likelihood function for a restricted system of equations (Model 4), a crucial step for maximum likelihood estimation and hypothesis testing.\n\n**Setting / Institutional Environment.** The setting is a two-equation system where parameters are restricted based on economic theory. The log-likelihood function involves a trace term based on the error covariance matrix `Σ` and the data matrix `Z`.\n\n### Data / Model Specification\n\nThe system's errors can be written as a `T x 2` matrix `V = ZA`, where `Z` is the matrix of all data and `A = [a₁, a₂]` is a matrix of parameters. The error covariance matrix for Model 4 is:\n\n```latex\n\\Sigma = \n\\begin{pmatrix}\n\\sigma^2 & \\gamma\\sigma^2 \\\\\n\\gamma\\sigma^2 & \\sigma^2\n\\end{pmatrix}\n\\quad \\text{(Eq. (1))}\n```\n\nThe log-likelihood function for Model 4 involves the term `trace(Σ⁻¹A'Z'ZA)`. Let `S = Z'Z`.\n\nAfter substituting the correct `Σ⁻¹` for the matrix in Eq. (1) and simplifying, what is this trace term equal to?", "model_evaluation_result": {"model_raw_response": "I need to find the trace of Σ⁻¹A'Z'ZA where S = Z'Z.\n\nFirst, let me find Σ⁻¹ for the given covariance matrix:\n\nΣ = [σ²     γσ²]\n    [γσ²    σ²]\n\nFor a 2×2 matrix [a b; c d], the inverse is 1/(ad-bc) × [d -b; -c a]\n\nHere: a = σ², b = γσ², c = γσ², d = σ²\nDeterminant = σ² × σ² - γσ² × γσ² = σ⁴ - γ²σ⁴ = σ⁴(1-γ²)\n\nSo: Σ⁻¹ = 1/[σ⁴(1-γ²)] × [σ²    -γσ²]\n                           [-γσ²   σ²]\n\nΣ⁻¹ = 1/[σ²(1-γ²)] × [1   -γ]\n                      [-γ   1]\n\nNow I need to compute trace(Σ⁻¹A'SA) where A = [a₁, a₂].\n\nA'SA = [a₁'Sa₁  a₁'Sa₂]\n       [a₂'Sa₁  a₂'Sa₂]\n\nSince S is symmetric, a₁'Sa₂ = a₂'Sa₁.\n\nΣ⁻¹A'SA = 1/[σ²(1-γ²)] × [1   -γ] × [a₁'Sa₁  a₁'Sa₂]\n                          [-γ   1]   [a₂'Sa₁  a₂'Sa₂]\n\n= 1/[σ²(1-γ²)] × [a₁'Sa₁ - γa₂'Sa₁    a₁'Sa₂ - γa₂", "model_answer": "A", "error": null}}
{"ID": 247, "Question": "### Background\n\nThe paper's model of long-run development culminates in a balanced growth path (BGP), a final phase where effort allocations and growth rates are constant. This problem tests the ability to calculate a key BGP effort allocation using the model's calibrated parameter values.\n\n### Data / Model Specification\n\nThe household's optimal choices on the BGP lead to the following key equations:\n- The normalized shadow value of human capital, `x_b`:\n```latex\nx_b = \\frac{1 + \\gamma \\left(1 + \\frac{A_R \\varepsilon}{r}\\right)}{r + 2\\gamma\\delta_b} \\quad \\text{(Eq. 1)}\n```\n- Effort allocations, which depend on the composite term `\\delta_b x_b`:\n```latex\ne_{Mb} = \\frac{1}{\\delta_b x_b} \\quad \\text{(Eq. 2)}\n```\n```latex\ne_{Rb} = \\frac{A_R \\varepsilon}{r \\delta_b x_b} - 1 \\quad \\text{(Eq. 3)}\n```\n- The time constraint, from which learning effort `e_{Lb}` is derived:\n```latex\ne_{Lb} = 1 - e_{Rb} - e_{Mb} \\quad \\text{(Eq. 4)}\n```\n\nThe model is calibrated using the parameters provided in **Table 1** below.\n\n**Table 1. Selected Parameters and BGP Values**\n| Parameter/Value | Symbol | Value |\n|---|---|---|\n| **Set Parameters** | | |\n| Discount rate | `r` | 0.03 |\n| Learning externality | `γ` | 0.35 |\n| **Implied Parameters** | | |\n| Child-raising productivity | `A_R` | 0.121 |\n| Learning productivity (BGP) | `δ_b` | 0.066 |\n| Preference for family size | `ε` | 0.55 |\n\n### Question\n\nUsing the provided equations and parameters, what is the balanced-growth path value for the effort allocated to learning, `e_{Lb}`?", "Options": {"A": "0.253", "B": "0.543", "C": "0.204", "D": "0.017"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 10.0). The original QA problem is purely computational with a single correct numerical answer, making it perfectly suited for conversion (Conceptual Clarity = 10/10). The multi-step calculation provides numerous opportunities for predictable errors, allowing for the creation of high-fidelity distractors (Discriminability = 10/10). This item tests the ability to perform a multi-step calculation using the model's core balanced-growth equations. It uses the Computational Judgment strategy. Distractors are based on common calculation errors, such as reporting an intermediate value (`e_Mb` or `e_Rb`) instead of the final answer, or confusing an effort share with a growth rate.", "qid": "247", "question": "### Background\n\nThe paper's model of long-run development culminates in a balanced growth path (BGP), a final phase where effort allocations and growth rates are constant. This problem tests the ability to calculate a key BGP effort allocation using the model's calibrated parameter values.\n\n### Data / Model Specification\n\nThe household's optimal choices on the BGP lead to the following key equations:\n- The normalized shadow value of human capital, `x_b`:\n```latex\nx_b = \\frac{1 + \\gamma \\left(1 + \\frac{A_R \\varepsilon}{r}\\right)}{r + 2\\gamma\\delta_b} \\quad \\text{(Eq. 1)}\n```\n- Effort allocations, which depend on the composite term `\\delta_b x_b`:\n```latex\ne_{Mb} = \\frac{1}{\\delta_b x_b} \\quad \\text{(Eq. 2)}\n```\n```latex\ne_{Rb} = \\frac{A_R \\varepsilon}{r \\delta_b x_b} - 1 \\quad \\text{(Eq. 3)}\n```\n- The time constraint, from which learning effort `e_{Lb}` is derived:\n```latex\ne_{Lb} = 1 - e_{Rb} - e_{Mb} \\quad \\text{(Eq. 4)}\n```\n\nThe model is calibrated using the parameters provided in **Table 1** below.\n\n**Table 1. Selected Parameters and BGP Values**\n| Parameter/Value | Symbol | Value |\n|---|---|---|\n| **Set Parameters** | | |\n| Discount rate | `r` | 0.03 |\n| Learning externality | `γ` | 0.35 |\n| **Implied Parameters** | | |\n| Child-raising productivity | `A_R` | 0.121 |\n| Learning productivity (BGP) | `δ_b` | 0.066 |\n| Preference for family size | `ε` | 0.55 |\n\n### Question\n\nUsing the provided equations and parameters, what is the balanced-growth path value for the effort allocated to learning, `e_{Lb}`?", "model_evaluation_result": {"model_raw_response": "I need to calculate the balanced growth path value for learning effort e_{Lb} using the given equations and parameters.\n\nGiven parameters:\n- r = 0.03\n- γ = 0.35\n- A_R = 0.121\n- δ_b = 0.066\n- ε = 0.55\n\nStep 1: Calculate x_b using Equation 1\nx_b = [1 + γ(1 + A_R·ε/r)] / (r + 2γδ_b)\n\nFirst, calculate the numerator:\n- A_R·ε/r = 0.121 × 0.55 / 0.03 = 2.218\n- 1 + A_R·ε/r = 1 + 2.218 = 3.218\n- γ(1 + A_R·ε/r) = 0.35 × 3.218 = 1.126\n- Numerator = 1 + 1.126 = 2.126\n\nNext, calculate the denominator:\n- 2γδ_b = 2 × 0.35 × 0.066 = 0.0462\n- Denominator = r + 2γδ_b = 0.03 + 0.0462 = 0.0762\n\nTherefore: x_b = 2.126 / 0.0762 = 27.90\n\nStep 2: Calculate δ_b·x_b\nδ_b·x_b = 0.066 × 27.90 = 1.841\n\nStep 3: Calculate e_{Mb} using Equation 2\ne_{Mb} = 1/(δ_b·x_b) = 1/1.841 = 0.543\n\nStep 4: Calculate e_{Rb} using Equation 3\ne_{Rb} = (A_R·ε)/(r·δ_b·x_b) - 1\n= (0.121 × 0.55)/(0.03 × 1.841) - 1\n= 0.", "model_answer": "I need to calculate the balanced growth path value for learning effort e_{Lb} using the given equations and parameters.\n\nGiven parameters:\n- r = 0.03\n- γ = 0.35\n- A_R = 0.121\n- δ_b = 0.066\n- ε = 0.55\n\nStep 1: Calculate x_b using Equation 1\nx_b = [1 + γ(1 + A_R·ε/r)] / (r + 2γδ_b)\n\nFirst, calculate the numerator:\n- A_R·ε/r = 0.121 × 0.55 / 0.03 = 2.218\n- 1 + A_R·ε/r = 1 + 2.218 = 3.218\n- γ(1 + A_R·ε/r) = 0.35 × 3.218 = 1.126\n- Numerator = 1 + 1.126 = 2.126\n\nNext, calculate the denominator:\n- 2γδ_b = 2 × 0.35 × 0.066 = 0.0462\n- Denominator = r + 2γδ_b = 0.03 + 0.0462 = 0.0762\n\nTherefore: x_b = 2.126 / 0.0762 = 27.90\n\nStep 2: Calculate δ_b·x_b\nδ_b·x_b = 0.066 × 27.90 = 1.841\n\nStep 3: Calculate e_{Mb} using Equation 2\ne_{Mb} = 1/(δ_b·x_b) = 1/1.841 = 0.543\n\nStep 4: Calculate e_{Rb} using Equation 3\ne_{Rb} = (A_R·ε)/(r·δ_b·x_b) - 1\n= (0.121 × 0.55)/(0.03 × 1.841) - 1\n= 0.", "error": "Warning: Could not extract single choice letter from: I need to calculate the balanced growth path value..."}}
{"ID": 236, "Question": "### Background\n\nThis problem examines the static, one-period microfoundations of the model, focusing on how different types of \"transacting capital\" determine production outcomes, sorting across markets, and overall economic efficiency. The core of the model is a symmetric holdup problem where contract enforcement, and thus investment, depends on an individual's skills and their trading partner.\n\n### Data / Model Specification\n\n**1. Agents and Locations:** The economy has two types of agents: those with local skills (L-capital) and those with market skills (M-capital). They can trade in a local \"village\" or an anonymous \"market\".\n\n**2. Production and Payoffs:** Any two matched agents can form a partnership. Each can make a relationship-specific investment at cost `k` to increase output by `y > k`. The effective price of output is 1 in the village and `θ` in the market. We assume `θ > 1`.\n\n**3. Contract Enforcement:**\n- In the market, an agent with M-capital can enforce contracts, preventing holdup. An agent with L-capital cannot.\n- In the village, an agent with L-capital can enforce contracts. An agent with M-capital cannot.\n\n**4. Bargaining and Holdup:** When contracts cannot be enforced, post-investment surplus is split equally (Nash Bargaining). The parameters are such that holdup is a binding constraint: `θy < (4/3)k`, which implies that an agent will not invest if they expect to be held up.\n\n### Question\n\nIn the market, when an agent with M-capital is matched with an agent with L-capital, what is the equilibrium payoff for the M-agent?", "Options": {"A": "```latex\n\\theta y - k\n```", "B": "```latex\n0.5(\\theta y - k)\n```", "C": "```latex\n0.5(\\theta y + k)\n```", "D": "```latex\n0\n```"}, "Answer": "B", "pi_justification": "This item assesses the foundational understanding of the holdup problem and bargaining outcome in the model. It uses a Computational Judgment strategy. The correct answer requires calculating the surplus and the correct split. Distractors are designed based on common errors: confusing total surplus with individual payoff (A), misinterpreting revenue share as net payoff (C), and incorrectly assuming no investment occurs (D).", "qid": "236", "question": "### Background\n\nThis problem examines the static, one-period microfoundations of the model, focusing on how different types of \"transacting capital\" determine production outcomes, sorting across markets, and overall economic efficiency. The core of the model is a symmetric holdup problem where contract enforcement, and thus investment, depends on an individual's skills and their trading partner.\n\n### Data / Model Specification\n\n**1. Agents and Locations:** The economy has two types of agents: those with local skills (L-capital) and those with market skills (M-capital). They can trade in a local \"village\" or an anonymous \"market\".\n\n**2. Production and Payoffs:** Any two matched agents can form a partnership. Each can make a relationship-specific investment at cost `k` to increase output by `y > k`. The effective price of output is 1 in the village and `θ` in the market. We assume `θ > 1`.\n\n**3. Contract Enforcement:**\n- In the market, an agent with M-capital can enforce contracts, preventing holdup. An agent with L-capital cannot.\n- In the village, an agent with L-capital can enforce contracts. An agent with M-capital cannot.\n\n**4. Bargaining and Holdup:** When contracts cannot be enforced, post-investment surplus is split equally (Nash Bargaining). The parameters are such that holdup is a binding constraint: `θy < (4/3)k`, which implies that an agent will not invest if they expect to be held up.\n\n### Question\n\nIn the market, when an agent with M-capital is matched with an agent with L-capital, what is the equilibrium payoff for the M-agent?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze what happens when an M-capital agent is matched with an L-capital agent in the market.\n\nKey facts:\n- In the market, only M-capital agents can enforce contracts\n- L-capital agents cannot enforce contracts in the market\n- When contracts cannot be enforced, post-investment surplus is split equally\n- The holdup constraint θy < (4/3)k means agents won't invest if they expect to be held up\n\nWhen M-capital meets L-capital in the market:\n- The M-agent can enforce contracts (preventing holdup against themselves)\n- The L-agent cannot enforce contracts (vulnerable to holdup)\n\nSince the L-agent faces holdup (cannot enforce contracts), they will not invest because θy < (4/3)k makes investment unprofitable under holdup conditions.\n\nThe M-agent, knowing the L-agent won't invest, must decide whether to invest themselves. Since the M-agent can enforce contracts, they can secure their investment returns. However, with only one agent investing, the total output increase is y (not 2y from both investing).\n\nThe M-agent's payoff from investing would be:\n- Revenue: θy (price × output increase)\n- Cost: k (investment cost)\n- Net payoff: θy - k\n\nSince we're told this is about equilibrium payoffs and the M-agent can enforce contracts to protect their investment, they will invest and receive the full return.\n\nThe answer is A) θy - k\n\nA", "model_answer": "A", "error": null}}
{"ID": 240, "Question": "### Background\n\n**Research Question.** This problem develops the theoretical and empirical framework used to identify the determinants of firm sales growth in transition economies. The core challenge is to distinguish the impact of changes in productive inputs from the influence of historical legacies of the central planning era.\n\n### Data / Model Specification\n\nThe paper estimates a first-differenced model to explain the growth in firm sales:\n\n```latex\n\\Delta\\ln S_{i} = \\alpha\\Delta\\ln L_{i} + \\beta\\Delta\\ln K_{i} + \\text{other factors} + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n```\n\nwhere \\(\\Delta\\) denotes the change over the sample period.\n\n### Question\n\nConsider the first-differenced model in Eq. (1). Suppose there is an unobserved factor, 'learning-by-doing,' which represents a firm's true efficiency growth (\\(\\Delta \\ln A^*_i\\)). This factor positively affects sales growth. Furthermore, assume that firms with faster efficiency growth are also more likely to invest in new capital (i.e., \\(\\Delta \\ln K_i\\) is positively correlated with \\(\\Delta \\ln A^*_i\\)). If 'learning-by-doing' is omitted from the regression, what is the resulting bias on the estimated coefficient for capital growth (\\(\\hat{\\beta}\\))?", "Options": {"A": "It will be overestimated (biased away from zero).", "B": "It will be underestimated (biased toward zero).", "C": "It will be unbiased, because first-differencing removes all sources of bias.", "D": "The direction of the bias cannot be determined without knowing the magnitude of the correlations."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to apply the theory of omitted variable bias in a dynamic context, a high-level skill targeted by Q4 of the source QA. It uses a Reverse-Reasoning strategy. The distractors are based on predictable errors in applying the OVB formula: B results from a sign error in one of the two key relationships; C reflects a misunderstanding of the limits of the first-differencing method; D is a common guess when the formal logic is not understood.", "qid": "240", "question": "### Background\n\n**Research Question.** This problem develops the theoretical and empirical framework used to identify the determinants of firm sales growth in transition economies. The core challenge is to distinguish the impact of changes in productive inputs from the influence of historical legacies of the central planning era.\n\n### Data / Model Specification\n\nThe paper estimates a first-differenced model to explain the growth in firm sales:\n\n```latex\n\\Delta\\ln S_{i} = \\alpha\\Delta\\ln L_{i} + \\beta\\Delta\\ln K_{i} + \\text{other factors} + \\varepsilon_{i} \\quad \\text{(Eq. (1))}\n```\n\nwhere \\(\\Delta\\) denotes the change over the sample period.\n\n### Question\n\nConsider the first-differenced model in Eq. (1). Suppose there is an unobserved factor, 'learning-by-doing,' which represents a firm's true efficiency growth (\\(\\Delta \\ln A^*_i\\)). This factor positively affects sales growth. Furthermore, assume that firms with faster efficiency growth are also more likely to invest in new capital (i.e., \\(\\Delta \\ln K_i\\) is positively correlated with \\(\\Delta \\ln A^*_i\\)). If 'learning-by-doing' is omitted from the regression, what is the resulting bias on the estimated coefficient for capital growth (\\(\\hat{\\beta}\\))?", "model_evaluation_result": {"model_raw_response": "Looking at this omitted variable bias problem, I need to analyze the relationship between the omitted variable (learning-by-doing efficiency growth) and both the dependent variable (sales growth) and the included regressor (capital growth).\n\nGiven information:\n- Omitted variable: Δln A*ᵢ (learning-by-doing efficiency growth)\n- Δln A*ᵢ positively affects sales growth (Δln Sᵢ)\n- Δln A*ᵢ is positively correlated with capital growth (Δln Kᵢ)\n\nThe omitted variable bias formula states that:\nE[β̂] = β + γ × Cov(Δln Kᵢ, Δln A*ᵢ)/Var(Δln Kᵢ)\n\nwhere γ is the coefficient on the omitted variable in the true model.\n\nFrom the problem:\n- γ > 0 (learning-by-doing positively affects sales growth)\n- Cov(Δln Kᵢ, Δln A*ᵢ) > 0 (positive correlation between capital growth and efficiency growth)\n- Var(Δln Kᵢ) > 0 (always positive)\n\nTherefore: γ × Cov(Δln Kᵢ, Δln A*ᵢ)/Var(Δln Kᵢ) > 0\n\nThis means E[β̂] = β + (positive term) > β\n\nThe estimated coefficient β̂ will be biased upward (overestimated, biased away from zero).\n\nThis makes economic sense: when we omit the efficiency growth variable, some of its positive effect on sales growth gets incorrectly attributed to capital growth, making capital appear more productive than it actually is.\n\nA", "model_answer": "A", "error": null}}
{"ID": 255, "Question": "### Background\n\n**Research Question.** This problem explores how to model consumer purchase behavior when the population is characterized by unobserved heterogeneity, leading to overdispersion in aggregate count data.\n\n**Setting / Institutional Environment.** The setting is a cross-sectional analysis of consumer purchase counts. The core modeling idea is that the population consists of a finite number of unobserved segments, each with distinct purchasing behavior.\n\n---\n\n### Data / Model Specification\n\nThe finite mixture Poisson regression model assumes that the probability of observing a purchase count `x_{nt}` is a weighted average over `S` segments, where `s` is the segment identity. The segment-specific rate is `λ_{nt|s}`.\n\n---\n\nIn a finite mixture Poisson model, the unconditional variance of the purchase count `X_{nt}` can be related to its unconditional mean `E[X_{nt}]`. Using the law of total variance (`Var(X) = E[Var(X|s)] + Var(E[X|s])`), which equation correctly describes this relationship, where `Var(λ_{nt|s})` is the variance of the segment-specific rates across segments?", "Options": {"A": "```latex\n\\text{Var}(X_{nt}) = \\text{E}[X_{nt}] + \\text{Var}(\\lambda_{nt|s})\n```", "B": "```latex\n\\text{Var}(X_{nt}) = \\text{E}[X_{nt}]\n```", "C": "```latex\n\\text{Var}(X_{nt}) = \\text{Var}(\\lambda_{nt|s})\n```", "D": "```latex\n\\text{Var}(X_{nt}) = \\text{E}[X_{nt}] - \\text{Var}(\\lambda_{nt|s})\n```"}, "Answer": "A", "pi_justification": "Assesses understanding of how finite mixture models generate overdispersion, a core concept. This is a Computational Judgment item testing the application of the law of total variance. Distractors represent classic errors: stating the null hypothesis of the standard Poisson model (B, Formula Misuse), omitting a term from the law of total variance (C, Step-Omission Error), or making a sign error (D, Sign Error).", "qid": "255", "question": "### Background\n\n**Research Question.** This problem explores how to model consumer purchase behavior when the population is characterized by unobserved heterogeneity, leading to overdispersion in aggregate count data.\n\n**Setting / Institutional Environment.** The setting is a cross-sectional analysis of consumer purchase counts. The core modeling idea is that the population consists of a finite number of unobserved segments, each with distinct purchasing behavior.\n\n---\n\n### Data / Model Specification\n\nThe finite mixture Poisson regression model assumes that the probability of observing a purchase count `x_{nt}` is a weighted average over `S` segments, where `s` is the segment identity. The segment-specific rate is `λ_{nt|s}`.\n\n---\n\nIn a finite mixture Poisson model, the unconditional variance of the purchase count `X_{nt}` can be related to its unconditional mean `E[X_{nt}]`. Using the law of total variance (`Var(X) = E[Var(X|s)] + Var(E[X|s])`), which equation correctly describes this relationship, where `Var(λ_{nt|s})` is the variance of the segment-specific rates across segments?", "model_evaluation_result": {"model_raw_response": "I need to derive the unconditional variance of X_{nt} in a finite mixture Poisson model using the law of total variance.\n\nGiven information:\n- Finite mixture Poisson model with S segments\n- X_{nt} is the purchase count\n- λ_{nt|s} is the segment-specific rate\n- Law of total variance: Var(X) = E[Var(X|s)] + Var(E[X|s])\n\nLet me work through this step by step:\n\n1) For a Poisson distribution, we know that:\n   - E[X_{nt}|s] = λ_{nt|s}\n   - Var(X_{nt}|s) = λ_{nt|s}\n\n2) Applying the law of total variance:\n   Var(X_{nt}) = E[Var(X_{nt}|s)] + Var(E[X_{nt}|s])\n\n3) Substituting the Poisson properties:\n   Var(X_{nt}) = E[λ_{nt|s}] + Var(λ_{nt|s})\n\n4) Since E[λ_{nt|s}] is the unconditional mean of the segment-specific rates, and this equals E[X_{nt}] (the unconditional mean of X_{nt}):\n   Var(X_{nt}) = E[X_{nt}] + Var(λ_{nt|s})\n\nThis shows that in a finite mixture Poisson model, the unconditional variance equals the unconditional mean plus the variance of the segment-specific rates across segments. This captures the overdispersion that arises from unobserved heterogeneity - the variance exceeds the mean (unlike a simple Poisson where variance equals mean).\n\nA", "model_answer": "A", "error": null}}
{"ID": 270, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical relationship between structural dynamic models and their reduced-form time-series representations, specifically how the structure of the underlying economic model imposes testable restrictions on the parameters of the final ARMA form.\n\n**Setting.** We consider a system of two equations: a structural model linking an endogenous variable `y_t` to an exogenous variable `x_t` and a structural error, and a time-series model describing the data generating process for `x_t`.\n\n**Variables & Parameters.**\n- `y_t`: An endogenous variable at time `t`.\n- `x_t`: An exogenous explanatory variable at time `t`.\n- `η_t`, `ε_t`: White-noise stochastic disturbances at time `t`.\n- `B`: The backshift (lag) operator, where `B^k z_t = z_{t-k}`.\n- `α(B)`, `β(B)`, `θ(B)`, `ω(B)`, `μ(B)`: Finite-order polynomials in the lag operator `B`. For example, `α(B) = 1 - α_1 B - α_2 B^2 - ...`.\n\n---\n\n### Data / Model Specification\n\nThe system is described by the following equations:\n\n```latex\n\\alpha(B)y_{t}=\\beta(B)x_{t}+\\theta(B)\\eta_{t} \\quad \\text{(Eq. (1))}\n```\n\n```latex\n\\omega(B)x_{t}=\\mu(B)\\varepsilon_{t} \\quad \\text{(Eq. (2))}\n```\n\n---\n\n### The Question\n\nGiven the structural model in Eq. (1) and the ARMA process for the explanatory variable `x_t` in Eq. (2), which of the following expressions represents the correct final form equation for `y_t`?", "Options": {"A": "```latex\n\\omega(B)\\alpha(B)y_t = \\beta(B)\\mu(B)\\varepsilon_t + \\theta(B)\\eta_t\n```", "B": "```latex\n\\alpha(B)y_t = \\beta(B)\\mu(B)\\varepsilon_t + \\omega(B)\\theta(B)\\eta_t\n```", "C": "```latex\n\\omega(B)\\alpha(B)y_t = \\beta(B)\\mu(B)\\varepsilon_t + \\omega(B)\\theta(B)\\eta_t\n```", "D": "```latex\n\\alpha(B)y_t = \\beta(B)\\mu(B)\\varepsilon_t + \\theta(B)\\eta_t\n```"}, "Answer": "C", "pi_justification": "REPLACE with Choice Questions (Score: 10.0). This item assesses the ability to perform the core algebraic derivation that links a structural model to its time-series final form, which was the focus of Q1 in the source QA. It uses a Computational Judgment strategy. The distractors are designed based on highly plausible step-omission errors. Distractor A is the most common error: failing to multiply the structural disturbance term by the exogenous variable's AR polynomial. Distractors B and D result from failing to multiply the left-hand side of the equation.", "qid": "270", "question": "### Background\n\n**Research Question.** This problem explores the theoretical relationship between structural dynamic models and their reduced-form time-series representations, specifically how the structure of the underlying economic model imposes testable restrictions on the parameters of the final ARMA form.\n\n**Setting.** We consider a system of two equations: a structural model linking an endogenous variable `y_t` to an exogenous variable `x_t` and a structural error, and a time-series model describing the data generating process for `x_t`.\n\n**Variables & Parameters.**\n- `y_t`: An endogenous variable at time `t`.\n- `x_t`: An exogenous explanatory variable at time `t`.\n- `η_t`, `ε_t`: White-noise stochastic disturbances at time `t`.\n- `B`: The backshift (lag) operator, where `B^k z_t = z_{t-k}`.\n- `α(B)`, `β(B)`, `θ(B)`, `ω(B)`, `μ(B)`: Finite-order polynomials in the lag operator `B`. For example, `α(B) = 1 - α_1 B - α_2 B^2 - ...`.\n\n---\n\n### Data / Model Specification\n\nThe system is described by the following equations:\n\n```latex\n\\alpha(B)y_{t}=\\beta(B)x_{t}+\\theta(B)\\eta_{t} \\quad \\text{(Eq. (1))}\n```\n\n```latex\n\\omega(B)x_{t}=\\mu(B)\\varepsilon_{t} \\quad \\text{(Eq. (2))}\n```\n\n---\n\n### The Question\n\nGiven the structural model in Eq. (1) and the ARMA process for the explanatory variable `x_t` in Eq. (2), which of the following expressions represents the correct final form equation for `y_t`?", "model_evaluation_result": {"model_raw_response": "I need to derive the final form equation for y_t by substituting the ARMA process for x_t from Eq. (2) into the structural equation Eq. (1).\n\nGiven:\n- Eq. (1): α(B)y_t = β(B)x_t + θ(B)η_t\n- Eq. (2): ω(B)x_t = μ(B)ε_t\n\nFrom Eq. (2), I can solve for x_t:\nx_t = [ω(B)]^(-1)μ(B)ε_t\n\nHowever, to avoid dealing with the inverse of ω(B), I'll multiply Eq. (1) by ω(B):\n\nω(B)α(B)y_t = ω(B)β(B)x_t + ω(B)θ(B)η_t\n\nNow I substitute the expression for ω(B)x_t from Eq. (2):\nω(B)x_t = μ(B)ε_t\n\nTherefore:\nω(B)α(B)y_t = β(B)[μ(B)ε_t] + ω(B)θ(B)η_t\n\nSimplifying:\nω(B)α(B)y_t = β(B)μ(B)ε_t + ω(B)θ(B)η_t\n\nThis matches option C exactly.\n\nC", "model_answer": "B", "error": null}}
{"ID": 251, "Question": "### Background\n\n**Research Question.** This problem investigates the causal factors behind lower cooperation rates in repeated games with stochastic horizons. The Short-Stochastic (SS) treatment differs from the baseline Short-Deterministic (SD) treatment in two ways: the length of supergames is **variable** (changes from game to game), and the exact endpoint of any given game is **unpredictable**. This analysis seeks to disentangle the separate causal impacts of these two factors.\n\n**Setting / Institutional Environment.** To isolate the effects, a special 'Variable-Deterministic' (VD) treatment was designed. In this treatment, supergame durations were variable and matched the exact sequence of realized durations from the SS treatment, but the duration of each supergame was announced to subjects at the start, thus removing unpredictability. The analysis uses a linear regression model to compare mean cooperation rates across these three treatments, controlling for individual characteristics.\n\n### Data / Model Specification\n\nThe design features of the three relevant treatments are summarized below:\n\n*   **Short-Deterministic (SD):** No variability, No unpredictability.\n*   **Variable-Deterministic (VD):** Variability, No unpredictability.\n*   **Short-Stochastic (SS):** Variability, Unpredictability.\n\nThe results of a linear regression estimating the treatment effects on the mean cooperation rate are presented in Table 1.\n\n**Table 1: Linear Regression on Mean Cooperation Rate**\n\n| Variable                 | Coefficient | (Std. Err.) |\n| :----------------------- | :---------- | :---------- |\n| Short-Deterministic      | 11.574***   | (2.359)     |\n| Variable-Deterministic   | 6.966**     | (3.094)     |\n| Constant                 | 39.488***   | (13.287)    |\n| Controls                 | Yes         |             |\n\n*Notes: The dependent variable is the mean cooperation rate in percentage points. The omitted baseline treatment is Short-Stochastic (SS). *, **, *** denote significance at 10%, 5%, and 1% levels.*\n\n### Question\n\nBased on the regression results, which factor—duration variability or duration unpredictability—has a larger negative causal impact on the mean cooperation rate, and what is the estimated magnitude of that larger impact?", "Options": {"A": "Unpredictability; a negative impact of approximately 7.0 percentage points.", "B": "Variability; a negative impact of approximately 4.6 percentage points.", "C": "Unpredictability; a negative impact of approximately 11.6 percentage points.", "D": "Variability; a negative impact of approximately 11.6 percentage points."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). The original QA problem's core task is a multi-step calculation and comparison based on a clear identification strategy, making it highly suitable for conversion. Conceptual Clarity = 9/10, Discriminability = 10/10. This item uses the 'Computational Judgment' strategy. It assesses the ability to execute the paper's identification strategy: isolating the effect of unpredictability by comparing the Short-Stochastic (SS) and Variable-Deterministic (VD) treatments. The correct answer requires calculating Coop(SS) - Coop(VD) = 39.488 - (39.488 + 6.966) = -6.966. Distractor B is the correctly calculated effect of the *other* factor (variability), an 'Almost Right' error. Distractors C and D use an incorrect comparison (SS vs. SD), a 'Step-Omission' error that fails to properly isolate the causal effect.", "qid": "251", "question": "### Background\n\n**Research Question.** This problem investigates the causal factors behind lower cooperation rates in repeated games with stochastic horizons. The Short-Stochastic (SS) treatment differs from the baseline Short-Deterministic (SD) treatment in two ways: the length of supergames is **variable** (changes from game to game), and the exact endpoint of any given game is **unpredictable**. This analysis seeks to disentangle the separate causal impacts of these two factors.\n\n**Setting / Institutional Environment.** To isolate the effects, a special 'Variable-Deterministic' (VD) treatment was designed. In this treatment, supergame durations were variable and matched the exact sequence of realized durations from the SS treatment, but the duration of each supergame was announced to subjects at the start, thus removing unpredictability. The analysis uses a linear regression model to compare mean cooperation rates across these three treatments, controlling for individual characteristics.\n\n### Data / Model Specification\n\nThe design features of the three relevant treatments are summarized below:\n\n*   **Short-Deterministic (SD):** No variability, No unpredictability.\n*   **Variable-Deterministic (VD):** Variability, No unpredictability.\n*   **Short-Stochastic (SS):** Variability, Unpredictability.\n\nThe results of a linear regression estimating the treatment effects on the mean cooperation rate are presented in Table 1.\n\n**Table 1: Linear Regression on Mean Cooperation Rate**\n\n| Variable                 | Coefficient | (Std. Err.) |\n| :----------------------- | :---------- | :---------- |\n| Short-Deterministic      | 11.574***   | (2.359)     |\n| Variable-Deterministic   | 6.966**     | (3.094)     |\n| Constant                 | 39.488***   | (13.287)    |\n| Controls                 | Yes         |             |\n\n*Notes: The dependent variable is the mean cooperation rate in percentage points. The omitted baseline treatment is Short-Stochastic (SS). *, **, *** denote significance at 10%, 5%, and 1% levels.*\n\n### Question\n\nBased on the regression results, which factor—duration variability or duration unpredictability—has a larger negative causal impact on the mean cooperation rate, and what is the estimated magnitude of that larger impact?", "model_evaluation_result": {"model_raw_response": "Looking at this step-by-step:\n\nFirst, I need to understand what each treatment represents:\n- **SD (Short-Deterministic):** No variability, No unpredictability\n- **VD (Variable-Deterministic):** Variability, No unpredictability  \n- **SS (Short-Stochastic):** Variability, Unpredictability (baseline/omitted)\n\nThe regression coefficients show the difference in cooperation rates compared to the SS baseline:\n- SD coefficient: 11.574 (SD has 11.574 percentage points higher cooperation than SS)\n- VD coefficient: 6.966 (VD has 6.966 percentage points higher cooperation than SS)\n\nTo isolate the causal effects:\n\n**Effect of Variability:** Compare SD (no variability) vs VD (has variability), holding unpredictability constant (both have no unpredictability):\n- Effect = 11.574 - 6.966 = 4.608 percentage points\n- Since SD > VD, variability has a negative impact of ~4.6 percentage points\n\n**Effect of Unpredictability:** Compare VD (no unpredictability) vs SS (has unpredictability), holding variability constant (both have variability):\n- Effect = 6.966 - 0 = 6.966 percentage points  \n- Since VD > SS, unpredictability has a negative impact of ~7.0 percentage points\n\nComparing the magnitudes: 7.0 > 4.6, so unpredictability has the larger negative impact.\n\nB", "model_answer": "B", "error": null}}
{"ID": 329, "Question": "### Background\n\n**Research Question.** This problem examines a key condition for the validity of standard asymptotic tests.\n\n**Setting.** For a fixed threshold `r`, the standard Lagrange Multiplier (LM) statistic `T_n(r)` for a threshold effect converges to a `χ^2` distribution under the assumption of i.i.d. errors.\n\n### Data / Model Specification\n\nThe Threshold Autoregressive TAR(p) model is:\n```latex\nX_{t}=\\phi_{0}+\\sum_{i=1}^{p}\\phi_{i}X_{t-i}+\\left(\\psi_{0}+\\sum_{i=1}^{p}\\psi_{i}X_{t-i}\\right)I(X_{t-d}\\leq r)+\\varepsilon_{t} \\quad \\text{(Eq. 1)}\n```\nWhat is the primary reason the `χ^2` result for the test statistic `T_n(r)` fails if the errors `ε_t` follow a GARCH process (i.e., are conditionally heteroskedastic)?", "Options": {"A": "Conditional heteroskedasticity violates the information matrix equality, so the score's variance is no longer the inverse of the Hessian.", "B": "The GARCH errors are not normally distributed, which is a requirement for the χ² test.", "C": "The parameter estimates under the null hypothesis become biased and inconsistent under GARCH.", "D": "The GARCH process makes the observed series Xₜ non-stationary, which violates a key assumption of the test."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses a crucial concept in robust inference: why standard tests fail under heteroskedasticity. The distractors are designed to be plausible but incorrect explanations common among students: (B) confuses heteroskedasticity with non-normality, (C) confuses inconsistency with the effects of GARCH on OLS, and (D) confuses conditional variance dynamics with the stationarity of the process level.", "qid": "329", "question": "### Background\n\n**Research Question.** This problem examines a key condition for the validity of standard asymptotic tests.\n\n**Setting.** For a fixed threshold `r`, the standard Lagrange Multiplier (LM) statistic `T_n(r)` for a threshold effect converges to a `χ^2` distribution under the assumption of i.i.d. errors.\n\n### Data / Model Specification\n\nThe Threshold Autoregressive TAR(p) model is:\n```latex\nX_{t}=\\phi_{0}+\\sum_{i=1}^{p}\\phi_{i}X_{t-i}+\\left(\\psi_{0}+\\sum_{i=1}^{p}\\psi_{i}X_{t-i}\\right)I(X_{t-d}\\leq r)+\\varepsilon_{t} \\quad \\text{(Eq. 1)}\n```\nWhat is the primary reason the `χ^2` result for the test statistic `T_n(r)` fails if the errors `ε_t` follow a GARCH process (i.e., are conditionally heteroskedastic)?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 321, "Question": "### Background\n\nThe author's proposed empirical strategy is a two-step procedure: (1) Estimate market-specific returns `β̂ⱼ` for each market `j` by running the regression `yᵢⱼ = Xᵢβⱼ + εᵢ`. (2) Use these estimates as the dependent variable in a second-stage regression to explain the returns: `β̂ⱼ = LⱼΛ + ωⱼ`. This is known as a 'generated regressor' problem.\n\n### Question\n\nWhat is the primary econometric problem this two-step procedure creates for statistical inference on the coefficients `Λ` in the second stage, and what is the appropriate correction?", "Options": {"A": "Endogeneity, because `Lⱼ` is correlated with the second-stage error term. The appropriate correction is to use an instrumental variable (IV) for `Lⱼ`.", "B": "Heteroskedasticity, because the first-stage estimation error in `β̂ⱼ` has a variance that differs across markets `j`. The appropriate correction is to use Weighted Least Squares (WLS).", "C": "Sample selection bias, because markets `j` are not randomly selected. The appropriate correction is a Heckman selection model.", "D": "Measurement error in the dependent variable `β̂ⱼ`. This leads to larger standard errors but does not bias the estimates of `Λ`, so no correction is needed."}, "Answer": "B", "pi_justification": "This item assesses knowledge of a specific, advanced econometric issue: the consequences of using a generated regressor as a dependent variable. It uses a Computational Judgment strategy, asking for the correct diagnosis and solution. Distractor A confuses the issue with endogeneity (Conceptual: Classic Confusion). Distractor C points to a different, irrelevant problem (True but Irrelevant). Distractor D correctly identifies the source as measurement error but incorrectly states the consequences and the need for correction, making it a subtle but critical error (Conceptual: Almost Right).", "qid": "321", "question": "### Background\n\nThe author's proposed empirical strategy is a two-step procedure: (1) Estimate market-specific returns `β̂ⱼ` for each market `j` by running the regression `yᵢⱼ = Xᵢβⱼ + εᵢ`. (2) Use these estimates as the dependent variable in a second-stage regression to explain the returns: `β̂ⱼ = LⱼΛ + ωⱼ`. This is known as a 'generated regressor' problem.\n\n### Question\n\nWhat is the primary econometric problem this two-step procedure creates for statistical inference on the coefficients `Λ` in the second stage, and what is the appropriate correction?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 267, "Question": "### Background\n\nAn agent's preferences are defined over a set of decision problems, `Z`. Each decision problem `z ∈ Z` is a compact set of alternatives. An alternative is a triplet `(c, d, x)`, specifying current non-drug consumption `c`, current drug consumption `d`, and the continuation decision problem `x`. The agent's preferences are indexed by a state `s ∈ [0,1]`, representing their drug consumption in the last period.\n\n### Data / Model Specification\n\nThe agent's welfare from facing decision problem `z` in state `s` is given by the recursive utility function:\n\n```latex\nW(s,z) = \\operatorname*{max}_{(c,d,x) \\in z} [u(c,d) + \\sigma(s)\\upsilon(d) + \\delta W(d,x)] - \\operatorname*{max}_{(\\hat{c},\\hat{d},\\hat{x}) \\in z} \\sigma(s)\\upsilon(\\hat{d}) \\quad \\text{(Eq. 1)}\n```\nwhere `u(c,d) + δW(d,x)` is the *commitment utility* of an alternative, and `υ(d)` is the *temptation utility*, which is strictly increasing in `d`.\n\n### Question\n\nSuppose the agent chooses alternative `(c*, d*, x*)` from the set `z`, while the most tempting alternative available in that set is `(ĉ, d̂, x̂)`. How is the \"cost of self-control\" formally expressed in this model?", "Options": {"A": "`σ(s) * [υ(d̂) - υ(d*)]`", "B": "`σ(s) * υ(d*)`", "C": "`u(ĉ, d̂) - u(c*, d*)`", "D": "`σ(s) * υ(d̂)`"}, "Answer": "A", "pi_justification": "This item assesses the ability to decompose the welfare function and correctly identify the self-control cost term. It is a Computational Judgment task based on algebraic interpretation. The distractors are designed from plausible errors: B (Step-Omission Error) is the temptation of the chosen good, not the cost of resisting a better one; D (Step-Omission Error) is the temptation of the most tempting good, not the net cost; and C (Formula Misuse) confuses the psychological cost with the difference in commitment utility.", "qid": "267", "question": "### Background\n\nAn agent's preferences are defined over a set of decision problems, `Z`. Each decision problem `z ∈ Z` is a compact set of alternatives. An alternative is a triplet `(c, d, x)`, specifying current non-drug consumption `c`, current drug consumption `d`, and the continuation decision problem `x`. The agent's preferences are indexed by a state `s ∈ [0,1]`, representing their drug consumption in the last period.\n\n### Data / Model Specification\n\nThe agent's welfare from facing decision problem `z` in state `s` is given by the recursive utility function:\n\n```latex\nW(s,z) = \\operatorname*{max}_{(c,d,x) \\in z} [u(c,d) + \\sigma(s)\\upsilon(d) + \\delta W(d,x)] - \\operatorname*{max}_{(\\hat{c},\\hat{d},\\hat{x}) \\in z} \\sigma(s)\\upsilon(\\hat{d}) \\quad \\text{(Eq. 1)}\n```\nwhere `u(c,d) + δW(d,x)` is the *commitment utility* of an alternative, and `υ(d)` is the *temptation utility*, which is strictly increasing in `d`.\n\n### Question\n\nSuppose the agent chooses alternative `(c*, d*, x*)` from the set `z`, while the most tempting alternative available in that set is `(ĉ, d̂, x̂)`. How is the \"cost of self-control\" formally expressed in this model?", "model_evaluation_result": {"model_raw_response": "Looking at the recursive utility function in Equation 1:\n\n```\nW(s,z) = max_{(c,d,x) ∈ z} [u(c,d) + σ(s)υ(d) + δW(d,x)] - max_{(ĉ,d̂,x̂) ∈ z} σ(s)υ(d̂)\n```\n\nThe welfare function has two components:\n1. The first max term represents the utility from the chosen alternative (c*, d*, x*), which includes both commitment utility and temptation utility\n2. The second max term represents the maximum temptation utility available in the choice set\n\nThe \"cost of self-control\" in this model represents the psychological cost of resisting temptation when the agent doesn't choose the most tempting alternative.\n\nWhen the agent chooses (c*, d*, x*) but the most tempting alternative is (ĉ, d̂, x̂), the cost of self-control is the difference between:\n- The temptation utility from the most tempting alternative: σ(s)υ(d̂)\n- The temptation utility from the chosen alternative: σ(s)υ(d*)\n\nThis gives us: σ(s) * [υ(d̂) - υ(d*)]\n\nThis represents the utility cost of not succumbing to the maximum temptation available, weighted by the state-dependent temptation parameter σ(s).\n\nA", "model_answer": "A", "error": null}}
{"ID": 243, "Question": "### Background\n\n**Research Question:** This problem investigates the practical performance of the Generalized Stochastic Simulation Algorithm (GSSA), focusing on its tractability in high-dimensional settings.\n\n**Setting / Institutional Environment:** The analysis uses a high-dimensional multi-country model to test for the \"curse of dimensionality.\"\n\n**Variables & Parameters:**\n- `MC(1)`: One-node Monte Carlo integration.\n- `Q(1)`: One-node Gauss-Hermite quadrature.\n- `M1`: A monomial integration rule suitable for high dimensions.\n- `N`: The number of countries.\n- `E_mean`: Mean absolute unit-free Euler error (in log10 units).\n\n---\n\n### Data / Model Specification\n\nThe following table shows GSSA performance in a multi-country model.\n\n**Table 1: GSSA Performance in the Multi-Country Model**\n\n| N | Poly. Deg. | Method | `E_mean` (log10) | CPU (sec) |\n|:---:|:---:|:---|:---:|:---:|\n| 20 | 1st | RLS-Tikh., Q(1) | -4.75 | 56 |\n| 20 | 2nd | RLS-Tikh., Q(1) | -5.40 | 1079 |\n| 100| 1st | RLS-Tikh., Q(1) | -4.64 | 225 |\n| 100| 1st | RLS-TSVD, M1 | -4.63 | 13,846 |\n\n---\n\n### Question\n\nA research team wants to solve a 50-country (`N=50`) version of this model. Their goal is to achieve a mean absolute error of at least `10^{-5.0}` (i.e., a log10 error of -5.0 or lower) as efficiently as possible. Based on the patterns in Table 1, which of the following is the most appropriate and efficient GSSA strategy?\n", "Options": {"A": "Use a 2nd-degree polynomial with the Q(1) integration rule.", "B": "Use a 1st-degree polynomial with the Q(1) integration rule.", "C": "Use a 2nd-degree polynomial with the M1 integration rule.", "D": "Use a 1st-degree polynomial with the M1 integration rule."}, "Answer": "A", "pi_justification": "This item assesses the strategic recommendation from Part 3 of the original QA. It uses a Scenario Application strategy, asking the student to extrapolate from the provided data to a new case. The distractors represent plausible but suboptimal choices. Distractor B fails to meet the accuracy target, as 1st-degree polynomials consistently yield errors around -4.7. Distractor C meets the accuracy target but is highly inefficient, as the M1 rule is shown to be far slower than Q(1) for similar accuracy. Distractor D fails on both accuracy and efficiency.", "qid": "243", "question": "### Background\n\n**Research Question:** This problem investigates the practical performance of the Generalized Stochastic Simulation Algorithm (GSSA), focusing on its tractability in high-dimensional settings.\n\n**Setting / Institutional Environment:** The analysis uses a high-dimensional multi-country model to test for the \"curse of dimensionality.\"\n\n**Variables & Parameters:**\n- `MC(1)`: One-node Monte Carlo integration.\n- `Q(1)`: One-node Gauss-Hermite quadrature.\n- `M1`: A monomial integration rule suitable for high dimensions.\n- `N`: The number of countries.\n- `E_mean`: Mean absolute unit-free Euler error (in log10 units).\n\n---\n\n### Data / Model Specification\n\nThe following table shows GSSA performance in a multi-country model.\n\n**Table 1: GSSA Performance in the Multi-Country Model**\n\n| N | Poly. Deg. | Method | `E_mean` (log10) | CPU (sec) |\n|:---:|:---:|:---|:---:|:---:|\n| 20 | 1st | RLS-Tikh., Q(1) | -4.75 | 56 |\n| 20 | 2nd | RLS-Tikh., Q(1) | -5.40 | 1079 |\n| 100| 1st | RLS-Tikh., Q(1) | -4.64 | 225 |\n| 100| 1st | RLS-TSVD, M1 | -4.63 | 13,846 |\n\n---\n\n### Question\n\nA research team wants to solve a 50-country (`N=50`) version of this model. Their goal is to achieve a mean absolute error of at least `10^{-5.0}` (i.e., a log10 error of -5.0 or lower) as efficiently as possible. Based on the patterns in Table 1, which of the following is the most appropriate and efficient GSSA strategy?\n", "model_evaluation_result": {"model_raw_response": "Looking at the data in Table 1, I need to analyze the patterns to determine the best strategy for N=50 countries with a target accuracy of -5.0 or better.\n\nKey observations from the table:\n\n1. **Accuracy patterns:**\n   - 1st degree polynomial achieves around -4.6 to -4.75 accuracy\n   - 2nd degree polynomial achieves -5.40 accuracy (exceeds the -5.0 target)\n\n2. **Computational efficiency:**\n   - Q(1) integration is much faster than M1\n   - For N=20: Q(1) takes 56-1079 seconds vs M1 taking 13,846 seconds\n   - The M1 method is dramatically slower (over 10x slower even for N=100)\n\n3. **Scalability:**\n   - CPU time increases with N (from 56 sec at N=20 to 225 sec at N=100 for 1st degree Q(1))\n   - The curse of dimensionality affects computational cost\n\nFor N=50 with a target of -5.0:\n- 1st degree polynomials only achieve around -4.6 to -4.75, which doesn't meet the -5.0 target\n- 2nd degree polynomial with Q(1) achieved -5.40 at N=20, well exceeding the target\n- M1 integration is computationally prohibitive given the massive time difference\n\nThe most efficient strategy to achieve the accuracy target would be using a 2nd-degree polynomial with Q(1) integration, as it meets the accuracy requirement while being computationally feasible.\n\nA", "model_answer": "A", "error": null}}
{"ID": 261, "Question": "### Background\n\n**Research Question.** This problem analyzes the main empirical findings of India's Operation Blackboard (OB) program, focusing on the heterogeneous impacts on primary school completion by gender.\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences (DiD) approach to estimate the causal effect of the OB teacher provision program.\n\n### Data / Model Specification\n\nThe effect on primary school completion is estimated using an individual-level DiD model:\n\n```latex\ny_{ijk} = \\alpha + \\beta (\\mathrm{Intensity}_{j} \\times \\mathrm{Post-OB}_{k}) + \\text{Controls} + \\varepsilon_{ijk}\n```\n\n**Table 1: Effect on Primary School Completion for Boys vs. Girls**\n\n| | Girls | Boys |\n| :--- | :---: | :---: |\n| **Specification** | **Coeff. (Std. Err.)** | **Coeff. (Std. Err.)** |\n| (2) HH Controls | 0.0165* (0.0026) | 0.0144* (0.0026) |\n| (4) State-specific Trend | 0.0093* (0.0044) | 0.0034 (0.0045) |\n\n*Source: Adapted from paper's Tables 5 & 6. The coefficient is for `Intensity_j × Post-OB_k`. * denotes significance at 5%.*\n\n### Question\n\nBased on the results in Table 1, what is the most accurate conclusion about the robustness of the program's impact on boys versus girls?", "Options": {"A": "The program had a robust positive effect on girls, but the apparent effect on boys was likely spurious and driven by pre-existing trends.", "B": "The program had a similarly significant and robust positive effect on both boys and girls.", "C": "The program had a significant effect on both groups, but the effect was simply smaller for boys after controlling for state-specific trends.", "D": "The program had no robust effect on either group, as the estimated coefficients for both boys and girls decreased after adding trend controls."}, "Answer": "A", "pi_justification": "This item assesses the ability to interpret the results of a robustness check, specifically how adding state-specific trends affects the estimated treatment effect for different subgroups. The correct answer (A) accurately notes that the effect for girls remains significant while the effect for boys becomes insignificant, implying the initial finding for boys was not robust. Distractor (B) ignores the robustness check entirely. Distractor (C) misinterprets statistical insignificance, confusing it with a smaller but still valid effect. Distractor (D) incorrectly concludes that a decrease in coefficient magnitude for girls implies a lack of robustness, even though it remains statistically significant.", "qid": "261", "question": "### Background\n\n**Research Question.** This problem analyzes the main empirical findings of India's Operation Blackboard (OB) program, focusing on the heterogeneous impacts on primary school completion by gender.\n\n**Setting / Institutional Environment.** The study uses a difference-in-differences (DiD) approach to estimate the causal effect of the OB teacher provision program.\n\n### Data / Model Specification\n\nThe effect on primary school completion is estimated using an individual-level DiD model:\n\n```latex\ny_{ijk} = \\alpha + \\beta (\\mathrm{Intensity}_{j} \\times \\mathrm{Post-OB}_{k}) + \\text{Controls} + \\varepsilon_{ijk}\n```\n\n**Table 1: Effect on Primary School Completion for Boys vs. Girls**\n\n| | Girls | Boys |\n| :--- | :---: | :---: |\n| **Specification** | **Coeff. (Std. Err.)** | **Coeff. (Std. Err.)** |\n| (2) HH Controls | 0.0165* (0.0026) | 0.0144* (0.0026) |\n| (4) State-specific Trend | 0.0093* (0.0044) | 0.0034 (0.0045) |\n\n*Source: Adapted from paper's Tables 5 & 6. The coefficient is for `Intensity_j × Post-OB_k`. * denotes significance at 5%.*\n\n### Question\n\nBased on the results in Table 1, what is the most accurate conclusion about the robustness of the program's impact on boys versus girls?", "model_evaluation_result": {"model_raw_response": "Looking at the results in Table 1, I need to compare the effects across specifications and between boys and girls.\n\nFor Girls:\n- Specification (2) with HH Controls: 0.0165* (significant)\n- Specification (4) with State-specific Trend: 0.0093* (still significant)\n\nFor Boys:\n- Specification (2) with HH Controls: 0.0144* (significant)  \n- Specification (4) with State-specific Trend: 0.0034 (not significant)\n\nThe key insight is what happens when state-specific trends are added in specification (4). State-specific trends control for differential pre-existing trends across states that could confound the treatment effect.\n\nFor girls, the effect remains statistically significant even after controlling for state-specific trends, suggesting the effect is robust to potential confounding from pre-existing trends.\n\nFor boys, the effect becomes statistically insignificant when state-specific trends are added, suggesting that what appeared to be a program effect in specification (2) was likely driven by pre-existing differential trends across states rather than the actual program impact.\n\nThis pattern indicates that the program had a genuine, robust positive effect on girls, but the apparent effect on boys was spurious and driven by pre-existing trends.\n\nA", "model_answer": "A", "error": null}}
{"ID": 335, "Question": "### Background\n\nThe BCV auction was designed to produce outcomes that are stable with respect to bids (`core*`). However, when bidder values are private information, its performance is poor compared to the standard Vickrey auction.\n\n### Data / Model Specification\n\n| Metric | BCV vs. Vickrey |\n|:---|:---|\n| Relative Avg. Efficiency | 98% |\n| Relative Avg. Revenue | 91% |\n| Relative Avg. Distance to Core | 126% |\n\n**Table 1:** Performance of BCV Auction Relative to Vickrey Auction\n\nThe BCV auction was designed to ensure outcomes are in the `core*` (stable with respect to bids), yet Table 1 shows its outcomes are, on average, less efficient, less profitable for the seller, and further from the true `core` than the standard Vickrey auction. What is the best explanation for this paradox?", "Options": {"A": "The `core*` concept is mathematically flawed and does not guarantee stability even with respect to bids.", "B": "The auction fails because local bidders behave irrationally, failing to understand the complex payment rule.", "C": "The payment rule, while ensuring stability relative to bids, incentivizes strategic bid-shading. This makes the bids a distorted signal of true value, leading to inefficient allocations that are far from the true `core`.", "D": "The Vickrey auction's superior performance is only due to the specific uniform distributions assumed; under other distributions, the BCV auction would be superior."}, "Answer": "C", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to synthesize analysis into a coherent narrative explaining a paradoxical result. It uses a Reverse-Reasoning strategy. Distractor A misattributes the problem to the theory (`core*`) rather than its implementation. Distractor B posits irrationality, a common but incorrect explanation for non-truthful bidding. Distractor D is a 'true but irrelevant' type of argument that attempts to dismiss the paper's main finding as an artifact.", "qid": "335", "question": "### Background\n\nThe BCV auction was designed to produce outcomes that are stable with respect to bids (`core*`). However, when bidder values are private information, its performance is poor compared to the standard Vickrey auction.\n\n### Data / Model Specification\n\n| Metric | BCV vs. Vickrey |\n|:---|:---|\n| Relative Avg. Efficiency | 98% |\n| Relative Avg. Revenue | 91% |\n| Relative Avg. Distance to Core | 126% |\n\n**Table 1:** Performance of BCV Auction Relative to Vickrey Auction\n\nThe BCV auction was designed to ensure outcomes are in the `core*` (stable with respect to bids), yet Table 1 shows its outcomes are, on average, less efficient, less profitable for the seller, and further from the true `core` than the standard Vickrey auction. What is the best explanation for this paradox?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 349, "Question": "### Background\n\n**Research Question.** This problem assesses the performance of machine learning models for one-year-ahead local conflict prediction and benchmarks this performance against standard econometric models to quantify the value added of more complex algorithms.\n\n**Setting and Sample.** The analysis uses a municipality-level panel dataset from Colombia, with out-of-sample predictions generated for each year from 1998 to 2005. The performance of a full-featured machine learning ensemble is compared against several simpler models.\n\n### Data / Model Specification\n\n- **Unit of Observation:** Colombian municipality-year.\n- **Outcome Variable:** `Five or more violent events`: Indicator (1/0) for a “hot spot.” Occurs in ~8% of municipality-years.\n- **Model:** `EBMA`: An Ensemble Bayesian Model Average of four machine learning algorithms (including LASSO, Random Forests) using a full set of over 310 predictors.\n- `AUC`: Area Under the ROC Curve, a performance metric (0.5 = random, 1.0 = perfect).\n\nTable 1 presents the out-of-sample AUC for the EBMA model predicting conflict in Colombia.\n\n**Table 1: Performance of EBMA Model (Colombia, AUC)**\n\n| | EBMA |\n| :--- | :---: |\n| **Colombia (attacks and clashes, 1998-2005)** | |\n| Five or more violent events | 0.915 |\n\n### Question\n\nThe Area Under the ROC Curve (AUC) is a key performance metric. Based on the results for the EBMA model in Table 1, what is the precise probabilistic interpretation of the AUC value of 0.915 for predicting `Five or more violent events`?", "Options": {"A": "If one municipality that will experience 5+ events and one that will not are chosen at random, there is a 91.5% probability the model assigns a higher risk score to the one that will experience 5+ events.", "B": "The model correctly classifies whether a municipality will experience 5+ events 91.5% of the time.", "C": "Of all municipalities the model predicts will experience 5+ events, 91.5% of them actually do.", "D": "The model successfully identifies 91.5% of all municipalities that actually experience 5+ events."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the foundational knowledge of interpreting the AUC metric, a core concept in classification model evaluation. It uses a Conceptual Judgment strategy. The distractors are designed based on classic, high-frequency misconceptions, specifically confusing AUC with other common but distinct performance metrics like accuracy (B), precision (C), and sensitivity (D).", "qid": "349", "question": "### Background\n\n**Research Question.** This problem assesses the performance of machine learning models for one-year-ahead local conflict prediction and benchmarks this performance against standard econometric models to quantify the value added of more complex algorithms.\n\n**Setting and Sample.** The analysis uses a municipality-level panel dataset from Colombia, with out-of-sample predictions generated for each year from 1998 to 2005. The performance of a full-featured machine learning ensemble is compared against several simpler models.\n\n### Data / Model Specification\n\n- **Unit of Observation:** Colombian municipality-year.\n- **Outcome Variable:** `Five or more violent events`: Indicator (1/0) for a “hot spot.” Occurs in ~8% of municipality-years.\n- **Model:** `EBMA`: An Ensemble Bayesian Model Average of four machine learning algorithms (including LASSO, Random Forests) using a full set of over 310 predictors.\n- `AUC`: Area Under the ROC Curve, a performance metric (0.5 = random, 1.0 = perfect).\n\nTable 1 presents the out-of-sample AUC for the EBMA model predicting conflict in Colombia.\n\n**Table 1: Performance of EBMA Model (Colombia, AUC)**\n\n| | EBMA |\n| :--- | :---: |\n| **Colombia (attacks and clashes, 1998-2005)** | |\n| Five or more violent events | 0.915 |\n\n### Question\n\nThe Area Under the ROC Curve (AUC) is a key performance metric. Based on the results for the EBMA model in Table 1, what is the precise probabilistic interpretation of the AUC value of 0.915 for predicting `Five or more violent events`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 278, "Question": "### Background\n\n**Research Question.** This problem investigates the extent to which short-run estimates of the Elasticity of Taxable Income (ETI) are biased by temporary intertemporal income shifting around a tax reform.\n\n**Setting / Institutional Environment.** The ETI is a key parameter measuring the responsiveness of reported income to changes in the net-of-tax rate. It is often estimated using tax reforms as natural experiments. This analysis uses a difference-in-differences (DiD) regression on monthly Danish panel data from 2009-2010 to estimate the ETI from a reform that took effect on Jan 1, 2010. The core strategy is to re-estimate the ETI on samples that systematically exclude the months immediately surrounding the reform to isolate the contribution of income shifting.\n\n**Variables & Parameters.**\n- `ETI`: The Elasticity of Taxable Income.\n- `w_{y,m,i}`: Normalized percentage change in monthly wage.\n- `d_y,i^2010`: Year dummy (1 if 2010).\n- `d_i^T`: Treatment group dummy.\n- `τ_{y,i}`: Marginal tax rate.\n\n---\n\n### Data / Model Specification\n\nThe DiD regression model used to estimate the ETI is:\n```latex\nw_{y,m,i} = \\beta_{0} + \\beta_{1}d_{y,i}^{2010} + \\beta_{2}d_{i}^{T} + \\beta_{3} \\frac{1-\\tau_{y,i}}{1-\\tau_{2009,i}} + \\varepsilon_{y,m,i} \n\\quad \\text{(Eq. (1))}\n```\nThe coefficient `β₃` is the ETI estimate. It can be shown that `β₃` is the ratio of the DiD change in income to the DiD change in the net-of-tax rate.\n\n**Table 1: Importance of Shifting for ETI Estimates**\n\n| Income Group | All months (1) | Excl. D09 & J10 (2) | Excl. N09, D09, & J10 (3) |\n| :--- | :--- | :--- | :--- |\n| Full sample | 0.10 (0.08, 0.11) | 0.03 (0.01, 0.04) | 0.01 (-0.00, 0.03) |\n| P99 < income | 0.26 (0.21, 0.31) | -0.00 (-0.06, 0.05) | -0.06 (-0.11, -0.01) |\n\n*Note: The table reports ETI estimates and 95% confidence intervals. The estimation sample in column (1) includes all months from Jan 2009 to Dec 2010. Columns (2) and (3) exclude the indicated months.*\n\n---\n\n### The Question\n\nBased on the results in Table 1, what is the most accurate conclusion about the source of the short-run Elasticity of Taxable Income (ETI) for the full sample?", "Options": {"A": "The baseline ETI of 0.10 is almost entirely attributable to temporary income shifting, as the estimate becomes statistically insignificant from zero when the months around the reform (Nov 2009 - Jan 2010) are excluded.", "B": "The ETI of 0.10 represents a real behavioral response to the tax change, with income shifting playing only a minor, statistically insignificant role.", "C": "Income shifting in December and January accounts for a significant portion of the ETI, but a real elasticity of approximately 0.03 remains after accounting for it.", "D": "The ETI is significantly higher for the top 1% of earners than for the full sample, indicating that only the wealthiest individuals respond to tax changes."}, "Answer": "A", "pi_justification": "This item assesses the core empirical finding of the paper using a Computational Judgment strategy. The user must interpret the regression results in Table 1, comparing the baseline ETI estimate with estimates from restricted samples to deduce the impact of income shifting. The correct option (A) accurately synthesizes this comparison. Distractors are designed based on common errors: Conceptual Opposite (B: ignoring the shifting effect), Step-Omission Error (C: drawing a conclusion from an intermediate step, column 2, instead of the final result in column 3), and True but Irrelevant (D: focusing on subgroups instead of the main question).", "qid": "278", "question": "### Background\n\n**Research Question.** This problem investigates the extent to which short-run estimates of the Elasticity of Taxable Income (ETI) are biased by temporary intertemporal income shifting around a tax reform.\n\n**Setting / Institutional Environment.** The ETI is a key parameter measuring the responsiveness of reported income to changes in the net-of-tax rate. It is often estimated using tax reforms as natural experiments. This analysis uses a difference-in-differences (DiD) regression on monthly Danish panel data from 2009-2010 to estimate the ETI from a reform that took effect on Jan 1, 2010. The core strategy is to re-estimate the ETI on samples that systematically exclude the months immediately surrounding the reform to isolate the contribution of income shifting.\n\n**Variables & Parameters.**\n- `ETI`: The Elasticity of Taxable Income.\n- `w_{y,m,i}`: Normalized percentage change in monthly wage.\n- `d_y,i^2010`: Year dummy (1 if 2010).\n- `d_i^T`: Treatment group dummy.\n- `τ_{y,i}`: Marginal tax rate.\n\n---\n\n### Data / Model Specification\n\nThe DiD regression model used to estimate the ETI is:\n```latex\nw_{y,m,i} = \\beta_{0} + \\beta_{1}d_{y,i}^{2010} + \\beta_{2}d_{i}^{T} + \\beta_{3} \\frac{1-\\tau_{y,i}}{1-\\tau_{2009,i}} + \\varepsilon_{y,m,i} \n\\quad \\text{(Eq. (1))}\n```\nThe coefficient `β₃` is the ETI estimate. It can be shown that `β₃` is the ratio of the DiD change in income to the DiD change in the net-of-tax rate.\n\n**Table 1: Importance of Shifting for ETI Estimates**\n\n| Income Group | All months (1) | Excl. D09 & J10 (2) | Excl. N09, D09, & J10 (3) |\n| :--- | :--- | :--- | :--- |\n| Full sample | 0.10 (0.08, 0.11) | 0.03 (0.01, 0.04) | 0.01 (-0.00, 0.03) |\n| P99 < income | 0.26 (0.21, 0.31) | -0.00 (-0.06, 0.05) | -0.06 (-0.11, -0.01) |\n\n*Note: The table reports ETI estimates and 95% confidence intervals. The estimation sample in column (1) includes all months from Jan 2009 to Dec 2010. Columns (2) and (3) exclude the indicated months.*\n\n---\n\n### The Question\n\nBased on the results in Table 1, what is the most accurate conclusion about the source of the short-run Elasticity of Taxable Income (ETI) for the full sample?", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to analyze how the ETI estimates change as months around the reform are excluded.\n\nFor the full sample:\n- Column (1) - All months: ETI = 0.10 (0.08, 0.11) - statistically significant\n- Column (2) - Excluding Dec 2009 & Jan 2010: ETI = 0.03 (0.01, 0.04) - still statistically significant  \n- Column (3) - Excluding Nov 2009, Dec 2009, & Jan 2010: ETI = 0.01 (-0.00, 0.03) - becomes statistically insignificant\n\nThe pattern shows:\n1. The baseline ETI drops from 0.10 to 0.03 when excluding just Dec 2009 and Jan 2010\n2. This indicates that income shifting in these two months accounts for a substantial portion (0.07 out of 0.10) of the original estimate\n3. However, a meaningful elasticity of 0.03 remains and is still statistically significant\n4. Only when Nov 2009 is also excluded does the estimate become statistically insignificant\n\nThis suggests that while temporary income shifting around the reform contributes significantly to the baseline ETI estimate, there is still a real behavioral response of approximately 0.03 that persists after accounting for the most obvious shifting periods.\n\nC", "model_answer": "C", "error": null}}
{"ID": 351, "Question": "### Background\n\n**Research Question.** This problem assesses the performance of machine learning models for one-year-ahead local conflict prediction and benchmarks this performance against standard econometric models to quantify the value added of more complex algorithms.\n\n**Setting and Sample.** The analysis uses a municipality-level panel dataset from Colombia, with out-of-sample predictions generated for each year from 1998 to 2005. The performance of a full-featured machine learning ensemble is compared against several simpler models.\n\n### Data / Model Specification\n\nThe paper notes that with over 310 predictors, a standard OLS model performs poorly (AUC=0.821) compared to a LASSO model (AUC=0.845) for predicting `Any violent event`. This is because LASSO uses a penalty term to prevent overfitting.\n\n### Question\n\nWhat is the primary mechanism by which the LASSO penalty achieves this?", "Options": {"A": "It shrinks the coefficients of less important predictors, forcing many of them to be exactly zero, which simplifies the model.", "B": "It shrinks coefficients towards zero but does not perform variable selection, retaining all predictors in the model.", "C": "It sequentially builds decision rules and prunes branches that do not improve predictive accuracy.", "D": "It identifies complex non-linear relationships and interaction effects between predictors."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses understanding of the fundamental mechanism of LASSO regularization, a key technique mentioned in the paper. It uses a Reverse-Reasoning strategy (given the method, infer its mechanism). The distractors are designed to be highly plausible by describing the mechanisms of other common and related machine learning/statistical methods: Ridge regression (B), decision trees (C), and a more general goal of other ML algorithms (D).", "qid": "351", "question": "### Background\n\n**Research Question.** This problem assesses the performance of machine learning models for one-year-ahead local conflict prediction and benchmarks this performance against standard econometric models to quantify the value added of more complex algorithms.\n\n**Setting and Sample.** The analysis uses a municipality-level panel dataset from Colombia, with out-of-sample predictions generated for each year from 1998 to 2005. The performance of a full-featured machine learning ensemble is compared against several simpler models.\n\n### Data / Model Specification\n\nThe paper notes that with over 310 predictors, a standard OLS model performs poorly (AUC=0.821) compared to a LASSO model (AUC=0.845) for predicting `Any violent event`. This is because LASSO uses a penalty term to prevent overfitting.\n\n### Question\n\nWhat is the primary mechanism by which the LASSO penalty achieves this?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 350, "Question": "### Background\n\n**Research Question.** This problem assesses the performance of machine learning models for one-year-ahead local conflict prediction and benchmarks this performance against standard econometric models to quantify the value added of more complex algorithms.\n\n**Setting and Sample.** The analysis uses a municipality-level panel dataset from Colombia, with out-of-sample predictions generated for each year from 1998 to 2005. The performance of a full-featured machine learning ensemble is compared against several simpler models.\n\n### Data / Model Specification\n\n- **Unit of Observation:** Colombian municipality-year.\n- **Outcome Variable:** `Any violent event`: Indicator (1/0) for at least one attack or clash.\n- **Models:**\n  - `Lagged Predictand`: A simple AR(1) model using only a single lag of the outcome indicator.\n  - `Location FE Only`: An OLS model where the prediction for a location is its historical mean level of violence (i.e., its estimated location fixed effect).\n- `AUC`: Area Under the ROC Curve, a performance metric (0.5 = random, 1.0 = perfect).\n\nTable 1 presents the out-of-sample AUC for various models predicting conflict in Colombia.\n\n**Table 1: Performance Versus Benchmarks (Colombia, AUC)**\n\n| | Lagged Predictand | Location FE Only |\n| :--- | :---: | :---: |\n| **Colombia (attacks and clashes, 1998-2005)** | |\n| Any violent event | 0.743 | 0.828 |\n\n### Question\n\nTable 1 compares the performance of a simple autoregressive model (`Lagged Predictand`) with a model based on a location's historical average risk (`Location FE Only`). Based on the results for predicting `Any violent event`, what does the superior performance of the Location FE model (AUC 0.828 vs. 0.743) imply about the nature of local conflict risk?", "Options": {"A": "Long-term, persistent spatial risk factors are substantially more important for prediction than short-term temporal persistence.", "B": "Year-to-year persistence is a stronger predictor of conflict than a location's historical average risk.", "C": "Machine learning models like EBMA can outperform simpler benchmarks.", "D": "Neither a location's history nor its recent past are useful for predicting conflict."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item tests the ability to interpret a direct comparison of two benchmark models and draw a conclusion central to the paper's main argument. It uses a Computational Judgment strategy (interpreting table values to make a judgment). The distractors include the conceptual opposite (B), a true but irrelevant statement that distracts from the specific comparison being made (C), and a direct misinterpretation of the data (D).", "qid": "350", "question": "### Background\n\n**Research Question.** This problem assesses the performance of machine learning models for one-year-ahead local conflict prediction and benchmarks this performance against standard econometric models to quantify the value added of more complex algorithms.\n\n**Setting and Sample.** The analysis uses a municipality-level panel dataset from Colombia, with out-of-sample predictions generated for each year from 1998 to 2005. The performance of a full-featured machine learning ensemble is compared against several simpler models.\n\n### Data / Model Specification\n\n- **Unit of Observation:** Colombian municipality-year.\n- **Outcome Variable:** `Any violent event`: Indicator (1/0) for at least one attack or clash.\n- **Models:**\n  - `Lagged Predictand`: A simple AR(1) model using only a single lag of the outcome indicator.\n  - `Location FE Only`: An OLS model where the prediction for a location is its historical mean level of violence (i.e., its estimated location fixed effect).\n- `AUC`: Area Under the ROC Curve, a performance metric (0.5 = random, 1.0 = perfect).\n\nTable 1 presents the out-of-sample AUC for various models predicting conflict in Colombia.\n\n**Table 1: Performance Versus Benchmarks (Colombia, AUC)**\n\n| | Lagged Predictand | Location FE Only |\n| :--- | :---: | :---: |\n| **Colombia (attacks and clashes, 1998-2005)** | |\n| Any violent event | 0.743 | 0.828 |\n\n### Question\n\nTable 1 compares the performance of a simple autoregressive model (`Lagged Predictand`) with a model based on a location's historical average risk (`Location FE Only`). Based on the results for predicting `Any violent event`, what does the superior performance of the Location FE model (AUC 0.828 vs. 0.743) imply about the nature of local conflict risk?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 355, "Question": "### Background\n\nThe theoretical test requires calculating the curvature of the true null manifold `S`, which is unknown because it depends on the true parameter `θ₀`. The baseline feasible procedure replaces this with the maximal curvature of the *observed* manifold `S* = {Σ⁻¹/²θ(β)}`.\n\n### Data / Model Specification\n\nThis baseline procedure calculates the maximal curvature over the entire observed manifold `S*`. While this ensures the test correctly controls size, it can lead to a loss of statistical power if `S*` has regions of high curvature that are far from the true parameter value.\n\n### The Question\n\nWhich of the following best describes the reason for this power loss and the logic of the paper's proposed modifications to address it?", "Options": {"A": "Power is lost if `S*` has regions of high curvature far from the true parameter value; the modifications address this by localizing the curvature calculation or isolating the specific parameters causing high curvature.", "B": "Power is lost because the baseline procedure systematically underestimates the relevant local curvature; the modifications correct this by using a more flexible optimization algorithm.", "C": "Power is lost because the procedure defaults to the conservative projection method's `χ²ₖ` bound; the modifications allow it to converge to the standard `χ²_{k-p}` bound.", "D": "Power is lost because the global optimization is computationally infeasible; the modifications' primary purpose is to make the test feasible by restricting the search space."}, "Answer": "A", "pi_justification": "This item assesses the core rationale for the paper's practical modifications. It uses a Reverse-Reasoning strategy, asking the user to identify the cause (distant high curvature) of an outcome (low power). Distractors are designed to be plausible errors: B is a conceptual opposite (under vs. overestimation), C confuses the geometric test with the simpler projection method, and D misidentifies the core problem as feasibility rather than power.", "qid": "355", "question": "### Background\n\nThe theoretical test requires calculating the curvature of the true null manifold `S`, which is unknown because it depends on the true parameter `θ₀`. The baseline feasible procedure replaces this with the maximal curvature of the *observed* manifold `S* = {Σ⁻¹/²θ(β)}`.\n\n### Data / Model Specification\n\nThis baseline procedure calculates the maximal curvature over the entire observed manifold `S*`. While this ensures the test correctly controls size, it can lead to a loss of statistical power if `S*` has regions of high curvature that are far from the true parameter value.\n\n### The Question\n\nWhich of the following best describes the reason for this power loss and the logic of the paper's proposed modifications to address it?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 348, "Question": "### Background\n\n**Research Question.** This problem explores the asymptotic theory of the Fully Modified OLS (FM-OLS) estimator, contrasting the standard theory with the more refined fixed-b theory to reveal limitations of the standard approach.\n\n**Setting.** The FM-OLS estimator, $\\widehat{\\theta}^+$, applies two transformations to correct for issues in OLS estimation of cointegrating regressions. Its properties can be analyzed using two different asymptotic frameworks.\n\n### Data / Model Specification\n\nUnder **standard asymptotic theory**, which assumes consistent estimation of long-run variance matrices, the FM-OLS estimator has a clean, zero-mean Gaussian mixture distribution:\n\n```latex\nA^{-1}(\\widehat{\\theta}^{+}-\\theta) \\Rightarrow \\left(\\int B_{v}^{*}(r)B_{v}^{*}(r)^{\\prime}d r\\right)^{-1}\\int B_{v}^{*}(r)d B_{u\\cdot v}(r) \\quad \\text{(Eq. (1))}\n```\n\nUnder **fixed-b asymptotic theory**, which treats the bandwidth-to-sample-size ratio $b=M/T$ as fixed, the estimator has a more complex distribution that includes bias terms:\n\n```latex\nA^{-1}(\\widehat{\\theta}_{b}^{+}-\\theta) \\Rightarrow \\left(\\int B_{v}^{*}(r)B_{v}^{*}(r)^{\\prime}d r\\right)^{-1} \\times\\left(\\int B_{v}^{*}(r)d B_{u v}^{b}(r)+\\mathcal{B}_{1}-\\mathcal{B}_{2}\\right) \\quad \\text{(Eq. (2))}\n```\n\n### Question\n\nBased on a comparison of the standard (Eq. (1)) and fixed-b (Eq. (2)) asymptotic results, what is the primary critique of the standard FM-OLS theory revealed by the fixed-b analysis?", "Options": {"A": "The standard theory is misleading because it assumes perfect estimation of the nuisance parameters ($\\Omega, \\Delta$), while the fixed-b theory correctly models their sampling variability, which in turn creates a residual asymptotic bias ($\\mathcal{B}_1, \\mathcal{B}_2$).", "B": "The fixed-b result proves that the standard asymptotic theory is fundamentally flawed, as the two distributions are irreconcilable.", "C": "The standard theory fails to account for the fact that different kernel functions (e.g., Bartlett vs. Quadratic Spectral) will produce different estimates in finite samples.", "D": "The standard theory is invalid because its assumption that the bandwidth ratio $b \\to 0$ implies that no lags can be used for long-run variance estimation."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to synthesize the paper's core theoretical critique of FM-OLS. It uses a Reverse-Reasoning strategy, asking for the main implication of the fixed-b result. The correct option (A) identifies the fundamental issue: the practical impact of estimating nuisance parameters, which standard theory ignores. Distractors are designed to be plausible but incorrect interpretations: (B) is a conceptual opposite about reconcilability, (C) identifies a true but superficial consequence rather than the root cause, and (D) reflects a common misunderstanding of asymptotic limits.", "qid": "348", "question": "### Background\n\n**Research Question.** This problem explores the asymptotic theory of the Fully Modified OLS (FM-OLS) estimator, contrasting the standard theory with the more refined fixed-b theory to reveal limitations of the standard approach.\n\n**Setting.** The FM-OLS estimator, $\\widehat{\\theta}^+$, applies two transformations to correct for issues in OLS estimation of cointegrating regressions. Its properties can be analyzed using two different asymptotic frameworks.\n\n### Data / Model Specification\n\nUnder **standard asymptotic theory**, which assumes consistent estimation of long-run variance matrices, the FM-OLS estimator has a clean, zero-mean Gaussian mixture distribution:\n\n```latex\nA^{-1}(\\widehat{\\theta}^{+}-\\theta) \\Rightarrow \\left(\\int B_{v}^{*}(r)B_{v}^{*}(r)^{\\prime}d r\\right)^{-1}\\int B_{v}^{*}(r)d B_{u\\cdot v}(r) \\quad \\text{(Eq. (1))}\n```\n\nUnder **fixed-b asymptotic theory**, which treats the bandwidth-to-sample-size ratio $b=M/T$ as fixed, the estimator has a more complex distribution that includes bias terms:\n\n```latex\nA^{-1}(\\widehat{\\theta}_{b}^{+}-\\theta) \\Rightarrow \\left(\\int B_{v}^{*}(r)B_{v}^{*}(r)^{\\prime}d r\\right)^{-1} \\times\\left(\\int B_{v}^{*}(r)d B_{u v}^{b}(r)+\\mathcal{B}_{1}-\\mathcal{B}_{2}\\right) \\quad \\text{(Eq. (2))}\n```\n\n### Question\n\nBased on a comparison of the standard (Eq. (1)) and fixed-b (Eq. (2)) asymptotic results, what is the primary critique of the standard FM-OLS theory revealed by the fixed-b analysis?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 176, "Question": "## Background\n\n**Research Question.** This problem examines the robust method for testing whether a simple first-difference model is a valid simplification of a more general dynamic model, contrasting it with methodologically flawed approaches.\n\n**Setting.** An analyst begins with a general Autoregressive Distributed Lag (ADL) model and wants to determine if it can be simplified to a model in first differences. This simplification is only valid if specific, testable restrictions on the general model's parameters hold.\n\n---\n\n## Data / Model Specification\n\nConsider the following models:\n1.  **The General ADL(1,1) Model:**\n    ```latex\n    y_{t}=\\beta_{1}y_{t-1}+\\gamma_{0}x_{t}+\\gamma_{1}x_{t-1}+v_{t} \\quad \\text{(Eq. (1))}\n    ```\n2.  **The Error-Correction Model (ECM) Reparameterization of Eq. (1):**\n    ```latex\n    \\Delta y_{t}=\\gamma_{0}\\Delta x_{t}+(\\beta_{1}-1)y_{t-1}+(\\gamma_{0}+\\gamma_{1})x_{t-1}+v_{t} \\quad \\text{(Eq. (2))}\n    ```\n\n---\n\n## Question\n\nAn analyst wants to test if a simple first-difference model (i.e., \\(\\Delta y_{t}=\\gamma_{0}\\Delta x_{t}+v_{t}\\)) is a valid simplification of the general ADL(1,1) model given by Eq. (1). What is the most robust statistical procedure to conduct this test?", "Options": {"A": "Estimate the general model in its error-correction form (Eq. 2) and perform a joint test on the null hypothesis that the coefficients on the lagged level terms (\\(y_{t-1}\\) and \\(x_{t-1}\\)) are both zero.", "B": "Estimate the simple first-difference model and test if the Durbin-Watson statistic from its residuals is close to 2.", "C": "Estimate the general ADL(1,1) model (Eq. 1) and test the single hypothesis that the coefficient on \\(y_{t-1}\\) is equal to 1.", "D": "Estimate both the general model and the first-difference model and choose the one with the higher R-squared value."}, "Answer": "A", "pi_justification": "This item assesses the student's understanding of the 'general-to-specific' testing methodology, a central theme of the paper. It uses a Reverse-Reasoning strategy (what is the correct procedure for a given goal?). Distractors represent flawed or incomplete approaches: (B) is the specific-to-general fallacy critiqued in the paper, (C) is an incomplete test, and (D) represents a common but invalid method for model selection.", "qid": "176", "question": "## Background\n\n**Research Question.** This problem examines the robust method for testing whether a simple first-difference model is a valid simplification of a more general dynamic model, contrasting it with methodologically flawed approaches.\n\n**Setting.** An analyst begins with a general Autoregressive Distributed Lag (ADL) model and wants to determine if it can be simplified to a model in first differences. This simplification is only valid if specific, testable restrictions on the general model's parameters hold.\n\n---\n\n## Data / Model Specification\n\nConsider the following models:\n1.  **The General ADL(1,1) Model:**\n    ```latex\n    y_{t}=\\beta_{1}y_{t-1}+\\gamma_{0}x_{t}+\\gamma_{1}x_{t-1}+v_{t} \\quad \\text{(Eq. (1))}\n    ```\n2.  **The Error-Correction Model (ECM) Reparameterization of Eq. (1):**\n    ```latex\n    \\Delta y_{t}=\\gamma_{0}\\Delta x_{t}+(\\beta_{1}-1)y_{t-1}+(\\gamma_{0}+\\gamma_{1})x_{t-1}+v_{t} \\quad \\text{(Eq. (2))}\n    ```\n\n---\n\n## Question\n\nAn analyst wants to test if a simple first-difference model (i.e., \\(\\Delta y_{t}=\\gamma_{0}\\Delta x_{t}+v_{t}\\)) is a valid simplification of the general ADL(1,1) model given by Eq. (1). What is the most robust statistical procedure to conduct this test?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine what restrictions on the general ADL(1,1) model would make it equivalent to a simple first-difference model.\n\nThe general ADL(1,1) model is:\n$$y_t = \\beta_1 y_{t-1} + \\gamma_0 x_t + \\gamma_1 x_{t-1} + v_t$$\n\nThe simple first-difference model is:\n$$\\Delta y_t = \\gamma_0 \\Delta x_t + v_t$$\n\nFor the general model to simplify to the first-difference model, I need to identify the necessary parameter restrictions.\n\nFrom the ECM reparameterization (Eq. 2):\n$$\\Delta y_t = \\gamma_0 \\Delta x_t + (\\beta_1 - 1)y_{t-1} + (\\gamma_0 + \\gamma_1)x_{t-1} + v_t$$\n\nFor this to reduce to the simple first-difference model, the lagged level terms must disappear, which requires:\n1. $\\beta_1 - 1 = 0$ (coefficient on $y_{t-1}$ equals zero)\n2. $\\gamma_0 + \\gamma_1 = 0$ (coefficient on $x_{t-1}$ equals zero)\n\nThis is equivalent to:\n1. $\\beta_1 = 1$ \n2. $\\gamma_1 = -\\gamma_0$\n\nNow examining the options:\n\nA) Tests the joint hypothesis that both lagged level coefficients in the ECM are zero - this directly tests the exact restrictions needed for the simplification to be valid.\n\nB) Testing the Durbin-Watson statistic doesn't address whether the parameter restrictions hold.\n\nC) Only tests $\\beta_1 = 1$, which is necessary but not sufficient (missing the test for $\\gamma_1 = -\\gamma_0$).\n\nD) Comparing R-squared values is not a formal statistical test of the parameter restrictions.\n\nOption A provides the most robust approach because it directly tests the joint restrictions that are both necessary and sufficient for the simplification to be valid.\n\nA", "model_answer": "A", "error": null}}
{"ID": 302, "Question": "### Background\n\n**Research Question.** This problem addresses the core challenge of estimating demand systems derived from *nonlinear* parallel preference structures, where consumer utility `u` is unobservable and the standard method of deriving Marshallian demands via an invertible cost function fails.\n\n**Setting / Institutional Environment.** We consider a general parallel preference structure where the income-consumption curves may be nonlinear. The key issue is that the cost function `C(u,p)` cannot be easily inverted to find an indirect utility function `V(p,y)`, precluding the use of Roy's identity. An alternative estimation strategy is therefore required.\n\n**Variables & Parameters.**\n- `x_i`: Hicksian demand for good `i` (`i = 1, ..., N`).\n- `p`: An `N x 1` vector of prices.\n- `u`: The unobservable scalar utility index.\n- `f_i(·)`: A utility-dependent function, potentially nonlinear.\n- `ψ^i(p)`: A price-dependent function, `∂A(p)/∂p_i`.\n- `x_1`: The observed consumption of the numeraire good (`i=1`).\n\n---\n\n### Data / Model Specification\n\nThe theoretical model is the system of `N` Hicksian demand functions for a general parallel preference structure:\n```latex\nx_i = f_i(u) + ψ^i(p) \n\n\\quad\\text{for } i = 1, ..., N \\quad\\text{(Eq. 1)}\n```\nFor nonlinear `f_i(·)`, this system is not directly estimable because `u` is unobservable.\n\n---\n\n### Question\n\nStarting from the Hicksian demand system in Eq. (1), what is the final estimable 'generalized Ashenfelter-Heckman' (GAH) form for the demand of a non-numeraire good `x_i` (`i > 1`) after using the numeraire good `x_1` to substitute for the unobservable utility `u`?", "Options": {"A": "```latex\nx_i = f_i(f_1^{-1}(x_1 - ψ^1(p))) + ψ^i(p)\n```", "B": "```latex\nx_i = f_i(f_1^{-1}(x_1 + ψ^1(p))) + ψ^i(p)\n```", "C": "```latex\nx_i = f_1(f_i^{-1}(x_1 - ψ^1(p))) + ψ^i(p)\n```", "D": "```latex\nx_i = f_i(u) + ψ^i(p) - ψ^1(p)\n```"}, "Answer": "A", "pi_justification": "This item assesses the student's ability to perform the key algebraic derivation that makes the nonlinear model estimable. It uses a Computational Judgment strategy. Distractors are based on common algebraic errors (sign error), conceptual confusion (swapping functions), and incomplete reasoning.", "qid": "302", "question": "### Background\n\n**Research Question.** This problem addresses the core challenge of estimating demand systems derived from *nonlinear* parallel preference structures, where consumer utility `u` is unobservable and the standard method of deriving Marshallian demands via an invertible cost function fails.\n\n**Setting / Institutional Environment.** We consider a general parallel preference structure where the income-consumption curves may be nonlinear. The key issue is that the cost function `C(u,p)` cannot be easily inverted to find an indirect utility function `V(p,y)`, precluding the use of Roy's identity. An alternative estimation strategy is therefore required.\n\n**Variables & Parameters.**\n- `x_i`: Hicksian demand for good `i` (`i = 1, ..., N`).\n- `p`: An `N x 1` vector of prices.\n- `u`: The unobservable scalar utility index.\n- `f_i(·)`: A utility-dependent function, potentially nonlinear.\n- `ψ^i(p)`: A price-dependent function, `∂A(p)/∂p_i`.\n- `x_1`: The observed consumption of the numeraire good (`i=1`).\n\n---\n\n### Data / Model Specification\n\nThe theoretical model is the system of `N` Hicksian demand functions for a general parallel preference structure:\n```latex\nx_i = f_i(u) + ψ^i(p) \n\n\\quad\\text{for } i = 1, ..., N \\quad\\text{(Eq. 1)}\n```\nFor nonlinear `f_i(·)`, this system is not directly estimable because `u` is unobservable.\n\n---\n\n### Question\n\nStarting from the Hicksian demand system in Eq. (1), what is the final estimable 'generalized Ashenfelter-Heckman' (GAH) form for the demand of a non-numeraire good `x_i` (`i > 1`) after using the numeraire good `x_1` to substitute for the unobservable utility `u`?", "model_evaluation_result": {"model_raw_response": "Looking at this step-by-step:\n\nFrom the Hicksian demand system in Eq. (1):\n```\nx_i = f_i(u) + ψ^i(p)  for i = 1, ..., N\n```\n\nFor the numeraire good (i = 1):\n```\nx_1 = f_1(u) + ψ^1(p)\n```\n\nTo eliminate the unobservable utility u, I need to solve for u from the numeraire equation:\n```\nx_1 - ψ^1(p) = f_1(u)\n```\n\nApplying the inverse function f_1^{-1} to both sides:\n```\nu = f_1^{-1}(x_1 - ψ^1(p))\n```\n\nNow substituting this expression for u back into the demand equation for any non-numeraire good x_i (i > 1):\n```\nx_i = f_i(f_1^{-1}(x_1 - ψ^1(p))) + ψ^i(p)\n```\n\nThis gives us an estimable form where the unobservable utility u has been replaced by observable quantities through the numeraire good x_1.\n\nA", "model_answer": "A", "error": null}}
{"ID": 356, "Question": "### Background\n\nA researcher is testing a DSGE model where the parameter vector `β` is partitioned into `β_J`, containing weakly identified preference parameters, and `β_{-J}`, containing well-identified technology parameters.\n\n### Data / Model Specification\n\nWeak identification is known to induce high manifold curvature. In this context, the high curvature is a global property of the manifold's geometry, not a localized 'kink' in a distant region. The researcher wishes to apply the paper's modified testing procedure.\n\n### The Question\n\nTo maximize the test's statistical power, what is the optimal strategy for choosing the parameter subset `J` for curvature calculation and the localization radius `R`?", "Options": {"A": "Set `J` to include only the weakly identified preference parameters and choose a large value for `R`.", "B": "Set `J` to include only the weakly identified preference parameters but choose a small value for `R` to exclude the high-curvature region.", "C": "Set `J` to include only the well-identified technology parameters to ensure a stable calculation and choose a large value for `R`.", "D": "Set `J` to include all parameters and choose a small value for `R` to reduce the computational burden of the optimization."}, "Answer": "A", "pi_justification": "This item uses a Scenario Application strategy to test the practical application of the paper's most advanced tools in a relevant economic context. The distractors are designed around high-fidelity misconceptions: B correctly identifies J but misapplies the logic for R in the context of global curvature; C makes the conceptual opposite error for J; D fails to apply either modification correctly, reflecting a superficial understanding of their purpose.", "qid": "356", "question": "### Background\n\nA researcher is testing a DSGE model where the parameter vector `β` is partitioned into `β_J`, containing weakly identified preference parameters, and `β_{-J}`, containing well-identified technology parameters.\n\n### Data / Model Specification\n\nWeak identification is known to induce high manifold curvature. In this context, the high curvature is a global property of the manifold's geometry, not a localized 'kink' in a distant region. The researcher wishes to apply the paper's modified testing procedure.\n\n### The Question\n\nTo maximize the test's statistical power, what is the optimal strategy for choosing the parameter subset `J` for curvature calculation and the localization radius `R`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 272, "Question": "### Background\n\n**Research Question.** This problem examines the optimal marginal trade-off for an ambiguity-averse agent in a complete markets equilibrium, leading to the derivation and interpretation of the economy's stochastic discount factor (SDF).\n\n**Setting.** An agent `i` with smooth ambiguity preferences optimizes their consumption plan in a dynamic, complete markets economy. We analyze the first-order condition that governs their optimal choice, assuming an interior, no-default equilibrium.\n\n**Variables and Parameters.**\n- `q_t(ω^t)`: Price at time 0 of one unit of consumption delivered at time `t` if state `ω^t` occurs.\n- `β`: Discount factor.\n- `c_t^i(ω^t)`: Agent `i`'s consumption at `(t, ω^t)`.\n- `u_i'(·)`: Agent `i`'s marginal utility of consumption.\n- `c^i`: The agent's entire consumption plan.\n- `U_i(c^i)`: The total discounted utility of the plan under a given model, `∑_τ β^τ u_i(c_τ^i)`.\n- `P(ω^t)`: Probability of state `ω^t` under a first-order model `P`.\n- `φ_i'(·)`: The derivative of agent `i`'s ambiguity attitude function.\n- `μ^i`: Agent `i`'s second-order belief over models.\n- `E_{μ^i}[·]`: Expectation with respect to the second-order belief.\n- `λ(i)`: Agent `i`'s Lagrange multiplier on their budget constraint.\n\n---\n\n### Data / Model Specification\n\nAn agent `i` with preferences represented by the functional `V^i` chooses a consumption plan `c^i` to maximize their lifetime utility:\n```latex\nV^{i}(c^{i}) = \\mathbb{E}_{\\mu^{i}}\\left\\{\\phi_{i}\\left(\\mathbb{E}_{P}\\left[U_{i}(c^{i})\\right]\\right)\\right\\}\n```\nIn a no-default equilibrium, the optimal plan must satisfy the following first-order condition (FOC) for every time-state node `(t, ω^t)`:\n```latex\n\\beta^{t}u_{i}^{\\prime}(c_{t}^{i}(\\omega^{t}))\\mathbb{E}_{\\mu^{i}}\\{\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})\\}=\\lambda(i)q_{t}(\\omega^{t})\n```\nThis FOC can be rearranged to define the agent's stochastic discount factor (SDF), `m_t^i(ω^t)`, which prices assets such that `q_t(ω^t) = (1/λ(i)) · m_t^i(ω^t)`.\n\n---\n\n### Question\n\nBased on the first-order condition, which component of the agent's stochastic discount factor, `m_t^i(ω^t)`, uniquely captures their attitude towards ambiguity?", "Options": {"A": "The term `β^t`", "B": "The term `u_i'(c_t^i(ω^t))`", "C": "The term `\\mathbb{E}_{\\mu^{i}}\\{\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})\\}`", "D": "The Lagrange multiplier `λ(i)`"}, "Answer": "C", "pi_justification": "This question assesses the student's ability to interpret the components of the stochastic discount factor derived from the model's first-order conditions. The core task is to correctly identify the novel term representing ambiguity aversion. The distractors are designed based on common conceptual errors: (A) confusing ambiguity with time preference, (B) confusing ambiguity with standard risk aversion (a very common error), and (D) misidentifying a part of the budget constraint's shadow price as part of the SDF's preference components.", "qid": "272", "question": "### Background\n\n**Research Question.** This problem examines the optimal marginal trade-off for an ambiguity-averse agent in a complete markets equilibrium, leading to the derivation and interpretation of the economy's stochastic discount factor (SDF).\n\n**Setting.** An agent `i` with smooth ambiguity preferences optimizes their consumption plan in a dynamic, complete markets economy. We analyze the first-order condition that governs their optimal choice, assuming an interior, no-default equilibrium.\n\n**Variables and Parameters.**\n- `q_t(ω^t)`: Price at time 0 of one unit of consumption delivered at time `t` if state `ω^t` occurs.\n- `β`: Discount factor.\n- `c_t^i(ω^t)`: Agent `i`'s consumption at `(t, ω^t)`.\n- `u_i'(·)`: Agent `i`'s marginal utility of consumption.\n- `c^i`: The agent's entire consumption plan.\n- `U_i(c^i)`: The total discounted utility of the plan under a given model, `∑_τ β^τ u_i(c_τ^i)`.\n- `P(ω^t)`: Probability of state `ω^t` under a first-order model `P`.\n- `φ_i'(·)`: The derivative of agent `i`'s ambiguity attitude function.\n- `μ^i`: Agent `i`'s second-order belief over models.\n- `E_{μ^i}[·]`: Expectation with respect to the second-order belief.\n- `λ(i)`: Agent `i`'s Lagrange multiplier on their budget constraint.\n\n---\n\n### Data / Model Specification\n\nAn agent `i` with preferences represented by the functional `V^i` chooses a consumption plan `c^i` to maximize their lifetime utility:\n```latex\nV^{i}(c^{i}) = \\mathbb{E}_{\\mu^{i}}\\left\\{\\phi_{i}\\left(\\mathbb{E}_{P}\\left[U_{i}(c^{i})\\right]\\right)\\right\\}\n```\nIn a no-default equilibrium, the optimal plan must satisfy the following first-order condition (FOC) for every time-state node `(t, ω^t)`:\n```latex\n\\beta^{t}u_{i}^{\\prime}(c_{t}^{i}(\\omega^{t}))\\mathbb{E}_{\\mu^{i}}\\{\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})\\}=\\lambda(i)q_{t}(\\omega^{t})\n```\nThis FOC can be rearranged to define the agent's stochastic discount factor (SDF), `m_t^i(ω^t)`, which prices assets such that `q_t(ω^t) = (1/λ(i)) · m_t^i(ω^t)`.\n\n---\n\n### Question\n\nBased on the first-order condition, which component of the agent's stochastic discount factor, `m_t^i(ω^t)`, uniquely captures their attitude towards ambiguity?", "model_evaluation_result": {"model_raw_response": "Looking at the first-order condition:\n\n```latex\n\\beta^{t}u_{i}^{\\prime}(c_{t}^{i}(\\omega^{t}))\\mathbb{E}_{\\mu^{i}}\\{\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})\\}=\\lambda(i)q_{t}(\\omega^{t})\n```\n\nLet me analyze each component:\n\nA) `β^t` - This is the standard discount factor that appears in any intertemporal optimization problem, regardless of ambiguity preferences.\n\nB) `u_i'(c_t^i(ω^t))` - This is the marginal utility of consumption, which appears in standard expected utility models without ambiguity.\n\nC) `\\mathbb{E}_{\\mu^{i}}\\{\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})\\}` - This term contains:\n   - `μ^i`: The second-order belief over models (key feature of ambiguity)\n   - `φ_i'(·)`: The derivative of the ambiguity attitude function (captures ambiguity aversion)\n   - The expectation structure that reflects uncertainty about which probability model is correct\n\nD) `λ(i)` - This is the Lagrange multiplier on the budget constraint, standard in any constrained optimization.\n\nThe term in option C is what distinguishes smooth ambiguity preferences from standard expected utility. It captures both the agent's uncertainty about probability models (through `μ^i`) and their attitude toward this ambiguity (through `φ_i'`). Without ambiguity, this would collapse to a simple probability weight.\n\nC", "model_answer": "A", "error": null}}
{"ID": 237, "Question": "### Background\n\nThis problem examines the static, one-period microfoundations of the model, focusing on how different types of \"transacting capital\" determine production outcomes, sorting across markets, and overall economic efficiency. The core of the model is a symmetric holdup problem where contract enforcement, and thus investment, depends on an individual's skills and their trading partner.\n\n### Data / Model Specification\n\n**1. Agents and Locations:** The economy has two types of agents: those with local skills (L-capital) and those with market skills (M-capital). They can trade in a local \"village\" or an anonymous \"market\".\n\n**2. Production and Payoffs:** Any two matched agents can form a partnership. Each can make a relationship-specific investment at cost `k` to increase output by `y > k`. The effective price of output is 1 in the village and `θ` in the market. We assume `θ > 1`.\n\n**3. Contract Enforcement:**\n- In the market, an agent with M-capital can enforce contracts, preventing holdup. An agent with L-capital cannot.\n- In the village, an agent with L-capital can enforce contracts. An agent with M-capital cannot.\n\n**4. Bargaining and Holdup:** When contracts cannot be enforced, post-investment surplus is split equally (Nash Bargaining). The parameters are such that holdup is a binding constraint: `θy < (4/3)k`, which implies that an agent will not invest if they expect to be held up.\n\n### Question\n\nL-agents are indifferent between trading in the village (payoff `y-k`) and the market when the fraction of M-traders in the market is `x_0`. What is the correct expression for `x_0`?", "Options": {"A": "```latex\n\\frac{y-k}{\\theta y - k}\n```", "B": "```latex\n\\frac{2(y-k)}{\\theta y - k}\n```", "C": "```latex\n\\frac{2(y-k)}{\\theta y + k}\n```", "D": "```latex\n\\frac{\\theta y - k}{y-k}\n```"}, "Answer": "B", "pi_justification": "This item tests the student's ability to set up and solve the equilibrium indifference condition that governs agent sorting. It uses a Computational Judgment strategy. The correct answer requires correctly identifying the payoffs for an L-agent in both locations and solving for the equilibrium market composition. Distractors are based on plausible errors: omitting the surplus-sharing factor (A), using an incorrect payoff term (C), and a common algebraic slip (D).", "qid": "237", "question": "### Background\n\nThis problem examines the static, one-period microfoundations of the model, focusing on how different types of \"transacting capital\" determine production outcomes, sorting across markets, and overall economic efficiency. The core of the model is a symmetric holdup problem where contract enforcement, and thus investment, depends on an individual's skills and their trading partner.\n\n### Data / Model Specification\n\n**1. Agents and Locations:** The economy has two types of agents: those with local skills (L-capital) and those with market skills (M-capital). They can trade in a local \"village\" or an anonymous \"market\".\n\n**2. Production and Payoffs:** Any two matched agents can form a partnership. Each can make a relationship-specific investment at cost `k` to increase output by `y > k`. The effective price of output is 1 in the village and `θ` in the market. We assume `θ > 1`.\n\n**3. Contract Enforcement:**\n- In the market, an agent with M-capital can enforce contracts, preventing holdup. An agent with L-capital cannot.\n- In the village, an agent with L-capital can enforce contracts. An agent with M-capital cannot.\n\n**4. Bargaining and Holdup:** When contracts cannot be enforced, post-investment surplus is split equally (Nash Bargaining). The parameters are such that holdup is a binding constraint: `θy < (4/3)k`, which implies that an agent will not invest if they expect to be held up.\n\n### Question\n\nL-agents are indifferent between trading in the village (payoff `y-k`) and the market when the fraction of M-traders in the market is `x_0`. What is the correct expression for `x_0`?", "model_evaluation_result": {"model_raw_response": "I need to analyze the payoffs for L-agents in both locations to find when they're indifferent.\n\n**L-agent's payoff in the village:**\n- L-agents can enforce contracts in the village\n- Both agents invest (since contracts are enforceable)\n- Payoff = output - investment cost = y - k\n\n**L-agent's payoff in the market:**\n- L-agents cannot enforce contracts in the market\n- They will be matched with either M-agents (fraction x₀) or other L-agents (fraction 1-x₀)\n\nWhen matched with M-agent:\n- M-agent can enforce contracts\n- Both invest\n- Output is θy (market price premium)\n- Surplus split equally: θy/2 each, minus investment cost k\n- Payoff = θy/2 - k\n\nWhen matched with another L-agent:\n- Neither can enforce contracts\n- Due to holdup problem, neither invests (given θy < (4/3)k constraint)\n- Payoff = 0\n\n**Expected payoff in market:**\nE[payoff] = x₀(θy/2 - k) + (1-x₀)(0) = x₀(θy/2 - k)\n\n**Indifference condition:**\ny - k = x₀(θy/2 - k)\n\nSolving for x₀:\nx₀ = (y - k)/(θy/2 - k)\nx₀ = (y - k)/((θy - 2k)/2)\nx₀ = 2(y - k)/(θy - 2k)\n\nWait, let me reconsider the constraint. The problem states θy < (4/3)k, which means θy - k < k/3, so θy - 2k < -2k/3 < 0.\n\nThis suggests I need to be more careful about the holdup constraint and when agents invest.\n\nActually, let me reconsider the matching payoffs. When an L-agent matches with an M-agent in the market, the M-agent can enforce the contract, so both invest. The total surplus is θy, split equally, so each gets θy/2.", "model_answer": "I need to analyze the payoffs for L-agents in both locations to find when they're indifferent.\n\n**L-agent's payoff in the village:**\n- L-agents can enforce contracts in the village\n- Both agents invest (since contracts are enforceable)\n- Payoff = output - investment cost = y - k\n\n**L-agent's payoff in the market:**\n- L-agents cannot enforce contracts in the market\n- They will be matched with either M-agents (fraction x₀) or other L-agents (fraction 1-x₀)\n\nWhen matched with M-agent:\n- M-agent can enforce contracts\n- Both invest\n- Output is θy (market price premium)\n- Surplus split equally: θy/2 each, minus investment cost k\n- Payoff = θy/2 - k\n\nWhen matched with another L-agent:\n- Neither can enforce contracts\n- Due to holdup problem, neither invests (given θy < (4/3)k constraint)\n- Payoff = 0\n\n**Expected payoff in market:**\nE[payoff] = x₀(θy/2 - k) + (1-x₀)(0) = x₀(θy/2 - k)\n\n**Indifference condition:**\ny - k = x₀(θy/2 - k)\n\nSolving for x₀:\nx₀ = (y - k)/(θy/2 - k)\nx₀ = (y - k)/((θy - 2k)/2)\nx₀ = 2(y - k)/(θy - 2k)\n\nWait, let me reconsider the constraint. The problem states θy < (4/3)k, which means θy - k < k/3, so θy - 2k < -2k/3 < 0.\n\nThis suggests I need to be more careful about the holdup constraint and when agents invest.\n\nActually, let me reconsider the matching payoffs. When an L-agent matches with an M-agent in the market, the M-agent can enforce the contract, so both invest. The total surplus is θy, split equally, so each gets θy/2.", "error": "Warning: Could not extract single choice letter from: I need to analyze the payoffs for L-agents in both..."}}
{"ID": 274, "Question": "### Background\n\n**Research Question.** This problem analyzes the full information benchmark for managing a global commons to determine the socially optimal policy and quantify the individual incentive to free-ride.\n\n**Setting / Institutional Environment.** In an infinite-horizon model with `n` countries, an International Agency (IA) acts as a social planner. Country types `θ_i`, representing the preference for consumption versus conservation, are common knowledge. Consumption is modeled as an extraction rate `e_it` such that `c_it = e_it * ω_t`.\n\n### Data / Model Specification\n\nThe IA's problem is to choose an extraction path `{e_it}` to maximize `Σ_i U_i`. The solution yields the **socially optimal (first-best) stationary extraction rate** for country `i`:\n\n```latex\ne_{i}^{*}(\\theta)=\\frac{\\theta_{i}(1-\\delta\\gamma)}{n} \\quad \\text{(Eq. 1)}\n```\n\nSeparately, a single country `i` choosing its extraction rate to maximize its own utility, taking others' rates `E_{-it}` as given, will follow its **best-response function**:\n\n```latex\nBR_{i}(\\theta_{i},\\mathcal{E}_{-i t})=\\theta_{i}(1-\\delta\\gamma)(1-\\mathcal{E}_{-i t}) \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAssume all countries `j ≠ i` adhere to the socially optimal quota prescribed by Eq. (1). Given country `i`'s best-response function in Eq. (2), what is its optimal unilateral extraction rate, `e_i^{dev}`, and how does it compare to its socially optimal quota, `e_i*`?", "Options": {"A": "`e_i^{dev} = θ_i(1-δγ)(1 - (1-δγ)/n * Σ_{j≠i} θ_j)`, which is strictly greater than `e_i*`.", "B": "`e_i^{dev} = θ_i(1-δγ)(1 - (1-δγ)/n * Σ_{j} θ_j)`, which is strictly less than `e_i*`.", "C": "`e_i^{dev} = θ_i(1-δγ)(1 - (1-δγ)/n * Σ_{j≠i} θ_j)`, which is exactly equal to `e_i*`.", "D": "`e_i^{dev} = θ_i(1-δγ) / (1 + Σ_{j≠i} θ_j)`, which is strictly greater than `e_i*`."}, "Answer": "A", "pi_justification": "This item assesses the core quantitative insight of the full information benchmark: the existence and magnitude of the free-rider problem. It uses a Computational Judgment strategy, requiring students to perform a multi-step algebraic derivation (calculating the aggregate behavior of others and then finding the best response to it) to arrive at a correct formula and a qualitative comparison. Distractors are based on plausible algebraic errors (using the wrong aggregate sum in B) and conceptual misunderstandings of the free-rider incentive (concluding equality in C or using an incorrect formula in D).", "qid": "274", "question": "### Background\n\n**Research Question.** This problem analyzes the full information benchmark for managing a global commons to determine the socially optimal policy and quantify the individual incentive to free-ride.\n\n**Setting / Institutional Environment.** In an infinite-horizon model with `n` countries, an International Agency (IA) acts as a social planner. Country types `θ_i`, representing the preference for consumption versus conservation, are common knowledge. Consumption is modeled as an extraction rate `e_it` such that `c_it = e_it * ω_t`.\n\n### Data / Model Specification\n\nThe IA's problem is to choose an extraction path `{e_it}` to maximize `Σ_i U_i`. The solution yields the **socially optimal (first-best) stationary extraction rate** for country `i`:\n\n```latex\ne_{i}^{*}(\\theta)=\\frac{\\theta_{i}(1-\\delta\\gamma)}{n} \\quad \\text{(Eq. 1)}\n```\n\nSeparately, a single country `i` choosing its extraction rate to maximize its own utility, taking others' rates `E_{-it}` as given, will follow its **best-response function**:\n\n```latex\nBR_{i}(\\theta_{i},\\mathcal{E}_{-i t})=\\theta_{i}(1-\\delta\\gamma)(1-\\mathcal{E}_{-i t}) \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nAssume all countries `j ≠ i` adhere to the socially optimal quota prescribed by Eq. (1). Given country `i`'s best-response function in Eq. (2), what is its optimal unilateral extraction rate, `e_i^{dev}`, and how does it compare to its socially optimal quota, `e_i*`?", "model_evaluation_result": {"model_raw_response": "I need to find country i's optimal unilateral extraction rate when all other countries follow their socially optimal quotas, then compare it to the socially optimal rate.\n\nFirst, let me establish what we know:\n- Socially optimal rate for country i: $e_i^* = \\frac{\\theta_i(1-\\delta\\gamma)}{n}$\n- Country i's best response: $BR_i(\\theta_i, \\mathcal{E}_{-it}) = \\theta_i(1-\\delta\\gamma)(1-\\mathcal{E}_{-it})$\n- All other countries j ≠ i follow their socially optimal quotas: $e_j^* = \\frac{\\theta_j(1-\\delta\\gamma)}{n}$\n\nNow I need to find $\\mathcal{E}_{-it}$, which is the sum of extraction rates of all other countries:\n\n$\\mathcal{E}_{-it} = \\sum_{j \\neq i} e_j^* = \\sum_{j \\neq i} \\frac{\\theta_j(1-\\delta\\gamma)}{n} = \\frac{(1-\\delta\\gamma)}{n} \\sum_{j \\neq i} \\theta_j$\n\nSubstituting this into country i's best response function:\n\n$e_i^{dev} = \\theta_i(1-\\delta\\gamma)\\left(1 - \\frac{(1-\\delta\\gamma)}{n} \\sum_{j \\neq i} \\theta_j\\right)$\n\nThis matches the form in option A: $e_i^{dev} = \\theta_i(1-\\delta\\gamma)\\left(1 - \\frac{(1-\\delta\\gamma)}{n} \\sum_{j \\neq i} \\theta_j\\right)$\n\nNow I need to compare this to $e_i^*$:\n\n$e_i^{dev} = \\theta_i(1-\\delta\\gamma) - \\theta_i(1-\\delta\\gamma) \\cdot \\frac{(1-\\delta\\gamma)}{n} \\sum_{j", "model_answer": "A", "error": null}}
{"ID": 360, "Question": "### Background\n\n**Research Question.** This problem requires you to analyze a re-examination of a field experiment that estimated consumer valuation for a new product packaging (vacuum skin) for beef steaks relative to a standard packaging (styrofoam). The analysis tests for the presence of two methodological confounds: field-price censoring and affiliated beliefs.\n\n**Setting / Institutional Environment.** In the original experiment, subjects submitted bids for beef steaks in a series of auctions. A key institutional detail is that subjects were explicitly told that the standard steak \"currently retail for about $6.00 a pound at major supermarkets.\" The experiment included three information treatments regarding the new vacuum-packed product: a baseline with no extra information, an \"Info\" treatment describing its virtues, and a \"Demo\" treatment that included a demonstration. After each of the later auction periods, the market-clearing price was announced to all participants.\n\n**Variables & Parameters.**\n*   `Bid`: Subject's bid in dollars per pound (dependent variable).\n*   `Vacuum`: Indicator variable; 1 if the product is the new vacuum skin package, 0 for standard styrofoam.\n*   `Info`: Indicator variable; 1 if subjects received the \"information\" treatment.\n*   `Demo`: Indicator variable; 1 if subjects received the \"demonstration\" treatment.\n*   `VacInfo`: Interaction term `Vacuum` × `Info`.\n*   `VacDemo`: Interaction term `Vacuum` × `Demo`.\n*   `priceL1`: The market-clearing (selling) price from the previous auction period, `t-1`.\n\n### Data / Model Specification\n\nTable 1 and Table 2 below present results from the re-analysis. Panel A of Table 1 uses a Panel Generalized Least Squares (GLS) model, which ignores censoring and affiliation effects. Panel C of Table 1 uses a Panel Tobit model, which accounts for the censoring of bids at the known $6.00 field price and includes controls for affiliation effects (market prices). The Panel Tobit model is considered the most appropriate specification.\n\n**Table 1. Estimated effects of experimental treatments on bids for meat**\n(Estimates obtained using random effects specification)\n\n| Treatment | Coef. | Std. err. | Z | P>|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| **A. Panel GLS** | | | | | | |\n| Vacuum | -.0549387 | .0333573 | -1.65 | 0.100 | -.1203179 | .0104404 |\n| Info | .1824116 | .0987915 | 1.85 | 0.065 | -.0112161 | .3760393 |\n| VacInfo | .449179 | .0470355 | 9.55 | 0.000 | .3569912 | .5413669 |\n| VacDemo | .0739316 | .0468498 | 1.58 | 0.115 | -.0178924 | .1657555 |\n| **C. Panel Tobit Model with market price effects** | | | | | | |\n| Vacuum | -.0440431 | .0346063 | -1.27 | 0.203 | -.1118703 | .023784 |\n| Info | -.2454266 | .0623487 | -3.94 | 0.000 | -.3676278 | -.1232254 |\n| VacInfo | .4198155 | .0488044 | 8.60 | 0.000 | .3241607 | .5154703 |\n| VacDemo | .0686025 | .0484393 | 1.42 | 0.157 | -.0263367 | .1635418 |\n\n**Table 2. Estimated effects of market prices on bids for meat**\n(Estimates obtained using Panel Tobit specification with random effects)\n\n| Variable | Coef. | Std. err. | Z | P >|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| tbid_vs | .3996299 | .0247609 | 16.14 | 0.000 | .3510994 | .4481605 |\n| tbid-st | .2883476 | .0249211 | 11.57 | 0.000 | .2395032 | .337192 |\n| priceL1 | .3555019 | .0172282 | 20.63 | 0.000 | .3217353 | .3892686 |\n\nAccording to the results in Table 2, what is the most accurate interpretation of the coefficient on `priceL1`?", "Options": {"A": "A $1 increase in the previous auction's selling price is associated with a statistically significant increase of about $0.36 in a subject's current bid, supporting the hypothesis of affiliated beliefs.", "B": "The previous auction's selling price has no statistically significant effect on current bids, suggesting bids are independent across rounds.", "C": "Subjects' bids converge to the true market price of $6.00 over time, as shown by the positive coefficient.", "D": "The model likely suffers from multicollinearity because the lagged price is correlated with the average trial bids."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the user's ability to interpret a coefficient that serves as a direct test of a key theoretical concept (affiliated beliefs). It uses a Conceptual Judgment strategy. Distractors include the conceptual opposite (B), a plausible but overly strong interpretation of the mechanism (C), and a true but irrelevant statistical artifact (D).", "qid": "360", "question": "### Background\n\n**Research Question.** This problem requires you to analyze a re-examination of a field experiment that estimated consumer valuation for a new product packaging (vacuum skin) for beef steaks relative to a standard packaging (styrofoam). The analysis tests for the presence of two methodological confounds: field-price censoring and affiliated beliefs.\n\n**Setting / Institutional Environment.** In the original experiment, subjects submitted bids for beef steaks in a series of auctions. A key institutional detail is that subjects were explicitly told that the standard steak \"currently retail for about $6.00 a pound at major supermarkets.\" The experiment included three information treatments regarding the new vacuum-packed product: a baseline with no extra information, an \"Info\" treatment describing its virtues, and a \"Demo\" treatment that included a demonstration. After each of the later auction periods, the market-clearing price was announced to all participants.\n\n**Variables & Parameters.**\n*   `Bid`: Subject's bid in dollars per pound (dependent variable).\n*   `Vacuum`: Indicator variable; 1 if the product is the new vacuum skin package, 0 for standard styrofoam.\n*   `Info`: Indicator variable; 1 if subjects received the \"information\" treatment.\n*   `Demo`: Indicator variable; 1 if subjects received the \"demonstration\" treatment.\n*   `VacInfo`: Interaction term `Vacuum` × `Info`.\n*   `VacDemo`: Interaction term `Vacuum` × `Demo`.\n*   `priceL1`: The market-clearing (selling) price from the previous auction period, `t-1`.\n\n### Data / Model Specification\n\nTable 1 and Table 2 below present results from the re-analysis. Panel A of Table 1 uses a Panel Generalized Least Squares (GLS) model, which ignores censoring and affiliation effects. Panel C of Table 1 uses a Panel Tobit model, which accounts for the censoring of bids at the known $6.00 field price and includes controls for affiliation effects (market prices). The Panel Tobit model is considered the most appropriate specification.\n\n**Table 1. Estimated effects of experimental treatments on bids for meat**\n(Estimates obtained using random effects specification)\n\n| Treatment | Coef. | Std. err. | Z | P>|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| **A. Panel GLS** | | | | | | |\n| Vacuum | -.0549387 | .0333573 | -1.65 | 0.100 | -.1203179 | .0104404 |\n| Info | .1824116 | .0987915 | 1.85 | 0.065 | -.0112161 | .3760393 |\n| VacInfo | .449179 | .0470355 | 9.55 | 0.000 | .3569912 | .5413669 |\n| VacDemo | .0739316 | .0468498 | 1.58 | 0.115 | -.0178924 | .1657555 |\n| **C. Panel Tobit Model with market price effects** | | | | | | |\n| Vacuum | -.0440431 | .0346063 | -1.27 | 0.203 | -.1118703 | .023784 |\n| Info | -.2454266 | .0623487 | -3.94 | 0.000 | -.3676278 | -.1232254 |\n| VacInfo | .4198155 | .0488044 | 8.60 | 0.000 | .3241607 | .5154703 |\n| VacDemo | .0686025 | .0484393 | 1.42 | 0.157 | -.0263367 | .1635418 |\n\n**Table 2. Estimated effects of market prices on bids for meat**\n(Estimates obtained using Panel Tobit specification with random effects)\n\n| Variable | Coef. | Std. err. | Z | P >|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| tbid_vs | .3996299 | .0247609 | 16.14 | 0.000 | .3510994 | .4481605 |\n| tbid-st | .2883476 | .0249211 | 11.57 | 0.000 | .2395032 | .337192 |\n| priceL1 | .3555019 | .0172282 | 20.63 | 0.000 | .3217353 | .3892686 |\n\nAccording to the results in Table 2, what is the most accurate interpretation of the coefficient on `priceL1`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 256, "Question": "### Background\n\n**Research Question.** This problem explores how to apply insights from a finite mixture model to develop a targeted marketing strategy.\n\n**Setting / Institutional Environment.** A firm analyzes cross-sectional purchase data using a finite mixture Poisson regression model. The goal is to understand how different consumer segments respond to price changes.\n\n---\n\n### Data / Model Specification\n\nSuppose a firm estimates a two-segment model where the covariate `z_{nt}` is the log of price. They find the price coefficient for Segment 1 (`π_1 = 0.15`) is `β_{price|1} = -2.5`, while for Segment 2 (`π_2 = 0.85`) it is `β_{price|2} = -0.1`.\n\n---\n\nBased on these estimates, what is the most efficient pricing strategy?", "Options": {"A": "Offer targeted promotions or coupons to consumers likely belonging to Segment 1.", "B": "Implement a store-wide price cut to increase sales from all consumers.", "C": "Focus on non-price promotions, as the majority of consumers are price-insensitive.", "D": "Offer targeted price increases to consumers in Segment 2 to maximize revenue."}, "Answer": "A", "pi_justification": "Assesses the ability to translate model parameters into actionable business strategy. This is a Scenario Application item. The correct answer identifies the efficiency of targeting. Distractors represent common strategic errors: an inefficient universal policy (B, Conceptual Error), ignoring a profitable niche despite its small size (C, Almost Right), or a plausible but suboptimal alternative (D).", "qid": "256", "question": "### Background\n\n**Research Question.** This problem explores how to apply insights from a finite mixture model to develop a targeted marketing strategy.\n\n**Setting / Institutional Environment.** A firm analyzes cross-sectional purchase data using a finite mixture Poisson regression model. The goal is to understand how different consumer segments respond to price changes.\n\n---\n\n### Data / Model Specification\n\nSuppose a firm estimates a two-segment model where the covariate `z_{nt}` is the log of price. They find the price coefficient for Segment 1 (`π_1 = 0.15`) is `β_{price|1} = -2.5`, while for Segment 2 (`π_2 = 0.85`) it is `β_{price|2} = -0.1`.\n\n---\n\nBased on these estimates, what is the most efficient pricing strategy?", "model_evaluation_result": {"model_raw_response": "Looking at this finite mixture model, I need to analyze the price sensitivity of each segment and their relative sizes.\n\nGiven information:\n- Segment 1: π₁ = 0.15 (15% of consumers), β_price|1 = -2.5\n- Segment 2: π₂ = 0.85 (85% of consumers), β_price|2 = -0.1\n\nThe price coefficients represent elasticities in this log-price specification:\n- Segment 1 is highly price-sensitive (β = -2.5)\n- Segment 2 is relatively price-insensitive (β = -0.1)\n\nEvaluating each option:\n\nA) Targeted promotions to Segment 1 makes economic sense because this small but highly price-sensitive group will respond strongly to price reductions, generating significant additional purchases.\n\nB) Store-wide price cuts would benefit the price-sensitive Segment 1, but would unnecessarily reduce margins on the large Segment 2 (85% of consumers) who don't respond much to price changes.\n\nC) This ignores the opportunity to capture significant response from the highly price-sensitive Segment 1, even though they're a minority.\n\nD) Price increases to Segment 2 would generate minimal demand reduction but also minimal revenue gains due to their low price sensitivity, while missing the opportunity to activate Segment 1.\n\nThe most efficient strategy is to target the segment where price changes have the highest impact per dollar of price reduction - the highly elastic Segment 1.\n\nA", "model_answer": "A", "error": null}}
{"ID": 275, "Question": "### Background\n\n**Research Question.** This problem analyzes the full information benchmark for managing a global commons to determine the socially optimal policy and the mechanism required to implement it.\n\n**Setting / Institutional Environment.** In an infinite-horizon model with `n` countries, an International Agency (IA) acts as a social planner. Country types `θ_i` are common knowledge. The model features a logarithmic utility function, `θ_i log(c_it) + (1-θ_i) log(ω_t - C_t)`. The paper proves that the socially optimal quota can be implemented as a Subgame Perfect Equilibrium (SPE) using a system of graduated punishments, despite a proven incentive for each country to unilaterally increase its extraction rate (the free-rider problem).\n\n### Question\n\nWhat is the critical feature of the benchmark model that makes the implementation of the socially optimal quota via punishments possible for *any* discount factor `δ > 0`?", "Options": {"A": "The logarithmic utility function implies that payoffs are unbounded below, allowing for arbitrarily severe punishments that deter any finite gain from deviation.", "B": "Countries are assumed to be sufficiently patient (a high discount factor `δ`), making them value future cooperation more than a one-shot deviation.", "C": "The resource stock is assumed to be renewable (`γ > 1`), ensuring that punishments are not permanent and thus are credible to implement.", "D": "The number of countries `n` is small, which limits the magnitude of the free-rider problem and makes cooperation easier to sustain."}, "Answer": "A", "pi_justification": "This item tests the student's understanding of the specific implementation mechanism used in the paper's benchmark model and why it is robust to the discount factor. The question requires students to identify the key model assumption (unbounded payoffs) that enables the punishment strategy. The primary distractor (B) is designed to be highly attractive by referencing the logic of a more standard 'Nash Reversion' strategy, which the paper explicitly contrasts, thereby testing for a deep and specific understanding of the paper's argument.", "qid": "275", "question": "### Background\n\n**Research Question.** This problem analyzes the full information benchmark for managing a global commons to determine the socially optimal policy and the mechanism required to implement it.\n\n**Setting / Institutional Environment.** In an infinite-horizon model with `n` countries, an International Agency (IA) acts as a social planner. Country types `θ_i` are common knowledge. The model features a logarithmic utility function, `θ_i log(c_it) + (1-θ_i) log(ω_t - C_t)`. The paper proves that the socially optimal quota can be implemented as a Subgame Perfect Equilibrium (SPE) using a system of graduated punishments, despite a proven incentive for each country to unilaterally increase its extraction rate (the free-rider problem).\n\n### Question\n\nWhat is the critical feature of the benchmark model that makes the implementation of the socially optimal quota via punishments possible for *any* discount factor `δ > 0`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 361, "Question": "### Background\n\n**Research Question.** This problem requires you to analyze a re-examination of a field experiment that estimated consumer valuation for a new product packaging (vacuum skin) for beef steaks relative to a standard packaging (styrofoam). The analysis tests for the presence of two methodological confounds: field-price censoring and affiliated beliefs.\n\n**Setting / Institutional Environment.** In the original experiment, subjects submitted bids for beef steaks in a series of auctions. A key institutional detail is that subjects were explicitly told that the standard steak \"currently retail for about $6.00 a pound at major supermarkets.\" The experiment included three information treatments regarding the new vacuum-packed product: a baseline with no extra information, an \"Info\" treatment describing its virtues, and a \"Demo\" treatment that included a demonstration. After each of the later auction periods, the market-clearing price was announced to all participants.\n\n**Variables & Parameters.**\n*   `Bid`: Subject's bid in dollars per pound (dependent variable).\n*   `Vacuum`: Indicator variable; 1 if the product is the new vacuum skin package, 0 for standard styrofoam.\n*   `Info`: Indicator variable; 1 if subjects received the \"information\" treatment.\n*   `Demo`: Indicator variable; 1 if subjects received the \"demonstration\" treatment.\n*   `VacInfo`: Interaction term `Vacuum` × `Info`.\n*   `VacDemo`: Interaction term `Vacuum` × `Demo`.\n*   `priceL1`: The market-clearing (selling) price from the previous auction period, `t-1`.\n\n### Data / Model Specification\n\nTable 1 below presents results from the re-analysis. Panel A uses a Panel Generalized Least Squares (GLS) model, which ignores censoring and affiliation effects. Panel C uses a Panel Tobit model, which accounts for the censoring of bids at the known $6.00 field price and includes controls for affiliation effects (market prices). The Panel Tobit model is considered the most appropriate specification.\n\n**Table 1. Estimated effects of experimental treatments on bids for meat**\n(Estimates obtained using random effects specification)\n\n| Treatment | Coef. | Std. err. | Z | P>|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| **A. Panel GLS** | | | | | | |\n| Vacuum | -.0549387 | .0333573 | -1.65 | 0.100 | -.1203179 | .0104404 |\n| Info | .1824116 | .0987915 | 1.85 | 0.065 | -.0112161 | .3760393 |\n| VacInfo | .449179 | .0470355 | 9.55 | 0.000 | .3569912 | .5413669 |\n| VacDemo | .0739316 | .0468498 | 1.58 | 0.115 | -.0178924 | .1657555 |\n| **C. Panel Tobit Model with market price effects** | | | | | | |\n| Vacuum | -.0440431 | .0346063 | -1.27 | 0.203 | -.1118703 | .023784 |\n| Info | -.2454266 | .0623487 | -3.94 | 0.000 | -.3676278 | -.1232254 |\n| VacInfo | .4198155 | .0488044 | 8.60 | 0.000 | .3241607 | .5154703 |\n| VacDemo | .0686025 | .0484393 | 1.42 | 0.157 | -.0263367 | .1635418 |\n\nThe re-analysis prefers the Panel Tobit model (Panel C) over the Panel GLS model (Panel A). What feature of the experimental setting provides the strongest rationale for this choice?", "Options": {"A": "Subjects were explicitly told that the steaks retailed for about $6.00, creating an upper bound that censors bids.", "B": "The data is a panel dataset with repeated observations from the same subjects.", "C": "The dependent variable (bids) is not normally distributed, as seen in histograms of the data.", "D": "Subjects participated in both trial and binding auction periods."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the ability to connect a specific feature of the experimental design to the choice of an appropriate econometric model. It uses a Reverse-Reasoning strategy. Distractors are designed to be true statements about the experiment that are not the direct cause for choosing Tobit over GLS (B, D) or a related statistical symptom rather than the root economic cause (C).", "qid": "361", "question": "### Background\n\n**Research Question.** This problem requires you to analyze a re-examination of a field experiment that estimated consumer valuation for a new product packaging (vacuum skin) for beef steaks relative to a standard packaging (styrofoam). The analysis tests for the presence of two methodological confounds: field-price censoring and affiliated beliefs.\n\n**Setting / Institutional Environment.** In the original experiment, subjects submitted bids for beef steaks in a series of auctions. A key institutional detail is that subjects were explicitly told that the standard steak \"currently retail for about $6.00 a pound at major supermarkets.\" The experiment included three information treatments regarding the new vacuum-packed product: a baseline with no extra information, an \"Info\" treatment describing its virtues, and a \"Demo\" treatment that included a demonstration. After each of the later auction periods, the market-clearing price was announced to all participants.\n\n**Variables & Parameters.**\n*   `Bid`: Subject's bid in dollars per pound (dependent variable).\n*   `Vacuum`: Indicator variable; 1 if the product is the new vacuum skin package, 0 for standard styrofoam.\n*   `Info`: Indicator variable; 1 if subjects received the \"information\" treatment.\n*   `Demo`: Indicator variable; 1 if subjects received the \"demonstration\" treatment.\n*   `VacInfo`: Interaction term `Vacuum` × `Info`.\n*   `VacDemo`: Interaction term `Vacuum` × `Demo`.\n*   `priceL1`: The market-clearing (selling) price from the previous auction period, `t-1`.\n\n### Data / Model Specification\n\nTable 1 below presents results from the re-analysis. Panel A uses a Panel Generalized Least Squares (GLS) model, which ignores censoring and affiliation effects. Panel C uses a Panel Tobit model, which accounts for the censoring of bids at the known $6.00 field price and includes controls for affiliation effects (market prices). The Panel Tobit model is considered the most appropriate specification.\n\n**Table 1. Estimated effects of experimental treatments on bids for meat**\n(Estimates obtained using random effects specification)\n\n| Treatment | Coef. | Std. err. | Z | P>|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| **A. Panel GLS** | | | | | | |\n| Vacuum | -.0549387 | .0333573 | -1.65 | 0.100 | -.1203179 | .0104404 |\n| Info | .1824116 | .0987915 | 1.85 | 0.065 | -.0112161 | .3760393 |\n| VacInfo | .449179 | .0470355 | 9.55 | 0.000 | .3569912 | .5413669 |\n| VacDemo | .0739316 | .0468498 | 1.58 | 0.115 | -.0178924 | .1657555 |\n| **C. Panel Tobit Model with market price effects** | | | | | | |\n| Vacuum | -.0440431 | .0346063 | -1.27 | 0.203 | -.1118703 | .023784 |\n| Info | -.2454266 | .0623487 | -3.94 | 0.000 | -.3676278 | -.1232254 |\n| VacInfo | .4198155 | .0488044 | 8.60 | 0.000 | .3241607 | .5154703 |\n| VacDemo | .0686025 | .0484393 | 1.42 | 0.157 | -.0263367 | .1635418 |\n\nThe re-analysis prefers the Panel Tobit model (Panel C) over the Panel GLS model (Panel A). What feature of the experimental setting provides the strongest rationale for this choice?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 364, "Question": "## Background\n\n**Research Question.** This problem investigates the dynamic causal relationship between sectoral reallocation shocks (CSV) and the aggregate unemployment rate (\\(U_t\\)).\n\n## Data / Model Specification\n\nTwo specifications are used to model the unemployment rate:\n\n**Level Specification:**\n```latex\nU_{t} = \\beta_{0} + \\sum_{k=0}^{15} \\beta_{1k} CSV_{t-k} + \\epsilon_{t} \\quad \\text{(Eq. (1))}\n```\n**Lag Specification (Autoregressive Distributed Lag):**\n```latex\nU_{t} = \\beta_{0} + \\sum_{k=0}^{7} \\beta_{1k} CSV_{t-k} + \\sum_{k=1}^{4} \\rho_{k} U_{t-k} + \\epsilon_{t} \\quad \\text{(Eq. (2))}\n```\n\n## Question\n\nWhat is the primary econometric purpose of including the lagged unemployment terms (\\(\\sum \\rho_{k} U_{t-k}\\)) in Eq. (2), and how does this change the interpretation of the \\(\\beta_{1k}\\) coefficients compared to Eq. (1)?", "Options": {"A": "To account for serial correlation in unemployment; this changes the \\(\\beta_{1k}\\) to represent the direct impact of a past CSV shock, distinct from the shock's propagation through unemployment's own persistence.", "B": "To ensure the model is stationary; this makes the \\(\\beta_{1k}\\) coefficients represent the total long-run effect of a CSV shock on unemployment.", "C": "To control for omitted variables that are correlated with past unemployment; this makes the \\(\\beta_{1k}\\) coefficients unbiased estimates of the immediate, contemporaneous effect of CSV.", "D": "To allow for hysteresis in unemployment; this ensures that the \\(\\beta_{1k}\\) coefficients capture only the permanent effects of CSV shocks."}, "Answer": "A", "pi_justification": "Assessment Target: Assesses understanding of dynamic time-series model specification (ARDL models). Strategy: Conceptual Judgment. Distractor Logic: The correct answer (A) correctly identifies both the purpose (serial correlation) and the consequence (interpretation of coefficients as direct effects). Distractor (B) confuses ARDL with differencing for stationarity and incorrectly claims the coefficients are total long-run effects. Distractor (C) correctly identifies a potential benefit (OVB control) but misinterprets the coefficients as only contemporaneous. Distractor (D) introduces a plausible but distinct economic concept (hysteresis) and misinterprets the coefficients' meaning.", "qid": "364", "question": "## Background\n\n**Research Question.** This problem investigates the dynamic causal relationship between sectoral reallocation shocks (CSV) and the aggregate unemployment rate (\\(U_t\\)).\n\n## Data / Model Specification\n\nTwo specifications are used to model the unemployment rate:\n\n**Level Specification:**\n```latex\nU_{t} = \\beta_{0} + \\sum_{k=0}^{15} \\beta_{1k} CSV_{t-k} + \\epsilon_{t} \\quad \\text{(Eq. (1))}\n```\n**Lag Specification (Autoregressive Distributed Lag):**\n```latex\nU_{t} = \\beta_{0} + \\sum_{k=0}^{7} \\beta_{1k} CSV_{t-k} + \\sum_{k=1}^{4} \\rho_{k} U_{t-k} + \\epsilon_{t} \\quad \\text{(Eq. (2))}\n```\n\n## Question\n\nWhat is the primary econometric purpose of including the lagged unemployment terms (\\(\\sum \\rho_{k} U_{t-k}\\)) in Eq. (2), and how does this change the interpretation of the \\(\\beta_{1k}\\) coefficients compared to Eq. (1)?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 371, "Question": "### Background\n\n**Research Question.** This problem explores how an agent's time horizon affects their choice between incremental refinement (intensive search) and breakthrough innovation (extensive search).\n\n**Setting.** We compare the decision of a standard one-period agent with that of a two-period agent who can make two consecutive search decisions. The comparison is made at a point where the one-period agent is exactly indifferent between intensive and extensive search, meaning the expected payoffs in the first period are identical.\n\n### Data / Model Specification\n\nTheorem 6 in the paper shows that under these conditions of indifference, the two-period agent strictly prefers to undertake **extensive search** in the first period.\n\n### Question\n\nWhat is the core economic mechanism that explains why a two-period agent, who has a longer time horizon, strictly prefers extensive search when a one-period agent is indifferent?", "Options": {"A": "Extensive search creates a valuable option to conduct a potentially high-return intensive search in the second period, a future benefit the one-period agent ignores.", "B": "A two-period agent is more willing to undertake the higher risk associated with extensive search because they have a second period to recover from a potential loss.", "C": "The expected direct payoff from extensive search is higher in the second period than the payoff from intensive search, regardless of the first-period outcome.", "D": "The cost `c` of extensive search is paid only in the first period, making any second-period search more profitable and favoring the initially costly option."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.0). The original QA problem's core assessment target—identifying the concept of 'option value'—is highly suitable for a choice format. This question uses a Reverse-Reasoning strategy, asking for the cause of a given outcome. Distractors are designed based on common misconceptions: B (Conceptual Opposite) incorrectly introduces risk aversion into a risk-neutral model; C (Almost Right) misinterprets the payoff structure, as the first-period payoffs are equal by assumption; D (True but Irrelevant) points to a true feature of the cost structure that does not explain the choice between the two paths.", "qid": "371", "question": "### Background\n\n**Research Question.** This problem explores how an agent's time horizon affects their choice between incremental refinement (intensive search) and breakthrough innovation (extensive search).\n\n**Setting.** We compare the decision of a standard one-period agent with that of a two-period agent who can make two consecutive search decisions. The comparison is made at a point where the one-period agent is exactly indifferent between intensive and extensive search, meaning the expected payoffs in the first period are identical.\n\n### Data / Model Specification\n\nTheorem 6 in the paper shows that under these conditions of indifference, the two-period agent strictly prefers to undertake **extensive search** in the first period.\n\n### Question\n\nWhat is the core economic mechanism that explains why a two-period agent, who has a longer time horizon, strictly prefers extensive search when a one-period agent is indifferent?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 265, "Question": "### Background\n\nThe paper uses a calibrated general equilibrium model to simulate the impact of policy changes on India's relative price of investment (RPI). The model's core mechanism links the RPI (`q`) to the domestic price of imported capital (`p^m`) via the relation `q = A(p^m)^(1-η)`, where `A` and `η` are parameters.\n\n### Data / Model Specification\n\nIn 1991, the RPI was elevated due to both a 78% tariff and a binding quantitative restriction (QR) on capital imports. A counterfactual analysis in the paper decomposes the model's predicted fall in RPI during the reform period as follows:\n\n*   The removal of the quantitative restriction (QR) accounts for a 22.5 percentage point fall in the RPI.\n*   The subsequent reduction in tariffs (from 78% to 7%) accounts for an 11 percentage point fall.\n\n### Question\n\nBased on this decomposition, the QR in 1991 created a scarcity rent that made the domestic price of imported capital approximately how many times higher than the tariff-inclusive world price?", "Options": {"A": "3.04 times higher", "B": "2.05 times higher", "C": "1.78 times higher", "D": "0.33 times higher"}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to perform a multi-step calculation to back out an unobserved economic quantity from model outputs. This is a Computational Judgment item. Distractors are based on plausible error paths: C ignores the QR effect, B uses a naive ratio of the effects (22.5/11), and D represents an inversion of the correct answer.", "qid": "265", "question": "### Background\n\nThe paper uses a calibrated general equilibrium model to simulate the impact of policy changes on India's relative price of investment (RPI). The model's core mechanism links the RPI (`q`) to the domestic price of imported capital (`p^m`) via the relation `q = A(p^m)^(1-η)`, where `A` and `η` are parameters.\n\n### Data / Model Specification\n\nIn 1991, the RPI was elevated due to both a 78% tariff and a binding quantitative restriction (QR) on capital imports. A counterfactual analysis in the paper decomposes the model's predicted fall in RPI during the reform period as follows:\n\n*   The removal of the quantitative restriction (QR) accounts for a 22.5 percentage point fall in the RPI.\n*   The subsequent reduction in tariffs (from 78% to 7%) accounts for an 11 percentage point fall.\n\n### Question\n\nBased on this decomposition, the QR in 1991 created a scarcity rent that made the domestic price of imported capital approximately how many times higher than the tariff-inclusive world price?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand how the quantitative restriction (QR) and tariffs affected the domestic price of imported capital.\n\nLet me work through the logic:\n\n1) The QR removal caused a 22.5 percentage point fall in RPI\n2) The tariff reduction (from 78% to 7%) caused an 11 percentage point fall in RPI\n3) I need to find how much higher the domestic price was due to the QR's scarcity rent\n\nThe key insight is that the QR created a scarcity rent that inflated prices beyond what tariffs alone would create.\n\nIf the QR removal led to a 22.5 percentage point fall, this suggests the scarcity rent was substantial. \n\nLet me think about this in terms of price ratios:\n- If removing the QR caused a 22.5 percentage point fall in RPI\n- And the tariff reduction from 78% to 7% caused an 11 percentage point fall\n- The QR effect was about twice as large as the tariff effect\n\nGiven that a 78% tariff means the tariff-inclusive price is 1.78 times the world price, and the QR had roughly twice the impact on RPI, the scarcity rent would make the domestic price significantly higher.\n\nWorking backwards from the magnitudes:\n- A 22.5 percentage point fall suggests the QR was creating a substantial price distortion\n- This magnitude is consistent with the domestic price being approximately 2.05 times higher than the tariff-inclusive world price\n\nB", "model_answer": "A", "error": null}}
{"ID": 238, "Question": "### Background\n\nThis problem examines the static, one-period microfoundations of the model, focusing on how different types of \"transacting capital\" determine production outcomes, sorting across markets, and overall economic efficiency. The core of the model is a symmetric holdup problem where contract enforcement, and thus investment, depends on an individual's skills and their trading partner.\n\n### Data / Model Specification\n\n**1. Agents and Locations:** The economy has two types of agents: those with local skills (L-capital) and those with market skills (M-capital). They can trade in a local \"village\" or an anonymous \"market\".\n\n**2. Production and Payoffs:** Any two matched agents can form a partnership. Each can make a relationship-specific investment at cost `k` to increase output by `y > k`. The effective price of output is 1 in the village and `θ` in the market. We assume `θ > 1`.\n\n**3. Contract Enforcement:**\n- In the market, an agent with M-capital can enforce contracts, preventing holdup. An agent with L-capital cannot.\n- In the village, an agent with L-capital can enforce contracts. An agent with M-capital cannot.\n\n**4. Bargaining and Holdup:** When contracts cannot be enforced, post-investment surplus is split equally (Nash Bargaining). The parameters are such that holdup is a binding constraint: `θy < (4/3)k`, which implies that an agent will not invest if they expect to be held up.\n\n### Question\n\nThe entry of L-agents into the market imposes a negative externality on M-agents. What is the minimum Pigouvian tax `τ` that must be levied on an L-agent entering the market to restore the socially efficient outcome (i.e., all L-agents trading in the village)?", "Options": {"A": "```latex\n0.5(\\theta y - k) - (y-k)\n```", "B": "```latex\ny-k\n```", "C": "```latex\n0.5(\\theta y - k)\n```", "D": "```latex\nm[0.5(\\theta y-k) - (y-k)]\n```"}, "Answer": "A", "pi_justification": "This advanced item assesses the ability to apply welfare concepts to the model, specifically designing a corrective Pigouvian tax. It uses a Computational Judgment strategy. The correct answer requires identifying the L-agent's private incentive to enter the market and calculating the tax needed to offset it. Distractors represent common conceptual errors in externality problems: confusing the tax with an agent's payoff (B), calculating only one side of the incentive equation (C), or confusing the total externality value with the required marginal tax (D).", "qid": "238", "question": "### Background\n\nThis problem examines the static, one-period microfoundations of the model, focusing on how different types of \"transacting capital\" determine production outcomes, sorting across markets, and overall economic efficiency. The core of the model is a symmetric holdup problem where contract enforcement, and thus investment, depends on an individual's skills and their trading partner.\n\n### Data / Model Specification\n\n**1. Agents and Locations:** The economy has two types of agents: those with local skills (L-capital) and those with market skills (M-capital). They can trade in a local \"village\" or an anonymous \"market\".\n\n**2. Production and Payoffs:** Any two matched agents can form a partnership. Each can make a relationship-specific investment at cost `k` to increase output by `y > k`. The effective price of output is 1 in the village and `θ` in the market. We assume `θ > 1`.\n\n**3. Contract Enforcement:**\n- In the market, an agent with M-capital can enforce contracts, preventing holdup. An agent with L-capital cannot.\n- In the village, an agent with L-capital can enforce contracts. An agent with M-capital cannot.\n\n**4. Bargaining and Holdup:** When contracts cannot be enforced, post-investment surplus is split equally (Nash Bargaining). The parameters are such that holdup is a binding constraint: `θy < (4/3)k`, which implies that an agent will not invest if they expect to be held up.\n\n### Question\n\nThe entry of L-agents into the market imposes a negative externality on M-agents. What is the minimum Pigouvian tax `τ` that must be levied on an L-agent entering the market to restore the socially efficient outcome (i.e., all L-agents trading in the village)?", "model_evaluation_result": {"model_raw_response": "I need to analyze this holdup problem to determine the Pigouvian tax required to restore efficiency.\n\nLet me work through the key elements:\n\n**Understanding the Setup:**\n- L-agents can enforce contracts in village, M-agents can enforce in market\n- When contracts can't be enforced, surplus is split equally (Nash bargaining)\n- The constraint θy < (4/3)k means agents won't invest if expecting holdup\n- θ > 1 (market price premium)\n\n**Efficient Outcome:**\n- L-agents should trade in village (where they can enforce contracts)\n- M-agents should trade in market (where they can enforce contracts)\n- This maximizes total surplus since each agent type trades where they have enforcement power\n\n**The Externality:**\nWhen an L-agent enters the market:\n1. The L-agent cannot enforce contracts in the market\n2. If matched with an M-agent, the M-agent can enforce, so both invest\n3. But if matched with another L-agent, neither can enforce, leading to holdup\n\n**Calculating the Tax:**\n\nFor an L-agent in the market:\n- Expected payoff when matched with M-agent: 0.5(θy - k) (surplus split equally, but both invest since M-agent enforces)\n- Expected payoff when matched with L-agent: 0 (neither invests due to holdup)\n\nFor an L-agent in the village:\n- Expected payoff: y - k (can enforce contracts, so both invest, gets full surplus from own investment)\n\nThe L-agent will choose market over village if their private benefit exceeds the village payoff. However, their entry into the market creates negative externalities on other agents.\n\nThe minimum tax needed is the amount that makes the L-agent indifferent between market and village, which restores the efficient allocation.\n\nThe tax should equal the L-agent's private benefit from entering the market minus their village payoff:\n\nτ = 0.5(θy - k) - (y - k)\n\nThis makes the L-agent indifferent and restores efficiency.\n\nA", "model_answer": "A", "error": null}}
{"ID": 316, "Question": "### Background\n\n**Research Question.** This problem interrogates the full causal inference strategy used to estimate the effect of broadband on firms and workers. The paper uses both an Intention-to-Treat (ITT) design and an Instrumental Variable (IV) design.\n\n**Setting and Sample.** The study exploits a public program in Norway that progressively rolled out broadband infrastructure across different municipalities (`m`) at different times (`t`). This staggered rollout provides quasi-experimental variation in broadband availability (`z_mt`), which is the share of households in a municipality with access to broadband.\n\n### Data / Model Specification\n\nThe **Intention-to-Treat (ITT)** model estimates the effect of broadband *availability* on an outcome `y_imt`:\n\n```latex\ny_{imt} = x_{imt}'\\delta_0 + z_{mt}x_{imt}'\\delta_1 + ... + \\eta_m + \\tau_t + u_{imt} \\quad \\text{(Eq. 1)}\n```\n\nThe **Instrumental Variable (IV)** model estimates the effect of broadband *adoption* (`D_imt`) on `y_imt`, using `z_mt` as an instrument. The second stage is:\n\n```latex\ny_{imt} = x_{imt}'\\beta_0 + D_{imt}x_{imt}'\\beta_1 + ... + \\lambda_m + \\theta_t + \\epsilon_{imt} \\quad \\text{(Eq. 2)}\n```\n\n**Empirical Evidence for IV Assumptions:**\n- **First Stage:** A regression of firm adoption `D_imt` on availability `z_mt` yields a coefficient of `0.23` with a standard error of `0.04`.\n- **Placebo Test:** The paper identifies \"always takers\" (firms with broadband when `z=0`) and \"never takers\" (firms without broadband when `z=1`). For this subsample of firms, whose adoption status is by definition unaffected by `z_mt`, the paper re-estimates the ITT model (Eq. 1). The results are in Table 1.\n\n**Table 1. Placebo Test: ITT Effects on Output Elasticities (from paper's Table VI)**\n\n| Variable | Baseline Sample | Always/Never Taker Firms Only |\n| :--- | :--- | :--- |\n| Availability x Log skilled | 0.0755*** | 0.00944 |\n\n*Notes: Significance: ***p<0.01.*\n\n### Question\n\nBased on the provided first-stage regression results, which of the following statements correctly assesses the instrument relevance condition?", "Options": {"A": "The F-statistic is approximately 33.1, but this is below the conventional threshold, indicating a weak instrument and a violation of the relevance condition.", "B": "The F-statistic is approximately 33.1, which is well above the conventional threshold of 10, indicating a strong instrument and satisfying the relevance condition.", "C": "The F-statistic is 5.75, which is below the conventional threshold of 10, indicating a weak instrument and a violation of the relevance condition.", "D": "The relevance condition cannot be assessed with the first-stage results; it can only be satisfied if the placebo test in Table 1 shows a significant effect."}, "Answer": "B", "pi_justification": "This item tests the student's ability to perform a standard diagnostic check for an instrumental variable: assessing instrument relevance. It uses a Computational Judgment strategy, requiring a two-step process of calculation (F-statistic) and evaluation (comparison to a rule of thumb). Distractors are based on a precise error matrix: a calculation error (forgetting to square the t-statistic, C), a judgment error (misinterpreting the threshold, A), and a fundamental conceptual error about IV diagnostics (D).", "qid": "316", "question": "### Background\n\n**Research Question.** This problem interrogates the full causal inference strategy used to estimate the effect of broadband on firms and workers. The paper uses both an Intention-to-Treat (ITT) design and an Instrumental Variable (IV) design.\n\n**Setting and Sample.** The study exploits a public program in Norway that progressively rolled out broadband infrastructure across different municipalities (`m`) at different times (`t`). This staggered rollout provides quasi-experimental variation in broadband availability (`z_mt`), which is the share of households in a municipality with access to broadband.\n\n### Data / Model Specification\n\nThe **Intention-to-Treat (ITT)** model estimates the effect of broadband *availability* on an outcome `y_imt`:\n\n```latex\ny_{imt} = x_{imt}'\\delta_0 + z_{mt}x_{imt}'\\delta_1 + ... + \\eta_m + \\tau_t + u_{imt} \\quad \\text{(Eq. 1)}\n```\n\nThe **Instrumental Variable (IV)** model estimates the effect of broadband *adoption* (`D_imt`) on `y_imt`, using `z_mt` as an instrument. The second stage is:\n\n```latex\ny_{imt} = x_{imt}'\\beta_0 + D_{imt}x_{imt}'\\beta_1 + ... + \\lambda_m + \\theta_t + \\epsilon_{imt} \\quad \\text{(Eq. 2)}\n```\n\n**Empirical Evidence for IV Assumptions:**\n- **First Stage:** A regression of firm adoption `D_imt` on availability `z_mt` yields a coefficient of `0.23` with a standard error of `0.04`.\n- **Placebo Test:** The paper identifies \"always takers\" (firms with broadband when `z=0`) and \"never takers\" (firms without broadband when `z=1`). For this subsample of firms, whose adoption status is by definition unaffected by `z_mt`, the paper re-estimates the ITT model (Eq. 1). The results are in Table 1.\n\n**Table 1. Placebo Test: ITT Effects on Output Elasticities (from paper's Table VI)**\n\n| Variable | Baseline Sample | Always/Never Taker Firms Only |\n| :--- | :--- | :--- |\n| Availability x Log skilled | 0.0755*** | 0.00944 |\n\n*Notes: Significance: ***p<0.01.*\n\n### Question\n\nBased on the provided first-stage regression results, which of the following statements correctly assesses the instrument relevance condition?", "model_evaluation_result": {"model_raw_response": "Looking at the first-stage regression results:\n- Coefficient on availability (z_mt): 0.23\n- Standard error: 0.04\n\nTo assess instrument relevance, I need to calculate the F-statistic for the first stage.\n\nThe t-statistic is: 0.23/0.04 = 5.75\n\nThe F-statistic for a single instrument is the square of the t-statistic:\nF = (5.75)² = 33.06 ≈ 33.1\n\nThe conventional threshold for instrument strength is an F-statistic of 10. Since 33.1 > 10, this indicates a strong instrument that satisfies the relevance condition.\n\nThe relevance condition requires that the instrument is sufficiently correlated with the endogenous variable (broadband adoption). An F-statistic well above 10 demonstrates this correlation is strong enough to avoid weak instrument problems.\n\nB", "model_answer": "B", "error": null}}
{"ID": 288, "Question": "### Background\n\n**Research Question.** This problem addresses the potential for endogenous legislation (i.e., laws are passed in response to rising divorce rates), a critical challenge to the causal interpretation of the paper's findings.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel (1968-1988). The validity of the event-study design rests on the assumption that the timing of law adoption is not driven by pre-existing trends in divorce rates.\n\n### Data / Model Specification\n\nTo test for endogeneity, the paper's preferred specification (with state/year FE and state-specific trends) is augmented with a lead variable for the policy change:\n\n```latex\n\\text{divrate}_{st} = \\beta \\cdot \\mathrm{unilateral}_{st} + \\delta_{-1} D_{st}^{-1} + \\text{Controls} + u_{st} \\quad \\text{(Eq. (1))}\n```\n- `unilateral_st`: Indicator that is 1 for all years a law is in effect.\n- `D_st^{-1}`: A \"lead\" dummy, 1 only in the year *before* the law change.\n\n**Table 1: Test for Pre-Trend**\n\n| Variable | (1) Test for Pre-Trend |\n| :--- | :---: |\n| **Unilateral** (`β`) | 0.449 (0.057) |\n| **1 period later (Lead, `δ_{-1}`)** | 0.004 (0.067) |\n\n*Notes: Dependent variable is the divorce rate per 1,000 people. The model includes state/year FE and state-specific trends.*\n\nColumn (1) of Table 1 presents a test for pre-existing trends by including a 'lead' variable for the year prior to the law change. Based on the coefficient and standard error for '1 period later (Lead)', what is the most accurate conclusion?", "Options": {"A": "The coefficient is statistically indistinguishable from zero, which supports the key identifying assumption that the timing of the law was not driven by pre-existing trends in the divorce rate.", "B": "The coefficient is positive and significant, indicating that states were already experiencing rising divorce rates before the law change, which invalidates the causal interpretation.", "C": "The coefficient shows that the full effect of the law is delayed by one year, as the impact is not felt until after the law is passed.", "D": "The coefficient is statistically indistinguishable from zero, which proves that the law change had a causal effect on divorce rates."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to interpret a standard pre-trend test in an event-study design. It uses a Reverse-Reasoning strategy, asking for the conclusion given the evidence. The distractors are designed to catch common errors: (B) misinterpreting statistical significance (Conceptual Opposite), (C) confusing a lead variable with a lag variable (Conceptual: Almost Right), and (D) overstating the strength of the evidence (True but Irrelevant implication).", "qid": "288", "question": "### Background\n\n**Research Question.** This problem addresses the potential for endogenous legislation (i.e., laws are passed in response to rising divorce rates), a critical challenge to the causal interpretation of the paper's findings.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel (1968-1988). The validity of the event-study design rests on the assumption that the timing of law adoption is not driven by pre-existing trends in divorce rates.\n\n### Data / Model Specification\n\nTo test for endogeneity, the paper's preferred specification (with state/year FE and state-specific trends) is augmented with a lead variable for the policy change:\n\n```latex\n\\text{divrate}_{st} = \\beta \\cdot \\mathrm{unilateral}_{st} + \\delta_{-1} D_{st}^{-1} + \\text{Controls} + u_{st} \\quad \\text{(Eq. (1))}\n```\n- `unilateral_st`: Indicator that is 1 for all years a law is in effect.\n- `D_st^{-1}`: A \"lead\" dummy, 1 only in the year *before* the law change.\n\n**Table 1: Test for Pre-Trend**\n\n| Variable | (1) Test for Pre-Trend |\n| :--- | :---: |\n| **Unilateral** (`β`) | 0.449 (0.057) |\n| **1 period later (Lead, `δ_{-1}`)** | 0.004 (0.067) |\n\n*Notes: Dependent variable is the divorce rate per 1,000 people. The model includes state/year FE and state-specific trends.*\n\nColumn (1) of Table 1 presents a test for pre-existing trends by including a 'lead' variable for the year prior to the law change. Based on the coefficient and standard error for '1 period later (Lead)', what is the most accurate conclusion?", "model_evaluation_result": {"model_raw_response": "Looking at the coefficient for the lead variable (1 period later): 0.004 with a standard error of 0.067.\n\nTo determine statistical significance, I need to calculate whether this coefficient is statistically different from zero:\n- t-statistic = 0.004 / 0.067 ≈ 0.06\n- This is much less than 1.96 (the critical value for 95% confidence), so the coefficient is not statistically significant.\n\nThe lead variable tests whether there was already a change in divorce rates in the year BEFORE the law was implemented. If states were passing unilateral divorce laws in response to rising divorce rates (endogenous legislation), we would expect to see a positive and significant coefficient on this lead variable.\n\nSince the lead coefficient is statistically indistinguishable from zero (0.004 with p-value >> 0.05), this suggests there was no pre-existing upward trend in divorce rates in the year before the law change. This supports the identifying assumption that the timing of the law adoption was not driven by pre-existing trends in divorce rates.\n\nOption A correctly interprets this result as supporting the key identifying assumption against endogenous legislation concerns.\n\nA", "model_answer": "A", "error": null}}
{"ID": 268, "Question": "### Background\n\nAn agent's preferences are described by the Gul-Pesendorfer model of addiction. The state of addiction, `s`, is last period's drug consumption. The function `σ(s)` represents the weight on temptation utility, and `υ(d)` is the temptation utility from current drug consumption `d`.\n\n### Question\n\nThe model predicts that drug consumption is *reinforcing*: an agent in a higher state of addiction (`s̄ > s`) will consume more (`d̄ ≥ d`) from a given choice set. This result is derived from the inequality `(σ(s̄) - σ(s))(υ(d̄) - υ(d)) ≥ 0`. For a drug to be defined as addictive in this model, what key property must the function `σ(s)` have?", "Options": {"A": "It must be strictly positive.", "B": "It must be non-decreasing in the state `s`.", "C": "It must be a convex function of the state `s`.", "D": "It must be independent of the state `s`."}, "Answer": "B", "pi_justification": "This item assesses the link between the formal definition of an addictive drug and its behavioral consequence (reinforcement). It uses a Reverse-Reasoning strategy, asking for the core assumption given the behavioral result. Distractor A (True but Irrelevant) is a property of the model but doesn't define addiction. Distractor C (Almost Right) is a stronger condition (Assumption 2) used for some comparative statics but is not the core definition of addiction. Distractor D (Conceptual Opposite) would describe a non-addictive good.", "qid": "268", "question": "### Background\n\nAn agent's preferences are described by the Gul-Pesendorfer model of addiction. The state of addiction, `s`, is last period's drug consumption. The function `σ(s)` represents the weight on temptation utility, and `υ(d)` is the temptation utility from current drug consumption `d`.\n\n### Question\n\nThe model predicts that drug consumption is *reinforcing*: an agent in a higher state of addiction (`s̄ > s`) will consume more (`d̄ ≥ d`) from a given choice set. This result is derived from the inequality `(σ(s̄) - σ(s))(υ(d̄) - υ(d)) ≥ 0`. For a drug to be defined as addictive in this model, what key property must the function `σ(s)` have?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand what makes drug consumption \"reinforcing\" in the Gul-Pesendorfer addiction model.\n\nThe key inequality given is: `(σ(s̄) - σ(s))(υ(d̄) - υ(d)) ≥ 0`\n\nThis inequality must hold for the model to predict that higher addiction states lead to higher consumption (i.e., `s̄ > s` implies `d̄ ≥ d`).\n\nLet me analyze what this means:\n- We have `s̄ > s` (higher addiction state)\n- We want `d̄ ≥ d` (higher or equal consumption)\n- Since `υ(d)` represents temptation utility from drug consumption, and more consumption typically provides more temptation utility, we expect `υ(d̄) - υ(d) ≥ 0`\n\nFor the inequality `(σ(s̄) - σ(s))(υ(d̄) - υ(d)) ≥ 0` to hold when both factors are non-negative, we need:\n- `σ(s̄) - σ(s) ≥ 0` when `s̄ > s`\n\nThis means `σ(s)` must be non-decreasing in `s`. In other words, as the addiction state increases, the weight on temptation utility must not decrease (and typically increases).\n\nThis makes economic sense: a higher addiction state should make the agent more susceptible to temptation, not less.\n\nLooking at the options:\nA) Being strictly positive is not sufficient for reinforcement\nB) Being non-decreasing ensures the reinforcement property holds\nC) Convexity is a stronger condition than necessary\nD) Independence would make σ(s̄) = σ(s), contradicting reinforcement\n\nB", "model_answer": "A", "error": null}}
{"ID": 295, "Question": "### Background\n\nIn a duopoly model of price competition, consumer preferences `θ` are uniformly distributed on the interval `[\\underline{θ}, \\overline{θ}]`. Firms have constant marginal costs `c₁` and `c₂`.\n\n---\n\n### Data / Model Specification\n\nFor an interior solution where both firms are active, the firms' best response functions are given by the following system of linear equations:\n\n```latex\np₁ = 0.5[p₂ - \\underline{θ} + c₁] \\quad \\text{(Eq. (1))}\n```\n\n```latex\np₂ = 0.5[p₁ + \\overline{θ} + c₂] \\quad \\text{(Eq. (2))}\n```\n\n---\n\nBy solving this system of equations, what is the Nash equilibrium price for firm 1, `p₁*`?", "Options": {"A": "```latex\n(1/3)(c₂ + \\overline{θ}) + (2/3)(c₁ - \\underline{θ})\n```", "B": "```latex\n(2/3)(c₂ + \\overline{θ}) + (1/3)(c₁ - \\underline{θ})\n```", "C": "```latex\n(1/3)(c₂ + \\overline{θ}) + (2/3)(c₁ + \\underline{θ})\n```", "D": "```latex\n0.5(c₁ + c₂ + \\overline{θ} - \\underline{θ})\n```"}, "Answer": "A", "pi_justification": "This question assesses the ability to solve a 2x2 system of linear equations, a core computational skill required by the model. It uses a Computational Judgment strategy. The distractors are designed based on common algebraic errors: (B) swaps the coefficients (1/3 and 2/3), a common substitution mistake; (C) introduces a sign error on `\\underline{θ}`, testing attention to detail; (D) presents a simple average that ignores the strategic interaction captured by the correct solution process.", "qid": "295", "question": "### Background\n\nIn a duopoly model of price competition, consumer preferences `θ` are uniformly distributed on the interval `[\\underline{θ}, \\overline{θ}]`. Firms have constant marginal costs `c₁` and `c₂`.\n\n---\n\n### Data / Model Specification\n\nFor an interior solution where both firms are active, the firms' best response functions are given by the following system of linear equations:\n\n```latex\np₁ = 0.5[p₂ - \\underline{θ} + c₁] \\quad \\text{(Eq. (1))}\n```\n\n```latex\np₂ = 0.5[p₁ + \\overline{θ} + c₂] \\quad \\text{(Eq. (2))}\n```\n\n---\n\nBy solving this system of equations, what is the Nash equilibrium price for firm 1, `p₁*`?", "model_evaluation_result": {"model_raw_response": "I need to solve the system of linear equations to find the Nash equilibrium price for firm 1.\n\nGiven system:\n- p₁ = 0.5[p₂ - θ̄ + c₁] ... (Eq. 1)\n- p₂ = 0.5[p₁ + θ̄ + c₂] ... (Eq. 2)\n\nLet me substitute equation (2) into equation (1):\n\np₁ = 0.5[0.5(p₁ + θ̄ + c₂) - θ̄ + c₁]\np₁ = 0.5[0.5p₁ + 0.5θ̄ + 0.5c₂ - θ̄ + c₁]\np₁ = 0.5[0.5p₁ - 0.5θ̄ + 0.5c₂ + c₁]\np₁ = 0.25p₁ - 0.25θ̄ + 0.25c₂ + 0.5c₁\n\nRearranging:\np₁ - 0.25p₁ = -0.25θ̄ + 0.25c₂ + 0.5c₁\n0.75p₁ = -0.25θ̄ + 0.25c₂ + 0.5c₁\np₁ = (-0.25θ̄ + 0.25c₂ + 0.5c₁)/0.75\n\nSimplifying:\np₁ = (-θ̄ + c₂ + 2c₁)/3\np₁ = (1/3)(-θ̄ + c₂ + 2c₁)\np₁ = (1/3)(c₂ - θ̄) + (2/3)c₁\n\nWait, let me double-check this. Looking at the original equations more carefully:\n\np₁ =", "model_answer": "I need to solve the system of linear equations to find the Nash equilibrium price for firm 1.\n\nGiven system:\n- p₁ = 0.5[p₂ - θ̄ + c₁] ... (Eq. 1)\n- p₂ = 0.5[p₁ + θ̄ + c₂] ... (Eq. 2)\n\nLet me substitute equation (2) into equation (1):\n\np₁ = 0.5[0.5(p₁ + θ̄ + c₂) - θ̄ + c₁]\np₁ = 0.5[0.5p₁ + 0.5θ̄ + 0.5c₂ - θ̄ + c₁]\np₁ = 0.5[0.5p₁ - 0.5θ̄ + 0.5c₂ + c₁]\np₁ = 0.25p₁ - 0.25θ̄ + 0.25c₂ + 0.5c₁\n\nRearranging:\np₁ - 0.25p₁ = -0.25θ̄ + 0.25c₂ + 0.5c₁\n0.75p₁ = -0.25θ̄ + 0.25c₂ + 0.5c₁\np₁ = (-0.25θ̄ + 0.25c₂ + 0.5c₁)/0.75\n\nSimplifying:\np₁ = (-θ̄ + c₂ + 2c₁)/3\np₁ = (1/3)(-θ̄ + c₂ + 2c₁)\np₁ = (1/3)(c₂ - θ̄) + (2/3)c₁\n\nWait, let me double-check this. Looking at the original equations more carefully:\n\np₁ =", "error": "Warning: Could not extract single choice letter from: I need to solve the system of linear equations to ..."}}
{"ID": 283, "Question": "### Background\n\nIn a sequential *demand game* for a natural monopoly, players are arranged in a fixed order `w`. The unique subgame perfect equilibrium payoff vector `d_N` is determined via backward induction. The last player's payoff is their autarky value, and each preceding player's payoff is the maximum they can claim while leaving enough surplus for all subsequent players to achieve their predetermined equilibrium payoffs.\n\n### Data / Model Specification\n\nConsider the following example case:\n- `N = {1, 2, 3}` with order `(1, 2, 3)`.\n- Production function: `f(x) = x^2`.\n- Initial endowments: `a_1 = a_2 = a_3 = 1`.\n- Utility functions (in terms of input contribution `x` and output `y`): \n  - `u_1(x,y) = 2y - x`\n  - `u_2(x,y) = y - x`\n  - `u_3(x,y) = y/2 - x`\n\nNow, suppose a regulator imposes a constraint on player 2: player 2 is forbidden from contributing more than half of their endowment, i.e., `x_2 ≤ 0.5`. All other aspects of the example case remain the same.\n\n### Question\n\nUnder this new constraint, what is the new equilibrium payoff for player 1, `d'_1`?", "Options": {"A": "9", "B": "7", "C": "-0.25", "D": "6.25"}, "Answer": "B", "pi_justification": "This item tests the ability to apply the demand game model to a new scenario, a key skill in economic modeling. It uses a Scenario Application strategy. The student must re-calculate the entire backward induction chain with the new constraint on player 2's input. Distractor A (9) is the original equilibrium payoff, selected if the student ignores the new constraint. Distractor C (-0.25) is an intermediate value in the calculation (`d'_2`), testing if the student can follow the process to the end. Distractor D (6.25) is the total output of the grand coalition, a plausible error for students who confuse output with utility.", "qid": "283", "question": "### Background\n\nIn a sequential *demand game* for a natural monopoly, players are arranged in a fixed order `w`. The unique subgame perfect equilibrium payoff vector `d_N` is determined via backward induction. The last player's payoff is their autarky value, and each preceding player's payoff is the maximum they can claim while leaving enough surplus for all subsequent players to achieve their predetermined equilibrium payoffs.\n\n### Data / Model Specification\n\nConsider the following example case:\n- `N = {1, 2, 3}` with order `(1, 2, 3)`.\n- Production function: `f(x) = x^2`.\n- Initial endowments: `a_1 = a_2 = a_3 = 1`.\n- Utility functions (in terms of input contribution `x` and output `y`): \n  - `u_1(x,y) = 2y - x`\n  - `u_2(x,y) = y - x`\n  - `u_3(x,y) = y/2 - x`\n\nNow, suppose a regulator imposes a constraint on player 2: player 2 is forbidden from contributing more than half of their endowment, i.e., `x_2 ≤ 0.5`. All other aspects of the example case remain the same.\n\n### Question\n\nUnder this new constraint, what is the new equilibrium payoff for player 1, `d'_1`?", "model_evaluation_result": {"model_raw_response": "I need to solve this sequential demand game with the new constraint on player 2.\n\nFirst, let me establish the setup:\n- Players: {1, 2, 3} in order (1, 2, 3)\n- Production function: f(x) = x²\n- Endowments: a₁ = a₂ = a₃ = 1\n- Utility functions:\n  - u₁(x,y) = 2y - x\n  - u₂(x,y) = y - x  \n  - u₃(x,y) = y/2 - x\n- New constraint: x₂ ≤ 0.5\n\nI'll solve by backward induction.\n\n**Step 1: Find autarky values**\nEach player's autarky value is their utility when contributing their entire endowment alone:\n- Player 1: u₁(1, f(1)) = u₁(1, 1) = 2(1) - 1 = 1\n- Player 2: u₂(1, f(1)) = u₂(1, 1) = 1 - 1 = 0\n- Player 3: u₃(1, f(1)) = u₃(1, 1) = 1/2 - 1 = -0.5\n\n**Step 2: Player 3's decision (last player)**\nPlayer 3 gets their autarky value: d'₃ = -0.5\n\n**Step 3: Player 2's decision**\nPlayer 2 needs to leave enough surplus for player 3 to get d'₃ = -0.5.\n\nLet total contributions be X = x₁ + x₂ + x₃, so output is Y = X².\n\nPlayer 3's utility: u₃(x₃, Y) = Y/2 - x₃ = X²/2 - x₃\n\nFor player 3 to get -0.5: X²/2 - x₃ = -0.5\nSo: x₃ = X²/2 + 0.5\n\nPlayer 2's utility", "model_answer": "I need to solve this sequential demand game with the new constraint on player 2.\n\nFirst, let me establish the setup:\n- Players: {1, 2, 3} in order (1, 2, 3)\n- Production function: f(x) = x²\n- Endowments: a₁ = a₂ = a₃ = 1\n- Utility functions:\n  - u₁(x,y) = 2y - x\n  - u₂(x,y) = y - x  \n  - u₃(x,y) = y/2 - x\n- New constraint: x₂ ≤ 0.5\n\nI'll solve by backward induction.\n\n**Step 1: Find autarky values**\nEach player's autarky value is their utility when contributing their entire endowment alone:\n- Player 1: u₁(1, f(1)) = u₁(1, 1) = 2(1) - 1 = 1\n- Player 2: u₂(1, f(1)) = u₂(1, 1) = 1 - 1 = 0\n- Player 3: u₃(1, f(1)) = u₃(1, 1) = 1/2 - 1 = -0.5\n\n**Step 2: Player 3's decision (last player)**\nPlayer 3 gets their autarky value: d'₃ = -0.5\n\n**Step 3: Player 2's decision**\nPlayer 2 needs to leave enough surplus for player 3 to get d'₃ = -0.5.\n\nLet total contributions be X = x₁ + x₂ + x₃, so output is Y = X².\n\nPlayer 3's utility: u₃(x₃, Y) = Y/2 - x₃ = X²/2 - x₃\n\nFor player 3 to get -0.5: X²/2 - x₃ = -0.5\nSo: x₃ = X²/2 + 0.5\n\nPlayer 2's utility", "error": "Warning: Could not extract single choice letter from: I need to solve this sequential demand game with t..."}}
{"ID": 201, "Question": "### Background\n\n**Research Question.** This problem examines the theoretical underpinnings of the paper's generalized cointegration model, focusing on the relative efficiency of the proposed Quasi-Differencing (QD) detrending procedure compared to standard Ordinary Least Squares (OLS).\n\n**Setting / Institutional Environment.** The framework is a Vector Autoregression (VAR) generalized with a quasi-difference (QD) operator to model local-to-unity roots. The efficiency comparison depends on the localizing parameter `c`.\n\n**Variables & Parameters.**\n- `c`: The localizing parameter. `c=0` corresponds to a pure unit root.\n- `V_ols`, `V_gls`: Asymptotic variances of the OLS and QD estimators for the trend coefficient in the non-stationary directions.\n- `R_c`: The relative efficiency, defined as `R_c = V_ols / V_gls`.\n\n---\n\n### Data / Model Specification\n\nFor the case where there is one non-stationary direction (`s=1`) and the trend is `x_t=t`, the asymptotic variances of the QD (`gls`) and OLS (`ols`) estimators for the trend coefficient are given by:\n```latex\nV_{gls}=\\frac{3\\sigma^{2}}{3-3c+c^{2}} \n\n\\quad \\text{(Eq. 1)}\n```\n```latex\nV_{ols}=\\frac{3\\sigma^{2}[3e^{2c}(c-1)^{2}+2c^{3}+3c^{2}-3]}{2c^{5}} \n\n\\quad \\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nUsing the variance formulas in Eq. (1) and Eq. (2), what is the limit of the relative efficiency `R_c = V_ols / V_gls` as the localizing parameter `c → 0`? This result indicates the efficiency gain of QD detrending when the alternative hypothesis is extremely close to the null of a perfect unit root.", "Options": {"A": "1", "B": "0", "C": "∞", "D": "3"}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests the ability to perform a formal mathematical derivation and arrive at a precise conclusion, a task designated as 'Computational Judgment'. It converts an open-ended calculation from the source QA into a single-choice question. The distractors are designed based on common errors: 0 (incorrect limit evaluation), ∞ (conceptual error about efficiency), and 3 (superficial formula reading).", "qid": "201", "question": "### Background\n\n**Research Question.** This problem examines the theoretical underpinnings of the paper's generalized cointegration model, focusing on the relative efficiency of the proposed Quasi-Differencing (QD) detrending procedure compared to standard Ordinary Least Squares (OLS).\n\n**Setting / Institutional Environment.** The framework is a Vector Autoregression (VAR) generalized with a quasi-difference (QD) operator to model local-to-unity roots. The efficiency comparison depends on the localizing parameter `c`.\n\n**Variables & Parameters.**\n- `c`: The localizing parameter. `c=0` corresponds to a pure unit root.\n- `V_ols`, `V_gls`: Asymptotic variances of the OLS and QD estimators for the trend coefficient in the non-stationary directions.\n- `R_c`: The relative efficiency, defined as `R_c = V_ols / V_gls`.\n\n---\n\n### Data / Model Specification\n\nFor the case where there is one non-stationary direction (`s=1`) and the trend is `x_t=t`, the asymptotic variances of the QD (`gls`) and OLS (`ols`) estimators for the trend coefficient are given by:\n```latex\nV_{gls}=\\frac{3\\sigma^{2}}{3-3c+c^{2}} \n\n\\quad \\text{(Eq. 1)}\n```\n```latex\nV_{ols}=\\frac{3\\sigma^{2}[3e^{2c}(c-1)^{2}+2c^{3}+3c^{2}-3]}{2c^{5}} \n\n\\quad \\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nUsing the variance formulas in Eq. (1) and Eq. (2), what is the limit of the relative efficiency `R_c = V_ols / V_gls` as the localizing parameter `c → 0`? This result indicates the efficiency gain of QD detrending when the alternative hypothesis is extremely close to the null of a perfect unit root.", "model_evaluation_result": {"model_raw_response": "I need to find the limit of the relative efficiency R_c = V_ols / V_gls as c → 0.\n\nGiven:\n- V_gls = 3σ²/(3-3c+c²)\n- V_ols = 3σ²[3e^(2c)(c-1)² + 2c³ + 3c² - 3]/(2c⁵)\n\nFirst, let me find R_c = V_ols / V_gls:\n\nR_c = [3σ²[3e^(2c)(c-1)² + 2c³ + 3c² - 3]/(2c⁵)] / [3σ²/(3-3c+c²)]\n\nR_c = [3e^(2c)(c-1)² + 2c³ + 3c² - 3] × (3-3c+c²) / (2c⁵)\n\nNow I need to evaluate this limit as c → 0.\n\nFor the numerator as c → 0:\n- e^(2c) → 1\n- (c-1)² → 1\n- So 3e^(2c)(c-1)² → 3(1)(1) = 3\n- 2c³ → 0\n- 3c² → 0\n- Therefore: 3e^(2c)(c-1)² + 2c³ + 3c² - 3 → 3 + 0 + 0 - 3 = 0\n\nFor (3-3c+c²) as c → 0:\n- This approaches 3\n\nSo the numerator approaches 0 × 3 = 0, and the denominator 2c⁵ → 0.\n\nThis gives us a 0/0 indeterminate form, so I need to use L'Hôpital's rule or Taylor expansions.\n\nLet me use Taylor expansions around c = 0:\n- e^(2c) ≈ 1 + 2c + 2c² + (4c³)/3 + ...", "model_answer": "I need to find the limit of the relative efficiency R_c = V_ols / V_gls as c → 0.\n\nGiven:\n- V_gls = 3σ²/(3-3c+c²)\n- V_ols = 3σ²[3e^(2c)(c-1)² + 2c³ + 3c² - 3]/(2c⁵)\n\nFirst, let me find R_c = V_ols / V_gls:\n\nR_c = [3σ²[3e^(2c)(c-1)² + 2c³ + 3c² - 3]/(2c⁵)] / [3σ²/(3-3c+c²)]\n\nR_c = [3e^(2c)(c-1)² + 2c³ + 3c² - 3] × (3-3c+c²) / (2c⁵)\n\nNow I need to evaluate this limit as c → 0.\n\nFor the numerator as c → 0:\n- e^(2c) → 1\n- (c-1)² → 1\n- So 3e^(2c)(c-1)² → 3(1)(1) = 3\n- 2c³ → 0\n- 3c² → 0\n- Therefore: 3e^(2c)(c-1)² + 2c³ + 3c² - 3 → 3 + 0 + 0 - 3 = 0\n\nFor (3-3c+c²) as c → 0:\n- This approaches 3\n\nSo the numerator approaches 0 × 3 = 0, and the denominator 2c⁵ → 0.\n\nThis gives us a 0/0 indeterminate form, so I need to use L'Hôpital's rule or Taylor expansions.\n\nLet me use Taylor expansions around c = 0:\n- e^(2c) ≈ 1 + 2c + 2c² + (4c³)/3 + ...", "error": "Warning: Could not extract single choice letter from: I need to find the limit of the relative efficienc..."}}
{"ID": 340, "Question": "### Background\n\nA study analyzes the probability of an active economist being selected as an Econometric Society (ES) Fellow. It uses a logit model to estimate the effect of gender, controlling for an extensive set of career achievements like cumulative publications and citations.\n\n### Data / Model Specification\n\nThe table below shows key logit coefficients from the most comprehensive model specification for two different time periods.\n\n**Table 1: Logit Coefficients for Selection as ES Fellow**\n| Variable | 1933-1979 | 2000-2019 |\n|:---|---:|---:|\n| Female | -1.412 | 0.931 |\n| # Papers in *Econometrica* | 0.243 | 0.301 |\n\n*Source: Adapted from Table IV of the paper.*\n\n### Question\n\nUsing the coefficients provided for the 2000-2019 period, how can the estimated premium for being a female candidate be interpreted?", "Options": {"A": "As an advantage equivalent to having approximately three additional publications in *Econometrica*.", "B": "As an advantage equivalent to having approximately 0.32 additional publications in *Econometrica*.", "C": "As a penalty equivalent to having approximately three fewer publications in *Econometrica*.", "D": "As proof that the selection process was unbiased, since the number of women in the profession had increased."}, "Answer": "A", "pi_justification": "Assessment Target: Interpreting the magnitude of a regression coefficient by benchmarking it against another continuous variable. Strategy: Computational Judgment. Distractor Logic: The correct answer is the ratio of the 'Female' coefficient to the 'Econometrica' coefficient (0.931 / 0.301 ≈ 3.09). (B) is an inversion error (0.301 / 0.931). (C) is a sign error. (D) is a conceptual error, confusing the paper's finding of a conditional premium with a statement about the pipeline of candidates.", "qid": "340", "question": "### Background\n\nA study analyzes the probability of an active economist being selected as an Econometric Society (ES) Fellow. It uses a logit model to estimate the effect of gender, controlling for an extensive set of career achievements like cumulative publications and citations.\n\n### Data / Model Specification\n\nThe table below shows key logit coefficients from the most comprehensive model specification for two different time periods.\n\n**Table 1: Logit Coefficients for Selection as ES Fellow**\n| Variable | 1933-1979 | 2000-2019 |\n|:---|---:|---:|\n| Female | -1.412 | 0.931 |\n| # Papers in *Econometrica* | 0.243 | 0.301 |\n\n*Source: Adapted from Table IV of the paper.*\n\n### Question\n\nUsing the coefficients provided for the 2000-2019 period, how can the estimated premium for being a female candidate be interpreted?", "model_evaluation_result": {"model_raw_response": "Looking at the 2000-2019 period coefficients:\n- Female: 0.931\n- # Papers in Econometrica: 0.301\n\nTo find the equivalent number of Econometrica papers that would provide the same advantage as being female, I need to divide the female coefficient by the papers coefficient:\n\n0.931 ÷ 0.301 ≈ 3.09\n\nThis means that being female provides an advantage equivalent to having approximately 3 additional publications in Econometrica.\n\nSince the female coefficient is positive (0.931), this represents an advantage, not a penalty.\n\nA", "model_answer": "A", "error": null}}
{"ID": 380, "Question": "### Background\n\nIn social choice theory, a conflict exists between the Pareto criterion (efficiency) and equity criteria like no-envy. The paper explores lexicographic principles to resolve this. This question focuses on the **equity-first, efficiency-second principle**.\n\n### Data / Model Specification\n\nKey social preference relations are:\n\n1.  **The Pareto Relation (`≻_P`):** `x ≻_P y` if `x` is a Pareto improvement over `y`.\n2.  **The Equity Relation (`≻_F`):** Based on the set of envy instances `H(x)`, `x ≻_F y` if and only if `#H(x) < #H(y)`.\n3.  **The Equity-First Relation (`≻_FP`):** This relation ranks allocation `x` over `y` as follows:\n\n    ```latex\n    x \\succ_{FP} y \\iff (x \\succ_F y) \\lor (\\#H(x) = \\#H(y) \\text{ and } x \\succ_P y) \\quad \\text{(Eq. (1))}\n    ```\n\n### Question\n\nWhich statement best describes the value judgment embedded in the equity-first social preference relation, `≻_FP`?", "Options": {"A": "Equity is the primary criterion; efficiency is only used as a tie-breaker for allocations with the same number of envy instances.", "B": "Efficiency is the primary criterion; equity is only used as a tie-breaker for allocations that are not Pareto-comparable.", "C": "A weighted average of equity and efficiency is used to rank allocations, with a higher weight placed on equity.", "D": "Only allocations that are completely envy-free are considered socially desirable; all other allocations cannot be ranked."}, "Answer": "A", "pi_justification": "This item assesses the foundational understanding of the equity-first principle's lexicographic structure, a core concept of the paper. The distractors are designed to identify common confusions: B describes the efficiency-first principle (Conceptual Opposite), C posits a utilitarian trade-off model (Formula Misuse), and D incorrectly describes how non-envy-free allocations are handled (Conceptual Error).", "qid": "380", "question": "### Background\n\nIn social choice theory, a conflict exists between the Pareto criterion (efficiency) and equity criteria like no-envy. The paper explores lexicographic principles to resolve this. This question focuses on the **equity-first, efficiency-second principle**.\n\n### Data / Model Specification\n\nKey social preference relations are:\n\n1.  **The Pareto Relation (`≻_P`):** `x ≻_P y` if `x` is a Pareto improvement over `y`.\n2.  **The Equity Relation (`≻_F`):** Based on the set of envy instances `H(x)`, `x ≻_F y` if and only if `#H(x) < #H(y)`.\n3.  **The Equity-First Relation (`≻_FP`):** This relation ranks allocation `x` over `y` as follows:\n\n    ```latex\n    x \\succ_{FP} y \\iff (x \\succ_F y) \\lor (\\#H(x) = \\#H(y) \\text{ and } x \\succ_P y) \\quad \\text{(Eq. (1))}\n    ```\n\n### Question\n\nWhich statement best describes the value judgment embedded in the equity-first social preference relation, `≻_FP`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 294, "Question": "### Background\n\nThis problem examines how to aggregate sub-industry concentration measures to an industry-wide level.\n\n### Data / Model Specification\n\nAn input-output industry is composed of two sub-industries: a large one (L) and a small one (S).\n- `w_L` and `w_S` are the employment shares, with `w_L > 0.5` and `w_L + w_S = 1`.\n- `H_L` and `H_S` are the Herfindahl indices for the sub-industries.\n\nTwo aggregate concentration measures are proposed:\n- Unweighted average: `H_1 = (H_L + H_S) / 2`\n- Employment-weighted average: `H_2 = w_L H_L + w_S H_S`\n\n### Question\n\nUnder which condition will the employment-weighted Herfindahl index (`H_2`) be strictly greater than the unweighted index (`H_1`)?", "Options": {"A": "The larger sub-industry is the less concentrated one (`H_L < H_S`).", "B": "The weighted average of concentration is positive (`H_2 > 0`).", "C": "The two sub-industries have unequal concentration levels (`H_L ≠ H_S`).", "D": "The larger sub-industry is also the more concentrated one (`H_L > H_S`)."}, "Answer": "D", "pi_justification": "Assessment Target: This item tests the ability to perform or reason through a simple algebraic derivation (`H_2 - H_1 = (w_L - 0.5)(H_L - H_S)`) and identify the correct comparative static result. Strategy: Scenario Application. Distractor Logic: Distractors are based on specific algebraic or logical errors. (A) Sign Error: reverses the required inequality, leading to the opposite conclusion. (C) Step-Omission Error: identifies a necessary but insufficient condition, as the direction of the inequality matters. (B) True but Irrelevant: states a condition that is always true and thus provides no information.", "qid": "294", "question": "### Background\n\nThis problem examines how to aggregate sub-industry concentration measures to an industry-wide level.\n\n### Data / Model Specification\n\nAn input-output industry is composed of two sub-industries: a large one (L) and a small one (S).\n- `w_L` and `w_S` are the employment shares, with `w_L > 0.5` and `w_L + w_S = 1`.\n- `H_L` and `H_S` are the Herfindahl indices for the sub-industries.\n\nTwo aggregate concentration measures are proposed:\n- Unweighted average: `H_1 = (H_L + H_S) / 2`\n- Employment-weighted average: `H_2 = w_L H_L + w_S H_S`\n\n### Question\n\nUnder which condition will the employment-weighted Herfindahl index (`H_2`) be strictly greater than the unweighted index (`H_1`)?", "model_evaluation_result": {"model_raw_response": "I need to determine when H₂ > H₁.\n\nGiven:\n- H₁ = (H_L + H_S) / 2 (unweighted average)\n- H₂ = w_L H_L + w_S H_S (employment-weighted average)\n- w_L > 0.5 and w_L + w_S = 1, so w_S < 0.5\n\nI want to find when H₂ > H₁:\n\nw_L H_L + w_S H_S > (H_L + H_S) / 2\n\nMultiply both sides by 2:\n2(w_L H_L + w_S H_S) > H_L + H_S\n\n2w_L H_L + 2w_S H_S > H_L + H_S\n\nRearranging:\n2w_L H_L - H_L > H_S - 2w_S H_S\n\nH_L(2w_L - 1) > H_S(1 - 2w_S)\n\nSince w_L + w_S = 1, we have w_S = 1 - w_L, so:\n1 - 2w_S = 1 - 2(1 - w_L) = 1 - 2 + 2w_L = 2w_L - 1\n\nTherefore:\nH_L(2w_L - 1) > H_S(2w_L - 1)\n\nSince w_L > 0.5, we have 2w_L - 1 > 0.\n\nDividing both sides by (2w_L - 1):\nH_L > H_S\n\nThis means H₂ > H₁ if and only if the larger sub-industry (L) is also the more concentrated one.\n\nD", "model_answer": "D", "error": null}}
{"ID": 332, "Question": "### Background\n\n**Research Question.** This problem tests the quantitative application of the estimated Vector Error Correction Model (VECM), which links deviations from long-run equilibrium (the forward premiums) to short-run changes in the spot exchange rate.\n\n**Setting / Institutional Environment.** The VECM for the change in the spot rate (`Δs_t`) includes several lagged error correction terms, `(s - f_h)_{t-1}`, which represent the previous period's deviation of the spot rate from the h-week forward rate.\n\n### Data / Model Specification\n\nThe estimated VECM equation for the change in the Dollar-Mark spot rate (`Δs_t`) includes the following terms:\n\n**Table 1. VECM Estimates for `Δs_t` Equation (Dollar-Mark)**\n\n| Explanatory Variable | Coeff. | SE |\n|:---|:---:|:---:|\n| `(s - f_4)_{t-1}` | -0.981 | 0.213 |\n| `(s - f_{13})_{t-1}` | -0.743 | 0.202 |\n\n### Question\n\nSuppose that in week `t-1`, the 4-week forward premium for the Dollar-Mark was `(s - f_4)_{t-1} = -0.02` (a 2% forward discount on the dollar). Assume all other explanatory variables in the `Δs_t` equation were zero. Based on the estimates in Table 1, what is the predicted change in the log spot rate, `Δs_t`, for the next week?", "Options": {"A": "+0.01962", "B": "-0.01962", "C": "+0.01486", "D": "-0.00426"}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a precise calculation using estimated model parameters, a fundamental skill for applying empirical models. It uses a Computational Judgment strategy. The correct answer (A) is found by multiplying the coefficient by the given value (`-0.981 * -0.02`). Distractor B is a 'Sign Error', a very common mistake in such calculations. Distractor C is a 'Formula Misuse' error, resulting from using the wrong coefficient (for the 13-week premium). Distractor D is another 'Formula Misuse' error, resulting from incorrectly using the standard error instead of the coefficient.", "qid": "332", "question": "### Background\n\n**Research Question.** This problem tests the quantitative application of the estimated Vector Error Correction Model (VECM), which links deviations from long-run equilibrium (the forward premiums) to short-run changes in the spot exchange rate.\n\n**Setting / Institutional Environment.** The VECM for the change in the spot rate (`Δs_t`) includes several lagged error correction terms, `(s - f_h)_{t-1}`, which represent the previous period's deviation of the spot rate from the h-week forward rate.\n\n### Data / Model Specification\n\nThe estimated VECM equation for the change in the Dollar-Mark spot rate (`Δs_t`) includes the following terms:\n\n**Table 1. VECM Estimates for `Δs_t` Equation (Dollar-Mark)**\n\n| Explanatory Variable | Coeff. | SE |\n|:---|:---:|:---:|\n| `(s - f_4)_{t-1}` | -0.981 | 0.213 |\n| `(s - f_{13})_{t-1}` | -0.743 | 0.202 |\n\n### Question\n\nSuppose that in week `t-1`, the 4-week forward premium for the Dollar-Mark was `(s - f_4)_{t-1} = -0.02` (a 2% forward discount on the dollar). Assume all other explanatory variables in the `Δs_t` equation were zero. Based on the estimates in Table 1, what is the predicted change in the log spot rate, `Δs_t`, for the next week?", "model_evaluation_result": {"model_raw_response": "I need to calculate the predicted change in the log spot rate using the VECM equation.\n\nGiven information:\n- The 4-week forward premium: (s - f_4)_{t-1} = -0.02\n- The coefficient for (s - f_4)_{t-1} is -0.981\n- All other explanatory variables are zero (including (s - f_{13})_{t-1})\n\nThe VECM equation for Δs_t includes:\nΔs_t = coefficient × (s - f_4)_{t-1} + coefficient × (s - f_{13})_{t-1} + other terms\n\nSince all other explanatory variables are zero, the predicted change is:\nΔs_t = -0.981 × (-0.02) + (-0.743) × (0) + 0\nΔs_t = -0.981 × (-0.02)\nΔs_t = +0.01962\n\nThe calculation shows that when the dollar is at a 2% forward discount (negative forward premium), the error correction mechanism predicts the spot rate will increase by approximately 0.01962 in the next week, which represents a correction toward equilibrium.\n\nA", "model_answer": "A", "error": null}}
{"ID": 328, "Question": "### Background\n\n**Research Question.** This problem examines the econometric challenges of testing for a threshold effect in an autoregressive model.\n\n**Setting.** In testing a linear AR model against a Threshold AR (TAR) model, the threshold parameter, `r`, is not identified under the null hypothesis of linearity.\n\n### Data / Model Specification\n\nThe Threshold Autoregressive TAR(p) model is:\n```latex\nX_{t}=\\phi_{0}+\\sum_{i=1}^{p}\\phi_{i}X_{t-i}+\\left(\\psi_{0}+\\sum_{i=1}^{p}\\psi_{i}X_{t-i}\\right)I(X_{t-d}\\leq r)+\\varepsilon_{t} \\quad \\text{(Eq. 1)}\n```\nThe null hypothesis of a linear AR(p) model is `H_0: \\boldsymbol{\\psi} = \\mathbf{0}`. To handle the unidentified parameter `r`, the test is constructed as the supremum of the Lagrange Multiplier (LM) statistics over a range of possible values for `r`:\n```latex\nT_{n}=\\operatorname*{sup}_{r\\in[r_{L},r_{U}]}T_{n}(r) \\quad \\text{(Eq. 2)}\n```\nIn testing for a threshold in the model given by Eq. (1), why is the `supLM` test statistic `T_n` in Eq. (2) used, and what is the nature of its asymptotic distribution under i.i.d. errors?", "Options": {"A": "The `sup` is taken because `r` is unknown under H₀; the resulting distribution is non-standard because it is the maximum of a stochastic process, not a single χ² variable.", "B": "The `sup` is taken because `r` is unknown under H₀; the resulting distribution is a χ² distribution with more degrees of freedom to account for the search over `r`.", "C": "The `sup` is taken to increase the test's power; the distribution is a standard χ² because the LM principle is robust to nuisance parameters.", "D": "The `sup` is taken because the model is nonlinear; the distribution is non-standard because the bootstrap is required for all nonlinear models."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses understanding of the fundamental 'nuisance parameter' problem and its solution via a `sup` test, a core concept in modern econometrics. It uses a **Reverse-Reasoning** strategy. The distractors target key misconceptions: (B) incorrectly assuming the distribution remains χ² (a classic error), (C) misstating the reason for the `sup` test and the nature of the distribution, and (D) providing a true but irrelevant premise with a faulty conclusion.", "qid": "328", "question": "### Background\n\n**Research Question.** This problem examines the econometric challenges of testing for a threshold effect in an autoregressive model.\n\n**Setting.** In testing a linear AR model against a Threshold AR (TAR) model, the threshold parameter, `r`, is not identified under the null hypothesis of linearity.\n\n### Data / Model Specification\n\nThe Threshold Autoregressive TAR(p) model is:\n```latex\nX_{t}=\\phi_{0}+\\sum_{i=1}^{p}\\phi_{i}X_{t-i}+\\left(\\psi_{0}+\\sum_{i=1}^{p}\\psi_{i}X_{t-i}\\right)I(X_{t-d}\\leq r)+\\varepsilon_{t} \\quad \\text{(Eq. 1)}\n```\nThe null hypothesis of a linear AR(p) model is `H_0: \\boldsymbol{\\psi} = \\mathbf{0}`. To handle the unidentified parameter `r`, the test is constructed as the supremum of the Lagrange Multiplier (LM) statistics over a range of possible values for `r`:\n```latex\nT_{n}=\\operatorname*{sup}_{r\\in[r_{L},r_{U}]}T_{n}(r) \\quad \\text{(Eq. 2)}\n```\nIn testing for a threshold in the model given by Eq. (1), why is the `supLM` test statistic `T_n` in Eq. (2) used, and what is the nature of its asymptotic distribution under i.i.d. errors?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 317, "Question": "### Background\n\n**Research Question.** This question examines the descriptive evidence supporting the paper's central theory that the make-or-buy decision for clinical trials is driven by the trade-off between 'knowledge production' and 'data production'.\n\n**Setting / Institutional Environment.** The paper's theory posits that firms insource knowledge-intensive projects and outsource data-intensive projects. The drug development process is sequential, with early phases (I-III) being more uncertain and knowledge-intensive than post-approval Phase IV trials. The paper notes that the available data exclude 'turnkey' outsourced trials, where the CRO handles all aspects, which may understate the true difference in outsourcing rates between phases.\n\n### Data / Model Specification\n\nThe following table provides descriptive statistics on outsourcing patterns across different trial phases.\n\n**Table 1: Distribution of Outsourcing Across Trial Phase**\n\n| Trial Phase | Mean Outsourcing Level | Freq. |\n|:---|---:|---:|\n| PHASE I Overall | 0.242 | 1,663 |\n| PHASE I—Oncology | 0.094 | 213 |\n| PHASE II | 0.139 | 1,594 |\n| PHASE III | 0.148 | 2,985 |\n| PHASE IV | 0.204 | 584 |\n\n### Question\n\nAssume the true number of Phase III trials was 10% higher than observed in Table 1, and that these missing trials were all 'turnkey' outsourced projects. Based on this assumption, what would be the adjusted outsourcing rate for Phase III trials?", "Options": {"A": "15.6%", "B": "22.5%", "C": "24.8%", "D": "14.8%"}, "Answer": "B", "pi_justification": "Assessment Target: This item tests the ability to perform a multi-step counterfactual calculation based on descriptive data. Strategy: Computational Judgment. Distractor Logic: Distractors are based on predictable errors. (A) is the correctly calculated adjusted rate for Phase II, not Phase III. (C) results from a common denominator error (dividing the new number of outsourced trials by the old total number of trials). (D) represents a conceptual error where the rate is assumed to be unchanged.", "qid": "317", "question": "### Background\n\n**Research Question.** This question examines the descriptive evidence supporting the paper's central theory that the make-or-buy decision for clinical trials is driven by the trade-off between 'knowledge production' and 'data production'.\n\n**Setting / Institutional Environment.** The paper's theory posits that firms insource knowledge-intensive projects and outsource data-intensive projects. The drug development process is sequential, with early phases (I-III) being more uncertain and knowledge-intensive than post-approval Phase IV trials. The paper notes that the available data exclude 'turnkey' outsourced trials, where the CRO handles all aspects, which may understate the true difference in outsourcing rates between phases.\n\n### Data / Model Specification\n\nThe following table provides descriptive statistics on outsourcing patterns across different trial phases.\n\n**Table 1: Distribution of Outsourcing Across Trial Phase**\n\n| Trial Phase | Mean Outsourcing Level | Freq. |\n|:---|---:|---:|\n| PHASE I Overall | 0.242 | 1,663 |\n| PHASE I—Oncology | 0.094 | 213 |\n| PHASE II | 0.139 | 1,594 |\n| PHASE III | 0.148 | 2,985 |\n| PHASE IV | 0.204 | 584 |\n\n### Question\n\nAssume the true number of Phase III trials was 10% higher than observed in Table 1, and that these missing trials were all 'turnkey' outsourced projects. Based on this assumption, what would be the adjusted outsourcing rate for Phase III trials?", "model_evaluation_result": {"model_raw_response": "I need to calculate the adjusted outsourcing rate for Phase III trials given that the true number was 10% higher and the missing trials were all 'turnkey' outsourced.\n\nFrom Table 1:\n- Observed Phase III trials: 2,985\n- Observed outsourcing rate: 0.148 (14.8%)\n\nStep 1: Calculate the number of currently outsourced Phase III trials\nCurrently outsourced = 2,985 × 0.148 = 441.78 ≈ 442 trials\n\nStep 2: Calculate the true total number of Phase III trials\nIf observed trials represent 90% of the true total (since true number is 10% higher):\nTrue total = 2,985 ÷ 0.9 = 3,316.67 ≈ 3,317 trials\n\nStep 3: Calculate the number of missing trials\nMissing trials = 3,317 - 2,985 = 332 trials\n\nStep 4: Calculate total outsourced trials (observed + missing)\nSince all missing trials were 'turnkey' outsourced:\nTotal outsourced = 442 + 332 = 774 trials\n\nStep 5: Calculate adjusted outsourcing rate\nAdjusted rate = 774 ÷ 3,317 = 0.233 = 23.3%\n\nThe closest option to 23.3% is 22.5%.\n\nB", "model_answer": "B", "error": null}}
{"ID": 276, "Question": "### Background\n\nA benevolent social planner chooses a threshold `t` to allocate consumers between two network goods, `a` and `b`, to maximize ex-ante expected aggregate welfare. The paper shows that aggregate welfare `W(t)` can be decomposed into three components:\n1.  **Vertical Quality Surplus:** `W_V(t, θ) = θ^a - 2θΦ((t-θ)√β)`\n2.  **Idiosyncratic Taste Surplus:** `W_H(t, θ) = √(2/(πβ)) * e^(-(t-θ)²β/2)`\n3.  **Network Effect Surplus:** `W_N(t, θ) = 2[Φ((t-θ)√β)]² - 2Φ((t-θ)√β) + 1`\n\nHere, `θ` is the realized relative quality of good `a`, `β` is the precision of idiosyncratic tastes, and `Φ(·)` is the standard normal CDF. The optimal allocation `t*` must balance the trade-offs between these three objectives.\n\n### Question\n\nA social planner analyzes the trade-offs in allocating consumers. Which of the following statements correctly identifies the allocation rule that maximizes one of the welfare components *in isolation*?", "Options": {"A": "The Network Effect Surplus is maximized by assigning all consumers to a single network (e.g., setting the threshold `t = -∞`).", "B": "The Idiosyncratic Taste Surplus is maximized by splitting consumers equally between the two networks (i.e., setting `t` such that `n^a = n^b = 0.5`).", "C": "The Vertical Quality Surplus is maximized by assigning consumers to the network with the highest *expected* quality `y`.", "D": "All three surplus components are maximized by a single, asymmetric allocation."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the core understanding of the fundamental trade-offs between network effects, vertical quality, and horizontal differentiation. Strategy: Conceptual Judgment. Distractor Logic: (B) is a classic misconception confusing equal splits with taste-based sorting. (C) is subtly wrong, confusing the ex-ante signal `y` with the ex-post realization `θ` relevant for this component. (D) is the conceptual opposite of the paper's main tension.", "qid": "276", "question": "### Background\n\nA benevolent social planner chooses a threshold `t` to allocate consumers between two network goods, `a` and `b`, to maximize ex-ante expected aggregate welfare. The paper shows that aggregate welfare `W(t)` can be decomposed into three components:\n1.  **Vertical Quality Surplus:** `W_V(t, θ) = θ^a - 2θΦ((t-θ)√β)`\n2.  **Idiosyncratic Taste Surplus:** `W_H(t, θ) = √(2/(πβ)) * e^(-(t-θ)²β/2)`\n3.  **Network Effect Surplus:** `W_N(t, θ) = 2[Φ((t-θ)√β)]² - 2Φ((t-θ)√β) + 1`\n\nHere, `θ` is the realized relative quality of good `a`, `β` is the precision of idiosyncratic tastes, and `Φ(·)` is the standard normal CDF. The optimal allocation `t*` must balance the trade-offs between these three objectives.\n\n### Question\n\nA social planner analyzes the trade-offs in allocating consumers. Which of the following statements correctly identifies the allocation rule that maximizes one of the welfare components *in isolation*?", "model_evaluation_result": {"model_raw_response": "I need to analyze each welfare component to determine which allocation rule maximizes each one in isolation.\n\nLet me examine each component:\n\n**1. Network Effect Surplus: W_N(t, θ) = 2[Φ((t-θ)√β)]² - 2Φ((t-θ)√β) + 1**\n\nLet p = Φ((t-θ)√β), which represents the proportion of consumers choosing network a.\nThen W_N = 2p² - 2p + 1 = 2p(p-1) + 1\n\nTaking the derivative with respect to p: dW_N/dp = 4p - 2\nSetting equal to zero: 4p - 2 = 0, so p = 0.5\n\nBut let's check the endpoints:\n- When p = 0: W_N = 1\n- When p = 1: W_N = 1  \n- When p = 0.5: W_N = 2(0.25) - 2(0.5) + 1 = 0.5 - 1 + 1 = 0.5\n\nSo the network effect surplus is maximized when all consumers go to one network (p = 0 or p = 1), giving W_N = 1.\n\n**2. Idiosyncratic Taste Surplus: W_H(t, θ) = √(2/(πβ)) * e^(-(t-θ)²β/2)**\n\nThis is maximized when the exponent is maximized, which occurs when (t-θ)² is minimized. This happens when t = θ, meaning the threshold equals the realized quality difference.\n\n**3. Vertical Quality Surplus: W_V(t, θ) = θ^a - 2θΦ((t-θ)√β)**\n\nThis depends on θ^a (which seems to be θ times the indicator that network a has higher quality) minus a term involving the allocation proportion.\n\nNow checking the options:\n\nA) Correct - Network Effect Surplus is indeed maximized by assigning all consumers to a single network (t = -∞ or t = +", "model_answer": "A", "error": null}}
{"ID": 389, "Question": "### Background\n\n**Research Question.** This problem evaluates whether a parsimonious, theory-driven parametric model is superior to a flexible, data-driven nonparametric model for capturing the forecastable structure of asset returns, with a focus on diagnosing 'overfitting'.\n\n**Setting.** The paper compares its primary parametric GARCH-M model against a nonparametric kernel regression model using both in-sample diagnostics (BDS test) and an out-of-sample forecasting competition.\n\n**Variables and Parameters.**\n*   `BDS Statistic`: A test for the null hypothesis that a series is independently and identically distributed (i.i.d.). A value greater than 1.96 is significant at the 5% level.\n*   `MSE`: Mean Squared Error of out-of-sample forecasts.\n*   `R²`: R-squared from a regression of actual returns on forecasts.\n\n---\n\n### Data / Model Specification\n\n**Table 1: In-Sample BDS Statistics for Model Residuals**\n\n| Embedding Dimension | Raw Series | Parametric Residuals | Kernel Residuals |\n| :--- | :--- | :--- | :--- |\n| 5 | 2.0378 | 0.2290 | -0.1170 |\n| 7 | 2.6651 | 1.1762 | 0.2056 |\n\n**Table 2: Out-of-Sample Forecast Evaluation**\n\n| Model | R² | MSE |\n| :--- | :--- | :--- |\n| Kernel | 0.0428 | 0.0056 |\n| Parametric | 0.0589 | 0.0050 |\n\n---\n\n### Question\n\nTable 1 shows that the residuals from both the parametric and kernel models appear to be i.i.d. in-sample (BDS statistics are insignificant). Table 2 compares their out-of-sample forecast performance. What is the main conclusion from this combined evidence?", "Options": {"A": "The kernel model is superior because its in-sample residuals pass the BDS test, indicating a better fit to the data's structure.", "B": "Both models are equally adequate because their in-sample residuals pass the BDS test and their out-of-sample performance metrics are very close.", "C": "The kernel model is superior because its out-of-sample MSE is higher, indicating it captured more of the return variance.", "D": "The parametric model is superior because its better out-of-sample performance (lower MSE, higher R²) suggests the kernel model's good in-sample fit was due to overfitting."}, "Answer": "D", "pi_justification": "Assesses the ability to synthesize in-sample and out-of-sample evidence to correctly diagnose overfitting. This is a Scenario Application item. The correct option (D) properly concludes that out-of-sample superiority (lower MSE, higher R²) is the decisive factor and correctly attributes the kernel model's poorer performance to overfitting. Distractor A incorrectly prioritizes in-sample evidence. Distractor B fails to make a decisive choice based on the clear OOS advantage. Distractor C misinterprets MSE (lower is better).", "qid": "389", "question": "### Background\n\n**Research Question.** This problem evaluates whether a parsimonious, theory-driven parametric model is superior to a flexible, data-driven nonparametric model for capturing the forecastable structure of asset returns, with a focus on diagnosing 'overfitting'.\n\n**Setting.** The paper compares its primary parametric GARCH-M model against a nonparametric kernel regression model using both in-sample diagnostics (BDS test) and an out-of-sample forecasting competition.\n\n**Variables and Parameters.**\n*   `BDS Statistic`: A test for the null hypothesis that a series is independently and identically distributed (i.i.d.). A value greater than 1.96 is significant at the 5% level.\n*   `MSE`: Mean Squared Error of out-of-sample forecasts.\n*   `R²`: R-squared from a regression of actual returns on forecasts.\n\n---\n\n### Data / Model Specification\n\n**Table 1: In-Sample BDS Statistics for Model Residuals**\n\n| Embedding Dimension | Raw Series | Parametric Residuals | Kernel Residuals |\n| :--- | :--- | :--- | :--- |\n| 5 | 2.0378 | 0.2290 | -0.1170 |\n| 7 | 2.6651 | 1.1762 | 0.2056 |\n\n**Table 2: Out-of-Sample Forecast Evaluation**\n\n| Model | R² | MSE |\n| :--- | :--- | :--- |\n| Kernel | 0.0428 | 0.0056 |\n| Parametric | 0.0589 | 0.0050 |\n\n---\n\n### Question\n\nTable 1 shows that the residuals from both the parametric and kernel models appear to be i.i.d. in-sample (BDS statistics are insignificant). Table 2 compares their out-of-sample forecast performance. What is the main conclusion from this combined evidence?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 289, "Question": "### Background\n\n**Research Question.** This problem addresses the dynamic nature of the response to a law change, distinguishing between a temporary \"backlog\" effect and a permanent shift.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel (1968-1988). The model must correctly specify the post-treatment effects to accurately measure the law's impact over time.\n\n### Data / Model Specification\n\nTo model dynamics, the paper's preferred specification is augmented with lag variables for the policy change:\n\n```latex\n\\text{divrate}_{st} = \\beta \\cdot \\mathrm{unilateral}_{st} + \\delta_0 D_{st}^0 + \\delta_1 D_{st}^1 + \\delta_2 D_{st}^2 + \\text{Controls} + u_{st} \\quad \\text{(Eq. (1))}\n```\n- `unilateral_st`: Indicator that is 1 for all years a law is in effect.\n- `D_st^0`, `D_st^1`, `D_st^2`: \"Lag\" dummies, 1 only in the year of, one year after, and two years after the law change, respectively.\n- `β`: The estimated long-run effect of the law (for 3+ years post-adoption).\n- `δ_k`: The estimated deviation from the long-run effect in period `k`.\n\n**Table 1: Dynamic Model Results**\n\n| Variable | (1) Dynamic Model |\n| :--- | :---: |\n| **Unilateral** (`β`) | 0.536 (0.065) |\n| **Dummy for law change occurring:** | |\n| In this period (`δ_0`) | -0.117 (0.069) |\n| 1 period ago (`δ_1`) | -0.090 (0.066) |\n| 2 periods ago (`δ_2`) | -0.108 (0.062) |\n\n*Notes: Dependent variable is the divorce rate per 1,000 people. The model includes state/year FE and state-specific trends.*\n\nThe model in Table 1 allows the law's effect to vary over time. Using the estimated coefficients, what is the total estimated impact of the unilateral divorce law on the divorce rate *in the year of the law change*?", "Options": {"A": "0.419", "B": "0.536", "C": "-0.117", "D": "0.446"}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to correctly calculate dynamic treatment effects from a regression with lag dummies. It uses a Computational Judgment strategy. The distractors are based on a design matrix of common errors: (B) reporting the long-run coefficient instead of the specific period's effect (Formula Misuse), (C) reporting only the deviation dummy instead of the total effect (Step-Omission Error), and (D) calculating the effect for the wrong period (Numerical Error).", "qid": "289", "question": "### Background\n\n**Research Question.** This problem addresses the dynamic nature of the response to a law change, distinguishing between a temporary \"backlog\" effect and a permanent shift.\n\n**Setting / Institutional Environment.** The analysis uses a state-year panel (1968-1988). The model must correctly specify the post-treatment effects to accurately measure the law's impact over time.\n\n### Data / Model Specification\n\nTo model dynamics, the paper's preferred specification is augmented with lag variables for the policy change:\n\n```latex\n\\text{divrate}_{st} = \\beta \\cdot \\mathrm{unilateral}_{st} + \\delta_0 D_{st}^0 + \\delta_1 D_{st}^1 + \\delta_2 D_{st}^2 + \\text{Controls} + u_{st} \\quad \\text{(Eq. (1))}\n```\n- `unilateral_st`: Indicator that is 1 for all years a law is in effect.\n- `D_st^0`, `D_st^1`, `D_st^2`: \"Lag\" dummies, 1 only in the year of, one year after, and two years after the law change, respectively.\n- `β`: The estimated long-run effect of the law (for 3+ years post-adoption).\n- `δ_k`: The estimated deviation from the long-run effect in period `k`.\n\n**Table 1: Dynamic Model Results**\n\n| Variable | (1) Dynamic Model |\n| :--- | :---: |\n| **Unilateral** (`β`) | 0.536 (0.065) |\n| **Dummy for law change occurring:** | |\n| In this period (`δ_0`) | -0.117 (0.069) |\n| 1 period ago (`δ_1`) | -0.090 (0.066) |\n| 2 periods ago (`δ_2`) | -0.108 (0.062) |\n\n*Notes: Dependent variable is the divorce rate per 1,000 people. The model includes state/year FE and state-specific trends.*\n\nThe model in Table 1 allows the law's effect to vary over time. Using the estimated coefficients, what is the total estimated impact of the unilateral divorce law on the divorce rate *in the year of the law change*?", "model_evaluation_result": {"model_raw_response": "Looking at the dynamic model specification, I need to determine the total impact in the year of the law change.\n\nIn the year of the law change:\n- `unilateral_st = 1` (the law is in effect)\n- `D_st^0 = 1` (this is the year of the law change)\n- `D_st^1 = 0` and `D_st^2 = 0` (these are for future years)\n\nThe total effect in the year of the law change is:\nβ × 1 + δ₀ × 1 + δ₁ × 0 + δ₂ × 0 = β + δ₀\n\nFrom Table 1:\n- β = 0.536 (long-run effect)\n- δ₀ = -0.117 (deviation in the year of change)\n\nTotal impact = 0.536 + (-0.117) = 0.419\n\nA", "model_answer": "A", "error": null}}
{"ID": 318, "Question": "### Background\n\n**Research Question.** This problem analyzes the bias in regulatory assessments of firm conduct when the regulator uses an incomplete or static measure of marginal cost in a dynamic setting with capital adjustment costs.\n\n**Setting.** A regulator observes a firm's price and can estimate a proxy for its marginal cost, but this proxy omits a forward-looking component related to future investment plans, which are private information to the firm.\n\n**Variables and Parameters.**\n- `x_t`: Level of the productive input (capital) at time `t`.\n- `I_t`: Gross investment at time `t`.\n- `c(I)`: A strictly convex cost function for investment, `c''(I) > 0`.\n- `f'(x)`: Marginal product of input `x`.\n- `\\mathcal{MC}_t^d`: True dynamic marginal cost of output at time `t`.\n- `r`: Interest rate.\n- `\\delta`: Depreciation rate.\n- `\\eta`: Capital survival rate, `\\eta = 1 - \\delta`.\n\n---\n\n### Data / Model Specification\n\nThe true dynamic marginal cost of output is:\n```latex\n\\mathcal{MC}_{t}^{d}=\\frac{(1+r)[1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})]}{f^{\\prime}(x_{t})} \n```\nThis can be decomposed as `\\mathcal{MC}_{t}^{d} = \\widehat{MC}_{t} + \\widehat{E}_{t}`, where `\\widehat{MC}_{t} = \\frac{(r+\\delta)[1+c'(I_{t-1})]}{f'(x_t)}` is an observable proxy and `\\widehat{E}_{t}` is an unobservable error term.\n\n---\n\n### Question\n\nGiven the decomposition `\\mathcal{MC}_{t}^{d} = \\widehat{MC}_{t} + \\widehat{E}_{t}`, what is the correct formula for the error term `\\widehat{E}_{t}` and what is its sign during a period of investment expansion (i.e., `I_t > I_{t-1}`)?", "Options": {"A": "`\\widehat{E}_{t} = \\frac{\\eta[c'(I_{t-1}) - c'(I_t)]}{f'(x_t)}`; Sign is negative.", "B": "`\\widehat{E}_{t} = \\frac{\\eta[c'(I_{t-1}) - c'(I_t)]}{f'(x_t)}`; Sign is positive.", "C": "`\\widehat{E}_{t} = \\frac{\\eta[c'(I_{t}) - c'(I_{t-1})]}{f'(x_t)}`; Sign is positive.", "D": "`\\widehat{E}_{t} = \\frac{(r+\\delta)[c'(I_{t-1}) - c'(I_t)]}{f'(x_t)}`; Sign is negative."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to perform a key algebraic derivation and apply a convexity assumption to determine the direction of bias in marginal cost measurement. It uses a Computational Judgment strategy. The distractors are designed to catch common errors: a sign error from misinterpreting convexity (B), an algebraic slip in the derivation (C), and using an incorrect but plausible term from a related formula (D).", "qid": "318", "question": "### Background\n\n**Research Question.** This problem analyzes the bias in regulatory assessments of firm conduct when the regulator uses an incomplete or static measure of marginal cost in a dynamic setting with capital adjustment costs.\n\n**Setting.** A regulator observes a firm's price and can estimate a proxy for its marginal cost, but this proxy omits a forward-looking component related to future investment plans, which are private information to the firm.\n\n**Variables and Parameters.**\n- `x_t`: Level of the productive input (capital) at time `t`.\n- `I_t`: Gross investment at time `t`.\n- `c(I)`: A strictly convex cost function for investment, `c''(I) > 0`.\n- `f'(x)`: Marginal product of input `x`.\n- `\\mathcal{MC}_t^d`: True dynamic marginal cost of output at time `t`.\n- `r`: Interest rate.\n- `\\delta`: Depreciation rate.\n- `\\eta`: Capital survival rate, `\\eta = 1 - \\delta`.\n\n---\n\n### Data / Model Specification\n\nThe true dynamic marginal cost of output is:\n```latex\n\\mathcal{MC}_{t}^{d}=\\frac{(1+r)[1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})]}{f^{\\prime}(x_{t})} \n```\nThis can be decomposed as `\\mathcal{MC}_{t}^{d} = \\widehat{MC}_{t} + \\widehat{E}_{t}`, where `\\widehat{MC}_{t} = \\frac{(r+\\delta)[1+c'(I_{t-1})]}{f'(x_t)}` is an observable proxy and `\\widehat{E}_{t}` is an unobservable error term.\n\n---\n\n### Question\n\nGiven the decomposition `\\mathcal{MC}_{t}^{d} = \\widehat{MC}_{t} + \\widehat{E}_{t}`, what is the correct formula for the error term `\\widehat{E}_{t}` and what is its sign during a period of investment expansion (i.e., `I_t > I_{t-1}`)?", "model_evaluation_result": {"model_raw_response": "I need to find the error term $\\widehat{E}_{t}$ by subtracting $\\widehat{MC}_{t}$ from $\\mathcal{MC}_{t}^{d}$.\n\nGiven:\n- $\\mathcal{MC}_{t}^{d} = \\frac{(1+r)[1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})]}{f^{\\prime}(x_{t})}$\n- $\\widehat{MC}_{t} = \\frac{(r+\\delta)[1+c'(I_{t-1})]}{f'(x_t)}$\n- $\\eta = 1 - \\delta$, so $(1+r) = (r+\\delta) + (1-\\delta) = (r+\\delta) + \\eta$\n\nLet me calculate $\\widehat{E}_{t} = \\mathcal{MC}_{t}^{d} - \\widehat{MC}_{t}$:\n\n$\\widehat{E}_{t} = \\frac{(1+r)[1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})]}{f^{\\prime}(x_{t})} - \\frac{(r+\\delta)[1+c'(I_{t-1})]}{f'(x_t)}$\n\n$= \\frac{(1+r)[1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})] - (r+\\delta)[1+c'(I_{t-1})]}{f^{\\prime}(x_{t})}$\n\nSince $(1+r) = (r+\\delta) + \\eta$:\n\n$= \\frac{[(r+\\delta) + \\eta][1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})] - (r+\\delta)[1+c'(I_{", "model_answer": "I need to find the error term $\\widehat{E}_{t}$ by subtracting $\\widehat{MC}_{t}$ from $\\mathcal{MC}_{t}^{d}$.\n\nGiven:\n- $\\mathcal{MC}_{t}^{d} = \\frac{(1+r)[1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})]}{f^{\\prime}(x_{t})}$\n- $\\widehat{MC}_{t} = \\frac{(r+\\delta)[1+c'(I_{t-1})]}{f'(x_t)}$\n- $\\eta = 1 - \\delta$, so $(1+r) = (r+\\delta) + (1-\\delta) = (r+\\delta) + \\eta$\n\nLet me calculate $\\widehat{E}_{t} = \\mathcal{MC}_{t}^{d} - \\widehat{MC}_{t}$:\n\n$\\widehat{E}_{t} = \\frac{(1+r)[1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})]}{f^{\\prime}(x_{t})} - \\frac{(r+\\delta)[1+c'(I_{t-1})]}{f'(x_t)}$\n\n$= \\frac{(1+r)[1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})] - (r+\\delta)[1+c'(I_{t-1})]}{f^{\\prime}(x_{t})}$\n\nSince $(1+r) = (r+\\delta) + \\eta$:\n\n$= \\frac{[(r+\\delta) + \\eta][1+c^{\\prime}(I_{t-1})]-\\eta[1+c^{\\prime}(I_{t})] - (r+\\delta)[1+c'(I_{", "error": "Warning: Could not extract single choice letter from: I need to find the error term $\\widehat{E}_{t}$ by..."}}
{"ID": 343, "Question": "### Background\n\n**Research Question.** This question assesses the ability to apply the paper's main empirical finding to a concrete scenario.\n\n**Setting / Institutional Environment.** The analysis uses a panel of 26 advanced economies from 1970-2013 to estimate the effect of Employment Protection Legislation (EPL) deregulation on the labor share.\n\n### Data / Model Specification\n\nThe paper estimates the aggregate dynamic response of the labor share to a reform using a country-level model. The baseline results are presented in Table 1.\n\n**Table 1: Country-Level Analysis – Baseline Labor Share Results**\n\n| Specification | Impact | 1y | 2y | 3y | 4y | 5y |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| **Baseline** | -0.26* | -0.60* | -0.95** | -0.70** | -0.75* | -0.87** |\n| | (0.13) | (0.34) | (0.39) | (0.32) | (0.40) | (0.35) |\n\n*Notes: Coefficients are the estimated effect in percentage points for a liberalizing reform. Standard errors in parentheses. *, **, *** indicate significance at the 90%, 95%, and 99% level.*\n\n### Question\n\nSuppose a country's labor share fell by a total of 7.0 percentage points over the study period. During this time, it implemented a net of two major liberalizing EPL reforms. Based on the 5-year estimate from the baseline model in Table 1, what percentage of this country's total labor share decline can be attributed to these EPL reforms?", "Options": {"A": "12.4%", "B": "1.74%", "C": "24.9%", "D": "28.6%"}, "Answer": "C", "pi_justification": "This item uses a Computational Judgment strategy to assess the student's ability to apply a key empirical finding to a concrete scenario. It requires extracting the correct coefficient, performing a multi-step calculation, and interpreting the result. Distractors are designed to catch common errors: omitting a step (A, forgetting to multiply by 2 reforms), misinterpreting the final unit (B, reporting the percentage point change), and misusing the formula (D).", "qid": "343", "question": "### Background\n\n**Research Question.** This question assesses the ability to apply the paper's main empirical finding to a concrete scenario.\n\n**Setting / Institutional Environment.** The analysis uses a panel of 26 advanced economies from 1970-2013 to estimate the effect of Employment Protection Legislation (EPL) deregulation on the labor share.\n\n### Data / Model Specification\n\nThe paper estimates the aggregate dynamic response of the labor share to a reform using a country-level model. The baseline results are presented in Table 1.\n\n**Table 1: Country-Level Analysis – Baseline Labor Share Results**\n\n| Specification | Impact | 1y | 2y | 3y | 4y | 5y |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| **Baseline** | -0.26* | -0.60* | -0.95** | -0.70** | -0.75* | -0.87** |\n| | (0.13) | (0.34) | (0.39) | (0.32) | (0.40) | (0.35) |\n\n*Notes: Coefficients are the estimated effect in percentage points for a liberalizing reform. Standard errors in parentheses. *, **, *** indicate significance at the 90%, 95%, and 99% level.*\n\n### Question\n\nSuppose a country's labor share fell by a total of 7.0 percentage points over the study period. During this time, it implemented a net of two major liberalizing EPL reforms. Based on the 5-year estimate from the baseline model in Table 1, what percentage of this country's total labor share decline can be attributed to these EPL reforms?", "model_evaluation_result": {"model_raw_response": "I need to calculate what percentage of the total labor share decline can be attributed to the EPL reforms.\n\nGiven information:\n- Total labor share decline: 7.0 percentage points\n- Number of liberalizing EPL reforms: 2\n- 5-year effect per reform: -0.87 percentage points\n\nStep 1: Calculate the total effect of the two EPL reforms\nTotal EPL effect = 2 reforms × (-0.87 percentage points per reform) = -1.74 percentage points\n\nStep 2: Calculate what percentage this represents of the total decline\nPercentage attributable to EPL = (1.74 / 7.0) × 100% = 24.86% ≈ 24.9%\n\nThe EPL reforms account for 1.74 percentage points out of the total 7.0 percentage point decline, which equals approximately 24.9% of the total decline.\n\nC", "model_answer": "C", "error": null}}
{"ID": 352, "Question": "### Background\n\n**Research Question.** This problem evaluates the empirical gains from using an asymptotically efficient estimator (2S2SLS) versus a consistent but inefficient one (NL2SLS) in a structural macroeconomic model. It further explores how estimator choice can impact policy conclusions and how to test for systemic model misspecification.\n\n**Setting / Institutional Environment.** The analysis uses limited-information (single-equation) instrumental variable methods to estimate the parameters of Taylor's (1979) model of the U.S. economy. The model's structural equations contain both rational expectations terms and serially correlated disturbances, a setting where 2S2SLS is theoretically more efficient than NL2SLS. The problem also contrasts these limited-information estimates with full-system estimates, which are efficient under correct specification but vulnerable to misspecification anywhere in the system.\n\n### Data / Model Specification\n\nTaylor's model consists of an aggregate demand (output) equation and an aggregate supply (price-setting) equation:\n\n```latex\ny_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\beta_3(m_t - p_t) + \\beta_4(m_{t-1} - p_{t-1}) + \\beta_5(_{t-1}\\pi_t) + \\beta_0 + u_t \n```\n```latex\n\\pi_t = \\pi_{t-1} + \\gamma_1(_{t-1}y_t) + \\gamma_0 + v_t \n```\nwhere `y_t` is the output gap, `π_t` is the inflation rate, `(m_t - p_t)` are log real money balances, `_{t-1}π_t` is the rational expectation of inflation, and `_{t-1}y_t` is the rational expectation of the output gap. The structural disturbances `u_t` and `v_t` are known to be serially correlated.\n\nTable 1 presents estimation results for the period 1954:I - 1975:I from three methods: 2S2SLS, NL2SLS, and Taylor's full-system method.\n\n**Table 1: Limited-information and Full-system Estimates of Taylor's Model**\n\n| | **Output Equation** | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Estimator** | `y_{t-1}` (`β_1`) | `y_{t-2}` | `(m_t-p_t)` | `(m_{t-1}-p_{t-1})` | `_{t-1}π_t` |\n| 2S2SLS | 1.354 (0.108) | -0.484 (0.093) | 0.620 (0.217) | -0.541 (0.203) | -0.164 (0.133) |\n| NL2SLS | 1.423 (0.166) | -0.552 (0.185) | 0.599 (0.230) | -0.532 (0.217) | -0.041 (0.188) |\n| Taylor | 1.167 (0.088) | -0.324 (0.090) | 0.578 (0.175) | -0.484 (0.194) | -0.447 (0.391) |\n| | **Price Equation** | | | | |\n| **Estimator** | `π_{t-1}` | `_{t-1}y_t` (`γ_1`) | `Constant` | | |\n| 2S2SLS | 1.0 (fixed) | 0.0164 (0.0070) | 0.398 (0.252) | | |\n| NL2SLS | 1.0 (fixed) | 0.0132 (0.0109) | 0.304 (0.340) | | |\n| Taylor | 1.0 (fixed) | 0.0180 (0.0058) | 0.515 (0.172) | | |\n\n*Standard errors are in parentheses.* \n\nUsing the 2S2SLS estimates for the output equation from Table 1, test the null hypothesis that the coefficient on the first lag of output (`β_1`) is equal to 1 (`H_0: β_1 = 1`). At the 5% significance level (two-sided critical value ≈ 1.96), what is the resulting t-statistic and conclusion?", "Options": {"A": "The t-statistic is approximately 3.28, leading to a rejection of the null hypothesis.", "B": "The t-statistic is approximately 2.55, leading to a rejection of the null hypothesis.", "C": "The t-statistic is approximately 1.23, leading to a failure to reject the null hypothesis.", "D": "The t-statistic is approximately 12.53, leading to a rejection of the null hypothesis."}, "Answer": "A", "pi_justification": "This item assesses the ability to perform and interpret a standard hypothesis test using table data, a core skill in empirical analysis. It uses a Computational Judgment strategy. The correct option (A) requires identifying the correct estimator's results, applying the t-statistic formula, and comparing to the critical value. Distractor (B) uses data from the less efficient NL2SLS estimator, testing attention to detail. Distractor (C) presents a plausible but incorrect calculation that leads to the wrong conclusion. Distractor (D) is based on the common error of testing against a null of zero instead of the specified null of one.", "qid": "352", "question": "### Background\n\n**Research Question.** This problem evaluates the empirical gains from using an asymptotically efficient estimator (2S2SLS) versus a consistent but inefficient one (NL2SLS) in a structural macroeconomic model. It further explores how estimator choice can impact policy conclusions and how to test for systemic model misspecification.\n\n**Setting / Institutional Environment.** The analysis uses limited-information (single-equation) instrumental variable methods to estimate the parameters of Taylor's (1979) model of the U.S. economy. The model's structural equations contain both rational expectations terms and serially correlated disturbances, a setting where 2S2SLS is theoretically more efficient than NL2SLS. The problem also contrasts these limited-information estimates with full-system estimates, which are efficient under correct specification but vulnerable to misspecification anywhere in the system.\n\n### Data / Model Specification\n\nTaylor's model consists of an aggregate demand (output) equation and an aggregate supply (price-setting) equation:\n\n```latex\ny_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\beta_3(m_t - p_t) + \\beta_4(m_{t-1} - p_{t-1}) + \\beta_5(_{t-1}\\pi_t) + \\beta_0 + u_t \n```\n```latex\n\\pi_t = \\pi_{t-1} + \\gamma_1(_{t-1}y_t) + \\gamma_0 + v_t \n```\nwhere `y_t` is the output gap, `π_t` is the inflation rate, `(m_t - p_t)` are log real money balances, `_{t-1}π_t` is the rational expectation of inflation, and `_{t-1}y_t` is the rational expectation of the output gap. The structural disturbances `u_t` and `v_t` are known to be serially correlated.\n\nTable 1 presents estimation results for the period 1954:I - 1975:I from three methods: 2S2SLS, NL2SLS, and Taylor's full-system method.\n\n**Table 1: Limited-information and Full-system Estimates of Taylor's Model**\n\n| | **Output Equation** | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Estimator** | `y_{t-1}` (`β_1`) | `y_{t-2}` | `(m_t-p_t)` | `(m_{t-1}-p_{t-1})` | `_{t-1}π_t` |\n| 2S2SLS | 1.354 (0.108) | -0.484 (0.093) | 0.620 (0.217) | -0.541 (0.203) | -0.164 (0.133) |\n| NL2SLS | 1.423 (0.166) | -0.552 (0.185) | 0.599 (0.230) | -0.532 (0.217) | -0.041 (0.188) |\n| Taylor | 1.167 (0.088) | -0.324 (0.090) | 0.578 (0.175) | -0.484 (0.194) | -0.447 (0.391) |\n| | **Price Equation** | | | | |\n| **Estimator** | `π_{t-1}` | `_{t-1}y_t` (`γ_1`) | `Constant` | | |\n| 2S2SLS | 1.0 (fixed) | 0.0164 (0.0070) | 0.398 (0.252) | | |\n| NL2SLS | 1.0 (fixed) | 0.0132 (0.0109) | 0.304 (0.340) | | |\n| Taylor | 1.0 (fixed) | 0.0180 (0.0058) | 0.515 (0.172) | | |\n\n*Standard errors are in parentheses.* \n\nUsing the 2S2SLS estimates for the output equation from Table 1, test the null hypothesis that the coefficient on the first lag of output (`β_1`) is equal to 1 (`H_0: β_1 = 1`). At the 5% significance level (two-sided critical value ≈ 1.96), what is the resulting t-statistic and conclusion?", "model_evaluation_result": {"model_raw_response": "I need to calculate the t-statistic for testing H₀: β₁ = 1 using the 2S2SLS estimates from Table 1.\n\nFrom the table, for the output equation using 2S2SLS:\n- Coefficient estimate: β̂₁ = 1.354\n- Standard error: SE(β̂₁) = 0.108\n\nThe t-statistic for testing H₀: β₁ = 1 is:\n\nt = (β̂₁ - 1) / SE(β̂₁)\nt = (1.354 - 1) / 0.108\nt = 0.354 / 0.108\nt = 3.278\n\nRounding to two decimal places: t ≈ 3.28\n\nSince |3.28| > 1.96 (the critical value at 5% significance level for a two-sided test), we reject the null hypothesis.\n\nA", "model_answer": "A", "error": null}}
{"ID": 269, "Question": "### Background\n\nAn agent solves a stationary consumption problem, choosing non-drug consumption `c` and drug consumption `d` in each period. The drug is addictive, meaning the temptation-weighting function `σ(s)` is increasing in past consumption `s`.\n\n### Data / Model Specification\n\nFor an interior solution `d_t`, the choice satisfies the first-order condition (FOC), where `d_{t-1}` is the state, `d_{t+1}` is optimal future consumption, and `q` is the maximum legally allowed consumption:\n\n```latex\n-p u'(1-p d_t) + \\sigma(d_{t-1})v'(d_t) + \\delta\\sigma'(d_t)(v(d_{t+1}) - v(q)) = 0 \\quad \\text{(Eq. 1)}\n```\n\n### Question\n\nA new government policy reduces the legal maximum consumption from `q` to `q' < q`. The agent's consumption `d_t` remains strictly below this new, lower cap. According to the model, how does this \"non-binding\" prohibitive policy affect the agent's current drug demand `d_t`?", "Options": {"A": "It increases `d_t`, because the future cost of self-control is now lower, making current consumption less costly in dynamic terms.", "B": "It decreases `d_t`, because the policy signals a stronger anti-drug stance, encouraging less consumption.", "C": "It has no effect on `d_t`, because the new cap `q'` is not binding on the agent's choice.", "D": "It decreases `d_t`, because a lower `q` makes the term `(v(d_{t+1}) - v(q))` larger, increasing the marginal cost of `d_t`."}, "Answer": "A", "pi_justification": "This item assesses a key, counter-intuitive comparative static (Proposition 6) using a Scenario Application strategy. The correct answer, A, includes the economic intuition, making it a strong assessment tool. Distractor C (Step-Omission Error) represents the naive intuition that a non-binding constraint has no effect, ignoring the dynamic term in the FOC. Distractor D (Sign Error) represents a plausible but incorrect calculation of the FOC's response. Distractor B (True but Irrelevant) offers a narrative explanation outside the model's formal logic.", "qid": "269", "question": "### Background\n\nAn agent solves a stationary consumption problem, choosing non-drug consumption `c` and drug consumption `d` in each period. The drug is addictive, meaning the temptation-weighting function `σ(s)` is increasing in past consumption `s`.\n\n### Data / Model Specification\n\nFor an interior solution `d_t`, the choice satisfies the first-order condition (FOC), where `d_{t-1}` is the state, `d_{t+1}` is optimal future consumption, and `q` is the maximum legally allowed consumption:\n\n```latex\n-p u'(1-p d_t) + \\sigma(d_{t-1})v'(d_t) + \\delta\\sigma'(d_t)(v(d_{t+1}) - v(q)) = 0 \\quad \\text{(Eq. 1)}\n```\n\n### Question\n\nA new government policy reduces the legal maximum consumption from `q` to `q' < q`. The agent's consumption `d_t` remains strictly below this new, lower cap. According to the model, how does this \"non-binding\" prohibitive policy affect the agent's current drug demand `d_t`?", "model_evaluation_result": {"model_raw_response": "Looking at the first-order condition:\n\n```latex\n-p u'(1-p d_t) + \\sigma(d_{t-1})v'(d_t) + \\delta\\sigma'(d_t)(v(d_{t+1}) - v(q)) = 0\n```\n\nThe key insight is to examine how a reduction in the legal maximum from `q` to `q' < q` affects this equation, even when the agent's consumption remains below the new cap.\n\nThe critical term is `(v(d_{t+1}) - v(q))` in the third component. When `q` decreases to `q'`:\n\n1. The term becomes `(v(d_{t+1}) - v(q'))`\n2. Since `q' < q` and `v(·)` is increasing (utility from drug consumption), we have `v(q') < v(q)`\n3. Therefore: `(v(d_{t+1}) - v(q')) > (v(d_{t+1}) - v(q))`\n\nThis means the third term `δσ'(d_t)(v(d_{t+1}) - v(q))` becomes larger (more positive) when `q` is reduced to `q'`.\n\nFor the FOC to hold (equal zero), if the third term increases, the other terms must adjust. Since `σ'(d_t) > 0` (addiction makes the agent more sensitive to future consequences), a larger third term means higher marginal cost of current consumption `d_t`.\n\nTo restore equilibrium, `d_t` must decrease, which:\n- Increases the first term (making it less negative)\n- Decreases the second term\n- Decreases the third term back toward its original level\n\nThe economic intuition: Even though the policy doesn't directly constrain the agent, it reduces the future \"escape valve\" of maximum legal consumption, making the long-term consequences of current addiction more severe.\n\nD", "model_answer": "D", "error": null}}
{"ID": 282, "Question": "### Background\n\nIn a sequential *demand game* for a natural monopoly, players are arranged in a fixed order `w`. The unique subgame perfect equilibrium payoff vector `d_N` is determined via backward induction. The last player's payoff is their autarky value, and each preceding player's payoff is the maximum they can claim while leaving enough surplus for all subsequent players to achieve their predetermined equilibrium payoffs.\n\n### Data / Model Specification\n\nConsider the following example case:\n- `N = {1, 2, 3}` with order `(1, 2, 3)`.\n- Production function: `f(x) = x^2`.\n- Initial endowments: `a_1 = a_2 = a_3 = 1`.\n- Utility functions (in terms of input contribution `x` and output `y`): \n  - `u_1(x,y) = 2y - x`\n  - `u_2(x,y) = y - x`\n  - `u_3(x,y) = y/2 - x`\n\nThe equilibrium payoff for player `k`, `d_k`, is calculated recursively, starting from `d_3` and working backwards to `d_1`.\n\n### Question\n\nWhat is the equilibrium payoff for player 1, `d_1`?", "Options": {"A": "10", "B": "9", "C": "7", "D": "5"}, "Answer": "B", "pi_justification": "This item assesses the ability to perform the multi-step backward induction calculation from the demand game. It uses a Computational Judgment strategy. The correct answer requires calculating `d_3`, then `d_2`, and finally `d_1`. Distractor A (10) results from a common calculation slip (omitting the input cost from utility). Distractor C (7) is the correct answer for a different scenario presented in the original paper, testing for attention to detail. Distractor D (5) results from a conceptual error in defining the relevant coalition for player 1's calculation (using {1,2} instead of {1,2,3}).", "qid": "282", "question": "### Background\n\nIn a sequential *demand game* for a natural monopoly, players are arranged in a fixed order `w`. The unique subgame perfect equilibrium payoff vector `d_N` is determined via backward induction. The last player's payoff is their autarky value, and each preceding player's payoff is the maximum they can claim while leaving enough surplus for all subsequent players to achieve their predetermined equilibrium payoffs.\n\n### Data / Model Specification\n\nConsider the following example case:\n- `N = {1, 2, 3}` with order `(1, 2, 3)`.\n- Production function: `f(x) = x^2`.\n- Initial endowments: `a_1 = a_2 = a_3 = 1`.\n- Utility functions (in terms of input contribution `x` and output `y`): \n  - `u_1(x,y) = 2y - x`\n  - `u_2(x,y) = y - x`\n  - `u_3(x,y) = y/2 - x`\n\nThe equilibrium payoff for player `k`, `d_k`, is calculated recursively, starting from `d_3` and working backwards to `d_1`.\n\n### Question\n\nWhat is the equilibrium payoff for player 1, `d_1`?", "model_evaluation_result": {"model_raw_response": "I need to solve this sequential demand game using backward induction.\n\nGiven information:\n- Players: N = {1, 2, 3} with order (1, 2, 3)\n- Production function: f(x) = x²\n- Initial endowments: a₁ = a₂ = a₃ = 1\n- Utility functions:\n  - u₁(x,y) = 2y - x\n  - u₂(x,y) = y - x  \n  - u₃(x,y) = y/2 - x\n\nFirst, I need to find each player's autarky value (what they get if they don't participate):\n\nFor autarky, each player uses their own endowment optimally:\n- Player i maximizes uᵢ(x,f(x)) subject to x ≤ aᵢ = 1\n\nPlayer 1 autarky: max(2x² - x) for x ≤ 1\nTaking derivative: 4x - 1 = 0, so x = 1/4\nu₁(1/4, (1/4)²) = 2(1/16) - 1/4 = 1/8 - 1/4 = -1/8\n\nPlayer 2 autarky: max(x² - x) for x ≤ 1  \nTaking derivative: 2x - 1 = 0, so x = 1/2\nu₂(1/2, (1/2)²) = 1/4 - 1/2 = -1/4\n\nPlayer 3 autarky: max(x²/2 - x) for x ≤ 1\nTaking derivative: x - 1 = 0, so x = 1\nu₃(1, 1²) = 1/2 - 1 = -1/2\n\nNow using backward induction:\n\nStep 1: d₃ = -1/2 (autarky value of last player)\n\nStep 2: Find d₂\nPlayer 2 can claim the maximum while leaving player 3 with ", "model_answer": "I need to solve this sequential demand game using backward induction.\n\nGiven information:\n- Players: N = {1, 2, 3} with order (1, 2, 3)\n- Production function: f(x) = x²\n- Initial endowments: a₁ = a₂ = a₃ = 1\n- Utility functions:\n  - u₁(x,y) = 2y - x\n  - u₂(x,y) = y - x  \n  - u₃(x,y) = y/2 - x\n\nFirst, I need to find each player's autarky value (what they get if they don't participate):\n\nFor autarky, each player uses their own endowment optimally:\n- Player i maximizes uᵢ(x,f(x)) subject to x ≤ aᵢ = 1\n\nPlayer 1 autarky: max(2x² - x) for x ≤ 1\nTaking derivative: 4x - 1 = 0, so x = 1/4\nu₁(1/4, (1/4)²) = 2(1/16) - 1/4 = 1/8 - 1/4 = -1/8\n\nPlayer 2 autarky: max(x² - x) for x ≤ 1  \nTaking derivative: 2x - 1 = 0, so x = 1/2\nu₂(1/2, (1/2)²) = 1/4 - 1/2 = -1/4\n\nPlayer 3 autarky: max(x²/2 - x) for x ≤ 1\nTaking derivative: x - 1 = 0, so x = 1\nu₃(1, 1²) = 1/2 - 1 = -1/2\n\nNow using backward induction:\n\nStep 1: d₃ = -1/2 (autarky value of last player)\n\nStep 2: Find d₂\nPlayer 2 can claim the maximum while leaving player 3 with", "error": "Warning: Could not extract single choice letter from: I need to solve this sequential demand game using ..."}}
{"ID": 392, "Question": "### Background\n\nIn a strategic redistricting game, the equilibrium strategy is a **`p`-segregation plan**. This strategy involves maximally segregating unfavorable districts ('packing' the opposition) and pooling favorable voters into uniform districts ('cracking' one's own supporters).\n\n### Data / Model Specification\n\nThe probability that a party wins a district with effective characteristic `θ` (where higher `θ` is more favorable) is given by the District Outcome Function (DOF), `π(θ)`. A key feature of the DOF is that it is S-shaped: it is convex for low values of `θ` (where the party is the underdog) and concave for high values of `θ` (where the party is the favorite).\n\n### Question\n\nWhat property of the District Outcome Function `π(·)` makes the 'pack and crack' `p`-segregation strategy optimal for maximizing the number of seats won?", "Options": {"A": "The function is convex where the party is the underdog and concave where it is the favorite, making it optimal to use extreme voter distributions in the convex region and uniform distributions in the concave region.", "B": "The function is symmetric around `θ=1/2`, which ensures that packing and cracking have equal and opposite effects on seat share.", "C": "The function is strictly increasing, meaning that a higher district characteristic always leads to a higher probability of winning.", "D": "The function is continuous, which allows for a unique critical state `s*` to be found where the election is tied."}, "Answer": "A", "pi_justification": "This item tests the core economic intuition behind the paper's main result: why 'packing and cracking' is the optimal response to an S-shaped returns function. It uses a Reverse-Reasoning strategy (given the strategy, what is the cause?). The correct answer (A) correctly links convexity to packing and concavity to cracking. Distractors are designed to be 'True but Irrelevant': (B), (C), and (D) all state correct properties of the `π` function described in the paper, but none of them provide the causal reason for the optimality of the `p`-segregation strategy.", "qid": "392", "question": "### Background\n\nIn a strategic redistricting game, the equilibrium strategy is a **`p`-segregation plan**. This strategy involves maximally segregating unfavorable districts ('packing' the opposition) and pooling favorable voters into uniform districts ('cracking' one's own supporters).\n\n### Data / Model Specification\n\nThe probability that a party wins a district with effective characteristic `θ` (where higher `θ` is more favorable) is given by the District Outcome Function (DOF), `π(θ)`. A key feature of the DOF is that it is S-shaped: it is convex for low values of `θ` (where the party is the underdog) and concave for high values of `θ` (where the party is the favorite).\n\n### Question\n\nWhat property of the District Outcome Function `π(·)` makes the 'pack and crack' `p`-segregation strategy optimal for maximizing the number of seats won?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 301, "Question": "### Background\n\n**Research Question.** This problem investigates the micro-level drivers of the change in the aggregate imported input share following a large devaluation, focusing on the puzzle of why import-intensive firms expand.\n\n**Setting / Institutional Environment.** The analysis uses firm-level data from Mexico (post-1995 devaluation) to decompose the change in the aggregate imported input share into components attributable to within-firm changes versus between-firm reallocation of market share.\n\n### Data / Model Specification\n\nThe growth in the aggregate imported input share (\\(\\Delta S_I\\)) is decomposed into several components. The two key components for continuing firms are:\n-   **Within:** Captures changes in firms' own import shares, holding their initial market shares constant. It reflects firm-level substitution.\n-   **Between:** Captures the reallocation of market share across firms, holding their initial import shares constant.\n\n**Table 1: Decomposition for Mexico (change from 1994 to 1997, in percentage points)**\n| Year | Within | Between | Covariance | Net entry | All |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| 1997 | -1.54 | 12.34 | 2.61 | 6.95 | 20.36 |\n\nFurther analysis, not shown in the table, reveals that firms that are also exporters account for more than 100% of the positive reallocation effect (the sum of the 'Between' and 'Covariance' terms). What does this finding imply about the behavior of non-exporting importers, and how does it resolve the puzzle of why import-intensive firms expanded?", "Options": {"A": "It implies non-exporting importers must have contracted (a negative reallocation effect), behaving as standard theory predicts. The puzzle is thus confined to and explained by the behavior of 'global firms'.", "B": "It implies non-exporting importers must have had a zero reallocation effect, meaning the puzzle is entirely driven by exporters and remains unexplained.", "C": "It implies that both exporters and non-exporters expanded, but exporters expanded more, deepening the puzzle of why any importer would expand.", "D": "It implies that exporting is more profitable than selling domestically, but does not resolve the puzzle of why import-intensive firms gained market share."}, "Answer": "A", "pi_justification": "This item assesses deductive reasoning and the ability to synthesize a new piece of information to resolve a stated puzzle. It uses a Reverse-Reasoning strategy. Distractors are based on a mathematical error (B), a direct contradiction of the logic (C), and a conclusion that is plausible but irrelevant to the specific question asked (D).", "qid": "301", "question": "### Background\n\n**Research Question.** This problem investigates the micro-level drivers of the change in the aggregate imported input share following a large devaluation, focusing on the puzzle of why import-intensive firms expand.\n\n**Setting / Institutional Environment.** The analysis uses firm-level data from Mexico (post-1995 devaluation) to decompose the change in the aggregate imported input share into components attributable to within-firm changes versus between-firm reallocation of market share.\n\n### Data / Model Specification\n\nThe growth in the aggregate imported input share (\\(\\Delta S_I\\)) is decomposed into several components. The two key components for continuing firms are:\n-   **Within:** Captures changes in firms' own import shares, holding their initial market shares constant. It reflects firm-level substitution.\n-   **Between:** Captures the reallocation of market share across firms, holding their initial import shares constant.\n\n**Table 1: Decomposition for Mexico (change from 1994 to 1997, in percentage points)**\n| Year | Within | Between | Covariance | Net entry | All |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| 1997 | -1.54 | 12.34 | 2.61 | 6.95 | 20.36 |\n\nFurther analysis, not shown in the table, reveals that firms that are also exporters account for more than 100% of the positive reallocation effect (the sum of the 'Between' and 'Covariance' terms). What does this finding imply about the behavior of non-exporting importers, and how does it resolve the puzzle of why import-intensive firms expanded?", "model_evaluation_result": {"model_raw_response": "Looking at this decomposition problem, I need to understand what it means when exporters account for \"more than 100%\" of the positive reallocation effect.\n\nThe key insight is in the mathematical constraint. The total reallocation effect (Between + Covariance = 12.34 + 2.61 = 14.95) represents the net change across all firms. If exporters contribute more than 100% of this positive effect, they must contribute more than 14.95 percentage points.\n\nSince the total net effect is exactly 14.95, and exporters contribute more than this amount, non-exporters must contribute a negative amount to balance the equation:\n\nExporters' contribution + Non-exporters' contribution = 14.95\n\nIf Exporters' contribution > 14.95, then Non-exporters' contribution < 0\n\nA negative reallocation effect means these firms lost market share, which is exactly what standard trade theory predicts should happen to import-intensive firms after a devaluation - they should contract because their imported inputs become more expensive.\n\nThis resolves the puzzle because:\n1. Non-exporters behaved as theory predicts (contracted)\n2. The puzzle is confined to exporters (\"global firms\") who can offset higher import costs with export revenues\n3. The aggregate expansion of import-intensive firms is explained by the specific behavior of firms that both import and export\n\nThe answer is A.", "model_answer": "A", "error": null}}
{"ID": 263, "Question": "### Background\n\n**Research Question.** This problem examines the asymptotic properties of forecast encompassing test statistics when applied to nested linear models, focusing on how parameter estimation uncertainty influences their limiting distributions.\n\n**Setting / Institutional Environment.** We consider two nested linear forecasting models. The total sample is split into an in-sample period of size `R` and an out-of-sample period of size `P`.\n\n---\n\n### Data / Model Specification\n\nThe analysis considers an asymptotic regime where the in-sample period grows much faster than the out-of-sample period (`lim P/R = 0`). In this regime, parameter estimation uncertainty is asymptotically negligible.\n\nTwo test statistics are considered:\n```latex\n\\mathrm{ENC-T}=(P-1)^{1/2}\\frac{\\bar{c}}{\\sqrt{P^{-1}\\sum_{t}(c_{t+1}-\\bar{c})^{2}}} \\quad \\text{(Eq. 1)}\n```\n```latex\n\\mathrm{ENC-NEW}=P\\frac{\\bar{c}}{\\mathrm{MSE}_{2}} \\quad \\text{(Eq. 2)}\n```\nwhere `c̄` is the sample average of `û_{1,t+1}(û_{1,t+1} - û_{2,t+1})` and `MSE_2` is the mean squared error of Model 2.\n\nUnder the null hypothesis and this asymptotic regime, ENC-T converges to a standard normal distribution, while ENC-NEW converges in probability to 0 (a degenerate distribution).\n\n---\n\n### Question\n\nWhat is the key theoretical reason for the different limiting behaviors of the ENC-T and ENC-NEW statistics when `lim P/R = 0`?", "Options": {"A": "The numerator of both statistics converges to zero, but the denominator of ENC-NEW converges to a positive constant while the denominator of ENC-T also converges to zero.", "B": "The ENC-NEW statistic is scaled by `P`, which grows to infinity, while the ENC-T statistic is scaled by a smaller term, `(P-1)^(1/2)`.", "C": "The numerator of ENC-NEW converges to zero, while the numerator of ENC-T converges to a positive constant.", "D": "The denominator of ENC-NEW (`MSE_2`) converges to zero, while the denominator of ENC-T converges to a positive constant."}, "Answer": "A", "pi_justification": "This item assesses the theoretical understanding of why two similar-looking statistics have different asymptotic properties. The correct answer (A) captures the core heuristic derivation: ENC-NEW is a ratio converging to `0 / constant -> 0`, while ENC-T is a ratio converging to `0 / 0`, which can be non-degenerate. Distractor (B) is 'True but Irrelevant'; the scaling factor is part of the statistic but doesn't explain the degeneracy. Distractor (C) is a 'Conceptual Opposite', incorrectly stating that one numerator behaves differently from the other. Distractor (D) is also a 'Conceptual Opposite', reversing the behavior of the denominators.", "qid": "263", "question": "### Background\n\n**Research Question.** This problem examines the asymptotic properties of forecast encompassing test statistics when applied to nested linear models, focusing on how parameter estimation uncertainty influences their limiting distributions.\n\n**Setting / Institutional Environment.** We consider two nested linear forecasting models. The total sample is split into an in-sample period of size `R` and an out-of-sample period of size `P`.\n\n---\n\n### Data / Model Specification\n\nThe analysis considers an asymptotic regime where the in-sample period grows much faster than the out-of-sample period (`lim P/R = 0`). In this regime, parameter estimation uncertainty is asymptotically negligible.\n\nTwo test statistics are considered:\n```latex\n\\mathrm{ENC-T}=(P-1)^{1/2}\\frac{\\bar{c}}{\\sqrt{P^{-1}\\sum_{t}(c_{t+1}-\\bar{c})^{2}}} \\quad \\text{(Eq. 1)}\n```\n```latex\n\\mathrm{ENC-NEW}=P\\frac{\\bar{c}}{\\mathrm{MSE}_{2}} \\quad \\text{(Eq. 2)}\n```\nwhere `c̄` is the sample average of `û_{1,t+1}(û_{1,t+1} - û_{2,t+1})` and `MSE_2` is the mean squared error of Model 2.\n\nUnder the null hypothesis and this asymptotic regime, ENC-T converges to a standard normal distribution, while ENC-NEW converges in probability to 0 (a degenerate distribution).\n\n---\n\n### Question\n\nWhat is the key theoretical reason for the different limiting behaviors of the ENC-T and ENC-NEW statistics when `lim P/R = 0`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the asymptotic behavior of both statistics when lim P/R = 0.\n\nLet me examine each component:\n\n**Numerator behavior:**\n- Both statistics have numerators involving c̄, which is the sample average of û₁,t+1(û₁,t+1 - û₂,t+1)\n- Under the null hypothesis of forecast encompassing, this should converge to 0 as P → ∞\n- So both numerators converge to 0\n\n**Denominator behavior:**\n- ENC-T denominator: √(P⁻¹∑ₜ(cₜ₊₁-c̄)²) - This is the standard error of c̄, which converges to a positive constant (the standard deviation of the c process)\n- ENC-NEW denominator: MSE₂ - This is the mean squared error of Model 2, which should converge to a positive constant (the true forecast error variance)\n\n**Scaling factors:**\n- ENC-T is scaled by (P-1)^(1/2) ≈ P^(1/2)\n- ENC-NEW is scaled by P\n\n**Overall behavior:**\n- ENC-T: (P^(1/2) × 0) / (positive constant) = 0, but the rate creates a non-degenerate normal distribution\n- ENC-NEW: (P × 0) / (positive constant) = 0, and converges in probability to 0\n\nThe key insight is that while both numerators go to zero, the different scaling factors (P^(1/2) vs P) combined with the denominators create different limiting behaviors.\n\nLooking at the options, option A correctly identifies that both numerators converge to zero, both denominators converge to positive constants, but this doesn't explain the different behaviors.\n\nOption B correctly identifies the crucial difference: ENC-NEW is scaled by P (which grows without bound), while ENC-T is scaled by (P-1)^(1/2) (which grows more slowly). This difference in scaling, combined with the numerators both going to zero, explains why ENC-NEW degenerates to 0 while ENC-", "model_answer": "A", "error": null}}
{"ID": 330, "Question": "### Background\n\n**Research Question.** This problem evaluates the first step of the paper's in-sample empirical evidence: establishing the number of long-run relationships (the cointegrating rank) in a system of spot and forward exchange rates.\n\n**Setting / Institutional Environment.** The analysis uses a five-variable system (`n=5`) comprising the log spot exchange rate and four log forward exchange rates (`j=4`). The empirical strategy involves a Vector Error Correction Model (VECM). The number of cointegrating relationships (`r`) is determined using the Johansen test procedure. The number of common stochastic trends is `n-r`.\n\n### Data / Model Specification\n\nThe Johansen test for cointegrating rank produces two statistics, the Trace statistic and the λ-max statistic. The testing procedure is sequential. One starts from the null hypothesis of the lowest rank and proceeds until a null hypothesis is not rejected.\n\n**Table 1. Tests of Cointegrating Rank for Dollar-Mark**\n\n| | λ-max Statistic | 5% Critical Value | Trace Statistic | 5% Critical Value |\n|:---|:---:|:---:|:---:|:---:|\n| H₀: r ≤ 4 | 0.196 | 3.762 | 0.196 | 3.762 |\n| H₀: r ≤ 3 | 16.245 | 14.069 | 16.441 | 15.410 |\n\n### Question\n\nBased on the Trace statistic results in Table 1 for the Dollar-Mark system, what is the concluded cointegrating rank `r`?", "Options": {"A": "r = 3, because the null hypothesis H₀: r ≤ 3 is rejected.", "B": "r = 4, because H₀: r ≤ 3 is rejected, but H₀: r ≤ 4 is not rejected.", "C": "r = 5, because all tested null hypotheses up to r ≤ 4 are rejected.", "D": "r ≤ 3, because the test for H₀: r ≤ 4 is not significant."}, "Answer": "B", "pi_justification": "This item assesses the procedural knowledge of interpreting a sequential Johansen cointegration test, a cornerstone of the paper's empirical methodology. It uses a Computational Judgment strategy. The correct answer requires applying the sequential testing logic correctly. Distractor A represents the common error of stopping at the first rejected hypothesis without considering the next step. Distractor C incorrectly interprets the test results. Distractor D confuses the final non-rejection with the conclusion for the rank.", "qid": "330", "question": "### Background\n\n**Research Question.** This problem evaluates the first step of the paper's in-sample empirical evidence: establishing the number of long-run relationships (the cointegrating rank) in a system of spot and forward exchange rates.\n\n**Setting / Institutional Environment.** The analysis uses a five-variable system (`n=5`) comprising the log spot exchange rate and four log forward exchange rates (`j=4`). The empirical strategy involves a Vector Error Correction Model (VECM). The number of cointegrating relationships (`r`) is determined using the Johansen test procedure. The number of common stochastic trends is `n-r`.\n\n### Data / Model Specification\n\nThe Johansen test for cointegrating rank produces two statistics, the Trace statistic and the λ-max statistic. The testing procedure is sequential. One starts from the null hypothesis of the lowest rank and proceeds until a null hypothesis is not rejected.\n\n**Table 1. Tests of Cointegrating Rank for Dollar-Mark**\n\n| | λ-max Statistic | 5% Critical Value | Trace Statistic | 5% Critical Value |\n|:---|:---:|:---:|:---:|:---:|\n| H₀: r ≤ 4 | 0.196 | 3.762 | 0.196 | 3.762 |\n| H₀: r ≤ 3 | 16.245 | 14.069 | 16.441 | 15.410 |\n\n### Question\n\nBased on the Trace statistic results in Table 1 for the Dollar-Mark system, what is the concluded cointegrating rank `r`?", "model_evaluation_result": {"model_raw_response": "Looking at the Trace statistic results in Table 1:\n\nFor H₀: r ≤ 4:\n- Trace statistic = 0.196\n- 5% Critical Value = 3.762\n- Since 0.196 < 3.762, we fail to reject H₀: r ≤ 4\n\nFor H₀: r ≤ 3:\n- Trace statistic = 16.441\n- 5% Critical Value = 15.410\n- Since 16.441 > 15.410, we reject H₀: r ≤ 3\n\nIn the Johansen sequential testing procedure, we start with the lowest rank and continue until we fail to reject a null hypothesis. Here:\n\n1. We reject H₀: r ≤ 3 (the trace statistic exceeds the critical value)\n2. We fail to reject H₀: r ≤ 4 (the trace statistic is below the critical value)\n\nWhen we reject H₀: r ≤ 3 but fail to reject H₀: r ≤ 4, this means the cointegrating rank is r = 4.\n\nB", "model_answer": "B", "error": null}}
{"ID": 315, "Question": "### Background\n\n**Research Question.** This problem interrogates the full causal inference strategy used to estimate the effect of broadband on firms and workers. The paper uses both an Intention-to-Treat (ITT) design and an Instrumental Variable (IV) design.\n\n**Setting and Sample.** The study exploits a public program in Norway that progressively rolled out broadband infrastructure across different municipalities (`m`) at different times (`t`). This staggered rollout provides quasi-experimental variation in broadband availability (`z_mt`), which is the share of households in a municipality with access to broadband.\n\n### Data / Model Specification\n\nThe **Intention-to-Treat (ITT)** model estimates the effect of broadband *availability* on an outcome `y_imt`:\n\n```latex\ny_{imt} = x_{imt}'\\delta_0 + z_{mt}x_{imt}'\\delta_1 + ... + \\eta_m + \\tau_t + u_{imt} \\quad \\text{(Eq. 1)}\n```\n\nThe **Instrumental Variable (IV)** model estimates the effect of broadband *adoption* (`D_imt`) on `y_imt`, using `z_mt` as an instrument. The second stage is:\n\n```latex\ny_{imt} = x_{imt}'\\beta_0 + D_{imt}x_{imt}'\\beta_1 + ... + \\lambda_m + \\theta_t + \\epsilon_{imt} \\quad \\text{(Eq. 2)}\n```\n\n**Empirical Evidence for IV Assumptions:**\n- **First Stage:** A regression of firm adoption `D_imt` on availability `z_mt` yields a coefficient of `0.23` with a standard error of `0.04`.\n- **Placebo Test:** The paper identifies \"always takers\" (firms with broadband when `z=0`) and \"never takers\" (firms without broadband when `z=1`). For this subsample of firms, whose adoption status is by definition unaffected by `z_mt`, the paper re-estimates the ITT model (Eq. 1). The results are in Table 1.\n\n**Table 1. Placebo Test: ITT Effects on Output Elasticities (from paper's Table VI)**\n\n| Variable | Baseline Sample | Always/Never Taker Firms Only |\n| :--- | :--- | :--- |\n| Availability x Log skilled | 0.0755*** | 0.00944 |\n\n*Notes: Significance: ***p<0.01.*\n\n### Question\n\nWhat do the results of the Placebo Test in **Table 1** imply about the validity of using broadband availability (`z_mt`) as an instrument for broadband adoption (`D_imt`)?", "Options": {"A": "The results support the exclusion restriction, as the instrument's effect on the outcome disappears in the subsample where the adoption channel is shut down.", "B": "The results violate the exclusion restriction, because the effect of availability on skilled labor productivity is significantly different between the baseline and the placebo sample.", "C": "The results show that the instrument is weak (has low relevance), because the coefficient on availability is statistically insignificant in the always/never taker sample.", "D": "The results suggest that the treatment effect of broadband is heterogeneous, which means that a linear IV model is misspecified and therefore invalid."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to interpret a placebo test designed to validate an instrumental variable's exclusion restriction. It uses a Reverse-Reasoning strategy, asking students to infer the methodological conclusion from a specific empirical result. Distractors are designed to target common misconceptions in causal inference: confusing the exclusion restriction with the relevance condition (C), misinterpreting the test's logic (B), and identifying a true but secondary implication instead of the primary methodological point (D).", "qid": "315", "question": "### Background\n\n**Research Question.** This problem interrogates the full causal inference strategy used to estimate the effect of broadband on firms and workers. The paper uses both an Intention-to-Treat (ITT) design and an Instrumental Variable (IV) design.\n\n**Setting and Sample.** The study exploits a public program in Norway that progressively rolled out broadband infrastructure across different municipalities (`m`) at different times (`t`). This staggered rollout provides quasi-experimental variation in broadband availability (`z_mt`), which is the share of households in a municipality with access to broadband.\n\n### Data / Model Specification\n\nThe **Intention-to-Treat (ITT)** model estimates the effect of broadband *availability* on an outcome `y_imt`:\n\n```latex\ny_{imt} = x_{imt}'\\delta_0 + z_{mt}x_{imt}'\\delta_1 + ... + \\eta_m + \\tau_t + u_{imt} \\quad \\text{(Eq. 1)}\n```\n\nThe **Instrumental Variable (IV)** model estimates the effect of broadband *adoption* (`D_imt`) on `y_imt`, using `z_mt` as an instrument. The second stage is:\n\n```latex\ny_{imt} = x_{imt}'\\beta_0 + D_{imt}x_{imt}'\\beta_1 + ... + \\lambda_m + \\theta_t + \\epsilon_{imt} \\quad \\text{(Eq. 2)}\n```\n\n**Empirical Evidence for IV Assumptions:**\n- **First Stage:** A regression of firm adoption `D_imt` on availability `z_mt` yields a coefficient of `0.23` with a standard error of `0.04`.\n- **Placebo Test:** The paper identifies \"always takers\" (firms with broadband when `z=0`) and \"never takers\" (firms without broadband when `z=1`). For this subsample of firms, whose adoption status is by definition unaffected by `z_mt`, the paper re-estimates the ITT model (Eq. 1). The results are in Table 1.\n\n**Table 1. Placebo Test: ITT Effects on Output Elasticities (from paper's Table VI)**\n\n| Variable | Baseline Sample | Always/Never Taker Firms Only |\n| :--- | :--- | :--- |\n| Availability x Log skilled | 0.0755*** | 0.00944 |\n\n*Notes: Significance: ***p<0.01.*\n\n### Question\n\nWhat do the results of the Placebo Test in **Table 1** imply about the validity of using broadband availability (`z_mt`) as an instrument for broadband adoption (`D_imt`)?", "model_evaluation_result": {"model_raw_response": "Looking at this placebo test, I need to understand what it's testing and what the results show.\n\nThe placebo test uses a subsample of \"always takers\" (firms with broadband when z=0) and \"never takers\" (firms without broadband when z=1). By definition, these firms' adoption status is unaffected by the instrument z_mt - their adoption doesn't vary with availability.\n\nThis test is designed to check the exclusion restriction, which requires that the instrument (broadband availability) affects the outcome only through its effect on the treatment (broadband adoption), not through any other channels.\n\nLooking at the results:\n- In the baseline sample: coefficient = 0.0755*** (highly significant)\n- In the always/never taker sample: coefficient = 0.00944 (not significant)\n\nThe key insight is that in the always/never taker subsample, the adoption channel is effectively \"shut down\" - availability doesn't affect adoption for these firms. If the exclusion restriction holds, we should see no effect of availability on the outcome in this subsample, since availability can only affect outcomes through adoption.\n\nThe results show exactly this: the significant effect (0.0755***) disappears and becomes insignificant (0.00944) when the adoption channel is removed. This supports the exclusion restriction because it suggests availability only affects outcomes through the adoption channel.\n\nThe answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 408, "Question": "### Background\n\nThe model involves `n` agents who sign a contract `(d, t)` at period 1. In period 2, they choose a private investment `y_i`. In period 3, their private type `a_i` is realized. In period 4, the contract is executed based on their announced types.\n\n### Data / Model Specification\n\nAgent `i`'s utility is quasi-linear:\n```latex\nU_i = v_{i}(x,a_{i}) + I \\quad \\text{(Eq. 1)}\n```\nwhere `x` is the collective choice and `I` is a monetary transfer.\n\n### Question\n\nBased on the utility function in Eq. (1), what does the model assume about agents' attitude towards risk, and why is this assumption critical for the paper's proposed mechanisms?", "Options": {"A": "Agents are risk-neutral, which is critical because the mechanisms use monetary transfers to exactly offset social costs/benefits, requiring a linear valuation of money.", "B": "Agents are risk-averse, which is critical because they must be willing to accept the uncertainty of the mechanism's outcome.", "C": "Agents are risk-loving, which is necessary to incentivize them to make speculative ex-ante investments.", "D": "Agents' risk attitude is irrelevant as long as utility is separable in money, which is only needed for the budget to balance."}, "Answer": "A", "pi_justification": "This item assesses understanding of the foundational assumption of risk neutrality (quasi-linearity) and its specific function in enabling incentive alignment via transfers. The distractors test common confusions: B and C propose incorrect risk attitudes, while D misidentifies the reason for the assumption's importance, a 'True but Irrelevant' style error.", "qid": "408", "question": "### Background\n\nThe model involves `n` agents who sign a contract `(d, t)` at period 1. In period 2, they choose a private investment `y_i`. In period 3, their private type `a_i` is realized. In period 4, the contract is executed based on their announced types.\n\n### Data / Model Specification\n\nAgent `i`'s utility is quasi-linear:\n```latex\nU_i = v_{i}(x,a_{i}) + I \\quad \\text{(Eq. 1)}\n```\nwhere `x` is the collective choice and `I` is a monetary transfer.\n\n### Question\n\nBased on the utility function in Eq. (1), what does the model assume about agents' attitude towards risk, and why is this assumption critical for the paper's proposed mechanisms?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 308, "Question": "### Background\n\n**Research Question.** This problem compares the properties of two specific \"flexible functional forms\" derived from the linear parallel structure: the Parallel Linear Asymmetric Quadratic (PLAQ) and the Parallel Linear Generalized Linear (PLGL). It focuses on how their different mathematical structures lead to starkly different economic implications for substitution and satiation.\n\n**Setting / Institutional Environment.** We analyze a system of `N` goods where good 1 is the numeraire. All other prices (`p_i*`) are normalized by the price of the numeraire. The analysis focuses on the compensated (Hicksian) demand for the non-numeraire goods (`i, j > 1`).\n\n**Variables & Parameters.**\n- `φ^i(u,p*)`: Hicksian demand for non-numeraire good `i`.\n- `p_i*`: Normalized price of good `i`.\n\n---\n\n### Data / Model Specification\n\nThe Hicksian demand functions for non-numeraire goods (`i > 1`) for the two models are given by:\n\n**PLAQ Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + α_i + \\sum_{j=2}^{N} β_{ij} p_j^* \n\n\\quad\\text{(Eq. 1)}\n```\n\n**PLGL Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + \\frac{1}{2}α_i(p_i*)^{-1/2} + \\frac{1}{2}\\sum_{j=2}^{N} β_{ij} \\left(\\frac{p_j^*}{p_i^*}\\right)^{1/2}\n\n\\quad\\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nThe paper notes that one of these models implies \"satiation effects\" for non-numeraire goods, while the other implies infinite demand as the price approaches zero. Which model implies satiation, and what feature of its demand function causes this?", "Options": {"A": "The PLAQ model implies satiation because its demand function is linear in price, meaning demand does not approach infinity as price approaches zero.", "B": "The PLGL model implies satiation because the term `(p_j*/p_i*)^{1/2}` approaches zero for low prices.", "C": "The PLAQ model implies satiation because the parameter `α_i` can be negative.", "D": "The PLGL model implies satiation because it is a generalization of the Leontief form, which has zero substitution."}, "Answer": "A", "pi_justification": "This item assesses the interpretation of the models' global properties, the focus of the original Q3. It requires linking the model name (PLAQ) to the property (satiation) and the correct mathematical reason (linearity). Distractors provide incorrect pairings or flawed reasoning.", "qid": "308", "question": "### Background\n\n**Research Question.** This problem compares the properties of two specific \"flexible functional forms\" derived from the linear parallel structure: the Parallel Linear Asymmetric Quadratic (PLAQ) and the Parallel Linear Generalized Linear (PLGL). It focuses on how their different mathematical structures lead to starkly different economic implications for substitution and satiation.\n\n**Setting / Institutional Environment.** We analyze a system of `N` goods where good 1 is the numeraire. All other prices (`p_i*`) are normalized by the price of the numeraire. The analysis focuses on the compensated (Hicksian) demand for the non-numeraire goods (`i, j > 1`).\n\n**Variables & Parameters.**\n- `φ^i(u,p*)`: Hicksian demand for non-numeraire good `i`.\n- `p_i*`: Normalized price of good `i`.\n\n---\n\n### Data / Model Specification\n\nThe Hicksian demand functions for non-numeraire goods (`i > 1`) for the two models are given by:\n\n**PLAQ Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + α_i + \\sum_{j=2}^{N} β_{ij} p_j^* \n\n\\quad\\text{(Eq. 1)}\n```\n\n**PLGL Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + \\frac{1}{2}α_i(p_i*)^{-1/2} + \\frac{1}{2}\\sum_{j=2}^{N} β_{ij} \\left(\\frac{p_j^*}{p_i^*}\\right)^{1/2}\n\n\\quad\\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nThe paper notes that one of these models implies \"satiation effects\" for non-numeraire goods, while the other implies infinite demand as the price approaches zero. Which model implies satiation, and what feature of its demand function causes this?", "model_evaluation_result": {"model_raw_response": "Looking at the two demand functions, I need to analyze their behavior as prices approach zero to determine which model implies satiation versus infinite demand.\n\nFor the PLAQ model:\nφ^i(u,p*) = h(u)δ_i + α_i + Σβ_{ij}p_j*\n\nThis is linear in prices. As any price p_j* approaches zero, the demand approaches h(u)δ_i + α_i + Σβ_{ij}p_j* where the price terms become very small. The demand remains finite and bounded.\n\nFor the PLGL model:\nφ^i(u,p*) = h(u)δ_i + (1/2)α_i(p_i*)^{-1/2} + (1/2)Σβ_{ij}(p_j*/p_i*)^{1/2}\n\nThe key term here is α_i(p_i*)^{-1/2}. As p_i* approaches zero, (p_i*)^{-1/2} approaches infinity, meaning demand approaches infinity unless α_i = 0.\n\nTherefore:\n- PLAQ model implies satiation (finite demand as prices approach zero)\n- PLGL model implies infinite demand as own price approaches zero\n\nThe feature that causes satiation in the PLAQ model is that it is linear in prices, so demand remains bounded even as prices approach zero.\n\nA", "model_answer": "A", "error": null}}
{"ID": 405, "Question": "### Background\n\nA central concern is that the instrumental variables (IV) estimate of the transitory income elasticity (`β_T`) might be upwardly biased. This could happen if the instruments are correlated with permanent income shocks that have been misclassified as transitory shocks (a form of specification error).\n\nTo address this, the authors conduct a robustness check: they re-estimate the model on a subsample of households with longer panel histories (at least 6 surveys vs. the baseline of at least 3). The statistical logic is that a longer panel allows for a more precise estimation of each household's permanent income trend, thereby reducing the misclassification (specification error) in the remaining transitory component.\n\nUpon performing this check, the authors find that the IV estimate of `β_T` actually *increases slightly* in the long-panel sample.\n\n### Question\n\nWhat is the most accurate conclusion from this finding?", "Options": {"A": "The result contradicts the hypothesis that the IV estimate is primarily inflated by specification error, thus strengthening the paper's main conclusion.", "B": "The result suggests that the IV strategy is flawed, because if it were working correctly, the estimate of `β_T` should have decreased after the reduction in specification error.", "C": "The increase in the `β_T` estimate indicates that longer panel histories must be associated with more severe classical measurement error, which the IV is also correcting.", "D": "The result confirms that specification error was a major upward bias, and its removal revealed an even larger underlying true elasticity."}, "Answer": "A", "pi_justification": "This item assesses a deep understanding of a specific robustness check. It uses a Scenario Application strategy, where the 'scenario' is the logic and outcome of the test. The correct answer (A) requires following a two-step logical chain: (1) If the concern were valid, the estimate should have decreased. (2) It did not decrease, therefore the concern is likely invalid. Distractor B correctly states the premise of the test but draws the wrong final conclusion. Distractor C confuses the roles of specification and measurement error. Distractor D presents a logically incoherent argument (removing an upward bias cannot lead to a higher estimate).", "qid": "405", "question": "### Background\n\nA central concern is that the instrumental variables (IV) estimate of the transitory income elasticity (`β_T`) might be upwardly biased. This could happen if the instruments are correlated with permanent income shocks that have been misclassified as transitory shocks (a form of specification error).\n\nTo address this, the authors conduct a robustness check: they re-estimate the model on a subsample of households with longer panel histories (at least 6 surveys vs. the baseline of at least 3). The statistical logic is that a longer panel allows for a more precise estimation of each household's permanent income trend, thereby reducing the misclassification (specification error) in the remaining transitory component.\n\nUpon performing this check, the authors find that the IV estimate of `β_T` actually *increases slightly* in the long-panel sample.\n\n### Question\n\nWhat is the most accurate conclusion from this finding?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 409, "Question": "### Background\n\nThe model involves `n` agents who sign a contract `(d, t)` at period 1. In period 2, they choose a private investment `y_i`. In period 3, their private type `a_i` is realized. In period 4, the contract is executed based on their announced types.\n\n### Data / Model Specification\n\nA first-best outcome `(φ*, y*)` must be both:\n1.  **Exchange-Efficient:** The ex-post choice `φ*(a)` maximizes total surplus for realized types `a`: `φ*(a) = argmax_x ∑_i v_i(x, a_i)`.\n2.  **Investment-Efficient:** The ex-ante investment `y*` maximizes expected total surplus: `y* = argmax_y E[∑_i v_i(φ*(a), a_i) | y]`.\n\n### Question\n\nA proposed contract guarantees that for any given set of investments and realized types, the collective choice will always maximize the total surplus (`φ*(a)`). However, empirical analysis shows that agents consistently under-invest relative to the social optimum (`y*`). What is the most likely explanation for this outcome?", "Options": {"A": "The contract is exchange-efficient but not investment-efficient, because agents anticipate not receiving their full marginal return on investment and thus have weak ex-ante incentives.", "B": "The contract is investment-efficient but not exchange-efficient, meaning agents invest correctly but the ex-post decision-making is flawed.", "C": "The agents are likely risk-averse, which makes them unwilling to invest optimally even if the contract is fully efficient.", "D": "The contract fails to satisfy the ex-post individual rationality constraint, causing agents to fear making losses."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to diagnose a classic market failure. It tests the crucial distinction between ex-post (exchange) and ex-ante (investment) efficiency. The correct answer (A) identifies the core hold-up problem. Distractors represent plausible but incorrect diagnoses: B reverses the logic, C blames risk aversion instead of incentive structure, and D confuses the hold-up problem with participation constraints.", "qid": "409", "question": "### Background\n\nThe model involves `n` agents who sign a contract `(d, t)` at period 1. In period 2, they choose a private investment `y_i`. In period 3, their private type `a_i` is realized. In period 4, the contract is executed based on their announced types.\n\n### Data / Model Specification\n\nA first-best outcome `(φ*, y*)` must be both:\n1.  **Exchange-Efficient:** The ex-post choice `φ*(a)` maximizes total surplus for realized types `a`: `φ*(a) = argmax_x ∑_i v_i(x, a_i)`.\n2.  **Investment-Efficient:** The ex-ante investment `y*` maximizes expected total surplus: `y* = argmax_y E[∑_i v_i(φ*(a), a_i) | y]`.\n\n### Question\n\nA proposed contract guarantees that for any given set of investments and realized types, the collective choice will always maximize the total surplus (`φ*(a)`). However, empirical analysis shows that agents consistently under-invest relative to the social optimum (`y*`). What is the most likely explanation for this outcome?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 407, "Question": "### Background\n\n**Research Question.** The paper's model identifies a large theoretical 'wedge' (distortion) in China's high-skill-intensive services (HSS) sector. This question asks for the real-world institutional feature that the paper links to this wedge.\n\n**Variables & Parameters.**\n*   `Wedge`: A model-calibrated measure of distortion in a sector, relative to the goods sector (wedge=1 means no relative distortion).\n*   `Nom. Prod. Gap`: Nominal labor productivity of a sector relative to the goods sector, which serves as an empirical proxy for the wedge.\n*   `SOE Emp. (% in ind.)`: Share of an industry's workforce employed by State-Owned Enterprises.\n\n---\n\n### Data / Model Specification\n\nThe table below provides data on distortions and the presence of State-Owned Enterprises (SOEs) for China in 2009, with the U.S. in 1950 included as a benchmark for a country at a similar stage of development.\n\n**Table 1: Wedges and SOE Presence in China (2009) and U.S. (1950)**\n\n| | SOE Emp. (% in ind.) | Nom. Prod. Gap (rel. to goods) | Wedge |\n| :--- | :--- | :--- | :--- |\n| **China in 2009** | | | |\n| Low-skill-intensive Services | 0.09 | 1.02 | 1.00 |\n| High-skill-intensive Services | **0.71** | **3.03** | **2.60** |\n| **U.S. in 1950** | | | |\n| High-skill-intensive Services | — | 1.23 | 1.17 |\n\n---\n\n### Question\n\nWhat does the evidence in **Table 1** most strongly suggest about the nature of the large HSS wedge in China?", "Options": {"A": "The wedge is strongly associated with the high prevalence of State-Owned Enterprises (SOEs) and is abnormally large compared to a historical U.S. benchmark.", "B": "The wedge is a normal feature of economic development, as the U.S. had an HSS wedge of a similar magnitude at a comparable stage of development.", "C": "The wedge is primarily located in the Low-skill-intensive Services sector, which has a higher nominal productivity gap than the HSS sector.", "D": "The wedge is likely due to a scarcity of high-skilled labor, which drives up nominal productivity and is a common feature in developing economies."}, "Answer": "A", "pi_justification": "This item assesses the ability to synthesize descriptive data to link a theoretical concept (the wedge) to an institutional feature (SOEs). It uses a Reverse-Reasoning strategy. The correct option (A) correctly identifies the correlation between the SOE share and the wedge size within China and notes the abnormal magnitude relative to the U.S. Distractor (B) is a 'Conceptual Opposite' directly contradicted by the data (2.60 vs 1.17). Distractor (C) misreads the table, confusing LSS and HSS. Distractor (D) introduces a 'True but Irrelevant' concept (skill scarcity) that is not supported by this specific table and was ruled out elsewhere in the paper.", "qid": "407", "question": "### Background\n\n**Research Question.** The paper's model identifies a large theoretical 'wedge' (distortion) in China's high-skill-intensive services (HSS) sector. This question asks for the real-world institutional feature that the paper links to this wedge.\n\n**Variables & Parameters.**\n*   `Wedge`: A model-calibrated measure of distortion in a sector, relative to the goods sector (wedge=1 means no relative distortion).\n*   `Nom. Prod. Gap`: Nominal labor productivity of a sector relative to the goods sector, which serves as an empirical proxy for the wedge.\n*   `SOE Emp. (% in ind.)`: Share of an industry's workforce employed by State-Owned Enterprises.\n\n---\n\n### Data / Model Specification\n\nThe table below provides data on distortions and the presence of State-Owned Enterprises (SOEs) for China in 2009, with the U.S. in 1950 included as a benchmark for a country at a similar stage of development.\n\n**Table 1: Wedges and SOE Presence in China (2009) and U.S. (1950)**\n\n| | SOE Emp. (% in ind.) | Nom. Prod. Gap (rel. to goods) | Wedge |\n| :--- | :--- | :--- | :--- |\n| **China in 2009** | | | |\n| Low-skill-intensive Services | 0.09 | 1.02 | 1.00 |\n| High-skill-intensive Services | **0.71** | **3.03** | **2.60** |\n| **U.S. in 1950** | | | |\n| High-skill-intensive Services | — | 1.23 | 1.17 |\n\n---\n\n### Question\n\nWhat does the evidence in **Table 1** most strongly suggest about the nature of the large HSS wedge in China?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 304, "Question": "### Background\n\n**Research Question.** This problem explores the fundamental theoretical properties of parallel preference structures, starting from the general cost function and specializing to the tractable linear case, highlighting the key implications for consumer behavior and empirical analysis.\n\n**Setting / Institutional Environment.** We consider a standard consumer choice problem with N goods. The preference structure is described by its cost (expenditure) function. The analysis is restricted to interior solutions.\n\n**Variables & Parameters.**\n- `C(u,p)`: The cost function.\n- `u`: A scalar utility index.\n- `p`: An `N x 1` vector of prices.\n\n---\n\n### Data / Model Specification\n\nThe general cost function for a parallel preference structure is given by:\n```latex\nC(u,p) = \\sum_{i=1}^{N} p_i f_i(u) + A(p) \n\n\\quad\\text{(Eq. 1)}\n```\n\n---\n\n### Question\n\nGiven the general cost function for a parallel preference structure in Eq. (1), what is the fundamental property of the Slutsky substitution matrix, `S(p) = ∇_{pp} C(u,p)`?", "Options": {"A": "It is independent of the level of utility, `u`.", "B": "It is independent of prices, `p`.", "C": "It is a diagonal matrix, implying no cross-price effects.", "D": "It is equal to the identity matrix multiplied by `u`."}, "Answer": "A", "pi_justification": "This item assesses the single most important theoretical property of parallel structures, which is the core of the original QA's first two parts. It uses a Computational Judgment strategy (mentally performing the differentiation). Distractors represent classic alternative properties of other preference structures (B, C) or nonsensical combinations (D).", "qid": "304", "question": "### Background\n\n**Research Question.** This problem explores the fundamental theoretical properties of parallel preference structures, starting from the general cost function and specializing to the tractable linear case, highlighting the key implications for consumer behavior and empirical analysis.\n\n**Setting / Institutional Environment.** We consider a standard consumer choice problem with N goods. The preference structure is described by its cost (expenditure) function. The analysis is restricted to interior solutions.\n\n**Variables & Parameters.**\n- `C(u,p)`: The cost function.\n- `u`: A scalar utility index.\n- `p`: An `N x 1` vector of prices.\n\n---\n\n### Data / Model Specification\n\nThe general cost function for a parallel preference structure is given by:\n```latex\nC(u,p) = \\sum_{i=1}^{N} p_i f_i(u) + A(p) \n\n\\quad\\text{(Eq. 1)}\n```\n\n---\n\n### Question\n\nGiven the general cost function for a parallel preference structure in Eq. (1), what is the fundamental property of the Slutsky substitution matrix, `S(p) = ∇_{pp} C(u,p)`?", "model_evaluation_result": {"model_raw_response": "Looking at the given cost function for a parallel preference structure:\n\nC(u,p) = Σᵢ pᵢ fᵢ(u) + A(p)\n\nI need to find the Slutsky substitution matrix S(p) = ∇ₚₚ C(u,p), which is the Hessian of the cost function with respect to prices.\n\nLet me compute the second derivatives:\n\nFirst, taking the first derivative with respect to price pⱼ:\n∂C/∂pⱼ = fⱼ(u) + ∂A(p)/∂pⱼ\n\nThen, taking the second derivative with respect to price pₖ:\n∂²C/∂pⱼ∂pₖ = ∂²A(p)/∂pⱼ∂pₖ\n\nNotice that the terms involving fᵢ(u) completely disappear when taking the second derivative, since fⱼ(u) doesn't depend on any price pₖ.\n\nTherefore, the Slutsky substitution matrix is:\nS(p) = ∇ₚₚ A(p)\n\nThis shows that S(p) depends only on A(p) and is completely independent of the utility level u. The fᵢ(u) terms, which contain all the utility dependence, vanish in the second derivative.\n\nThis is the fundamental property of parallel preference structures - the substitution effects between goods are independent of the consumer's utility level.\n\nA", "model_answer": "C", "error": null}}
{"ID": 313, "Question": "### Background\n\n**Research Question:** How can the topological structure of the Nash equilibrium correspondence be exploited to create a computationally feasible algorithm for finding equilibria in finite games?\n\n**Setting:** The core challenge in computing equilibria is the dimensional mismatch between the vast space of games (`Γ`) and the smaller space of mixed strategies (`Σ`). The Kohlberg-Mertens structure theorem provides a homeomorphism `H` that connects the equilibrium graph `E` to the game space `Γ`, enabling a more efficient computational approach by working in a space of dimension `m`, the total number of pure strategies.\n\n**Variables and Parameters.**\n- `(G̃, g)`: A parameterized decomposition of a game `G`.\n- `σ`: A mixed-strategy profile.\n- `v_s(σ)`: The expected payoff for pure strategy `s` given profile `σ`.\n- `z`: An `m`-dimensional vector in the image space of the homeomorphism `H`.\n- `H`: The homeomorphism from the equilibrium graph `E` to the space of games `Γ`.\n- `r(z)`: A retraction map from `ℝ^m` to the strategy space `Σ`.\n- `h(z)`: A map from `ℝ^m` to the `g`-parameter space.\n\n### Data / Model Specification\n\nThe analysis relies on the following key relationships:\n\n1.  A game `G` is decomposed into `(G̃, g)`. The expected payoff for player `n`'s pure strategy `s ∈ S_n` is:\n    ```latex\n    v_s(\\sigma) = g_s + \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n    Let `G̃(σ)_s` denote the summation term.\n\n2.  The Kohlberg-Mertens homeomorphism `H` maps a point on the equilibrium graph `(G̃, g, σ)` to the space of games. For a fixed `G̃`, it is defined as `H(G̃, g, σ) = (G̃, z)`, where:\n    ```latex\n    z_s = \\sigma_s + v_s(\\sigma) \n    ```\n\n3.  The retraction map `r: ℝ^m → Σ` maps a vector `z` to a mixed-strategy profile `σ = r(z)`. The components `σ_s` are defined by `σ_s = (z_s - v_n(z))^+`, where `v_n(z)` is the unique real number for player `n` that solves:\n    ```latex\n    \\sum_{s \\in S_n} (z_s - v_n(z))^+ = 1\n    ```\n    where `(x)⁺ ≡ max{0, x}`.\n\n4.  The inverse map `H⁻¹(G̃, z)` yields the point `(G̃, g, σ)` on the equilibrium graph where `σ = r(z)` and `g_s` is given by:\n    ```latex\n    g_s = z_s - \\sigma_s - \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n\n### Question\n\nThe Global Newton Method relies on the differentiability of the map `ψ(z)`, which in turn depends on `h(z)`. The map `h(z)` is described as piecewise-smooth, meaning it is not differentiable everywhere. At which points `z` does `h(z)` fail to be differentiable, and what is the game-theoretic interpretation of these points?", "Options": {"A": "At points where `r(z)` corresponds to a pure-strategy equilibrium. These are vertices of the strategy simplex `Σ`.", "B": "At points where the target game `g*` is degenerate. These points correspond to games with a continuum of equilibria.", "C": "At points where `G̃(r(z)) = 0`. These are points where the game has no payoff interdependencies.", "D": "At points where `z_s - v_n(z) = 0` for some strategy `s`. These are boundaries where a pure strategy `s` is about to enter or leave the support of the equilibrium strategy `r(z)`."}, "Answer": "D", "pi_justification": "This item assesses the deep connection between the analytical properties of the `h(z)` map and the underlying game theory. It uses a Reverse-Reasoning strategy. The distractors target common conceptual errors: confusing non-differentiability with a specific equilibrium type (A), misattributing the cause to the target game's properties (B, True but Irrelevant), or focusing on the wrong component of the `h(z)` function (C).", "qid": "313", "question": "### Background\n\n**Research Question:** How can the topological structure of the Nash equilibrium correspondence be exploited to create a computationally feasible algorithm for finding equilibria in finite games?\n\n**Setting:** The core challenge in computing equilibria is the dimensional mismatch between the vast space of games (`Γ`) and the smaller space of mixed strategies (`Σ`). The Kohlberg-Mertens structure theorem provides a homeomorphism `H` that connects the equilibrium graph `E` to the game space `Γ`, enabling a more efficient computational approach by working in a space of dimension `m`, the total number of pure strategies.\n\n**Variables and Parameters.**\n- `(G̃, g)`: A parameterized decomposition of a game `G`.\n- `σ`: A mixed-strategy profile.\n- `v_s(σ)`: The expected payoff for pure strategy `s` given profile `σ`.\n- `z`: An `m`-dimensional vector in the image space of the homeomorphism `H`.\n- `H`: The homeomorphism from the equilibrium graph `E` to the space of games `Γ`.\n- `r(z)`: A retraction map from `ℝ^m` to the strategy space `Σ`.\n- `h(z)`: A map from `ℝ^m` to the `g`-parameter space.\n\n### Data / Model Specification\n\nThe analysis relies on the following key relationships:\n\n1.  A game `G` is decomposed into `(G̃, g)`. The expected payoff for player `n`'s pure strategy `s ∈ S_n` is:\n    ```latex\n    v_s(\\sigma) = g_s + \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n    Let `G̃(σ)_s` denote the summation term.\n\n2.  The Kohlberg-Mertens homeomorphism `H` maps a point on the equilibrium graph `(G̃, g, σ)` to the space of games. For a fixed `G̃`, it is defined as `H(G̃, g, σ) = (G̃, z)`, where:\n    ```latex\n    z_s = \\sigma_s + v_s(\\sigma) \n    ```\n\n3.  The retraction map `r: ℝ^m → Σ` maps a vector `z` to a mixed-strategy profile `σ = r(z)`. The components `σ_s` are defined by `σ_s = (z_s - v_n(z))^+`, where `v_n(z)` is the unique real number for player `n` that solves:\n    ```latex\n    \\sum_{s \\in S_n} (z_s - v_n(z))^+ = 1\n    ```\n    where `(x)⁺ ≡ max{0, x}`.\n\n4.  The inverse map `H⁻¹(G̃, z)` yields the point `(G̃, g, σ)` on the equilibrium graph where `σ = r(z)` and `g_s` is given by:\n    ```latex\n    g_s = z_s - \\sigma_s - \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n\n### Question\n\nThe Global Newton Method relies on the differentiability of the map `ψ(z)`, which in turn depends on `h(z)`. The map `h(z)` is described as piecewise-smooth, meaning it is not differentiable everywhere. At which points `z` does `h(z)` fail to be differentiable, and what is the game-theoretic interpretation of these points?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand when the map h(z) fails to be differentiable and what this means game-theoretically.\n\nThe key insight is in the definition of the retraction map r(z). The components σ_s are defined as:\nσ_s = (z_s - v_n(z))^+\n\nwhere (x)^+ = max{0, x}.\n\nThe max function is not differentiable at x = 0. Therefore, r(z) - and consequently h(z) which depends on r(z) - fails to be differentiable when:\n\nz_s - v_n(z) = 0 for some strategy s\n\nThis occurs precisely at the boundaries where a pure strategy s transitions between being in the support (σ_s > 0) and being out of the support (σ_s = 0) of the mixed strategy profile.\n\nGame-theoretically, these are the critical points where:\n- A pure strategy is about to enter the support of the equilibrium (transitioning from σ_s = 0 to σ_s > 0)\n- A pure strategy is about to leave the support of the equilibrium (transitioning from σ_s > 0 to σ_s = 0)\n\nAt these boundary points, the equilibrium structure changes qualitatively - the set of pure strategies being played with positive probability changes. This creates the non-differentiability in the mapping.\n\nD", "model_answer": "A", "error": null}}
{"ID": 345, "Question": "### Background\n\nStandard evolutionary models with mutations, such as those by Kandori, Mailath, and Rob (KMR), often yield a unique long-run prediction by assuming a uniform mutation rate \\(\\varepsilon\\) across all states. This paper critiques that assumption by allowing mutation rates to be state-dependent. The core insight is that the relative probability of mutations, not just the number of mutations, determines long-run stability.\n\n### Data / Model Specification\n\nA population of \\(N\\) agents repeatedly plays the following coordination game:\n```latex\n          Player 2\n          1      2\nPlayer 1\n   1    (8, 8) (0, 4)\n   2    (4, 0) (6, 6)\n```\nThe system is modeled as a Markov process where the state \\(i \\in S = \\{0, 1, ..., N\\}\\) is the number of agents playing strategy 2. The system has two absorbing states corresponding to the strict Nash equilibria: state 0 (all play strategy 1) and state \\(N\\) (all play strategy 2).\n\nThere is an integer threshold \\(i^* < N/2\\). Without mutations:\n*   The set \\(\\{0, 1, ..., i^*-1\\}\\) is the basin of attraction for state 0.\n*   The set \\(\\{i^*, ..., N\\}\\) is the basin of attraction for state \\(N\\).\n\nMutations are introduced with state-dependent rates:\n*   In the basin of attraction for state 0 (states \\(0, ..., i^*-1\\)), the mutation rate is \\(\\varepsilon^\\alpha\\).\n*   In the basin of attraction for state \\(N\\) (states \\(i^*, ..., N\\)), the mutation rate is \\(\\varepsilon\\).\n\nHere, \\(\\varepsilon > 0\\) is a small baseline rate and \\(\\alpha > 0\\) is a constant. The critical threshold for \\(\\alpha\\) to select state 0 is \\(\\alpha^* = (N-i^*+1)/i^*\\).\n\n### Question\n\nSuppose the payoff for mutual cooperation on strategy 1 is increased to \\((8+\\delta, 8+\\delta)\\). This change does not affect \\(i^*\\), but the reduced incentive to experiment increases \\(\\alpha\\) according to the relationship \\(\\alpha(\\delta) = \\alpha_0 + \\gamma\\delta\\) for constants \\(\\alpha_0, \\gamma > 0\\). Assume the system initially selects state \\(N\\) (i.e., \\(\\alpha_0 < \\alpha^*\\)). What is the minimum payoff increase, \\(\\delta_{min}\\), required to reverse the equilibrium selection and make state 0 the unique long-run outcome?", "Options": {"A": "\\(\\frac{1}{\\gamma} \\left( \\frac{N-i^{*}+1}{i^{*}} - \\alpha_0 \\right)\\)", "B": "\\(\\frac{1}{\\gamma} \\left( \\alpha_0 - \\frac{N-i^{*}+1}{i^{*}} \\right)\\)", "C": "\\(\\gamma \\left( \\frac{N-i^{*}+1}{i^{*}} - \\alpha_0 \\right)\\)", "D": "\\(\\frac{1}{\\gamma} \\left( \\frac{N-i^{*}+1}{i^{*}} \\right)\\)"}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests the ability to apply the model to a novel scenario, a 'Scenario Application' strategy requiring a multi-step 'Computational Judgment'. The distractors are derived from a matrix of plausible errors: (B) a sign error from mishandling the inequality, (C) a formula misuse error (multiplying by \\(\\gamma\\) instead of dividing), and (D) a step-omission error (failing to subtract the initial \\(\\alpha_0\\)). This assesses a deeper understanding beyond simple derivation.", "qid": "345", "question": "### Background\n\nStandard evolutionary models with mutations, such as those by Kandori, Mailath, and Rob (KMR), often yield a unique long-run prediction by assuming a uniform mutation rate \\(\\varepsilon\\) across all states. This paper critiques that assumption by allowing mutation rates to be state-dependent. The core insight is that the relative probability of mutations, not just the number of mutations, determines long-run stability.\n\n### Data / Model Specification\n\nA population of \\(N\\) agents repeatedly plays the following coordination game:\n```latex\n          Player 2\n          1      2\nPlayer 1\n   1    (8, 8) (0, 4)\n   2    (4, 0) (6, 6)\n```\nThe system is modeled as a Markov process where the state \\(i \\in S = \\{0, 1, ..., N\\}\\) is the number of agents playing strategy 2. The system has two absorbing states corresponding to the strict Nash equilibria: state 0 (all play strategy 1) and state \\(N\\) (all play strategy 2).\n\nThere is an integer threshold \\(i^* < N/2\\). Without mutations:\n*   The set \\(\\{0, 1, ..., i^*-1\\}\\) is the basin of attraction for state 0.\n*   The set \\(\\{i^*, ..., N\\}\\) is the basin of attraction for state \\(N\\).\n\nMutations are introduced with state-dependent rates:\n*   In the basin of attraction for state 0 (states \\(0, ..., i^*-1\\)), the mutation rate is \\(\\varepsilon^\\alpha\\).\n*   In the basin of attraction for state \\(N\\) (states \\(i^*, ..., N\\)), the mutation rate is \\(\\varepsilon\\).\n\nHere, \\(\\varepsilon > 0\\) is a small baseline rate and \\(\\alpha > 0\\) is a constant. The critical threshold for \\(\\alpha\\) to select state 0 is \\(\\alpha^* = (N-i^*+1)/i^*\\).\n\n### Question\n\nSuppose the payoff for mutual cooperation on strategy 1 is increased to \\((8+\\delta, 8+\\delta)\\). This change does not affect \\(i^*\\), but the reduced incentive to experiment increases \\(\\alpha\\) according to the relationship \\(\\alpha(\\delta) = \\alpha_0 + \\gamma\\delta\\) for constants \\(\\alpha_0, \\gamma > 0\\). Assume the system initially selects state \\(N\\) (i.e., \\(\\alpha_0 < \\alpha^*\\)). What is the minimum payoff increase, \\(\\delta_{min}\\), required to reverse the equilibrium selection and make state 0 the unique long-run outcome?", "model_evaluation_result": {"model_raw_response": "I need to find the minimum payoff increase δ_min that makes state 0 the unique long-run outcome.\n\nGiven information:\n- Initially, the system selects state N (so α₀ < α*)\n- The critical threshold is α* = (N - i* + 1)/i*\n- The relationship is α(δ) = α₀ + γδ\n- For state 0 to be selected, we need α(δ) ≥ α*\n\nTo reverse the equilibrium selection from state N to state 0, I need:\nα(δ) ≥ α*\n\nSubstituting the relationship:\nα₀ + γδ ≥ α*\n\nSolving for δ:\nγδ ≥ α* - α₀\nδ ≥ (α* - α₀)/γ\n\nSince α* = (N - i* + 1)/i*, the minimum value is:\nδ_min = (α* - α₀)/γ = (1/γ)((N - i* + 1)/i* - α₀)\n\nThis can be written as:\nδ_min = (1/γ)((N - i* + 1)/i* - α₀)\n\nA", "model_answer": "A", "error": null}}
{"ID": 404, "Question": "### Background\n\nAfter first-differencing, the estimated transitory income component, `\\Delta \\ln Y_{it}^T`, is still imperfect. The authors discuss two potential sources of error that create opposing biases in OLS estimates of the transitory income elasticity, `β_T`:\n1.  **Classical Measurement Error:** Random misreporting of income leads to attenuation bias (downward bias).\n2.  **Specification Error:** Misclassifying persistent permanent income shocks as transitory shocks leads to an upward bias, as maintenance spending is more responsive to permanent shocks.\n\nTo address these issues, the authors employ an instrumental variables (IV) strategy. The key results are summarized below.\n\n**Table 1. OLS and IV Estimates of `β_T`**\n| Estimator | `β_T` Estimate | Std. Error |\n| :--- | :--- | :--- |\n| OLS | 0.384** | (0.082) |\n| IV | 0.412** | (0.085) |\n*Notes: ** denotes significance at the 5% level.*\n\n### Question\n\nGiven the two potential sources of bias and the empirical results in Table 1, what can be inferred about the dominant source of bias in the OLS estimate of `β_T`?", "Options": {"A": "Since the IV estimate (0.412) is larger than the OLS estimate (0.384), it suggests the net bias in OLS was negative, meaning attenuation from classical measurement error was the dominant issue.", "B": "Since the IV estimate (0.412) is larger than the OLS estimate (0.384), it suggests the IV procedure corrected an upward bias, meaning specification error was the dominant issue.", "C": "The fact that both OLS and IV estimates are statistically significant and similar in magnitude implies that neither measurement error nor specification error is a serious concern.", "D": "The small difference between the OLS and IV estimates indicates that the upward bias from specification error and the downward bias from measurement error are of roughly equal magnitude, largely cancelling each other out."}, "Answer": "A", "pi_justification": "This item assesses the ability to interpret the results of an IV strategy in the context of competing biases. It uses a Reverse-Reasoning strategy. The correct answer requires understanding that IV is designed to correct bias, and if the corrected estimate is larger, the original net bias must have been negative. Distractor B represents the most common logical error (reversing the inference). Distractor C draws an incorrect conclusion from significance. Distractor D is a plausible but less precise interpretation that ignores the direction of the correction.", "qid": "404", "question": "### Background\n\nAfter first-differencing, the estimated transitory income component, `\\Delta \\ln Y_{it}^T`, is still imperfect. The authors discuss two potential sources of error that create opposing biases in OLS estimates of the transitory income elasticity, `β_T`:\n1.  **Classical Measurement Error:** Random misreporting of income leads to attenuation bias (downward bias).\n2.  **Specification Error:** Misclassifying persistent permanent income shocks as transitory shocks leads to an upward bias, as maintenance spending is more responsive to permanent shocks.\n\nTo address these issues, the authors employ an instrumental variables (IV) strategy. The key results are summarized below.\n\n**Table 1. OLS and IV Estimates of `β_T`**\n| Estimator | `β_T` Estimate | Std. Error |\n| :--- | :--- | :--- |\n| OLS | 0.384** | (0.082) |\n| IV | 0.412** | (0.085) |\n*Notes: ** denotes significance at the 5% level.*\n\n### Question\n\nGiven the two potential sources of bias and the empirical results in Table 1, what can be inferred about the dominant source of bias in the OLS estimate of `β_T`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 403, "Question": "### Background\n\nThe authors model log maintenance spending (`ln M_irt`) with a levels model that includes an unobserved, time-invariant household-specific effect `α_i`:\n```latex\n\\ln M_{irt} = \\alpha_i + \\beta_P \\ln Y_{it}^P + \\beta_T \\ln Y_{it}^T + X_{it}\\delta + Z_{rt}\\gamma + \\eta_{it} \\quad \\text{(Eq. (1))}\n```\nTo eliminate `α_i`, the primary estimating equation is the first-differenced version of this model:\n```latex\n\\Delta\\ln M_{irt} = \\beta_P \\Delta\\ln Y_{it}^P + \\beta_T \\Delta\\ln Y_{it}^T + \\Delta X_{it}\\delta + \\Delta Z_{rt}\\gamma + \\Delta\\eta_{it} \\quad \\text{(Eq. (2))}\n```\n### Question\n\nThe authors transform the levels model (Eq. (1)) into the first-differenced model (Eq. (2)). What is the primary econometric problem this transformation is designed to solve, and what is the key assumption required for its success?", "Options": {"A": "To eliminate omitted variable bias from time-invariant household characteristics (`α_i`), assuming these characteristics are constant over time.", "B": "To correct for measurement error in the income variables, assuming the errors are classical (mean zero and uncorrelated with true values).", "C": "To resolve simultaneity between income and maintenance spending, assuming income shocks are exogenous to maintenance decisions.", "D": "To eliminate omitted variable bias from time-invariant household characteristics (`α_i`), assuming these characteristics are uncorrelated with income."}, "Answer": "A", "pi_justification": "This item assesses understanding of the fundamental purpose of a first-differencing (fixed effects) strategy. It uses a direct question format. The distractors are designed to pinpoint common confusions in econometrics: B and C name other types of endogeneity that first-differencing does not solve, while D correctly identifies the problem but states an incorrect assumption (the whole point of fixed effects is that `α_i` *can* be correlated with regressors).", "qid": "403", "question": "### Background\n\nThe authors model log maintenance spending (`ln M_irt`) with a levels model that includes an unobserved, time-invariant household-specific effect `α_i`:\n```latex\n\\ln M_{irt} = \\alpha_i + \\beta_P \\ln Y_{it}^P + \\beta_T \\ln Y_{it}^T + X_{it}\\delta + Z_{rt}\\gamma + \\eta_{it} \\quad \\text{(Eq. (1))}\n```\nTo eliminate `α_i`, the primary estimating equation is the first-differenced version of this model:\n```latex\n\\Delta\\ln M_{irt} = \\beta_P \\Delta\\ln Y_{it}^P + \\beta_T \\Delta\\ln Y_{it}^T + \\Delta X_{it}\\delta + \\Delta Z_{rt}\\gamma + \\Delta\\eta_{it} \\quad \\text{(Eq. (2))}\n```\n### Question\n\nThe authors transform the levels model (Eq. (1)) into the first-differenced model (Eq. (2)). What is the primary econometric problem this transformation is designed to solve, and what is the key assumption required for its success?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 322, "Question": "### Background\n\nAn industry has `n` symmetric incumbent firms and one potential entrant. The equilibrium outcome depends on the fixed entry cost, `D`, relative to two critical thresholds, `D_1` and `D_2`.\n\n### Data / Model Specification\n\nLet `ξ_k` be the unique symmetric Cournot equilibrium output for each firm in a market with `k` identical firms.\n\n1.  **Accommodated Entry Threshold (`D_1`):** This is the operating profit an entrant would earn if, upon its entry, all `n+1` firms play the standard `(n+1)`-firm Cournot equilibrium.\n    ```latex\n    D_1 = ξ_{n+1}φ((n+1)ξ_{n+1}) - c(ξ_{n+1})\n    ```\n\n2.  **Deterred Entry Threshold (`D_2`):** This is the maximum possible operating profit an entrant could earn if the `n` incumbents are committed to producing their `n`-firm Cournot equilibrium quantities, `ξ_n`.\n    ```latex\n    D_2 = \\max_{q≥0} [qφ(nξ_n + q) - c(q)]\n    ```\n\n### Question\n\nWhich statement best describes the economic interpretation of the strategic beliefs underlying the thresholds `D_1` and `D_2`?", "Options": {"A": "`D_1` represents the entrant's profit if incumbents accommodate entry by reducing output to the `(n+1)`-firm level, while `D_2` represents the entrant's maximum profit if incumbents deter entry by maintaining the `n`-firm output level.", "B": "`D_1` represents the entrant's profit if incumbents deter entry by maintaining the `n`-firm output level, while `D_2` represents the entrant's maximum profit if incumbents accommodate entry by reducing output to the `(n+1)`-firm level.", "C": "`D_1` represents the total profit of an incumbent firm after entry is accommodated, while `D_2` represents the total profit of an incumbent firm if entry is successfully deterred.", "D": "`D_1` is the limit price that incumbents must set to prevent entry, while `D_2` is the monopoly profit the entrant could earn if all incumbents exited the market."}, "Answer": "A", "pi_justification": "This item assesses the core conceptual understanding of the two critical entry cost thresholds. It uses a Reverse-Reasoning strategy, asking for the underlying strategic assumptions given the definitions. Distractor B is the primary conceptual opposite, swapping the definitions. Distractor C incorrectly shifts the perspective from the entrant's potential profit to the incumbent's profit. Distractor D introduces related but incorrect concepts from industrial organization (limit pricing, monopoly profit) that are true but irrelevant here.", "qid": "322", "question": "### Background\n\nAn industry has `n` symmetric incumbent firms and one potential entrant. The equilibrium outcome depends on the fixed entry cost, `D`, relative to two critical thresholds, `D_1` and `D_2`.\n\n### Data / Model Specification\n\nLet `ξ_k` be the unique symmetric Cournot equilibrium output for each firm in a market with `k` identical firms.\n\n1.  **Accommodated Entry Threshold (`D_1`):** This is the operating profit an entrant would earn if, upon its entry, all `n+1` firms play the standard `(n+1)`-firm Cournot equilibrium.\n    ```latex\n    D_1 = ξ_{n+1}φ((n+1)ξ_{n+1}) - c(ξ_{n+1})\n    ```\n\n2.  **Deterred Entry Threshold (`D_2`):** This is the maximum possible operating profit an entrant could earn if the `n` incumbents are committed to producing their `n`-firm Cournot equilibrium quantities, `ξ_n`.\n    ```latex\n    D_2 = \\max_{q≥0} [qφ(nξ_n + q) - c(q)]\n    ```\n\n### Question\n\nWhich statement best describes the economic interpretation of the strategic beliefs underlying the thresholds `D_1` and `D_2`?", "model_evaluation_result": {"model_raw_response": "Looking at the definitions and economic logic:\n\n**D₁ (Accommodated Entry Threshold):**\n- Formula: `ξ_{n+1}φ((n+1)ξ_{n+1}) - c(ξ_{n+1})`\n- This calculates the entrant's profit when there are (n+1) firms total\n- Each firm produces `ξ_{n+1}`, which is the symmetric Cournot equilibrium output for (n+1) firms\n- This represents the scenario where incumbents **accommodate** entry by adjusting their output to the new equilibrium with more firms\n\n**D₂ (Deterred Entry Threshold):**\n- Formula: `max_{q≥0} [qφ(nξ_n + q) - c(q)]`\n- This calculates the entrant's maximum possible profit when incumbents maintain their n-firm output levels `ξ_n`\n- The incumbents are \"committed\" to their original output levels, not adjusting to accommodate the entrant\n- This represents the scenario where incumbents attempt to **deter** entry by maintaining higher output levels\n\nThe key insight is that:\n- In accommodation (D₁): All firms adjust to the new (n+1)-firm equilibrium\n- In deterrence (D₂): Incumbents maintain their n-firm output levels, forcing the entrant to optimize against this fixed capacity\n\nA", "model_answer": "A", "error": null}}
{"ID": 413, "Question": "### Background\n\nAn auctioneer seeks to find a Walrasian equilibrium price vector `p*` in a market with substitute goods (`S₁`) and complement goods (`S₂`). The equilibrium prices are the global minimizers of a Lyapunov function `L(p)`.\n\n### Data / Model Specification\n\n**The Global Dynamic Double-Track (GDDT) Procedure:**\nThis procedure is guaranteed to find an equilibrium from any starting integer price vector `p(0)`.\n- **Phase 1:** An \"uphill\" search phase where prices of substitutes (`S₁`) are raised and prices of complements (`S₂`) are lowered, seeking to minimize `L(p)`. This continues until no such move can further decrease `L(p)`.\n- **Phase 2:** A \"downhill\" search phase where prices of substitutes (`S₁`) are lowered and prices of complements (`S₂`) are raised. This continues until no such move can further decrease `L(p)`.\n\nThe procedure stops when it finds a price vector that is a local minimum with respect to both sets of price movements.\n\n### Question\n\nWhich mathematical property of the Lyapunov function `L(p)` is most critical for ensuring that the final stopping point of the GDDT procedure is a *global* minimum, and therefore a true Walrasian equilibrium?", "Options": {"A": "Convexity, because for a convex function, any local minimum is also a global minimum.", "B": "Generalized submodularity, because it ensures the set of equilibria forms a well-structured generalized lattice.", "C": "Continuity, because it prevents jumps in the function that could hide the true minimum from an iterative search.", "D": "Integer-valuedness, because it ensures the algorithm terminates in a finite number of steps."}, "Answer": "A", "pi_justification": "This item assesses the core theoretical insight behind the algorithm's success. The distractors are all true properties of the Lyapunov function mentioned in the paper, forcing the test-taker to discriminate between necessary-but-insufficient properties (Continuity), properties relevant to the structure of the solution set but not the search algorithm's success (Generalized Submodularity), and properties explaining termination rather than global optimality (Integer-valuedness).", "qid": "413", "question": "### Background\n\nAn auctioneer seeks to find a Walrasian equilibrium price vector `p*` in a market with substitute goods (`S₁`) and complement goods (`S₂`). The equilibrium prices are the global minimizers of a Lyapunov function `L(p)`.\n\n### Data / Model Specification\n\n**The Global Dynamic Double-Track (GDDT) Procedure:**\nThis procedure is guaranteed to find an equilibrium from any starting integer price vector `p(0)`.\n- **Phase 1:** An \"uphill\" search phase where prices of substitutes (`S₁`) are raised and prices of complements (`S₂`) are lowered, seeking to minimize `L(p)`. This continues until no such move can further decrease `L(p)`.\n- **Phase 2:** A \"downhill\" search phase where prices of substitutes (`S₁`) are lowered and prices of complements (`S₂`) are raised. This continues until no such move can further decrease `L(p)`.\n\nThe procedure stops when it finds a price vector that is a local minimum with respect to both sets of price movements.\n\n### Question\n\nWhich mathematical property of the Lyapunov function `L(p)` is most critical for ensuring that the final stopping point of the GDDT procedure is a *global* minimum, and therefore a true Walrasian equilibrium?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 372, "Question": "### Background\n\n**Research Question.** This problem explores how an agent's time horizon affects their choice between incremental refinement (intensive search) and breakthrough innovation (extensive search).\n\n**Setting.** We compare the decision of a standard one-period agent with that of a two-period agent who can make two consecutive search decisions. The comparison is made at a point where the one-period agent is exactly indifferent between intensive and extensive search, meaning the expected payoffs in the first period are identical.\n\n### Data / Model Specification\n\nKey results from the model that inform the two-period decision:\n*   If an agent starts with **intensive search**, the subsequent search in the second period will necessarily be **extensive**.\n*   If an agent starts with **extensive search**, the subsequent search can be either **intensive** (if the initial discovery is promising) or **extensive**.\n\n### Question\n\nGiven that the first-period payoffs are identical, the two-period agent's choice depends on the expected payoff in the second period. What is the key structural difference in the second-period decision problem that makes starting with extensive search more valuable?", "Options": {"A": "After an initial extensive search, the agent holds an option to choose the best of two search modes in the second period, whereas after an initial intensive search, the second-period action is fixed.", "B": "After an initial intensive search, the agent can choose between refining the new result or starting an extensive search, whereas after an initial extensive search, the second-period action is fixed.", "C": "The cost `c` of extensive search must be paid again in the second period if the first search was also extensive, making an initial intensive search preferable.", "D": "An initial extensive search guarantees that the second-period search will be an intensive one, which has a higher expected growth rate `g_I`."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.0). This question complements the previous one by focusing on the specific structural difference in the choice set that gives rise to option value. It uses a Computational Judgment strategy, requiring the student to compare the structure of the two decision paths. Distractor B (Conceptual Opposite) directly reverses the logic of the model. Distractor C (Formula Misuse) introduces an incorrect assumption about costs. Distractor D (Almost Right) correctly identifies that intensive search can be high-growth but incorrectly states it's a guaranteed outcome and misses the core idea of the *choice* itself being valuable.", "qid": "372", "question": "### Background\n\n**Research Question.** This problem explores how an agent's time horizon affects their choice between incremental refinement (intensive search) and breakthrough innovation (extensive search).\n\n**Setting.** We compare the decision of a standard one-period agent with that of a two-period agent who can make two consecutive search decisions. The comparison is made at a point where the one-period agent is exactly indifferent between intensive and extensive search, meaning the expected payoffs in the first period are identical.\n\n### Data / Model Specification\n\nKey results from the model that inform the two-period decision:\n*   If an agent starts with **intensive search**, the subsequent search in the second period will necessarily be **extensive**.\n*   If an agent starts with **extensive search**, the subsequent search can be either **intensive** (if the initial discovery is promising) or **extensive**.\n\n### Question\n\nGiven that the first-period payoffs are identical, the two-period agent's choice depends on the expected payoff in the second period. What is the key structural difference in the second-period decision problem that makes starting with extensive search more valuable?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the structural difference in second-period options based on the first-period choice.\n\nFrom the given information:\n- If starting with **intensive search**: second period search is **necessarily extensive** (fixed choice)\n- If starting with **extensive search**: second period can be either **intensive** or **extensive** (flexible choice based on whether initial discovery is promising)\n\nThe key insight is about optionality and flexibility in the second period.\n\nWhen the agent starts with extensive search, they retain the ability to choose between two different search modes in the second period based on the outcome of their first search. This creates an option value - they can adapt their second-period strategy optimally.\n\nWhen the agent starts with intensive search, their second-period choice is constrained to be extensive search only, eliminating any flexibility to adapt based on first-period outcomes.\n\nThis difference in optionality - having a choice versus being locked into a specific action - is what makes the extensive-first strategy more valuable despite identical first-period payoffs.\n\nA", "model_answer": "A", "error": null}}
{"ID": 412, "Question": "### Background\n\nAn auctioneer seeks to find a Walrasian equilibrium price vector `p*` in a market with substitute goods (`S₁`) and complement goods (`S₂`). The equilibrium prices are the global minimizers of a convex Lyapunov function `L(p)`.\n\n### Data / Model Specification\n\n**The Dynamic Double-Track (DDT) Procedure:**\nThis is an iterative price-adjustment process. At each step `t`, it updates the price vector `p(t+1) = p(t) + δ(t)`. The adjustment `δ(t)` is chosen to decrease `L(p)` as much as possible, subject to the constraint that prices for substitute goods (`S₁`) can only increase or stay the same, and prices for complement goods (`S₂`) can only decrease or stay the same. This defines a monotonic, \"uphill\" search according to a generalized price order `≤_g`.\n\n### Question\n\nThe simple DDT procedure is only guaranteed to find an equilibrium if it starts from a price `p(0)` that is \"below\" the equilibrium set (i.e., `p(0) ≤_g p*`). If started at a price that is already \"above\" all equilibrium prices, the procedure fails to find an equilibrium. What is the fundamental reason for this failure?", "Options": {"A": "The procedure is monotonic and cannot reverse direction; it is an \"uphill\" search that gets stuck if it starts above the target.", "B": "The Lyapunov function `L(p)` is not convex at prices far from equilibrium, leading the algorithm to a saddle point.", "C": "The procedure oscillates between two non-equilibrium price vectors without converging.", "D": "Buyers would strategically misreport their demands at such high prices, causing the auctioneer's optimization to be invalid."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to assess understanding of a key algorithmic limitation. The distractors are designed to be plausible but incorrect explanations: B incorrectly challenges a given property of the function rather than the algorithm, C introduces a failure mode from a different class of algorithms (e.g., Scarf's example for general tatonnement), and D introduces a strategic consideration irrelevant to the mechanical failure being asked about.", "qid": "412", "question": "### Background\n\nAn auctioneer seeks to find a Walrasian equilibrium price vector `p*` in a market with substitute goods (`S₁`) and complement goods (`S₂`). The equilibrium prices are the global minimizers of a convex Lyapunov function `L(p)`.\n\n### Data / Model Specification\n\n**The Dynamic Double-Track (DDT) Procedure:**\nThis is an iterative price-adjustment process. At each step `t`, it updates the price vector `p(t+1) = p(t) + δ(t)`. The adjustment `δ(t)` is chosen to decrease `L(p)` as much as possible, subject to the constraint that prices for substitute goods (`S₁`) can only increase or stay the same, and prices for complement goods (`S₂`) can only decrease or stay the same. This defines a monotonic, \"uphill\" search according to a generalized price order `≤_g`.\n\n### Question\n\nThe simple DDT procedure is only guaranteed to find an equilibrium if it starts from a price `p(0)` that is \"below\" the equilibrium set (i.e., `p(0) ≤_g p*`). If started at a price that is already \"above\" all equilibrium prices, the procedure fails to find an equilibrium. What is the fundamental reason for this failure?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 358, "Question": "### Background\n\nThe nutrition-based efficiency wage model (EWM) predicts that employers will not hire lower-wage workers if doing so increases the cost per efficiency unit of labor. The condition for wage rigidity is:\n\n```latex\n\\frac{M}{E(C)} \\geq \\frac{W}{E(C^*)} \\quad \\text{(Eq. (1))}\n```\nwhere `M` and `C` are the cost and calories for an undercutting worker, and `W` and `C*` are the wage and calories for an employed worker.\n\n### Data / Model Specification\n\nIn a scenario from Tamil Nadu, an employed worker earns a wage `W=3` rupees and consumes `C*=5000` calories. An unemployed worker offers to work for a meal costing `M=1` rupee that provides `C=2601` calories. The employed worker is estimated to be 34.58% more productive, which is insufficient for the EWM condition to hold.\n\n### Question\n\nWhat is the *minimum* percentage productivity gain (`p`) a 5000-calorie worker must have over a 2601-calorie worker for the employer to be indifferent between them (i.e., for Eq. (1) to hold with equality)?", "Options": {"A": "100%", "B": "200%", "C": "300%", "D": "66.7%"}, "Answer": "B", "pi_justification": "This item assesses higher-order quantitative reasoning by asking for a reverse-engineering calculation. It uses a Reverse-Reasoning strategy. The student must set up the EWM indifference condition (`1/1 = 3/(1+p)`) and solve for the unknown productivity gain `p`. The correct answer is `p=2`, or 200%. Distractor A (100%) results from an error where the wage is mistakenly assumed to be `W=2`. Distractor C (300%) results from misinterpreting `p` as the total productivity factor `1+p`. Distractor D (66.7%) could result from an inverted calculation like `(W-M)/W`.", "qid": "358", "question": "### Background\n\nThe nutrition-based efficiency wage model (EWM) predicts that employers will not hire lower-wage workers if doing so increases the cost per efficiency unit of labor. The condition for wage rigidity is:\n\n```latex\n\\frac{M}{E(C)} \\geq \\frac{W}{E(C^*)} \\quad \\text{(Eq. (1))}\n```\nwhere `M` and `C` are the cost and calories for an undercutting worker, and `W` and `C*` are the wage and calories for an employed worker.\n\n### Data / Model Specification\n\nIn a scenario from Tamil Nadu, an employed worker earns a wage `W=3` rupees and consumes `C*=5000` calories. An unemployed worker offers to work for a meal costing `M=1` rupee that provides `C=2601` calories. The employed worker is estimated to be 34.58% more productive, which is insufficient for the EWM condition to hold.\n\n### Question\n\nWhat is the *minimum* percentage productivity gain (`p`) a 5000-calorie worker must have over a 2601-calorie worker for the employer to be indifferent between them (i.e., for Eq. (1) to hold with equality)?", "model_evaluation_result": {"model_raw_response": "I need to find the minimum percentage productivity gain for the employer to be indifferent between the two workers.\n\nGiven information:\n- Employed worker: W = 3 rupees, C* = 5000 calories\n- Unemployed worker: M = 1 rupee, C = 2601 calories\n- Current productivity difference: 34.58% (insufficient)\n\nThe EWM condition for indifference is:\nM/E(C) = W/E(C*)\n\nLet me denote the efficiency function as E(C), where efficiency increases with calorie consumption.\n\nIf the 5000-calorie worker has a productivity gain of p% over the 2601-calorie worker, then:\nE(C*) = E(C) × (1 + p/100)\n\nFor indifference (equality in Eq. 1):\n1/E(C) = 3/E(C*)\n1/E(C) = 3/[E(C) × (1 + p/100)]\n\nSimplifying:\n1 = 3/(1 + p/100)\n1 + p/100 = 3\np/100 = 2\np = 200%\n\nTherefore, the 5000-calorie worker must be 200% more productive (i.e., 3 times as productive) as the 2601-calorie worker for the employer to be indifferent.\n\nB", "model_answer": "C", "error": null}}
{"ID": 341, "Question": "### Background\n\nA study finds that in the 2000-2019 period, female economists had a higher probability of being selected as an Econometric Society Fellow than male economists with identical, observable publication and citation records. This conclusion relies on a 'selection on observables' assumption: that after controlling for the rich set of observable achievements (`X`), gender is not correlated with any remaining unobserved determinants of election.\n\n### Question\n\nA potential critique of the study is that, due to higher publishing barriers, female scholars with the same publication record (`X`) as male scholars may have higher unobserved 'true quality' (`Q`). If this is true, how would this unobserved quality bias the estimated coefficient on `Female` for the 2000-2019 period and what is the implication for the paper's conclusion?", "Options": {"A": "The coefficient is biased upwards, which weakens the conclusion that there is a female premium.", "B": "The coefficient is biased upwards, which strengthens the conclusion that there is a female premium.", "C": "The coefficient is biased downwards, which weakens the conclusion that there is a female premium.", "D": "The coefficient is biased downwards, which strengthens the conclusion that there is a female premium."}, "Answer": "A", "pi_justification": "Assessment Target: Applying the logic of omitted variable bias (OVB) to a specific research context. Strategy: Scenario Application. Distractor Logic: The correct reasoning is: 1) Unobserved quality `Q` is positively correlated with election. 2) Conditional on `X`, `Q` is positively correlated with `Female`. 3) The product of these two effects is positive, so the bias on the `Female` coefficient is positive (upward). 4) An upward bias means the estimated premium is an overestimate of the true premium, thus weakening the conclusion. The distractors represent a 2x2 matrix of common errors: getting the bias direction wrong (C, D) or getting the implication wrong (B).", "qid": "341", "question": "### Background\n\nA study finds that in the 2000-2019 period, female economists had a higher probability of being selected as an Econometric Society Fellow than male economists with identical, observable publication and citation records. This conclusion relies on a 'selection on observables' assumption: that after controlling for the rich set of observable achievements (`X`), gender is not correlated with any remaining unobserved determinants of election.\n\n### Question\n\nA potential critique of the study is that, due to higher publishing barriers, female scholars with the same publication record (`X`) as male scholars may have higher unobserved 'true quality' (`Q`). If this is true, how would this unobserved quality bias the estimated coefficient on `Female` for the 2000-2019 period and what is the implication for the paper's conclusion?", "model_evaluation_result": {"model_raw_response": "Looking at this step by step:\n\n1. The study finds that female economists have a higher probability of being selected as Fellows than males with identical observable records (positive coefficient on `Female`).\n\n2. The critique suggests that due to higher publishing barriers, females with the same publication record `X` as males may have higher unobserved 'true quality' `Q`.\n\n3. This means: E[Q|Female=1, X] > E[Q|Female=0, X]\n\n4. Since higher true quality `Q` should increase the probability of being selected as a Fellow, we have a positive correlation between the unobserved quality and the outcome.\n\n5. In the regression, `Female` is positively correlated with unobserved quality `Q`, and `Q` is positively correlated with the outcome (Fellow selection).\n\n6. This creates an omitted variable bias. The coefficient on `Female` captures both:\n   - The direct effect of gender (what we want to measure)\n   - The indirect effect through higher unobserved quality\n\n7. Therefore, the estimated coefficient on `Female` is biased upwards - it's picking up the effect of the higher unobserved quality that females possess.\n\n8. If the true gender effect were smaller (or even zero/negative), this would weaken the conclusion that there is a genuine female premium in Fellow selection.\n\nThe answer is A.", "model_answer": "A", "error": null}}
{"ID": 323, "Question": "### Background\n\nAn industry has `n` symmetric incumbent firms and one potential entrant. The equilibrium outcome depends on the fixed entry cost, `D`, relative to two critical thresholds, `D_1` and `D_2`.\n\n### Data / Model Specification\n\nLet `ξ_k` be the unique symmetric Cournot equilibrium output for each firm in a market with `k` identical firms.\n\n1.  **Accommodated Entry Threshold (`D_1`):** This is the operating profit an entrant would earn if, upon its entry, all `n+1` firms play the standard `(n+1)`-firm Cournot equilibrium.\n\n2.  **Deterred Entry Threshold (`D_2`):** This is the maximum possible operating profit an entrant could earn if the `n` incumbents are committed to producing their `n`-firm Cournot equilibrium quantities, `ξ_n`.\n\nIt is established that `D_1 < D_2`.\n\n### Question\n\nIn the \"low cost\" region where the entry cost `D` is less than `D_1`, what is the noncooperative equilibrium outcome?", "Options": {"A": "The entrant enters with certainty (`δ* = 1`), and all `n+1` firms produce the `(n+1)`-firm Cournot quantity, `ξ_{n+1}`.", "B": "The entrant stays out (`δ* = 0`), and the `n` incumbents produce the `n`-firm Cournot quantity, `ξ_n`.", "C": "The entrant randomizes its entry decision (`0 < δ* < 1`), and incumbents produce a quantity between `ξ_n` and `ξ_{n+1}`.", "D": "The entrant enters with certainty (`δ* = 1`), but the `n` incumbents continue to produce their `n`-firm Cournot quantity, `ξ_n`."}, "Answer": "A", "pi_justification": "This item assesses the ability to apply the model's central result to a specific scenario. It uses a Scenario Application strategy. The correct answer identifies the pure strategy equilibrium for the low-cost case. Distractor B is the conceptual opposite (the high-cost outcome). Distractor C describes the intermediate-cost outcome. Distractor D presents an internally inconsistent scenario (entry occurs, but incumbents play a non-best-response), representing a common comprehension slip.", "qid": "323", "question": "### Background\n\nAn industry has `n` symmetric incumbent firms and one potential entrant. The equilibrium outcome depends on the fixed entry cost, `D`, relative to two critical thresholds, `D_1` and `D_2`.\n\n### Data / Model Specification\n\nLet `ξ_k` be the unique symmetric Cournot equilibrium output for each firm in a market with `k` identical firms.\n\n1.  **Accommodated Entry Threshold (`D_1`):** This is the operating profit an entrant would earn if, upon its entry, all `n+1` firms play the standard `(n+1)`-firm Cournot equilibrium.\n\n2.  **Deterred Entry Threshold (`D_2`):** This is the maximum possible operating profit an entrant could earn if the `n` incumbents are committed to producing their `n`-firm Cournot equilibrium quantities, `ξ_n`.\n\nIt is established that `D_1 < D_2`.\n\n### Question\n\nIn the \"low cost\" region where the entry cost `D` is less than `D_1`, what is the noncooperative equilibrium outcome?", "model_evaluation_result": {"model_raw_response": "I need to analyze this entry game with symmetric incumbent firms and determine the equilibrium outcome when entry costs are in the \"low cost\" region (D < D₁).\n\nLet me understand the key elements:\n\n1. D₁ is the accommodated entry threshold - the operating profit an entrant would earn if all n+1 firms play the standard (n+1)-firm Cournot equilibrium after entry.\n\n2. D₂ is the deterred entry threshold - the maximum operating profit an entrant could earn if incumbents are committed to their n-firm Cournot quantities.\n\n3. We know D₁ < D₂.\n\n4. We're analyzing the case where D < D₁ (low cost region).\n\nIn the low cost region where D < D₁:\n\n- Since D < D₁, the entry cost is less than the profit the entrant would earn in the accommodated entry scenario (standard (n+1)-firm Cournot equilibrium).\n\n- This means entry is profitable even if incumbents accommodate by adjusting to the (n+1)-firm Cournot equilibrium.\n\n- Since entry is profitable in the accommodated scenario, and this represents the \"worst case\" for the entrant among rational incumbent responses, the entrant will definitely enter.\n\n- Knowing that entry will occur, the rational response for incumbents is to accommodate entry and play the (n+1)-firm Cournot equilibrium, since trying to deter entry would be futile when D < D₁.\n\nTherefore, in the low cost region:\n- The entrant enters with certainty (δ* = 1)\n- All n+1 firms produce the (n+1)-firm Cournot quantity ξₙ₊₁\n\nA", "model_answer": "D", "error": null}}
{"ID": 319, "Question": "### Background\n\n**Research Question.** This problem analyzes the bias in regulatory assessments of firm conduct when the regulator uses an incomplete or static measure of marginal cost in a dynamic setting with capital adjustment costs.\n\n**Setting.** A regulator observes a firm's price and can estimate a proxy for its marginal cost, but this proxy omits a forward-looking component related to future investment plans, which are private information to the firm.\n\n**Variables and Parameters.**\n- `I_t`: Gross investment at time `t`.\n- `c(I)`: A strictly convex cost function for investment, `c''(I) > 0`.\n- `P_t`: Price of output at time `t`.\n- `\\mathcal{MC}_t^d`: True dynamic marginal cost of output at time `t`.\n- `\\widehat{MC}_t`: An observable but incomplete proxy for marginal cost.\n- `RN`: The Ramsey Number, `RN = \\frac{P-MC}{P}|\\varepsilon_D|`, which is 0 for a first-best firm and 1 for a pure monopolist.\n\n---\n\n### Data / Model Specification\n\nThe true dynamic marginal cost `\\mathcal{MC}_t^d` can be decomposed as `\\mathcal{MC}_{t}^{d} = \\widehat{MC}_{t} + \\widehat{E}_{t}`, where `\\widehat{E}_{t}` is an unobservable error term. During a period of investment expansion (`I_t > I_{t-1}`), the error term `\\widehat{E}_{t}` is negative.\n\n---\n\n### Question\n\nA profit-maximizing monopolist, whose true Ramsey Number is 1, is observed by a regulator during a period of rapid investment expansion (`I_t > I_{t-1}`). The regulator calculates an estimated Ramsey Number, `\\widehat{RN}_t`, using the proxy `\\widehat{MC}_t`. Which of the following correctly describes the value of `\\widehat{RN}_t` and the implication for regulatory practice?", "Options": {"A": "`\\widehat{RN}_t < 1`. This implies that a low calculated Ramsey Number is not sufficient evidence to rule out monopolistic behavior.", "B": "`\\widehat{RN}_t > 1`. This implies that regulators might incorrectly accuse a firm of excessive pricing during expansion.", "C": "`\\widehat{RN}_t < 1`. This implies that the firm is likely behaving competitively, as the number is closer to the first-best value of 0.", "D": "`\\widehat{RN}_t = 1`. This implies that the proxy measure is unbiased and accurately reflects the firm's monopolistic behavior."}, "Answer": "A", "pi_justification": "This item tests the student's ability to synthesize the result from the previous step into a final regulatory conclusion, assessing the core policy takeaway of this part of the paper. It uses a Computational Judgment strategy. The distractors are designed around key conceptual errors: getting the direction of the bias wrong (B), getting the calculation right but the policy implication wrong (C), and incorrectly assuming the proxy is unbiased (D).", "qid": "319", "question": "### Background\n\n**Research Question.** This problem analyzes the bias in regulatory assessments of firm conduct when the regulator uses an incomplete or static measure of marginal cost in a dynamic setting with capital adjustment costs.\n\n**Setting.** A regulator observes a firm's price and can estimate a proxy for its marginal cost, but this proxy omits a forward-looking component related to future investment plans, which are private information to the firm.\n\n**Variables and Parameters.**\n- `I_t`: Gross investment at time `t`.\n- `c(I)`: A strictly convex cost function for investment, `c''(I) > 0`.\n- `P_t`: Price of output at time `t`.\n- `\\mathcal{MC}_t^d`: True dynamic marginal cost of output at time `t`.\n- `\\widehat{MC}_t`: An observable but incomplete proxy for marginal cost.\n- `RN`: The Ramsey Number, `RN = \\frac{P-MC}{P}|\\varepsilon_D|`, which is 0 for a first-best firm and 1 for a pure monopolist.\n\n---\n\n### Data / Model Specification\n\nThe true dynamic marginal cost `\\mathcal{MC}_t^d` can be decomposed as `\\mathcal{MC}_{t}^{d} = \\widehat{MC}_{t} + \\widehat{E}_{t}`, where `\\widehat{E}_{t}` is an unobservable error term. During a period of investment expansion (`I_t > I_{t-1}`), the error term `\\widehat{E}_{t}` is negative.\n\n---\n\n### Question\n\nA profit-maximizing monopolist, whose true Ramsey Number is 1, is observed by a regulator during a period of rapid investment expansion (`I_t > I_{t-1}`). The regulator calculates an estimated Ramsey Number, `\\widehat{RN}_t`, using the proxy `\\widehat{MC}_t`. Which of the following correctly describes the value of `\\widehat{RN}_t` and the implication for regulatory practice?", "model_evaluation_result": {"model_raw_response": "I need to analyze this step by step.\n\nGiven information:\n- True dynamic marginal cost: $\\mathcal{MC}_t^d = \\widehat{MC}_t + \\widehat{E}_t$\n- During investment expansion ($I_t > I_{t-1}$): $\\widehat{E}_t < 0$\n- True Ramsey Number for a monopolist: $RN = 1$\n- Estimated Ramsey Number: $\\widehat{RN}_t = \\frac{P-\\widehat{MC}_t}{P}|\\varepsilon_D|$\n\nSince $\\mathcal{MC}_t^d = \\widehat{MC}_t + \\widehat{E}_t$ and $\\widehat{E}_t < 0$ during expansion:\n$\\widehat{MC}_t = \\mathcal{MC}_t^d - \\widehat{E}_t > \\mathcal{MC}_t^d$\n\nThis means the proxy marginal cost $\\widehat{MC}_t$ overestimates the true dynamic marginal cost.\n\nFor a profit-maximizing monopolist, the true relationship is:\n$P - \\mathcal{MC}_t^d = \\frac{P}{|\\varepsilon_D|}$ (giving $RN = 1$)\n\nBut the regulator calculates:\n$\\widehat{RN}_t = \\frac{P - \\widehat{MC}_t}{P}|\\varepsilon_D|$\n\nSince $\\widehat{MC}_t > \\mathcal{MC}_t^d$:\n$P - \\widehat{MC}_t < P - \\mathcal{MC}_t^d$\n\nTherefore: $\\widehat{RN}_t < 1$\n\nThis means the estimated Ramsey Number underestimates the true monopolistic behavior. A low calculated Ramsey Number (closer to the competitive benchmark of 0) does not provide sufficient evidence to rule out monopolistic behavior, as the firm could still be a pure monopolist but appear more competitive due to the measurement bias.\n\nA", "model_answer": "A", "error": null}}
{"ID": 374, "Question": "### Background\n\n**Research Question.** This problem investigates the central empirical puzzle motivating the paper: how to reconcile the massive reallocation of labor towards the service sector with the observed stability of relative wages. It then uses the model's counterfactual simulations to test whether significant labor market frictions are compatible with this empirical fact and to quantify the aggregate cost of these frictions.\n\n**Setting / Institutional Environment.** The analysis uses the paper's estimated general equilibrium model of the U.S. economy (1968-2000). The core exercise is to compare the baseline model simulation against counterfactuals where inter-sectoral mobility costs are set to zero or to a prohibitively high level.\n\n### Data / Model Specification\n\n**Table 1: Effect of Mobility Costs on Sectoral Output (Index, 1968 Goods Sector = 1.0)**\n\n| Period | Predicted (Baseline) | Counterfactual 1 (Prohibitive Cost) | Counterfactual 3 (Zero Cost) |\n| :--- | :---: | :---: | :---: |\n| **Goods Sector Output** | |\n| 1995-2000 | 1.59 | 0.65 | 2.94 |\n| **Service Sector Output** | |\n| 1995-2000 | 5.21 | 4.45 | 11.72 |\n\n*Source: Abridged from Table VIII in the paper.*\n\n### Question\n\nUsing the data for the 1995-2000 period in Table 1, calculate the approximate percentage increase in total aggregate output (Goods + Services) that would occur if the economy moved from the 'Predicted (Baseline)' scenario to the 'Counterfactual 3 (Zero Cost)' scenario. What does this imply about the estimated mobility costs?", "Options": {"A": "116%; this implies that the estimated mobility costs create substantial allocative inefficiency, leading to a large loss of potential aggregate output.", "B": "54%; this implies that the estimated mobility costs have a moderate but significant impact on allocative efficiency.", "C": "116%; this implies that mobility costs are beneficial for the economy, as they prevent excessive reallocation and instability.", "D": "210%; this implies that the costs of mobility are catastrophically high, making the baseline model unrealistic."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.38). The original QA problem was a sequence of highly convertible parts. This Single Choice item uses the Computational Judgment strategy, requiring a multi-step calculation followed by a conceptual interpretation. The correct answer combines the correct calculation with the correct inference. Distractor B is based on a common calculation error (dividing the change by the wrong base: (14.66 - 6.80) / 14.66 ≈ 54%). Distractor C has the correct calculation but an incorrect, conceptually opposite interpretation. Distractor D represents a different calculation error.", "qid": "374", "question": "### Background\n\n**Research Question.** This problem investigates the central empirical puzzle motivating the paper: how to reconcile the massive reallocation of labor towards the service sector with the observed stability of relative wages. It then uses the model's counterfactual simulations to test whether significant labor market frictions are compatible with this empirical fact and to quantify the aggregate cost of these frictions.\n\n**Setting / Institutional Environment.** The analysis uses the paper's estimated general equilibrium model of the U.S. economy (1968-2000). The core exercise is to compare the baseline model simulation against counterfactuals where inter-sectoral mobility costs are set to zero or to a prohibitively high level.\n\n### Data / Model Specification\n\n**Table 1: Effect of Mobility Costs on Sectoral Output (Index, 1968 Goods Sector = 1.0)**\n\n| Period | Predicted (Baseline) | Counterfactual 1 (Prohibitive Cost) | Counterfactual 3 (Zero Cost) |\n| :--- | :---: | :---: | :---: |\n| **Goods Sector Output** | |\n| 1995-2000 | 1.59 | 0.65 | 2.94 |\n| **Service Sector Output** | |\n| 1995-2000 | 5.21 | 4.45 | 11.72 |\n\n*Source: Abridged from Table VIII in the paper.*\n\n### Question\n\nUsing the data for the 1995-2000 period in Table 1, calculate the approximate percentage increase in total aggregate output (Goods + Services) that would occur if the economy moved from the 'Predicted (Baseline)' scenario to the 'Counterfactual 3 (Zero Cost)' scenario. What does this imply about the estimated mobility costs?", "model_evaluation_result": {"model_raw_response": "I need to calculate the percentage increase in total aggregate output when moving from the Baseline to the Zero Cost scenario.\n\nFirst, let me calculate the total output for each scenario in 1995-2000:\n\n**Baseline (Predicted):**\n- Goods Sector: 1.59\n- Service Sector: 5.21\n- Total: 1.59 + 5.21 = 6.80\n\n**Zero Cost (Counterfactual 3):**\n- Goods Sector: 2.94\n- Service Sector: 11.72\n- Total: 2.94 + 11.72 = 14.66\n\nNow I'll calculate the percentage increase:\nPercentage increase = [(New Value - Original Value) / Original Value] × 100\n= [(14.66 - 6.80) / 6.80] × 100\n= [7.86 / 6.80] × 100\n= 1.156 × 100\n= 115.6%\n\nThis rounds to approximately 116%.\n\nThe economic interpretation: A 116% increase in aggregate output when mobility costs are eliminated indicates that the estimated mobility costs create substantial allocative inefficiency. Workers are prevented from moving to their most productive sectors, leading to a large loss of potential aggregate output.\n\nA", "model_answer": "A", "error": null}}
{"ID": 365, "Question": "## Background\n\n**Research Question.** This problem assesses whether industry-level stock market excess returns (\\(\\eta_{j,t}\\)) can predict real labor market adjustments, specifically future excess employment growth (\\(\\hat{\\xi}_{j,t}^{q}\\)).\n\n## Data / Model Specification\n\nAn interactive model is estimated to test if the effect of current excess returns depends on the persistence of past returns:\n```latex\n\\hat{\\xi}_{j,t}^{q} = \\tau_{q} + [\\delta_{1q} + \\delta_{2q} |\\sum_{k=0}^{7} \\eta_{j,t-k}|] \\eta_{j,t} + \\nu_{j,t}^{q} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1. Regression Estimates for 2-Year Employment Growth (q=8)**\n\n| Coefficient | Estimate | Std. Error |\n| :--- | :--- | :--- |\n| \\(\\hat{\\delta}_{1,8}\\) | 0.0357 | (0.0237) |\n| \\(\\hat{\\delta}_{2,8}\\) | 1.208 | (0.475) |\n\n## Question\n\nConsider two industries. The 'Utility' industry has experienced a string of positive shocks totaling 20% over the prior two years (\\(\\sum_{k=0}^{7} \\eta_{U,t-k} = 0.20\\)). The 'Tech' industry has experienced volatile but cumulatively zero excess returns over the same period (\\(\\sum_{k=0}^{7} \\eta_{T,t-k} = 0\\)). Both industries now experience an identical positive 10% excess return shock (\\(\\eta_{j,t}=0.10\\)).\n\nUsing the model and estimates provided, what is the predicted 2-year excess employment growth for the **Utility** industry?", "Options": {"A": "2.77%", "B": "0.36%", "C": "2.42%", "D": "0.04%"}, "Answer": "A", "pi_justification": "Assessment Target: Tests the ability to apply a non-linear model to a specific scenario, requiring careful reading, selection of correct coefficients, and accurate calculation. Strategy: Scenario Application / Computational Judgment. Distractor Logic: The correct answer (A) is from the full calculation: `[0.0357 + 1.208 * 0.20] * 0.10 = 0.0277`. Distractor (B) is the correct answer for the *Tech* industry (`[0.0357 + 1.208 * 0] * 0.10 = 0.0036`), catching students who apply the model to the wrong case. Distractor (C) results from a calculation error, specifically multiplying the second term by `eta_j,t` twice (`0.0357*0.1 + 1.208*0.2*0.1 = 0.0277`). No, let's find a better one. Distractor (C) could be `1.208 * 0.20 = 0.2416`, forgetting to multiply by the final `eta_j,t`. Let's recalculate. A better distractor is (C) `2.42%`, which is the value of the interaction term alone (`1.208 * 0.20 * 0.10 = 0.02416`). Distractor (D) `0.04%` is the value of the base term alone (`0.0357 * 0.10 = 0.000357`), which is almost the same as the Tech industry answer.", "qid": "365", "question": "## Background\n\n**Research Question.** This problem assesses whether industry-level stock market excess returns (\\(\\eta_{j,t}\\)) can predict real labor market adjustments, specifically future excess employment growth (\\(\\hat{\\xi}_{j,t}^{q}\\)).\n\n## Data / Model Specification\n\nAn interactive model is estimated to test if the effect of current excess returns depends on the persistence of past returns:\n```latex\n\\hat{\\xi}_{j,t}^{q} = \\tau_{q} + [\\delta_{1q} + \\delta_{2q} |\\sum_{k=0}^{7} \\eta_{j,t-k}|] \\eta_{j,t} + \\nu_{j,t}^{q} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1. Regression Estimates for 2-Year Employment Growth (q=8)**\n\n| Coefficient | Estimate | Std. Error |\n| :--- | :--- | :--- |\n| \\(\\hat{\\delta}_{1,8}\\) | 0.0357 | (0.0237) |\n| \\(\\hat{\\delta}_{2,8}\\) | 1.208 | (0.475) |\n\n## Question\n\nConsider two industries. The 'Utility' industry has experienced a string of positive shocks totaling 20% over the prior two years (\\(\\sum_{k=0}^{7} \\eta_{U,t-k} = 0.20\\)). The 'Tech' industry has experienced volatile but cumulatively zero excess returns over the same period (\\(\\sum_{k=0}^{7} \\eta_{T,t-k} = 0\\)). Both industries now experience an identical positive 10% excess return shock (\\(\\eta_{j,t}=0.10\\)).\n\nUsing the model and estimates provided, what is the predicted 2-year excess employment growth for the **Utility** industry?", "model_evaluation_result": {"model_raw_response": "I need to calculate the predicted 2-year excess employment growth for the Utility industry using the given model and estimates.\n\nGiven information:\n- Utility industry: $\\sum_{k=0}^{7} \\eta_{U,t-k} = 0.20$\n- Current shock for Utility: $\\eta_{U,t} = 0.10$\n- Estimates: $\\hat{\\delta}_{1,8} = 0.0357$, $\\hat{\\delta}_{2,8} = 1.208$\n\nUsing the model:\n$$\\hat{\\xi}_{j,t}^{q} = \\tau_{q} + [\\delta_{1q} + \\delta_{2q} |\\sum_{k=0}^{7} \\eta_{j,t-k}|] \\eta_{j,t} + \\nu_{j,t}^{q}$$\n\nFor the Utility industry with q=8:\n$$\\hat{\\xi}_{U,t}^{8} = \\tau_{8} + [\\delta_{1,8} + \\delta_{2,8} |\\sum_{k=0}^{7} \\eta_{U,t-k}|] \\eta_{U,t}$$\n\nSubstituting the values:\n- $|\\sum_{k=0}^{7} \\eta_{U,t-k}| = |0.20| = 0.20$\n- $\\eta_{U,t} = 0.10$\n- $\\delta_{1,8} = 0.0357$\n- $\\delta_{2,8} = 1.208$\n\nThe coefficient on the current return is:\n$\\delta_{1,8} + \\delta_{2,8} \\times 0.20 = 0.0357 + 1.208 \\times 0.20 = 0.0357 + 0.2416 = 0.2773$\n\nTherefore, the predicted excess employment growth is:\n$\\hat{\\xi}_{U,t}^{8} = 0.2773 \\times 0.10 = 0.02773 = 2.773\\%$", "model_answer": "I need to calculate the predicted 2-year excess employment growth for the Utility industry using the given model and estimates.\n\nGiven information:\n- Utility industry: $\\sum_{k=0}^{7} \\eta_{U,t-k} = 0.20$\n- Current shock for Utility: $\\eta_{U,t} = 0.10$\n- Estimates: $\\hat{\\delta}_{1,8} = 0.0357$, $\\hat{\\delta}_{2,8} = 1.208$\n\nUsing the model:\n$$\\hat{\\xi}_{j,t}^{q} = \\tau_{q} + [\\delta_{1q} + \\delta_{2q} |\\sum_{k=0}^{7} \\eta_{j,t-k}|] \\eta_{j,t} + \\nu_{j,t}^{q}$$\n\nFor the Utility industry with q=8:\n$$\\hat{\\xi}_{U,t}^{8} = \\tau_{8} + [\\delta_{1,8} + \\delta_{2,8} |\\sum_{k=0}^{7} \\eta_{U,t-k}|] \\eta_{U,t}$$\n\nSubstituting the values:\n- $|\\sum_{k=0}^{7} \\eta_{U,t-k}| = |0.20| = 0.20$\n- $\\eta_{U,t} = 0.10$\n- $\\delta_{1,8} = 0.0357$\n- $\\delta_{2,8} = 1.208$\n\nThe coefficient on the current return is:\n$\\delta_{1,8} + \\delta_{2,8} \\times 0.20 = 0.0357 + 1.208 \\times 0.20 = 0.0357 + 0.2416 = 0.2773$\n\nTherefore, the predicted excess employment growth is:\n$\\hat{\\xi}_{U,t}^{8} = 0.2773 \\times 0.10 = 0.02773 = 2.773\\%$", "error": "Warning: Could not extract single choice letter from: I need to calculate the predicted 2-year excess em..."}}
{"ID": 320, "Question": "### Background\n\nThe paper contrasts two models of earnings determination. The true data generating process is assumed to be a random coefficients model where returns to skills can differ by geographic labor market. Individual earnings `yᵢⱼ` are determined by individual attributes `Xᵢ` with market-specific returns `βᵢⱼ`:\n```latex\ny_{ij} = X_{i}β_{ij} + ε_{i} \\quad \\text{(Eq. (1))}\n```\nThe expected returns in a market `j`, `β̄ⱼ`, are a function of observable market characteristics `Lⱼ`:\n```latex\nE(β_{ij}) = β̄_{j} = L_{j}Λ + ω_{j} \\quad \\text{(Eq. (2))}\n```\nAn alternative, misspecified “gross standardization” model is proposed, which assumes constant returns `β̄` and adds market characteristics `Lⱼ` directly to the earnings equation:\n```latex\ny_{ij} = X_{i}β̄ + L_{j}Λ^{*} + ν_{i} \\quad \\text{(Eq. (3))}\n```\n\n### Question\n\nAccording to the framework presented, the “gross standardization” model in Eq. (3) is misspecified because it omits a key term present in the true data generating process implied by Eq. (1) and Eq. (2). What is the nature of this omitted term, and under what condition does its omission become a more severe problem?", "Options": {"A": "An interaction term between individual attributes and market characteristics (`XᵢLⱼ`). The problem is more severe when the variance of market characteristics (`Var(Lⱼ)`) is large.", "B": "A squared term for individual attributes (`Xᵢ²`) to capture non-linear returns. The problem is more severe when returns to experience are highly non-linear.", "C": "An interaction term between individual attributes and market characteristics (`XᵢLⱼ`). The problem is more severe when the variance of the returns to those attributes across markets (`Var(β̄ⱼ)`) is large.", "D": "A market-level fixed effect. The problem is more severe when there are large unobserved differences in productivity across markets."}, "Answer": "C", "pi_justification": "This item assesses the student's ability to identify the specific form of misspecification (an omitted interaction term) in a random coefficients context. It uses a Reverse-Reasoning strategy, asking for the cause of the model's failure. Distractor A correctly identifies the term but provides an incorrect condition for the severity of the bias (Almost Right). Distractors B and D point to other plausible but incorrect forms of misspecification (True but Irrelevant; Conceptual Opposite).", "qid": "320", "question": "### Background\n\nThe paper contrasts two models of earnings determination. The true data generating process is assumed to be a random coefficients model where returns to skills can differ by geographic labor market. Individual earnings `yᵢⱼ` are determined by individual attributes `Xᵢ` with market-specific returns `βᵢⱼ`:\n```latex\ny_{ij} = X_{i}β_{ij} + ε_{i} \\quad \\text{(Eq. (1))}\n```\nThe expected returns in a market `j`, `β̄ⱼ`, are a function of observable market characteristics `Lⱼ`:\n```latex\nE(β_{ij}) = β̄_{j} = L_{j}Λ + ω_{j} \\quad \\text{(Eq. (2))}\n```\nAn alternative, misspecified “gross standardization” model is proposed, which assumes constant returns `β̄` and adds market characteristics `Lⱼ` directly to the earnings equation:\n```latex\ny_{ij} = X_{i}β̄ + L_{j}Λ^{*} + ν_{i} \\quad \\text{(Eq. (3))}\n```\n\n### Question\n\nAccording to the framework presented, the “gross standardization” model in Eq. (3) is misspecified because it omits a key term present in the true data generating process implied by Eq. (1) and Eq. (2). What is the nature of this omitted term, and under what condition does its omission become a more severe problem?", "model_evaluation_result": {"model_raw_response": "I need to analyze the difference between the true data generating process (Equations 1 and 2) and the misspecified \"gross standardization\" model (Equation 3).\n\nLet me first derive what the true model implies when we substitute Equation 2 into Equation 1:\n\nFrom Eq. (1): y_{ij} = X_i β_{ij} + ε_i\nFrom Eq. (2): E(β_{ij}) = β̄_j = L_j Λ + ω_j\n\nSo β_{ij} = L_j Λ + ω_j + (β_{ij} - β̄_j)\n\nSubstituting this into Eq. (1):\ny_{ij} = X_i(L_j Λ + ω_j + (β_{ij} - β̄_j)) + ε_i\ny_{ij} = X_i L_j Λ + X_i ω_j + X_i(β_{ij} - β̄_j) + ε_i\n\nThis can be rewritten as:\ny_{ij} = X_i L_j Λ + (X_i ω_j + X_i(β_{ij} - β̄_j) + ε_i)\n\nThe true model contains the interaction term X_i L_j Λ, which represents how individual attributes X_i interact with market characteristics L_j.\n\nNow comparing with the misspecified model in Eq. (3):\ny_{ij} = X_i β̄ + L_j Λ* + ν_i\n\nThe key difference is that the true model has X_i L_j (interaction terms) while the misspecified model only has X_i and L_j as separate additive terms.\n\nThe omitted term is the interaction between individual attributes and market characteristics: X_i L_j.\n\nWhen would this omission be more severe? The problem becomes worse when there's more variation in how markets reward individual attributes - that is, when Var(β̄_j) is large. This represents the variance in expected returns across markets, which directly affects the magnitude of the interaction effects", "model_answer": "I need to analyze the difference between the true data generating process (Equations 1 and 2) and the misspecified \"gross standardization\" model (Equation 3).\n\nLet me first derive what the true model implies when we substitute Equation 2 into Equation 1:\n\nFrom Eq. (1): y_{ij} = X_i β_{ij} + ε_i\nFrom Eq. (2): E(β_{ij}) = β̄_j = L_j Λ + ω_j\n\nSo β_{ij} = L_j Λ + ω_j + (β_{ij} - β̄_j)\n\nSubstituting this into Eq. (1):\ny_{ij} = X_i(L_j Λ + ω_j + (β_{ij} - β̄_j)) + ε_i\ny_{ij} = X_i L_j Λ + X_i ω_j + X_i(β_{ij} - β̄_j) + ε_i\n\nThis can be rewritten as:\ny_{ij} = X_i L_j Λ + (X_i ω_j + X_i(β_{ij} - β̄_j) + ε_i)\n\nThe true model contains the interaction term X_i L_j Λ, which represents how individual attributes X_i interact with market characteristics L_j.\n\nNow comparing with the misspecified model in Eq. (3):\ny_{ij} = X_i β̄ + L_j Λ* + ν_i\n\nThe key difference is that the true model has X_i L_j (interaction terms) while the misspecified model only has X_i and L_j as separate additive terms.\n\nThe omitted term is the interaction between individual attributes and market characteristics: X_i L_j.\n\nWhen would this omission be more severe? The problem becomes worse when there's more variation in how markets reward individual attributes - that is, when Var(β̄_j) is large. This represents the variance in expected returns across markets, which directly affects the magnitude of the interaction effects", "error": "Warning: Could not extract single choice letter from: I need to analyze the difference between the true ..."}}
{"ID": 326, "Question": "### Background\n\n**Research Question.** This problem involves the application of the bootstrap threshold test to a real-world climate science dataset and the interpretation of the resulting nonlinear model.\n\n**Setting.** The monthly time series of the Greenland ice sheet mass balance is analyzed. After testing for nonlinearity, a Threshold Autoregressive Moving Average (TARMA) model is fitted to capture the dynamics.\n\n### Data / Model Specification\n\nTable 1 presents the results of the supLM tests for a threshold effect in the detrended time series, using an AR order of 13 (selected by AIC) and varying the delay parameter `d`.\n\n**Table 1: sLM Test Results for Greenland Ice Sheet Mass Balance (AR order=13)**\n\n| d   | sLMa (statistic) | sLMi (p-value) | sLMw (p-value) | sLMh (p-value) |\n| :-- | :--------------- | :------------- | :------------- | :------------- |\n| 1   | 33.5             | 0.042          | 0.102          | 0.236          |\n| 3   | 48.9             | 0.000          | 0.001          | 0.026          |\n| 6   | 62.5             | 0.000          | 0.000          | 0.001          |\n\nBased on the test results, a TARMA model was estimated with `d=6`:\n\n```latex\nX_t = 0.46 X_{t-1} + 0.75 \\varepsilon_{t-1} + \\text{RegimeComponent}_t + \\varepsilon_t\n```\nwhere:\n- If `X_{t-6} \\leq -5.86` (Lower Regime): `\\text{RegimeComponent}_t = 56.39 + 0.21 X_{t-6} + 0.30 X_{t-11}`\n- If `X_{t-6} > -5.86` (Upper Regime): `\\text{RegimeComponent}_t = -24.14 + 0.21 X_{t-12} - ...`\n\nBased on the test results in Table 1 (for d=6) and the estimated model, what is the correct statistical conclusion at the 1% significance level and the corresponding climatological interpretation of the model's regimes?", "Options": {"A": "Reject the null hypothesis of linearity; the Lower Regime (positive intercept) represents winter accumulation and the Upper Regime (negative intercept) represents summer melt.", "B": "Reject the null hypothesis of linearity; the Lower Regime (positive intercept) represents summer melt and the Upper Regime (negative intercept) represents winter accumulation.", "C": "Fail to reject the null hypothesis of linearity; the Lower Regime (positive intercept) represents winter accumulation and the Upper Regime (negative intercept) represents summer melt.", "D": "Fail to reject the null hypothesis of linearity; the Lower Regime (positive intercept) represents summer melt and the Upper Regime (negative intercept) represents winter accumulation."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to perform a standard hypothesis test using a p-value and then interpret the parameters of the resulting model in a real-world context. It uses a **Computational Judgment** strategy. The distractors are designed to isolate specific errors: (B) misinterprets the model parameters (conceptual opposite), (C) misinterprets the p-value (classic statistical error), and (D) combines both errors.", "qid": "326", "question": "### Background\n\n**Research Question.** This problem involves the application of the bootstrap threshold test to a real-world climate science dataset and the interpretation of the resulting nonlinear model.\n\n**Setting.** The monthly time series of the Greenland ice sheet mass balance is analyzed. After testing for nonlinearity, a Threshold Autoregressive Moving Average (TARMA) model is fitted to capture the dynamics.\n\n### Data / Model Specification\n\nTable 1 presents the results of the supLM tests for a threshold effect in the detrended time series, using an AR order of 13 (selected by AIC) and varying the delay parameter `d`.\n\n**Table 1: sLM Test Results for Greenland Ice Sheet Mass Balance (AR order=13)**\n\n| d   | sLMa (statistic) | sLMi (p-value) | sLMw (p-value) | sLMh (p-value) |\n| :-- | :--------------- | :------------- | :------------- | :------------- |\n| 1   | 33.5             | 0.042          | 0.102          | 0.236          |\n| 3   | 48.9             | 0.000          | 0.001          | 0.026          |\n| 6   | 62.5             | 0.000          | 0.000          | 0.001          |\n\nBased on the test results, a TARMA model was estimated with `d=6`:\n\n```latex\nX_t = 0.46 X_{t-1} + 0.75 \\varepsilon_{t-1} + \\text{RegimeComponent}_t + \\varepsilon_t\n```\nwhere:\n- If `X_{t-6} \\leq -5.86` (Lower Regime): `\\text{RegimeComponent}_t = 56.39 + 0.21 X_{t-6} + 0.30 X_{t-11}`\n- If `X_{t-6} > -5.86` (Upper Regime): `\\text{RegimeComponent}_t = -24.14 + 0.21 X_{t-12} - ...`\n\nBased on the test results in Table 1 (for d=6) and the estimated model, what is the correct statistical conclusion at the 1% significance level and the corresponding climatological interpretation of the model's regimes?", "model_evaluation_result": {"model_raw_response": "Looking at the test results in Table 1 for d=6:\n\n**Statistical Analysis:**\n- sLMa statistic = 62.5\n- sLMi p-value = 0.000\n- sLMw p-value = 0.000  \n- sLMh p-value = 0.001\n\nAll p-values are well below the 1% significance level (0.01), so we **reject the null hypothesis of linearity**.\n\n**Climatological Interpretation:**\nFrom the estimated TARMA model:\n- Lower Regime (X_{t-6} ≤ -5.86): RegimeComponent_t = **56.39** + 0.21 X_{t-6} + 0.30 X_{t-11}\n- Upper Regime (X_{t-6} > -5.86): RegimeComponent_t = **-24.14** + 0.21 X_{t-12} - ...\n\nThe Lower Regime has a **positive intercept (56.39)**, indicating mass gain/accumulation, which corresponds to **winter accumulation** when snow and ice build up on the Greenland ice sheet.\n\nThe Upper Regime has a **negative intercept (-24.14)**, indicating mass loss, which corresponds to **summer melt** when higher temperatures cause ice loss.\n\nThe threshold value of -5.86 suggests the system switches between these regimes based on lagged conditions, capturing the seasonal dynamics of ice sheet mass balance.\n\nA", "model_answer": "A", "error": null}}
{"ID": 424, "Question": "### Background\n\nThe paper tests its theory of CEO pay using a panel dataset of firms over time. The empirical analysis compares a simple OLS regression (Column 1) with a model that includes firm fixed effects (Column 4).\n\n### Data / Model Specification\n\nThe empirical model is:\n\n```latex\n\\ln(w_{i,t+1}) = d + e \\cdot \\ln(S_{n_*,t}) + f \\cdot \\ln(S_{i,t}) + \\text{Controls} + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1: Panel Evidence: CEO Pay, Own Firm Size, and Reference Firm Size**\n\n| | (1) OLS | (4) Firm FE |\n|:---|:---:|:---:|\n| ln(Market cap) (`f`) | 0.37 | 0.26 |\n| ln(Market cap of firm #250) (`e`) | 0.72 | 0.78 |\n| Firm fixed effects | No | Yes |\n| R² | 0.23 | 0.60 |\n\n### Question\n\nWhat is the primary econometric justification for preferring the firm fixed-effects model (Column 4) over the OLS model (Column 1) for estimating the effect of a firm's own size on its CEO's pay?", "Options": {"A": "It controls for time-invariant unobserved firm characteristics, such as corporate culture or brand value, that could be correlated with both firm size and CEO pay.", "B": "It controls for time-varying aggregate shocks, such as business cycles or changes in investor sentiment, that affect all firms in a given year.", "C": "It solves the problem of reverse causality, where higher CEO pay might lead to an increase in firm size through better performance.", "D": "It corrects for measurement error in the firm size variable, which typically biases OLS estimates toward zero."}, "Answer": "A", "pi_justification": "This item assesses understanding of a fundamental identification strategy in panel data econometrics: the use of fixed effects. The correct option (A) accurately describes the role of firm fixed effects in mitigating omitted variable bias from stable, unobserved firm-level factors. Distractor (B) describes the role of year fixed effects, not firm fixed effects. Distractor (C) is a plausible concern (endogeneity), but fixed effects do not fully solve reverse causality. Distractor (D) is incorrect; fixed-effects estimation can actually worsen attenuation bias from measurement error.", "qid": "424", "question": "### Background\n\nThe paper tests its theory of CEO pay using a panel dataset of firms over time. The empirical analysis compares a simple OLS regression (Column 1) with a model that includes firm fixed effects (Column 4).\n\n### Data / Model Specification\n\nThe empirical model is:\n\n```latex\n\\ln(w_{i,t+1}) = d + e \\cdot \\ln(S_{n_*,t}) + f \\cdot \\ln(S_{i,t}) + \\text{Controls} + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n```\n\n**Table 1: Panel Evidence: CEO Pay, Own Firm Size, and Reference Firm Size**\n\n| | (1) OLS | (4) Firm FE |\n|:---|:---:|:---:|\n| ln(Market cap) (`f`) | 0.37 | 0.26 |\n| ln(Market cap of firm #250) (`e`) | 0.72 | 0.78 |\n| Firm fixed effects | No | Yes |\n| R² | 0.23 | 0.60 |\n\n### Question\n\nWhat is the primary econometric justification for preferring the firm fixed-effects model (Column 4) over the OLS model (Column 1) for estimating the effect of a firm's own size on its CEO's pay?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 337, "Question": "### Background\n\nAn infinite-horizon model features a logarithmic utility function, `u(c) = ln(c)`, and a strictly convex production function, `f(x) = x^2`, representing perpetual increasing returns to scale.\n\n### Data / Model Specification\n\nFor discount factors `\\delta < 1/2`, the optimal consumption policy is `g(x) = (1-2\\delta)x`.\n\n### Question\n\nGiven the optimal policy, what are the dynamic properties of the non-zero steady-state capital stock `\\bar{x}`?", "Options": {"A": "The steady state is `\\bar{x} = 1/(4\\delta^2)` and it is unstable.", "B": "The steady state is `\\bar{x} = 1/(4\\delta^2)` and it is stable.", "C": "The steady state is `\\bar{x} = 1/(2\\delta)` and it is unstable.", "D": "The steady state is `\\bar{x} = 1/(2\\delta)` and it is stable."}, "Answer": "A", "pi_justification": "This item assesses the ability to analyze the dynamics of the system given the policy function. It uses the Computational Judgment strategy, requiring a two-step calculation: first, find the steady state `\\bar{x}`, and second, evaluate the derivative of the law of motion at `\\bar{x}` to determine stability. The distractor design matrix covers both possible errors: Distractor B has the correct steady state but incorrect stability judgment (a common conceptual error). Distractor C has an incorrect steady state (from an algebraic slip) but the correct stability judgment. Distractor D is incorrect on both counts.", "qid": "337", "question": "### Background\n\nAn infinite-horizon model features a logarithmic utility function, `u(c) = ln(c)`, and a strictly convex production function, `f(x) = x^2`, representing perpetual increasing returns to scale.\n\n### Data / Model Specification\n\nFor discount factors `\\delta < 1/2`, the optimal consumption policy is `g(x) = (1-2\\delta)x`.\n\n### Question\n\nGiven the optimal policy, what are the dynamic properties of the non-zero steady-state capital stock `\\bar{x}`?", "model_evaluation_result": {"model_raw_response": "I need to analyze the dynamic properties of the steady-state capital stock in this infinite-horizon model.\n\nGiven information:\n- Utility function: u(c) = ln(c)\n- Production function: f(x) = x²\n- Optimal consumption policy: g(x) = (1-2δ)x\n- Discount factor: δ < 1/2\n\nFirst, I need to find the steady-state capital stock. In steady state, the capital stock remains constant, so:\nx_{t+1} = x_t = x̄\n\nThe capital accumulation equation is:\nx_{t+1} = f(x_t) - c_t = x_t² - g(x_t)\n\nSubstituting the optimal consumption policy:\nx_{t+1} = x_t² - (1-2δ)x_t\n\nIn steady state:\nx̄ = x̄² - (1-2δ)x̄\n\nRearranging:\nx̄ = x̄² - (1-2δ)x̄\n0 = x̄² - (1-2δ)x̄ - x̄\n0 = x̄² - x̄ - (1-2δ)x̄\n0 = x̄² - x̄[1 + (1-2δ)]\n0 = x̄² - x̄(2-2δ)\n0 = x̄[x̄ - (2-2δ)]\n\nSo either x̄ = 0 or x̄ = 2-2δ = 2(1-δ)\n\nSince we want the non-zero steady state: x̄ = 2(1-δ)\n\nWait, let me recalculate this more carefully. The capital accumulation is:\nx_{t+1} = f(x_t) - c_t = x_t² - (1-2δ)x_t\n\nIn steady state:\nx̄ = x̄² - (1-2δ)x̄\nx̄ + (1-2δ)x̄", "model_answer": "I need to analyze the dynamic properties of the steady-state capital stock in this infinite-horizon model.\n\nGiven information:\n- Utility function: u(c) = ln(c)\n- Production function: f(x) = x²\n- Optimal consumption policy: g(x) = (1-2δ)x\n- Discount factor: δ < 1/2\n\nFirst, I need to find the steady-state capital stock. In steady state, the capital stock remains constant, so:\nx_{t+1} = x_t = x̄\n\nThe capital accumulation equation is:\nx_{t+1} = f(x_t) - c_t = x_t² - g(x_t)\n\nSubstituting the optimal consumption policy:\nx_{t+1} = x_t² - (1-2δ)x_t\n\nIn steady state:\nx̄ = x̄² - (1-2δ)x̄\n\nRearranging:\nx̄ = x̄² - (1-2δ)x̄\n0 = x̄² - (1-2δ)x̄ - x̄\n0 = x̄² - x̄ - (1-2δ)x̄\n0 = x̄² - x̄[1 + (1-2δ)]\n0 = x̄² - x̄(2-2δ)\n0 = x̄[x̄ - (2-2δ)]\n\nSo either x̄ = 0 or x̄ = 2-2δ = 2(1-δ)\n\nSince we want the non-zero steady state: x̄ = 2(1-δ)\n\nWait, let me recalculate this more carefully. The capital accumulation is:\nx_{t+1} = f(x_t) - c_t = x_t² - (1-2δ)x_t\n\nIn steady state:\nx̄ = x̄² - (1-2δ)x̄\nx̄ + (1-2δ)x̄", "error": "Warning: Could not extract single choice letter from: I need to analyze the dynamic properties of the st..."}}
{"ID": 333, "Question": "### Background\n\nAn auction is held with two local bidders (1 and 2) and one global bidder (3) for two items (A and B). Bidder 1 values only item A at `$v_1$`, bidder 2 values only item B at `$v_2$`, and bidder 3 values only the package AB at `$V$`. The allocation is based on bids: the global bidder wins if `$B > b_1+b_2$`, otherwise the local bidders win their respective items.\n\n### Data / Model Specification\n\nIn the BCV (Bidder-optimal Core-selecting with Vickrey-minimizing) auction, the payment rule for a local bidder `$i$` (when local bidders win) is:\n\n```latex\n\\hat{h}_{i}(b) = \\operatorname*{max}(0,B-b_{-i}) + \\frac{1}{2}(B - \\operatorname*{max}(0,B-b_{i}) - \\operatorname*{max}(0,B-b_{-i})) \\quad \\text{(Eq. 1)}\n```\n\nIn a standard Vickrey auction, a bidder's payment is independent of their own bid. By contrast, the BCV payment rule in Eq. (1) creates a strategic incentive to 'shade' bids below true values. Which component of the formula is directly responsible for this incentive?", "Options": {"A": "`$\\operatorname*{max}(0,B-b_{-i})$`", "B": "`$-\\frac{1}{2}\\operatorname*{max}(0,B-b_{i})$`", "C": "`$\\frac{1}{2}B$`", "D": "The allocation rule condition `$b_1+b_2 \\ge B$`"}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to analyze a formal mathematical expression and connect it to economic incentives. It uses a direct identification strategy. Distractor A is the Vickrey-like part of the payment, a plausible but incorrect source of the *deviation* from Vickrey incentives. Distractor C isolates a term that does not depend on the bidder's own bid `$b_i$`. Distractor D is true but irrelevant, confusing the condition for the rule with the rule's content.", "qid": "333", "question": "### Background\n\nAn auction is held with two local bidders (1 and 2) and one global bidder (3) for two items (A and B). Bidder 1 values only item A at `$v_1$`, bidder 2 values only item B at `$v_2$`, and bidder 3 values only the package AB at `$V$`. The allocation is based on bids: the global bidder wins if `$B > b_1+b_2$`, otherwise the local bidders win their respective items.\n\n### Data / Model Specification\n\nIn the BCV (Bidder-optimal Core-selecting with Vickrey-minimizing) auction, the payment rule for a local bidder `$i$` (when local bidders win) is:\n\n```latex\n\\hat{h}_{i}(b) = \\operatorname*{max}(0,B-b_{-i}) + \\frac{1}{2}(B - \\operatorname*{max}(0,B-b_{i}) - \\operatorname*{max}(0,B-b_{-i})) \\quad \\text{(Eq. 1)}\n```\n\nIn a standard Vickrey auction, a bidder's payment is independent of their own bid. By contrast, the BCV payment rule in Eq. (1) creates a strategic incentive to 'shade' bids below true values. Which component of the formula is directly responsible for this incentive?", "model_evaluation_result": {"model_raw_response": "I need to analyze the BCV payment formula to identify which component creates the incentive for bid shading.\n\nLet me break down the payment formula for local bidder i:\n$$\\hat{h}_{i}(b) = \\max(0,B-b_{-i}) + \\frac{1}{2}(B - \\max(0,B-b_{i}) - \\max(0,B-b_{-i}))$$\n\nThis can be rewritten as:\n$$\\hat{h}_{i}(b) = \\max(0,B-b_{-i}) + \\frac{1}{2}B - \\frac{1}{2}\\max(0,B-b_{i}) - \\frac{1}{2}\\max(0,B-b_{-i})$$\n\nSimplifying:\n$$\\hat{h}_{i}(b) = \\frac{1}{2}\\max(0,B-b_{-i}) + \\frac{1}{2}B - \\frac{1}{2}\\max(0,B-b_{i})$$\n\nNow I need to identify what creates bid shading incentives. In a Vickrey auction, payments are independent of one's own bid, which eliminates strategic incentives. Here, I need to find which term depends on bidder i's own bid $b_i$.\n\nLooking at each component:\n- $\\max(0,B-b_{-i})$: Depends only on the other bidder's bid, not on $b_i$\n- $\\frac{1}{2}B$: Constant with respect to $b_i$\n- $-\\frac{1}{2}\\max(0,B-b_i)$: This term directly depends on bidder i's own bid $b_i$\n\nThe term $-\\frac{1}{2}\\max(0,B-b_i)$ creates the strategic incentive because:\n1. When $B > b_i$, this term equals $-\\frac{1}{2}(B-b_i)$\n2. As bidder i increases their bid $b_i$, this negative term becomes less negative (closer to zero)\n3. This means", "model_answer": "B", "error": null}}
{"ID": 357, "Question": "### Background\n\n**Research Question.** The nutrition-based efficiency wage model (EWM) posits that employers pay wages above the market-clearing rate to ensure workers are well-nourished and productive, making it unprofitable to hire unemployed workers who offer to work for less. This paper empirically tests this core premise: is \"cheap labor\" truly \"dear labor\"?\n\n### Data / Model Specification\n\nThe central premise of the EWM is that an unemployed worker's offer will be rejected if the following condition holds:\n\n```latex\n\\frac{M}{E(C)} \\geq \\frac{W}{E(C^*)} \\quad \\text{(Eq. (1))}\n```\nThis inequality states that the cost per efficiency unit for a worker paid in meals (cost `M`, calories `C`) is greater than or equal to that of a currently employed worker (wage `W`, calories consumed `C*`). `E(·)` is the function mapping calorie consumption to worker productivity.\n\n**Case: Randam Village, Tamil Nadu, 1974**\n\n**Table 1: Daily wage rates of adult male casual labor**\n| Operation             | Wage rate                                    |\n| :-------------------- | :------------------------------------------- |\n| Plucking seedlings    | 2 rupees + meal + porridge made of rice and ragi |\n| Coolie (well-digging) | 2 rupees + meal + porridge made of rice and ragi |\n| First threshing       | 2.725-4.09 rupees                            |\n| Second threshing      | 8.18 rupees                                  |\n\n*   **Productivity Gain:** A worker consuming 5000 calories is **34.58%** more productive than one consuming 2601 calories.\n\n### Question\n\nConsider the \"Plucking seedlings\" operation from Table 1. An unemployed worker offers to work for a meal costing `M = 1` rupee, which provides `C = 2601` calories. The currently employed worker receives a total wage `W` equivalent to 3 rupees and is assumed to consume `C* = 5000` calories. Given this, which of the following statements correctly evaluates the EWM's condition for wage rigidity (Eq. (1))?", "Options": {"A": "The condition is not met because the cost per efficiency unit is lower for the undercutting worker (~1.00) than for the employed worker (~2.23), indicating that hiring the unemployed worker would be more profitable.", "B": "The condition is met because the cost per efficiency unit is higher for the undercutting worker (~1.00) than for the employed worker (~0.74), indicating that hiring the unemployed worker would be less profitable.", "C": "The condition is not met, but this implies the employer should keep the current, more productive worker because the cost difference is minimal.", "D": "The costs per efficiency unit are equal for both workers, making the employer indifferent and thus upholding wage rigidity."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to apply the core EWM inequality to a concrete scenario. It uses a Computational Judgment strategy. The student must perform a multi-step calculation (determine productivity levels, then costs per efficiency unit) and interpret the result in the context of the model's prediction. Distractor B represents a calculation error (e.g., inverting the cost formula) leading to the opposite conclusion. Distractor C represents a correct calculation but a flawed interpretation of what the inequality implies. Distractor D represents a different calculation error.", "qid": "357", "question": "### Background\n\n**Research Question.** The nutrition-based efficiency wage model (EWM) posits that employers pay wages above the market-clearing rate to ensure workers are well-nourished and productive, making it unprofitable to hire unemployed workers who offer to work for less. This paper empirically tests this core premise: is \"cheap labor\" truly \"dear labor\"?\n\n### Data / Model Specification\n\nThe central premise of the EWM is that an unemployed worker's offer will be rejected if the following condition holds:\n\n```latex\n\\frac{M}{E(C)} \\geq \\frac{W}{E(C^*)} \\quad \\text{(Eq. (1))}\n```\nThis inequality states that the cost per efficiency unit for a worker paid in meals (cost `M`, calories `C`) is greater than or equal to that of a currently employed worker (wage `W`, calories consumed `C*`). `E(·)` is the function mapping calorie consumption to worker productivity.\n\n**Case: Randam Village, Tamil Nadu, 1974**\n\n**Table 1: Daily wage rates of adult male casual labor**\n| Operation             | Wage rate                                    |\n| :-------------------- | :------------------------------------------- |\n| Plucking seedlings    | 2 rupees + meal + porridge made of rice and ragi |\n| Coolie (well-digging) | 2 rupees + meal + porridge made of rice and ragi |\n| First threshing       | 2.725-4.09 rupees                            |\n| Second threshing      | 8.18 rupees                                  |\n\n*   **Productivity Gain:** A worker consuming 5000 calories is **34.58%** more productive than one consuming 2601 calories.\n\n### Question\n\nConsider the \"Plucking seedlings\" operation from Table 1. An unemployed worker offers to work for a meal costing `M = 1` rupee, which provides `C = 2601` calories. The currently employed worker receives a total wage `W` equivalent to 3 rupees and is assumed to consume `C* = 5000` calories. Given this, which of the following statements correctly evaluates the EWM's condition for wage rigidity (Eq. (1))?", "model_evaluation_result": {"model_raw_response": "I need to evaluate the EWM condition from Equation (1): M/E(C) ≥ W/E(C*)\n\nGiven information:\n- Unemployed worker: M = 1 rupee, C = 2601 calories\n- Employed worker: W = 3 rupees, C* = 5000 calories\n- A worker consuming 5000 calories is 34.58% more productive than one consuming 2601 calories\n\nFirst, I need to establish the productivity relationship. If we set E(2601) = 1 as the baseline, then:\nE(5000) = 1 × (1 + 0.3458) = 1.3458\n\nNow I can calculate the cost per efficiency unit for each worker:\n\nFor the unemployed worker:\nM/E(C) = 1/E(2601) = 1/1 = 1.00\n\nFor the employed worker:\nW/E(C*) = 3/E(5000) = 3/1.3458 ≈ 2.23\n\nChecking the condition M/E(C) ≥ W/E(C*):\n1.00 ≥ 2.23?\n\nThis is FALSE (1.00 < 2.23), so the condition is NOT met.\n\nSince the cost per efficiency unit is lower for the undercutting worker (1.00) than for the employed worker (2.23), hiring the unemployed worker would indeed be more profitable from a cost-efficiency perspective.\n\nA", "model_answer": "C", "error": null}}
{"ID": 339, "Question": "### Background\n\nAnalysis of Econometric Society (ES) Fellow elections reveals a strong \"endorsement effect\": after controlling for qualifications, candidates nominated by the Nominating Committee are far more likely to be elected. This effect is captured by the coefficient on a `Committee Nominee` indicator in a logit model of election probability.\n\n### Data / Model Specification\n\nThe probability of election for a nominee with characteristics `X` is given by a logit model. The log-odds of election for a female Committee nominee are determined by:\n\n`log-odds = β_0 + β_F * Female + β_C * CommitteeNominee + X'γ`\n\nKey coefficients for the 2012-2019 period are:\n*   `β_F` (Female): 0.639\n*   `β_C` (Committee Nominee): 2.723\n\nAssume the baseline probability of election for a male non-Committee nominee with average qualifications `X` is `P_0 = Λ(β_0 + X'γ) = 0.10`.\n\n### Question\n\nSuppose the ES removes the identifier for Committee nominees on the ballot, causing the endorsement effect to disappear (i.e., the coefficient `β_C` becomes zero). Given the baseline probability, by approximately how much would the election probability for a female Committee nominee with the same average qualifications change?", "Options": {"A": "Decrease by 59 percentage points", "B": "Decrease by 76 percentage points", "C": "Decrease by 17 percentage points", "D": "Increase by 59 percentage points"}, "Answer": "A", "pi_justification": "Assessment Target: Applying the logit model to perform a counterfactual calculation. Strategy: Computational Judgment. Distractor Logic: The correct answer requires calculating the baseline log-odds from the 10% probability (which is -2.197), calculating the 'status quo' probability (76.2%), calculating the 'counterfactual' probability (17.4%), and finding the difference (-58.8pp). (B) is the original probability, not the change. (C) is the counterfactual probability, not the change. (D) is a sign error.", "qid": "339", "question": "### Background\n\nAnalysis of Econometric Society (ES) Fellow elections reveals a strong \"endorsement effect\": after controlling for qualifications, candidates nominated by the Nominating Committee are far more likely to be elected. This effect is captured by the coefficient on a `Committee Nominee` indicator in a logit model of election probability.\n\n### Data / Model Specification\n\nThe probability of election for a nominee with characteristics `X` is given by a logit model. The log-odds of election for a female Committee nominee are determined by:\n\n`log-odds = β_0 + β_F * Female + β_C * CommitteeNominee + X'γ`\n\nKey coefficients for the 2012-2019 period are:\n*   `β_F` (Female): 0.639\n*   `β_C` (Committee Nominee): 2.723\n\nAssume the baseline probability of election for a male non-Committee nominee with average qualifications `X` is `P_0 = Λ(β_0 + X'γ) = 0.10`.\n\n### Question\n\nSuppose the ES removes the identifier for Committee nominees on the ballot, causing the endorsement effect to disappear (i.e., the coefficient `β_C` becomes zero). Given the baseline probability, by approximately how much would the election probability for a female Committee nominee with the same average qualifications change?", "model_evaluation_result": {"model_raw_response": "I need to calculate the change in election probability for a female Committee nominee when the Committee endorsement effect disappears.\n\nGiven information:\n- β_F (Female) = 0.639\n- β_C (Committee Nominee) = 2.723\n- Baseline probability P_0 = 0.10 for male non-Committee nominee with average qualifications\n- This means Λ(β_0 + X'γ) = 0.10\n\nStep 1: Find the current probability for a female Committee nominee.\nFor a female Committee nominee:\nlog-odds = β_0 + β_F + β_C + X'γ = β_0 + 0.639 + 2.723 + X'γ = β_0 + X'γ + 3.362\n\nSince Λ(β_0 + X'γ) = 0.10, I can find β_0 + X'γ:\nIf P = 0.10, then log-odds = ln(0.10/0.90) = ln(1/9) ≈ -2.197\nSo β_0 + X'γ = -2.197\n\nTherefore, for a female Committee nominee:\nlog-odds = -2.197 + 3.362 = 1.165\nP_current = Λ(1.165) = e^1.165/(1 + e^1.165) = 3.205/(1 + 3.205) = 3.205/4.205 ≈ 0.762\n\nStep 2: Find the new probability when β_C = 0.\nFor a female non-Committee nominee:\nlog-odds = β_0 + β_F + X'γ = -2.197 + 0.639 = -1.558\nP_new = Λ(-1.558) = e^(-1.558)/(1 + e^(-1.558)) = 0.211/(1 + 0.211) = 0.211/1.211 ≈ 0.174\n\nStep 3: Calculate the change.\nChange = P_new - P_current = 0", "model_answer": "I need to calculate the change in election probability for a female Committee nominee when the Committee endorsement effect disappears.\n\nGiven information:\n- β_F (Female) = 0.639\n- β_C (Committee Nominee) = 2.723\n- Baseline probability P_0 = 0.10 for male non-Committee nominee with average qualifications\n- This means Λ(β_0 + X'γ) = 0.10\n\nStep 1: Find the current probability for a female Committee nominee.\nFor a female Committee nominee:\nlog-odds = β_0 + β_F + β_C + X'γ = β_0 + 0.639 + 2.723 + X'γ = β_0 + X'γ + 3.362\n\nSince Λ(β_0 + X'γ) = 0.10, I can find β_0 + X'γ:\nIf P = 0.10, then log-odds = ln(0.10/0.90) = ln(1/9) ≈ -2.197\nSo β_0 + X'γ = -2.197\n\nTherefore, for a female Committee nominee:\nlog-odds = -2.197 + 3.362 = 1.165\nP_current = Λ(1.165) = e^1.165/(1 + e^1.165) = 3.205/(1 + 3.205) = 3.205/4.205 ≈ 0.762\n\nStep 2: Find the new probability when β_C = 0.\nFor a female non-Committee nominee:\nlog-odds = β_0 + β_F + X'γ = -2.197 + 0.639 = -1.558\nP_new = Λ(-1.558) = e^(-1.558)/(1 + e^(-1.558)) = 0.211/(1 + 0.211) = 0.211/1.211 ≈ 0.174\n\nStep 3: Calculate the change.\nChange = P_new - P_current = 0", "error": "Warning: Could not extract single choice letter from: I need to calculate the change in election probabi..."}}
{"ID": 431, "Question": "### Background\n\n**Research Question.** This problem requires interpreting empirical results from a GMM estimation to distinguish between two competing theories of preference heterogeneity: whether the mean of consumption growth (linked to the Rate of Time Preference, RTP) or the volatility of consumption growth (linked to the Intertemporal Elasticity of Substitution, IES) varies with household wealth.\n\n**Setting / Institutional Environment.** The analysis is based on GMM estimation of a consumption growth model using a panel of Indian households. The model is designed to separately estimate a parameter for wealth-dependent mean growth (`$b_y$`) and a parameter for wealth-dependent volatility (`$\\gamma$`). The GMM estimation uses instruments such as land holding class, which are assumed to be correlated with true permanent income but not with the model's error term.\n\n**Variables & Parameters.**\n- `$\\gamma$`: Subsistence parameter, linked to wealth-varying IES. A positive `$\\gamma$` implies the IES rises with wealth.\n- `$b_y$`: Coefficient on a permanent income proxy, linked to wealth-varying RTP. A non-zero `$b_y$` implies the RTP varies with wealth.\n- `$J$`-statistic: Hansen's test statistic for overidentifying restrictions.\n- `$C$`-statistic: A likelihood ratio-type test statistic for parameter restrictions, calculated as a difference in `$J$`-statistics.\n\n---\n\n### Data / Model Specification\n\nThe estimated statistical model is:\n```latex\n\\ln[C_{h}(t+1) - \\gamma] - \\ln[C_{h}(t) - \\gamma] = \\phi(t) + b_{y} y_{h}^{c} + v_{h}(t) \\quad \\text{(Eq. 1)}\n```\nKey results for total consumption, based on Table 2 in the paper, are summarized below.\n\n**Table 1: GMM Results for Total Consumption**\n| Model Specification | `$\\hat{\\gamma}$` (s.e.) | `$\\hat{b}_y$` (s.e.) | J-statistic (p-value %) | C-statistic (p-value %) | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1. Unrestricted | 177.6 (6.70) | -0.021 (0.049) | 34.44 (18.7) | 0.255 (61.4) | Tests `$b_y=0$` vs Model 2 |\n| 2. Restricted (`$b_y=0$`) | 177.2 (7.45) | 0 (imposed) | 34.70 (21.5) | — | Preferred Specification |\n| 3. Restricted (`$\\gamma=b_y=0$`) | 0 (imposed) | 0 (imposed) | 98.89 (0.0) | 64.428 (0.0) | Tests joint null vs Model 2 |\n\n---\n\n### The Question\n\nThe `$J$`-statistic for the preferred model (Row 2) has a p-value of 21.5%. What is the primary implication of this result for the study's conclusion?", "Options": {"A": "The model's key parameters, `$\\gamma$` and `$\\phi(t)$`, are jointly statistically significant.", "B": "The model's overidentifying restrictions are not rejected, supporting the validity of the instruments and model specification.", "C": "The model has low explanatory power, as a high p-value indicates a poor fit to the data.", "D": "The instruments used in the estimation are weak and do not have sufficient predictive power for the endogenous variables."}, "Answer": "B", "pi_justification": "This item assesses the user's understanding of a critical specification test in GMM, Hansen's J-test. It uses a Reverse-Reasoning strategy (inferring the meaning of a test result). The distractors are classic, high-frequency misconceptions about the J-test, distinguishing it from tests of parameter significance (A), goodness-of-fit (C), and instrument strength (D).", "qid": "431", "question": "### Background\n\n**Research Question.** This problem requires interpreting empirical results from a GMM estimation to distinguish between two competing theories of preference heterogeneity: whether the mean of consumption growth (linked to the Rate of Time Preference, RTP) or the volatility of consumption growth (linked to the Intertemporal Elasticity of Substitution, IES) varies with household wealth.\n\n**Setting / Institutional Environment.** The analysis is based on GMM estimation of a consumption growth model using a panel of Indian households. The model is designed to separately estimate a parameter for wealth-dependent mean growth (`$b_y$`) and a parameter for wealth-dependent volatility (`$\\gamma$`). The GMM estimation uses instruments such as land holding class, which are assumed to be correlated with true permanent income but not with the model's error term.\n\n**Variables & Parameters.**\n- `$\\gamma$`: Subsistence parameter, linked to wealth-varying IES. A positive `$\\gamma$` implies the IES rises with wealth.\n- `$b_y$`: Coefficient on a permanent income proxy, linked to wealth-varying RTP. A non-zero `$b_y$` implies the RTP varies with wealth.\n- `$J$`-statistic: Hansen's test statistic for overidentifying restrictions.\n- `$C$`-statistic: A likelihood ratio-type test statistic for parameter restrictions, calculated as a difference in `$J$`-statistics.\n\n---\n\n### Data / Model Specification\n\nThe estimated statistical model is:\n```latex\n\\ln[C_{h}(t+1) - \\gamma] - \\ln[C_{h}(t) - \\gamma] = \\phi(t) + b_{y} y_{h}^{c} + v_{h}(t) \\quad \\text{(Eq. 1)}\n```\nKey results for total consumption, based on Table 2 in the paper, are summarized below.\n\n**Table 1: GMM Results for Total Consumption**\n| Model Specification | `$\\hat{\\gamma}$` (s.e.) | `$\\hat{b}_y$` (s.e.) | J-statistic (p-value %) | C-statistic (p-value %) | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1. Unrestricted | 177.6 (6.70) | -0.021 (0.049) | 34.44 (18.7) | 0.255 (61.4) | Tests `$b_y=0$` vs Model 2 |\n| 2. Restricted (`$b_y=0$`) | 177.2 (7.45) | 0 (imposed) | 34.70 (21.5) | — | Preferred Specification |\n| 3. Restricted (`$\\gamma=b_y=0$`) | 0 (imposed) | 0 (imposed) | 98.89 (0.0) | 64.428 (0.0) | Tests joint null vs Model 2 |\n\n---\n\n### The Question\n\nThe `$J$`-statistic for the preferred model (Row 2) has a p-value of 21.5%. What is the primary implication of this result for the study's conclusion?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 344, "Question": "### Background\n\nStandard evolutionary models with mutations, such as those by Kandori, Mailath, and Rob (KMR), often yield a unique long-run prediction by assuming a uniform mutation rate \\(\\varepsilon\\) across all states. This paper critiques that assumption by allowing mutation rates to be state-dependent. The core insight is that the relative probability of mutations, not just the number of mutations, determines long-run stability.\n\nThis problem explores a specific 2x2 coordination game where state-dependent mutations can reverse the standard KMR prediction.\n\n### Data / Model Specification\n\nA population of \\(N\\) agents repeatedly plays the following coordination game:\n```latex\n          Player 2\n          1      2\nPlayer 1\n   1    (8, 8) (0, 4)\n   2    (4, 0) (6, 6)\n```\nThe system is modeled as a Markov process where the state \\(i \\in S = \\{0, 1, ..., N\\}\\) is the number of agents playing strategy 2. The system has two absorbing states corresponding to the strict Nash equilibria: state 0 (all play strategy 1) and state \\(N\\) (all play strategy 2).\n\nThere is an integer threshold \\(i^* < N/2\\). Without mutations:\n*   The set \\(\\{0, 1, ..., i^*-1\\}\\) is the basin of attraction for state 0.\n*   The set \\(\\{i^*, ..., N\\}\\) is the basin of attraction for state \\(N\\).\n\nMutations are introduced with state-dependent rates:\n*   In the basin of attraction for state 0 (states \\(0, ..., i^*-1\\)), the mutation rate is \\(\\varepsilon^\\alpha\\).\n*   In the basin of attraction for state \\(N\\) (states \\(i^*, ..., N\\)), the mutation rate is \\(\\varepsilon\\).\n\nHere, \\(\\varepsilon > 0\\) is a small baseline rate and \\(\\alpha > 0\\) is a constant. If \\(\\alpha > 1\\), mutations are less likely when the system is near the Pareto-efficient equilibrium (state 0).\n\nThe ratio of the long-run probabilities (invariant distribution mass) of being in the two absorbing states is given by:\n```latex\n\\frac{q_{0}^{\\prime}}{q_{N}^{\\prime}}=\\varepsilon^{N-i^{*}+1-\\alpha i^{*}}\\frac{k_{1}[1+f_{1}(\\varepsilon)]}{k_{2}[1+f_{2}(\\varepsilon)]} \\quad \\text{(Eq. (1))}\n```\nwhere \\(k_1, k_2\\) are positive constants and \\(f_1(\\varepsilon), f_2(\\varepsilon)\\) are functions that go to zero as \\(\\varepsilon \\to 0\\).\n\n### Question\n\nThe standard KMR model (where \\(\\alpha=1\\)) selects state \\(N\\) because its basin of attraction is larger. This paper's model shows this can be reversed if \\(\\alpha\\) is large enough. Based on Eq. (1), what is the correct expression for the critical threshold \\(\\alpha^*\\) and the intuition for why a high \\(\\alpha\\) favors state 0?", "Options": {"A": "\\(\\alpha^* = (N-i^*+1)/i^*\\). A high \\(\\alpha\\) makes mutations out of state 0's smaller basin prohibitively rare, making it 'deeper' and more stable.", "B": "\\(\\alpha^* = i* / (N-i*+1)\\). A high \\(\\alpha\\) makes mutations into state 0's basin more frequent, increasing its stability.", "C": "\\(\\alpha^* = (N-i^*+1)/i^*\\). A high \\(\\alpha\\) increases the size of state 0's basin of attraction, making it more stable.", "D": "\\(\\alpha^* = (N-i^*+1)/i* - 1\\). A high \\(\\alpha\\) makes players in state 0 experiment more, which paradoxically reinforces the equilibrium."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses both the derivation of the critical threshold \\(\\alpha^*\\) and the core economic intuition behind the paper's example. It combines 'Computational Judgment' with conceptual understanding. Distractors are designed to separate these skills: (B) uses an inverted formula from a common algebraic error, (C) has the correct formula but a classic conceptual error (confusing mutation probability/depth with basin size), and (D) has an incorrect formula and faulty reasoning.", "qid": "344", "question": "### Background\n\nStandard evolutionary models with mutations, such as those by Kandori, Mailath, and Rob (KMR), often yield a unique long-run prediction by assuming a uniform mutation rate \\(\\varepsilon\\) across all states. This paper critiques that assumption by allowing mutation rates to be state-dependent. The core insight is that the relative probability of mutations, not just the number of mutations, determines long-run stability.\n\nThis problem explores a specific 2x2 coordination game where state-dependent mutations can reverse the standard KMR prediction.\n\n### Data / Model Specification\n\nA population of \\(N\\) agents repeatedly plays the following coordination game:\n```latex\n          Player 2\n          1      2\nPlayer 1\n   1    (8, 8) (0, 4)\n   2    (4, 0) (6, 6)\n```\nThe system is modeled as a Markov process where the state \\(i \\in S = \\{0, 1, ..., N\\}\\) is the number of agents playing strategy 2. The system has two absorbing states corresponding to the strict Nash equilibria: state 0 (all play strategy 1) and state \\(N\\) (all play strategy 2).\n\nThere is an integer threshold \\(i^* < N/2\\). Without mutations:\n*   The set \\(\\{0, 1, ..., i^*-1\\}\\) is the basin of attraction for state 0.\n*   The set \\(\\{i^*, ..., N\\}\\) is the basin of attraction for state \\(N\\).\n\nMutations are introduced with state-dependent rates:\n*   In the basin of attraction for state 0 (states \\(0, ..., i^*-1\\)), the mutation rate is \\(\\varepsilon^\\alpha\\).\n*   In the basin of attraction for state \\(N\\) (states \\(i^*, ..., N\\)), the mutation rate is \\(\\varepsilon\\).\n\nHere, \\(\\varepsilon > 0\\) is a small baseline rate and \\(\\alpha > 0\\) is a constant. If \\(\\alpha > 1\\), mutations are less likely when the system is near the Pareto-efficient equilibrium (state 0).\n\nThe ratio of the long-run probabilities (invariant distribution mass) of being in the two absorbing states is given by:\n```latex\n\\frac{q_{0}^{\\prime}}{q_{N}^{\\prime}}=\\varepsilon^{N-i^{*}+1-\\alpha i^{*}}\\frac{k_{1}[1+f_{1}(\\varepsilon)]}{k_{2}[1+f_{2}(\\varepsilon)]} \\quad \\text{(Eq. (1))}\n```\nwhere \\(k_1, k_2\\) are positive constants and \\(f_1(\\varepsilon), f_2(\\varepsilon)\\) are functions that go to zero as \\(\\varepsilon \\to 0\\).\n\n### Question\n\nThe standard KMR model (where \\(\\alpha=1\\)) selects state \\(N\\) because its basin of attraction is larger. This paper's model shows this can be reversed if \\(\\alpha\\) is large enough. Based on Eq. (1), what is the correct expression for the critical threshold \\(\\alpha^*\\) and the intuition for why a high \\(\\alpha\\) favors state 0?", "model_evaluation_result": {"model_raw_response": "Looking at equation (1), I need to determine when state 0 becomes more stable than state N in the long run.\n\nFrom the equation:\n$$\\frac{q_{0}^{\\prime}}{q_{N}^{\\prime}}=\\varepsilon^{N-i^{*}+1-\\alpha i^{*}}\\frac{k_{1}[1+f_{1}(\\varepsilon)]}{k_{2}[1+f_{2}(\\varepsilon)]}$$\n\nAs $\\varepsilon \\to 0$, the terms $f_1(\\varepsilon)$ and $f_2(\\varepsilon)$ go to zero, so the ratio is dominated by the exponential term $\\varepsilon^{N-i^{*}+1-\\alpha i^{*}}$.\n\nFor state 0 to be selected over state N in the long run, we need:\n$$\\frac{q_{0}^{\\prime}}{q_{N}^{\\prime}} > 1$$\n\nThis requires the exponent to be negative (since $\\varepsilon < 1$):\n$$N-i^{*}+1-\\alpha i^{*} < 0$$\n\nSolving for $\\alpha$:\n$$\\alpha i^{*} > N-i^{*}+1$$\n$$\\alpha > \\frac{N-i^{*}+1}{i^{*}}$$\n\nTherefore, the critical threshold is:\n$$\\alpha^* = \\frac{N-i^{*}+1}{i^{*}}$$\n\nThe intuition is that when $\\alpha > 1$, mutations are less likely in state 0's basin (rate $\\varepsilon^\\alpha$) compared to state N's basin (rate $\\varepsilon$). When $\\alpha$ is sufficiently large, mutations out of state 0's basin become prohibitively rare compared to mutations out of state N's basin. Even though state 0's basin is smaller, the dramatically reduced mutation rate makes it effectively \"deeper\" and more stable than the larger but more mutation-prone basin around state N.\n\nA", "model_answer": "A", "error": null}}
{"ID": 354, "Question": "### Background\n\n**Research Question.** This problem evaluates the empirical gains from using an asymptotically efficient estimator (2S2SLS) versus a consistent but inefficient one (NL2SLS) in a structural macroeconomic model. It further explores how estimator choice can impact policy conclusions and how to test for systemic model misspecification.\n\n**Setting / Institutional Environment.** The analysis uses limited-information (single-equation) instrumental variable methods to estimate the parameters of Taylor's (1979) model of the U.S. economy. The model's structural equations contain both rational expectations terms and serially correlated disturbances, a setting where 2S2SLS is theoretically more efficient than NL2SLS. The problem also contrasts these limited-information estimates with full-system estimates, which are efficient under correct specification but vulnerable to misspecification anywhere in the system.\n\n### Data / Model Specification\n\nTaylor's model consists of an aggregate demand (output) equation and an aggregate supply (price-setting) equation:\n\n```latex\ny_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\beta_3(m_t - p_t) + \\beta_4(m_{t-1} - p_{t-1}) + \\beta_5(_{t-1}\\pi_t) + \\beta_0 + u_t \n```\n```latex\n\\pi_t = \\pi_{t-1} + \\gamma_1(_{t-1}y_t) + \\gamma_0 + v_t \n```\nwhere `y_t` is the output gap, `π_t` is the inflation rate, `(m_t - p_t)` are log real money balances, `_{t-1}π_t` is the rational expectation of inflation, and `_{t-1}y_t` is the rational expectation of the output gap. The structural disturbances `u_t` and `v_t` are known to be serially correlated.\n\nTable 1 presents estimation results for the period 1954:I - 1975:I from three methods: 2S2SLS, NL2SLS, and Taylor's full-system method.\n\n**Table 1: Limited-information and Full-system Estimates of Taylor's Model**\n\n| | **Output Equation** | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Estimator** | `y_{t-1}` (`β_1`) | `y_{t-2}` | `(m_t-p_t)` | `(m_{t-1}-p_{t-1})` | `_{t-1}π_t` |\n| 2S2SLS | 1.354 (0.108) | -0.484 (0.093) | 0.620 (0.217) | -0.541 (0.203) | -0.164 (0.133) |\n| NL2SLS | 1.423 (0.166) | -0.552 (0.185) | 0.599 (0.230) | -0.532 (0.217) | -0.041 (0.188) |\n| Taylor | 1.167 (0.088) | -0.324 (0.090) | 0.578 (0.175) | -0.484 (0.194) | -0.447 (0.391) |\n| | **Price Equation** | | | | |\n| **Estimator** | `π_{t-1}` | `_{t-1}y_t` (`γ_1`) | `Constant` | | |\n| 2S2SLS | 1.0 (fixed) | 0.0164 (0.0070) | 0.398 (0.252) | | |\n| NL2SLS | 1.0 (fixed) | 0.0132 (0.0109) | 0.304 (0.340) | | |\n| Taylor | 1.0 (fixed) | 0.0180 (0.0058) | 0.515 (0.172) | | |\n\n*Standard errors are in parentheses.* \n\nThe paper conducts a Hausman-type specification test by comparing the price equation estimates from 2S2SLS with those from the full-system method. The null hypothesis is that the full model is correctly specified. Given a test statistic of 0.40 and a 95% critical value of 5.99, what is the correct interpretation?", "Options": {"A": "The null hypothesis is not rejected, providing no evidence that the output equation (or other parts of the full system) is misspecified.", "B": "The null hypothesis is rejected, indicating that the output equation is likely misspecified.", "C": "The null hypothesis is not rejected, which proves that the full model is correctly specified.", "D": "The test is invalid because 2S2SLS is a limited-information estimator while the full-system estimator is a full-information estimator."}, "Answer": "A", "pi_justification": "This item tests the ability to interpret the result of a specification test, a key diagnostic tool in econometrics. It uses a Computational Judgment strategy (simple comparison). The correct option (A) accurately states the outcome (fail to reject) and the correct, nuanced interpretation. Distractor (B) represents a failure to correctly compare the test statistic to the critical value. Distractor (C) represents a common conceptual error, equating 'failure to reject' with 'proof of the null'. Distractor (D) is a 'True but Irrelevant' statement that misinterprets the logic of the Hausman test.", "qid": "354", "question": "### Background\n\n**Research Question.** This problem evaluates the empirical gains from using an asymptotically efficient estimator (2S2SLS) versus a consistent but inefficient one (NL2SLS) in a structural macroeconomic model. It further explores how estimator choice can impact policy conclusions and how to test for systemic model misspecification.\n\n**Setting / Institutional Environment.** The analysis uses limited-information (single-equation) instrumental variable methods to estimate the parameters of Taylor's (1979) model of the U.S. economy. The model's structural equations contain both rational expectations terms and serially correlated disturbances, a setting where 2S2SLS is theoretically more efficient than NL2SLS. The problem also contrasts these limited-information estimates with full-system estimates, which are efficient under correct specification but vulnerable to misspecification anywhere in the system.\n\n### Data / Model Specification\n\nTaylor's model consists of an aggregate demand (output) equation and an aggregate supply (price-setting) equation:\n\n```latex\ny_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\beta_3(m_t - p_t) + \\beta_4(m_{t-1} - p_{t-1}) + \\beta_5(_{t-1}\\pi_t) + \\beta_0 + u_t \n```\n```latex\n\\pi_t = \\pi_{t-1} + \\gamma_1(_{t-1}y_t) + \\gamma_0 + v_t \n```\nwhere `y_t` is the output gap, `π_t` is the inflation rate, `(m_t - p_t)` are log real money balances, `_{t-1}π_t` is the rational expectation of inflation, and `_{t-1}y_t` is the rational expectation of the output gap. The structural disturbances `u_t` and `v_t` are known to be serially correlated.\n\nTable 1 presents estimation results for the period 1954:I - 1975:I from three methods: 2S2SLS, NL2SLS, and Taylor's full-system method.\n\n**Table 1: Limited-information and Full-system Estimates of Taylor's Model**\n\n| | **Output Equation** | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Estimator** | `y_{t-1}` (`β_1`) | `y_{t-2}` | `(m_t-p_t)` | `(m_{t-1}-p_{t-1})` | `_{t-1}π_t` |\n| 2S2SLS | 1.354 (0.108) | -0.484 (0.093) | 0.620 (0.217) | -0.541 (0.203) | -0.164 (0.133) |\n| NL2SLS | 1.423 (0.166) | -0.552 (0.185) | 0.599 (0.230) | -0.532 (0.217) | -0.041 (0.188) |\n| Taylor | 1.167 (0.088) | -0.324 (0.090) | 0.578 (0.175) | -0.484 (0.194) | -0.447 (0.391) |\n| | **Price Equation** | | | | |\n| **Estimator** | `π_{t-1}` | `_{t-1}y_t` (`γ_1`) | `Constant` | | |\n| 2S2SLS | 1.0 (fixed) | 0.0164 (0.0070) | 0.398 (0.252) | | |\n| NL2SLS | 1.0 (fixed) | 0.0132 (0.0109) | 0.304 (0.340) | | |\n| Taylor | 1.0 (fixed) | 0.0180 (0.0058) | 0.515 (0.172) | | |\n\n*Standard errors are in parentheses.* \n\nThe paper conducts a Hausman-type specification test by comparing the price equation estimates from 2S2SLS with those from the full-system method. The null hypothesis is that the full model is correctly specified. Given a test statistic of 0.40 and a 95% critical value of 5.99, what is the correct interpretation?", "model_evaluation_result": {"model_raw_response": "Looking at this Hausman-type specification test, I need to interpret the results correctly.\n\nThe test compares:\n- 2S2SLS estimates (consistent under both null and alternative hypotheses)\n- Full-system estimates (efficient under correct specification but inconsistent if any part of the system is misspecified)\n\nGiven information:\n- Test statistic: 0.40\n- 95% critical value: 5.99\n- Null hypothesis: The full model is correctly specified\n\nSince 0.40 < 5.99, we fail to reject the null hypothesis at the 5% significance level.\n\nNow examining the options:\n\nA) States we don't reject the null and this provides \"no evidence\" of misspecification in the output equation or other parts. This is the correct statistical interpretation - failing to reject doesn't prove the null is true, but indicates no evidence against it.\n\nB) Incorrectly states we reject the null hypothesis, which contradicts the test statistic being below the critical value.\n\nC) Claims that not rejecting the null \"proves\" the model is correctly specified. This is a fundamental statistical error - failing to reject a null hypothesis never proves it's true.\n\nD) Questions the validity of the test itself, but Hausman-type tests are specifically designed to compare limited-information and full-information estimators in this context.\n\nThe correct interpretation is that we fail to reject the null hypothesis, which means we found no statistical evidence of misspecification, but this doesn't constitute proof that the model is correct.\n\nA", "model_answer": "A", "error": null}}
{"ID": 353, "Question": "### Background\n\n**Research Question.** This problem evaluates the empirical gains from using an asymptotically efficient estimator (2S2SLS) versus a consistent but inefficient one (NL2SLS) in a structural macroeconomic model. It further explores how estimator choice can impact policy conclusions and how to test for systemic model misspecification.\n\n**Setting / Institutional Environment.** The analysis uses limited-information (single-equation) instrumental variable methods to estimate the parameters of Taylor's (1979) model of the U.S. economy. The model's structural equations contain both rational expectations terms and serially correlated disturbances, a setting where 2S2SLS is theoretically more efficient than NL2SLS. The problem also contrasts these limited-information estimates with full-system estimates, which are efficient under correct specification but vulnerable to misspecification anywhere in the system.\n\n### Data / Model Specification\n\nTaylor's model consists of an aggregate demand (output) equation and an aggregate supply (price-setting) equation:\n\n```latex\ny_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\beta_3(m_t - p_t) + \\beta_4(m_{t-1} - p_{t-1}) + \\beta_5(_{t-1}\\pi_t) + \\beta_0 + u_t \n```\n```latex\n\\pi_t = \\pi_{t-1} + \\gamma_1(_{t-1}y_t) + \\gamma_0 + v_t \n```\nwhere `y_t` is the output gap, `π_t` is the inflation rate, `(m_t - p_t)` are log real money balances, `_{t-1}π_t` is the rational expectation of inflation, and `_{t-1}y_t` is the rational expectation of the output gap. The structural disturbances `u_t` and `v_t` are known to be serially correlated.\n\nTable 1 presents estimation results for the period 1954:I - 1975:I from three methods: 2S2SLS, NL2SLS, and Taylor's full-system method.\n\n**Table 1: Limited-information and Full-system Estimates of Taylor's Model**\n\n| | **Output Equation** | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Estimator** | `y_{t-1}` (`β_1`) | `y_{t-2}` | `(m_t-p_t)` | `(m_{t-1}-p_{t-1})` | `_{t-1}π_t` |\n| 2S2SLS | 1.354 (0.108) | -0.484 (0.093) | 0.620 (0.217) | -0.541 (0.203) | -0.164 (0.133) |\n| NL2SLS | 1.423 (0.166) | -0.552 (0.185) | 0.599 (0.230) | -0.532 (0.217) | -0.041 (0.188) |\n| Taylor | 1.167 (0.088) | -0.324 (0.090) | 0.578 (0.175) | -0.484 (0.194) | -0.447 (0.391) |\n| | **Price Equation** | | | | |\n| **Estimator** | `π_{t-1}` | `_{t-1}y_t` (`γ_1`) | `Constant` | | |\n| 2S2SLS | 1.0 (fixed) | 0.0164 (0.0070) | 0.398 (0.252) | | |\n| NL2SLS | 1.0 (fixed) | 0.0132 (0.0109) | 0.304 (0.340) | | |\n| Taylor | 1.0 (fixed) | 0.0180 (0.0058) | 0.515 (0.172) | | |\n\n*Standard errors are in parentheses.* \n\nA central question in macroeconomics is whether there is a short-run Phillips curve trade-off, represented by a non-zero coefficient `γ_1` in the price equation. Based on the results in Table 1, how does the conclusion about this trade-off at the 5% significance level change when moving from the NL2SLS estimator to the 2S2SLS estimator?", "Options": {"A": "The coefficient is statistically insignificant under NL2SLS but becomes statistically significant under 2S2SLS, demonstrating the practical importance of the efficiency gain.", "B": "The coefficient is statistically significant under both estimators, with 2S2SLS simply providing a more precise estimate.", "C": "The coefficient is statistically insignificant under both estimators, providing no evidence for a Phillips curve trade-off.", "D": "The coefficient is statistically significant under NL2SLS but becomes insignificant under 2S2SLS, suggesting the efficiency correction is problematic."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to synthesize statistical results to understand the paper's core argument about estimator efficiency having real-world inferential consequences. It uses a Scenario Application strategy. The correct option (A) requires comparing the significance of the same coefficient under two estimators and drawing the correct conclusion about the value of the 2S2SLS method. Distractors represent all other logical possibilities: (B) significant in both cases, (C) insignificant in both cases, and (D) the conceptual opposite where the more efficient estimator leads to insignificance.", "qid": "353", "question": "### Background\n\n**Research Question.** This problem evaluates the empirical gains from using an asymptotically efficient estimator (2S2SLS) versus a consistent but inefficient one (NL2SLS) in a structural macroeconomic model. It further explores how estimator choice can impact policy conclusions and how to test for systemic model misspecification.\n\n**Setting / Institutional Environment.** The analysis uses limited-information (single-equation) instrumental variable methods to estimate the parameters of Taylor's (1979) model of the U.S. economy. The model's structural equations contain both rational expectations terms and serially correlated disturbances, a setting where 2S2SLS is theoretically more efficient than NL2SLS. The problem also contrasts these limited-information estimates with full-system estimates, which are efficient under correct specification but vulnerable to misspecification anywhere in the system.\n\n### Data / Model Specification\n\nTaylor's model consists of an aggregate demand (output) equation and an aggregate supply (price-setting) equation:\n\n```latex\ny_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\beta_3(m_t - p_t) + \\beta_4(m_{t-1} - p_{t-1}) + \\beta_5(_{t-1}\\pi_t) + \\beta_0 + u_t \n```\n```latex\n\\pi_t = \\pi_{t-1} + \\gamma_1(_{t-1}y_t) + \\gamma_0 + v_t \n```\nwhere `y_t` is the output gap, `π_t` is the inflation rate, `(m_t - p_t)` are log real money balances, `_{t-1}π_t` is the rational expectation of inflation, and `_{t-1}y_t` is the rational expectation of the output gap. The structural disturbances `u_t` and `v_t` are known to be serially correlated.\n\nTable 1 presents estimation results for the period 1954:I - 1975:I from three methods: 2S2SLS, NL2SLS, and Taylor's full-system method.\n\n**Table 1: Limited-information and Full-system Estimates of Taylor's Model**\n\n| | **Output Equation** | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Estimator** | `y_{t-1}` (`β_1`) | `y_{t-2}` | `(m_t-p_t)` | `(m_{t-1}-p_{t-1})` | `_{t-1}π_t` |\n| 2S2SLS | 1.354 (0.108) | -0.484 (0.093) | 0.620 (0.217) | -0.541 (0.203) | -0.164 (0.133) |\n| NL2SLS | 1.423 (0.166) | -0.552 (0.185) | 0.599 (0.230) | -0.532 (0.217) | -0.041 (0.188) |\n| Taylor | 1.167 (0.088) | -0.324 (0.090) | 0.578 (0.175) | -0.484 (0.194) | -0.447 (0.391) |\n| | **Price Equation** | | | | |\n| **Estimator** | `π_{t-1}` | `_{t-1}y_t` (`γ_1`) | `Constant` | | |\n| 2S2SLS | 1.0 (fixed) | 0.0164 (0.0070) | 0.398 (0.252) | | |\n| NL2SLS | 1.0 (fixed) | 0.0132 (0.0109) | 0.304 (0.340) | | |\n| Taylor | 1.0 (fixed) | 0.0180 (0.0058) | 0.515 (0.172) | | |\n\n*Standard errors are in parentheses.* \n\nA central question in macroeconomics is whether there is a short-run Phillips curve trade-off, represented by a non-zero coefficient `γ_1` in the price equation. Based on the results in Table 1, how does the conclusion about this trade-off at the 5% significance level change when moving from the NL2SLS estimator to the 2S2SLS estimator?", "model_evaluation_result": {"model_raw_response": "I need to analyze the statistical significance of the coefficient γ₁ (the coefficient on ₜ₋₁yₜ in the price equation) under both NL2SLS and 2S2SLS estimators at the 5% significance level.\n\nFrom Table 1, for the price equation coefficient γ₁:\n\n**NL2SLS:**\n- Coefficient: 0.0132\n- Standard error: 0.0109\n- t-statistic: 0.0132/0.0109 ≈ 1.21\n- Critical value at 5% (two-tailed): ±1.96\n- Since |1.21| < 1.96, the coefficient is NOT statistically significant\n\n**2S2SLS:**\n- Coefficient: 0.0164\n- Standard error: 0.0070\n- t-statistic: 0.0164/0.0070 ≈ 2.34\n- Critical value at 5% (two-tailed): ±1.96\n- Since |2.34| > 1.96, the coefficient IS statistically significant\n\nThe coefficient γ₁ represents the short-run Phillips curve trade-off. Under NL2SLS, we cannot reject the null hypothesis of no trade-off (coefficient is insignificant). However, under 2S2SLS, we can reject the null hypothesis and conclude there is evidence of a Phillips curve trade-off (coefficient is significant).\n\nThis demonstrates that the efficiency gain from using 2S2SLS over NL2SLS has practical importance - it changes the economic conclusion about whether a Phillips curve trade-off exists.\n\nA", "model_answer": "A", "error": null}}
{"ID": 310, "Question": "### Background\n\n**Research Question.** This problem investigates the causal effect of an income shock on the timing of fertility and how the choice of estimation strategy is critical for identification. A key concern is that unobserved factors, such as a farmer's innate ability, might be correlated with both the likelihood of experiencing a shock and fertility decisions, biasing simple correlations.\n\n**Setting / Institutional Environment.** The analysis uses a panel of women from the Kagera region of Tanzania. The income shock is a dummy variable for whether the household experienced an accidental crop loss. The outcome of interest is whether a woman gave birth in the 7 months since the last survey round. For a birth to occur, conception must have happened at least 9 months prior, so the relevant shock is one that occurred 7-14 months before the current survey.\n\n### Data / Model Specification\n\nThe study's preferred estimation strategy is a woman-level fixed effects (FE) linear probability model, which controls for all time-invariant individual characteristics (`μᵢ`). The model is:\n\n```latex\nY_{i,t} = \\beta_1 S_{i,t} + \\alpha_t + \\mu_i + \\varepsilon_{i,t} \\quad \\text{(Eq. 1)}\n```\n\nWhere `Y_{i,t}` is the birth outcome and `S_{i,t}` is the crop loss shock. Table 1 presents results from three specifications for the dependent variable `Birth since last survey`: a simple OLS model, a model with community-level fixed effects, and the preferred woman-level fixed effects model.\n\n**Table 1: The Effects of Crop Loss on Likelihood of a Recent Birth**\n\n| | OLS | Community FE | Woman FE |\n| :--- | :---: | :---: | :---: |\n| Crop loss (7-14 months) | -0.115*** | -0.130*** | -0.175*** |\n| | (0.038) | (0.040) | (0.049) |\n| Observations | 747 | 747 | 747 |\n\n*Note: Robust standard errors in parentheses. *** p<0.01.*\n\n### The Question\n\nBased on the results in Table 1, which statement provides the most accurate interpretation of the findings?", "Options": {"A": "Experiencing a crop loss reduces the probability of a birth by 17.5 percent. The OLS estimate is biased away from zero because less able farmers are both more likely to experience shocks and have more children.", "B": "Experiencing a crop loss reduces the probability of a birth by 0.175. The systematic increase in the coefficient's magnitude from OLS to Woman FE suggests that unobserved time-invariant factors create a negative bias in the OLS estimate.", "C": "Experiencing a crop loss reduces the probability of a birth by 17.5 percentage points. The OLS estimate is biased toward zero because less able farmers are more likely to experience shocks but also tend to have fewer children due to better planning.", "D": "Experiencing a crop loss reduces the probability of a birth by 17.5 percentage points. The systematic increase in the coefficient's magnitude from OLS to Woman FE suggests that unobserved time-invariant factors create a positive bias in the OLS estimate, making it appear smaller than the true effect."}, "Answer": "D", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the student's ability to interpret a regression coefficient from a linear probability model and to correctly infer the direction of omitted variable bias by comparing estimates across different specifications. It uses a Computational Judgment strategy. Distractor A uses the wrong units ('percent' instead of 'percentage points') and incorrectly signs the bias. Distractor B uses ambiguous units and incorrectly signs the bias. Distractor C correctly interprets the coefficient's units but provides an incorrect explanation for the bias.", "qid": "310", "question": "### Background\n\n**Research Question.** This problem investigates the causal effect of an income shock on the timing of fertility and how the choice of estimation strategy is critical for identification. A key concern is that unobserved factors, such as a farmer's innate ability, might be correlated with both the likelihood of experiencing a shock and fertility decisions, biasing simple correlations.\n\n**Setting / Institutional Environment.** The analysis uses a panel of women from the Kagera region of Tanzania. The income shock is a dummy variable for whether the household experienced an accidental crop loss. The outcome of interest is whether a woman gave birth in the 7 months since the last survey round. For a birth to occur, conception must have happened at least 9 months prior, so the relevant shock is one that occurred 7-14 months before the current survey.\n\n### Data / Model Specification\n\nThe study's preferred estimation strategy is a woman-level fixed effects (FE) linear probability model, which controls for all time-invariant individual characteristics (`μᵢ`). The model is:\n\n```latex\nY_{i,t} = \\beta_1 S_{i,t} + \\alpha_t + \\mu_i + \\varepsilon_{i,t} \\quad \\text{(Eq. 1)}\n```\n\nWhere `Y_{i,t}` is the birth outcome and `S_{i,t}` is the crop loss shock. Table 1 presents results from three specifications for the dependent variable `Birth since last survey`: a simple OLS model, a model with community-level fixed effects, and the preferred woman-level fixed effects model.\n\n**Table 1: The Effects of Crop Loss on Likelihood of a Recent Birth**\n\n| | OLS | Community FE | Woman FE |\n| :--- | :---: | :---: | :---: |\n| Crop loss (7-14 months) | -0.115*** | -0.130*** | -0.175*** |\n| | (0.038) | (0.040) | (0.049) |\n| Observations | 747 | 747 | 747 |\n\n*Note: Robust standard errors in parentheses. *** p<0.01.*\n\n### The Question\n\nBased on the results in Table 1, which statement provides the most accurate interpretation of the findings?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to carefully analyze the coefficient interpretation and the pattern of bias across the three estimation strategies.\n\nFirst, let me interpret the coefficient from the Woman FE model: -0.175. Since this is a linear probability model, this means experiencing a crop loss reduces the probability of a birth by 0.175, which equals 17.5 percentage points (not 17.5 percent).\n\nNext, I need to analyze the bias pattern. The coefficients show a systematic increase in magnitude:\n- OLS: -0.115\n- Community FE: -0.130  \n- Woman FE: -0.175\n\nThe Woman FE model is the preferred specification because it controls for all time-invariant individual characteristics (like innate ability) that could confound the relationship. The fact that the coefficient becomes more negative as we add more controls suggests that the OLS estimate is biased toward zero.\n\nThis bias pattern indicates that unobserved time-invariant factors (like farmer ability) create a positive bias in the OLS estimate. Here's the logic: if less able farmers are both more likely to experience crop losses AND more likely to have children (perhaps due to poorer family planning), then this positive correlation would make the negative effect of crop loss appear smaller in magnitude than it actually is.\n\nWhen we control for these time-invariant factors with woman fixed effects, we remove this positive bias, revealing the true larger negative effect.\n\nOption D correctly identifies:\n1. The effect is 17.5 percentage points (correct interpretation)\n2. The systematic increase in magnitude suggests positive bias in OLS\n3. This makes the OLS estimate appear smaller than the true effect\n\nD", "model_answer": "D", "error": null}}
{"ID": 338, "Question": "### Background\n\nThe selection of Econometric Society (ES) Fellows is a multi-stage process. First, candidates are nominated, either by the Nominating Committee or by existing Fellows. Since 2012, the Nominating Committee has had an explicit mandate to put forward highly qualified female candidates. Second, the full body of Fellows votes on the slate of nominees to elect new Fellows.\n\n### Data / Model Specification\n\nTable 1 provides descriptive statistics on nominations and elections for the 2012-2019 period. Table 2 provides coefficients from logit models estimated on the population of active economists (Panel A) and the sample of nominees (Panel B) for the same period. The models control for a comprehensive vector of publication and citation records.\n\n**Table 1: Summary of Nomination and Election Rates, 2012-2019**\n| Probability / Share | Value |\n|:---|---:|\n| `P(F|N)`: Fraction Females among Nominees | 12.6% |\n| `P(F|N_c)`: Fraction Females among Committee Nominees | 22.7% |\n| `P(E|N_c)`: Election Rate for Committee Nominees | 50.7% |\n| `P(E|N_f)`: Election Rate for Fellow's Nominees | 12.4% |\n\n*Source: Adapted from Table V of the paper.*\n\n**Table 2: Logit Coefficients for Nomination and Election, 2012-2019**\n| Model (Dependent Variable) | Coef. on `Female` | Coef. on `Committee Nominee` |\n|:---|:---:|:---:|\n| **Panel A: Population at Risk** | |\n| 1. Nomination by Committee | 1.209 (0.252) | - |\n| 2. Nomination by Fellows | 0.280 (0.288) | - |\n| **Panel B: Sample of Nominees** | |\n| 3. Selection as Fellow, Cond. on Nom. | 0.639 (0.478) | 2.723 (0.290) |\n\n*Note: Standard errors in parentheses. All models include a full set of controls for publications and citations. Source: Adapted from Table VI, column 3.*\n\n### Question\n\nBased on the data provided, what is the most accurate conclusion regarding the sharp increase in the share of female ES Fellows in the 2012-2019 period?", "Options": {"A": "The Nominating Committee's explicit preference for nominating qualified women, amplified by a strong voter preference for Committee-endorsed candidates, was the primary driver.", "B": "The primary driver was the growing pool of qualified female economists, which led to a proportional increase in nominations from both the Committee and the general body of Fellows.", "C": "The general body of Fellows, not the Nominating Committee, began showing a strong preference for nominating and electing female candidates after 2012.", "D": "The main reason was that voters began strongly preferring any candidate nominated by the Committee, and the Committee happened to nominate a few more women by chance."}, "Answer": "A", "pi_justification": "Assessment Target: Synthesizing descriptive statistics and regression results to identify a key mechanism. Strategy: Computational Judgment. Distractor Logic: (B) represents a 'pipeline' explanation that is refuted by the regression evidence of a preference shift. (C) incorrectly assigns the preference shift to the Fellows instead of the Committee. (D) correctly identifies the voter endorsement effect but understates the Committee's active, preferential role in nominating women, which is evident from the large coefficient in Model 1.", "qid": "338", "question": "### Background\n\nThe selection of Econometric Society (ES) Fellows is a multi-stage process. First, candidates are nominated, either by the Nominating Committee or by existing Fellows. Since 2012, the Nominating Committee has had an explicit mandate to put forward highly qualified female candidates. Second, the full body of Fellows votes on the slate of nominees to elect new Fellows.\n\n### Data / Model Specification\n\nTable 1 provides descriptive statistics on nominations and elections for the 2012-2019 period. Table 2 provides coefficients from logit models estimated on the population of active economists (Panel A) and the sample of nominees (Panel B) for the same period. The models control for a comprehensive vector of publication and citation records.\n\n**Table 1: Summary of Nomination and Election Rates, 2012-2019**\n| Probability / Share | Value |\n|:---|---:|\n| `P(F|N)`: Fraction Females among Nominees | 12.6% |\n| `P(F|N_c)`: Fraction Females among Committee Nominees | 22.7% |\n| `P(E|N_c)`: Election Rate for Committee Nominees | 50.7% |\n| `P(E|N_f)`: Election Rate for Fellow's Nominees | 12.4% |\n\n*Source: Adapted from Table V of the paper.*\n\n**Table 2: Logit Coefficients for Nomination and Election, 2012-2019**\n| Model (Dependent Variable) | Coef. on `Female` | Coef. on `Committee Nominee` |\n|:---|:---:|:---:|\n| **Panel A: Population at Risk** | |\n| 1. Nomination by Committee | 1.209 (0.252) | - |\n| 2. Nomination by Fellows | 0.280 (0.288) | - |\n| **Panel B: Sample of Nominees** | |\n| 3. Selection as Fellow, Cond. on Nom. | 0.639 (0.478) | 2.723 (0.290) |\n\n*Note: Standard errors in parentheses. All models include a full set of controls for publications and citations. Source: Adapted from Table VI, column 3.*\n\n### Question\n\nBased on the data provided, what is the most accurate conclusion regarding the sharp increase in the share of female ES Fellows in the 2012-2019 period?", "model_evaluation_result": {"model_raw_response": "Looking at the data systematically:\n\nFrom Table 1:\n- Female nominees make up 12.6% overall, but 22.7% of Committee nominees\n- Committee nominees have a much higher election rate (50.7% vs 12.4%)\n\nFrom Table 2:\n- The Committee shows a strong preference for nominating women (coefficient 1.209)\n- Fellows show weaker preference for nominating women (coefficient 0.280) \n- There's a very strong voter preference for Committee nominees (coefficient 2.723)\n- Being female provides additional advantage in final election (coefficient 0.639)\n\nThe mechanism is clear: The Committee explicitly targets qualified women (22.7% vs 12.6% baseline), and Committee nominees have dramatically higher success rates. This creates a multiplicative effect - women benefit both from preferential Committee nomination AND from the Committee's endorsement effect with voters.\n\nOption A correctly identifies both components: the Committee's explicit preference for nominating qualified women AND the amplification through strong voter preference for Committee-endorsed candidates.\n\nOption B is contradicted by the differential nomination rates between Committee and Fellows. Option C mischaracterizes where the preference originates. Option D downplays the Committee's intentional strategy as mere chance.\n\nA", "model_answer": "A", "error": null}}
{"ID": 437, "Question": "### Background\n\n**Research Question.** A key goal of the paper is to determine not just the size of the Federal Reserve's reaction to the stock market, but also the *reason* for it. The central question is whether the Fed reacts to stock prices as an independent goal, or only because stock prices affect the Fed's primary mandate of macroeconomic stability.\n\n### Question\n\nThe paper's empirical method estimates that a 5% rise in the S&P 500 leads to an actual policy response equivalent to a 14 basis point increase in the expected federal funds rate. A separate 'back-of-the-envelope' calculation, based on the wealth effect on aggregate demand, suggests a 'warranted' response to fully stabilize the economy would be in the range of 12-23 basis points. What is the most accurate conclusion from comparing these two results?", "Options": {"A": "The Fed's response is consistent with its mandate to stabilize the macroeconomy, reacting to stock prices only to the extent they affect aggregate demand.", "B": "The Fed systematically under-reacts to stock market shocks, suggesting a reluctance to intervene based on asset prices.", "C": "The Fed appears to be targeting stock prices directly, as the response is too rapid and large for a purely macroeconomic reaction.", "D": "The estimated response is significantly smaller than the warranted response, indicating the wealth effect on consumption is overestimated."}, "Answer": "A", "pi_justification": "This item assesses the ability to draw a high-level conclusion by comparing an empirical estimate to a theoretical benchmark. It uses a Reverse-Reasoning strategy, asking for the interpretation that best explains the evidence. The correct option (A) accurately reflects the paper's main conclusion. Distractor B is an 'Almost Right' misinterpretation of the numbers (14 is within the 12-23 range, not systematically under). Distractor C is a 'Conceptual Opposite' that posits the very behavior the paper argues against. Distractor D draws an incorrect causal inference from the comparison.", "qid": "437", "question": "### Background\n\n**Research Question.** A key goal of the paper is to determine not just the size of the Federal Reserve's reaction to the stock market, but also the *reason* for it. The central question is whether the Fed reacts to stock prices as an independent goal, or only because stock prices affect the Fed's primary mandate of macroeconomic stability.\n\n### Question\n\nThe paper's empirical method estimates that a 5% rise in the S&P 500 leads to an actual policy response equivalent to a 14 basis point increase in the expected federal funds rate. A separate 'back-of-the-envelope' calculation, based on the wealth effect on aggregate demand, suggests a 'warranted' response to fully stabilize the economy would be in the range of 12-23 basis points. What is the most accurate conclusion from comparing these two results?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 378, "Question": "A unit mass of agents, each with a private type `q_i` (likelihood of needing care), decide whether to visit a hospital with capacity `\\bar{n}`. The distribution of types is given by the CDF `H(q)`. In the no-congestion regime, an agent visits if `q_i \\ge c/\\tilde{s}`, where `c` is the visit cost and `\\tilde{s}` is the public belief about disease severity.\n\nThe **exhaustion level**, `s_0`, is the critical public belief at which the number of visitors, behaving as if there is no congestion, exactly equals the hospital's capacity `\\bar{n}`. Which of the following is the correct formula for `s_0`?", "Options": {"A": "`c / H^{-1}(1-\\bar{n})`", "B": "`c / H^{-1}(\\bar{n})`", "C": "`\\bar{n} / H^{-1}(1-c)`", "D": "`c`"}, "Answer": "A", "pi_justification": "This item uses the Computational Judgment strategy to test the derivation of a key model parameter. The student must correctly set up the equilibrium condition at the threshold of congestion (`1 - H(c/s_0) = \\bar{n}`) and solve for `s_0`. Distractor (B) represents a common error in using a CDF (confusing `H(x)` with `1-H(x)`). Distractor (C) scrambles the parameters (Formula Misuse). Distractor (D) identifies a different, less important threshold in the model (Conceptual Error).", "qid": "378", "question": "A unit mass of agents, each with a private type `q_i` (likelihood of needing care), decide whether to visit a hospital with capacity `\\bar{n}`. The distribution of types is given by the CDF `H(q)`. In the no-congestion regime, an agent visits if `q_i \\ge c/\\tilde{s}`, where `c` is the visit cost and `\\tilde{s}` is the public belief about disease severity.\n\nThe **exhaustion level**, `s_0`, is the critical public belief at which the number of visitors, behaving as if there is no congestion, exactly equals the hospital's capacity `\\bar{n}`. Which of the following is the correct formula for `s_0`?", "model_evaluation_result": {"model_raw_response": "I need to find the exhaustion level s₀, which is the critical public belief where the number of visitors equals hospital capacity n̄.\n\nLet me work through this step by step:\n\n1) In the no-congestion regime, an agent visits if q_i ≥ c/s̃, where s̃ is the public belief about disease severity.\n\n2) The number of visitors is the mass of agents with q_i ≥ c/s̃.\n\n3) Since types are distributed according to CDF H(q), the fraction of agents with q_i ≥ c/s̃ is:\n   1 - H(c/s̃)\n\n4) Since we have a unit mass of agents, the total number of visitors is:\n   1 - H(c/s̃)\n\n5) At the exhaustion level s₀, this number of visitors exactly equals the hospital capacity n̄:\n   1 - H(c/s₀) = n̄\n\n6) Rearranging:\n   H(c/s₀) = 1 - n̄\n\n7) Taking the inverse function:\n   c/s₀ = H⁻¹(1 - n̄)\n\n8) Solving for s₀:\n   s₀ = c / H⁻¹(1 - n̄)\n\nTherefore, the exhaustion level s₀ = c / H⁻¹(1 - n̄).\n\nA", "model_answer": "A", "error": null}}
{"ID": 284, "Question": "### Background\n\nIn the sequential *demand game*, the equilibrium payoff vector `d_N = (d_1, ..., d_n)` is defined recursively for a given player order `w=(1, 2, ..., n)`. After determining the payoffs for players `k+1` through `n`, the payoff for player `k` is determined by the following maximization problem:\n\n```latex\nd_k = \\operatorname*{max} u_k(z_k) \\quad \\text{s.t.} \\quad \\exists (z_{k+1}, ..., z_n) \\text{ where } (z_k, ..., z_n) \\text{ is feasible for } \\{k,...,n\\} \\text{ and } u_j(z_j) \\ge d_j \\text{ for } j=k+1,...,n\n```\n\n### Question\n\nWhich statement provides the most accurate economic interpretation of the equilibrium payoff `d_k` for player `k`?", "Options": {"A": "`d_k` is the maximum utility player `k` can obtain by forming a coalition with all subsequent players `{k, ..., n}`, under the constraint that each subsequent player `j` receives their equilibrium payoff `d_j`.", "B": "`d_k` is the utility player `k` receives by ensuring all subsequent players `j` receive at least their individually rational payoff `I_j`.", "C": "`d_k` represents player `k`'s marginal contribution to the grand coalition `N`, as calculated by the Shapley value.", "D": "`d_k` is the utility player `k` can achieve by producing alone, since they have a first-mover advantage over subsequent players."}, "Answer": "A", "pi_justification": "This item assesses the conceptual understanding of the backward induction logic in the demand game. It uses a Reverse-Reasoning strategy, asking for the interpretation of a formula. The correct option (A) accurately describes the maximization problem. Distractor B presents a common misconception, replacing the recursive equilibrium payoff `d_j` with the simpler autarky payoff `I_j`. Distractor C confuses the order-dependent `d_k` with the symmetric Shapley value. Distractor D incorrectly equates the payoff with autarky, ignoring the coalition formation.", "qid": "284", "question": "### Background\n\nIn the sequential *demand game*, the equilibrium payoff vector `d_N = (d_1, ..., d_n)` is defined recursively for a given player order `w=(1, 2, ..., n)`. After determining the payoffs for players `k+1` through `n`, the payoff for player `k` is determined by the following maximization problem:\n\n```latex\nd_k = \\operatorname*{max} u_k(z_k) \\quad \\text{s.t.} \\quad \\exists (z_{k+1}, ..., z_n) \\text{ where } (z_k, ..., z_n) \\text{ is feasible for } \\{k,...,n\\} \\text{ and } u_j(z_j) \\ge d_j \\text{ for } j=k+1,...,n\n```\n\n### Question\n\nWhich statement provides the most accurate economic interpretation of the equilibrium payoff `d_k` for player `k`?", "model_evaluation_result": {"model_raw_response": "Looking at the mathematical definition of the equilibrium payoff `d_k`, I need to carefully analyze what this optimization problem represents.\n\nThe key elements are:\n1. Player `k` maximizes their utility `u_k(z_k)`\n2. Subject to the constraint that there exists an allocation `(z_{k+1}, ..., z_n)` such that:\n   - The entire allocation `(z_k, ..., z_n)` is feasible for the coalition `{k,...,n}`\n   - Each subsequent player `j` receives at least their equilibrium payoff `d_j`\n\nThis means player `k` is finding the best outcome they can achieve when working with players `{k+1, ..., n}`, but must ensure that each of these subsequent players gets their predetermined equilibrium payoff.\n\nLet me evaluate each option:\n\nA) This correctly captures that player `k` is maximizing utility by forming a coalition with subsequent players `{k, ..., n}`, and must satisfy the constraint that each subsequent player `j` receives their equilibrium payoff `d_j`. This matches the mathematical formulation exactly.\n\nB) This mentions \"individually rational payoff `I_j`\" rather than equilibrium payoff `d_j`, which is incorrect terminology for this context.\n\nC) The Shapley value is a completely different solution concept that involves averaging marginal contributions across all possible orderings. This has nothing to do with the sequential demand game structure described.\n\nD) This suggests player `k` acts alone, which contradicts the feasibility constraint that requires coordination with the coalition `{k,...,n}`.\n\nOption A provides the most accurate interpretation of the mathematical definition given.\n\nA", "model_answer": "A", "error": null}}
{"ID": 391, "Question": "### Background\n\nIn a strategic redistricting game, a party's equilibrium strategy is a **`p`-segregation plan**. This plan divides voters based on a cutoff characteristic `z`. Voters in districts with characteristics `ω ≤ z` are 'packed' (left as segregated as possible), while voters in districts with `ω > z` are 'cracked' (pooled into uniform favorable districts with an average characteristic `ω_z`).\n\n### Data / Model Specification\n\nThe probability that Party 1 wins a district with effective characteristic `θ = ω+s` is given by the District Outcome Function (DOF), `π(θ)`. The optimal choice of the cutoff `z` is determined by a first-order condition that must hold at the critical state `s*` where the election is tied:\n\n```latex\n\\pi'(\\omega_{z}+s^{*})(\\omega_{z}-z) = \\pi(\\omega_{z}+s^{*})-\\pi(z+s^{*}) \\quad \\text{(Eq. (1))}\n```\n\nThis can be rearranged as:\n\n```latex\n\\pi'(\\omega_{z}+s^{*}) = \\frac{\\pi(\\omega_{z}+s^{*})-\\pi(z+s^{*})}{\\omega_{z}-z}\n```\n\n### Question\n\nWhat is the correct graphical interpretation of the condition in Eq. (1) on a plot of the District Outcome Function `π(θ)` versus the effective district characteristic `θ`?", "Options": {"A": "The slope of the tangent line to the `π` curve at the favorable district point (`θ = ω_z+s*`) is equal to the slope of the secant line connecting the marginal packed district point (`θ = z+s*`) and the favorable district point.", "B": "The area under the `π` curve between `z+s*` and `ω_z+s*` is maximized.", "C": "The `π` curve is at its inflection point at `θ = ω_z+s*`, where the marginal return to increasing district favorability is highest.", "D": "The vertical distance between the `π` curve at `ω_z+s*` and the tangent line at `z+s*` is maximized."}, "Answer": "A", "pi_justification": "This item assesses the ability to translate a complex mathematical first-order condition into its precise graphical representation, a key skill in microeconomic theory. It uses a Computational Judgment strategy. The correct answer (A) is the exact geometric interpretation. Distractors are designed based on common misinterpretations of optimization conditions: (B) confuses the marginal condition with an integral (total area) condition, (C) confuses the optimal point with the point of maximum marginal returns (inflection point), and (D) proposes an alternative, incorrect geometric optimization.", "qid": "391", "question": "### Background\n\nIn a strategic redistricting game, a party's equilibrium strategy is a **`p`-segregation plan**. This plan divides voters based on a cutoff characteristic `z`. Voters in districts with characteristics `ω ≤ z` are 'packed' (left as segregated as possible), while voters in districts with `ω > z` are 'cracked' (pooled into uniform favorable districts with an average characteristic `ω_z`).\n\n### Data / Model Specification\n\nThe probability that Party 1 wins a district with effective characteristic `θ = ω+s` is given by the District Outcome Function (DOF), `π(θ)`. The optimal choice of the cutoff `z` is determined by a first-order condition that must hold at the critical state `s*` where the election is tied:\n\n```latex\n\\pi'(\\omega_{z}+s^{*})(\\omega_{z}-z) = \\pi(\\omega_{z}+s^{*})-\\pi(z+s^{*}) \\quad \\text{(Eq. (1))}\n```\n\nThis can be rearranged as:\n\n```latex\n\\pi'(\\omega_{z}+s^{*}) = \\frac{\\pi(\\omega_{z}+s^{*})-\\pi(z+s^{*})}{\\omega_{z}-z}\n```\n\n### Question\n\nWhat is the correct graphical interpretation of the condition in Eq. (1) on a plot of the District Outcome Function `π(θ)` versus the effective district characteristic `θ`?", "model_evaluation_result": {"model_raw_response": "Looking at the given equation:\n\nπ'(ω_z + s*) = [π(ω_z + s*) - π(z + s*)] / (ω_z - z)\n\nThe left side represents the derivative (slope of the tangent line) of the π function at the point θ = ω_z + s*.\n\nThe right side represents the slope of a secant line connecting two points:\n- Point 1: (z + s*, π(z + s*)) - the marginal packed district point\n- Point 2: (ω_z + s*, π(ω_z + s*)) - the favorable district point\n\nThe equation states that these two slopes are equal at the optimal cutoff z.\n\nThis is the classic condition where the marginal rate (tangent slope) equals the average rate (secant slope) between two points on the curve.\n\nA", "model_answer": "A", "error": null}}
{"ID": 286, "Question": "This problem deconstructs the paper's core identification strategy: a panel Instrumental Variables (IV) approach that is equivalent to a Difference-in-Differences (DD) estimator. The analysis compares two groups of hospitals over two years (1988 and 1989). The 'treatment group' consists of hospitals that opened a new catheterization lab in 1989 ('Open cath lab'). The 'control group' consists of hospitals that already had a catheterization lab in both years ('With cath lab'). The adoption of the technology serves as the instrument for patient-level treatment.\n\nTable 1 provides data for a simple 2x2 DD calculation. The panel IV estimate is the ratio of the outcome DD (`β`) to the treatment DD (`α`). The paper states that the DD for the 30-day mortality rate (`β`) is -0.4 percentage points.\n\n**Table 1: Trends in Catheterization and Mortality Rates (in percent)**\n\n| Hospital Group | Year | 90-day Catheterization Rate | 30-day Mortality Rate |\n| :--- | :---: | :---: | :---: |\n| With cath lab (Control) | 1988 | 33.0 | 22.8 |\n| | 1989 | 34.5 | 22.2 |\n| Open cath lab (Treated) | 1988 | 20.0 | 23.3 |\n| | 1989 | 28.3 | 23.1 |\n\nUsing the data in Table 1, what is the panel IV (or Difference-in-Differences) estimate of the effect of catheterization on 30-day mortality?", "Options": {"A": "A reduction of 5.9 percentage points.", "B": "A reduction of 0.4 percentage points.", "C": "An increase of 6.8 percentage points.", "D": "A reduction of 0.059 percentage points."}, "Answer": "A", "pi_justification": "This item assesses the ability to correctly calculate a Difference-in-Differences IV (Wald) estimate. It uses a Computational Judgment strategy. The correct answer requires calculating the first stage (`α = (28.3-20.0) - (34.5-33.0) = 6.8`), then the ratio `β/α = -0.4/6.8 = -0.0588`, and converting to percentage points (-5.9). Distractor B is the reduced-form effect (`β`), a Step-Omission Error. Distractor C is the first-stage effect (`α`), a Formula Misuse error. Distractor D is a Unit Error, failing to multiply by 100.", "qid": "286", "question": "This problem deconstructs the paper's core identification strategy: a panel Instrumental Variables (IV) approach that is equivalent to a Difference-in-Differences (DD) estimator. The analysis compares two groups of hospitals over two years (1988 and 1989). The 'treatment group' consists of hospitals that opened a new catheterization lab in 1989 ('Open cath lab'). The 'control group' consists of hospitals that already had a catheterization lab in both years ('With cath lab'). The adoption of the technology serves as the instrument for patient-level treatment.\n\nTable 1 provides data for a simple 2x2 DD calculation. The panel IV estimate is the ratio of the outcome DD (`β`) to the treatment DD (`α`). The paper states that the DD for the 30-day mortality rate (`β`) is -0.4 percentage points.\n\n**Table 1: Trends in Catheterization and Mortality Rates (in percent)**\n\n| Hospital Group | Year | 90-day Catheterization Rate | 30-day Mortality Rate |\n| :--- | :---: | :---: | :---: |\n| With cath lab (Control) | 1988 | 33.0 | 22.8 |\n| | 1989 | 34.5 | 22.2 |\n| Open cath lab (Treated) | 1988 | 20.0 | 23.3 |\n| | 1989 | 28.3 | 23.1 |\n\nUsing the data in Table 1, what is the panel IV (or Difference-in-Differences) estimate of the effect of catheterization on 30-day mortality?", "model_evaluation_result": {"model_raw_response": "I need to calculate the panel IV estimate using the difference-in-differences approach.\n\nFirst, let me calculate the treatment DD (α) - the change in catheterization rates:\n\nFor \"Open cath lab\" group:\n- 1988: 20.0%\n- 1989: 28.3%\n- Change: 28.3 - 20.0 = 8.3 percentage points\n\nFor \"With cath lab\" group:\n- 1988: 33.0%\n- 1989: 34.5%\n- Change: 34.5 - 33.0 = 1.5 percentage points\n\nTreatment DD (α) = 8.3 - 1.5 = 6.8 percentage points\n\nNext, let me calculate the outcome DD (β) - the change in 30-day mortality rates:\n\nFor \"Open cath lab\" group:\n- 1988: 23.3%\n- 1989: 23.1%\n- Change: 23.1 - 23.3 = -0.2 percentage points\n\nFor \"With cath lab\" group:\n- 1988: 22.8%\n- 1989: 22.2%\n- Change: 22.2 - 22.8 = -0.6 percentage points\n\nOutcome DD (β) = -0.2 - (-0.6) = -0.2 + 0.6 = 0.4 percentage points\n\nWait, the problem states that β = -0.4 percentage points. Let me recalculate:\n\nDD for mortality = (23.1 - 23.3) - (22.2 - 22.8) = -0.2 - (-0.6) = 0.4\n\nThis gives +0.4, but the problem states it's -0.4. Let me check the DD calculation order:\n\nDD = (Treated_after - Treated_before) - (Control_after - Control_before)\nDD = (23.1 - 23.3) - (22.2 - 22.8) = -0.2 - (-0.6) = 0.4\n\nThe problem", "model_answer": "I need to calculate the panel IV estimate using the difference-in-differences approach.\n\nFirst, let me calculate the treatment DD (α) - the change in catheterization rates:\n\nFor \"Open cath lab\" group:\n- 1988: 20.0%\n- 1989: 28.3%\n- Change: 28.3 - 20.0 = 8.3 percentage points\n\nFor \"With cath lab\" group:\n- 1988: 33.0%\n- 1989: 34.5%\n- Change: 34.5 - 33.0 = 1.5 percentage points\n\nTreatment DD (α) = 8.3 - 1.5 = 6.8 percentage points\n\nNext, let me calculate the outcome DD (β) - the change in 30-day mortality rates:\n\nFor \"Open cath lab\" group:\n- 1988: 23.3%\n- 1989: 23.1%\n- Change: 23.1 - 23.3 = -0.2 percentage points\n\nFor \"With cath lab\" group:\n- 1988: 22.8%\n- 1989: 22.2%\n- Change: 22.2 - 22.8 = -0.6 percentage points\n\nOutcome DD (β) = -0.2 - (-0.6) = -0.2 + 0.6 = 0.4 percentage points\n\nWait, the problem states that β = -0.4 percentage points. Let me recalculate:\n\nDD for mortality = (23.1 - 23.3) - (22.2 - 22.8) = -0.2 - (-0.6) = 0.4\n\nThis gives +0.4, but the problem states it's -0.4. Let me check the DD calculation order:\n\nDD = (Treated_after - Treated_before) - (Control_after - Control_before)\nDD = (23.1 - 23.3) - (22.2 - 22.8) = -0.2 - (-0.6) = 0.4\n\nThe problem", "error": "Warning: Could not extract single choice letter from: I need to calculate the panel IV estimate using th..."}}
{"ID": 334, "Question": "### Background\n\nAn auction is held with two local bidders (1 and 2) and one global bidder (3) for two items (A and B). Bidder 1 values only item A at `$v_1$`, bidder 2 values only item B at `$v_2$`, and bidder 3 values only the package AB at `$V$`. Local bidders' values are i.i.d. `$U[0,1]$`, and the global bidder's value is `$U[0,2]$`. The allocation is based on bids: the global bidder wins if `$B > b_1+b_2$`, otherwise the local bidders win.\n\n### Data / Model Specification\n\nIn the Bayesian Nash Equilibrium of the BCV auction, the global bidder bids truthfully (`$B(V)=V$`), but local bidders strategically shade their bids according to:\n\n```latex\nb_i(v_i) = \\operatorname*{max}(0, v_i - \\alpha) \\quad \\text{(Eq. 1)}\n```\nwhere `$\\alpha = 3-2\\sqrt{2} > 0$`.\n\nThis bid-shading can lead to an inefficient allocation where the global bidder wins even though the sum of the local bidders' true values is higher. Under which of the following conditions on the true values (`$v_1, v_2, V$`) does this specific type of inefficiency occur?", "Options": {"A": "`$V > v_1 + v_2$`", "B": "`$v_1 + v_2 > V > v_1 + v_2 - 2\\alpha$`", "C": "`$v_1 + v_2 - 2\\alpha > V$`", "D": "`$V > v_1 + v_2 + 2\\alpha$`"}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item tests the ability to apply an equilibrium strategy to determine its consequences for allocative efficiency. It uses a Computational Judgment strategy. Distractor A represents the condition for efficiency, a conceptual opposite. Distractor C represents the condition under which the local bidders win, failing to identify the inefficiency region. Distractor D introduces a sign error on the bid-shading term, a plausible numerical slip.", "qid": "334", "question": "### Background\n\nAn auction is held with two local bidders (1 and 2) and one global bidder (3) for two items (A and B). Bidder 1 values only item A at `$v_1$`, bidder 2 values only item B at `$v_2$`, and bidder 3 values only the package AB at `$V$`. Local bidders' values are i.i.d. `$U[0,1]$`, and the global bidder's value is `$U[0,2]$`. The allocation is based on bids: the global bidder wins if `$B > b_1+b_2$`, otherwise the local bidders win.\n\n### Data / Model Specification\n\nIn the Bayesian Nash Equilibrium of the BCV auction, the global bidder bids truthfully (`$B(V)=V$`), but local bidders strategically shade their bids according to:\n\n```latex\nb_i(v_i) = \\operatorname*{max}(0, v_i - \\alpha) \\quad \\text{(Eq. 1)}\n```\nwhere `$\\alpha = 3-2\\sqrt{2} > 0$`.\n\nThis bid-shading can lead to an inefficient allocation where the global bidder wins even though the sum of the local bidders' true values is higher. Under which of the following conditions on the true values (`$v_1, v_2, V$`) does this specific type of inefficiency occur?", "model_evaluation_result": {"model_raw_response": "I need to analyze when the BCV auction leads to inefficient allocation where the global bidder wins despite having lower total value than the local bidders.\n\nGiven information:\n- Global bidder bids truthfully: B(V) = V\n- Local bidders shade their bids: b_i(v_i) = max(0, v_i - α) where α = 3 - 2√2 > 0\n- Global bidder wins if B > b_1 + b_2\n- Inefficiency occurs when global bidder wins but v_1 + v_2 > V\n\nFor the global bidder to win: B > b_1 + b_2\nSince B = V and b_i = max(0, v_i - α), we have:\nV > b_1 + b_2 = max(0, v_1 - α) + max(0, v_2 - α)\n\nAssuming v_1, v_2 > α (otherwise bids would be 0), this becomes:\nV > (v_1 - α) + (v_2 - α) = v_1 + v_2 - 2α\n\nSo the global bidder wins when: V > v_1 + v_2 - 2α\n\nFor this to be inefficient, we need the local bidders to have higher total value:\nv_1 + v_2 > V\n\nCombining these conditions for inefficiency:\n1. Global bidder wins: V > v_1 + v_2 - 2α\n2. Local bidders should win (efficiency): v_1 + v_2 > V\n\nThis gives us: v_1 + v_2 > V > v_1 + v_2 - 2α\n\nThis is exactly the condition where the global bidder wins due to bid-shading by local bidders, even though the efficient allocation would give the items to the local bidders.\n\nB", "model_answer": "B", "error": null}}
{"ID": 342, "Question": "### Background\n\n**Research Question.** This question examines the paper's strategy for identifying the causal effect of Employment Protection Legislation (EPL) deregulation on the labor share, focusing on a key threat to identification.\n\n**Setting / Institutional Environment.** The analysis uses a panel of 26 advanced economies from 1970-2013. A key concern is the 'crisis-induces-reform' hypothesis, which suggests that liberalizing reforms are often enacted during economic downturns, potentially biasing the estimated effect of the reform.\n\n### Data / Model Specification\n\nThe paper estimates the aggregate dynamic response of the labor share to a reform using a country-level model. Table 1 presents results from the baseline model and a model that adds the lagged unemployment rate to control for economic crises.\n\n**Table 1: Country-Level Analysis – Labor Share Results**\n\n| Specification | Impact | 1y | 2y | 3y | 4y | 5y |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| **Baseline** | -0.26* | -0.60* | -0.95** | -0.70** | -0.75* | -0.87** |\n| | (0.13) | (0.34) | (0.39) | (0.32) | (0.40) | (0.35) |\n| **Controlling for lagged unemployment** | -0.23 | -0.62* | -0.94** | -0.74** | -0.75* | -0.84** |\n| | (0.14) | (0.33) | (0.37) | (0.33) | (0.39) | (0.33) |\n\n*Notes: Coefficients are the estimated effect in percentage points for a liberalizing reform. Standard errors in parentheses. *, **, *** indicate significance at the 90%, 95%, and 99% level.*\n\n### Question\n\nConsidering the 'crisis-induces-reform' hypothesis, what is the likely direction of the omitted variable bias in the 'Baseline' estimate, and what does the comparison with the 'Controlling for lagged unemployment' result imply about the causal interpretation of the reform's effect?", "Options": {"A": "The hypothesis implies a positive bias, understating the reform's negative effect. The robustness check supports a causal interpretation because the coefficient remains negative.", "B": "The hypothesis implies a negative bias, overstating the reform's negative effect. The robustness check supports a causal interpretation because the 5-year coefficient remains stable and significant (-0.87 vs. -0.84).", "C": "The hypothesis implies a negative bias, overstating the reform's negative effect. The robustness check undermines the causal interpretation because the impact coefficient (-0.23) is no longer statistically significant.", "D": "The hypothesis is not a threat to the baseline model because country fixed effects already control for economic crises."}, "Answer": "B", "pi_justification": "This item uses a Reverse-Reasoning strategy to test the student's ability to identify a specific endogeneity threat ('crisis-induces-reform'), predict the direction of the resulting bias, and correctly interpret a robustness check designed to mitigate it. Distractors are based on a sign error for the bias (A), a misinterpretation of the robustness check (C), and a conceptual error about the power of fixed effects (D).", "qid": "342", "question": "### Background\n\n**Research Question.** This question examines the paper's strategy for identifying the causal effect of Employment Protection Legislation (EPL) deregulation on the labor share, focusing on a key threat to identification.\n\n**Setting / Institutional Environment.** The analysis uses a panel of 26 advanced economies from 1970-2013. A key concern is the 'crisis-induces-reform' hypothesis, which suggests that liberalizing reforms are often enacted during economic downturns, potentially biasing the estimated effect of the reform.\n\n### Data / Model Specification\n\nThe paper estimates the aggregate dynamic response of the labor share to a reform using a country-level model. Table 1 presents results from the baseline model and a model that adds the lagged unemployment rate to control for economic crises.\n\n**Table 1: Country-Level Analysis – Labor Share Results**\n\n| Specification | Impact | 1y | 2y | 3y | 4y | 5y |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| **Baseline** | -0.26* | -0.60* | -0.95** | -0.70** | -0.75* | -0.87** |\n| | (0.13) | (0.34) | (0.39) | (0.32) | (0.40) | (0.35) |\n| **Controlling for lagged unemployment** | -0.23 | -0.62* | -0.94** | -0.74** | -0.75* | -0.84** |\n| | (0.14) | (0.33) | (0.37) | (0.33) | (0.39) | (0.33) |\n\n*Notes: Coefficients are the estimated effect in percentage points for a liberalizing reform. Standard errors in parentheses. *, **, *** indicate significance at the 90%, 95%, and 99% level.*\n\n### Question\n\nConsidering the 'crisis-induces-reform' hypothesis, what is the likely direction of the omitted variable bias in the 'Baseline' estimate, and what does the comparison with the 'Controlling for lagged unemployment' result imply about the causal interpretation of the reform's effect?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze the \"crisis-induces-reform\" hypothesis and its implications for omitted variable bias.\n\nThe \"crisis-induces-reform\" hypothesis suggests that:\n- Economic crises (downturns) lead to both lower labor share AND reform implementation\n- This creates a correlation between the reform variable and unobserved economic conditions\n\nLet me trace through the bias logic:\n1. Economic crises → Lower labor share (negative effect)\n2. Economic crises → More likely to implement EPL reforms (positive correlation)\n3. This means the reform variable is positively correlated with an omitted variable (crisis) that negatively affects labor share\n\nIn omitted variable bias: Bias = (correlation between included and omitted variable) × (effect of omitted variable on outcome)\n\nHere: Bias = (+) × (-) = negative bias\n\nThis negative bias would make the estimated reform effect more negative than the true causal effect, thus overstating the reform's negative impact.\n\nLooking at the results:\n- Baseline impact: -0.26* (significant)  \n- With unemployment control: -0.23 (not significant)\n- The 5-year effects remain very similar: -0.87** vs -0.84**\n\nThe key issue is that the impact coefficient loses statistical significance when controlling for lagged unemployment, which undermines confidence in the immediate causal effect of the reform.\n\nThe answer is C) The hypothesis implies a negative bias, overstating the reform's negative effect. The robustness check undermines the causal interpretation because the impact coefficient (-0.23) is no longer statistically significant.\n\nC", "model_answer": "C", "error": null}}
{"ID": 370, "Question": "### Background\n\n**Research Question.** When symmetric equilibria fail to exist due to demand uncertainty, what are the characteristics of the resulting asymmetric pure-strategy equilibria, and what strategic forces sustain them?\n\n**Setting / Institutional Environment.** We analyze a two-stage capacity-price game with a two-point demand distribution: demand is `\\underline{a}` with probability `θ` and `\\bar{a}` with probability `1-θ`. In an asymmetric equilibrium with a large firm (capacity `x`) and a small firm (capacity `y`), the small firm's marginal revenue of capacity is negative in the low-demand state but positive in the high-demand state.\n\n---\n\n### Data / Model Specification\n\nThe first-order condition for the small firm's optimal capacity choice `y` is given by:\n\n```latex\n\\frac{\\partial \\pi(y,x)}{\\partial y} = \\underbrace{ \\frac{\\theta}{4x}(\\underline{a}-y)(\\underline{a}-3y) }_{MR_{low}} + \\underbrace{ (1-\\theta)(\\bar{a}-x-2y) }_{MR_{high}} - c = 0\n\\quad \\text{(Eq. 1)}\n```\n\nAt an equilibrium `(x*, y*)`, the marginal revenue in the low-demand state, `MR_low`, is negative, while the marginal revenue in the high-demand state, `MR_high`, is positive.\n\nConsider an exogenous increase in `θ`, the probability of the low-demand state. How must the small firm's equilibrium capacity, `y*`, adjust to maintain the first-order condition, and what is the economic intuition?\n", "Options": {"A": "`y*` must decrease, because the overall expected marginal revenue of capacity falls as more weight is placed on the negative-revenue low-demand state.", "B": "`y*` must increase, because the firm needs to be more competitive in the low-demand state, which is now more frequent.", "C": "`y*` is unaffected, because the change in probability is already accounted for in the firm's reaction function.", "D": "`y*` must decrease, because the large firm will increase its capacity `x*`, forcing the small firm to reduce its own capacity."}, "Answer": "A", "pi_justification": "This question assesses the ability to perform comparative statics and interpret the result economically. It uses a Reverse-Reasoning strategy: given a change, infer the consequence. The correct answer identifies that increasing the weight `θ` on the negative `MR_low` term reduces the overall expected marginal revenue, requiring a decrease in `y` to restore optimality. Distractor B provides the opposite effect with flawed reasoning. Distractor C suggests no effect, a common misconception. Distractor D offers a plausible strategic story (a general equilibrium effect) but misses the direct, partial equilibrium mechanism described in the first-order condition.", "qid": "370", "question": "### Background\n\n**Research Question.** When symmetric equilibria fail to exist due to demand uncertainty, what are the characteristics of the resulting asymmetric pure-strategy equilibria, and what strategic forces sustain them?\n\n**Setting / Institutional Environment.** We analyze a two-stage capacity-price game with a two-point demand distribution: demand is `\\underline{a}` with probability `θ` and `\\bar{a}` with probability `1-θ`. In an asymmetric equilibrium with a large firm (capacity `x`) and a small firm (capacity `y`), the small firm's marginal revenue of capacity is negative in the low-demand state but positive in the high-demand state.\n\n---\n\n### Data / Model Specification\n\nThe first-order condition for the small firm's optimal capacity choice `y` is given by:\n\n```latex\n\\frac{\\partial \\pi(y,x)}{\\partial y} = \\underbrace{ \\frac{\\theta}{4x}(\\underline{a}-y)(\\underline{a}-3y) }_{MR_{low}} + \\underbrace{ (1-\\theta)(\\bar{a}-x-2y) }_{MR_{high}} - c = 0\n\\quad \\text{(Eq. 1)}\n```\n\nAt an equilibrium `(x*, y*)`, the marginal revenue in the low-demand state, `MR_low`, is negative, while the marginal revenue in the high-demand state, `MR_high`, is positive.\n\nConsider an exogenous increase in `θ`, the probability of the low-demand state. How must the small firm's equilibrium capacity, `y*`, adjust to maintain the first-order condition, and what is the economic intuition?\n", "model_evaluation_result": {"model_raw_response": "Looking at the first-order condition for the small firm's optimal capacity choice:\n\n$$\\frac{\\partial \\pi(y,x)}{\\partial y} = \\underbrace{ \\frac{\\theta}{4x}(\\underline{a}-y)(\\underline{a}-3y) }_{MR_{low}} + \\underbrace{ (1-\\theta)(\\bar{a}-x-2y) }_{MR_{high}} - c = 0$$\n\nAt equilibrium, we know that:\n- $MR_{low} < 0$ (negative marginal revenue in low-demand state)\n- $MR_{high} > 0$ (positive marginal revenue in high-demand state)\n\nWhen $\\theta$ increases (probability of low-demand state increases), the weight on the negative $MR_{low}$ term increases, while the weight on the positive $MR_{high}$ term decreases (since $1-\\theta$ decreases).\n\nThis means the overall expected marginal revenue of capacity falls. For the first-order condition to hold again at equilibrium, the firm must adjust $y*$ to restore the balance.\n\nSince $MR_{low}$ is negative and becomes less negative as $y$ decreases (the term $(\\underline{a}-y)(\\underline{a}-3y)$ becomes less negative), and $MR_{high}$ becomes more positive as $y$ decreases, the small firm must reduce its capacity $y*$ to maintain equilibrium.\n\nThe economic intuition is clear: as the probability of the unprofitable low-demand state increases, the small firm reduces its capacity investment because the expected return to capacity falls.\n\nA", "model_answer": "A", "error": null}}
{"ID": 390, "Question": "### Background\n\n**Research Question.** This problem evaluates the quality of forecasts from a parametric and a nonparametric model, specifically testing for forecast unbiasedness.\n\n**Setting.** The paper generates 30 one-step-ahead out-of-sample forecasts from both models and evaluates them by regressing the actual realized returns on the forecasts using the model: `Actual Return_t = a + b * Forecast_t + error_t`.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Out-of-Sample Forecast Evaluation**\n\n| Model | Constant (`a`) | Forecast (`b`) | R² | MSE |\n| :--- | :--- | :--- | :--- | :--- |\n| | | (Std. Error) | | |\n| Kernel | 0.0215 | 1.2493 | 0.0428 | 0.0056 |\n| | (0.0148) | (1.1160) | | |\n| Parametric | -0.0089 | 1.5370 | 0.0589 | 0.0050 |\n| | (0.0311) | (1.1614) | | |\n\n---\n\n### Question\n\nA common test for forecast unbiasedness checks if the slope coefficient `b` in the evaluation regression is equal to 1. Using the results in Table 1, what can be concluded about the slope bias of the two models at the 5% significance level (critical t-value ≈ ±1.96)?", "Options": {"A": "Neither the kernel model's slope (t ≈ 0.22) nor the parametric model's slope (t ≈ 0.46) is statistically different from 1.", "B": "Both the kernel model's slope and the parametric model's slope are statistically different from 1, indicating both forecasts are biased.", "C": "The kernel model's slope is not statistically different from 1, but the parametric model's slope is, indicating the parametric forecast is biased.", "D": "The parametric model's slope is not statistically different from 1, but the kernel model's slope is, indicating the kernel forecast is biased."}, "Answer": "A", "pi_justification": "Assesses the ability to perform a t-test for a specific null hypothesis (`H₀: b=1`) and interpret the result correctly. This is a Computational Judgment item. The correct option (A) requires correctly calculating the t-statistics for both models [Kernel: (1.2493-1)/1.1160 ≈ 0.22; Parametric: (1.5370-1)/1.1614 ≈ 0.46] and concluding that neither is significant. Distractors B, C, and D represent the other three possible outcomes, catching either calculation errors or misinterpretations of significance for one or both models.", "qid": "390", "question": "### Background\n\n**Research Question.** This problem evaluates the quality of forecasts from a parametric and a nonparametric model, specifically testing for forecast unbiasedness.\n\n**Setting.** The paper generates 30 one-step-ahead out-of-sample forecasts from both models and evaluates them by regressing the actual realized returns on the forecasts using the model: `Actual Return_t = a + b * Forecast_t + error_t`.\n\n---\n\n### Data / Model Specification\n\n**Table 1: Out-of-Sample Forecast Evaluation**\n\n| Model | Constant (`a`) | Forecast (`b`) | R² | MSE |\n| :--- | :--- | :--- | :--- | :--- |\n| | | (Std. Error) | | |\n| Kernel | 0.0215 | 1.2493 | 0.0428 | 0.0056 |\n| | (0.0148) | (1.1160) | | |\n| Parametric | -0.0089 | 1.5370 | 0.0589 | 0.0050 |\n| | (0.0311) | (1.1614) | | |\n\n---\n\n### Question\n\nA common test for forecast unbiasedness checks if the slope coefficient `b` in the evaluation regression is equal to 1. Using the results in Table 1, what can be concluded about the slope bias of the two models at the 5% significance level (critical t-value ≈ ±1.96)?", "model_evaluation_result": {"model_raw_response": "I need to test whether the slope coefficients are statistically different from 1 using a t-test.\n\nFor testing H₀: b = 1 vs H₁: b ≠ 1, the t-statistic is:\nt = (b̂ - 1) / SE(b̂)\n\nFor the Kernel model:\n- b̂ = 1.2493\n- SE(b̂) = 1.1160\n- t = (1.2493 - 1) / 1.1160 = 0.2493 / 1.1160 ≈ 0.22\n\nFor the Parametric model:\n- b̂ = 1.5370\n- SE(b̂) = 1.1614\n- t = (1.5370 - 1) / 1.1614 = 0.5370 / 1.1614 ≈ 0.46\n\nAt the 5% significance level with critical value ±1.96:\n- Kernel model: |0.22| < 1.96, so we fail to reject H₀ (slope not significantly different from 1)\n- Parametric model: |0.46| < 1.96, so we fail to reject H₀ (slope not significantly different from 1)\n\nBoth t-statistics are well below the critical value, indicating neither slope is statistically different from 1.\n\nA", "model_answer": "A", "error": null}}
{"ID": 369, "Question": "### Background\n\n**Research Question.** When symmetric equilibria fail to exist due to demand uncertainty, what are the characteristics of the resulting asymmetric pure-strategy equilibria, and what strategic forces sustain them?\n\n**Setting / Institutional Environment.** We analyze a two-stage capacity-price game with linear inverse demand `P(q,a) = a - q` and a two-point demand distribution: demand is `\\underline{a}` with probability `θ` and `\\bar{a}` with probability `1-θ`. We focus on an asymmetric equilibrium where firm 1 has a large capacity `x` and firm 2 has a small capacity `y` (`x > y`).\n\n---\n\n### Data / Model Specification\n\nIn the asymmetric equilibrium `(x,y)` with `x>y`, capacities are assumed to be in the mixed-strategy Region C(`\\underline{a}`) in the low-demand state and the pure-strategy Region A(`\\bar{a}`) in the high-demand state.\n\nThe relevant subgame revenues for the two states are:\n*   **Low demand (`\\underline{a}`):** Revenue for the large firm is `r(x,y,\\underline{a}) = (1/4)(\\underline{a}-y)^2`. Revenue for the small firm is `r(y,x,\\underline{a}) = (y/x) * (1/4)(\\underline{a}-y)^2`.\n*   **High demand (`\\bar{a}`):** Revenue for both firms is `q_i(\\bar{a}-x-y)`, where `q_i` is the firm's own capacity.\n\nThe expected profit for the small firm (firm 2, capacity `y`) is therefore:\n\n```latex\nπ(y,x) = θ \\left[ \\frac{y}{x} \\frac{(\\underline{a}-y)^2}{4} \\right] + (1-θ) \\left[ y(\\bar{a}-x-y) \\right] - cy\n\\quad \\text{(Eq. 1)}\n```\n\nWhich of the following equations represents the first-order condition (`∂π(y,x)/∂y = 0`) that implicitly defines the small firm's optimal capacity choice `y`?", "Options": {"A": "`(θ/(4x))(\\underline{a}-y)(\\underline{a}-3y) + (1-θ)(\\bar{a}-x-2y) - c = 0`", "B": "`(θ/(4x))(\\underline{a}-y)^2 + (1-θ)(\\bar{a}-x-2y) - c = 0`", "C": "`θ(1/2)(\\underline{a}-y) + (1-θ)(\\bar{a}-x-2y) - c = 0`", "D": "`(θ/(4x))(\\underline{a}-y)(\\underline{a}-3y) + (\\bar{a}-x-2y) - c = 0`"}, "Answer": "A", "pi_justification": "This question assesses the ability to correctly derive the first-order condition for the small firm in an asymmetric equilibrium. It uses a Computational Judgment strategy. The correct answer requires correctly applying the product rule to the low-demand revenue term. Distractor B represents a failure to apply the product rule correctly. Distractor C uses the wrong revenue function for the low-demand state (derivative of the large firm's revenue). Distractor D makes a conceptual error by omitting the probability weight `(1-θ)` on the high-demand state's marginal revenue.", "qid": "369", "question": "### Background\n\n**Research Question.** When symmetric equilibria fail to exist due to demand uncertainty, what are the characteristics of the resulting asymmetric pure-strategy equilibria, and what strategic forces sustain them?\n\n**Setting / Institutional Environment.** We analyze a two-stage capacity-price game with linear inverse demand `P(q,a) = a - q` and a two-point demand distribution: demand is `\\underline{a}` with probability `θ` and `\\bar{a}` with probability `1-θ`. We focus on an asymmetric equilibrium where firm 1 has a large capacity `x` and firm 2 has a small capacity `y` (`x > y`).\n\n---\n\n### Data / Model Specification\n\nIn the asymmetric equilibrium `(x,y)` with `x>y`, capacities are assumed to be in the mixed-strategy Region C(`\\underline{a}`) in the low-demand state and the pure-strategy Region A(`\\bar{a}`) in the high-demand state.\n\nThe relevant subgame revenues for the two states are:\n*   **Low demand (`\\underline{a}`):** Revenue for the large firm is `r(x,y,\\underline{a}) = (1/4)(\\underline{a}-y)^2`. Revenue for the small firm is `r(y,x,\\underline{a}) = (y/x) * (1/4)(\\underline{a}-y)^2`.\n*   **High demand (`\\bar{a}`):** Revenue for both firms is `q_i(\\bar{a}-x-y)`, where `q_i` is the firm's own capacity.\n\nThe expected profit for the small firm (firm 2, capacity `y`) is therefore:\n\n```latex\nπ(y,x) = θ \\left[ \\frac{y}{x} \\frac{(\\underline{a}-y)^2}{4} \\right] + (1-θ) \\left[ y(\\bar{a}-x-y) \\right] - cy\n\\quad \\text{(Eq. 1)}\n```\n\nWhich of the following equations represents the first-order condition (`∂π(y,x)/∂y = 0`) that implicitly defines the small firm's optimal capacity choice `y`?", "model_evaluation_result": {"model_raw_response": "I need to find the first-order condition by taking the derivative of the expected profit function with respect to y.\n\nGiven the expected profit function:\nπ(y,x) = θ[y/x · (ā-y)²/4] + (1-θ)[y(ā̄-x-y)] - cy\n\nLet me differentiate each term with respect to y:\n\nFor the first term: θ[y/x · (ā-y)²/4]\nUsing the product rule:\n∂/∂y[y/x · (ā-y)²/4] = (1/x) · (ā-y)²/4 + (y/x) · ∂/∂y[(ā-y)²/4]\n= (1/x) · (ā-y)²/4 + (y/x) · (1/4) · 2(ā-y)(-1)\n= (1/x) · (ā-y)²/4 - (y/x) · (ā-y)/2\n= (1/(4x))[(ā-y)² - 2y(ā-y)]\n= (1/(4x))(ā-y)[(ā-y) - 2y]\n= (1/(4x))(ā-y)(ā-3y)\n\nFor the second term: (1-θ)[y(ā̄-x-y)]\n∂/∂y[y(ā̄-x-y)] = (ā̄-x-y) + y(-1) = ā̄-x-2y\n\nFor the third term: -cy\n∂/∂y(-cy) = -c\n\nCombining all terms:\n∂π(y,x)/∂y = θ · (1/(4x))(ā-y)(ā-3y) + (1-θ)(ā̄-x-2y) - c\n\nSetting this equal to zero for the first-order condition:\n(θ/(4x))(ā-y)(ā-3y) + (1-θ)(", "model_answer": "I need to find the first-order condition by taking the derivative of the expected profit function with respect to y.\n\nGiven the expected profit function:\nπ(y,x) = θ[y/x · (ā-y)²/4] + (1-θ)[y(ā̄-x-y)] - cy\n\nLet me differentiate each term with respect to y:\n\nFor the first term: θ[y/x · (ā-y)²/4]\nUsing the product rule:\n∂/∂y[y/x · (ā-y)²/4] = (1/x) · (ā-y)²/4 + (y/x) · ∂/∂y[(ā-y)²/4]\n= (1/x) · (ā-y)²/4 + (y/x) · (1/4) · 2(ā-y)(-1)\n= (1/x) · (ā-y)²/4 - (y/x) · (ā-y)/2\n= (1/(4x))[(ā-y)² - 2y(ā-y)]\n= (1/(4x))(ā-y)[(ā-y) - 2y]\n= (1/(4x))(ā-y)(ā-3y)\n\nFor the second term: (1-θ)[y(ā̄-x-y)]\n∂/∂y[y(ā̄-x-y)] = (ā̄-x-y) + y(-1) = ā̄-x-2y\n\nFor the third term: -cy\n∂/∂y(-cy) = -c\n\nCombining all terms:\n∂π(y,x)/∂y = θ · (1/(4x))(ā-y)(ā-3y) + (1-θ)(ā̄-x-2y) - c\n\nSetting this equal to zero for the first-order condition:\n(θ/(4x))(ā-y)(ā-3y) + (1-θ)(", "error": "Warning: Could not extract single choice letter from: I need to find the first-order condition by taking..."}}
{"ID": 362, "Question": "### Background\n\n**Research Question.** This problem explores three competing theoretical models of social learning in a continuous action space: the rational benchmark (Perfect Bayesian Equilibrium), a model of information redundancy neglect (Best Response Trailing Naïve Inference), and a model of relative overconfidence.\n\n**Setting / Institutional Environment.** A sequence of agents `t=1, 2, ..., T` make decisions. Agent `t` observes the actions of all `t-1` predecessors and receives a private binary signal `s_t ∈ {0, 1}` with precision `q_t > 0.5` about a true state `V ∈ {0, 100}`. The agent's action `a_t ∈ [0, 100]` is their belief that `V=100`.\n\n### Data / Model Specification\n\nThe models make distinct predictions about an agent's posterior belief, expressed as a log-odds ratio, `ln(a_t / (100-a_t))`. Let `L_i = (2s_i - 1)ln(q_i / (1-q_i))` be the log-likelihood ratio of signal `s_i`.\n\n1.  **Perfect Bayesian Equilibrium (PBE):** Agents are fully rational and perfectly infer all past signals from past actions. The belief is the sum of all log-likelihood ratios up to time `t`.\n    ```latex\n    \\ln\\left(\\frac{a_{t}^{PBE}}{100-a_{t}^{PBE}}\\right) = \\sum_{i=1}^{t} L_i\n    ```\n\n2.  **Best Response Trailing Naïve Inference (BRTNI):** Agents naively believe predecessors act only on their private signal, leading to redundant information being repeatedly counted. The weight on a predecessor's signal `s_i` is `2^{t-i-1}`.\n    ```latex\n    \\ln\\left(\\frac{a_{t}^{BRTNI}}{100-a_{t}^{BRTNI}}\\right) = \\sum_{i=1}^{t-1} 2^{t-i-1} L_i + L_t\n    ```\n\n3.  **Overconfidence (OC):** Agents believe predecessors are less capable. They discount the inferred signals of all predecessors by a constant factor `k ∈ (0, 1)` but use their own signal correctly. This belief is common knowledge.\n    ```latex\n    \\ln\\left(\\frac{a_{t}^{OC}}{100-a_{t}^{OC}}\\right) = \\sum_{i=1}^{t-1} k L_i + L_t\n    ```\n\nConsider an agent at `t=3` where signal precision is constant at `q=0.7`. The realized sequence of signals is `{s_1=0, s_2=1, s_3=1}`. For the OC model, assume `k=0.5`. Which of the following correctly describes the agent's final log-odds belief (`Belief`) across the three models?", "Options": {"A": "`Belief_PBE` = `Belief_OC` > `Belief_BRTNI`", "B": "`Belief_PBE` > `Belief_OC` > `Belief_BRTNI`", "C": "`Belief_BRTNI` > `Belief_PBE` > `Belief_OC`", "D": "`Belief_PBE` > `Belief_BRTNI` > `Belief_OC`"}, "Answer": "A", "pi_justification": "This item assesses the ability to quantitatively apply the three core theoretical models. It uses the 'Computational Judgment' strategy, requiring students to perform multi-step calculations for each model and then compare the results. The scenario `{s_1=0, s_2=1, s_3=1}` is chosen specifically because it yields a non-obvious ranking (`PBE=OC > BRTNI`), which prevents simply guessing based on qualitative intuition. Distractor C is based on a different signal sequence, trapping students who pattern-match without recalculating. Distractor B reflects a plausible but incorrect qualitative intuition about the models.", "qid": "362", "question": "### Background\n\n**Research Question.** This problem explores three competing theoretical models of social learning in a continuous action space: the rational benchmark (Perfect Bayesian Equilibrium), a model of information redundancy neglect (Best Response Trailing Naïve Inference), and a model of relative overconfidence.\n\n**Setting / Institutional Environment.** A sequence of agents `t=1, 2, ..., T` make decisions. Agent `t` observes the actions of all `t-1` predecessors and receives a private binary signal `s_t ∈ {0, 1}` with precision `q_t > 0.5` about a true state `V ∈ {0, 100}`. The agent's action `a_t ∈ [0, 100]` is their belief that `V=100`.\n\n### Data / Model Specification\n\nThe models make distinct predictions about an agent's posterior belief, expressed as a log-odds ratio, `ln(a_t / (100-a_t))`. Let `L_i = (2s_i - 1)ln(q_i / (1-q_i))` be the log-likelihood ratio of signal `s_i`.\n\n1.  **Perfect Bayesian Equilibrium (PBE):** Agents are fully rational and perfectly infer all past signals from past actions. The belief is the sum of all log-likelihood ratios up to time `t`.\n    ```latex\n    \\ln\\left(\\frac{a_{t}^{PBE}}{100-a_{t}^{PBE}}\\right) = \\sum_{i=1}^{t} L_i\n    ```\n\n2.  **Best Response Trailing Naïve Inference (BRTNI):** Agents naively believe predecessors act only on their private signal, leading to redundant information being repeatedly counted. The weight on a predecessor's signal `s_i` is `2^{t-i-1}`.\n    ```latex\n    \\ln\\left(\\frac{a_{t}^{BRTNI}}{100-a_{t}^{BRTNI}}\\right) = \\sum_{i=1}^{t-1} 2^{t-i-1} L_i + L_t\n    ```\n\n3.  **Overconfidence (OC):** Agents believe predecessors are less capable. They discount the inferred signals of all predecessors by a constant factor `k ∈ (0, 1)` but use their own signal correctly. This belief is common knowledge.\n    ```latex\n    \\ln\\left(\\frac{a_{t}^{OC}}{100-a_{t}^{OC}}\\right) = \\sum_{i=1}^{t-1} k L_i + L_t\n    ```\n\nConsider an agent at `t=3` where signal precision is constant at `q=0.7`. The realized sequence of signals is `{s_1=0, s_2=1, s_3=1}`. For the OC model, assume `k=0.5`. Which of the following correctly describes the agent's final log-odds belief (`Belief`) across the three models?", "model_evaluation_result": {"model_raw_response": "I need to calculate the log-odds beliefs for agent t=3 under each model given the signal sequence {s₁=0, s₂=1, s₃=1} with q=0.7 and k=0.5 for the OC model.\n\nFirst, let me calculate the log-likelihood ratios:\n- L₁ = (2×0 - 1)ln(0.7/0.3) = -1×ln(7/3) = -ln(7/3)\n- L₂ = (2×1 - 1)ln(0.7/0.3) = 1×ln(7/3) = ln(7/3)  \n- L₃ = (2×1 - 1)ln(0.7/0.3) = 1×ln(7/3) = ln(7/3)\n\nNow I'll calculate each model's prediction:\n\n**PBE Model:**\nln(a₃^PBE/(100-a₃^PBE)) = L₁ + L₂ + L₃ = -ln(7/3) + ln(7/3) + ln(7/3) = ln(7/3)\n\n**BRTNI Model:**\nln(a₃^BRTNI/(100-a₃^BRTNI)) = 2^(3-1-1)L₁ + 2^(3-2-1)L₂ + L₃\n= 2¹L₁ + 2⁰L₂ + L₃\n= 2(-ln(7/3)) + 1(ln(7/3)) + ln(7/3)\n= -2ln(7/3) + ln(7/3) + ln(7/3)\n= 0\n\n**OC Model:**\nln(a₃^OC/(100-a₃^OC)) = k(L₁ + L₂) + L₃\n= 0.5(-ln(7/3) + ln(7/3)) + ln(7/3)\n= 0.5(0) + ln(", "model_answer": "I need to calculate the log-odds beliefs for agent t=3 under each model given the signal sequence {s₁=0, s₂=1, s₃=1} with q=0.7 and k=0.5 for the OC model.\n\nFirst, let me calculate the log-likelihood ratios:\n- L₁ = (2×0 - 1)ln(0.7/0.3) = -1×ln(7/3) = -ln(7/3)\n- L₂ = (2×1 - 1)ln(0.7/0.3) = 1×ln(7/3) = ln(7/3)  \n- L₃ = (2×1 - 1)ln(0.7/0.3) = 1×ln(7/3) = ln(7/3)\n\nNow I'll calculate each model's prediction:\n\n**PBE Model:**\nln(a₃^PBE/(100-a₃^PBE)) = L₁ + L₂ + L₃ = -ln(7/3) + ln(7/3) + ln(7/3) = ln(7/3)\n\n**BRTNI Model:**\nln(a₃^BRTNI/(100-a₃^BRTNI)) = 2^(3-1-1)L₁ + 2^(3-2-1)L₂ + L₃\n= 2¹L₁ + 2⁰L₂ + L₃\n= 2(-ln(7/3)) + 1(ln(7/3)) + ln(7/3)\n= -2ln(7/3) + ln(7/3) + ln(7/3)\n= 0\n\n**OC Model:**\nln(a₃^OC/(100-a₃^OC)) = k(L₁ + L₂) + L₃\n= 0.5(-ln(7/3) + ln(7/3)) + ln(7/3)\n= 0.5(0) + ln(", "error": "Warning: Could not extract single choice letter from: I need to calculate the log-odds beliefs for agent..."}}
{"ID": 305, "Question": "### Background\n\n**Research Question.** This problem explores the fundamental theoretical properties of parallel preference structures, starting from the general cost function and specializing to the tractable linear case, highlighting the key implications for consumer behavior and empirical analysis.\n\n**Setting / Institutional Environment.** We consider a standard consumer choice problem with N goods. The preference structure is described by its cost (expenditure) function. The analysis is restricted to interior solutions.\n\n**Variables & Parameters.**\n- `C(u,p)`: The cost function.\n- `u`: A scalar utility index.\n- `p`: An `N x 1` vector of prices.\n- `h(u)`, `δ_i`: Parameters/functions governing income effects.\n- `A(p)`: A function governing substitution effects.\n\n---\n\n### Data / Model Specification\n\nA **linear parallel structure** is a special case of the general parallel structure where the cost function takes the form:\n```latex\nC(u,p) = h(u) \\sum_{i=1}^{N} \\delta_i p_i + A(p) \n\n\\quad\\text{(Eq. 1)}\n```\n\n---\n\n### Question\n\nA key feature of the *linear* parallel structure, whose cost function is given in Eq. (1), is its implication for the shape of income-consumption curves (ICCs). What is this implication?", "Options": {"A": "All income-consumption curves are linear and parallel.", "B": "All income-consumption curves are linear and pass through the origin.", "C": "Income-consumption curves can be nonlinear, depending on the form of `A(p)`.", "D": "All goods are normal goods, so the income-consumption curves are always upward sloping."}, "Answer": "A", "pi_justification": "This item tests the main result of the specialization part of the original QA. The primary distractor (B) targets the classic and highly frequent misconception of confusing quasi-homothetic (linear parallel ICCs) with homothetic preferences.", "qid": "305", "question": "### Background\n\n**Research Question.** This problem explores the fundamental theoretical properties of parallel preference structures, starting from the general cost function and specializing to the tractable linear case, highlighting the key implications for consumer behavior and empirical analysis.\n\n**Setting / Institutional Environment.** We consider a standard consumer choice problem with N goods. The preference structure is described by its cost (expenditure) function. The analysis is restricted to interior solutions.\n\n**Variables & Parameters.**\n- `C(u,p)`: The cost function.\n- `u`: A scalar utility index.\n- `p`: An `N x 1` vector of prices.\n- `h(u)`, `δ_i`: Parameters/functions governing income effects.\n- `A(p)`: A function governing substitution effects.\n\n---\n\n### Data / Model Specification\n\nA **linear parallel structure** is a special case of the general parallel structure where the cost function takes the form:\n```latex\nC(u,p) = h(u) \\sum_{i=1}^{N} \\delta_i p_i + A(p) \n\n\\quad\\text{(Eq. 1)}\n```\n\n---\n\n### Question\n\nA key feature of the *linear* parallel structure, whose cost function is given in Eq. (1), is its implication for the shape of income-consumption curves (ICCs). What is this implication?", "model_evaluation_result": {"model_raw_response": "I need to analyze the implications of the linear parallel structure cost function for income-consumption curves (ICCs).\n\nGiven the cost function:\nC(u,p) = h(u) ∑ᵢ₌₁ᴺ δᵢpᵢ + A(p)\n\nTo find the demand functions, I use Shephard's lemma:\nxᵢ = ∂C(u,p)/∂pᵢ = h(u)δᵢ + ∂A(p)/∂pᵢ\n\nThis gives us the Hicksian demand functions. To get Marshallian demands, I need to substitute the indirect utility function, but the key insight comes from the structure itself.\n\nThe demand function has two components:\n1. h(u)δᵢ - this is the income effect component\n2. ∂A(p)/∂pᵢ - this is the substitution effect component\n\nFor income-consumption curves, I hold prices constant and vary income (which changes utility u). The key insight is that when prices are fixed:\n\n- The substitution component ∂A(p)/∂pᵢ remains constant\n- Only the income component h(u)δᵢ varies with utility/income\n\nSince h(u) is the same function for all goods, and δᵢ is a constant for each good, the income effect is proportional across all goods. This means that as income changes, the change in demand for each good is proportional to δᵢh'(u).\n\nThis proportionality means that the income-consumption curves are linear. Moreover, since this proportionality relationship holds for all goods simultaneously, these linear ICCs are parallel to each other - they have the same slope but different intercepts depending on the substitution effects.\n\nThe answer is A.\n\nA", "model_answer": "C", "error": null}}
{"ID": 309, "Question": "### Background\n\n**Research Question.** This problem compares the properties of two specific \"flexible functional forms\" derived from the linear parallel structure: the Parallel Linear Asymmetric Quadratic (PLAQ) and the Parallel Linear Generalized Linear (PLGL). It focuses on how their different mathematical structures lead to starkly different economic implications for substitution and satiation.\n\n**Setting / Institutional Environment.** We analyze a system of `N` goods where good 1 is the numeraire. All other prices (`p_i*`) are normalized by the price of the numeraire. The analysis focuses on the compensated (Hicksian) demand for the non-numeraire goods (`i, j > 1`).\n\n**Variables & Parameters.**\n- `φ^i(u,p*)`: Hicksian demand for non-numeraire good `i`.\n- `p_i*`: Normalized price of good `i`.\n\n---\n\n### Data / Model Specification\n\nThe Hicksian demand functions for non-numeraire goods (`i > 1`) for the two models are given by:\n\n**PLAQ Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + α_i + \\sum_{j=2}^{N} β_{ij} p_j^* \n\n\\quad\\text{(Eq. 1)}\n```\n\n**PLGL Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + \\frac{1}{2}α_i(p_i*)^{-1/2} + \\frac{1}{2}\\sum_{j=2}^{N} β_{ij} \\left(\\frac{p_j^*}{p_i^*}\\right)^{1/2}\n\n\\quad\\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nSuppose a researcher estimates a PLAQ model (i.e., a linear regression of quantity on price) when the true data generating process is PLGL. The PLGL's true substitution effect is large at low prices and small at high prices. How will the single estimated PLAQ coefficient for the price `p_j*` likely be biased as an estimate of the true substitution effect at very **low** price levels?", "Options": {"A": "It will underestimate the true substitution effect.", "B": "It will overestimate the true substitution effect.", "C": "It will be an unbiased estimate of the average substitution effect.", "D": "The direction of the bias cannot be determined without knowing the specific parameter values."}, "Answer": "A", "pi_justification": "This is a high-level Scenario Application question that directly converts the original apex question. It tests a deep understanding of functional form misspecification. The primary distractor (B) is the opposite error. Distractor C is 'Almost Right' but misses the point about bias at a specific price range. Distractor D suggests indecisiveness where a clear direction can be inferred.", "qid": "309", "question": "### Background\n\n**Research Question.** This problem compares the properties of two specific \"flexible functional forms\" derived from the linear parallel structure: the Parallel Linear Asymmetric Quadratic (PLAQ) and the Parallel Linear Generalized Linear (PLGL). It focuses on how their different mathematical structures lead to starkly different economic implications for substitution and satiation.\n\n**Setting / Institutional Environment.** We analyze a system of `N` goods where good 1 is the numeraire. All other prices (`p_i*`) are normalized by the price of the numeraire. The analysis focuses on the compensated (Hicksian) demand for the non-numeraire goods (`i, j > 1`).\n\n**Variables & Parameters.**\n- `φ^i(u,p*)`: Hicksian demand for non-numeraire good `i`.\n- `p_i*`: Normalized price of good `i`.\n\n---\n\n### Data / Model Specification\n\nThe Hicksian demand functions for non-numeraire goods (`i > 1`) for the two models are given by:\n\n**PLAQ Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + α_i + \\sum_{j=2}^{N} β_{ij} p_j^* \n\n\\quad\\text{(Eq. 1)}\n```\n\n**PLGL Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + \\frac{1}{2}α_i(p_i*)^{-1/2} + \\frac{1}{2}\\sum_{j=2}^{N} β_{ij} \\left(\\frac{p_j^*}{p_i^*}\\right)^{1/2}\n\n\\quad\\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nSuppose a researcher estimates a PLAQ model (i.e., a linear regression of quantity on price) when the true data generating process is PLGL. The PLGL's true substitution effect is large at low prices and small at high prices. How will the single estimated PLAQ coefficient for the price `p_j*` likely be biased as an estimate of the true substitution effect at very **low** price levels?", "model_evaluation_result": {"model_raw_response": "I need to analyze how a linear PLAQ model would approximate the nonlinear PLGL model, particularly focusing on the substitution effects at low prices.\n\nLet me examine the key differences between the models:\n\n**PLAQ Model (Equation 1):**\n- Linear in prices: `∑ β_{ij} p_j*`\n- Constant substitution effects (coefficients β_{ij} don't vary with price levels)\n\n**PLGL Model (Equation 2):**\n- Nonlinear in prices: `∑ β_{ij} (p_j*/p_i*)^{1/2}`\n- Variable substitution effects that depend on price levels\n\nTo find the substitution effect in PLGL, I need the partial derivative with respect to price:\n\nFor PLGL: `∂φ^i/∂p_j* = (1/2)β_{ij} × (1/2) × (p_j*)^{-1/2} × (p_i*)^{-1/2} = (1/4)β_{ij}(p_j*p_i*)^{-1/2}`\n\nThe problem states that \"PLGL's true substitution effect is large at low prices and small at high prices.\" This is consistent with the derivative above - as prices decrease, `(p_j*p_i*)^{-1/2}` increases, making the substitution effect larger.\n\nNow, when a researcher fits a linear PLAQ model to PLGL-generated data:\n- The PLAQ model assumes a constant coefficient β_{ij}\n- This constant coefficient will represent some average of the true varying substitution effects across the price range\n- At very low prices, the true PLGL substitution effect is large\n- The estimated constant PLAQ coefficient will be smaller than this large true effect at low prices (since it averages across all price levels, including higher prices where the true effect is smaller)\n\nTherefore, at very low price levels, the PLAQ coefficient will underestimate the true substitution effect.\n\nA", "model_answer": "A", "error": null}}
{"ID": 368, "Question": "### Background\n\n**Research Question.** This problem analyzes a monopolist's endogenous choice of contractual liability (`w`) and its interaction with product safety (`\\pi`), demonstrating how liability can serve as a tool for price discrimination and identifying when this private contracting fails, necessitating legal intervention.\n\n**Setting.** A monopolist can offer a product via a simple contract `(\\pi, p, w)`, where the stipulated damage payment `w` is a choice variable for the firm. The firm's optimal strategy and the resulting market efficiency depend on how consumer risk (`x`) correlates with their gross benefit, a relationship captured by the parameter `b`.\n\n### Data / Model Specification\n\n**Model Components.**\n- Consumer type `x` is distributed on `[\\underline{x}, \\overline{x}]`.\n- A type-`x` consumer's gross benefit is `b_0 + bx`.\n- The probability of an accident is `\\pi x`, and the harm is `h`.\n- The socially optimal safety level `\\pi^{**}` is defined by `W'(\\pi^{**}) = 0`.\n- The threshold `b^{**}` is defined as `b^{**} = \\pi^{**}h`.\n- A consumer's net benefit from the contract is: `NB(x) = b_0 + bx - \\pi(h-w)x - p`.\n\n### Question\n\nConsider the case where `0 < b \\le b^{**}`. In this regime, the firm can use the stipulated damage payment `w` as a tool for price discrimination. To maximize profits, the firm sets the safety level efficiently (`\\pi = \\pi^{**}`) and chooses a specific `w^*` that makes consumers' net benefit independent of their type `x`, thereby extracting all surplus. What is the firm's optimal choice for the damage payment, `w^*`?", "Options": {"A": "`w^* = h - b/\\pi^{**}`", "B": "`w^* = h`", "C": "`w^* = b/\\pi^{**}`", "D": "`w^* = 0`"}, "Answer": "A", "pi_justification": "This item assesses the student's ability to execute a key calculation from the paper, using a Computational Judgment strategy. The student must derive the value of `w` that achieves perfect price discrimination. The correct answer (A) comes from setting the coefficient of `x` in the net benefit function to zero. Distractor (B) is the socially optimal policy, not the firm's private choice. Distractor (C) is a Step-Omission Error, missing the `h` term. Distractor (D) is the firm's choice in the alternate `b > b^{**}` regime, a common conceptual mix-up.", "qid": "368", "question": "### Background\n\n**Research Question.** This problem analyzes a monopolist's endogenous choice of contractual liability (`w`) and its interaction with product safety (`\\pi`), demonstrating how liability can serve as a tool for price discrimination and identifying when this private contracting fails, necessitating legal intervention.\n\n**Setting.** A monopolist can offer a product via a simple contract `(\\pi, p, w)`, where the stipulated damage payment `w` is a choice variable for the firm. The firm's optimal strategy and the resulting market efficiency depend on how consumer risk (`x`) correlates with their gross benefit, a relationship captured by the parameter `b`.\n\n### Data / Model Specification\n\n**Model Components.**\n- Consumer type `x` is distributed on `[\\underline{x}, \\overline{x}]`.\n- A type-`x` consumer's gross benefit is `b_0 + bx`.\n- The probability of an accident is `\\pi x`, and the harm is `h`.\n- The socially optimal safety level `\\pi^{**}` is defined by `W'(\\pi^{**}) = 0`.\n- The threshold `b^{**}` is defined as `b^{**} = \\pi^{**}h`.\n- A consumer's net benefit from the contract is: `NB(x) = b_0 + bx - \\pi(h-w)x - p`.\n\n### Question\n\nConsider the case where `0 < b \\le b^{**}`. In this regime, the firm can use the stipulated damage payment `w` as a tool for price discrimination. To maximize profits, the firm sets the safety level efficiently (`\\pi = \\pi^{**}`) and chooses a specific `w^*` that makes consumers' net benefit independent of their type `x`, thereby extracting all surplus. What is the firm's optimal choice for the damage payment, `w^*`?", "model_evaluation_result": {"model_raw_response": "I need to analyze this monopolist's optimization problem where the firm uses the damage payment `w` as a price discrimination tool.\n\nGiven information:\n- Consumer type `x` distributed on `[\\underline{x}, \\overline{x}]`\n- Gross benefit for type-`x`: `b_0 + bx`\n- Accident probability: `\\pi x`\n- Harm from accident: `h`\n- Net benefit: `NB(x) = b_0 + bx - \\pi(h-w)x - p`\n- We're in the regime `0 < b ≤ b^{**}` where `b^{**} = \\pi^{**}h`\n- Firm sets `\\pi = \\pi^{**}` (efficient safety level)\n- Firm wants to make net benefit independent of type `x` to extract all surplus\n\nFor the firm to extract all consumer surplus through perfect price discrimination, it needs to make the net benefit independent of the consumer type `x`.\n\nLooking at the net benefit function:\n`NB(x) = b_0 + bx - \\pi(h-w)x - p`\n\nRearranging:\n`NB(x) = b_0 - p + x[b - \\pi(h-w)]`\n\nFor this to be independent of `x`, the coefficient of `x` must equal zero:\n`b - \\pi(h-w) = 0`\n\nSince the firm sets `\\pi = \\pi^{**}`, we have:\n`b - \\pi^{**}(h-w) = 0`\n\nSolving for `w`:\n`b = \\pi^{**}(h-w)`\n`b/\\pi^{**} = h-w`\n`w = h - b/\\pi^{**}`\n\nTherefore, `w^* = h - b/\\pi^{**}`.\n\nA", "model_answer": "A", "error": null}}
{"ID": 325, "Question": "### Background\n\nConsider a standard `n`-firm symmetric Cournot oligopoly. Each firm `j` chooses an output quantity `q_j` to maximize its profit. A symmetric noncooperative equilibrium is an output `ξ_n` such that if all `n` firms produce `ξ_n`, no firm has an incentive to deviate.\n\n### Data / Model Specification\n\n*   **Profit:** `π_j = q_j φ(∑_{i=1}^{n} q_i) - c(q_j)`\n*   **Costs:** `c'(q) > 0`, `c''(q) ≥ 0` (positive, non-decreasing marginal cost).\n*   **Demand:** `φ'(q) < 0`, `φ''(q) ≤ 0` (downward sloping, linear or concave demand).\n\n### Question\n\nAs the number of firms `n` in this symmetric Cournot model approaches infinity, to what value does the equilibrium market price, `P_n = φ(nξ_n)`, converge?", "Options": {"A": "Marginal cost at zero output, `c'(0)`.", "B": "The monopoly price.", "C": "Zero.", "D": "Average variable cost at the equilibrium output."}, "Answer": "A", "pi_justification": "This item assesses understanding of the classic result that the Cournot equilibrium converges to the perfectly competitive outcome. It tests a sharp, factual conclusion. Distractor B is the conceptual opposite (the `n=1` case). Distractor C is a plausible but incorrect guess. Distractor D targets the common confusion between marginal and average cost, a classic misconception.", "qid": "325", "question": "### Background\n\nConsider a standard `n`-firm symmetric Cournot oligopoly. Each firm `j` chooses an output quantity `q_j` to maximize its profit. A symmetric noncooperative equilibrium is an output `ξ_n` such that if all `n` firms produce `ξ_n`, no firm has an incentive to deviate.\n\n### Data / Model Specification\n\n*   **Profit:** `π_j = q_j φ(∑_{i=1}^{n} q_i) - c(q_j)`\n*   **Costs:** `c'(q) > 0`, `c''(q) ≥ 0` (positive, non-decreasing marginal cost).\n*   **Demand:** `φ'(q) < 0`, `φ''(q) ≤ 0` (downward sloping, linear or concave demand).\n\n### Question\n\nAs the number of firms `n` in this symmetric Cournot model approaches infinity, to what value does the equilibrium market price, `P_n = φ(nξ_n)`, converge?", "model_evaluation_result": {"model_raw_response": "I need to analyze what happens to the equilibrium price in a symmetric Cournot oligopoly as the number of firms approaches infinity.\n\nIn a symmetric Cournot equilibrium, each firm chooses output to maximize profit given that all other firms choose the same output level ξ_n.\n\nThe first-order condition for firm j is:\n∂π_j/∂q_j = φ(nξ_n) + ξ_n φ'(nξ_n) - c'(ξ_n) = 0\n\nThis gives us:\nφ(nξ_n) + ξ_n φ'(nξ_n) = c'(ξ_n)\n\nRearranging:\nφ(nξ_n) = c'(ξ_n) - ξ_n φ'(nξ_n)\n\nAs n → ∞, I need to determine what happens to ξ_n and consequently to P_n = φ(nξ_n).\n\nFrom the first-order condition, as n increases, the term ξ_n φ'(nξ_n) represents the strategic effect of one firm's output on price. Since φ'(q) < 0, this term is negative.\n\nAs n → ∞, if the equilibrium is to remain interior (ξ_n > 0), the strategic effect ξ_n φ'(nξ_n) must approach zero. This happens when either:\n1. ξ_n → 0, or\n2. The total output nξ_n approaches a finite limit\n\nFor the market to clear and firms to have positive output, we need nξ_n to remain bounded. As n → ∞, individual firm output ξ_n must approach zero, but total output Q_n = nξ_n approaches some limit.\n\nIn the limit, the first-order condition becomes:\nφ(Q_∞) = c'(0)\n\nThis means the price converges to the marginal cost at zero output.\n\nThis result makes economic sense: as the number of firms becomes very large, the market becomes perfectly competitive, and price equals marg", "model_answer": "I need to analyze what happens to the equilibrium price in a symmetric Cournot oligopoly as the number of firms approaches infinity.\n\nIn a symmetric Cournot equilibrium, each firm chooses output to maximize profit given that all other firms choose the same output level ξ_n.\n\nThe first-order condition for firm j is:\n∂π_j/∂q_j = φ(nξ_n) + ξ_n φ'(nξ_n) - c'(ξ_n) = 0\n\nThis gives us:\nφ(nξ_n) + ξ_n φ'(nξ_n) = c'(ξ_n)\n\nRearranging:\nφ(nξ_n) = c'(ξ_n) - ξ_n φ'(nξ_n)\n\nAs n → ∞, I need to determine what happens to ξ_n and consequently to P_n = φ(nξ_n).\n\nFrom the first-order condition, as n increases, the term ξ_n φ'(nξ_n) represents the strategic effect of one firm's output on price. Since φ'(q) < 0, this term is negative.\n\nAs n → ∞, if the equilibrium is to remain interior (ξ_n > 0), the strategic effect ξ_n φ'(nξ_n) must approach zero. This happens when either:\n1. ξ_n → 0, or\n2. The total output nξ_n approaches a finite limit\n\nFor the market to clear and firms to have positive output, we need nξ_n to remain bounded. As n → ∞, individual firm output ξ_n must approach zero, but total output Q_n = nξ_n approaches some limit.\n\nIn the limit, the first-order condition becomes:\nφ(Q_∞) = c'(0)\n\nThis means the price converges to the marginal cost at zero output.\n\nThis result makes economic sense: as the number of firms becomes very large, the market becomes perfectly competitive, and price equals marg", "error": "Warning: Could not extract single choice letter from: I need to analyze what happens to the equilibrium ..."}}
{"ID": 376, "Question": "### Background\n\n**Research Question.** What is the fundamental trade-off between providing ex-ante incentives (effort) and ensuring ex-post selection efficiency (accuracy) when choosing between informed and blind review?\n\n**Setting and Sample.** The setting is a specific numerical example from the paper with two equally likely agent types, `θ = 1/2` (low-ability) and `θ = 3/2` (high-ability). The mean ability `E[θ] = 1` is set to equal the critical type `θ*`, which represents an ideal condition for blind review to boost incentives. The signal technology is based on triangular densities on [0, 1], and key parameters are set to `υ=c=u=1`.\n\n**Variables and Parameters.**\n- `θ`: Agent's ability type, `θ ∈ {1/2, 3/2}`.\n- `s`: The acceptance standard.\n- `p`: The agent's effort level (probability of high quality).\n- `Pr{ACCEPT|q=l}`: Probability of a false acceptance (Type I error).\n- `Pr{REJECT|q=h}`: Probability of a false rejection (Type II error).\n\n---\n\n### Data / Model Specification\n\nThe signal `σ` is drawn from one of two triangular densities on [0, 1]: `f_h(σ) = 2σ` if project quality is high, or `f_l(σ) = 2(1-σ)` if it is low. This implies the cumulative distribution functions are `F_h(s) = s²` and `F_l(s) = 1 - (1-s)² = 2s - s²`.\n\nThe probability of a false acceptance for a given agent is `(1-p) * (1-F_l(s))` and a false rejection is `p * F_h(s)`.\n\n**Table 1: Equilibrium Outcomes in a Two-Type Example**\n\n| | INFORMED | BLIND |\n| :--- | :---: | :---: |\n| **STANDARD** | |\n| `θ = 1/2` | 1 | 1/2 |\n| `θ = 3/2` | 1/3 | 1/2 |\n| **EFFORT** | |\n| `θ = 1/2` | 0 | 1/4 |\n| `θ = 3/2` | 2/3 | 3/4 |\n\n---\n\n### Question\n\nBased on the data in Table 1 and the provided formulas, what is the average probability of the principal making an error (either a false acceptance or a false rejection) under the **Informed Review** regime?", "Options": {"A": "2/9", "B": "1/9", "C": "1/4", "D": "0"}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests the student's ability to perform a precise multi-step calculation based on model formulas and data. It uses a Computational Judgment strategy. The correct answer (B) requires calculating the error probability for each agent type under the Informed regime and then correctly averaging them. Distractors are designed based on common errors: (A) and (D) represent failing to average the two types' error rates (a step-omission error), while (C) represents calculating the value for the wrong regime (a comprehension slip).", "qid": "376", "question": "### Background\n\n**Research Question.** What is the fundamental trade-off between providing ex-ante incentives (effort) and ensuring ex-post selection efficiency (accuracy) when choosing between informed and blind review?\n\n**Setting and Sample.** The setting is a specific numerical example from the paper with two equally likely agent types, `θ = 1/2` (low-ability) and `θ = 3/2` (high-ability). The mean ability `E[θ] = 1` is set to equal the critical type `θ*`, which represents an ideal condition for blind review to boost incentives. The signal technology is based on triangular densities on [0, 1], and key parameters are set to `υ=c=u=1`.\n\n**Variables and Parameters.**\n- `θ`: Agent's ability type, `θ ∈ {1/2, 3/2}`.\n- `s`: The acceptance standard.\n- `p`: The agent's effort level (probability of high quality).\n- `Pr{ACCEPT|q=l}`: Probability of a false acceptance (Type I error).\n- `Pr{REJECT|q=h}`: Probability of a false rejection (Type II error).\n\n---\n\n### Data / Model Specification\n\nThe signal `σ` is drawn from one of two triangular densities on [0, 1]: `f_h(σ) = 2σ` if project quality is high, or `f_l(σ) = 2(1-σ)` if it is low. This implies the cumulative distribution functions are `F_h(s) = s²` and `F_l(s) = 1 - (1-s)² = 2s - s²`.\n\nThe probability of a false acceptance for a given agent is `(1-p) * (1-F_l(s))` and a false rejection is `p * F_h(s)`.\n\n**Table 1: Equilibrium Outcomes in a Two-Type Example**\n\n| | INFORMED | BLIND |\n| :--- | :---: | :---: |\n| **STANDARD** | |\n| `θ = 1/2` | 1 | 1/2 |\n| `θ = 3/2` | 1/3 | 1/2 |\n| **EFFORT** | |\n| `θ = 1/2` | 0 | 1/4 |\n| `θ = 3/2` | 2/3 | 3/4 |\n\n---\n\n### Question\n\nBased on the data in Table 1 and the provided formulas, what is the average probability of the principal making an error (either a false acceptance or a false rejection) under the **Informed Review** regime?", "model_evaluation_result": {"model_raw_response": "I need to calculate the average probability of error under the Informed Review regime using the data from Table 1 and the given formulas.\n\nFrom Table 1, under Informed Review:\n- For θ = 1/2: standard s = 1, effort p = 0\n- For θ = 3/2: standard s = 1/3, effort p = 2/3\n\nThe probability of error for each type consists of:\n1. False acceptance: (1-p) × (1-F_l(s))\n2. False rejection: p × F_h(s)\n\nGiven the formulas:\n- F_h(s) = s²\n- F_l(s) = 2s - s²\n\nFor θ = 1/2 (s = 1, p = 0):\n- False acceptance: (1-0) × (1-F_l(1)) = 1 × (1-(2×1-1²)) = 1 × (1-1) = 0\n- False rejection: 0 × F_h(1) = 0 × 1² = 0\n- Total error probability = 0 + 0 = 0\n\nFor θ = 3/2 (s = 1/3, p = 2/3):\n- F_l(1/3) = 2×(1/3) - (1/3)² = 2/3 - 1/9 = 6/9 - 1/9 = 5/9\n- F_h(1/3) = (1/3)² = 1/9\n- False acceptance: (1-2/3) × (1-5/9) = 1/3 × 4/9 = 4/27\n- False rejection: 2/3 × 1/9 = 2/27\n- Total error probability = 4/27 + 2/27 = 6/27 = 2/9\n\nSince both types are equally likely (probability 1/2 each), the average probability of error is:\n(1/2) × 0 + (1/2) × (2/9) = ", "model_answer": "I need to calculate the average probability of error under the Informed Review regime using the data from Table 1 and the given formulas.\n\nFrom Table 1, under Informed Review:\n- For θ = 1/2: standard s = 1, effort p = 0\n- For θ = 3/2: standard s = 1/3, effort p = 2/3\n\nThe probability of error for each type consists of:\n1. False acceptance: (1-p) × (1-F_l(s))\n2. False rejection: p × F_h(s)\n\nGiven the formulas:\n- F_h(s) = s²\n- F_l(s) = 2s - s²\n\nFor θ = 1/2 (s = 1, p = 0):\n- False acceptance: (1-0) × (1-F_l(1)) = 1 × (1-(2×1-1²)) = 1 × (1-1) = 0\n- False rejection: 0 × F_h(1) = 0 × 1² = 0\n- Total error probability = 0 + 0 = 0\n\nFor θ = 3/2 (s = 1/3, p = 2/3):\n- F_l(1/3) = 2×(1/3) - (1/3)² = 2/3 - 1/9 = 6/9 - 1/9 = 5/9\n- F_h(1/3) = (1/3)² = 1/9\n- False acceptance: (1-2/3) × (1-5/9) = 1/3 × 4/9 = 4/27\n- False rejection: 2/3 × 1/9 = 2/27\n- Total error probability = 4/27 + 2/27 = 6/27 = 2/9\n\nSince both types are equally likely (probability 1/2 each), the average probability of error is:\n(1/2) × 0 + (1/2) × (2/9) =", "error": "Warning: Could not extract single choice letter from: I need to calculate the average probability of err..."}}
{"ID": 327, "Question": "### Background\n\n**Research Question.** This problem involves the application of a fitted nonlinear model to a counterfactual scenario to understand its dynamic properties.\n\n**Setting.** A Threshold Autoregressive Moving Average (TARMA) model was fitted to the Greenland ice sheet mass balance data to capture its asymmetric seasonal cycle.\n\n### Data / Model Specification\n\nThe estimated TARMA model is:\n\n```latex\nX_t = 0.46 X_{t-1} + 0.75 \\varepsilon_{t-1} + \\text{RegimeComponent}_t + \\varepsilon_t\n```\nwhere:\n- If `X_{t-6} \\leq -5.86` (Lower Regime): `\\text{RegimeComponent}_t = 56.39 + 0.21 X_{t-6} + 0.30 X_{t-11}`\n- If `X_{t-6} > -5.86` (Upper Regime): `\\text{RegimeComponent}_t = -24.14 + 0.21 X_{t-12} - ...`\n\nConsider a counterfactual scenario where an unusually severe summer melt season leaves the ice sheet mass balance at `X_{t-6} = -150`. According to the model, which regime will be active six months later, and what does this imply about the system's stability?", "Options": {"A": "The Lower Regime will be active, implying a stabilizing, mean-reverting feedback.", "B": "The Upper Regime will be active, implying a stabilizing, mean-reverting feedback.", "C": "The Lower Regime will be active, implying an unstable, explosive feedback.", "D": "The Upper Regime will be active, implying an unstable, explosive feedback."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the ability to apply the estimated model to a new scenario and draw conclusions about the system's dynamics. It uses a **Scenario Application** strategy. The distractors are designed based on a matrix of plausible errors: (B) a numerical sign error in applying the threshold rule, (C) a conceptual error in interpreting the feedback mechanism (conceptual opposite), and (D) a combination of both errors.", "qid": "327", "question": "### Background\n\n**Research Question.** This problem involves the application of a fitted nonlinear model to a counterfactual scenario to understand its dynamic properties.\n\n**Setting.** A Threshold Autoregressive Moving Average (TARMA) model was fitted to the Greenland ice sheet mass balance data to capture its asymmetric seasonal cycle.\n\n### Data / Model Specification\n\nThe estimated TARMA model is:\n\n```latex\nX_t = 0.46 X_{t-1} + 0.75 \\varepsilon_{t-1} + \\text{RegimeComponent}_t + \\varepsilon_t\n```\nwhere:\n- If `X_{t-6} \\leq -5.86` (Lower Regime): `\\text{RegimeComponent}_t = 56.39 + 0.21 X_{t-6} + 0.30 X_{t-11}`\n- If `X_{t-6} > -5.86` (Upper Regime): `\\text{RegimeComponent}_t = -24.14 + 0.21 X_{t-12} - ...`\n\nConsider a counterfactual scenario where an unusually severe summer melt season leaves the ice sheet mass balance at `X_{t-6} = -150`. According to the model, which regime will be active six months later, and what does this imply about the system's stability?", "model_evaluation_result": {"model_raw_response": "I need to analyze this TARMA model to determine which regime will be active and what it implies about system stability.\n\nGiven information:\n- The threshold value is -5.86\n- We have X_{t-6} = -150 (an unusually severe summer melt)\n- Lower Regime: If X_{t-6} ≤ -5.86\n- Upper Regime: If X_{t-6} > -5.86\n\nFirst, let me determine which regime will be active:\nSince X_{t-6} = -150, and -150 ≤ -5.86, the Lower Regime will be active.\n\nNow I need to analyze the stability implications. In the Lower Regime:\nRegimeComponent_t = 56.39 + 0.21 X_{t-6} + 0.30 X_{t-11}\n\nThe key insight is that when X_{t-6} = -150 (a very negative value representing severe mass loss), the regime component becomes:\n56.39 + 0.21(-150) + 0.30 X_{t-11} = 56.39 - 31.5 + 0.30 X_{t-11} = 24.89 + 0.30 X_{t-11}\n\nThis shows that when the ice sheet experiences severe mass loss (very negative values), the Lower Regime adds a positive component (24.89 plus additional terms), which works to counteract the negative deviation. This is characteristic of mean-reverting behavior - when the system moves far from equilibrium in one direction, the regime dynamics push it back toward the mean.\n\nThe coefficient 0.21 on X_{t-6} in the Lower Regime is positive, which means that as X_{t-6} becomes more negative (more severe mass loss), it contributes a more negative term, but the large positive intercept (56.39) dominates, creating a stabilizing effect.\n\nThis represents a stabilizing, mean-reverting feedback mechanism where extreme negative deviations trigger dynamics that help restore the system toward equilibrium.\n\nA", "model_answer": "A", "error": null}}
{"ID": 377, "Question": "In the model, the **exhaustion level** `s_0` is the public belief `\\tilde{s}` at which the number of hospital visitors exactly equals the hospital's capacity `\\bar{n}`. For any true disease severity `s > s_0`, a state of congestion would arise under full information, which is socially inefficient.\n\nA benevolent social planner's goal is to induce a public belief `\\tilde{s}^*` that makes agents in the decentralized equilibrium choose to visit the hospital in a way that is ex-post socially optimal. What is this optimal public belief `\\tilde{s}^*` when the true severity is `s > s_0`?", "Options": {"A": "`s_0`", "B": "`s`", "C": "`c`", "D": "A value strictly greater than `s` to signal the high danger."}, "Answer": "A", "pi_justification": "This item uses the Computational Judgment strategy to replace parts (b) and (c) of a QA problem. It requires the student to first reason that the socially optimal allocation is for the `\\bar{n}` highest-risk agents to visit (filling capacity perfectly), and then to identify the unique public belief that induces this precise behavior in equilibrium. The correct answer is `s_0`. Distractors represent common conceptual errors: (B) is the belief under full disclosure, which is inefficient; (C) is a key parameter but not the target belief; (D) represents a misunderstanding of how to manage congestion.", "qid": "377", "question": "In the model, the **exhaustion level** `s_0` is the public belief `\\tilde{s}` at which the number of hospital visitors exactly equals the hospital's capacity `\\bar{n}`. For any true disease severity `s > s_0`, a state of congestion would arise under full information, which is socially inefficient.\n\nA benevolent social planner's goal is to induce a public belief `\\tilde{s}^*` that makes agents in the decentralized equilibrium choose to visit the hospital in a way that is ex-post socially optimal. What is this optimal public belief `\\tilde{s}^*` when the true severity is `s > s_0`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand the key elements:\n\n1. **Exhaustion level** `s_0`: The public belief where hospital visitors exactly equal capacity `\\bar{n}`\n2. **True severity** `s > s_0`: Would cause congestion under full information (socially inefficient)\n3. **Social planner's goal**: Induce optimal public belief `\\tilde{s}^*` for ex-post social optimality\n\nThe key insight is that when the true severity `s > s_0`, full information would lead to congestion (more than `\\bar{n}` visitors), which is socially inefficient.\n\nFor ex-post social optimality, the planner wants exactly `\\bar{n}` people to visit the hospital - utilizing full capacity without congestion.\n\nSince `s_0` is defined as the belief level that generates exactly `\\bar{n}` visitors (hospital capacity), setting the public belief to `\\tilde{s}^* = s_0` would:\n\n- Induce exactly `\\bar{n}` people to visit\n- Avoid congestion that would occur with truthful revelation of `s > s_0`\n- Achieve the socially optimal outcome of full capacity utilization without inefficient overcrowding\n\nThe other options don't work:\n- Option B (`s`): Would cause congestion since `s > s_0`\n- Option C (`c`): Unclear what `c` represents, likely a cost parameter\n- Option D: Would exacerbate the congestion problem\n\nA", "model_answer": "B", "error": null}}
{"ID": 324, "Question": "### Background\n\nConsider a standard `n`-firm symmetric Cournot oligopoly. Each firm `j` chooses an output quantity `q_j` to maximize its profit. A symmetric noncooperative equilibrium is an output `ξ_k` such that if all `k` firms produce `ξ_k`, no firm has an incentive to deviate.\n\n### Data / Model Specification\n\n*   **Profit:** `π_j = q_j φ(∑_{i=1}^{n} q_i) - c(q_j)`\n*   **Costs:** `c'(q) > 0`, `c''(q) ≥ 0` (positive, non-decreasing marginal cost).\n*   **Demand:** `φ'(q) < 0`, `φ''(q) ≤ 0` (downward sloping, linear or concave demand).\n\n### Question\n\nIf the number of firms in the industry increases from `n` to `m` (where `m > n`), how do the equilibrium individual firm output (`ξ`) and total industry output (`Q`) change?", "Options": {"A": "Individual output `ξ` decreases, and total output `Q` increases.", "B": "Individual output `ξ` increases, and total output `Q` increases.", "C": "Individual output `ξ` decreases, and total output `Q` decreases.", "D": "Individual output `ξ` increases, and total output `Q` decreases."}, "Answer": "A", "pi_justification": "This item assesses knowledge of the fundamental comparative statics of the Cournot model, which is a prerequisite for understanding the entry game. It tests a sharp, factual conclusion. The distractors represent the three possible combinations of sign errors, targeting the common misconception that more firms might lead to lower total output, or that firms might produce more individually to compete.", "qid": "324", "question": "### Background\n\nConsider a standard `n`-firm symmetric Cournot oligopoly. Each firm `j` chooses an output quantity `q_j` to maximize its profit. A symmetric noncooperative equilibrium is an output `ξ_k` such that if all `k` firms produce `ξ_k`, no firm has an incentive to deviate.\n\n### Data / Model Specification\n\n*   **Profit:** `π_j = q_j φ(∑_{i=1}^{n} q_i) - c(q_j)`\n*   **Costs:** `c'(q) > 0`, `c''(q) ≥ 0` (positive, non-decreasing marginal cost).\n*   **Demand:** `φ'(q) < 0`, `φ''(q) ≤ 0` (downward sloping, linear or concave demand).\n\n### Question\n\nIf the number of firms in the industry increases from `n` to `m` (where `m > n`), how do the equilibrium individual firm output (`ξ`) and total industry output (`Q`) change?", "model_evaluation_result": {"model_raw_response": "I need to analyze how equilibrium output changes when the number of firms increases in a symmetric Cournot oligopoly.\n\nIn a symmetric Cournot equilibrium, each firm chooses output to maximize profit given others' outputs. The first-order condition for firm j is:\n\n∂π_j/∂q_j = φ(Q) + q_j φ'(Q) - c'(q_j) = 0\n\nwhere Q = Σq_i is total output.\n\nIn symmetric equilibrium, all firms produce the same quantity ξ, so Q = nξ.\n\nThe equilibrium condition becomes:\nφ(nξ) + ξφ'(nξ) - c'(ξ) = 0\n\nThis can be rewritten as:\nφ(nξ) + ξφ'(nξ) = c'(ξ)\n\nNow, when the number of firms increases from n to m (m > n):\n\n1) **Individual firm output (ξ)**: \n   - With more firms, each firm faces more competition\n   - The marginal revenue from producing an additional unit decreases because the impact on market price is shared among more competitors\n   - This leads each firm to reduce its individual output\n   - Therefore, ξ decreases\n\n2) **Total industry output (Q)**:\n   - While each firm produces less, there are more firms\n   - The net effect depends on which dominates: the decrease in individual output or the increase in number of firms\n   - In standard Cournot models with the given assumptions, the increase in the number of firms dominates\n   - More firms lead to more competitive behavior, driving total output closer to the competitive level\n   - Therefore, Q = mξ_m > nξ_n, so total output increases\n\nThis is a well-established result in Cournot competition theory: more firms lead to lower individual output but higher total output, moving the market closer to perfect competition.\n\nA", "model_answer": "A", "error": null}}
{"ID": 363, "Question": "## Background\n\n**Research Question.** This problem investigates the causal relationship between sectoral reallocation shocks, measured by Cross-Section Volatility (CSV), and the aggregate unemployment rate (UN). A key challenge is potential omitted variable bias from aggregate economic forces like the stock market return (MRET).\n\n## Data / Model Specification\n\nAssume the true model for unemployment is:\n```latex\nU_t = \\alpha_0 + \\alpha_1 CSV_t + \\alpha_2 MRET_t + u_t\n```\nwhere theory suggests \\(\\alpha_1 > 0\\) (reallocation increases unemployment) and \\(\\alpha_2 < 0\\) (a booming market reduces unemployment).\n\nInstead, a researcher estimates the simple model:\n```latex\nU_t = \\beta_0 + \\beta_1 CSV_t + \\epsilon_t\n```\n\n**Table 1. Selected Correlations**\n\n| | CSV | MRET | UN |\n| :--- | :--- | :--- | :--- |\n| **CSV** | 1.000 | | |\n| **MRET**| -0.400| 1.000 | |\n| **UN** | 0.216 | -0.324| 1.000 |\n\n## Question\n\nGiven the information above, what is the direction of the omitted variable bias for the estimator \\(\\hat{\\beta}_1\\) in the simple model? In other words, does omitting MRET lead to an over- or under-estimation of the true effect of CSV on unemployment?", "Options": {"A": "The effect is overestimated because the omitted variable (MRET) is negatively correlated with the included variable (CSV) and negatively correlated with the outcome (UN).", "B": "The effect is underestimated because the omitted variable (MRET) is negatively correlated with the included variable (CSV).", "C": "The effect is underestimated because the omitted variable (MRET) is negatively correlated with the outcome (UN).", "D": "The effect is unbiased because the correlation between CSV and MRET is not statistically significant at conventional levels."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the ability to formally derive the direction of omitted variable bias, a core econometrics skill. Strategy: Computational Judgment. Distractor Logic: The correct answer (A) requires correctly identifying the two components of bias (corr(X,Z) and corr(Z,Y)) and their signs. Distractor (B) makes an error by only considering the first component. Distractor (C) makes an error by only considering the second component. Distractor (D) offers a plausible but incorrect conceptual escape hatch.", "qid": "363", "question": "## Background\n\n**Research Question.** This problem investigates the causal relationship between sectoral reallocation shocks, measured by Cross-Section Volatility (CSV), and the aggregate unemployment rate (UN). A key challenge is potential omitted variable bias from aggregate economic forces like the stock market return (MRET).\n\n## Data / Model Specification\n\nAssume the true model for unemployment is:\n```latex\nU_t = \\alpha_0 + \\alpha_1 CSV_t + \\alpha_2 MRET_t + u_t\n```\nwhere theory suggests \\(\\alpha_1 > 0\\) (reallocation increases unemployment) and \\(\\alpha_2 < 0\\) (a booming market reduces unemployment).\n\nInstead, a researcher estimates the simple model:\n```latex\nU_t = \\beta_0 + \\beta_1 CSV_t + \\epsilon_t\n```\n\n**Table 1. Selected Correlations**\n\n| | CSV | MRET | UN |\n| :--- | :--- | :--- | :--- |\n| **CSV** | 1.000 | | |\n| **MRET**| -0.400| 1.000 | |\n| **UN** | 0.216 | -0.324| 1.000 |\n\n## Question\n\nGiven the information above, what is the direction of the omitted variable bias for the estimator \\(\\hat{\\beta}_1\\) in the simple model? In other words, does omitting MRET lead to an over- or under-estimation of the true effect of CSV on unemployment?", "model_evaluation_result": {"model_raw_response": "Looking at this omitted variable bias problem, I need to apply the standard formula for the bias in the coefficient when a relevant variable is omitted.\n\nThe bias in β̂₁ is given by:\nBias(β̂₁) = α₂ × Corr(CSV, MRET) × [SD(MRET)/SD(CSV)]\n\nFrom the given information:\n- α₂ < 0 (true coefficient on MRET is negative)\n- Corr(CSV, MRET) = -0.400 (negative correlation)\n- Standard deviations are positive by definition\n\nTherefore:\nBias(β̂₁) = (negative) × (negative) × (positive) = positive\n\nThis means E[β̂₁] = α₁ + positive bias > α₁\n\nSince the bias is positive, the estimator β̂₁ will be larger than the true effect α₁, meaning the effect is overestimated.\n\nLooking at the logic: MRET is negatively correlated with both CSV (-0.400) and UN (-0.324). When we omit MRET from the regression, some of its negative effect on unemployment gets incorrectly attributed to CSV, making CSV appear to have a stronger positive effect on unemployment than it actually does.\n\nOption A correctly identifies that the effect is overestimated and provides the correct reasoning: the omitted variable (MRET) is negatively correlated with both the included variable (CSV) and the outcome (UN).\n\nA", "model_answer": "A", "error": null}}
{"ID": 311, "Question": "### Background\n\n**Research Question.** This problem investigates the causal effect of an income shock on the timing of fertility and how the choice of estimation strategy is critical for identification. A key concern is that unobserved factors, such as a farmer's innate ability, might be correlated with both the likelihood of experiencing a shock and fertility decisions, biasing simple correlations.\n\n**Setting / Institutional Environment.** The analysis uses a panel of women from the Kagera region of Tanzania. The income shock is a dummy variable for whether the household experienced an accidental crop loss. The outcome of interest is whether a woman gave birth in the 7 months since the last survey round. For a birth to occur, conception must have happened at least 9 months prior, so the relevant shock is one that occurred 7-14 months before the current survey.\n\n### Data / Model Specification\n\nThe study's preferred estimation strategy is a woman-level fixed effects (FE) linear probability model, which controls for all time-invariant individual characteristics (`μᵢ`). The model is:\n\n```latex\nY_{i,t} = \\beta_1 S_{i,t} + \\alpha_t + \\mu_i + \\varepsilon_{i,t} \\quad \\text{(Eq. 1)}\n```\n\nWhere `Y_{i,t}` is the birth outcome and `S_{i,t}` is the crop loss shock. Table 1 presents results from three specifications for the dependent variable `Birth since last survey`: a simple OLS model, a model with community-level fixed effects, and the preferred woman-level fixed effects model.\n\n**Table 1: The Effects of Crop Loss on Likelihood of a Recent Birth**\n\n| | OLS | Community FE | Woman FE |\n| :--- | :---: | :---: | :---: |\n| Crop loss (7-14 months) | -0.115*** | -0.130*** | -0.175*** |\n| | (0.038) | (0.040) | (0.049) |\n| Observations | 747 | 747 | 747 |\n\n*Note: Robust standard errors in parentheses. *** p<0.01.*\n\n### The Question\n\nThe pattern in Table 1—where the estimated effect of a crop loss on births becomes more negative as more granular fixed effects are added—suggests the OLS estimate is biased toward zero. According to the paper's reasoning, what unobserved factor and associated correlations explain this positive bias? Let 'Ability' be an unobserved, time-invariant characteristic of the woman/household.", "Options": {"A": "Higher Ability is associated with a *lower* likelihood of crop loss, and higher Ability is associated with a *higher* likelihood of birth. This creates a negative bias.", "B": "Higher Ability is associated with a *higher* likelihood of crop loss, and higher Ability is associated with a *lower* likelihood of birth. This creates a negative bias.", "C": "Higher Ability is associated with a *higher* likelihood of crop loss, and higher Ability is associated with a *higher* likelihood of birth. This creates a positive bias.", "D": "Higher Ability is associated with a *lower* likelihood of crop loss, and higher Ability is associated with a *lower* likelihood of birth. This creates a positive bias."}, "Answer": "D", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item provides a sharp test of the student's ability to apply the omitted variable bias formula. It uses a Reverse-Reasoning strategy, asking for the underlying causal story that generates the observed empirical pattern. The distractors are systematically designed by incorrectly signing the two key correlations (`Corr(Shock, Ability)` and `Corr(Birth, Ability)`), which represent the most common and predictable errors in this type of analysis.", "qid": "311", "question": "### Background\n\n**Research Question.** This problem investigates the causal effect of an income shock on the timing of fertility and how the choice of estimation strategy is critical for identification. A key concern is that unobserved factors, such as a farmer's innate ability, might be correlated with both the likelihood of experiencing a shock and fertility decisions, biasing simple correlations.\n\n**Setting / Institutional Environment.** The analysis uses a panel of women from the Kagera region of Tanzania. The income shock is a dummy variable for whether the household experienced an accidental crop loss. The outcome of interest is whether a woman gave birth in the 7 months since the last survey round. For a birth to occur, conception must have happened at least 9 months prior, so the relevant shock is one that occurred 7-14 months before the current survey.\n\n### Data / Model Specification\n\nThe study's preferred estimation strategy is a woman-level fixed effects (FE) linear probability model, which controls for all time-invariant individual characteristics (`μᵢ`). The model is:\n\n```latex\nY_{i,t} = \\beta_1 S_{i,t} + \\alpha_t + \\mu_i + \\varepsilon_{i,t} \\quad \\text{(Eq. 1)}\n```\n\nWhere `Y_{i,t}` is the birth outcome and `S_{i,t}` is the crop loss shock. Table 1 presents results from three specifications for the dependent variable `Birth since last survey`: a simple OLS model, a model with community-level fixed effects, and the preferred woman-level fixed effects model.\n\n**Table 1: The Effects of Crop Loss on Likelihood of a Recent Birth**\n\n| | OLS | Community FE | Woman FE |\n| :--- | :---: | :---: | :---: |\n| Crop loss (7-14 months) | -0.115*** | -0.130*** | -0.175*** |\n| | (0.038) | (0.040) | (0.049) |\n| Observations | 747 | 747 | 747 |\n\n*Note: Robust standard errors in parentheses. *** p<0.01.*\n\n### The Question\n\nThe pattern in Table 1—where the estimated effect of a crop loss on births becomes more negative as more granular fixed effects are added—suggests the OLS estimate is biased toward zero. According to the paper's reasoning, what unobserved factor and associated correlations explain this positive bias? Let 'Ability' be an unobserved, time-invariant characteristic of the woman/household.", "model_evaluation_result": {"model_raw_response": "I need to analyze the bias pattern in the regression results and determine what unobserved factor could explain why the OLS estimate is biased toward zero (positive bias).\n\nFrom Table 1, I observe:\n- OLS: -0.115\n- Community FE: -0.130  \n- Woman FE: -0.175\n\nThe coefficient becomes more negative (larger in absolute value) as we add more granular fixed effects. This means the OLS estimate (-0.115) is less negative than the \"true\" effect (-0.175), indicating the OLS estimate is biased toward zero (positive bias).\n\nFor there to be a positive bias in OLS, the omitted variable (Ability) must be correlated with both the treatment (crop loss) and outcome (birth) in a way that creates this upward bias.\n\nLet me think about the bias formula: Bias = β₂ × Corr(X₁, X₂)\n\nWhere:\n- β₂ is the effect of the omitted variable (Ability) on the outcome (birth)\n- Corr(X₁, X₂) is the correlation between the included variable (crop loss) and omitted variable (Ability)\n\nFor positive bias (making the coefficient less negative), we need:\n- Either both correlations positive, or both negative\n\nLooking at the options:\n\nA) Lower ability → lower crop loss (negative correlation)\n   Higher ability → higher birth (positive correlation)\n   This gives negative bias (negative × positive = negative)\n\nB) Higher ability → higher crop loss (positive correlation)  \n   Higher ability → lower birth (negative correlation)\n   This gives negative bias (positive × negative = negative)\n\nC) Higher ability → higher crop loss (positive correlation)\n   Higher ability → higher birth (positive correlation)  \n   This gives positive bias (positive × positive = positive)\n\nD) Higher ability → lower crop loss (negative correlation)\n   Higher ability → lower birth (negative correlation)\n   This gives positive bias (negative × negative = positive)\n\nBoth C and D would create positive bias. However, thinking about the economic context:\n\n- Higher ability farmers would likely be LESS prone to accidental crop losses (better farming practices, risk management)\n- Higher ability individuals might delay childbearing for career/education reasons, or alternatively,", "model_answer": "A", "error": null}}
{"ID": 347, "Question": "### Background\n\nIn a general equilibrium model with heterogeneous firms, a free-entry condition for new firms must hold, which determines the market wage rate `w`. \n\n### Data / Model Specification\n\nThe equilibrium wage rate `w` is determined by the free-entry condition, which states that if entry is positive, the expected value of a new firm (which starts with `k=0`) must be zero:\n```latex\n\\int p(0,z;w)\\varphi(d z) = 0 \n```\nwhere `φ(z)` is the distribution of initial productivity draws for entrants.\n\nThe model's structure implies the following property for the value function:\n**Proposition:** The value function `p(k,z;w)` is continuous and strictly decreasing in the wage rate `w`.\n\n### Question\n\nSuppose the government introduces a lump-sum tax `T > 0` that must be paid by any firm upon entry. How would this policy affect the economy's stationary equilibrium wage rate `w*`, and what is the economic intuition?", "Options": {"A": "The wage `w*` will decrease, because the tax makes entry less attractive, reducing labor demand and thus lowering the wage required to restore the zero-profit entry condition.", "B": "The wage `w*` will increase, because firms pass the cost of the tax on to workers in the form of lower net profits, which must be offset by higher wages to attract labor.", "C": "The wage `w*` will not change, because the tax is a fixed cost that only affects the entry decision, not the operating decisions of incumbent firms who set the market wage.", "D": "The wage `w*` will decrease, because the tax reduces the cash flow of entering firms, leaving them with less money to pay worker salaries."}, "Answer": "A", "pi_justification": "This item tests the user's ability to reason about the model's general equilibrium properties, directly targeting the logic of the original QA's apex question. It uses a 'Scenario Application' strategy, asking for the effect of a new policy (a tax). The distractors are designed based on common errors in equilibrium analysis: B) a simple sign error with flawed reasoning (Sign Error), C) incorrectly concluding no effect by ignoring the entry margin (Conceptual Opposite), and D) providing the correct outcome but with flawed, partial-equilibrium reasoning instead of the correct market-clearing logic (Almost Right).", "qid": "347", "question": "### Background\n\nIn a general equilibrium model with heterogeneous firms, a free-entry condition for new firms must hold, which determines the market wage rate `w`. \n\n### Data / Model Specification\n\nThe equilibrium wage rate `w` is determined by the free-entry condition, which states that if entry is positive, the expected value of a new firm (which starts with `k=0`) must be zero:\n```latex\n\\int p(0,z;w)\\varphi(d z) = 0 \n```\nwhere `φ(z)` is the distribution of initial productivity draws for entrants.\n\nThe model's structure implies the following property for the value function:\n**Proposition:** The value function `p(k,z;w)` is continuous and strictly decreasing in the wage rate `w`.\n\n### Question\n\nSuppose the government introduces a lump-sum tax `T > 0` that must be paid by any firm upon entry. How would this policy affect the economy's stationary equilibrium wage rate `w*`, and what is the economic intuition?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze how a lump-sum entry tax affects the equilibrium wage in a heterogeneous firms model with free entry.\n\nThe key insight is understanding the free-entry condition:\n∫ p(0,z;w)φ(dz) = 0\n\nThis condition must hold in equilibrium - the expected value of entering firms must be zero.\n\nWhen a lump-sum tax T > 0 is introduced upon entry, the free-entry condition becomes:\n∫ p(0,z;w)φ(dz) - T = 0\n\nOr equivalently:\n∫ p(0,z;w)φ(dz) = T\n\nSince the proposition states that p(k,z;w) is strictly decreasing in w, this means:\n- Higher wages reduce firm values\n- Lower wages increase firm values\n\nBefore the tax: Expected firm value = 0\nAfter the tax: Expected firm value must = T > 0 to compensate for the tax\n\nTo achieve this higher expected firm value (T instead of 0), and given that firm values decrease with wages, the wage must fall.\n\nThe economic intuition: The entry tax makes entry less attractive. To restore the zero-profit entry condition (accounting for the tax), firms need higher expected values. Since firm values are decreasing in wages, this requires lower wages. Lower wages reduce labor costs, making firms more profitable and offsetting the entry tax cost.\n\nThis matches option A: wages decrease because the tax makes entry less attractive, reducing labor demand and lowering the equilibrium wage needed to satisfy the free-entry condition.\n\nA", "model_answer": "A", "error": null}}
{"ID": 381, "Question": "### Background\n\nIn social choice theory, a conflict exists between the Pareto criterion (efficiency) and equity criteria like no-envy. The paper explores lexicographic principles to resolve this. This question focuses on the **equity-first, efficiency-second principle**.\n\n### Data / Model Specification\n\nKey social preference relations are:\n\n1.  **The Pareto Relation (`≻_P`):** `x ≻_P y` if `x` is a Pareto improvement over `y`. `≻_P` is transitive.\n2.  **The Equity Relation (`≻_F`):** Based on the set of envy instances `H(x)`, `x ≻_F y` if and only if `#H(x) < #H(y)`.\n3.  **The Equity-First Relation (`≻_FP`):** This relation ranks allocation `x` over `y` as follows:\n\n    ```latex\n    x \\succ_{FP} y \\iff (x \\succ_F y) \\lor (\\#H(x) = \\#H(y) \\text{ and } x \\succ_P y) \\quad \\text{(Eq. (1))}\n    ```\n\n### Question\n\nIn the proof that the equity-first relation `≻_FP` is transitive, one must show that if `x ≻_FP y` and `y ≻_FP z`, then `x ≻_FP z`. Consider the specific case where the tie-breaking rule is used for both initial comparisons: `#H(x) = #H(y)` with `x ≻_P y`, AND `#H(y) = #H(z)` with `y ≻_P z`. Why does it necessarily follow that `x ≻_FP z`?", "Options": {"A": "Because `#H(x) = #H(z)` holds, and `x ≻_P z` follows from the transitivity of the Pareto relation `≻_P`.", "B": "Because the number of envy instances must be decreasing, so `x ≻_F z` must be true.", "C": "The relationship between `x` and `z` is indeterminate without knowing the specific utility functions.", "D": "Because any allocation that is Pareto-superior to another must also be more equitable."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to probe understanding of the most complex step in the transitivity proof for `≻_FP`. Instead of reproducing the proof, the student must identify the critical logical link—the transitivity of the underlying Pareto relation—that makes the argument work. Distractors target specific logical failures: B confuses the primary and secondary criteria, C represents a failure to apply the transitivity property (Step-Omission Error), and D posits a false relationship between efficiency and equity.", "qid": "381", "question": "### Background\n\nIn social choice theory, a conflict exists between the Pareto criterion (efficiency) and equity criteria like no-envy. The paper explores lexicographic principles to resolve this. This question focuses on the **equity-first, efficiency-second principle**.\n\n### Data / Model Specification\n\nKey social preference relations are:\n\n1.  **The Pareto Relation (`≻_P`):** `x ≻_P y` if `x` is a Pareto improvement over `y`. `≻_P` is transitive.\n2.  **The Equity Relation (`≻_F`):** Based on the set of envy instances `H(x)`, `x ≻_F y` if and only if `#H(x) < #H(y)`.\n3.  **The Equity-First Relation (`≻_FP`):** This relation ranks allocation `x` over `y` as follows:\n\n    ```latex\n    x \\succ_{FP} y \\iff (x \\succ_F y) \\lor (\\#H(x) = \\#H(y) \\text{ and } x \\succ_P y) \\quad \\text{(Eq. (1))}\n    ```\n\n### Question\n\nIn the proof that the equity-first relation `≻_FP` is transitive, one must show that if `x ≻_FP y` and `y ≻_FP z`, then `x ≻_FP z`. Consider the specific case where the tie-breaking rule is used for both initial comparisons: `#H(x) = #H(y)` with `x ≻_P y`, AND `#H(y) = #H(z)` with `y ≻_P z`. Why does it necessarily follow that `x ≻_FP z`?", "model_evaluation_result": {"model_raw_response": "Looking at this step-by-step:\n\nGiven conditions:\n- `#H(x) = #H(y)` with `x ≻_P y` (so `x ≻_FP y` by tie-breaking)\n- `#H(y) = #H(z)` with `y ≻_P z` (so `y ≻_FP z` by tie-breaking)\n\nTo show: `x ≻_FP z`\n\nFrom the given conditions:\n- `#H(x) = #H(y)` and `#H(y) = #H(z)`\n- Therefore, by transitivity of equality: `#H(x) = #H(z)`\n\nSince we have `x ≻_P y` and `y ≻_P z`, and the Pareto relation `≻_P` is stated to be transitive, we get:\n- `x ≻_P z`\n\nNow applying the definition of `≻_FP`:\nSince `#H(x) = #H(z)` and `x ≻_P z`, by equation (1), we have `x ≻_FP z`.\n\nThis follows exactly the logic in option A: we have equal numbers of envy instances (`#H(x) = #H(z)`), and the Pareto superiority (`x ≻_P z`) follows from transitivity of the Pareto relation.\n\nA", "model_answer": "A", "error": null}}
{"ID": 375, "Question": "### Background\n\n**Research Question.** What is the fundamental trade-off between providing ex-ante incentives (effort) and ensuring ex-post selection efficiency (accuracy) when choosing between informed and blind review?\n\n**Setting and Sample.** The setting is a specific numerical example from the paper with two equally likely agent types, `θ = 1/2` (low-ability) and `θ = 3/2` (high-ability). The mean ability `E[θ] = 1` is set to equal the critical type `θ*`, which represents an ideal condition for blind review to boost incentives. The signal technology is based on triangular densities on [0, 1], and key parameters are set to `υ=c=u=1`.\n\n**Variables and Parameters.**\n- `θ`: Agent's ability type, `θ ∈ {1/2, 3/2}`.\n- `s`: The acceptance standard.\n- `p`: The agent's effort level (probability of high quality).\n\n---\n\n### Data / Model Specification\n\nThe signal `σ` is drawn from one of two triangular densities on [0, 1]: `f_h(σ) = 2σ` if project quality is high, or `f_l(σ) = 2(1-σ)` if it is low. This implies the cumulative distribution functions are `F_h(s) = s²` and `F_l(s) = 1 - (1-s)² = 2s - s²`.\n\nThe agent's utility function is `U(p,s;θ) = u[p(1-F_h(s))+(1-p)(1-F_l(s))] - C(p;θ)`, where the cost of effort is `C(p;θ) = p²/(2θ)` and the payoff from acceptance is `u=1`.\n\n**Table 1: Equilibrium Outcomes in a Two-Type Example**\n\n| | INFORMED | BLIND |\n| :--- | :---: | :---: |\n| **STANDARD** | |\n| `θ = 1/2` | 1 | 1/2 |\n| `θ = 3/2` | 1/3 | 1/2 |\n| **EFFORT** | |\n| `θ = 1/2` | 0 | 1/4 |\n| `θ = 3/2` | 2/3 | 3/4 |\n\n---\n\n### Question\n\nBased on the information provided, which of the following statements most accurately describes the welfare outcomes for the agents when switching from an informed to a blind review regime?", "Options": {"A": "Blind review improves the low-type agent's utility but harms the high-type agent's utility.", "B": "Blind review harms the low-type agent's utility but improves the high-type agent's utility.", "C": "Blind review elicits higher effort from both agent types, leading to a higher average probability of error for the principal.", "D": "The switch to blind review is a Pareto improvement from the agents' perspective, as both prefer the fairer, uniform standard."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's ability to synthesize the numerical results of the model to draw a conclusion about agent welfare. It uses a Scenario Application strategy. The correct answer (A) requires performing the utility calculations from the original QA problem. Distractor (B) is the conceptual opposite. Distractor (C) presents a true but incomplete statement about effort and error rates, testing if the student focuses on the specific question of agent utility. Distractor (D) targets a common misconception by conflating the normative concept of 'fairness' with the technical definition of a Pareto improvement.", "qid": "375", "question": "### Background\n\n**Research Question.** What is the fundamental trade-off between providing ex-ante incentives (effort) and ensuring ex-post selection efficiency (accuracy) when choosing between informed and blind review?\n\n**Setting and Sample.** The setting is a specific numerical example from the paper with two equally likely agent types, `θ = 1/2` (low-ability) and `θ = 3/2` (high-ability). The mean ability `E[θ] = 1` is set to equal the critical type `θ*`, which represents an ideal condition for blind review to boost incentives. The signal technology is based on triangular densities on [0, 1], and key parameters are set to `υ=c=u=1`.\n\n**Variables and Parameters.**\n- `θ`: Agent's ability type, `θ ∈ {1/2, 3/2}`.\n- `s`: The acceptance standard.\n- `p`: The agent's effort level (probability of high quality).\n\n---\n\n### Data / Model Specification\n\nThe signal `σ` is drawn from one of two triangular densities on [0, 1]: `f_h(σ) = 2σ` if project quality is high, or `f_l(σ) = 2(1-σ)` if it is low. This implies the cumulative distribution functions are `F_h(s) = s²` and `F_l(s) = 1 - (1-s)² = 2s - s²`.\n\nThe agent's utility function is `U(p,s;θ) = u[p(1-F_h(s))+(1-p)(1-F_l(s))] - C(p;θ)`, where the cost of effort is `C(p;θ) = p²/(2θ)` and the payoff from acceptance is `u=1`.\n\n**Table 1: Equilibrium Outcomes in a Two-Type Example**\n\n| | INFORMED | BLIND |\n| :--- | :---: | :---: |\n| **STANDARD** | |\n| `θ = 1/2` | 1 | 1/2 |\n| `θ = 3/2` | 1/3 | 1/2 |\n| **EFFORT** | |\n| `θ = 1/2` | 0 | 1/4 |\n| `θ = 3/2` | 2/3 | 3/4 |\n\n---\n\n### Question\n\nBased on the information provided, which of the following statements most accurately describes the welfare outcomes for the agents when switching from an informed to a blind review regime?", "model_evaluation_result": {"model_raw_response": "I need to analyze the welfare outcomes for both agent types when switching from informed to blind review.\n\nLet me calculate the utility for each agent type under both regimes using the utility function:\nU(p,s;θ) = u[p(1-F_h(s))+(1-p)(1-F_l(s))] - C(p;θ)\n\nWhere:\n- F_h(s) = s²\n- F_l(s) = 2s - s²\n- C(p;θ) = p²/(2θ)\n- u = 1\n\n**For θ = 1/2 (low-type):**\n\nInformed review: s = 1, p = 0\n- U = 0[1-(1)²] + (1-0)[1-(2(1)-1²)] - 0²/(2×1/2) = 0 + 0 - 0 = 0\n\nBlind review: s = 1/2, p = 1/4\n- U = (1/4)[1-(1/2)²] + (3/4)[1-(2(1/2)-(1/2)²)] - (1/4)²/(2×1/2)\n- U = (1/4)(3/4) + (3/4)(1/4) - 1/16\n- U = 3/16 + 3/16 - 1/16 = 5/16\n\n**For θ = 3/2 (high-type):**\n\nInformed review: s = 1/3, p = 2/3\n- U = (2/3)[1-(1/3)²] + (1/3)[1-(2(1/3)-(1/3)²)] - (2/3)²/(2×3/2)\n- U = (2/3)(8/9) + (1/3)(1/9) - (4/9)/3\n- U = 16/27 + 1/27 - 4/27 = 13/27\n\nBlind review: s = 1/2, p = 3/4\n- U = (3/", "model_answer": "C", "error": null}}
{"ID": 460, "Question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of estimating a linear water demand function when households face non-linear, multi-block price schedules.\n\n**Setting / Institutional Environment.** To address endogeneity in a linear demand model, several instrumental variable (IV) strategies are attempted.\n\n---\n\n### Data / Model Specification\n\n**Table 1** presents the estimated price coefficient (`α̂`) from three different IV specifications for the linear demand model `w_it = αP_it + βX_it + u_it`.\n\n**Table 1: Price Coefficient Estimates Using Alternative IV Specifications**\n\n| Specification | (1) | (2) | (3) | (4) |\n| :--- | :--- | :--- | :--- | :--- |\n| **Baseline** | -0.620 (0.024) | -0.608 (0.024) | -0.551 (0.023) | -0.554 (0.023) |\n| **Olmstead (2009)** | 0.006 (0.293) | -0.245 (0.427) | -0.359 (0.348) | -0.500 (0.268) |\n| **McFadden (1977)** | 0.641 (0.038) | -0.561 (0.036) | -0.584 (0.033) | -0.610 (0.037) |\n\n*Notes: Robust standard errors in parentheses. Columns (1)-(4) differ by the set of control variables included.*\n\n---\n\n### Question\n\nBased on the results in Table 1, what is the strongest piece of evidence that the Instrumental Variable (IV) estimates of the price coefficient are unreliable?", "Options": {"A": "The estimates are not robust; they change dramatically in sign, magnitude, and statistical significance depending on the specific IV specification used.", "B": "The significant estimates are consistently negative, which is consistent with economic theory and suggests the IV strategy is generally successful.", "C": "The standard errors in the 'Olmstead (2009)' specification are very large, which is the primary indicator of a failed estimation.", "D": "The 'Baseline' specification provides stable and statistically significant negative coefficients, confirming the validity of at least one IV approach."}, "Answer": "A", "pi_justification": "This question assesses the ability to interpret regression output to critique a model, replacing Q2 of the source QA. It uses a Computational Judgment strategy. The correct answer (A) identifies the core problem of non-robustness. Distractor B (Misinterpretation) cherry-picks the results that fit theory. Distractor C (Step-Omission Error) focuses on one symptom (large SEs) rather than the overall pattern of instability. Distractor D (Misinterpretation) focuses only on the 'best' specification while ignoring the contradictory evidence from the others, which is the key takeaway.", "qid": "460", "question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of estimating a linear water demand function when households face non-linear, multi-block price schedules.\n\n**Setting / Institutional Environment.** To address endogeneity in a linear demand model, several instrumental variable (IV) strategies are attempted.\n\n---\n\n### Data / Model Specification\n\n**Table 1** presents the estimated price coefficient (`α̂`) from three different IV specifications for the linear demand model `w_it = αP_it + βX_it + u_it`.\n\n**Table 1: Price Coefficient Estimates Using Alternative IV Specifications**\n\n| Specification | (1) | (2) | (3) | (4) |\n| :--- | :--- | :--- | :--- | :--- |\n| **Baseline** | -0.620 (0.024) | -0.608 (0.024) | -0.551 (0.023) | -0.554 (0.023) |\n| **Olmstead (2009)** | 0.006 (0.293) | -0.245 (0.427) | -0.359 (0.348) | -0.500 (0.268) |\n| **McFadden (1977)** | 0.641 (0.038) | -0.561 (0.036) | -0.584 (0.033) | -0.610 (0.037) |\n\n*Notes: Robust standard errors in parentheses. Columns (1)-(4) differ by the set of control variables included.*\n\n---\n\n### Question\n\nBased on the results in Table 1, what is the strongest piece of evidence that the Instrumental Variable (IV) estimates of the price coefficient are unreliable?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 382, "Question": "### Background\n\nIn social choice theory, a conflict exists between the Pareto criterion (efficiency) and equity criteria like no-envy. The paper explores lexicographic principles to resolve this. This question focuses on the **equity-first, efficiency-second principle**.\n\n### Data / Model Specification\n\nThe **Equity-First Relation (`≻_FP`)** prioritizes equity (fewer envy instances) first, and uses Pareto superiority as a tie-breaker. An allocation `x*` is a **maximal element** for this relation if no other feasible allocation `y` exists such that `y ≻_FP x*`. The set of maximal elements is denoted `M_{≻_FP}(Z)`.\n\nThe paper shows `M_{≻_FP}(Z) = P[F(Z)]`, where `F(Z)` is the set of all envy-free feasible allocations and `P[Y]` denotes the set of Pareto-optimal allocations *within* a given set `Y`.\n\n### Question\n\nThe paper establishes that the set of maximal elements for the equity-first relation, `M_{≻_FP}(Z)`, is always non-empty, even if agent preferences are not convex. What is the fundamental reason for this guaranteed existence?", "Options": {"A": "The set of envy-free allocations `F(Z)` is always non-empty and compact, and a Pareto-optimal allocation is guaranteed to exist on any non-empty, compact set.", "B": "The guaranteed transitivity of the equity-first relation `≻_FP` is a sufficient condition for a maximal element to exist in any finite set.", "C": "The existence of maximal elements is only guaranteed if all agent preferences are assumed to be convex, which ensures `P(Z)` and `F(Z)` intersect.", "D": "Every envy-free allocation is, by definition, a maximal element for the equity-first relation."}, "Answer": "A", "pi_justification": "This item assesses the understanding of a key result: the guaranteed existence of 'equity-first' optima. It requires connecting the formal characterization `P[F(Z)]` to foundational concepts of economic theory (existence of optima on compact sets). The distractors pinpoint specific misunderstandings: B confuses transitivity with existence on an infinite set (Almost Right), C introduces a condition (convexity) the paper explicitly shows is not needed for this principle (Conceptual Opposite), and D is false because an envy-free allocation can be Pareto-dominated by another envy-free allocation (Conceptual Error).", "qid": "382", "question": "### Background\n\nIn social choice theory, a conflict exists between the Pareto criterion (efficiency) and equity criteria like no-envy. The paper explores lexicographic principles to resolve this. This question focuses on the **equity-first, efficiency-second principle**.\n\n### Data / Model Specification\n\nThe **Equity-First Relation (`≻_FP`)** prioritizes equity (fewer envy instances) first, and uses Pareto superiority as a tie-breaker. An allocation `x*` is a **maximal element** for this relation if no other feasible allocation `y` exists such that `y ≻_FP x*`. The set of maximal elements is denoted `M_{≻_FP}(Z)`.\n\nThe paper shows `M_{≻_FP}(Z) = P[F(Z)]`, where `F(Z)` is the set of all envy-free feasible allocations and `P[Y]` denotes the set of Pareto-optimal allocations *within* a given set `Y`.\n\n### Question\n\nThe paper establishes that the set of maximal elements for the equity-first relation, `M_{≻_FP}(Z)`, is always non-empty, even if agent preferences are not convex. What is the fundamental reason for this guaranteed existence?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand why the set of maximal elements M_{≻_FP}(Z) is guaranteed to be non-empty.\n\nThe key insight is in the relationship: M_{≻_FP}(Z) = P[F(Z)]\n\nThis means the maximal elements are the Pareto-optimal allocations within the set of envy-free allocations F(Z).\n\nLet me analyze each option:\n\nA) This states that F(Z) is always non-empty and compact, and that Pareto-optimal allocations exist on any non-empty, compact set. This would indeed guarantee existence - if F(Z) is non-empty and compact, then P[F(Z)] would be non-empty, ensuring M_{≻_FP}(Z) is non-empty.\n\nB) This focuses on transitivity of the relation, but transitivity alone doesn't guarantee maximal elements exist. This is not the fundamental reason.\n\nC) This contradicts the premise of the question, which explicitly states that existence is guaranteed \"even if agent preferences are not convex.\"\n\nD) This is incorrect - not every envy-free allocation is maximal for the equity-first relation. Only the Pareto-optimal ones among the envy-free allocations are maximal.\n\nThe fundamental reason for guaranteed existence is that envy-free allocations always exist (F(Z) is non-empty) and the feasible set has the compactness properties needed to ensure Pareto-optimal elements exist within any non-empty subset. This is a standard result in economic theory - Pareto-optimal allocations exist on compact sets.\n\nA", "model_answer": "A", "error": null}}
{"ID": 459, "Question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of estimating a linear water demand function when households face non-linear, multi-block price schedules.\n\n**Setting / Institutional Environment.** The analysis begins by considering a standard linear demand model. However, because the price a household faces is determined by its chosen consumption level under a block tariff, the price variable is endogenous.\n\n---\n\n### Data / Model Specification\n\nThe baseline linear demand model is specified as:\n\n```latex\nw_{it} = \\alpha P_{it} + \\beta X_{it} + u_{it} \\quad \\text{(Eq. 1)}\n```\n\nTo address the endogeneity of `P_it`, an instrumental variable (IV) strategy is considered, using features of the tariff schedule (e.g., marginal prices at kink points) as instruments.\n\n---\n\n### Question\n\nWhen estimating the linear demand model in Eq. (1) under a multi-block tariff, both OLS and standard Instrumental Variable (IV) methods are likely to fail. What is the most fundamental theoretical reason for the failure of the IV strategy in this context?", "Options": {"A": "A rational consumer's choice is based on the entire price schedule, so unobserved preference shocks (`u_it`) are inherently correlated with all features of that schedule, violating the IV exclusion restriction.", "B": "Under an increasing block tariff, unobserved demand shocks are positively correlated with the marginal price, which leads to biased OLS estimates but is corrected by the IV strategy.", "C": "The marginal prices at the kink points are not strongly correlated with the actual marginal price paid by the consumer, making them weak instruments.", "D": "The price variable `P_it` is measured with error because households do not know the exact marginal price they face, a problem that IV cannot solve."}, "Answer": "A", "pi_justification": "This question assesses the core theoretical critique of using IV for non-linear pricing models, replacing Q1 and Q3 of the source QA. It uses a Reverse-Reasoning strategy. The correct answer (A) precisely identifies the violation of the exclusion restriction. Distractor B (Almost Right) correctly identifies the OLS problem but incorrectly claims IV solves it, missing the deeper issue. Distractor C (Conceptual) describes the weak instrument problem, which is a potential empirical issue but not the fundamental theoretical failure. Distractor D offers another plausible but incorrect econometric issue.", "qid": "459", "question": "### Background\n\n**Research Question.** This problem investigates the econometric challenges of estimating a linear water demand function when households face non-linear, multi-block price schedules.\n\n**Setting / Institutional Environment.** The analysis begins by considering a standard linear demand model. However, because the price a household faces is determined by its chosen consumption level under a block tariff, the price variable is endogenous.\n\n---\n\n### Data / Model Specification\n\nThe baseline linear demand model is specified as:\n\n```latex\nw_{it} = \\alpha P_{it} + \\beta X_{it} + u_{it} \\quad \\text{(Eq. 1)}\n```\n\nTo address the endogeneity of `P_it`, an instrumental variable (IV) strategy is considered, using features of the tariff schedule (e.g., marginal prices at kink points) as instruments.\n\n---\n\n### Question\n\nWhen estimating the linear demand model in Eq. (1) under a multi-block tariff, both OLS and standard Instrumental Variable (IV) methods are likely to fail. What is the most fundamental theoretical reason for the failure of the IV strategy in this context?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 462, "Question": "### Background\n\n**Research Question.** This problem concerns the necessary statistical pre-testing required to validate the use of an error-correction model (ECM) for analyzing long-run relationships between non-stationary time series, a cornerstone of the paper's empirical strategy.\n\n**Setting.** The study uses quarterly time-series data from 1965-1986. Before estimating the main behavioral models, the authors conduct Augmented Dickey-Fuller (ADF) tests to determine the order of integration of the variables and to test for cointegration among them. The validity of the entire empirical exercise rests on these preliminary findings.\n\n### Data / Model Specification\n\nThe paper's primary empirical specification is an error-correction model (ECM), which is only valid if the variables are integrated of order one, I(1), and are cointegrated. Table 1 below presents a selection of results from the Augmented Dickey-Fuller (ADF) tests used to verify these conditions. An asterisk (*) denotes rejection of the null hypothesis at the 5% level (critical value -3.77).\n\n**Table 1: Augmented Dickey-Fuller Unit Root & Cointegration Tests**\n| Test Specification (Dependent Variable is Real Price) | ln(I33/IPP) | ln(IAF/IPP) | ln(IMM/IPP) |\n| :--- | :--- | :--- | :--- |\n| 1. Intercept only (Unit Root Test) | -1.59 | -2.08 | -0.97 |\n| 2. Cointegration Test with ln(EXR/RPP) | -3.13 | -4.10* | -1.29 |\n| 3. Cointegration Test with ln(EXR/RPP), ln(SERV/Ix) | -4.84* | -4.07* | -3.59* |\n\n### Question\n\nThe tests in Table 1 lead the authors to conclude that the real price series are non-stationary (I(1)) and that a cointegrating relationship exists among the variables. Why is this pair of findings a crucial prerequisite for their chosen error-correction modeling (ECM) strategy?", "Options": {"A": "It proves that the variables are causally related in the long run, which is a necessary assumption for any regression analysis.", "B": "It allows for the estimation of the model using OLS on the levels of the variables, as cointegration guarantees the estimates will be unbiased.", "C": "It ensures that the residuals from the levels regression will be homoskedastic, a key assumption for valid inference.", "D": "It confirms a stable long-run equilibrium between non-stationary variables, which allows an ECM to validly model short-run dynamics while avoiding spurious regression."}, "Answer": "D", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item replaces Part 2 of the original QA. It tests the core methodological justification for using an ECM, a key concept in time-series econometrics. Strategy: Reverse-Reasoning (given the findings, what is the justification?). Distractor Logic: (A) is a Conceptual Error, confusing cointegration with causality. (B) is a Conceptual Opposite; regressing I(1) variables in levels is precisely what causes spurious regression. (C) is True but Irrelevant, confusing the role of cointegration (which ensures stationary errors) with the separate issue of homoskedasticity.", "qid": "462", "question": "### Background\n\n**Research Question.** This problem concerns the necessary statistical pre-testing required to validate the use of an error-correction model (ECM) for analyzing long-run relationships between non-stationary time series, a cornerstone of the paper's empirical strategy.\n\n**Setting.** The study uses quarterly time-series data from 1965-1986. Before estimating the main behavioral models, the authors conduct Augmented Dickey-Fuller (ADF) tests to determine the order of integration of the variables and to test for cointegration among them. The validity of the entire empirical exercise rests on these preliminary findings.\n\n### Data / Model Specification\n\nThe paper's primary empirical specification is an error-correction model (ECM), which is only valid if the variables are integrated of order one, I(1), and are cointegrated. Table 1 below presents a selection of results from the Augmented Dickey-Fuller (ADF) tests used to verify these conditions. An asterisk (*) denotes rejection of the null hypothesis at the 5% level (critical value -3.77).\n\n**Table 1: Augmented Dickey-Fuller Unit Root & Cointegration Tests**\n| Test Specification (Dependent Variable is Real Price) | ln(I33/IPP) | ln(IAF/IPP) | ln(IMM/IPP) |\n| :--- | :--- | :--- | :--- |\n| 1. Intercept only (Unit Root Test) | -1.59 | -2.08 | -0.97 |\n| 2. Cointegration Test with ln(EXR/RPP) | -3.13 | -4.10* | -1.29 |\n| 3. Cointegration Test with ln(EXR/RPP), ln(SERV/Ix) | -4.84* | -4.07* | -3.59* |\n\n### Question\n\nThe tests in Table 1 lead the authors to conclude that the real price series are non-stationary (I(1)) and that a cointegrating relationship exists among the variables. Why is this pair of findings a crucial prerequisite for their chosen error-correction modeling (ECM) strategy?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 393, "Question": "### Background\n\n**Research Question.** This problem investigates the fundamental identification and dynamic consistency failures of the standard α-maxmin expected utility (α-MEU) model. It explores how the inability to uniquely identify the model's parameters from choice behavior leads to ambiguity in predicting choices after new information is received.\n\n**Setting.** A decision-maker's (DM's) preferences `≿` over acts are represented by an α-MEU functional. The analysis focuses on two issues: (1) multiple combinations of ambiguity attitude (`α`) and perceived ambiguity (`P`) can generate the same `≿`, and (2) applying a standard belief updating rule to these equivalent representations can lead to different, conflicting conditional preferences.\n\n### Data / Model Specification\n\nThe subjective preference `≿` is represented by the α-MEU functional `V(f)`:\n```latex\nV(f) = \\alpha \\min_{\\mu \\in P} \\mathbb{E}_{\\mu}[u(f)] + (1-\\alpha) \\max_{\\mu \\in P} \\mathbb{E}_{\\mu}[u(f)]\n```\n**Proposition 1** states that two α-MEU representations, `(u, P_1, α_1)` and `(u, P_2, α_2)` with `α_i > 1/2`, represent the same preference if and only if `P_1` is the `γ`-expansion of `P_2`, where:\n```latex\n\\gamma = \\frac{\\alpha_1 + \\alpha_2 - 1}{2\\alpha_1 - 1} \\quad \\text{(Eq. (1))}\n```\nThe **`γ`-expansion** of a set `P` is defined as `Q = \\gamma P + (1-\\gamma)P := \\{\\gamma\\nu + (1-\\gamma)\\nu' : \\nu, \\nu' \\in P\\}`.\n\n**Example 1 Data:** Consider two α-MEU representations over `S={1,2,3}` that are stated to represent the same ex-ante preference `≿`.\n\n| Representation | Weight (`α`) | Set of Priors (`P`)                                                                                             |\n| :------------- | :----------: | :-------------------------------------------------------------------------------------------------------------- |\n| Rep 1          | `α_1 = 3/4`  | `P_1 = co\\{(5/6, 1/12, 1/12), (1/6, 5/12, 5/12)\\} `                                                              |\n| Rep 2          | `α_2 = 1`    | `P_2 = co\\{(2/3, 1/6, 1/6), (1/3, 1/3, 1/3)\\} `                                                                  |\n\n### Question\n\nUsing the data provided, calculate the `γ`-expansion factor that must relate `P_1` and `P_2` for them to be observationally equivalent, and identify the correct relationship.", "Options": {"A": "`γ = 3/2`, and `P_1` is the `γ`-expansion of `P_2`.", "B": "`γ = 3/4`, and `P_1` is the `γ`-expansion of `P_2`.", "C": "`γ = 3/2`, and `P_2` is the `γ`-expansion of `P_1`.", "D": "The representations are not equivalent because `α_1 ≠ α_2` and `P_1 ≠ P_2`."}, "Answer": "A", "pi_justification": "This item assesses the user's ability to perform a key calculation from the paper (the γ-expansion factor) and correctly interpret its meaning. It uses a Computational Judgment strategy. Distractors are designed based on common errors: using the wrong formula for γ (Formula Misuse), reversing the relationship between the sets (Conceptual Opposite), and a superficial misunderstanding of observational equivalence (Conceptual Error).", "qid": "393", "question": "### Background\n\n**Research Question.** This problem investigates the fundamental identification and dynamic consistency failures of the standard α-maxmin expected utility (α-MEU) model. It explores how the inability to uniquely identify the model's parameters from choice behavior leads to ambiguity in predicting choices after new information is received.\n\n**Setting.** A decision-maker's (DM's) preferences `≿` over acts are represented by an α-MEU functional. The analysis focuses on two issues: (1) multiple combinations of ambiguity attitude (`α`) and perceived ambiguity (`P`) can generate the same `≿`, and (2) applying a standard belief updating rule to these equivalent representations can lead to different, conflicting conditional preferences.\n\n### Data / Model Specification\n\nThe subjective preference `≿` is represented by the α-MEU functional `V(f)`:\n```latex\nV(f) = \\alpha \\min_{\\mu \\in P} \\mathbb{E}_{\\mu}[u(f)] + (1-\\alpha) \\max_{\\mu \\in P} \\mathbb{E}_{\\mu}[u(f)]\n```\n**Proposition 1** states that two α-MEU representations, `(u, P_1, α_1)` and `(u, P_2, α_2)` with `α_i > 1/2`, represent the same preference if and only if `P_1` is the `γ`-expansion of `P_2`, where:\n```latex\n\\gamma = \\frac{\\alpha_1 + \\alpha_2 - 1}{2\\alpha_1 - 1} \\quad \\text{(Eq. (1))}\n```\nThe **`γ`-expansion** of a set `P` is defined as `Q = \\gamma P + (1-\\gamma)P := \\{\\gamma\\nu + (1-\\gamma)\\nu' : \\nu, \\nu' \\in P\\}`.\n\n**Example 1 Data:** Consider two α-MEU representations over `S={1,2,3}` that are stated to represent the same ex-ante preference `≿`.\n\n| Representation | Weight (`α`) | Set of Priors (`P`)                                                                                             |\n| :------------- | :----------: | :-------------------------------------------------------------------------------------------------------------- |\n| Rep 1          | `α_1 = 3/4`  | `P_1 = co\\{(5/6, 1/12, 1/12), (1/6, 5/12, 5/12)\\} `                                                              |\n| Rep 2          | `α_2 = 1`    | `P_2 = co\\{(2/3, 1/6, 1/6), (1/3, 1/3, 1/3)\\} `                                                                  |\n\n### Question\n\nUsing the data provided, calculate the `γ`-expansion factor that must relate `P_1` and `P_2` for them to be observationally equivalent, and identify the correct relationship.", "model_evaluation_result": {"model_raw_response": "I need to calculate the γ-expansion factor using the given formula and determine the relationship between P₁ and P₂.\n\nGiven data:\n- α₁ = 3/4\n- α₂ = 1\n- Both representations have α > 1/2\n\nUsing Equation (1):\nγ = (α₁ + α₂ - 1)/(2α₁ - 1)\n\nSubstituting the values:\nγ = (3/4 + 1 - 1)/(2(3/4) - 1)\nγ = (3/4)/(3/2 - 1)\nγ = (3/4)/(1/2)\nγ = (3/4) × (2/1)\nγ = 3/2\n\nNow I need to determine which set is the γ-expansion of the other.\n\nAccording to Proposition 1, if two α-MEU representations are equivalent, then P₁ is the γ-expansion of P₂ when the formula gives the relationship between them.\n\nLet me verify this makes sense:\n- P₂ = co{(2/3, 1/6, 1/6), (1/3, 1/3, 1/3)}\n- P₁ = co{(5/6, 1/12, 1/12), (1/6, 5/12, 5/12)}\n\nThe γ-expansion with γ = 3/2 means:\nQ = (3/2)P + (1 - 3/2)P = (3/2)P + (-1/2)P = (3/2)P - (1/2)P\n\nThis would expand P₂ to create P₁.\n\nTherefore, γ = 3/2 and P₁ is the γ-expansion of P₂.\n\nA", "model_answer": "A", "error": null}}
{"ID": 471, "Question": "### Background\n\nA key justification for a central bank's role in bank supervision is that it may provide access to confidential information that improves monetary policy. This analysis examines whether confidential supervisory information contains predictive power for macroeconomic outcomes that is not already incorporated in official forecasts. It compares the Federal Reserve's internal \"Greenbook\" forecasts to those of private forecasters using a panel of quarterly forecasts from 1968-1994.\n\n### Data / Model Specification\n\nThe model regresses the actual, realized unemployment rate on the forecast, a measure of confidential supervisory information (`CAMEL5`), and an interaction term to see if the Greenbook uses this information differently.\n\n**The Model:**\n\n```latex\nU_{t+i} = \\alpha_{0} + \\alpha_{1}U_{j,t+i}^{e} + \\alpha_{2}CAMEL5_{t} + \\alpha_{3}(GB_{t} \\cdot CAMEL5_{t}) + \\epsilon_{j,t+i}\n```\n\n*   `U_{t+i}`: Actual realized unemployment rate.\n*   `U^e_{j,t+i}`: Forecast of unemployment made by forecaster `j`.\n*   `CAMEL5_t`: Percentage of assets in CAMEL 5-rated banks (confidential signal of distress).\n*   `GB_t`: A dummy variable equal to 1 for Greenbook forecasts, 0 for private forecasts.\n\n**The Data:**\nResults for the 4-quarter-ahead unemployment forecasts are in Table 1.\n\n**Table 1: Contribution of Supervisory Information to 4-Quarter-Ahead Unemployment Forecasts**\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| `CAMEL5` | 0.283* | (0.132) |\n| `Greenbook * CAMEL5` | -0.035 | (0.021) |\n\n*Notes: The dependent variable is the actual unemployment rate. * significant at 5%.*\n\n### Question\n\nBased on the model and results in Table 1, what is the correct interpretation of the coefficient on `CAMEL5` (0.283)?", "Options": {"A": "When banking distress (`CAMEL5`) is higher, private sector forecasts systematically underestimate the future unemployment rate.", "B": "When banking distress (`CAMEL5`) is higher, private sector forecasts systematically overestimate the future unemployment rate.", "C": "The Greenbook forecast is significantly more accurate than private forecasts when `CAMEL5` is high.", "D": "A 1 percentage point increase in `CAMEL5` causes a 0.283 percentage point increase in the unemployment rate."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the core finding of forecast inefficiency. Strategy: Direct interpretation of a regression coefficient in a difference-in-differences context. Distractors are designed based on common errors: Conceptual Opposite (sign error), True but Irrelevant (confusing the interpretation of the base coefficient `α₂` with the interaction term `α₃`), and Formula Misuse (making an unwarranted causal claim).", "qid": "471", "question": "### Background\n\nA key justification for a central bank's role in bank supervision is that it may provide access to confidential information that improves monetary policy. This analysis examines whether confidential supervisory information contains predictive power for macroeconomic outcomes that is not already incorporated in official forecasts. It compares the Federal Reserve's internal \"Greenbook\" forecasts to those of private forecasters using a panel of quarterly forecasts from 1968-1994.\n\n### Data / Model Specification\n\nThe model regresses the actual, realized unemployment rate on the forecast, a measure of confidential supervisory information (`CAMEL5`), and an interaction term to see if the Greenbook uses this information differently.\n\n**The Model:**\n\n```latex\nU_{t+i} = \\alpha_{0} + \\alpha_{1}U_{j,t+i}^{e} + \\alpha_{2}CAMEL5_{t} + \\alpha_{3}(GB_{t} \\cdot CAMEL5_{t}) + \\epsilon_{j,t+i}\n```\n\n*   `U_{t+i}`: Actual realized unemployment rate.\n*   `U^e_{j,t+i}`: Forecast of unemployment made by forecaster `j`.\n*   `CAMEL5_t`: Percentage of assets in CAMEL 5-rated banks (confidential signal of distress).\n*   `GB_t`: A dummy variable equal to 1 for Greenbook forecasts, 0 for private forecasts.\n\n**The Data:**\nResults for the 4-quarter-ahead unemployment forecasts are in Table 1.\n\n**Table 1: Contribution of Supervisory Information to 4-Quarter-Ahead Unemployment Forecasts**\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| `CAMEL5` | 0.283* | (0.132) |\n| `Greenbook * CAMEL5` | -0.035 | (0.021) |\n\n*Notes: The dependent variable is the actual unemployment rate. * significant at 5%.*\n\n### Question\n\nBased on the model and results in Table 1, what is the correct interpretation of the coefficient on `CAMEL5` (0.283)?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 406, "Question": "### Background\n\n**Research Question.** This question addresses the paper's central finding: identifying the primary cause of China's HSS underdevelopment. The analysis uses a calibrated structural model of the 2009 Chinese economy to run counterfactual experiments.\n\n**Variables & Parameters.**\n*   `n_h`: Employment share of the HSS sector.\n*   `τ_h`: The distortionary output wedge in the HSS sector.\n*   `Z_h`: Labor productivity in the HSS sector.\n*   `Ω^h`: The economy-wide supply of high-skilled labor.\n*   `Y`: Aggregate real GDP per capita (normalized to 1.00 in the benchmark).\n\n---\n\n### Data / Model Specification\n\nThe table below summarizes results from counterfactual experiments on the 2009 calibrated model. Each row shows the effect of changing one or more parameters from their benchmark values.\n\n**Table 1: Counterfactual Results (2009 Calibration)**\n\n| Parameter Values | n_h (HSS Emp. Share) | Y (GDP per capita) |\n| :--- | :--- | :--- |\n| **Benchmark** | **0.07** | **1.00** |\n| `τ_h = 0` (Wedge removed) | 0.15 | 1.05 |\n| `Z_h = 2 * Z_{h,09}` (Productivity doubled) | 0.07 | 1.13 |\n| `Ω^h = 2 * Ω^h_{09}` (Skill supply doubled) | 0.09 | 1.04 |\n\n---\n\n### Question\n\nBased on the counterfactual results in **Table 1**, what is the main conclusion regarding the cause of China's underdeveloped HSS sector and the potential gains from reform?", "Options": {"A": "Distortions (wedges) are the primary cause of low HSS employment; removing them would more than double the sector's size and increase aggregate GDP by 5% through improved allocative efficiency.", "B": "Low labor productivity is the primary cause of low HSS employment; doubling productivity would significantly increase the sector's size and yield the largest gains in aggregate GDP.", "C": "A scarcity of high-skilled labor is the primary cause of low HSS employment; doubling the supply of skilled workers would cause the sector's size to double and increase aggregate GDP.", "D": "Distortions (wedges) are the primary cause of low HSS employment, but removing them would have a negligible effect on aggregate GDP, suggesting the misallocation costs are small."}, "Answer": "A", "pi_justification": "This item assesses the ability to interpret counterfactual model results to identify a primary causal factor. It uses a Computational Judgment strategy. The correct option (A) accurately synthesizes the impact of removing the wedge on both sectoral employment and aggregate GDP. Distractor (B) incorrectly identifies low productivity as the cause for low employment, a direct contradiction of the table. Distractor (C) incorrectly identifies skill scarcity and overstates its impact. Distractor (D) is an 'Almost Right' option that correctly identifies the cause but incorrectly states the aggregate effect, testing for a complete reading of the evidence.", "qid": "406", "question": "### Background\n\n**Research Question.** This question addresses the paper's central finding: identifying the primary cause of China's HSS underdevelopment. The analysis uses a calibrated structural model of the 2009 Chinese economy to run counterfactual experiments.\n\n**Variables & Parameters.**\n*   `n_h`: Employment share of the HSS sector.\n*   `τ_h`: The distortionary output wedge in the HSS sector.\n*   `Z_h`: Labor productivity in the HSS sector.\n*   `Ω^h`: The economy-wide supply of high-skilled labor.\n*   `Y`: Aggregate real GDP per capita (normalized to 1.00 in the benchmark).\n\n---\n\n### Data / Model Specification\n\nThe table below summarizes results from counterfactual experiments on the 2009 calibrated model. Each row shows the effect of changing one or more parameters from their benchmark values.\n\n**Table 1: Counterfactual Results (2009 Calibration)**\n\n| Parameter Values | n_h (HSS Emp. Share) | Y (GDP per capita) |\n| :--- | :--- | :--- |\n| **Benchmark** | **0.07** | **1.00** |\n| `τ_h = 0` (Wedge removed) | 0.15 | 1.05 |\n| `Z_h = 2 * Z_{h,09}` (Productivity doubled) | 0.07 | 1.13 |\n| `Ω^h = 2 * Ω^h_{09}` (Skill supply doubled) | 0.09 | 1.04 |\n\n---\n\n### Question\n\nBased on the counterfactual results in **Table 1**, what is the main conclusion regarding the cause of China's underdeveloped HSS sector and the potential gains from reform?", "model_evaluation_result": {"model_raw_response": "Looking at the counterfactual results in Table 1:\n\n**Benchmark**: n_h = 0.07, Y = 1.00\n\n**Removing wedges (τ_h = 0)**: n_h increases from 0.07 to 0.15 (more than doubles), Y increases to 1.05 (+5%)\n\n**Doubling productivity (Z_h = 2 * Z_h,09)**: n_h stays at 0.07 (no change), Y increases to 1.13 (+13%)\n\n**Doubling skill supply (Ω^h = 2 * Ω^h_09)**: n_h increases modestly from 0.07 to 0.09, Y increases to 1.04 (+4%)\n\nThe key findings are:\n1. Removing distortions has the largest impact on HSS employment share (more than doubling it from 0.07 to 0.15)\n2. This generates a meaningful 5% increase in aggregate GDP\n3. While doubling productivity yields higher GDP gains (13%), it doesn't increase HSS employment at all\n4. Doubling skill supply has modest effects on both employment and GDP\n\nThis indicates that distortions are the primary constraint on HSS sector development - removing them dramatically expands the sector size and generates significant allocative efficiency gains.\n\nA", "model_answer": "A", "error": null}}
{"ID": 383, "Question": "### Background\n\nAn upstream monopolist sells an input to a downstream Cournot duopoly consisting of a multi-market “chain” (firm 1) and a single-market “local store” (firm 2). The chain operates in $K \\geq 2$ markets, while the local store operates in only one.\n\n### Data / Model Specification\n\nDownstream firms can produce the input themselves using a technology with a total cost function given by:\n```latex\nC(y) = F + vy\n```\nwhere $y$ is the quantity of the input produced, $F > 0$ is a fixed cost, and $v$ is a marginal cost.\n\n### Question\n\nAccording to the model, why is the chain's threat of backward integration more credible or “potent” than the local store's threat?", "Options": {"A": "The chain can spread the fixed cost of integration ($F$) over its sales in multiple markets, lowering its per-unit integration cost.", "B": "The chain is assumed to be a more efficient downstream producer, giving it a larger market share to leverage against the monopolist.", "C": "The chain faces a lower marginal cost of self-supply ($v$) than the local store.", "D": "The monopolist's marginal cost ($c$) is lower than the self-supply marginal cost ($v$), making integration an unprofitable strategy for any firm."}, "Answer": "A", "pi_justification": "This item assesses the foundational concept of economies of scale as the driver of the chain's bargaining power. It replaces Q1 of the original QA problem. The distractors are designed to isolate this core concept from other plausible but incorrect explanations. (B) is a common intuition but not the mechanism in this model. (C) is a direct contradiction of the model's assumptions. (D) correctly states a fact about social efficiency but fails to explain the difference in threat credibility between the two firms.", "qid": "383", "question": "### Background\n\nAn upstream monopolist sells an input to a downstream Cournot duopoly consisting of a multi-market “chain” (firm 1) and a single-market “local store” (firm 2). The chain operates in $K \\geq 2$ markets, while the local store operates in only one.\n\n### Data / Model Specification\n\nDownstream firms can produce the input themselves using a technology with a total cost function given by:\n```latex\nC(y) = F + vy\n```\nwhere $y$ is the quantity of the input produced, $F > 0$ is a fixed cost, and $v$ is a marginal cost.\n\n### Question\n\nAccording to the model, why is the chain's threat of backward integration more credible or “potent” than the local store's threat?", "model_evaluation_result": {"model_raw_response": "Looking at this backward integration problem, I need to analyze why the chain's threat is more credible than the local store's threat.\n\nThe key insight is in the cost structure for self-production: C(y) = F + vy\n\nThis has a fixed cost F and marginal cost v. The critical difference between the chain and local store is their scale of operations.\n\nThe chain operates in K ≥ 2 markets, so if it integrates backward, it can produce input for all K markets and spread the fixed cost F across a much larger quantity of output. This significantly reduces the average cost of self-production.\n\nThe local store operates in only one market, so it must bear the full fixed cost F for a smaller quantity, resulting in a higher average cost of self-production.\n\nWhen the upstream monopolist sets its price, it must consider each firm's outside option (self-production). The chain's lower average cost of self-production gives it a better outside option, making its threat to integrate backward more credible. The monopolist must therefore offer the chain better terms to prevent integration.\n\nThe local store, with its higher average cost due to inability to spread fixed costs, has a weaker outside option and less bargaining power.\n\nOptions B, C, and D don't capture this fundamental economics of fixed cost spreading across multiple markets.\n\nA", "model_answer": "C", "error": null}}
{"ID": 379, "Question": "A unit mass of agents decide whether to visit a hospital with capacity `\\bar{n}`. The probability of admission for a visitor is `p(n, \\bar{n}) = \\operatorname*{min}\\{1, \\bar{n}/n\\}`, where `n` is the total mass of visitors.\n\nSuppose the public belief about disease severity `\\tilde{s}` is high enough that the system is in a congestion regime (`n > \\bar{n}`). If hospital capacity `\\bar{n}` is marginally increased, what is the effect on the equilibrium number of hospital visits `n(\\tilde{s})`?", "Options": {"A": "`n(\\tilde{s})` increases, because the higher probability of admission incentivizes more marginal agents to visit.", "B": "`n(\\tilde{s})` decreases, because with more capacity, the situation is less dire, reducing agents' perceived urgency.", "C": "`n(\\tilde{s})` remains unchanged, as it is determined by the belief `\\tilde{s}`, not the capacity `\\bar{n}`.", "D": "`n(\\tilde{s})` decreases, because the hospital can now screen patients more effectively."}, "Answer": "A", "pi_justification": "This item assesses understanding of the strategic interactions in the congestion game, a key comparative static. The correct answer requires reasoning about how a change in `\\bar{n}` affects the admission probability `p`, which in turn alters individual incentives. Distractor (B) is the Conceptual Opposite, representing a plausible but incorrect intuition about agent behavior. Distractor (C) ignores the strategic channel through which `\\bar{n}` affects decisions. Distractor (D) introduces an irrelevant mechanism (screening) not present in this part of the model.", "qid": "379", "question": "A unit mass of agents decide whether to visit a hospital with capacity `\\bar{n}`. The probability of admission for a visitor is `p(n, \\bar{n}) = \\operatorname*{min}\\{1, \\bar{n}/n\\}`, where `n` is the total mass of visitors.\n\nSuppose the public belief about disease severity `\\tilde{s}` is high enough that the system is in a congestion regime (`n > \\bar{n}`). If hospital capacity `\\bar{n}` is marginally increased, what is the effect on the equilibrium number of hospital visits `n(\\tilde{s})`?", "model_evaluation_result": {"model_raw_response": "I need to analyze how a marginal increase in hospital capacity affects the equilibrium number of visits in a congestion regime.\n\nGiven information:\n- Unit mass of agents deciding whether to visit\n- Hospital capacity: $\\bar{n}$\n- Admission probability: $p(n, \\bar{n}) = \\min\\{1, \\bar{n}/n\\}$\n- Currently in congestion regime: $n > \\bar{n}$\n- Public belief about disease severity: $\\tilde{s}$ (high enough to cause congestion)\n\nIn the congestion regime where $n > \\bar{n}$, the admission probability is $p(n, \\bar{n}) = \\bar{n}/n < 1$.\n\nAt equilibrium, the marginal agent must be indifferent between visiting and not visiting. The expected benefit of visiting depends on:\n1. The probability of admission: $\\bar{n}/n$\n2. The benefit if admitted (related to disease severity belief $\\tilde{s}$)\n3. The cost of visiting\n\nThe equilibrium condition requires that the expected benefit equals the cost for the marginal agent. This can be written as:\n$(\\bar{n}/n) \\times \\text{benefit if admitted} = \\text{marginal cost}$\n\nWhen $\\bar{n}$ increases marginally:\n- The admission probability $\\bar{n}/n$ increases for any given $n$\n- This makes visiting more attractive\n- More agents will want to visit\n- The equilibrium $n$ must increase to restore the equilibrium condition\n\nThe key insight is that even though more capacity is available, the increased probability of admission incentivizes more agents to visit, leading to a higher equilibrium number of visits.\n\nThe answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 396, "Question": "### Background\n\n**Research Question:** This problem examines the implications of the paper's main theorem for the theory of two-stage budgeting, showing how simultaneous direct and indirect weak separability leads to a modified version of the Strotz-Gorman aggregation framework.\n\n**Setting:** We consider a region of the normalized price space, `y ∈ B_s`, where the conditions of the main theorem hold. Specifically, the indirect sub-utility `v^s` is not quasi-homothetic, but all other `v^t` (`t ≠ s`) are homogeneous of degree one and the aggregator `G` is `C_s H_0`.\n\n### Data / Model Specification\n\nIn the region `y ∈ B_s`, the `C_s H_0` property of the aggregator `G` implies the following relationship for `ν(y) = ∑ v_i y_i`:\n```latex\n\\nu(y) = G_s(w) [\\nu^s(y^s) + \\eta^s(v^s(y^s))] \\quad \\text{(Eq. 1)}\n```\nwhere `v^s(y^s)` is the indirect sub-utility for group `s`, `ν^s(y^s) = ∑_{i∈N_s} v_i^s y_i`, and `η^s` is a function of `v^s`.\n\nThe first-order conditions for the dual problem for goods `i ∈ N_s` are:\n```latex\nv_i(y) = G_s(w) v_i^s(y^s) = \\lambda x_i \\quad \\text{(Eq. 2)}\n```\nwhere `λ = ν(y)` at the optimum. Expenditure on group `s` is `I_s = I y^s x^s`.\n\nThe paper shows that these conditions lead to the following expenditure equation for `y ∈ B_s`:\n```latex\nI_s = \\frac{\\nu^s(y^s)}{\\nu^s(y^s) + \\eta^s(v^s(y^s))} I \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nIn the derived expenditure equation for group `s` (Eq. 3), the budget share is determined by two distinct functions of the group's normalized prices, `v^s(y^s)` and `ν^s(y^s)`. What are the correct economic interpretations of these two functions?", "Options": {"A": "`v^s(y^s)` is a welfare index representing the sub-utility level, while `ν^s(y^s)` is a price sensitivity index reflecting the weighted average price level.", "B": "`v^s(y^s)` is a price sensitivity index reflecting the weighted average price level, while `ν^s(y^s)` is a welfare index representing the sub-utility level.", "C": "Both `v^s(y^s)` and `ν^s(y^s)` are alternative forms of a Klein price aggregator, making the choice between them arbitrary for budgeting purposes.", "D": "`v^s(y^s)` measures the income elasticity of the group, while `ν^s(y^s)` measures the substitution effect within the group."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the student's ability to interpret the economic meaning of the key terms in a derived formula, a crucial skill for linking theory to application. It uses a conceptual interpretation strategy. The distractors are designed based on common errors: B is a direct conceptual swap; C confuses the result with a standard concept (Klein Aggregator); D misapplies other valid economic concepts to the terms.", "qid": "396", "question": "### Background\n\n**Research Question:** This problem examines the implications of the paper's main theorem for the theory of two-stage budgeting, showing how simultaneous direct and indirect weak separability leads to a modified version of the Strotz-Gorman aggregation framework.\n\n**Setting:** We consider a region of the normalized price space, `y ∈ B_s`, where the conditions of the main theorem hold. Specifically, the indirect sub-utility `v^s` is not quasi-homothetic, but all other `v^t` (`t ≠ s`) are homogeneous of degree one and the aggregator `G` is `C_s H_0`.\n\n### Data / Model Specification\n\nIn the region `y ∈ B_s`, the `C_s H_0` property of the aggregator `G` implies the following relationship for `ν(y) = ∑ v_i y_i`:\n```latex\n\\nu(y) = G_s(w) [\\nu^s(y^s) + \\eta^s(v^s(y^s))] \\quad \\text{(Eq. 1)}\n```\nwhere `v^s(y^s)` is the indirect sub-utility for group `s`, `ν^s(y^s) = ∑_{i∈N_s} v_i^s y_i`, and `η^s` is a function of `v^s`.\n\nThe first-order conditions for the dual problem for goods `i ∈ N_s` are:\n```latex\nv_i(y) = G_s(w) v_i^s(y^s) = \\lambda x_i \\quad \\text{(Eq. 2)}\n```\nwhere `λ = ν(y)` at the optimum. Expenditure on group `s` is `I_s = I y^s x^s`.\n\nThe paper shows that these conditions lead to the following expenditure equation for `y ∈ B_s`:\n```latex\nI_s = \\frac{\\nu^s(y^s)}{\\nu^s(y^s) + \\eta^s(v^s(y^s))} I \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nIn the derived expenditure equation for group `s` (Eq. 3), the budget share is determined by two distinct functions of the group's normalized prices, `v^s(y^s)` and `ν^s(y^s)`. What are the correct economic interpretations of these two functions?", "model_evaluation_result": {"model_raw_response": "Looking at the given definitions and context:\n\nFrom the problem setup:\n- `v^s(y^s)` is defined as \"the indirect sub-utility for group s\"\n- `ν^s(y^s) = ∑_{i∈N_s} v_i^s y_i` is defined as a weighted sum\n\nIn consumer theory, indirect utility functions represent welfare or utility levels that can be achieved at given prices and income levels. The term \"indirect sub-utility\" clearly indicates that `v^s(y^s)` represents a welfare measure - specifically, the utility level achievable within group s at the normalized prices `y^s`.\n\nThe function `ν^s(y^s) = ∑_{i∈N_s} v_i^s y_i` is a weighted average of normalized prices, where the weights `v_i^s` are the marginal utilities (price sensitivities) and `y_i` are the normalized prices. This structure makes it a price sensitivity index that reflects how the group responds to price changes.\n\nIn the expenditure equation (Eq. 3), these two functions work together to determine the budget share: `v^s` captures the welfare level achievable in the group, while `ν^s` captures the price-weighted sensitivity measure.\n\nA", "model_answer": "A", "error": null}}
{"ID": 373, "Question": "### Background\n\n**Research Question.** This problem investigates the central empirical puzzle motivating the paper: how to reconcile the massive reallocation of labor towards the service sector with the observed stability of relative wages. It then uses the model's counterfactual simulations to test whether significant labor market frictions are compatible with this empirical fact.\n\n**Setting / Institutional Environment.** The analysis uses the paper's estimated general equilibrium model of the U.S. economy (1968-2000). The core exercise is to compare the baseline model simulation against counterfactuals where inter-sectoral mobility costs are set to zero or to a prohibitively high level.\n\n### Data / Model Specification\n\nThe key empirical puzzle and the model's ability to replicate it are shown in Table 1. Table 2 shows results from counterfactual experiments where mobility costs are altered.\n\n**Table 1: Actual and Predicted Sectoral Differences in Growth (Avg. Annual % Change, 1968-2000)**\n\n| Service-Goods Growth Rate | Actual | Predicted |\n| :--- | :---: | :---: |\n| Employment | 2.23 | 2.13 |\n| Hourly Wage | 0.23 | 0.51 |\n\n*Source: Abridged from Table VII in the paper.*\n\n**Table 2: Effect of Mobility Costs on Skill Rental Prices (Annual % Change, 1981-2000)**\n\n| Sector / Occupation | Predicted (Baseline) | Counterfactual 1 (Prohibitive Cost) | Counterfactual 3 (Zero Cost) |\n| :--- | :---: | :---: | :---: |\n| **Goods Sector** | |\n| White Collar | 0.71 | 0.75 | 0.71 |\n| **Service Sector** | |\n| White Collar | 0.46 | 0.49 | 0.54 |\n\n*Source: Abridged from Table IX in the paper.*\n\n### Question\n\nBased on the data in Table 1 and Table 2, what is the paper's central argument regarding the relationship between labor mobility costs and the stability of relative wages in the U.S. economy?", "Options": {"A": "Despite the stability of relative wages, mobility costs are substantial; wage stability is maintained by other adjustment margins, as shown by the fact that skill rental prices are largely invariant to the level of mobility costs.", "B": "The stability of relative wages in the face of large employment shifts (Table 1) proves that mobility costs must be close to zero, allowing for a nearly perfectly elastic labor supply.", "C": "Prohibitively high mobility costs cause skill rental prices to become highly volatile (Table 2), which contradicts the observed stability of wages, confirming that costs must be low.", "D": "The growth in service-sector employment was much faster than the growth in its relative wage (Table 1), indicating that mobility costs primarily affect employment levels but not wages."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.38). The original QA problem was a sequence of highly convertible parts. This Single Choice item assesses the paper's core argument by requiring the synthesis of two tables. It uses a Reverse-Reasoning strategy. Distractor B represents the 'naive view' the paper refutes. Distractor C involves a direct misreading of Table 2. Distractor D is a plausible but incomplete conclusion that misses the central point about frictions.", "qid": "373", "question": "### Background\n\n**Research Question.** This problem investigates the central empirical puzzle motivating the paper: how to reconcile the massive reallocation of labor towards the service sector with the observed stability of relative wages. It then uses the model's counterfactual simulations to test whether significant labor market frictions are compatible with this empirical fact.\n\n**Setting / Institutional Environment.** The analysis uses the paper's estimated general equilibrium model of the U.S. economy (1968-2000). The core exercise is to compare the baseline model simulation against counterfactuals where inter-sectoral mobility costs are set to zero or to a prohibitively high level.\n\n### Data / Model Specification\n\nThe key empirical puzzle and the model's ability to replicate it are shown in Table 1. Table 2 shows results from counterfactual experiments where mobility costs are altered.\n\n**Table 1: Actual and Predicted Sectoral Differences in Growth (Avg. Annual % Change, 1968-2000)**\n\n| Service-Goods Growth Rate | Actual | Predicted |\n| :--- | :---: | :---: |\n| Employment | 2.23 | 2.13 |\n| Hourly Wage | 0.23 | 0.51 |\n\n*Source: Abridged from Table VII in the paper.*\n\n**Table 2: Effect of Mobility Costs on Skill Rental Prices (Annual % Change, 1981-2000)**\n\n| Sector / Occupation | Predicted (Baseline) | Counterfactual 1 (Prohibitive Cost) | Counterfactual 3 (Zero Cost) |\n| :--- | :---: | :---: | :---: |\n| **Goods Sector** | |\n| White Collar | 0.71 | 0.75 | 0.71 |\n| **Service Sector** | |\n| White Collar | 0.46 | 0.49 | 0.54 |\n\n*Source: Abridged from Table IX in the paper.*\n\n### Question\n\nBased on the data in Table 1 and Table 2, what is the paper's central argument regarding the relationship between labor mobility costs and the stability of relative wages in the U.S. economy?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to analyze what the tables reveal about the relationship between labor mobility costs and wage stability.\n\nFrom Table 1, I can see the key empirical puzzle: there was massive reallocation of labor toward services (2.23% annual employment growth difference) but relative wages remained quite stable (only 0.23% annual wage growth difference).\n\nFrom Table 2, I can examine how different mobility cost scenarios affect skill rental prices:\n- Baseline (actual mobility costs): White collar goods 0.71%, services 0.46%\n- Prohibitive costs: White collar goods 0.75%, services 0.49% \n- Zero costs: White collar goods 0.71%, services 0.54%\n\nThe key insight is that skill rental prices are remarkably similar across all three scenarios - they're \"largely invariant\" to the level of mobility costs. The differences are quite small (within about 0.08 percentage points).\n\nThis suggests that even with substantial mobility costs (the baseline case), wage stability can be maintained through other economic adjustment mechanisms beyond just labor mobility. The economy can accommodate large employment shifts while keeping relative wages stable, even when workers face significant costs to move between sectors.\n\nOption A captures this central argument: mobility costs are substantial, but wage stability is maintained by other adjustment margins, as evidenced by skill rental prices being largely invariant to mobility cost levels.\n\nA", "model_answer": "A", "error": null}}
{"ID": 463, "Question": "### Background\n\n**Research Question.** This problem concerns the necessary statistical pre-testing required to validate the use of an error-correction model (ECM) for analyzing long-run relationships between non-stationary time series, a cornerstone of the paper's empirical strategy.\n\n**Setting.** The study uses quarterly time-series data from 1965-1986. Before estimating the main behavioral models, the authors conduct Augmented Dickey-Fuller (ADF) tests to determine the order of integration of the variables. The sample period includes the major OPEC oil shocks of the 1970s.\n\n### Question\n\nA known limitation of ADF tests is their low power to reject the unit root null hypothesis when a series is actually stationary around a structural break. If a more appropriate test (e.g., a Zivot-Andrews test) revealed that the paper's real price series were in fact stationary around a structural break from the oil shocks, what would be the most significant implication for the paper's analysis?", "Options": {"A": "The cointegration analysis and the use of an error-correction model would be methodologically invalid.", "B": "The model would need to be re-estimated with dummy variables for the break, but the ECM structure would remain appropriate.", "C": "The estimated long-run elasticities would still be valid, but the short-run dynamic coefficients would be biased.", "D": "The results would be less precise, with larger standard errors, but the overall conclusions would likely still hold."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item replaces Part 3 of the original QA. It assesses a deeper methodological critique and its logical consequences, a key skill in evaluating empirical research. Strategy: Scenario Application. Distractor Logic: (A) is the correct, fatal implication. (B) is a Conceptual Opposite; an ECM is fundamentally incorrect for stationary variables. (C) is a Step-Omission Error; if the series is stationary, the concept of a 'cointegrating' long-run elasticity is itself invalid. (D) is a weak claim that understates the severity of the methodological failure.", "qid": "463", "question": "### Background\n\n**Research Question.** This problem concerns the necessary statistical pre-testing required to validate the use of an error-correction model (ECM) for analyzing long-run relationships between non-stationary time series, a cornerstone of the paper's empirical strategy.\n\n**Setting.** The study uses quarterly time-series data from 1965-1986. Before estimating the main behavioral models, the authors conduct Augmented Dickey-Fuller (ADF) tests to determine the order of integration of the variables. The sample period includes the major OPEC oil shocks of the 1970s.\n\n### Question\n\nA known limitation of ADF tests is their low power to reject the unit root null hypothesis when a series is actually stationary around a structural break. If a more appropriate test (e.g., a Zivot-Andrews test) revealed that the paper's real price series were in fact stationary around a structural break from the oil shocks, what would be the most significant implication for the paper's analysis?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 416, "Question": "### Background\n\n**Research Question.** This problem evaluates the Time-Varying Parameter (TVP) ARCH-M model against the standard Fixed-Parameter (FP) ARCH-M model, investigating whether allowing the price of risk to vary over time can resolve key empirical puzzles in modeling stock returns.\n\n**Setting / Institutional Environment.** Standard FP ARCH-M models, which assume a constant risk-return tradeoff, have been found to produce unstable parameter estimates across different sample periods and to systematically over-predict the equity risk premium. The TVP ARCH-M model is proposed as a solution by allowing the price of risk to evolve as a random walk.\n\n### Data / Model Specification\n\nThe standard **Fixed-Parameter (FP) ARCH-M** model is:\n```latex\ny_t = c h_t + e_t\n```\nThe proposed **Time-Varying Parameter (TVP) ARCH-M** model is specified in a state-space form:\n```latex\ny_t = b_t h_t + e_t\n```\n```latex\nb_t = b_{t-1} + v_t\n```\nwhere `c` is the constant price of risk and `b_t` is the time-varying price of risk.\n\n**Table 1: Predictive Regressions for Realized Returns (1926-1985)**\n\n| Model | Intercept | Coeff on TVP Prediction | Coeff on FP Prediction | R² |\n| :--- | :--- | :--- | :--- | :--- |\n| 1. TVP only | -0.044 (0.277) | 1.268 (0.333) | | 0.020 |\n| 2. FP only | 0.342 (0.265) | | 0.311 (0.166) | 0.005 |\n| 3. Horse Race | -0.724 (0.351) | 1.576 (0.345) | -0.533 (0.171) | 0.032 |\n\n*Standard errors are in parentheses.*\n\n### Question\n\nConsider the \"Horse Race\" regression (Model 3 in Table 1), where the realized return is regressed on the predictions from both the TVP and FP models simultaneously. What is the primary conclusion from this regression?", "Options": {"A": "The TVP model's predictions subsume the information in the FP model's predictions, which appear to have a negative marginal relationship with realized returns once the TVP prediction is included.", "B": "Both the TVP and FP models provide statistically significant and independent positive information for predicting future returns.", "C": "The FP model is the superior predictor, and the TVP model's predictions serve as a significant negative correction term.", "D": "Neither model is a good predictor, as the coefficients are not close to 1 and the R-squared value is very low."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the user's ability to interpret a 'horse race' regression, a crucial skill for evaluating competing empirical models. It uses a Reverse-Reasoning strategy, asking for the primary inference from the results. The distractors are based on a classic design matrix: B (Almost Right) ignores the crucial negative sign on the losing model's coefficient; C (Conceptual Opposite) reverses the roles of the two models; D (True but Irrelevant) focuses on secondary issues (coefficient magnitude, R-squared) to distract from the main conclusion about which model's information content subsumes the other.", "qid": "416", "question": "### Background\n\n**Research Question.** This problem evaluates the Time-Varying Parameter (TVP) ARCH-M model against the standard Fixed-Parameter (FP) ARCH-M model, investigating whether allowing the price of risk to vary over time can resolve key empirical puzzles in modeling stock returns.\n\n**Setting / Institutional Environment.** Standard FP ARCH-M models, which assume a constant risk-return tradeoff, have been found to produce unstable parameter estimates across different sample periods and to systematically over-predict the equity risk premium. The TVP ARCH-M model is proposed as a solution by allowing the price of risk to evolve as a random walk.\n\n### Data / Model Specification\n\nThe standard **Fixed-Parameter (FP) ARCH-M** model is:\n```latex\ny_t = c h_t + e_t\n```\nThe proposed **Time-Varying Parameter (TVP) ARCH-M** model is specified in a state-space form:\n```latex\ny_t = b_t h_t + e_t\n```\n```latex\nb_t = b_{t-1} + v_t\n```\nwhere `c` is the constant price of risk and `b_t` is the time-varying price of risk.\n\n**Table 1: Predictive Regressions for Realized Returns (1926-1985)**\n\n| Model | Intercept | Coeff on TVP Prediction | Coeff on FP Prediction | R² |\n| :--- | :--- | :--- | :--- | :--- |\n| 1. TVP only | -0.044 (0.277) | 1.268 (0.333) | | 0.020 |\n| 2. FP only | 0.342 (0.265) | | 0.311 (0.166) | 0.005 |\n| 3. Horse Race | -0.724 (0.351) | 1.576 (0.345) | -0.533 (0.171) | 0.032 |\n\n*Standard errors are in parentheses.*\n\n### Question\n\nConsider the \"Horse Race\" regression (Model 3 in Table 1), where the realized return is regressed on the predictions from both the TVP and FP models simultaneously. What is the primary conclusion from this regression?", "model_evaluation_result": {"model_raw_response": "Looking at the \"Horse Race\" regression (Model 3) results:\n\n- TVP coefficient: 1.576 (0.345) - positive and statistically significant\n- FP coefficient: -0.533 (0.171) - negative and statistically significant\n- R² increases from 0.020 (TVP only) and 0.005 (FP only) to 0.032 (both models)\n\nThe key insight is that when both predictions are included simultaneously, the TVP model maintains a strong positive relationship with realized returns (coefficient of 1.576), while the FP model's coefficient becomes significantly negative (-0.533). This suggests that once we account for the TVP model's prediction, the FP model's prediction actually has a negative marginal relationship with returns.\n\nThis pattern indicates that the TVP model captures the relevant information for predicting returns, and any additional information from the FP model actually works against accurate prediction. In other words, the TVP model's predictions subsume or dominate the information contained in the FP model's predictions.\n\nA", "model_answer": "A", "error": null}}
{"ID": 464, "Question": "### Background\n\n**Research Question.** What are the practical implications of the `GDPe`/`GDPo` distinction for applied empirical research, particularly for studies of productivity and the causal effect of trade on income?\n\n**Setting / Institutional Environment.** The analysis provides guidelines for empirical researchers using cross-country data. It highlights how using the wrong GDP measure can lead to biased estimates and flawed causal inference.\n\n**Variables & Parameters.**\n- `Real GDPe`: Expenditure-side real GDP, a measure of national wealth and purchasing power. Influenced by terms of trade.\n- `Real GDPo`: Output-side real GDP, a measure of national productive capacity. Excludes terms of trade effects.\n\n### Question\n\nBased on the paper's recommendations, which GDP measures should be used for studying technology gaps and national income, respectively?", "Options": {"A": "`Real GDPo` for technology gaps; `Real GDPe` for national income.", "B": "`Real GDPe` for technology gaps; `Real GDPo` for national income.", "C": "`Real GDPo` for both technology gaps and national income.", "D": "`Real GDPe` for both technology gaps and national income."}, "Answer": "A", "pi_justification": "This item assesses the user's grasp of the paper's core practical recommendation for applied researchers. It uses a Single Choice format because there is one correct pairing. The distractors are designed based on the 'Conceptual Opposite' (B) and 'Almost Right' (C, D) principles, targeting the most common and critical confusion between the two GDP measures.", "qid": "464", "question": "### Background\n\n**Research Question.** What are the practical implications of the `GDPe`/`GDPo` distinction for applied empirical research, particularly for studies of productivity and the causal effect of trade on income?\n\n**Setting / Institutional Environment.** The analysis provides guidelines for empirical researchers using cross-country data. It highlights how using the wrong GDP measure can lead to biased estimates and flawed causal inference.\n\n**Variables & Parameters.**\n- `Real GDPe`: Expenditure-side real GDP, a measure of national wealth and purchasing power. Influenced by terms of trade.\n- `Real GDPo`: Output-side real GDP, a measure of national productive capacity. Excludes terms of trade effects.\n\n### Question\n\nBased on the paper's recommendations, which GDP measures should be used for studying technology gaps and national income, respectively?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 394, "Question": "### Background\n\n**Research Question.** This problem investigates the fundamental identification and dynamic consistency failures of the standard α-maxmin expected utility (α-MEU) model. It explores how the inability to uniquely identify the model's parameters from choice behavior leads to ambiguity in predicting choices after new information is received.\n\n**Setting.** A decision-maker's (DM's) preferences `≿` over acts are represented by an α-MEU functional. The analysis focuses on two issues: (1) multiple combinations of ambiguity attitude (`α`) and perceived ambiguity (`P`) can generate the same `≿`, and (2) applying a standard belief updating rule to these equivalent representations can lead to different, conflicting conditional preferences.\n\n### Data / Model Specification\n\n**Prior-by-prior Bayesian updating** is a rule to form a conditional preference `≿_E` after learning an event `E ⊆ S`. The new set of priors `P^E` is formed by updating each prior `μ ∈ P` according to Bayes' rule:\n```latex\nP^{E} := \\{\\mu^{E} : \\mu \\in P\\}, \\quad \\text{where } \\mu^{E}(F) := \\frac{\\mu(E \\cap F)}{\\mu(E)}\n```\n**Example 1 Data:** Consider two α-MEU representations over `S={1,2,3}`.\n\n| Representation | Weight (`α`) | Set of Priors (`P`)                                                                                             |\n| :------------- | :----------: | :-------------------------------------------------------------------------------------------------------------- |\n| Rep 1          | `α_1 = 3/4`  | `P_1 = co\\{(5/6, 1/12, 1/12), (1/6, 5/12, 5/12)\\} `                                                              |\n| Rep 2          | `α_2 = 1`    | `P_2 = co\\{(2/3, 1/6, 1/6), (1/3, 1/3, 1/3)\\} `                                                                  |\n\n### Question\n\nConsider the event `E = {1, 2}`. What is the updated set of priors `P_2^E` resulting from prior-by-prior Bayesian updating?", "Options": {"A": "`co{(4/5, 1/5, 0), (1/2, 1/2, 0)}`", "B": "`co{(10/11, 1/11, 0), (2/7, 5/7, 0)}`", "C": "`co{(2/3, 1/6, 0), (1/3, 1/3, 0)}`", "D": "`co{(4/5, 1/5, 0), (1/3, 2/3, 0)}`"}, "Answer": "A", "pi_justification": "This item tests the procedural knowledge of applying prior-by-prior Bayesian updating, a core mechanism discussed in the paper. It uses a Computational Judgment strategy. Distractors are based on plausible errors: using the wrong initial set of priors (Conceptual Error), failing to normalize the updated vectors (Step-Omission Error), and a simple calculation mistake.", "qid": "394", "question": "### Background\n\n**Research Question.** This problem investigates the fundamental identification and dynamic consistency failures of the standard α-maxmin expected utility (α-MEU) model. It explores how the inability to uniquely identify the model's parameters from choice behavior leads to ambiguity in predicting choices after new information is received.\n\n**Setting.** A decision-maker's (DM's) preferences `≿` over acts are represented by an α-MEU functional. The analysis focuses on two issues: (1) multiple combinations of ambiguity attitude (`α`) and perceived ambiguity (`P`) can generate the same `≿`, and (2) applying a standard belief updating rule to these equivalent representations can lead to different, conflicting conditional preferences.\n\n### Data / Model Specification\n\n**Prior-by-prior Bayesian updating** is a rule to form a conditional preference `≿_E` after learning an event `E ⊆ S`. The new set of priors `P^E` is formed by updating each prior `μ ∈ P` according to Bayes' rule:\n```latex\nP^{E} := \\{\\mu^{E} : \\mu \\in P\\}, \\quad \\text{where } \\mu^{E}(F) := \\frac{\\mu(E \\cap F)}{\\mu(E)}\n```\n**Example 1 Data:** Consider two α-MEU representations over `S={1,2,3}`.\n\n| Representation | Weight (`α`) | Set of Priors (`P`)                                                                                             |\n| :------------- | :----------: | :-------------------------------------------------------------------------------------------------------------- |\n| Rep 1          | `α_1 = 3/4`  | `P_1 = co\\{(5/6, 1/12, 1/12), (1/6, 5/12, 5/12)\\} `                                                              |\n| Rep 2          | `α_2 = 1`    | `P_2 = co\\{(2/3, 1/6, 1/6), (1/3, 1/3, 1/3)\\} `                                                                  |\n\n### Question\n\nConsider the event `E = {1, 2}`. What is the updated set of priors `P_2^E` resulting from prior-by-prior Bayesian updating?", "model_evaluation_result": {"model_raw_response": "I need to apply prior-by-prior Bayesian updating to find P₂^E where E = {1,2}.\n\nGiven:\n- P₂ = co{(2/3, 1/6, 1/6), (1/3, 1/3, 1/3)}\n- E = {1,2}\n\nFor prior-by-prior Bayesian updating, I need to update each prior μ ∈ P₂ using:\nμ^E(F) = μ(E ∩ F)/μ(E)\n\nFirst prior: μ₁ = (2/3, 1/6, 1/6)\n- μ₁(E) = μ₁({1,2}) = 2/3 + 1/6 = 4/6 + 1/6 = 5/6\n- μ₁^E({1}) = μ₁({1})/μ₁(E) = (2/3)/(5/6) = (2/3) × (6/5) = 4/5\n- μ₁^E({2}) = μ₁({2})/μ₁(E) = (1/6)/(5/6) = (1/6) × (6/5) = 1/5\n- μ₁^E({3}) = μ₁(∅)/μ₁(E) = 0\n\nSo μ₁^E = (4/5, 1/5, 0)\n\nSecond prior: μ₂ = (1/3, 1/3, 1/3)\n- μ₂(E) = μ₂({1,2}) = 1/3 + 1/3 = 2/3\n- μ₂^E({1}) = μ₂({1})/μ₂(E) = (1/3)/(2/3) = 1/2\n- μ₂^E({2}) = μ₂({2})/μ₂(E) = (1/3)/(2/3) = ", "model_answer": "I need to apply prior-by-prior Bayesian updating to find P₂^E where E = {1,2}.\n\nGiven:\n- P₂ = co{(2/3, 1/6, 1/6), (1/3, 1/3, 1/3)}\n- E = {1,2}\n\nFor prior-by-prior Bayesian updating, I need to update each prior μ ∈ P₂ using:\nμ^E(F) = μ(E ∩ F)/μ(E)\n\nFirst prior: μ₁ = (2/3, 1/6, 1/6)\n- μ₁(E) = μ₁({1,2}) = 2/3 + 1/6 = 4/6 + 1/6 = 5/6\n- μ₁^E({1}) = μ₁({1})/μ₁(E) = (2/3)/(5/6) = (2/3) × (6/5) = 4/5\n- μ₁^E({2}) = μ₁({2})/μ₁(E) = (1/6)/(5/6) = (1/6) × (6/5) = 1/5\n- μ₁^E({3}) = μ₁(∅)/μ₁(E) = 0\n\nSo μ₁^E = (4/5, 1/5, 0)\n\nSecond prior: μ₂ = (1/3, 1/3, 1/3)\n- μ₂(E) = μ₂({1,2}) = 1/3 + 1/3 = 2/3\n- μ₂^E({1}) = μ₂({1})/μ₂(E) = (1/3)/(2/3) = 1/2\n- μ₂^E({2}) = μ₂({2})/μ₂(E) = (1/3)/(2/3) =", "error": "Warning: Could not extract single choice letter from: I need to apply prior-by-prior Bayesian updating t..."}}
{"ID": 402, "Question": "### Background\n\n**Research Question.** This problem investigates whether a system of intergovernmental transfers, specifically matching grants, can correct the inefficiencies of decentralized public good provision and achieve the first-best (socially optimal) outcome in a union of heterogeneous countries.\n\n**Setting / Institutional Environment.** The union establishes a matching grant system. For each unit of public good `g_i` a country provides, the union provides a matching subsidy `s`. This is financed by a tax on each country `i` equal to `T_i = s \\cdot \\bar{g}_{-i}`, where `\\bar{g}_{-i}` is the average spending of all other countries. The subsidy rate `s` is chosen by majority vote, so the median voter's preference `α_m` is decisive.\n\n### Data / Model Specification\n\nThe utility from the public good is logarithmic, `H(x) = ln(x)`. The first-best optimal subsidy rate, which aligns private and social incentives, is `s^* = β(N-1)`. The politico-economic equilibrium subsidy `s_m` chosen by the median voter is given by the expression in Eq. (1):\n\n```latex\ns_{m} = \\cfrac{1+\\beta(N-1)}{1+\\cfrac{N}{(1-\\beta)(N-1)}\\left(\\cfrac{\\bar{\\alpha}-\\alpha_{m}}{\\alpha_{m}}\\right)} - 1 \n```\n\nwhere `α_m` is the preference parameter of the median voter and `\\bar{\\alpha} = (1/N)\\sum_i α_i` is the average preference parameter in the union.\n\n### Question\n\nIn a union where the median preference for public goods is less than the mean (`α_m < \\bar{\\alpha}`), the median voter strategically sets a subsidy `s_m` that is below the socially optimal level `s^*`. What is the primary political mechanism driving this inefficient outcome?", "Options": {"A": "The median voter sets a low subsidy to limit their own tax payments, which are inflated by the high spending of countries with above-average preferences.", "B": "The median voter sets a low subsidy to punish high-preference countries for not contributing enough to the union's budget.", "C": "The median voter has imperfect information about the optimal subsidy `s^*` and therefore makes a mistake.", "D": "The median voter is concerned that a high subsidy would lead to excessive centralization and a loss of national sovereignty."}, "Answer": "A", "pi_justification": "Assessment Target: Tests understanding of the political economy mechanism behind policy distortions. Strategy: Reverse-Reasoning, asking for the cause of a given outcome. Distractor Logic: (B) Conceptual Opposite: incorrectly assigns a punitive motive. (C) Almost Right: attributes the outcome to a behavioral error (imperfect information) rather than strategic choice. (D) True but Irrelevant: imports a valid concern from other models in the paper that is not the direct mechanism here.", "qid": "402", "question": "### Background\n\n**Research Question.** This problem investigates whether a system of intergovernmental transfers, specifically matching grants, can correct the inefficiencies of decentralized public good provision and achieve the first-best (socially optimal) outcome in a union of heterogeneous countries.\n\n**Setting / Institutional Environment.** The union establishes a matching grant system. For each unit of public good `g_i` a country provides, the union provides a matching subsidy `s`. This is financed by a tax on each country `i` equal to `T_i = s \\cdot \\bar{g}_{-i}`, where `\\bar{g}_{-i}` is the average spending of all other countries. The subsidy rate `s` is chosen by majority vote, so the median voter's preference `α_m` is decisive.\n\n### Data / Model Specification\n\nThe utility from the public good is logarithmic, `H(x) = ln(x)`. The first-best optimal subsidy rate, which aligns private and social incentives, is `s^* = β(N-1)`. The politico-economic equilibrium subsidy `s_m` chosen by the median voter is given by the expression in Eq. (1):\n\n```latex\ns_{m} = \\cfrac{1+\\beta(N-1)}{1+\\cfrac{N}{(1-\\beta)(N-1)}\\left(\\cfrac{\\bar{\\alpha}-\\alpha_{m}}{\\alpha_{m}}\\right)} - 1 \n```\n\nwhere `α_m` is the preference parameter of the median voter and `\\bar{\\alpha} = (1/N)\\sum_i α_i` is the average preference parameter in the union.\n\n### Question\n\nIn a union where the median preference for public goods is less than the mean (`α_m < \\bar{\\alpha}`), the median voter strategically sets a subsidy `s_m` that is below the socially optimal level `s^*`. What is the primary political mechanism driving this inefficient outcome?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the political mechanism behind the median voter's choice of subsidy when α_m < ᾱ.\n\nFrom equation (1), when α_m < ᾱ, the term (ᾱ - α_m)/α_m > 0, which makes the denominator larger and thus s_m < s*.\n\nThe key insight is in the tax structure: T_i = s · ḡ₋ᵢ\n\nEach country pays taxes based on the average spending of OTHER countries, not their own spending. This creates a strategic incentive problem.\n\nWhen α_m < ᾱ, the median voter has below-average preferences for public goods. However, they must pay taxes based on the spending of countries with higher preferences (since the average is above theirs). Countries with α_i > α_m will spend more on public goods, driving up ḡ₋ᵢ and thus increasing the median voter's tax burden.\n\nThe median voter anticipates this: if they set a high subsidy, it will encourage high-preference countries to spend even more (since they get matching funds), which increases ḡ₋ᵢ and inflates the median voter's tax payments. To avoid this, they strategically choose a lower subsidy s_m < s*.\n\nThis is a classic problem in fiscal federalism where the financing mechanism creates perverse incentives - the median voter internalizes the cost of others' higher spending through the tax system.\n\nA", "model_answer": "A", "error": null}}
{"ID": 478, "Question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. Defecting is a dominant strategy in the stage game, but cooperating (`$C_1, C_2$`) is the socially efficient outcome.\n\n### Data / Model Specification\n\nThe monitoring structure has the following properties:\n- Player 2's effort (`$C_2$` vs `$D_2$`) has a state-dependent effect. In state `$\\omega_1$`, effort is more productive than in state `$\\omega_2$`. \n- If Player 2 chooses to Defect (`$D_2$`), the public signal distribution is the same for both states, regardless of Player 1's action.\n\nThe paper demonstrates that a folk theorem can fail when **only Player 2 knows the state**.\n\n### Question\n\nWhat is the primary reason for the failure of the folk theorem in the case where only Player 2 knows their own productivity?", "Options": {"A": "The efficient outcome (`$C_1, C_2$`) is not individually rational for Player 1 in the low-productivity state.", "B": "Player 2 can unilaterally prevent Player 1 from learning the state (by defecting), making it impossible to enforce continuation payoffs that would punish him for being in the high-productivity state.", "C": "Player 1, being uninformed, cannot credibly commit to punishing Player 2 for failing to reveal the state.", "D": "The monitoring technology is state-independent when both players cooperate, so learning is never possible."}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This capstone item assesses the student's ability to synthesize the game's mechanics and incentives to explain the main result. It uses a Reverse-Reasoning strategy. The correct answer correctly links Player 2's ability to jam signals with his incentive to avoid punishments. Distractors include an Almost Right answer that misidentifies the core constraint (commitment vs. jamming), a Factually Incorrect statement about the monitoring technology, and a True but Irrelevant claim about stage-game payoffs.", "qid": "478", "question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. Defecting is a dominant strategy in the stage game, but cooperating (`$C_1, C_2$`) is the socially efficient outcome.\n\n### Data / Model Specification\n\nThe monitoring structure has the following properties:\n- Player 2's effort (`$C_2$` vs `$D_2$`) has a state-dependent effect. In state `$\\omega_1$`, effort is more productive than in state `$\\omega_2$`. \n- If Player 2 chooses to Defect (`$D_2$`), the public signal distribution is the same for both states, regardless of Player 1's action.\n\nThe paper demonstrates that a folk theorem can fail when **only Player 2 knows the state**.\n\n### Question\n\nWhat is the primary reason for the failure of the folk theorem in the case where only Player 2 knows their own productivity?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 474, "Question": "### Background\n\nTo isolate the effect of the *confidential* component of a supervisory signal like `CAMEL5`, an analyst must separate it from the influence of publicly available information.\n\n### Question\n\nAn analyst proposes a two-step procedure: first, regress `CAMEL5` on all publicly available bank data and collect the residuals; second, use these residuals as the key explanatory variable in the main forecast error regression. What is the most critical statistical pitfall of this approach?", "Options": {"A": "The standard errors in the second stage will be incorrect because the model fails to account for the estimation uncertainty of the first-stage residual.", "B": "The residuals from the first stage will be, by construction, correlated with the dependent variable in the second stage, causing omitted variable bias.", "C": "This procedure induces severe multicollinearity in the second stage because the residual is a linear combination of the public variables.", "D": "The first-stage regression is invalid because `CAMEL5` is an endogenous variable determined by bank health."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses advanced econometric knowledge about two-step estimation procedures. Strategy: Reverse-Reasoning (identifying a flaw in a proposed method). The correct answer identifies the 'generated regressor problem.' Distractors are other plausible but incorrect econometric issues that a student might confuse it with: a false claim about omitted variable bias, a false claim about multicollinearity (the residual is orthogonal to the regressors), and a more general endogeneity claim that misses the specific pitfall of the two-step method itself.", "qid": "474", "question": "### Background\n\nTo isolate the effect of the *confidential* component of a supervisory signal like `CAMEL5`, an analyst must separate it from the influence of publicly available information.\n\n### Question\n\nAn analyst proposes a two-step procedure: first, regress `CAMEL5` on all publicly available bank data and collect the residuals; second, use these residuals as the key explanatory variable in the main forecast error regression. What is the most critical statistical pitfall of this approach?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 473, "Question": "### Background\n\nA key justification for a central bank's role in bank supervision is that it may provide access to confidential information that improves monetary policy. This analysis examines whether confidential supervisory information contains predictive power for macroeconomic outcomes that is not already incorporated in official forecasts.\n\n### Data / Model Specification\n\nThe analysis starts with a baseline model:\n\n```latex\nActual Outcome = f(Forecast, CAMEL5, ...)\n```\n\nA second specification adds a vector of publicly available bank health variables (e.g., nonperforming loan ratio, capital-to-asset ratio) to this model. In the results for 4-quarter-ahead unemployment forecasts, the coefficient on `CAMEL5` remains positive and significant, while the coefficients on the public variables are insignificant.\n\n### Question\n\nWhat is the primary econometric purpose and conclusion of adding the publicly available bank health data to the model?", "Options": {"A": "To test if `CAMEL5`'s predictive power comes from its confidential component; the results suggest its power is from confidential information.", "B": "To improve the model's overall fit (R-squared); the results show that adding more variables makes the model better.", "C": "To check for multicollinearity between `CAMEL5` and public data; the results show they are highly correlated, making the `CAMEL5` coefficient unreliable.", "D": "To find the best predictive model of unemployment; the results indicate that `CAMEL5` is the only useful variable for forecasting."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the understanding of a key identification strategy: isolating a specific information channel by controlling for confounding variables. Strategy: Reverse-Reasoning (inferring purpose from method). Distractors are designed based on common misunderstandings of econometric practice: Almost Right (stating a true side-effect, like improved R-squared, as the primary purpose), Conceptual Opposite (confusing the goal of controlling for OVB with checking for multicollinearity), and Formula Misuse (misstating the research goal as pure prediction rather than causal channel identification).", "qid": "473", "question": "### Background\n\nA key justification for a central bank's role in bank supervision is that it may provide access to confidential information that improves monetary policy. This analysis examines whether confidential supervisory information contains predictive power for macroeconomic outcomes that is not already incorporated in official forecasts.\n\n### Data / Model Specification\n\nThe analysis starts with a baseline model:\n\n```latex\nActual Outcome = f(Forecast, CAMEL5, ...)\n```\n\nA second specification adds a vector of publicly available bank health variables (e.g., nonperforming loan ratio, capital-to-asset ratio) to this model. In the results for 4-quarter-ahead unemployment forecasts, the coefficient on `CAMEL5` remains positive and significant, while the coefficients on the public variables are insignificant.\n\n### Question\n\nWhat is the primary econometric purpose and conclusion of adding the publicly available bank health data to the model?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 367, "Question": "### Background\n\n**Research Question.** This problem analyzes a monopolist's endogenous choice of contractual liability (`w`) and its interaction with product safety (`\\pi`), demonstrating how liability can serve as a tool for price discrimination and identifying when this private contracting fails, necessitating legal intervention.\n\n**Setting.** A monopolist can offer a product via a simple contract `(\\pi, p, w)`, where the stipulated damage payment `w` is a choice variable for the firm. The firm's optimal strategy and the resulting market efficiency depend on how consumer risk (`x`) correlates with their gross benefit, a relationship captured by the parameter `b`.\n\n### Data / Model Specification\n\n**Model Components.**\n- Consumer type `x` is distributed on `[\\underline{x}, \\overline{x}]`.\n- A type-`x` consumer's gross benefit is `b_0 + bx`.\n- The probability of an accident is `\\pi x`, and the harm is `h`.\n- The socially optimal safety level `\\pi^{**}` is defined by `W'(\\pi^{**}) = 0`.\n- The threshold `b^{**}` is defined as `b^{**} = \\pi^{**}h`.\n\n### Question\n\nIn the case where `b > b^{**}`, the firm's private incentives diverge from the social optimum, leading to a market failure. Which of the following best describes the firm's profit-maximizing strategy and the underlying economic reason?", "Options": {"A": "The firm sets `w^*=0` and underinvests in safety (`\\pi > \\pi^{**}`) because a positive `w` would subsidize high-valuation inframarginal consumers, increasing the rents the firm seeks to extract.", "B": "The firm sets `w^*=h` and invests efficiently in safety (`\\pi = \\pi^{**}`) because this maximizes the total social surplus that the firm can capture.", "C": "The firm sets `w^*=0` and underinvests in safety (`\\pi > \\pi^{**}`) because the high cost of producing safer products makes it unprofitable to meet the social optimum.", "D": "The firm sets `w^* > 0` and overinvests in safety (`\\pi < \\pi^{**}`) because it needs to cater to the high-risk marginal consumer to maintain full market coverage."}, "Answer": "A", "pi_justification": "This item assesses the student's understanding of the paper's central market failure result. It uses a Reverse-Reasoning strategy: given the condition for market failure (`b > b^{**}`), the student must identify the correct firm strategy and its underlying cause. Distractor (B) is the policy solution, not the firm's choice. Distractor (C) offers a naive 'True but Irrelevant' reason, ignoring the core rent-extraction motive. Distractor (D) is a Conceptual Mix-up, applying the logic from the `b < b^{**}` case.", "qid": "367", "question": "### Background\n\n**Research Question.** This problem analyzes a monopolist's endogenous choice of contractual liability (`w`) and its interaction with product safety (`\\pi`), demonstrating how liability can serve as a tool for price discrimination and identifying when this private contracting fails, necessitating legal intervention.\n\n**Setting.** A monopolist can offer a product via a simple contract `(\\pi, p, w)`, where the stipulated damage payment `w` is a choice variable for the firm. The firm's optimal strategy and the resulting market efficiency depend on how consumer risk (`x`) correlates with their gross benefit, a relationship captured by the parameter `b`.\n\n### Data / Model Specification\n\n**Model Components.**\n- Consumer type `x` is distributed on `[\\underline{x}, \\overline{x}]`.\n- A type-`x` consumer's gross benefit is `b_0 + bx`.\n- The probability of an accident is `\\pi x`, and the harm is `h`.\n- The socially optimal safety level `\\pi^{**}` is defined by `W'(\\pi^{**}) = 0`.\n- The threshold `b^{**}` is defined as `b^{**} = \\pi^{**}h`.\n\n### Question\n\nIn the case where `b > b^{**}`, the firm's private incentives diverge from the social optimum, leading to a market failure. Which of the following best describes the firm's profit-maximizing strategy and the underlying economic reason?", "model_evaluation_result": {"model_raw_response": "I need to analyze this monopolist's problem where the firm chooses liability (w), safety (π), and price (p).\n\nKey elements:\n- Consumer type x distributed on [x̄, x̲]\n- Gross benefit: b₀ + bx\n- Accident probability: πx\n- Harm: h\n- Social optimum: π** where W'(π**) = 0\n- Threshold: b** = π**h\n- Given condition: b > b**\n\nWhen b > b**, this means the correlation between consumer risk (x) and gross benefit is high - high-risk consumers also have high valuations.\n\nLet me think through the firm's incentives:\n\n1) The firm wants to extract consumer surplus while managing the trade-off between safety costs and liability payments.\n\n2) When b > b**, high-risk consumers (high x) also have high valuations (high b₀ + bx). These are the consumers the firm most wants to serve and extract surplus from.\n\n3) Setting w > 0 would provide compensation to consumers who experience accidents. However, this would particularly benefit high-risk consumers (who have higher accident probability πx).\n\n4) Since high-risk consumers already have high valuations when b > b**, providing them additional compensation through w > 0 would be like giving a subsidy to the consumers the firm most wants to extract surplus from.\n\n5) Therefore, the firm sets w* = 0 to avoid subsidizing these high-valuation consumers.\n\n6) With w = 0, the firm has reduced incentives for safety investment since it doesn't bear the accident costs, leading to π > π** (underinvestment in safety).\n\nThe economic reason is that positive liability would subsidize high-valuation consumers, reducing the rents the firm can extract.\n\nA", "model_answer": "A", "error": null}}
{"ID": 400, "Question": "### Background\n\nThe Knapsack Deferred Acceptance (KDA) mechanism is not strategy-proof because its acceptance rule can be myopic: a family's acceptance in an early round may be revoked in a later round if a higher-priority, large family applies. The Threshold Knapsack Deferred Acceptance (TKDA) mechanism modifies KDA's rule to restore strategy-proofness.\n\n---\n\n### Question\n\nWhat is the core intuition behind TKDA's 'threshold' system that makes it strategy-proof?", "Options": {"A": "It performs 'precautionary rejections' of families whose tentative acceptance is not robust to the potential future arrival of any higher-priority proposers.", "B": "It reserves a portion of each locality's capacity to ensure that high-priority families who apply in later rounds can always be accommodated.", "C": "It prioritizes matching smaller families first to maximize the total number of matched families, thereby increasing overall efficiency.", "D": "It calculates a family's 'manipulation potential' and rejects any family whose preference report seems strategically designed to alter the outcome."}, "Answer": "A", "pi_justification": "This item assesses the conceptual understanding of the theoretical fix for KDA's manipulability. The correct answer (A) captures the core intuition of ensuring 'robust' acceptances. Distractors are designed to be plausible but incorrect explanations: (B) suggests a different kind of conservative mechanism (capacity reservation), (C) is a conceptually opposite efficiency argument, and (D) misinterprets strategy-proofness as manipulation detection.", "qid": "400", "question": "### Background\n\nThe Knapsack Deferred Acceptance (KDA) mechanism is not strategy-proof because its acceptance rule can be myopic: a family's acceptance in an early round may be revoked in a later round if a higher-priority, large family applies. The Threshold Knapsack Deferred Acceptance (TKDA) mechanism modifies KDA's rule to restore strategy-proofness.\n\n---\n\n### Question\n\nWhat is the core intuition behind TKDA's 'threshold' system that makes it strategy-proof?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 419, "Question": "### Background\n\nThe ultimate measure of the economic cost of price dispersion is in terms of welfare. This can be measured as a consumption equivalent, `Φ`, which is the permanent percentage increase in consumption required in the distorted economy to make the representative agent indifferent to living in the frictionless (`Δ=1`) economy.\n\n### Data / Model Specification\n\nFor a steady state, the consumption equivalent `Φ` is given by:\n\n```latex\n\\log(\\Phi) = DU^* \\times (\\Delta^{v+1} - 1)\n```\n(Eq. 1)\n\nwhere `DU*` is the disutility of labor in the efficient economy. A standard calibration implies `DU*` is approximately 0.12.\n\n**Table 1: Welfare Loss Parameters**\n\n| | Benchmark |\n| :--- | :--- |\n| `Δ` | 1.04 |\n| `v` (labor supply) | 1.8 |\n\nUsing the data from Table 1, the calibrated value for `DU*`, and Eq. (1), calculate the consumption-equivalent welfare cost (`Φ - 1`). Which of the following is closest to your result?", "Options": {"A": "1.4%", "B": "0.014%", "C": "2.2%", "D": "0.12%"}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a high-difficulty welfare calculation that requires careful application of a formula involving exponents and logarithms. It uses a Computational Judgment strategy. The correct answer (A) requires correctly calculating `Δ^{v+1}`, multiplying by `DU*`, and then exponentiating the result to find `Φ`. Distractor (B) is a Step-Omission Error from failing to exponentiate `log(Φ)` and/or misinterpreting the percentage conversion. Distractor (C) is the value reported in the paper's table, catching students who do not perform the requested calculation. Distractor (D) is the value of `DU*` itself, catching students who misunderstand the formula.", "qid": "419", "question": "### Background\n\nThe ultimate measure of the economic cost of price dispersion is in terms of welfare. This can be measured as a consumption equivalent, `Φ`, which is the permanent percentage increase in consumption required in the distorted economy to make the representative agent indifferent to living in the frictionless (`Δ=1`) economy.\n\n### Data / Model Specification\n\nFor a steady state, the consumption equivalent `Φ` is given by:\n\n```latex\n\\log(\\Phi) = DU^* \\times (\\Delta^{v+1} - 1)\n```\n(Eq. 1)\n\nwhere `DU*` is the disutility of labor in the efficient economy. A standard calibration implies `DU*` is approximately 0.12.\n\n**Table 1: Welfare Loss Parameters**\n\n| | Benchmark |\n| :--- | :--- |\n| `Δ` | 1.04 |\n| `v` (labor supply) | 1.8 |\n\nUsing the data from Table 1, the calibrated value for `DU*`, and Eq. (1), calculate the consumption-equivalent welfare cost (`Φ - 1`). Which of the following is closest to your result?", "model_evaluation_result": {"model_raw_response": "I need to calculate the consumption-equivalent welfare cost using the given formula and parameters.\n\nGiven:\n- Δ = 1.04\n- v = 1.8\n- DU* = 0.12\n- Formula: log(Φ) = DU* × (Δ^(v+1) - 1)\n\nStep 1: Calculate v + 1\nv + 1 = 1.8 + 1 = 2.8\n\nStep 2: Calculate Δ^(v+1)\nΔ^(v+1) = 1.04^2.8\n\nUsing logarithms: ln(1.04^2.8) = 2.8 × ln(1.04) = 2.8 × 0.03922 = 0.10982\nSo Δ^2.8 = e^0.10982 ≈ 1.1161\n\nStep 3: Calculate (Δ^(v+1) - 1)\nΔ^(v+1) - 1 = 1.1161 - 1 = 0.1161\n\nStep 4: Calculate log(Φ)\nlog(Φ) = DU* × (Δ^(v+1) - 1) = 0.12 × 0.1161 = 0.01393\n\nStep 5: Calculate Φ\nΦ = e^0.01393 ≈ 1.014\n\nStep 6: Calculate the welfare cost (Φ - 1)\nΦ - 1 = 1.014 - 1 = 0.014 = 1.4%\n\nB", "model_answer": "B", "error": null}}
{"ID": 423, "Question": "### Background\n\nThe paper's central claim is that CEO talent exhibits constant returns to scale (γ=1). This hypothesis can be tested using the results from the empirical model.\n\n### Data / Model Specification\n\nThe log-linear empirical specification is:\n\n```latex\n\\ln(w_{i,t+1}) = d + e \\cdot \\ln(S_{n_*,t}) + f \\cdot \\ln(S_{i,t}) + \\text{Controls} + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n```\n\nIn this model, `γ = e + f`. Therefore, testing `H₀: γ = 1` is equivalent to testing `H₀: e + f = 1`.\n\nThe regression results using firm fixed effects (the paper's preferred specification for this test) are:\n*   `ê = 0.78`\n*   `f̂ = 0.26`\n\nThe year-clustered standard errors and their covariance are:\n*   `SE(ê) = 0.083`\n*   `SE(f̂) = 0.043`\n*   `Cov(ê,f̂) = -0.003`\n\nThe variance of a sum of two coefficients is `Var(ê+f̂) = Var(ê) + Var(f̂) + 2Cov(ê,f̂)`.\n\n### Question\n\nUsing the provided results, conduct a t-test of the null hypothesis `H₀: γ = 1`. What is the correct conclusion at the 5% significance level (critical t-value ≈ 1.96)?", "Options": {"A": "Fail to reject H₀, as the t-statistic is approximately 0.76.", "B": "Reject H₀, as the point estimate of γ (1.04) is statistically greater than 1.", "C": "Fail to reject H₀, as the t-statistic is approximately 0.43.", "D": "Reject H₀, as the t-statistic is approximately 2.15."}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a formal hypothesis test on a linear combination of regression coefficients, a key skill in empirical analysis. It uses the Computational Judgment strategy. The correct answer (A) requires correctly calculating the variance of the sum including the covariance term: Var = (0.083)² + (0.043)² + 2(-0.003) = 0.002738. The SE is sqrt(0.002738) ≈ 0.0523. The t-stat is (1.04 - 1) / 0.0523 ≈ 0.765. Since |0.765| < 1.96, we fail to reject. Distractor (C) is the result of a classic error: ignoring the covariance term. In that case, Var = (0.083)² + (0.043)² = 0.008738, SE ≈ 0.0935, and t ≈ 0.43. Distractors (B) and (D) represent incorrect conclusions.", "qid": "423", "question": "### Background\n\nThe paper's central claim is that CEO talent exhibits constant returns to scale (γ=1). This hypothesis can be tested using the results from the empirical model.\n\n### Data / Model Specification\n\nThe log-linear empirical specification is:\n\n```latex\n\\ln(w_{i,t+1}) = d + e \\cdot \\ln(S_{n_*,t}) + f \\cdot \\ln(S_{i,t}) + \\text{Controls} + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n```\n\nIn this model, `γ = e + f`. Therefore, testing `H₀: γ = 1` is equivalent to testing `H₀: e + f = 1`.\n\nThe regression results using firm fixed effects (the paper's preferred specification for this test) are:\n*   `ê = 0.78`\n*   `f̂ = 0.26`\n\nThe year-clustered standard errors and their covariance are:\n*   `SE(ê) = 0.083`\n*   `SE(f̂) = 0.043`\n*   `Cov(ê,f̂) = -0.003`\n\nThe variance of a sum of two coefficients is `Var(ê+f̂) = Var(ê) + Var(f̂) + 2Cov(ê,f̂)`.\n\n### Question\n\nUsing the provided results, conduct a t-test of the null hypothesis `H₀: γ = 1`. What is the correct conclusion at the 5% significance level (critical t-value ≈ 1.96)?", "model_evaluation_result": {"model_raw_response": "I need to conduct a t-test for H₀: γ = 1, where γ = e + f.\n\nFirst, let me calculate the point estimate of γ:\nγ̂ = ê + f̂ = 0.78 + 0.26 = 1.04\n\nNext, I need to calculate the standard error of γ̂ using the formula:\nVar(ê + f̂) = Var(ê) + Var(f̂) + 2Cov(ê,f̂)\n\nGiven:\n- SE(ê) = 0.083, so Var(ê) = (0.083)² = 0.006889\n- SE(f̂) = 0.043, so Var(f̂) = (0.043)² = 0.001849\n- Cov(ê,f̂) = -0.003\n\nTherefore:\nVar(ê + f̂) = 0.006889 + 0.001849 + 2(-0.003)\nVar(ê + f̂) = 0.006889 + 0.001849 - 0.006\nVar(ê + f̂) = 0.002738\n\nSE(ê + f̂) = √0.002738 ≈ 0.0523\n\nNow I can calculate the t-statistic:\nt = (γ̂ - 1) / SE(γ̂) = (1.04 - 1) / 0.0523 = 0.04 / 0.0523 ≈ 0.765\n\nSince |0.765| < 1.96, we fail to reject H₀ at the 5% significance level.\n\nThe t-statistic is approximately 0.76.\n\nC", "model_answer": "C", "error": null}}
{"ID": 388, "Question": "### Background\n\n**Research Question.** This problem examines the statistical and economic significance of a parametric model for time-varying risk premia in asset returns.\n\n**Setting.** The analysis uses a bivariate GARCH-in-Mean (GARCH-M) specification for Japanese and World equity index excess returns. The model's validity is assessed through both specification tests (Likelihood Ratio test) and measures of fit (R-squared).\n\n**Variables and Parameters.**\n*   `R_{j,t}^*`: Excess return on the Japanese equity portfolio at time `t`.\n*   `μ`: The coefficient on the conditional risk premium term.\n\n---\n\n### Data / Model Specification\n\nThe empirical model for Japanese excess returns is:\n```latex\nR_{j,t}^{*} = \\gamma_{0j} + \\mu \\left( \\frac{h_{jw,t}}{h_{w,t}} \\right) \\mathrm{E}_{t-1}[R_{w,t}^*] + \\varepsilon_{j,t} \\quad \\text{(Eq. 1)}\n```\n\n**Table 1: Evidence on Fit and Risk Premia for the Japanese Return Equation**\n\n| Statistic | Value |\n| :--- | :--- |\n| Total Sum of Squares (`R*ⱼ`) | 0.70809 |\n| Residual Sum of Squares (`R*ⱼ`) | 0.63146 |\n| LR Test (`μ=0`) | 15.94 |\n| p-value for LR Test | (0.000) |\n\n---\n\n### Question\n\nTable 1 provides a Likelihood Ratio (LR) test for the significance of the risk premium (`μ=0`) and data to calculate the model's in-sample R-squared for Japanese excess returns (`R*ⱼ`). Which statement best synthesizes these two pieces of evidence?", "Options": {"A": "The risk premium is statistically significant and explains the vast majority (over 89%) of return variation.", "B": "The risk premium is statistically insignificant (p-value ≈ 0), and the model's low explanatory power of about 10.8% confirms this.", "C": "The risk premium is statistically significant (p-value ≈ 0), but its economic significance is modest, as the model explains only about 10.8% of the return variation.", "D": "Because the risk premium is highly statistically significant, the model's R-squared of about 10.8% must also be considered economically large and important."}, "Answer": "C", "pi_justification": "Assesses the ability to distinguish between statistical significance (from a p-value) and economic significance (from R-squared). This is a Computational Judgment item. The correct option (C) requires correctly interpreting the p-value as indicating high statistical significance while also correctly calculating R² = 1 - (0.63146 / 0.70809) ≈ 10.8% and judging it as modest. Distractor A represents a calculation error (using RSS/TSS for R²). Distractor B misinterprets the p-value. Distractor D represents the classic conceptual fallacy of equating statistical significance with large explanatory power.", "qid": "388", "question": "### Background\n\n**Research Question.** This problem examines the statistical and economic significance of a parametric model for time-varying risk premia in asset returns.\n\n**Setting.** The analysis uses a bivariate GARCH-in-Mean (GARCH-M) specification for Japanese and World equity index excess returns. The model's validity is assessed through both specification tests (Likelihood Ratio test) and measures of fit (R-squared).\n\n**Variables and Parameters.**\n*   `R_{j,t}^*`: Excess return on the Japanese equity portfolio at time `t`.\n*   `μ`: The coefficient on the conditional risk premium term.\n\n---\n\n### Data / Model Specification\n\nThe empirical model for Japanese excess returns is:\n```latex\nR_{j,t}^{*} = \\gamma_{0j} + \\mu \\left( \\frac{h_{jw,t}}{h_{w,t}} \\right) \\mathrm{E}_{t-1}[R_{w,t}^*] + \\varepsilon_{j,t} \\quad \\text{(Eq. 1)}\n```\n\n**Table 1: Evidence on Fit and Risk Premia for the Japanese Return Equation**\n\n| Statistic | Value |\n| :--- | :--- |\n| Total Sum of Squares (`R*ⱼ`) | 0.70809 |\n| Residual Sum of Squares (`R*ⱼ`) | 0.63146 |\n| LR Test (`μ=0`) | 15.94 |\n| p-value for LR Test | (0.000) |\n\n---\n\n### Question\n\nTable 1 provides a Likelihood Ratio (LR) test for the significance of the risk premium (`μ=0`) and data to calculate the model's in-sample R-squared for Japanese excess returns (`R*ⱼ`). Which statement best synthesizes these two pieces of evidence?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze both the statistical significance of the risk premium and the economic significance as measured by R-squared.\n\nFirst, let me calculate the R-squared:\nR² = 1 - (Residual Sum of Squares / Total Sum of Squares)\nR² = 1 - (0.63146 / 0.70809) = 1 - 0.8918 = 0.1082 ≈ 10.8%\n\nNext, let me interpret the LR test:\n- LR Test statistic = 15.94\n- p-value = 0.000 (essentially zero)\n- This tests H₀: μ = 0 vs H₁: μ ≠ 0\n\nWith a p-value of approximately 0, we reject the null hypothesis, meaning the risk premium coefficient μ is statistically significant.\n\nNow I need to synthesize these findings:\n\n1. Statistical significance: The risk premium is statistically significant (p ≈ 0)\n2. Economic significance: The model explains only 10.8% of return variation\n\nLet me evaluate each option:\n\nA) Incorrect - says \"over 89%\" but R² is only 10.8%\nB) Incorrect - says risk premium is insignificant, but p-value ≈ 0 means it IS significant\nC) Correct - acknowledges statistical significance (p ≈ 0) but notes modest economic significance (10.8% explanatory power)\nD) Incorrect - conflates statistical and economic significance; high statistical significance doesn't make 10.8% R² \"economically large\"\n\nThe key insight is distinguishing between statistical significance (whether an effect exists) and economic significance (whether the effect is practically meaningful). A coefficient can be statistically significant but still have limited economic impact.\n\nC", "model_answer": "A", "error": null}}
{"ID": 450, "Question": "### Background\n\nA central challenge in economic modeling is to ensure that the model's core assumptions are grounded in empirical reality. This problem examines how stylized facts about the Brazilian labor market are used to justify key structural assumptions in a two-sector model of informality.\n\n### Data / Model Specification\n\nThe following table presents key data on Brazil's labor market dynamics.\n\n**Table 1: Transition Matrix (in %)**\n\n| Origin (Row) | Destination (Column) | | | |\n|---|---|---|---|---|\n| | **Formal** | **Informal** | **Unemployed** | **Self-employed** |\n| **Formal salaried** | 84.9 | 7.2 | 4.7 | 3.2 |\n| **Informal salaried** | 24.3 | 53.6 | 8.4 | 13.7 |\n| **Unemployed** | 23.5 | 25.3 | 38.7 | 12.6 |\n\nOne key assumption of the theoretical model is that the exogenous job separation rate is higher in the informal sector than the formal sector (`$s_I > s_F$`).\n\n### Question\n\nBased on the transition matrix in Table 1, what are the approximate annual exit probabilities for formal and informal salaried workers, and what key model assumption does this evidence support?", "Options": {"A": "Formal exit probability is 15.1%; Informal is 46.4%. This supports the assumption that the informal sector separation rate is higher (`$s_I > s_F$`).", "B": "Formal exit probability is 84.9%; Informal is 53.6%. This supports the assumption that the formal sector separation rate is higher (`$s_F > s_I$`).", "C": "Formal exit probability is 15.1%; Informal is 46.4%. This supports the assumption that the formal sector separation rate is higher (`$s_F > s_I$`).", "D": "The probability of an unemployed worker finding a formal job is 23.5%. This supports the assumption of undirected search."}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a direct calculation from tabular data and link the result to a core theoretical assumption of the model. It uses a Computational Judgment strategy. Distractors are designed based on common errors: B uses the raw 'stay' probabilities instead of calculating 'exit' probabilities; C performs the calculation correctly but draws the opposite conceptual conclusion; D uses correct data from the table but links it to an irrelevant assumption, testing precision.", "qid": "450", "question": "### Background\n\nA central challenge in economic modeling is to ensure that the model's core assumptions are grounded in empirical reality. This problem examines how stylized facts about the Brazilian labor market are used to justify key structural assumptions in a two-sector model of informality.\n\n### Data / Model Specification\n\nThe following table presents key data on Brazil's labor market dynamics.\n\n**Table 1: Transition Matrix (in %)**\n\n| Origin (Row) | Destination (Column) | | | |\n|---|---|---|---|---|\n| | **Formal** | **Informal** | **Unemployed** | **Self-employed** |\n| **Formal salaried** | 84.9 | 7.2 | 4.7 | 3.2 |\n| **Informal salaried** | 24.3 | 53.6 | 8.4 | 13.7 |\n| **Unemployed** | 23.5 | 25.3 | 38.7 | 12.6 |\n\nOne key assumption of the theoretical model is that the exogenous job separation rate is higher in the informal sector than the formal sector (`$s_I > s_F$`).\n\n### Question\n\nBased on the transition matrix in Table 1, what are the approximate annual exit probabilities for formal and informal salaried workers, and what key model assumption does this evidence support?", "model_evaluation_result": {"model_raw_response": "Looking at the transition matrix, I need to calculate the exit probabilities for formal and informal salaried workers.\n\nFor **formal salaried workers**:\n- Stay in formal: 84.9%\n- Exit probability = 100% - 84.9% = 15.1%\n\nFor **informal salaried workers**:\n- Stay in informal: 53.6%  \n- Exit probability = 100% - 53.6% = 46.4%\n\nThe exit probability represents the job separation rate. Since informal workers have a much higher exit probability (46.4%) compared to formal workers (15.1%), this means the informal sector has a higher separation rate.\n\nThis empirical evidence supports the theoretical model's assumption that s_I > s_F (the exogenous job separation rate is higher in the informal sector than the formal sector).\n\nA", "model_answer": "A", "error": null}}
{"ID": 447, "Question": "### Background\n\n**Setting.** In the general model of an optimal hierarchy, the firm chooses effort `a_t`, span of control `s_t`, and wages `w_t` at each tier `t`.\n\n---\n\n### Data / Model Specification\n\nThe solution to the firm's problem yields two key results:\n1.  The rate of change of wages is given by: `\\dot{w}_t / w_t = 1 - \\log(s_t)`\n2.  The optimal span of control satisfies the inequality: `s_t > e` (where `e` is Euler's number, approx. 2.718)\n\n---\n\n### Question\n\nGiven these two results from the model, what can be definitively concluded about the optimal wage profile?", "Options": {"A": "The rate of change of wages, `\\dot{w}_t`, must be negative.", "B": "The rate of change of wages, `\\dot{w}_t`, must be positive.", "C": "The rate of change of wages, `\\dot{w}_t`, must be zero.", "D": "The sign of `\\dot{w}_t` cannot be determined without knowing the exact value of `s_t`."}, "Answer": "A", "pi_justification": "This item uses a Computational Judgment strategy to test the student's ability to perform a crucial step in the paper's main proof. It requires connecting the condition `s_t > e` to the wage dynamics equation to deduce the sign of `\\dot{w}_t`. Distractors represent a sign error (B), a confusion with other models (C), and a failure to see the definitive implication of the provided information (D).", "qid": "447", "question": "### Background\n\n**Setting.** In the general model of an optimal hierarchy, the firm chooses effort `a_t`, span of control `s_t`, and wages `w_t` at each tier `t`.\n\n---\n\n### Data / Model Specification\n\nThe solution to the firm's problem yields two key results:\n1.  The rate of change of wages is given by: `\\dot{w}_t / w_t = 1 - \\log(s_t)`\n2.  The optimal span of control satisfies the inequality: `s_t > e` (where `e` is Euler's number, approx. 2.718)\n\n---\n\n### Question\n\nGiven these two results from the model, what can be definitively concluded about the optimal wage profile?", "model_evaluation_result": {"model_raw_response": "Looking at the given information:\n\n1. The rate of change of wages: `\\dot{w}_t / w_t = 1 - \\log(s_t)`\n2. The optimal span of control constraint: `s_t > e`\n\nSince `s_t > e`, we have:\n- `\\log(s_t) > \\log(e)`\n- `\\log(s_t) > 1` (since `\\log(e) = 1`)\n\nTherefore:\n- `1 - \\log(s_t) < 1 - 1 = 0`\n\nThis means `\\dot{w}_t / w_t < 0`.\n\nSince wages `w_t` are positive (wages cannot be negative), we have `w_t > 0`.\n\nTherefore: `\\dot{w}_t = w_t \\times (1 - \\log(s_t)) < 0`\n\nThe rate of change of wages must be negative.\n\nA", "model_answer": "A", "error": null}}
{"ID": 415, "Question": "### Background\n\n**Research Question.** This problem evaluates the Time-Varying Parameter (TVP) ARCH-M model against the standard Fixed-Parameter (FP) ARCH-M model, investigating whether allowing the price of risk to vary over time can resolve key empirical puzzles in modeling stock returns.\n\n**Setting / Institutional Environment.** Standard FP ARCH-M models, which assume a constant risk-return tradeoff, have been found to produce unstable parameter estimates across different sample periods and to systematically over-predict the equity risk premium. The TVP ARCH-M model is proposed as a solution by allowing the price of risk to evolve as a random walk.\n\n### Data / Model Specification\n\nThe standard **Fixed-Parameter (FP) ARCH-M** model is:\n```latex\ny_t = c h_t + e_t\n```\nThe proposed **Time-Varying Parameter (TVP) ARCH-M** model is specified in a state-space form:\n```latex\ny_t = b_t h_t + e_t\n```\n```latex\nb_t = b_{t-1} + v_t\n```\nwhere `c` is the constant price of risk and `b_t` is the time-varying price of risk.\n\n**Table 1: Summary Statistics of Risk Premiums (Monthly, %)**\n\n| Risk Premium | Mean |\n| :--- | :--- |\n| Realized (`rs`) | 0.64 |\n| TVP Predicted | 0.54 |\n| FP Predicted | 0.96 |\n\n**Table 2: Predictive Regressions for Realized Returns (1926-1985)**\n\n| Model | Intercept | Coeff on TVP Prediction | Coeff on FP Prediction | R² |\n| :--- | :--- | :--- | :--- | :--- |\n| 1. TVP only | -0.044 (0.277) | 1.268 (0.333) | | 0.020 |\n| 2. FP only | 0.342 (0.265) | | 0.311 (0.166) | 0.005 |\n\n*Standard errors are in parentheses.*\n\n### Question\n\nBased on the evidence in Table 1 and Table 2, which of the following statements most accurately summarizes the performance of the FP and TVP models?", "Options": {"A": "The FP model's average predicted premium is 50% higher than the realized average, and a t-test fails to reject the hypothesis that the TVP model's predictions are unbiased.", "B": "The FP model's average predicted premium is 50% higher than the realized average, but a t-test rejects the hypothesis that the TVP model's predictions are unbiased at the 5% level.", "C": "The FP model's average predicted premium is 33% higher than the realized average, and a t-test fails to reject the hypothesis that the TVP model's predictions are unbiased.", "D": "The FP model's average predicted premium is 33% higher than the realized average, and a t-test rejects the hypothesis that the TVP model's predictions are unbiased at the 5% level."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses core quantitative takeaways from the paper's model comparison using a Computational Judgment strategy. The user must perform two distinct calculations/judgments: (1) calculate the percentage overprediction of the FP model from Table 1, and (2) conduct and interpret a t-test for unbiasedness (H0: coeff=1) on the TVP model's coefficient from Table 2. Distractors are designed based on common errors: B correctly calculates the overprediction but incorrectly concludes the t-test; C makes a calculation error ((0.96-0.64)/0.96) but gets the t-test right; D is incorrect on both fronts.", "qid": "415", "question": "### Background\n\n**Research Question.** This problem evaluates the Time-Varying Parameter (TVP) ARCH-M model against the standard Fixed-Parameter (FP) ARCH-M model, investigating whether allowing the price of risk to vary over time can resolve key empirical puzzles in modeling stock returns.\n\n**Setting / Institutional Environment.** Standard FP ARCH-M models, which assume a constant risk-return tradeoff, have been found to produce unstable parameter estimates across different sample periods and to systematically over-predict the equity risk premium. The TVP ARCH-M model is proposed as a solution by allowing the price of risk to evolve as a random walk.\n\n### Data / Model Specification\n\nThe standard **Fixed-Parameter (FP) ARCH-M** model is:\n```latex\ny_t = c h_t + e_t\n```\nThe proposed **Time-Varying Parameter (TVP) ARCH-M** model is specified in a state-space form:\n```latex\ny_t = b_t h_t + e_t\n```\n```latex\nb_t = b_{t-1} + v_t\n```\nwhere `c` is the constant price of risk and `b_t` is the time-varying price of risk.\n\n**Table 1: Summary Statistics of Risk Premiums (Monthly, %)**\n\n| Risk Premium | Mean |\n| :--- | :--- |\n| Realized (`rs`) | 0.64 |\n| TVP Predicted | 0.54 |\n| FP Predicted | 0.96 |\n\n**Table 2: Predictive Regressions for Realized Returns (1926-1985)**\n\n| Model | Intercept | Coeff on TVP Prediction | Coeff on FP Prediction | R² |\n| :--- | :--- | :--- | :--- | :--- |\n| 1. TVP only | -0.044 (0.277) | 1.268 (0.333) | | 0.020 |\n| 2. FP only | 0.342 (0.265) | | 0.311 (0.166) | 0.005 |\n\n*Standard errors are in parentheses.*\n\n### Question\n\nBased on the evidence in Table 1 and Table 2, which of the following statements most accurately summarizes the performance of the FP and TVP models?", "model_evaluation_result": {"model_raw_response": "I need to analyze the performance of both models based on the provided tables.\n\nFrom Table 1:\n- Realized risk premium: 0.64%\n- TVP predicted: 0.54%\n- FP predicted: 0.96%\n\nFirst, let me calculate how much higher the FP model's prediction is compared to the realized average:\nFP over-prediction = (0.96 - 0.64) / 0.64 = 0.32 / 0.64 = 0.50 = 50%\n\nSo the FP model's average predicted premium is 50% higher than the realized average.\n\nNext, I need to test whether the TVP model's predictions are unbiased. From Table 2, Model 1 (TVP only):\n- Intercept: -0.044 (0.277)\n- Coefficient on TVP prediction: 1.268 (0.333)\n\nFor unbiased predictions, we would expect:\n- Intercept = 0\n- Coefficient = 1\n\nTesting the intercept = 0:\nt-statistic = -0.044 / 0.277 = -0.159\nThis is not significant at the 5% level (|t| < 1.96).\n\nTesting the coefficient = 1:\nt-statistic = (1.268 - 1) / 0.333 = 0.268 / 0.333 = 0.805\nThis is also not significant at the 5% level.\n\nSince both tests fail to reject their null hypotheses (intercept = 0 and coefficient = 1), we fail to reject the hypothesis that the TVP model's predictions are unbiased.\n\nTherefore, the FP model's average predicted premium is 50% higher than the realized average, and a t-test fails to reject the hypothesis that the TVP model's predictions are unbiased.\n\nA", "model_answer": "A", "error": null}}
{"ID": 426, "Question": "### Background\n\nA key finding from the paper's calibrated model is that swapping the 250th-ranked CEO for the 1st-ranked CEO would increase the firm's value by 0.016%. Given the firm's market capitalization of $25 billion, this translates to an absolute value creation of approximately $4.0 million.\n\nThe paper argues that the dramatic rise in CEO pay is primarily due to the increase in firm scale over time.\n\n### Question\n\nConsider a counterfactual 1960s economy where the distributions of talent and firm size (`α, β, γ`) were the same as today, but the scale of firms was 100 times smaller. The 250th firm had a market capitalization of $250 million. In this economy, what would be the absolute dollar value created by swapping the #250 CEO for the #1 CEO, and what is the main conclusion from this comparison?", "Options": {"A": "Value created: $40,000. Conclusion: The rise in CEO pay is driven by the magnification of talent's value due to the enormous increase in firm scale.", "B": "Value created: $4.0 million. Conclusion: The rise in CEO pay is driven by a significant increase in the dispersion of CEO talent since the 1960s.", "C": "Value created: $400,000. Conclusion: The rise in CEO pay is primarily due to a decline in corporate governance, allowing CEOs to extract more rents.", "D": "Value created: $40,000. Conclusion: The rise in CEO pay is due to the fact that modern CEOs are vastly more talented than their 1960s counterparts."}, "Answer": "A", "pi_justification": "This item uses a Scenario Application strategy to test the core intuition of the paper's main quantitative result. The user must perform a simple calculation and draw the correct inference. The correct calculation is $4.0 million / 100 = $40,000 (or 0.016% of $250 million). The correct conclusion is that scale is the magnifier. Option (A) is correct. Option (B) gets the calculation wrong and draws a conclusion that contradicts the premise (that talent distribution is the same). Option (C) gets the calculation wrong (off by 10x) and offers an alternative explanation (governance) that is not the mechanism tested here. Option (D) has the right value but the wrong interpretation, contradicting the premise that the talent distribution is constant.", "qid": "426", "question": "### Background\n\nA key finding from the paper's calibrated model is that swapping the 250th-ranked CEO for the 1st-ranked CEO would increase the firm's value by 0.016%. Given the firm's market capitalization of $25 billion, this translates to an absolute value creation of approximately $4.0 million.\n\nThe paper argues that the dramatic rise in CEO pay is primarily due to the increase in firm scale over time.\n\n### Question\n\nConsider a counterfactual 1960s economy where the distributions of talent and firm size (`α, β, γ`) were the same as today, but the scale of firms was 100 times smaller. The 250th firm had a market capitalization of $250 million. In this economy, what would be the absolute dollar value created by swapping the #250 CEO for the #1 CEO, and what is the main conclusion from this comparison?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand the relationship between firm scale and value creation from CEO talent.\n\nGiven information:\n- Current economy: Swapping #250 CEO for #1 CEO creates 0.016% value increase\n- Current firm market cap: $25 billion\n- Current absolute value created: $4.0 million (0.016% × $25 billion)\n\nCounterfactual 1960s economy:\n- Same talent distribution (α, β, γ)\n- Firm scale is 100 times smaller\n- 250th firm market cap: $250 million (vs $25 billion today)\n\nThe key insight is that if talent distributions are the same but firm scale is 100 times smaller, then the percentage impact of swapping CEOs should be the same (0.016%), but applied to a much smaller base.\n\nValue created in 1960s economy:\n0.016% × $250 million = $40,000\n\nThis is exactly 1/100th of the current value creation ($4.0 million ÷ 100 = $40,000).\n\nThe comparison shows that even with identical talent distributions, the absolute dollar value of CEO talent is magnified by firm scale. When firms are 100 times larger, the same talent differential creates 100 times more absolute value.\n\nThis supports the paper's argument that rising CEO pay is primarily driven by increasing firm scale over time, not changes in talent distribution or governance issues.\n\nA", "model_answer": "A", "error": null}}
{"ID": 436, "Question": "### Background\n\n**Research Question.** After estimating the causal response of monetary policy to the stock market, a crucial step is to assess its real-world plausibility. This can be done by comparing the estimated response to a theoretical benchmark derived from the stock market's known effects on aggregate demand via the wealth channel, as disciplined by a standard monetary policy rule.\n\n### Data / Model Specification\n\nThe paper's primary estimate for the policy response parameter, `β`, is 0.0214. For the macroeconomic plausibility check, use the following parameters, which are stated or implied in the paper:\n\n- Total US GDP: $10 trillion\n- Household equity holdings: $11.6 trillion\n- Marginal Propensity to Consume (MPC) out of stock market wealth: 4% (0.04)\n- Taylor rule coefficient on the output gap: 1.0\n\n### Question\n\nBased on the provided macroeconomic parameters, what is the 'warranted' change in the federal funds rate (in basis points) to fully offset the aggregate demand impact of an unexpected 5 percent rise in the S&P 500?", "Options": {"A": "11.6 basis points", "B": "14.3 basis points", "C": "23.2 basis points", "D": "58.0 basis points"}, "Answer": "C", "pi_justification": "This item assesses the ability to perform a multi-step 'back-of-the-envelope' calculation to evaluate the plausibility of an econometric finding. It uses a Computational Judgment strategy. The correct answer requires correctly chaining four calculation steps. Distractor A is a Formula Misuse error, using an alternative parameter mentioned in the text (a Taylor coefficient of 0.5). Distractor B is a 'True but Irrelevant' conceptual error, representing the paper's actual estimated policy response rather than the warranted response. Distractor D represents a plausible unit or step-omission error.", "qid": "436", "question": "### Background\n\n**Research Question.** After estimating the causal response of monetary policy to the stock market, a crucial step is to assess its real-world plausibility. This can be done by comparing the estimated response to a theoretical benchmark derived from the stock market's known effects on aggregate demand via the wealth channel, as disciplined by a standard monetary policy rule.\n\n### Data / Model Specification\n\nThe paper's primary estimate for the policy response parameter, `β`, is 0.0214. For the macroeconomic plausibility check, use the following parameters, which are stated or implied in the paper:\n\n- Total US GDP: $10 trillion\n- Household equity holdings: $11.6 trillion\n- Marginal Propensity to Consume (MPC) out of stock market wealth: 4% (0.04)\n- Taylor rule coefficient on the output gap: 1.0\n\n### Question\n\nBased on the provided macroeconomic parameters, what is the 'warranted' change in the federal funds rate (in basis points) to fully offset the aggregate demand impact of an unexpected 5 percent rise in the S&P 500?", "model_evaluation_result": {"model_raw_response": "I need to calculate the warranted change in the federal funds rate to offset the aggregate demand impact of a 5% rise in the S&P 500.\n\nLet me work through this step by step:\n\n1) First, calculate the wealth effect from the stock market increase:\n   - 5% rise in S&P 500 means household equity holdings increase by 5%\n   - Increase in wealth = 0.05 × $11.6 trillion = $0.58 trillion\n\n2) Calculate the consumption response:\n   - MPC out of stock market wealth = 0.04\n   - Increase in consumption = 0.04 × $0.58 trillion = $0.0232 trillion\n\n3) Calculate the output gap effect:\n   - Output gap = (Change in consumption) / (Total GDP)\n   - Output gap = $0.0232 trillion / $10 trillion = 0.00232 = 0.232%\n\n4) Calculate the warranted federal funds rate change:\n   - Taylor rule coefficient on output gap = 1.0\n   - Required rate change = 1.0 × 0.232% = 0.232 percentage points\n   - Convert to basis points: 0.232 × 100 = 23.2 basis points\n\nThe calculation shows that to fully offset the aggregate demand impact of a 5% rise in the S&P 500, the federal funds rate should increase by 23.2 basis points.\n\nC", "model_answer": "C", "error": null}}
{"ID": 397, "Question": "### Background\n\n**Research Question:** This problem examines the implications of the paper's main theorem for the theory of two-stage budgeting, showing how simultaneous direct and indirect weak separability leads to a modified version of the Strotz-Gorman aggregation framework.\n\n**Setting:** We consider a region of the normalized price space, `y ∈ B_s`, where the conditions of the main theorem hold. Specifically, the indirect sub-utility `v^s` is not quasi-homothetic, but all other `v^t` (`t ≠ s`) are homogeneous of degree one and the aggregator `G` is `C_s H_0`.\n\n### Data / Model Specification\n\nIn the region `y ∈ B_s`, the `C_s H_0` property of the aggregator `G` implies the following relationship for `ν(y) = ∑ v_i y_i`:\n```latex\n\\nu(y) = G_s(w) [\\nu^s(y^s) + \\eta^s(v^s(y^s))] \\quad \\text{(Eq. 1)}\n```\nwhere `v^s(y^s)` is the indirect sub-utility for group `s`, `ν^s(y^s) = ∑_{i∈N_s} v_i^s y_i`, and `η^s` is a function of `v^s`.\n\nThe first-order conditions for the dual problem for goods `i ∈ N_s` are:\n```latex\nv_i(y) = G_s(w) v_i^s(y^s) = \\lambda x_i \\quad \\text{(Eq. 2)}\n```\nwhere `λ = ν(y)` at the optimum. Expenditure on group `s` is `I_s = I y^s x^s`.\n\nThe paper shows that these conditions lead to the following expenditure equation for `y ∈ B_s`:\n```latex\nI_s = \\frac{\\nu^s(y^s)}{\\nu^s(y^s) + \\eta^s(v^s(y^s))} I \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nThe paper demonstrates that when preferences are both directly and indirectly weakly separable, the conditions for standard Strotz-Gorman 'perfect aggregation' are not met in the region `y ∈ B_s`. Based on the expenditure equation for group `s` (Eq. 3), what is the primary reason for this deviation?", "Options": {"A": "The budget share for group `s` depends on two separate functions of its prices (`v^s` and `ν^s`), not a single price index, which necessitates an additional differential price index (`σ_s`) for local aggregation.", "B": "The expenditure `I_s` is no longer a function of total income `I`, violating the basic premise of two-stage budgeting.", "C": "The aggregator function `G` becomes non-monotonic in the region `B_s`, which invalidates the first-order conditions.", "D": "All sub-utility functions `v^t` (for `t=1,...,S`) become quasi-homothetic, leading to simple Klein aggregation, which is a special case of, not a deviation from, Gorman aggregation."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the ability to identify the core reason for a theoretical result's departure from a standard benchmark. It uses a Reverse-Reasoning strategy. Distractor B is a direct misreading of the formula. Distractor C introduces a claim that contradicts the model's core assumptions. Distractor D correctly describes the behavior in a different region of the price space (`B_0`), making it a sophisticated 'true but irrelevant' option.", "qid": "397", "question": "### Background\n\n**Research Question:** This problem examines the implications of the paper's main theorem for the theory of two-stage budgeting, showing how simultaneous direct and indirect weak separability leads to a modified version of the Strotz-Gorman aggregation framework.\n\n**Setting:** We consider a region of the normalized price space, `y ∈ B_s`, where the conditions of the main theorem hold. Specifically, the indirect sub-utility `v^s` is not quasi-homothetic, but all other `v^t` (`t ≠ s`) are homogeneous of degree one and the aggregator `G` is `C_s H_0`.\n\n### Data / Model Specification\n\nIn the region `y ∈ B_s`, the `C_s H_0` property of the aggregator `G` implies the following relationship for `ν(y) = ∑ v_i y_i`:\n```latex\n\\nu(y) = G_s(w) [\\nu^s(y^s) + \\eta^s(v^s(y^s))] \\quad \\text{(Eq. 1)}\n```\nwhere `v^s(y^s)` is the indirect sub-utility for group `s`, `ν^s(y^s) = ∑_{i∈N_s} v_i^s y_i`, and `η^s` is a function of `v^s`.\n\nThe first-order conditions for the dual problem for goods `i ∈ N_s` are:\n```latex\nv_i(y) = G_s(w) v_i^s(y^s) = \\lambda x_i \\quad \\text{(Eq. 2)}\n```\nwhere `λ = ν(y)` at the optimum. Expenditure on group `s` is `I_s = I y^s x^s`.\n\nThe paper shows that these conditions lead to the following expenditure equation for `y ∈ B_s`:\n```latex\nI_s = \\frac{\\nu^s(y^s)}{\\nu^s(y^s) + \\eta^s(v^s(y^s))} I \\quad \\text{(Eq. 3)}\n```\n\n### Question\n\nThe paper demonstrates that when preferences are both directly and indirectly weakly separable, the conditions for standard Strotz-Gorman 'perfect aggregation' are not met in the region `y ∈ B_s`. Based on the expenditure equation for group `s` (Eq. 3), what is the primary reason for this deviation?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze why the standard Strotz-Gorman 'perfect aggregation' conditions are not met in region B_s based on the given expenditure equation.\n\nLet me examine the key expenditure equation (Eq. 3):\n```\nI_s = [ν^s(y^s)] / [ν^s(y^s) + η^s(v^s(y^s))] × I\n```\n\nIn standard Strotz-Gorman aggregation, the budget share for a group should depend only on a single price index for that group. However, looking at this equation, I can see that the budget share for group s depends on:\n\n1. ν^s(y^s) = Σ_{i∈N_s} v_i^s y_i (which is one function of the prices in group s)\n2. η^s(v^s(y^s)) (which is another function, specifically a function of the indirect sub-utility v^s(y^s))\n\nSo the budget share depends on TWO separate functions of the group's prices: ν^s and η^s(v^s), rather than a single price index as required for perfect aggregation.\n\nLet me check the other options:\n\nB) The expenditure I_s is still a function of total income I (it's proportional to I), so this doesn't violate two-stage budgeting.\n\nC) There's no indication that G becomes non-monotonic - this isn't the issue being highlighted.\n\nD) The problem states that v^s is NOT quasi-homothetic, and the other v^t are homogeneous of degree one, so this description is incorrect.\n\nThe answer is A - the budget share depends on two separate functions of prices rather than a single price index, which necessitates additional complexity (the differential price index σ_s mentioned) for local aggregation.\n\nA", "model_answer": "B", "error": null}}
{"ID": 452, "Question": "### Background\n\nA central thesis of the paper is that a coherent definition of causality in static simultaneous equations can be grounded in the structure of an underlying dynamic system.\n\n### Data / Model Specification\n\nConsider the following two-variable triangular dynamic system, where `a, b, c, m, n` are positive constants. The system's structure implies that `x` is causally prior to `y`.\n```latex\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -ax + m \\quad \\text{(Eq. 1)}\n```\n```latex\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = bx - cy + n \\quad \\text{(Eq. 2)}\n```\nIn equilibrium, the time derivatives are zero.\n\n### Question\n\nWhat is the equilibrium value `y*` for this system?", "Options": {"A": "`y* = (n + bm/a) / c`", "B": "`y* = (n + bx*) / c`", "C": "`y* = n/c`", "D": "`y* = (n - bm/a) / c`"}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a multi-step calculation to find an equilibrium value, a core skill for applying the paper's concepts. It uses the Computational Judgment strategy. Distractor B represents a Step-Omission Error (failing to substitute for x*). Distractor C represents a Formula Misuse error (ignoring the causal influence of x). Distractor D represents a common Sign Error.", "qid": "452", "question": "### Background\n\nA central thesis of the paper is that a coherent definition of causality in static simultaneous equations can be grounded in the structure of an underlying dynamic system.\n\n### Data / Model Specification\n\nConsider the following two-variable triangular dynamic system, where `a, b, c, m, n` are positive constants. The system's structure implies that `x` is causally prior to `y`.\n```latex\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -ax + m \\quad \\text{(Eq. 1)}\n```\n```latex\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = bx - cy + n \\quad \\text{(Eq. 2)}\n```\nIn equilibrium, the time derivatives are zero.\n\n### Question\n\nWhat is the equilibrium value `y*` for this system?", "model_evaluation_result": {"model_raw_response": "I need to find the equilibrium value y* by setting the time derivatives equal to zero and solving the resulting system.\n\nIn equilibrium, both time derivatives equal zero:\n- dx/dt = 0\n- dy/dt = 0\n\nFrom Equation 1:\ndx/dt = -ax + m = 0\nThis gives us: ax = m\nTherefore: x* = m/a\n\nFrom Equation 2:\ndy/dt = bx - cy + n = 0\nThis gives us: bx - cy + n = 0\nRearranging: cy = bx + n\nTherefore: y = (bx + n)/c\n\nSubstituting the equilibrium value x* = m/a into the expression for y:\ny* = (b(m/a) + n)/c\ny* = (bm/a + n)/c\ny* = (n + bm/a)/c\n\nA", "model_answer": "A", "error": null}}
{"ID": 448, "Question": "### Background\n\n**Research Question.** In a dynamic general equilibrium setting with a constant mark-up pricing rule, what determines the equilibrium rate of output price inflation?\n\n**Setting.** The model considers a firm with a Cobb-Douglas production technology that faces constantly changing input prices and ongoing technological progress. The core assumption is that the firm maintains a constant markup, which implies that the output price \\(P_t\\) must grow at the same rate as the minimized unit cost of production.\n\n**Variables & Parameters.**\n- \\( p \\): Expected rate of change of the firm's output price (\\(\\dot{P}/P\\)).\n- \\( w \\): Expected rate of growth of the money wage rate (\\(\\dot{W}/W\\)).\n- \\( g \\): Rate of Harrod-neutral (labor-augmenting) embodied technical progress.\n- \\( q^* \\): Expected rate of change of the price of capital goods relative to the price of output.\n- \\( a \\): The output elasticity of labor in the Cobb-Douglas production function.\n\n---\n\n### Data / Model Specification\n\nThe equilibrium rate of output price inflation is determined by the growth rates of factor prices and technology:\n\n```latex\np = w - g + q^{*} \\frac{1-a}{a} \n\\quad \\text{(Eq. (1))}\n```\n\n---\n\nAccording to the model's equilibrium price dynamics (Eq. (1)), what is the combined effect of an increase in the rate of Harrod-neutral technological progress (\\(g\\)) and an increase in the expected rate of wage growth (\\(w\\)) on the equilibrium rate of output price inflation (\\(p\\))?", "Options": {"A": "Both changes increase \\(p\\).", "B": "Both changes decrease \\(p\\).", "C": "An increase in \\(g\\) decreases \\(p\\), while an increase in \\(w\\) increases \\(p\\).", "D": "An increase in \\(g\\) increases \\(p\\), while an increase in \\(w\\) decreases \\(p\\)."}, "Answer": "C", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5)\nThis item assesses the student's understanding of the core price dynamics equation. It uses a Scenario Application strategy by asking for the effect of simultaneous changes. Distractors are based on common sign errors, including a direct conceptual opposite (D), ensuring high discriminability.", "qid": "448", "question": "### Background\n\n**Research Question.** In a dynamic general equilibrium setting with a constant mark-up pricing rule, what determines the equilibrium rate of output price inflation?\n\n**Setting.** The model considers a firm with a Cobb-Douglas production technology that faces constantly changing input prices and ongoing technological progress. The core assumption is that the firm maintains a constant markup, which implies that the output price \\(P_t\\) must grow at the same rate as the minimized unit cost of production.\n\n**Variables & Parameters.**\n- \\( p \\): Expected rate of change of the firm's output price (\\(\\dot{P}/P\\)).\n- \\( w \\): Expected rate of growth of the money wage rate (\\(\\dot{W}/W\\)).\n- \\( g \\): Rate of Harrod-neutral (labor-augmenting) embodied technical progress.\n- \\( q^* \\): Expected rate of change of the price of capital goods relative to the price of output.\n- \\( a \\): The output elasticity of labor in the Cobb-Douglas production function.\n\n---\n\n### Data / Model Specification\n\nThe equilibrium rate of output price inflation is determined by the growth rates of factor prices and technology:\n\n```latex\np = w - g + q^{*} \\frac{1-a}{a} \n\\quad \\text{(Eq. (1))}\n```\n\n---\n\nAccording to the model's equilibrium price dynamics (Eq. (1)), what is the combined effect of an increase in the rate of Harrod-neutral technological progress (\\(g\\)) and an increase in the expected rate of wage growth (\\(w\\)) on the equilibrium rate of output price inflation (\\(p\\))?", "model_evaluation_result": {"model_raw_response": "Looking at equation (1):\n\np = w - g + q* (1-a)/a\n\nI need to analyze how changes in g and w affect p by examining the partial derivatives:\n\n∂p/∂w = +1 (positive coefficient)\n∂p/∂g = -1 (negative coefficient)\n\nThis means:\n- An increase in w (wage growth rate) increases p by the same amount\n- An increase in g (technological progress rate) decreases p by the same amount\n\nThe economic intuition is clear:\n- Higher wage growth increases production costs, leading to higher output price inflation\n- Faster technological progress reduces unit costs through improved productivity, leading to lower output price inflation\n\nTherefore, an increase in g decreases p, while an increase in w increases p.\n\nC", "model_answer": "C", "error": null}}
{"ID": 501, "Question": "### Background\n\n**Research Question.** This question examines the causal logic behind the specific design of a \"role model\" intervention in a micro-entrepreneurship training program.\n\n**Setting and Sample.** The study was conducted with low-income micro-entrepreneurs in Chile, who were predominantly women (92%) with an average age of 45. One treatment arm involved a visit from a role model, who was a successful alumnus of the same program.\n\n### Data / Model Specification\n\n**Table 1: Role Model Characteristics**\n\n| Variable | Mean |\n| :--- | :--- |\n| Women | 0.93 |\n| Age | 48 |\n| Income (CLP$) | 1,998,439 |\n\n*Notes: The average household income for participants was CLP$390,000.* \n\n### Question\n\nThe study selected role models who were demographically similar to participants (e.g., 93% women, avg. age 48) but significantly more successful (avg. income >5x higher). What is the most likely causal inference goal behind this specific 'relatable yet aspirational' design choice?", "Options": {"A": "To isolate the psychological impact of seeing an attainable success story, separating motivation from other factors like expert advice or gender dynamics.", "B": "To provide the most effective and advanced business strategies from proven experts, maximizing knowledge transfer.", "C": "To minimize logistical costs by recruiting from a pool of local, readily available program alumni.", "D": "To ensure that the treatment effect was not driven by differences in business sector between role models and participants."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of the core causal logic behind the 'relatable yet aspirational' intervention design, a key innovation of the paper. It uses a Reverse-Reasoning strategy. The correct option (A) identifies the goal of isolating a psychological mechanism. Distractors are designed to be plausible but incorrect: (B) confounds the role model's purpose with that of a consultant (Conceptual Opposite), (C) offers a practical reason instead of a causal one (True but Irrelevant), and (D) focuses on a secondary control variable rather than the primary theoretical motivation (Conceptual Error).", "qid": "501", "question": "### Background\n\n**Research Question.** This question examines the causal logic behind the specific design of a \"role model\" intervention in a micro-entrepreneurship training program.\n\n**Setting and Sample.** The study was conducted with low-income micro-entrepreneurs in Chile, who were predominantly women (92%) with an average age of 45. One treatment arm involved a visit from a role model, who was a successful alumnus of the same program.\n\n### Data / Model Specification\n\n**Table 1: Role Model Characteristics**\n\n| Variable | Mean |\n| :--- | :--- |\n| Women | 0.93 |\n| Age | 48 |\n| Income (CLP$) | 1,998,439 |\n\n*Notes: The average household income for participants was CLP$390,000.* \n\n### Question\n\nThe study selected role models who were demographically similar to participants (e.g., 93% women, avg. age 48) but significantly more successful (avg. income >5x higher). What is the most likely causal inference goal behind this specific 'relatable yet aspirational' design choice?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 395, "Question": "### Background\n\n**Research Question.** This problem investigates the fundamental identification and dynamic consistency failures of the standard α-maxmin expected utility (α-MEU) model.\n\n### Data / Model Specification\n\nTwo α-MEU representations, Rep 1 (`α_1 = 3/4`, `P_1`) and Rep 2 (`α_2 = 1`, `P_2`), are observationally equivalent ex-ante. The relationship between the prior sets is that `P_1` is the `γ`-expansion of `P_2` with `γ=3/2`.\n\nAfter learning event `E = {1, 2}`, the prior sets are updated using prior-by-prior Bayesian updating:\n- `P_1^E = co{(10/11, 1/11, 0), (2/7, 5/7, 0)}`\n- `P_2^E = co{(4/5, 1/5, 0), (1/2, 1/2, 0)}`\n\nFurther calculation shows that the `γ`-expansion of `P_2^E` with `γ=3/2` yields the set `Q = co{(19/20, 1/20, 0), (7/20, 13/20, 0)}`.\n\n### Question\n\nGiven that the calculated `γ`-expansion of `P_2^E` is not equal to `P_1^E`, what is the primary implication of this finding for the α-MEU model?", "Options": {"A": "The prior-by-prior updating rule is ill-defined, as equivalent ex-ante representations lead to different ex-post preferences.", "B": "The `γ`-expansion relationship is not preserved under Bayesian updating, but the conditional preferences remain the same.", "C": "The initial representations must not have been equivalent, because a well-defined rule should preserve equivalence.", "D": "Bayesian updating is preserved, but the `γ`-expansion formula from Proposition 1 does not apply to conditional preferences."}, "Answer": "A", "pi_justification": "This item assesses the user's ability to synthesize the results of the previous calculations to grasp the paper's central critique of the standard α-MEU model. It uses a Reverse-Reasoning strategy. Distractors represent common logical fallacies: concluding the opposite (Conceptual Opposite), missing the key implication (True but Irrelevant), or misattributing the cause of the problem (Misinterpretation).", "qid": "395", "question": "### Background\n\n**Research Question.** This problem investigates the fundamental identification and dynamic consistency failures of the standard α-maxmin expected utility (α-MEU) model.\n\n### Data / Model Specification\n\nTwo α-MEU representations, Rep 1 (`α_1 = 3/4`, `P_1`) and Rep 2 (`α_2 = 1`, `P_2`), are observationally equivalent ex-ante. The relationship between the prior sets is that `P_1` is the `γ`-expansion of `P_2` with `γ=3/2`.\n\nAfter learning event `E = {1, 2}`, the prior sets are updated using prior-by-prior Bayesian updating:\n- `P_1^E = co{(10/11, 1/11, 0), (2/7, 5/7, 0)}`\n- `P_2^E = co{(4/5, 1/5, 0), (1/2, 1/2, 0)}`\n\nFurther calculation shows that the `γ`-expansion of `P_2^E` with `γ=3/2` yields the set `Q = co{(19/20, 1/20, 0), (7/20, 13/20, 0)}`.\n\n### Question\n\nGiven that the calculated `γ`-expansion of `P_2^E` is not equal to `P_1^E`, what is the primary implication of this finding for the α-MEU model?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand what's happening with the α-MEU model and the relationship between the two representations.\n\nKey facts:\n1. Rep 1 and Rep 2 are observationally equivalent ex-ante\n2. P₁ is the γ-expansion of P₂ with γ = 3/2\n3. After Bayesian updating on event E, we get P₁ᴱ and P₂ᴱ\n4. The γ-expansion of P₂ᴱ does NOT equal P₁ᴱ\n\nThis is a fundamental problem in the α-MEU literature known as \"dynamic inconsistency\" or the failure of \"rectangularity.\"\n\nLet me analyze each option:\n\nA) This correctly identifies the core issue: two representations that are equivalent ex-ante (they generate the same preferences before learning) lead to different preferences after learning through the same updating rule. This makes the updating rule ill-defined because equivalent representations should remain equivalent after any well-defined updating process.\n\nB) This is incorrect because if the γ-expansion relationship isn't preserved, then the conditional preferences cannot remain the same - that's exactly what observational equivalence means.\n\nC) This is backwards reasoning. The representations were equivalent ex-ante by construction, but the updating rule fails to preserve this equivalence, which shows the rule is problematic, not the initial equivalence.\n\nD) This misses the point. The issue isn't about whether some formula applies, but about the fundamental failure of the updating rule to preserve equivalence.\n\nThe finding demonstrates that prior-by-prior Bayesian updating violates dynamic consistency in the α-MEU model - equivalent ex-ante representations become non-equivalent after learning, making the model's predictions ill-defined.\n\nA", "model_answer": "A", "error": null}}
{"ID": 428, "Question": "### Background\n\n**Research Question.** This problem seeks to identify the specific nature of the learning bias that drives a government's beliefs away from a self-confirming equilibrium.\n\n**Setting.** In a simplified static model, the escape dynamics are described by augmenting the mean dynamics with a perturbation term, `v̇`. This term represents the difference between the expected update under a special conditional shock distribution (which generates the escape) and the unconditional shock distribution (which generates the mean dynamics).\n\n### Data / Model Specification\n\nThe escape dynamics are governed by the ODE:\n\n```latex\n\\dot{\\gamma} = R^{-1}\\bar{g}(\\gamma) + \\dot{v} \\quad \\text{(Eq. (1))}\n```\n\nThe dominant escape path is generated by a specific conditional shock distribution, leading to the following dynamics:\n\n```latex\n\\dot{\\gamma}_{escape} = R^{-1}\\left[\\begin{array}{c}{{-\\frac{\\gamma_{-1}\\gamma_{1}}{1+\\gamma_{1}^{2}}\\left(u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}\\right)-(\\theta+\\gamma_{1})\\sigma_{2}^{2}+\\sigma_{1}\\sigma_{2}}}\\\\ {{u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}}}\\end{array}\\right] \\quad \\text{(Eq. (2))}\n```\n\nThe unconditional mean dynamics are:\n\n```latex\n\\dot{\\gamma}_{mean} = R^{-1}\\bar{g}(\\gamma) = R^{-1}\\left[\\begin{array}{c}{{-\\frac{\\gamma_{-1}\\gamma_{1}}{1+\\gamma_{1}^{2}}\\left(u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}\\right)-(\\theta+\\gamma_{1})\\sigma_{2}^{2}}}\\\\ {{u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}}}\\end{array}\\right] \\quad \\text{(Eq. (3))}\n```\n\n### Question\n\nGiven the equations for the escape dynamics (Eq. (2)) and the mean dynamics (Eq. (3)), what is the explicit formula for the perturbation term `v̇` that drives the dominant escape path?", "Options": {"A": "```latex\nR^{-1} \\left[\\begin{array}{c}{\\sigma_{1}\\sigma_{2}}\\\\ {0}\\end{array}\\right]\n```", "B": "```latex\nR^{-1} \\left[\\begin{array}{c}{0}\\\\ {u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}}\\end{array}\\right]\n```", "C": "```latex\nR^{-1} \\left[\\begin{array}{c}{-\\sigma_{1}\\sigma_{2}}\\\\ {0}\\end{array}\\right]\n```", "D": "```latex\n0\n```"}, "Answer": "A", "pi_justification": "This item tests the ability to perform the algebraic derivation of the key perturbation term `v̇`. It uses a 'Computational Judgment' strategy. The distractors are designed based on plausible algebraic errors: (B) incorrectly identifies the non-zero component, (C) represents a sign error, and (D) represents the conceptual error of there being no perturbation.", "qid": "428", "question": "### Background\n\n**Research Question.** This problem seeks to identify the specific nature of the learning bias that drives a government's beliefs away from a self-confirming equilibrium.\n\n**Setting.** In a simplified static model, the escape dynamics are described by augmenting the mean dynamics with a perturbation term, `v̇`. This term represents the difference between the expected update under a special conditional shock distribution (which generates the escape) and the unconditional shock distribution (which generates the mean dynamics).\n\n### Data / Model Specification\n\nThe escape dynamics are governed by the ODE:\n\n```latex\n\\dot{\\gamma} = R^{-1}\\bar{g}(\\gamma) + \\dot{v} \\quad \\text{(Eq. (1))}\n```\n\nThe dominant escape path is generated by a specific conditional shock distribution, leading to the following dynamics:\n\n```latex\n\\dot{\\gamma}_{escape} = R^{-1}\\left[\\begin{array}{c}{{-\\frac{\\gamma_{-1}\\gamma_{1}}{1+\\gamma_{1}^{2}}\\left(u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}\\right)-(\\theta+\\gamma_{1})\\sigma_{2}^{2}+\\sigma_{1}\\sigma_{2}}}\\\\ {{u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}}}\\end{array}\\right] \\quad \\text{(Eq. (2))}\n```\n\nThe unconditional mean dynamics are:\n\n```latex\n\\dot{\\gamma}_{mean} = R^{-1}\\bar{g}(\\gamma) = R^{-1}\\left[\\begin{array}{c}{{-\\frac{\\gamma_{-1}\\gamma_{1}}{1+\\gamma_{1}^{2}}\\left(u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}\\right)-(\\theta+\\gamma_{1})\\sigma_{2}^{2}}}\\\\ {{u-\\frac{\\gamma_{-1}}{1+\\gamma_{1}^{2}}}}\\end{array}\\right] \\quad \\text{(Eq. (3))}\n```\n\n### Question\n\nGiven the equations for the escape dynamics (Eq. (2)) and the mean dynamics (Eq. (3)), what is the explicit formula for the perturbation term `v̇` that drives the dominant escape path?", "model_evaluation_result": {"model_raw_response": "Looking at the given equations, I need to find the perturbation term v̇ by comparing the escape dynamics with the mean dynamics.\n\nFrom Equation (1): γ̇ = R⁻¹ḡ(γ) + v̇\n\nThis can be rearranged to: v̇ = γ̇ - R⁻¹ḡ(γ)\n\nFor the dominant escape path:\nv̇ = γ̇_escape - γ̇_mean\n\nLet me subtract Equation (3) from Equation (2):\n\nγ̇_escape - γ̇_mean = R⁻¹[\n[-γ₋₁γ₁/(1+γ₁²)(u - γ₋₁/(1+γ₁²)) - (θ+γ₁)σ₂² + σ₁σ₂]\n[u - γ₋₁/(1+γ₁²)]\n] - R⁻¹[\n[-γ₋₁γ₁/(1+γ₁²)(u - γ₋₁/(1+γ₁²)) - (θ+γ₁)σ₂²]\n[u - γ₋₁/(1+γ₁²)]\n]\n\nComparing the components:\n\nFirst component:\n[-γ₋₁γ₁/(1+γ₁²)(u - γ₋₁/(1+γ₁²)) - (θ+γ₁)σ₂² + σ₁σ₂] - [-γ₋₁γ₁/(1+γ₁²)(u - γ₋₁/(1+γ₁²)) - (θ+γ₁)σ₂²]\n= σ₁σ₂\n\nSecond component:\n[u - γ₋₁/(1+γ₁²)] - [u - γ₋₁/(1+γ₁²)] = 0\n\nTherefore:\nv", "model_answer": "Looking at the given equations, I need to find the perturbation term v̇ by comparing the escape dynamics with the mean dynamics.\n\nFrom Equation (1): γ̇ = R⁻¹ḡ(γ) + v̇\n\nThis can be rearranged to: v̇ = γ̇ - R⁻¹ḡ(γ)\n\nFor the dominant escape path:\nv̇ = γ̇_escape - γ̇_mean\n\nLet me subtract Equation (3) from Equation (2):\n\nγ̇_escape - γ̇_mean = R⁻¹[\n[-γ₋₁γ₁/(1+γ₁²)(u - γ₋₁/(1+γ₁²)) - (θ+γ₁)σ₂² + σ₁σ₂]\n[u - γ₋₁/(1+γ₁²)]\n] - R⁻¹[\n[-γ₋₁γ₁/(1+γ₁²)(u - γ₋₁/(1+γ₁²)) - (θ+γ₁)σ₂²]\n[u - γ₋₁/(1+γ₁²)]\n]\n\nComparing the components:\n\nFirst component:\n[-γ₋₁γ₁/(1+γ₁²)(u - γ₋₁/(1+γ₁²)) - (θ+γ₁)σ₂² + σ₁σ₂] - [-γ₋₁γ₁/(1+γ₁²)(u - γ₋₁/(1+γ₁²)) - (θ+γ₁)σ₂²]\n= σ₁σ₂\n\nSecond component:\n[u - γ₋₁/(1+γ₁²)] - [u - γ₋₁/(1+γ₁²)] = 0\n\nTherefore:\nv", "error": "Warning: Could not extract single choice letter from: Looking at the given equations, I need to find the..."}}
{"ID": 417, "Question": "### Background\n\nThe analysis uses the theoretical framework of a sticky-price model to perform two 'back-of-the-envelope' calculations. First, it maps a steady-state inflation rate into an equivalent permanent loss in total factor productivity (TFP). Second, it uses an approximate formula to link an empirically observable statistic—the coefficient of variation (`cvar`) of prices—to the model's price dispersion measure `Δ`, and from there to productivity and consumption-equivalent welfare losses.\n\n### Data / Model Specification\n\nThe model establishes that price dispersion `Δ` acts as a drag on productivity. This can be quantified by defining an \"effective\" productivity level `A^R = A Δ^{-1/ϕ}`. The productivity-equivalent cost is the percentage change, `1 - Δ^{-1/ϕ}`.\n\n**Table 1: Approximate Productivity Equivalent Cost of Inflation**\n\n| | I (Benchmark) |\n| :--- | :--- |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n| `π` (annual) | 2.5% |\n| `α` | 0.5 |\n| `Δ` | 1.034 |\n| `1 - Δ^{-1/ϕ}` | -2.4% |\n\n**Table 2: Approximate Productivity Equivalent Cost of Price Dispersion**\n\n| | II (Benchmark) |\n| :--- | :--- |\n| `cvar` | 10% |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n| `Δ` | 1.04 |\n| `1 - Δ^{-1/ϕ}` | -2.7% |\n\n**Table 3: Welfare Loss from Price Dispersion: Consumption Equivalent**\n\n| | II (Benchmark) |\n| :--- | :--- |\n| Coeff. of Variation | 10% |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n| `Δ` | 1.04 |\n| `v` (labor supply) | 1.8 |\n| `Φ` (cost) | 2.2% |\n\nThe authors state that \"column II in Table 2 corresponds quite closely to column I in Table 1, in terms of the ultimate productivity-equivalent impact.\" What is the primary significance of this correspondence?", "Options": {"A": "It suggests the model is quantitatively plausible, as the level of price dispersion generated by moderate inflation is consistent with levels inferred from real-world price data.", "B": "It proves that a 2.5% inflation rate is the sole cause of the 10% coefficient of variation observed in empirical studies.", "C": "It demonstrates that the productivity-equivalent cost is highly sensitive to the price stickiness parameter `α`, which is the main driver of the result.", "D": "It reveals an inconsistency in the model, as two different sets of assumptions (one theoretical, one empirical) lead to a similar outcome, undermining the mechanism's specificity."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to synthesize information from two different tables to grasp the paper's central argument about quantitative plausibility. It uses a Reverse-Reasoning strategy. The correct option (A) accurately identifies the 'reality check' function of the comparison. Distractor (B) makes an overly strong causal claim (Conceptual Opposite). Distractor (C) is True but Irrelevant, focusing on a different parameter comparison not central to this specific point. Distractor (D) misinterprets the correspondence as a weakness rather than a strength (Conceptual Opposite).", "qid": "417", "question": "### Background\n\nThe analysis uses the theoretical framework of a sticky-price model to perform two 'back-of-the-envelope' calculations. First, it maps a steady-state inflation rate into an equivalent permanent loss in total factor productivity (TFP). Second, it uses an approximate formula to link an empirically observable statistic—the coefficient of variation (`cvar`) of prices—to the model's price dispersion measure `Δ`, and from there to productivity and consumption-equivalent welfare losses.\n\n### Data / Model Specification\n\nThe model establishes that price dispersion `Δ` acts as a drag on productivity. This can be quantified by defining an \"effective\" productivity level `A^R = A Δ^{-1/ϕ}`. The productivity-equivalent cost is the percentage change, `1 - Δ^{-1/ϕ}`.\n\n**Table 1: Approximate Productivity Equivalent Cost of Inflation**\n\n| | I (Benchmark) |\n| :--- | :--- |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n| `π` (annual) | 2.5% |\n| `α` | 0.5 |\n| `Δ` | 1.034 |\n| `1 - Δ^{-1/ϕ}` | -2.4% |\n\n**Table 2: Approximate Productivity Equivalent Cost of Price Dispersion**\n\n| | II (Benchmark) |\n| :--- | :--- |\n| `cvar` | 10% |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n| `Δ` | 1.04 |\n| `1 - Δ^{-1/ϕ}` | -2.7% |\n\n**Table 3: Welfare Loss from Price Dispersion: Consumption Equivalent**\n\n| | II (Benchmark) |\n| :--- | :--- |\n| Coeff. of Variation | 10% |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n| `Δ` | 1.04 |\n| `v` (labor supply) | 1.8 |\n| `Φ` (cost) | 2.2% |\n\nThe authors state that \"column II in Table 2 corresponds quite closely to column I in Table 1, in terms of the ultimate productivity-equivalent impact.\" What is the primary significance of this correspondence?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand what the correspondence between Table 1 and Table 2 means.\n\nTable 1 shows the productivity-equivalent cost of inflation using a theoretical approach:\n- Uses a 2.5% annual inflation rate\n- Calculates price dispersion Δ = 1.034\n- Results in productivity cost of -2.4%\n\nTable 2 shows the productivity-equivalent cost using an empirical approach:\n- Uses an observed coefficient of variation (cvar) of 10% from real-world price data\n- Calculates price dispersion Δ = 1.04\n- Results in productivity cost of -2.7%\n\nThe key insight is that these two different approaches - one starting from a theoretical inflation rate, the other from empirically observed price dispersion - yield very similar results (-2.4% vs -2.7%).\n\nLet me evaluate each option:\n\nA) This suggests the model is quantitatively plausible because the price dispersion that would be generated by moderate inflation (2.5%) matches closely with what we actually observe in real-world data (10% coefficient of variation). This correspondence validates the model's realism.\n\nB) This is incorrect - the correspondence doesn't prove causation, just consistency between theoretical and empirical approaches.\n\nC) This is wrong - the parameter α (0.5) is the same in both calculations, so sensitivity to α isn't what's being demonstrated here.\n\nD) This misinterprets the result - the correspondence actually supports the model's consistency rather than revealing an inconsistency.\n\nThe correspondence is significant because it shows the model's theoretical predictions align well with empirical reality, making it quantitatively credible.\n\nA", "model_answer": "A", "error": null}}
{"ID": 492, "Question": "### Background\n\nA key result of the paper is that the household can become effectively risk neutral under certain conditions, even if the utility function is strictly concave in consumption. This occurs when the household can absorb wealth shocks along a margin that has a constant marginal utility cost.\n\n### Data / Model Specification\n\nConsider the additively separable utility function:\n```latex\nu(c_{t},l_{t})=\\frac{c_{t}^{1-\\gamma}}{1-\\gamma}-\\eta\\frac{l_{t}^{1+\\chi}}{1+\\chi}\n```\nThe correctly measured relative risk aversion for this function is:\n```latex\nR^{c} = \\frac{\\gamma}{1+\\frac{\\gamma}{\\chi}\\frac{wl}{c}}\n```\nIn the special case where the disutility of labor becomes linear, which corresponds to `χ → 0`, the household becomes effectively risk neutral (`R^c → 0`).\n\n### Question\n\nWhich of the following statements provides the most accurate economic intuition for why `R^c → 0` as `χ → 0`?", "Options": {"A": "With linear labor disutility, the household's utility becomes linear in consumption, making them risk neutral.", "B": "The household's labor supply becomes perfectly inelastic, fixing labor income and making wealth shocks irrelevant.", "C": "Additive separability between consumption and labor means that shocks to the labor market do not spill over into consumption decisions.", "D": "The household can absorb wealth shocks by adjusting hours worked at a constant marginal utility cost, thereby perfectly insulating consumption from the shock."}, "Answer": "D", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of the economic intuition behind one of the paper's most striking results. It uses a Reverse-Reasoning strategy. The distractors are designed based on common confusions: (A) misattributing the source of risk neutrality (Conceptual Opposite), (B) confusing elastic with inelastic labor supply (Conceptual Opposite), and (C) citing a true but irrelevant feature of the utility function as the primary cause.", "qid": "492", "question": "### Background\n\nA key result of the paper is that the household can become effectively risk neutral under certain conditions, even if the utility function is strictly concave in consumption. This occurs when the household can absorb wealth shocks along a margin that has a constant marginal utility cost.\n\n### Data / Model Specification\n\nConsider the additively separable utility function:\n```latex\nu(c_{t},l_{t})=\\frac{c_{t}^{1-\\gamma}}{1-\\gamma}-\\eta\\frac{l_{t}^{1+\\chi}}{1+\\chi}\n```\nThe correctly measured relative risk aversion for this function is:\n```latex\nR^{c} = \\frac{\\gamma}{1+\\frac{\\gamma}{\\chi}\\frac{wl}{c}}\n```\nIn the special case where the disutility of labor becomes linear, which corresponds to `χ → 0`, the household becomes effectively risk neutral (`R^c → 0`).\n\n### Question\n\nWhich of the following statements provides the most accurate economic intuition for why `R^c → 0` as `χ → 0`?", "model_evaluation_result": {"model_raw_response": "D", "model_answer": "D", "error": null}}
{"ID": 422, "Question": "### Background\n\nThe paper's \"dual scaling\" theory posits that a CEO's pay is determined not only by their own firm's size but also by the size of a typical large firm in the market, which reflects the aggregate demand for talent. This theory is tested using a panel dataset of top U.S. companies from 1992-2004.\n\n### Data / Model Specification\n\nThe theoretical model `w(n) = D(n_*) S(n_*)^{β/α} S(n)^{γ - β/α}` is tested using the following log-linear empirical specification:\n\n```latex\n\\ln(w_{i,t+1}) = d + e \\cdot \\ln(S_{n_*,t}) + f \\cdot \\ln(S_{i,t}) + \\text{Controls} + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n```\n\nwhere `w_{i,t+1}` is CEO compensation, `S_{i,t}` is the firm's own market value, and `S_{n_*,t}` is the market value of the 250th largest firm (the reference size). The coefficients `e` and `f` map to the theoretical parameters as follows: `e = β/α` and `f = γ - β/α`, which implies `γ = e + f`.\n\nResults for the top 1000 firms including industry fixed effects are presented in Table 1.\n\n**Table 1: Regression of ln(Total compensation)**\n\n| | (2) Industry FE |\n|:---|:---:|\n| ln(Market cap) (`f`) | 0.37 |\n| | (0.020) |\n| ln(Market cap of firm #250) (`e`) | 0.66 |\n| | (0.054) |\n| Observations | 7,936 |\n| R² | 0.29 |\n\n*Standard errors in parentheses.*\n\n### Question\n\nBased on the regression results in Table 1, what is the most accurate interpretation of the estimated parameters?", "Options": {"A": "The results support the hypothesis of constant returns to scale (γ ≈ 1.03), and imply that the talent distribution is thinner-tailed than the firm size distribution (β/α ≈ 0.66).", "B": "The results suggest diminishing returns to scale (γ ≈ 0.37), and imply that the talent distribution is thicker-tailed than the firm size distribution (β/α ≈ 0.66).", "C": "The results support the hypothesis of constant returns to scale (γ ≈ 1.03), and imply that the firm size distribution is thinner-tailed than the talent distribution (β/α ≈ 0.37).", "D": "The results imply that if all firms in the market double in size, average CEO pay is predicted to increase by approximately 37%, consistent with Roberts's Law."}, "Answer": "A", "pi_justification": "This item assesses the ability to map empirical regression coefficients back to the model's structural parameters and interpret them economically. It uses a Computational Judgment strategy. The correct option (A) requires correctly calculating γ = e + f = 0.66 + 0.37 = 1.03 and correctly identifying e = β/α = 0.66. Distractor (B) incorrectly uses f as the estimate for γ. Distractor (C) incorrectly swaps the estimates for β/α and γ-β/α. Distractor (D) makes a classic error by applying the cross-sectional elasticity (f ≈ 0.37) to a time-series question (all firms doubling in size), for which the correct elasticity is γ ≈ 1.", "qid": "422", "question": "### Background\n\nThe paper's \"dual scaling\" theory posits that a CEO's pay is determined not only by their own firm's size but also by the size of a typical large firm in the market, which reflects the aggregate demand for talent. This theory is tested using a panel dataset of top U.S. companies from 1992-2004.\n\n### Data / Model Specification\n\nThe theoretical model `w(n) = D(n_*) S(n_*)^{β/α} S(n)^{γ - β/α}` is tested using the following log-linear empirical specification:\n\n```latex\n\\ln(w_{i,t+1}) = d + e \\cdot \\ln(S_{n_*,t}) + f \\cdot \\ln(S_{i,t}) + \\text{Controls} + \\epsilon_{i,t+1} \\quad \\text{(Eq. (1))}\n```\n\nwhere `w_{i,t+1}` is CEO compensation, `S_{i,t}` is the firm's own market value, and `S_{n_*,t}` is the market value of the 250th largest firm (the reference size). The coefficients `e` and `f` map to the theoretical parameters as follows: `e = β/α` and `f = γ - β/α`, which implies `γ = e + f`.\n\nResults for the top 1000 firms including industry fixed effects are presented in Table 1.\n\n**Table 1: Regression of ln(Total compensation)**\n\n| | (2) Industry FE |\n|:---|:---:|\n| ln(Market cap) (`f`) | 0.37 |\n| | (0.020) |\n| ln(Market cap of firm #250) (`e`) | 0.66 |\n| | (0.054) |\n| Observations | 7,936 |\n| R² | 0.29 |\n\n*Standard errors in parentheses.*\n\n### Question\n\nBased on the regression results in Table 1, what is the most accurate interpretation of the estimated parameters?", "model_evaluation_result": {"model_raw_response": "Looking at the regression results from Table 1:\n\n- `f = 0.37` (coefficient on ln(Market cap))\n- `e = 0.66` (coefficient on ln(Market cap of firm #250))\n\nFrom the model specification, we know:\n- `e = β/α = 0.66`\n- `f = γ - β/α = 0.37`\n- Therefore: `γ = e + f = 0.66 + 0.37 = 1.03`\n\nNow I need to interpret these parameters:\n\n1. **Returns to scale**: γ ≈ 1.03 is very close to 1, suggesting approximately constant returns to scale.\n\n2. **Talent vs. firm size distribution**: The parameter β/α ≈ 0.66 relates to the relative thickness of the talent distribution compared to the firm size distribution. When β/α < 1, this indicates that the talent distribution is thinner-tailed than the firm size distribution (there's less extreme variation in talent compared to firm size).\n\n3. **Market scaling effect**: If all firms double in size, the effect would be through the `e` coefficient (0.66), not the `f` coefficient, so the 37% interpretation in option D is incorrect.\n\nComparing the options:\n\n- Option A correctly identifies γ ≈ 1.03 (constant returns) and correctly interprets β/α ≈ 0.66 as indicating the talent distribution is thinner-tailed than the firm size distribution.\n\n- Option B incorrectly states γ ≈ 0.37 (confusing it with the f coefficient).\n\n- Option C incorrectly interprets the tail thickness relationship.\n\n- Option D misinterprets the scaling effect.\n\nA", "model_answer": "D", "error": null}}
{"ID": 346, "Question": "### Background\n\nA firm is characterized by its capital stock `k` and an idiosyncratic productivity shock `z`. In each period, it makes optimal investment and exit decisions to maximize its market value. External financing is costly.\n\n### Data / Model Specification\n\nThe value of a firm, `p(k,z;w)`, is the solution to the Bellman equation:\n```latex\np(k,z;w) = \\operatorname*{max}_{k'\\geq0} \\Biggl\\{ \\pi(k,z;w) - i(k,k') - \\lambda(a) + \\beta E_{z'|z}[\\operatorname*{max}\\{(1-\\delta)k', p(k',z';w)\\}] \\Biggr\\} \n```\nwhere `a = i(k,k') - π(k,z;w)` is the amount of external finance, and `i(k,k') = k' - (1-δ)k` is investment. The financing cost function `λ(a)` is zero for `a ≤ 0` and positive and increasing for `a > 0` (`λ'(a) > 0`).\n\n### Question\n\nConsider a firm that finds it optimal to invest to a new capital stock `k'` such that it must rely on external funds (i.e., the amount of external finance `a > 0`). Which of the following correctly describes the firm's first-order condition (FOC) for `k'` and its exit rule for the subsequent period?", "Options": {"A": "FOC: `1 + λ'(a) = βE[∂p(k',z')/∂k']`; Exit Rule: Exit if `E[p(k',z')] < (1-δ)k'`", "B": "FOC: `1 = βE[∂p(k',z')/∂k']`; Exit Rule: Exit if `E[p(k',z')] < (1-δ)k'`", "C": "FOC: `1 + λ(a) = βE[∂p(k',z')/∂k']`; Exit Rule: Exit if `E[p(k',z')] < (1-δ)k'`", "D": "FOC: `1 + λ'(a) = βE[∂p(k',z')/∂k']`; Exit Rule: Exit if `E[p(k',z')] < 0`"}, "Answer": "A", "pi_justification": "This item assesses the user's understanding of the model's core microfoundations, combining the interpretation of the exit condition and the derivation of the investment FOC. It uses a 'Computational Judgment' strategy, asking the user to identify the correct optimality conditions. Distractors are designed to target specific, high-frequency errors in dynamic programming: B) ignoring the financing friction (Conceptual Opposite), C) misapplying the chain rule in the FOC (Formula Misuse), and D) misidentifying the firm's outside option as zero instead of its liquidation value (Conceptual Opposite).", "qid": "346", "question": "### Background\n\nA firm is characterized by its capital stock `k` and an idiosyncratic productivity shock `z`. In each period, it makes optimal investment and exit decisions to maximize its market value. External financing is costly.\n\n### Data / Model Specification\n\nThe value of a firm, `p(k,z;w)`, is the solution to the Bellman equation:\n```latex\np(k,z;w) = \\operatorname*{max}_{k'\\geq0} \\Biggl\\{ \\pi(k,z;w) - i(k,k') - \\lambda(a) + \\beta E_{z'|z}[\\operatorname*{max}\\{(1-\\delta)k', p(k',z';w)\\}] \\Biggr\\} \n```\nwhere `a = i(k,k') - π(k,z;w)` is the amount of external finance, and `i(k,k') = k' - (1-δ)k` is investment. The financing cost function `λ(a)` is zero for `a ≤ 0` and positive and increasing for `a > 0` (`λ'(a) > 0`).\n\n### Question\n\nConsider a firm that finds it optimal to invest to a new capital stock `k'` such that it must rely on external funds (i.e., the amount of external finance `a > 0`). Which of the following correctly describes the firm's first-order condition (FOC) for `k'` and its exit rule for the subsequent period?", "model_evaluation_result": {"model_raw_response": "Looking at this dynamic programming problem, I need to derive the first-order condition for optimal investment and the exit rule.\n\n**First-Order Condition:**\nThe firm maximizes:\n```\nπ(k,z;w) - i(k,k') - λ(a) + βE_{z'|z}[max{(1-δ)k', p(k',z';w)}]\n```\n\nTaking the derivative with respect to k':\n- ∂i(k,k')/∂k' = 1 (since i(k,k') = k' - (1-δ)k)\n- ∂a/∂k' = ∂i(k,k')/∂k' = 1 (since a = i(k,k') - π(k,z;w))\n- ∂λ(a)/∂k' = λ'(a) × ∂a/∂k' = λ'(a)\n\nFor the continuation value term, using the envelope theorem on the max operator:\n∂E[max{(1-δ)k', p(k',z';w)}]/∂k' = E[∂p(k',z';w)/∂k']\n\nSetting the FOC equal to zero:\n-1 - λ'(a) + βE[∂p(k',z';w)/∂k'] = 0\n\nTherefore: **1 + λ'(a) = βE[∂p(k',z';w)/∂k']**\n\n**Exit Rule:**\nIn the subsequent period, the firm chooses between:\n- Continuing: receives value p(k',z';w)  \n- Exiting: receives liquidation value (1-δ)k'\n\nThe firm exits when the continuation value is less than the liquidation value:\n**Exit if p(k',z';w) < (1-δ)k'**\n\nIn expectation terms: **Exit if E[p(k',z';w)] < (1-δ)k'**\n\nA", "model_answer": "A", "error": null}}
{"ID": 505, "Question": "### Background\n\nThe paper tests the cross-sectional restrictions of the Gaussian model using a panel of `N` bond yields over `T` time periods. The econometric specification is:\n```latex\ny_t = a + B(κ)z_t + v_t \\quad \\text{(Eq. 1)}\n```\n\n### Data / Model Specification\n\nThe estimation strategy involves two key choices:\n1.  The unobserved factors `z_t` are treated as time-specific fixed effects and concentrated out of the likelihood, rather than being modeled with a parametric time-series process.\n2.  Inference on the structural parameters `κ` is conducted using Driscoll-Kraay standard errors, which are robust to general forms of cross-sectional and temporal dependence in the errors `v_t`.\n\n### Question\n\nWhat is the central rationale for this two-part econometric strategy (fixed-effects estimation + Driscoll-Kraay standard errors)?", "Options": {"A": "The fixed-effects estimator is consistent, and Driscoll-Kraay errors are used to improve efficiency.", "B": "To isolate the test of the cross-sectional restrictions `B(κ)` from time-series assumptions, while acknowledging the resulting estimator for `κ` may be inconsistent but can still be used for testing with robust standard errors.", "C": "The fixed-effects estimator is inconsistent, and Driscoll-Kraay errors are used to correct this inconsistency and make the point estimates of `κ` unbiased.", "D": "To allow for non-linearities in the factor dynamics, which makes OLS inconsistent but allows for robust inference using the delta method."}, "Answer": "B", "pi_justification": "This item assesses the student's understanding of the paper's core econometric strategy. It uses a Reverse-Reasoning approach, asking for the 'why' behind the chosen methods. The distractors target key misconceptions: (A) incorrectly claims consistency, (C) misinterprets the function of robust standard errors (correcting inference, not bias), and (D) introduces an irrelevant concept.", "qid": "505", "question": "### Background\n\nThe paper tests the cross-sectional restrictions of the Gaussian model using a panel of `N` bond yields over `T` time periods. The econometric specification is:\n```latex\ny_t = a + B(κ)z_t + v_t \\quad \\text{(Eq. 1)}\n```\n\n### Data / Model Specification\n\nThe estimation strategy involves two key choices:\n1.  The unobserved factors `z_t` are treated as time-specific fixed effects and concentrated out of the likelihood, rather than being modeled with a parametric time-series process.\n2.  Inference on the structural parameters `κ` is conducted using Driscoll-Kraay standard errors, which are robust to general forms of cross-sectional and temporal dependence in the errors `v_t`.\n\n### Question\n\nWhat is the central rationale for this two-part econometric strategy (fixed-effects estimation + Driscoll-Kraay standard errors)?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 290, "Question": "### Background\n\nIndividuals are assumed to model inflation as a first-order autoregressive, AR(1), process. They recursively estimate the parameters of this model based on the inflation rates they have experienced during their lifetimes. The rate at which they update their beliefs in response to an \"inflation surprise\" is governed by an age-dependent gain function.\n\n### Data / Model Specification\n\nThe perceived law of motion for inflation is an AR(1) process:\n\n```latex\n\\pi_{t+1} = \\alpha + \\phi \\pi_{t} + \\eta_{t+1} \\quad \\text{(Eq. 1)}\n```\n\nThe key modification to standard learning models is the age-dependent decreasing-gain specification for a person of age `t-s`:\n\n```latex\n\\gamma_{t,s} = \n\\begin{cases} \n\\frac{\\theta}{t-s} & \\text{if } t-s \\ge \\theta \\\\ \n1 & \\text{if } t-s < \\theta \n\\end{cases}\n\\quad \\text{(Eq. 2)}\n```\n\nwhere `\\theta` is a constant parameter that shapes the weighting of past experiences.\n\n### Question\n\nThe paper's baseline estimate for the gain parameter is `\\hat{\\theta} = 3.044`, which is significantly greater than 1. What does this finding imply about how individuals weigh their lifetime inflation experiences?", "Options": {"A": "Recent experiences are weighted more heavily than experiences from early in life.", "B": "Experiences from early in life are weighted more heavily than recent experiences.", "C": "All experiences since birth are weighted equally.", "D": "Older individuals update their beliefs more strongly than younger individuals in response to an inflation surprise."}, "Answer": "A", "pi_justification": "This item assesses the interpretation of the model's central parameter, `\\theta`. It uses a Reverse-Reasoning strategy, asking for the implication of an empirical estimate. The distractors are designed based on high-fidelity misconceptions: option B is the conceptual opposite (`\\theta < 1`), option C is a key benchmark case (`\\theta = 1`), and option D confuses the weighting of past data with the age-dependent gain.", "qid": "290", "question": "### Background\n\nIndividuals are assumed to model inflation as a first-order autoregressive, AR(1), process. They recursively estimate the parameters of this model based on the inflation rates they have experienced during their lifetimes. The rate at which they update their beliefs in response to an \"inflation surprise\" is governed by an age-dependent gain function.\n\n### Data / Model Specification\n\nThe perceived law of motion for inflation is an AR(1) process:\n\n```latex\n\\pi_{t+1} = \\alpha + \\phi \\pi_{t} + \\eta_{t+1} \\quad \\text{(Eq. 1)}\n```\n\nThe key modification to standard learning models is the age-dependent decreasing-gain specification for a person of age `t-s`:\n\n```latex\n\\gamma_{t,s} = \n\\begin{cases} \n\\frac{\\theta}{t-s} & \\text{if } t-s \\ge \\theta \\\\ \n1 & \\text{if } t-s < \\theta \n\\end{cases}\n\\quad \\text{(Eq. 2)}\n```\n\nwhere `\\theta` is a constant parameter that shapes the weighting of past experiences.\n\n### Question\n\nThe paper's baseline estimate for the gain parameter is `\\hat{\\theta} = 3.044`, which is significantly greater than 1. What does this finding imply about how individuals weigh their lifetime inflation experiences?", "model_evaluation_result": {"model_raw_response": "Looking at the gain function specification in Equation 2:\n\nFor individuals of age `t-s` where `t-s ≥ θ`, the gain is `γ_{t,s} = θ/(t-s)`.\n\nWith `θ = 3.044 > 1`, let me analyze what this means:\n\n- For younger individuals (smaller `t-s`): The denominator is smaller, so the gain `γ_{t,s}` is larger\n- For older individuals (larger `t-s`): The denominator is larger, so the gain `γ_{t,s}` is smaller\n\nA larger gain means individuals update their beliefs more strongly in response to new information (inflation surprises). A smaller gain means they update less strongly.\n\nSince younger individuals have higher gains, they are more responsive to recent inflation experiences. Conversely, older individuals with lower gains are less responsive to recent experiences, meaning their beliefs are more anchored by their accumulated lifetime experiences.\n\nWhen `θ > 1`, this creates a pattern where:\n- Recent experiences (when individuals were younger and had higher gains) received more weight in forming current beliefs\n- Early life experiences (when gains were higher due to younger age) are weighted more heavily than experiences later in life\n\nTherefore, the finding that `θ = 3.044 > 1` implies that experiences from early in life are weighted more heavily than recent experiences in individuals' inflation expectations.\n\nB", "model_answer": "A", "error": null}}
{"ID": 399, "Question": "### Background\n\nIn the matching model, a locality's choice function (which families it accepts from a set of proposers) is crucial for determining if a mechanism is strategy-proof. The Knapsack Deferred Acceptance (KDA) algorithm's choice function can violate cardinal monotonicity, a key property for strategy-proofness.\n\n---\n\n### Data / Model Specification\n\nConsider a locality `ℓ` with capacity `κ=2`. It has priorities over three families: `f_A triangleright f_B triangleright f_C`. The family sizes are `ν(f_A)=2`, `ν(f_B)=1`, and `ν(f_C)=1`.\n\n---\n\n### Question\n\nA violation of cardinal monotonicity occurs if the locality accepts fewer families from a set of proposers `P_B` than from a strict subset `P_A ⊂ P_B`. Which of the following scenarios demonstrates this violation for locality `ℓ`?", "Options": {"A": "When `P_A = {f_B, f_C}`, the locality accepts 2 families. When `P_B = {f_A, f_B, f_C}`, it accepts 1 family.", "B": "When `P_A = {f_A}`, the locality accepts 1 family. When `P_B = {f_A, f_B}`, it accepts 1 family.", "C": "When `P_A = {f_C}`, the locality accepts 1 family. When `P_B = {f_B, f_C}`, it accepts 2 families.", "D": "When `P_A = {f_B}`, the locality accepts 1 family. When `P_B = {f_A, f_C}`, it accepts 1 family."}, "Answer": "A", "pi_justification": "This item tests the deep understanding of cardinal monotonicity, a key condition for strategy-proofness. It uses a Scenario Application strategy. The user must apply the KDA acceptance rule to different sets of proposers to identify the specific case that constitutes a violation. Distractors represent common misunderstandings: (B) and (C) are valid applications of the rule that do not result in a violation, while (D) uses a distractor set `P_B` that is not a superset of `P_A`, failing a precondition for the test.", "qid": "399", "question": "### Background\n\nIn the matching model, a locality's choice function (which families it accepts from a set of proposers) is crucial for determining if a mechanism is strategy-proof. The Knapsack Deferred Acceptance (KDA) algorithm's choice function can violate cardinal monotonicity, a key property for strategy-proofness.\n\n---\n\n### Data / Model Specification\n\nConsider a locality `ℓ` with capacity `κ=2`. It has priorities over three families: `f_A triangleright f_B triangleright f_C`. The family sizes are `ν(f_A)=2`, `ν(f_B)=1`, and `ν(f_C)=1`.\n\n---\n\n### Question\n\nA violation of cardinal monotonicity occurs if the locality accepts fewer families from a set of proposers `P_B` than from a strict subset `P_A ⊂ P_B`. Which of the following scenarios demonstrates this violation for locality `ℓ`?", "model_evaluation_result": {"model_raw_response": "I need to analyze each scenario to find where the locality accepts fewer families from a larger set than from its strict subset, which would violate cardinal monotonicity.\n\nGiven information:\n- Locality ℓ has capacity κ = 2\n- Priority order: f_A ≻ f_B ≻ f_C\n- Family sizes: ν(f_A) = 2, ν(f_B) = 1, ν(f_C) = 1\n\nLet me analyze each option:\n\n**Option A:** P_A = {f_B, f_C} vs P_B = {f_A, f_B, f_C}\n- For P_A = {f_B, f_C}: Both families have size 1, total size = 2, which equals capacity. Locality accepts both families (2 families).\n- For P_B = {f_A, f_B, f_C}: f_A has highest priority but size 2, which fills the entire capacity. Locality accepts only f_A (1 family).\n- This shows 2 families accepted from P_A but only 1 from P_B, where P_A ⊂ P_B. This violates cardinal monotonicity.\n\n**Option B:** P_A = {f_A} vs P_B = {f_A, f_B}\n- For P_A = {f_A}: Size 2, fills capacity. Accepts 1 family.\n- For P_B = {f_A, f_B}: f_A has higher priority and fills capacity. Accepts 1 family.\n- Same number accepted from both sets (1 = 1). No violation.\n\n**Option C:** P_A = {f_C} vs P_B = {f_B, f_C}\n- For P_A = {f_C}: Size 1, accepts 1 family.\n- For P_B = {f_B, f_C}: f_B has higher priority than f_C, both have size 1, total size = 2. Accepts both families (2 families).\n- More families accepted from larger set (2 > 1). No violation.", "model_answer": "A", "error": null}}
{"ID": 487, "Question": "### Background\n\nThe paper's main analysis uses a two-stage least squares (2SLS) instrumental variable (IV) model to estimate the causal effect of taking Algebra I by eighth grade (\\(T_{ildc}\\)) on student outcomes (\\(y_{ildc}\\)).\n\n**First Stage:**\n```latex\nT_{ildc} = \\Phi_{l} + \\Phi_{dc} + \\rho X_{ildc} + \\delta_{ldc} + \\eta_{ildc} \n```\n**Eq. (1)**\n\n**Second Stage:**\n```latex\ny_{ildc} = \\alpha_{l} + \\alpha_{dc} + \\beta X_{ildc} + \\gamma T_{ildc} + \\varepsilon_{ildc} \n```\n**Eq. (2)**\n\nIn this model, \\(\\delta_{ldc}\\) is a set of instruments representing the policy shock for a student in district \\(l\\), cohort \\(c\\), and prior-achievement decile \\(d\\). The model also includes cohort-by-decile fixed effects (\\(\\alpha_{dc}\\)).\n\n---\n\n### The Question\n\nWhat is the primary econometric reason for this detailed specification, where instruments (\\(\\delta_{ldc}\\)) and fixed effects (\\(\\alpha_{dc}\\)) are both defined at the decile level?", "Options": {"A": "The decile-level instruments capture that the policy's intensity varied by prior achievement, while the decile-level fixed effects control for statewide trends affecting specific achievement groups, making the exclusion restriction more plausible.", "B": "The decile-level instruments serve to control for unobserved student ability, while the decile-level fixed effects are included to increase the first-stage F-statistic.", "C": "This specification is required to estimate the Average Treatment Effect (ATE) for the entire student population, rather than just the effect on compliers.", "D": "The decile-level instruments are used to solve the problem of selection into the sample of test-takers, while the fixed effects correct for measurement error in the outcome variables."}, "Answer": "A", "pi_justification": "This item assesses a nuanced understanding of IV model specification, specifically the distinct roles of the instrument and the fixed effects. It tests the logic from the original QA's Q2 and Q3a. The correct answer correctly assigns the roles: instruments capture heterogeneous policy intensity (relevance), while fixed effects absorb confounders (validity). Distractor B conflates the roles of instruments and fixed effects. Distractor C incorrectly identifies the estimand (IV estimates a LATE, not ATE). Distractor D misattributes the purpose of the instruments and fixed effects to solving different econometric problems.", "qid": "487", "question": "### Background\n\nThe paper's main analysis uses a two-stage least squares (2SLS) instrumental variable (IV) model to estimate the causal effect of taking Algebra I by eighth grade (\\(T_{ildc}\\)) on student outcomes (\\(y_{ildc}\\)).\n\n**First Stage:**\n```latex\nT_{ildc} = \\Phi_{l} + \\Phi_{dc} + \\rho X_{ildc} + \\delta_{ldc} + \\eta_{ildc} \n```\n**Eq. (1)**\n\n**Second Stage:**\n```latex\ny_{ildc} = \\alpha_{l} + \\alpha_{dc} + \\beta X_{ildc} + \\gamma T_{ildc} + \\varepsilon_{ildc} \n```\n**Eq. (2)**\n\nIn this model, \\(\\delta_{ldc}\\) is a set of instruments representing the policy shock for a student in district \\(l\\), cohort \\(c\\), and prior-achievement decile \\(d\\). The model also includes cohort-by-decile fixed effects (\\(\\alpha_{dc}\\)).\n\n---\n\n### The Question\n\nWhat is the primary econometric reason for this detailed specification, where instruments (\\(\\delta_{ldc}\\)) and fixed effects (\\(\\alpha_{dc}\\)) are both defined at the decile level?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 499, "Question": "### Background\n\n**Research Question.** This problem analyzes the sources of the earnings gap between relative payment and piece-rate contracts by moving from a simple regression estimate to a formal decomposition of the gap into a portion explained by worker characteristics and a portion explained by the contractual wage structure itself.\n\n**Setting and Sample.** The analysis uses a sample of 105 shrimp gatherers in Honduras. The dependent variable is the natural logarithm of daily larva income. The total observed difference in mean log wages between the two groups is 0.346.\n\n### Data / Model Specification\n\nAn initial Ordinary Least Squares (OLS) model is estimated on the pooled sample of all workers, with results in Table 1.\n\n**Table 1: OLS Wage Differentials by Contract**\n\n| Variable | Pooled Coefficient (t-stat) |\n| :--- | :---: |\n| Constant | 3.09 (23.04) |\n| Literacy | 0.13 (2.29) |\n| Larva experience | 0.05 (3.27) |\n| Gatherer age | 0.002 (-0.84) |\n| Village B | -0.35 (-3.65) |\n| Village C | 0.15 (-2.14) |\n| **Relative payment** | **0.18 (1.92)** |\n| Sample size | 105 |\n| Adjusted R2 | 0.43 |\n\nTo further investigate the wage gap, an Oaxaca-Blinder decomposition is performed. This method partitions the total gap into a component due to differences in mean characteristics (the 'explained' part) and a component due to differences in the returns to those characteristics (the 'unexplained' part, or wage structure). The results are in Table 2.\n\n**Table 2: Decomposition of Relative Payment – Piece-rate Log Wage Differential**\n(Total Difference in Mean Log Wages = 0.346)\n\n| | Evaluated at piece-rate means | Evaluated at relative payment means | Cotton method |\n| :--- | :---: | :---: | :---: |\n| **Difference due to:** | | |\n| Wage structure | 82% (0.28) | 39% (0.13) | 60% (0.21) |\n| Gatherer characteristics | 18% (0.06) | 61% (0.21) | 40% (0.14) |\n\n*Note: Numbers in parentheses are log point differences.* \n\n### Question\n\nAccording to the 'Cotton method' results in Table 2, 60% of the total wage gap is attributed to the 'Wage structure'. What is the correct economic interpretation of this finding?", "Options": {"A": "It represents the portion of the earnings gap that is unexplained by observable worker characteristics, suggesting the presence of a contractual rent.", "B": "It represents the portion of the earnings gap that is explained by the fact that workers in relative payment contracts have more productive characteristics.", "C": "It confirms that workers in relative payment contracts are approximately 60% more productive than those in piece-rate contracts.", "D": "It is the simple wage premium estimated by the OLS model in Table 1, which does not account for differences in worker characteristics."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). The original QA problem's components are highly suitable for conversion. Conceptual Clarity = 8/10 (interpreting specific regression and decomposition outputs is a structured task). Discriminability = 10/10 (classic misconceptions exist around explained vs. unexplained components and log-level interpretations). This item assesses the core conceptual takeaway of the Oaxaca decomposition. The distractors are designed to capture common misinterpretations: B confuses the 'unexplained' component with the 'explained' component; C misinterprets the 'wage structure' as a measure of productivity; D confuses the sophisticated decomposition result with the 'naive' OLS estimate.", "qid": "499", "question": "### Background\n\n**Research Question.** This problem analyzes the sources of the earnings gap between relative payment and piece-rate contracts by moving from a simple regression estimate to a formal decomposition of the gap into a portion explained by worker characteristics and a portion explained by the contractual wage structure itself.\n\n**Setting and Sample.** The analysis uses a sample of 105 shrimp gatherers in Honduras. The dependent variable is the natural logarithm of daily larva income. The total observed difference in mean log wages between the two groups is 0.346.\n\n### Data / Model Specification\n\nAn initial Ordinary Least Squares (OLS) model is estimated on the pooled sample of all workers, with results in Table 1.\n\n**Table 1: OLS Wage Differentials by Contract**\n\n| Variable | Pooled Coefficient (t-stat) |\n| :--- | :---: |\n| Constant | 3.09 (23.04) |\n| Literacy | 0.13 (2.29) |\n| Larva experience | 0.05 (3.27) |\n| Gatherer age | 0.002 (-0.84) |\n| Village B | -0.35 (-3.65) |\n| Village C | 0.15 (-2.14) |\n| **Relative payment** | **0.18 (1.92)** |\n| Sample size | 105 |\n| Adjusted R2 | 0.43 |\n\nTo further investigate the wage gap, an Oaxaca-Blinder decomposition is performed. This method partitions the total gap into a component due to differences in mean characteristics (the 'explained' part) and a component due to differences in the returns to those characteristics (the 'unexplained' part, or wage structure). The results are in Table 2.\n\n**Table 2: Decomposition of Relative Payment – Piece-rate Log Wage Differential**\n(Total Difference in Mean Log Wages = 0.346)\n\n| | Evaluated at piece-rate means | Evaluated at relative payment means | Cotton method |\n| :--- | :---: | :---: | :---: |\n| **Difference due to:** | | |\n| Wage structure | 82% (0.28) | 39% (0.13) | 60% (0.21) |\n| Gatherer characteristics | 18% (0.06) | 61% (0.21) | 40% (0.14) |\n\n*Note: Numbers in parentheses are log point differences.* \n\n### Question\n\nAccording to the 'Cotton method' results in Table 2, 60% of the total wage gap is attributed to the 'Wage structure'. What is the correct economic interpretation of this finding?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 432, "Question": "### Background\n\n**Research Question.** This problem explores the core identification challenge of empirically distinguishing between two competing hypotheses for consumption inequality: that richer households are more patient (wealth-varying Rate of Time Preference, RTP) versus that they are more willing to substitute consumption over time (wealth-varying Intertemporal Elasticity of Substitution, IES).\n\n**Setting / Institutional Environment.** The analysis is framed within a complete markets model with heterogeneous agents. The key to the identification strategy is the use of panel data where the underlying intertemporal price of consumption fluctuates, experiencing periods of both growth and decline.\n\n**Variables & Parameters.**\n- `$\\hat{C}^{h}(t)$`: Log consumption growth for household `h` at time `t`.\n- `$\\delta_{h}$`: The household-specific Rate of Time Preference (RTP).\n- `$\\sigma_{h}(t)$`: The household-specific Intertemporal Elasticity of Substitution (IES).\n- `$r(t)$`: The log real interest rate, or intertemporal price of consumption.\n\n---\n\n### Data / Model Specification\n\nThe approximate relationship between individual consumption growth, preferences, and prices is given by:\n```latex\n\\hat{C}^{h}(t) \\cong \\sigma_{h}(t) \\{r(t) - \\delta_{h}\\} \\quad \\text{(Eq. 1)}\n```\n\n---\n\n### The Question\n\nConsider a hypothetical economy where the real interest rate `$r(t)$` is always high and constant over time (`$r(t) = r > \\delta_h$` for all households). In this scenario, why would it be difficult to empirically distinguish between a wealth-varying RTP model and a wealth-varying IES model?", "Options": {"A": "Both models predict that richer households have more volatile consumption growth.", "B": "The two models make opposite predictions about consumption growth, which would cancel out.", "C": "Both models predict that richer households have a higher rate of consumption growth, making them observationally equivalent.", "D": "A constant interest rate makes it impossible to estimate any preference parameters."}, "Answer": "C", "pi_justification": "This item uses a Scenario Application strategy to test the core concept of observational equivalence, a key hurdle the paper seeks to overcome. The distractors target specific misunderstandings of the models' predictions and the implications of a constant interest rate.", "qid": "432", "question": "### Background\n\n**Research Question.** This problem explores the core identification challenge of empirically distinguishing between two competing hypotheses for consumption inequality: that richer households are more patient (wealth-varying Rate of Time Preference, RTP) versus that they are more willing to substitute consumption over time (wealth-varying Intertemporal Elasticity of Substitution, IES).\n\n**Setting / Institutional Environment.** The analysis is framed within a complete markets model with heterogeneous agents. The key to the identification strategy is the use of panel data where the underlying intertemporal price of consumption fluctuates, experiencing periods of both growth and decline.\n\n**Variables & Parameters.**\n- `$\\hat{C}^{h}(t)$`: Log consumption growth for household `h` at time `t`.\n- `$\\delta_{h}$`: The household-specific Rate of Time Preference (RTP).\n- `$\\sigma_{h}(t)$`: The household-specific Intertemporal Elasticity of Substitution (IES).\n- `$r(t)$`: The log real interest rate, or intertemporal price of consumption.\n\n---\n\n### Data / Model Specification\n\nThe approximate relationship between individual consumption growth, preferences, and prices is given by:\n```latex\n\\hat{C}^{h}(t) \\cong \\sigma_{h}(t) \\{r(t) - \\delta_{h}\\} \\quad \\text{(Eq. 1)}\n```\n\n---\n\n### The Question\n\nConsider a hypothetical economy where the real interest rate `$r(t)$` is always high and constant over time (`$r(t) = r > \\delta_h$` for all households). In this scenario, why would it be difficult to empirically distinguish between a wealth-varying RTP model and a wealth-varying IES model?", "model_evaluation_result": {"model_raw_response": "Looking at this identification problem, I need to analyze what happens when the real interest rate r(t) is constant and high.\n\nGiven the equation: Ĉʰ(t) ≅ σₕ(t){r(t) - δₕ}\n\nWith r(t) = r constant and r > δₕ for all households, this becomes:\nĈʰ(t) ≅ σₕ(t)(r - δₕ)\n\nNow let me consider the two competing models:\n\n**Wealth-varying RTP model:** Richer households are more patient (lower δₕ)\n- If wealth ↑, then δₚ ↓, so (r - δₕ) ↑\n- This leads to higher consumption growth for richer households\n\n**Wealth-varying IES model:** Richer households have higher willingness to substitute (higher σₕ)  \n- If wealth ↑, then σₕ ↑, so σₕ(r - δₕ) ↑\n- This also leads to higher consumption growth for richer households\n\nBoth models predict the same relationship: richer households have higher consumption growth rates. With a constant interest rate, we cannot separate whether this higher growth comes from:\n1. Lower time preference (δₕ) making the term (r - δₕ) larger, or\n2. Higher elasticity of substitution (σₕ) making the overall product larger\n\nThe models are observationally equivalent because they both generate the same prediction about the relationship between wealth and consumption growth.\n\nC", "model_answer": "C", "error": null}}
{"ID": 427, "Question": "### Background\n\n**Research Question.** This problem analyzes why the optimal, zero-inflation Ramsey outcome is not a stable equilibrium for a government learning with an approximating model.\n\n**Setting.** A government's beliefs are represented by coefficients `γ` in a perceived Phillips curve. A self-confirming equilibrium (SCE) is a set of beliefs that, when acted upon, generates data that confirms those same beliefs. The Ramsey outcome (zero inflation) is supported by the belief that the Phillips curve is vertical.\n\n### Data / Model Specification\n\nThe government's **approximating model** is a linear Phillips curve:\n\n```latex\nU_{n} = \\gamma_{1}\\pi_{n} + \\gamma^{\\prime}_{-1}X_{n-1} + \\eta_{n} \\quad \\text{(Eq. (1))}\n```\n\nThe **true data generating process (DGP)** embodies the natural rate hypothesis:\n\n```latex\nU_{n} = u - \\theta(\\pi_{n} - \\hat{x}_{n}) + \\sigma_{1}W_{1n} \\quad \\text{(Eq. (2))}\n```\n\nwhere `π_n` is inflation, `x̂_n` is expected inflation, `θ > 0` is the true trade-off parameter, and `W_1n` is a shock independent of inflation shocks.\n\n### Question\n\nSuppose the government holds the 'Ramsey' belief that the Phillips curve is vertical (`γ_1 = 0`) and, as a result, sets the systematic part of inflation `x_n` to zero. Given that the public has rational expectations (`x̂_n = x_n`), what value for the Phillips curve slope (`γ_1`) would the government's least squares estimation recover from the data generated by this policy?", "Options": {"A": "-θ", "B": "0", "C": "θ", "D": "u"}, "Answer": "A", "pi_justification": "This item assesses the core insight of why the optimal (Ramsey) outcome is not a self-confirming equilibrium. It uses a 'Computational Judgment' strategy, requiring the student to derive the OLS estimand under a hypothetical scenario. Distractors are designed based on common errors: (B) represents the flawed logic that beliefs are always self-confirming, (C) is a sign error, and (D) is a true but irrelevant parameter from the model.", "qid": "427", "question": "### Background\n\n**Research Question.** This problem analyzes why the optimal, zero-inflation Ramsey outcome is not a stable equilibrium for a government learning with an approximating model.\n\n**Setting.** A government's beliefs are represented by coefficients `γ` in a perceived Phillips curve. A self-confirming equilibrium (SCE) is a set of beliefs that, when acted upon, generates data that confirms those same beliefs. The Ramsey outcome (zero inflation) is supported by the belief that the Phillips curve is vertical.\n\n### Data / Model Specification\n\nThe government's **approximating model** is a linear Phillips curve:\n\n```latex\nU_{n} = \\gamma_{1}\\pi_{n} + \\gamma^{\\prime}_{-1}X_{n-1} + \\eta_{n} \\quad \\text{(Eq. (1))}\n```\n\nThe **true data generating process (DGP)** embodies the natural rate hypothesis:\n\n```latex\nU_{n} = u - \\theta(\\pi_{n} - \\hat{x}_{n}) + \\sigma_{1}W_{1n} \\quad \\text{(Eq. (2))}\n```\n\nwhere `π_n` is inflation, `x̂_n` is expected inflation, `θ > 0` is the true trade-off parameter, and `W_1n` is a shock independent of inflation shocks.\n\n### Question\n\nSuppose the government holds the 'Ramsey' belief that the Phillips curve is vertical (`γ_1 = 0`) and, as a result, sets the systematic part of inflation `x_n` to zero. Given that the public has rational expectations (`x̂_n = x_n`), what value for the Phillips curve slope (`γ_1`) would the government's least squares estimation recover from the data generated by this policy?", "model_evaluation_result": {"model_raw_response": "I need to analyze what happens when a government believes the Phillips curve is vertical (γ₁ = 0) and sets systematic inflation to zero, then estimates the Phillips curve slope from the resulting data.\n\nGiven information:\n- Government believes γ₁ = 0 (vertical Phillips curve)\n- Government sets systematic inflation x_n = 0\n- Public has rational expectations: x̂_n = x_n = 0\n- True DGP: U_n = u - θ(π_n - x̂_n) + σ₁W₁ₙ\n- Government's approximating model: U_n = γ₁π_n + γ'₋₁X_{n-1} + η_n\n\nSince the government sets x_n = 0 and the public has rational expectations (x̂_n = x_n = 0), the true DGP becomes:\n\nU_n = u - θ(π_n - 0) + σ₁W₁ₙ = u - θπ_n + σ₁W₁ₙ\n\nWhen the government runs least squares regression of U_n on π_n using its approximating model, it will recover the coefficient on π_n from the actual data generated.\n\nFrom the true DGP with x̂_n = 0:\nU_n = u - θπ_n + σ₁W₁ₙ\n\nComparing this to the government's approximating model:\nU_n = γ₁π_n + γ'₋₁X_{n-1} + η_n\n\nThe coefficient on π_n in the true relationship is -θ.\n\nTherefore, when the government estimates γ₁ using least squares on the data generated by its zero-inflation policy, it will recover γ₁ = -θ.\n\nA", "model_answer": "A", "error": null}}
{"ID": 421, "Question": "### Background\n\n**Research Question.** This problem examines the causal impact of public exposure for low performance on an individual's subsequent intrinsic motivation and effort.\n\n**Setting and Sample.** The study uses a multi-part laboratory experiment where subjects perform a real-effort task. In Part 3, the lowest-ranked performer (Rank 3) is publicly exposed. This exposure event is the 'treatment'. Part 4 measures performance after the treatment. The analysis focuses on the subsample of individuals who were the least performers in Part 3, comparing those who were actually exposed to those whose exposure was waived by their peers.\n\n### Data / Model Specification\n\nTable 1 presents mean performance data, while Table 2 shows a regression estimate of the treatment effect.\n\n**Table 1: Mean Performance of Least Performers in Part 3**\n\n| Group       | Part 1 | Part 2 | Part 3 | Part 4 | N  |\n|-------------|--------|--------|--------|--------|----|\n| Not exposed | 116.22 | 123.80 | 126.58 | 130.54 | 81 |\n| Exposed     | 121.25 | 123.47 | 127.39 | 127.71 | 51 |\n\n**Table 2: Determinants of the Evolution of Performance for Rank 3 Performers**\n\n| Dependent variable: Perf. in part 4 - Perf. in part 3 | Coefficient (Std. Err.) |\n|:------------------------------------------------------|:------------------------|\n| Exposure of rank 3 in part 3                          | -3.123** (1.472)        |\n\n*Notes: OLS estimates. ** indicates significance at the 5% level. Model includes other controls.* \n\n### Question\n\nA simple Difference-in-Differences (DiD) analysis using the data in Table 1 yields an estimated effect of exposure of -3.64. However, the regression model in Table 2, which includes additional controls, estimates the effect to be -3.123. Given the pre-treatment performance trends from Part 1 to Part 2 shown in Table 1, what is the most likely explanation for this discrepancy?", "Options": {"A": "The simple DiD estimate is biased downwards (more negative) because the 'Not exposed' control group had a more positive performance trend than the 'Exposed' group even before the treatment.", "B": "The simple DiD estimate is biased upwards (less negative) because the 'Exposed' treatment group was already improving faster than the 'Not exposed' group before the treatment.", "C": "The regression estimate is more precise because it has a smaller standard error, which is common when adding control variables.", "D": "The simple DiD estimate is larger in magnitude because it fails to account for the baseline performance difference between the groups in Part 3."}, "Answer": "A", "pi_justification": "This item assesses a deep understanding of omitted variable bias in the context of DiD, using a Reverse-Reasoning strategy. It requires connecting the violation of the parallel trends assumption to the direction of bias in the naive estimator. The correct answer (A) correctly identifies the direction of bias and its source in the pre-trends. Distractor B (Conceptual Opposite) gets both the direction and the reason wrong. Distractor C (True but Irrelevant) confuses bias with precision. Distractor D (Formula Misuse) points to a common but incorrect understanding of what DiD controls for.", "qid": "421", "question": "### Background\n\n**Research Question.** This problem examines the causal impact of public exposure for low performance on an individual's subsequent intrinsic motivation and effort.\n\n**Setting and Sample.** The study uses a multi-part laboratory experiment where subjects perform a real-effort task. In Part 3, the lowest-ranked performer (Rank 3) is publicly exposed. This exposure event is the 'treatment'. Part 4 measures performance after the treatment. The analysis focuses on the subsample of individuals who were the least performers in Part 3, comparing those who were actually exposed to those whose exposure was waived by their peers.\n\n### Data / Model Specification\n\nTable 1 presents mean performance data, while Table 2 shows a regression estimate of the treatment effect.\n\n**Table 1: Mean Performance of Least Performers in Part 3**\n\n| Group       | Part 1 | Part 2 | Part 3 | Part 4 | N  |\n|-------------|--------|--------|--------|--------|----|\n| Not exposed | 116.22 | 123.80 | 126.58 | 130.54 | 81 |\n| Exposed     | 121.25 | 123.47 | 127.39 | 127.71 | 51 |\n\n**Table 2: Determinants of the Evolution of Performance for Rank 3 Performers**\n\n| Dependent variable: Perf. in part 4 - Perf. in part 3 | Coefficient (Std. Err.) |\n|:------------------------------------------------------|:------------------------|\n| Exposure of rank 3 in part 3                          | -3.123** (1.472)        |\n\n*Notes: OLS estimates. ** indicates significance at the 5% level. Model includes other controls.* \n\n### Question\n\nA simple Difference-in-Differences (DiD) analysis using the data in Table 1 yields an estimated effect of exposure of -3.64. However, the regression model in Table 2, which includes additional controls, estimates the effect to be -3.123. Given the pre-treatment performance trends from Part 1 to Part 2 shown in Table 1, what is the most likely explanation for this discrepancy?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to compare the simple DiD estimate (-3.64) with the regression estimate (-3.123) and understand why they differ based on the pre-treatment trends.\n\nFirst, let me calculate the pre-treatment trends from Table 1:\n- Not exposed group: Part 2 - Part 1 = 123.80 - 116.22 = 7.58\n- Exposed group: Part 2 - Part 1 = 123.47 - 121.25 = 2.22\n\nThe \"Not exposed\" group had a much stronger positive performance trend (+7.58) compared to the \"Exposed\" group (+2.22) before treatment.\n\nIn a simple DiD analysis, we assume parallel trends - that both groups would have continued with the same trend difference in the absence of treatment. However, when pre-treatment trends differ significantly (as they do here), the simple DiD estimate can be biased.\n\nThe simple DiD estimate (-3.64) is more negative than the regression estimate (-3.123). This suggests that the simple DiD is overestimating the negative effect of exposure.\n\nThis happens because the \"Not exposed\" group was already on a steeper upward trajectory before treatment. If this differential trend continued into Part 4, the \"Not exposed\" group would naturally perform better relative to the \"Exposed\" group, even without any treatment effect. The simple DiD would incorrectly attribute this performance difference to the treatment, making the treatment effect appear more negative than it actually is.\n\nThe regression model with additional controls likely accounts for these pre-treatment trend differences, yielding a less biased (less negative) estimate.\n\nA", "model_answer": "A", "error": null}}
{"ID": 429, "Question": "### Background\n\n**Research Question.** This problem seeks to identify the specific nature of the learning bias that drives a government's beliefs away from a self-confirming equilibrium.\n\n**Setting.** In a simplified static model, the escape dynamics are driven by a perturbation term, `v̇`, which represents a systematic learning bias induced by a rare sequence of shocks. This term is derived to be:\n\n```latex\n\\dot{v} = R^{-1} \\left[\\begin{array}{c}{\\sigma_{1}\\sigma_{2}}\\\\ {0}\\end{array}\\right]\n```\n\nThis bias affects the government's estimate of the Phillips curve slope, `γ_1`.\n\n### Question\n\nThe perturbation term `v̇` represents the learning bias induced by the 'most likely unlikely' sequence of shocks. What is the economic interpretation of this shock sequence?", "Options": {"A": "A persistent sequence of shocks where the unemployment shock (`W_1n`) and inflation shock (`W_2n`) are positively correlated.", "B": "A persistent sequence of shocks where the unemployment shock (`W_1n`) and inflation shock (`W_2n`) are negatively correlated.", "C": "A sequence of unusually large, but still independent, inflation shocks (`W_2n`).", "D": "A sequence where the government's policy shocks are perfectly anticipated by the public."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to connect the mathematical result for `v̇` to its underlying economic cause. It uses a 'Reverse-Reasoning' strategy. The distractors represent common conceptual errors: (B) is the conceptual opposite (negative correlation), (C) suggests an alternative but incorrect mechanism (shock size vs. correlation), and (D) is a true statement about the model's assumptions that is irrelevant to the specific escape mechanism.", "qid": "429", "question": "### Background\n\n**Research Question.** This problem seeks to identify the specific nature of the learning bias that drives a government's beliefs away from a self-confirming equilibrium.\n\n**Setting.** In a simplified static model, the escape dynamics are driven by a perturbation term, `v̇`, which represents a systematic learning bias induced by a rare sequence of shocks. This term is derived to be:\n\n```latex\n\\dot{v} = R^{-1} \\left[\\begin{array}{c}{\\sigma_{1}\\sigma_{2}}\\\\ {0}\\end{array}\\right]\n```\n\nThis bias affects the government's estimate of the Phillips curve slope, `γ_1`.\n\n### Question\n\nThe perturbation term `v̇` represents the learning bias induced by the 'most likely unlikely' sequence of shocks. What is the economic interpretation of this shock sequence?", "model_evaluation_result": {"model_raw_response": "Looking at the perturbation term structure:\n\n```latex\n\\dot{v} = R^{-1} \\left[\\begin{array}{c}{\\sigma_{1}\\sigma_{2}}\\\\ {0}\\end{array}\\right]\n```\n\nThe key insight is that this term affects the government's estimate of the Phillips curve slope γ₁, which captures the relationship between unemployment and inflation.\n\nThe presence of the product σ₁σ₂ (where σ₁ and σ₂ are the standard deviations of the unemployment and inflation shocks respectively) in the first component suggests that the learning bias emerges from the correlation structure between these two types of shocks.\n\nIn the context of Phillips curve estimation, when unemployment shocks (W₁ₙ) and inflation shocks (W₂ₙ) are negatively correlated, this creates a systematic pattern that mimics a steeper Phillips curve relationship than actually exists. This would lead the government to systematically overestimate the trade-off between unemployment and inflation.\n\nThe \"most likely unlikely\" sequence refers to a persistent pattern of negative correlation between these shocks - while each individual realization might not be extremely rare, the persistence of this correlation pattern over time becomes increasingly unlikely under the assumption of independence, yet it's the most probable among all such unlikely sequences.\n\nB", "model_answer": "B", "error": null}}
{"ID": 410, "Question": "### Background\n\n**Research Question.** This problem evaluates the paper's main policy proposal: a unilateral 'fiscal devaluation' by a financially distressed 'periphery' country to stabilize its economy within a monetary union.\n\n**Setting / Institutional Environment.** The home (periphery) country is in a recession caused by an asymmetric financial shock. The government implements a counter-cyclical fiscal policy consisting of a payroll subsidy (`$\\varsigma_t^P$`) for its firms, financed in a revenue-neutral way by a value-added tax (`$\\tau_t^V$`) levied on all domestic sales (of both home and foreign goods).\n\n### Data / Model Specification\n\nThe policy is governed by three key equations:\n\n1.  A Taylor-type fiscal rule sets the VAT rate based on the output gap:\n    ```latex\n    \\tau_{t}^{V}=\\frac{\\Delta_{t}}{1+\\Delta_{t}},\\quad \\mathrm{with} \\quad \\Delta_{t}=-\\alpha^{FD}\\ln\\bigg(\\frac{y_{t}}{y}\\bigg) \\quad \\text{(Eq. (1))}\n    ```\n\n2.  A revenue-neutrality constraint determines the payroll subsidy:\n    ```latex\n    \\varsigma_{t}^{P}w_{t}h_{t}=\\tau_{t}^{V}(p_{h,t}c_{h,t}+p_{f,t}c_{f,t}) \\quad \\text{(Eq. (2))}\n    ```\n\n3.  These policies modify the home firm's equity issuance threshold, `$a_t^E$`, which is the breakeven level of an idiosyncratic cost shock above which the firm must issue costly equity:\n    ```latex\n    a_{t}^{E}=\\frac{A_{t}}{\\left(1-\\varsigma_{t}^{P}\\right)w_{t}}\\left[\\frac{p_{h,t}\\left(1-\\tau_{t}^{V}\\right)c_{h,t}+q_{t}p_{h,t}^{*}c_{h,t}^{*}}{\\left(\\phi+c_{h,t}+c_{h,t}^{*}\\right)^{\\frac{1}{\\alpha}}}\\right] \\quad \\text{(Eq. (3))}\n    ```\n\n### Question\n\nThe paper argues that this revenue-neutral fiscal devaluation policy successfully alleviates financial constraints for home country firms through a 'firm balance sheet channel'. What is the core economic reason for this positive net effect on firm liquidity?", "Options": {"A": "The payroll subsidy applies to the firm's total wage bill, which is a larger base than the domestic sales revenue to which the VAT applies.", "B": "The payroll subsidy rate (`$\\varsigma_t^P$`) is, by construction of the fiscal rules, always larger than the VAT rate (`$\\tau_t^V$`).", "C": "The policy stimulates aggregate demand, and the resulting increase in total sales revenue more than compensates for the tax on domestic sales.", "D": "The VAT revenue reduction is exactly offset by the payroll subsidy cost reduction, but the policy signals government support, which lowers the cost of external finance."}, "Answer": "A", "pi_justification": "This item assesses understanding of the paper's novel 'firm balance sheet channel' for fiscal devaluation. It uses a Reverse-Reasoning strategy, asking for the core mechanism behind the policy's success. The distractors are designed to target specific, plausible errors: A is correct. B (Formula Misuse) confuses the rate with the total effect. C (True but Irrelevant) describes a potential secondary demand channel, not the direct balance sheet channel. D (Conceptual Opposite) invokes the most common misconception of ignoring the different tax/subsidy bases and adds an irrelevant behavioral story.", "qid": "410", "question": "### Background\n\n**Research Question.** This problem evaluates the paper's main policy proposal: a unilateral 'fiscal devaluation' by a financially distressed 'periphery' country to stabilize its economy within a monetary union.\n\n**Setting / Institutional Environment.** The home (periphery) country is in a recession caused by an asymmetric financial shock. The government implements a counter-cyclical fiscal policy consisting of a payroll subsidy (`$\\varsigma_t^P$`) for its firms, financed in a revenue-neutral way by a value-added tax (`$\\tau_t^V$`) levied on all domestic sales (of both home and foreign goods).\n\n### Data / Model Specification\n\nThe policy is governed by three key equations:\n\n1.  A Taylor-type fiscal rule sets the VAT rate based on the output gap:\n    ```latex\n    \\tau_{t}^{V}=\\frac{\\Delta_{t}}{1+\\Delta_{t}},\\quad \\mathrm{with} \\quad \\Delta_{t}=-\\alpha^{FD}\\ln\\bigg(\\frac{y_{t}}{y}\\bigg) \\quad \\text{(Eq. (1))}\n    ```\n\n2.  A revenue-neutrality constraint determines the payroll subsidy:\n    ```latex\n    \\varsigma_{t}^{P}w_{t}h_{t}=\\tau_{t}^{V}(p_{h,t}c_{h,t}+p_{f,t}c_{f,t}) \\quad \\text{(Eq. (2))}\n    ```\n\n3.  These policies modify the home firm's equity issuance threshold, `$a_t^E$`, which is the breakeven level of an idiosyncratic cost shock above which the firm must issue costly equity:\n    ```latex\n    a_{t}^{E}=\\frac{A_{t}}{\\left(1-\\varsigma_{t}^{P}\\right)w_{t}}\\left[\\frac{p_{h,t}\\left(1-\\tau_{t}^{V}\\right)c_{h,t}+q_{t}p_{h,t}^{*}c_{h,t}^{*}}{\\left(\\phi+c_{h,t}+c_{h,t}^{*}\\right)^{\\frac{1}{\\alpha}}}\\right] \\quad \\text{(Eq. (3))}\n    ```\n\n### Question\n\nThe paper argues that this revenue-neutral fiscal devaluation policy successfully alleviates financial constraints for home country firms through a 'firm balance sheet channel'. What is the core economic reason for this positive net effect on firm liquidity?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand the mechanism by which the revenue-neutral fiscal devaluation improves firm liquidity despite being revenue-neutral overall.\n\nLet me analyze each option:\n\nA) This focuses on the different tax bases - payroll (total wage bill) vs. domestic sales revenue. This could create asymmetric effects even in a revenue-neutral policy.\n\nB) This claims the subsidy rate is always larger than the VAT rate, but this isn't necessarily true from the equations provided.\n\nC) This suggests the policy works through demand stimulation, but the question specifically asks about the \"firm balance sheet channel\" rather than demand effects.\n\nD) This suggests a signaling effect, but revenue neutrality means there's no net fiscal cost to signal with.\n\nThe key insight is in the asymmetry of the policy's application. Looking at equation (2), the revenue neutrality constraint equates:\n- Payroll subsidy applied to total wage bill: `$\\varsigma_{t}^{P}w_{t}h_{t}$`\n- VAT applied only to domestic sales: `$\\tau_{t}^{V}(p_{h,t}c_{h,t}+p_{f,t}c_{f,t})$`\n\nFor firms that export (sell both domestically and abroad), they receive subsidies on their entire wage bill but only pay VAT on their domestic sales. The export portion of their sales generates revenue without the VAT burden, while still benefiting from the payroll subsidy.\n\nThis asymmetry in the tax base is what creates the positive net effect on firm liquidity through the balance sheet channel - firms benefit from subsidies on a broader base (total employment) while facing taxes on a narrower base (domestic sales only).\n\nA", "model_answer": "A", "error": null}}
{"ID": 439, "Question": "### Background\n\n**Research Question:** This question analyzes the formal proof of the welfare implications of using asymmetric reserve prices.\n\n**Setting / Institutional Environment:** Consider a symmetric first-price auction with two groups of bidders (1 and 2) with `N_1` and `N_2` members, respectively. They face reserve prices `r_1` and `r_2` with `r_1 < r_2`. The auction is in a semi-separating equilibrium, characterized by a critical valuation `hat{s}`.\n\n### Data / Model Specification\n\nThe equilibrium expected utility for a group-2 (unfavored) bidder with valuation `s` in the range `s ∈ [r_2, hat{s}]` is given by:\n\n```latex\nV_{2}(r_{1},r_{2}|s) = F(\\hat{s})^{N_{1}}\\displaystyle\\int_{r_{2}}^{s}F(z)^{N_{2}-1}d z \\quad \\text{(Eq. (1))}\n```\n\nA key result from the paper (Lemma 2) states that if `r_1` is lowered to `r_1' < r_1` (holding `r_2` fixed), the critical valuation `hat{s}` increases to `hat{s}' > hat{s}`.\n\n### The Question\n\nConsider a group-2 bidder with valuation `s` such that `r_2 < s < hat{s}`. When `r_1` is lowered to `r_1'`, the critical threshold increases to `hat{s}'`. Based on the provided expression for expected utility in Eq. (1), what is the direct mathematical reason for this bidder's welfare increase?", "Options": {"A": "The bidder's probability of winning against all group-1 bidders, represented by the term `F(hat{s})^{N_1}`, increases because `hat{s}` increases to `hat{s}'`.", "B": "The integral term `∫_{r_2}^{s} F(z)^{N_2-1} dz` increases in value because the integration range `[r_2, s]` expands.", "C": "The term `F(hat{s})^{N_1}` decreases because a higher `hat{s}` signifies stronger competition from group 1, reducing the probability of winning.", "D": "The bidder's valuation `s` is now in a different segment of the piecewise utility function, which uses a more favorable formula to calculate payoff."}, "Answer": "A", "pi_justification": "This Single Choice item tests the student's ability to perform a computational judgment by applying a key lemma to the formal expression for expected utility. It isolates the core mathematical step from the proof in the original QA. The distractors are based on specific errors: (B) misinterprets the integral as changing; (C) makes a sign error in reasoning about the effect of `hat{s}`; (D) incorrectly applies the piecewise function definition, a common error in such problems.", "qid": "439", "question": "### Background\n\n**Research Question:** This question analyzes the formal proof of the welfare implications of using asymmetric reserve prices.\n\n**Setting / Institutional Environment:** Consider a symmetric first-price auction with two groups of bidders (1 and 2) with `N_1` and `N_2` members, respectively. They face reserve prices `r_1` and `r_2` with `r_1 < r_2`. The auction is in a semi-separating equilibrium, characterized by a critical valuation `hat{s}`.\n\n### Data / Model Specification\n\nThe equilibrium expected utility for a group-2 (unfavored) bidder with valuation `s` in the range `s ∈ [r_2, hat{s}]` is given by:\n\n```latex\nV_{2}(r_{1},r_{2}|s) = F(\\hat{s})^{N_{1}}\\displaystyle\\int_{r_{2}}^{s}F(z)^{N_{2}-1}d z \\quad \\text{(Eq. (1))}\n```\n\nA key result from the paper (Lemma 2) states that if `r_1` is lowered to `r_1' < r_1` (holding `r_2` fixed), the critical valuation `hat{s}` increases to `hat{s}' > hat{s}`.\n\n### The Question\n\nConsider a group-2 bidder with valuation `s` such that `r_2 < s < hat{s}`. When `r_1` is lowered to `r_1'`, the critical threshold increases to `hat{s}'`. Based on the provided expression for expected utility in Eq. (1), what is the direct mathematical reason for this bidder's welfare increase?", "model_evaluation_result": {"model_raw_response": "Looking at the expected utility formula for a group-2 bidder:\n\nV₂(r₁,r₂|s) = F(ŝ)^N₁ ∫_{r₂}^s F(z)^{N₂-1} dz\n\nWhen r₁ is lowered to r₁' < r₁, the critical valuation ŝ increases to ŝ' > ŝ.\n\nFor a bidder with valuation s where r₂ < s < ŝ, I need to analyze how each component of the utility function changes:\n\n1. The term F(ŝ)^N₁ represents the probability that all N₁ group-1 bidders have valuations below ŝ (i.e., the probability of winning against all group-1 bidders).\n\n2. When ŝ increases to ŝ', we have F(ŝ') > F(ŝ) since F is a cumulative distribution function.\n\n3. Therefore, F(ŝ')^N₁ > F(ŝ)^N₁, meaning the probability of winning against all group-1 bidders increases.\n\n4. The integral term ∫_{r₂}^s F(z)^{N₂-1} dz remains unchanged since neither r₂ nor s changes, and this integral doesn't depend on ŝ.\n\nThe direct mathematical reason for the welfare increase is that F(ŝ)^N₁ increases when ŝ increases, which directly multiplies the entire expected utility expression.\n\nA", "model_answer": "A", "error": null}}
{"ID": 441, "Question": "### Background\n\nThis problem examines the theoretical structure of the incremental Expected Utility (EU) advantage model, a probabilistic theory of choice under risk. The model uses a binary choice probability function, $P(p,q)$, representing the probability that an individual chooses gamble $p$ over gamble $q$.\n\n### Data / Model Specification\n\nThe probabilistic choice rule is given by:\n\n```latex\n\\frac{P(p,q)}{P(q,p)} = \\rho\\left(\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)}\\right)\n```\n**Eq. (1)**\n\nwhere $\\mathcal{A}(p,q)$ is the incremental advantage of $p$ over $q$. The model is consistent with standard EU theory, such that $\\mathcal{A}(p,q) > \\mathcal{A}(q,p)$ if and only if $E(u,p) > E(u,q)$.\n\n### Question\n\nConsider a specific functional form for the choice function, $\\rho(z) = z^\\gamma$, where $\\gamma > 0$. This parameter governs the stochasticity of choice. In the case where gamble $p$ has a higher expected utility than gamble $q$, what is the limit of the choice probability $P(p,q)$ as the determinism parameter $\\gamma$ approaches infinity ($\\gamma \\to \\infty$)?", "Options": {"A": "1", "B": "1/2", "C": "0", "D": "$\\infty$"}, "Answer": "A", "pi_justification": "This item tests the ability to perform a Computational Judgment by analyzing the limiting behavior of the model. It assesses whether the student can connect the model's parameters to the benchmark deterministic EU theory. Distractors are based on common mathematical and conceptual errors in limit calculations: convergence to indifference (B), sign error (C), and confusing the limit of the probability with the limit of its components (D).", "qid": "441", "question": "### Background\n\nThis problem examines the theoretical structure of the incremental Expected Utility (EU) advantage model, a probabilistic theory of choice under risk. The model uses a binary choice probability function, $P(p,q)$, representing the probability that an individual chooses gamble $p$ over gamble $q$.\n\n### Data / Model Specification\n\nThe probabilistic choice rule is given by:\n\n```latex\n\\frac{P(p,q)}{P(q,p)} = \\rho\\left(\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)}\\right)\n```\n**Eq. (1)**\n\nwhere $\\mathcal{A}(p,q)$ is the incremental advantage of $p$ over $q$. The model is consistent with standard EU theory, such that $\\mathcal{A}(p,q) > \\mathcal{A}(q,p)$ if and only if $E(u,p) > E(u,q)$.\n\n### Question\n\nConsider a specific functional form for the choice function, $\\rho(z) = z^\\gamma$, where $\\gamma > 0$. This parameter governs the stochasticity of choice. In the case where gamble $p$ has a higher expected utility than gamble $q$, what is the limit of the choice probability $P(p,q)$ as the determinism parameter $\\gamma$ approaches infinity ($\\gamma \\to \\infty$)?", "model_evaluation_result": {"model_raw_response": "I need to analyze what happens to the choice probability P(p,q) as γ approaches infinity in the given model.\n\nGiven information:\n- The probabilistic choice rule: P(p,q)/P(q,p) = ρ(A(p,q)/A(q,p))\n- The functional form: ρ(z) = z^γ where γ > 0\n- Gamble p has higher expected utility than gamble q, which means A(p,q) > A(q,p)\n\nSince P(p,q) + P(q,p) = 1 (these are the only two choices), I can write:\nP(q,p) = 1 - P(p,q)\n\nSubstituting into the choice rule:\nP(p,q)/(1 - P(p,q)) = (A(p,q)/A(q,p))^γ\n\nLet me denote A(p,q)/A(q,p) = k, where k > 1 since A(p,q) > A(q,p).\n\nSo: P(p,q)/(1 - P(p,q)) = k^γ\n\nSolving for P(p,q):\nP(p,q) = k^γ(1 - P(p,q))\nP(p,q) = k^γ - k^γP(p,q)\nP(p,q) + k^γP(p,q) = k^γ\nP(p,q)(1 + k^γ) = k^γ\nP(p,q) = k^γ/(1 + k^γ)\n\nNow, as γ → ∞:\nSince k > 1, we have k^γ → ∞\nTherefore: P(p,q) = k^γ/(1 + k^γ) = 1/(1/k^γ + 1) → 1/(0 + 1) = 1\n\nThis makes intuitive sense: as γ increases, the model becomes more deterministic. When γ → ∞, the choice becomes completely deterministic, and the gamble with higher expecte", "model_answer": "A", "error": null}}
{"ID": 465, "Question": "### Background\n\n**Research Question.** What are the practical implications of the `GDPe`/`GDPo` distinction for applied empirical research, particularly for studies of productivity and the causal effect of trade on income?\n\n**Setting / Institutional Environment.** The analysis provides guidelines for empirical researchers using cross-country data. It highlights how using the wrong GDP measure can lead to biased estimates and flawed causal inference, particularly in the context of instrumental variable (IV) strategies like that of Frankel and Romer (1999), which studies the effect of trade on income.\n\n**Variables & Parameters.**\n- `Real GDPe`: Expenditure-side real GDP, a measure of national wealth and purchasing power. Influenced by terms of trade.\n- `Real GDPo`: Output-side real GDP, a measure of national productive capacity. Excludes terms of trade effects.\n- `Terms of Trade (ToT)`: The relative price of a country's exports to its imports.\n\n**Model Specification.**\nThe Frankel and Romer (1999) study uses an IV strategy to estimate the causal effect of trade on income. The regression is `Income = β * TradeOpenness + ...`, where `TradeOpenness` is instrumented by geographic characteristics. A key assumption for the IV to be valid is the **exclusion restriction**: geography must not affect income through any channel other than its effect on trade openness.\n\n### Question\n\nIn a Frankel and Romer-style IV regression, a researcher suspects their geography-based instrument violates the exclusion restriction. They believe geography affects the outcome not just through trade volume, but also by directly improving a country's Terms of Trade (e.g., via proximity to markets). To mitigate this specific bias, which of the following is the most effective strategy?", "Options": {"A": "Switch the outcome variable from `Real GDPe` to `Real GDPo`.", "B": "Switch the outcome variable from `Real GDPo` to `Real GDPe`.", "C": "Add the country's nominal exchange rate as a control variable.", "D": "Use `Nominal Openness` instead of `Real Openness` as the endogenous regressor."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to test a deep understanding of econometric identification as applied to the paper's concepts. The user must infer the correct solution given a specific problem (a violation of the exclusion restriction). The distractors are 'Conceptual Opposite' (B), 'True but Irrelevant' (C), and 'Formula Misuse' (D), each representing a plausible but incorrect line of reasoning about IV estimation.", "qid": "465", "question": "### Background\n\n**Research Question.** What are the practical implications of the `GDPe`/`GDPo` distinction for applied empirical research, particularly for studies of productivity and the causal effect of trade on income?\n\n**Setting / Institutional Environment.** The analysis provides guidelines for empirical researchers using cross-country data. It highlights how using the wrong GDP measure can lead to biased estimates and flawed causal inference, particularly in the context of instrumental variable (IV) strategies like that of Frankel and Romer (1999), which studies the effect of trade on income.\n\n**Variables & Parameters.**\n- `Real GDPe`: Expenditure-side real GDP, a measure of national wealth and purchasing power. Influenced by terms of trade.\n- `Real GDPo`: Output-side real GDP, a measure of national productive capacity. Excludes terms of trade effects.\n- `Terms of Trade (ToT)`: The relative price of a country's exports to its imports.\n\n**Model Specification.**\nThe Frankel and Romer (1999) study uses an IV strategy to estimate the causal effect of trade on income. The regression is `Income = β * TradeOpenness + ...`, where `TradeOpenness` is instrumented by geographic characteristics. A key assumption for the IV to be valid is the **exclusion restriction**: geography must not affect income through any channel other than its effect on trade openness.\n\n### Question\n\nIn a Frankel and Romer-style IV regression, a researcher suspects their geography-based instrument violates the exclusion restriction. They believe geography affects the outcome not just through trade volume, but also by directly improving a country's Terms of Trade (e.g., via proximity to markets). To mitigate this specific bias, which of the following is the most effective strategy?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 425, "Question": "### Background\n\nThe paper calibrates its competitive assignment model to quantify the \"economics of superstars,\" where small differences in talent can justify large differences in pay due to the enormous scale of modern firms.\n\n### Data / Model Specification\n\nThe model is calibrated using three empirical facts:\n1.  Firm size distribution follows Zipf's Law (`α ≈ 1`).\n2.  CEO pay scales one-for-one with aggregate firm size (`γ ≈ 1`).\n3.  The cross-sectional elasticity of pay to own-firm size is `κ ≈ 1/3`.\n\nThe model links these via the equation `κ = γ - β/α`.\n\nThe percentage increase in a reference firm's value if it could hire the #1 CEO is given by:\n```latex\n\\text{Pct. Value Increase} = \\left(\\frac{\\alpha\\gamma}{\\beta}-1\\right) \\left(1-n_{*}^{-\\beta}\\right) \\frac{w_{*}}{S(n_{*})} \\quad \\text{(Eq. (1))}\n```\nFor the thought experiment, `n_*=250`, `w_* = $8.34 million`, and `S(n_*) = $25.0 billion`.\n\n### Question\n\nFirst, use the empirical facts to determine the calibrated value of the talent distribution parameter, `β`. Then, using this value and the provided data, calculate the percentage increase in firm #250's value if it could hire the #1 CEO.", "Options": {"A": "β ≈ 2/3; the value increase is approximately 0.016%.", "B": "β ≈ 1/3; the value increase is approximately 0.056%.", "C": "β ≈ 2/3; the value increase is approximately 530%.", "D": "β ≈ 1; the value increase is approximately 0.033%."}, "Answer": "A", "pi_justification": "This item is a multi-step Computational Judgment problem that tests the full chain of reasoning from calibration to quantitative prediction. The user must first derive β ≈ 2/3 from 1/3 ≈ 1 - β/1. Then, they must plug this into the complex formula to get the final answer. The correct answer is (A). Distractor (B) is based on the high-fidelity error of incorrectly setting β=1/3 (confusing β with κ), which leads to a different calculated value. Distractor (C) confuses the value increase with the pay premium, another plausible error. Distractor (D) uses an incorrect β=1, which would imply κ=0.", "qid": "425", "question": "### Background\n\nThe paper calibrates its competitive assignment model to quantify the \"economics of superstars,\" where small differences in talent can justify large differences in pay due to the enormous scale of modern firms.\n\n### Data / Model Specification\n\nThe model is calibrated using three empirical facts:\n1.  Firm size distribution follows Zipf's Law (`α ≈ 1`).\n2.  CEO pay scales one-for-one with aggregate firm size (`γ ≈ 1`).\n3.  The cross-sectional elasticity of pay to own-firm size is `κ ≈ 1/3`.\n\nThe model links these via the equation `κ = γ - β/α`.\n\nThe percentage increase in a reference firm's value if it could hire the #1 CEO is given by:\n```latex\n\\text{Pct. Value Increase} = \\left(\\frac{\\alpha\\gamma}{\\beta}-1\\right) \\left(1-n_{*}^{-\\beta}\\right) \\frac{w_{*}}{S(n_{*})} \\quad \\text{(Eq. (1))}\n```\nFor the thought experiment, `n_*=250`, `w_* = $8.34 million`, and `S(n_*) = $25.0 billion`.\n\n### Question\n\nFirst, use the empirical facts to determine the calibrated value of the talent distribution parameter, `β`. Then, using this value and the provided data, calculate the percentage increase in firm #250's value if it could hire the #1 CEO.", "model_evaluation_result": {"model_raw_response": "I need to solve this step by step.\n\nFirst, I'll find β using the given relationship and empirical facts.\n\nGiven:\n- α ≈ 1 (Zipf's Law parameter)\n- γ ≈ 1 (CEO pay scales one-for-one with aggregate firm size)\n- κ ≈ 1/3 (cross-sectional elasticity of pay to own-firm size)\n- The relationship: κ = γ - β/α\n\nSolving for β:\nκ = γ - β/α\n1/3 = 1 - β/1\n1/3 = 1 - β\nβ = 1 - 1/3 = 2/3\n\nSo β ≈ 2/3.\n\nNow I'll calculate the percentage value increase using Equation (1):\n\nGiven data:\n- n* = 250\n- w* = $8.34 million\n- S(n*) = $25.0 billion = $25,000 million\n- α = 1\n- γ = 1\n- β = 2/3\n\nUsing the formula:\nPct. Value Increase = (αγ/β - 1)(1 - n*^(-β)) × w*/S(n*)\n\nCalculating each component:\n- αγ/β = (1×1)/(2/3) = 3/2 = 1.5\n- αγ/β - 1 = 1.5 - 1 = 0.5\n- n*^(-β) = 250^(-2/3) = 250^(-2/3) = (250^2)^(-1/3) = (62,500)^(-1/3) ≈ 0.0253\n- 1 - n*^(-β) = 1 - 0.0253 = 0.9747\n- w*/S(n*) = 8.34/25,000 = 0.0003336\n\nFinal calculation:\nPct. Value Increase = 0.5 × 0.9747 × 0.0003336 ≈", "model_answer": "I need to solve this step by step.\n\nFirst, I'll find β using the given relationship and empirical facts.\n\nGiven:\n- α ≈ 1 (Zipf's Law parameter)\n- γ ≈ 1 (CEO pay scales one-for-one with aggregate firm size)\n- κ ≈ 1/3 (cross-sectional elasticity of pay to own-firm size)\n- The relationship: κ = γ - β/α\n\nSolving for β:\nκ = γ - β/α\n1/3 = 1 - β/1\n1/3 = 1 - β\nβ = 1 - 1/3 = 2/3\n\nSo β ≈ 2/3.\n\nNow I'll calculate the percentage value increase using Equation (1):\n\nGiven data:\n- n* = 250\n- w* = $8.34 million\n- S(n*) = $25.0 billion = $25,000 million\n- α = 1\n- γ = 1\n- β = 2/3\n\nUsing the formula:\nPct. Value Increase = (αγ/β - 1)(1 - n*^(-β)) × w*/S(n*)\n\nCalculating each component:\n- αγ/β = (1×1)/(2/3) = 3/2 = 1.5\n- αγ/β - 1 = 1.5 - 1 = 0.5\n- n*^(-β) = 250^(-2/3) = 250^(-2/3) = (250^2)^(-1/3) = (62,500)^(-1/3) ≈ 0.0253\n- 1 - n*^(-β) = 1 - 0.0253 = 0.9747\n- w*/S(n*) = 8.34/25,000 = 0.0003336\n\nFinal calculation:\nPct. Value Increase = 0.5 × 0.9747 × 0.0003336 ≈", "error": "Warning: Could not extract single choice letter from: I need to solve this step by step.\n\nFirst, I'll fi..."}}
{"ID": 385, "Question": "### Background\n\nAn upstream monopolist sells an input to a downstream duopoly. The monopolist can set discriminatory prices $(w_1, w_2)$ but is constrained by the threat of backward integration from the larger buyer (the “chain”).\n\n### Data / Model Specification\n\nThe monopolist chooses $(w_1, w_2)$ to maximize its profit $U^m(w_1, w_2)$, subject to the constraint that the chain does not integrate. It is a key assumption of the model that the unconstrained profit-maximizing price pair $(w^*, w^*)$ is infeasible because it would induce the chain to integrate.\n\n### Question\n\nGiven these assumptions, which of the following conditions must hold at the monopolist's optimal discriminatory price pair $(w_1^D, w_2^D)$?", "Options": {"A": "The chain is exactly indifferent between integrating and not integrating.", "B": "The chain strictly prefers not to integrate, giving the monopolist a buffer.", "C": "The monopolist sets prices equal to its unconstrained optimum $(w^*, w^*)$.", "D": "The prices are set such that the chain earns zero economic profit."}, "Answer": "A", "pi_justification": "This item assesses the understanding of constrained optimization in this specific economic context, replacing Q4 of the original QA problem. The correct answer (A) identifies that the constraint must be binding at the optimum. This is a fundamental result of the setup. Distractor (B) represents a non-binding constraint, which is incorrect because the unconstrained optimum is infeasible (Almost Right). Distractor (C) is explicitly ruled out by the problem's premise (Conceptual Opposite). Distractor (D) confuses the integration constraint with a zero-profit condition, which is not part of the model.", "qid": "385", "question": "### Background\n\nAn upstream monopolist sells an input to a downstream duopoly. The monopolist can set discriminatory prices $(w_1, w_2)$ but is constrained by the threat of backward integration from the larger buyer (the “chain”).\n\n### Data / Model Specification\n\nThe monopolist chooses $(w_1, w_2)$ to maximize its profit $U^m(w_1, w_2)$, subject to the constraint that the chain does not integrate. It is a key assumption of the model that the unconstrained profit-maximizing price pair $(w^*, w^*)$ is infeasible because it would induce the chain to integrate.\n\n### Question\n\nGiven these assumptions, which of the following conditions must hold at the monopolist's optimal discriminatory price pair $(w_1^D, w_2^D)$?", "model_evaluation_result": {"model_raw_response": "Looking at this constrained optimization problem, I need to analyze what happens when a monopolist faces the threat of backward integration from the larger buyer (chain).\n\nKey elements:\n- The monopolist wants to maximize profit by setting discriminatory prices $(w_1, w_2)$\n- There's a constraint: the chain must not find it profitable to integrate backward\n- The unconstrained optimum $(w^*, w^*)$ would cause integration, so it's infeasible\n\nThis is a classic constrained optimization scenario. The monopolist faces:\n- Objective: Maximize $U^m(w_1, w_2)$  \n- Constraint: Chain's profit from buying ≥ Chain's profit from integrating\n\nSince the unconstrained optimum violates the integration constraint, the monopolist must choose a constrained optimum. In constrained optimization problems like this, when the constraint is binding (which it must be, since the unconstrained solution is infeasible), the optimal solution occurs exactly at the constraint boundary.\n\nIf the chain strictly preferred not to integrate (option B), the monopolist could increase prices slightly and earn higher profits while still satisfying the constraint. This contradicts optimality.\n\nIf the constraint weren't binding, the monopolist would choose the unconstrained optimum, but we're told this causes integration.\n\nTherefore, at the optimal discriminatory prices, the integration constraint must be binding - meaning the chain is exactly indifferent between integrating and not integrating.\n\nA", "model_answer": "B", "error": null}}
{"ID": 434, "Question": "### Background\n\n**Research Question.** This problem explores the critical condition that determines whether a durable-goods monopolist can credibly threaten to maintain high prices in a subgame with one high-valuation buyer (`H`) and `n_0^L` low-valuation buyers (`L`).\n\n**Setting / Institutional Environment.** The seller considers two strategies: (i) a \"Coasian\" strategy of selling to all `n_0^L+1` buyers immediately at price `L`, and (ii) a \"Pacman\" strategy of selling to the high-valuation buyer today at their reservation price `w` and then selling to the `n_0^L` low-valuation buyers in the next period at price `L`.\n\n**Variables & Parameters.**\n- `H`, `L`: High and low buyer valuations, `H > L > 0`.\n- `n_0^L`: The number of low-valuation buyers.\n- `δ`: Per-period discount factor, `δ ∈ (0,1)`.\n\n---\n\n### Data / Model Specification\n\nThe reservation price `w` that makes a high-valuation buyer indifferent between buying today and waiting one period for a price of `L` is given by:\n```latex\nw = (1-\\delta)H + \\delta L \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nUnder which of the following conditions does the \"Pacman\" strategy yield a strictly higher profit for the seller than the \"Coasian\" strategy?\n", "Options": {"A": "`n_0^L L > H - L`", "B": "`H - L > n_0^L L`", "C": "`H > n_0^L L`", "D": "`H - L > (n_0^L + 1)L`"}, "Answer": "B", "pi_justification": "This item assesses the student's ability to formally derive the key condition for the Pacman outcome to be credible, which is the core of the paper's first result. It uses a Computational Judgment strategy. The distractors are designed based on common errors: (A) reverses the inequality, representing the condition under which a Coasian equilibrium is possible; (C) omits the `-L` term from the price discrimination premium, a plausible simplification error; (D) incorrectly compares the premium to the total Coasian profit instead of just the opportunity cost of delaying sales to L-buyers.", "qid": "434", "question": "### Background\n\n**Research Question.** This problem explores the critical condition that determines whether a durable-goods monopolist can credibly threaten to maintain high prices in a subgame with one high-valuation buyer (`H`) and `n_0^L` low-valuation buyers (`L`).\n\n**Setting / Institutional Environment.** The seller considers two strategies: (i) a \"Coasian\" strategy of selling to all `n_0^L+1` buyers immediately at price `L`, and (ii) a \"Pacman\" strategy of selling to the high-valuation buyer today at their reservation price `w` and then selling to the `n_0^L` low-valuation buyers in the next period at price `L`.\n\n**Variables & Parameters.**\n- `H`, `L`: High and low buyer valuations, `H > L > 0`.\n- `n_0^L`: The number of low-valuation buyers.\n- `δ`: Per-period discount factor, `δ ∈ (0,1)`.\n\n---\n\n### Data / Model Specification\n\nThe reservation price `w` that makes a high-valuation buyer indifferent between buying today and waiting one period for a price of `L` is given by:\n```latex\nw = (1-\\delta)H + \\delta L \\quad \\text{(Eq. (1))}\n```\n\n---\n\n### Question\n\nUnder which of the following conditions does the \"Pacman\" strategy yield a strictly higher profit for the seller than the \"Coasian\" strategy?\n", "model_evaluation_result": {"model_raw_response": "I need to compare the profits from the two strategies to determine when the \"Pacman\" strategy yields strictly higher profit.\n\n**Coasian Strategy Profit:**\nSell to all buyers immediately at price L:\n- Revenue = (n₀ᴸ + 1) × L\n- Profit = (n₀ᴸ + 1)L\n\n**Pacman Strategy Profit:**\n- Today: Sell to high-valuation buyer at reservation price w\n- Next period: Sell to n₀ᴸ low-valuation buyers at price L (discounted)\n\nFrom equation (1): w = (1-δ)H + δL\n\nPacman profit = w + δ × n₀ᴸL\n= (1-δ)H + δL + δn₀ᴸL\n= (1-δ)H + δL(1 + n₀ᴸ)\n\n**Condition for Pacman > Coasian:**\n(1-δ)H + δL(1 + n₀ᴸ) > (n₀ᴸ + 1)L\n\nExpanding:\n(1-δ)H + δL + δn₀ᴸL > n₀ᴸL + L\n\nRearranging:\n(1-δ)H > n₀ᴸL + L - δL - δn₀ᴸL\n(1-δ)H > L(n₀ᴸ + 1 - δ - δn₀ᴸ)\n(1-δ)H > L(n₀ᴸ + 1 - δ(1 + n₀ᴸ))\n(1-δ)H > L(n₀ᴸ + 1)(1 - δ)\n\nDividing both sides by (1-δ):\nH > L(n₀ᴸ + 1)\n\nThis can be rewritten as:\nH - L > n₀ᴸL\n\nThe answer is B.", "model_answer": "B", "error": null}}
{"ID": 411, "Question": "### Background\n\n**Research Question.** This problem evaluates the paper's main policy proposal: a unilateral 'fiscal devaluation' by a financially distressed 'periphery' country to stabilize its economy within a monetary union.\n\n**Setting / Institutional Environment.** The home (periphery) country is in a recession caused by an asymmetric financial shock. The government implements a counter-cyclical fiscal policy consisting of a payroll subsidy (`$\\varsigma_t^P$`) for its firms, financed in a revenue-neutral way by a value-added tax (`$\\tau_t^V$`) levied on all domestic sales (of both home and foreign goods).\n\n### Data / Model Specification\n\nThe policy is governed by three key equations:\n\n1.  A Taylor-type fiscal rule sets the VAT rate based on the output gap:\n    ```latex\n    \\tau_{t}^{V}=\\frac{\\Delta_{t}}{1+\\Delta_{t}},\\quad \\mathrm{with} \\quad \\Delta_{t}=-\\alpha^{FD}\\ln\\bigg(\\frac{y_{t}}{y}\\bigg) \\quad \\text{(Eq. (1))}\n    ```\n\n2.  A revenue-neutrality constraint determines the payroll subsidy:\n    ```latex\n    \\varsigma_{t}^{P}w_{t}h_{t}=\\tau_{t}^{V}(p_{h,t}c_{h,t}+p_{f,t}c_{f,t}) \\quad \\text{(Eq. (2))}\n    ```\n\n3.  These policies modify the home firm's equity issuance threshold, `$a_t^E$`, which is the breakeven level of an idiosyncratic cost shock above which the firm must issue costly equity:\n    ```latex\n    a_{t}^{E}=\\frac{A_{t}}{\\left(1-\\varsigma_{t}^{P}\\right)w_{t}}\\left[\\frac{p_{h,t}\\left(1-\\tau_{t}^{V}\\right)c_{h,t}+q_{t}p_{h,t}^{*}c_{h,t}^{*}}{\\left(\\phi+c_{h,t}+c_{h,t}^{*}\\right)^{\\frac{1}{\\alpha}}}\\right] \\quad \\text{(Eq. (3))}\n    ```\n\n### Question\n\nConsider a foreign (core) firm that exports its goods to the home market. This firm is subject to the VAT (`$\\tau_t^V$`) on its sales in the home country but does not receive the payroll subsidy. How does the home country's unilateral fiscal devaluation policy affect this foreign firm's optimal pricing strategy?", "Options": {"A": "It weakens the foreign firm's financial position, mitigating its ability to engage in 'predatory' price-cutting in the home market.", "B": "It encourages the foreign firm to cut its prices more aggressively to offset the tax and defend its market share from subsidized home firms.", "C": "It has no significant effect on the foreign firm's pricing strategy, as the firm simply passes the full cost of the VAT on to home consumers.", "D": "It improves the foreign firm's financial position by stabilizing aggregate demand in its export market, allowing it to increase its markups."}, "Answer": "A", "pi_justification": "This item tests the ability to apply the model to a new context (a foreign firm) and analyze international spillovers, a key part of the paper's policy argument. It uses a Scenario Application strategy. A is correct. B (Conceptual Opposite) posits the opposite strategic reaction. C (Step-Omission Error) ignores the strategic effect of the tax on the firm's own liquidity and pricing incentives. D (Conceptual Confusion) focuses only on a potential secondary demand effect while ignoring the direct, negative impact of the VAT on the foreign firm's revenue.", "qid": "411", "question": "### Background\n\n**Research Question.** This problem evaluates the paper's main policy proposal: a unilateral 'fiscal devaluation' by a financially distressed 'periphery' country to stabilize its economy within a monetary union.\n\n**Setting / Institutional Environment.** The home (periphery) country is in a recession caused by an asymmetric financial shock. The government implements a counter-cyclical fiscal policy consisting of a payroll subsidy (`$\\varsigma_t^P$`) for its firms, financed in a revenue-neutral way by a value-added tax (`$\\tau_t^V$`) levied on all domestic sales (of both home and foreign goods).\n\n### Data / Model Specification\n\nThe policy is governed by three key equations:\n\n1.  A Taylor-type fiscal rule sets the VAT rate based on the output gap:\n    ```latex\n    \\tau_{t}^{V}=\\frac{\\Delta_{t}}{1+\\Delta_{t}},\\quad \\mathrm{with} \\quad \\Delta_{t}=-\\alpha^{FD}\\ln\\bigg(\\frac{y_{t}}{y}\\bigg) \\quad \\text{(Eq. (1))}\n    ```\n\n2.  A revenue-neutrality constraint determines the payroll subsidy:\n    ```latex\n    \\varsigma_{t}^{P}w_{t}h_{t}=\\tau_{t}^{V}(p_{h,t}c_{h,t}+p_{f,t}c_{f,t}) \\quad \\text{(Eq. (2))}\n    ```\n\n3.  These policies modify the home firm's equity issuance threshold, `$a_t^E$`, which is the breakeven level of an idiosyncratic cost shock above which the firm must issue costly equity:\n    ```latex\n    a_{t}^{E}=\\frac{A_{t}}{\\left(1-\\varsigma_{t}^{P}\\right)w_{t}}\\left[\\frac{p_{h,t}\\left(1-\\tau_{t}^{V}\\right)c_{h,t}+q_{t}p_{h,t}^{*}c_{h,t}^{*}}{\\left(\\phi+c_{h,t}+c_{h,t}^{*}\\right)^{\\frac{1}{\\alpha}}}\\right] \\quad \\text{(Eq. (3))}\n    ```\n\n### Question\n\nConsider a foreign (core) firm that exports its goods to the home market. This firm is subject to the VAT (`$\\tau_t^V$`) on its sales in the home country but does not receive the payroll subsidy. How does the home country's unilateral fiscal devaluation policy affect this foreign firm's optimal pricing strategy?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze how the fiscal devaluation policy affects a foreign firm that exports to the home country.\n\nKey facts about the foreign firm:\n- Exports goods to the home market\n- Subject to VAT (τₜᵛ) on sales in home country\n- Does NOT receive the payroll subsidy (ςₜᴾ)\n- Competes with home firms that DO receive the subsidy\n\nThe fiscal devaluation creates an asymmetric competitive environment:\n\n1. **Home firms benefit from both policies**: They receive payroll subsidies (reducing their costs) AND the VAT applies to all domestic sales, so it doesn't disadvantage them relative to foreign competitors.\n\n2. **Foreign firms face a cost disadvantage**: They pay the VAT but don't receive offsetting subsidies, making their products relatively more expensive in the home market.\n\n3. **Competitive response**: Faced with subsidized home competitors and having to pay VAT, the foreign firm would need to respond strategically to maintain market share.\n\nLet me evaluate the options:\n\nA) This suggests the policy weakens the foreign firm financially - this seems backwards since the policy burdens the foreign firm with taxes.\n\nB) This suggests the foreign firm would cut prices aggressively to offset the VAT burden and compete with subsidized home firms - this makes economic sense as a competitive response.\n\nC) This assumes perfect pass-through with no competitive effects - unrealistic given that home firms are subsidized.\n\nD) This suggests the policy helps the foreign firm - but the foreign firm faces VAT without receiving subsidies, so this seems incorrect.\n\nThe most logical response is that the foreign firm, facing both the VAT burden and competition from subsidized home firms, would need to cut prices more aggressively to maintain competitiveness in the home market.\n\nB", "model_answer": "A", "error": null}}
{"ID": 469, "Question": "### Background\n\nThis problem examines the paper's empirical estimation of the technological frontier for power plants. The core of the analysis is a hedonic approach, where the price and operating characteristics of a plant are explained by its physical attributes, primarily its capacity. These estimated relationships form the foundation for the subsequent behavioral models.\n\n### Data / Model Specification\n\nThe paper estimates a fuel efficiency function that relates fuel efficiency to capacity in a log-log form:\n```latex\n\\log{(\\mathrm{EFF}_{st})} = \\alpha_{20} + \\alpha_{21}\\log{(\\mathrm{UCAP}_{st})} + \\text{Controls}_{st} + \\text{error}_{st}\n\\quad \\text{(Eq. 1)}\n```\nwhere `EFF` is fuel efficiency in megawatt hours per million BTU and `UCAP` is unit capacity in megawatts.\n\nSelected unconstrained estimates for this model are provided below.\n\n**Table 1: Selected Fuel Efficiency Estimates (from paper's Table II)**\n| Variable | Parameter | Estimate |\n| :--- | :--- | :--- |\n| Constant | `α_20` | -2.956 |\n| Log of unit capacity | `α_21` | 0.124 |\n| Main effect: Coal burnt | `δ_21` | -0.015 |\n\n### Question\n\nUsing the unconstrained estimates from Table 1, what is the predicted fuel efficiency (`EFF`) for a new 250 MW coal-burning plant? (Note: `log(250) ≈ 5.521`)", "Options": {"A": "0.051 MWh per million BTU", "B": "-2.286 MWh per million BTU", "C": "0.102 MWh per million BTU", "D": "0.103 MWh per million BTU"}, "Answer": "C", "pi_justification": "This item assesses the ability to correctly extract coefficients from a regression table and apply a log-linear formula, a core skill in applied econometrics. It is a 'Computational Judgment' item. Distractor A results from omitting the capacity term entirely. Distractor B results from failing to exponentiate the final log value. Distractor D results from omitting the dummy variable for 'Coal burnt'.", "qid": "469", "question": "### Background\n\nThis problem examines the paper's empirical estimation of the technological frontier for power plants. The core of the analysis is a hedonic approach, where the price and operating characteristics of a plant are explained by its physical attributes, primarily its capacity. These estimated relationships form the foundation for the subsequent behavioral models.\n\n### Data / Model Specification\n\nThe paper estimates a fuel efficiency function that relates fuel efficiency to capacity in a log-log form:\n```latex\n\\log{(\\mathrm{EFF}_{st})} = \\alpha_{20} + \\alpha_{21}\\log{(\\mathrm{UCAP}_{st})} + \\text{Controls}_{st} + \\text{error}_{st}\n\\quad \\text{(Eq. 1)}\n```\nwhere `EFF` is fuel efficiency in megawatt hours per million BTU and `UCAP` is unit capacity in megawatts.\n\nSelected unconstrained estimates for this model are provided below.\n\n**Table 1: Selected Fuel Efficiency Estimates (from paper's Table II)**\n| Variable | Parameter | Estimate |\n| :--- | :--- | :--- |\n| Constant | `α_20` | -2.956 |\n| Log of unit capacity | `α_21` | 0.124 |\n| Main effect: Coal burnt | `δ_21` | -0.015 |\n\n### Question\n\nUsing the unconstrained estimates from Table 1, what is the predicted fuel efficiency (`EFF`) for a new 250 MW coal-burning plant? (Note: `log(250) ≈ 5.521`)", "model_evaluation_result": {"model_raw_response": "I need to calculate the predicted fuel efficiency for a 250 MW coal-burning plant using the given regression equation and estimates.\n\nGiven information:\n- Log-log regression: log(EFF) = α₂₀ + α₂₁log(UCAP) + Controls + error\n- α₂₀ = -2.956 (constant)\n- α₂₁ = 0.124 (coefficient on log of unit capacity)\n- δ₂₁ = -0.015 (main effect for coal burnt)\n- UCAP = 250 MW\n- log(250) ≈ 5.521\n\nFor a coal-burning plant, the controls include the coal-burning effect, so:\nlog(EFF) = α₂₀ + α₂₁log(UCAP) + δ₂₁\n\nSubstituting the values:\nlog(EFF) = -2.956 + 0.124 × 5.521 + (-0.015)\nlog(EFF) = -2.956 + 0.685 - 0.015\nlog(EFF) = -2.286\n\nTo get EFF, I need to take the exponential:\nEFF = exp(-2.286) ≈ 0.102\n\nTherefore, the predicted fuel efficiency is approximately 0.102 MWh per million BTU.\n\nC", "model_answer": "C", "error": null}}
{"ID": 449, "Question": "### Background\n\n**Research Question.** In a dynamic general equilibrium setting with a constant mark-up pricing rule and corporate taxes, what determines a firm's optimal capital intensity?\n\n**Setting.** The model considers a firm with a Cobb-Douglas production technology that faces constantly changing input prices and ongoing technological progress. The firm's choice of capital intensity depends on the relative price of capital and the user cost of capital, which is influenced by taxes and inflation expectations.\n\n**Variables & Parameters.**\n- \\( \\hat{k}_t \\): Optimal capital-output ratio at time \\(t\\).\n- \\( u, k, z' \\): Corporate tax rate, investment tax credit rate, and present value of depreciation allowances.\n- \\( r, p, d \\): Nominal interest rate, expected output price inflation, and physical depreciation rate.\n- \\( P/Q \\): Relative price of output to capital goods.\n- \\( M, D, c, a \\): Model parameters (markup, constants, labor elasticity).\n\n---\n\n### Data / Model Specification\n\nThe final, empirically operational approximation for the optimal capital-output ratio is:\n\n```latex\n\\hat{k}_{t}=\\frac{(1-a)}{M}\\left(\\frac{1-u}{1-k-u z^{\\prime}}\\right)\\left(\\frac{P}{Q}\\right)\\left(\\frac{D}{r-p+d+c}\\right) \n\\quad \\text{(Eq. (1))}\n```\n\n---\n\nBased on the model for the optimal capital-output ratio (\\(\\hat{k}_t\\)) in Eq. (1), which of the following changes would unambiguously lead to a firm choosing a *more* capital-intensive technology (i.e., a higher \\(\\hat{k}_t\\))?", "Options": {"A": "An increase in the nominal interest rate (\\(r\\)).", "B": "A decrease in the investment tax credit (\\(k\\)).", "C": "An increase in the expected rate of output price inflation (\\(p\\)).", "D": "A decrease in the relative price of output to capital (\\(P/Q\\))."}, "Answer": "C", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5)\nThis item tests the ability to interpret the final equation for the optimal capital-output ratio, a central result of the paper. It uses a Reverse-Reasoning strategy, asking for a cause that produces a specific outcome. The distractors are designed based on the 'Sign Error' principle, where each incorrect option describes a change that would have the opposite effect on capital intensity, making them highly plausible to students who misread the formula.", "qid": "449", "question": "### Background\n\n**Research Question.** In a dynamic general equilibrium setting with a constant mark-up pricing rule and corporate taxes, what determines a firm's optimal capital intensity?\n\n**Setting.** The model considers a firm with a Cobb-Douglas production technology that faces constantly changing input prices and ongoing technological progress. The firm's choice of capital intensity depends on the relative price of capital and the user cost of capital, which is influenced by taxes and inflation expectations.\n\n**Variables & Parameters.**\n- \\( \\hat{k}_t \\): Optimal capital-output ratio at time \\(t\\).\n- \\( u, k, z' \\): Corporate tax rate, investment tax credit rate, and present value of depreciation allowances.\n- \\( r, p, d \\): Nominal interest rate, expected output price inflation, and physical depreciation rate.\n- \\( P/Q \\): Relative price of output to capital goods.\n- \\( M, D, c, a \\): Model parameters (markup, constants, labor elasticity).\n\n---\n\n### Data / Model Specification\n\nThe final, empirically operational approximation for the optimal capital-output ratio is:\n\n```latex\n\\hat{k}_{t}=\\frac{(1-a)}{M}\\left(\\frac{1-u}{1-k-u z^{\\prime}}\\right)\\left(\\frac{P}{Q}\\right)\\left(\\frac{D}{r-p+d+c}\\right) \n\\quad \\text{(Eq. (1))}\n```\n\n---\n\nBased on the model for the optimal capital-output ratio (\\(\\hat{k}_t\\)) in Eq. (1), which of the following changes would unambiguously lead to a firm choosing a *more* capital-intensive technology (i.e., a higher \\(\\hat{k}_t\\))?", "model_evaluation_result": {"model_raw_response": "Looking at the equation for the optimal capital-output ratio:\n\n$$\\hat{k}_{t}=\\frac{(1-a)}{M}\\left(\\frac{1-u}{1-k-u z^{\\prime}}\\right)\\left(\\frac{P}{Q}\\right)\\left(\\frac{D}{r-p+d+c}\\right)$$\n\nI need to analyze how each variable affects $\\hat{k}_t$:\n\nA) An increase in the nominal interest rate ($r$): \n- $r$ appears in the denominator of the last term $(r-p+d+c)$\n- As $r$ increases, $(r-p+d+c)$ increases, making $\\frac{D}{r-p+d+c}$ smaller\n- This decreases $\\hat{k}_t$ → **less** capital-intensive\n\nB) A decrease in the investment tax credit ($k$):\n- $k$ appears in the denominator of the second term $(1-k-uz')$\n- As $k$ decreases, $(1-k-uz')$ increases, making $\\frac{1-u}{1-k-uz'}$ smaller\n- This decreases $\\hat{k}_t$ → **less** capital-intensive\n\nC) An increase in the expected rate of output price inflation ($p$):\n- $p$ appears with a negative sign in the denominator $(r-p+d+c)$\n- As $p$ increases, $(r-p+d+c)$ decreases, making $\\frac{D}{r-p+d+c}$ larger\n- This increases $\\hat{k}_t$ → **more** capital-intensive\n\nD) A decrease in the relative price of output to capital ($P/Q$):\n- $(P/Q)$ appears as a multiplicative term\n- As $(P/Q)$ decreases, $\\hat{k}_t$ decreases → **less** capital-intensive\n\nOnly option C leads to unambiguously higher capital intensity.\n\nC", "model_answer": "D", "error": null}}
{"ID": 443, "Question": "### Background\n\n**Research Question.** This problem provides a complete, worked-out example of a buyer-optimal information structure for the uniform prior case, including a formal proof of its robustness against seller extensions.\n\n**Setting / Institutional Environment.** The buyer's valuation `v` is drawn from a uniform distribution on `[0, 1]`. The regulator's goal is to find an extensionproof information structure that maximizes buyer surplus.\n\n---\n\n### Data / Model Specification\n\nThe proposed buyer-optimal information structure for the uniform case is as follows: if `v > 1/2`, the valuation is perfectly revealed; if `v ≤ 1/2`, valuations are pooled such that the posterior mean is always `1/4`. To check for extensionproofness, we must verify that the seller cannot profit by extending this structure.\n\nThe seller's optimal extension strategy for a new price `q ∈ (1/4, 1/2)` involves sending a `BUY` signal with a specific probability `x(v)` for low valuations `v`:\n```latex\nx(v) := \\frac{1/2 - v - q}{q - v} \\quad \\text{(Eq. 1)}\n```\n\n---\n\nSuppose a buyer's initial signal `s` indicates their valuation is either `v_L = 1/4 - s` or `v_H = 1/4 + s`, each with 50% probability. The seller then extends the information structure to try to persuade the buyer to purchase at a new price `q`, and sends an additional `BUY` signal. What is the buyer's updated posterior expected valuation, `E[v|s, BUY]`?", "Options": {"A": "q", "B": "1/4", "C": "1/4 + s", "D": "(1/4 + s + q)/2"}, "Answer": "A", "pi_justification": "This item tests the understanding of how a strategic information extension (persuasion) works. It uses a Scenario Application strategy, asking the student to apply the logic of Bayesian persuasion to a specific case. The correct answer is the core result of this mechanism: the posterior is moved exactly to the new price. Distractors represent common conceptual errors: (B) ignoring the new information (Step-Omission Error), (C) assuming the new information is perfectly revealing (Conceptual Error), and (D) applying an incorrect heuristic for belief updating (Formula Misuse).", "qid": "443", "question": "### Background\n\n**Research Question.** This problem provides a complete, worked-out example of a buyer-optimal information structure for the uniform prior case, including a formal proof of its robustness against seller extensions.\n\n**Setting / Institutional Environment.** The buyer's valuation `v` is drawn from a uniform distribution on `[0, 1]`. The regulator's goal is to find an extensionproof information structure that maximizes buyer surplus.\n\n---\n\n### Data / Model Specification\n\nThe proposed buyer-optimal information structure for the uniform case is as follows: if `v > 1/2`, the valuation is perfectly revealed; if `v ≤ 1/2`, valuations are pooled such that the posterior mean is always `1/4`. To check for extensionproofness, we must verify that the seller cannot profit by extending this structure.\n\nThe seller's optimal extension strategy for a new price `q ∈ (1/4, 1/2)` involves sending a `BUY` signal with a specific probability `x(v)` for low valuations `v`:\n```latex\nx(v) := \\frac{1/2 - v - q}{q - v} \\quad \\text{(Eq. 1)}\n```\n\n---\n\nSuppose a buyer's initial signal `s` indicates their valuation is either `v_L = 1/4 - s` or `v_H = 1/4 + s`, each with 50% probability. The seller then extends the information structure to try to persuade the buyer to purchase at a new price `q`, and sends an additional `BUY` signal. What is the buyer's updated posterior expected valuation, `E[v|s, BUY]`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to find the buyer's updated posterior expected valuation after receiving both the initial signal `s` and the seller's additional `BUY` signal.\n\nGiven information:\n- Initial signal `s` indicates valuation is either `v_L = 1/4 - s` or `v_H = 1/4 + s`, each with 50% probability\n- Seller extends with a new price `q` and sends a `BUY` signal\n- The seller's optimal extension strategy uses probability `x(v) = (1/2 - v - q)/(q - v)` for sending `BUY` signals\n\nLet me apply Bayes' theorem to find the posterior:\n\nFor `v_L = 1/4 - s`:\n`x(v_L) = (1/2 - (1/4 - s) - q)/(q - (1/4 - s)) = (1/4 + s - q)/(q - 1/4 + s)`\n\nFor `v_H = 1/4 + s`:\n`x(v_H) = (1/2 - (1/4 + s) - q)/(q - (1/4 + s)) = (1/4 - s - q)/(q - 1/4 - s)`\n\nUsing Bayes' theorem:\n`P(v_L | s, BUY) = P(BUY | v_L) × P(v_L | s) / P(BUY | s)`\n\nWhere:\n- `P(v_L | s) = P(v_H | s) = 1/2`\n- `P(BUY | s) = (1/2) × x(v_L) + (1/2) × x(v_H)`\n\nLet me calculate `P(BUY | s)`:\n`P(BUY | s) = (1/2) × [(1/4 + s - q)/(q - 1/4 + s) + (1/4 - s - q)/(q - 1/4 - s)]`\n\nAfter algebraic manipulation (finding common denominators", "model_answer": "Looking at this problem, I need to find the buyer's updated posterior expected valuation after receiving both the initial signal `s` and the seller's additional `BUY` signal.\n\nGiven information:\n- Initial signal `s` indicates valuation is either `v_L = 1/4 - s` or `v_H = 1/4 + s`, each with 50% probability\n- Seller extends with a new price `q` and sends a `BUY` signal\n- The seller's optimal extension strategy uses probability `x(v) = (1/2 - v - q)/(q - v)` for sending `BUY` signals\n\nLet me apply Bayes' theorem to find the posterior:\n\nFor `v_L = 1/4 - s`:\n`x(v_L) = (1/2 - (1/4 - s) - q)/(q - (1/4 - s)) = (1/4 + s - q)/(q - 1/4 + s)`\n\nFor `v_H = 1/4 + s`:\n`x(v_H) = (1/2 - (1/4 + s) - q)/(q - (1/4 + s)) = (1/4 - s - q)/(q - 1/4 - s)`\n\nUsing Bayes' theorem:\n`P(v_L | s, BUY) = P(BUY | v_L) × P(v_L | s) / P(BUY | s)`\n\nWhere:\n- `P(v_L | s) = P(v_H | s) = 1/2`\n- `P(BUY | s) = (1/2) × x(v_L) + (1/2) × x(v_H)`\n\nLet me calculate `P(BUY | s)`:\n`P(BUY | s) = (1/2) × [(1/4 + s - q)/(q - 1/4 + s) + (1/4 - s - q)/(q - 1/4 - s)]`\n\nAfter algebraic manipulation (finding common denominators", "error": "Warning: Could not extract single choice letter from: Looking at this problem, I need to find the buyer'..."}}
{"ID": 510, "Question": "### Background\n\n**Research Question.** This problem addresses a major open question in social choice theory: whether the possibility of constructing a desirable social welfare function on a given preference domain is independent of the number of people in society. This property is crucial for creating general theories of social choice.\n\n**Setting / Institutional Environment.** The analysis takes place in the paper's general model of mixed alternatives, where social outcomes have both public and private components. The core challenge is to relate the existence of an $n$-person Arrow Social Welfare Function (ASWF) to that of an $(n-1)$-person ASWF.\n\n### Data / Model Specification\n\nA common technique used in prior literature for private-goods-only models to prove independence from group size is **'freezing'**. This involves taking an $n$-person ASWF and constructing an $(n-1)$-person ASWF from it. The procedure is to fix the preferences ($r^*_n$) and private alternative ($x^*_n$) of one individual, say person $n$, and then use the original $n$-person ASWF to make decisions for the remaining $n-1$ individuals.\n\nThis technique is claimed to be inapplicable to the general mixed-alternatives case, where a social alternative is of the form $X = (x_0; x_1, ..., x_n)$.\n\n### Question\n\nIn the mixed-alternatives model, why does the 'freezing' technique—fixing the preferences ($r^*_n$) and private alternative ($x^*_n$) of individual $n$—fail to create a valid $(n-1)$-person ASWF?", "Options": {"A": "The 'frozen' individual's utility is not truly fixed; it still varies with the public good ($x_0$) chosen by the other $n-1$ individuals, making their preferences relevant to the outcome.", "B": "The technique violates the Independence of Irrelevant Alternatives (IIA) for the remaining $n-1$ individuals because the frozen preference $r^*_n$ introduces external information.", "C": "The set of available private alternatives for the remaining $n-1$ individuals might depend on the public good $x_0$, which makes the choice set unstable.", "D": "The 'frozen' individual can still manipulate the outcome by strategically choosing their fixed preference $r^*_n$ before the procedure begins."}, "Answer": "A", "pi_justification": "This item assesses the core conceptual reason for the failure of a key proof technique. It uses a Reverse-Reasoning strategy: given that the technique fails, the user must identify the correct cause. Distractor A is the correct answer, identifying the entanglement caused by the public good. Distractor B offers a plausible but incorrect technical failure (violating IIA). Distractor C identifies a secondary issue that is not the primary point of failure. Distractor D misinterprets the proof technique by assigning strategic choice to the 'frozen' individual.", "qid": "510", "question": "### Background\n\n**Research Question.** This problem addresses a major open question in social choice theory: whether the possibility of constructing a desirable social welfare function on a given preference domain is independent of the number of people in society. This property is crucial for creating general theories of social choice.\n\n**Setting / Institutional Environment.** The analysis takes place in the paper's general model of mixed alternatives, where social outcomes have both public and private components. The core challenge is to relate the existence of an $n$-person Arrow Social Welfare Function (ASWF) to that of an $(n-1)$-person ASWF.\n\n### Data / Model Specification\n\nA common technique used in prior literature for private-goods-only models to prove independence from group size is **'freezing'**. This involves taking an $n$-person ASWF and constructing an $(n-1)$-person ASWF from it. The procedure is to fix the preferences ($r^*_n$) and private alternative ($x^*_n$) of one individual, say person $n$, and then use the original $n$-person ASWF to make decisions for the remaining $n-1$ individuals.\n\nThis technique is claimed to be inapplicable to the general mixed-alternatives case, where a social alternative is of the form $X = (x_0; x_1, ..., x_n)$.\n\n### Question\n\nIn the mixed-alternatives model, why does the 'freezing' technique—fixing the preferences ($r^*_n$) and private alternative ($x^*_n$) of individual $n$—fail to create a valid $(n-1)$-person ASWF?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 508, "Question": "### Background\n\n**Research Question.** This study investigates whether exporting to one destination (e.g., a regional common market) generates learning effects that subsequently enhance export performance in other, more demanding markets.\n\n**Setting / Institutional Environment.** The analysis focuses on Costa Rican manufactured exports from 1955-1980. A key institutional feature is Costa Rica's entry into the Central American Common Market (CACM) in 1963, which created a protected regional market.\n\n**Variables & Parameters.**\n- `y_t`, `x_t`: Time series of real export earnings for a specific industry from Costa Rica to two different destinations (e.g., `y_t` to the South, `x_t` to the CACM) at time `t`.\n- `L`: The lag operator, such that `L^k x_t = x_{t-k}`.\n- `β(L)`, `δ(L)`: Polynomials in the lag operator with a lag length of five years. For example, `δ(L)x_t = δ_1 x_{t-1} + ... + δ_5 x_{t-5}`.\n- `z_t`: A vector of control variables, including measures of real GDP and export competitiveness.\n- `e_t`: A random error term.\n\n---\n\n### Data / Model Specification\n\nThe study employs a bivariate Granger-causality framework. To test whether exports to destination `x` (e.g., CACM) Granger-cause exports to destination `y` (e.g., South), the following regression is estimated:\n\n```latex\ny_{t}=\\alpha+\\beta(L)y_{t}+\\delta(L)x_{t}+\\phi z_{t}+e_{t} \\quad \\text{(Eq. 1)}\n```\n\nSuppose that after estimating Eq. (1) for exports to the South (`y_t`) and the CACM (`x_t`), the joint F-test for the coefficients in `δ(L)` yields a p-value of 0.03, and the sum of the coefficients is positive.\n\nWhat is the correct economic interpretation of this statistical finding?", "Options": {"A": "Past export performance in the CACM is a statistically significant predictor of current export performance in the South, which is consistent with the hypothesis that firms use the CACM as a 'stepping-stone' to learn and subsequently enter other markets.", "B": "The result proves that exporting to the CACM causally increases exports to the South, ruling out other potential explanations for the growth in South exports.", "C": "Firms are substituting away from the less profitable South market to focus on the protected CACM market, causing South exports to decline.", "D": "Exports to the CACM and the South are simply correlated because they are both driven by the same underlying factors, such as overall economic growth in Costa Rica."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's ability to correctly translate a statistical result (significant Granger causality) into its corresponding economic interpretation, as intended by the paper. It uses a Scenario Application strategy. The distractors are designed to trap common errors: (B) overstating the claim by confusing Granger causality with true causality (Conceptual Opposite); (C) misinterpreting the sign of the effect (Sign Error); and (D) offering a simple correlation explanation that the Granger test, by conditioning on other factors, is designed to move beyond (True but Irrelevant).", "qid": "508", "question": "### Background\n\n**Research Question.** This study investigates whether exporting to one destination (e.g., a regional common market) generates learning effects that subsequently enhance export performance in other, more demanding markets.\n\n**Setting / Institutional Environment.** The analysis focuses on Costa Rican manufactured exports from 1955-1980. A key institutional feature is Costa Rica's entry into the Central American Common Market (CACM) in 1963, which created a protected regional market.\n\n**Variables & Parameters.**\n- `y_t`, `x_t`: Time series of real export earnings for a specific industry from Costa Rica to two different destinations (e.g., `y_t` to the South, `x_t` to the CACM) at time `t`.\n- `L`: The lag operator, such that `L^k x_t = x_{t-k}`.\n- `β(L)`, `δ(L)`: Polynomials in the lag operator with a lag length of five years. For example, `δ(L)x_t = δ_1 x_{t-1} + ... + δ_5 x_{t-5}`.\n- `z_t`: A vector of control variables, including measures of real GDP and export competitiveness.\n- `e_t`: A random error term.\n\n---\n\n### Data / Model Specification\n\nThe study employs a bivariate Granger-causality framework. To test whether exports to destination `x` (e.g., CACM) Granger-cause exports to destination `y` (e.g., South), the following regression is estimated:\n\n```latex\ny_{t}=\\alpha+\\beta(L)y_{t}+\\delta(L)x_{t}+\\phi z_{t}+e_{t} \\quad \\text{(Eq. 1)}\n```\n\nSuppose that after estimating Eq. (1) for exports to the South (`y_t`) and the CACM (`x_t`), the joint F-test for the coefficients in `δ(L)` yields a p-value of 0.03, and the sum of the coefficients is positive.\n\nWhat is the correct economic interpretation of this statistical finding?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 511, "Question": "### Background\n\n**Research Question.** This problem addresses a major open question in social choice theory: whether the possibility of constructing a desirable social welfare function on a given preference domain is independent of the number of people in society. This property is crucial for creating general theories of social choice.\n\n**Setting / Institutional Environment.** The paper successfully characterizes the preference domains that admit a two-person Arrow Social Welfare Function (ASWF). However, it notes that a full characterization for any number of individuals would require proving the 'independence from group size' property.\n\n### Question\n\nWhat is the primary significance of proving the 'independence from group size' property for a class of preference domains in social choice theory?", "Options": {"A": "It is a necessary precondition for proving that a domain admits a two-person ASWF in the first place.", "B": "It proves that the exact same social welfare function can be used for any group size, ensuring consistency across societies.", "C": "It allows a characterization of domains that admit an ASWF for a small, fixed number of people (e.g., two) to be generalized as a valid characterization for any number of people.", "D": "It demonstrates that the non-dictatorship condition is robust to changes in the number of voters in the society."}, "Answer": "C", "pi_justification": "This item assesses the user's understanding of the methodological role of a key theoretical property. It uses a Conceptual Judgment strategy. Option C is the correct answer, as it correctly identifies the property as a bridge from a specific result (for n=2) to a general one. Distractor A reverses the logical dependency. Distractor B contains a subtle but important error, confusing the existence of *some* ASWF with the existence of the *same* ASWF. Distractor D states a true but secondary consequence, not the primary significance for the research program.", "qid": "511", "question": "### Background\n\n**Research Question.** This problem addresses a major open question in social choice theory: whether the possibility of constructing a desirable social welfare function on a given preference domain is independent of the number of people in society. This property is crucial for creating general theories of social choice.\n\n**Setting / Institutional Environment.** The paper successfully characterizes the preference domains that admit a two-person Arrow Social Welfare Function (ASWF). However, it notes that a full characterization for any number of individuals would require proving the 'independence from group size' property.\n\n### Question\n\nWhat is the primary significance of proving the 'independence from group size' property for a class of preference domains in social choice theory?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 479, "Question": "### Background\n\n**Research Question.** This problem evaluates the finite-sample performance of different Ordinary Least Squares (OLS) covariance matrix estimators. The analysis focuses on the trade-off between using the standard OLS estimator, which is efficient but biased under heteroskedasticity, and various heteroskedasticity-consistent (HC) estimators, which are robust but may be less precise in small samples.\n\n**Setting / Institutional Environment.** The analysis is based on Monte Carlo simulations. The performance of t-statistics derived from four different covariance estimators (OLS, HC1, HC3) is compared under two distinct scenarios: (1) homoskedastic errors, where the OLS assumptions hold, and (2) severe heteroskedasticity, where the OLS assumptions are violated.\n\n**Variables & Parameters.**\n- `Estimator`: The covariance matrix estimator used to construct the t-statistic for the coefficient `β_1`.\n- `S.D.`: The sample standard deviation of the 2000 simulated t-statistics. The theoretical value under the null hypothesis for a N(0,1) distributed statistic is 1.0.\n- `5% Rej. Freq.`: The empirical rejection frequency of a two-sided test with a nominal size of 5%. The theoretical value is 0.05.\n\n---\n\n### Data / Model Specification\n\nThe following table presents a curated summary of simulation results for t-statistics on the coefficient `β_1` with a sample size of `n=50`.\n\n**Table 1: Performance of t-statistics for `β_1` (n=50)**\n\n| Scenario | Estimator | S.D. | 5% Rej. Freq. |\n| :--- | :--- | :--- | :--- |\n| **Case 1: No Heteroskedasticity** | OLS | 1.037 | 0.057 |\n| | HC1 | 1.217 | 0.094 |\n| | HC3 | 1.074 | 0.067 |\n| **Case 5: Severe Heteroskedasticity** | OLS | 2.205 | 0.398 |\n| | HC1 | 1.483 | 0.172 |\n| | HC3 | 1.177 | 0.082 |\n\n*Note: The heteroskedasticity in Case 5 is generated from a random coefficients model where the error variance is strongly correlated with the regressor corresponding to `β_1`.*\n\n---\n\n### Question\n\nBased on the results in Table 1, how does the magnitude of the OLS estimator's failure (its size distortion) in the presence of severe heteroskedasticity compare to the cost of using the HC3 estimator (its size distortion) when it is not needed? Specifically, the size distortion of OLS in Case 5 is approximately how many times larger than the size distortion of HC3 in Case 1?", "Options": {"A": "20.5 times larger", "B": "5.9 times larger", "C": "7.9 times larger", "D": "0.05 times larger"}, "Answer": "A", "pi_justification": "This item replaces the calculation-focused parts of the original QA problem. It uses a 'Computational Judgment' strategy, requiring the user to perform two separate calculations (size distortion for OLS in one case, for HC3 in another) and then compute a ratio. This single question efficiently captures the core quantitative trade-off between the 'cost' and 'benefit' of using robust standard errors. Distractors are designed based on plausible calculation errors: (B) compares raw rejection frequencies instead of size distortions, (C) incorrectly uses the HC1 estimator for the cost calculation, and (D) inverts the comparison, representing a conceptual reversal.", "qid": "479", "question": "### Background\n\n**Research Question.** This problem evaluates the finite-sample performance of different Ordinary Least Squares (OLS) covariance matrix estimators. The analysis focuses on the trade-off between using the standard OLS estimator, which is efficient but biased under heteroskedasticity, and various heteroskedasticity-consistent (HC) estimators, which are robust but may be less precise in small samples.\n\n**Setting / Institutional Environment.** The analysis is based on Monte Carlo simulations. The performance of t-statistics derived from four different covariance estimators (OLS, HC1, HC3) is compared under two distinct scenarios: (1) homoskedastic errors, where the OLS assumptions hold, and (2) severe heteroskedasticity, where the OLS assumptions are violated.\n\n**Variables & Parameters.**\n- `Estimator`: The covariance matrix estimator used to construct the t-statistic for the coefficient `β_1`.\n- `S.D.`: The sample standard deviation of the 2000 simulated t-statistics. The theoretical value under the null hypothesis for a N(0,1) distributed statistic is 1.0.\n- `5% Rej. Freq.`: The empirical rejection frequency of a two-sided test with a nominal size of 5%. The theoretical value is 0.05.\n\n---\n\n### Data / Model Specification\n\nThe following table presents a curated summary of simulation results for t-statistics on the coefficient `β_1` with a sample size of `n=50`.\n\n**Table 1: Performance of t-statistics for `β_1` (n=50)**\n\n| Scenario | Estimator | S.D. | 5% Rej. Freq. |\n| :--- | :--- | :--- | :--- |\n| **Case 1: No Heteroskedasticity** | OLS | 1.037 | 0.057 |\n| | HC1 | 1.217 | 0.094 |\n| | HC3 | 1.074 | 0.067 |\n| **Case 5: Severe Heteroskedasticity** | OLS | 2.205 | 0.398 |\n| | HC1 | 1.483 | 0.172 |\n| | HC3 | 1.177 | 0.082 |\n\n*Note: The heteroskedasticity in Case 5 is generated from a random coefficients model where the error variance is strongly correlated with the regressor corresponding to `β_1`.*\n\n---\n\n### Question\n\nBased on the results in Table 1, how does the magnitude of the OLS estimator's failure (its size distortion) in the presence of severe heteroskedasticity compare to the cost of using the HC3 estimator (its size distortion) when it is not needed? Specifically, the size distortion of OLS in Case 5 is approximately how many times larger than the size distortion of HC3 in Case 1?", "model_evaluation_result": {"model_raw_response": "I need to calculate the size distortions and compare them.\n\nSize distortion is measured as the deviation from the nominal 5% rejection frequency (0.05).\n\nFor OLS in Case 5 (severe heteroskedasticity):\n- Rejection frequency = 0.398\n- Size distortion = 0.398 - 0.05 = 0.348\n\nFor HC3 in Case 1 (no heteroskedasticity):\n- Rejection frequency = 0.067\n- Size distortion = 0.067 - 0.05 = 0.017\n\nTo find how many times larger the OLS distortion is:\n0.348 ÷ 0.017 = 20.47 ≈ 20.5\n\nA", "model_answer": "A", "error": null}}
{"ID": 509, "Question": "### Background\n\n**Research Question.** This study investigates whether exporting to one destination (e.g., a regional common market) generates learning effects that subsequently enhance export performance in other, more demanding markets.\n\n**Setting / Institutional Environment.** The analysis focuses on Costa Rican manufactured exports from 1955-1980. A key institutional feature is Costa Rica's entry into the Central American Common Market (CACM) in 1963, which created a protected regional market.\n\n**Variables & Parameters.**\n- `y_t`, `x_t`: Time series of real export earnings for a specific industry from Costa Rica to two different destinations (e.g., `y_t` to the South, `x_t` to the CACM) at time `t`.\n- `L`: The lag operator, such that `L^k x_t = x_{t-k}`.\n- `β(L)`, `δ(L)`: Polynomials in the lag operator with a lag length of five years. For example, `δ(L)x_t = δ_1 x_{t-1} + ... + δ_5 x_{t-5}`.\n- `z_t`: A vector of control variables, including measures of real GDP and export competitiveness.\n- `e_t`: A random error term.\n\n---\n\n### Data / Model Specification\n\nThe study employs a bivariate Granger-causality framework. To test whether exports to destination `x` (e.g., CACM) Granger-cause exports to destination `y` (e.g., South), the following regression is estimated:\n\n```latex\ny_{t}=\\alpha+\\beta(L)y_{t}+\\delta(L)x_{t}+\\phi z_{t}+e_{t} \\quad \\text{(Eq. 1)}\n```\n\nA critic argues that the test may be misleading. They propose a scenario where an unobserved, industry-specific technological innovation is adopted first by firms to enter the less-demanding CACM market (at times `t-1`, `t-2`, etc.), and the same innovation later allows them to meet the quality standards for other Southern markets (at time `t`). This technological factor is not included in the control vector `z_t`.\n\nIn this scenario, finding that CACM exports Granger-cause South exports would be a spurious result driven by a violation of which key assumption?", "Options": {"A": "Strict exogeneity, because the regressor (lagged CACM exports) is correlated with the error term via the unobserved technology shock.", "B": "The assumption of no perfect multicollinearity between the lagged `x_t` and `y_t` variables.", "C": "The assumption that the error term `e_t` is homoskedastic and not serially correlated.", "D": "The assumption that the model's lag length of five years is correctly specified to capture the full dynamics of the relationship."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item tests a deeper understanding of the econometric methodology by asking students to identify the specific assumption violated by a classic omitted variable bias scenario. It uses a Reverse-Reasoning strategy. The distractors represent common points of confusion in econometrics: (B) multicollinearity is about relationships between regressors, not regressors and the error term (Conceptual: Almost Right); (C) while serial correlation in the error might also be present, the fundamental violation causing the spurious causality is the endogeneity problem described in A (Conceptual: Almost Right); (D) lag length specification is a different type of model error and not the one described in the scenario (Conceptual: True but Irrelevant).", "qid": "509", "question": "### Background\n\n**Research Question.** This study investigates whether exporting to one destination (e.g., a regional common market) generates learning effects that subsequently enhance export performance in other, more demanding markets.\n\n**Setting / Institutional Environment.** The analysis focuses on Costa Rican manufactured exports from 1955-1980. A key institutional feature is Costa Rica's entry into the Central American Common Market (CACM) in 1963, which created a protected regional market.\n\n**Variables & Parameters.**\n- `y_t`, `x_t`: Time series of real export earnings for a specific industry from Costa Rica to two different destinations (e.g., `y_t` to the South, `x_t` to the CACM) at time `t`.\n- `L`: The lag operator, such that `L^k x_t = x_{t-k}`.\n- `β(L)`, `δ(L)`: Polynomials in the lag operator with a lag length of five years. For example, `δ(L)x_t = δ_1 x_{t-1} + ... + δ_5 x_{t-5}`.\n- `z_t`: A vector of control variables, including measures of real GDP and export competitiveness.\n- `e_t`: A random error term.\n\n---\n\n### Data / Model Specification\n\nThe study employs a bivariate Granger-causality framework. To test whether exports to destination `x` (e.g., CACM) Granger-cause exports to destination `y` (e.g., South), the following regression is estimated:\n\n```latex\ny_{t}=\\alpha+\\beta(L)y_{t}+\\delta(L)x_{t}+\\phi z_{t}+e_{t} \\quad \\text{(Eq. 1)}\n```\n\nA critic argues that the test may be misleading. They propose a scenario where an unobserved, industry-specific technological innovation is adopted first by firms to enter the less-demanding CACM market (at times `t-1`, `t-2`, etc.), and the same innovation later allows them to meet the quality standards for other Southern markets (at time `t`). This technological factor is not included in the control vector `z_t`.\n\nIn this scenario, finding that CACM exports Granger-cause South exports would be a spurious result driven by a violation of which key assumption?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 480, "Question": "### Background\n\nSimulation studies show that when OLS assumptions are violated by severe heteroskedasticity, the standard OLS t-test can have a true rejection frequency of nearly 40% for a nominal 5% test. In contrast, the HC3 robust t-test has a rejection frequency of around 8%. When there is no heteroskedasticity, the OLS t-test performs almost perfectly (5.7% rejection rate), while the HC3 t-test over-rejects slightly (6.7% rejection rate). The paper also notes that statistical tests for heteroskedasticity often have low power, meaning they frequently fail to detect the problem even when it is severe.\n\n---\n\n### Question\n\nGiven this evidence, what is the most prudent and methodologically defensible strategy for an applied researcher conducting inference?", "Options": {"A": "Adopt a 'pre-testing' approach: first test for heteroskedasticity, and if the test is not significant, use the more efficient OLS estimator.", "B": "Default to using the HC3 estimator for all inference, accepting its minor performance cost in the homoskedastic case as 'insurance' against catastrophic failure in the heteroskedastic case.", "C": "Default to using the OLS estimator, as its performance is optimal under the null and the risk of heteroskedasticity is often overstated.", "D": "Default to using the HC1 estimator, as it is robust to heteroskedasticity and is computationally simpler than the HC3 estimator."}, "Answer": "B", "pi_justification": "This item assesses the strategic conclusion of the original QA problem using a 'Scenario Application' strategy. It frames the choice of estimator as a risk-management problem. The correct option (B) articulates the paper's core 'minimax' argument. Distractors are designed to target specific fallacies: (A) represents the classic 'pre-testing fallacy' which the paper explicitly warns against, (C) ignores the catastrophic failure of OLS documented in the evidence, and (D) selects a sub-optimal robust estimator (HC1), which is a 'true but irrelevant' distractor that ignores the clear performance superiority of HC3 shown in the paper's results.", "qid": "480", "question": "### Background\n\nSimulation studies show that when OLS assumptions are violated by severe heteroskedasticity, the standard OLS t-test can have a true rejection frequency of nearly 40% for a nominal 5% test. In contrast, the HC3 robust t-test has a rejection frequency of around 8%. When there is no heteroskedasticity, the OLS t-test performs almost perfectly (5.7% rejection rate), while the HC3 t-test over-rejects slightly (6.7% rejection rate). The paper also notes that statistical tests for heteroskedasticity often have low power, meaning they frequently fail to detect the problem even when it is severe.\n\n---\n\n### Question\n\nGiven this evidence, what is the most prudent and methodologically defensible strategy for an applied researcher conducting inference?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 433, "Question": "### Background\n\n**Research Question.** This problem explores the core identification challenge of empirically distinguishing between two competing hypotheses for consumption inequality: that richer households are more patient (wealth-varying Rate of Time Preference, RTP) versus that they are more willing to substitute consumption over time (wealth-varying Intertemporal Elasticity of Substitution, IES).\n\n**Setting / Institutional Environment.** The analysis is framed within a complete markets model with heterogeneous agents. The key to the identification strategy is the use of panel data where the underlying intertemporal price of consumption fluctuates, experiencing periods of both growth and decline.\n\n**Variables & Parameters.**\n- `$\\hat{C}^{h}(t)$`: Log consumption growth for household `h` at time `t`.\n- `$\\delta_{h}$`: The household-specific Rate of Time Preference (RTP).\n- `$\\sigma_{h}(t)$`: The household-specific Intertemporal Elasticity of Substitution (IES).\n- `$r(t)$`: The log real interest rate, or intertemporal price of consumption.\n\n---\n\n### Data / Model Specification\n\nThe approximate relationship between individual consumption growth, preferences, and prices is given by:\n```latex\n\\hat{C}^{h}(t) \\cong \\sigma_{h}(t) \\{r(t) - \\delta_{h}\\} \\quad \\text{(Eq. 1)}\n```\n\n---\n\n### The Question\n\nThe authors argue that their data, which includes periods of both aggregate consumption growth and shrinkage, allows them to distinguish between the two competing models. What is the distinct, testable prediction made by the wealth-varying IES model that is NOT made by the wealth-varying RTP model?", "Options": {"A": "Richer households have a higher average consumption growth rate across all periods.", "B": "Richer households have more volatile consumption growth, growing faster in good times and shrinking faster in bad times.", "C": "Richer households are more patient, which is directly measured by their savings.", "D": "The consumption growth of richer households is less sensitive to interest rate changes."}, "Answer": "B", "pi_justification": "This item assesses understanding of the paper's central identification strategy. It uses a Reverse-Reasoning strategy (what prediction allows identification?). The distractors include a non-unique prediction (A), an irrelevant statement (C), and a direct contradiction of the theory (D), making for a sharp assessment.", "qid": "433", "question": "### Background\n\n**Research Question.** This problem explores the core identification challenge of empirically distinguishing between two competing hypotheses for consumption inequality: that richer households are more patient (wealth-varying Rate of Time Preference, RTP) versus that they are more willing to substitute consumption over time (wealth-varying Intertemporal Elasticity of Substitution, IES).\n\n**Setting / Institutional Environment.** The analysis is framed within a complete markets model with heterogeneous agents. The key to the identification strategy is the use of panel data where the underlying intertemporal price of consumption fluctuates, experiencing periods of both growth and decline.\n\n**Variables & Parameters.**\n- `$\\hat{C}^{h}(t)$`: Log consumption growth for household `h` at time `t`.\n- `$\\delta_{h}$`: The household-specific Rate of Time Preference (RTP).\n- `$\\sigma_{h}(t)$`: The household-specific Intertemporal Elasticity of Substitution (IES).\n- `$r(t)$`: The log real interest rate, or intertemporal price of consumption.\n\n---\n\n### Data / Model Specification\n\nThe approximate relationship between individual consumption growth, preferences, and prices is given by:\n```latex\n\\hat{C}^{h}(t) \\cong \\sigma_{h}(t) \\{r(t) - \\delta_{h}\\} \\quad \\text{(Eq. 1)}\n```\n\n---\n\n### The Question\n\nThe authors argue that their data, which includes periods of both aggregate consumption growth and shrinkage, allows them to distinguish between the two competing models. What is the distinct, testable prediction made by the wealth-varying IES model that is NOT made by the wealth-varying RTP model?", "model_evaluation_result": {"model_raw_response": "Looking at equation (1): $\\hat{C}^{h}(t) \\cong \\sigma_{h}(t) \\{r(t) - \\delta_{h}\\}$\n\nLet me analyze what each model predicts:\n\n**Wealth-varying RTP model**: Richer households have lower $\\delta_h$ (more patient), but $\\sigma_h$ is constant across wealth levels.\n\n**Wealth-varying IES model**: Richer households have higher $\\sigma_h$ (more willing to substitute), but $\\delta_h$ is constant across wealth levels.\n\nNow examining each option:\n\nA) Both models could predict higher average consumption growth for richer households - this doesn't distinguish them.\n\nB) Under wealth-varying IES: Higher $\\sigma_h$ for rich households means their consumption responds more strongly to $(r(t) - \\delta_h)$. When $r(t) > \\delta_h$ (good times), they grow faster. When $r(t) < \\delta_h$ (bad times), they shrink more. This creates higher volatility.\n\nUnder wealth-varying RTP: Lower $\\delta_h$ for rich households affects the level of $(r(t) - \\delta_h)$ but doesn't necessarily create more volatility in response to interest rate fluctuations.\n\nC) This describes the RTP model assumption, not a testable prediction that distinguishes the models.\n\nD) This is backwards - the IES model predicts richer households are MORE sensitive to interest rate changes (higher $\\sigma_h$).\n\nThe key insight is that with panel data covering periods of both consumption growth and decline, the wealth-varying IES model uniquely predicts that richer households will have more volatile consumption - amplifying both positive and negative responses to interest rate changes.\n\nB", "model_answer": "C", "error": null}}
{"ID": 500, "Question": "### Background\n\n**Research Question.** This problem analyzes the sources of the earnings gap between relative payment and piece-rate contracts by moving from a simple regression estimate to a formal decomposition of the gap into a portion explained by worker characteristics and a portion explained by the contractual wage structure itself.\n\n**Setting and Sample.** The analysis uses a sample of 105 shrimp gatherers in Honduras. The dependent variable is the natural logarithm of daily larva income. The total observed difference in mean log wages between the two groups is 0.346.\n\n### Data / Model Specification\n\nAn initial Ordinary Least Squares (OLS) model is estimated on the pooled sample of all workers, with results in Table 1.\n\n**Table 1: OLS Wage Differentials by Contract**\n\n| Variable | Pooled Coefficient (t-stat) |\n| :--- | :---: |\n| Constant | 3.09 (23.04) |\n| Literacy | 0.13 (2.29) |\n| Larva experience | 0.05 (3.27) |\n| Gatherer age | 0.002 (-0.84) |\n| Village B | -0.35 (-3.65) |\n| Village C | 0.15 (-2.14) |\n| **Relative payment** | **0.18 (1.92)** |\n| Sample size | 105 |\n| Adjusted R2 | 0.43 |\n\nTo further investigate the wage gap, an Oaxaca-Blinder decomposition is performed. This method partitions the total gap into a component due to differences in mean characteristics (the 'explained' part) and a component due to differences in the returns to those characteristics (the 'unexplained' part, or wage structure). The results are in Table 2.\n\n**Table 2: Decomposition of Relative Payment – Piece-rate Log Wage Differential**\n(Total Difference in Mean Log Wages = 0.346)\n\n| | Evaluated at piece-rate means | Evaluated at relative payment means | Cotton method |\n| :--- | :---: | :---: | :---: |\n| **Difference due to:** | | |\n| Wage structure | 82% (0.28) | 39% (0.13) | 60% (0.21) |\n| Gatherer characteristics | 18% (0.06) | 61% (0.21) | 40% (0.14) |\n\n*Note: Numbers in parentheses are log point differences.* \n\n### Question\n\nThe 'Evaluated at piece-rate means' column in Table 2 provides a counterfactual analysis. Using the 'Wage structure' component from this column (0.28 log points), what would be the expected percentage increase in the daily wage for an average piece-rate worker if they were paid according to the relative payment wage structure?", "Options": {"A": "28.0%", "B": "6.2%", "C": "32.3%", "D": "41.3%"}, "Answer": "C", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item uses the 'Computational Judgment' strategy to test a key quantitative finding from the paper. The calculation is straightforward but requires knowing the correct formula for converting log points to percentages, a common point of error. The distractors are based on predictable errors: A (Sign Error/Formula Misuse) treats the 0.28 log points directly as a percentage; B (Step-Omission Error) uses the wrong value from the table (the 'characteristics' component, 0.06); D (Formula Misuse) uses the total wage gap (0.346) instead of the specific counterfactual component.", "qid": "500", "question": "### Background\n\n**Research Question.** This problem analyzes the sources of the earnings gap between relative payment and piece-rate contracts by moving from a simple regression estimate to a formal decomposition of the gap into a portion explained by worker characteristics and a portion explained by the contractual wage structure itself.\n\n**Setting and Sample.** The analysis uses a sample of 105 shrimp gatherers in Honduras. The dependent variable is the natural logarithm of daily larva income. The total observed difference in mean log wages between the two groups is 0.346.\n\n### Data / Model Specification\n\nAn initial Ordinary Least Squares (OLS) model is estimated on the pooled sample of all workers, with results in Table 1.\n\n**Table 1: OLS Wage Differentials by Contract**\n\n| Variable | Pooled Coefficient (t-stat) |\n| :--- | :---: |\n| Constant | 3.09 (23.04) |\n| Literacy | 0.13 (2.29) |\n| Larva experience | 0.05 (3.27) |\n| Gatherer age | 0.002 (-0.84) |\n| Village B | -0.35 (-3.65) |\n| Village C | 0.15 (-2.14) |\n| **Relative payment** | **0.18 (1.92)** |\n| Sample size | 105 |\n| Adjusted R2 | 0.43 |\n\nTo further investigate the wage gap, an Oaxaca-Blinder decomposition is performed. This method partitions the total gap into a component due to differences in mean characteristics (the 'explained' part) and a component due to differences in the returns to those characteristics (the 'unexplained' part, or wage structure). The results are in Table 2.\n\n**Table 2: Decomposition of Relative Payment – Piece-rate Log Wage Differential**\n(Total Difference in Mean Log Wages = 0.346)\n\n| | Evaluated at piece-rate means | Evaluated at relative payment means | Cotton method |\n| :--- | :---: | :---: | :---: |\n| **Difference due to:** | | |\n| Wage structure | 82% (0.28) | 39% (0.13) | 60% (0.21) |\n| Gatherer characteristics | 18% (0.06) | 61% (0.21) | 40% (0.14) |\n\n*Note: Numbers in parentheses are log point differences.* \n\n### Question\n\nThe 'Evaluated at piece-rate means' column in Table 2 provides a counterfactual analysis. Using the 'Wage structure' component from this column (0.28 log points), what would be the expected percentage increase in the daily wage for an average piece-rate worker if they were paid according to the relative payment wage structure?", "model_evaluation_result": {"model_raw_response": "I need to calculate the percentage increase in daily wage when moving from a log difference of 0.28.\n\nWhen we have a log difference of 0.28, the percentage change is calculated as:\nPercentage change = (e^0.28 - 1) × 100%\n\nLet me calculate this:\ne^0.28 ≈ 1.323\n\nTherefore:\nPercentage change = (1.323 - 1) × 100% = 0.323 × 100% = 32.3%\n\nThis means that if an average piece-rate worker were paid according to the relative payment wage structure, their daily wage would increase by approximately 32.3%.\n\nC", "model_answer": "C", "error": null}}
{"ID": 414, "Question": "### Background\n\nIn the Dynamic Double-Track (DDT) procedure, an auctioneer iteratively adjusts prices `p(t)` to find a Walrasian equilibrium. At each step, the auctioneer must choose a price adjustment `δ` that maximally decreases the Lyapunov function `L(p)`, a measure of total market surplus.\n\n### Data / Model Specification\n\nAt each step `t`, the auctioneer solves the following optimization problem to determine the next price adjustment `δ(t)`, using only the demand sets `Dⁱ(p(t))` reported by each buyer `i`:\n```latex\n\\operatorname*{max}_{\\delta\\in\\Delta}\\left\\{\\sum_{i\\in I}\\left(\\operatorname*{min}_{S\\in D^{i}(p(t))}\\sum_{\\beta_{h}\\in S}\\delta_{h}\\right) - \\sum_{\\beta_{h}\\in N}\\delta_{h}\\right\\} \\quad \\text{(Eq. 1)}\n```\nwhere `Δ` is the set of allowed integer price adjustments.\n\n### Question\n\nWhat is the primary economic function of the calculation in Eq. (1)?", "Options": {"A": "It allows the auctioneer to calculate the change in total market surplus for any potential price adjustment `δ` using only publicly reported demand sets.", "B": "It forces buyers to reveal their single most-preferred bundle, resolving any indifference and simplifying the market clearing problem.", "C": "It directly calculates the revenue-maximizing price adjustment for the seller.", "D": "It requires the auctioneer to know each buyer's private utility function `uⁱ` to evaluate the inner `min` term."}, "Answer": "A", "pi_justification": "This item assesses the interpretation of the core mathematical formula of the auction mechanism. The distractors target common misinterpretations: B focuses on a secondary effect rather than the primary goal, C misidentifies the objective function (surplus vs. revenue), and D states the exact opposite of a key feature of the mechanism (its informational efficiency).", "qid": "414", "question": "### Background\n\nIn the Dynamic Double-Track (DDT) procedure, an auctioneer iteratively adjusts prices `p(t)` to find a Walrasian equilibrium. At each step, the auctioneer must choose a price adjustment `δ` that maximally decreases the Lyapunov function `L(p)`, a measure of total market surplus.\n\n### Data / Model Specification\n\nAt each step `t`, the auctioneer solves the following optimization problem to determine the next price adjustment `δ(t)`, using only the demand sets `Dⁱ(p(t))` reported by each buyer `i`:\n```latex\n\\operatorname*{max}_{\\delta\\in\\Delta}\\left\\{\\sum_{i\\in I}\\left(\\operatorname*{min}_{S\\in D^{i}(p(t))}\\sum_{\\beta_{h}\\in S}\\delta_{h}\\right) - \\sum_{\\beta_{h}\\in N}\\delta_{h}\\right\\} \\quad \\text{(Eq. 1)}\n```\nwhere `Δ` is the set of allowed integer price adjustments.\n\n### Question\n\nWhat is the primary economic function of the calculation in Eq. (1)?", "model_evaluation_result": {"model_raw_response": "Looking at Equation (1), I need to understand what this optimization problem accomplishes in the DDT procedure context.\n\nThe equation has two main components:\n1. The sum over buyers i of the minimum over their demand sets of the dot product of price adjustments with bundles\n2. Minus the sum of all price adjustments\n\nLet me analyze each option:\n\nA) This suggests the calculation measures changes in total market surplus using only demand sets. In auction theory, the Lyapunov function L(p) typically measures market disequilibrium or total surplus. The DDT procedure aims to find Walrasian equilibrium by maximally decreasing this function. The first term captures how price changes affect buyer surplus (via their reported demand sets), while the second term relates to seller/market effects. This uses only publicly reported information.\n\nB) This claims it forces revelation of single most-preferred bundles. However, the equation works with entire demand sets D^i(p(t)), not single bundles, and the min operation doesn't force single bundle revelation.\n\nC) This suggests it's about revenue maximization for the seller. But the structure doesn't align with revenue maximization - it's about finding equilibrium prices that clear markets, not maximizing seller revenue.\n\nD) This claims private utility functions are needed. However, the equation only uses demand sets D^i(p(t)), which are publicly reported. The auctioneer doesn't need to know private utilities u^i to compute this.\n\nThe key insight is that this optimization problem allows the auctioneer to determine which price adjustment will most effectively move toward equilibrium (decrease the Lyapunov function) using only the demand correspondence information that buyers report, without needing private utility information.\n\nA", "model_answer": "A", "error": null}}
{"ID": 440, "Question": "### Background\n\nThis problem examines the theoretical structure of the incremental Expected Utility (EU) advantage model, a probabilistic theory of choice under risk. The model uses a binary choice probability function, $P(p,q)$, representing the probability that an individual chooses gamble $p$ over gamble $q$.\n\n### Data / Model Specification\n\nLet $x_1 > x_2 > \\cdots > x_n$ be the ordered outcomes for gambles $p$ or $q$. Let $\\Delta_k(p,q) = p\\{\\geq x_k\\} - q\\{\\geq x_k\\}$ be the difference in cumulative probabilities.\n\nThe total incremental expected utility advantage of $p$ over $q$ is:\n\n```latex\n\\mathcal{A}(p,q) = \\sum_{\\{k: \\Delta_k(p,q)>0\\}} \\Delta_k(p,q)[u(x_k) - u(x_{k+1})]\n```\n\nThe difference in standard expected utilities relates to these advantages as:\n\n```latex\nE(u,p) - E(u,q) = \\mathcal{A}(p,q) - \\mathcal{A}(q,p)\n```\n**Eq. (1)**\n\nThe probabilistic choice rule is given by:\n\n```latex\n\\frac{P(p,q)}{P(q,p)} = \\rho\\left(\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)}\\right)\n```\n**Eq. (2)**\n\nwhere $\\rho(\\cdot)$ is an increasing function with $\\rho(1)=1$.\n\n### Question\n\nThe incremental EU advantage model is consistent with standard EU theory, meaning $P(p,q) > 1/2$ if and only if $E(u,p) > E(u,q)$. The proof of this consistency relies on a chain of equivalences. Which property of the function $\\rho(\\cdot)$ is essential for ensuring that the odds ratio $\\frac{P(p,q)}{P(q,p)} > 1$ is equivalent to the advantage ratio $\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)} > 1$?", "Options": {"A": "$\\rho$ is strictly increasing and $\\rho(1)=1$", "B": "$\\rho(0)=0$", "C": "$\\rho(z)\\rho(1/z)=1$", "D": "$\\rho$ is continuous"}, "Answer": "A", "pi_justification": "This item assesses understanding of the logical bridge between the probabilistic choice rule and standard EU preference ordering. It uses a Reverse-Reasoning strategy, asking the student to identify the critical assumption (properties of rho) that makes the derivation work. Distractors are designed based on other true-but-irrelevant properties of rho (B, C) or a property explicitly stated as not required (D), testing for precise comprehension of the model's structure.", "qid": "440", "question": "### Background\n\nThis problem examines the theoretical structure of the incremental Expected Utility (EU) advantage model, a probabilistic theory of choice under risk. The model uses a binary choice probability function, $P(p,q)$, representing the probability that an individual chooses gamble $p$ over gamble $q$.\n\n### Data / Model Specification\n\nLet $x_1 > x_2 > \\cdots > x_n$ be the ordered outcomes for gambles $p$ or $q$. Let $\\Delta_k(p,q) = p\\{\\geq x_k\\} - q\\{\\geq x_k\\}$ be the difference in cumulative probabilities.\n\nThe total incremental expected utility advantage of $p$ over $q$ is:\n\n```latex\n\\mathcal{A}(p,q) = \\sum_{\\{k: \\Delta_k(p,q)>0\\}} \\Delta_k(p,q)[u(x_k) - u(x_{k+1})]\n```\n\nThe difference in standard expected utilities relates to these advantages as:\n\n```latex\nE(u,p) - E(u,q) = \\mathcal{A}(p,q) - \\mathcal{A}(q,p)\n```\n**Eq. (1)**\n\nThe probabilistic choice rule is given by:\n\n```latex\n\\frac{P(p,q)}{P(q,p)} = \\rho\\left(\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)}\\right)\n```\n**Eq. (2)**\n\nwhere $\\rho(\\cdot)$ is an increasing function with $\\rho(1)=1$.\n\n### Question\n\nThe incremental EU advantage model is consistent with standard EU theory, meaning $P(p,q) > 1/2$ if and only if $E(u,p) > E(u,q)$. The proof of this consistency relies on a chain of equivalences. Which property of the function $\\rho(\\cdot)$ is essential for ensuring that the odds ratio $\\frac{P(p,q)}{P(q,p)} > 1$ is equivalent to the advantage ratio $\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)} > 1$?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine which property of ρ(·) ensures that the odds ratio being greater than 1 is equivalent to the advantage ratio being greater than 1.\n\nFrom Equation (2):\n$$\\frac{P(p,q)}{P(q,p)} = \\rho\\left(\\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)}\\right)$$\n\nFor the model to be consistent with EU theory, I need:\n$$\\frac{P(p,q)}{P(q,p)} > 1 \\iff \\frac{\\mathcal{A}(p,q)}{\\mathcal{A}(q,p)} > 1$$\n\nLet me examine each option:\n\n**Option A**: ρ is strictly increasing and ρ(1)=1\n- This ensures that ρ(z) > ρ(1) = 1 when z > 1\n- This gives us the equivalence we need\n\n**Option B**: ρ(0)=0\n- This is about behavior at zero, not relevant for the equivalence around 1\n\n**Option C**: ρ(z)ρ(1/z)=1\n- This is a reciprocal property. If we swap p and q in Equation (2):\n$$\\frac{P(q,p)}{P(p,q)} = \\rho\\left(\\frac{\\mathcal{A}(q,p)}{\\mathcal{A}(p,q)}\\right) = \\rho\\left(\\frac{1}{\\mathcal{A}(p,q)/\\mathcal{A}(q,p)}\\right)$$\n\n- For consistency, we need: $\\frac{P(q,p)}{P(p,q)} = \\frac{1}{P(p,q)/P(q,p)}$\n- This means: $\\rho\\left(\\frac{1}{z}\\right) = \\frac{1}{\\rho(z)}$, which is exactly ρ(z)ρ(1/z)=1\n\n**Option", "model_answer": "A", "error": null}}
{"ID": 461, "Question": "### Background\n\n**Research Question.** This problem concerns the necessary statistical pre-testing required to validate the use of an error-correction model (ECM) for analyzing long-run relationships between non-stationary time series, a cornerstone of the paper's empirical strategy.\n\n**Setting.** The study uses quarterly time-series data from 1965-1986. Before estimating the main behavioral models, the authors conduct Augmented Dickey-Fuller (ADF) tests to determine the order of integration of the variables and to test for cointegration among them. The validity of the entire empirical exercise rests on these preliminary findings.\n\n### Data / Model Specification\n\nThe paper's primary empirical specification is an error-correction model (ECM), which is only valid if the variables are integrated of order one, I(1), and are cointegrated. Table 1 below presents a selection of results from the Augmented Dickey-Fuller (ADF) tests used to verify these conditions. An asterisk (*) denotes rejection of the null hypothesis at the 5% level (critical value -3.77).\n\n**Table 1: Augmented Dickey-Fuller Unit Root & Cointegration Tests**\n| Test Specification (Dependent Variable is Real Price) | ln(I33/IPP) | ln(IAF/IPP) | ln(IMM/IPP) |\n| :--- | :--- | :--- | :--- |\n| 1. Intercept only (Unit Root Test) | -1.59 | -2.08 | -0.97 |\n| 2. Cointegration Test with ln(EXR/RPP) | -3.13 | -4.10* | -1.29 |\n| 3. Cointegration Test with ln(EXR/RPP), ln(SERV/Ix) | -4.84* | -4.07* | -3.59* |\n\n### Question\n\nBased on the 'Intercept only' tests reported in Row 1 of Table 1, what is the correct conclusion regarding the real commodity price series?", "Options": {"A": "The series are stationary (I(0)) because the test statistics are negative.", "B": "The null hypothesis of stationarity is rejected for all three series.", "C": "The null hypothesis of a unit root cannot be rejected, indicating the series are likely non-stationary (I(1)).", "D": "The series are cointegrated with a constant term."}, "Answer": "C", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item replaces Part 1 of the original QA. It assesses the fundamental skill of interpreting statistical test results from a table. The answer is atomic and the potential for creating high-fidelity distractors is very high. Strategy: Direct Interpretation. Distractor Logic: (A) is a Conceptual Opposite, confusing the sign with significance. (B) is a Conceptual Error, misstating the null hypothesis of the ADF test. (D) is True but Irrelevant, confusing a unit root test with a cointegration test.", "qid": "461", "question": "### Background\n\n**Research Question.** This problem concerns the necessary statistical pre-testing required to validate the use of an error-correction model (ECM) for analyzing long-run relationships between non-stationary time series, a cornerstone of the paper's empirical strategy.\n\n**Setting.** The study uses quarterly time-series data from 1965-1986. Before estimating the main behavioral models, the authors conduct Augmented Dickey-Fuller (ADF) tests to determine the order of integration of the variables and to test for cointegration among them. The validity of the entire empirical exercise rests on these preliminary findings.\n\n### Data / Model Specification\n\nThe paper's primary empirical specification is an error-correction model (ECM), which is only valid if the variables are integrated of order one, I(1), and are cointegrated. Table 1 below presents a selection of results from the Augmented Dickey-Fuller (ADF) tests used to verify these conditions. An asterisk (*) denotes rejection of the null hypothesis at the 5% level (critical value -3.77).\n\n**Table 1: Augmented Dickey-Fuller Unit Root & Cointegration Tests**\n| Test Specification (Dependent Variable is Real Price) | ln(I33/IPP) | ln(IAF/IPP) | ln(IMM/IPP) |\n| :--- | :--- | :--- | :--- |\n| 1. Intercept only (Unit Root Test) | -1.59 | -2.08 | -0.97 |\n| 2. Cointegration Test with ln(EXR/RPP) | -3.13 | -4.10* | -1.29 |\n| 3. Cointegration Test with ln(EXR/RPP), ln(SERV/Ix) | -4.84* | -4.07* | -3.59* |\n\n### Question\n\nBased on the 'Intercept only' tests reported in Row 1 of Table 1, what is the correct conclusion regarding the real commodity price series?", "model_evaluation_result": {"model_raw_response": "Looking at Row 1 of Table 1, I need to interpret the Augmented Dickey-Fuller (ADF) unit root test results.\n\nThe ADF test has a null hypothesis of a unit root (non-stationarity). The test statistics are:\n- ln(I33/IPP): -1.59\n- ln(IAF/IPP): -2.08  \n- ln(IMM/IPP): -0.97\n\nThe critical value at 5% significance level is -3.77 (as stated in the problem).\n\nFor the ADF test:\n- To reject the null hypothesis (unit root), the test statistic must be more negative than the critical value\n- All three test statistics (-1.59, -2.08, -0.97) are less negative than -3.77\n- Therefore, we cannot reject the null hypothesis of a unit root for any of the series\n\nThis means the series are likely non-stationary and integrated of order one, I(1).\n\nLet me evaluate the options:\n\nA) Incorrect - negative test statistics don't automatically mean stationarity; they must exceed the critical value in magnitude.\n\nB) Incorrect - the null hypothesis is unit root (non-stationarity), not stationarity.\n\nC) Correct - the null hypothesis of a unit root cannot be rejected since none of the test statistics exceed the critical value, indicating the series are likely I(1).\n\nD) Incorrect - Row 1 shows unit root tests, not cointegration tests.\n\nC", "model_answer": "A", "error": null}}
{"ID": 486, "Question": "### Background\n\nIn 2002, two North Carolina districts, Charlotte-Mecklenburg (CMS) and Guilford, abruptly increased eighth-grade Algebra I enrollment. The study uses this policy shift as a natural experiment. To estimate the policy's impact, the authors first use a difference-in-differences (DiD) model, which compares the change in outcomes over time in the 'treated' districts (CMS and Guilford) to the change in outcomes in a set of 'control' districts where enrollment rates remained stable.\n\n---\n\n### The Question\n\nWhy was it crucial to include \"control\" districts in the difference-in-differences (DiD) analysis, rather than just comparing outcomes in CMS and Guilford before and after their policy changes?", "Options": {"A": "To provide an estimate of the counterfactual trend that would have occurred in the treated districts had the policy not been implemented, based on the parallel trends assumption.", "B": "To increase the overall sample size of the study, thereby improving the statistical precision of the estimates.", "C": "To ensure that the student populations in the treated and control districts were identical in terms of prior achievement and demographics before the policy change.", "D": "To directly measure the spillover effects of the algebra acceleration policy on neighboring school districts."}, "Answer": "A", "pi_justification": "This item assesses understanding of the fundamental purpose of a control group in a difference-in-differences design. It uses a Reverse-Reasoning strategy by asking for the 'why' behind a specific methodological choice. Distractor B is 'True but Irrelevant'—while a larger sample is good, it's not the primary reason for the DiD design. Distractor C describes the goal of a randomized controlled trial, not DiD, and confuses the 'parallel trends' assumption with an assumption of identical levels. Distractor D describes a different research question entirely.", "qid": "486", "question": "### Background\n\nIn 2002, two North Carolina districts, Charlotte-Mecklenburg (CMS) and Guilford, abruptly increased eighth-grade Algebra I enrollment. The study uses this policy shift as a natural experiment. To estimate the policy's impact, the authors first use a difference-in-differences (DiD) model, which compares the change in outcomes over time in the 'treated' districts (CMS and Guilford) to the change in outcomes in a set of 'control' districts where enrollment rates remained stable.\n\n---\n\n### The Question\n\nWhy was it crucial to include \"control\" districts in the difference-in-differences (DiD) analysis, rather than just comparing outcomes in CMS and Guilford before and after their policy changes?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 466, "Question": "### Background\n\n**Research Question.** What are the practical implications of the `GDPe`/`GDPo` distinction for applied empirical research, particularly for studies of productivity and the causal effect of trade on income?\n\n**Setting / Institutional Environment.** The analysis provides guidelines for empirical researchers using cross-country data. It highlights how using the wrong GDP measure can lead to biased estimates and flawed causal inference.\n\n**Variables & Parameters.**\n- `Real GDPe`: Expenditure-side real GDP, a measure of national wealth and purchasing power. Influenced by terms of trade.\n- `Real GDPo`: Output-side real GDP, a measure of national productive capacity. Excludes terms of trade effects.\n- `Terms of Trade (ToT)`: The relative price of a country's exports to its imports.\n\n### Question\n\nA researcher regresses `Real GDPe` on `Nominal Openness`, where `Nominal Openness = (Nominal Exports + Imports) / Nominal GDP`. The paper argues that a country's Terms of Trade (ToT) are positively correlated with both `Real GDPe` (by definition) and `Nominal Openness` (e.g., via high export prices). If the ToT are omitted from the regression, what is the resulting econometric problem and the likely direction of bias on the `Nominal Openness` coefficient?", "Options": {"A": "Omitted variable bias; the coefficient will be biased upwards.", "B": "Omitted variable bias; the coefficient will be biased downwards.", "C": "Simultaneity bias; the direction of bias is indeterminate.", "D": "Measurement error in `Nominal Openness`; the coefficient will be biased towards zero (attenuation bias)."}, "Answer": "A", "pi_justification": "This item assesses the ability to identify a classic omitted variable bias scenario and predict the direction of the bias, a critical skill in applied econometrics. The distractors are designed to be highly plausible. (B) represents a 'Sign Error' in applying the OVB formula. (C) and (D) name other common, relevant econometric problems ('Conceptual: Almost Right') that are distinct from the specific mechanism described in the question stem, forcing the user to precisely identify the problem at hand.", "qid": "466", "question": "### Background\n\n**Research Question.** What are the practical implications of the `GDPe`/`GDPo` distinction for applied empirical research, particularly for studies of productivity and the causal effect of trade on income?\n\n**Setting / Institutional Environment.** The analysis provides guidelines for empirical researchers using cross-country data. It highlights how using the wrong GDP measure can lead to biased estimates and flawed causal inference.\n\n**Variables & Parameters.**\n- `Real GDPe`: Expenditure-side real GDP, a measure of national wealth and purchasing power. Influenced by terms of trade.\n- `Real GDPo`: Output-side real GDP, a measure of national productive capacity. Excludes terms of trade effects.\n- `Terms of Trade (ToT)`: The relative price of a country's exports to its imports.\n\n### Question\n\nA researcher regresses `Real GDPe` on `Nominal Openness`, where `Nominal Openness = (Nominal Exports + Imports) / Nominal GDP`. The paper argues that a country's Terms of Trade (ToT) are positively correlated with both `Real GDPe` (by definition) and `Nominal Openness` (e.g., via high export prices). If the ToT are omitted from the regression, what is the resulting econometric problem and the likely direction of bias on the `Nominal Openness` coefficient?", "model_evaluation_result": {"model_raw_response": "Looking at this econometric problem, I need to identify the type of bias and its direction when Terms of Trade (ToT) is omitted from the regression.\n\nGiven information:\n- Dependent variable: Real GDPe\n- Independent variable: Nominal Openness\n- Omitted variable: Terms of Trade (ToT)\n- ToT is positively correlated with both Real GDPe and Nominal Openness\n\nThis is a classic omitted variable bias scenario. When a relevant variable (ToT) is excluded from the regression, and this variable is correlated with both the dependent variable and the included independent variable, omitted variable bias occurs.\n\nThe direction of bias follows the formula:\nBias = β₃ × Corr(X₂, X₃)\n\nWhere:\n- β₃ is the true effect of the omitted variable (ToT) on the dependent variable (Real GDPe)\n- Corr(X₂, X₃) is the correlation between the included variable (Nominal Openness) and omitted variable (ToT)\n\nFrom the problem:\n- ToT has a positive effect on Real GDPe (β₃ > 0)\n- ToT is positively correlated with Nominal Openness (Corr > 0)\n\nTherefore: Bias = (+) × (+) = positive bias\n\nThis means the coefficient on Nominal Openness will be biased upwards, capturing not only its direct effect on Real GDPe but also the effect that should be attributed to the omitted ToT variable.\n\nA", "model_answer": "A", "error": null}}
{"ID": 455, "Question": "### Background\n\nThe paper argues that nearly decomposable systems, where within-subsystem linkages are strong and between-subsystem linkages are weak, can be simplified for long-run analysis through aggregation. This simplification is only valid under specific conditions related to the system's characteristic roots (eigenvalues).\n\n### Data / Model Specification\n\nFor aggregation to be valid, two strong conditions must be met:\n1.  **Existence of Primary Roots:** Each subsystem has a 'primary' root whose real part is significantly smaller in absolute magnitude than its other 'non-primary' roots.\n2.  **Separation of Time Scales:** The absolute value of the real part of every primary root must be smaller than that of any non-primary root from any subsystem.\n\nConsider a hypothetical dynamic system with two subsystems with the following characteristic roots:\n- **Roots of Subsystem 1:** `{-2.0, -5.0, -6.0}`\n- **Roots of Subsystem 2:** `{-0.5, -1.0, -7.0}`\n\n### Question\n\nDoes this system satisfy the conditions for aggregation, and why?", "Options": {"A": "No, because the magnitude of a primary root of Subsystem 1 (|-2.0|) is greater than the magnitude of a non-primary root of Subsystem 2 (|-1.0|).", "B": "Yes, because each subsystem has a distinct primary root with a magnitude smaller than its other roots.", "C": "Yes, because all characteristic roots are negative, ensuring the overall system is stable.", "D": "No, because the primary root of Subsystem 2 (|-0.5|) is smaller in magnitude than the primary root of Subsystem 1 (|-2.0|)."}, "Answer": "A", "pi_justification": "This item assesses the ability to apply abstract theoretical conditions to a concrete numerical case. It uses the Scenario Application strategy. The correct answer requires correctly identifying primary/non-primary roots and performing the cross-subsystem comparison specified in Condition 2. Distractor B represents a common error of only checking Condition 1. Distractor C is True but Irrelevant to the question of aggregation. Distractor D represents a misapplication of Condition 2, comparing a primary root to another primary root.", "qid": "455", "question": "### Background\n\nThe paper argues that nearly decomposable systems, where within-subsystem linkages are strong and between-subsystem linkages are weak, can be simplified for long-run analysis through aggregation. This simplification is only valid under specific conditions related to the system's characteristic roots (eigenvalues).\n\n### Data / Model Specification\n\nFor aggregation to be valid, two strong conditions must be met:\n1.  **Existence of Primary Roots:** Each subsystem has a 'primary' root whose real part is significantly smaller in absolute magnitude than its other 'non-primary' roots.\n2.  **Separation of Time Scales:** The absolute value of the real part of every primary root must be smaller than that of any non-primary root from any subsystem.\n\nConsider a hypothetical dynamic system with two subsystems with the following characteristic roots:\n- **Roots of Subsystem 1:** `{-2.0, -5.0, -6.0}`\n- **Roots of Subsystem 2:** `{-0.5, -1.0, -7.0}`\n\n### Question\n\nDoes this system satisfy the conditions for aggregation, and why?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to evaluate whether the two conditions for valid aggregation are met.\n\nFirst, let me identify the primary roots for each subsystem. The primary root should have the smallest absolute magnitude of the real part within each subsystem:\n\n**Subsystem 1 roots:** {-2.0, -5.0, -6.0}\n- Absolute magnitudes: |−2.0| = 2.0, |−5.0| = 5.0, |−6.0| = 6.0\n- Primary root: -2.0 (smallest magnitude)\n- Non-primary roots: -5.0, -6.0\n\n**Subsystem 2 roots:** {-0.5, -1.0, -7.0}\n- Absolute magnitudes: |−0.5| = 0.5, |−1.0| = 1.0, |−7.0| = 7.0\n- Primary root: -0.5 (smallest magnitude)\n- Non-primary roots: -1.0, -7.0\n\nNow checking **Condition 1** (each subsystem has a primary root with smaller magnitude than its non-primary roots):\n- Subsystem 1: |−2.0| = 2.0 < 5.0 and 2.0 < 6.0 ✓\n- Subsystem 2: |−0.5| = 0.5 < 1.0 and 0.5 < 7.0 ✓\n\n**Condition 1 is satisfied.**\n\nNow checking **Condition 2** (magnitude of every primary root < magnitude of any non-primary root):\n- Primary roots: -2.0 (magnitude 2.0) and -0.5 (magnitude 0.5)\n- Non-primary roots: -5.0, -6.0, -1.0, -7.0 (magnitudes 5.0, 6.0, 1.0, 7.0)\n\nFor this condition to hold, both 2.0 and 0.5 must be less than all of {5.0, 6.0, 1.0, ", "model_answer": "Looking at this problem, I need to evaluate whether the two conditions for valid aggregation are met.\n\nFirst, let me identify the primary roots for each subsystem. The primary root should have the smallest absolute magnitude of the real part within each subsystem:\n\n**Subsystem 1 roots:** {-2.0, -5.0, -6.0}\n- Absolute magnitudes: |−2.0| = 2.0, |−5.0| = 5.0, |−6.0| = 6.0\n- Primary root: -2.0 (smallest magnitude)\n- Non-primary roots: -5.0, -6.0\n\n**Subsystem 2 roots:** {-0.5, -1.0, -7.0}\n- Absolute magnitudes: |−0.5| = 0.5, |−1.0| = 1.0, |−7.0| = 7.0\n- Primary root: -0.5 (smallest magnitude)\n- Non-primary roots: -1.0, -7.0\n\nNow checking **Condition 1** (each subsystem has a primary root with smaller magnitude than its non-primary roots):\n- Subsystem 1: |−2.0| = 2.0 < 5.0 and 2.0 < 6.0 ✓\n- Subsystem 2: |−0.5| = 0.5 < 1.0 and 0.5 < 7.0 ✓\n\n**Condition 1 is satisfied.**\n\nNow checking **Condition 2** (magnitude of every primary root < magnitude of any non-primary root):\n- Primary roots: -2.0 (magnitude 2.0) and -0.5 (magnitude 0.5)\n- Non-primary roots: -5.0, -6.0, -1.0, -7.0 (magnitudes 5.0, 6.0, 1.0, 7.0)\n\nFor this condition to hold, both 2.0 and 0.5 must be less than all of {5.0, 6.0, 1.0,", "error": "Warning: Could not extract single choice letter from: Looking at this problem, I need to evaluate whethe..."}}
{"ID": 489, "Question": "### Background\n\nBefore presenting its main causal estimates, the paper first runs a benchmark Ordinary Least Squares (OLS) regression to replicate the correlational findings in prior literature. The model is:\n\n```latex\nY_i = \\beta_0 + \\beta_1 T_i + D'_i\\gamma + \\varepsilon_i\n```\n**Eq. (1)**\n\nHere, \\(Y_i\\) is an indicator for passing Algebra II, \\(T_i\\) is an indicator for taking Algebra I by eighth grade, and \\(D_i\\) is a vector of controls including the student's prior math score decile. The OLS estimate for \\(\\beta_1\\) is found to be **+0.09**.\n\n---\n\n### The Question\n\nGiven that this is a correlational result and not a causal estimate, how should the OLS coefficient \\(\\hat{\\beta}_1 = +0.09\\) be interpreted?", "Options": {"A": "It indicates a partial correlation: holding prior test score decile constant, students who took algebra early were 9 percentage points more likely to pass Algebra II.", "B": "It shows a causal effect: taking algebra early causes a 9 percentage point increase in the probability of passing Algebra II.", "C": "It means that 9% of all students who took algebra early went on to pass Algebra II.", "D": "It is a statistically significant result, which proves that the positive relationship between early algebra and later success is real and not due to chance."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to correctly interpret a regression coefficient from a non-causal model, a foundational skill. It is based on the original QA's Q1. The correct answer properly identifies the estimate as a partial correlation. Distractor B makes the cardinal sin of interpreting correlation as causation. Distractor C fundamentally misunderstands how to interpret a regression coefficient. Distractor D conflates statistical significance with causal validity, a classic misconception.", "qid": "489", "question": "### Background\n\nBefore presenting its main causal estimates, the paper first runs a benchmark Ordinary Least Squares (OLS) regression to replicate the correlational findings in prior literature. The model is:\n\n```latex\nY_i = \\beta_0 + \\beta_1 T_i + D'_i\\gamma + \\varepsilon_i\n```\n**Eq. (1)**\n\nHere, \\(Y_i\\) is an indicator for passing Algebra II, \\(T_i\\) is an indicator for taking Algebra I by eighth grade, and \\(D_i\\) is a vector of controls including the student's prior math score decile. The OLS estimate for \\(\\beta_1\\) is found to be **+0.09**.\n\n---\n\n### The Question\n\nGiven that this is a correlational result and not a causal estimate, how should the OLS coefficient \\(\\hat{\\beta}_1 = +0.09\\) be interpreted?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 490, "Question": "### Background\n\nA simple OLS regression finds that taking Algebra I by eighth grade is associated with a 9 percentage point *increase* in the probability of passing Algebra II. However, the paper's main instrumental variable (IV) analysis finds that the true causal effect is negative. This discrepancy is attributed to omitted variable bias (OVB) in the OLS model, stemming from unobserved student characteristics like motivation or innate ability.\n\n---\n\n### The Question\n\nWhat is the correct econometric explanation for why the OLS estimate is positively biased, creating this contradiction?", "Options": {"A": "The bias is positive because unobserved ability is positively correlated with both taking Algebra I early and with passing Algebra II.", "B": "The bias is negative because higher-ability students are often held back from early algebra, and this unobserved ability is positively correlated with passing Algebra II.", "C": "The bias is positive because the OLS model controls for prior test scores, which artificially inflates the coefficient on the early algebra indicator.", "D": "The bias is zero because any unobserved student ability is captured by the error term, which is by definition uncorrelated with the regressors."}, "Answer": "A", "pi_justification": "This item assesses a deep understanding of omitted variable bias, requiring the student to sign both components of the bias formula in context. It uses a Computational Judgment strategy to test the logic from the original QA's Q2 and Q3. The correct answer correctly identifies the two positive correlations that create the positive bias. Distractor B gets the sign of the bias wrong by incorrectly signing the first correlation (Conceptual Opposite). Distractor C misidentifies the role of control variables. Distractor D states the OLS assumption as if it were a fact, ignoring that it is violated here, which is the entire point of OVB.", "qid": "490", "question": "### Background\n\nA simple OLS regression finds that taking Algebra I by eighth grade is associated with a 9 percentage point *increase* in the probability of passing Algebra II. However, the paper's main instrumental variable (IV) analysis finds that the true causal effect is negative. This discrepancy is attributed to omitted variable bias (OVB) in the OLS model, stemming from unobserved student characteristics like motivation or innate ability.\n\n---\n\n### The Question\n\nWhat is the correct econometric explanation for why the OLS estimate is positively biased, creating this contradiction?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 467, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical micro-foundations of a model for measuring differences in real income across households. The model posits that economic well-being is derived from unobserved 'service flows' (e.g., entertainment) which are 'produced' by observable market goods (e.g., a TV). The efficiency of this production process is the key parameter that varies with household structure.\n\n**Setting / Institutional Environment.** Individuals demand service flows, which have an unobserved 'effective price'. This price depends on both the market price of the input good and the household's efficiency in transforming that good into a service. A change in household structure (e.g., two people forming a couple) is modeled as a shock to this transformation efficiency.\n\n**Variables & Parameters (Exhaustive and Standardized).**\n- `$S_1$`: The quantity of a service flow (e.g., entertainment services).\n- `$X_1$`: The quantity of a purchased market good (e.g., a TV set).\n- `$\\alpha_1$`: The rate of transformation, or average product, of good `$X_1$` into service flow `$S_1$`. Units: service units per unit of good.\n- `$P_1$`: The market price of the good `$X_1$`. Units: dollars per unit of good.\n- `$P_{s1}$`: The effective or implicit price of the service flow `$S_1$`. Units: dollars per service unit.\n- `$Y$`: An individual's nominal money income. Units: dollars.\n- `$J_1$`: The percentage change in `$\\alpha_1$` due to a change in household environment. Dimensionless.\n- `$\\eta_{S_1, P_{s1}}$`: The price elasticity of demand for the service flow `$S_1$`.\n\n---\n\n### Data / Model Specification\n\nThe model is defined by the following relationships:\n```latex\n\\alpha_{1} \\equiv S_{1}/X_{1} \\quad \\text{(Eq. (1))}\n```\n```latex\nP_{s1} = P_{1}/\\alpha_{1} \\quad \\text{(Eq. (2))}\n```\n```latex\nS_{1} = d(P_{s1}, Y) \\quad \\text{(Eq. (3))}\n```\nA change in household environment alters the transformation rate from `$\\alpha_1$` to `$\\alpha_1(1+J_1)$`.\n\n---\n\n### The Questions\n\nBased on the model, which of the following expressions correctly represents the elasticity of demand for the *market good* `$X_1$` with respect to the transformation parameter `$\\alpha_1$`, denoted `$\\eta_{X_1, \\alpha_1}$`? The final expression should be in terms of the price elasticity of demand for the service, `$\\eta_{S_1, P_{s1}}$`.", "Options": {"A": "```latex\n-\\eta_{S_1, P_{s1}} - 1\n```", "B": "```latex\n-\\eta_{S_1, P_{s1}}\n```", "C": "```latex\n\\eta_{S_1, P_{s1}} - 1\n```", "D": "```latex\n1 - \\eta_{S_1, P_{s1}}\n```"}, "Answer": "A", "pi_justification": "This question assesses the user's ability to derive a key theoretical relationship within the paper's model. It uses a Computational Judgment strategy. The distractors are designed based on common mathematical errors in elasticity derivations: (B) is a step-omission error (missing the second term from the log-differentiated expression), while (C) and (D) represent plausible sign errors.", "qid": "467", "question": "### Background\n\n**Research Question.** This problem explores the theoretical micro-foundations of a model for measuring differences in real income across households. The model posits that economic well-being is derived from unobserved 'service flows' (e.g., entertainment) which are 'produced' by observable market goods (e.g., a TV). The efficiency of this production process is the key parameter that varies with household structure.\n\n**Setting / Institutional Environment.** Individuals demand service flows, which have an unobserved 'effective price'. This price depends on both the market price of the input good and the household's efficiency in transforming that good into a service. A change in household structure (e.g., two people forming a couple) is modeled as a shock to this transformation efficiency.\n\n**Variables & Parameters (Exhaustive and Standardized).**\n- `$S_1$`: The quantity of a service flow (e.g., entertainment services).\n- `$X_1$`: The quantity of a purchased market good (e.g., a TV set).\n- `$\\alpha_1$`: The rate of transformation, or average product, of good `$X_1$` into service flow `$S_1$`. Units: service units per unit of good.\n- `$P_1$`: The market price of the good `$X_1$`. Units: dollars per unit of good.\n- `$P_{s1}$`: The effective or implicit price of the service flow `$S_1$`. Units: dollars per service unit.\n- `$Y$`: An individual's nominal money income. Units: dollars.\n- `$J_1$`: The percentage change in `$\\alpha_1$` due to a change in household environment. Dimensionless.\n- `$\\eta_{S_1, P_{s1}}$`: The price elasticity of demand for the service flow `$S_1$`.\n\n---\n\n### Data / Model Specification\n\nThe model is defined by the following relationships:\n```latex\n\\alpha_{1} \\equiv S_{1}/X_{1} \\quad \\text{(Eq. (1))}\n```\n```latex\nP_{s1} = P_{1}/\\alpha_{1} \\quad \\text{(Eq. (2))}\n```\n```latex\nS_{1} = d(P_{s1}, Y) \\quad \\text{(Eq. (3))}\n```\nA change in household environment alters the transformation rate from `$\\alpha_1$` to `$\\alpha_1(1+J_1)$`.\n\n---\n\n### The Questions\n\nBased on the model, which of the following expressions correctly represents the elasticity of demand for the *market good* `$X_1$` with respect to the transformation parameter `$\\alpha_1$`, denoted `$\\eta_{X_1, \\alpha_1}$`? The final expression should be in terms of the price elasticity of demand for the service, `$\\eta_{S_1, P_{s1}}$`.", "model_evaluation_result": {"model_raw_response": "I need to find the elasticity of demand for the market good X₁ with respect to the transformation parameter α₁.\n\nLet me work through this step by step using the given relationships:\n\nFrom the equations:\n- α₁ = S₁/X₁ (Eq. 1)\n- P_{s1} = P₁/α₁ (Eq. 2)\n- S₁ = d(P_{s1}, Y) (Eq. 3)\n\nFrom Eq. 1, I can express X₁ in terms of S₁ and α₁:\nX₁ = S₁/α₁\n\nTo find η_{X₁,α₁}, I need to calculate:\nη_{X₁,α₁} = (∂X₁/∂α₁) × (α₁/X₁)\n\nTaking the derivative of X₁ = S₁/α₁ with respect to α₁:\n∂X₁/∂α₁ = ∂/∂α₁(S₁/α₁) = (1/α₁)(∂S₁/∂α₁) - S₁/α₁²\n\nFrom Eq. 2, P_{s1} = P₁/α₁, so:\n∂P_{s1}/∂α₁ = -P₁/α₁² = -P_{s1}/α₁\n\nFrom Eq. 3, S₁ depends on P_{s1}, so:\n∂S₁/∂α₁ = (∂S₁/∂P_{s1}) × (∂P_{s1}/∂α₁) = (∂S₁/∂P_{s1}) × (-P_{s1}/α₁)\n\nThe price elasticity of demand for the service is:\nη_{S₁,P_{s1}} = (∂S₁/∂P_{s1}) × (P_{s1}/S₁)\n\nTherefore:\n∂S₁/∂P_{s1} = η_{S", "model_answer": "I need to find the elasticity of demand for the market good X₁ with respect to the transformation parameter α₁.\n\nLet me work through this step by step using the given relationships:\n\nFrom the equations:\n- α₁ = S₁/X₁ (Eq. 1)\n- P_{s1} = P₁/α₁ (Eq. 2)\n- S₁ = d(P_{s1}, Y) (Eq. 3)\n\nFrom Eq. 1, I can express X₁ in terms of S₁ and α₁:\nX₁ = S₁/α₁\n\nTo find η_{X₁,α₁}, I need to calculate:\nη_{X₁,α₁} = (∂X₁/∂α₁) × (α₁/X₁)\n\nTaking the derivative of X₁ = S₁/α₁ with respect to α₁:\n∂X₁/∂α₁ = ∂/∂α₁(S₁/α₁) = (1/α₁)(∂S₁/∂α₁) - S₁/α₁²\n\nFrom Eq. 2, P_{s1} = P₁/α₁, so:\n∂P_{s1}/∂α₁ = -P₁/α₁² = -P_{s1}/α₁\n\nFrom Eq. 3, S₁ depends on P_{s1}, so:\n∂S₁/∂α₁ = (∂S₁/∂P_{s1}) × (∂P_{s1}/∂α₁) = (∂S₁/∂P_{s1}) × (-P_{s1}/α₁)\n\nThe price elasticity of demand for the service is:\nη_{S₁,P_{s1}} = (∂S₁/∂P_{s1}) × (P_{s1}/S₁)\n\nTherefore:\n∂S₁/∂P_{s1} = η_{S", "error": "Warning: Could not extract single choice letter from: I need to find the elasticity of demand for the ma..."}}
{"ID": 456, "Question": "### Background\n\nThe paper proposes approximating complex dynamic systems with 'mixed structures' containing dynamic, equilibrium, and exogenous equations. For such a structure to be valid, it must be 'self-contained'. A key condition relates to whether a dynamic equation is 'self-regulating' (the variable whose derivative is on the left-hand side also appears on the right-hand side).\n\n### Data / Model Specification\n\nConsider the following 2-variable dynamic system:\n```latex\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = a y \\quad \\text{(Eq. 1 - non-self-regulating)}\n```\n```latex\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = -b y \\quad \\text{(Eq. 2 - self-regulating)}\n```\n\n### Question\n\nIf the non-self-regulating equation for `x` (Eq. 1) is converted into its static equilibrium form (`0 = ay`) while the equation for `y` remains dynamic, what is the fundamental consequence for the resulting mixed model?", "Options": {"A": "The system becomes over-determined, as the static equation imposes a constraint on `y` that conflicts with its own dynamic law.", "B": "The system becomes under-determined, as there is no longer an equation governing the dynamics of `x`.", "C": "The system remains a valid self-contained mixed structure, but its only solution is the trivial one where `y` is always zero.", "D": "The causal ordering is reversed, with `y` now causing `x` in a static relationship."}, "Answer": "A", "pi_justification": "This item assesses the understanding of a subtle but crucial technical condition for creating valid mixed models. It uses the Reverse-Reasoning strategy, asking for the consequence of a specific action. The correct answer identifies the fundamental structural flaw. Distractor B is a plausible but incorrect inference. Distractor C identifies a symptom (`y=0`) but misses the underlying flaw ('over-determined'). Distractor D misinterprets the nature of the problem.", "qid": "456", "question": "### Background\n\nThe paper proposes approximating complex dynamic systems with 'mixed structures' containing dynamic, equilibrium, and exogenous equations. For such a structure to be valid, it must be 'self-contained'. A key condition relates to whether a dynamic equation is 'self-regulating' (the variable whose derivative is on the left-hand side also appears on the right-hand side).\n\n### Data / Model Specification\n\nConsider the following 2-variable dynamic system:\n```latex\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = a y \\quad \\text{(Eq. 1 - non-self-regulating)}\n```\n```latex\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = -b y \\quad \\text{(Eq. 2 - self-regulating)}\n```\n\n### Question\n\nIf the non-self-regulating equation for `x` (Eq. 1) is converted into its static equilibrium form (`0 = ay`) while the equation for `y` remains dynamic, what is the fundamental consequence for the resulting mixed model?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze what happens when we convert the non-self-regulating equation for x into its static equilibrium form.\n\nOriginal system:\n- dx/dt = ay (non-self-regulating)\n- dy/dt = -by (self-regulating)\n\nAfter conversion:\n- 0 = ay (static equilibrium form)\n- dy/dt = -by (remains dynamic)\n\nLet me examine each option:\n\nThe static equation 0 = ay creates a constraint. Since we typically assume a ≠ 0 (otherwise the original dynamic equation would be trivial), this constraint forces y = 0.\n\nHowever, we still have the dynamic equation dy/dt = -by. If y must equal zero due to the static constraint, then dy/dt = -b(0) = 0, which is consistent - y remains at zero.\n\nBut this creates a fundamental problem: the static constraint essentially overrides the dynamic behavior of y. The dynamic equation dy/dt = -by describes how y should evolve over time, but the static constraint 0 = ay forces y to always be zero regardless of what the dynamic equation says about its evolution.\n\nThis represents a conflict between the static constraint and the dynamic law governing y. The static equation imposes a rigid constraint (y = 0) that conflicts with the natural dynamic evolution that y would otherwise follow according to its differential equation.\n\nThe answer is A) The system becomes over-determined, as the static equation imposes a constraint on `y` that conflicts with its own dynamic law.\n\nA", "model_answer": "A", "error": null}}
{"ID": 435, "Question": "### Background\n\n**Research Question.** This problem applies the paper's finding on equilibrium multiplicity to a specific market scenario, including a regulatory intervention.\n\n**Setting / Institutional Environment.** A durable-goods monopolist sells to two high-valuation (`H`) buyers and `n_0^L` low-valuation (`L`) buyers. When the condition `n_0^L L ≥ H - L` holds, a range of total profits for the seller can be supported in real-time efficient Subgame Perfect Equilibria (SPEs) as the time between offers shrinks (`δ→1`). The range is bounded by the low-profit \"Coasian\" outcome and the high-profit \"Pacman\" (perfect price discrimination) outcome.\n\n---\n\n### Data / Model Specification\n\nConsider a market with the following parameters:\n- High valuation, `H = 10`\n- Low valuation, `L = 2`\n- Number of low-valuation buyers, `n_0^L = 5`\n\nA regulator imposes a price ceiling `p_max = 8`.\n\n---\n\n### Question\n\nGiven the market parameters and the price ceiling, what is the new range of total profits for the seller that can be supported in real-time efficient SPEs?\n", "Options": {"A": "[14, 30]", "B": "[14, 26]", "C": "[14, 56]", "D": "[16, 26]"}, "Answer": "B", "pi_justification": "This item tests the application of the paper's folk theorem result to a specific policy scenario (a price ceiling). It uses a Scenario Application strategy, requiring multi-step calculation. The distractors target predictable errors: (A) correctly calculates the baseline profit range but ignores the effect of the price ceiling on the maximum profit; (C) correctly calculates the minimum (Coasian) profit but miscalculates the maximum profit by assuming the seller charges the ceiling price to all buyers; (D) incorrectly calculates the minimum (Coasian) profit, possibly by assuming the price ceiling affects the lowest possible outcome.", "qid": "435", "question": "### Background\n\n**Research Question.** This problem applies the paper's finding on equilibrium multiplicity to a specific market scenario, including a regulatory intervention.\n\n**Setting / Institutional Environment.** A durable-goods monopolist sells to two high-valuation (`H`) buyers and `n_0^L` low-valuation (`L`) buyers. When the condition `n_0^L L ≥ H - L` holds, a range of total profits for the seller can be supported in real-time efficient Subgame Perfect Equilibria (SPEs) as the time between offers shrinks (`δ→1`). The range is bounded by the low-profit \"Coasian\" outcome and the high-profit \"Pacman\" (perfect price discrimination) outcome.\n\n---\n\n### Data / Model Specification\n\nConsider a market with the following parameters:\n- High valuation, `H = 10`\n- Low valuation, `L = 2`\n- Number of low-valuation buyers, `n_0^L = 5`\n\nA regulator imposes a price ceiling `p_max = 8`.\n\n---\n\n### Question\n\nGiven the market parameters and the price ceiling, what is the new range of total profits for the seller that can be supported in real-time efficient SPEs?\n", "model_evaluation_result": {"model_raw_response": "I need to analyze this durable goods monopoly problem with equilibrium multiplicity under a price ceiling.\n\nFirst, let me verify the condition for multiplicity:\n- n₀ᴸ L ≥ H - L\n- 5 × 2 ≥ 10 - 2\n- 10 ≥ 8 ✓\n\nThe condition holds, so multiple equilibria exist.\n\nWithout the price ceiling, the profit range would be:\n- Coasian (low-profit) bound: n₀ᴸ × L = 5 × 2 = 10\n- Pacman (high-profit) bound: 2H + n₀ᴸ × L = 2 × 10 + 5 × 2 = 30\n\nNow with the price ceiling p_max = 8:\n\nFor the Coasian outcome:\n- The seller still gets the low-valuation buyers at price L = 2\n- Profit = 5 × 2 = 10\n\nWait, let me reconsider the Coasian outcome. In the Coasian limit, the seller's profit approaches the competitive level. With the price ceiling of 8, which is below H = 10, the high-valuation buyers can still be served at price 8.\n\nActually, let me think about this more systematically. The price ceiling p_max = 8 constrains the maximum price the seller can charge.\n\nFor the Coasian (low-profit) outcome:\n- This represents the limit where the seller's commitment power is weak\n- The seller ends up selling to all buyers at the low valuation\n- Profit = (2 + 5) × 2 = 14\n\nFor the Pacman (high-profit) outcome with price ceiling:\n- High-valuation buyers: can be charged at most min(H, p_max) = min(10, 8) = 8\n- Low-valuation buyers: charged L = 2\n- Total profit = 2 × 8 + 5 × 2 = 16 + 10 = 26\n\nTherefore, the range is [14, 26].\n\nB", "model_answer": "B", "error": null}}
{"ID": 472, "Question": "### Background\n\nA key justification for a central bank's role in bank supervision is that it may provide access to confidential information that improves monetary policy. This analysis examines whether confidential supervisory information contains predictive power for macroeconomic outcomes that is not already incorporated in official forecasts. It compares the Federal Reserve's internal \"Greenbook\" forecasts to those of private forecasters using a panel of quarterly forecasts from 1968-1994.\n\n### Data / Model Specification\n\nThe model regresses the actual, realized unemployment rate on the forecast, a measure of confidential supervisory information (`CAMEL5`), and an interaction term to see if the Greenbook uses this information differently.\n\n**The Model:**\n\n```latex\nU_{t+i} = \\alpha_{0} + \\alpha_{1}U_{j,t+i}^{e} + \\alpha_{2}CAMEL5_{t} + \\alpha_{3}(GB_{t} \\cdot CAMEL5_{t}) + \\epsilon_{j,t+i}\n```\n\n*   `U_{t+i}`: Actual realized unemployment rate.\n*   `U^e_{j,t+i}`: Forecast of unemployment made by forecaster `j`.\n*   `CAMEL5_t`: Percentage of assets in CAMEL 5-rated banks (confidential signal of distress).\n*   `GB_t`: A dummy variable equal to 1 for Greenbook forecasts, 0 for private forecasts.\n\n**The Data:**\nResults for the 4-quarter-ahead unemployment forecasts are in Table 1.\n\n**Table 1: Contribution of Supervisory Information to 4-Quarter-Ahead Unemployment Forecasts**\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| `CAMEL5` | 0.283 | (0.132) |\n| `Greenbook * CAMEL5` | -0.035 | (0.021) |\n\n*Notes: The dependent variable is the actual unemployment rate.*\n\n### Question\n\nThe paper tests if the Greenbook staff fully incorporates the `CAMEL5` information, which implies a null hypothesis of `α₂ + α₃ = 0`. Based on the results in Table 1, what is the outcome of this test at a 5% significance level (critical value ≈ 1.96)? (Assume zero covariance between the estimators).", "Options": {"A": "Fail to reject the null; the test statistic is approximately 1.86, so we cannot conclude the Greenbook's use of the information is incomplete.", "B": "Reject the null; the coefficient on `Greenbook * CAMEL5` (-0.035) is statistically insignificant, proving the information is not used.", "C": "Reject the null; the test statistic is approximately 10.8, indicating the information is not fully used.", "D": "Fail to reject the null; this indicates that the Greenbook forecasts are statistically identical to private forecasts regarding `CAMEL5`."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to perform a multi-step hypothesis test and interpret the result. Strategy: Computational Judgment. Distractors are designed to catch specific errors: Step-Omission Error (testing the wrong hypothesis, `α₃=0`, instead of the correct one), Calculation Error (using an incorrect formula for the Wald statistic's standard error), and Conceptual Opposite (reaching the correct test outcome but for the wrong conceptual reason).", "qid": "472", "question": "### Background\n\nA key justification for a central bank's role in bank supervision is that it may provide access to confidential information that improves monetary policy. This analysis examines whether confidential supervisory information contains predictive power for macroeconomic outcomes that is not already incorporated in official forecasts. It compares the Federal Reserve's internal \"Greenbook\" forecasts to those of private forecasters using a panel of quarterly forecasts from 1968-1994.\n\n### Data / Model Specification\n\nThe model regresses the actual, realized unemployment rate on the forecast, a measure of confidential supervisory information (`CAMEL5`), and an interaction term to see if the Greenbook uses this information differently.\n\n**The Model:**\n\n```latex\nU_{t+i} = \\alpha_{0} + \\alpha_{1}U_{j,t+i}^{e} + \\alpha_{2}CAMEL5_{t} + \\alpha_{3}(GB_{t} \\cdot CAMEL5_{t}) + \\epsilon_{j,t+i}\n```\n\n*   `U_{t+i}`: Actual realized unemployment rate.\n*   `U^e_{j,t+i}`: Forecast of unemployment made by forecaster `j`.\n*   `CAMEL5_t`: Percentage of assets in CAMEL 5-rated banks (confidential signal of distress).\n*   `GB_t`: A dummy variable equal to 1 for Greenbook forecasts, 0 for private forecasts.\n\n**The Data:**\nResults for the 4-quarter-ahead unemployment forecasts are in Table 1.\n\n**Table 1: Contribution of Supervisory Information to 4-Quarter-Ahead Unemployment Forecasts**\n\n| Variable | Coefficient | Std. Error |\n| :--- | :---: | :---: |\n| `CAMEL5` | 0.283 | (0.132) |\n| `Greenbook * CAMEL5` | -0.035 | (0.021) |\n\n*Notes: The dependent variable is the actual unemployment rate.*\n\n### Question\n\nThe paper tests if the Greenbook staff fully incorporates the `CAMEL5` information, which implies a null hypothesis of `α₂ + α₃ = 0`. Based on the results in Table 1, what is the outcome of this test at a 5% significance level (critical value ≈ 1.96)? (Assume zero covariance between the estimators).", "model_evaluation_result": {"model_raw_response": "I need to test the null hypothesis that α₂ + α₃ = 0, which means the Greenbook staff fully incorporates the CAMEL5 information.\n\nFrom Table 1:\n- α₂ (CAMEL5 coefficient) = 0.283 with standard error 0.132\n- α₃ (Greenbook * CAMEL5 coefficient) = -0.035 with standard error 0.021\n\nFirst, I'll calculate α₂ + α₃:\nα₂ + α₃ = 0.283 + (-0.035) = 0.248\n\nNext, I need the standard error of (α₂ + α₃). Since the problem states to assume zero covariance between the estimators:\nSE(α₂ + α₃) = √[SE(α₂)² + SE(α₃)²]\nSE(α₂ + α₃) = √[0.132² + 0.021²]\nSE(α₂ + α₃) = √[0.017424 + 0.000441]\nSE(α₂ + α₃) = √0.017865 ≈ 0.134\n\nNow I can calculate the test statistic:\nt = (α₂ + α₃) / SE(α₂ + α₃)\nt = 0.248 / 0.134 ≈ 1.85\n\nComparing to the critical value of 1.96 at the 5% significance level:\n|1.85| < 1.96\n\nSince the test statistic (1.85) is less than the critical value (1.96), we fail to reject the null hypothesis α₂ + α₃ = 0.\n\nThis means we cannot conclude that the Greenbook's use of the CAMEL5 information is incomplete.\n\nA", "model_answer": "A", "error": null}}
{"ID": 446, "Question": "### Background\n\n**Setting.** We analyze a simplified model where employee effort is fixed at `a=1`. The firm's hierarchy has a fixed total number of tiers, `T`, and workers are sufficiently productive (`\\theta > ge`).\n\n---\n\n### Data / Model Specification\n\nThe optimal path for the span of control `s_t` is governed by the general solution:\n```latex\n\\log(s_t / e) = C e^{-t} \\quad \\text{(Eq. 1)}\n```\nwhere `C` is a constant. The terminal condition for this fixed-`T` problem implies that `s_T = \\theta/g`.\n\n---\n\n### Question\n\nGiven the model setup and the terminal condition, what can be concluded about the optimal path of the span of control, `s_t`, as one moves from the top (`t=0`) to the bottom (`t=T`) of the hierarchy?", "Options": {"A": "The span of control is strictly decreasing.", "B": "The span of control is strictly increasing.", "C": "The span of control is constant.", "D": "The path of the span of control is ambiguous."}, "Answer": "A", "pi_justification": "This item tests the student's ability to execute a key part of the model's derivation. It uses a Computational Judgment strategy, requiring the student to use the condition `\\theta > ge` to solve for the sign of the integration constant `C` (which will be positive) and then determine its implication for the dynamics of `s_t`. Distractors represent common errors: (B) a sign error in the derivation, (C) confusing the fixed-T case with the endogenous-T case, and (D) confusing this simple model with the ambiguous result from the paper's general model.", "qid": "446", "question": "### Background\n\n**Setting.** We analyze a simplified model where employee effort is fixed at `a=1`. The firm's hierarchy has a fixed total number of tiers, `T`, and workers are sufficiently productive (`\\theta > ge`).\n\n---\n\n### Data / Model Specification\n\nThe optimal path for the span of control `s_t` is governed by the general solution:\n```latex\n\\log(s_t / e) = C e^{-t} \\quad \\text{(Eq. 1)}\n```\nwhere `C` is a constant. The terminal condition for this fixed-`T` problem implies that `s_T = \\theta/g`.\n\n---\n\n### Question\n\nGiven the model setup and the terminal condition, what can be concluded about the optimal path of the span of control, `s_t`, as one moves from the top (`t=0`) to the bottom (`t=T`) of the hierarchy?", "model_evaluation_result": {"model_raw_response": "I need to analyze the optimal path of the span of control s_t from the top (t=0) to the bottom (t=T) of the hierarchy.\n\nGiven information:\n- The general solution: log(s_t/e) = C e^(-t)\n- Terminal condition: s_T = θ/g\n- We move from t=0 (top) to t=T (bottom)\n\nLet me analyze how s_t changes as t increases from 0 to T.\n\nFrom the equation log(s_t/e) = C e^(-t), I can solve for s_t:\ns_t = e · exp(C e^(-t))\n\nNow I need to examine how this changes as t increases:\n\nTaking the derivative with respect to t:\nd/dt[log(s_t/e)] = d/dt[C e^(-t)] = -C e^(-t)\n\nThis means:\n(1/s_t) · (ds_t/dt) = -C e^(-t)\n\nTherefore:\nds_t/dt = -C e^(-t) · s_t\n\nSince s_t > 0 (span of control must be positive), the sign of ds_t/dt depends on the sign of -C e^(-t).\n\nSince e^(-t) > 0 for all t, the sign depends on -C, which means it depends on whether C is positive or negative.\n\nTo determine the sign of C, I can use the terminal condition s_T = θ/g:\nlog(s_T/e) = log(θ/g)/e = C e^(-T)\n\nSince we're told that θ > ge (workers are sufficiently productive), we have:\nθ/g > e, which means log(θ/g) > log(e) = 1 > 0\n\nTherefore: C e^(-T) > 0, which implies C > 0.\n\nWith C > 0, we have:\nds_t/dt = -C e^(-t) · s_t < 0\n\nThis means s_t is strictly decreasing as t increases (moving from top to bottom of the hierarchy).\n\nThe answer is A.\n\nA", "model_answer": "C", "error": null}}
{"ID": 444, "Question": "### Background\n\n**Research Question.** This problem provides a complete, worked-out example of a buyer-optimal information structure for the uniform prior case, including a formal proof of its robustness against seller extensions.\n\n**Setting / Institutional Environment.** The buyer's valuation `v` is drawn from a uniform distribution on `[0, 1]`. The regulator's goal is to find an extensionproof information structure that maximizes buyer surplus. The proposed structure yields a seller profit of 1/4.\n\n---\n\n### Data / Model Specification\n\nThe proposed buyer-optimal information structure for the uniform case is as follows: if `v > 1/2`, the valuation is perfectly revealed; if `v ≤ 1/2`, valuations are pooled such that the posterior mean is always `1/4`. To check for extensionproofness, we must verify that the seller cannot profit by extending this structure.\n\nThe seller's profit from extending the information structure to a new price `q ∈ (1/4, 1/2)` is given by:\n```latex\n\\Pi_{ext}(q) = q \\cdot \\left(1-q + \\int_0^{1/2-q} x(v)dv\\right)\n```\nwhere `x(v)` is the probability of persuading a buyer with valuation `v` to buy.\n\n---\n\nWhich of the following statements correctly evaluates this extension strategy and the extensionproofness of the original structure?", "Options": {"A": "The seller's profit from the extension, `Π_ext(q)`, is always strictly less than 1/4, so the structure is extensionproof.", "B": "The seller's profit from the extension, `Π_ext(q)`, is always strictly greater than 1/4, so the structure is not extensionproof.", "C": "The seller's profit from the extension, `Π_ext(q)`, is maximized at `q=3/8` with a value of 9/32 (> 1/4), so the structure is not extensionproof.", "D": "The profitability of the extension cannot be determined without knowing the exact value of `q` the seller would choose."}, "Answer": "A", "pi_justification": "This item assesses the final step in the example's argument: verifying the extensionproofness constraint. It uses a Computational Judgment strategy. The correct answer requires understanding that the extension must be unprofitable for *all* possible new prices `q`. Distractors are designed to trap common errors: (B) is the Conceptual Opposite; (C) presents a specific, plausible-looking but incorrect calculation that leads to the wrong conclusion, testing for oversimplification (Formula Misuse); (D) reflects an unwillingness to draw a general conclusion, failing to complete the required logical step (Step-Omission Error).", "qid": "444", "question": "### Background\n\n**Research Question.** This problem provides a complete, worked-out example of a buyer-optimal information structure for the uniform prior case, including a formal proof of its robustness against seller extensions.\n\n**Setting / Institutional Environment.** The buyer's valuation `v` is drawn from a uniform distribution on `[0, 1]`. The regulator's goal is to find an extensionproof information structure that maximizes buyer surplus. The proposed structure yields a seller profit of 1/4.\n\n---\n\n### Data / Model Specification\n\nThe proposed buyer-optimal information structure for the uniform case is as follows: if `v > 1/2`, the valuation is perfectly revealed; if `v ≤ 1/2`, valuations are pooled such that the posterior mean is always `1/4`. To check for extensionproofness, we must verify that the seller cannot profit by extending this structure.\n\nThe seller's profit from extending the information structure to a new price `q ∈ (1/4, 1/2)` is given by:\n```latex\n\\Pi_{ext}(q) = q \\cdot \\left(1-q + \\int_0^{1/2-q} x(v)dv\\right)\n```\nwhere `x(v)` is the probability of persuading a buyer with valuation `v` to buy.\n\n---\n\nWhich of the following statements correctly evaluates this extension strategy and the extensionproofness of the original structure?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze whether the seller can profitably extend the given information structure.\n\nThe original structure gives the seller a profit of 1/4. For extensionproofness, any extension must not increase seller profit beyond this level.\n\nThe seller's profit from extending to price q ∈ (1/4, 1/2) is:\nΠ_ext(q) = q · (1-q + ∫₀^(1/2-q) x(v)dv)\n\nTo determine extensionproofness, I need to find the maximum value of Π_ext(q) over the relevant range.\n\nFor buyer-optimal structures with uniform priors, the standard approach involves pooling lower valuations to minimize seller profit while maintaining incentive compatibility. The structure described (perfect revelation for v > 1/2, pooling at posterior mean 1/4 for v ≤ 1/2) is designed to be buyer-optimal.\n\nThe key insight is that if the seller could profitably extend this structure, it wouldn't be truly buyer-optimal. However, the question asks us to evaluate the extension strategy mathematically.\n\nGiven the functional form and the constraint that q ∈ (1/4, 1/2), the profit function Π_ext(q) will have a maximum somewhere in this interval. \n\nOption C specifically states that this maximum occurs at q = 3/8 with a value of 9/32. Let me verify: 9/32 = 0.28125, which is indeed greater than 1/4 = 0.25.\n\nIf the maximum profit from extension (9/32) exceeds the original profit (1/4), then the structure is not extensionproof.\n\nC", "model_answer": "C", "error": null}}
{"ID": 451, "Question": "### Background\n\nA central challenge in economic modeling is to ensure that the model's core assumptions are grounded in empirical reality. This problem examines how stylized facts about the Brazilian labor market are used to justify key structural assumptions in a two-sector model of informality.\n\n### Data / Model Specification\n\nThe model assumes \"undirected search,\" where unemployed workers form a single pool (`$u$`) and can be matched with firms from either sector. The matching function for each sector `$j$` is thus `$m_j(u, v_j)$`. An alternative is \"directed search,\" where workers must choose to search in either a formal (`$u_F$`) or informal (`$u_I$`) sub-market, with `$u = u_F + u_I$`.\n\nData from the Brazilian labor market shows that flows from unemployment to formal jobs (23.5%) and to informal jobs (a combined 37.9%) are both substantial.\n\n### Question\n\nHow would the primary transmission mechanism of a policy that reduces formal entry costs (`$K_F$`) differ in a directed search model compared to the baseline undirected search model?", "Options": {"A": "In a directed search model, the primary effect would be the reallocation of unemployed workers' search effort from the informal to the more attractive formal market, shrinking the informal search pool (`$u_I$`).", "B": "In a directed search model, the effect would be an increase in formal vacancies (`$v_F$`), which raises the job-finding rate for the entire aggregate pool of unemployed workers (`$u$`).", "C": "In a directed search model, the primary effect would be the reallocation of firms' vacancy posting effort from the informal to the formal market.", "D": "In a directed search model, the policy would have no effect on the informal market, as the two search pools (`$u_F$` and `$u_I$`) are completely separate."}, "Answer": "A", "pi_justification": "This item tests a deep understanding of search theory by asking the user to contrast the model's assumption with a major alternative. It uses a Scenario Application strategy. The distractors are designed to pinpoint specific conceptual misunderstandings: B incorrectly applies the undirected search mechanism to the directed search scenario; C confuses the agent making the reallocation decision (workers, not firms); D misunderstands how markets are linked in a directed search equilibrium.", "qid": "451", "question": "### Background\n\nA central challenge in economic modeling is to ensure that the model's core assumptions are grounded in empirical reality. This problem examines how stylized facts about the Brazilian labor market are used to justify key structural assumptions in a two-sector model of informality.\n\n### Data / Model Specification\n\nThe model assumes \"undirected search,\" where unemployed workers form a single pool (`$u$`) and can be matched with firms from either sector. The matching function for each sector `$j$` is thus `$m_j(u, v_j)$`. An alternative is \"directed search,\" where workers must choose to search in either a formal (`$u_F$`) or informal (`$u_I$`) sub-market, with `$u = u_F + u_I$`.\n\nData from the Brazilian labor market shows that flows from unemployment to formal jobs (23.5%) and to informal jobs (a combined 37.9%) are both substantial.\n\n### Question\n\nHow would the primary transmission mechanism of a policy that reduces formal entry costs (`$K_F$`) differ in a directed search model compared to the baseline undirected search model?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to compare how a policy reducing formal entry costs (K_F) would work differently under directed search versus undirected search models.\n\nIn the baseline undirected search model:\n- All unemployed workers form a single pool (u)\n- Workers can be matched with firms from either sector\n- A reduction in K_F makes formal jobs more attractive, leading to more formal vacancies\n- This benefits all unemployed workers since they're all in the same pool\n\nIn a directed search model:\n- Unemployed workers must choose which sector to search in (u_F or u_I, where u = u_F + u_I)\n- Workers make strategic decisions about where to direct their search effort\n- When K_F falls, formal jobs become more attractive relative to informal jobs\n- This creates an incentive for unemployed workers to reallocate their search effort from the informal market (u_I) to the formal market (u_F)\n\nThe key difference is that in directed search, workers have agency over where they search, so the primary mechanism is the reallocation of search effort by workers themselves.\n\nLooking at the options:\n\nA) Describes the reallocation of unemployed workers' search effort from informal to formal markets, shrinking u_I - this captures the key mechanism in directed search.\n\nB) Describes an increase in formal vacancies affecting the entire unemployed pool - this is more characteristic of undirected search.\n\nC) Focuses on firms reallocating vacancy posting effort - this isn't the primary mechanism we'd expect from reducing entry costs.\n\nD) Claims no effect on informal market due to complete separation - this is incorrect as the markets are connected through workers' search decisions.\n\nA", "model_answer": "A", "error": null}}
{"ID": 442, "Question": "### Background\n\n**Research Question.** This problem provides a complete, worked-out example of a buyer-optimal information structure for the uniform prior case, including a formal proof of its robustness against seller extensions.\n\n**Setting / Institutional Environment.** The buyer's valuation `v` is drawn from a uniform distribution on `[0, 1]`. From this, we can calculate the seller's reservation profit (her payoff under perfect information) as `Π* = max_p p(1-p) = 1/4`, and the maximum social surplus as `E[v] = 1/2`. The regulator's goal is to find an extensionproof information structure that maximizes buyer surplus.\n\n---\n\n### Data / Model Specification\n\nThe proposed buyer-optimal information structure for the uniform case is as follows: if `v > 1/2`, the valuation is perfectly revealed; if `v ≤ 1/2`, valuations are pooled such that the posterior mean is always `1/4`. This induces the following CDF of posterior valuations:\n```latex\nH(w)=\\left\\{\\begin{array}{ll} 0 & \\mathrm{if} \\; w<1/4 \\\\ 1/2 & \\mathrm{if} \\; 1/4 \\leq w \\leq 1/2 \\\\ w & \\mathrm{if} \\; w > 1/2 \\end{array}\\right. \\quad \\text{(Eq. 1)}\n```\n\n---\n\nBased on the induced distribution of posterior valuations `H(w)` in Eq. (1), what are the seller's profit (Π) and the buyer's surplus (U) if the seller sets the optimal price that is most favorable to the buyer?", "Options": {"A": "Π = 1/4, U = 1/4", "B": "Π = 1/4, U = 1/8", "C": "Π = 1/8, U = 3/8", "D": "Π = 1/2, U = 0"}, "Answer": "A", "pi_justification": "This item assesses the ability to calculate equilibrium payoffs from a given distribution of posterior valuations, a foundational skill in this literature. It uses a Computational Judgment strategy. The distractors are designed based on plausible errors: (B) correctly identifies the seller's maximum profit but calculates buyer surplus assuming the seller chooses the higher optimal price (p=1/2); (C) results from a common error in calculating the probability of trade at p=1/4 (assuming it's 1/2); (D) represents a conceptual error where the seller captures all social surplus.", "qid": "442", "question": "### Background\n\n**Research Question.** This problem provides a complete, worked-out example of a buyer-optimal information structure for the uniform prior case, including a formal proof of its robustness against seller extensions.\n\n**Setting / Institutional Environment.** The buyer's valuation `v` is drawn from a uniform distribution on `[0, 1]`. From this, we can calculate the seller's reservation profit (her payoff under perfect information) as `Π* = max_p p(1-p) = 1/4`, and the maximum social surplus as `E[v] = 1/2`. The regulator's goal is to find an extensionproof information structure that maximizes buyer surplus.\n\n---\n\n### Data / Model Specification\n\nThe proposed buyer-optimal information structure for the uniform case is as follows: if `v > 1/2`, the valuation is perfectly revealed; if `v ≤ 1/2`, valuations are pooled such that the posterior mean is always `1/4`. This induces the following CDF of posterior valuations:\n```latex\nH(w)=\\left\\{\\begin{array}{ll} 0 & \\mathrm{if} \\; w<1/4 \\\\ 1/2 & \\mathrm{if} \\; 1/4 \\leq w \\leq 1/2 \\\\ w & \\mathrm{if} \\; w > 1/2 \\end{array}\\right. \\quad \\text{(Eq. 1)}\n```\n\n---\n\nBased on the induced distribution of posterior valuations `H(w)` in Eq. (1), what are the seller's profit (Π) and the buyer's surplus (U) if the seller sets the optimal price that is most favorable to the buyer?", "model_evaluation_result": {"model_raw_response": "I need to analyze the seller's optimal pricing strategy under the given information structure and calculate the resulting profits and buyer surplus.\n\nGiven the CDF H(w) from Eq. (1):\n- H(w) = 0 if w < 1/4\n- H(w) = 1/2 if 1/4 ≤ w ≤ 1/2  \n- H(w) = w if w > 1/2\n\nThis means:\n- With probability 1/2, the posterior valuation is exactly 1/4\n- With probability 1/2, the posterior valuation is uniformly distributed on (1/2, 1]\n\nThe seller chooses a price p to maximize profit Π = p(1 - H(p)).\n\nLet me consider different pricing strategies:\n\n1) If p < 1/4: Π = p(1 - 0) = p → maximized at p approaching 1/4\n2) If 1/4 ≤ p ≤ 1/2: Π = p(1 - 1/2) = p/2 → maximized at p = 1/2, giving Π = 1/4\n3) If p > 1/2: Π = p(1 - p) → maximized at p = 1/2 (boundary), giving Π = 1/4\n\nThe seller's optimal price is p* = 1/2, yielding profit Π = 1/4.\n\nNow for buyer surplus:\n- When posterior valuation is 1/4 and price is 1/2: buyer doesn't purchase (since 1/4 < 1/2), surplus = 0\n- When posterior valuation is w > 1/2 and price is 1/2: buyer purchases, surplus = w - 1/2\n\nExpected buyer surplus:\nU = (1/2) × 0 + (1/2) × E[w - 1/2 | w > 1/2]\n\nFor w uniformly distributed on (1/2, 1]:\nE[w | w > 1/2] = 3/4\n\nTherefore:\nU = (1/2) ×", "model_answer": "I need to analyze the seller's optimal pricing strategy under the given information structure and calculate the resulting profits and buyer surplus.\n\nGiven the CDF H(w) from Eq. (1):\n- H(w) = 0 if w < 1/4\n- H(w) = 1/2 if 1/4 ≤ w ≤ 1/2  \n- H(w) = w if w > 1/2\n\nThis means:\n- With probability 1/2, the posterior valuation is exactly 1/4\n- With probability 1/2, the posterior valuation is uniformly distributed on (1/2, 1]\n\nThe seller chooses a price p to maximize profit Π = p(1 - H(p)).\n\nLet me consider different pricing strategies:\n\n1) If p < 1/4: Π = p(1 - 0) = p → maximized at p approaching 1/4\n2) If 1/4 ≤ p ≤ 1/2: Π = p(1 - 1/2) = p/2 → maximized at p = 1/2, giving Π = 1/4\n3) If p > 1/2: Π = p(1 - p) → maximized at p = 1/2 (boundary), giving Π = 1/4\n\nThe seller's optimal price is p* = 1/2, yielding profit Π = 1/4.\n\nNow for buyer surplus:\n- When posterior valuation is 1/4 and price is 1/2: buyer doesn't purchase (since 1/4 < 1/2), surplus = 0\n- When posterior valuation is w > 1/2 and price is 1/2: buyer purchases, surplus = w - 1/2\n\nExpected buyer surplus:\nU = (1/2) × 0 + (1/2) × E[w - 1/2 | w > 1/2]\n\nFor w uniformly distributed on (1/2, 1]:\nE[w | w > 1/2] = 3/4\n\nTherefore:\nU = (1/2) ×", "error": "Warning: Could not extract single choice letter from: I need to analyze the seller's optimal pricing str..."}}
{"ID": 488, "Question": "### Background\n\nThe study notes that one of the treated districts, Charlotte-Mecklenburg Schools (CMS), implemented several other major policies around the same time as the algebra acceleration, including ending busing for desegregation and introducing a new school choice plan. To address this, the authors perform a robustness check where they re-run their main instrumental variable (IV) model on a sample that omits CMS entirely, using only variation from Guilford County and the control districts.\n\n---\n\n### The Question\n\nWhat specific threat to the IV model's validity is this robustness check primarily designed to address?", "Options": {"A": "A violation of the exclusion restriction, where the instrument (being in a CMS policy-affected cell) might be correlated with changes in student outcomes caused by the confounding policies.", "B": "A weak instrument problem, where the policy change in CMS was not strong enough to reliably predict eighth-grade algebra enrollment.", "C": "A failure of the parallel trends assumption from the study's initial difference-in-differences model.", "D": "A concern about external validity, suggesting the results from CMS might not generalize to other school districts."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to assess understanding of the IV exclusion restriction and how it can be violated in practice. It is based on the original QA's Q3b and Q3c. The correct answer identifies the specific threat: the instrument may be correlated with the error term through a channel other than the treatment. Distractor B points to the wrong IV assumption (relevance, not validity). Distractor C refers to an assumption from a different model (DiD). Distractor D confuses a threat to internal validity (the purpose of the check) with a concern about external validity.", "qid": "488", "question": "### Background\n\nThe study notes that one of the treated districts, Charlotte-Mecklenburg Schools (CMS), implemented several other major policies around the same time as the algebra acceleration, including ending busing for desegregation and introducing a new school choice plan. To address this, the authors perform a robustness check where they re-run their main instrumental variable (IV) model on a sample that omits CMS entirely, using only variation from Guilford County and the control districts.\n\n---\n\n### The Question\n\nWhat specific threat to the IV model's validity is this robustness check primarily designed to address?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 445, "Question": "### Background\n\n**Setting.** We analyze a simplified model where employee effort is a binary choice (`a=0` or `a=1`) and the firm implements `a=1` everywhere, eliminating cumulative loss of control. The wage required to incentivize effort is `w_t = g s_t`, where `g` is the disutility of high effort and `s_t` is the span of control.\n\n---\n\n### Question\n\nAn analysis of a firm's optimal hierarchical design reveals that it pays every employee the same wage, `w_t = ge`, regardless of their tier. Which of the following modeling assumptions is necessary to generate this egalitarian wage outcome?", "Options": {"A": "The total number of tiers `T` is a choice variable for the firm.", "B": "The total number of tiers `T` is exogenously fixed.", "C": "Workers are highly productive, such that their output `\\theta` exceeds `ge`.", "D": "The firm experiences a significant cumulative loss of control."}, "Answer": "A", "pi_justification": "This item assesses the student's understanding of the critical role of modeling assumptions in determining hierarchical structure. It uses a Reverse-Reasoning strategy: given the outcome (egalitarian wages), the student must infer the necessary condition (endogenous T). Distractors are designed to be high-fidelity: (B) is the direct conceptual opposite which leads to decreasing wages, (C) is a true but irrelevant condition from the model, and (D) incorrectly imports a feature from the paper's general model.", "qid": "445", "question": "### Background\n\n**Setting.** We analyze a simplified model where employee effort is a binary choice (`a=0` or `a=1`) and the firm implements `a=1` everywhere, eliminating cumulative loss of control. The wage required to incentivize effort is `w_t = g s_t`, where `g` is the disutility of high effort and `s_t` is the span of control.\n\n---\n\n### Question\n\nAn analysis of a firm's optimal hierarchical design reveals that it pays every employee the same wage, `w_t = ge`, regardless of their tier. Which of the following modeling assumptions is necessary to generate this egalitarian wage outcome?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine what assumption leads to equal wages across all tiers.\n\nGiven information:\n- The wage formula is w_t = g s_t (where g is disutility of effort, s_t is span of control)\n- The outcome is w_t = ge for all tiers (equal wages)\n- This implies s_t = e for all tiers (constant span of control)\n\nLet me analyze each option:\n\n**Option A: T is a choice variable**\nIf the firm can choose the number of tiers optimally, it would typically adjust both the span of control and number of tiers to minimize costs. This flexibility would likely lead to varying spans of control across tiers, not the constant span needed for equal wages.\n\n**Option B: T is exogenously fixed**\nIf the number of tiers is fixed externally, the firm loses a key degree of freedom in its optimization. With T constrained, the firm might find it optimal to set a uniform span of control across all tiers. This constraint could force the egalitarian outcome where s_t = e for all t, leading to equal wages.\n\n**Option C: High worker productivity (θ > ge)**\nThis affects the profitability of the arrangement but doesn't directly determine the wage structure across tiers. High productivity makes the arrangement viable but doesn't explain why wages would be equal across tiers.\n\n**Option D: Significant cumulative loss of control**\nThe problem explicitly states this is eliminated in the model, so this cannot be the driving factor.\n\nThe key insight is that equal wages require equal spans of control across tiers. This uniform structure is most likely to emerge when the firm's optimization is constrained by having a fixed number of tiers, removing the flexibility that would typically lead to tier-specific optimization.\n\nB", "model_answer": "A", "error": null}}
{"ID": 484, "Question": "### Background\n\n**Research Question.** This question asks to interpret the prior beliefs encoded in the shape of the Maximal Data Information Prior (MDIP) for a binomial proportion, `θ`.\n\n### Data / Model Specification\n\nThe MDIP for a binomial proportion `θ` is given by `π*(θ) ∝ θ^θ (1-θ)^(1-θ)`. This density is symmetric and U-shaped, with a minimum at `θ=0.5` and finite peaks at `θ=0` and `θ=1`.\n\nThe U-shaped Maximal Data Information Prior (MDIP) for a binomial proportion `θ` contrasts with the flat Bayes-Laplace (uniform) prior. What prior belief does the MDIP's shape represent?", "Options": {"A": "A belief that the process is likely polarized, assigning higher probability to outcomes being either consistently successes (`θ` near 1) or consistently failures (`θ` near 0).", "B": "A belief that the process is most likely balanced, assigning the highest probability to `θ` being near 0.5.", "C": "A belief that the prior should be invariant to reparameterization, a key property of Jeffreys' prior.", "D": "A state of maximal ignorance, where all values of `θ` are considered equally likely."}, "Answer": "A", "pi_justification": "This item assesses the student's ability to interpret the meaning of a prior distribution's shape, a critical skill in applied Bayesian analysis. It uses a Conceptual Judgment strategy. Distractors are designed to test for common confusions: B represents the opposite belief (a bell-shaped prior), C correctly states a property of a different prior (Jeffreys), and D describes the uniform prior, which the question explicitly contrasts against.", "qid": "484", "question": "### Background\n\n**Research Question.** This question asks to interpret the prior beliefs encoded in the shape of the Maximal Data Information Prior (MDIP) for a binomial proportion, `θ`.\n\n### Data / Model Specification\n\nThe MDIP for a binomial proportion `θ` is given by `π*(θ) ∝ θ^θ (1-θ)^(1-θ)`. This density is symmetric and U-shaped, with a minimum at `θ=0.5` and finite peaks at `θ=0` and `θ=1`.\n\nThe U-shaped Maximal Data Information Prior (MDIP) for a binomial proportion `θ` contrasts with the flat Bayes-Laplace (uniform) prior. What prior belief does the MDIP's shape represent?", "model_evaluation_result": {"model_raw_response": "Looking at the MDIP π*(θ) ∝ θ^θ (1-θ)^(1-θ), I need to analyze what this U-shaped distribution represents in terms of prior beliefs.\n\nKey characteristics given:\n- U-shaped density\n- Symmetric\n- Minimum at θ = 0.5\n- Finite peaks at θ = 0 and θ = 1\n\nLet me evaluate each option:\n\nA) This describes a belief in polarization - higher probability for extreme values (θ near 0 or 1) and lower probability for moderate values (θ near 0.5). This matches the U-shape perfectly.\n\nB) This suggests highest probability near θ = 0.5, which contradicts the stated minimum at θ = 0.5.\n\nC) This describes Jeffreys' prior properties, but the question is specifically about interpreting the MDIP's shape, not its invariance properties.\n\nD) This describes a uniform prior (maximal ignorance), but the MDIP is explicitly contrasted with the uniform Bayes-Laplace prior and has a distinctive U-shape rather than being flat.\n\nThe U-shaped nature with peaks at the extremes (0 and 1) and a minimum at the center (0.5) directly encodes a belief that the binomial process is likely to be polarized - either consistently producing successes or consistently producing failures, rather than being balanced.\n\nA", "model_answer": "A", "error": null}}
{"ID": 475, "Question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. Defecting is a dominant strategy in the stage game, but cooperating (`$C_1, C_2$`) is the socially efficient outcome.\n\n### Data / Model Specification\n\nThe monitoring structure has the following properties:\n- Player 2's effort (`$C_2$` vs `$D_2$`) has a state-dependent effect. In state `$\\omega_1$`, effort is more productive than in state `$\\omega_2$`. \n- If Player 2 chooses to Defect (`$D_2$`), the public signal distribution is the same for both states, regardless of Player 1's action.\n\nConsider the case where **only Player 2 knows the state** (`$\\Theta_2 = \\{(\\omega_1), (\\omega_2)\\}$`), while Player 1 does not (`$\\Theta_1 = \\{(\\omega_1, \\omega_2)\\}$`).\n\n### Question\n\nIn this scenario, if the informed Player 2 chooses to Defect (`$D_2$`), can the uninformed Player 1 learn the true state of the world from the public signals?", "Options": {"A": "Yes, because Player 1's own actions still generate signals that depend on the state.", "B": "No, because when Player 2 defects, the resulting public signal distribution becomes independent of the state.", "C": "Yes, because Player 2's choice to defect is itself a credible signal of the state.", "D": "No, because Player 1 has no private information and therefore can never learn."}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the core information-jamming mechanic of the partnership game. It uses a Scenario Application strategy. The correct answer identifies the specific action that renders the signal uninformative. Distractors include a Conceptual Error (misattributing state-dependence to Player 1's actions), another Conceptual Error (misinterpreting defection as a revealing signal), and a True but Irrelevant reason.", "qid": "475", "question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. Defecting is a dominant strategy in the stage game, but cooperating (`$C_1, C_2$`) is the socially efficient outcome.\n\n### Data / Model Specification\n\nThe monitoring structure has the following properties:\n- Player 2's effort (`$C_2$` vs `$D_2$`) has a state-dependent effect. In state `$\\omega_1$`, effort is more productive than in state `$\\omega_2$`. \n- If Player 2 chooses to Defect (`$D_2$`), the public signal distribution is the same for both states, regardless of Player 1's action.\n\nConsider the case where **only Player 2 knows the state** (`$\\Theta_2 = \\{(\\omega_1), (\\omega_2)\\}$`), while Player 1 does not (`$\\Theta_1 = \\{(\\omega_1, \\omega_2)\\}$`).\n\n### Question\n\nIn this scenario, if the informed Player 2 chooses to Defect (`$D_2$`), can the uninformed Player 1 learn the true state of the world from the public signals?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze what happens to the public signal distribution when Player 2 defects.\n\nThe key information provided is:\n- Player 2's effort has state-dependent effects (more productive in ω₁ than ω₂)\n- **Crucially**: \"If Player 2 chooses to Defect (D₂), the public signal distribution is the same for both states, regardless of Player 1's action\"\n\nThis last point is the critical constraint. When Player 2 defects, the monitoring structure is designed such that the public signals generated are identical across both states ω₁ and ω₂, no matter what Player 1 does.\n\nLet me evaluate each option:\n\nA) This suggests Player 1's actions generate state-dependent signals, but the problem explicitly states that when Player 2 defects, signals are the same for both states \"regardless of Player 1's action.\"\n\nB) This directly matches the given constraint - when Player 2 defects, the public signal distribution becomes state-independent.\n\nC) While Player 2's defection choice might correlate with the state, the question asks specifically about learning from public signals, not from Player 2's action itself.\n\nD) This is incorrect because having private information isn't necessary to learn from public signals.\n\nThe problem explicitly states that defection by Player 2 makes the public signal distribution identical across states. This means the signals contain no information that would allow Player 1 to distinguish between ω₁ and ω₂.\n\nB", "model_answer": "A", "error": null}}
{"ID": 483, "Question": "### Background\n\n**Research Question.** This question concerns the application of the Maximal Data Information Prior (MDIP) framework to derive a prior for a binomial proportion, `θ`.\n\n### Data / Model Specification\n\nThe probability mass function (PMF) for a single Bernoulli trial with success probability `θ` is:\n\n```latex\np(y|\\theta) = \\theta^y (1-\\theta)^{1-y}, \\quad y \\in \\{0, 1\\}\n```\n\nThe Shannon information `I(θ)` is the expected value of the log-density, `I(θ) = E[ln p(y|θ)] = θ ln(θ) + (1-θ) ln(1-θ)`. The general solution for the MDIP is `π*(θ) ∝ exp(I(θ))`.\n\nBased on the provided specifications, what is the functional form of the Maximal Data Information Prior (MDIP), `π*(θ)`, for the binomial proportion `θ`?", "Options": {"A": "```latex\n\\theta^\\theta (1-\\theta)^{1-\\theta}\n```", "B": "```latex\n\\theta \\ln(\\theta) + (1-\\theta) \\ln(1-\\theta)\n```", "C": "```latex\n\\theta^{-1/2} (1-\\theta)^{-1/2}\n```", "D": "```latex\n\\theta^\\theta + (1-\\theta)^{1-\\theta}\n```"}, "Answer": "A", "pi_justification": "This item assesses the student's ability to perform the two-step derivation of the MDIP for a binomial proportion. It uses a Computational Judgment strategy. The distractors are designed to catch common errors: B is the intermediate information measure `I(θ)` instead of `exp(I(θ))`, C is a conceptually related but distinct prior (Jeffreys), and D represents a frequent algebraic mistake in exponentiation.", "qid": "483", "question": "### Background\n\n**Research Question.** This question concerns the application of the Maximal Data Information Prior (MDIP) framework to derive a prior for a binomial proportion, `θ`.\n\n### Data / Model Specification\n\nThe probability mass function (PMF) for a single Bernoulli trial with success probability `θ` is:\n\n```latex\np(y|\\theta) = \\theta^y (1-\\theta)^{1-y}, \\quad y \\in \\{0, 1\\}\n```\n\nThe Shannon information `I(θ)` is the expected value of the log-density, `I(θ) = E[ln p(y|θ)] = θ ln(θ) + (1-θ) ln(1-θ)`. The general solution for the MDIP is `π*(θ) ∝ exp(I(θ))`.\n\nBased on the provided specifications, what is the functional form of the Maximal Data Information Prior (MDIP), `π*(θ)`, for the binomial proportion `θ`?", "model_evaluation_result": {"model_raw_response": "I need to find the functional form of the Maximal Data Information Prior (MDIP) for the binomial proportion θ.\n\nGiven information:\n- The Shannon information is: I(θ) = θ ln(θ) + (1-θ) ln(1-θ)\n- The MDIP is: π*(θ) ∝ exp(I(θ))\n\nTo find π*(θ), I need to substitute the Shannon information into the exponential:\n\nπ*(θ) ∝ exp(I(θ))\nπ*(θ) ∝ exp(θ ln(θ) + (1-θ) ln(1-θ))\n\nUsing the property that exp(a + b) = exp(a) × exp(b):\n\nπ*(θ) ∝ exp(θ ln(θ)) × exp((1-θ) ln(1-θ))\n\nUsing the property that exp(a ln(b)) = b^a:\n\nπ*(θ) ∝ θ^θ × (1-θ)^(1-θ)\n\nTherefore:\nπ*(θ) ∝ θ^θ (1-θ)^(1-θ)\n\nLooking at the options:\n- Option A: θ^θ (1-θ)^(1-θ) - This matches my derivation\n- Option B: θ ln(θ) + (1-θ) ln(1-θ) - This is I(θ), not exp(I(θ))\n- Option C: θ^(-1/2) (1-θ)^(-1/2) - This is the Jeffreys prior, not MDIP\n- Option D: θ^θ + (1-θ)^(1-θ) - This uses addition instead of multiplication\n\nA", "model_answer": "A", "error": null}}
{"ID": 497, "Question": "### Background\n\n**Research Question.** This problem analyzes experimental evidence from the Dirty Faces game to assess whether group behavior conforms to the theoretical equilibrium, particularly as the game's complexity (number of players `n`) and the required level of iterated reasoning (`k-1` steps) increase.\n\n**Setting.** An experiment is conducted on the Dirty Faces game with two different group sizes: `n=2` and `n=3`. At the start of each round, it is publicly announced that at least one player has type 'X'. Group behavior is then compared to the theoretical predictions of the unique Nash Equilibrium.\n\n**Variables and Parameters.**\n- `n`: Number of players (2 or 3).\n- `k`: Number of players with type 'X'.\n- `Predicted Behavior`: The sequence of actions corresponding to the unique Nash equilibrium.\n\n### Data / Model Specification\n\nThe theoretical equilibrium requires `k-1` levels of iterated knowledge of rationality, where `k` is the number of players with type 'X'. The experimental results for group-level behavior are summarized in Table 1 (for `n=2`) and Table 2 (for `n=3`).\n\n**Table 1. Summary of Group Results for n=2**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XO | 8 | (DU) | 7 | 0.88 |\n| XX | 18 | ((UU)(DD)) | 4 | 0.22 |\n| **Total** | **26** | | **11** | **0.42** |\n\n**Table 2. Summary of Group Results for n=3**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XOO | 6 | (DUU) | 3 | 0.50 |\n| XXO | 10 | (UUU)(DDU) | 1 | 0.10 |\n| XXX | 12 | (UUU)(UUU)(DDD) | 0 | 0.00 |\n| **Total** | **28** | | **4** | **0.14** |\n\n### Question\n\nBased on the data in Tables 1 and 2, what is the most accurate conclusion about how group conformity to the theoretical equilibrium changes with the required level of iterated reasoning?", "Options": {"A": "Conformity is high when only base rationality is required but drops sharply with each additional level of iterated reasoning, suggesting higher-order beliefs are the primary barrier.", "B": "Conformity declines primarily as the number of players (n) increases, but is largely independent of the number of 'X' types (k) within a given group size.", "C": "Groups are more likely to conform to the equilibrium in more complex situations (higher k) because the potential payoffs from coordinating are greater.", "D": "The overall success rate is low in both the n=2 and n=3 games, indicating that subjects are generally not rational, regardless of the game's complexity."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question assesses the core empirical finding of the paper by asking for a comparative interpretation of the data. It uses a Scenario Application strategy. The correct answer (A) accurately synthesizes the key trend. Distractors are designed based on plausible misinterpretations: (B) confuses group size (n) with reasoning steps (k), (C) is a conceptual opposite of the findings, and (D) offers a true but overly simplistic conclusion that misses the crucial gradient of failure.", "qid": "497", "question": "### Background\n\n**Research Question.** This problem analyzes experimental evidence from the Dirty Faces game to assess whether group behavior conforms to the theoretical equilibrium, particularly as the game's complexity (number of players `n`) and the required level of iterated reasoning (`k-1` steps) increase.\n\n**Setting.** An experiment is conducted on the Dirty Faces game with two different group sizes: `n=2` and `n=3`. At the start of each round, it is publicly announced that at least one player has type 'X'. Group behavior is then compared to the theoretical predictions of the unique Nash Equilibrium.\n\n**Variables and Parameters.**\n- `n`: Number of players (2 or 3).\n- `k`: Number of players with type 'X'.\n- `Predicted Behavior`: The sequence of actions corresponding to the unique Nash equilibrium.\n\n### Data / Model Specification\n\nThe theoretical equilibrium requires `k-1` levels of iterated knowledge of rationality, where `k` is the number of players with type 'X'. The experimental results for group-level behavior are summarized in Table 1 (for `n=2`) and Table 2 (for `n=3`).\n\n**Table 1. Summary of Group Results for n=2**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XO | 8 | (DU) | 7 | 0.88 |\n| XX | 18 | ((UU)(DD)) | 4 | 0.22 |\n| **Total** | **26** | | **11** | **0.42** |\n\n**Table 2. Summary of Group Results for n=3**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XOO | 6 | (DUU) | 3 | 0.50 |\n| XXO | 10 | (UUU)(DDU) | 1 | 0.10 |\n| XXX | 12 | (UUU)(UUU)(DDD) | 0 | 0.00 |\n| **Total** | **28** | | **4** | **0.14** |\n\n### Question\n\nBased on the data in Tables 1 and 2, what is the most accurate conclusion about how group conformity to the theoretical equilibrium changes with the required level of iterated reasoning?", "model_evaluation_result": {"model_raw_response": "Looking at the data systematically:\n\n**For n=2:**\n- XO (k=1, requires 0 levels of iterated reasoning): 88% conformity\n- XX (k=2, requires 1 level of iterated reasoning): 22% conformity\n\n**For n=3:**\n- XOO (k=1, requires 0 levels of iterated reasoning): 50% conformity  \n- XXO (k=2, requires 1 level of iterated reasoning): 10% conformity\n- XXX (k=3, requires 2 levels of iterated reasoning): 0% conformity\n\nThe pattern is clear: conformity decreases dramatically as the required level of iterated reasoning increases. From 0 to 1 level of iterated reasoning, conformity drops from 88% to 22% (n=2) and from 50% to 10% (n=3). From 1 to 2 levels, it drops further to 0%.\n\nThis shows that while groups can handle situations requiring only base rationality reasonably well, they struggle significantly when higher-order beliefs about others' rationality are required. The sharp decline with each additional level of iterated reasoning indicates that the complexity of reasoning about what others know about what others know creates the primary barrier to equilibrium play.\n\nA", "model_answer": "A", "error": null}}
{"ID": 457, "Question": "### Background\n\n**Research Question.** This problem assesses the implications of a violated identifying assumption in a structural model.\n\n**Setting / Institutional Environment.** A structural model of farmer behavior assumes **Rational Expectations**, meaning the farmer's subjective belief about the variance of production shocks (`σ_p^2`) is equal to the objective sample variance (`σ̂^2`) estimated from the data. This assumption is used to separately identify the risk aversion parameter, `R`.\n\n### The Question\n\nSuppose the true agricultural environment has recently become much riskier (true variance `σ_true^2` is high), but farmers' beliefs are 'sticky' and lag behind, so they still perceive the environment as relatively safe (`σ_p^2` is low). An econometrician, observing data from this new, risky environment, correctly estimates the high sample variance (`σ̂^2 ≈ σ_true^2`) and imposes the rational expectations assumption (`σ_p^2 = σ̂^2`) to estimate risk aversion.\n\nGiven this misspecification, how will the estimated risk aversion parameter, `R̂`, be biased relative to the farmers' true risk aversion, `R_true`?", "Options": {"A": "`R̂` will be biased downwards (underestimated).", "B": "`R̂` will be biased upwards (overestimated).", "C": "`R̂` will be unbiased because the econometrician used a consistent estimate of the true variance.", "D": "The direction of the bias cannot be determined without knowing the specific value of the farmers' beliefs."}, "Answer": "A", "pi_justification": "This item tests deep understanding of identification and model misspecification bias using a Reverse-Reasoning strategy. The student must reason from a violation of the rational expectations assumption back to its effect on a parameter estimate. The correct answer (A) requires this logical chain: the farmer's behavior is less cautious than the model would predict for a truly risk-averse person facing the high risk the econometrician assumes. To rationalize this 'brave' behavior, the model must conclude the farmer is less risk-averse than they truly are. The primary distractor (B) represents the opposite, and most common, reasoning error. Distractor (C) tests for a superficial understanding of statistical consistency, while (D) is for students who fail to see that the direction of bias is determinate.", "qid": "457", "question": "### Background\n\n**Research Question.** This problem assesses the implications of a violated identifying assumption in a structural model.\n\n**Setting / Institutional Environment.** A structural model of farmer behavior assumes **Rational Expectations**, meaning the farmer's subjective belief about the variance of production shocks (`σ_p^2`) is equal to the objective sample variance (`σ̂^2`) estimated from the data. This assumption is used to separately identify the risk aversion parameter, `R`.\n\n### The Question\n\nSuppose the true agricultural environment has recently become much riskier (true variance `σ_true^2` is high), but farmers' beliefs are 'sticky' and lag behind, so they still perceive the environment as relatively safe (`σ_p^2` is low). An econometrician, observing data from this new, risky environment, correctly estimates the high sample variance (`σ̂^2 ≈ σ_true^2`) and imposes the rational expectations assumption (`σ_p^2 = σ̂^2`) to estimate risk aversion.\n\nGiven this misspecification, how will the estimated risk aversion parameter, `R̂`, be biased relative to the farmers' true risk aversion, `R_true`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze how the misspecification of the rational expectations assumption affects the estimation of the risk aversion parameter.\n\nLet me break down the situation:\n- True environment: high variance (σ²_true is high)\n- Farmers' beliefs: low perceived variance (σ²_p is low) due to sticky beliefs\n- Econometrician's estimate: correctly estimates high sample variance (σ̂² ≈ σ²_true)\n- Misspecification: Econometrician assumes σ²_p = σ̂² (rational expectations), but actually σ²_p < σ̂²\n\nIn structural models of farmer behavior under uncertainty, risk aversion typically enters through the farmer's optimization problem where they balance expected returns against perceived risk. The key insight is that farmers make decisions based on their perceived variance (σ²_p), not the true variance.\n\nWhen the econometrician observes farmer behavior (which was based on low perceived risk σ²_p) but assumes farmers perceived the high variance σ̂², the econometrician will interpret the farmers' risk-taking behavior as indicating lower risk aversion than it actually represents.\n\nSpecifically:\n- Farmers behaved as if facing low risk (σ²_p), so their observed choices reflect their true risk aversion R_true under that perceived risk level\n- The econometrician sees this same behavior but attributes it to farmers facing high risk (σ̂²)\n- To rationalize the same level of risk-taking under higher perceived risk, the model requires lower risk aversion\n- Therefore, R̂ < R_true\n\nThe estimated risk aversion will be biased downwards (underestimated).\n\nA", "model_answer": "A", "error": null}}
{"ID": 504, "Question": "### Background\n\nA key testable implication of the Gaussian term structure model is that the factor loading function `b(τ)` should be the same whether estimated from yield levels `Y_t(τ)` or from forward differences `Ỹ_t(τ) = Y_t(τ) - F_{t-h}(τ)`.\n\n### Data / Model Specification\n\nThe model implies that `Y_t(τ) = a(τ) + b(τ)'z_t` (Eq. 1), where the factor loading for factor `i` is given by:\n```latex\nb_i(τ) = \\frac{1 - e^{-κ_i τ}}{κ_i τ} \\quad \\text{(Eq. 2)}\n```\nThe forward rate `F_{t-h}(τ)` is defined as:\n```latex\nF_{t-h}(τ) = \\frac{τ+h}{τ}Y_{t-h}(τ+h) - \\frac{h}{τ}Y_{t-h}(h) \\quad \\text{(Eq. 3)}\n```\nUsing these equations, it can be shown that the factor-dependent part of `Ỹ_t(τ)` relies on the following identity:\n```latex\n\\frac{τ+h}{τ}b_i(τ+h) - \\frac{h}{τ}b_i(h) = e^{-κ_i h}b_i(τ) \\quad \\text{(Eq. 4)}\n```\n\n### Question\n\nGiven the identity in Eq. (4), the factor-dependent part of the forward difference `Ỹ_t(τ)` can be written as `b(τ)'z̃_t`. What is the correct expression for the `i`-th component of the transformed factor, `z̃_{it}`?", "Options": {"A": "`z_{it} - e^{-κ_i h}z_{i,t-h}`", "B": "`z_{it} - z_{i,t-h}`", "C": "`e^{-κ_i h}z_{it} - z_{i,t-h}`", "D": "`z_{it} - b_i(h)z_{i,t-h}`"}, "Answer": "A", "pi_justification": "This item tests the student's ability to complete the derivation of the model's key overidentifying restriction. It uses a Derivational Judgment strategy, focusing on the final algebraic step. The distractors are based on plausible errors in applying the given identity: (B) omits the crucial decay factor, (C) misapplies the decay factor, and (D) substitutes an incorrect but conceptually related term.", "qid": "504", "question": "### Background\n\nA key testable implication of the Gaussian term structure model is that the factor loading function `b(τ)` should be the same whether estimated from yield levels `Y_t(τ)` or from forward differences `Ỹ_t(τ) = Y_t(τ) - F_{t-h}(τ)`.\n\n### Data / Model Specification\n\nThe model implies that `Y_t(τ) = a(τ) + b(τ)'z_t` (Eq. 1), where the factor loading for factor `i` is given by:\n```latex\nb_i(τ) = \\frac{1 - e^{-κ_i τ}}{κ_i τ} \\quad \\text{(Eq. 2)}\n```\nThe forward rate `F_{t-h}(τ)` is defined as:\n```latex\nF_{t-h}(τ) = \\frac{τ+h}{τ}Y_{t-h}(τ+h) - \\frac{h}{τ}Y_{t-h}(h) \\quad \\text{(Eq. 3)}\n```\nUsing these equations, it can be shown that the factor-dependent part of `Ỹ_t(τ)` relies on the following identity:\n```latex\n\\frac{τ+h}{τ}b_i(τ+h) - \\frac{h}{τ}b_i(h) = e^{-κ_i h}b_i(τ) \\quad \\text{(Eq. 4)}\n```\n\n### Question\n\nGiven the identity in Eq. (4), the factor-dependent part of the forward difference `Ỹ_t(τ)` can be written as `b(τ)'z̃_t`. What is the correct expression for the `i`-th component of the transformed factor, `z̃_{it}`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to determine how the factors transform when we go from yield levels to forward differences.\n\nGiven:\n- Yield levels: `Y_t(τ) = a(τ) + b(τ)'z_t`\n- Forward differences: `Ỹ_t(τ) = Y_t(τ) - F_{t-h}(τ)`\n- The key identity: `(τ+h)/τ · b_i(τ+h) - h/τ · b_i(h) = e^{-κ_i h} · b_i(τ)`\n\nLet me work through the algebra:\n\nStarting with the forward difference:\n`Ỹ_t(τ) = Y_t(τ) - F_{t-h}(τ)`\n\nSubstituting the expressions:\n`Ỹ_t(τ) = [a(τ) + b(τ)'z_t] - [(τ+h)/τ · Y_{t-h}(τ+h) - h/τ · Y_{t-h}(h)]`\n\n`= [a(τ) + b(τ)'z_t] - [(τ+h)/τ · (a(τ+h) + b(τ+h)'z_{t-h}) - h/τ · (a(h) + b(h)'z_{t-h})]`\n\nFocusing on the factor-dependent terms:\n`b(τ)'z_t - (τ+h)/τ · b(τ+h)'z_{t-h} + h/τ · b(h)'z_{t-h}`\n\n`= b(τ)'z_t - [(τ+h)/τ · b(τ+h) - h/τ · b(h)]'z_{t-h}`\n\nUsing the identity from Eq. (4):\n`(τ+h)/τ · b_i(τ+h) - h/τ · b_i(h) = e^{-κ_i h} · b_i(τ)`", "model_answer": "Looking at this problem, I need to determine how the factors transform when we go from yield levels to forward differences.\n\nGiven:\n- Yield levels: `Y_t(τ) = a(τ) + b(τ)'z_t`\n- Forward differences: `Ỹ_t(τ) = Y_t(τ) - F_{t-h}(τ)`\n- The key identity: `(τ+h)/τ · b_i(τ+h) - h/τ · b_i(h) = e^{-κ_i h} · b_i(τ)`\n\nLet me work through the algebra:\n\nStarting with the forward difference:\n`Ỹ_t(τ) = Y_t(τ) - F_{t-h}(τ)`\n\nSubstituting the expressions:\n`Ỹ_t(τ) = [a(τ) + b(τ)'z_t] - [(τ+h)/τ · Y_{t-h}(τ+h) - h/τ · Y_{t-h}(h)]`\n\n`= [a(τ) + b(τ)'z_t] - [(τ+h)/τ · (a(τ+h) + b(τ+h)'z_{t-h}) - h/τ · (a(h) + b(h)'z_{t-h})]`\n\nFocusing on the factor-dependent terms:\n`b(τ)'z_t - (τ+h)/τ · b(τ+h)'z_{t-h} + h/τ · b(h)'z_{t-h}`\n\n`= b(τ)'z_t - [(τ+h)/τ · b(τ+h) - h/τ · b(h)]'z_{t-h}`\n\nUsing the identity from Eq. (4):\n`(τ+h)/τ · b_i(τ+h) - h/τ · b_i(h) = e^{-κ_i h} · b_i(τ)`", "error": "Warning: Could not extract single choice letter from: Looking at this problem, I need to determine how t..."}}
{"ID": 468, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical micro-foundations of a model for measuring differences in real income across households. The model posits that economic well-being is derived from unobserved 'service flows' (e.g., entertainment) which are 'produced' by observable market goods (e.g., a TV). The efficiency of this production process is the key parameter that varies with household structure.\n\n**Setting / Institutional Environment.** Individuals demand service flows, which have an unobserved 'effective price'. This price depends on both the market price of the input good and the household's efficiency in transforming that good into a service. A change in household structure (e.g., two people forming a couple) is modeled as a shock to this transformation efficiency.\n\n**Variables & Parameters (Exhaustive and Standardized).**\n- `$S_1$`: The quantity of a service flow (e.g., entertainment services).\n- `$X_1$`: The quantity of a purchased market good (e.g., a TV set).\n- `$\\alpha_1$`: The rate of transformation, or average product, of good `$X_1$` into service flow `$S_1$`. Units: service units per unit of good.\n- `$P_1$`: The market price of the good `$X_1$`. Units: dollars per unit of good.\n- `$P_{s1}$`: The effective or implicit price of the service flow `$S_1$`. Units: dollars per service unit.\n- `$Y$`: An individual's nominal money income. Units: dollars.\n- `$J_1$`: The percentage change in `$\\alpha_1$` due to a change in household environment. Dimensionless.\n- `$\\eta_{S_1, P_{s1}}$`: The price elasticity of demand for the service flow `$S_1$`.\n\n---\n\n### Data / Model Specification\n\nThe model is defined by the following relationships:\n```latex\n\\alpha_{1} \\equiv S_{1}/X_{1} \\quad \\text{(Eq. (1))}\n```\n```latex\nP_{s1} = P_{1}/\\alpha_{1} \\quad \\text{(Eq. (2))}\n```\n```latex\nS_{1} = d(P_{s1}, Y) \\quad \\text{(Eq. (3))}\n```\nA change in household environment alters the transformation rate from `$\\alpha_1$` to `$\\alpha_1(1+J_1)$`.\n\n---\n\n### The Questions\n\nSuppose the demand for the service flow `$S_1$` is known to be price-elastic (i.e., `$\\eta_{S_1, P_{s1}} < -1$`). According to the model, what is the effect of an increase in household production efficiency (a rise in `$\\alpha_1$`) on the quantity demanded of the observable market good `$X_1$`?", "Options": {"A": "It increases.", "B": "It decreases.", "C": "It remains unchanged.", "D": "The effect is ambiguous without more information."}, "Answer": "A", "pi_justification": "This question tests the application of the model's theoretical results to a specific economic scenario, a strategy known as Scenario Application. It specifically targets the counter-intuitive 'rebound effect'. The distractors are designed to be highly plausible: (B) is the intuitive but incorrect answer for the elastic case, (C) represents the knife-edge case (`$\\eta = -1$`), and (D) is a common fallback for students who are unable to derive a definitive result.", "qid": "468", "question": "### Background\n\n**Research Question.** This problem explores the theoretical micro-foundations of a model for measuring differences in real income across households. The model posits that economic well-being is derived from unobserved 'service flows' (e.g., entertainment) which are 'produced' by observable market goods (e.g., a TV). The efficiency of this production process is the key parameter that varies with household structure.\n\n**Setting / Institutional Environment.** Individuals demand service flows, which have an unobserved 'effective price'. This price depends on both the market price of the input good and the household's efficiency in transforming that good into a service. A change in household structure (e.g., two people forming a couple) is modeled as a shock to this transformation efficiency.\n\n**Variables & Parameters (Exhaustive and Standardized).**\n- `$S_1$`: The quantity of a service flow (e.g., entertainment services).\n- `$X_1$`: The quantity of a purchased market good (e.g., a TV set).\n- `$\\alpha_1$`: The rate of transformation, or average product, of good `$X_1$` into service flow `$S_1$`. Units: service units per unit of good.\n- `$P_1$`: The market price of the good `$X_1$`. Units: dollars per unit of good.\n- `$P_{s1}$`: The effective or implicit price of the service flow `$S_1$`. Units: dollars per service unit.\n- `$Y$`: An individual's nominal money income. Units: dollars.\n- `$J_1$`: The percentage change in `$\\alpha_1$` due to a change in household environment. Dimensionless.\n- `$\\eta_{S_1, P_{s1}}$`: The price elasticity of demand for the service flow `$S_1$`.\n\n---\n\n### Data / Model Specification\n\nThe model is defined by the following relationships:\n```latex\n\\alpha_{1} \\equiv S_{1}/X_{1} \\quad \\text{(Eq. (1))}\n```\n```latex\nP_{s1} = P_{1}/\\alpha_{1} \\quad \\text{(Eq. (2))}\n```\n```latex\nS_{1} = d(P_{s1}, Y) \\quad \\text{(Eq. (3))}\n```\nA change in household environment alters the transformation rate from `$\\alpha_1$` to `$\\alpha_1(1+J_1)$`.\n\n---\n\n### The Questions\n\nSuppose the demand for the service flow `$S_1$` is known to be price-elastic (i.e., `$\\eta_{S_1, P_{s1}} < -1$`). According to the model, what is the effect of an increase in household production efficiency (a rise in `$\\alpha_1$`) on the quantity demanded of the observable market good `$X_1$`?", "model_evaluation_result": {"model_raw_response": "I need to analyze how an increase in household production efficiency (α₁) affects the quantity demanded of the market good X₁.\n\nLet me work through this step by step:\n\n1) From the given relationships:\n   - α₁ ≡ S₁/X₁ (Eq. 1)\n   - P_{s1} = P₁/α₁ (Eq. 2)\n   - S₁ = d(P_{s1}, Y) (Eq. 3)\n\n2) When α₁ increases:\n   - From Eq. 2: P_{s1} = P₁/α₁ decreases (since α₁ is in the denominator)\n   - A decrease in the effective price P_{s1} of the service flow\n\n3) Given that demand for S₁ is price-elastic (η_{S₁,P_{s1}} < -1):\n   - When P_{s1} decreases, the quantity demanded S₁ increases\n   - Since demand is elastic, the percentage increase in S₁ is greater than the percentage decrease in P_{s1}\n\n4) Now I need to find the effect on X₁:\n   - From Eq. 1: X₁ = S₁/α₁\n   - When α₁ increases and S₁ increases, the net effect on X₁ depends on which effect dominates\n\n5) Let's consider the elasticity condition more carefully:\n   - Since η_{S₁,P_{s1}} < -1, when P_{s1} decreases by some percentage, S₁ increases by a larger percentage\n   - From P_{s1} = P₁/α₁, a percentage increase in α₁ causes the same percentage decrease in P_{s1}\n   - So if α₁ increases by x%, then P_{s1} decreases by x%, and S₁ increases by more than x%\n   - Therefore, X₁ = S₁/α₁ increases because S₁ increases by more than α₁ increases\n\nThe answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 502, "Question": "### Background\n\n**Research Question.** This question examines potential threats to valid causal inference in experimental design.\n\n**Setting and Sample.** A study aims to estimate the causal impact of a *relatable, aspirational peer* on the motivation and success of micro-entrepreneurs. The intended treatment is a visit from a successful alumnus of the same training program who shares a similar demographic background with the participants.\n\n### Question\n\nSuppose a hypothetical alternative intervention used highly successful, male CEOs of large corporations as speakers instead of program alumni. From a causal inference perspective, what is the primary threat this alternative design introduces when trying to estimate the effect of a *relatable peer*?", "Options": {"A": "Treatment Confounding: The effect of the CEO speaker would bundle the impact of gender, extreme wealth, and inapplicable advice, making it impossible to isolate the intended 'relatable peer' motivation effect.", "B": "Poor Compliance: Participants would be less likely to attend the session, leading to a weaker Intention-to-Treat (ITT) effect.", "C": "Attrition Bias: The discouraging nature of the CEO speaker might cause more participants in the treatment group to drop out of the study, biasing the results.", "D": "Positive Selection Bias: Only the most ambitious participants would be inspired by the CEOs, leading to an overestimation of the average treatment effect."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item tests the ability to apply causal inference concepts to critique a hypothetical experimental design. It uses a Scenario Application strategy. The correct option (A) precisely identifies treatment confounding as the primary threat. Distractors represent other plausible but secondary econometric issues: (B) focuses on compliance (Almost Right), (C) on attrition (True but Irrelevant to the core identification problem), and (D) on a specific type of heterogeneous effect rather than the primary threat of a confounded treatment (Conceptual Error).", "qid": "502", "question": "### Background\n\n**Research Question.** This question examines potential threats to valid causal inference in experimental design.\n\n**Setting and Sample.** A study aims to estimate the causal impact of a *relatable, aspirational peer* on the motivation and success of micro-entrepreneurs. The intended treatment is a visit from a successful alumnus of the same training program who shares a similar demographic background with the participants.\n\n### Question\n\nSuppose a hypothetical alternative intervention used highly successful, male CEOs of large corporations as speakers instead of program alumni. From a causal inference perspective, what is the primary threat this alternative design introduces when trying to estimate the effect of a *relatable peer*?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 387, "Question": "### Background\n\n**Research Question.** This problem examines the theoretical foundation and empirical specification of a parametric model for time-varying risk premia in asset returns. The core idea is that an asset's expected excess return is determined by its time-varying conditional covariance with a benchmark portfolio.\n\n**Setting.** The analysis begins with a general consumption-based asset pricing model which is then specialized to a conditional single-beta Capital Asset Pricing Model (CAPM). This theoretical model is implemented empirically using a bivariate GARCH-in-Mean (GARCH-M) specification for Japanese and World equity index excess returns.\n\n**Variables and Parameters.**\n*   `R_{j,t}^*`: Excess return on the Japanese equity portfolio at time `t`.\n*   `R_{w,t}^*`: Excess return on the MSCI World equity (benchmark) portfolio at time `t`.\n*   `E_{t-1}[·]`: Expectation conditional on information at `t-1`.\n*   `h_{jw,t}`: Conditional covariance between `R_{j,t}^*` and `R_{w,t}^*`.\n*   `h_{w,t}`: Conditional variance of `R_{w,t}^*`.\n*   `β_t = h_{jw,t} / h_{w,t}`: The time-varying conditional beta of Japanese returns.\n*   `γ_{0j}`: The intercept (alpha) in the Japanese excess return equation.\n*   `μ`: The coefficient on the conditional risk premium term.\n\n---\n\n### Data / Model Specification\n\nThe theoretical conditional single-beta asset pricing relation is given by:\n```latex\n\\mathrm{E}_{t-1}[R_{j,t}^*] = \\frac{\\mathrm{cov}_{t-1}[R_{j,t}^*, R_{w,t}^*]}{\\mathrm{var}_{t-1}[R_{w,t}^*]} \\mathrm{E}_{t-1}[R_{w,t}^*] = \\beta_t \\mathrm{E}_{t-1}[R_{w,t}^*] \\quad \\text{(Eq. 1)}\n```\nThis is implemented empirically with a bivariate GARCH-M system. The equation for Japanese excess returns is:\n```latex\nR_{j,t}^{*} = \\gamma_{0j} + \\mu \\left( \\frac{h_{jw,t}}{h_{w,t}} \\right) \\mathrm{E}_{t-1}[R_{w,t}^*] + \\varepsilon_{j,t} \\quad \\text{(Eq. 2)}\n```\n\n**Table 1: Parametric Model Estimates (Japanese Equity Equation)**\n\n| Parameter | Coefficient | Robust Std. Error |\n| :--- | :--- | :--- |\n| Intercept (`γ₀ⱼ`) | 0.0068 | (0.0047) |\n| Risk Premium (`μ`) | 0.8251 | (0.5186) |\n\n---\n\n### Question\n\nAccording to the theoretical asset pricing model in Eq. (1), the empirical model in Eq. (2) should satisfy the joint null hypothesis `H₀: (γ₀ⱼ, μ) = (0, 1)`. Based on the results in Table 1, what is the correct conclusion from testing this hypothesis at the 5% significance level (critical t-value ≈ ±1.96)?", "Options": {"A": "Reject `H₀: γ₀ⱼ = 0` but fail to reject `H₀: μ = 1`, indicating a significant alpha.", "B": "Fail to reject both `H₀: γ₀ⱼ = 0` and `H₀: μ = 1`, indicating the model is consistent with theory.", "C": "Fail to reject `H₀: γ₀ⱼ = 0` but reject `H₀: μ = 1`, indicating the price of risk is misspecified.", "D": "Reject both `H₀: γ₀ⱼ = 0` and `H₀: μ = 1`, indicating the model is inconsistent with theory."}, "Answer": "B", "pi_justification": "Assesses the ability to connect asset pricing theory to empirical model restrictions and perform standard t-tests. This is a Computational Judgment item. The correct option (B) requires correctly calculating two t-statistics: t_γ = (0.0068-0)/0.0047 ≈ 1.45 and t_μ = (0.8251-1)/0.5186 ≈ -0.34, and concluding that both are insignificant. Distractors A, C, and D represent the three other possible outcomes of the joint test, isolating errors in testing the intercept, the risk premium coefficient, or both.", "qid": "387", "question": "### Background\n\n**Research Question.** This problem examines the theoretical foundation and empirical specification of a parametric model for time-varying risk premia in asset returns. The core idea is that an asset's expected excess return is determined by its time-varying conditional covariance with a benchmark portfolio.\n\n**Setting.** The analysis begins with a general consumption-based asset pricing model which is then specialized to a conditional single-beta Capital Asset Pricing Model (CAPM). This theoretical model is implemented empirically using a bivariate GARCH-in-Mean (GARCH-M) specification for Japanese and World equity index excess returns.\n\n**Variables and Parameters.**\n*   `R_{j,t}^*`: Excess return on the Japanese equity portfolio at time `t`.\n*   `R_{w,t}^*`: Excess return on the MSCI World equity (benchmark) portfolio at time `t`.\n*   `E_{t-1}[·]`: Expectation conditional on information at `t-1`.\n*   `h_{jw,t}`: Conditional covariance between `R_{j,t}^*` and `R_{w,t}^*`.\n*   `h_{w,t}`: Conditional variance of `R_{w,t}^*`.\n*   `β_t = h_{jw,t} / h_{w,t}`: The time-varying conditional beta of Japanese returns.\n*   `γ_{0j}`: The intercept (alpha) in the Japanese excess return equation.\n*   `μ`: The coefficient on the conditional risk premium term.\n\n---\n\n### Data / Model Specification\n\nThe theoretical conditional single-beta asset pricing relation is given by:\n```latex\n\\mathrm{E}_{t-1}[R_{j,t}^*] = \\frac{\\mathrm{cov}_{t-1}[R_{j,t}^*, R_{w,t}^*]}{\\mathrm{var}_{t-1}[R_{w,t}^*]} \\mathrm{E}_{t-1}[R_{w,t}^*] = \\beta_t \\mathrm{E}_{t-1}[R_{w,t}^*] \\quad \\text{(Eq. 1)}\n```\nThis is implemented empirically with a bivariate GARCH-M system. The equation for Japanese excess returns is:\n```latex\nR_{j,t}^{*} = \\gamma_{0j} + \\mu \\left( \\frac{h_{jw,t}}{h_{w,t}} \\right) \\mathrm{E}_{t-1}[R_{w,t}^*] + \\varepsilon_{j,t} \\quad \\text{(Eq. 2)}\n```\n\n**Table 1: Parametric Model Estimates (Japanese Equity Equation)**\n\n| Parameter | Coefficient | Robust Std. Error |\n| :--- | :--- | :--- |\n| Intercept (`γ₀ⱼ`) | 0.0068 | (0.0047) |\n| Risk Premium (`μ`) | 0.8251 | (0.5186) |\n\n---\n\n### Question\n\nAccording to the theoretical asset pricing model in Eq. (1), the empirical model in Eq. (2) should satisfy the joint null hypothesis `H₀: (γ₀ⱼ, μ) = (0, 1)`. Based on the results in Table 1, what is the correct conclusion from testing this hypothesis at the 5% significance level (critical t-value ≈ ±1.96)?", "model_evaluation_result": {"model_raw_response": "I need to test the joint null hypothesis H₀: (γ₀ⱼ, μ) = (0, 1) using the parameter estimates and standard errors provided.\n\nLet me calculate the t-statistics for each parameter:\n\nFor the intercept γ₀ⱼ:\n- Null hypothesis: γ₀ⱼ = 0\n- Estimate: 0.0068\n- Standard error: 0.0047\n- t-statistic = 0.0068 / 0.0047 = 1.45\n\nFor the risk premium coefficient μ:\n- Null hypothesis: μ = 1\n- Estimate: 0.8251\n- Standard error: 0.5186\n- t-statistic = (0.8251 - 1) / 0.5186 = -0.1749 / 0.5186 = -0.34\n\nNow I'll compare these t-statistics to the critical value of ±1.96 at the 5% significance level:\n\nFor γ₀ⱼ = 0:\n- |t-statistic| = |1.45| = 1.45 < 1.96\n- Fail to reject H₀: γ₀ⱼ = 0\n\nFor μ = 1:\n- |t-statistic| = |-0.34| = 0.34 < 1.96\n- Fail to reject H₀: μ = 1\n\nSince we fail to reject both individual null hypotheses, we fail to reject the joint null hypothesis. This means the empirical results are consistent with the theoretical asset pricing model, where the intercept should be zero (no abnormal returns/alpha) and the risk premium coefficient should be one.\n\nB", "model_answer": "B", "error": null}}
{"ID": 506, "Question": "### Background\n\nThe paper estimates the structural parameters `κ` of the Gaussian model using an OLS procedure after concentrating out the fixed-effect factors `z_t`. The model is:\n```latex\ny_t = a + B(κ)z_t + v_t \\quad \\text{(Eq. 1)}\n```\n\n### Data / Model Specification\n\nThe first-order conditions for the OLS estimator of `κ`, evaluated at the true value `κ₀`, converge in probability to a term that is proportional to:\n```latex\n\\text{dg}((B'B)⁻¹B'ΨC_⊥) \\quad \\text{(Eq. 2)}\n```\nwhere `Ψ` is the error covariance matrix `E[v_t v_t']`, `C_⊥` is a matrix related to the derivatives of `B(κ)`, and `dg(·)` extracts the main diagonal. Because this term is generally non-zero, the OLS estimator `κ̂` is inconsistent.\n\n### Question\n\nThe inconsistency of the OLS estimator `κ̂` arises because the expression in Eq. (2) is non-zero for a general error covariance matrix `Ψ`. The expression *does* evaluate to zero (implying consistency) if `Ψ` is diagonal, even if it has non-constant diagonal elements (heteroskedasticity). What does this imply is the specific feature of the error structure that causes the inconsistency?", "Options": {"A": "Heteroskedasticity (i.e., `Var(v_{it})` varies with maturity `i`).", "B": "Cross-sectional correlation (i.e., `Cov(v_{it}, v_{jt}) ≠ 0` for `i ≠ j`).", "C": "Serial correlation (i.e., `Cov(v_{it}, v_{i,t-1}) ≠ 0`).", "D": "Non-normality of the error terms."}, "Answer": "B", "pi_justification": "This item tests the student's ability to diagnose the precise source of an econometric problem by interpreting a formal mathematical condition. It uses a Reverse-Reasoning strategy. The distractors are designed to isolate the correct answer from other plausible but incorrect features of the error term: (A) is explicitly ruled out by the prompt, while (C) and (D) are common error properties that are not the source of this specific inconsistency problem.", "qid": "506", "question": "### Background\n\nThe paper estimates the structural parameters `κ` of the Gaussian model using an OLS procedure after concentrating out the fixed-effect factors `z_t`. The model is:\n```latex\ny_t = a + B(κ)z_t + v_t \\quad \\text{(Eq. 1)}\n```\n\n### Data / Model Specification\n\nThe first-order conditions for the OLS estimator of `κ`, evaluated at the true value `κ₀`, converge in probability to a term that is proportional to:\n```latex\n\\text{dg}((B'B)⁻¹B'ΨC_⊥) \\quad \\text{(Eq. 2)}\n```\nwhere `Ψ` is the error covariance matrix `E[v_t v_t']`, `C_⊥` is a matrix related to the derivatives of `B(κ)`, and `dg(·)` extracts the main diagonal. Because this term is generally non-zero, the OLS estimator `κ̂` is inconsistent.\n\n### Question\n\nThe inconsistency of the OLS estimator `κ̂` arises because the expression in Eq. (2) is non-zero for a general error covariance matrix `Ψ`. The expression *does* evaluate to zero (implying consistency) if `Ψ` is diagonal, even if it has non-constant diagonal elements (heteroskedasticity). What does this imply is the specific feature of the error structure that causes the inconsistency?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand what causes the expression in Eq. (2) to be non-zero versus zero.\n\nThe key insight is given in the question: the expression `dg((B'B)⁻¹B'ΨC_⊥)` evaluates to zero when `Ψ` is diagonal, even if it has non-constant diagonal elements (heteroskedasticity).\n\nLet me analyze what this means:\n\n- When `Ψ` is diagonal: This means `Ψ_{ij} = 0` for `i ≠ j`, so there are no off-diagonal elements\n- The expression becomes zero in this case, implying consistency\n- Heteroskedasticity (non-constant diagonal elements) is explicitly stated to NOT cause inconsistency\n\nSince `Ψ = E[v_t v_t']` is the error covariance matrix, the diagonal elements represent variances of individual error terms, while off-diagonal elements represent covariances between different error terms.\n\nWhen `Ψ` is diagonal, we have:\n- `Ψ_{ii} = Var(v_{it})` (can vary with `i` - heteroskedasticity is allowed)\n- `Ψ_{ij} = Cov(v_{it}, v_{jt}) = 0` for `i ≠ j` (no cross-sectional correlation)\n\nThe fact that the expression is zero when `Ψ` is diagonal but non-zero for a general `Ψ` indicates that the inconsistency arises from the off-diagonal elements of `Ψ`, which represent cross-sectional correlations between error terms at the same time period.\n\nSerial correlation would involve correlations across time periods, which wouldn't be captured in the contemporaneous covariance matrix `E[v_t v_t']`.\n\nB", "model_answer": "B", "error": null}}
{"ID": 507, "Question": "### Background\n\n**Research Question.** How can a specific functional form for the Lorenz curve be used to derive a closed-form expression for the Gini concentration ratio (CR), and how can we then conduct statistical inference on this CR?\n\n**Setting.** The analysis uses a novel parametric Lorenz curve to model the income distribution. The Gini concentration ratio is derived from this functional form, and its statistical uncertainty is assessed using the Delta Method.\n\n**Variables and Parameters.**\n*   `π`: The cumulative proportion of income-receiving units (0 to 1).\n*   `η`: The cumulative proportion of total income (0 to 1).\n*   `β`: A parameter governing the level of inequality (`β ≥ 0`).\n*   `CR`: The Gini concentration ratio.\n*   `hat(β)`: A consistent estimator of `β`.\n\n---\n\n### Data / Model Specification\n\nThe Lorenz curve is given by `η(π) = π * e^(-β(1-π))`. From this, the Gini concentration ratio `CR` can be derived as a function of `β`:\n\n```latex\nCR(\\beta) = 1 - 2\\frac{(\\beta-1)}{\\beta^{2}} - \\frac{2e^{-\\beta}}{\\beta^{2}} \\quad \\text{(Eq. 1)}\n```\n\nTo conduct inference on an estimate `hat(CR) = CR(hat(β))`, the Delta Method is used, which requires the derivative of `CR(β)` with respect to `β`:\n\n```latex\n\\frac{\\partial(CR)}{\\partial\\beta} = \\frac{2}{\\beta^3}[(\\beta-2) + (\\beta+2)e^{-\\beta}] \\quad \\text{(Eq. 2)}\n```\n\nThe variance of the estimated CR is then `Var(hat(CR)) ≈ [∂(CR)/∂β]^2 * Var(hat(β))`. A researcher estimates `hat(β) = 1.443` with a standard error `SE(hat(β)) = 0.013`. They wish to test the null hypothesis that the true CR is 0.35 (`H_0: CR = 0.35`) at the 5% significance level.\n\n(Note: `e^(-1.443) ≈ 0.236`)\n\n---\n\n### The Question\n\nCalculate the Wald test statistic for the null hypothesis `H_0: CR = 0.35` and determine the correct statistical conclusion at the 5% significance level (critical value ≈ ±1.96).", "Options": {"A": "Wald statistic ≈ -1.0; Fail to reject the null hypothesis.", "B": "Wald statistic ≈ 1.0; Fail to reject the null hypothesis.", "C": "Wald statistic ≈ -0.69; Fail to reject the null hypothesis.", "D": "Wald statistic ≈ -1.0; Reject the null hypothesis."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Suitability Score: 9.5). This item uses the 'Computational Judgment' strategy. It tests the student's ability to execute a multi-step hypothesis test using the Delta Method, a core applied statistical skill. The calculation is complex enough to be discriminating but results in a single correct answer, making it ideal for a choice format. The distractors are designed based on a matrix of common errors: (B) is a Sign Error in the numerator of the Wald statistic; (C) results from a Formula Misuse error in calculating the derivative (a plausible mistake in applying the quotient rule); (D) represents a Conceptual Error, where the student calculates the correct statistic but draws the wrong conclusion about statistical significance.", "qid": "507", "question": "### Background\n\n**Research Question.** How can a specific functional form for the Lorenz curve be used to derive a closed-form expression for the Gini concentration ratio (CR), and how can we then conduct statistical inference on this CR?\n\n**Setting.** The analysis uses a novel parametric Lorenz curve to model the income distribution. The Gini concentration ratio is derived from this functional form, and its statistical uncertainty is assessed using the Delta Method.\n\n**Variables and Parameters.**\n*   `π`: The cumulative proportion of income-receiving units (0 to 1).\n*   `η`: The cumulative proportion of total income (0 to 1).\n*   `β`: A parameter governing the level of inequality (`β ≥ 0`).\n*   `CR`: The Gini concentration ratio.\n*   `hat(β)`: A consistent estimator of `β`.\n\n---\n\n### Data / Model Specification\n\nThe Lorenz curve is given by `η(π) = π * e^(-β(1-π))`. From this, the Gini concentration ratio `CR` can be derived as a function of `β`:\n\n```latex\nCR(\\beta) = 1 - 2\\frac{(\\beta-1)}{\\beta^{2}} - \\frac{2e^{-\\beta}}{\\beta^{2}} \\quad \\text{(Eq. 1)}\n```\n\nTo conduct inference on an estimate `hat(CR) = CR(hat(β))`, the Delta Method is used, which requires the derivative of `CR(β)` with respect to `β`:\n\n```latex\n\\frac{\\partial(CR)}{\\partial\\beta} = \\frac{2}{\\beta^3}[(\\beta-2) + (\\beta+2)e^{-\\beta}] \\quad \\text{(Eq. 2)}\n```\n\nThe variance of the estimated CR is then `Var(hat(CR)) ≈ [∂(CR)/∂β]^2 * Var(hat(β))`. A researcher estimates `hat(β) = 1.443` with a standard error `SE(hat(β)) = 0.013`. They wish to test the null hypothesis that the true CR is 0.35 (`H_0: CR = 0.35`) at the 5% significance level.\n\n(Note: `e^(-1.443) ≈ 0.236`)\n\n---\n\n### The Question\n\nCalculate the Wald test statistic for the null hypothesis `H_0: CR = 0.35` and determine the correct statistical conclusion at the 5% significance level (critical value ≈ ±1.96).", "model_evaluation_result": {"model_raw_response": "I need to calculate the Wald test statistic for testing H₀: CR = 0.35.\n\nFirst, let me calculate the estimated CR using the given formula and β̂ = 1.443:\n\nCR(β̂) = 1 - 2(β̂-1)/β̂² - 2e^(-β̂)/β̂²\n\nWith β̂ = 1.443 and e^(-1.443) ≈ 0.236:\n\nCR(1.443) = 1 - 2(1.443-1)/(1.443)² - 2(0.236)/(1.443)²\n= 1 - 2(0.443)/(2.082) - 2(0.236)/(2.082)\n= 1 - 0.886/2.082 - 0.472/2.082\n= 1 - 0.425 - 0.227\n= 0.348\n\nNext, I need to calculate the derivative ∂(CR)/∂β at β̂ = 1.443:\n\n∂(CR)/∂β = (2/β³)[(β-2) + (β+2)e^(-β)]\n\nAt β̂ = 1.443:\n∂(CR)/∂β = (2/(1.443)³)[(1.443-2) + (1.443+2)(0.236)]\n= (2/3.005)[(-0.557) + (3.443)(0.236)]\n= (2/3.005)[-0.557 + 0.813]\n= (2/3.005)(0.256)\n= 0.170\n\nNow I can calculate the variance of ĈR using the Delta Method:\nVar(ĈR) ≈ [∂(CR)/∂β]² × Var(β̂)\n= (0.170)² × (0.013)²\n= 0.0289 × 0.000169\n= 0.00000488\n\nThe standard error of ĈR is:\nSE(ĈR", "model_answer": "I need to calculate the Wald test statistic for testing H₀: CR = 0.35.\n\nFirst, let me calculate the estimated CR using the given formula and β̂ = 1.443:\n\nCR(β̂) = 1 - 2(β̂-1)/β̂² - 2e^(-β̂)/β̂²\n\nWith β̂ = 1.443 and e^(-1.443) ≈ 0.236:\n\nCR(1.443) = 1 - 2(1.443-1)/(1.443)² - 2(0.236)/(1.443)²\n= 1 - 2(0.443)/(2.082) - 2(0.236)/(2.082)\n= 1 - 0.886/2.082 - 0.472/2.082\n= 1 - 0.425 - 0.227\n= 0.348\n\nNext, I need to calculate the derivative ∂(CR)/∂β at β̂ = 1.443:\n\n∂(CR)/∂β = (2/β³)[(β-2) + (β+2)e^(-β)]\n\nAt β̂ = 1.443:\n∂(CR)/∂β = (2/(1.443)³)[(1.443-2) + (1.443+2)(0.236)]\n= (2/3.005)[(-0.557) + (3.443)(0.236)]\n= (2/3.005)[-0.557 + 0.813]\n= (2/3.005)(0.256)\n= 0.170\n\nNow I can calculate the variance of ĈR using the Delta Method:\nVar(ĈR) ≈ [∂(CR)/∂β]² × Var(β̂)\n= (0.170)² × (0.013)²\n= 0.0289 × 0.000169\n= 0.00000488\n\nThe standard error of ĈR is:\nSE(ĈR", "error": "Warning: Could not extract single choice letter from: I need to calculate the Wald test statistic for te..."}}
{"ID": 496, "Question": "### Background\n\n**Research Question.** How does the nature of strategic interaction in the underlying game—specifically, whether actions are strategic substitutes or complements—alter the conditions required for information to be fully revealed?\n\n**Setting.** Consider a duopoly (`n=2`) where firms' private information ('type') affects their profitability. The firms first make a certified announcement about their type, then compete in a second-stage game.\n\n### Data / Model Specification\n\nIn a game with **Strategic Complements (SC)** (e.g., Bertrand price competition), the marginal payoff `∂π_i/∂s_i` is *increasing* in the rival's action `s_j`.\n\nFor the crucial property of **weak positive-monotonicity** to hold in this setting, the paper shows that the marginal payoff `∂π_i/∂s_i` must be *decreasing* in an agent's own type `t_i` (Assumption 4'(c)).\n\nConsider a specific Bertrand duopoly model where:\n- Firms set prices `p_i`.\n- The action is defined as `s_i = -p_i` (a higher `s_i` means a lower, more aggressive price).\n- A firm's type is its constant marginal cost, `t_i = c_i` (a higher type `t_i` means a higher, less favorable cost).\n\nIn this specific model, it can be shown that the marginal payoff `∂π_i/∂s_i` is *increasing* in type `t_i = c_i`, which violates the condition in Assumption 4'(c).\n\n### Question\n\nGiven that this Bertrand model violates the key condition (Assumption 4'(c)) required for positive monotonicity, what is the resulting incentive for information revelation?", "Options": {"A": "The high-cost firm has an incentive to reveal its type, as this credibly signals it will set a higher price, inducing its rival to also set a higher price, which is beneficial.", "B": "The low-cost firm has an incentive to reveal its type, as this signals its competitive strength and forces the rival to accommodate with higher prices.", "C": "Both firms have an incentive to reveal their types, as more information always leads to more profitable outcomes in Bertrand competition.", "D": "Neither firm has an incentive to reveal its type, because the violation of Assumption 4'(c) means that any signal about cost will be met with a profit-destroying price war."}, "Answer": "A", "pi_justification": "This question uses a Scenario Application strategy to test a deeper understanding of how theoretical conditions apply to a concrete economic model. It requires the student to reason from a stated violation of an assumption to the correct strategic incentive. Distractors are based on plausible errors: B misidentifies the revealing firm, C makes a false general claim, and D incorrectly concludes that no revelation occurs.", "qid": "496", "question": "### Background\n\n**Research Question.** How does the nature of strategic interaction in the underlying game—specifically, whether actions are strategic substitutes or complements—alter the conditions required for information to be fully revealed?\n\n**Setting.** Consider a duopoly (`n=2`) where firms' private information ('type') affects their profitability. The firms first make a certified announcement about their type, then compete in a second-stage game.\n\n### Data / Model Specification\n\nIn a game with **Strategic Complements (SC)** (e.g., Bertrand price competition), the marginal payoff `∂π_i/∂s_i` is *increasing* in the rival's action `s_j`.\n\nFor the crucial property of **weak positive-monotonicity** to hold in this setting, the paper shows that the marginal payoff `∂π_i/∂s_i` must be *decreasing* in an agent's own type `t_i` (Assumption 4'(c)).\n\nConsider a specific Bertrand duopoly model where:\n- Firms set prices `p_i`.\n- The action is defined as `s_i = -p_i` (a higher `s_i` means a lower, more aggressive price).\n- A firm's type is its constant marginal cost, `t_i = c_i` (a higher type `t_i` means a higher, less favorable cost).\n\nIn this specific model, it can be shown that the marginal payoff `∂π_i/∂s_i` is *increasing* in type `t_i = c_i`, which violates the condition in Assumption 4'(c).\n\n### Question\n\nGiven that this Bertrand model violates the key condition (Assumption 4'(c)) required for positive monotonicity, what is the resulting incentive for information revelation?", "model_evaluation_result": {"model_raw_response": "Looking at this step-by-step:\n\n1. **Understanding the setup**: In this Bertrand model with strategic complements:\n   - Action s_i = -p_i (higher s_i means lower, more aggressive price)\n   - Type t_i = c_i (marginal cost - higher type means higher cost)\n   - Strategic complements means ∂π_i/∂s_i is increasing in rival's action s_j\n\n2. **The violation**: The model shows that ∂π_i/∂s_i is *increasing* in type t_i = c_i, which violates Assumption 4'(c) that requires it to be *decreasing* in own type.\n\n3. **Economic intuition**: When a high-cost firm reveals its type:\n   - It signals it cannot compete as aggressively (cannot set prices as low)\n   - This is credible information that the firm will set higher prices\n   - In Bertrand competition with strategic complements, when one firm sets higher prices, the rival's best response is also to set higher prices\n   - This leads to a less competitive equilibrium that benefits both firms\n\n4. **Why high-cost firms benefit from revelation**:\n   - High-cost firms gain from signaling their constraint (high costs)\n   - This induces accommodating behavior from rivals\n   - The resulting higher prices benefit the high-cost firm\n\n5. **Why low-cost firms don't benefit similarly**:\n   - Revealing low costs signals competitive strength and aggressive pricing capability\n   - This would induce more competitive responses from rivals\n   - This doesn't provide the same strategic advantage\n\nThe answer is **A**.\n\nA", "model_answer": "A", "error": null}}
{"ID": 495, "Question": "### Background\n\n**Research Question.** How does the nature of strategic interaction in the underlying game—specifically, whether actions are strategic substitutes or complements—alter the conditions required for information to be fully revealed?\n\n**Setting.** Consider a duopoly (`n=2`) where firms' private information ('type') affects their profitability. The firms first make a certified announcement about their type, then compete in a second-stage game. This problem contrasts the conditions needed for information revelation in two different competitive environments.\n\n### Data / Model Specification\n\nThe paper provides sufficient conditions for the crucial property of **weak positive-monotonicity** (where an agent's payoff increases if they are perceived as a 'higher' type).\n\nIn the case of **Strategic Substitutes (SS)** (e.g., Cournot quantity competition), two key conditions are:\n- **Assumption 4(b):** Marginal payoff `∂π_i/∂s_i` is *decreasing* in the rival's action `s_j`.\n- **Assumption 4(c):** Marginal payoff `∂π_i/∂s_i` is *increasing* in own type `t_i`.\n\n**Theorem 2:** Under these (and other regularity) conditions, payoffs are weakly positive-monotone.\n\n### Question\n\nIn a game with strategic substitutes (e.g., Cournot competition), what is the correct causal mechanism that gives a firm an incentive to reveal it is of a 'higher' type (e.g., has lower costs), as described by Theorem 2?", "Options": {"A": "Revealing a higher type signals more aggressive future action (e.g., higher quantity). Due to strategic substitutability, the rival firm responds by acting less aggressively (lower quantity), which increases the revealing firm's payoff.", "B": "Revealing a higher type signals less aggressive future action. The rival firm responds by also acting less aggressively, and this mutual de-escalation increases the revealing firm's payoff.", "C": "Revealing a higher type signals more aggressive future action. The rival firm is forced to match this aggression, but the revealing firm's superior type allows it to profit from the intensified competition.", "D": "Revealing a higher type has no impact on the rival's action but directly increases the firm's own payoff due to its intrinsic efficiency, regardless of strategic interaction."}, "Answer": "A", "pi_justification": "This question assesses the student's understanding of the core causal chain for information revelation under strategic substitutes. It uses a Reverse-Reasoning strategy, asking for the mechanism behind a known result. Distractors are designed based on common conceptual errors: B applies the logic of strategic complements, C incorrectly models the rival's reaction, and D ignores the strategic channel entirely.", "qid": "495", "question": "### Background\n\n**Research Question.** How does the nature of strategic interaction in the underlying game—specifically, whether actions are strategic substitutes or complements—alter the conditions required for information to be fully revealed?\n\n**Setting.** Consider a duopoly (`n=2`) where firms' private information ('type') affects their profitability. The firms first make a certified announcement about their type, then compete in a second-stage game. This problem contrasts the conditions needed for information revelation in two different competitive environments.\n\n### Data / Model Specification\n\nThe paper provides sufficient conditions for the crucial property of **weak positive-monotonicity** (where an agent's payoff increases if they are perceived as a 'higher' type).\n\nIn the case of **Strategic Substitutes (SS)** (e.g., Cournot quantity competition), two key conditions are:\n- **Assumption 4(b):** Marginal payoff `∂π_i/∂s_i` is *decreasing* in the rival's action `s_j`.\n- **Assumption 4(c):** Marginal payoff `∂π_i/∂s_i` is *increasing* in own type `t_i`.\n\n**Theorem 2:** Under these (and other regularity) conditions, payoffs are weakly positive-monotone.\n\n### Question\n\nIn a game with strategic substitutes (e.g., Cournot competition), what is the correct causal mechanism that gives a firm an incentive to reveal it is of a 'higher' type (e.g., has lower costs), as described by Theorem 2?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to trace through the causal mechanism in strategic substitutes games like Cournot competition.\n\nIn strategic substitutes:\n- Actions are substitutes - when one firm increases its action, the other firm optimally decreases its action\n- A \"higher type\" typically means lower costs or higher efficiency\n- The key assumptions are that marginal payoff decreases in rival's action and increases in own type\n\nLet me work through the logic:\n\n1. A firm with a higher type (lower costs) will optimally choose a more aggressive action (higher quantity in Cournot)\n\n2. When this firm reveals its higher type, rivals anticipate this more aggressive future behavior\n\n3. Due to strategic substitutability, the rival responds by choosing a less aggressive action (lower quantity)\n\n4. This less aggressive response by the rival benefits the revealing firm - in Cournot, when the rival produces less, it increases the market price and the revealing firm's profits\n\n5. This creates the incentive for truthful revelation - the firm benefits from being perceived as a higher type because it induces a favorable (less aggressive) response from rivals\n\nThis matches exactly with option A: \"Revealing a higher type signals more aggressive future action (e.g., higher quantity). Due to strategic substitutability, the rival firm responds by acting less aggressively (lower quantity), which increases the revealing firm's payoff.\"\n\nOptions B and C get the strategic interaction wrong, and option D ignores the strategic interaction entirely.\n\nA", "model_answer": "A", "error": null}}
{"ID": 503, "Question": "### Background\n\nIn a `k`-factor Gaussian term structure model, the yield `Y_t(τ)` on a zero-coupon bond with maturity `τ` is an affine function of `k` unobserved factors `z_t`:\n```latex\nY_t(τ) = a(τ) + b(τ)'z_t \\quad \text{(Eq. 1)}\n```\nThe factor loadings `b(τ)` are determined by the factors' risk-neutral mean-reversion speeds, `κᵢ`.\n\n### Data / Model Specification\n\nThe functional form for the factor loadings is derived by solving a system of ordinary differential equations. Letting `B_i(τ) = τb_i(τ)`, the relevant ODE for the `i`-th component is:\n```latex\n\\frac{dB_i(τ)}{dτ} = -κ_i B_i(τ) + 1, \\quad \\text{with boundary condition } B_i(0) = 0 \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nBy solving the ordinary differential equation in Eq. (2) with its boundary condition, what is the correct expression for the factor loading `b_i(τ)`?", "Options": {"A": "`\\frac{1 - e^{-κ_i τ}}{κ_i τ}`", "B": "`\\frac{1 - e^{-κ_i τ}}{κ_i}`", "C": "`\\frac{e^{κ_i τ} - 1}{κ_i τ}`", "D": "`\\frac{1 - e^{-κ_i τ}}{τ}`"}, "Answer": "A", "pi_justification": "This item assesses the student's ability to solve a foundational differential equation from the Gaussian term structure model. It uses a Derivational Judgment strategy. The distractors are designed based on high-frequency errors in solving ODEs and algebraic manipulation: (B) provides the intermediate solution `B(τ)` instead of the final `b(τ)`, (C) represents a common sign error, and (D) omits a key parameter from the final expression.", "qid": "503", "question": "### Background\n\nIn a `k`-factor Gaussian term structure model, the yield `Y_t(τ)` on a zero-coupon bond with maturity `τ` is an affine function of `k` unobserved factors `z_t`:\n```latex\nY_t(τ) = a(τ) + b(τ)'z_t \\quad \text{(Eq. 1)}\n```\nThe factor loadings `b(τ)` are determined by the factors' risk-neutral mean-reversion speeds, `κᵢ`.\n\n### Data / Model Specification\n\nThe functional form for the factor loadings is derived by solving a system of ordinary differential equations. Letting `B_i(τ) = τb_i(τ)`, the relevant ODE for the `i`-th component is:\n```latex\n\\frac{dB_i(τ)}{dτ} = -κ_i B_i(τ) + 1, \\quad \\text{with boundary condition } B_i(0) = 0 \\quad \\text{(Eq. 2)}\n```\n\n### Question\n\nBy solving the ordinary differential equation in Eq. (2) with its boundary condition, what is the correct expression for the factor loading `b_i(τ)`?", "model_evaluation_result": {"model_raw_response": "I need to solve the ordinary differential equation (ODE) in Eq. (2) and then find the expression for b_i(τ).\n\nGiven ODE:\ndB_i(τ)/dτ = -κ_i B_i(τ) + 1\n\nWith boundary condition: B_i(0) = 0\n\nThis is a first-order linear ODE. I can solve it using an integrating factor or by recognizing it as a standard form.\n\nRearranging: dB_i(τ)/dτ + κ_i B_i(τ) = 1\n\nThe integrating factor is e^(κ_i τ).\n\nMultiplying both sides by e^(κ_i τ):\ne^(κ_i τ) dB_i(τ)/dτ + κ_i e^(κ_i τ) B_i(τ) = e^(κ_i τ)\n\nThe left side is the derivative of [e^(κ_i τ) B_i(τ)]:\nd/dτ[e^(κ_i τ) B_i(τ)] = e^(κ_i τ)\n\nIntegrating both sides:\ne^(κ_i τ) B_i(τ) = ∫e^(κ_i τ) dτ = (1/κ_i)e^(κ_i τ) + C\n\nTherefore:\nB_i(τ) = (1/κ_i) + C e^(-κ_i τ)\n\nUsing the boundary condition B_i(0) = 0:\n0 = (1/κ_i) + C e^0 = (1/κ_i) + C\n\nSo C = -1/κ_i\n\nTherefore:\nB_i(τ) = (1/κ_i) - (1/κ_i)e^(-κ_i τ) = (1/κ_i)(1 - e^(-κ_i τ))\n\nSince B_i(τ)", "model_answer": "C", "error": null}}
{"ID": 491, "Question": "### Background\n\nThis problem explores the quantitative impact of the labor margin on measured risk aversion for a canonical utility function. The presence of a labor margin allows households to absorb shocks through changes in hours worked, which attenuates risk aversion compared to a consumption-only model.\n\n### Data / Model Specification\n\nConsider an additively separable period utility function from Example 2 of the paper:\n```latex\nu(c_{t},l_{t})=\\frac{c_{t}^{1-\\gamma}}{1-\\gamma}-\\eta\\frac{l_{t}^{1+\\chi}}{1+\\chi} \\quad \\text{(Eq. 1)}\n```\nwhere `γ, χ, η > 0`. The traditional measure of relative risk aversion for these preferences is `γ`.\n\nThe consumption-only coefficient of relative risk aversion, `R^c`, which correctly accounts for the labor margin, is given by the general formula:\n```latex\nR^{c}(a;\\theta)=\\frac{-u_{11}+\\lambda u_{12}}{u_{1}}\\frac{c}{1+w\\lambda} \\quad \\text{(Eq. 2)}\n```\nwhere `λ` is a term related to the marginal rate of substitution between consumption and labor adjustments, and `w` is the wage.\n\n### Question\n\nFor the utility function in Eq. (1), what is the correct expression for the coefficient of relative risk aversion, `R^c`, evaluated at the steady state?", "Options": {"A": "```latex\n\\gamma\n```", "B": "```latex\n\\frac{1}{\\frac{1}{\\gamma}+\\frac{1}{\\chi}}\n```", "C": "```latex\n\\frac{\\gamma}{1+\\frac{\\gamma}{\\chi}}\n```", "D": "```latex\n\\frac{\\gamma}{1+\\frac{\\gamma}{\\chi}\\frac{wl}{c}}\n```"}, "Answer": "D", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's ability to perform the key derivation in Example 2 of the paper. It uses a Computational Judgment strategy. The distractors are designed to capture specific, plausible errors: (A) ignoring the labor margin entirely (the traditional measure), (B) using a simplifying approximation from the paper that does not hold in general, and (C) a step-omission error in the final algebraic substitution.", "qid": "491", "question": "### Background\n\nThis problem explores the quantitative impact of the labor margin on measured risk aversion for a canonical utility function. The presence of a labor margin allows households to absorb shocks through changes in hours worked, which attenuates risk aversion compared to a consumption-only model.\n\n### Data / Model Specification\n\nConsider an additively separable period utility function from Example 2 of the paper:\n```latex\nu(c_{t},l_{t})=\\frac{c_{t}^{1-\\gamma}}{1-\\gamma}-\\eta\\frac{l_{t}^{1+\\chi}}{1+\\chi} \\quad \\text{(Eq. 1)}\n```\nwhere `γ, χ, η > 0`. The traditional measure of relative risk aversion for these preferences is `γ`.\n\nThe consumption-only coefficient of relative risk aversion, `R^c`, which correctly accounts for the labor margin, is given by the general formula:\n```latex\nR^{c}(a;\\theta)=\\frac{-u_{11}+\\lambda u_{12}}{u_{1}}\\frac{c}{1+w\\lambda} \\quad \\text{(Eq. 2)}\n```\nwhere `λ` is a term related to the marginal rate of substitution between consumption and labor adjustments, and `w` is the wage.\n\n### Question\n\nFor the utility function in Eq. (1), what is the correct expression for the coefficient of relative risk aversion, `R^c`, evaluated at the steady state?", "model_evaluation_result": {"model_raw_response": "I need to find the coefficient of relative risk aversion R^c for the given utility function when accounting for the labor margin.\n\nGiven utility function:\nu(c_t, l_t) = c_t^(1-γ)/(1-γ) - η·l_t^(1+χ)/(1+χ)\n\nFirst, I'll find the partial derivatives:\n- u_1 = ∂u/∂c = c^(-γ)\n- u_11 = ∂²u/∂c² = -γc^(-γ-1)\n- u_12 = ∂²u/∂c∂l = 0 (since utility is additively separable)\n\nThe general formula is:\nR^c = [-u_11 + λu_12]/u_1 · c/(1 + wλ)\n\nSince u_12 = 0 for additively separable utility:\nR^c = -u_11/u_1 · c/(1 + wλ)\n\nSubstituting the derivatives:\nR^c = -(-γc^(-γ-1))/c^(-γ) · c/(1 + wλ)\nR^c = γc^(-γ-1)/c^(-γ) · c/(1 + wλ)\nR^c = γc · c/(1 + wλ)\nR^c = γc/(1 + wλ)\n\nNow I need to determine λ. For the labor margin, λ relates to how labor adjusts to consumption changes. From the first-order conditions and the structure of this problem, λ involves the elasticity of labor supply.\n\nFor this utility function, the Frisch elasticity of labor supply is 1/χ. The relationship between consumption and labor through the wage gives us the key connection.\n\nAt steady state, the labor margin adjustment means:\nλ = (1/χ)·(wl/c)\n\nTherefore:\nR^c = γ/(1 + w·(1/χ)·(wl/c))\nR^c = γ/(1 + (γ/χ)·(", "model_answer": "I need to find the coefficient of relative risk aversion R^c for the given utility function when accounting for the labor margin.\n\nGiven utility function:\nu(c_t, l_t) = c_t^(1-γ)/(1-γ) - η·l_t^(1+χ)/(1+χ)\n\nFirst, I'll find the partial derivatives:\n- u_1 = ∂u/∂c = c^(-γ)\n- u_11 = ∂²u/∂c² = -γc^(-γ-1)\n- u_12 = ∂²u/∂c∂l = 0 (since utility is additively separable)\n\nThe general formula is:\nR^c = [-u_11 + λu_12]/u_1 · c/(1 + wλ)\n\nSince u_12 = 0 for additively separable utility:\nR^c = -u_11/u_1 · c/(1 + wλ)\n\nSubstituting the derivatives:\nR^c = -(-γc^(-γ-1))/c^(-γ) · c/(1 + wλ)\nR^c = γc^(-γ-1)/c^(-γ) · c/(1 + wλ)\nR^c = γc · c/(1 + wλ)\nR^c = γc/(1 + wλ)\n\nNow I need to determine λ. For the labor margin, λ relates to how labor adjusts to consumption changes. From the first-order conditions and the structure of this problem, λ involves the elasticity of labor supply.\n\nFor this utility function, the Frisch elasticity of labor supply is 1/χ. The relationship between consumption and labor through the wage gives us the key connection.\n\nAt steady state, the labor margin adjustment means:\nλ = (1/χ)·(wl/c)\n\nTherefore:\nR^c = γ/(1 + w·(1/χ)·(wl/c))\nR^c = γ/(1 + (γ/χ)·(", "error": "Warning: Could not extract single choice letter from: I need to find the coefficient of relative risk av..."}}
{"ID": 477, "Question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. The limit set of equilibrium payoffs is characterized by the intersection of half-spaces, where each half-space is defined by a direction vector `$\\lambda = ((\\lambda_1^{\\omega_1}, \\lambda_2^{\\omega_1}), (\\lambda_1^{\\omega_2}, \\lambda_2^{\\omega_2}))$`.\n\n### Data / Model Specification\n\nConsider the case where **only Player 2 knows the state** (`$\\Theta_2 = \\{(\\omega_1), (\\omega_2)\\}$`), while Player 1 does not (`$\\Theta_1 = \\{(\\omega_1, \\omega_2)\\}$`). The paper shows that the folk theorem can fail in this case because the maximal score for certain directions `$\\lambda$` is too low.\n\n### Question\n\nWhat strategic incentive does the direction `$\\lambda = ((0,-1),(0,1))$` place on the informed Player 2?", "Options": {"A": "It creates an incentive for Player 2 to reveal the state if it is `$\\omega_1$` (high productivity) and conceal it if it is `$\\omega_2$` (low productivity).", "B": "It creates an incentive for Player 2 to conceal the state if it is `$\\omega_1$` (high productivity) and reveal it if it is `$\\omega_2$` (low productivity).", "C": "It creates an equal incentive for Player 2 to cooperate in both states to maximize the joint payoff.", "D": "It creates an incentive for Player 2 to always defect, as his payoff is being minimized in at least one state."}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the ability to translate a formal mathematical object (`$\\lambda$` vector) into its implied economic incentive structure. It uses a Computational Judgment strategy. The correct answer correctly interprets the negative weight as a punishment and the positive weight as a reward. Distractors are based on a Sign Error, a conceptual misinterpretation of `$\\lambda$` as social welfare weights, and a misunderstanding of how continuation values function.", "qid": "477", "question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. The limit set of equilibrium payoffs is characterized by the intersection of half-spaces, where each half-space is defined by a direction vector `$\\lambda = ((\\lambda_1^{\\omega_1}, \\lambda_2^{\\omega_1}), (\\lambda_1^{\\omega_2}, \\lambda_2^{\\omega_2}))$`.\n\n### Data / Model Specification\n\nConsider the case where **only Player 2 knows the state** (`$\\Theta_2 = \\{(\\omega_1), (\\omega_2)\\}$`), while Player 1 does not (`$\\Theta_1 = \\{(\\omega_1, \\omega_2)\\}$`). The paper shows that the folk theorem can fail in this case because the maximal score for certain directions `$\\lambda$` is too low.\n\n### Question\n\nWhat strategic incentive does the direction `$\\lambda = ((0,-1),(0,1))$` place on the informed Player 2?", "model_evaluation_result": {"model_raw_response": "Looking at the direction vector λ = ((0,-1),(0,1)), I need to analyze what strategic incentive this creates for the informed Player 2.\n\nThe direction vector has the structure:\n- In state ω₁: (λ₁^ω₁, λ₂^ω₁) = (0, -1)\n- In state ω₂: (λ₁^ω₂, λ₂^ω₂) = (0, 1)\n\nThis means:\n- In state ω₁ (high productivity): Player 2's payoff gets weight -1 (negative)\n- In state ω₂ (low productivity): Player 2's payoff gets weight +1 (positive)\n\nFrom Player 2's perspective, this direction vector creates opposing incentives across states:\n- In ω₁: Player 2 wants to minimize the weighted objective (since his payoff has negative weight)\n- In ω₂: Player 2 wants to maximize the weighted objective (since his payoff has positive weight)\n\nSince Player 2 knows the state and Player 1 doesn't, Player 2 has private information. The asymmetric weighting across states means Player 2 faces different incentives depending on which state has occurred.\n\nIn the high productivity state (ω₁), the negative weight on Player 2's payoff creates an incentive to take actions that might signal or reveal this state. In the low productivity state (ω₂), the positive weight creates an incentive to take actions that conceal the state or behave differently.\n\nThis corresponds to Player 2 having an incentive to reveal when in the high productivity state and conceal when in the low productivity state.\n\nA", "model_answer": "A", "error": null}}
{"ID": 453, "Question": "### Background\n\nA central thesis of the paper is that a coherent definition of causality in static simultaneous equations can be grounded in the structure of an underlying dynamic system. The stability of such systems is determined by the eigenvalues of their coefficient matrices.\n\n### Data / Model Specification\n\nConsider a two-variable triangular dynamic system, where `a, b, c, m, n` are positive constants. A weak feedback loop is introduced, modifying the first equation:\n```latex\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -ax + \\varepsilon y + m \\quad \\text{(Eq. 1)}\n```\n```latex\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = bx - cy + n \\quad \\text{(Eq. 2)}\n```\nwhere `ε` is a small real number.\n\n### Question\n\nA necessary condition for the stability of this 2x2 system is that all coefficients of its characteristic polynomial in `λ` are positive. Given `a, b, c > 0`, what condition must `ε` satisfy for the system to remain stable?", "Options": {"A": "`ac > εb`", "B": "`ac < εb`", "C": "`a+c > ε`", "D": "`ac > -εb`"}, "Answer": "A", "pi_justification": "This item assesses a more advanced skill: analyzing the stability of a perturbed dynamic system. It uses the Computational Judgment strategy. The correct answer requires deriving the characteristic equation and interpreting the condition on its constant term. Distractor B is the Conceptual Opposite (the condition for instability). Distractor C is a Formula Misuse error (confusing the trace and determinant conditions). Distractor D represents a Sign Error in the determinant calculation.", "qid": "453", "question": "### Background\n\nA central thesis of the paper is that a coherent definition of causality in static simultaneous equations can be grounded in the structure of an underlying dynamic system. The stability of such systems is determined by the eigenvalues of their coefficient matrices.\n\n### Data / Model Specification\n\nConsider a two-variable triangular dynamic system, where `a, b, c, m, n` are positive constants. A weak feedback loop is introduced, modifying the first equation:\n```latex\n\\frac{\\mathrm{d}x}{\\mathrm{d}t} = -ax + \\varepsilon y + m \\quad \\text{(Eq. 1)}\n```\n```latex\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = bx - cy + n \\quad \\text{(Eq. 2)}\n```\nwhere `ε` is a small real number.\n\n### Question\n\nA necessary condition for the stability of this 2x2 system is that all coefficients of its characteristic polynomial in `λ` are positive. Given `a, b, c > 0`, what condition must `ε` satisfy for the system to remain stable?", "model_evaluation_result": {"model_raw_response": "I need to analyze the stability of this 2×2 dynamic system by finding the characteristic polynomial and applying the stability condition.\n\nFirst, let me write the system in matrix form:\n```\nd/dt [x] = [-a   ε ] [x] + [m]\n      [y]   [ b  -c] [y]   [n]\n```\n\nThe coefficient matrix is:\n```\nA = [-a   ε ]\n    [ b  -c]\n```\n\nFor stability, I need to find the characteristic polynomial det(A - λI) = 0:\n\n```\ndet([-a-λ    ε  ]) = 0\n   ([ b    -c-λ])\n```\n\nExpanding the determinant:\n```\n(-a-λ)(-c-λ) - εb = 0\n(a+λ)(c+λ) - εb = 0\nac + aλ + cλ + λ² - εb = 0\nλ² + (a+c)λ + (ac-εb) = 0\n```\n\nSo the characteristic polynomial is:\n```\nλ² + (a+c)λ + (ac-εb) = 0\n```\n\nFor a 2×2 system to be stable, both eigenvalues must have negative real parts. For a quadratic polynomial λ² + pλ + q = 0, the Routh-Hurwitz stability criterion requires:\n1. All coefficients must be positive: p > 0 and q > 0\n\nFrom our characteristic polynomial:\n- Coefficient of λ: (a+c) > 0 ✓ (automatically satisfied since a, c > 0)\n- Constant term: (ac-εb) > 0\n\nTherefore, we need:\nac - εb > 0\nac > εb\n\nThe answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 494, "Question": "### Background\n\n**Research Question.** This problem analyzes the paper's central normative claim: that a systematic, state-contingent monetary policy can be designed to eliminate informational inefficiencies.\n\n**Setting.** In the baseline model, random money demand shocks (`ε_{2t}`) contaminate price signals. The monetary authority can commit to a policy rule where the money supply in the next period (`t+1`) is adjusted based on the realized economic shocks in the current period (`t`).\n\n### Data / Model Specification\n\nThe monetary authority implements the following feedback rule:\n\n```latex\nM_{t+1}^{s} = M_{t} + γ_1 ε_{1t} + γ_2 ε_{2t} \\quad \\text{(Eq. (1))}\n```\n\nThe objective is to choose the policy parameters `γ_1` and `γ_2` to perfectly stabilize the current price level `P̄_t` such that it is insulated from current-period shocks, resulting in an equilibrium where `P̄_t = M_t`.\n\nThe money market equilibrium condition is:\n\n```latex\n\\bar{P}_{t} = \\frac{1}{1+m_1}M_t + \\frac{m_1}{1+m_1}E^{*}(\\bar{P}_{t+1}) + \\frac{m_2 k_1}{(1+m_1)(k_1+k_2)}\\varepsilon_{1t} - \\frac{1}{1+m_1}\\varepsilon_{2t} \\quad \\text{(Eq. (2))}\n```\n\nUnder the proposed policy, agents form rational expectations of `P̄_{t+1}`. With a sufficient statistic for `ε_{1t}`, `E*(ε_{1t}) = ε_{1t}`. Agents' average expectation of the monetary shock, based on their private signals, is `E^{*}(ε_{2t}) = ε_{2t} / (1+σ_ρ^2)`, where `σ_ρ^2` is the variance of idiosyncratic money demand shocks.\n\n### The Question\n\nTo achieve the policy goal of `P̄_t = M_t`, the monetary authority must set the feedback parameter `γ_2` to a specific value. What is the correct expression for the optimal `γ_2`?", "Options": {"A": "```latex\n\\frac{(1+m_1)(1+\\sigma_{\\rho}^{2})}{m_1}\n```", "B": "```latex\n\\frac{1+m_1}{m_1}\n```", "C": "```latex\n\\frac{m_1}{(1+m_1)(1+\\sigma_{\\rho}^{2})}\n```", "D": "```latex\n-\\frac{m_2 k_1}{m_1(k_1+k_2)}\n```"}, "Answer": "A", "pi_justification": "This item uses the Computational Judgment strategy to test the core derivation from the original QA problem. The correct answer (A) requires correctly setting up the rational expectations equilibrium and solving for the parameter. The distractors are based on high-fidelity, plausible errors: (B) is a Step-Omission Error, where the user forgets to account for the noisy private signal (`σ_ρ^2`) when forming expectations of `ε_{2t}`; (C) is a Formula Misuse error from algebraic inversion; (D) is a Conceptual Confusion error, as it is the correct formula for the *other* policy parameter, `γ_1`.", "qid": "494", "question": "### Background\n\n**Research Question.** This problem analyzes the paper's central normative claim: that a systematic, state-contingent monetary policy can be designed to eliminate informational inefficiencies.\n\n**Setting.** In the baseline model, random money demand shocks (`ε_{2t}`) contaminate price signals. The monetary authority can commit to a policy rule where the money supply in the next period (`t+1`) is adjusted based on the realized economic shocks in the current period (`t`).\n\n### Data / Model Specification\n\nThe monetary authority implements the following feedback rule:\n\n```latex\nM_{t+1}^{s} = M_{t} + γ_1 ε_{1t} + γ_2 ε_{2t} \\quad \\text{(Eq. (1))}\n```\n\nThe objective is to choose the policy parameters `γ_1` and `γ_2` to perfectly stabilize the current price level `P̄_t` such that it is insulated from current-period shocks, resulting in an equilibrium where `P̄_t = M_t`.\n\nThe money market equilibrium condition is:\n\n```latex\n\\bar{P}_{t} = \\frac{1}{1+m_1}M_t + \\frac{m_1}{1+m_1}E^{*}(\\bar{P}_{t+1}) + \\frac{m_2 k_1}{(1+m_1)(k_1+k_2)}\\varepsilon_{1t} - \\frac{1}{1+m_1}\\varepsilon_{2t} \\quad \\text{(Eq. (2))}\n```\n\nUnder the proposed policy, agents form rational expectations of `P̄_{t+1}`. With a sufficient statistic for `ε_{1t}`, `E*(ε_{1t}) = ε_{1t}`. Agents' average expectation of the monetary shock, based on their private signals, is `E^{*}(ε_{2t}) = ε_{2t} / (1+σ_ρ^2)`, where `σ_ρ^2` is the variance of idiosyncratic money demand shocks.\n\n### The Question\n\nTo achieve the policy goal of `P̄_t = M_t`, the monetary authority must set the feedback parameter `γ_2` to a specific value. What is the correct expression for the optimal `γ_2`?", "model_evaluation_result": {"model_raw_response": "I need to find the optimal γ₂ that achieves the policy goal of P̄ₜ = Mₜ.\n\nGiven information:\n- Policy rule: M^s_{t+1} = Mₜ + γ₁ε₁ₜ + γ₂ε₂ₜ\n- Money market equilibrium: P̄ₜ = (1/(1+m₁))Mₜ + (m₁/(1+m₁))E*(P̄ₜ₊₁) + (m₂k₁/((1+m₁)(k₁+k₂)))ε₁ₜ - (1/(1+m₁))ε₂ₜ\n- Goal: P̄ₜ = Mₜ\n- E*(ε₁ₜ) = ε₁ₜ\n- E*(ε₂ₜ) = ε₂ₜ/(1+σ²ᵨ)\n\nUnder rational expectations, agents expect:\nE*(P̄ₜ₊₁) = E*(M^s_{t+1}) = Mₜ + γ₁E*(ε₁ₜ) + γ₂E*(ε₂ₜ)\nE*(P̄ₜ₊₁) = Mₜ + γ₁ε₁ₜ + γ₂ε₂ₜ/(1+σ²ᵨ)\n\nSubstituting into the equilibrium condition:\nP̄ₜ = (1/(1+m₁))Mₜ + (m₁/(1+m₁))[Mₜ + γ₁ε₁ₜ + γ₂ε₂ₜ/(1+σ²ᵨ)] + (m₂k₁/((1+m₁)(k₁+k₂)))ε₁ₜ -", "model_answer": "I need to find the optimal γ₂ that achieves the policy goal of P̄ₜ = Mₜ.\n\nGiven information:\n- Policy rule: M^s_{t+1} = Mₜ + γ₁ε₁ₜ + γ₂ε₂ₜ\n- Money market equilibrium: P̄ₜ = (1/(1+m₁))Mₜ + (m₁/(1+m₁))E*(P̄ₜ₊₁) + (m₂k₁/((1+m₁)(k₁+k₂)))ε₁ₜ - (1/(1+m₁))ε₂ₜ\n- Goal: P̄ₜ = Mₜ\n- E*(ε₁ₜ) = ε₁ₜ\n- E*(ε₂ₜ) = ε₂ₜ/(1+σ²ᵨ)\n\nUnder rational expectations, agents expect:\nE*(P̄ₜ₊₁) = E*(M^s_{t+1}) = Mₜ + γ₁E*(ε₁ₜ) + γ₂E*(ε₂ₜ)\nE*(P̄ₜ₊₁) = Mₜ + γ₁ε₁ₜ + γ₂ε₂ₜ/(1+σ²ᵨ)\n\nSubstituting into the equilibrium condition:\nP̄ₜ = (1/(1+m₁))Mₜ + (m₁/(1+m₁))[Mₜ + γ₁ε₁ₜ + γ₂ε₂ₜ/(1+σ²ᵨ)] + (m₂k₁/((1+m₁)(k₁+k₂)))ε₁ₜ -", "error": "Warning: Could not extract single choice letter from: I need to find the optimal γ₂ that achieves the po..."}}
{"ID": 481, "Question": "### Background\n\n**Research Question.** This problem examines the relative power of different tests for heteroskedasticity, comparing a general-purpose 'portmanteau' test against tests designed for a specific alternative hypothesis.\n\n**Setting / Institutional Environment.** The analysis is based on a Monte Carlo simulation where the true data generating process (DGP) exhibits heteroskedasticity arising from a random coefficients model. Specifically, the error variance is given by `Var(u_t) ∝ 1 + X_1t^2 + 9 X_2t^2`, making it a strongly increasing function of `X_2t^2`.\n\n**Variables & Parameters.**\n- `H`: The general White test for heteroskedasticity of unknown form.\n- `F2`: A Goldfeld-Quandt F-test where the data are ordered by the `X_1` regressor before splitting the sample.\n- `F3`: A Goldfeld-Quandt F-test where the data are ordered by the `X_2` regressor before splitting the sample.\n- `Power (5% level)`: The empirical rejection frequency of the test at a nominal 5% significance level when heteroskedasticity is present.\n\n---\n\n### Data / Model Specification\n\nThe following table summarizes the power of different heteroskedasticity tests to detect this specific form of heteroskedasticity in a sample of `n=100`.\n\n**Table 1: Power of Heteroskedasticity Tests (n=100, Case 6)**\n\n| Test | Description | Power (5% level) |\n| :--- | :--- | :--- |\n| H | White's general test | 0.589 |\n| F2 | Goldfeld-Quandt ordered by `X_1` | 0.185 |\n| F3 | Goldfeld-Quandt ordered by `X_2` | 0.713 |\n\n---\n\n### Question\n\nThe results in Table 1 show that the F3 test is substantially more powerful than the F2 test. Given the data generating process, what is the correct statistical explanation for this large difference in power?", "Options": {"A": "The F3 test orders observations by `X_2`, which maximally separates the sample into low-variance and high-variance groups, aligning perfectly with the true source of heteroskedasticity.", "B": "The F2 test is based on an F-distribution which is known to be less reliable in small samples than the chi-squared distribution used by the H test.", "C": "The Goldfeld-Quandt test procedure requires omitting the middle third of observations, which causes the F2 test to lose too much information to be effective.", "D": "The power of the F3 test is artificially inflated because the regressor `X_2` has a larger coefficient in the underlying variance function (9) than `X_1` (1)."}, "Answer": "A", "pi_justification": "This item uses a 'Reverse-Reasoning' strategy to test the core insight of the original QA problem's second part. It requires the user to connect the empirical result (F3 is more powerful) back to the underlying statistical mechanism (test design interacting with the DGP). The correct option (A) provides the precise explanation. Distractors are designed to be plausible but incorrect: (B) misattributes the issue to the test's distribution, (C) misattributes the issue to a general feature of the test rather than its specific application here, and (D) uses confusing language ('artificially inflated') to describe the very reason the test *should* be powerful, making it a subtle conceptual error.", "qid": "481", "question": "### Background\n\n**Research Question.** This problem examines the relative power of different tests for heteroskedasticity, comparing a general-purpose 'portmanteau' test against tests designed for a specific alternative hypothesis.\n\n**Setting / Institutional Environment.** The analysis is based on a Monte Carlo simulation where the true data generating process (DGP) exhibits heteroskedasticity arising from a random coefficients model. Specifically, the error variance is given by `Var(u_t) ∝ 1 + X_1t^2 + 9 X_2t^2`, making it a strongly increasing function of `X_2t^2`.\n\n**Variables & Parameters.**\n- `H`: The general White test for heteroskedasticity of unknown form.\n- `F2`: A Goldfeld-Quandt F-test where the data are ordered by the `X_1` regressor before splitting the sample.\n- `F3`: A Goldfeld-Quandt F-test where the data are ordered by the `X_2` regressor before splitting the sample.\n- `Power (5% level)`: The empirical rejection frequency of the test at a nominal 5% significance level when heteroskedasticity is present.\n\n---\n\n### Data / Model Specification\n\nThe following table summarizes the power of different heteroskedasticity tests to detect this specific form of heteroskedasticity in a sample of `n=100`.\n\n**Table 1: Power of Heteroskedasticity Tests (n=100, Case 6)**\n\n| Test | Description | Power (5% level) |\n| :--- | :--- | :--- |\n| H | White's general test | 0.589 |\n| F2 | Goldfeld-Quandt ordered by `X_1` | 0.185 |\n| F3 | Goldfeld-Quandt ordered by `X_2` | 0.713 |\n\n---\n\n### Question\n\nThe results in Table 1 show that the F3 test is substantially more powerful than the F2 test. Given the data generating process, what is the correct statistical explanation for this large difference in power?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand why the F3 test (ordering by X₂) has much higher power (0.713) than the F2 test (ordering by X₁) which only has power of 0.185.\n\nThe key is in the data generating process: `Var(u_t) ∝ 1 + X₁t² + 9X₂t²`\n\nThis shows that the error variance depends much more strongly on X₂ than on X₁ (coefficient of 9 vs 1).\n\nThe Goldfeld-Quandt test works by:\n1. Ordering observations by a chosen variable\n2. Splitting the sample (typically omitting middle observations)\n3. Comparing variances between the low and high subsamples\n\nFor this test to have high power, the ordering variable should create maximum separation between low-variance and high-variance observations.\n\nSince variance increases much more dramatically with X₂² (due to the coefficient of 9), ordering by X₂ will create the sharpest distinction between low-variance observations (small X₂ values) and high-variance observations (large X₂ values).\n\nOrdering by X₁ provides much less separation because X₁ has a weaker relationship with the error variance.\n\nOption A correctly identifies that F3 orders by X₂, which \"maximally separates the sample into low-variance and high-variance groups, aligning perfectly with the true source of heteroskedasticity.\"\n\nOptions B and C discuss technical aspects but don't explain the power difference between F2 and F3. Option D incorrectly suggests the power is \"artificially inflated\" when it's actually appropriately high due to the test design matching the true heteroskedasticity pattern.\n\nA", "model_answer": "A", "error": null}}
{"ID": 485, "Question": "### Background\n\n**Research Question.** This question tests the generalization of the Maximal Data Information Prior (MDIP) framework from a binomial to a trinomial process.\n\n### Data / Model Specification\n\nA single trial from a trinomial process has three possible outcomes with probabilities `θ₁`, `θ₂`, and `θ₃ = 1-θ₁-θ₂`. The general solution for the MDIP is `π* ∝ exp(I)`, where `I` is the Shannon information (the expected log-probability of the process).\n\nFor a single trial from a trinomial process with outcome probabilities `(θ₁, θ₂, 1-θ₁-θ₂)`, what is the functional form of the multivariate Maximal Data Information Prior (MDIP), `π*(θ₁, θ₂)`?", "Options": {"A": "```latex\n\\theta_1^{\\theta_1} \\theta_2^{\\theta_2} (1-\\theta_1-\\theta_2)^{1-\\theta_1-\\theta_2}\n```", "B": "```latex\n\\theta_1 \\ln(\\theta_1) + \\theta_2 \\ln(\\theta_2) + (1-\\theta_1-\\theta_2) \\ln(1-\\theta_1-\\theta_2)\n```", "C": "```latex\n\\theta_1^{\\alpha-1} \\theta_2^{\\beta-1} (1-\\theta_1-\\theta_2)^{\\gamma-1}\n```", "D": "```latex\n\\theta_1^{\\theta_1} + \\theta_2^{\\theta_2} + (1-\\theta_1-\\theta_2)^{1-\\theta_1-\\theta_2}\n```"}, "Answer": "A", "pi_justification": "This item tests the ability to generalize the MDIP derivation from the binomial to the trinomial case. It uses a Computational Judgment strategy. The distractor design mirrors that of the binomial item: B is the un-exponentiated information measure, C is the form of the standard conjugate prior (Dirichlet), and D represents a common algebraic error.", "qid": "485", "question": "### Background\n\n**Research Question.** This question tests the generalization of the Maximal Data Information Prior (MDIP) framework from a binomial to a trinomial process.\n\n### Data / Model Specification\n\nA single trial from a trinomial process has three possible outcomes with probabilities `θ₁`, `θ₂`, and `θ₃ = 1-θ₁-θ₂`. The general solution for the MDIP is `π* ∝ exp(I)`, where `I` is the Shannon information (the expected log-probability of the process).\n\nFor a single trial from a trinomial process with outcome probabilities `(θ₁, θ₂, 1-θ₁-θ₂)`, what is the functional form of the multivariate Maximal Data Information Prior (MDIP), `π*(θ₁, θ₂)`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to find the functional form of the MDIP for a trinomial process.\n\nGiven information:\n- Trinomial process with probabilities (θ₁, θ₂, θ₃ = 1-θ₁-θ₂)\n- MDIP formula: π* ∝ exp(I), where I is Shannon information\n- Shannon information I is the expected log-probability of the process\n\nFor a trinomial process, the Shannon information (expected log-probability) is:\nI = θ₁ ln(θ₁) + θ₂ ln(θ₂) + θ₃ ln(θ₃)\nI = θ₁ ln(θ₁) + θ₂ ln(θ₂) + (1-θ₁-θ₂) ln(1-θ₁-θ₂)\n\nSince π* ∝ exp(I), we have:\nπ*(θ₁, θ₂) ∝ exp[θ₁ ln(θ₁) + θ₂ ln(θ₂) + (1-θ₁-θ₂) ln(1-θ₁-θ₂)]\n\nUsing the property that exp(ln(x)) = x, this becomes:\nπ*(θ₁, θ₂) ∝ exp(θ₁ ln(θ₁)) × exp(θ₂ ln(θ₂)) × exp((1-θ₁-θ₂) ln(1-θ₁-θ₂))\nπ*(θ₁, θ₂) ∝ θ₁^θ₁ × θ₂^θ₂ × (1-θ₁-θ₂)^(1-θ₁-θ₂)\n\nThis matches option A exactly.\n\nOption B shows the Shannon information I itself, not exp(I).\nOption C shows a Dirichlet distribution form, not the MDIP.\nOption D shows a sum rather than a product", "model_answer": "A", "error": null}}
{"ID": 482, "Question": "### Background\n\nThe paper's results demonstrate that a statistical test for heteroskedasticity can be very powerful if its design is targeted to the true form of the problem (e.g., a Goldfeld-Quandt test ordered by the correct variable). However, the paper's main recommendation is to use the HC3 estimator by default, partly to avoid the 'pre-testing fallacy' where a researcher makes their choice of estimator (OLS vs. HC) dependent on a test result that may come from a low-power test.\n\n---\n\n### Question\n\nA researcher is estimating a wage equation and has a strong theoretical reason to suspect that the variance of the error term increases with the 'education' level of the individual. Based on the principles demonstrated in the paper, what is the most methodologically sound strategy for this researcher to follow?", "Options": {"A": "First, run a Goldfeld-Quandt test ordered by 'education'. If it is significant, use HC3 standard errors. Otherwise, use OLS standard errors.", "B": "Use the general White's H test to check for any form of heteroskedasticity, and only use HC3 standard errors if the H test is significant.", "C": "Perform a Goldfeld-Quandt test ordered by 'education' to diagnose and confirm the specific nature of the heteroskedasticity, but use the HC3 estimator for inference on the wage equation coefficients regardless of the test's p-value.", "D": "Since there is a strong prior, the researcher should immediately use HC3 standard errors and not bother with any diagnostic testing for heteroskedasticity."}, "Answer": "C", "pi_justification": "This item uses a 'Scenario Application' strategy to assess the nuanced, practical takeaway from the original QA problem. It tests whether the user can synthesize the value of targeted testing with the danger of the pre-testing fallacy. The correct option (C) represents the most sophisticated strategy: use the powerful, targeted test for diagnostic purposes but commit to the robust estimator for inference based on the initial theoretical suspicion. Distractors represent common errors: (A) is the pre-testing fallacy with a targeted test, (B) is the pre-testing fallacy with a less-powerful general test, and (D) is an overly simplistic interpretation of the paper's advice that misses the value of diagnostic testing for understanding the data.", "qid": "482", "question": "### Background\n\nThe paper's results demonstrate that a statistical test for heteroskedasticity can be very powerful if its design is targeted to the true form of the problem (e.g., a Goldfeld-Quandt test ordered by the correct variable). However, the paper's main recommendation is to use the HC3 estimator by default, partly to avoid the 'pre-testing fallacy' where a researcher makes their choice of estimator (OLS vs. HC) dependent on a test result that may come from a low-power test.\n\n---\n\n### Question\n\nA researcher is estimating a wage equation and has a strong theoretical reason to suspect that the variance of the error term increases with the 'education' level of the individual. Based on the principles demonstrated in the paper, what is the most methodologically sound strategy for this researcher to follow?", "model_evaluation_result": {"model_raw_response": "C", "model_answer": "C", "error": null}}
{"ID": 149, "Question": "## Background\n\n**Research Question.** This problem evaluates the experimental design used to test theoretical predictions about charitable competition, focusing on how the design isolates causal effects.\n\n**Setting.** A series of controlled laboratory experiments where subjects make real donation decisions. The design varies two key dimensions: the relationship between the charitable causes and the number of donors.\n\n---\n\n## Data / Model Specification\n\nThe experimental design is summarized in Table 1.\n\n**Table 1. Summary of the Experiments**\n| Experiment | No. of donors per recipient | Relationship between causes |\n| :--- | :--- | :--- |\n| Subs | 1 | Substitutes |\n| Comp | 1 | Complements |\n| Comp-W | 1 | Weak complements |\n| Subs-M | 2 | Substitutes |\n\n---\n\n## Question\n\nWhat is the primary causal question addressed by comparing the results of the `Subs` experiment to the `Subs-M` experiment?", "Options": {"A": "To test whether the \"business stealing\" effect is robust to the introduction of free-riding incentives.", "B": "To measure how the effect of a rebate campaign differs when charities are substitutes versus complements.", "C": "To determine the optimal rebate level that maximizes total donations for substitute charities.", "D": "To isolate the pure effect of warm-glow giving by removing altruistic motives."}, "Answer": "A", "pi_justification": "This item assesses understanding of experimental identification strategy, using a Reverse-Reasoning approach. The correct answer (A) correctly identifies that the only change between `Subs` and `Subs-M` is the number of donors, which introduces strategic free-riding concerns. Distractor B describes the `Subs` vs. `Comp` comparison (True but Irrelevant). Distractor C describes a question answered *within* an experiment, not by comparing these two. Distractor D is an 'Almost Right' conceptual error; this comparison *allows* a test of warm-glow vs. altruism, but the primary causal question isolated by the design is about the robustness of the main effect to the strategic environment.", "qid": "149", "question": "## Background\n\n**Research Question.** This problem evaluates the experimental design used to test theoretical predictions about charitable competition, focusing on how the design isolates causal effects.\n\n**Setting.** A series of controlled laboratory experiments where subjects make real donation decisions. The design varies two key dimensions: the relationship between the charitable causes and the number of donors.\n\n---\n\n## Data / Model Specification\n\nThe experimental design is summarized in Table 1.\n\n**Table 1. Summary of the Experiments**\n| Experiment | No. of donors per recipient | Relationship between causes |\n| :--- | :--- | :--- |\n| Subs | 1 | Substitutes |\n| Comp | 1 | Complements |\n| Comp-W | 1 | Weak complements |\n| Subs-M | 2 | Substitutes |\n\n---\n\n## Question\n\nWhat is the primary causal question addressed by comparing the results of the `Subs` experiment to the `Subs-M` experiment?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 146, "Question": "### Background\n\n**Research Question.** This problem investigates the drivers of the observed decline in 'world efficiency', a measure of how close the world's average productivity is to the technological frontier. The central question is whether this decline is due to falling productivity within countries or a reallocation of economic activity across countries.\n\n**Setting / Institutional Environment.** The study analyzes data for 38 economies from 1995 to 2011. A key empirical finding is that world efficiency in the market sector fell from 51% to 46%. This occurred as the share of world inputs used by the frontier country (the US) declined, while the shares of large, less-efficient emerging economies (China, India) increased.\n\n---\n\n### Data / Model Specification\n\nWorld efficiency is defined as the ratio of world average productivity to the productivity frontier:\n```latex\nE_t = \\frac{T_t}{T_{t,max}} = \\frac{\\sum_{k=1}^K \\omega_{kt} T_{kt}}{T_{t,max}} \\quad \\text{(Eq. (1))}\n```\nwhere `T_{kt}` is the TFP level for country `k` at time `t` and `\\omega_{kt}` is its share of world inputs.\n\nA formal decomposition of the total change in world average productivity (`\\Delta T_t`) separates it into a 'pure productivity effect' (holding input shares `\\omega` constant) and a 'pure composition effect' (holding productivity `T` constant).\n\n---\n\n### The Question\n\nIn this type of decomposition, the pure productivity effect and the pure composition effect often do not sum perfectly to the total observed change. What is the correct explanation for this discrepancy?", "Options": {"A": "The discrepancy is due to an 'interaction effect', which captures whether input shares grew more in countries that also experienced higher productivity growth.", "B": "The discrepancy is due to the use of the Törnqvist index, which is a statistical approximation and introduces measurement error.", "C": "The discrepancy is due to data revisions and inconsistencies in how TFP is measured across different countries and time periods.", "D": "There should be no discrepancy; if the calculations are done correctly, the two effects must sum to the total change by definition."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the student's deep understanding of decomposition methods, specifically the concept of an interaction term. Strategy: Reverse-Reasoning. Given a common empirical artifact (a decomposition gap), the student must infer the correct statistical cause. Distractor Logic: A is the correct statistical reason. B (Formula Misuse) and C (True but Irrelevant) point to other potential data issues but are not the mathematical reason for the decomposition gap. D (Conceptual Opposite) represents a common but incorrect belief that such decompositions are perfectly additive.", "qid": "146", "question": "### Background\n\n**Research Question.** This problem investigates the drivers of the observed decline in 'world efficiency', a measure of how close the world's average productivity is to the technological frontier. The central question is whether this decline is due to falling productivity within countries or a reallocation of economic activity across countries.\n\n**Setting / Institutional Environment.** The study analyzes data for 38 economies from 1995 to 2011. A key empirical finding is that world efficiency in the market sector fell from 51% to 46%. This occurred as the share of world inputs used by the frontier country (the US) declined, while the shares of large, less-efficient emerging economies (China, India) increased.\n\n---\n\n### Data / Model Specification\n\nWorld efficiency is defined as the ratio of world average productivity to the productivity frontier:\n```latex\nE_t = \\frac{T_t}{T_{t,max}} = \\frac{\\sum_{k=1}^K \\omega_{kt} T_{kt}}{T_{t,max}} \\quad \\text{(Eq. (1))}\n```\nwhere `T_{kt}` is the TFP level for country `k` at time `t` and `\\omega_{kt}` is its share of world inputs.\n\nA formal decomposition of the total change in world average productivity (`\\Delta T_t`) separates it into a 'pure productivity effect' (holding input shares `\\omega` constant) and a 'pure composition effect' (holding productivity `T` constant).\n\n---\n\n### The Question\n\nIn this type of decomposition, the pure productivity effect and the pure composition effect often do not sum perfectly to the total observed change. What is the correct explanation for this discrepancy?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 245, "Question": "### Background\n\n**Research Question:** This problem investigates the robustness of the Generalized Stochastic Simulation Algorithm (GSSA) by examining its performance when model parameters, such as risk aversion, create numerical challenges.\n\n**Setting / Institutional Environment:** When the coefficient of relative risk aversion (`γ`) is high, the GSSA's fixed-point iteration can become unstable. The paper tests two distinct strategies to restore stability: (1) reducing the damping parameter `ξ` in the update step, and (2) changing the policy function being approximated from capital to marginal utility.\n\n---\n\n### Data / Model Specification\n\nThe following table shows the performance of these two strategies for a model with high risk aversion (`γ=10`) using a 5th degree polynomial.\n\n**Table 1: Stability under High Risk Aversion (γ=10)**\n\n| Policy Function Approximated | Damping `ξ` | `E_mean` (log10) | CPU (sec) |\n| :--- | :---: | :---: | :---: |\n| Capital | 0.01 | -7.13 | 580 |\n| Marginal Utility | 0.5 | -5.41 | 3102 |\n\n---\n\n### Question\n\nBased on the data in Table 1, which strategy for restoring stability under high risk aversion is superior, and why?\n", "Options": {"A": "Reducing the damping parameter is superior because it achieves a significantly higher accuracy (lower error) and is more than five times faster.", "B": "Approximating the marginal utility function is superior because it allows for a much larger damping parameter, leading to faster convergence.", "C": "Reducing the damping parameter is superior in accuracy, but approximating marginal utility is superior in speed, presenting a trade-off.", "D": "Both strategies are equally effective, as they both successfully restore convergence and achieve acceptable levels of accuracy."}, "Answer": "A", "pi_justification": "This item assesses the comparative analysis from Part 2 of the original QA. It uses a Computational Judgment strategy, requiring the student to interpret the table to determine the superior method based on multiple criteria (accuracy and speed). Distractor B is a Conceptual Opposite; while it allows a larger `ξ`, the actual CPU time shows it does not lead to faster convergence. Distractor C incorrectly identifies the speed comparison. Distractor D is an 'Almost Right' option that ignores the very large quantitative differences in performance, failing to make the required judgment.", "qid": "245", "question": "### Background\n\n**Research Question:** This problem investigates the robustness of the Generalized Stochastic Simulation Algorithm (GSSA) by examining its performance when model parameters, such as risk aversion, create numerical challenges.\n\n**Setting / Institutional Environment:** When the coefficient of relative risk aversion (`γ`) is high, the GSSA's fixed-point iteration can become unstable. The paper tests two distinct strategies to restore stability: (1) reducing the damping parameter `ξ` in the update step, and (2) changing the policy function being approximated from capital to marginal utility.\n\n---\n\n### Data / Model Specification\n\nThe following table shows the performance of these two strategies for a model with high risk aversion (`γ=10`) using a 5th degree polynomial.\n\n**Table 1: Stability under High Risk Aversion (γ=10)**\n\n| Policy Function Approximated | Damping `ξ` | `E_mean` (log10) | CPU (sec) |\n| :--- | :---: | :---: | :---: |\n| Capital | 0.01 | -7.13 | 580 |\n| Marginal Utility | 0.5 | -5.41 | 3102 |\n\n---\n\n### Question\n\nBased on the data in Table 1, which strategy for restoring stability under high risk aversion is superior, and why?\n", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 493, "Question": "### Background\n\n**Research Question.** This problem analyzes the paper's central normative claim: that a systematic, state-contingent monetary policy can be designed to eliminate informational inefficiencies.\n\n**Setting.** In the baseline model, random money demand shocks (`ε_{2t}`) contaminate price signals. The monetary authority can commit to a policy rule where the money supply in the next period (`t+1`) is adjusted based on the realized economic shocks in the current period (`t`).\n\n### Data / Model Specification\n\nThe monetary authority implements the following feedback rule:\n\n```latex\nM_{t+1}^{s} = M_{t} + γ_1 ε_{1t} + γ_2 ε_{2t} \\quad \\text{(Eq. (1))}\n```\n\nThe objective is to choose the policy parameters `γ_1` and `γ_2` to perfectly stabilize the current price level `P̄_t` such that it is insulated from current-period shocks, resulting in an equilibrium where `P̄_t = M_t`.\n\n### The Question\n\nThe policy rule in Eq. (1) adjusts the *future* money supply `M^s_{t+1}` based on *current* shocks. What is the primary economic mechanism through which this commitment influences the *current* price level `P̄_t`?", "Options": {"A": "It alters agents' expectations of future inflation, changing the expected real return on holding money today.", "B": "It directly injects money into the economy in period `t` to counteract the money demand shock.", "C": "It causes the real price of capital `(V_t - P̄_t)` to become a sufficient statistic for the real productivity shock.", "D": "It increases the expected real return on money, encouraging agents to hold the fixed money supply."}, "Answer": "A", "pi_justification": "This item assesses understanding of the policy's core transmission mechanism (the expectations channel). It replaces the explanatory part of a larger QA problem. The distractors are designed to pinpoint specific misunderstandings: (B) is a timing error, confusing future action with current action; (C) identifies a correct *consequence* of the policy, not its mechanism; (D) represents a conceptual opposite, misstating the direction of the effect on the return to money.", "qid": "493", "question": "### Background\n\n**Research Question.** This problem analyzes the paper's central normative claim: that a systematic, state-contingent monetary policy can be designed to eliminate informational inefficiencies.\n\n**Setting.** In the baseline model, random money demand shocks (`ε_{2t}`) contaminate price signals. The monetary authority can commit to a policy rule where the money supply in the next period (`t+1`) is adjusted based on the realized economic shocks in the current period (`t`).\n\n### Data / Model Specification\n\nThe monetary authority implements the following feedback rule:\n\n```latex\nM_{t+1}^{s} = M_{t} + γ_1 ε_{1t} + γ_2 ε_{2t} \\quad \\text{(Eq. (1))}\n```\n\nThe objective is to choose the policy parameters `γ_1` and `γ_2` to perfectly stabilize the current price level `P̄_t` such that it is insulated from current-period shocks, resulting in an equilibrium where `P̄_t = M_t`.\n\n### The Question\n\nThe policy rule in Eq. (1) adjusts the *future* money supply `M^s_{t+1}` based on *current* shocks. What is the primary economic mechanism through which this commitment influences the *current* price level `P̄_t`?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 293, "Question": "### Background\n\nThis problem examines the construction of alternative measures of market concentration used to study the structure-performance relationship in multi-product industries. The unit of observation is an \"input-output industry,\" which may contain multiple distinct products.\n\n### Data / Model Specification\n\nThe author proposes a novel concentration ratio, `K`, defined as:\n\n```latex\nK = \\frac{s_k}{S} \\quad \\text{(Eq. (1))}\n```\n\nWhere:\n- `s_k`: The value of sales by the five largest firms in the single product with the highest 5-firm sales within an input-output industry.\n- `S`: The value of total sales of all products within the input-output industry.\n\n### Question\n\nThe author states that the K-ratio, as defined in Eq. (1), likely underestimates the true 5-firm concentration ratio for the input-output industry. What is the primary reason for this downward bias?", "Options": {"A": "The K-ratio only considers the sales of the top five firms in the single 'lead' product, ignoring their potentially large sales in other products within the same industry.", "B": "The K-ratio overestimates concentration because it divides by the total sales of all products (S), which is too large a denominator.", "C": "The Herfindahl index is a theoretically superior measure of market structure.", "D": "The K-ratio is based on sales data, which is less accurate than employment data for measuring concentration."}, "Answer": "A", "pi_justification": "Assessment Target: This item assesses the student's understanding of the specific measurement bias in the author's novel `K` variable, as described in the paper. Strategy: Conceptual Judgment. Distractor Logic: (B) Conceptual Opposite: presents incorrect logic about the formula's components. (C) True but Irrelevant: states a correct fact from the paper that does not answer the specific question about bias. (D) Formula Misuse: raises a plausible but generic data concern that is not the specific reason for the bias identified by the author.", "qid": "293", "question": "### Background\n\nThis problem examines the construction of alternative measures of market concentration used to study the structure-performance relationship in multi-product industries. The unit of observation is an \"input-output industry,\" which may contain multiple distinct products.\n\n### Data / Model Specification\n\nThe author proposes a novel concentration ratio, `K`, defined as:\n\n```latex\nK = \\frac{s_k}{S} \\quad \\text{(Eq. (1))}\n```\n\nWhere:\n- `s_k`: The value of sales by the five largest firms in the single product with the highest 5-firm sales within an input-output industry.\n- `S`: The value of total sales of all products within the input-output industry.\n\n### Question\n\nThe author states that the K-ratio, as defined in Eq. (1), likely underestimates the true 5-firm concentration ratio for the input-output industry. What is the primary reason for this downward bias?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 458, "Question": "### Background\n\n**Research Question.** This problem examines the microfoundations and econometric specification of the structural model used to estimate water demand under non-linear prices.\n\n**Setting / Institutional Environment.** A household's choice of water consumption `w` is modeled by maximizing a utility function subject to a piecewise-linear budget constraint. The model must account for observed characteristics `Z`, unobserved taste heterogeneity `η`, and random optimization errors `ε`.\n\n---\n\n### Data / Model Specification\n\nThe econometric model for observed consumption `w_it` incorporates two distinct error terms:\n\n```latex\nw_{it} = w^*(P(\\cdot)) + \\varepsilon_{it} = \\begin{cases} Z_{it}\\delta + \\alpha P_{it} + \\gamma Y_{it} + \\eta_{it} + \\varepsilon_{it} & \\text{if the optimum is on a segment} \\\\ \\bar{w}_{k} + \\varepsilon_{it} & \\text{if the optimum is at kink } k \\end{cases} \\quad \\text{(Eq. 1)}\n```\n\nFor preferences to be convex and demand to be unique, the condition `γw + α < 0` must hold. This is enforced by truncating the distribution of the taste shock `η`.\n\n---\n\n### Question\n\nThe model's ability to separately identify the variance of the preference shock (`η`) and the optimization error (`ε`) is a key feature. Which of the following correctly describes the identification strategy?", "Options": {"A": "Variation in consumption observed *around* known, fixed kink points directly identifies the distribution of the optimization error `ε`, while the remaining variation in consumption *on segments* identifies the preference shock `η`.", "B": "Variation in consumption observed *on segments* directly identifies the distribution of the optimization error `ε`, while the clustering of consumption *at kink points* identifies the preference shock `η`.", "C": "The model relies on the large number of observations and the significant price variation across 20 different tariff schedules to ensure the two error terms are uncorrelated.", "D": "The two error terms cannot be separately identified; the model only identifies the variance of their sum, `η + ε`, which is a limitation of the structural approach."}, "Answer": "A", "pi_justification": "This question assesses understanding of the model's identification strategy, replacing Q2 and Q3 of the source QA. It uses a Reverse-Reasoning strategy. Distractor B (Almost Right) reverses the core logic of the identification. Distractor C (True but Irrelevant) states a true fact about the data that is necessary for overall estimation but does not explain the specific mechanism for separating the error terms. Distractor D (Conceptual Opposite) describes the problem in standard linear models that this structural model is specifically designed to overcome.", "qid": "458", "question": "### Background\n\n**Research Question.** This problem examines the microfoundations and econometric specification of the structural model used to estimate water demand under non-linear prices.\n\n**Setting / Institutional Environment.** A household's choice of water consumption `w` is modeled by maximizing a utility function subject to a piecewise-linear budget constraint. The model must account for observed characteristics `Z`, unobserved taste heterogeneity `η`, and random optimization errors `ε`.\n\n---\n\n### Data / Model Specification\n\nThe econometric model for observed consumption `w_it` incorporates two distinct error terms:\n\n```latex\nw_{it} = w^*(P(\\cdot)) + \\varepsilon_{it} = \\begin{cases} Z_{it}\\delta + \\alpha P_{it} + \\gamma Y_{it} + \\eta_{it} + \\varepsilon_{it} & \\text{if the optimum is on a segment} \\\\ \\bar{w}_{k} + \\varepsilon_{it} & \\text{if the optimum is at kink } k \\end{cases} \\quad \\text{(Eq. 1)}\n```\n\nFor preferences to be convex and demand to be unique, the condition `γw + α < 0` must hold. This is enforced by truncating the distribution of the taste shock `η`.\n\n---\n\n### Question\n\nThe model's ability to separately identify the variance of the preference shock (`η`) and the optimization error (`ε`) is a key feature. Which of the following correctly describes the identification strategy?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 438, "Question": "### Background\n\n**Research Question:** This question analyzes the welfare implications of using asymmetric reserve prices, specifically how lowering the reserve price for one group can create a Pareto improvement for all active bidders.\n\n**Setting / Institutional Environment:** Consider a symmetric first-price auction with two groups of bidders (1 and 2) facing reserve prices `r_1` and `r_2` respectively, with `r_1 < r_2`. The auction is in a semi-separating equilibrium. The paper finds that if the reserve price for the favored group-1 bidders (`r_1`) is lowered while `r_2` is held fixed, the expected welfare of the unfavored group-2 bidders surprisingly increases.\n\n### The Question\n\nAccording to the paper's analysis, which of the following best explains the economic mechanism through which unfavored group-2 bidders benefit from this policy change?", "Options": {"A": "The change incentivizes group-1 bidders with lower valuations to bid less aggressively and delays their entry into direct competition with group-2 bidders, softening the overall competitive pressure on group 2.", "B": "Group-1 bidders are forced to bid more aggressively to signal their strength despite the lower reserve price, which paradoxically makes it easier for high-valuation group-2 bidders to win.", "C": "The lower `r_1` increases the total number of bidders participating in the auction, which increases the seller's expected revenue, and these gains are then shared with all bidders.", "D": "The bidding strategy of group-2 bidders, `β_2(s)`, is directly a function of `r_1`, and a lower `r_1` mechanically allows them to place lower winning bids for any given valuation."}, "Answer": "A", "pi_justification": "This Single Choice item assesses the student's grasp of the core economic intuition behind Theorem 4, a key and counter-intuitive welfare result. It uses a Reverse-Reasoning strategy, asking for the cause of a given effect. The distractors are designed to be highly plausible: (B) is the conceptual opposite, representing a common intuitive error; (C) confuses revenue with welfare and misattributes the cause; (D) points to a direct functional dependence that does not exist, testing a more technical misunderstanding of the model.", "qid": "438", "question": "### Background\n\n**Research Question:** This question analyzes the welfare implications of using asymmetric reserve prices, specifically how lowering the reserve price for one group can create a Pareto improvement for all active bidders.\n\n**Setting / Institutional Environment:** Consider a symmetric first-price auction with two groups of bidders (1 and 2) facing reserve prices `r_1` and `r_2` respectively, with `r_1 < r_2`. The auction is in a semi-separating equilibrium. The paper finds that if the reserve price for the favored group-1 bidders (`r_1`) is lowered while `r_2` is held fixed, the expected welfare of the unfavored group-2 bidders surprisingly increases.\n\n### The Question\n\nAccording to the paper's analysis, which of the following best explains the economic mechanism through which unfavored group-2 bidders benefit from this policy change?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 158, "Question": "### Background\n\n**Setting.** The paper's empirical analysis begins by estimating a regression of individual bids on various bond and auction characteristics to control for observable heterogeneity before estimating the structural auction model. A selection of results from this regression (Specification I) is shown in Table 1.\n\n### Data / Model Specification\n\n**Table 1: Selected Coefficients from First-Stage Bid Regression**\n| Variable                  | Coefficient | S.E.   |\n| :------------------------ | :---------- | :----- |\n| *(Low market uncertainty)*|             |        |\n| High market uncertainty   | -0.1775**   | .0173  |\n| *(AAA credit rating)*     |             |        |\n| A credit rating           | 6.7172**    | .5895  |\n| AA credit rating          | 5.3292**    | .5544  |\n\n*Note: `**p<.01`; reference case in parentheses.*\n\n### Question\n\nThe results in Table 1 show that bonds with lower credit ratings (e.g., 'A') attract significantly higher bids than 'AAA' rated bonds. Which mechanism, discussed in the paper, best explains this seemingly counterintuitive result?", "Options": {"A": "The credit ratings are systematically inaccurate, and bidders' private information correctly identifies these bonds as less risky.", "B": "Lower-rated bonds have fewer substitutes, allowing the winning underwriter to earn larger monopoly rents in the secondary market.", "C": "Investors in the secondary market have a strong preference for higher-risk, higher-yield assets.", "D": "Government regulations provide implicit guarantees for lower-rated municipal bonds, reducing their effective risk."}, "Answer": "B", "pi_justification": "This item tests the ability to recall and understand a specific economic explanation for a puzzling empirical result from the paper. It uses a Reverse-Reasoning strategy. Distractor A suggests a data problem, which is a common first thought but not the paper's argument. Distractor C is a demand-side story that is plausible but not the supply-side market power argument made in the paper. Distractor D introduces an external fact that is generally untrue and not mentioned in the paper.", "qid": "158", "question": "### Background\n\n**Setting.** The paper's empirical analysis begins by estimating a regression of individual bids on various bond and auction characteristics to control for observable heterogeneity before estimating the structural auction model. A selection of results from this regression (Specification I) is shown in Table 1.\n\n### Data / Model Specification\n\n**Table 1: Selected Coefficients from First-Stage Bid Regression**\n| Variable                  | Coefficient | S.E.   |\n| :------------------------ | :---------- | :----- |\n| *(Low market uncertainty)*|             |        |\n| High market uncertainty   | -0.1775**   | .0173  |\n| *(AAA credit rating)*     |             |        |\n| A credit rating           | 6.7172**    | .5895  |\n| AA credit rating          | 5.3292**    | .5544  |\n\n*Note: `**p<.01`; reference case in parentheses.*\n\n### Question\n\nThe results in Table 1 show that bonds with lower credit ratings (e.g., 'A') attract significantly higher bids than 'AAA' rated bonds. Which mechanism, discussed in the paper, best explains this seemingly counterintuitive result?", "model_evaluation_result": {"model_raw_response": "B", "model_answer": "B", "error": null}}
{"ID": 67, "Question": "### Background\n\nAn instrumental variable (IV) strategy is used to estimate the causal effect of black in-migration (`\\Delta B`) on white population change (`\\Delta W`). A potential threat to this strategy is that the instrument is mechanically correlated with the number of black residents in a neighborhood in 1900. If neighborhoods with larger initial black populations had different pre-existing trends in white population change, this could bias the results. To test this, the model is re-estimated including the number of blacks in 1900 as a control variable.\n\n### Data / Model Specification\n\n**Table 1: IV Robustness Check for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) Baseline IV | (2) IV with Control |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -3.389*** | -2.910*** |\n| | (0.386) | (0.644) |\n| Number of blacks in 1900 | | -0.343 |\n| | | (0.358) |\n\n*Notes: Spatial standard errors in parentheses. Both regressions include city fixed effects.*\n\n### Question\n\nWhat does the comparison of the coefficient on `\\Delta B` between column (1) and column (2) imply about the validity of the IV strategy?", "Options": {"A": "The coefficient on `\\Delta B` remains large, significant, and relatively stable, suggesting the main finding of substantial white flight is not primarily driven by confounding factors associated with the initial size of the black population.", "B": "The coefficient on `\\Delta B` decreases, indicating that the baseline IV estimate was biased and that the true causal effect is much smaller.", "C": "The standard error on `\\Delta B` increases substantially, which indicates that the instrument becomes weak once the control variable is included.", "D": "The coefficient on `Number of blacks in 1900` is not statistically significant, proving that there was no correlation between the instrument and initial neighborhood characteristics."}, "Answer": "A", "pi_justification": "Assessment Target: Interpreting the result of a key robustness check for an IV model. Strategy: Reverse-Reasoning. Distractor Logic: The correct answer (A) accurately interprets the stability of the coefficient as a sign of robustness against this specific threat. Distractor (B) is an over-interpretation of the modest change; the effect is still very large and economically similar. Distractor (C) confuses an increase in standard error (expected with more controls and a smaller sample) with a weak instrument problem (which is assessed with the first-stage F-statistic). Distractor (D) misinterprets the purpose of the check; the goal is to see the effect on the main coefficient, not to test the significance of the control itself.", "qid": "67", "question": "### Background\n\nAn instrumental variable (IV) strategy is used to estimate the causal effect of black in-migration (`\\Delta B`) on white population change (`\\Delta W`). A potential threat to this strategy is that the instrument is mechanically correlated with the number of black residents in a neighborhood in 1900. If neighborhoods with larger initial black populations had different pre-existing trends in white population change, this could bias the results. To test this, the model is re-estimated including the number of blacks in 1900 as a control variable.\n\n### Data / Model Specification\n\n**Table 1: IV Robustness Check for 1920-1930 Decade**\n\n| Dependent Variable: Change in White Population | (1) Baseline IV | (2) IV with Control |\n| :--- | :---: | :---: |\n| Change in black population (`\\Delta B`) | -3.389*** | -2.910*** |\n| | (0.386) | (0.644) |\n| Number of blacks in 1900 | | -0.343 |\n| | | (0.358) |\n\n*Notes: Spatial standard errors in parentheses. Both regressions include city fixed effects.*\n\n### Question\n\nWhat does the comparison of the coefficient on `\\Delta B` between column (1) and column (2) imply about the validity of the IV strategy?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 135, "Question": "## Background\n\n**Research Question.** This problem investigates why a profit-maximizing platform might subsidize transactions between its users when they bargain under asymmetric information.\n\n**Setting.** A platform facilitates interactions where end-users (buyers B, sellers S) bargain over a transaction price. Their ex-post benefits `b^i` are private information. The platform sets a total usage fee `a = a^B + a^S` and has a marginal cost `c` per transaction.\n\n---\n\n## Data / Model Specification\n\nIn markets where end-users bargain under asymmetric information, trade is often inefficiently low due to bargaining failures (e.g., monopoly price-setting by one party). Analysis shows that a profit-maximizing platform in such a market will optimally set its total usage fee `a*` to be less than its marginal cost `c`, effectively subsidizing every transaction.\n\n---\n\n## Question\n\nWhat is the primary economic rationale for a profit-maximizing platform to subsidize transactions (`a* < c`)?", "Options": {"A": "The subsidy corrects for user-level bargaining failures, increasing the total surplus which the platform can then extract via higher membership fees.", "B": "The subsidy acts as a predatory price, designed to drive competing platforms out of the market and establish a monopoly.", "C": "The platform has non-profit objectives and is willing to lose money on transactions to maximize social welfare or user participation.", "D": "The subsidy boosts transaction volume to a point where the high volume of fee revenue (`a*` per transaction) outweighs the per-transaction loss (`a* - c`)."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of the economic intuition behind subsidizing transactions. It uses a Reverse-Reasoning strategy. The correct answer identifies the two-part logic: correct a user-level market failure to increase total surplus, then capture that surplus via membership fees. Distractor B (True but Irrelevant) proposes an alternative motive not supported by the monopoly model. Distractor C (Conceptual Opposite) contradicts the profit-maximization assumption. Distractor D (Almost Right) correctly identifies the volume increase but misses the crucial role of membership fees in capturing the generated surplus, which is the core of the paper's argument.", "qid": "135", "question": "## Background\n\n**Research Question.** This problem investigates why a profit-maximizing platform might subsidize transactions between its users when they bargain under asymmetric information.\n\n**Setting.** A platform facilitates interactions where end-users (buyers B, sellers S) bargain over a transaction price. Their ex-post benefits `b^i` are private information. The platform sets a total usage fee `a = a^B + a^S` and has a marginal cost `c` per transaction.\n\n---\n\n## Data / Model Specification\n\nIn markets where end-users bargain under asymmetric information, trade is often inefficiently low due to bargaining failures (e.g., monopoly price-setting by one party). Analysis shows that a profit-maximizing platform in such a market will optimally set its total usage fee `a*` to be less than its marginal cost `c`, effectively subsidizing every transaction.\n\n---\n\n## Question\n\nWhat is the primary economic rationale for a profit-maximizing platform to subsidize transactions (`a* < c`)?", "model_evaluation_result": {"model_raw_response": "A", "model_answer": "A", "error": null}}
{"ID": 198, "Question": "### Background\n\n**Research Question.** This problem investigates the empirical relationship between parents' marriage type (homogamous vs. interfaith) and the effectiveness of religious transmission to their children, shedding light on marriage as a mechanism for purposeful socialization.\n\n**Setting.** The analysis uses U.S. survey data on the religious affiliation of parents and their children. The data allows for a direct comparison of socialization outcomes between families where parents share the same religion and families where they do not.\n\n**Variables and Parameters.**\n- **Marriage Type**: A categorical variable indicating the religious combination of parents. We focus on PP (Protestant-Protestant), CC (Catholic-Catholic), JJ (Jewish-Jewish), and PC (Protestant-Catholic).\n- **Child's Religion**: A categorical variable for the child's religious identity (Protestant, Catholic, Jewish, or Other).\n- **Socialization Probability**: The conditional probability that a child from a given marriage type adopts a specific religion, as reported in the table.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Socialization Probabilities for Selected Marriage Types**\n\n| Marriage Type | Child is Protestant | Child is Catholic | Child is Jew | Child is Other |\n| :--- | :---: | :---: | :---: | :---: |\n| PP (Prot.-Prot.) | **0.9179** | 0.0284 | 0.0000 | 0.0537 |\n| CC (Cath.-Cath.) | 0.0850 | **0.8571** | 0.0034 | 0.0544 |\n| JJ (Jew-Jew) | 0.0370 | 0.0000 | **0.9259** | 0.0370 |\n| PC (Prot.-Cath.) | 0.5116 | 0.3140 | 0.0000 | 0.1744 |\n\n*Source: Table 2 in the original paper. Each cell reports the sample probability that a child from the row's marriage type is a member of the column's religious group.*\n\n---\n\n### Question\n\nThe 'homogamy premium' for a religious group can be defined as the increase in the probability of a child adopting that group's faith when raised in a homogamous marriage (e.g., PP or CC) versus a specific interfaith marriage (PC). Based on Table 1, which statement accurately compares the homogamy premia for Protestants and Catholics when the alternative is a PC marriage?", "Options": {"A": "The homogamy premium is larger for Catholics than for Protestants.", "B": "The homogamy premium is larger for Protestants than for Catholics.", "C": "Protestants have a higher socialization success rate in homogamous marriages than Catholics do.", "D": "The homogamy premia for both groups are nearly identical."}, "Answer": "A", "pi_justification": "This item tests the ability to extract specific data points from a table, perform a two-step calculation (subtraction for each group), and then make a comparison. It uses a Computational Judgment strategy. Distractor (B) represents the conceptual opposite and a likely error. Distractor (C) presents a true but irrelevant fact from the table, testing if the student correctly understood the definition of the 'premium'. Distractor (D) targets simple calculation errors.", "qid": "198", "question": "### Background\n\n**Research Question.** This problem investigates the empirical relationship between parents' marriage type (homogamous vs. interfaith) and the effectiveness of religious transmission to their children, shedding light on marriage as a mechanism for purposeful socialization.\n\n**Setting.** The analysis uses U.S. survey data on the religious affiliation of parents and their children. The data allows for a direct comparison of socialization outcomes between families where parents share the same religion and families where they do not.\n\n**Variables and Parameters.**\n- **Marriage Type**: A categorical variable indicating the religious combination of parents. We focus on PP (Protestant-Protestant), CC (Catholic-Catholic), JJ (Jewish-Jewish), and PC (Protestant-Catholic).\n- **Child's Religion**: A categorical variable for the child's religious identity (Protestant, Catholic, Jewish, or Other).\n- **Socialization Probability**: The conditional probability that a child from a given marriage type adopts a specific religion, as reported in the table.\n\n---\n\n### Data / Model Specification\n\n**Table 1. Socialization Probabilities for Selected Marriage Types**\n\n| Marriage Type | Child is Protestant | Child is Catholic | Child is Jew | Child is Other |\n| :--- | :---: | :---: | :---: | :---: |\n| PP (Prot.-Prot.) | **0.9179** | 0.0284 | 0.0000 | 0.0537 |\n| CC (Cath.-Cath.) | 0.0850 | **0.8571** | 0.0034 | 0.0544 |\n| JJ (Jew-Jew) | 0.0370 | 0.0000 | **0.9259** | 0.0370 |\n| PC (Prot.-Cath.) | 0.5116 | 0.3140 | 0.0000 | 0.1744 |\n\n*Source: Table 2 in the original paper. Each cell reports the sample probability that a child from the row's marriage type is a member of the column's religious group.*\n\n---\n\n### Question\n\nThe 'homogamy premium' for a religious group can be defined as the increase in the probability of a child adopting that group's faith when raised in a homogamous marriage (e.g., PP or CC) versus a specific interfaith marriage (PC). Based on Table 1, which statement accurately compares the homogamy premia for Protestants and Catholics when the alternative is a PC marriage?", "model_evaluation_result": {"model_raw_response": "I need to calculate the homogamy premium for both Protestants and Catholics by comparing their socialization probabilities in homogamous marriages versus the PC (Protestant-Catholic) interfaith marriage.\n\nFor Protestants:\n- Homogamous marriage (PP): Probability child is Protestant = 0.9179\n- Interfaith marriage (PC): Probability child is Protestant = 0.5116\n- Protestant homogamy premium = 0.9179 - 0.5116 = 0.4063\n\nFor Catholics:\n- Homogamous marriage (CC): Probability child is Catholic = 0.8571\n- Interfaith marriage (PC): Probability child is Catholic = 0.3140\n- Catholic homogamy premium = 0.8571 - 0.3140 = 0.5431\n\nComparing the premia:\n- Protestant homogamy premium = 0.4063\n- Catholic homogamy premium = 0.5431\n\nThe Catholic homogamy premium (0.5431) is larger than the Protestant homogamy premium (0.4063).\n\nA", "model_answer": "A", "error": null}}
{"ID": 420, "Question": "### Background\n\n**Research Question.** This problem examines the causal impact of public exposure for low performance on an individual's subsequent intrinsic motivation and effort.\n\n**Setting and Sample.** The study uses a multi-part laboratory experiment where subjects perform a real-effort task. In Part 3, the lowest-ranked performer (Rank 3) is publicly exposed. This exposure event is the 'treatment'. Part 4 measures performance after the treatment. The analysis focuses on the subsample of individuals who were the least performers in Part 3, comparing those who were actually exposed to those whose exposure was waived by their peers.\n\n### Data / Model Specification\n\nTable 1 presents the mean performance for the subsample of least performers, broken down by whether they were exposed in Part 3.\n\n**Table 1: Mean Performance of Least Performers in Part 3**\n\n| Group       | Part 1 | Part 2 | Part 3 | Part 4 | N  |\n|-------------|--------|--------|--------|--------|----|\n| Not exposed | 116.22 | 123.80 | 126.58 | 130.54 | 81 |\n| Exposed     | 121.25 | 123.47 | 127.39 | 127.71 | 51 |\n\n### Question\n\nA key assumption for a Difference-in-Differences (DiD) analysis is that the treatment and control groups had parallel trends before the treatment. Based on the data in Table 1, first evaluate this assumption by comparing the performance change from Part 1 to Part 2 for the 'Exposed' versus 'Not exposed' groups. Then, calculate the DiD estimate of the effect of exposure on performance, using Part 3 as the pre-treatment period and Part 4 as the post-treatment period. Which of the following statements is correct?", "Options": {"A": "The parallel trends assumption is violated; the DiD estimate is -3.64.", "B": "The parallel trends assumption is violated; the DiD estimate is 0.32.", "C": "The parallel trends assumption holds; the DiD estimate is -3.64.", "D": "The parallel trends assumption holds; the DiD estimate is 3.96."}, "Answer": "A", "pi_justification": "This item assesses the ability to perform the core calculations of a Difference-in-Differences analysis and to test its key identifying assumption. It uses a Computational Judgment strategy. The correct answer requires both a correct judgment on the parallel trends assumption and a correct calculation of the DiD estimate. Distractor B is a step-omission error (raw change for treated group). Distractor C makes a conceptual error about the assumption. Distractor D combines a conceptual error with a different step-omission error (raw change for control group).", "qid": "420", "question": "### Background\n\n**Research Question.** This problem examines the causal impact of public exposure for low performance on an individual's subsequent intrinsic motivation and effort.\n\n**Setting and Sample.** The study uses a multi-part laboratory experiment where subjects perform a real-effort task. In Part 3, the lowest-ranked performer (Rank 3) is publicly exposed. This exposure event is the 'treatment'. Part 4 measures performance after the treatment. The analysis focuses on the subsample of individuals who were the least performers in Part 3, comparing those who were actually exposed to those whose exposure was waived by their peers.\n\n### Data / Model Specification\n\nTable 1 presents the mean performance for the subsample of least performers, broken down by whether they were exposed in Part 3.\n\n**Table 1: Mean Performance of Least Performers in Part 3**\n\n| Group       | Part 1 | Part 2 | Part 3 | Part 4 | N  |\n|-------------|--------|--------|--------|--------|----|\n| Not exposed | 116.22 | 123.80 | 126.58 | 130.54 | 81 |\n| Exposed     | 121.25 | 123.47 | 127.39 | 127.71 | 51 |\n\n### Question\n\nA key assumption for a Difference-in-Differences (DiD) analysis is that the treatment and control groups had parallel trends before the treatment. Based on the data in Table 1, first evaluate this assumption by comparing the performance change from Part 1 to Part 2 for the 'Exposed' versus 'Not exposed' groups. Then, calculate the DiD estimate of the effect of exposure on performance, using Part 3 as the pre-treatment period and Part 4 as the post-treatment period. Which of the following statements is correct?", "model_evaluation_result": {"model_raw_response": "I need to evaluate the parallel trends assumption and calculate the DiD estimate.\n\n**Step 1: Check Parallel Trends Assumption**\nThe parallel trends assumption requires that treatment and control groups have similar trends before treatment.\n\nFrom Part 1 to Part 2 (pre-treatment period):\n- Not exposed: 123.80 - 116.22 = 7.58\n- Exposed: 123.47 - 121.25 = 2.22\n\nThe change in performance from Part 1 to Part 2 differs between groups (7.58 vs 2.22), indicating the parallel trends assumption is violated.\n\n**Step 2: Calculate DiD Estimate**\nUsing Part 3 as pre-treatment and Part 4 as post-treatment:\n\nChanges from Part 3 to Part 4:\n- Not exposed: 130.54 - 126.58 = 3.96\n- Exposed: 127.71 - 127.39 = 0.32\n\nDiD estimate = (Change for exposed) - (Change for not exposed)\nDiD estimate = 0.32 - 3.96 = -3.64\n\nThe DiD estimate is -3.64, indicating that exposure reduced performance by 3.64 points.\n\nSince the parallel trends assumption is violated and the DiD estimate is -3.64, the correct answer is A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 470, "Question": "### Background\n\nFor a subsequent theoretical model (Model 2) to be analytically tractable, the paper requires that the technology be homogenous. This implies a capital cost function of the form `C(ỹ) = b_1*ỹ^γ` and an efficiency function `a = b_2*ỹ^(1-γ)`, where `ỹ` is plant capacity. This specification requires that the elasticity of capital cost with respect to capacity (`γ`) and the elasticity of efficiency with respect to capacity (`1-γ`) sum to one.\n\n### Data / Model Specification\n\nThe paper's unconstrained empirical estimates found the elasticity of price with respect to capacity to be approximately 0.92. However, to satisfy the theoretical model's requirements, the author imposed a constrained elasticity of `γ = 0.9` for the capital cost function.\n\n### Question\n\nThe parameter `γ` in the capital cost function `C(ỹ) = b_1*ỹ^γ` governs economies of scale, with a smaller `γ` implying stronger economies of scale (i.e., cost increases less than proportionally with capacity). Given this, what is the most likely consequence of the author imposing a cost elasticity of `γ = 0.9` when the unconstrained data suggested `γ ≈ 0.92`?", "Options": {"A": "The model will under-predict the optimal plant size chosen by firms.", "B": "The model will over-predict the optimal plant size chosen by firms.", "C": "The model will predict no economies of scale in construction.", "D": "The model's predictions will be unaffected, as the difference is small."}, "Answer": "B", "pi_justification": "This item assesses a deep understanding of how parameter choices in a structural model affect its behavioral predictions. It is a 'Scenario Application' item. The correct answer requires a three-step reasoning process connecting the parameter change to economies of scale, then to marginal costs, and finally to the optimal choice. Distractor A is the direct conceptual opposite, testing for confusion on the direction of the effect. Distractor C tests a fundamental misunderstanding of the parameter's meaning. Distractor D offers a plausible but incorrect dismissal of the problem.", "qid": "470", "question": "### Background\n\nFor a subsequent theoretical model (Model 2) to be analytically tractable, the paper requires that the technology be homogenous. This implies a capital cost function of the form `C(ỹ) = b_1*ỹ^γ` and an efficiency function `a = b_2*ỹ^(1-γ)`, where `ỹ` is plant capacity. This specification requires that the elasticity of capital cost with respect to capacity (`γ`) and the elasticity of efficiency with respect to capacity (`1-γ`) sum to one.\n\n### Data / Model Specification\n\nThe paper's unconstrained empirical estimates found the elasticity of price with respect to capacity to be approximately 0.92. However, to satisfy the theoretical model's requirements, the author imposed a constrained elasticity of `γ = 0.9` for the capital cost function.\n\n### Question\n\nThe parameter `γ` in the capital cost function `C(ỹ) = b_1*ỹ^γ` governs economies of scale, with a smaller `γ` implying stronger economies of scale (i.e., cost increases less than proportionally with capacity). Given this, what is the most likely consequence of the author imposing a cost elasticity of `γ = 0.9` when the unconstrained data suggested `γ ≈ 0.92`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand how the cost elasticity parameter γ affects optimal plant size decisions.\n\nKey relationships:\n- Cost function: C(ỹ) = b₁ỹᵞ\n- Smaller γ means stronger economies of scale (costs increase less than proportionally with capacity)\n- Data suggests γ ≈ 0.92, but author imposes γ = 0.9\n\nThe imposed γ = 0.9 is smaller than the data-suggested γ ≈ 0.92.\n\nThis means the model assumes stronger economies of scale than what the data actually shows.\n\nWhen a model assumes stronger economies of scale than reality:\n- It makes larger plants appear more cost-effective than they actually are\n- Firms in the model will find it optimal to build larger plants to capture these (overstated) scale economies\n- The model will predict larger optimal plant sizes than firms would actually choose in reality\n\nSince the model overstates the benefits of scale, it will over-predict the optimal plant size that firms choose.\n\nB", "model_answer": "C", "error": null}}
{"ID": 312, "Question": "### Background\n\n**Research Question:** How can the topological structure of the Nash equilibrium correspondence be exploited to create a computationally feasible algorithm for finding equilibria in finite games?\n\n**Setting:** The core challenge in computing equilibria is the dimensional mismatch between the vast space of games (`Γ`) and the smaller space of mixed strategies (`Σ`). The Kohlberg-Mertens structure theorem provides a homeomorphism `H` that connects the equilibrium graph `E` to the game space `Γ`, enabling a more efficient computational approach by working in a space of dimension `m`, the total number of pure strategies.\n\n**Variables and Parameters.**\n- `(G̃, g)`: A parameterized decomposition of a game `G`.\n- `σ`: A mixed-strategy profile.\n- `v_s(σ)`: The expected payoff for pure strategy `s` given profile `σ`.\n- `z`: An `m`-dimensional vector in the image space of the homeomorphism `H`.\n- `H`: The homeomorphism from the equilibrium graph `E` to the space of games `Γ`.\n- `r(z)`: A retraction map from `ℝ^m` to the strategy space `Σ`.\n- `h(z)`: A map from `ℝ^m` to the `g`-parameter space.\n\n### Data / Model Specification\n\nThe analysis relies on the following key relationships:\n\n1.  A game `G` is decomposed into `(G̃, g)`. The expected payoff for player `n`'s pure strategy `s ∈ S_n` is:\n    ```latex\n    v_s(\\sigma) = g_s + \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n    Let `G̃(σ)_s` denote the summation term.\n\n2.  The Kohlberg-Mertens homeomorphism `H` maps a point on the equilibrium graph `(G̃, g, σ)` to the space of games. For a fixed `G̃`, it is defined as `H(G̃, g, σ) = (G̃, z)`, where:\n    ```latex\n    z_s = \\sigma_s + v_s(\\sigma) \n    ```\n\n3.  The retraction map `r: ℝ^m → Σ` maps a vector `z` to a mixed-strategy profile `σ = r(z)`. The components `σ_s` are defined by `σ_s = (z_s - v_n(z))^+`, where `v_n(z)` is the unique real number for player `n` that solves:\n    ```latex\n    \\sum_{s \\in S_n} (z_s - v_n(z))^+ = 1\n    ```\n    where `(x)⁺ ≡ max{0, x}`.\n\n4.  The inverse map `H⁻¹(G̃, z)` yields the point `(G̃, g, σ)` on the equilibrium graph where `σ = r(z)` and `g_s` is given by:\n    ```latex\n    g_s = z_s - \\sigma_s - \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n\n### Question\n\nGiven the definitions in the model specification, the map `h(z)` finds the game parameter `g` for which `r(z)` is an equilibrium. To find an equilibrium for a target game `g*`, the problem is reformulated as finding a zero of `ψ(z)`. Which of the following correctly defines both `h(z)` and `ψ(z)`?", "Options": {"A": "`h(z) = z + r(z) - G̃(r(z))` and `ψ(z) = h(z) - g*`", "B": "`h(z) = z - r(z) - G̃(r(z))` and `ψ(z) = g* - h(z)`", "C": "`h(z) = z - r(z)` and `ψ(z) = h(z) - g*`", "D": "`h(z) = z - r(z) - G̃(r(z))` and `ψ(z) = h(z) - g*`"}, "Answer": "D", "pi_justification": "This item assesses the ability to perform the core derivation of the `h(z)` map and correctly formulate the root-finding problem for `ψ(z)`. It uses a Computational Judgment strategy. The distractors are designed based on plausible algebraic errors: a sign error on `r(z)` (A), a sign error in the definition of `ψ(z)` (B), and omitting the interaction term `G̃(r(z))` (C).", "qid": "312", "question": "### Background\n\n**Research Question:** How can the topological structure of the Nash equilibrium correspondence be exploited to create a computationally feasible algorithm for finding equilibria in finite games?\n\n**Setting:** The core challenge in computing equilibria is the dimensional mismatch between the vast space of games (`Γ`) and the smaller space of mixed strategies (`Σ`). The Kohlberg-Mertens structure theorem provides a homeomorphism `H` that connects the equilibrium graph `E` to the game space `Γ`, enabling a more efficient computational approach by working in a space of dimension `m`, the total number of pure strategies.\n\n**Variables and Parameters.**\n- `(G̃, g)`: A parameterized decomposition of a game `G`.\n- `σ`: A mixed-strategy profile.\n- `v_s(σ)`: The expected payoff for pure strategy `s` given profile `σ`.\n- `z`: An `m`-dimensional vector in the image space of the homeomorphism `H`.\n- `H`: The homeomorphism from the equilibrium graph `E` to the space of games `Γ`.\n- `r(z)`: A retraction map from `ℝ^m` to the strategy space `Σ`.\n- `h(z)`: A map from `ℝ^m` to the `g`-parameter space.\n\n### Data / Model Specification\n\nThe analysis relies on the following key relationships:\n\n1.  A game `G` is decomposed into `(G̃, g)`. The expected payoff for player `n`'s pure strategy `s ∈ S_n` is:\n    ```latex\n    v_s(\\sigma) = g_s + \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n    Let `G̃(σ)_s` denote the summation term.\n\n2.  The Kohlberg-Mertens homeomorphism `H` maps a point on the equilibrium graph `(G̃, g, σ)` to the space of games. For a fixed `G̃`, it is defined as `H(G̃, g, σ) = (G̃, z)`, where:\n    ```latex\n    z_s = \\sigma_s + v_s(\\sigma) \n    ```\n\n3.  The retraction map `r: ℝ^m → Σ` maps a vector `z` to a mixed-strategy profile `σ = r(z)`. The components `σ_s` are defined by `σ_s = (z_s - v_n(z))^+`, where `v_n(z)` is the unique real number for player `n` that solves:\n    ```latex\n    \\sum_{s \\in S_n} (z_s - v_n(z))^+ = 1\n    ```\n    where `(x)⁺ ≡ max{0, x}`.\n\n4.  The inverse map `H⁻¹(G̃, z)` yields the point `(G̃, g, σ)` on the equilibrium graph where `σ = r(z)` and `g_s` is given by:\n    ```latex\n    g_s = z_s - \\sigma_s - \\sum_{t \\in S_{-n}} \\tilde{G}_{s,t} \\prod_{i \\neq n} \\sigma_{t_i}\n    ```\n\n### Question\n\nGiven the definitions in the model specification, the map `h(z)` finds the game parameter `g` for which `r(z)` is an equilibrium. To find an equilibrium for a target game `g*`, the problem is reformulated as finding a zero of `ψ(z)`. Which of the following correctly defines both `h(z)` and `ψ(z)`?", "model_evaluation_result": {"model_raw_response": "Looking at the model specification, I need to determine the correct definitions for `h(z)` and `ψ(z)`.\n\nFrom the inverse map `H⁻¹(G̃, z)`, I can see that:\n```\ng_s = z_s - σ_s - ∑_{t ∈ S_{-n}} G̃_{s,t} ∏_{i ≠ n} σ_{t_i}\n```\n\nWhere `σ = r(z)` and the summation term is `G̃(r(z))_s`.\n\nTherefore, in vector form:\n```\ng = z - r(z) - G̃(r(z))\n```\n\nSince `h(z)` finds the game parameter `g` for which `r(z)` is an equilibrium, we have:\n```\nh(z) = z - r(z) - G̃(r(z))\n```\n\nFor finding an equilibrium of a target game `g*`, we need to solve for `z` such that `h(z) = g*`. This means we need to find the zero of:\n```\nψ(z) = h(z) - g*\n```\n\nWhen `ψ(z) = 0`, we have `h(z) = g*`, which means `r(z)` is an equilibrium of the target game `g*`.\n\nD", "model_answer": "D", "error": null}}
{"ID": 132, "Question": "### Background\n\n**Research Question.** This problem examines the paper's core empirical findings regarding strategic capacity choice.\n\n**Setting / Institutional Environment.** The study estimates a structural model of a Bayesian game where providers simultaneously choose capacity. A key challenge is the endogeneity of competitors' actions. The model addresses this by using competitors' *expected* capacity, conditional on exogenous market characteristics, as the relevant strategic variable in a provider's decision-making process.\n\n### Data / Model Specification\n\nThe simplified best-response function for provider *i* is:\n```latex\nK_i^* = \\max\\{0, X\\beta_i + \\sum_{j \\neq i} \\gamma_{ij} \\varphi_j(X) - b_i - \\varepsilon_i\\}\n```\nwhere $\\varphi_j(X) = \\mathbb{E}[K_j^*|X]$ is the expected capacity of competitor *j* and $\\varepsilon_i \\sim N(0, \\sigma_i^2)$. The parameter $\\gamma_{ij}$ captures the strategic interaction. The marginal effect of a 1-unit increase in a competitor's expected capacity on provider *i*'s own capacity is calculated as $\\gamma_{ij} \\times \\text{Pr}(K_i^* > 0 | X)$.\n\n**Table 1: GMM Estimates of Strategic Interaction Parameters ($\\{\\gamma_{i,j}\\}$)**\n*The table shows the estimated effect of the column player's expected capacity on the row player's latent profitability variable.*\n\n| Dependent Var (Row Player) | Effect of E(Capacity) of FMC | Effect of E(Capacity) of DaVita | Effect of E(Capacity) of Nonchain |\n| :--- | :--- | :--- | :--- |\n| **FMC** | --- | -2.32*** | -1.44*** |\n| **DaVita** | -2.17*** | --- | -1.45*** |\n| **Nonchain** | -0.82*** | -0.61*** | --- |\n\n*Note: *** denotes p < 0.01.*\n\n**Table 2: Model Fit Comparison**\n\n| Statistic | Provider | Observed | GMM Prediction |\n| :--- | :--- | :--- | :--- |\n| Mean Capacity | DaVita | 4.40 | 4.23 |\n| SD Capacity | DaVita | 9.23 | 4.46 |\n| Mean Entry Prob. | DaVita | 0.23 | 0.24 |\n\n### The Question\n\nBased on the GMM estimates in Table 1 and the model fit in Table 2, what is the nature of the strategic relationship between FMC and DaVita, and what is the marginal effect of a 1-unit increase in FMC's expected capacity on DaVita's chosen capacity in an average market?", "Options": {"A": "Strategic substitutes; a 1-unit increase in FMC's expected capacity decreases DaVita's capacity by approximately 0.52 units.", "B": "Strategic substitutes; a 1-unit increase in FMC's expected capacity decreases DaVita's capacity by approximately 2.17 units.", "C": "Strategic complements; a 1-unit increase in FMC's expected capacity increases DaVita's capacity by approximately 0.52 units.", "D": "Strategic complements; a 1-unit increase in FMC's expected capacity increases DaVita's capacity by approximately 2.17 units."}, "Answer": "A", "pi_justification": "This item uses the Computational Judgment strategy to assess two core skills from the original QA: interpreting the sign of a strategic interaction parameter and performing a multi-step calculation for a marginal effect. The distractors are designed based on a matrix of common errors: Conceptual Opposite (substitutes vs. complements) and Step-Omission Error (failing to multiply the coefficient by the entry probability).", "qid": "132", "question": "### Background\n\n**Research Question.** This problem examines the paper's core empirical findings regarding strategic capacity choice.\n\n**Setting / Institutional Environment.** The study estimates a structural model of a Bayesian game where providers simultaneously choose capacity. A key challenge is the endogeneity of competitors' actions. The model addresses this by using competitors' *expected* capacity, conditional on exogenous market characteristics, as the relevant strategic variable in a provider's decision-making process.\n\n### Data / Model Specification\n\nThe simplified best-response function for provider *i* is:\n```latex\nK_i^* = \\max\\{0, X\\beta_i + \\sum_{j \\neq i} \\gamma_{ij} \\varphi_j(X) - b_i - \\varepsilon_i\\}\n```\nwhere $\\varphi_j(X) = \\mathbb{E}[K_j^*|X]$ is the expected capacity of competitor *j* and $\\varepsilon_i \\sim N(0, \\sigma_i^2)$. The parameter $\\gamma_{ij}$ captures the strategic interaction. The marginal effect of a 1-unit increase in a competitor's expected capacity on provider *i*'s own capacity is calculated as $\\gamma_{ij} \\times \\text{Pr}(K_i^* > 0 | X)$.\n\n**Table 1: GMM Estimates of Strategic Interaction Parameters ($\\{\\gamma_{i,j}\\}$)**\n*The table shows the estimated effect of the column player's expected capacity on the row player's latent profitability variable.*\n\n| Dependent Var (Row Player) | Effect of E(Capacity) of FMC | Effect of E(Capacity) of DaVita | Effect of E(Capacity) of Nonchain |\n| :--- | :--- | :--- | :--- |\n| **FMC** | --- | -2.32*** | -1.44*** |\n| **DaVita** | -2.17*** | --- | -1.45*** |\n| **Nonchain** | -0.82*** | -0.61*** | --- |\n\n*Note: *** denotes p < 0.01.*\n\n**Table 2: Model Fit Comparison**\n\n| Statistic | Provider | Observed | GMM Prediction |\n| :--- | :--- | :--- | :--- |\n| Mean Capacity | DaVita | 4.40 | 4.23 |\n| SD Capacity | DaVita | 9.23 | 4.46 |\n| Mean Entry Prob. | DaVita | 0.23 | 0.24 |\n\n### The Question\n\nBased on the GMM estimates in Table 1 and the model fit in Table 2, what is the nature of the strategic relationship between FMC and DaVita, and what is the marginal effect of a 1-unit increase in FMC's expected capacity on DaVita's chosen capacity in an average market?", "model_evaluation_result": {"model_raw_response": "Looking at Table 1, I need to find the effect of FMC's expected capacity on DaVita's capacity.\n\nFrom the table, the effect of E(Capacity) of FMC on DaVita is -2.17***.\n\nSince this coefficient is negative (-2.17), this indicates **strategic substitutes** - when FMC increases its expected capacity, DaVita reduces its capacity.\n\nFor the marginal effect calculation, the formula given is:\nMarginal effect = γ_{ij} × Pr(K_i* > 0 | X)\n\nWhere:\n- γ_{ij} = -2.17 (the strategic interaction parameter)\n- Pr(K_i* > 0 | X) is the probability that DaVita enters the market\n\nFrom Table 2, DaVita's mean entry probability is 0.24.\n\nTherefore:\nMarginal effect = -2.17 × 0.24 ≈ -0.52\n\nThis means a 1-unit increase in FMC's expected capacity decreases DaVita's capacity by approximately 0.52 units.\n\nA", "model_answer": "A", "error": null}}
{"ID": 271, "Question": "### Background\n\n**Research Question.** This problem explores the theoretical relationship between structural dynamic models and their reduced-form time-series representations, specifically how the structure of the underlying economic model imposes testable restrictions on the parameters of the final ARMA form.\n\n**Setting.** We consider a system of two equations: a structural model linking an endogenous variable `y_t` to an exogenous variable `x_t` and a structural error, and a time-series model describing the data generating process for `x_t`.\n\n**Variables & Parameters.**\n- `y_t`: An endogenous variable at time `t`.\n- `x_t`: An exogenous explanatory variable at time `t`.\n- `η_t`, `ε_t`: White-noise stochastic disturbances at time `t`.\n- `B`: The backshift (lag) operator, where `B^k z_t = z_{t-k}`.\n- `α(B)`, `β(B)`, `θ(B)`, `ω(B)`, `μ(B)`: Finite-order polynomials in the lag operator `B`.\n\n---\n\n### Data / Model Specification\n\nConsider a specific case of the general model where the structural equation is `y_t = β_0 x_t + η_t` and the exogenous variable follows an AR(1) process, `(1 - ω_1 B)x_t = ε_t`. The resulting final form for `y_t` is an ARMA(1,1) process:\n\n```latex\ny_t - ω_1 y_{t-1} = β_0 ε_t + η_t - ω_1 η_{t-1}\n```\n\nThis structure imposes a testable cross-equation restriction that is ignored when estimating an unrestricted ARMA model.\n\n---\n\n### The Question\n\nWhat is this cross-equation restriction?", "Options": {"A": "The autoregressive parameter (`ω_1`) is identical to the moving-average parameter of the final form.", "B": "The parameter `ω_1` from the AR part of the model also determines the structure of the MA part.", "C": "The model is unidentified because `ω_1` appears in both the AR and MA parts.", "D": "The parameters of the final form are unrelated, which is the definition of a structural model."}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 10.0). This item assesses the understanding of cross-equation restrictions, the key takeaway from Q2 and Q3 of the source QA. It uses a Scenario Application strategy. The distractors are designed to probe for common misunderstandings: Distractor A ('Almost Right') confuses the parameter's appearance with equality to the final MA coefficient; Distractor C ('Conceptual Error') mistakes a restriction for an identification problem; Distractor D ('Conceptual Opposite') incorrectly states the implication of the finding.", "qid": "271", "question": "### Background\n\n**Research Question.** This problem explores the theoretical relationship between structural dynamic models and their reduced-form time-series representations, specifically how the structure of the underlying economic model imposes testable restrictions on the parameters of the final ARMA form.\n\n**Setting.** We consider a system of two equations: a structural model linking an endogenous variable `y_t` to an exogenous variable `x_t` and a structural error, and a time-series model describing the data generating process for `x_t`.\n\n**Variables & Parameters.**\n- `y_t`: An endogenous variable at time `t`.\n- `x_t`: An exogenous explanatory variable at time `t`.\n- `η_t`, `ε_t`: White-noise stochastic disturbances at time `t`.\n- `B`: The backshift (lag) operator, where `B^k z_t = z_{t-k}`.\n- `α(B)`, `β(B)`, `θ(B)`, `ω(B)`, `μ(B)`: Finite-order polynomials in the lag operator `B`.\n\n---\n\n### Data / Model Specification\n\nConsider a specific case of the general model where the structural equation is `y_t = β_0 x_t + η_t` and the exogenous variable follows an AR(1) process, `(1 - ω_1 B)x_t = ε_t`. The resulting final form for `y_t` is an ARMA(1,1) process:\n\n```latex\ny_t - ω_1 y_{t-1} = β_0 ε_t + η_t - ω_1 η_{t-1}\n```\n\nThis structure imposes a testable cross-equation restriction that is ignored when estimating an unrestricted ARMA model.\n\n---\n\n### The Question\n\nWhat is this cross-equation restriction?", "model_evaluation_result": {"model_raw_response": "Looking at the given ARMA(1,1) final form:\n\n```\ny_t - ω_1 y_{t-1} = β_0 ε_t + η_t - ω_1 η_{t-1}\n```\n\nThis can be rewritten in standard ARMA notation as:\n```\n(1 - ω_1 B)y_t = β_0 ε_t + (1 - ω_1 B)η_t\n```\n\nThe key insight is that the same parameter `ω_1` appears in both:\n1. The autoregressive part: coefficient of `y_{t-1}` is `ω_1`\n2. The moving average part: coefficient of `η_{t-1}` is `-ω_1`\n\nThis occurs because `ω_1` originates from the AR(1) process for the exogenous variable `x_t`. When we derive the final form for `y_t`, this single structural parameter `ω_1` constrains both the AR and MA components of the resulting ARMA(1,1) process.\n\nIn an unrestricted ARMA(1,1) estimation, these would be treated as independent parameters, but the structural model imposes the restriction that they must be related through the single parameter `ω_1`.\n\nB", "model_answer": "B", "error": null}}
{"ID": 234, "Question": "### Background\n\n**Research Question.** This problem requires the derivation of the log-likelihood function for a restricted system of equations (Model 4), a crucial step for maximum likelihood estimation and hypothesis testing.\n\n**Setting / Institutional Environment.** The setting is a two-equation system where parameters are restricted based on economic theory. Model 4 imposes a specific constraint on the structure of the error covariance matrix, `Σ`.\n\n### Data / Model Specification\n\nModel 4 imposes the following restriction on the error covariance structure:\n\n```latex\n\\Sigma = \n\\begin{pmatrix}\n\\sigma^2 & \\gamma\\sigma^2 \\\\\n\\gamma\\sigma^2 & \\sigma^2\n\\end{pmatrix}\n\\quad \\text{(Eq. (1))}\n```\n\nGiven this restricted covariance matrix `Σ`, what is its inverse, `Σ⁻¹`?", "Options": {"A": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} \n\\begin{pmatrix}\n1 & -\\gamma \\\\\n-\\gamma & 1\n\\end{pmatrix}\n```", "B": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma^2)} \n\\begin{pmatrix}\n1 & \\gamma \\\\\n\\gamma & 1\n\\end{pmatrix}\n```", "C": "```latex\n\\frac{1}{\\sigma^2(1-\\gamma)} \n\\begin{pmatrix}\n1 & -\\gamma \\\\\n-\\gamma & 1\n\\end{pmatrix}\n```", "D": "```latex\n\\frac{1}{\\sigma^2} \n\\begin{pmatrix}\n1 & -\\gamma \\\\\n-\\gamma & 1\n\\end{pmatrix}\n```"}, "Answer": "A", "pi_justification": "This item assesses the foundational matrix algebra required for the paper's likelihood derivation. It uses a Computational Judgment strategy. Distractors are designed based on common algebraic errors: B (Sign Error in off-diagonal elements), C (Formula Misuse in calculating the determinant), and D (Step-Omission Error, forgetting the determinant term).", "qid": "234", "question": "### Background\n\n**Research Question.** This problem requires the derivation of the log-likelihood function for a restricted system of equations (Model 4), a crucial step for maximum likelihood estimation and hypothesis testing.\n\n**Setting / Institutional Environment.** The setting is a two-equation system where parameters are restricted based on economic theory. Model 4 imposes a specific constraint on the structure of the error covariance matrix, `Σ`.\n\n### Data / Model Specification\n\nModel 4 imposes the following restriction on the error covariance structure:\n\n```latex\n\\Sigma = \n\\begin{pmatrix}\n\\sigma^2 & \\gamma\\sigma^2 \\\\\n\\gamma\\sigma^2 & \\sigma^2\n\\end{pmatrix}\n\\quad \\text{(Eq. (1))}\n```\n\nGiven this restricted covariance matrix `Σ`, what is its inverse, `Σ⁻¹`?", "model_evaluation_result": {"model_raw_response": "I need to find the inverse of the 2×2 covariance matrix Σ.\n\nGiven:\n```\nΣ = [σ²     γσ²  ]\n    [γσ²    σ²   ]\n```\n\nFor a 2×2 matrix A = [a b; c d], the inverse is:\nA⁻¹ = (1/det(A)) × [d -b; -c a]\n\nwhere det(A) = ad - bc\n\nLet me calculate the determinant of Σ:\ndet(Σ) = σ² × σ² - γσ² × γσ²\n       = σ⁴ - γ²σ⁴\n       = σ⁴(1 - γ²)\n       = σ²(σ²(1 - γ²))\n\nNow I can find the inverse:\nΣ⁻¹ = (1/det(Σ)) × [σ²    -γσ²]\n                    [-γσ²   σ² ]\n\nΣ⁻¹ = (1/(σ⁴(1-γ²))) × [σ²    -γσ²]\n                         [-γσ²   σ² ]\n\nΣ⁻¹ = (1/(σ²(1-γ²))) × [1   -γ]\n                        [-γ   1]\n\nTherefore:\nΣ⁻¹ = (1/(σ²(1-γ²))) × [1   -γ]\n                        [-γ   1]\n\nThis matches option A.\n\nA", "model_answer": "A", "error": null}}
{"ID": 102, "Question": "### Background\n\nThe paper modifies the standard Keynesian liquidity preference function. In the standard model, speculative money balances `M_s` depend only on the interest rate `r`. The author proposes a more general function where `M_s` also depends on an equity price index `z` and the general price level `P`.\n\n### Data / Model Specification\n\nThe general form for speculative balances is given by:\n```latex\nM_s = K(r, z, P)\n```\nBased on portfolio theory, the demand for speculative cash balances is assumed to decrease when the return on alternative assets increases. Therefore, the partial derivatives are:\n*   `∂K/∂r < 0`: Higher interest rates on bonds reduce demand for cash.\n*   `∂K/∂z < 0`: Higher equity prices (implying high returns) reduce demand for cash.\n*   `∂K/∂P < 0`: A higher general price level (inflation) erodes the value of cash, reducing demand for it.\n\n### Question\n\nConsider a stagflationary shock where the general price level `P` rises, but corporate profits fall, causing the equity price index `z` to fall. Based on the model `M_s = K(r, z, P)`, what is the net effect of these two changes on the demand for speculative money balances `M_s`?\n", "Options": {"A": "`M_s` will unambiguously increase.", "B": "`M_s` will unambiguously decrease.", "C": "The net effect on `M_s` is ambiguous.", "D": "`M_s` will remain unchanged as the two effects perfectly cancel."}, "Answer": "C", "pi_justification": "This item assesses the ability to apply the model's core monetary innovation to a novel scenario. It uses the 'Scenario Application' strategy. The correct answer requires synthesizing two opposing effects. Distractor A represents a 'Step-Omission Error' by only considering the falling equity prices (a flight to safety). Distractor B represents the same error by only considering rising general prices (the inflation hedge motive). Distractor D is implausible and tests for over-simplification.", "qid": "102", "question": "### Background\n\nThe paper modifies the standard Keynesian liquidity preference function. In the standard model, speculative money balances `M_s` depend only on the interest rate `r`. The author proposes a more general function where `M_s` also depends on an equity price index `z` and the general price level `P`.\n\n### Data / Model Specification\n\nThe general form for speculative balances is given by:\n```latex\nM_s = K(r, z, P)\n```\nBased on portfolio theory, the demand for speculative cash balances is assumed to decrease when the return on alternative assets increases. Therefore, the partial derivatives are:\n*   `∂K/∂r < 0`: Higher interest rates on bonds reduce demand for cash.\n*   `∂K/∂z < 0`: Higher equity prices (implying high returns) reduce demand for cash.\n*   `∂K/∂P < 0`: A higher general price level (inflation) erodes the value of cash, reducing demand for it.\n\n### Question\n\nConsider a stagflationary shock where the general price level `P` rises, but corporate profits fall, causing the equity price index `z` to fall. Based on the model `M_s = K(r, z, P)`, what is the net effect of these two changes on the demand for speculative money balances `M_s`?\n", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze how the two simultaneous changes affect the demand for speculative money balances.\n\nGiven information:\n- The model: M_s = K(r, z, P)\n- Partial derivatives: ∂K/∂r < 0, ∂K/∂z < 0, ∂K/∂P < 0\n\nThe stagflationary shock involves:\n1. General price level P rises (increases)\n2. Equity price index z falls (decreases)\n\nLet me analyze each effect:\n\nEffect of rising P:\n- Since ∂K/∂P < 0, when P increases, M_s decreases\n- Higher inflation erodes cash value, reducing demand for cash balances\n\nEffect of falling z:\n- Since ∂K/∂z < 0, when z decreases, M_s increases\n- Lower equity prices make equities less attractive, increasing demand for cash balances\n\nSo I have two opposing effects:\n- Rising P → decreases M_s\n- Falling z → increases M_s\n\nWithout knowing the relative magnitudes of these partial derivatives or the specific functional form K(r, z, P), I cannot determine which effect dominates. The net change in M_s depends on the relative strength of these two opposing forces.\n\nThe answer is C.\n\nC", "model_answer": "C", "error": null}}
{"ID": 366, "Question": "### Background\n\n**Research Question.** This problem demonstrates the theoretical equivalence of the Learning-to-Forecast (LtF) and Learning-to-Optimise (LtO) experimental treatments under the benchmark assumption of perfect agent rationality.\n\n**Setting / Institutional Environment.** We compare two market structures. In the LtO treatment, $I=6$ agents choose their asset quantities $z_{i,t}$ directly. In the LtF treatment, agents submit price forecasts $p_{i,t+1}^e$, which are then converted into asset demands by a computer. The underlying microeconomic model of agent preferences and market clearing is identical in both settings.\n\n**Variables & Parameters.**\n- `$p_t$`: Price of the asset in period $t$.\n- `$p_{i,t+1}^e$`: Agent $i$'s price forecast for period $t+1$.\n- `$\\bar{p}_{t+1}^e$`: Average price forecast across all $I$ agents.\n- `$z_{i,t}$`: Quantity of the asset demanded by agent $i$ in period $t$.\n- `$y$`: Constant dividend; $y=3.3$.\n- `$R$`: Gross risk-free rate; $R=1.05$.\n- `$a\\sigma^2$`: Composite parameter for risk aversion and perceived variance; $a\\sigma^2=6$.\n- `$\\lambda$`: Price adjustment parameter; $\\lambda=20/21$.\n- `$I$`: Number of agents; $I=6$.\n\n---\n\n### Data / Model Specification\n\nThe price adjustment rule in the LtO market, where agents submit quantities, is:\n```latex\np_{t+1} = p_t + \\lambda \\sum_{i=1}^I z_{i,t} + \\varepsilon_t \\quad \\text{(Eq. 1)}\n```\nThe optimal demand for a rational, price-taking agent with mean-variance preferences is:\n```latex\nz_{i,t}^* = \\frac{p_{i,t+1}^e + y - R p_t}{a\\sigma^2} \\quad \\text{(Eq. 2)}\n```\n\n---\n\n### The Question\n\nAssume all agents in the LtO market are rational optimizers. By substituting the optimal demand function (Eq. 2) into the price adjustment rule (Eq. 1), aggregating across all $I=6$ agents, and applying the given parameter values, what is the correct resulting law of motion for the price $p_{t+1}$?", "Options": {"A": "```latex\np_{t+1} = \\frac{20}{21} \\bar{p}_{t+1}^e + \\frac{66}{21} + \\varepsilon_t\n```", "B": "```latex\np_{t+1} = \\frac{1}{21}p_t + \\frac{20}{21} \\bar{p}_{t+1}^e + \\frac{66}{21} + \\varepsilon_t\n```", "C": "```latex\np_{t+1} = \\frac{5}{6}p_t + \\frac{10}{63} \\bar{p}_{t+1}^e + \\frac{33}{63} + \\varepsilon_t\n```", "D": "```latex\np_{t+1} = -5p_t + \\frac{120}{21} \\bar{p}_{t+1}^e + 19.8 + \\varepsilon_t\n```"}, "Answer": "A", "pi_justification": "This item assesses the ability to perform a multi-step algebraic derivation that is central to the paper's theoretical setup. It uses a Computational Judgment strategy. The correct answer requires substituting one equation into another, aggregating across agents, and correctly applying parameter values, including the key insight that the $p_t$ term cancels out. The distractors are designed based on plausible error paths: (B) results from a calculation error where the $p_t$ term fails to cancel; (C) stems from an aggregation error where the number of agents `I` is ignored; (D) arises from omitting the risk-aversion parameter `a*sigma^2` during substitution.", "qid": "366", "question": "### Background\n\n**Research Question.** This problem demonstrates the theoretical equivalence of the Learning-to-Forecast (LtF) and Learning-to-Optimise (LtO) experimental treatments under the benchmark assumption of perfect agent rationality.\n\n**Setting / Institutional Environment.** We compare two market structures. In the LtO treatment, $I=6$ agents choose their asset quantities $z_{i,t}$ directly. In the LtF treatment, agents submit price forecasts $p_{i,t+1}^e$, which are then converted into asset demands by a computer. The underlying microeconomic model of agent preferences and market clearing is identical in both settings.\n\n**Variables & Parameters.**\n- `$p_t$`: Price of the asset in period $t$.\n- `$p_{i,t+1}^e$`: Agent $i$'s price forecast for period $t+1$.\n- `$\\bar{p}_{t+1}^e$`: Average price forecast across all $I$ agents.\n- `$z_{i,t}$`: Quantity of the asset demanded by agent $i$ in period $t$.\n- `$y$`: Constant dividend; $y=3.3$.\n- `$R$`: Gross risk-free rate; $R=1.05$.\n- `$a\\sigma^2$`: Composite parameter for risk aversion and perceived variance; $a\\sigma^2=6$.\n- `$\\lambda$`: Price adjustment parameter; $\\lambda=20/21$.\n- `$I$`: Number of agents; $I=6$.\n\n---\n\n### Data / Model Specification\n\nThe price adjustment rule in the LtO market, where agents submit quantities, is:\n```latex\np_{t+1} = p_t + \\lambda \\sum_{i=1}^I z_{i,t} + \\varepsilon_t \\quad \\text{(Eq. 1)}\n```\nThe optimal demand for a rational, price-taking agent with mean-variance preferences is:\n```latex\nz_{i,t}^* = \\frac{p_{i,t+1}^e + y - R p_t}{a\\sigma^2} \\quad \\text{(Eq. 2)}\n```\n\n---\n\n### The Question\n\nAssume all agents in the LtO market are rational optimizers. By substituting the optimal demand function (Eq. 2) into the price adjustment rule (Eq. 1), aggregating across all $I=6$ agents, and applying the given parameter values, what is the correct resulting law of motion for the price $p_{t+1}$?", "model_evaluation_result": {"model_raw_response": "I need to derive the law of motion for price by substituting the optimal demand function into the price adjustment rule and aggregating across all agents.\n\nGiven:\n- Price adjustment rule: $p_{t+1} = p_t + \\lambda \\sum_{i=1}^I z_{i,t} + \\varepsilon_t$\n- Optimal demand: $z_{i,t}^* = \\frac{p_{i,t+1}^e + y - R p_t}{a\\sigma^2}$\n- Parameters: $\\lambda = 20/21$, $I = 6$, $y = 3.3$, $R = 1.05$, $a\\sigma^2 = 6$\n\nStep 1: Substitute optimal demand into price adjustment rule\n$p_{t+1} = p_t + \\lambda \\sum_{i=1}^I \\frac{p_{i,t+1}^e + y - R p_t}{a\\sigma^2} + \\varepsilon_t$\n\nStep 2: Aggregate across all agents\n$p_{t+1} = p_t + \\frac{\\lambda}{a\\sigma^2} \\sum_{i=1}^I (p_{i,t+1}^e + y - R p_t) + \\varepsilon_t$\n\nStep 3: Separate the summation\n$p_{t+1} = p_t + \\frac{\\lambda}{a\\sigma^2} \\left[\\sum_{i=1}^I p_{i,t+1}^e + I \\cdot y - I \\cdot R p_t\\right] + \\varepsilon_t$\n\nStep 4: Use average forecast notation\nSince $\\bar{p}_{t+1}^e = \\frac{1}{I}\\sum_{i=1}^I p_{i,t+1}^e$, we have $\\sum_{i=1}^I p_{i,t+1}^e = I \\cdot \\bar{p}_{t+1}^e$\n\n$p_{t+1} = p_t + \\frac{\\lambda}{a\\sigma^", "model_answer": "I need to derive the law of motion for price by substituting the optimal demand function into the price adjustment rule and aggregating across all agents.\n\nGiven:\n- Price adjustment rule: $p_{t+1} = p_t + \\lambda \\sum_{i=1}^I z_{i,t} + \\varepsilon_t$\n- Optimal demand: $z_{i,t}^* = \\frac{p_{i,t+1}^e + y - R p_t}{a\\sigma^2}$\n- Parameters: $\\lambda = 20/21$, $I = 6$, $y = 3.3$, $R = 1.05$, $a\\sigma^2 = 6$\n\nStep 1: Substitute optimal demand into price adjustment rule\n$p_{t+1} = p_t + \\lambda \\sum_{i=1}^I \\frac{p_{i,t+1}^e + y - R p_t}{a\\sigma^2} + \\varepsilon_t$\n\nStep 2: Aggregate across all agents\n$p_{t+1} = p_t + \\frac{\\lambda}{a\\sigma^2} \\sum_{i=1}^I (p_{i,t+1}^e + y - R p_t) + \\varepsilon_t$\n\nStep 3: Separate the summation\n$p_{t+1} = p_t + \\frac{\\lambda}{a\\sigma^2} \\left[\\sum_{i=1}^I p_{i,t+1}^e + I \\cdot y - I \\cdot R p_t\\right] + \\varepsilon_t$\n\nStep 4: Use average forecast notation\nSince $\\bar{p}_{t+1}^e = \\frac{1}{I}\\sum_{i=1}^I p_{i,t+1}^e$, we have $\\sum_{i=1}^I p_{i,t+1}^e = I \\cdot \\bar{p}_{t+1}^e$\n\n$p_{t+1} = p_t + \\frac{\\lambda}{a\\sigma^", "error": "Warning: Could not extract single choice letter from: I need to derive the law of motion for price by su..."}}
{"ID": 105, "Question": "### Background\n\nCasey Mulligan challenged the idea that labor indivisibility is the decisive factor for generating a high aggregate labor supply elasticity. He proposed a model that retains indivisible labor but introduces a population of heterogeneous agents, each with an idiosyncratic disutility of work, `v_i`, drawn from a distribution.\n\n### Data / Model Specification\n\nIn Mulligan's framework, an agent chooses to work if the net wage, `W`, exceeds their personal disutility of work, `v_i`. The aggregate labor supply `N` is the fraction of the population for whom this condition holds: `N = F(W)`, where `F(v)` is the cumulative distribution function (CDF) of work disutility. The elasticity of aggregate supply is determined by the density of workers at the margin.\n\n### Question\n\nAccording to Mulligan's framework, which characteristic of the work disutility distribution would result in a very *low* aggregate labor supply elasticity?", "Options": {"A": "The distribution is bimodal, with very low population density in the range of prevailing market wages.", "B": "The distribution is unimodal and tightly centered around the prevailing market wage.", "C": "The average disutility of work across the entire population is very high.", "D": "All individuals in the population have the identical disutility of work."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.0). This item assesses the student's understanding of a key theoretical critique discussed in the paper. It uses a Reverse-Reasoning strategy, asking for the cause of a specific outcome (low elasticity). The distractors are designed to probe for common misconceptions: (B) describes a situation leading to a *high* elasticity (Conceptual Opposite); (C) describes a factor affecting the level of employment but not necessarily its elasticity (True but Irrelevant); (D) describes the assumption of the homogeneous agent model that Mulligan's theory critiques, which leads to a high elasticity (Reverting to the critiqued model).", "qid": "105", "question": "### Background\n\nCasey Mulligan challenged the idea that labor indivisibility is the decisive factor for generating a high aggregate labor supply elasticity. He proposed a model that retains indivisible labor but introduces a population of heterogeneous agents, each with an idiosyncratic disutility of work, `v_i`, drawn from a distribution.\n\n### Data / Model Specification\n\nIn Mulligan's framework, an agent chooses to work if the net wage, `W`, exceeds their personal disutility of work, `v_i`. The aggregate labor supply `N` is the fraction of the population for whom this condition holds: `N = F(W)`, where `F(v)` is the cumulative distribution function (CDF) of work disutility. The elasticity of aggregate supply is determined by the density of workers at the margin.\n\n### Question\n\nAccording to Mulligan's framework, which characteristic of the work disutility distribution would result in a very *low* aggregate labor supply elasticity?", "model_evaluation_result": {"model_raw_response": "Looking at Mulligan's framework, the aggregate labor supply elasticity depends on the density of workers at the margin - that is, how many people are right at the threshold of deciding whether to work or not when wages change.\n\nLet me analyze each option:\n\nA) If the distribution is bimodal with very low density around prevailing wages, there are few people at the margin. Small wage changes won't bring many new workers into the labor force, resulting in low elasticity.\n\nB) If the distribution is tightly centered around the prevailing wage, there's high density at the margin. Many people would change their work decision with small wage changes, creating high elasticity.\n\nC) High average disutility affects the level of labor supply but not necessarily the elasticity, which depends on marginal responses.\n\nD) If everyone has identical disutility, then either everyone works or no one works - there's no gradual response to wage changes, which would mean zero elasticity (extremely low).\n\nBetween A and D, both suggest low elasticity, but D represents the most extreme case where elasticity would be essentially zero since there are no marginal workers to respond to wage changes.\n\nD", "model_answer": "A", "error": null}}
{"ID": 258, "Question": "### Background\n\n**Research Question.** This problem analyzes the severe finite-sample distortions of estimators and test statistics in Markov-switching models, critiquing the reliability of inference based on standard asymptotic theory and evaluating proposed corrections.\n\n**Setting / Institutional Environment.** A Monte Carlo simulation is conducted to evaluate the properties of Maximum Likelihood Estimation (MLE) for a simple symmetric two-state switching model. One thousand samples of size `T=50` are generated from a known data generating process. The true parameters are `μ₁=2`, `μ₂=-2`, `p₁₁=p₂₂=0.8`, and `σ₁²=σ₂²=1`.\n\n**Variables & Parameters.**\n\n*   `λ`: A generic parameter of the model (e.g., `σ₁²`).\n*   `T`: Sample size, `T=50`.\n*   `m`: Number of estimated parameters in the model.\n*   `l`: Degrees of freedom of an asymptotic `χ²` distribution.\n*   `Avg. Actual S.E.`: The true sampling standard deviation of an estimator, calculated from the Monte Carlo replications.\n*   `Avg. Estimated S.E.`: The average of the standard errors computed for each sample using the outer-product-of-gradient (OPG) estimator.\n\n---\n\n### Data / Model Specification\n\nThe following table consolidates key results from the paper's Monte Carlo study for a sample size of `T=50`.\n\n**Table 1. Monte Carlo Results for MLE and Specification Tests (T=50)**\n\n| Parameter / Test | Statistic | Value |\n| :--- | :--- | :--- |\n| **Parameter Estimation** | | |\n| `σ₁²` | Avg. Actual S.E. | 0.40 |\n| `σ₁²` | Avg. Estimated S.E. | 0.26 |\n| **t-statistic Distribution** | | |\n| t-stat for `σ₁²` | 5th percentile | -2.90 |\n| t-stat for `σ₁²` | 95th percentile | 1.00 |\n| `N(0,1)` distribution | 5th percentile | -1.64 |\n| `N(0,1)` distribution | 95th percentile | 1.64 |\n| **Specification Test Size** | | |\n| NTW Test for ARCH | Actual Rejection Rate (α=5%) | 17% |\n\nThe paper proposes a small-sample correction for a Newey-Tauchen-White (NTW) test statistic, which is asymptotically `χ²(l)`. The corrected statistic is calculated as:\n\n```latex\n\\text{Corrected Statistic} = (\\text{Raw NTW Statistic}) \\times \\frac{T-m}{T \\cdot l} \\quad \\text{(Eq. 1)}\n```\n\nThis corrected statistic should be compared to an `F(l, T-m)` distribution.\n\n---\n\n### Question\n\nA researcher conducts a two-sided test on `σ₁²` at a nominal 5% significance level, using the standard normal critical values (approx. ±1.96). Based on the empirical quantiles for the t-statistic in Table 1, which statement best describes the true properties of this test?", "Options": {"A": "The true rejection rate is greater than 5% and is highly asymmetric, with rejections occurring almost exclusively when the t-statistic is large and negative.", "B": "The true rejection rate is approximately 10% (5% in each tail), as indicated by the 5th and 95th percentiles.", "C": "The true rejection rate is less than 5% because the 95th percentile (1.00) is less than the critical value of 1.96.", "D": "The true rejection rate is approximately 5%, as the severe skew in the left tail is offset by the lack of mass in the right tail."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item uses the Scenario Application strategy. It tests the ability to interpret empirical quantiles to assess the true size and symmetry of a hypothesis test. Distractor B incorrectly applies the 5%/95% labels to define the rejection region. Distractor C focuses only on one tail, ignoring the other. Distractor D presents a plausible-sounding but incorrect argument about symmetry and offsetting effects.", "qid": "258", "question": "### Background\n\n**Research Question.** This problem analyzes the severe finite-sample distortions of estimators and test statistics in Markov-switching models, critiquing the reliability of inference based on standard asymptotic theory and evaluating proposed corrections.\n\n**Setting / Institutional Environment.** A Monte Carlo simulation is conducted to evaluate the properties of Maximum Likelihood Estimation (MLE) for a simple symmetric two-state switching model. One thousand samples of size `T=50` are generated from a known data generating process. The true parameters are `μ₁=2`, `μ₂=-2`, `p₁₁=p₂₂=0.8`, and `σ₁²=σ₂²=1`.\n\n**Variables & Parameters.**\n\n*   `λ`: A generic parameter of the model (e.g., `σ₁²`).\n*   `T`: Sample size, `T=50`.\n*   `m`: Number of estimated parameters in the model.\n*   `l`: Degrees of freedom of an asymptotic `χ²` distribution.\n*   `Avg. Actual S.E.`: The true sampling standard deviation of an estimator, calculated from the Monte Carlo replications.\n*   `Avg. Estimated S.E.`: The average of the standard errors computed for each sample using the outer-product-of-gradient (OPG) estimator.\n\n---\n\n### Data / Model Specification\n\nThe following table consolidates key results from the paper's Monte Carlo study for a sample size of `T=50`.\n\n**Table 1. Monte Carlo Results for MLE and Specification Tests (T=50)**\n\n| Parameter / Test | Statistic | Value |\n| :--- | :--- | :--- |\n| **Parameter Estimation** | | |\n| `σ₁²` | Avg. Actual S.E. | 0.40 |\n| `σ₁²` | Avg. Estimated S.E. | 0.26 |\n| **t-statistic Distribution** | | |\n| t-stat for `σ₁²` | 5th percentile | -2.90 |\n| t-stat for `σ₁²` | 95th percentile | 1.00 |\n| `N(0,1)` distribution | 5th percentile | -1.64 |\n| `N(0,1)` distribution | 95th percentile | 1.64 |\n| **Specification Test Size** | | |\n| NTW Test for ARCH | Actual Rejection Rate (α=5%) | 17% |\n\nThe paper proposes a small-sample correction for a Newey-Tauchen-White (NTW) test statistic, which is asymptotically `χ²(l)`. The corrected statistic is calculated as:\n\n```latex\n\\text{Corrected Statistic} = (\\text{Raw NTW Statistic}) \\times \\frac{T-m}{T \\cdot l} \\quad \\text{(Eq. 1)}\n```\n\nThis corrected statistic should be compared to an `F(l, T-m)` distribution.\n\n---\n\n### Question\n\nA researcher conducts a two-sided test on `σ₁²` at a nominal 5% significance level, using the standard normal critical values (approx. ±1.96). Based on the empirical quantiles for the t-statistic in Table 1, which statement best describes the true properties of this test?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to analyze the finite-sample behavior of the t-statistic for σ₁² compared to standard normal critical values.\n\nFrom Table 1, the empirical quantiles for the t-statistic are:\n- 5th percentile: -2.90\n- 95th percentile: 1.00\n\nFor comparison, the standard normal N(0,1) quantiles are:\n- 5th percentile: -1.64  \n- 95th percentile: 1.64\n\nFor a two-sided test at 5% significance level using critical values ±1.96:\n\n**Left tail rejection:** The test rejects when t-stat < -1.96\nSince the 5th percentile is -2.90, which is much more extreme than -1.96, significantly more than 5% of the distribution lies below -1.96.\n\n**Right tail rejection:** The test rejects when t-stat > 1.96\nSince the 95th percentile is only 1.00, which is less than 1.96, essentially 0% of the distribution lies above 1.96.\n\nThis creates a highly asymmetric situation where:\n1. The total rejection rate exceeds 5% (due to excessive left-tail rejections)\n2. Rejections occur almost exclusively in the left tail (negative t-statistics)\n3. There are virtually no rejections in the right tail\n\nThis pattern is typical of severe finite-sample distortions in Markov-switching models, where the sampling distribution of estimators can be highly skewed.\n\nA", "model_answer": "A", "error": null}}
{"ID": 454, "Question": "### Background\n\nThe paper analyzes the dynamic properties of complex systems, including feedback loops within them. The nature of a feedback loop (positive/destabilizing or negative/stabilizing) can be determined by the signs of the causal links that form it.\n\n### Data / Model Specification\n\nThe dynamics of a lake's ecosystem are described by the following system of equations, where `N`=nitrate, `P`=phosphate, `G`=green algae, `B`=blue-green algae, and all `c_i` are positive constants, except for `c₇ < 0`, which represents inhibition.\n\n```latex\n\\frac{\\mathrm{d}N}{\\mathrm{d}t} = -c_{1}G - c_{2}N \\quad \\text{(Eq. 1)}\n```\n```latex\n\\frac{\\mathrm{d}P}{\\mathrm{d}t} = -c_{3}G - c_{4}B - c_{5}P \\quad \\text{(Eq. 2)}\n```\n```latex\n\\frac{\\mathrm{d}B}{\\mathrm{d}t} = c_{6}P \\quad \\text{(Eq. 3)}\n```\n```latex\n\\frac{\\mathrm{d}G}{\\mathrm{d}t} = c_{7}B + c_{8}N + c_{9}P \\quad \\text{(Eq. 4)}\n```\n\n### Question\n\nAnalyze the feedback loop represented by the causal chain `P → B → G → P`. Based on the signs of the causal links in the system's Jacobian matrix, what is the nature of this feedback loop?", "Options": {"A": "Positive (destabilizing)", "B": "Negative (stabilizing)", "C": "There is no feedback loop involving these three variables.", "D": "The feedback is neutral (zero effect)."}, "Answer": "A", "pi_justification": "This item assesses the ability to identify and analyze a feedback loop within a dynamic system, a key application of causal analysis. It uses the Computational Judgment strategy. The student must trace the signs of the partial derivatives: `∂(dB/dt)/∂P` is `+`, `∂(dG/dt)/∂B` is `-`, and `∂(dP/dt)/∂G` is `-`. The product `(+) * (-) * (-)` is `+`. Distractor B is the Conceptual Opposite. Distractor C represents a failure to trace the causal path. Distractor D is an incorrect calculation.", "qid": "454", "question": "### Background\n\nThe paper analyzes the dynamic properties of complex systems, including feedback loops within them. The nature of a feedback loop (positive/destabilizing or negative/stabilizing) can be determined by the signs of the causal links that form it.\n\n### Data / Model Specification\n\nThe dynamics of a lake's ecosystem are described by the following system of equations, where `N`=nitrate, `P`=phosphate, `G`=green algae, `B`=blue-green algae, and all `c_i` are positive constants, except for `c₇ < 0`, which represents inhibition.\n\n```latex\n\\frac{\\mathrm{d}N}{\\mathrm{d}t} = -c_{1}G - c_{2}N \\quad \\text{(Eq. 1)}\n```\n```latex\n\\frac{\\mathrm{d}P}{\\mathrm{d}t} = -c_{3}G - c_{4}B - c_{5}P \\quad \\text{(Eq. 2)}\n```\n```latex\n\\frac{\\mathrm{d}B}{\\mathrm{d}t} = c_{6}P \\quad \\text{(Eq. 3)}\n```\n```latex\n\\frac{\\mathrm{d}G}{\\mathrm{d}t} = c_{7}B + c_{8}N + c_{9}P \\quad \\text{(Eq. 4)}\n```\n\n### Question\n\nAnalyze the feedback loop represented by the causal chain `P → B → G → P`. Based on the signs of the causal links in the system's Jacobian matrix, what is the nature of this feedback loop?", "model_evaluation_result": {"model_raw_response": "I need to analyze the feedback loop P → B → G → P by examining the signs of the causal links in the system.\n\nLet me trace through each link in the feedback loop:\n\n1) **P → B**: From Eq. 3: dB/dt = c₆P\n   Since c₆ > 0, an increase in P leads to an increase in B.\n   Sign of this link: **positive (+)**\n\n2) **B → G**: From Eq. 4: dG/dt = c₇B + c₈N + c₉P\n   The direct effect of B on G is through the term c₇B.\n   Since c₇ < 0, an increase in B leads to a decrease in G.\n   Sign of this link: **negative (-)**\n\n3) **G → P**: From Eq. 2: dP/dt = -c₃G - c₄B - c₅P\n   The direct effect of G on P is through the term -c₃G.\n   Since c₃ > 0, we have -c₃ < 0, so an increase in G leads to a decrease in P.\n   Sign of this link: **negative (-)**\n\nNow I need to determine the overall nature of the feedback loop by multiplying the signs:\n\nP → B → G → P: (+) × (-) × (-) = (+)\n\nThe overall sign is positive, which means this is a **positive feedback loop**.\n\nPositive feedback loops are destabilizing because they amplify changes - a small increase in P leads to effects that ultimately reinforce that increase in P.\n\nA", "model_answer": "B", "error": null}}
{"ID": 331, "Question": "### Background\n\n**Research Question.** This problem evaluates the complete in-sample empirical evidence for the paper's theoretical framework, which posits that spot and forward exchange rates are cointegrated and that the forward premiums form the basis of these long-run relationships.\n\n**Setting / Institutional Environment.** After establishing a cointegrating rank of `r=4` for the five-variable Dollar-Mark system, the paper tests whether the four forward premiums are a valid basis for this cointegration space. It then estimates the Vector Error Correction Model (VECM) to analyze the adjustment dynamics.\n\n### Data / Model Specification\n\n**Table 1. Test of Forward Premiums as Cointegration Basis (Dollar-Mark)**\n\n| | χ²(4) | Marginal Significance Level |\n|:---|:---:|:---:|\n| **Null Hypothesis** | 5.32 | 26% |\n\n*Null Hypothesis: The four forward premiums comprise a basis for the cointegration space.*\n\n**Table 2. VECM Estimates for `Δs_t` Equation (Dollar-Mark)**\n\n| Explanatory Variable | Coeff. |\n|:---|:---:|\n| `(s - f_4)_{t-1}` | -0.981 |\n| `(s - f_{13})_{t-1}` | -0.743 |\n| `(s - f_{26})_{t-1}` | -0.376 |\n| `(s - f_{52})_{t-1}` | -0.061 |\n\n### Question\n\nBased on the combined evidence from Table 1 and Table 2 for the Dollar-Mark, which statement provides the most accurate summary of the in-sample findings?", "Options": {"A": "The theory is supported: the forward premiums are a valid cointegrating basis, and the negative coefficients on the error correction terms imply that when the spot rate is above the forward rate, the dollar is expected to depreciate.", "B": "The theory is rejected: the test in Table 1 fails to reject the null hypothesis, and the coefficients in Table 2 are all negative, which is inconsistent with market efficiency.", "C": "The theory is partially supported: the forward premiums are a valid cointegrating basis, but the negative coefficients imply a momentum effect where a spot rate above the forward rate predicts further dollar appreciation.", "D": "The theory is rejected: the test in Table 1 shows with a 26% significance level that the forward premiums are not a valid basis for the long-run relationships."}, "Answer": "A", "pi_justification": "This item assesses the ability to synthesize results from two different empirical tests to form a coherent conclusion, a key skill in empirical analysis. It uses a Scenario Application strategy. The correct answer (A) correctly interprets the failure to reject the null in Table 1 and the economic meaning of the negative adjustment coefficients. Distractor B incorrectly interprets 'failing to reject'. Distractor C correctly interprets Table 1 but makes a 'Conceptual Opposite' error in interpreting the negative coefficients as momentum instead of error correction. Distractor D misinterprets the meaning of the marginal significance level.", "qid": "331", "question": "### Background\n\n**Research Question.** This problem evaluates the complete in-sample empirical evidence for the paper's theoretical framework, which posits that spot and forward exchange rates are cointegrated and that the forward premiums form the basis of these long-run relationships.\n\n**Setting / Institutional Environment.** After establishing a cointegrating rank of `r=4` for the five-variable Dollar-Mark system, the paper tests whether the four forward premiums are a valid basis for this cointegration space. It then estimates the Vector Error Correction Model (VECM) to analyze the adjustment dynamics.\n\n### Data / Model Specification\n\n**Table 1. Test of Forward Premiums as Cointegration Basis (Dollar-Mark)**\n\n| | χ²(4) | Marginal Significance Level |\n|:---|:---:|:---:|\n| **Null Hypothesis** | 5.32 | 26% |\n\n*Null Hypothesis: The four forward premiums comprise a basis for the cointegration space.*\n\n**Table 2. VECM Estimates for `Δs_t` Equation (Dollar-Mark)**\n\n| Explanatory Variable | Coeff. |\n|:---|:---:|\n| `(s - f_4)_{t-1}` | -0.981 |\n| `(s - f_{13})_{t-1}` | -0.743 |\n| `(s - f_{26})_{t-1}` | -0.376 |\n| `(s - f_{52})_{t-1}` | -0.061 |\n\n### Question\n\nBased on the combined evidence from Table 1 and Table 2 for the Dollar-Mark, which statement provides the most accurate summary of the in-sample findings?", "model_evaluation_result": {"model_raw_response": "Looking at this step by step:\n\n**Table 1 Analysis:**\n- The test has a χ²(4) = 5.32 with a marginal significance level of 26%\n- This means we fail to reject the null hypothesis at conventional significance levels (5% or 10%)\n- Since the null hypothesis states \"The four forward premiums comprise a basis for the cointegration space,\" failing to reject it means we accept that the forward premiums are a valid cointegrating basis\n\n**Table 2 Analysis:**\n- All coefficients on the error correction terms (s - f_k)_{t-1} are negative\n- These represent how the spot rate adjusts when it deviates from the forward rate\n- When (s - f_k)_{t-1} > 0 (spot rate above forward rate), the negative coefficients mean Δs_t < 0\n- This implies the spot rate decreases (dollar appreciates relative to the mark) to correct the deviation\n- This is consistent with the theory that forward rates contain information about future spot rate movements\n\n**Evaluation of Options:**\n- Option A correctly identifies that forward premiums are valid (Table 1) and correctly interprets the negative coefficients as indicating dollar depreciation when spot > forward\n- Option B incorrectly states the theory is rejected and misinterprets the significance of failing to reject the null\n- Option C incorrectly suggests momentum effects rather than error correction\n- Option D incorrectly interprets the 26% significance level as rejecting the theory\n\nA", "model_answer": "A", "error": null}}
{"ID": 401, "Question": "### Background\n\n**Research Question.** This problem investigates whether a system of intergovernmental transfers, specifically matching grants, can correct the inefficiencies of decentralized public good provision and achieve the first-best (socially optimal) outcome in a union of heterogeneous countries.\n\n**Setting / Institutional Environment.** The union establishes a matching grant system. For each unit of public good `g_i` a country provides, the union provides a matching subsidy `s`. This is financed by a tax on each country `i` equal to `T_i = s \\cdot \\bar{g}_{-i}`, where `\\bar{g}_{-i}` is the average spending of all other countries. The subsidy rate `s` is chosen by majority vote, so the median voter's preference `α_m` is decisive.\n\n### Data / Model Specification\n\nThe utility from the public good is logarithmic, `H(x) = ln(x)`. The first-best optimal subsidy rate, which aligns private and social incentives, is `s^* = β(N-1)`. The politico-economic equilibrium subsidy `s_m` chosen by the median voter is given by the expression in Eq. (1):\n\n```latex\ns_{m} = \\cfrac{1+\\beta(N-1)}{1+\\cfrac{N}{(1-\\beta)(N-1)}\\left(\\cfrac{\\bar{\\alpha}-\\alpha_{m}}{\\alpha_{m}}\\right)} - 1 \n```\n\nwhere `α_m` is the preference parameter of the median voter and `\\bar{\\alpha} = (1/N)\\sum_i α_i` is the average preference parameter in the union.\n\n### Question\n\nConsider a union where the distribution of preferences is skewed, such that the median preference is less than the mean preference (`α_m < \\bar{\\alpha}`). Which of the following statements correctly describes the resulting political equilibrium subsidy `s_m`?", "Options": {"A": "The equilibrium subsidy `s_m` will be inefficiently low (`s_m < s^*`).", "B": "The equilibrium subsidy `s_m` will be inefficiently high (`s_m > s^*`).", "C": "The equilibrium subsidy `s_m` will be efficient (`s_m = s^*`).", "D": "The equilibrium subsidy `s_m` will be negative, as the median voter seeks to tax public spending."}, "Answer": "A", "pi_justification": "Assessment Target: Tests the ability to analyze the direction of political bias in a fiscal federalism model. Strategy: Computational Judgment, requiring the student to evaluate a formula under a specific condition (`α_m < \\bar{\\alpha}`). Distractor Logic: (B) Sign Error: results from incorrectly evaluating the denominator in Eq. (1). (C) Conceptual Error: reflects a misunderstanding that the median voter outcome is not always efficient. (D) Formula Misuse: an extreme misinterpretation of the model's logic.", "qid": "401", "question": "### Background\n\n**Research Question.** This problem investigates whether a system of intergovernmental transfers, specifically matching grants, can correct the inefficiencies of decentralized public good provision and achieve the first-best (socially optimal) outcome in a union of heterogeneous countries.\n\n**Setting / Institutional Environment.** The union establishes a matching grant system. For each unit of public good `g_i` a country provides, the union provides a matching subsidy `s`. This is financed by a tax on each country `i` equal to `T_i = s \\cdot \\bar{g}_{-i}`, where `\\bar{g}_{-i}` is the average spending of all other countries. The subsidy rate `s` is chosen by majority vote, so the median voter's preference `α_m` is decisive.\n\n### Data / Model Specification\n\nThe utility from the public good is logarithmic, `H(x) = ln(x)`. The first-best optimal subsidy rate, which aligns private and social incentives, is `s^* = β(N-1)`. The politico-economic equilibrium subsidy `s_m` chosen by the median voter is given by the expression in Eq. (1):\n\n```latex\ns_{m} = \\cfrac{1+\\beta(N-1)}{1+\\cfrac{N}{(1-\\beta)(N-1)}\\left(\\cfrac{\\bar{\\alpha}-\\alpha_{m}}{\\alpha_{m}}\\right)} - 1 \n```\n\nwhere `α_m` is the preference parameter of the median voter and `\\bar{\\alpha} = (1/N)\\sum_i α_i` is the average preference parameter in the union.\n\n### Question\n\nConsider a union where the distribution of preferences is skewed, such that the median preference is less than the mean preference (`α_m < \\bar{\\alpha}`). Which of the following statements correctly describes the resulting political equilibrium subsidy `s_m`?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to compare the political equilibrium subsidy s_m with the first-best optimal subsidy s*.\n\nGiven information:\n- First-best optimal subsidy: s* = β(N-1)\n- Political equilibrium subsidy: s_m = [1+β(N-1)]/[1 + N/((1-β)(N-1)) × (α̅-α_m)/α_m] - 1\n- Condition: α_m < α̅ (median preference less than mean preference)\n\nLet me analyze the denominator of the fraction in s_m:\n\nSince α_m < α̅, we have:\n- (α̅ - α_m) > 0\n- (α̅ - α_m)/α_m > 0\n\nTherefore:\n- N/((1-β)(N-1)) × (α̅-α_m)/α_m > 0\n\nThis means the denominator equals:\n1 + [positive term] > 1\n\nSo the fraction [1+β(N-1)]/[1 + positive term] < [1+β(N-1)]/1 = 1+β(N-1)\n\nTherefore:\ns_m = [1+β(N-1)]/[1 + positive term] - 1 < [1+β(N-1)] - 1 = β(N-1) = s*\n\nThis shows that s_m < s*.\n\nThe economic intuition is clear: when the median voter has lower preferences for the public good than the average (α_m < α̅), they will choose a subsidy rate that is lower than what would be socially optimal, leading to underprovision of the public good.\n\nA", "model_answer": "A", "error": null}}
{"ID": 218, "Question": "### Background\n\nThis question analyzes the second-period pricing game in a duopoly model with switching costs. The setting is a standard spatial differentiation model (a Hotelling line of length *t*) where two firms, A and B, are located at the endpoints. In the second period, firms have established first-period market shares, *σ*ᴬ and *σ*ᴮ, and they set prices *p*₂ᴬ and *p*₂ᴮ simultaneously. Firms have a constant marginal cost *c*. Consumers incur a switching cost *s* if they change suppliers. The consumer base is heterogeneous: a fraction *ν* are new, a fraction *μ* have tastes that are now independent of their first-period location, and the remainder have unchanged tastes.\n\n### Data / Model Specification\n\nFirm A's second-period sales are given by:\n```latex\nq_{2}^{A}(p_{2}^{A},p_{2}^{B}) = \\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right] \\quad \\text{(Eq. 1)}\n```\nThe resulting Nash equilibrium price for Firm A is:\n```latex\np_{2}^{A} = c + \\frac{1}{\\mu+\\nu} \\left[ t + \\frac{1}{3}(2\\sigma^{A}-1)((1-\\mu-\\nu)t+\\mu s) \\right] \\quad \\text{(Eq. 2)}\n```\nAssume that the parameters are such that an interior solution exists and *μ*+*ν* > 0.\n\n### Question\n\nConsider a scenario where all consumers are 'mobile' (*μ*+*ν*=1), but the market is asymmetric, with Firm A holding a larger first-period market share (*σ*ᴬ > 1/2). Which statement best describes the equilibrium prices?", "Options": {"A": "Firm A sets a higher price than Firm B, and the price difference is directly proportional to the switching cost *s*.", "B": "Both firms set the same price, `p₂ = c + t`, because the effect of switching costs is completely nullified when *μ*+*ν*=1.", "C": "Firm A sets a lower price than Firm B to leverage its dominant position and further increase its market share.", "D": "The price difference between the firms is determined by the transport cost *t*, not the switching cost *s*, which becomes irrelevant."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This high-difficulty item assesses the student's ability to synthesize two key results and avoid overgeneralization. It uses a Reverse-Reasoning strategy. The distractors are designed to trap common errors: (B) incorrectly generalizes the symmetric result to the asymmetric case, (C) proposes an incorrect strategic motive, and (D) misattributes the source of the price difference in this specific scenario.", "qid": "218", "question": "### Background\n\nThis question analyzes the second-period pricing game in a duopoly model with switching costs. The setting is a standard spatial differentiation model (a Hotelling line of length *t*) where two firms, A and B, are located at the endpoints. In the second period, firms have established first-period market shares, *σ*ᴬ and *σ*ᴮ, and they set prices *p*₂ᴬ and *p*₂ᴮ simultaneously. Firms have a constant marginal cost *c*. Consumers incur a switching cost *s* if they change suppliers. The consumer base is heterogeneous: a fraction *ν* are new, a fraction *μ* have tastes that are now independent of their first-period location, and the remainder have unchanged tastes.\n\n### Data / Model Specification\n\nFirm A's second-period sales are given by:\n```latex\nq_{2}^{A}(p_{2}^{A},p_{2}^{B}) = \\frac{1}{2} \\left[ (\\sigma^{A}-\\sigma^{B})((1-\\mu-\\nu)t+\\mu s)+t+(\\mu+\\nu)(p_{2}^{B}-p_{2}^{A}) \\right] \\quad \\text{(Eq. 1)}\n```\nThe resulting Nash equilibrium price for Firm A is:\n```latex\np_{2}^{A} = c + \\frac{1}{\\mu+\\nu} \\left[ t + \\frac{1}{3}(2\\sigma^{A}-1)((1-\\mu-\\nu)t+\\mu s) \\right] \\quad \\text{(Eq. 2)}\n```\nAssume that the parameters are such that an interior solution exists and *μ*+*ν* > 0.\n\n### Question\n\nConsider a scenario where all consumers are 'mobile' (*μ*+*ν*=1), but the market is asymmetric, with Firm A holding a larger first-period market share (*σ*ᴬ > 1/2). Which statement best describes the equilibrium prices?", "model_evaluation_result": {"model_raw_response": "I need to analyze the equilibrium pricing when all consumers are mobile (μ+ν=1) but the market is asymmetric (σᴬ > 1/2).\n\nGiven information:\n- μ+ν = 1 (all consumers are mobile)\n- σᴬ > 1/2 (Firm A has larger first-period market share)\n- σᴮ = 1 - σᴬ < 1/2\n\nFrom Equation 2, Firm A's equilibrium price is:\np₂ᴬ = c + (1/(μ+ν))[t + (1/3)(2σᴬ-1)((1-μ-ν)t+μs)]\n\nSubstituting μ+ν = 1:\np₂ᴬ = c + [t + (1/3)(2σᴬ-1)((1-1)t+μs)]\np₂ᴬ = c + [t + (1/3)(2σᴬ-1)(μs)]\n\nSince μ+ν = 1, we have μ ≤ 1, and the term (1-μ-ν) = 0.\n\nBy symmetry, Firm B's price would be:\np₂ᴮ = c + [t + (1/3)(2σᴮ-1)(μs)]\np₂ᴮ = c + [t + (1/3)(2(1-σᴬ)-1)(μs)]\np₂ᴮ = c + [t + (1/3)(1-2σᴬ)(μs)]\n\nThe price difference is:\np₂ᴬ - p₂ᴮ = (1/3)(2σᴬ-1)(μs) - (1/3)(1-2σᴬ)(μs)\np₂ᴬ - p₂ᴮ = (1/3)μs[(2σᴬ-1) - (1-2", "model_answer": "A", "error": null}}
{"ID": 418, "Question": "### Background\n\nThe analysis uses an approximate formula to link an empirically observable statistic—the coefficient of variation (`cvar`) of prices—to the model's price dispersion measure `Δ`, and from there to an equivalent productivity loss.\n\n### Data / Model Specification\n\nThe model provides an approximate link between the theoretical `Δ` and the empirical `cvar`:\n\n```latex\n\\Delta \\simeq 1+\\frac{1}{2}\\frac{\\theta\\phi}{\\theta+1}(\\theta\\phi-\\theta+1)\\frac{\\mathrm{cvar}^{2}}{1-\\frac{1}{2}\\frac{\\theta}{\\theta+1}(\\mathrm{cvar}^{2}+1)}\n```\n(Eq. 1)\n\nThe productivity-equivalent cost is the percentage change, `1 - Δ^{-1/ϕ}`.\n\n**Table 1: Approximate Productivity Equivalent Cost of Price Dispersion**\n\n| | Benchmark |\n| :--- | :--- |\n| `cvar` | 10% |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n\nUsing the benchmark parameters from Table 1 and the formula in Eq. (1), calculate the productivity-equivalent cost of price dispersion. Which of the following is closest to your result?", "Options": {"A": "-2.8%", "B": "+2.8%", "C": "-0.7%", "D": "-1.6%"}, "Answer": "A", "pi_justification": "This item assesses the ability to execute a multi-step calculation using a complex formula and then apply the result in a second formula. It uses a Computational Judgment strategy. The correct answer (A) requires careful execution. Distractor (B) is a Sign Error. Distractor (C) corresponds to the result for a different `cvar` value from the full paper's Table 2, catching students who look up a number instead of calculating. Distractor (D) is the result of a plausible Formula Misuse error (miscalculating the denominator in Eq. 1).", "qid": "418", "question": "### Background\n\nThe analysis uses an approximate formula to link an empirically observable statistic—the coefficient of variation (`cvar`) of prices—to the model's price dispersion measure `Δ`, and from there to an equivalent productivity loss.\n\n### Data / Model Specification\n\nThe model provides an approximate link between the theoretical `Δ` and the empirical `cvar`:\n\n```latex\n\\Delta \\simeq 1+\\frac{1}{2}\\frac{\\theta\\phi}{\\theta+1}(\\theta\\phi-\\theta+1)\\frac{\\mathrm{cvar}^{2}}{1-\\frac{1}{2}\\frac{\\theta}{\\theta+1}(\\mathrm{cvar}^{2}+1)}\n```\n(Eq. 1)\n\nThe productivity-equivalent cost is the percentage change, `1 - Δ^{-1/ϕ}`.\n\n**Table 1: Approximate Productivity Equivalent Cost of Price Dispersion**\n\n| | Benchmark |\n| :--- | :--- |\n| `cvar` | 10% |\n| `θ` | 7 |\n| `ϕ` | 1.38 |\n\nUsing the benchmark parameters from Table 1 and the formula in Eq. (1), calculate the productivity-equivalent cost of price dispersion. Which of the following is closest to your result?", "model_evaluation_result": {"model_raw_response": "I need to calculate the productivity-equivalent cost using the given formula and parameters.\n\nGiven parameters:\n- cvar = 0.10 (10%)\n- θ = 7\n- ϕ = 1.38\n\nFirst, I'll calculate Δ using Equation 1:\n\nΔ ≈ 1 + (1/2) × (θϕ)/(θ+1) × (θϕ-θ+1) × cvar²/(1 - (1/2) × θ/(θ+1) × (cvar²+1))\n\nLet me substitute the values:\n- θϕ = 7 × 1.38 = 9.66\n- θ + 1 = 8\n- θϕ - θ + 1 = 9.66 - 7 + 1 = 3.66\n- cvar² = 0.01\n\nNumerator of the fraction:\n(1/2) × (9.66/8) × 3.66 × 0.01 = (1/2) × 1.2075 × 3.66 × 0.01 = 0.0221\n\nDenominator of the fraction:\n1 - (1/2) × (7/8) × (0.01 + 1) = 1 - (1/2) × 0.875 × 1.01 = 1 - 0.4419 = 0.5581\n\nSo: Δ ≈ 1 + 0.0221/0.5581 ≈ 1 + 0.0396 ≈ 1.0396\n\nNow I calculate the productivity-equivalent cost:\n1 - Δ^(-1/ϕ) = 1 - (1.0396)^(-1/1.38) = 1 - (1.0396)^(-0.7246)\n\n(1.0396)^(-0.7246) ≈ 0.972\n\nTherefore: 1 - 0.972 = 0.028 = 2.8", "model_answer": "I need to calculate the productivity-equivalent cost using the given formula and parameters.\n\nGiven parameters:\n- cvar = 0.10 (10%)\n- θ = 7\n- ϕ = 1.38\n\nFirst, I'll calculate Δ using Equation 1:\n\nΔ ≈ 1 + (1/2) × (θϕ)/(θ+1) × (θϕ-θ+1) × cvar²/(1 - (1/2) × θ/(θ+1) × (cvar²+1))\n\nLet me substitute the values:\n- θϕ = 7 × 1.38 = 9.66\n- θ + 1 = 8\n- θϕ - θ + 1 = 9.66 - 7 + 1 = 3.66\n- cvar² = 0.01\n\nNumerator of the fraction:\n(1/2) × (9.66/8) × 3.66 × 0.01 = (1/2) × 1.2075 × 3.66 × 0.01 = 0.0221\n\nDenominator of the fraction:\n1 - (1/2) × (7/8) × (0.01 + 1) = 1 - (1/2) × 0.875 × 1.01 = 1 - 0.4419 = 0.5581\n\nSo: Δ ≈ 1 + 0.0221/0.5581 ≈ 1 + 0.0396 ≈ 1.0396\n\nNow I calculate the productivity-equivalent cost:\n1 - Δ^(-1/ϕ) = 1 - (1.0396)^(-1/1.38) = 1 - (1.0396)^(-0.7246)\n\n(1.0396)^(-0.7246) ≈ 0.972\n\nTherefore: 1 - 0.972 = 0.028 = 2.8", "error": "Warning: Could not extract single choice letter from: I need to calculate the productivity-equivalent co..."}}
{"ID": 192, "Question": "### Background\n\n**Research Question.** This problem develops the paper's theoretical framework to understand how leader identity affects group coordination in a weakest-link game by introducing heterogeneity in player types.\n\n**Setting / Institutional Environment.** The model considers a four-player coordination game where players choose effort `x` on a continuum `[0, W]`. A fraction of the population are 'behavioral types' who follow leaders of their own religion but not others. 'Rational types' maximize expected payoffs, accounting for the possible presence of behavioral types. The analysis assumes the probability of having more than one behavioral player in a group is negligible.\n\n### Data / Model Specification\n\n1.  **No-Leader Equilibrium:** In the absence of a leader, rational players assume others' efforts are drawn from `U[0, W]`. The optimal strategy is to match the expected minimum of the other three players' efforts, leading to a baseline effort `x*`.\n\n2.  **Leader Equilibrium:** With a leader, the optimal effort choices for a rational player under a Hindu leader (`x_H`) and a Muslim leader (`x_M`) are given by:\n\n    ```latex\n    x_{H} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( (1-p)a_{H}\\frac{W}{3} + p a_{M}x^{**} \\right) \\quad \\text{(Eq. 1)}\n    ```\n\n    ```latex\n    x_{M} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( p a_{M}\\frac{W}{3} + (1-p)a_{H}x^{**} \\right) \\quad \\text{(Eq. 2)}\n    ```\n\n    where `p` is the population share of Muslims, `a_H` and `a_M` are the fractions of behavioral types for each religion, `\\tilde{a} = p a_M + (1-p) a_H` is the overall fraction of behavioral types, `x**` is a low effort level (`x** < x*`), and `W` is the maximum effort.\n\n### The Question\n\nThe paper empirically finds that coordination is higher under Muslim leaders than Hindu leaders. According to the model, which of the following conditions on the parameters must hold for a rational player's optimal effort to be higher under a Muslim leader (`x_M > x_H`)?", "Options": {"A": "`p * a_M > (1-p) * a_H`", "B": "`a_M > a_H`", "C": "`p * a_M < (1-p) * a_H`", "D": "`p / a_M > (1-p) / a_H`"}, "Answer": "A", "pi_justification": "Tests the ability to derive or recognize the central predictive condition of the model. Strategy: Reverse-Reasoning (inferring the condition from the result). Distractors represent the most common errors: ignoring population weights (Step-Omission), reversing the inequality (Conceptual Opposite), and algebraic mistakes (Formula Misuse).", "qid": "192", "question": "### Background\n\n**Research Question.** This problem develops the paper's theoretical framework to understand how leader identity affects group coordination in a weakest-link game by introducing heterogeneity in player types.\n\n**Setting / Institutional Environment.** The model considers a four-player coordination game where players choose effort `x` on a continuum `[0, W]`. A fraction of the population are 'behavioral types' who follow leaders of their own religion but not others. 'Rational types' maximize expected payoffs, accounting for the possible presence of behavioral types. The analysis assumes the probability of having more than one behavioral player in a group is negligible.\n\n### Data / Model Specification\n\n1.  **No-Leader Equilibrium:** In the absence of a leader, rational players assume others' efforts are drawn from `U[0, W]`. The optimal strategy is to match the expected minimum of the other three players' efforts, leading to a baseline effort `x*`.\n\n2.  **Leader Equilibrium:** With a leader, the optimal effort choices for a rational player under a Hindu leader (`x_H`) and a Muslim leader (`x_M`) are given by:\n\n    ```latex\n    x_{H} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( (1-p)a_{H}\\frac{W}{3} + p a_{M}x^{**} \\right) \\quad \\text{(Eq. 1)}\n    ```\n\n    ```latex\n    x_{M} = (1-\\tilde{a})^{3}x^{*} + 3(1-\\tilde{a})^{2} \\left( p a_{M}\\frac{W}{3} + (1-p)a_{H}x^{**} \\right) \\quad \\text{(Eq. 2)}\n    ```\n\n    where `p` is the population share of Muslims, `a_H` and `a_M` are the fractions of behavioral types for each religion, `\\tilde{a} = p a_M + (1-p) a_H` is the overall fraction of behavioral types, `x**` is a low effort level (`x** < x*`), and `W` is the maximum effort.\n\n### The Question\n\nThe paper empirically finds that coordination is higher under Muslim leaders than Hindu leaders. According to the model, which of the following conditions on the parameters must hold for a rational player's optimal effort to be higher under a Muslim leader (`x_M > x_H`)?", "model_evaluation_result": {"model_raw_response": "I need to analyze the conditions under which a rational player's optimal effort is higher under a Muslim leader than a Hindu leader (x_M > x_H).\n\nGiven equations:\n- x_H = (1-ã)³x* + 3(1-ã)²[(1-p)a_H(W/3) + pa_M x**]\n- x_M = (1-ã)³x* + 3(1-ã)²[pa_M(W/3) + (1-p)a_H x**]\n\nwhere ã = pa_M + (1-p)a_H\n\nFor x_M > x_H, I need:\nx_M - x_H > 0\n\nSubstituting the equations:\nx_M - x_H = 3(1-ã)²[(pa_M(W/3) + (1-p)a_H x**) - ((1-p)a_H(W/3) + pa_M x**)]\n\nSimplifying:\nx_M - x_H = 3(1-ã)²[pa_M(W/3) - pa_M x** + (1-p)a_H x** - (1-p)a_H(W/3)]\n\n= 3(1-ã)²[pa_M(W/3 - x**) - (1-p)a_H(W/3 - x**)]\n\n= 3(1-ã)²(W/3 - x**)[pa_M - (1-p)a_H]\n\nSince:\n- 3(1-ã)² > 0 (always positive)\n- (W/3 - x**) > 0 (since x** < x* and x* would be less than W/3 in equilibrium)\n\nFor x_M > x_H, we need:\npa_M - (1-p)a_H > 0\n\nTherefore: pa_M > (1-p)a_H\n\nA", "model_answer": "A", "error": null}}
{"ID": 430, "Question": "### Background\n\n**Research Question.** This problem requires interpreting empirical results from a GMM estimation to distinguish between two competing theories of preference heterogeneity: whether the mean of consumption growth (linked to the Rate of Time Preference, RTP) or the volatility of consumption growth (linked to the Intertemporal Elasticity of Substitution, IES) varies with household wealth.\n\n**Setting / Institutional Environment.** The analysis is based on GMM estimation of a consumption growth model using a panel of Indian households. The model is designed to separately estimate a parameter for wealth-dependent mean growth (`$b_y$`) and a parameter for wealth-dependent volatility (`$\\gamma$`).\n\n**Variables & Parameters.**\n- `$\\gamma$`: Subsistence parameter, linked to wealth-varying IES. A positive `$\\gamma$` implies the IES rises with wealth.\n- `$b_y$`: Coefficient on a permanent income proxy, linked to wealth-varying RTP. A non-zero `$b_y$` implies the RTP varies with wealth.\n- `$J$`-statistic: Hansen's test statistic for overidentifying restrictions.\n- `$C$`-statistic: A likelihood ratio-type test statistic for parameter restrictions, calculated as a difference in `$J$`-statistics.\n\n---\n\n### Data / Model Specification\n\nThe estimated statistical model is:\n```latex\n\\ln[C_{h}(t+1) - \\gamma] - \\ln[C_{h}(t) - \\gamma] = \\phi(t) + b_{y} y_{h}^{c} + v_{h}(t) \\quad \\text{(Eq. 1)}\n```\nKey results for total consumption, based on Table 2 in the paper, are summarized below.\n\n**Table 1: GMM Results for Total Consumption**\n| Model Specification | `$\\hat{\\gamma}$` (s.e.) | `$\\hat{b}_y$` (s.e.) | J-statistic (p-value %) | C-statistic (p-value %) | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1. Unrestricted | 177.6 (6.70) | -0.021 (0.049) | 34.44 (18.7) | 0.255 (61.4) | Tests `$b_y=0$` vs Model 2 |\n| 2. Restricted (`$b_y=0$`) | 177.2 (7.45) | 0 (imposed) | 34.70 (21.5) | — | Preferred Specification |\n| 3. Restricted (`$\\gamma=b_y=0$`) | 0 (imposed) | 0 (imposed) | 98.89 (0.0) | 64.428 (0.0) | Tests joint null vs Model 2 |\n\n---\n\n### The Question\n\nConsidering the full set of results in Table 1, which statement best explains why the model in Row 2 (`$b_y=0$`) is the authors' preferred specification?", "Options": {"A": "The unrestricted model (Row 1) is preferred because it has the highest p-value on its J-statistic, indicating the best model fit.", "B": "The simplest model (Row 3) is chosen because its parameters are easiest to interpret, despite its low p-value.", "C": "The model in Row 2 is selected because the data strongly reject the simplest model (Row 3), but do not reject the simplification from the full model (Row 1) to the model in Row 2.", "D": "The model in Row 2 is chosen because the C-statistic in Row 1 shows that `$b_y$` is significantly different from zero, confirming its importance."}, "Answer": "C", "pi_justification": "This item assesses the user's ability to synthesize multiple hypothesis tests (specifically, nested model tests using C-statistics) to understand the logic of model selection. It uses a Computational Judgment strategy. The distractors represent common fallacies: misinterpreting the J-test (A), ignoring statistical evidence (B), and misreading the result of a hypothesis test (D).", "qid": "430", "question": "### Background\n\n**Research Question.** This problem requires interpreting empirical results from a GMM estimation to distinguish between two competing theories of preference heterogeneity: whether the mean of consumption growth (linked to the Rate of Time Preference, RTP) or the volatility of consumption growth (linked to the Intertemporal Elasticity of Substitution, IES) varies with household wealth.\n\n**Setting / Institutional Environment.** The analysis is based on GMM estimation of a consumption growth model using a panel of Indian households. The model is designed to separately estimate a parameter for wealth-dependent mean growth (`$b_y$`) and a parameter for wealth-dependent volatility (`$\\gamma$`).\n\n**Variables & Parameters.**\n- `$\\gamma$`: Subsistence parameter, linked to wealth-varying IES. A positive `$\\gamma$` implies the IES rises with wealth.\n- `$b_y$`: Coefficient on a permanent income proxy, linked to wealth-varying RTP. A non-zero `$b_y$` implies the RTP varies with wealth.\n- `$J$`-statistic: Hansen's test statistic for overidentifying restrictions.\n- `$C$`-statistic: A likelihood ratio-type test statistic for parameter restrictions, calculated as a difference in `$J$`-statistics.\n\n---\n\n### Data / Model Specification\n\nThe estimated statistical model is:\n```latex\n\\ln[C_{h}(t+1) - \\gamma] - \\ln[C_{h}(t) - \\gamma] = \\phi(t) + b_{y} y_{h}^{c} + v_{h}(t) \\quad \\text{(Eq. 1)}\n```\nKey results for total consumption, based on Table 2 in the paper, are summarized below.\n\n**Table 1: GMM Results for Total Consumption**\n| Model Specification | `$\\hat{\\gamma}$` (s.e.) | `$\\hat{b}_y$` (s.e.) | J-statistic (p-value %) | C-statistic (p-value %) | Notes |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| 1. Unrestricted | 177.6 (6.70) | -0.021 (0.049) | 34.44 (18.7) | 0.255 (61.4) | Tests `$b_y=0$` vs Model 2 |\n| 2. Restricted (`$b_y=0$`) | 177.2 (7.45) | 0 (imposed) | 34.70 (21.5) | — | Preferred Specification |\n| 3. Restricted (`$\\gamma=b_y=0$`) | 0 (imposed) | 0 (imposed) | 98.89 (0.0) | 64.428 (0.0) | Tests joint null vs Model 2 |\n\n---\n\n### The Question\n\nConsidering the full set of results in Table 1, which statement best explains why the model in Row 2 (`$b_y=0$`) is the authors' preferred specification?", "model_evaluation_result": {"model_raw_response": "Looking at the empirical results in Table 1, I need to interpret the statistical tests to understand why Row 2 is the preferred specification.\n\nKey observations:\n\n1. **C-statistic in Row 1**: Tests whether b_y = 0 (comparing Row 1 vs Row 2)\n   - C-statistic = 0.255 with p-value = 61.4%\n   - This high p-value means we FAIL to reject the null hypothesis that b_y = 0\n   - Therefore, the restriction b_y = 0 is supported by the data\n\n2. **C-statistic in Row 3**: Tests the joint restriction γ = b_y = 0 (comparing Row 3 vs Row 2)\n   - C-statistic = 64.428 with p-value = 0.0%\n   - This very low p-value means we STRONGLY REJECT the joint restriction\n   - Therefore, we cannot simplify to the model where both parameters are zero\n\n3. **Model selection logic**:\n   - Row 3 (simplest model) is rejected due to its extremely low p-value (0.0%)\n   - Row 1 (unrestricted) can be simplified to Row 2 since b_y is not significantly different from zero\n   - Row 2 strikes the right balance: it retains the important γ parameter while eliminating the unnecessary b_y parameter\n\nThe data support keeping γ (wealth-varying IES) but not b_y (wealth-varying RTP), making Row 2 the most parsimonious model that fits the data adequately.\n\nC", "model_answer": "C", "error": null}}
{"ID": 307, "Question": "### Background\n\n**Research Question.** This problem compares the properties of two specific \"flexible functional forms\" derived from the linear parallel structure: the Parallel Linear Asymmetric Quadratic (PLAQ) and the Parallel Linear Generalized Linear (PLGL). It focuses on how their different mathematical structures lead to starkly different economic implications for substitution and satiation.\n\n**Setting / Institutional Environment.** We analyze a system of `N` goods where good 1 is the numeraire. All other prices (`p_i*`) are normalized by the price of the numeraire. The analysis focuses on the compensated (Hicksian) demand for the non-numeraire goods (`i, j > 1`).\n\n**Variables & Parameters.**\n- `φ^i(u,p*)`: Hicksian demand for non-numeraire good `i`.\n- `p_i*`: Normalized price of good `i`.\n\n---\n\n### Data / Model Specification\n\nThe Hicksian demand functions for non-numeraire goods (`i > 1`) for the two models are given by:\n\n**PLAQ Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + α_i + \\sum_{j=2}^{N} β_{ij} p_j^* \n\n\\quad\\text{(Eq. 1)}\n```\n\n**PLGL Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + \\frac{1}{2}α_i(p_i*)^{-1/2} + \\frac{1}{2}\\sum_{j=2}^{N} β_{ij} \\left(\\frac{p_j^*}{p_i^*}\\right)^{1/2}\n\n\\quad\\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nBased on the Hicksian demand functions for the PLAQ and PLGL models (Eq. 1 and Eq. 2), how does the compensated cross-price substitution effect, `s_{ij}(p*) = ∂φ^i/∂p_j*` (for `i, j > 1`), fundamentally differ between them?", "Options": {"A": "In PLAQ, the substitution effect is constant, while in PLGL, it diminishes as prices increase.", "B": "In PLAQ, the substitution effect diminishes as prices increase, while in PLGL, it is constant.", "C": "In both models, the substitution effect is constant, but the level is determined by different parameters.", "D": "In PLAQ, the substitution effect is always positive, while in PLGL, it can be negative."}, "Answer": "A", "pi_justification": "This item directly tests the core comparison from the original QA's first two parts. It uses a Computational Judgment strategy. The primary distractor (B) is a direct conceptual reversal, testing for careful reading and differentiation. Other distractors offer false comparisons.", "qid": "307", "question": "### Background\n\n**Research Question.** This problem compares the properties of two specific \"flexible functional forms\" derived from the linear parallel structure: the Parallel Linear Asymmetric Quadratic (PLAQ) and the Parallel Linear Generalized Linear (PLGL). It focuses on how their different mathematical structures lead to starkly different economic implications for substitution and satiation.\n\n**Setting / Institutional Environment.** We analyze a system of `N` goods where good 1 is the numeraire. All other prices (`p_i*`) are normalized by the price of the numeraire. The analysis focuses on the compensated (Hicksian) demand for the non-numeraire goods (`i, j > 1`).\n\n**Variables & Parameters.**\n- `φ^i(u,p*)`: Hicksian demand for non-numeraire good `i`.\n- `p_i*`: Normalized price of good `i`.\n\n---\n\n### Data / Model Specification\n\nThe Hicksian demand functions for non-numeraire goods (`i > 1`) for the two models are given by:\n\n**PLAQ Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + α_i + \\sum_{j=2}^{N} β_{ij} p_j^* \n\n\\quad\\text{(Eq. 1)}\n```\n\n**PLGL Model:**\n```latex\nφ^i(u,p*) = h(u)δ_i + \\frac{1}{2}α_i(p_i*)^{-1/2} + \\frac{1}{2}\\sum_{j=2}^{N} β_{ij} \\left(\\frac{p_j^*}{p_i^*}\\right)^{1/2}\n\n\\quad\\text{(Eq. 2)}\n```\n\n---\n\n### Question\n\nBased on the Hicksian demand functions for the PLAQ and PLGL models (Eq. 1 and Eq. 2), how does the compensated cross-price substitution effect, `s_{ij}(p*) = ∂φ^i/∂p_j*` (for `i, j > 1`), fundamentally differ between them?", "model_evaluation_result": {"model_raw_response": "I need to find the compensated cross-price substitution effect for both models by taking the partial derivative of the Hicksian demand with respect to the price of another good.\n\nFor PLAQ Model (Eq. 1):\nφ^i(u,p*) = h(u)δ_i + α_i + ∑_{j=2}^{N} β_{ij} p_j*\n\nTaking the partial derivative with respect to p_j* (where j ≠ i):\n∂φ^i/∂p_j* = β_{ij}\n\nThis is a constant - it doesn't depend on any prices.\n\nFor PLGL Model (Eq. 2):\nφ^i(u,p*) = h(u)δ_i + (1/2)α_i(p_i*)^{-1/2} + (1/2)∑_{j=2}^{N} β_{ij} (p_j*/p_i*)^{1/2}\n\nTaking the partial derivative with respect to p_j* (where j ≠ i):\n∂φ^i/∂p_j* = (1/2)β_{ij} × (1/2)(p_j*/p_i*)^{-1/2} × (1/p_i*)\n= (1/4)β_{ij} × (p_i*/p_j*)^{1/2} × (1/p_i*)\n= (1/4)β_{ij} × (p_i*)^{-1/2} × (p_j*)^{-1/2}\n\nThis depends on both p_i* and p_j* - as prices increase, the substitution effect diminishes (since both prices appear with negative exponents).\n\nComparing the results:\n- PLAQ: substitution effect = β_{ij} (constant)\n- PLGL: substitution effect = (1/4)β_{ij}(p_i*)^{-1/2}(p_j*)^{-1/2} (diminishes as prices increase)\n\nA", "model_answer": "A", "error": null}}
{"ID": 134, "Question": "## Background\n\n**Research Question.** This problem investigates why a profit-maximizing platform might subsidize transactions between its users when they bargain under asymmetric information.\n\n**Setting.** A platform facilitates interactions where end-users (buyers B, sellers S) bargain over a transaction price. Their ex-post benefits `b^i` are private information. The platform sets a total usage fee `a = a^B + a^S` and has a marginal cost `c` per transaction.\n\n**Variables & Parameters.**\n\n*   `a`: Total platform usage fee per transaction.\n*   `c`: Platform's marginal cost per transaction.\n*   `a*`: The optimal total usage fee set by the platform.\n*   `v(a)`: Average social surplus from a potential interaction, `v(a) = E[(b^B + b^S - c)x(b,a)]`.\n*   `x(b,a)`: Probability of trade given benefits `b` and fee `a`.\n*   `X(a)`: Expected volume of trade, `X(a) = E[x(b,a)]`.\n*   `L(a)`: The maximized joint surplus of the end-users, `L(a) = max E[(b^B + b^S - a)x(b,a)]`.\n\n---\n\n## Data / Model Specification\n\nThe platform's optimization problem involves setting the total usage fee `a` to maximize the average social surplus from interactions, `v(a)`. The first-order condition for the platform's choice of `a` is:\n```latex\nv'(a^*) = L'(a^*) + X(a^*) + (a^* - c)X'(a^*) = 0 \\quad \\text{(Eq. 1)}\n```\nIt is also known that `L'(a) < -X(a)` and a regularity assumption `X'(a) < 0` (volume of trade decreases with the fee) holds.\n\n---\n\n## Question\n\nGiven the first-order condition in Eq. (1) and the associated inequalities, what is the necessary relationship between the platform's optimal transaction fee `a*` and its marginal cost `c`?", "Options": {"A": "`a* < c`", "B": "`a* > c`", "C": "`a* = c`", "D": "The relationship cannot be determined from the information given."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This item assesses the student's ability to perform a mathematical derivation from a given first-order condition. It uses a Computational Judgment strategy. The correct answer requires correctly manipulating the inequalities. Distractor B (Sign Error) results from misinterpreting the inequalities. Distractor C (Formula Misuse) is the correct answer for the Coasian benchmark but incorrect here. Distractor D (Step-Omission Error) targets students unable to complete the derivation.", "qid": "134", "question": "## Background\n\n**Research Question.** This problem investigates why a profit-maximizing platform might subsidize transactions between its users when they bargain under asymmetric information.\n\n**Setting.** A platform facilitates interactions where end-users (buyers B, sellers S) bargain over a transaction price. Their ex-post benefits `b^i` are private information. The platform sets a total usage fee `a = a^B + a^S` and has a marginal cost `c` per transaction.\n\n**Variables & Parameters.**\n\n*   `a`: Total platform usage fee per transaction.\n*   `c`: Platform's marginal cost per transaction.\n*   `a*`: The optimal total usage fee set by the platform.\n*   `v(a)`: Average social surplus from a potential interaction, `v(a) = E[(b^B + b^S - c)x(b,a)]`.\n*   `x(b,a)`: Probability of trade given benefits `b` and fee `a`.\n*   `X(a)`: Expected volume of trade, `X(a) = E[x(b,a)]`.\n*   `L(a)`: The maximized joint surplus of the end-users, `L(a) = max E[(b^B + b^S - a)x(b,a)]`.\n\n---\n\n## Data / Model Specification\n\nThe platform's optimization problem involves setting the total usage fee `a` to maximize the average social surplus from interactions, `v(a)`. The first-order condition for the platform's choice of `a` is:\n```latex\nv'(a^*) = L'(a^*) + X(a^*) + (a^* - c)X'(a^*) = 0 \\quad \\text{(Eq. 1)}\n```\nIt is also known that `L'(a) < -X(a)` and a regularity assumption `X'(a) < 0` (volume of trade decreases with the fee) holds.\n\n---\n\n## Question\n\nGiven the first-order condition in Eq. (1) and the associated inequalities, what is the necessary relationship between the platform's optimal transaction fee `a*` and its marginal cost `c`?", "model_evaluation_result": {"model_raw_response": "I need to analyze the first-order condition and the given inequalities to determine the relationship between a* and c.\n\nGiven information:\n- First-order condition: v'(a*) = L'(a*) + X(a*) + (a* - c)X'(a*) = 0\n- L'(a) < -X(a)\n- X'(a) < 0 (regularity assumption)\n\nFrom the first-order condition:\nL'(a*) + X(a*) + (a* - c)X'(a*) = 0\n\nRearranging:\n(a* - c)X'(a*) = -L'(a*) - X(a*)\n\nSince X'(a*) < 0, I can divide both sides by X'(a*), which flips the inequality sign:\na* - c = [-L'(a*) - X(a*)] / X'(a*)\n\nGiven that L'(a*) < -X(a*), this means:\n-L'(a*) > X(a*)\n\nTherefore:\n-L'(a*) - X(a*) > X(a*) - X(a*) = 0\n\nSo: -L'(a*) - X(a*) > 0\n\nSince X'(a*) < 0, we have:\n[-L'(a*) - X(a*)] / X'(a*) < 0\n\nThis means:\na* - c < 0\n\nTherefore:\na* < c\n\nThe platform optimally sets its total usage fee below its marginal cost, effectively subsidizing transactions.\n\nA", "model_answer": "A", "error": null}}
{"ID": 222, "Question": "### Background\n\n**Research Question.** This question assesses a deep understanding of potential identification problems in stochastic frontier analysis (SFA), specifically how misspecification of one error component can bias the estimation of another.\n\n**Setting / Institutional Environment.** A cross-section of U.S. banks is analyzed using an SFA model. The model decomposes the deviation of a bank's observed cost from the efficient frontier into multiple components, including technical and allocative inefficiency.\n\n### Data / Model Specification\n\nThe econometric model for observed log cost for bank `s` is:\n\n```latex\n\\ln(w'\\cdot x)_{s} = \\ln c(y_s, w_s, z_s) + T_{s} + A + u_{o s} \\quad \\text{(Eq. 1)}\n```\n\nwhere `ln c(...)` is the efficient cost frontier, `Tₛ ≥ 0` is technical inefficiency, `A ≥ 0` is the cost of allocative inefficiency, and `uₒₛ` is random noise. A key assumption in the paper's model is that `A` is constant for all banks in the sample.\n\n### Question\n\nSuppose the model's assumption is wrong. In reality, large banks are perfectly allocatively efficient (their true `A` is 0), while small banks are not. If an econometrician estimates the misspecified model assuming a constant `A` for all banks, what will be the direction of bias in the estimated technical inefficiency, `T̂ₛ`, for the large, allocatively efficient banks?", "Options": {"A": "Biased downwards.", "B": "Biased upwards.", "C": "Unbiased.", "D": "The direction of bias cannot be determined."}, "Answer": "A", "pi_justification": "This high-difficulty item assesses deep understanding of model identification and how misspecification of one error component can bias another. It uses a Scenario Application strategy. The correct answer requires reasoning that the model will estimate an average `Â > 0` and impose it on the large banks (whose true `A` is 0), forcing the estimate of the other one-sided error term, `T̂ₛ`, to be lower to compensate. Distractors are based on high-fidelity misconceptions: (B) the conceptual opposite (upward bias); (C) a naive 'no effect' assumption; and (D) an appeal to ambiguity, which is incorrect as the direction of bias is determinate in this scenario.", "qid": "222", "question": "### Background\n\n**Research Question.** This question assesses a deep understanding of potential identification problems in stochastic frontier analysis (SFA), specifically how misspecification of one error component can bias the estimation of another.\n\n**Setting / Institutional Environment.** A cross-section of U.S. banks is analyzed using an SFA model. The model decomposes the deviation of a bank's observed cost from the efficient frontier into multiple components, including technical and allocative inefficiency.\n\n### Data / Model Specification\n\nThe econometric model for observed log cost for bank `s` is:\n\n```latex\n\\ln(w'\\cdot x)_{s} = \\ln c(y_s, w_s, z_s) + T_{s} + A + u_{o s} \\quad \\text{(Eq. 1)}\n```\n\nwhere `ln c(...)` is the efficient cost frontier, `Tₛ ≥ 0` is technical inefficiency, `A ≥ 0` is the cost of allocative inefficiency, and `uₒₛ` is random noise. A key assumption in the paper's model is that `A` is constant for all banks in the sample.\n\n### Question\n\nSuppose the model's assumption is wrong. In reality, large banks are perfectly allocatively efficient (their true `A` is 0), while small banks are not. If an econometrician estimates the misspecified model assuming a constant `A` for all banks, what will be the direction of bias in the estimated technical inefficiency, `T̂ₛ`, for the large, allocatively efficient banks?", "model_evaluation_result": {"model_raw_response": "Looking at this stochastic frontier analysis problem, I need to analyze what happens when the model assumes constant allocative inefficiency (A) across all banks, but in reality large banks have A = 0 while small banks have A > 0.\n\nLet me work through the logic:\n\n**True model for large banks:**\nln(w'·x)_s = ln c(y_s, w_s, z_s) + T_s + 0 + u_os\n\n**True model for small banks:**  \nln(w'·x)_s = ln c(y_s, w_s, z_s) + T_s + A_small + u_os (where A_small > 0)\n\n**Misspecified model (what the econometrician estimates):**\nln(w'·x)_s = ln c(y_s, w_s, z_s) + T_s + A_constant + u_os\n\nThe econometrician will estimate A_constant as some average value between 0 and A_small, so A_constant > 0.\n\nFor large banks specifically:\n- **True deviation from frontier:** T_s + 0 = T_s\n- **Estimated deviation from frontier:** T̂_s + A_constant\n\nSince the total observed deviation from the efficient frontier is the same in both cases, we have:\nT_s = T̂_s + A_constant\n\nTherefore: T̂_s = T_s - A_constant\n\nSince A_constant > 0, we get T̂_s < T_s.\n\nThe estimated technical inefficiency for large banks will be **biased downwards** because the model incorrectly attributes part of their true technical inefficiency to allocative inefficiency.\n\nB", "model_answer": "A", "error": null}}
{"ID": 498, "Question": "### Background\n\n**Research Question.** This problem analyzes experimental evidence from the Dirty Faces game to assess whether group behavior conforms to the theoretical equilibrium, particularly as the game's complexity (number of players `n`) and the required level of iterated reasoning (`k-1` steps) increase.\n\n**Setting.** An experiment is conducted on the Dirty Faces game with two different group sizes: `n=2` and `n=3`. At the start of each round, it is publicly announced that at least one player has type 'X'. In its analysis of the `n=2, XX` case, the paper notes that while only 4 of 18 groups conformed to the full predicted behavior, 14 of the 18 groups successfully completed Period 1 by playing `(UU)` and thus reached the second period.\n\n**Variables and Parameters.**\n- `n`: Number of players (2 or 3).\n- `k`: Number of players with type 'X'.\n- `Predicted Behavior`: The sequence of actions corresponding to the unique Nash equilibrium.\n\n### Data / Model Specification\n\nThe theoretical equilibrium requires `k-1` levels of iterated knowledge of rationality, where `k` is the number of players with type 'X'. The experimental results for group-level behavior are summarized in Table 1 (for `n=2`) and Table 2 (for `n=3`).\n\n**Table 1. Summary of Group Results for n=2**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XO | 8 | (DU) | 7 | 0.88 |\n| XX | 18 | ((UU)(DD)) | 4 | 0.22 |\n| **Total** | **26** | | **11** | **0.42** |\n\n**Table 2. Summary of Group Results for n=3**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XOO | 6 | (DUU) | 3 | 0.50 |\n| XXO | 10 | (UUU)(DDU) | 1 | 0.10 |\n| XXX | 12 | (UUU)(UUU)(DDD) | 0 | 0.00 |\n| **Total** | **28** | | **4** | **0.14** |\n\n### Question\n\nAn analyst proposes a simple error model where `p_E` is the probability a player who should play 'U' mistakenly plays 'D'. First, estimate `p_E` using the data for the `n=3, XOO` case from Table 2. Then, use this `p_E` to predict the success rate for Period 1 of the `n=2, XX` case. How does this prediction compare to the observed Period 1 success rate, and what does the comparison imply?", "Options": {"A": "The model predicts a ~50% success rate, which significantly under-predicts the observed ~78% success rate, suggesting the error rate is not constant and is likely lower in the simpler n=2 game.", "B": "The model predicts a ~71% success rate, which is close to the observed ~78% success rate, suggesting the simple error model is a good fit for the data.", "C": "The model predicts a ~29% success rate, which significantly under-predicts the observed ~78% success rate, suggesting players are far more rational than the model assumes.", "D": "The model predicts a ~50% success rate, which over-predicts the observed ~22% overall success rate for the XX case, suggesting the model fails to capture failures in later periods."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.0). This question uses the Computational Judgment strategy to test multi-step quantitative reasoning and interpretation. The correct answer (A) requires calculating an error rate, making an out-of-sample prediction, and correctly interpreting the discrepancy. Distractors are based on specific, plausible errors: (B) a calculation error (using `1-p_E` instead of `(1-p_E)^2`), (C) another calculation error (using `p_E` instead of `1-p_E`), and (D) a conceptual error (comparing the Period 1 prediction to the overall success rate).", "qid": "498", "question": "### Background\n\n**Research Question.** This problem analyzes experimental evidence from the Dirty Faces game to assess whether group behavior conforms to the theoretical equilibrium, particularly as the game's complexity (number of players `n`) and the required level of iterated reasoning (`k-1` steps) increase.\n\n**Setting.** An experiment is conducted on the Dirty Faces game with two different group sizes: `n=2` and `n=3`. At the start of each round, it is publicly announced that at least one player has type 'X'. In its analysis of the `n=2, XX` case, the paper notes that while only 4 of 18 groups conformed to the full predicted behavior, 14 of the 18 groups successfully completed Period 1 by playing `(UU)` and thus reached the second period.\n\n**Variables and Parameters.**\n- `n`: Number of players (2 or 3).\n- `k`: Number of players with type 'X'.\n- `Predicted Behavior`: The sequence of actions corresponding to the unique Nash equilibrium.\n\n### Data / Model Specification\n\nThe theoretical equilibrium requires `k-1` levels of iterated knowledge of rationality, where `k` is the number of players with type 'X'. The experimental results for group-level behavior are summarized in Table 1 (for `n=2`) and Table 2 (for `n=3`).\n\n**Table 1. Summary of Group Results for n=2**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XO | 8 | (DU) | 7 | 0.88 |\n| XX | 18 | ((UU)(DD)) | 4 | 0.22 |\n| **Total** | **26** | | **11** | **0.42** |\n\n**Table 2. Summary of Group Results for n=3**\n\n| Types | n (groups) | Predicted Behavior | Actual Behavior (n) | Actual Behavior (freq.) |\n| :--- | :---: | :---: | :---: | :---: |\n| XOO | 6 | (DUU) | 3 | 0.50 |\n| XXO | 10 | (UUU)(DDU) | 1 | 0.10 |\n| XXX | 12 | (UUU)(UUU)(DDD) | 0 | 0.00 |\n| **Total** | **28** | | **4** | **0.14** |\n\n### Question\n\nAn analyst proposes a simple error model where `p_E` is the probability a player who should play 'U' mistakenly plays 'D'. First, estimate `p_E` using the data for the `n=3, XOO` case from Table 2. Then, use this `p_E` to predict the success rate for Period 1 of the `n=2, XX` case. How does this prediction compare to the observed Period 1 success rate, and what does the comparison imply?", "model_evaluation_result": {"model_raw_response": "I need to analyze this step by step.\n\nFirst, let me estimate p_E using the n=3, XOO case:\n- In XOO case, predicted behavior is (DUU)\n- 3 out of 6 groups succeeded (0.50 success rate)\n- For success, the X player must play D correctly, and both O players must play U correctly\n- If p_E is the probability of error (playing D when should play U), then:\n  - Probability X player plays D correctly = 1 (since they should play D)\n  - Probability each O player plays U correctly = (1 - p_E)\n  - Success probability = 1 × (1 - p_E) × (1 - p_E) = (1 - p_E)²\n\nSetting this equal to observed success rate:\n(1 - p_E)² = 0.50\n1 - p_E = √0.50 ≈ 0.707\np_E ≈ 1 - 0.707 = 0.293\n\nNow, let me use this p_E to predict Period 1 success for n=2, XX case:\n- In XX case, Period 1 predicted behavior is (UU)\n- Both players should play U\n- Success probability = (1 - p_E) × (1 - p_E) = (1 - p_E)²\n- Success probability = (1 - 0.293)² = (0.707)² ≈ 0.50\n\nFrom the problem statement, 14 out of 18 groups (≈78%) successfully completed Period 1 in the XX case.\n\nComparing:\n- Model prediction: ~50%\n- Observed: ~78%\n- The model significantly under-predicts the observed success rate\n\nThis suggests that the error rate is not constant and is likely lower in the simpler n=2 game compared to the more complex n=3 game.\n\nA", "model_answer": "D", "error": null}}
{"ID": 336, "Question": "### Background\n\nAn infinite-horizon model features a logarithmic utility function, `u(c) = ln(c)`, and a strictly convex production function, `f(x) = x^2`, representing perpetual increasing returns to scale.\n\n### Data / Model Specification\n\nThe stationary optimal policy `g(x)` must satisfy the Euler equation:\n\n```latex\nu'[g(x)] = \\delta u'[g(H(x))] f'(x-g(x)) \\quad \\text{(Eq. 1)}\n```\n\nwhere `H(x) = f(x-g(x))` is the law of motion for capital and `\\delta` is the discount factor. A candidate solution for the policy is a linear function of the state, `g(x) = \\lambda x`.\n\n### Question\n\nFor what values of the discount factor `\\delta` does an economically meaningful interior solution of the form `g(x) = \\lambda x` (i.e., where `0 < \\lambda < 1`) exist?", "Options": {"A": "`\\delta < 1/2`", "B": "`\\delta \\ge 1/2`", "C": "An interior solution of this form exists for all `\\delta \\in (0, 1)`.", "D": "An interior solution of this form is never optimal due to the convexity of `f(x)`."}, "Answer": "A", "pi_justification": "This item assesses the ability to solve the model for its key parameter constraint. It uses the Computational Judgment strategy, requiring the student to solve the Euler equation for `\\lambda` and then derive the condition on `\\delta`. Distractor B is the conceptual opposite, representing the parameter region where the interior solution fails. Distractor C represents a failure to recognize the constraint imposed by the increasing returns. Distractor D offers a plausible but incorrect theoretical objection.", "qid": "336", "question": "### Background\n\nAn infinite-horizon model features a logarithmic utility function, `u(c) = ln(c)`, and a strictly convex production function, `f(x) = x^2`, representing perpetual increasing returns to scale.\n\n### Data / Model Specification\n\nThe stationary optimal policy `g(x)` must satisfy the Euler equation:\n\n```latex\nu'[g(x)] = \\delta u'[g(H(x))] f'(x-g(x)) \\quad \\text{(Eq. 1)}\n```\n\nwhere `H(x) = f(x-g(x))` is the law of motion for capital and `\\delta` is the discount factor. A candidate solution for the policy is a linear function of the state, `g(x) = \\lambda x`.\n\n### Question\n\nFor what values of the discount factor `\\delta` does an economically meaningful interior solution of the form `g(x) = \\lambda x` (i.e., where `0 < \\lambda < 1`) exist?", "model_evaluation_result": {"model_raw_response": "I need to analyze when a linear policy function g(x) = λx with 0 < λ < 1 satisfies the Euler equation in this infinite-horizon model.\n\nGiven:\n- Utility: u(c) = ln(c), so u'(c) = 1/c\n- Production: f(x) = x², so f'(x) = 2x\n- Policy: g(x) = λx (candidate solution)\n- Law of motion: H(x) = f(x - g(x)) = (x - λx)² = (1-λ)²x²\n\nSubstituting into the Euler equation:\nu'[g(x)] = δu'[g(H(x))]f'(x - g(x))\n\nLeft side: u'[λx] = 1/(λx)\n\nRight side: \n- g(H(x)) = λH(x) = λ(1-λ)²x²\n- u'[g(H(x))] = 1/[λ(1-λ)²x²]\n- f'(x - g(x)) = f'((1-λ)x) = 2(1-λ)x\n\nSo the Euler equation becomes:\n1/(λx) = δ · 1/[λ(1-λ)²x²] · 2(1-λ)x\n\nSimplifying:\n1/(λx) = δ · 2(1-λ)x/[λ(1-λ)²x²]\n1/(λx) = δ · 2/[λ(1-λ)x]\n1/(λx) = 2δ/[λ(1-λ)x]\n\nMultiplying both sides by λx:\n1 = 2δ/(1-λ)\n\nSolving for λ:\n1-λ = 2δ\nλ = 1-2δ\n\nFor an interior solution (0 < λ < 1):\n- λ > 0 requires: 1-2δ > 0, so δ < 1/2", "model_answer": "I need to analyze when a linear policy function g(x) = λx with 0 < λ < 1 satisfies the Euler equation in this infinite-horizon model.\n\nGiven:\n- Utility: u(c) = ln(c), so u'(c) = 1/c\n- Production: f(x) = x², so f'(x) = 2x\n- Policy: g(x) = λx (candidate solution)\n- Law of motion: H(x) = f(x - g(x)) = (x - λx)² = (1-λ)²x²\n\nSubstituting into the Euler equation:\nu'[g(x)] = δu'[g(H(x))]f'(x - g(x))\n\nLeft side: u'[λx] = 1/(λx)\n\nRight side: \n- g(H(x)) = λH(x) = λ(1-λ)²x²\n- u'[g(H(x))] = 1/[λ(1-λ)²x²]\n- f'(x - g(x)) = f'((1-λ)x) = 2(1-λ)x\n\nSo the Euler equation becomes:\n1/(λx) = δ · 1/[λ(1-λ)²x²] · 2(1-λ)x\n\nSimplifying:\n1/(λx) = δ · 2(1-λ)x/[λ(1-λ)²x²]\n1/(λx) = δ · 2/[λ(1-λ)x]\n1/(λx) = 2δ/[λ(1-λ)x]\n\nMultiplying both sides by λx:\n1 = 2δ/(1-λ)\n\nSolving for λ:\n1-λ = 2δ\nλ = 1-2δ\n\nFor an interior solution (0 < λ < 1):\n- λ > 0 requires: 1-2δ > 0, so δ < 1/2", "error": "Warning: Could not extract single choice letter from: I need to analyze when a linear policy function g(..."}}
{"ID": 476, "Question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. Defecting is a dominant strategy in the stage game, but cooperating (`$C_1, C_2$`) is the socially efficient outcome.\n\n### Data / Model Specification\n\nThe monitoring structure has the following properties:\n- Player 2's effort (`$C_2$` vs `$D_2$`) has a state-dependent effect. In state `$\\omega_1$`, effort is more productive than in state `$\\omega_2$`. \n- If Player 2 chooses to Defect (`$D_2$`), the public signal distribution is the same for both states, regardless of Player 1's action.\n\nConsider the case where **only Player 1 knows the state** (`$\\Theta_1 = \\{(\\omega_1), (\\omega_2)\\}$`), while Player 2 does not (`$\\Theta_2 = \\{(\\omega_1, \\omega_2)\\}$`).\n\n### Question\n\nIn this scenario, how can the uninformed Player 2 learn the true state, even if the informed Player 1 attempts to conceal it?", "Options": {"A": "By unilaterally choosing to Cooperate and observing whether her own effort is more or less productive from the resulting outcomes.", "B": "By observing Player 1's actions, which will perfectly reveal Player 1's private information.", "C": "Player 2, as the uninformed party, can never learn the state if Player 1 attempts to conceal it.", "D": "By threatening to Defect forever, which forces Player 1 to reveal the state."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses understanding of the asymmetric learning mechanic from the perspective of the uninformed player. It uses a Scenario Application strategy. The correct answer identifies the specific action the uninformed player can take to generate information. Distractors include a Conceptual Opposite (claiming learning is impossible), a common but incorrect assumption (that the informed player's actions are always revealing), and a True but Irrelevant point about equilibrium threats.", "qid": "476", "question": "### Background\n\n**Research Question.** This problem analyzes a two-player partnership game to illustrate how the initial allocation of private information critically determines whether efficient outcomes are attainable in equilibrium.\n\n**Setting.** Two players, 1 and 2, choose to Cooperate (`$C_i$`) or Defect (`$D_i$`). There are two states of the world, `$\\omega_1$` (high productivity) and `$\\omega_2$` (low productivity), which only affect the productivity of Player 2's effort. Defecting is a dominant strategy in the stage game, but cooperating (`$C_1, C_2$`) is the socially efficient outcome.\n\n### Data / Model Specification\n\nThe monitoring structure has the following properties:\n- Player 2's effort (`$C_2$` vs `$D_2$`) has a state-dependent effect. In state `$\\omega_1$`, effort is more productive than in state `$\\omega_2$`. \n- If Player 2 chooses to Defect (`$D_2$`), the public signal distribution is the same for both states, regardless of Player 1's action.\n\nConsider the case where **only Player 1 knows the state** (`$\\Theta_1 = \\{(\\omega_1), (\\omega_2)\\}$`), while Player 2 does not (`$\\Theta_2 = \\{(\\omega_1, \\omega_2)\\}$`).\n\n### Question\n\nIn this scenario, how can the uninformed Player 2 learn the true state, even if the informed Player 1 attempts to conceal it?", "model_evaluation_result": {"model_raw_response": "Looking at this partnership game setup, I need to analyze how Player 2 (uninformed) can learn the state when only Player 1 (informed) knows whether they're in state ω₁ or ω₂.\n\nKey facts from the problem:\n- Player 2's effort has state-dependent productivity (more productive in ω₁ than ω₂)\n- If Player 2 Defects, the public signal is the same across states regardless of Player 1's action\n- Player 1 knows the state, Player 2 doesn't\n\nLet me evaluate each option:\n\nA) By unilaterally choosing to Cooperate and observing whether her own effort is more or less productive from the resulting outcomes.\n\nThis makes sense because:\n- Player 2's effort (Cooperation) has different productivity in different states\n- By choosing to Cooperate, Player 2 can observe the outcomes\n- Since her effort is more productive in ω₁ than ω₂, she can infer the state from the productivity of her own effort\n- This learning mechanism doesn't depend on Player 1's cooperation or revelation\n\nB) Player 1's actions won't necessarily perfectly reveal the state, especially if Player 1 wants to conceal information.\n\nC) This is too strong - the problem setup suggests there are ways for learning to occur.\n\nD) Threats alone don't provide information about the underlying state - they're just strategic moves.\n\nOption A identifies the key learning mechanism: Player 2 can learn about the state by experimenting with her own effort level and observing the state-dependent productivity differences.\n\nA", "model_answer": "A", "error": null}}
{"ID": 257, "Question": "### Background\n\n**Research Question.** This problem analyzes the severe finite-sample distortions of estimators and test statistics in Markov-switching models, critiquing the reliability of inference based on standard asymptotic theory and evaluating proposed corrections.\n\n**Setting / Institutional Environment.** A Monte Carlo simulation is conducted to evaluate the properties of Maximum Likelihood Estimation (MLE) for a simple symmetric two-state switching model. One thousand samples of size `T=50` are generated from a known data generating process. The true parameters are `μ₁=2`, `μ₂=-2`, `p₁₁=p₂₂=0.8`, and `σ₁²=σ₂²=1`.\n\n**Variables & Parameters.**\n\n*   `λ`: A generic parameter of the model (e.g., `σ₁²`).\n*   `T`: Sample size, `T=50`.\n*   `m`: Number of estimated parameters in the model.\n*   `l`: Degrees of freedom of an asymptotic `χ²` distribution.\n*   `Avg. Actual S.E.`: The true sampling standard deviation of an estimator, calculated from the Monte Carlo replications.\n*   `Avg. Estimated S.E.`: The average of the standard errors computed for each sample using the outer-product-of-gradient (OPG) estimator.\n\n---\n\n### Data / Model Specification\n\nThe following table consolidates key results from the paper's Monte Carlo study for a sample size of `T=50`.\n\n**Table 1. Monte Carlo Results for MLE and Specification Tests (T=50)**\n\n| Parameter / Test | Statistic | Value |\n| :--- | :--- | :--- |\n| **Parameter Estimation** | | |\n| `σ₁²` | Avg. Actual S.E. | 0.40 |\n| `σ₁²` | Avg. Estimated S.E. | 0.26 |\n| **t-statistic Distribution** | | |\n| t-stat for `σ₁²` | 5th percentile | -2.90 |\n| t-stat for `σ₁²` | 95th percentile | 1.00 |\n| `N(0,1)` distribution | 5th percentile | -1.64 |\n| `N(0,1)` distribution | 95th percentile | 1.64 |\n| **Specification Test Size** | | |\n| NTW Test for ARCH | Actual Rejection Rate (α=5%) | 17% |\n\nThe paper proposes a small-sample correction for a Newey-Tauchen-White (NTW) test statistic, which is asymptotically `χ²(l)`. The corrected statistic is calculated as:\n\n```latex\n\\text{Corrected Statistic} = (\\text{Raw NTW Statistic}) \\times \\frac{T-m}{T \\cdot l} \\quad \\text{(Eq. 1)}\n```\n\nThis corrected statistic should be compared to an `F(l, T-m)` distribution.\n\n---\n\n### Question\n\nBased on the results for `σ₁²` in Table 1, the average estimated standard error understates the true sampling variability by approximately what percentage, and what is the primary consequence of this for hypothesis testing?", "Options": {"A": "35%; t-statistics are systematically inflated, leading to an excessive Type I error rate.", "B": "65%; t-statistics are systematically deflated, leading to a loss of power.", "C": "35%; t-statistics are systematically deflated, leading to a loss of power.", "D": "54%; confidence intervals are too wide, making it difficult to reject the null."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item uses the Computational Judgment strategy. It assesses the ability to perform a calculation (percentage understatement) and link it to the core statistical concept of Type I error inflation. Distractor B combines a common calculation error (reporting the ratio `0.26/0.40` instead of `1 - ratio`) with a conceptual opposite consequence. Distractor C has the correct calculation but the opposite consequence. Distractor D uses an incorrect calculation method (absolute difference) and wrong consequence.", "qid": "257", "question": "### Background\n\n**Research Question.** This problem analyzes the severe finite-sample distortions of estimators and test statistics in Markov-switching models, critiquing the reliability of inference based on standard asymptotic theory and evaluating proposed corrections.\n\n**Setting / Institutional Environment.** A Monte Carlo simulation is conducted to evaluate the properties of Maximum Likelihood Estimation (MLE) for a simple symmetric two-state switching model. One thousand samples of size `T=50` are generated from a known data generating process. The true parameters are `μ₁=2`, `μ₂=-2`, `p₁₁=p₂₂=0.8`, and `σ₁²=σ₂²=1`.\n\n**Variables & Parameters.**\n\n*   `λ`: A generic parameter of the model (e.g., `σ₁²`).\n*   `T`: Sample size, `T=50`.\n*   `m`: Number of estimated parameters in the model.\n*   `l`: Degrees of freedom of an asymptotic `χ²` distribution.\n*   `Avg. Actual S.E.`: The true sampling standard deviation of an estimator, calculated from the Monte Carlo replications.\n*   `Avg. Estimated S.E.`: The average of the standard errors computed for each sample using the outer-product-of-gradient (OPG) estimator.\n\n---\n\n### Data / Model Specification\n\nThe following table consolidates key results from the paper's Monte Carlo study for a sample size of `T=50`.\n\n**Table 1. Monte Carlo Results for MLE and Specification Tests (T=50)**\n\n| Parameter / Test | Statistic | Value |\n| :--- | :--- | :--- |\n| **Parameter Estimation** | | |\n| `σ₁²` | Avg. Actual S.E. | 0.40 |\n| `σ₁²` | Avg. Estimated S.E. | 0.26 |\n| **t-statistic Distribution** | | |\n| t-stat for `σ₁²` | 5th percentile | -2.90 |\n| t-stat for `σ₁²` | 95th percentile | 1.00 |\n| `N(0,1)` distribution | 5th percentile | -1.64 |\n| `N(0,1)` distribution | 95th percentile | 1.64 |\n| **Specification Test Size** | | |\n| NTW Test for ARCH | Actual Rejection Rate (α=5%) | 17% |\n\nThe paper proposes a small-sample correction for a Newey-Tauchen-White (NTW) test statistic, which is asymptotically `χ²(l)`. The corrected statistic is calculated as:\n\n```latex\n\\text{Corrected Statistic} = (\\text{Raw NTW Statistic}) \\times \\frac{T-m}{T \\cdot l} \\quad \\text{(Eq. 1)}\n```\n\nThis corrected statistic should be compared to an `F(l, T-m)` distribution.\n\n---\n\n### Question\n\nBased on the results for `σ₁²` in Table 1, the average estimated standard error understates the true sampling variability by approximately what percentage, and what is the primary consequence of this for hypothesis testing?", "model_evaluation_result": {"model_raw_response": "I need to analyze the relationship between the actual and estimated standard errors for σ₁² and determine the implications for hypothesis testing.\n\nFrom Table 1:\n- Avg. Actual S.E. for σ₁² = 0.40\n- Avg. Estimated S.E. for σ₁² = 0.26\n\nFirst, let me calculate by what percentage the estimated standard error understates the true sampling variability:\n\nPercentage understatement = (Actual S.E. - Estimated S.E.) / Actual S.E. × 100%\n= (0.40 - 0.26) / 0.40 × 100%\n= 0.14 / 0.40 × 100%\n= 35%\n\nSo the estimated standard error understates the true variability by 35%.\n\nNow, let me analyze the consequence for hypothesis testing:\n\nWhen the estimated standard error is smaller than the true standard error, the t-statistic will be:\nt = (estimate - null value) / estimated S.E.\n\nSince the denominator (estimated S.E.) is too small, the t-statistic will be systematically inflated (larger in absolute value than it should be).\n\nThis is confirmed by the t-statistic distribution data in the table:\n- The 5th percentile is -2.90 (more extreme than the standard normal -1.64)\n- The 95th percentile is 1.00 (less extreme than the standard normal 1.64, but this asymmetry suggests systematic bias)\n\nWhen t-statistics are systematically inflated, researchers will reject the null hypothesis more often than they should, leading to an excessive Type I error rate (rejecting true null hypotheses too frequently).\n\nA", "model_answer": "A", "error": null}}
{"ID": 59, "Question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nAssume the foreign country is more patient than the home country (`δ* > δ`). The home government announces a future temporary fiscal *cut* (`dG_s < 0`). This announcement has a primary effect of raising home wealth and lowering foreign wealth. What is the secondary effect of this wealth redistribution on the interest rate `r_{t-1}` for a period `t` far from `s`?", "Options": {"A": "The interest rate `r_{t-1}` will increase, because wealth is transferred from a high-saving country to a low-saving country, creating an incipient shortage of world savings.", "B": "The interest rate `r_{t-1}` will decrease, because the fiscal cut signals future abundance, which lowers the return to saving.", "C": "The interest rate `r_{t-1}` will decrease, because wealth is transferred from a high-saving country to a low-saving country, creating an incipient surplus of world savings.", "D": "The interest rate `r_{t-1}` will not change, as the shock is specific to period `s` and does not affect other periods."}, "Answer": "A", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This item assesses the subtle secondary effect of a future policy announcement on the term structure of interest rates, using a Scenario Application strategy. The correct answer (A) requires a two-step reasoning process: identify the direction of wealth transfer and then determine its impact on world savings given the saving propensities. Distractor (B) offers a plausible but incorrect narrative. Distractor (C) makes a sign error in the second step of the reasoning. Distractor (D) fails to recognize the full general equilibrium effects in an integrated capital market.", "qid": "59", "question": "### Background\nThis question concerns a two-country, one-good world economy with a fully integrated capital market. In this model, rational agents adjust their consumption based on their total wealth, which depends on the entire future path of income and interest rates. We analyze the international transmission of a *temporary* fiscal policy originating in the home country.\n\n### Data / Model Specification\n*   **Consumption & Wealth:** Private consumption in each country is proportional to private wealth. Wealth is the present value of the stream of output net of taxes.\n*   **Saving:** The home and foreign marginal propensities to save out of wealth are denoted by `δ` and `δ*`, respectively.\n*   **Fiscal Policy:** A temporary fiscal expansion in the home country at time `k` is an increase in government spending, `dG_k > 0`.\n\n### Question\nAssume the foreign country is more patient than the home country (`δ* > δ`). The home government announces a future temporary fiscal *cut* (`dG_s < 0`). This announcement has a primary effect of raising home wealth and lowering foreign wealth. What is the secondary effect of this wealth redistribution on the interest rate `r_{t-1}` for a period `t` far from `s`?", "model_evaluation_result": {"model_raw_response": "I need to analyze the secondary effect of wealth redistribution on the interest rate when the home government announces a future temporary fiscal cut.\n\nGiven information:\n- Foreign country is more patient: δ* > δ (foreign has higher marginal propensity to save)\n- Home government announces future fiscal cut: dG_s < 0\n- Primary effect: raises home wealth, lowers foreign wealth\n- Question asks about secondary effect on interest rate r_{t-1} for period t far from s\n\nLet me work through the logic:\n\n1) The announcement of a future fiscal cut (dG_s < 0) increases home wealth and decreases foreign wealth through present value effects.\n\n2) With the wealth redistribution:\n   - Home country (lower saving propensity δ) gains wealth\n   - Foreign country (higher saving propensity δ*) loses wealth\n\n3) Since δ* > δ, the foreign country saves a higher fraction of any given wealth level than the home country.\n\n4) When wealth is transferred from the high-saving foreign country to the low-saving home country:\n   - Foreign country reduces its saving by δ* × (wealth loss)\n   - Home country increases its saving by δ × (wealth gain)\n   - Since δ* > δ, the net effect is a reduction in world saving\n\n5) A reduction in world saving creates an incipient shortage of savings relative to investment demand.\n\n6) This shortage puts upward pressure on the interest rate to equilibrate the capital market.\n\nThe answer is A) The interest rate r_{t-1} will increase, because wealth is transferred from a high-saving country to a low-saving country, creating an incipient shortage of world savings.\n\nA", "model_answer": "A", "error": null}}
{"ID": 273, "Question": "### Background\n\n**Research Question.** This problem examines the optimal marginal trade-off for an ambiguity-averse agent in a complete markets equilibrium, leading to the derivation and interpretation of the economy's stochastic discount factor (SDF).\n\n**Setting.** An agent `i` with smooth ambiguity preferences optimizes their consumption plan in a dynamic, complete markets economy. We analyze the first-order condition that governs their optimal choice, assuming an interior, no-default equilibrium.\n\n**Variables and Parameters.**\n- `q_t(ω^t)`: Price at time 0 of one unit of consumption delivered at time `t` if state `ω^t` occurs.\n- `β`: Discount factor.\n- `c_t^i(ω^t)`: Agent `i`'s consumption at `(t, ω^t)`.\n- `u_i'(·)`: Agent `i`'s marginal utility of consumption.\n- `c^i`: The agent's entire consumption plan.\n- `U_i(c^i)`: The total discounted utility of the plan under a given model, `∑_τ β^τ u_i(c_τ^i)`.\n- `P(ω^t)`: Probability of state `ω^t` under a first-order model `P`.\n- `φ_i'(·)`: The derivative of agent `i`'s ambiguity attitude function.\n- `μ^i`: Agent `i`'s second-order belief over models.\n- `E_{μ^i}[·]`: Expectation with respect to the second-order belief.\n- `λ(i)`: Agent `i`'s Lagrange multiplier on their budget constraint.\n\n---\n\n### Data / Model Specification\n\nAn agent `i`'s stochastic discount factor (SDF), `m_t^i(ω^t)`, is given by:\n```latex\nm_t^i(\\omega^t) = \\beta^{t} \\cdot u_{i}^{\\prime}(c_{t}^{i}(\\omega^{t})) \\cdot \\mathbb{E}_{\\mu^{i}}[\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})]\n```\nThe equilibrium price of a claim on state `(t, ω^t)` is proportional to this SDF.\n\n---\n\n### Question\n\nConsider two future states, `A` and `B`, at the same time `t`. Agent `i`'s consumption is the same in both states, `c_t^i(A) = c_t^i(B)`. The agent's reduced belief assigns equal probability to both states: `ℙ^i(A) = ℙ^i(B)`. However, the agent perceives more ambiguity about state `A`: the probability `P(A)` varies significantly across the models `P` in the support of `μ^i`, while `P(B)` is nearly constant across all models. How will the equilibrium price of a claim on state `A`, `q_A`, compare to the price of a claim on state `B`, `q_B`?", "Options": {"A": "`q_A < q_B`", "B": "`q_A > q_B`", "C": "`q_A = q_B`", "D": "The relationship cannot be determined without knowing the agent's risk aversion."}, "Answer": "A", "pi_justification": "This is a Scenario Application question that tests a deep implication of the smooth ambiguity model. The correct answer requires understanding that ambiguity is a source of disutility, so an asset with more ambiguous payoffs is less desirable, leading to a lower price (or higher expected return). The distractors are designed to trap common misconceptions: (B) is the conceptual opposite, arising from incorrectly thinking ambiguity increases the value of insurance; (C) is a step-omission error, where a student looks only at the equal reduced beliefs (`ℙ^i`) and ignores the full ambiguity term in the SDF; (D) conflates the distinct concepts of risk aversion and ambiguity aversion.", "qid": "273", "question": "### Background\n\n**Research Question.** This problem examines the optimal marginal trade-off for an ambiguity-averse agent in a complete markets equilibrium, leading to the derivation and interpretation of the economy's stochastic discount factor (SDF).\n\n**Setting.** An agent `i` with smooth ambiguity preferences optimizes their consumption plan in a dynamic, complete markets economy. We analyze the first-order condition that governs their optimal choice, assuming an interior, no-default equilibrium.\n\n**Variables and Parameters.**\n- `q_t(ω^t)`: Price at time 0 of one unit of consumption delivered at time `t` if state `ω^t` occurs.\n- `β`: Discount factor.\n- `c_t^i(ω^t)`: Agent `i`'s consumption at `(t, ω^t)`.\n- `u_i'(·)`: Agent `i`'s marginal utility of consumption.\n- `c^i`: The agent's entire consumption plan.\n- `U_i(c^i)`: The total discounted utility of the plan under a given model, `∑_τ β^τ u_i(c_τ^i)`.\n- `P(ω^t)`: Probability of state `ω^t` under a first-order model `P`.\n- `φ_i'(·)`: The derivative of agent `i`'s ambiguity attitude function.\n- `μ^i`: Agent `i`'s second-order belief over models.\n- `E_{μ^i}[·]`: Expectation with respect to the second-order belief.\n- `λ(i)`: Agent `i`'s Lagrange multiplier on their budget constraint.\n\n---\n\n### Data / Model Specification\n\nAn agent `i`'s stochastic discount factor (SDF), `m_t^i(ω^t)`, is given by:\n```latex\nm_t^i(\\omega^t) = \\beta^{t} \\cdot u_{i}^{\\prime}(c_{t}^{i}(\\omega^{t})) \\cdot \\mathbb{E}_{\\mu^{i}}[\\phi_{i}^{\\prime}(\\mathbb{E}_{P}[U_{i}(c^{i})])P(\\omega^{t})]\n```\nThe equilibrium price of a claim on state `(t, ω^t)` is proportional to this SDF.\n\n---\n\n### Question\n\nConsider two future states, `A` and `B`, at the same time `t`. Agent `i`'s consumption is the same in both states, `c_t^i(A) = c_t^i(B)`. The agent's reduced belief assigns equal probability to both states: `ℙ^i(A) = ℙ^i(B)`. However, the agent perceives more ambiguity about state `A`: the probability `P(A)` varies significantly across the models `P` in the support of `μ^i`, while `P(B)` is nearly constant across all models. How will the equilibrium price of a claim on state `A`, `q_A`, compare to the price of a claim on state `B`, `q_B`?", "model_evaluation_result": {"model_raw_response": "I need to analyze how ambiguity affects the stochastic discount factor and thus the equilibrium prices.\n\nGiven information:\n- Same consumption in both states: c_t^i(A) = c_t^i(B)\n- Equal reduced probabilities: ℙ^i(A) = ℙ^i(B)\n- More ambiguity about state A: P(A) varies significantly across models, while P(B) is nearly constant\n\nThe SDF is:\nm_t^i(ω^t) = β^t · u_i'(c_t^i(ω^t)) · E_μ^i[φ_i'(E_P[U_i(c^i)])P(ω^t)]\n\nSince consumption is the same in both states, u_i'(c_t^i(A)) = u_i'(c_t^i(B)).\n\nThe key difference lies in the term E_μ^i[φ_i'(E_P[U_i(c^i)])P(ω^t)].\n\nFor ambiguity-averse agents, φ_i' is typically decreasing (concave φ_i function), meaning φ_i'(x) is lower when x is higher.\n\nWhen there's more ambiguity about state A:\n- P(A) varies significantly across models in the support of μ^i\n- This creates correlation between φ_i'(E_P[U_i(c^i)]) and P(A)\n- When P(A) is high in a model, that model likely gives higher expected utility E_P[U_i(c^i)]\n- But higher expected utility means lower φ_i'(E_P[U_i(c^i)]) due to ambiguity aversion\n- This negative correlation reduces E_μ^i[φ_i'(E_P[U_i(c^i)])P(A)]\n\nFor state B with little ambiguity:\n- P(B) is nearly constant across models\n- No significant correlation between φ_i'(E_P[U_i(c^i)]) and P(B)\n-", "model_answer": "A", "error": null}}
{"ID": 386, "Question": "### Background\n\n**Research Question:** This problem addresses the practical challenges of estimating PELVE from serially dependent data, such as financial time series, and understanding the statistical properties of the estimator.\n\n**Setting / Institutional Environment:** The analysis is based on a sample from a strictly stationary and `α`-mixing sequence of losses. This data structure is more realistic for financial returns than the i.i.d. assumption, as it allows for phenomena like volatility clustering.\n\n**Variables & Parameters:**\n- `ĉ_n`: The empirical PELVE estimator.\n- `W(·)`: A zero-mean Gaussian process.\n\n### Data / Model Specification\n\nFor a stationary `α`-mixing sequence, `sqrt(n)(ĉ_n - c)` converges in distribution to a normal random variable whose variance depends on the limiting Gaussian process `W`. The covariance of this process is given by:\n```latex\n\\operatorname{Cov}(W(t),W(s)) = \\sum_{i \\in \\mathbb{Z}} \\left( \\mathbb{P}(\\{F(X_{1}) \\le s\\} \\cap \\{F(X_{1+i}) \\le t\\}) - ts \\right)\n```\n(Eq. 1)\n\nFor i.i.d. data, the sum only includes the `i=0` term, simplifying to `min(t,s) - ts`.\n\n### Question\n\nFinancial loss series often exhibit volatility clustering (i.e., large losses tend to follow large losses), a form of `α`-mixing dependence. According to the asymptotic theory presented, how does this feature affect statistical inference for the PELVE estimator `ĉ_n` compared to a hypothetical i.i.d. scenario with the same marginal distribution?", "Options": {"A": "It decreases the true asymptotic variance, causing confidence intervals constructed under a naive i.i.d. assumption to be overly conservative (too wide).", "B": "It increases the true asymptotic variance, causing confidence intervals constructed under a naive i.i.d. assumption to be misleadingly precise (too narrow).", "C": "It has no effect on the true asymptotic variance, but it introduces a bias in the point estimate `ĉ_n`.", "D": "It violates the stationarity assumption required for the theory, making the asymptotic variance undefined."}, "Answer": "B", "pi_justification": "REPLACE with Choice Questions (Score: 9.5). This Single Choice item uses a Scenario Application strategy to test the 'Mathematical Apex' part of the original QA. It assesses the user's ability to connect a real-world data feature (volatility clustering) to its theoretical consequence within the paper's framework. The options are designed as a matrix of cause and effect. Option B is correct. Option A is the conceptual opposite. Option C confuses variance with bias. Option D introduces a plausible but incorrect technical objection.", "qid": "386", "question": "### Background\n\n**Research Question:** This problem addresses the practical challenges of estimating PELVE from serially dependent data, such as financial time series, and understanding the statistical properties of the estimator.\n\n**Setting / Institutional Environment:** The analysis is based on a sample from a strictly stationary and `α`-mixing sequence of losses. This data structure is more realistic for financial returns than the i.i.d. assumption, as it allows for phenomena like volatility clustering.\n\n**Variables & Parameters:**\n- `ĉ_n`: The empirical PELVE estimator.\n- `W(·)`: A zero-mean Gaussian process.\n\n### Data / Model Specification\n\nFor a stationary `α`-mixing sequence, `sqrt(n)(ĉ_n - c)` converges in distribution to a normal random variable whose variance depends on the limiting Gaussian process `W`. The covariance of this process is given by:\n```latex\n\\operatorname{Cov}(W(t),W(s)) = \\sum_{i \\in \\mathbb{Z}} \\left( \\mathbb{P}(\\{F(X_{1}) \\le s\\} \\cap \\{F(X_{1+i}) \\le t\\}) - ts \\right)\n```\n(Eq. 1)\n\nFor i.i.d. data, the sum only includes the `i=0` term, simplifying to `min(t,s) - ts`.\n\n### Question\n\nFinancial loss series often exhibit volatility clustering (i.e., large losses tend to follow large losses), a form of `α`-mixing dependence. According to the asymptotic theory presented, how does this feature affect statistical inference for the PELVE estimator `ĉ_n` compared to a hypothetical i.i.d. scenario with the same marginal distribution?", "model_evaluation_result": {"model_raw_response": "Looking at this problem, I need to understand how α-mixing dependence (specifically volatility clustering) affects the asymptotic variance of the PELVE estimator compared to the i.i.d. case.\n\nFrom Equation 1, the covariance of the limiting Gaussian process W is:\n```\nCov(W(t),W(s)) = Σ_{i∈ℤ} [P({F(X₁) ≤ s} ∩ {F(X₁₊ᵢ) ≤ t}) - ts]\n```\n\nFor i.i.d. data, this sum reduces to only the i=0 term, giving `min(t,s) - ts`.\n\nFor α-mixing data with volatility clustering, the sum includes additional terms for i ≠ 0. Volatility clustering means that large losses tend to follow large losses, creating positive dependence between observations. This means:\n\n- When both X₁ and X₁₊ᵢ tend to be large (or small) together\n- P({F(X₁) ≤ s} ∩ {F(X₁₊ᵢ) ≤ t}) will generally be larger than the independent case ts\n- Therefore, the terms [P({F(X₁) ≤ s} ∩ {F(X₁₊ᵢ) ≤ t}) - ts] for i ≠ 0 will be positive\n- Adding these positive terms to the i=0 term increases the overall covariance\n\nThis increased covariance in the limiting Gaussian process W translates to increased asymptotic variance for the estimator ĉₙ.\n\nIf one naively assumes i.i.d. data when constructing confidence intervals, they would underestimate the true asymptotic variance, leading to confidence intervals that are too narrow (misleadingly precise).\n\nThe answer is B.", "model_answer": "B", "error": null}}
{"ID": 292, "Question": "### Background\n\nIndividuals are assumed to model inflation as a first-order autoregressive, AR(1), process. They recursively estimate the parameters of this model based on the inflation rates they have experienced during their lifetimes.\n\n### Data / Model Specification\n\nThe perceived law of motion for inflation is an AR(1) process:\n\n```latex\n\\pi_{t+1} = \\alpha + \\phi \\pi_{t} + \\eta_{t+1} \\quad \\text{(Eq. 1)}\n```\n\n### Question\n\nAn individual perceives inflation according to the AR(1) process in Eq. (1). At time `t`, their parameter estimates are `\\alpha_t = 0.01` and `\\phi_t = 0.6`. What is their perceived long-run mean of inflation, `\\mu_t`, and what is the correct interpretation of `\\phi_t = 0.6`?", "Options": {"A": "The perceived long-run mean is 2.5%, and 60% of any deviation from this mean is expected to persist into the next period.", "B": "The perceived long-run mean is 0.625%, and it represents the intercept of the process.", "C": "The perceived long-run mean is 1.0%, and 60% of the current inflation rate is expected to be the next period's inflation rate.", "D": "The perceived long-run mean is 2.5%, and inflation is expected to grow by 60% each period."}, "Answer": "A", "pi_justification": "This item tests both the calculation of the long-run mean from an AR(1) process and the conceptual interpretation of the persistence parameter. It uses a Computational Judgment strategy. Distractors are based on specific, plausible errors: B uses the wrong formula for the mean (`1+\\phi` in denominator); C omits the denominator entirely; D has the correct calculation but a common, incorrect interpretation of the persistence parameter `\\phi`.", "qid": "292", "question": "### Background\n\nIndividuals are assumed to model inflation as a first-order autoregressive, AR(1), process. They recursively estimate the parameters of this model based on the inflation rates they have experienced during their lifetimes.\n\n### Data / Model Specification\n\nThe perceived law of motion for inflation is an AR(1) process:\n\n```latex\n\\pi_{t+1} = \\alpha + \\phi \\pi_{t} + \\eta_{t+1} \\quad \\text{(Eq. 1)}\n```\n\n### Question\n\nAn individual perceives inflation according to the AR(1) process in Eq. (1). At time `t`, their parameter estimates are `\\alpha_t = 0.01` and `\\phi_t = 0.6`. What is their perceived long-run mean of inflation, `\\mu_t`, and what is the correct interpretation of `\\phi_t = 0.6`?", "model_evaluation_result": {"model_raw_response": "I need to analyze the AR(1) process and calculate the long-run mean and interpret the autoregressive parameter.\n\nFor an AR(1) process: π_{t+1} = α + φπ_t + η_{t+1}\n\nGiven parameters:\n- α_t = 0.01\n- φ_t = 0.6\n\n**Step 1: Calculate the long-run mean (μ_t)**\n\nFor an AR(1) process, the long-run mean is found by setting π_{t+1} = π_t = μ in the steady state:\n\nμ = α + φμ + 0 (since E[η] = 0)\nμ = α + φμ\nμ - φμ = α\nμ(1 - φ) = α\nμ = α/(1 - φ)\n\nSubstituting the values:\nμ_t = 0.01/(1 - 0.6) = 0.01/0.4 = 0.025 = 2.5%\n\n**Step 2: Interpret φ_t = 0.6**\n\nThe autoregressive parameter φ represents the persistence of deviations from the long-run mean. \n\nIf we rewrite the AR(1) in deviation form:\n(π_t - μ) = φ(π_{t-1} - μ) + η_t\n\nThis shows that φ = 0.6 means that 60% of any deviation from the long-run mean persists into the next period.\n\n**Checking the options:**\n\nA) Long-run mean = 2.5% ✓, and 60% of deviation from mean persists ✓\nB) Long-run mean = 0.625% ✗ (incorrect calculation)\nC) Long-run mean = 1.0% ✗ (incorrect), and misinterprets φ\nD) Long-run mean = 2.5% ✓, but misinterprets φ as growth rate ✗\n\nA", "model_answer": "A", "error": null}}
{"ID": 163, "Question": "### Background\n\nIn Expected Uncertain Utility (EUU) theory, an agent's preferences `≿` over acts `f` (functions from a state space `Ω` to prizes) are represented by a functional `W(f)`. The theory distinguishes between \"ideal\" events, over which uncertainty is quantifiable via a prior `μ`, and non-ideal events.\n\nThe utility of any act `f` is determined by its **envelope**, `[f] = ([f]₁, [f]₂)`. The envelope is the tightest pair of **ideal acts** that bound `f` from below and above. The utility is then calculated as:\n\n```latex\nW(f) = \\int u([f]_1, [f]_2) d\\mu\n```\n\nwhere `u` is the agent's interval utility.\n\n### Data / Model Specification\n\nConsider a state space `Ω` which is the unit square, `[0,1] × [0,1]`. \n-   **Ideal events** are defined as all vertical rectangles of the form `[a,b] × [0,1]`. An ideal act is any act measurable with respect to these events (i.e., an act that is constant on any vertical line).\n-   The prior `μ` on ideal events is the standard length measure, e.g., `μ([a,b] × [0,1]) = b-a`.\n\nConsider a non-ideal act `f` that pays prize `y` on the event `A = {(ω₁, ω₂) | ω₂ > 0.5}` (the top half of the square) and prize `x` on its complement `A^c` (the bottom half), where `y > x`.\n\n### Question\n\nWhat is the utility `W(f)` of this act?", "Options": {"A": "`0.5 * u(y,y) + 0.5 * u(x,x)`", "B": "`u(x,y)`", "C": "`∫ u(f(ω), f(ω)) dμ`", "D": "The utility is undefined because the act `f` is not ideal."}, "Answer": "B", "pi_justification": "This question assesses the core technical concept of the 'envelope'. Strategy: Scenario Application. The correct answer requires realizing that for any ideal event (a vertical strip), the act `f` takes on both `x` and `y` values. Therefore, the tightest ideal lower bound `[f]₁` is the constant act `x`, and the tightest ideal upper bound `[f]₂` is the constant act `y`. The utility is thus `∫ u(x,y) dμ = u(x,y)`. Distractor A represents the incorrect Subjective Expected Utility calculation, treating the non-ideal event `A` as if it were ideal. Distractor C incorrectly equates the envelope with the act itself. Distractor D targets the misconception that utility is only defined for ideal acts.", "qid": "163", "question": "### Background\n\nIn Expected Uncertain Utility (EUU) theory, an agent's preferences `≿` over acts `f` (functions from a state space `Ω` to prizes) are represented by a functional `W(f)`. The theory distinguishes between \"ideal\" events, over which uncertainty is quantifiable via a prior `μ`, and non-ideal events.\n\nThe utility of any act `f` is determined by its **envelope**, `[f] = ([f]₁, [f]₂)`. The envelope is the tightest pair of **ideal acts** that bound `f` from below and above. The utility is then calculated as:\n\n```latex\nW(f) = \\int u([f]_1, [f]_2) d\\mu\n```\n\nwhere `u` is the agent's interval utility.\n\n### Data / Model Specification\n\nConsider a state space `Ω` which is the unit square, `[0,1] × [0,1]`. \n-   **Ideal events** are defined as all vertical rectangles of the form `[a,b] × [0,1]`. An ideal act is any act measurable with respect to these events (i.e., an act that is constant on any vertical line).\n-   The prior `μ` on ideal events is the standard length measure, e.g., `μ([a,b] × [0,1]) = b-a`.\n\nConsider a non-ideal act `f` that pays prize `y` on the event `A = {(ω₁, ω₂) | ω₂ > 0.5}` (the top half of the square) and prize `x` on its complement `A^c` (the bottom half), where `y > x`.\n\n### Question\n\nWhat is the utility `W(f)` of this act?", "model_evaluation_result": {"model_raw_response": "I need to analyze this Expected Uncertain Utility (EUU) problem step by step.\n\nFirst, let me understand the setup:\n- State space Ω = [0,1] × [0,1] (unit square)\n- Ideal events are vertical rectangles [a,b] × [0,1]\n- Prior μ is standard length measure on ideal events\n- Act f pays y on A = {(ω₁, ω₂) | ω₂ > 0.5} (top half) and x on A^c (bottom half), where y > x\n\nThe key insight is that f is NOT an ideal act because it depends on ω₂ (the vertical coordinate), while ideal acts must be constant on vertical lines.\n\nTo find W(f), I need to determine the envelope [f] = ([f]₁, [f]₂), which consists of the tightest ideal acts that bound f from below and above.\n\nFor any vertical line at coordinate ω₁:\n- The act f takes values x and y on this line (x in bottom half, y in top half)\n- The lower bound ideal act [f]₁ must be constant on each vertical line and ≤ f everywhere\n- The upper bound ideal act [f]₂ must be constant on each vertical line and ≥ f everywhere\n\nSince f takes both values x and y on every vertical line:\n- [f]₁(ω₁, ω₂) = x for all (ω₁, ω₂) (the minimum value f takes on any vertical line)\n- [f]₂(ω₁, ω₂) = y for all (ω₁, ω₂) (the maximum value f takes on any vertical line)\n\nNow I can calculate W(f):\nW(f) = ∫ u([f]₁, [f]₂) dμ = ∫ u(x, y) dμ\n\nSince [f]₁ and [f]₂ are constant across all ideal events, and μ is a probability measure on the ideal events with total measure 1:\n\nW(f) = u(x, y)\n\nThe answer", "model_answer": "A", "error": null}}
{"ID": 130, "Question": "### Background\n\n**Research Question.** This problem examines the foundational elements of the paper's model, specifically the economic trade-offs faced by a cost-minimizing public agency.\n\n**Setting / Institutional Environment.** A public agency hires a single worker type `i` to produce a total quantity `Q`. It minimizes total cost `Z = w_i n_i` subject to a production constraint `n_i e_i = Q` and a participation constraint for the workers.\n\n---\n\n### Data / Model Specification\n\nThe agency's cost-minimization problem yields the following first-order condition for optimal effort `e_i`:\n```latex\n\\left[\\theta_{i}C'(e_{i})-\\gamma_{i}V'(e_{i})\\right]-\\left[{\\frac{U_{i}^{*}+\\theta_{i}C(e_{i})-\\gamma_{i}V(e_{i})}{e_{i}}}\\right]=0 \\quad \\text{(Eq. 1)}\n```\n\n---\n\n### The Question\n\nWhat is the correct economic interpretation of the two main terms in the first-order condition shown in Eq. (1)?", "Options": {"A": "The first term is the marginal cost of increasing output on the intensive margin (more effort from current workers), while the second term is the marginal cost on the extensive margin (hiring a new worker).", "B": "The first term is the marginal cost on the extensive margin, while the second term is the marginal cost on the intensive margin.", "C": "The first term represents the worker's marginal disutility of effort, and the second term represents the average wage cost.", "D": "The first term is the agency's marginal profit from an additional unit of effort, while the second term is the total cost of production."}, "Answer": "A", "pi_justification": "This item tests the student's ability to interpret the economic logic of an optimization condition, a core skill in microeconomic theory. It uses a Reverse-Reasoning strategy. The correct answer correctly identifies the intensive and extensive margins of cost. Distractor B is a classic conceptual reversal. Distractor C provides a technically correct but incomplete description that misses the key economic trade-off. Distractor D misinterprets the agency's objective function and the nature of the cost terms.", "qid": "130", "question": "### Background\n\n**Research Question.** This problem examines the foundational elements of the paper's model, specifically the economic trade-offs faced by a cost-minimizing public agency.\n\n**Setting / Institutional Environment.** A public agency hires a single worker type `i` to produce a total quantity `Q`. It minimizes total cost `Z = w_i n_i` subject to a production constraint `n_i e_i = Q` and a participation constraint for the workers.\n\n---\n\n### Data / Model Specification\n\nThe agency's cost-minimization problem yields the following first-order condition for optimal effort `e_i`:\n```latex\n\\left[\\theta_{i}C'(e_{i})-\\gamma_{i}V'(e_{i})\\right]-\\left[{\\frac{U_{i}^{*}+\\theta_{i}C(e_{i})-\\gamma_{i}V(e_{i})}{e_{i}}}\\right]=0 \\quad \\text{(Eq. 1)}\n```\n\n---\n\n### The Question\n\nWhat is the correct economic interpretation of the two main terms in the first-order condition shown in Eq. (1)?", "model_evaluation_result": {"model_raw_response": "Looking at this first-order condition from the agency's cost-minimization problem, I need to interpret the economic meaning of each term.\n\nThe first-order condition is:\n```\n[θᵢC'(eᵢ) - γᵢV'(eᵢ)] - [Uᵢ* + θᵢC(eᵢ) - γᵢV(eᵢ)]/eᵢ = 0\n```\n\nLet me analyze each term:\n\n**First term: [θᵢC'(eᵢ) - γᵢV'(eᵢ)]**\nThis represents the marginal change in compensation needed when effort increases slightly. It's the marginal cost of getting existing workers to exert more effort - this is the **intensive margin**.\n\n**Second term: [Uᵢ* + θᵢC(eᵢ) - γᵢV(eᵢ)]/eᵢ**\nThe numerator (Uᵢ* + θᵢC(eᵢ) - γᵢV(eᵢ)) represents the total compensation per worker, and dividing by eᵢ gives the compensation cost per unit of effort. This represents the cost of hiring an additional worker to get more total effort - this is the **extensive margin**.\n\nIn cost minimization, the agency equates the marginal cost of increasing output through the intensive margin (more effort from current workers) with the marginal cost through the extensive margin (hiring additional workers).\n\nLooking at the options, option A correctly identifies that the first term is the marginal cost on the intensive margin and the second term is the marginal cost on the extensive margin.\n\nA", "model_answer": "A", "error": null}}
{"ID": 398, "Question": "### Background\n\nHow can locality priorities be respected in a multidimensional matching market where classical stability may not exist? The paper introduces a novel solution concept, interference-freeness, and a mechanism called Knapsack Deferred Acceptance (KDA) to find a family-optimal interference-free matching. However, KDA is not strategy-proof.\n\n---\n\n### Data / Model Specification\n\nConsider the following setup based on the paper's Example 1:\n- **Families & Sizes:** `f1`(size 1), `f2`(size 2), `f3`(size 1), `f4`(size 1)\n- **Localities & Capacities:** `ℓ1`(cap 2), `ℓ2`(cap 1), `ℓ3`(cap 2), `ℓ4`(cap 5)\n- **True Preferences of `f2`:** `ℓ1, ℓ3, ℓ4, ...`\n- **Priorities at `ℓ1`:** `f1 triangleright f2 triangleright f3 triangleright f4`\n- **Priorities at `ℓ2`:** `f3 triangleright f1 ...`\n- **Priorities at `ℓ3`:** `f4 triangleright f2 ...`\n\nUnder truthful reporting, the KDA algorithm matches `f2` to `ℓ4`.\n\n---\n\n### Question\n\nSuppose family `f2` misreports its preferences as `≻'_{f2}: ℓ3, ℓ1, ℓ4, ...` while all other families report their true preferences (`f1: ℓ2,ℓ1..`, `f3: ℓ1,ℓ2..`, `f4: ℓ1,ℓ3..`). If the Knapsack Deferred Acceptance (KDA) algorithm is run, what is the final matching for family `f2`?", "Options": {"A": "`ℓ1`", "B": "`ℓ4`", "C": "`ℓ3`", "D": "Unmatched"}, "Answer": "C", "pi_justification": "This item assesses the procedural understanding of the KDA algorithm. It uses a Computational Judgment strategy, requiring the user to trace the algorithm's steps to determine the outcome of a strategic manipulation. The correct answer is the outcome of the successful manipulation. Distractors are designed based on common errors: (B) is the outcome with truthful preferences, (A) is a desirable but unattainable outcome for the family, and (D) represents a plausible backfire scenario where the manipulation fails.", "qid": "398", "question": "### Background\n\nHow can locality priorities be respected in a multidimensional matching market where classical stability may not exist? The paper introduces a novel solution concept, interference-freeness, and a mechanism called Knapsack Deferred Acceptance (KDA) to find a family-optimal interference-free matching. However, KDA is not strategy-proof.\n\n---\n\n### Data / Model Specification\n\nConsider the following setup based on the paper's Example 1:\n- **Families & Sizes:** `f1`(size 1), `f2`(size 2), `f3`(size 1), `f4`(size 1)\n- **Localities & Capacities:** `ℓ1`(cap 2), `ℓ2`(cap 1), `ℓ3`(cap 2), `ℓ4`(cap 5)\n- **True Preferences of `f2`:** `ℓ1, ℓ3, ℓ4, ...`\n- **Priorities at `ℓ1`:** `f1 triangleright f2 triangleright f3 triangleright f4`\n- **Priorities at `ℓ2`:** `f3 triangleright f1 ...`\n- **Priorities at `ℓ3`:** `f4 triangleright f2 ...`\n\nUnder truthful reporting, the KDA algorithm matches `f2` to `ℓ4`.\n\n---\n\n### Question\n\nSuppose family `f2` misreports its preferences as `≻'_{f2}: ℓ3, ℓ1, ℓ4, ...` while all other families report their true preferences (`f1: ℓ2,ℓ1..`, `f3: ℓ1,ℓ2..`, `f4: ℓ1,ℓ3..`). If the Knapsack Deferred Acceptance (KDA) algorithm is run, what is the final matching for family `f2`?", "model_evaluation_result": {"model_raw_response": "I need to analyze how the KDA algorithm would work when family f2 misreports its preferences.\n\nGiven information:\n- f2 has size 2\n- f2's true preferences: ℓ1, ℓ3, ℓ4, ...\n- f2's reported preferences: ℓ3, ℓ1, ℓ4, ...\n- Other families report truthfully:\n  - f1: ℓ2, ℓ1, ...\n  - f3: ℓ1, ℓ2, ...\n  - f4: ℓ1, ℓ3, ...\n\nLocality capacities and priorities:\n- ℓ1 (cap 2): f1 ≻ f2 ≻ f3 ≻ f4\n- ℓ2 (cap 1): f3 ≻ f1 ...\n- ℓ3 (cap 2): f4 ≻ f2 ...\n- ℓ4 (cap 5): (priorities not fully specified but has large capacity)\n\nIn the KDA algorithm, families apply to localities in order of their reported preferences, and localities tentatively accept families based on their priority rankings and capacity constraints.\n\nWhen f2 misreports and puts ℓ3 first:\n\n1. f2 (size 2) will first apply to ℓ3\n2. At ℓ3 (cap 2), the priority is f4 ≻ f2\n3. If f4 also applies to ℓ3 (which it does, since f4's true preference includes ℓ3), f4 has higher priority\n4. However, f4 has size 1, so there's still capacity for f2 (size 2) at ℓ3 if the total doesn't exceed capacity 2\n5. Since f4 (size 1) + f2 (size 2) = 3 > 2 (capacity of ℓ3), f2 cannot be accommodated at ℓ3\n6. f2 would then be rejected from", "model_answer": "I need to analyze how the KDA algorithm would work when family f2 misreports its preferences.\n\nGiven information:\n- f2 has size 2\n- f2's true preferences: ℓ1, ℓ3, ℓ4, ...\n- f2's reported preferences: ℓ3, ℓ1, ℓ4, ...\n- Other families report truthfully:\n  - f1: ℓ2, ℓ1, ...\n  - f3: ℓ1, ℓ2, ...\n  - f4: ℓ1, ℓ3, ...\n\nLocality capacities and priorities:\n- ℓ1 (cap 2): f1 ≻ f2 ≻ f3 ≻ f4\n- ℓ2 (cap 1): f3 ≻ f1 ...\n- ℓ3 (cap 2): f4 ≻ f2 ...\n- ℓ4 (cap 5): (priorities not fully specified but has large capacity)\n\nIn the KDA algorithm, families apply to localities in order of their reported preferences, and localities tentatively accept families based on their priority rankings and capacity constraints.\n\nWhen f2 misreports and puts ℓ3 first:\n\n1. f2 (size 2) will first apply to ℓ3\n2. At ℓ3 (cap 2), the priority is f4 ≻ f2\n3. If f4 also applies to ℓ3 (which it does, since f4's true preference includes ℓ3), f4 has higher priority\n4. However, f4 has size 1, so there's still capacity for f2 (size 2) at ℓ3 if the total doesn't exceed capacity 2\n5. Since f4 (size 1) + f2 (size 2) = 3 > 2 (capacity of ℓ3), f2 cannot be accommodated at ℓ3\n6. f2 would then be rejected from", "error": "Warning: Could not extract single choice letter from: I need to analyze how the KDA algorithm would work..."}}
{"ID": 384, "Question": "### Background\n\nAn upstream monopolist sets discriminatory prices, $w_1$ for the multi-market “chain” and $w_2$ for the “local store”. The chain will integrate if its profit from doing so, $\\pi^e$, is sufficiently high relative to its profit from not integrating, $\\pi$.\n\n### Data / Model Specification\n\nThe “integration frontier,” denoted $w_1 = I[w_2]$, represents the price pairs $(w_1, w_2)$ where the chain is exactly indifferent between integrating and not integrating. The slope of this frontier is crucial for the monopolist's strategy.\n\n### Question\n\nThe paper describes a scenario where the integration frontier, $w_1 = I[w_2]$, is downward sloping. What is the key economic mechanism that would cause this relationship?", "Options": {"A": "The chain interprets a higher price to the local store ($w_2$) as a signal of the monopolist's weakness, which significantly increases the chain's expected post-integration profits ($\\pi^e$).", "B": "A higher price to the local store ($w_2$) weakens the local store as a competitor, increasing the chain's non-integration profits ($\\pi$) and making integration less attractive.", "C": "The monopolist's profit function is quasi-concave, meaning it must lower $w_1$ to maintain profits when it raises $w_2$.", "D": "A higher price to the chain ($w_1$) lowers its non-integration profits ($\\pi$), which must be offset by a lower price to the local store ($w_2$) to prevent integration."}, "Answer": "A", "pi_justification": "This item uses a Reverse-Reasoning strategy to assess understanding of the paper's most subtle mechanism: the signaling effect of prices on the integration decision. It replaces Q3 of the original QA problem. The correct answer (A) identifies the signaling channel. Distractor (B) describes the 'natural' case that leads to an *upward* sloping frontier (Conceptual Opposite). Distractor (C) is True but Irrelevant, confusing the monopolist's iso-profit curve with the chain's iso-profit (integration) frontier. Distractor (D) misidentifies the relationship between the variables.", "qid": "384", "question": "### Background\n\nAn upstream monopolist sets discriminatory prices, $w_1$ for the multi-market “chain” and $w_2$ for the “local store”. The chain will integrate if its profit from doing so, $\\pi^e$, is sufficiently high relative to its profit from not integrating, $\\pi$.\n\n### Data / Model Specification\n\nThe “integration frontier,” denoted $w_1 = I[w_2]$, represents the price pairs $(w_1, w_2)$ where the chain is exactly indifferent between integrating and not integrating. The slope of this frontier is crucial for the monopolist's strategy.\n\n### Question\n\nThe paper describes a scenario where the integration frontier, $w_1 = I[w_2]$, is downward sloping. What is the key economic mechanism that would cause this relationship?", "model_evaluation_result": {"model_raw_response": "Looking at this question, I need to understand what causes a downward-sloping integration frontier where $w_1 = I[w_2]$ has a negative slope.\n\nThe integration frontier represents combinations of prices $(w_1, w_2)$ where the chain is indifferent between integrating and not integrating. This means:\n$\\pi^e - \\pi = 0$ (constant along the frontier)\n\nFor the frontier to be downward sloping, when $w_2$ increases, $w_1$ must decrease to maintain indifference.\n\nLet me analyze each option:\n\nA) This suggests $w_2$ affects $\\pi^e$ through signaling. While possible, this seems like a secondary effect rather than the main economic mechanism.\n\nB) This describes how $w_2$ affects $\\pi$ (non-integration profits). When $w_2$ rises, the local store becomes a weaker competitor, which increases the chain's profits from not integrating ($\\pi$ rises). To maintain indifference ($\\pi^e - \\pi = 0$), since $\\pi$ increased, we'd need $\\pi^e$ to increase by the same amount or $w_1$ to decrease to reduce $\\pi$ back down. This creates the negative relationship between $w_1$ and $w_2$.\n\nC) This focuses on the monopolist's profit function properties, but the question is about the chain's integration decision, not the monopolist's profit maximization.\n\nD) This suggests $w_1$ affects $\\pi$ and must be offset by changes in $w_2$. However, this describes how $w_1$ changes affect the need for $w_2$ adjustments, not the fundamental mechanism causing the downward slope.\n\nThe key insight is that the integration frontier slopes downward when there's a competitive effect: higher prices to the local store weaken it as a competitor, making non-integration more profitable for the chain, which then requires lower prices to the chain to maintain indifference.\n\nB", "model_answer": "A", "error": null}}
{"ID": 359, "Question": "### Background\n\n**Research Question.** This problem requires you to analyze a re-examination of a field experiment that estimated consumer valuation for a new product packaging (vacuum skin) for beef steaks relative to a standard packaging (styrofoam). The analysis tests for the presence of two methodological confounds: field-price censoring and affiliated beliefs.\n\n**Setting / Institutional Environment.** In the original experiment, subjects submitted bids for beef steaks in a series of auctions. A key institutional detail is that subjects were explicitly told that the standard steak \"currently retail for about $6.00 a pound at major supermarkets.\" The experiment included three information treatments regarding the new vacuum-packed product: a baseline with no extra information, an \"Info\" treatment describing its virtues, and a \"Demo\" treatment that included a demonstration. After each of the later auction periods, the market-clearing price was announced to all participants.\n\n**Variables & Parameters.**\n*   `Bid`: Subject's bid in dollars per pound (dependent variable).\n*   `Vacuum`: Indicator variable; 1 if the product is the new vacuum skin package, 0 for standard styrofoam.\n*   `Info`: Indicator variable; 1 if subjects received the \"information\" treatment.\n*   `Demo`: Indicator variable; 1 if subjects received the \"demonstration\" treatment.\n*   `VacInfo`: Interaction term `Vacuum` × `Info`.\n*   `VacDemo`: Interaction term `Vacuum` × `Demo`.\n*   `priceL1`: The market-clearing (selling) price from the previous auction period, `t-1`.\n\n### Data / Model Specification\n\nTable 1 and Table 2 below present results from the re-analysis. Panel A of Table 1 uses a Panel Generalized Least Squares (GLS) model, which ignores censoring and affiliation effects. Panel C of Table 1 uses a Panel Tobit model, which accounts for the censoring of bids at the known $6.00 field price and includes controls for affiliation effects (market prices). The Panel Tobit model is considered the most appropriate specification.\n\n**Table 1. Estimated effects of experimental treatments on bids for meat**\n(Estimates obtained using random effects specification)\n\n| Treatment | Coef. | Std. err. | Z | P>|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| **A. Panel GLS** | | | | | | |\n| Vacuum | -.0549387 | .0333573 | -1.65 | 0.100 | -.1203179 | .0104404 |\n| Info | .1824116 | .0987915 | 1.85 | 0.065 | -.0112161 | .3760393 |\n| VacInfo | .449179 | .0470355 | 9.55 | 0.000 | .3569912 | .5413669 |\n| VacDemo | .0739316 | .0468498 | 1.58 | 0.115 | -.0178924 | .1657555 |\n| **C. Panel Tobit Model with market price effects** | | | | | | |\n| Vacuum | -.0440431 | .0346063 | -1.27 | 0.203 | -.1118703 | .023784 |\n| Info | -.2454266 | .0623487 | -3.94 | 0.000 | -.3676278 | -.1232254 |\n| VacInfo | .4198155 | .0488044 | 8.60 | 0.000 | .3241607 | .5154703 |\n| VacDemo | .0686025 | .0484393 | 1.42 | 0.157 | -.0263367 | .1635418 |\n\n**Table 2. Estimated effects of market prices on bids for meat**\n(Estimates obtained using Panel Tobit specification with random effects)\n\n| Variable | Coef. | Std. err. | Z | P >|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| tbid_vs | .3996299 | .0247609 | 16.14 | 0.000 | .3510994 | .4481605 |\n| tbid-st | .2883476 | .0249211 | 11.57 | 0.000 | .2395032 | .337192 |\n| priceL1 | .3555019 | .0172282 | 20.63 | 0.000 | .3217353 | .3892686 |\n\nUsing the results from the preferred specification in Panel C of Table 1, what is the estimated net marginal valuation of the vacuum skin packaging (relative to styrofoam) when the 'Info' treatment is provided?", "Options": {"A": "It is valued approximately $0.38 more per pound.", "B": "It is valued approximately $0.42 more per pound.", "C": "It is valued approximately $0.39 more per pound.", "D": "It is valued approximately $0.25 less per pound."}, "Answer": "A", "pi_justification": "Judgment: REPLACE with Choice Questions (Score: 9.5). This item tests the ability to correctly calculate and interpret the net effect of a treatment that includes an interaction term, a critical skill in reading regression tables. It uses a Computational Judgment strategy. Distractors are designed based on common errors: ignoring the main effect (B), using coefficients from an inappropriate model specification (C), and misapplying an unrelated coefficient (D).", "qid": "359", "question": "### Background\n\n**Research Question.** This problem requires you to analyze a re-examination of a field experiment that estimated consumer valuation for a new product packaging (vacuum skin) for beef steaks relative to a standard packaging (styrofoam). The analysis tests for the presence of two methodological confounds: field-price censoring and affiliated beliefs.\n\n**Setting / Institutional Environment.** In the original experiment, subjects submitted bids for beef steaks in a series of auctions. A key institutional detail is that subjects were explicitly told that the standard steak \"currently retail for about $6.00 a pound at major supermarkets.\" The experiment included three information treatments regarding the new vacuum-packed product: a baseline with no extra information, an \"Info\" treatment describing its virtues, and a \"Demo\" treatment that included a demonstration. After each of the later auction periods, the market-clearing price was announced to all participants.\n\n**Variables & Parameters.**\n*   `Bid`: Subject's bid in dollars per pound (dependent variable).\n*   `Vacuum`: Indicator variable; 1 if the product is the new vacuum skin package, 0 for standard styrofoam.\n*   `Info`: Indicator variable; 1 if subjects received the \"information\" treatment.\n*   `Demo`: Indicator variable; 1 if subjects received the \"demonstration\" treatment.\n*   `VacInfo`: Interaction term `Vacuum` × `Info`.\n*   `VacDemo`: Interaction term `Vacuum` × `Demo`.\n*   `priceL1`: The market-clearing (selling) price from the previous auction period, `t-1`.\n\n### Data / Model Specification\n\nTable 1 and Table 2 below present results from the re-analysis. Panel A of Table 1 uses a Panel Generalized Least Squares (GLS) model, which ignores censoring and affiliation effects. Panel C of Table 1 uses a Panel Tobit model, which accounts for the censoring of bids at the known $6.00 field price and includes controls for affiliation effects (market prices). The Panel Tobit model is considered the most appropriate specification.\n\n**Table 1. Estimated effects of experimental treatments on bids for meat**\n(Estimates obtained using random effects specification)\n\n| Treatment | Coef. | Std. err. | Z | P>|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| **A. Panel GLS** | | | | | | |\n| Vacuum | -.0549387 | .0333573 | -1.65 | 0.100 | -.1203179 | .0104404 |\n| Info | .1824116 | .0987915 | 1.85 | 0.065 | -.0112161 | .3760393 |\n| VacInfo | .449179 | .0470355 | 9.55 | 0.000 | .3569912 | .5413669 |\n| VacDemo | .0739316 | .0468498 | 1.58 | 0.115 | -.0178924 | .1657555 |\n| **C. Panel Tobit Model with market price effects** | | | | | | |\n| Vacuum | -.0440431 | .0346063 | -1.27 | 0.203 | -.1118703 | .023784 |\n| Info | -.2454266 | .0623487 | -3.94 | 0.000 | -.3676278 | -.1232254 |\n| VacInfo | .4198155 | .0488044 | 8.60 | 0.000 | .3241607 | .5154703 |\n| VacDemo | .0686025 | .0484393 | 1.42 | 0.157 | -.0263367 | .1635418 |\n\n**Table 2. Estimated effects of market prices on bids for meat**\n(Estimates obtained using Panel Tobit specification with random effects)\n\n| Variable | Coef. | Std. err. | Z | P >|z| | (95% Conf. interval) |\n|---|---|---|---|---|---|---|\n| tbid_vs | .3996299 | .0247609 | 16.14 | 0.000 | .3510994 | .4481605 |\n| tbid-st | .2883476 | .0249211 | 11.57 | 0.000 | .2395032 | .337192 |\n| priceL1 | .3555019 | .0172282 | 20.63 | 0.000 | .3217353 | .3892686 |\n\nUsing the results from the preferred specification in Panel C of Table 1, what is the estimated net marginal valuation of the vacuum skin packaging (relative to styrofoam) when the 'Info' treatment is provided?", "model_evaluation_result": {"model_raw_response": "I need to calculate the net marginal valuation of vacuum skin packaging relative to styrofoam when the 'Info' treatment is provided.\n\nFrom Panel C of Table 1 (Panel Tobit Model), I have:\n- Vacuum coefficient: -0.0440431\n- VacInfo coefficient: 0.4198155\n\nWhen the 'Info' treatment is provided, the total effect of vacuum packaging relative to styrofoam is:\n\nNet effect = Vacuum coefficient + VacInfo coefficient\nNet effect = -0.0440431 + 0.4198155\nNet effect = 0.3757724\n\nThis means the vacuum skin packaging is valued approximately $0.38 more per pound when the Info treatment is provided.\n\nA", "model_answer": "C", "error": null}}
